<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250917.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian\n  Head", "author": "Zhiling Ye and Cong Zhou and Xiubao Zhang and Haifeng Shen and Weihong Deng and Quan Lu", "abstract": "  In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2509.05582v2", "date": "2025-09-17", "relevancy": 3.6535, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7545}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20and%20Reenactment%20Separated%20Method%20for%20Realistic%20Gaussian%0A%20%20Head&body=Title%3A%20Reconstruction%20and%20Reenactment%20Separated%20Method%20for%20Realistic%20Gaussian%0A%20%20Head%0AAuthor%3A%20Zhiling%20Ye%20and%20Cong%20Zhou%20and%20Xiubao%20Zhang%20and%20Haifeng%20Shen%20and%20Weihong%20Deng%20and%20Quan%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20a%20reconstruction%20and%20reenactment%20separated%0Aframework%20for%203D%20Gaussians%20head%2C%20which%20requires%20only%20a%20single%20portrait%20image%20as%0Ainput%20to%20generate%20controllable%20avatar.%20Specifically%2C%20we%20developed%20a%20large-scale%0Aone-shot%20gaussian%20head%20generator%20built%20upon%20WebSSL%20and%20employed%20a%20two-stage%0Atraining%20approach%20that%20significantly%20enhances%20the%20capabilities%20of%0Ageneralization%20and%20high-frequency%20texture%20reconstruction.%20During%20inference%2C%20an%0Aultra-lightweight%20gaussian%20avatar%20driven%20by%20control%20signals%20enables%20high%0Aframe-rate%20rendering%2C%20achieving%2090%20FPS%20at%20a%20resolution%20of%20512x512.%20We%20further%0Ademonstrate%20that%20the%20proposed%20framework%20follows%20the%20scaling%20law%2C%20whereby%0Aincreasing%20the%20parameter%20scale%20of%20the%20reconstruction%20module%20leads%20to%20improved%0Aperformance.%20Moreover%2C%20thanks%20to%20the%20separation%20design%2C%20driving%20efficiency%0Aremains%20unaffected.%20Finally%2C%20extensive%20quantitative%20and%20qualitative%20experiments%0Avalidate%20that%20our%20approach%20outperforms%20current%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520and%2520Reenactment%2520Separated%2520Method%2520for%2520Realistic%2520Gaussian%250A%2520%2520Head%26entry.906535625%3DZhiling%2520Ye%2520and%2520Cong%2520Zhou%2520and%2520Xiubao%2520Zhang%2520and%2520Haifeng%2520Shen%2520and%2520Weihong%2520Deng%2520and%2520Quan%2520Lu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520reconstruction%2520and%2520reenactment%2520separated%250Aframework%2520for%25203D%2520Gaussians%2520head%252C%2520which%2520requires%2520only%2520a%2520single%2520portrait%2520image%2520as%250Ainput%2520to%2520generate%2520controllable%2520avatar.%2520Specifically%252C%2520we%2520developed%2520a%2520large-scale%250Aone-shot%2520gaussian%2520head%2520generator%2520built%2520upon%2520WebSSL%2520and%2520employed%2520a%2520two-stage%250Atraining%2520approach%2520that%2520significantly%2520enhances%2520the%2520capabilities%2520of%250Ageneralization%2520and%2520high-frequency%2520texture%2520reconstruction.%2520During%2520inference%252C%2520an%250Aultra-lightweight%2520gaussian%2520avatar%2520driven%2520by%2520control%2520signals%2520enables%2520high%250Aframe-rate%2520rendering%252C%2520achieving%252090%2520FPS%2520at%2520a%2520resolution%2520of%2520512x512.%2520We%2520further%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520follows%2520the%2520scaling%2520law%252C%2520whereby%250Aincreasing%2520the%2520parameter%2520scale%2520of%2520the%2520reconstruction%2520module%2520leads%2520to%2520improved%250Aperformance.%2520Moreover%252C%2520thanks%2520to%2520the%2520separation%2520design%252C%2520driving%2520efficiency%250Aremains%2520unaffected.%2520Finally%252C%2520extensive%2520quantitative%2520and%2520qualitative%2520experiments%250Avalidate%2520that%2520our%2520approach%2520outperforms%2520current%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20and%20Reenactment%20Separated%20Method%20for%20Realistic%20Gaussian%0A%20%20Head&entry.906535625=Zhiling%20Ye%20and%20Cong%20Zhou%20and%20Xiubao%20Zhang%20and%20Haifeng%20Shen%20and%20Weihong%20Deng%20and%20Quan%20Lu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20a%20reconstruction%20and%20reenactment%20separated%0Aframework%20for%203D%20Gaussians%20head%2C%20which%20requires%20only%20a%20single%20portrait%20image%20as%0Ainput%20to%20generate%20controllable%20avatar.%20Specifically%2C%20we%20developed%20a%20large-scale%0Aone-shot%20gaussian%20head%20generator%20built%20upon%20WebSSL%20and%20employed%20a%20two-stage%0Atraining%20approach%20that%20significantly%20enhances%20the%20capabilities%20of%0Ageneralization%20and%20high-frequency%20texture%20reconstruction.%20During%20inference%2C%20an%0Aultra-lightweight%20gaussian%20avatar%20driven%20by%20control%20signals%20enables%20high%0Aframe-rate%20rendering%2C%20achieving%2090%20FPS%20at%20a%20resolution%20of%20512x512.%20We%20further%0Ademonstrate%20that%20the%20proposed%20framework%20follows%20the%20scaling%20law%2C%20whereby%0Aincreasing%20the%20parameter%20scale%20of%20the%20reconstruction%20module%20leads%20to%20improved%0Aperformance.%20Moreover%2C%20thanks%20to%20the%20separation%20design%2C%20driving%20efficiency%0Aremains%20unaffected.%20Finally%2C%20extensive%20quantitative%20and%20qualitative%20experiments%0Avalidate%20that%20our%20approach%20outperforms%20current%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05582v2&entry.124074799=Read"},
{"title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for\n  High-Fidelity Mapping", "author": "Zhihao Cao and Hanyu Wu and Li Wa Tang and Zizhou Luo and Zihan Zhu and Wei Zhang and Marc Pollefeys and Martin R. Oswald", "abstract": "  Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.\n", "link": "http://arxiv.org/abs/2509.14191v1", "date": "2025-09-17", "relevancy": 3.5377, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6673}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCGS-SLAM%3A%20A%20Multi-Camera%20SLAM%20Framework%20Using%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Mapping&body=Title%3A%20MCGS-SLAM%3A%20A%20Multi-Camera%20SLAM%20Framework%20Using%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Mapping%0AAuthor%3A%20Zhihao%20Cao%20and%20Hanyu%20Wu%20and%20Li%20Wa%20Tang%20and%20Zizhou%20Luo%20and%20Zihan%20Zhu%20and%20Wei%20Zhang%20and%20Marc%20Pollefeys%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Recent%20progress%20in%20dense%20SLAM%20has%20primarily%20targeted%20monocular%20setups%2C%20often%0Aat%20the%20expense%20of%20robustness%20and%20geometric%20coverage.%20We%20present%20MCGS-SLAM%2C%20the%0Afirst%20purely%20RGB-based%20multi-camera%20SLAM%20system%20built%20on%203D%20Gaussian%20Splatting%0A%283DGS%29.%20Unlike%20prior%20methods%20relying%20on%20sparse%20maps%20or%20inertial%20data%2C%20MCGS-SLAM%0Afuses%20dense%20RGB%20inputs%20from%20multiple%20viewpoints%20into%20a%20unified%2C%20continuously%0Aoptimized%20Gaussian%20map.%20A%20multi-camera%20bundle%20adjustment%20%28MCBA%29%20jointly%20refines%0Aposes%20and%20depths%20via%20dense%20photometric%20and%20geometric%20residuals%2C%20while%20a%20scale%0Aconsistency%20module%20enforces%20metric%20alignment%20across%20views%20using%20low-rank%0Apriors.%20The%20system%20supports%20RGB%20input%20and%20maintains%20real-time%20performance%20at%0Alarge%20scale.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%0AMCGS-SLAM%20consistently%20yields%20accurate%20trajectories%20and%20photorealistic%0Areconstructions%2C%20usually%20outperforming%20monocular%20baselines.%20Notably%2C%20the%20wide%0Afield%20of%20view%20from%20multi-camera%20input%20enables%20reconstruction%20of%20side-view%0Aregions%20that%20monocular%20setups%20miss%2C%20critical%20for%20safe%20autonomous%20operation.%0AThese%20results%20highlight%20the%20promise%20of%20multi-camera%20Gaussian%20Splatting%20SLAM%20for%0Ahigh-fidelity%20mapping%20in%20robotics%20and%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCGS-SLAM%253A%2520A%2520Multi-Camera%2520SLAM%2520Framework%2520Using%2520Gaussian%2520Splatting%2520for%250A%2520%2520High-Fidelity%2520Mapping%26entry.906535625%3DZhihao%2520Cao%2520and%2520Hanyu%2520Wu%2520and%2520Li%2520Wa%2520Tang%2520and%2520Zizhou%2520Luo%2520and%2520Zihan%2520Zhu%2520and%2520Wei%2520Zhang%2520and%2520Marc%2520Pollefeys%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520dense%2520SLAM%2520has%2520primarily%2520targeted%2520monocular%2520setups%252C%2520often%250Aat%2520the%2520expense%2520of%2520robustness%2520and%2520geometric%2520coverage.%2520We%2520present%2520MCGS-SLAM%252C%2520the%250Afirst%2520purely%2520RGB-based%2520multi-camera%2520SLAM%2520system%2520built%2520on%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529.%2520Unlike%2520prior%2520methods%2520relying%2520on%2520sparse%2520maps%2520or%2520inertial%2520data%252C%2520MCGS-SLAM%250Afuses%2520dense%2520RGB%2520inputs%2520from%2520multiple%2520viewpoints%2520into%2520a%2520unified%252C%2520continuously%250Aoptimized%2520Gaussian%2520map.%2520A%2520multi-camera%2520bundle%2520adjustment%2520%2528MCBA%2529%2520jointly%2520refines%250Aposes%2520and%2520depths%2520via%2520dense%2520photometric%2520and%2520geometric%2520residuals%252C%2520while%2520a%2520scale%250Aconsistency%2520module%2520enforces%2520metric%2520alignment%2520across%2520views%2520using%2520low-rank%250Apriors.%2520The%2520system%2520supports%2520RGB%2520input%2520and%2520maintains%2520real-time%2520performance%2520at%250Alarge%2520scale.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520that%250AMCGS-SLAM%2520consistently%2520yields%2520accurate%2520trajectories%2520and%2520photorealistic%250Areconstructions%252C%2520usually%2520outperforming%2520monocular%2520baselines.%2520Notably%252C%2520the%2520wide%250Afield%2520of%2520view%2520from%2520multi-camera%2520input%2520enables%2520reconstruction%2520of%2520side-view%250Aregions%2520that%2520monocular%2520setups%2520miss%252C%2520critical%2520for%2520safe%2520autonomous%2520operation.%250AThese%2520results%2520highlight%2520the%2520promise%2520of%2520multi-camera%2520Gaussian%2520Splatting%2520SLAM%2520for%250Ahigh-fidelity%2520mapping%2520in%2520robotics%2520and%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCGS-SLAM%3A%20A%20Multi-Camera%20SLAM%20Framework%20Using%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Mapping&entry.906535625=Zhihao%20Cao%20and%20Hanyu%20Wu%20and%20Li%20Wa%20Tang%20and%20Zizhou%20Luo%20and%20Zihan%20Zhu%20and%20Wei%20Zhang%20and%20Marc%20Pollefeys%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Recent%20progress%20in%20dense%20SLAM%20has%20primarily%20targeted%20monocular%20setups%2C%20often%0Aat%20the%20expense%20of%20robustness%20and%20geometric%20coverage.%20We%20present%20MCGS-SLAM%2C%20the%0Afirst%20purely%20RGB-based%20multi-camera%20SLAM%20system%20built%20on%203D%20Gaussian%20Splatting%0A%283DGS%29.%20Unlike%20prior%20methods%20relying%20on%20sparse%20maps%20or%20inertial%20data%2C%20MCGS-SLAM%0Afuses%20dense%20RGB%20inputs%20from%20multiple%20viewpoints%20into%20a%20unified%2C%20continuously%0Aoptimized%20Gaussian%20map.%20A%20multi-camera%20bundle%20adjustment%20%28MCBA%29%20jointly%20refines%0Aposes%20and%20depths%20via%20dense%20photometric%20and%20geometric%20residuals%2C%20while%20a%20scale%0Aconsistency%20module%20enforces%20metric%20alignment%20across%20views%20using%20low-rank%0Apriors.%20The%20system%20supports%20RGB%20input%20and%20maintains%20real-time%20performance%20at%0Alarge%20scale.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%0AMCGS-SLAM%20consistently%20yields%20accurate%20trajectories%20and%20photorealistic%0Areconstructions%2C%20usually%20outperforming%20monocular%20baselines.%20Notably%2C%20the%20wide%0Afield%20of%20view%20from%20multi-camera%20input%20enables%20reconstruction%20of%20side-view%0Aregions%20that%20monocular%20setups%20miss%2C%20critical%20for%20safe%20autonomous%20operation.%0AThese%20results%20highlight%20the%20promise%20of%20multi-camera%20Gaussian%20Splatting%20SLAM%20for%0Ahigh-fidelity%20mapping%20in%20robotics%20and%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14191v1&entry.124074799=Read"},
{"title": "Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images", "author": "Simon Niedermayr and Christoph Neuhauser R\u00fcdiger Westermann", "abstract": "  We introduce an image upscaling technique tailored for 3D Gaussian Splatting\n(3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher\nrendering speeds and reduces artifacts commonly observed in 3DGS\nreconstructions. Our technique upscales low-resolution 3DGS renderings with a\nmarginal increase in cost by directly leveraging the analytical image gradients\nof Gaussians for gradient-based bicubic spline interpolation. The technique is\nagnostic to the specific 3DGS implementation, achieving novel view synthesis at\nrates 3x-4x higher than the baseline implementation. Through extensive\nexperiments on multiple datasets, we showcase the performance improvements and\nhigh reconstruction fidelity attainable with gradient-aware upscaling of 3DGS\nimages. We further demonstrate the integration of gradient-aware upscaling into\nthe gradient-based optimization of a 3DGS model and analyze its effects on\nreconstruction quality and performance.\n", "link": "http://arxiv.org/abs/2503.14171v2", "date": "2025-09-17", "relevancy": 3.3336, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6986}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Gradient-Aware%20Upscaling%20of%203D%20Gaussian%20Splatting%20Images&body=Title%3A%20Lightweight%20Gradient-Aware%20Upscaling%20of%203D%20Gaussian%20Splatting%20Images%0AAuthor%3A%20Simon%20Niedermayr%20and%20Christoph%20Neuhauser%20R%C3%BCdiger%20Westermann%0AAbstract%3A%20%20%20We%20introduce%20an%20image%20upscaling%20technique%20tailored%20for%203D%20Gaussian%20Splatting%0A%283DGS%29%20on%20lightweight%20GPUs.%20Compared%20to%203DGS%2C%20it%20achieves%20significantly%20higher%0Arendering%20speeds%20and%20reduces%20artifacts%20commonly%20observed%20in%203DGS%0Areconstructions.%20Our%20technique%20upscales%20low-resolution%203DGS%20renderings%20with%20a%0Amarginal%20increase%20in%20cost%20by%20directly%20leveraging%20the%20analytical%20image%20gradients%0Aof%20Gaussians%20for%20gradient-based%20bicubic%20spline%20interpolation.%20The%20technique%20is%0Aagnostic%20to%20the%20specific%203DGS%20implementation%2C%20achieving%20novel%20view%20synthesis%20at%0Arates%203x-4x%20higher%20than%20the%20baseline%20implementation.%20Through%20extensive%0Aexperiments%20on%20multiple%20datasets%2C%20we%20showcase%20the%20performance%20improvements%20and%0Ahigh%20reconstruction%20fidelity%20attainable%20with%20gradient-aware%20upscaling%20of%203DGS%0Aimages.%20We%20further%20demonstrate%20the%20integration%20of%20gradient-aware%20upscaling%20into%0Athe%20gradient-based%20optimization%20of%20a%203DGS%20model%20and%20analyze%20its%20effects%20on%0Areconstruction%20quality%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Gradient-Aware%2520Upscaling%2520of%25203D%2520Gaussian%2520Splatting%2520Images%26entry.906535625%3DSimon%2520Niedermayr%2520and%2520Christoph%2520Neuhauser%2520R%25C3%25BCdiger%2520Westermann%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520image%2520upscaling%2520technique%2520tailored%2520for%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520on%2520lightweight%2520GPUs.%2520Compared%2520to%25203DGS%252C%2520it%2520achieves%2520significantly%2520higher%250Arendering%2520speeds%2520and%2520reduces%2520artifacts%2520commonly%2520observed%2520in%25203DGS%250Areconstructions.%2520Our%2520technique%2520upscales%2520low-resolution%25203DGS%2520renderings%2520with%2520a%250Amarginal%2520increase%2520in%2520cost%2520by%2520directly%2520leveraging%2520the%2520analytical%2520image%2520gradients%250Aof%2520Gaussians%2520for%2520gradient-based%2520bicubic%2520spline%2520interpolation.%2520The%2520technique%2520is%250Aagnostic%2520to%2520the%2520specific%25203DGS%2520implementation%252C%2520achieving%2520novel%2520view%2520synthesis%2520at%250Arates%25203x-4x%2520higher%2520than%2520the%2520baseline%2520implementation.%2520Through%2520extensive%250Aexperiments%2520on%2520multiple%2520datasets%252C%2520we%2520showcase%2520the%2520performance%2520improvements%2520and%250Ahigh%2520reconstruction%2520fidelity%2520attainable%2520with%2520gradient-aware%2520upscaling%2520of%25203DGS%250Aimages.%2520We%2520further%2520demonstrate%2520the%2520integration%2520of%2520gradient-aware%2520upscaling%2520into%250Athe%2520gradient-based%2520optimization%2520of%2520a%25203DGS%2520model%2520and%2520analyze%2520its%2520effects%2520on%250Areconstruction%2520quality%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Gradient-Aware%20Upscaling%20of%203D%20Gaussian%20Splatting%20Images&entry.906535625=Simon%20Niedermayr%20and%20Christoph%20Neuhauser%20R%C3%BCdiger%20Westermann&entry.1292438233=%20%20We%20introduce%20an%20image%20upscaling%20technique%20tailored%20for%203D%20Gaussian%20Splatting%0A%283DGS%29%20on%20lightweight%20GPUs.%20Compared%20to%203DGS%2C%20it%20achieves%20significantly%20higher%0Arendering%20speeds%20and%20reduces%20artifacts%20commonly%20observed%20in%203DGS%0Areconstructions.%20Our%20technique%20upscales%20low-resolution%203DGS%20renderings%20with%20a%0Amarginal%20increase%20in%20cost%20by%20directly%20leveraging%20the%20analytical%20image%20gradients%0Aof%20Gaussians%20for%20gradient-based%20bicubic%20spline%20interpolation.%20The%20technique%20is%0Aagnostic%20to%20the%20specific%203DGS%20implementation%2C%20achieving%20novel%20view%20synthesis%20at%0Arates%203x-4x%20higher%20than%20the%20baseline%20implementation.%20Through%20extensive%0Aexperiments%20on%20multiple%20datasets%2C%20we%20showcase%20the%20performance%20improvements%20and%0Ahigh%20reconstruction%20fidelity%20attainable%20with%20gradient-aware%20upscaling%20of%203DGS%0Aimages.%20We%20further%20demonstrate%20the%20integration%20of%20gradient-aware%20upscaling%20into%0Athe%20gradient-based%20optimization%20of%20a%203DGS%20model%20and%20analyze%20its%20effects%20on%0Areconstruction%20quality%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14171v2&entry.124074799=Read"},
{"title": "UniPLV: Towards Label-Efficient Open-World 3D Scene Understanding by\n  Regional Visual Language Supervision", "author": "Yuru Wang and Pei Liu and Songtao Wang and Zehan Zhang and Xinyan Lu and Changwei Cai and Hao Li and Fu Liu and Peng Jia and Xianpeng Lang", "abstract": "  Open-world 3D scene understanding is a critical challenge that involves\nrecognizing and distinguishing diverse objects and categories from 3D data,\nsuch as point clouds, without relying on manual annotations. Traditional\nmethods struggle with this open-world task, especially due to the limitations\nof constructing extensive point cloud-text pairs and handling multimodal data\neffectively. In response to these challenges, we present UniPLV, a robust\nframework that unifies point clouds, images, and text within a single learning\nparadigm for comprehensive 3D scene understanding. UniPLV leverages images as a\nbridge to co-embed 3D points with pre-aligned images and text in a shared\nfeature space, eliminating the need for labor-intensive point cloud-text pair\ncrafting. Our framework achieves precise multimodal alignment through two\ninnovative strategies: (i) Logit and feature distillation modules between\nimages and point clouds to enhance feature coherence; (ii) A vision-point\nmatching module that implicitly corrects 3D semantic predictions affected by\nprojection inaccuracies from points to pixels. To further boost performance, we\nimplement four task-specific losses alongside a two-stage training strategy.\nExtensive experiments demonstrate that UniPLV significantly surpasses\nstate-of-the-art methods, with average improvements of 15.6% and 14.8% in\nsemantic segmentation for Base-Annotated and Annotation-Free tasks,\nrespectively. These results underscore UniPLV's efficacy in pushing the\nboundaries of open-world 3D scene understanding. We will release the code to\nsupport future research and development.\n", "link": "http://arxiv.org/abs/2412.18131v2", "date": "2025-09-17", "relevancy": 3.306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPLV%3A%20Towards%20Label-Efficient%20Open-World%203D%20Scene%20Understanding%20by%0A%20%20Regional%20Visual%20Language%20Supervision&body=Title%3A%20UniPLV%3A%20Towards%20Label-Efficient%20Open-World%203D%20Scene%20Understanding%20by%0A%20%20Regional%20Visual%20Language%20Supervision%0AAuthor%3A%20Yuru%20Wang%20and%20Pei%20Liu%20and%20Songtao%20Wang%20and%20Zehan%20Zhang%20and%20Xinyan%20Lu%20and%20Changwei%20Cai%20and%20Hao%20Li%20and%20Fu%20Liu%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%0AAbstract%3A%20%20%20Open-world%203D%20scene%20understanding%20is%20a%20critical%20challenge%20that%20involves%0Arecognizing%20and%20distinguishing%20diverse%20objects%20and%20categories%20from%203D%20data%2C%0Asuch%20as%20point%20clouds%2C%20without%20relying%20on%20manual%20annotations.%20Traditional%0Amethods%20struggle%20with%20this%20open-world%20task%2C%20especially%20due%20to%20the%20limitations%0Aof%20constructing%20extensive%20point%20cloud-text%20pairs%20and%20handling%20multimodal%20data%0Aeffectively.%20In%20response%20to%20these%20challenges%2C%20we%20present%20UniPLV%2C%20a%20robust%0Aframework%20that%20unifies%20point%20clouds%2C%20images%2C%20and%20text%20within%20a%20single%20learning%0Aparadigm%20for%20comprehensive%203D%20scene%20understanding.%20UniPLV%20leverages%20images%20as%20a%0Abridge%20to%20co-embed%203D%20points%20with%20pre-aligned%20images%20and%20text%20in%20a%20shared%0Afeature%20space%2C%20eliminating%20the%20need%20for%20labor-intensive%20point%20cloud-text%20pair%0Acrafting.%20Our%20framework%20achieves%20precise%20multimodal%20alignment%20through%20two%0Ainnovative%20strategies%3A%20%28i%29%20Logit%20and%20feature%20distillation%20modules%20between%0Aimages%20and%20point%20clouds%20to%20enhance%20feature%20coherence%3B%20%28ii%29%20A%20vision-point%0Amatching%20module%20that%20implicitly%20corrects%203D%20semantic%20predictions%20affected%20by%0Aprojection%20inaccuracies%20from%20points%20to%20pixels.%20To%20further%20boost%20performance%2C%20we%0Aimplement%20four%20task-specific%20losses%20alongside%20a%20two-stage%20training%20strategy.%0AExtensive%20experiments%20demonstrate%20that%20UniPLV%20significantly%20surpasses%0Astate-of-the-art%20methods%2C%20with%20average%20improvements%20of%2015.6%25%20and%2014.8%25%20in%0Asemantic%20segmentation%20for%20Base-Annotated%20and%20Annotation-Free%20tasks%2C%0Arespectively.%20These%20results%20underscore%20UniPLV%27s%20efficacy%20in%20pushing%20the%0Aboundaries%20of%20open-world%203D%20scene%20understanding.%20We%20will%20release%20the%20code%20to%0Asupport%20future%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPLV%253A%2520Towards%2520Label-Efficient%2520Open-World%25203D%2520Scene%2520Understanding%2520by%250A%2520%2520Regional%2520Visual%2520Language%2520Supervision%26entry.906535625%3DYuru%2520Wang%2520and%2520Pei%2520Liu%2520and%2520Songtao%2520Wang%2520and%2520Zehan%2520Zhang%2520and%2520Xinyan%2520Lu%2520and%2520Changwei%2520Cai%2520and%2520Hao%2520Li%2520and%2520Fu%2520Liu%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%26entry.1292438233%3D%2520%2520Open-world%25203D%2520scene%2520understanding%2520is%2520a%2520critical%2520challenge%2520that%2520involves%250Arecognizing%2520and%2520distinguishing%2520diverse%2520objects%2520and%2520categories%2520from%25203D%2520data%252C%250Asuch%2520as%2520point%2520clouds%252C%2520without%2520relying%2520on%2520manual%2520annotations.%2520Traditional%250Amethods%2520struggle%2520with%2520this%2520open-world%2520task%252C%2520especially%2520due%2520to%2520the%2520limitations%250Aof%2520constructing%2520extensive%2520point%2520cloud-text%2520pairs%2520and%2520handling%2520multimodal%2520data%250Aeffectively.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520present%2520UniPLV%252C%2520a%2520robust%250Aframework%2520that%2520unifies%2520point%2520clouds%252C%2520images%252C%2520and%2520text%2520within%2520a%2520single%2520learning%250Aparadigm%2520for%2520comprehensive%25203D%2520scene%2520understanding.%2520UniPLV%2520leverages%2520images%2520as%2520a%250Abridge%2520to%2520co-embed%25203D%2520points%2520with%2520pre-aligned%2520images%2520and%2520text%2520in%2520a%2520shared%250Afeature%2520space%252C%2520eliminating%2520the%2520need%2520for%2520labor-intensive%2520point%2520cloud-text%2520pair%250Acrafting.%2520Our%2520framework%2520achieves%2520precise%2520multimodal%2520alignment%2520through%2520two%250Ainnovative%2520strategies%253A%2520%2528i%2529%2520Logit%2520and%2520feature%2520distillation%2520modules%2520between%250Aimages%2520and%2520point%2520clouds%2520to%2520enhance%2520feature%2520coherence%253B%2520%2528ii%2529%2520A%2520vision-point%250Amatching%2520module%2520that%2520implicitly%2520corrects%25203D%2520semantic%2520predictions%2520affected%2520by%250Aprojection%2520inaccuracies%2520from%2520points%2520to%2520pixels.%2520To%2520further%2520boost%2520performance%252C%2520we%250Aimplement%2520four%2520task-specific%2520losses%2520alongside%2520a%2520two-stage%2520training%2520strategy.%250AExtensive%2520experiments%2520demonstrate%2520that%2520UniPLV%2520significantly%2520surpasses%250Astate-of-the-art%2520methods%252C%2520with%2520average%2520improvements%2520of%252015.6%2525%2520and%252014.8%2525%2520in%250Asemantic%2520segmentation%2520for%2520Base-Annotated%2520and%2520Annotation-Free%2520tasks%252C%250Arespectively.%2520These%2520results%2520underscore%2520UniPLV%2527s%2520efficacy%2520in%2520pushing%2520the%250Aboundaries%2520of%2520open-world%25203D%2520scene%2520understanding.%2520We%2520will%2520release%2520the%2520code%2520to%250Asupport%2520future%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPLV%3A%20Towards%20Label-Efficient%20Open-World%203D%20Scene%20Understanding%20by%0A%20%20Regional%20Visual%20Language%20Supervision&entry.906535625=Yuru%20Wang%20and%20Pei%20Liu%20and%20Songtao%20Wang%20and%20Zehan%20Zhang%20and%20Xinyan%20Lu%20and%20Changwei%20Cai%20and%20Hao%20Li%20and%20Fu%20Liu%20and%20Peng%20Jia%20and%20Xianpeng%20Lang&entry.1292438233=%20%20Open-world%203D%20scene%20understanding%20is%20a%20critical%20challenge%20that%20involves%0Arecognizing%20and%20distinguishing%20diverse%20objects%20and%20categories%20from%203D%20data%2C%0Asuch%20as%20point%20clouds%2C%20without%20relying%20on%20manual%20annotations.%20Traditional%0Amethods%20struggle%20with%20this%20open-world%20task%2C%20especially%20due%20to%20the%20limitations%0Aof%20constructing%20extensive%20point%20cloud-text%20pairs%20and%20handling%20multimodal%20data%0Aeffectively.%20In%20response%20to%20these%20challenges%2C%20we%20present%20UniPLV%2C%20a%20robust%0Aframework%20that%20unifies%20point%20clouds%2C%20images%2C%20and%20text%20within%20a%20single%20learning%0Aparadigm%20for%20comprehensive%203D%20scene%20understanding.%20UniPLV%20leverages%20images%20as%20a%0Abridge%20to%20co-embed%203D%20points%20with%20pre-aligned%20images%20and%20text%20in%20a%20shared%0Afeature%20space%2C%20eliminating%20the%20need%20for%20labor-intensive%20point%20cloud-text%20pair%0Acrafting.%20Our%20framework%20achieves%20precise%20multimodal%20alignment%20through%20two%0Ainnovative%20strategies%3A%20%28i%29%20Logit%20and%20feature%20distillation%20modules%20between%0Aimages%20and%20point%20clouds%20to%20enhance%20feature%20coherence%3B%20%28ii%29%20A%20vision-point%0Amatching%20module%20that%20implicitly%20corrects%203D%20semantic%20predictions%20affected%20by%0Aprojection%20inaccuracies%20from%20points%20to%20pixels.%20To%20further%20boost%20performance%2C%20we%0Aimplement%20four%20task-specific%20losses%20alongside%20a%20two-stage%20training%20strategy.%0AExtensive%20experiments%20demonstrate%20that%20UniPLV%20significantly%20surpasses%0Astate-of-the-art%20methods%2C%20with%20average%20improvements%20of%2015.6%25%20and%2014.8%25%20in%0Asemantic%20segmentation%20for%20Base-Annotated%20and%20Annotation-Free%20tasks%2C%0Arespectively.%20These%20results%20underscore%20UniPLV%27s%20efficacy%20in%20pushing%20the%0Aboundaries%20of%20open-world%203D%20scene%20understanding.%20We%20will%20release%20the%20code%20to%0Asupport%20future%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18131v2&entry.124074799=Read"},
{"title": "An Exploratory Study on Abstract Images and Visual Representations\n  Learned from Them", "author": "Haotian Li and Jianbo Jiao", "abstract": "  Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.\n", "link": "http://arxiv.org/abs/2509.14149v1", "date": "2025-09-17", "relevancy": 2.9693, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6028}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Exploratory%20Study%20on%20Abstract%20Images%20and%20Visual%20Representations%0A%20%20Learned%20from%20Them&body=Title%3A%20An%20Exploratory%20Study%20on%20Abstract%20Images%20and%20Visual%20Representations%0A%20%20Learned%20from%20Them%0AAuthor%3A%20Haotian%20Li%20and%20Jianbo%20Jiao%0AAbstract%3A%20%20%20Imagine%20living%20in%20a%20world%20composed%20solely%20of%20primitive%20shapes%2C%20could%20you%0Astill%20recognise%20familiar%20objects%3F%20Recent%20studies%20have%20shown%20that%20abstract%0Aimages-constructed%20by%20primitive%20shapes-can%20indeed%20convey%20visual%20semantic%0Ainformation%20to%20deep%20learning%20models.%20However%2C%20representations%20obtained%20from%0Asuch%20images%20often%20fall%20short%20compared%20to%20those%20derived%20from%20traditional%20raster%0Aimages.%20In%20this%20paper%2C%20we%20study%20the%20reasons%20behind%20this%20performance%20gap%20and%0Ainvestigate%20how%20much%20high-level%20semantic%20content%20can%20be%20captured%20at%20different%0Aabstraction%20levels.%20To%20this%20end%2C%20we%20introduce%20the%20Hierarchical%20Abstraction%0AImage%20Dataset%20%28HAID%29%2C%20a%20novel%20data%20collection%20that%20comprises%20abstract%20images%0Agenerated%20from%20normal%20raster%20images%20at%20multiple%20levels%20of%20abstraction.%20We%20then%0Atrain%20and%20evaluate%20conventional%20vision%20systems%20on%20HAID%20across%20various%20tasks%0Aincluding%20classification%2C%20segmentation%2C%20and%20object%20detection%2C%20providing%20a%0Acomprehensive%20study%20between%20rasterised%20and%20abstract%20image%20representations.%20We%0Aalso%20discuss%20if%20the%20abstract%20image%20can%20be%20considered%20as%20a%20potentially%20effective%0Aformat%20for%20conveying%20visual%20semantic%20information%20and%20contributing%20to%20vision%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Exploratory%2520Study%2520on%2520Abstract%2520Images%2520and%2520Visual%2520Representations%250A%2520%2520Learned%2520from%2520Them%26entry.906535625%3DHaotian%2520Li%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3D%2520%2520Imagine%2520living%2520in%2520a%2520world%2520composed%2520solely%2520of%2520primitive%2520shapes%252C%2520could%2520you%250Astill%2520recognise%2520familiar%2520objects%253F%2520Recent%2520studies%2520have%2520shown%2520that%2520abstract%250Aimages-constructed%2520by%2520primitive%2520shapes-can%2520indeed%2520convey%2520visual%2520semantic%250Ainformation%2520to%2520deep%2520learning%2520models.%2520However%252C%2520representations%2520obtained%2520from%250Asuch%2520images%2520often%2520fall%2520short%2520compared%2520to%2520those%2520derived%2520from%2520traditional%2520raster%250Aimages.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520reasons%2520behind%2520this%2520performance%2520gap%2520and%250Ainvestigate%2520how%2520much%2520high-level%2520semantic%2520content%2520can%2520be%2520captured%2520at%2520different%250Aabstraction%2520levels.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520Hierarchical%2520Abstraction%250AImage%2520Dataset%2520%2528HAID%2529%252C%2520a%2520novel%2520data%2520collection%2520that%2520comprises%2520abstract%2520images%250Agenerated%2520from%2520normal%2520raster%2520images%2520at%2520multiple%2520levels%2520of%2520abstraction.%2520We%2520then%250Atrain%2520and%2520evaluate%2520conventional%2520vision%2520systems%2520on%2520HAID%2520across%2520various%2520tasks%250Aincluding%2520classification%252C%2520segmentation%252C%2520and%2520object%2520detection%252C%2520providing%2520a%250Acomprehensive%2520study%2520between%2520rasterised%2520and%2520abstract%2520image%2520representations.%2520We%250Aalso%2520discuss%2520if%2520the%2520abstract%2520image%2520can%2520be%2520considered%2520as%2520a%2520potentially%2520effective%250Aformat%2520for%2520conveying%2520visual%2520semantic%2520information%2520and%2520contributing%2520to%2520vision%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Exploratory%20Study%20on%20Abstract%20Images%20and%20Visual%20Representations%0A%20%20Learned%20from%20Them&entry.906535625=Haotian%20Li%20and%20Jianbo%20Jiao&entry.1292438233=%20%20Imagine%20living%20in%20a%20world%20composed%20solely%20of%20primitive%20shapes%2C%20could%20you%0Astill%20recognise%20familiar%20objects%3F%20Recent%20studies%20have%20shown%20that%20abstract%0Aimages-constructed%20by%20primitive%20shapes-can%20indeed%20convey%20visual%20semantic%0Ainformation%20to%20deep%20learning%20models.%20However%2C%20representations%20obtained%20from%0Asuch%20images%20often%20fall%20short%20compared%20to%20those%20derived%20from%20traditional%20raster%0Aimages.%20In%20this%20paper%2C%20we%20study%20the%20reasons%20behind%20this%20performance%20gap%20and%0Ainvestigate%20how%20much%20high-level%20semantic%20content%20can%20be%20captured%20at%20different%0Aabstraction%20levels.%20To%20this%20end%2C%20we%20introduce%20the%20Hierarchical%20Abstraction%0AImage%20Dataset%20%28HAID%29%2C%20a%20novel%20data%20collection%20that%20comprises%20abstract%20images%0Agenerated%20from%20normal%20raster%20images%20at%20multiple%20levels%20of%20abstraction.%20We%20then%0Atrain%20and%20evaluate%20conventional%20vision%20systems%20on%20HAID%20across%20various%20tasks%0Aincluding%20classification%2C%20segmentation%2C%20and%20object%20detection%2C%20providing%20a%0Acomprehensive%20study%20between%20rasterised%20and%20abstract%20image%20representations.%20We%0Aalso%20discuss%20if%20the%20abstract%20image%20can%20be%20considered%20as%20a%20potentially%20effective%0Aformat%20for%20conveying%20visual%20semantic%20information%20and%20contributing%20to%20vision%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14149v1&entry.124074799=Read"},
{"title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with\n  Anomaly-Aware Calibration", "author": "Jingyi Yuan and Jianxiong Ye and Wenkang Chen and Chenqiang Gao", "abstract": "  Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.\n", "link": "http://arxiv.org/abs/2509.14084v1", "date": "2025-09-17", "relevancy": 2.9404, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD-DINOv3%3A%20Enhancing%20DINOv3%20for%20Zero-Shot%20Anomaly%20Detection%20with%0A%20%20Anomaly-Aware%20Calibration&body=Title%3A%20AD-DINOv3%3A%20Enhancing%20DINOv3%20for%20Zero-Shot%20Anomaly%20Detection%20with%0A%20%20Anomaly-Aware%20Calibration%0AAuthor%3A%20Jingyi%20Yuan%20and%20Jianxiong%20Ye%20and%20Wenkang%20Chen%20and%20Chenqiang%20Gao%0AAbstract%3A%20%20%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%20seeks%20to%20identify%20anomalies%20from%20arbitrary%0Anovel%20categories%2C%20offering%20a%20scalable%20and%20annotation-efficient%20solution.%0ATraditionally%2C%20most%20ZSAD%20works%20have%20been%20based%20on%20the%20CLIP%20model%2C%20which%0Aperforms%20anomaly%20detection%20by%20calculating%20the%20similarity%20between%20visual%20and%0Atext%20embeddings.%20Recently%2C%20vision%20foundation%20models%20such%20as%20DINOv3%20have%0Ademonstrated%20strong%20transferable%20representation%20capabilities.%20In%20this%20work%2C%20we%0Aare%20the%20first%20to%20adapt%20DINOv3%20for%20ZSAD.%20However%2C%20this%20adaptation%20presents%20two%0Akey%20challenges%3A%20%28i%29%20the%20domain%20bias%20between%20large-scale%20pretraining%20data%20and%0Aanomaly%20detection%20tasks%20leads%20to%20feature%20misalignment%3B%20and%20%28ii%29%20the%20inherent%0Abias%20toward%20global%20semantics%20in%20pretrained%20representations%20often%20leads%20to%0Asubtle%20anomalies%20being%20misinterpreted%20as%20part%20of%20the%20normal%20foreground%20objects%2C%0Arather%20than%20being%20distinguished%20as%20abnormal%20regions.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20AD-DINOv3%2C%20a%20novel%20vision-language%20multimodal%0Aframework%20designed%20for%20ZSAD.%20Specifically%2C%20we%20formulate%20anomaly%20detection%20as%20a%0Amultimodal%20contrastive%20learning%20problem%2C%20where%20DINOv3%20is%20employed%20as%20the%20visual%0Abackbone%20to%20extract%20patch%20tokens%20and%20a%20CLS%20token%2C%20and%20the%20CLIP%20text%20encoder%0Aprovides%20embeddings%20for%20both%20normal%20and%20abnormal%20prompts.%20To%20bridge%20the%20domain%0Agap%2C%20lightweight%20adapters%20are%20introduced%20in%20both%20modalities%2C%20enabling%20their%0Arepresentations%20to%20be%20recalibrated%20for%20the%20anomaly%20detection%20task.%20Beyond%20this%0Abaseline%20alignment%2C%20we%20further%20design%20an%20Anomaly-Aware%20Calibration%20Module%0A%28AACM%29%2C%20which%20explicitly%20guides%20the%20CLS%20token%20to%20attend%20to%20anomalous%20regions%0Arather%20than%20generic%20foreground%20semantics%2C%20thereby%20enhancing%20discriminability.%0AExtensive%20experiments%20on%20eight%20industrial%20and%20medical%20benchmarks%20demonstrate%0Athat%20AD-DINOv3%20consistently%20matches%20or%20surpasses%20state-of-the-art%20methods%2C%0Averifying%20its%20superiority%20as%20a%20general%20zero-shot%20anomaly%20detection%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD-DINOv3%253A%2520Enhancing%2520DINOv3%2520for%2520Zero-Shot%2520Anomaly%2520Detection%2520with%250A%2520%2520Anomaly-Aware%2520Calibration%26entry.906535625%3DJingyi%2520Yuan%2520and%2520Jianxiong%2520Ye%2520and%2520Wenkang%2520Chen%2520and%2520Chenqiang%2520Gao%26entry.1292438233%3D%2520%2520Zero-Shot%2520Anomaly%2520Detection%2520%2528ZSAD%2529%2520seeks%2520to%2520identify%2520anomalies%2520from%2520arbitrary%250Anovel%2520categories%252C%2520offering%2520a%2520scalable%2520and%2520annotation-efficient%2520solution.%250ATraditionally%252C%2520most%2520ZSAD%2520works%2520have%2520been%2520based%2520on%2520the%2520CLIP%2520model%252C%2520which%250Aperforms%2520anomaly%2520detection%2520by%2520calculating%2520the%2520similarity%2520between%2520visual%2520and%250Atext%2520embeddings.%2520Recently%252C%2520vision%2520foundation%2520models%2520such%2520as%2520DINOv3%2520have%250Ademonstrated%2520strong%2520transferable%2520representation%2520capabilities.%2520In%2520this%2520work%252C%2520we%250Aare%2520the%2520first%2520to%2520adapt%2520DINOv3%2520for%2520ZSAD.%2520However%252C%2520this%2520adaptation%2520presents%2520two%250Akey%2520challenges%253A%2520%2528i%2529%2520the%2520domain%2520bias%2520between%2520large-scale%2520pretraining%2520data%2520and%250Aanomaly%2520detection%2520tasks%2520leads%2520to%2520feature%2520misalignment%253B%2520and%2520%2528ii%2529%2520the%2520inherent%250Abias%2520toward%2520global%2520semantics%2520in%2520pretrained%2520representations%2520often%2520leads%2520to%250Asubtle%2520anomalies%2520being%2520misinterpreted%2520as%2520part%2520of%2520the%2520normal%2520foreground%2520objects%252C%250Arather%2520than%2520being%2520distinguished%2520as%2520abnormal%2520regions.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520AD-DINOv3%252C%2520a%2520novel%2520vision-language%2520multimodal%250Aframework%2520designed%2520for%2520ZSAD.%2520Specifically%252C%2520we%2520formulate%2520anomaly%2520detection%2520as%2520a%250Amultimodal%2520contrastive%2520learning%2520problem%252C%2520where%2520DINOv3%2520is%2520employed%2520as%2520the%2520visual%250Abackbone%2520to%2520extract%2520patch%2520tokens%2520and%2520a%2520CLS%2520token%252C%2520and%2520the%2520CLIP%2520text%2520encoder%250Aprovides%2520embeddings%2520for%2520both%2520normal%2520and%2520abnormal%2520prompts.%2520To%2520bridge%2520the%2520domain%250Agap%252C%2520lightweight%2520adapters%2520are%2520introduced%2520in%2520both%2520modalities%252C%2520enabling%2520their%250Arepresentations%2520to%2520be%2520recalibrated%2520for%2520the%2520anomaly%2520detection%2520task.%2520Beyond%2520this%250Abaseline%2520alignment%252C%2520we%2520further%2520design%2520an%2520Anomaly-Aware%2520Calibration%2520Module%250A%2528AACM%2529%252C%2520which%2520explicitly%2520guides%2520the%2520CLS%2520token%2520to%2520attend%2520to%2520anomalous%2520regions%250Arather%2520than%2520generic%2520foreground%2520semantics%252C%2520thereby%2520enhancing%2520discriminability.%250AExtensive%2520experiments%2520on%2520eight%2520industrial%2520and%2520medical%2520benchmarks%2520demonstrate%250Athat%2520AD-DINOv3%2520consistently%2520matches%2520or%2520surpasses%2520state-of-the-art%2520methods%252C%250Averifying%2520its%2520superiority%2520as%2520a%2520general%2520zero-shot%2520anomaly%2520detection%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD-DINOv3%3A%20Enhancing%20DINOv3%20for%20Zero-Shot%20Anomaly%20Detection%20with%0A%20%20Anomaly-Aware%20Calibration&entry.906535625=Jingyi%20Yuan%20and%20Jianxiong%20Ye%20and%20Wenkang%20Chen%20and%20Chenqiang%20Gao&entry.1292438233=%20%20Zero-Shot%20Anomaly%20Detection%20%28ZSAD%29%20seeks%20to%20identify%20anomalies%20from%20arbitrary%0Anovel%20categories%2C%20offering%20a%20scalable%20and%20annotation-efficient%20solution.%0ATraditionally%2C%20most%20ZSAD%20works%20have%20been%20based%20on%20the%20CLIP%20model%2C%20which%0Aperforms%20anomaly%20detection%20by%20calculating%20the%20similarity%20between%20visual%20and%0Atext%20embeddings.%20Recently%2C%20vision%20foundation%20models%20such%20as%20DINOv3%20have%0Ademonstrated%20strong%20transferable%20representation%20capabilities.%20In%20this%20work%2C%20we%0Aare%20the%20first%20to%20adapt%20DINOv3%20for%20ZSAD.%20However%2C%20this%20adaptation%20presents%20two%0Akey%20challenges%3A%20%28i%29%20the%20domain%20bias%20between%20large-scale%20pretraining%20data%20and%0Aanomaly%20detection%20tasks%20leads%20to%20feature%20misalignment%3B%20and%20%28ii%29%20the%20inherent%0Abias%20toward%20global%20semantics%20in%20pretrained%20representations%20often%20leads%20to%0Asubtle%20anomalies%20being%20misinterpreted%20as%20part%20of%20the%20normal%20foreground%20objects%2C%0Arather%20than%20being%20distinguished%20as%20abnormal%20regions.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20AD-DINOv3%2C%20a%20novel%20vision-language%20multimodal%0Aframework%20designed%20for%20ZSAD.%20Specifically%2C%20we%20formulate%20anomaly%20detection%20as%20a%0Amultimodal%20contrastive%20learning%20problem%2C%20where%20DINOv3%20is%20employed%20as%20the%20visual%0Abackbone%20to%20extract%20patch%20tokens%20and%20a%20CLS%20token%2C%20and%20the%20CLIP%20text%20encoder%0Aprovides%20embeddings%20for%20both%20normal%20and%20abnormal%20prompts.%20To%20bridge%20the%20domain%0Agap%2C%20lightweight%20adapters%20are%20introduced%20in%20both%20modalities%2C%20enabling%20their%0Arepresentations%20to%20be%20recalibrated%20for%20the%20anomaly%20detection%20task.%20Beyond%20this%0Abaseline%20alignment%2C%20we%20further%20design%20an%20Anomaly-Aware%20Calibration%20Module%0A%28AACM%29%2C%20which%20explicitly%20guides%20the%20CLS%20token%20to%20attend%20to%20anomalous%20regions%0Arather%20than%20generic%20foreground%20semantics%2C%20thereby%20enhancing%20discriminability.%0AExtensive%20experiments%20on%20eight%20industrial%20and%20medical%20benchmarks%20demonstrate%0Athat%20AD-DINOv3%20consistently%20matches%20or%20surpasses%20state-of-the-art%20methods%2C%0Averifying%20its%20superiority%20as%20a%20general%20zero-shot%20anomaly%20detection%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14084v1&entry.124074799=Read"},
{"title": "SAIL-VL2 Technical Report", "author": "Weijie Yin and Yongjie Ye and Fangxun Shu and Yue Liao and Zijian Kang and Hongyuan Dong and Haiyang Yu and Dingkang Yang and Jiacong Wang and Han Wang and Wenzhuo Liu and Xiao Liang and Shuicheng Yan and Chao Feng", "abstract": "  We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.\n", "link": "http://arxiv.org/abs/2509.14033v1", "date": "2025-09-17", "relevancy": 2.9234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAIL-VL2%20Technical%20Report&body=Title%3A%20SAIL-VL2%20Technical%20Report%0AAuthor%3A%20Weijie%20Yin%20and%20Yongjie%20Ye%20and%20Fangxun%20Shu%20and%20Yue%20Liao%20and%20Zijian%20Kang%20and%20Hongyuan%20Dong%20and%20Haiyang%20Yu%20and%20Dingkang%20Yang%20and%20Jiacong%20Wang%20and%20Han%20Wang%20and%20Wenzhuo%20Liu%20and%20Xiao%20Liang%20and%20Shuicheng%20Yan%20and%20Chao%20Feng%0AAbstract%3A%20%20%20We%20introduce%20SAIL-VL2%2C%20an%20open-suite%20vision-language%20foundation%20model%20%28LVM%29%0Afor%20comprehensive%20multimodal%20understanding%20and%20reasoning.%20As%20the%20successor%20to%0ASAIL-VL%2C%20SAIL-VL2%20achieves%20state-of-the-art%20performance%20at%20the%202B%20and%208B%0Aparameter%20scales%20across%20diverse%20image%20and%20video%20benchmarks%2C%20demonstrating%0Astrong%20capabilities%20from%20fine-grained%20perception%20to%20complex%20reasoning.%20Three%0Acore%20innovations%20drive%20its%20effectiveness.%20First%2C%20a%20large-scale%20data%20curation%0Apipeline%20with%20scoring%20and%20filtering%20strategies%20enhances%20both%20quality%20and%0Adistribution%20across%20captioning%2C%20OCR%2C%20QA%2C%20and%20video%20data%2C%20improving%20training%0Aefficiency.%20Second%2C%20a%20progressive%20training%20framework%20begins%20with%20a%20powerful%0Apre-trained%20vision%20encoder%20%28SAIL-ViT%29%2C%20advances%20through%20multimodal%0Apre-training%2C%20and%20culminates%20in%20a%20thinking-fusion%20SFT-RL%20hybrid%20paradigm%20that%0Asystematically%20strengthens%20model%20capabilities.%20Third%2C%20architectural%20advances%0Aextend%20beyond%20dense%20LLMs%20to%20efficient%20sparse%20Mixture-of-Experts%20%28MoE%29%20designs.%0AWith%20these%20contributions%2C%20SAIL-VL2%20demonstrates%20competitive%20performance%20across%0A106%20datasets%20and%20achieves%20state-of-the-art%20results%20on%20challenging%20reasoning%0Abenchmarks%20such%20as%20MMMU%20and%20MathVista.%20Furthermore%2C%20on%20the%20OpenCompass%0Aleaderboard%2C%20SAIL-VL2-2B%20ranks%20first%20among%20officially%20released%20open-source%0Amodels%20under%20the%204B%20parameter%20scale%2C%20while%20serving%20as%20an%20efficient%20and%0Aextensible%20foundation%20for%20the%20open-source%20multimodal%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAIL-VL2%2520Technical%2520Report%26entry.906535625%3DWeijie%2520Yin%2520and%2520Yongjie%2520Ye%2520and%2520Fangxun%2520Shu%2520and%2520Yue%2520Liao%2520and%2520Zijian%2520Kang%2520and%2520Hongyuan%2520Dong%2520and%2520Haiyang%2520Yu%2520and%2520Dingkang%2520Yang%2520and%2520Jiacong%2520Wang%2520and%2520Han%2520Wang%2520and%2520Wenzhuo%2520Liu%2520and%2520Xiao%2520Liang%2520and%2520Shuicheng%2520Yan%2520and%2520Chao%2520Feng%26entry.1292438233%3D%2520%2520We%2520introduce%2520SAIL-VL2%252C%2520an%2520open-suite%2520vision-language%2520foundation%2520model%2520%2528LVM%2529%250Afor%2520comprehensive%2520multimodal%2520understanding%2520and%2520reasoning.%2520As%2520the%2520successor%2520to%250ASAIL-VL%252C%2520SAIL-VL2%2520achieves%2520state-of-the-art%2520performance%2520at%2520the%25202B%2520and%25208B%250Aparameter%2520scales%2520across%2520diverse%2520image%2520and%2520video%2520benchmarks%252C%2520demonstrating%250Astrong%2520capabilities%2520from%2520fine-grained%2520perception%2520to%2520complex%2520reasoning.%2520Three%250Acore%2520innovations%2520drive%2520its%2520effectiveness.%2520First%252C%2520a%2520large-scale%2520data%2520curation%250Apipeline%2520with%2520scoring%2520and%2520filtering%2520strategies%2520enhances%2520both%2520quality%2520and%250Adistribution%2520across%2520captioning%252C%2520OCR%252C%2520QA%252C%2520and%2520video%2520data%252C%2520improving%2520training%250Aefficiency.%2520Second%252C%2520a%2520progressive%2520training%2520framework%2520begins%2520with%2520a%2520powerful%250Apre-trained%2520vision%2520encoder%2520%2528SAIL-ViT%2529%252C%2520advances%2520through%2520multimodal%250Apre-training%252C%2520and%2520culminates%2520in%2520a%2520thinking-fusion%2520SFT-RL%2520hybrid%2520paradigm%2520that%250Asystematically%2520strengthens%2520model%2520capabilities.%2520Third%252C%2520architectural%2520advances%250Aextend%2520beyond%2520dense%2520LLMs%2520to%2520efficient%2520sparse%2520Mixture-of-Experts%2520%2528MoE%2529%2520designs.%250AWith%2520these%2520contributions%252C%2520SAIL-VL2%2520demonstrates%2520competitive%2520performance%2520across%250A106%2520datasets%2520and%2520achieves%2520state-of-the-art%2520results%2520on%2520challenging%2520reasoning%250Abenchmarks%2520such%2520as%2520MMMU%2520and%2520MathVista.%2520Furthermore%252C%2520on%2520the%2520OpenCompass%250Aleaderboard%252C%2520SAIL-VL2-2B%2520ranks%2520first%2520among%2520officially%2520released%2520open-source%250Amodels%2520under%2520the%25204B%2520parameter%2520scale%252C%2520while%2520serving%2520as%2520an%2520efficient%2520and%250Aextensible%2520foundation%2520for%2520the%2520open-source%2520multimodal%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAIL-VL2%20Technical%20Report&entry.906535625=Weijie%20Yin%20and%20Yongjie%20Ye%20and%20Fangxun%20Shu%20and%20Yue%20Liao%20and%20Zijian%20Kang%20and%20Hongyuan%20Dong%20and%20Haiyang%20Yu%20and%20Dingkang%20Yang%20and%20Jiacong%20Wang%20and%20Han%20Wang%20and%20Wenzhuo%20Liu%20and%20Xiao%20Liang%20and%20Shuicheng%20Yan%20and%20Chao%20Feng&entry.1292438233=%20%20We%20introduce%20SAIL-VL2%2C%20an%20open-suite%20vision-language%20foundation%20model%20%28LVM%29%0Afor%20comprehensive%20multimodal%20understanding%20and%20reasoning.%20As%20the%20successor%20to%0ASAIL-VL%2C%20SAIL-VL2%20achieves%20state-of-the-art%20performance%20at%20the%202B%20and%208B%0Aparameter%20scales%20across%20diverse%20image%20and%20video%20benchmarks%2C%20demonstrating%0Astrong%20capabilities%20from%20fine-grained%20perception%20to%20complex%20reasoning.%20Three%0Acore%20innovations%20drive%20its%20effectiveness.%20First%2C%20a%20large-scale%20data%20curation%0Apipeline%20with%20scoring%20and%20filtering%20strategies%20enhances%20both%20quality%20and%0Adistribution%20across%20captioning%2C%20OCR%2C%20QA%2C%20and%20video%20data%2C%20improving%20training%0Aefficiency.%20Second%2C%20a%20progressive%20training%20framework%20begins%20with%20a%20powerful%0Apre-trained%20vision%20encoder%20%28SAIL-ViT%29%2C%20advances%20through%20multimodal%0Apre-training%2C%20and%20culminates%20in%20a%20thinking-fusion%20SFT-RL%20hybrid%20paradigm%20that%0Asystematically%20strengthens%20model%20capabilities.%20Third%2C%20architectural%20advances%0Aextend%20beyond%20dense%20LLMs%20to%20efficient%20sparse%20Mixture-of-Experts%20%28MoE%29%20designs.%0AWith%20these%20contributions%2C%20SAIL-VL2%20demonstrates%20competitive%20performance%20across%0A106%20datasets%20and%20achieves%20state-of-the-art%20results%20on%20challenging%20reasoning%0Abenchmarks%20such%20as%20MMMU%20and%20MathVista.%20Furthermore%2C%20on%20the%20OpenCompass%0Aleaderboard%2C%20SAIL-VL2-2B%20ranks%20first%20among%20officially%20released%20open-source%0Amodels%20under%20the%204B%20parameter%20scale%2C%20while%20serving%20as%20an%20efficient%20and%0Aextensible%20foundation%20for%20the%20open-source%20multimodal%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14033v1&entry.124074799=Read"},
{"title": "Dense Video Understanding with Gated Residual Tokenization", "author": "Haichao Zhang and Wenhao Chai and Shwai He and Ang Li and Yun Fu", "abstract": "  High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.\n", "link": "http://arxiv.org/abs/2509.14199v1", "date": "2025-09-17", "relevancy": 2.9063, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization&body=Title%3A%20Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization%0AAuthor%3A%20Haichao%20Zhang%20and%20Wenhao%20Chai%20and%20Shwai%20He%20and%20Ang%20Li%20and%20Yun%20Fu%0AAbstract%3A%20%20%20High%20temporal%20resolution%20is%20essential%20for%20capturing%20fine-grained%20details%20in%0Avideo%20understanding.%20However%2C%20current%20video%20large%20language%20models%20%28VLLMs%29%20and%0Abenchmarks%20mostly%20rely%20on%20low-frame-rate%20sampling%2C%20such%20as%20uniform%20sampling%20or%0Akeyframe%20selection%2C%20discarding%20dense%20temporal%20information.%20This%20compromise%0Aavoids%20the%20high%20cost%20of%20tokenizing%20every%20frame%2C%20which%20otherwise%20leads%20to%0Aredundant%20computation%20and%20linear%20token%20growth%20as%20video%20length%20increases.%20While%0Athis%20trade-off%20works%20for%20slowly%20changing%20content%2C%20it%20fails%20for%20tasks%20like%0Alecture%20comprehension%2C%20where%20information%20appears%20in%20nearly%20every%20frame%20and%0Arequires%20precise%20temporal%20alignment.%20To%20address%20this%20gap%2C%20we%20introduce%20Dense%0AVideo%20Understanding%20%28DVU%29%2C%20which%20enables%20high-FPS%20video%20comprehension%20by%0Areducing%20both%20tokenization%20time%20and%20token%20overhead.%20Existing%20benchmarks%20are%0Aalso%20limited%2C%20as%20their%20QA%20pairs%20focus%20on%20coarse%20content%20changes.%20We%20therefore%0Apropose%20DIVE%20%28Dense%20Information%20Video%20Evaluation%29%2C%20the%20first%20benchmark%20designed%0Afor%20dense%20temporal%20reasoning.%20To%20make%20DVU%20practical%2C%20we%20present%20Gated%20Residual%0ATokenization%20%28GRT%29%2C%20a%20two-stage%20framework%3A%20%281%29%20Motion-Compensated%20Inter-Gated%0ATokenization%20uses%20pixel-level%20motion%20estimation%20to%20skip%20static%20regions%20during%0Atokenization%2C%20achieving%20sub-linear%20growth%20in%20token%20count%20and%20compute.%20%282%29%0ASemantic-Scene%20Intra-Tokenization%20Merging%20fuses%20tokens%20across%20static%20regions%0Awithin%20a%20scene%2C%20further%20reducing%20redundancy%20while%20preserving%20dynamic%20semantics.%0AExperiments%20on%20DIVE%20show%20that%20GRT%20outperforms%20larger%20VLLM%20baselines%20and%20scales%0Apositively%20with%20FPS.%20These%20results%20highlight%20the%20importance%20of%20dense%20temporal%0Ainformation%20and%20demonstrate%20that%20GRT%20enables%20efficient%2C%20scalable%20high-FPS%20video%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Video%2520Understanding%2520with%2520Gated%2520Residual%2520Tokenization%26entry.906535625%3DHaichao%2520Zhang%2520and%2520Wenhao%2520Chai%2520and%2520Shwai%2520He%2520and%2520Ang%2520Li%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520High%2520temporal%2520resolution%2520is%2520essential%2520for%2520capturing%2520fine-grained%2520details%2520in%250Avideo%2520understanding.%2520However%252C%2520current%2520video%2520large%2520language%2520models%2520%2528VLLMs%2529%2520and%250Abenchmarks%2520mostly%2520rely%2520on%2520low-frame-rate%2520sampling%252C%2520such%2520as%2520uniform%2520sampling%2520or%250Akeyframe%2520selection%252C%2520discarding%2520dense%2520temporal%2520information.%2520This%2520compromise%250Aavoids%2520the%2520high%2520cost%2520of%2520tokenizing%2520every%2520frame%252C%2520which%2520otherwise%2520leads%2520to%250Aredundant%2520computation%2520and%2520linear%2520token%2520growth%2520as%2520video%2520length%2520increases.%2520While%250Athis%2520trade-off%2520works%2520for%2520slowly%2520changing%2520content%252C%2520it%2520fails%2520for%2520tasks%2520like%250Alecture%2520comprehension%252C%2520where%2520information%2520appears%2520in%2520nearly%2520every%2520frame%2520and%250Arequires%2520precise%2520temporal%2520alignment.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Dense%250AVideo%2520Understanding%2520%2528DVU%2529%252C%2520which%2520enables%2520high-FPS%2520video%2520comprehension%2520by%250Areducing%2520both%2520tokenization%2520time%2520and%2520token%2520overhead.%2520Existing%2520benchmarks%2520are%250Aalso%2520limited%252C%2520as%2520their%2520QA%2520pairs%2520focus%2520on%2520coarse%2520content%2520changes.%2520We%2520therefore%250Apropose%2520DIVE%2520%2528Dense%2520Information%2520Video%2520Evaluation%2529%252C%2520the%2520first%2520benchmark%2520designed%250Afor%2520dense%2520temporal%2520reasoning.%2520To%2520make%2520DVU%2520practical%252C%2520we%2520present%2520Gated%2520Residual%250ATokenization%2520%2528GRT%2529%252C%2520a%2520two-stage%2520framework%253A%2520%25281%2529%2520Motion-Compensated%2520Inter-Gated%250ATokenization%2520uses%2520pixel-level%2520motion%2520estimation%2520to%2520skip%2520static%2520regions%2520during%250Atokenization%252C%2520achieving%2520sub-linear%2520growth%2520in%2520token%2520count%2520and%2520compute.%2520%25282%2529%250ASemantic-Scene%2520Intra-Tokenization%2520Merging%2520fuses%2520tokens%2520across%2520static%2520regions%250Awithin%2520a%2520scene%252C%2520further%2520reducing%2520redundancy%2520while%2520preserving%2520dynamic%2520semantics.%250AExperiments%2520on%2520DIVE%2520show%2520that%2520GRT%2520outperforms%2520larger%2520VLLM%2520baselines%2520and%2520scales%250Apositively%2520with%2520FPS.%2520These%2520results%2520highlight%2520the%2520importance%2520of%2520dense%2520temporal%250Ainformation%2520and%2520demonstrate%2520that%2520GRT%2520enables%2520efficient%252C%2520scalable%2520high-FPS%2520video%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization&entry.906535625=Haichao%20Zhang%20and%20Wenhao%20Chai%20and%20Shwai%20He%20and%20Ang%20Li%20and%20Yun%20Fu&entry.1292438233=%20%20High%20temporal%20resolution%20is%20essential%20for%20capturing%20fine-grained%20details%20in%0Avideo%20understanding.%20However%2C%20current%20video%20large%20language%20models%20%28VLLMs%29%20and%0Abenchmarks%20mostly%20rely%20on%20low-frame-rate%20sampling%2C%20such%20as%20uniform%20sampling%20or%0Akeyframe%20selection%2C%20discarding%20dense%20temporal%20information.%20This%20compromise%0Aavoids%20the%20high%20cost%20of%20tokenizing%20every%20frame%2C%20which%20otherwise%20leads%20to%0Aredundant%20computation%20and%20linear%20token%20growth%20as%20video%20length%20increases.%20While%0Athis%20trade-off%20works%20for%20slowly%20changing%20content%2C%20it%20fails%20for%20tasks%20like%0Alecture%20comprehension%2C%20where%20information%20appears%20in%20nearly%20every%20frame%20and%0Arequires%20precise%20temporal%20alignment.%20To%20address%20this%20gap%2C%20we%20introduce%20Dense%0AVideo%20Understanding%20%28DVU%29%2C%20which%20enables%20high-FPS%20video%20comprehension%20by%0Areducing%20both%20tokenization%20time%20and%20token%20overhead.%20Existing%20benchmarks%20are%0Aalso%20limited%2C%20as%20their%20QA%20pairs%20focus%20on%20coarse%20content%20changes.%20We%20therefore%0Apropose%20DIVE%20%28Dense%20Information%20Video%20Evaluation%29%2C%20the%20first%20benchmark%20designed%0Afor%20dense%20temporal%20reasoning.%20To%20make%20DVU%20practical%2C%20we%20present%20Gated%20Residual%0ATokenization%20%28GRT%29%2C%20a%20two-stage%20framework%3A%20%281%29%20Motion-Compensated%20Inter-Gated%0ATokenization%20uses%20pixel-level%20motion%20estimation%20to%20skip%20static%20regions%20during%0Atokenization%2C%20achieving%20sub-linear%20growth%20in%20token%20count%20and%20compute.%20%282%29%0ASemantic-Scene%20Intra-Tokenization%20Merging%20fuses%20tokens%20across%20static%20regions%0Awithin%20a%20scene%2C%20further%20reducing%20redundancy%20while%20preserving%20dynamic%20semantics.%0AExperiments%20on%20DIVE%20show%20that%20GRT%20outperforms%20larger%20VLLM%20baselines%20and%20scales%0Apositively%20with%20FPS.%20These%20results%20highlight%20the%20importance%20of%20dense%20temporal%0Ainformation%20and%20demonstrate%20that%20GRT%20enables%20efficient%2C%20scalable%20high-FPS%20video%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14199v1&entry.124074799=Read"},
{"title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model", "author": "Ali Abouzeid and Malak Mansour and Zezhou Sun and Dezhen Song", "abstract": "  Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.\n", "link": "http://arxiv.org/abs/2509.14117v1", "date": "2025-09-17", "relevancy": 2.8731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoAware-VLA%3A%20Implicit%20Geometry%20Aware%20Vision-Language-Action%20Model&body=Title%3A%20GeoAware-VLA%3A%20Implicit%20Geometry%20Aware%20Vision-Language-Action%20Model%0AAuthor%3A%20Ali%20Abouzeid%20and%20Malak%20Mansour%20and%20Zezhou%20Sun%20and%20Dezhen%20Song%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20often%20fail%20to%20generalize%20to%20novel%20camera%0Aviewpoints%2C%20a%20limitation%20stemming%20from%20their%20difficulty%20in%20inferring%20robust%203D%0Ageometry%20from%202D%20images.%20We%20introduce%20GeoAware-VLA%2C%20a%20simple%20yet%20effective%0Aapproach%20that%20enhances%20viewpoint%20invariance%20by%20integrating%20strong%20geometric%0Apriors%20into%20the%20vision%20backbone.%20Instead%20of%20training%20a%20visual%20encoder%20or%0Arelying%20on%20explicit%203D%20data%2C%20we%20leverage%20a%20frozen%2C%20pretrained%20geometric%20vision%0Amodel%20as%20a%20feature%20extractor.%20A%20trainable%20projection%20layer%20then%20adapts%20these%0Ageometrically-rich%20features%20for%20the%20policy%20decoder%2C%20relieving%20it%20of%20the%20burden%0Aof%20learning%203D%20consistency%20from%20scratch.%20Through%20extensive%20evaluations%20on%0ALIBERO%20benchmark%20subsets%2C%20we%20show%20GeoAware-VLA%20achieves%20substantial%0Aimprovements%20in%20zero-shot%20generalization%20to%20novel%20camera%20poses%2C%20boosting%0Asuccess%20rates%20by%20over%202x%20in%20simulation.%20Crucially%2C%20these%20benefits%20translate%20to%0Athe%20physical%20world%3B%20our%20model%20shows%20a%20significant%20performance%20gain%20on%20a%20real%0Arobot%2C%20especially%20when%20evaluated%20from%20unseen%20camera%20angles.%20Our%20approach%20proves%0Aeffective%20across%20both%20continuous%20and%20discrete%20action%20spaces%2C%20highlighting%20that%0Arobust%20geometric%20grounding%20is%20a%20key%20component%20for%20creating%20more%20generalizable%0Arobotic%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoAware-VLA%253A%2520Implicit%2520Geometry%2520Aware%2520Vision-Language-Action%2520Model%26entry.906535625%3DAli%2520Abouzeid%2520and%2520Malak%2520Mansour%2520and%2520Zezhou%2520Sun%2520and%2520Dezhen%2520Song%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520often%2520fail%2520to%2520generalize%2520to%2520novel%2520camera%250Aviewpoints%252C%2520a%2520limitation%2520stemming%2520from%2520their%2520difficulty%2520in%2520inferring%2520robust%25203D%250Ageometry%2520from%25202D%2520images.%2520We%2520introduce%2520GeoAware-VLA%252C%2520a%2520simple%2520yet%2520effective%250Aapproach%2520that%2520enhances%2520viewpoint%2520invariance%2520by%2520integrating%2520strong%2520geometric%250Apriors%2520into%2520the%2520vision%2520backbone.%2520Instead%2520of%2520training%2520a%2520visual%2520encoder%2520or%250Arelying%2520on%2520explicit%25203D%2520data%252C%2520we%2520leverage%2520a%2520frozen%252C%2520pretrained%2520geometric%2520vision%250Amodel%2520as%2520a%2520feature%2520extractor.%2520A%2520trainable%2520projection%2520layer%2520then%2520adapts%2520these%250Ageometrically-rich%2520features%2520for%2520the%2520policy%2520decoder%252C%2520relieving%2520it%2520of%2520the%2520burden%250Aof%2520learning%25203D%2520consistency%2520from%2520scratch.%2520Through%2520extensive%2520evaluations%2520on%250ALIBERO%2520benchmark%2520subsets%252C%2520we%2520show%2520GeoAware-VLA%2520achieves%2520substantial%250Aimprovements%2520in%2520zero-shot%2520generalization%2520to%2520novel%2520camera%2520poses%252C%2520boosting%250Asuccess%2520rates%2520by%2520over%25202x%2520in%2520simulation.%2520Crucially%252C%2520these%2520benefits%2520translate%2520to%250Athe%2520physical%2520world%253B%2520our%2520model%2520shows%2520a%2520significant%2520performance%2520gain%2520on%2520a%2520real%250Arobot%252C%2520especially%2520when%2520evaluated%2520from%2520unseen%2520camera%2520angles.%2520Our%2520approach%2520proves%250Aeffective%2520across%2520both%2520continuous%2520and%2520discrete%2520action%2520spaces%252C%2520highlighting%2520that%250Arobust%2520geometric%2520grounding%2520is%2520a%2520key%2520component%2520for%2520creating%2520more%2520generalizable%250Arobotic%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoAware-VLA%3A%20Implicit%20Geometry%20Aware%20Vision-Language-Action%20Model&entry.906535625=Ali%20Abouzeid%20and%20Malak%20Mansour%20and%20Zezhou%20Sun%20and%20Dezhen%20Song&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20often%20fail%20to%20generalize%20to%20novel%20camera%0Aviewpoints%2C%20a%20limitation%20stemming%20from%20their%20difficulty%20in%20inferring%20robust%203D%0Ageometry%20from%202D%20images.%20We%20introduce%20GeoAware-VLA%2C%20a%20simple%20yet%20effective%0Aapproach%20that%20enhances%20viewpoint%20invariance%20by%20integrating%20strong%20geometric%0Apriors%20into%20the%20vision%20backbone.%20Instead%20of%20training%20a%20visual%20encoder%20or%0Arelying%20on%20explicit%203D%20data%2C%20we%20leverage%20a%20frozen%2C%20pretrained%20geometric%20vision%0Amodel%20as%20a%20feature%20extractor.%20A%20trainable%20projection%20layer%20then%20adapts%20these%0Ageometrically-rich%20features%20for%20the%20policy%20decoder%2C%20relieving%20it%20of%20the%20burden%0Aof%20learning%203D%20consistency%20from%20scratch.%20Through%20extensive%20evaluations%20on%0ALIBERO%20benchmark%20subsets%2C%20we%20show%20GeoAware-VLA%20achieves%20substantial%0Aimprovements%20in%20zero-shot%20generalization%20to%20novel%20camera%20poses%2C%20boosting%0Asuccess%20rates%20by%20over%202x%20in%20simulation.%20Crucially%2C%20these%20benefits%20translate%20to%0Athe%20physical%20world%3B%20our%20model%20shows%20a%20significant%20performance%20gain%20on%20a%20real%0Arobot%2C%20especially%20when%20evaluated%20from%20unseen%20camera%20angles.%20Our%20approach%20proves%0Aeffective%20across%20both%20continuous%20and%20discrete%20action%20spaces%2C%20highlighting%20that%0Arobust%20geometric%20grounding%20is%20a%20key%20component%20for%20creating%20more%20generalizable%0Arobotic%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14117v1&entry.124074799=Read"},
{"title": "Visible Yet Unreadable: A Systematic Blind Spot of Vision Language\n  Models Across Writing Systems", "author": "Jie Zhang and Ting Xu and Gelei Deng and Runyi Hu and Han Qiu and Tianwei Zhang and Qing Guo and Ivor Tsang", "abstract": "  Writing is a universal cultural technology that reuses vision for symbolic\ncommunication. Humans display striking resilience: we readily recognize words\neven when characters are fragmented, fused, or partially occluded. This paper\ninvestigates whether advanced vision language models (VLMs) share this\nresilience. We construct two psychophysics inspired benchmarks across distinct\nwriting systems, Chinese logographs and English alphabetic words, by splicing,\nrecombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli\nfor models while remaining legible to humans. Despite strong performance on\nclean text, contemporary VLMs show a severe drop under these perturbations,\nfrequently producing unrelated or incoherent outputs. The pattern suggests a\nstructural limitation: models heavily leverage generic visual invariances but\nunder rely on compositional priors needed for robust literacy. We release\nstimuli generation code, prompts, and evaluation protocols to facilitate\ntransparent replication and follow up work. Our findings motivate architectures\nand training strategies that encode symbol segmentation, composition, and\nbinding across scripts, and they delineate concrete challenges for deploying\nmultimodal systems in education, accessibility, cultural heritage, and\nsecurity.\n", "link": "http://arxiv.org/abs/2509.06996v2", "date": "2025-09-17", "relevancy": 2.8642, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visible%20Yet%20Unreadable%3A%20A%20Systematic%20Blind%20Spot%20of%20Vision%20Language%0A%20%20Models%20Across%20Writing%20Systems&body=Title%3A%20Visible%20Yet%20Unreadable%3A%20A%20Systematic%20Blind%20Spot%20of%20Vision%20Language%0A%20%20Models%20Across%20Writing%20Systems%0AAuthor%3A%20Jie%20Zhang%20and%20Ting%20Xu%20and%20Gelei%20Deng%20and%20Runyi%20Hu%20and%20Han%20Qiu%20and%20Tianwei%20Zhang%20and%20Qing%20Guo%20and%20Ivor%20Tsang%0AAbstract%3A%20%20%20Writing%20is%20a%20universal%20cultural%20technology%20that%20reuses%20vision%20for%20symbolic%0Acommunication.%20Humans%20display%20striking%20resilience%3A%20we%20readily%20recognize%20words%0Aeven%20when%20characters%20are%20fragmented%2C%20fused%2C%20or%20partially%20occluded.%20This%20paper%0Ainvestigates%20whether%20advanced%20vision%20language%20models%20%28VLMs%29%20share%20this%0Aresilience.%20We%20construct%20two%20psychophysics%20inspired%20benchmarks%20across%20distinct%0Awriting%20systems%2C%20Chinese%20logographs%20and%20English%20alphabetic%20words%2C%20by%20splicing%2C%0Arecombining%2C%20and%20overlaying%20glyphs%20to%20yield%20%27%27visible%20but%20unreadable%27%27%20stimuli%0Afor%20models%20while%20remaining%20legible%20to%20humans.%20Despite%20strong%20performance%20on%0Aclean%20text%2C%20contemporary%20VLMs%20show%20a%20severe%20drop%20under%20these%20perturbations%2C%0Afrequently%20producing%20unrelated%20or%20incoherent%20outputs.%20The%20pattern%20suggests%20a%0Astructural%20limitation%3A%20models%20heavily%20leverage%20generic%20visual%20invariances%20but%0Aunder%20rely%20on%20compositional%20priors%20needed%20for%20robust%20literacy.%20We%20release%0Astimuli%20generation%20code%2C%20prompts%2C%20and%20evaluation%20protocols%20to%20facilitate%0Atransparent%20replication%20and%20follow%20up%20work.%20Our%20findings%20motivate%20architectures%0Aand%20training%20strategies%20that%20encode%20symbol%20segmentation%2C%20composition%2C%20and%0Abinding%20across%20scripts%2C%20and%20they%20delineate%20concrete%20challenges%20for%20deploying%0Amultimodal%20systems%20in%20education%2C%20accessibility%2C%20cultural%20heritage%2C%20and%0Asecurity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06996v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisible%2520Yet%2520Unreadable%253A%2520A%2520Systematic%2520Blind%2520Spot%2520of%2520Vision%2520Language%250A%2520%2520Models%2520Across%2520Writing%2520Systems%26entry.906535625%3DJie%2520Zhang%2520and%2520Ting%2520Xu%2520and%2520Gelei%2520Deng%2520and%2520Runyi%2520Hu%2520and%2520Han%2520Qiu%2520and%2520Tianwei%2520Zhang%2520and%2520Qing%2520Guo%2520and%2520Ivor%2520Tsang%26entry.1292438233%3D%2520%2520Writing%2520is%2520a%2520universal%2520cultural%2520technology%2520that%2520reuses%2520vision%2520for%2520symbolic%250Acommunication.%2520Humans%2520display%2520striking%2520resilience%253A%2520we%2520readily%2520recognize%2520words%250Aeven%2520when%2520characters%2520are%2520fragmented%252C%2520fused%252C%2520or%2520partially%2520occluded.%2520This%2520paper%250Ainvestigates%2520whether%2520advanced%2520vision%2520language%2520models%2520%2528VLMs%2529%2520share%2520this%250Aresilience.%2520We%2520construct%2520two%2520psychophysics%2520inspired%2520benchmarks%2520across%2520distinct%250Awriting%2520systems%252C%2520Chinese%2520logographs%2520and%2520English%2520alphabetic%2520words%252C%2520by%2520splicing%252C%250Arecombining%252C%2520and%2520overlaying%2520glyphs%2520to%2520yield%2520%2527%2527visible%2520but%2520unreadable%2527%2527%2520stimuli%250Afor%2520models%2520while%2520remaining%2520legible%2520to%2520humans.%2520Despite%2520strong%2520performance%2520on%250Aclean%2520text%252C%2520contemporary%2520VLMs%2520show%2520a%2520severe%2520drop%2520under%2520these%2520perturbations%252C%250Afrequently%2520producing%2520unrelated%2520or%2520incoherent%2520outputs.%2520The%2520pattern%2520suggests%2520a%250Astructural%2520limitation%253A%2520models%2520heavily%2520leverage%2520generic%2520visual%2520invariances%2520but%250Aunder%2520rely%2520on%2520compositional%2520priors%2520needed%2520for%2520robust%2520literacy.%2520We%2520release%250Astimuli%2520generation%2520code%252C%2520prompts%252C%2520and%2520evaluation%2520protocols%2520to%2520facilitate%250Atransparent%2520replication%2520and%2520follow%2520up%2520work.%2520Our%2520findings%2520motivate%2520architectures%250Aand%2520training%2520strategies%2520that%2520encode%2520symbol%2520segmentation%252C%2520composition%252C%2520and%250Abinding%2520across%2520scripts%252C%2520and%2520they%2520delineate%2520concrete%2520challenges%2520for%2520deploying%250Amultimodal%2520systems%2520in%2520education%252C%2520accessibility%252C%2520cultural%2520heritage%252C%2520and%250Asecurity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06996v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visible%20Yet%20Unreadable%3A%20A%20Systematic%20Blind%20Spot%20of%20Vision%20Language%0A%20%20Models%20Across%20Writing%20Systems&entry.906535625=Jie%20Zhang%20and%20Ting%20Xu%20and%20Gelei%20Deng%20and%20Runyi%20Hu%20and%20Han%20Qiu%20and%20Tianwei%20Zhang%20and%20Qing%20Guo%20and%20Ivor%20Tsang&entry.1292438233=%20%20Writing%20is%20a%20universal%20cultural%20technology%20that%20reuses%20vision%20for%20symbolic%0Acommunication.%20Humans%20display%20striking%20resilience%3A%20we%20readily%20recognize%20words%0Aeven%20when%20characters%20are%20fragmented%2C%20fused%2C%20or%20partially%20occluded.%20This%20paper%0Ainvestigates%20whether%20advanced%20vision%20language%20models%20%28VLMs%29%20share%20this%0Aresilience.%20We%20construct%20two%20psychophysics%20inspired%20benchmarks%20across%20distinct%0Awriting%20systems%2C%20Chinese%20logographs%20and%20English%20alphabetic%20words%2C%20by%20splicing%2C%0Arecombining%2C%20and%20overlaying%20glyphs%20to%20yield%20%27%27visible%20but%20unreadable%27%27%20stimuli%0Afor%20models%20while%20remaining%20legible%20to%20humans.%20Despite%20strong%20performance%20on%0Aclean%20text%2C%20contemporary%20VLMs%20show%20a%20severe%20drop%20under%20these%20perturbations%2C%0Afrequently%20producing%20unrelated%20or%20incoherent%20outputs.%20The%20pattern%20suggests%20a%0Astructural%20limitation%3A%20models%20heavily%20leverage%20generic%20visual%20invariances%20but%0Aunder%20rely%20on%20compositional%20priors%20needed%20for%20robust%20literacy.%20We%20release%0Astimuli%20generation%20code%2C%20prompts%2C%20and%20evaluation%20protocols%20to%20facilitate%0Atransparent%20replication%20and%20follow%20up%20work.%20Our%20findings%20motivate%20architectures%0Aand%20training%20strategies%20that%20encode%20symbol%20segmentation%2C%20composition%2C%20and%0Abinding%20across%20scripts%2C%20and%20they%20delineate%20concrete%20challenges%20for%20deploying%0Amultimodal%20systems%20in%20education%2C%20accessibility%2C%20cultural%20heritage%2C%20and%0Asecurity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06996v2&entry.124074799=Read"},
{"title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive\n  Difficulty Curriculum Learning and Expert-Guided Self-Reformulation", "author": "Enci Zhang and Xingang Yan and Wei Lin and Tianxiang Zhang and Qianchun Lu", "abstract": "  Despite impressive progress in areas like mathematical reasoning, large\nlanguage models still face significant challenges in consistently solving\ncomplex problems. Drawing inspiration from key human learning strategies, we\npropose two novel strategies to enhance the capability of large language models\nto solve these complex problems. First, Adaptive Difficulty Curriculum Learning\n(ADCL) is a novel curriculum learning strategy that tackles the Difficulty\nShift phenomenon (i.e., a model's perception of problem difficulty dynamically\nchanges during training) by periodically re-estimating difficulty within\nupcoming data batches to maintain alignment with the model's evolving\ncapabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel\nreinforcement learning strategy that bridges the gap between imitation learning\nand pure exploration by guiding models to reformulate expert solutions within\ntheir own conceptual framework, rather than relying on direct imitation,\nfostering deeper understanding and knowledge assimilation. Extensive\nexperiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B\nas the base model, demonstrate that these human-inspired strategies\nsynergistically and significantly enhance performance. Notably, their combined\napplication improves performance over the standard Zero-RL baseline by 10% on\nthe AIME24 benchmark and 16.6% on AIME25.\n", "link": "http://arxiv.org/abs/2505.08364v2", "date": "2025-09-17", "relevancy": 2.7899, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Like%20Humans%3A%20Advancing%20LLM%20Reasoning%20Capabilities%20via%20Adaptive%0A%20%20Difficulty%20Curriculum%20Learning%20and%20Expert-Guided%20Self-Reformulation&body=Title%3A%20Learning%20Like%20Humans%3A%20Advancing%20LLM%20Reasoning%20Capabilities%20via%20Adaptive%0A%20%20Difficulty%20Curriculum%20Learning%20and%20Expert-Guided%20Self-Reformulation%0AAuthor%3A%20Enci%20Zhang%20and%20Xingang%20Yan%20and%20Wei%20Lin%20and%20Tianxiang%20Zhang%20and%20Qianchun%20Lu%0AAbstract%3A%20%20%20Despite%20impressive%20progress%20in%20areas%20like%20mathematical%20reasoning%2C%20large%0Alanguage%20models%20still%20face%20significant%20challenges%20in%20consistently%20solving%0Acomplex%20problems.%20Drawing%20inspiration%20from%20key%20human%20learning%20strategies%2C%20we%0Apropose%20two%20novel%20strategies%20to%20enhance%20the%20capability%20of%20large%20language%20models%0Ato%20solve%20these%20complex%20problems.%20First%2C%20Adaptive%20Difficulty%20Curriculum%20Learning%0A%28ADCL%29%20is%20a%20novel%20curriculum%20learning%20strategy%20that%20tackles%20the%20Difficulty%0AShift%20phenomenon%20%28i.e.%2C%20a%20model%27s%20perception%20of%20problem%20difficulty%20dynamically%0Achanges%20during%20training%29%20by%20periodically%20re-estimating%20difficulty%20within%0Aupcoming%20data%20batches%20to%20maintain%20alignment%20with%20the%20model%27s%20evolving%0Acapabilities.%20Second%2C%20Expert-Guided%20Self-Reformulation%20%28EGSR%29%20is%20a%20novel%0Areinforcement%20learning%20strategy%20that%20bridges%20the%20gap%20between%20imitation%20learning%0Aand%20pure%20exploration%20by%20guiding%20models%20to%20reformulate%20expert%20solutions%20within%0Atheir%20own%20conceptual%20framework%2C%20rather%20than%20relying%20on%20direct%20imitation%2C%0Afostering%20deeper%20understanding%20and%20knowledge%20assimilation.%20Extensive%0Aexperiments%20on%20challenging%20mathematical%20reasoning%20benchmarks%2C%20using%20Qwen2.5-7B%0Aas%20the%20base%20model%2C%20demonstrate%20that%20these%20human-inspired%20strategies%0Asynergistically%20and%20significantly%20enhance%20performance.%20Notably%2C%20their%20combined%0Aapplication%20improves%20performance%20over%20the%20standard%20Zero-RL%20baseline%20by%2010%25%20on%0Athe%20AIME24%20benchmark%20and%2016.6%25%20on%20AIME25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Like%2520Humans%253A%2520Advancing%2520LLM%2520Reasoning%2520Capabilities%2520via%2520Adaptive%250A%2520%2520Difficulty%2520Curriculum%2520Learning%2520and%2520Expert-Guided%2520Self-Reformulation%26entry.906535625%3DEnci%2520Zhang%2520and%2520Xingang%2520Yan%2520and%2520Wei%2520Lin%2520and%2520Tianxiang%2520Zhang%2520and%2520Qianchun%2520Lu%26entry.1292438233%3D%2520%2520Despite%2520impressive%2520progress%2520in%2520areas%2520like%2520mathematical%2520reasoning%252C%2520large%250Alanguage%2520models%2520still%2520face%2520significant%2520challenges%2520in%2520consistently%2520solving%250Acomplex%2520problems.%2520Drawing%2520inspiration%2520from%2520key%2520human%2520learning%2520strategies%252C%2520we%250Apropose%2520two%2520novel%2520strategies%2520to%2520enhance%2520the%2520capability%2520of%2520large%2520language%2520models%250Ato%2520solve%2520these%2520complex%2520problems.%2520First%252C%2520Adaptive%2520Difficulty%2520Curriculum%2520Learning%250A%2528ADCL%2529%2520is%2520a%2520novel%2520curriculum%2520learning%2520strategy%2520that%2520tackles%2520the%2520Difficulty%250AShift%2520phenomenon%2520%2528i.e.%252C%2520a%2520model%2527s%2520perception%2520of%2520problem%2520difficulty%2520dynamically%250Achanges%2520during%2520training%2529%2520by%2520periodically%2520re-estimating%2520difficulty%2520within%250Aupcoming%2520data%2520batches%2520to%2520maintain%2520alignment%2520with%2520the%2520model%2527s%2520evolving%250Acapabilities.%2520Second%252C%2520Expert-Guided%2520Self-Reformulation%2520%2528EGSR%2529%2520is%2520a%2520novel%250Areinforcement%2520learning%2520strategy%2520that%2520bridges%2520the%2520gap%2520between%2520imitation%2520learning%250Aand%2520pure%2520exploration%2520by%2520guiding%2520models%2520to%2520reformulate%2520expert%2520solutions%2520within%250Atheir%2520own%2520conceptual%2520framework%252C%2520rather%2520than%2520relying%2520on%2520direct%2520imitation%252C%250Afostering%2520deeper%2520understanding%2520and%2520knowledge%2520assimilation.%2520Extensive%250Aexperiments%2520on%2520challenging%2520mathematical%2520reasoning%2520benchmarks%252C%2520using%2520Qwen2.5-7B%250Aas%2520the%2520base%2520model%252C%2520demonstrate%2520that%2520these%2520human-inspired%2520strategies%250Asynergistically%2520and%2520significantly%2520enhance%2520performance.%2520Notably%252C%2520their%2520combined%250Aapplication%2520improves%2520performance%2520over%2520the%2520standard%2520Zero-RL%2520baseline%2520by%252010%2525%2520on%250Athe%2520AIME24%2520benchmark%2520and%252016.6%2525%2520on%2520AIME25.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Like%20Humans%3A%20Advancing%20LLM%20Reasoning%20Capabilities%20via%20Adaptive%0A%20%20Difficulty%20Curriculum%20Learning%20and%20Expert-Guided%20Self-Reformulation&entry.906535625=Enci%20Zhang%20and%20Xingang%20Yan%20and%20Wei%20Lin%20and%20Tianxiang%20Zhang%20and%20Qianchun%20Lu&entry.1292438233=%20%20Despite%20impressive%20progress%20in%20areas%20like%20mathematical%20reasoning%2C%20large%0Alanguage%20models%20still%20face%20significant%20challenges%20in%20consistently%20solving%0Acomplex%20problems.%20Drawing%20inspiration%20from%20key%20human%20learning%20strategies%2C%20we%0Apropose%20two%20novel%20strategies%20to%20enhance%20the%20capability%20of%20large%20language%20models%0Ato%20solve%20these%20complex%20problems.%20First%2C%20Adaptive%20Difficulty%20Curriculum%20Learning%0A%28ADCL%29%20is%20a%20novel%20curriculum%20learning%20strategy%20that%20tackles%20the%20Difficulty%0AShift%20phenomenon%20%28i.e.%2C%20a%20model%27s%20perception%20of%20problem%20difficulty%20dynamically%0Achanges%20during%20training%29%20by%20periodically%20re-estimating%20difficulty%20within%0Aupcoming%20data%20batches%20to%20maintain%20alignment%20with%20the%20model%27s%20evolving%0Acapabilities.%20Second%2C%20Expert-Guided%20Self-Reformulation%20%28EGSR%29%20is%20a%20novel%0Areinforcement%20learning%20strategy%20that%20bridges%20the%20gap%20between%20imitation%20learning%0Aand%20pure%20exploration%20by%20guiding%20models%20to%20reformulate%20expert%20solutions%20within%0Atheir%20own%20conceptual%20framework%2C%20rather%20than%20relying%20on%20direct%20imitation%2C%0Afostering%20deeper%20understanding%20and%20knowledge%20assimilation.%20Extensive%0Aexperiments%20on%20challenging%20mathematical%20reasoning%20benchmarks%2C%20using%20Qwen2.5-7B%0Aas%20the%20base%20model%2C%20demonstrate%20that%20these%20human-inspired%20strategies%0Asynergistically%20and%20significantly%20enhance%20performance.%20Notably%2C%20their%20combined%0Aapplication%20improves%20performance%20over%20the%20standard%20Zero-RL%20baseline%20by%2010%25%20on%0Athe%20AIME24%20benchmark%20and%2016.6%25%20on%20AIME25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08364v2&entry.124074799=Read"},
{"title": "HAM: Hierarchical Adapter Merging for Scalable Continual Learning", "author": "Eric Nuertey Coleman and Luigi Quarantiello and Samrat Mukherjee and Julio Hurtado and Vincenzo Lomonaco", "abstract": "  Continual learning is an essential capability of human cognition, yet it\nposes significant challenges for current deep learning models. The primary\nissue is that new knowledge can interfere with previously learned information,\ncausing the model to forget earlier knowledge in favor of the new, a phenomenon\nknown as catastrophic forgetting. Although large pre-trained models can\npartially mitigate forgetting by leveraging their existing knowledge and\nover-parameterization, they often struggle when confronted with novel data\ndistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\nenable efficient adaptation to new knowledge. However, they still face\nchallenges in scaling to dynamic learning scenarios and long sequences of\ntasks, as maintaining one adapter per task introduces complexity and increases\nthe potential for interference. In this paper, we introduce Hierarchical\nAdapters Merging (HAM), a novel framework that dynamically combines adapters\nfrom different tasks during training. This approach enables HAM to scale\neffectively, allowing it to manage more tasks than competing baselines with\nimproved efficiency. To achieve this, HAM maintains a fixed set of groups that\nhierarchically consolidate new adapters. For each task, HAM trains a low-rank\nadapter along with an importance scalar, then dynamically groups tasks based on\nadapter similarity. Within each group, adapters are pruned, scaled and merge,\nfacilitating transfer learning between related tasks. Extensive experiments on\nthree vision benchmarks show that HAM significantly outperforms\nstate-of-the-art methods, particularly as the number of tasks increases.\n", "link": "http://arxiv.org/abs/2509.13211v2", "date": "2025-09-17", "relevancy": 2.7801, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&body=Title%3A%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning%0AAuthor%3A%20Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAM%253A%2520Hierarchical%2520Adapter%2520Merging%2520for%2520Scalable%2520Continual%2520Learning%26entry.906535625%3DEric%2520Nuertey%2520Coleman%2520and%2520Luigi%2520Quarantiello%2520and%2520Samrat%2520Mukherjee%2520and%2520Julio%2520Hurtado%2520and%2520Vincenzo%2520Lomonaco%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520an%2520essential%2520capability%2520of%2520human%2520cognition%252C%2520yet%2520it%250Aposes%2520significant%2520challenges%2520for%2520current%2520deep%2520learning%2520models.%2520The%2520primary%250Aissue%2520is%2520that%2520new%2520knowledge%2520can%2520interfere%2520with%2520previously%2520learned%2520information%252C%250Acausing%2520the%2520model%2520to%2520forget%2520earlier%2520knowledge%2520in%2520favor%2520of%2520the%2520new%252C%2520a%2520phenomenon%250Aknown%2520as%2520catastrophic%2520forgetting.%2520Although%2520large%2520pre-trained%2520models%2520can%250Apartially%2520mitigate%2520forgetting%2520by%2520leveraging%2520their%2520existing%2520knowledge%2520and%250Aover-parameterization%252C%2520they%2520often%2520struggle%2520when%2520confronted%2520with%2520novel%2520data%250Adistributions.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520LoRA%252C%250Aenable%2520efficient%2520adaptation%2520to%2520new%2520knowledge.%2520However%252C%2520they%2520still%2520face%250Achallenges%2520in%2520scaling%2520to%2520dynamic%2520learning%2520scenarios%2520and%2520long%2520sequences%2520of%250Atasks%252C%2520as%2520maintaining%2520one%2520adapter%2520per%2520task%2520introduces%2520complexity%2520and%2520increases%250Athe%2520potential%2520for%2520interference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Hierarchical%250AAdapters%2520Merging%2520%2528HAM%2529%252C%2520a%2520novel%2520framework%2520that%2520dynamically%2520combines%2520adapters%250Afrom%2520different%2520tasks%2520during%2520training.%2520This%2520approach%2520enables%2520HAM%2520to%2520scale%250Aeffectively%252C%2520allowing%2520it%2520to%2520manage%2520more%2520tasks%2520than%2520competing%2520baselines%2520with%250Aimproved%2520efficiency.%2520To%2520achieve%2520this%252C%2520HAM%2520maintains%2520a%2520fixed%2520set%2520of%2520groups%2520that%250Ahierarchically%2520consolidate%2520new%2520adapters.%2520For%2520each%2520task%252C%2520HAM%2520trains%2520a%2520low-rank%250Aadapter%2520along%2520with%2520an%2520importance%2520scalar%252C%2520then%2520dynamically%2520groups%2520tasks%2520based%2520on%250Aadapter%2520similarity.%2520Within%2520each%2520group%252C%2520adapters%2520are%2520pruned%252C%2520scaled%2520and%2520merge%252C%250Afacilitating%2520transfer%2520learning%2520between%2520related%2520tasks.%2520Extensive%2520experiments%2520on%250Athree%2520vision%2520benchmarks%2520show%2520that%2520HAM%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%252C%2520particularly%2520as%2520the%2520number%2520of%2520tasks%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&entry.906535625=Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13211v2&entry.124074799=Read"},
{"title": "Identity-Preserving Text-to-Video Generation Guided by Simple yet\n  Effective Spatial-Temporal Decoupled Representations", "author": "Yuji Wang and Moran Li and Xiaobin Hu and Ran Yi and Jiangning Zhang and Han Feng and Weijian Cao and Yabiao Wang and Chengjie Wang and Lizhuang Ma", "abstract": "  Identity-preserving text-to-video (IPT2V) generation, which aims to create\nhigh-fidelity videos with consistent human identity, has become crucial for\ndownstream applications. However, current end-to-end frameworks suffer a\ncritical spatial-temporal trade-off: optimizing for spatially coherent layouts\nof key elements (e.g., character identity preservation) often compromises\ninstruction-compliant temporal smoothness, while prioritizing dynamic realism\nrisks disrupting the spatial coherence of visual structures. To tackle this\nissue, we propose a simple yet effective spatial-temporal decoupled framework\nthat decomposes representations into spatial features for layouts and temporal\nfeatures for motion dynamics. Specifically, our paper proposes a semantic\nprompt optimization mechanism and stage-wise decoupled generation paradigm. The\nformer module decouples the prompt into spatial and temporal components.\nAligned with the subsequent stage-wise decoupled approach, the spatial prompts\nguide the text-to-image (T2I) stage to generate coherent spatial features,\nwhile the temporal prompts direct the sequential image-to-video (I2V) stage to\nensure motion consistency. Experimental results validate that our approach\nachieves excellent spatiotemporal consistency, demonstrating outstanding\nperformance in identity preservation, text relevance, and video quality. By\nleveraging this simple yet robust mechanism, our algorithm secures the\nrunner-up position in 2025 ACM MultiMedia Challenge.\n", "link": "http://arxiv.org/abs/2507.04705v2", "date": "2025-09-17", "relevancy": 2.7659, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7474}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6823}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity-Preserving%20Text-to-Video%20Generation%20Guided%20by%20Simple%20yet%0A%20%20Effective%20Spatial-Temporal%20Decoupled%20Representations&body=Title%3A%20Identity-Preserving%20Text-to-Video%20Generation%20Guided%20by%20Simple%20yet%0A%20%20Effective%20Spatial-Temporal%20Decoupled%20Representations%0AAuthor%3A%20Yuji%20Wang%20and%20Moran%20Li%20and%20Xiaobin%20Hu%20and%20Ran%20Yi%20and%20Jiangning%20Zhang%20and%20Han%20Feng%20and%20Weijian%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Identity-preserving%20text-to-video%20%28IPT2V%29%20generation%2C%20which%20aims%20to%20create%0Ahigh-fidelity%20videos%20with%20consistent%20human%20identity%2C%20has%20become%20crucial%20for%0Adownstream%20applications.%20However%2C%20current%20end-to-end%20frameworks%20suffer%20a%0Acritical%20spatial-temporal%20trade-off%3A%20optimizing%20for%20spatially%20coherent%20layouts%0Aof%20key%20elements%20%28e.g.%2C%20character%20identity%20preservation%29%20often%20compromises%0Ainstruction-compliant%20temporal%20smoothness%2C%20while%20prioritizing%20dynamic%20realism%0Arisks%20disrupting%20the%20spatial%20coherence%20of%20visual%20structures.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20effective%20spatial-temporal%20decoupled%20framework%0Athat%20decomposes%20representations%20into%20spatial%20features%20for%20layouts%20and%20temporal%0Afeatures%20for%20motion%20dynamics.%20Specifically%2C%20our%20paper%20proposes%20a%20semantic%0Aprompt%20optimization%20mechanism%20and%20stage-wise%20decoupled%20generation%20paradigm.%20The%0Aformer%20module%20decouples%20the%20prompt%20into%20spatial%20and%20temporal%20components.%0AAligned%20with%20the%20subsequent%20stage-wise%20decoupled%20approach%2C%20the%20spatial%20prompts%0Aguide%20the%20text-to-image%20%28T2I%29%20stage%20to%20generate%20coherent%20spatial%20features%2C%0Awhile%20the%20temporal%20prompts%20direct%20the%20sequential%20image-to-video%20%28I2V%29%20stage%20to%0Aensure%20motion%20consistency.%20Experimental%20results%20validate%20that%20our%20approach%0Aachieves%20excellent%20spatiotemporal%20consistency%2C%20demonstrating%20outstanding%0Aperformance%20in%20identity%20preservation%2C%20text%20relevance%2C%20and%20video%20quality.%20By%0Aleveraging%20this%20simple%20yet%20robust%20mechanism%2C%20our%20algorithm%20secures%20the%0Arunner-up%20position%20in%202025%20ACM%20MultiMedia%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity-Preserving%2520Text-to-Video%2520Generation%2520Guided%2520by%2520Simple%2520yet%250A%2520%2520Effective%2520Spatial-Temporal%2520Decoupled%2520Representations%26entry.906535625%3DYuji%2520Wang%2520and%2520Moran%2520Li%2520and%2520Xiaobin%2520Hu%2520and%2520Ran%2520Yi%2520and%2520Jiangning%2520Zhang%2520and%2520Han%2520Feng%2520and%2520Weijian%2520Cao%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Identity-preserving%2520text-to-video%2520%2528IPT2V%2529%2520generation%252C%2520which%2520aims%2520to%2520create%250Ahigh-fidelity%2520videos%2520with%2520consistent%2520human%2520identity%252C%2520has%2520become%2520crucial%2520for%250Adownstream%2520applications.%2520However%252C%2520current%2520end-to-end%2520frameworks%2520suffer%2520a%250Acritical%2520spatial-temporal%2520trade-off%253A%2520optimizing%2520for%2520spatially%2520coherent%2520layouts%250Aof%2520key%2520elements%2520%2528e.g.%252C%2520character%2520identity%2520preservation%2529%2520often%2520compromises%250Ainstruction-compliant%2520temporal%2520smoothness%252C%2520while%2520prioritizing%2520dynamic%2520realism%250Arisks%2520disrupting%2520the%2520spatial%2520coherence%2520of%2520visual%2520structures.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520spatial-temporal%2520decoupled%2520framework%250Athat%2520decomposes%2520representations%2520into%2520spatial%2520features%2520for%2520layouts%2520and%2520temporal%250Afeatures%2520for%2520motion%2520dynamics.%2520Specifically%252C%2520our%2520paper%2520proposes%2520a%2520semantic%250Aprompt%2520optimization%2520mechanism%2520and%2520stage-wise%2520decoupled%2520generation%2520paradigm.%2520The%250Aformer%2520module%2520decouples%2520the%2520prompt%2520into%2520spatial%2520and%2520temporal%2520components.%250AAligned%2520with%2520the%2520subsequent%2520stage-wise%2520decoupled%2520approach%252C%2520the%2520spatial%2520prompts%250Aguide%2520the%2520text-to-image%2520%2528T2I%2529%2520stage%2520to%2520generate%2520coherent%2520spatial%2520features%252C%250Awhile%2520the%2520temporal%2520prompts%2520direct%2520the%2520sequential%2520image-to-video%2520%2528I2V%2529%2520stage%2520to%250Aensure%2520motion%2520consistency.%2520Experimental%2520results%2520validate%2520that%2520our%2520approach%250Aachieves%2520excellent%2520spatiotemporal%2520consistency%252C%2520demonstrating%2520outstanding%250Aperformance%2520in%2520identity%2520preservation%252C%2520text%2520relevance%252C%2520and%2520video%2520quality.%2520By%250Aleveraging%2520this%2520simple%2520yet%2520robust%2520mechanism%252C%2520our%2520algorithm%2520secures%2520the%250Arunner-up%2520position%2520in%25202025%2520ACM%2520MultiMedia%2520Challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity-Preserving%20Text-to-Video%20Generation%20Guided%20by%20Simple%20yet%0A%20%20Effective%20Spatial-Temporal%20Decoupled%20Representations&entry.906535625=Yuji%20Wang%20and%20Moran%20Li%20and%20Xiaobin%20Hu%20and%20Ran%20Yi%20and%20Jiangning%20Zhang%20and%20Han%20Feng%20and%20Weijian%20Cao%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Identity-preserving%20text-to-video%20%28IPT2V%29%20generation%2C%20which%20aims%20to%20create%0Ahigh-fidelity%20videos%20with%20consistent%20human%20identity%2C%20has%20become%20crucial%20for%0Adownstream%20applications.%20However%2C%20current%20end-to-end%20frameworks%20suffer%20a%0Acritical%20spatial-temporal%20trade-off%3A%20optimizing%20for%20spatially%20coherent%20layouts%0Aof%20key%20elements%20%28e.g.%2C%20character%20identity%20preservation%29%20often%20compromises%0Ainstruction-compliant%20temporal%20smoothness%2C%20while%20prioritizing%20dynamic%20realism%0Arisks%20disrupting%20the%20spatial%20coherence%20of%20visual%20structures.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20yet%20effective%20spatial-temporal%20decoupled%20framework%0Athat%20decomposes%20representations%20into%20spatial%20features%20for%20layouts%20and%20temporal%0Afeatures%20for%20motion%20dynamics.%20Specifically%2C%20our%20paper%20proposes%20a%20semantic%0Aprompt%20optimization%20mechanism%20and%20stage-wise%20decoupled%20generation%20paradigm.%20The%0Aformer%20module%20decouples%20the%20prompt%20into%20spatial%20and%20temporal%20components.%0AAligned%20with%20the%20subsequent%20stage-wise%20decoupled%20approach%2C%20the%20spatial%20prompts%0Aguide%20the%20text-to-image%20%28T2I%29%20stage%20to%20generate%20coherent%20spatial%20features%2C%0Awhile%20the%20temporal%20prompts%20direct%20the%20sequential%20image-to-video%20%28I2V%29%20stage%20to%0Aensure%20motion%20consistency.%20Experimental%20results%20validate%20that%20our%20approach%0Aachieves%20excellent%20spatiotemporal%20consistency%2C%20demonstrating%20outstanding%0Aperformance%20in%20identity%20preservation%2C%20text%20relevance%2C%20and%20video%20quality.%20By%0Aleveraging%20this%20simple%20yet%20robust%20mechanism%2C%20our%20algorithm%20secures%20the%0Arunner-up%20position%20in%202025%20ACM%20MultiMedia%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04705v2&entry.124074799=Read"},
{"title": "Cin\u00e9aste: A Fine-grained Contextual Movie Question Answering\n  Benchmark", "author": "Nisarg A. Shah and Amir Ziai and Chaitanya Ekanadham and Vishal M. Patel", "abstract": "  While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.\n", "link": "http://arxiv.org/abs/2509.14227v1", "date": "2025-09-17", "relevancy": 2.7518, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cin%C3%A9aste%3A%20A%20Fine-grained%20Contextual%20Movie%20Question%20Answering%0A%20%20Benchmark&body=Title%3A%20Cin%C3%A9aste%3A%20A%20Fine-grained%20Contextual%20Movie%20Question%20Answering%0A%20%20Benchmark%0AAuthor%3A%20Nisarg%20A.%20Shah%20and%20Amir%20Ziai%20and%20Chaitanya%20Ekanadham%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20While%20recent%20advancements%20in%20vision-language%20models%20have%20improved%20video%0Aunderstanding%2C%20diagnosing%20their%20capacity%20for%20deep%2C%20narrative%20comprehension%0Aremains%20a%20challenge.%20Existing%20benchmarks%20often%20test%20short-clip%20recognition%20or%0Ause%20template-based%20questions%2C%20leaving%20a%20critical%20gap%20in%20evaluating%20fine-grained%0Areasoning%20over%20long-form%20narrative%20content.%20To%20address%20these%20gaps%2C%20we%20introduce%0A%24%5Cmathsf%7BCin%5Cacute%7Be%7Daste%7D%24%2C%20a%20comprehensive%20benchmark%20for%20long-form%20movie%0Aunderstanding.%20Our%20dataset%20comprises%203%2C119%20multiple-choice%20question-answer%0Apairs%20derived%20from%201%2C805%20scenes%20across%20200%20diverse%20movies%2C%20spanning%20five%20novel%0Afine-grained%20contextual%20reasoning%20categories.%20We%20use%20GPT-4o%20to%20generate%0Adiverse%2C%20context-rich%20questions%20by%20integrating%20visual%20descriptions%2C%20captions%2C%0Ascene%20titles%2C%20and%20summaries%2C%20which%20require%20deep%20narrative%20understanding.%20To%0Aensure%20high-quality%20evaluation%2C%20our%20pipeline%20incorporates%20a%20two-stage%20filtering%0Aprocess%3A%20Context-Independence%20filtering%20ensures%20questions%20require%20video%0Acontext%2C%20while%20Contextual%20Veracity%20filtering%20validates%20factual%20consistency%0Aagainst%20the%20movie%20content%2C%20mitigating%20hallucinations.%20Experiments%20show%20that%0Aexisting%20MLLMs%20struggle%20on%20%24%5Cmathsf%7BCin%5Cacute%7Be%7Daste%7D%24%3B%20our%20analysis%20reveals%0Athat%20long-range%20temporal%20reasoning%20is%20a%20primary%20bottleneck%2C%20with%20the%20top%0Aopen-source%20model%20achieving%20only%2063.15%5C%25%20accuracy.%20This%20underscores%20significant%0Achallenges%20in%20fine-grained%20contextual%20understanding%20and%20the%20need%20for%0Aadvancements%20in%20long-form%20movie%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCin%25C3%25A9aste%253A%2520A%2520Fine-grained%2520Contextual%2520Movie%2520Question%2520Answering%250A%2520%2520Benchmark%26entry.906535625%3DNisarg%2520A.%2520Shah%2520and%2520Amir%2520Ziai%2520and%2520Chaitanya%2520Ekanadham%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520While%2520recent%2520advancements%2520in%2520vision-language%2520models%2520have%2520improved%2520video%250Aunderstanding%252C%2520diagnosing%2520their%2520capacity%2520for%2520deep%252C%2520narrative%2520comprehension%250Aremains%2520a%2520challenge.%2520Existing%2520benchmarks%2520often%2520test%2520short-clip%2520recognition%2520or%250Ause%2520template-based%2520questions%252C%2520leaving%2520a%2520critical%2520gap%2520in%2520evaluating%2520fine-grained%250Areasoning%2520over%2520long-form%2520narrative%2520content.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%250A%2524%255Cmathsf%257BCin%255Cacute%257Be%257Daste%257D%2524%252C%2520a%2520comprehensive%2520benchmark%2520for%2520long-form%2520movie%250Aunderstanding.%2520Our%2520dataset%2520comprises%25203%252C119%2520multiple-choice%2520question-answer%250Apairs%2520derived%2520from%25201%252C805%2520scenes%2520across%2520200%2520diverse%2520movies%252C%2520spanning%2520five%2520novel%250Afine-grained%2520contextual%2520reasoning%2520categories.%2520We%2520use%2520GPT-4o%2520to%2520generate%250Adiverse%252C%2520context-rich%2520questions%2520by%2520integrating%2520visual%2520descriptions%252C%2520captions%252C%250Ascene%2520titles%252C%2520and%2520summaries%252C%2520which%2520require%2520deep%2520narrative%2520understanding.%2520To%250Aensure%2520high-quality%2520evaluation%252C%2520our%2520pipeline%2520incorporates%2520a%2520two-stage%2520filtering%250Aprocess%253A%2520Context-Independence%2520filtering%2520ensures%2520questions%2520require%2520video%250Acontext%252C%2520while%2520Contextual%2520Veracity%2520filtering%2520validates%2520factual%2520consistency%250Aagainst%2520the%2520movie%2520content%252C%2520mitigating%2520hallucinations.%2520Experiments%2520show%2520that%250Aexisting%2520MLLMs%2520struggle%2520on%2520%2524%255Cmathsf%257BCin%255Cacute%257Be%257Daste%257D%2524%253B%2520our%2520analysis%2520reveals%250Athat%2520long-range%2520temporal%2520reasoning%2520is%2520a%2520primary%2520bottleneck%252C%2520with%2520the%2520top%250Aopen-source%2520model%2520achieving%2520only%252063.15%255C%2525%2520accuracy.%2520This%2520underscores%2520significant%250Achallenges%2520in%2520fine-grained%2520contextual%2520understanding%2520and%2520the%2520need%2520for%250Aadvancements%2520in%2520long-form%2520movie%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cin%C3%A9aste%3A%20A%20Fine-grained%20Contextual%20Movie%20Question%20Answering%0A%20%20Benchmark&entry.906535625=Nisarg%20A.%20Shah%20and%20Amir%20Ziai%20and%20Chaitanya%20Ekanadham%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20While%20recent%20advancements%20in%20vision-language%20models%20have%20improved%20video%0Aunderstanding%2C%20diagnosing%20their%20capacity%20for%20deep%2C%20narrative%20comprehension%0Aremains%20a%20challenge.%20Existing%20benchmarks%20often%20test%20short-clip%20recognition%20or%0Ause%20template-based%20questions%2C%20leaving%20a%20critical%20gap%20in%20evaluating%20fine-grained%0Areasoning%20over%20long-form%20narrative%20content.%20To%20address%20these%20gaps%2C%20we%20introduce%0A%24%5Cmathsf%7BCin%5Cacute%7Be%7Daste%7D%24%2C%20a%20comprehensive%20benchmark%20for%20long-form%20movie%0Aunderstanding.%20Our%20dataset%20comprises%203%2C119%20multiple-choice%20question-answer%0Apairs%20derived%20from%201%2C805%20scenes%20across%20200%20diverse%20movies%2C%20spanning%20five%20novel%0Afine-grained%20contextual%20reasoning%20categories.%20We%20use%20GPT-4o%20to%20generate%0Adiverse%2C%20context-rich%20questions%20by%20integrating%20visual%20descriptions%2C%20captions%2C%0Ascene%20titles%2C%20and%20summaries%2C%20which%20require%20deep%20narrative%20understanding.%20To%0Aensure%20high-quality%20evaluation%2C%20our%20pipeline%20incorporates%20a%20two-stage%20filtering%0Aprocess%3A%20Context-Independence%20filtering%20ensures%20questions%20require%20video%0Acontext%2C%20while%20Contextual%20Veracity%20filtering%20validates%20factual%20consistency%0Aagainst%20the%20movie%20content%2C%20mitigating%20hallucinations.%20Experiments%20show%20that%0Aexisting%20MLLMs%20struggle%20on%20%24%5Cmathsf%7BCin%5Cacute%7Be%7Daste%7D%24%3B%20our%20analysis%20reveals%0Athat%20long-range%20temporal%20reasoning%20is%20a%20primary%20bottleneck%2C%20with%20the%20top%0Aopen-source%20model%20achieving%20only%2063.15%5C%25%20accuracy.%20This%20underscores%20significant%0Achallenges%20in%20fine-grained%20contextual%20understanding%20and%20the%20need%20for%0Aadvancements%20in%20long-form%20movie%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14227v1&entry.124074799=Read"},
{"title": "GenExam: A Multidisciplinary Text-to-Image Exam", "author": "Zhaokai Wang and Penghao Yin and Xiangyu Zhao and Changyao Tian and Yu Qiao and Wenhai Wang and Jifeng Dai and Gen Luo", "abstract": "  Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.\n", "link": "http://arxiv.org/abs/2509.14232v1", "date": "2025-09-17", "relevancy": 2.7347, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5585}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam&body=Title%3A%20GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam%0AAuthor%3A%20Zhaokai%20Wang%20and%20Penghao%20Yin%20and%20Xiangyu%20Zhao%20and%20Changyao%20Tian%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Gen%20Luo%0AAbstract%3A%20%20%20Exams%20are%20a%20fundamental%20test%20of%20expert-level%20intelligence%20and%20require%0Aintegrated%20understanding%2C%20reasoning%2C%20and%20generation.%20Existing%20exam-style%0Abenchmarks%20mainly%20focus%20on%20understanding%20and%20reasoning%20tasks%2C%20and%20current%0Ageneration%20benchmarks%20emphasize%20the%20illustration%20of%20world%20knowledge%20and%20visual%0Aconcepts%2C%20neglecting%20the%20evaluation%20of%20rigorous%20drawing%20exams.%20We%20introduce%0AGenExam%2C%20the%20first%20benchmark%20for%20multidisciplinary%20text-to-image%20exams%2C%0Afeaturing%201%2C000%20samples%20across%2010%20subjects%20with%20exam-style%20prompts%20organized%0Aunder%20a%20four-level%20taxonomy.%20Each%20problem%20is%20equipped%20with%20ground-truth%20images%0Aand%20fine-grained%20scoring%20points%20to%20enable%20a%20precise%20evaluation%20of%20semantic%0Acorrectness%20and%20visual%20plausibility.%20Experiments%20show%20that%20even%0Astate-of-the-art%20models%20such%20as%20GPT-Image-1%20and%20Gemini-2.5-Flash-Image%20achieve%0Aless%20than%2015%25%20strict%20scores%2C%20and%20most%20models%20yield%20almost%200%25%2C%20suggesting%20the%0Agreat%20challenge%20of%20our%20benchmark.%20By%20framing%20image%20generation%20as%20an%20exam%2C%0AGenExam%20offers%20a%20rigorous%20assessment%20of%20models%27%20ability%20to%20integrate%20knowledge%2C%0Areasoning%2C%20and%20generation%2C%20providing%20insights%20on%20the%20path%20to%20general%20AGI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenExam%253A%2520A%2520Multidisciplinary%2520Text-to-Image%2520Exam%26entry.906535625%3DZhaokai%2520Wang%2520and%2520Penghao%2520Yin%2520and%2520Xiangyu%2520Zhao%2520and%2520Changyao%2520Tian%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520Exams%2520are%2520a%2520fundamental%2520test%2520of%2520expert-level%2520intelligence%2520and%2520require%250Aintegrated%2520understanding%252C%2520reasoning%252C%2520and%2520generation.%2520Existing%2520exam-style%250Abenchmarks%2520mainly%2520focus%2520on%2520understanding%2520and%2520reasoning%2520tasks%252C%2520and%2520current%250Ageneration%2520benchmarks%2520emphasize%2520the%2520illustration%2520of%2520world%2520knowledge%2520and%2520visual%250Aconcepts%252C%2520neglecting%2520the%2520evaluation%2520of%2520rigorous%2520drawing%2520exams.%2520We%2520introduce%250AGenExam%252C%2520the%2520first%2520benchmark%2520for%2520multidisciplinary%2520text-to-image%2520exams%252C%250Afeaturing%25201%252C000%2520samples%2520across%252010%2520subjects%2520with%2520exam-style%2520prompts%2520organized%250Aunder%2520a%2520four-level%2520taxonomy.%2520Each%2520problem%2520is%2520equipped%2520with%2520ground-truth%2520images%250Aand%2520fine-grained%2520scoring%2520points%2520to%2520enable%2520a%2520precise%2520evaluation%2520of%2520semantic%250Acorrectness%2520and%2520visual%2520plausibility.%2520Experiments%2520show%2520that%2520even%250Astate-of-the-art%2520models%2520such%2520as%2520GPT-Image-1%2520and%2520Gemini-2.5-Flash-Image%2520achieve%250Aless%2520than%252015%2525%2520strict%2520scores%252C%2520and%2520most%2520models%2520yield%2520almost%25200%2525%252C%2520suggesting%2520the%250Agreat%2520challenge%2520of%2520our%2520benchmark.%2520By%2520framing%2520image%2520generation%2520as%2520an%2520exam%252C%250AGenExam%2520offers%2520a%2520rigorous%2520assessment%2520of%2520models%2527%2520ability%2520to%2520integrate%2520knowledge%252C%250Areasoning%252C%2520and%2520generation%252C%2520providing%2520insights%2520on%2520the%2520path%2520to%2520general%2520AGI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam&entry.906535625=Zhaokai%20Wang%20and%20Penghao%20Yin%20and%20Xiangyu%20Zhao%20and%20Changyao%20Tian%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Gen%20Luo&entry.1292438233=%20%20Exams%20are%20a%20fundamental%20test%20of%20expert-level%20intelligence%20and%20require%0Aintegrated%20understanding%2C%20reasoning%2C%20and%20generation.%20Existing%20exam-style%0Abenchmarks%20mainly%20focus%20on%20understanding%20and%20reasoning%20tasks%2C%20and%20current%0Ageneration%20benchmarks%20emphasize%20the%20illustration%20of%20world%20knowledge%20and%20visual%0Aconcepts%2C%20neglecting%20the%20evaluation%20of%20rigorous%20drawing%20exams.%20We%20introduce%0AGenExam%2C%20the%20first%20benchmark%20for%20multidisciplinary%20text-to-image%20exams%2C%0Afeaturing%201%2C000%20samples%20across%2010%20subjects%20with%20exam-style%20prompts%20organized%0Aunder%20a%20four-level%20taxonomy.%20Each%20problem%20is%20equipped%20with%20ground-truth%20images%0Aand%20fine-grained%20scoring%20points%20to%20enable%20a%20precise%20evaluation%20of%20semantic%0Acorrectness%20and%20visual%20plausibility.%20Experiments%20show%20that%20even%0Astate-of-the-art%20models%20such%20as%20GPT-Image-1%20and%20Gemini-2.5-Flash-Image%20achieve%0Aless%20than%2015%25%20strict%20scores%2C%20and%20most%20models%20yield%20almost%200%25%2C%20suggesting%20the%0Agreat%20challenge%20of%20our%20benchmark.%20By%20framing%20image%20generation%20as%20an%20exam%2C%0AGenExam%20offers%20a%20rigorous%20assessment%20of%20models%27%20ability%20to%20integrate%20knowledge%2C%0Areasoning%2C%20and%20generation%2C%20providing%20insights%20on%20the%20path%20to%20general%20AGI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14232v1&entry.124074799=Read"},
{"title": "Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost\n  imaging", "author": "Mathieu Manni and Dmitry Karpov and K. Joost Batenburg and Sharon Shwartz and Nicola Vigan\u00f2", "abstract": "  We present a new self-supervised deep-learning-based Ghost Imaging (GI)\nreconstruction method, which provides unparalleled reconstruction performance\nfor noisy acquisitions among unsupervised methods. We present the supporting\nmathematical framework and results from theoretical and real data use cases.\nSelf-supervision removes the need for clean reference data while offering\nstrong noise reduction. This provides the necessary tools for addressing\nsignal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge\nlow-light GI scenarios. Notable examples include micro- and nano-scale x-ray\nemission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples.\nTheir applications include in-vivo and in-operando case studies for biological\nsamples and batteries.\n", "link": "http://arxiv.org/abs/2504.10288v2", "date": "2025-09-17", "relevancy": 2.726, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.557}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5409}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise2Ghost%3A%20Self-supervised%20deep%20convolutional%20reconstruction%20for%20ghost%0A%20%20imaging&body=Title%3A%20Noise2Ghost%3A%20Self-supervised%20deep%20convolutional%20reconstruction%20for%20ghost%0A%20%20imaging%0AAuthor%3A%20Mathieu%20Manni%20and%20Dmitry%20Karpov%20and%20K.%20Joost%20Batenburg%20and%20Sharon%20Shwartz%20and%20Nicola%20Vigan%C3%B2%0AAbstract%3A%20%20%20We%20present%20a%20new%20self-supervised%20deep-learning-based%20Ghost%20Imaging%20%28GI%29%0Areconstruction%20method%2C%20which%20provides%20unparalleled%20reconstruction%20performance%0Afor%20noisy%20acquisitions%20among%20unsupervised%20methods.%20We%20present%20the%20supporting%0Amathematical%20framework%20and%20results%20from%20theoretical%20and%20real%20data%20use%20cases.%0ASelf-supervision%20removes%20the%20need%20for%20clean%20reference%20data%20while%20offering%0Astrong%20noise%20reduction.%20This%20provides%20the%20necessary%20tools%20for%20addressing%0Asignal-to-noise%20ratio%20concerns%20for%20GI%20acquisitions%20in%20emerging%20and%20cutting-edge%0Alow-light%20GI%20scenarios.%20Notable%20examples%20include%20micro-%20and%20nano-scale%20x-ray%0Aemission%20imaging%2C%20e.g.%2C%20x-ray%20fluorescence%20imaging%20of%20dose-sensitive%20samples.%0ATheir%20applications%20include%20in-vivo%20and%20in-operando%20case%20studies%20for%20biological%0Asamples%20and%20batteries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise2Ghost%253A%2520Self-supervised%2520deep%2520convolutional%2520reconstruction%2520for%2520ghost%250A%2520%2520imaging%26entry.906535625%3DMathieu%2520Manni%2520and%2520Dmitry%2520Karpov%2520and%2520K.%2520Joost%2520Batenburg%2520and%2520Sharon%2520Shwartz%2520and%2520Nicola%2520Vigan%25C3%25B2%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520self-supervised%2520deep-learning-based%2520Ghost%2520Imaging%2520%2528GI%2529%250Areconstruction%2520method%252C%2520which%2520provides%2520unparalleled%2520reconstruction%2520performance%250Afor%2520noisy%2520acquisitions%2520among%2520unsupervised%2520methods.%2520We%2520present%2520the%2520supporting%250Amathematical%2520framework%2520and%2520results%2520from%2520theoretical%2520and%2520real%2520data%2520use%2520cases.%250ASelf-supervision%2520removes%2520the%2520need%2520for%2520clean%2520reference%2520data%2520while%2520offering%250Astrong%2520noise%2520reduction.%2520This%2520provides%2520the%2520necessary%2520tools%2520for%2520addressing%250Asignal-to-noise%2520ratio%2520concerns%2520for%2520GI%2520acquisitions%2520in%2520emerging%2520and%2520cutting-edge%250Alow-light%2520GI%2520scenarios.%2520Notable%2520examples%2520include%2520micro-%2520and%2520nano-scale%2520x-ray%250Aemission%2520imaging%252C%2520e.g.%252C%2520x-ray%2520fluorescence%2520imaging%2520of%2520dose-sensitive%2520samples.%250ATheir%2520applications%2520include%2520in-vivo%2520and%2520in-operando%2520case%2520studies%2520for%2520biological%250Asamples%2520and%2520batteries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise2Ghost%3A%20Self-supervised%20deep%20convolutional%20reconstruction%20for%20ghost%0A%20%20imaging&entry.906535625=Mathieu%20Manni%20and%20Dmitry%20Karpov%20and%20K.%20Joost%20Batenburg%20and%20Sharon%20Shwartz%20and%20Nicola%20Vigan%C3%B2&entry.1292438233=%20%20We%20present%20a%20new%20self-supervised%20deep-learning-based%20Ghost%20Imaging%20%28GI%29%0Areconstruction%20method%2C%20which%20provides%20unparalleled%20reconstruction%20performance%0Afor%20noisy%20acquisitions%20among%20unsupervised%20methods.%20We%20present%20the%20supporting%0Amathematical%20framework%20and%20results%20from%20theoretical%20and%20real%20data%20use%20cases.%0ASelf-supervision%20removes%20the%20need%20for%20clean%20reference%20data%20while%20offering%0Astrong%20noise%20reduction.%20This%20provides%20the%20necessary%20tools%20for%20addressing%0Asignal-to-noise%20ratio%20concerns%20for%20GI%20acquisitions%20in%20emerging%20and%20cutting-edge%0Alow-light%20GI%20scenarios.%20Notable%20examples%20include%20micro-%20and%20nano-scale%20x-ray%0Aemission%20imaging%2C%20e.g.%2C%20x-ray%20fluorescence%20imaging%20of%20dose-sensitive%20samples.%0ATheir%20applications%20include%20in-vivo%20and%20in-operando%20case%20studies%20for%20biological%0Asamples%20and%20batteries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10288v2&entry.124074799=Read"},
{"title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for\n  Audio-Visual Video Parsing", "author": "Yaru Chen and Ruohao Guo and Liting Gao and Yang Xiang and Qingyu Luo and Zhenbo Li and Wenwu Wang", "abstract": "  Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.\n", "link": "http://arxiv.org/abs/2509.14097v1", "date": "2025-09-17", "relevancy": 2.7229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teacher-Guided%20Pseudo%20Supervision%20and%20Cross-Modal%20Alignment%20for%0A%20%20Audio-Visual%20Video%20Parsing&body=Title%3A%20Teacher-Guided%20Pseudo%20Supervision%20and%20Cross-Modal%20Alignment%20for%0A%20%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Yaru%20Chen%20and%20Ruohao%20Guo%20and%20Liting%20Gao%20and%20Yang%20Xiang%20and%20Qingyu%20Luo%20and%20Zhenbo%20Li%20and%20Wenwu%20Wang%0AAbstract%3A%20%20%20Weakly-supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20seeks%20to%20detect%20audible%2C%0Avisible%2C%20and%20audio-visual%20events%20without%20temporal%20annotations.%20Previous%20work%0Ahas%20emphasized%20refining%20global%20predictions%20through%20contrastive%20or%20collaborative%0Alearning%2C%20but%20neglected%20stable%20segment-level%20supervision%20and%20class-aware%0Across-modal%20alignment.%20To%20address%20this%2C%20we%20propose%20two%20strategies%3A%20%281%29%20an%0Aexponential%20moving%20average%20%28EMA%29-guided%20pseudo%20supervision%20framework%20that%0Agenerates%20reliable%20segment-level%20masks%20via%20adaptive%20thresholds%20or%20top-k%0Aselection%2C%20offering%20stable%20temporal%20guidance%20beyond%20video-level%20labels%3B%20and%20%282%29%0Aa%20class-aware%20cross-modal%20agreement%20%28CMA%29%20loss%20that%20aligns%20audio%20and%20visual%0Aembeddings%20at%20reliable%20segment-class%20pairs%2C%20ensuring%20consistency%20across%0Amodalities%20while%20preserving%20temporal%20structure.%20Evaluations%20on%20LLP%20and%20UnAV-100%0Adatasets%20shows%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%0Aacross%20multiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeacher-Guided%2520Pseudo%2520Supervision%2520and%2520Cross-Modal%2520Alignment%2520for%250A%2520%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DYaru%2520Chen%2520and%2520Ruohao%2520Guo%2520and%2520Liting%2520Gao%2520and%2520Yang%2520Xiang%2520and%2520Qingyu%2520Luo%2520and%2520Zhenbo%2520Li%2520and%2520Wenwu%2520Wang%26entry.1292438233%3D%2520%2520Weakly-supervised%2520audio-visual%2520video%2520parsing%2520%2528AVVP%2529%2520seeks%2520to%2520detect%2520audible%252C%250Avisible%252C%2520and%2520audio-visual%2520events%2520without%2520temporal%2520annotations.%2520Previous%2520work%250Ahas%2520emphasized%2520refining%2520global%2520predictions%2520through%2520contrastive%2520or%2520collaborative%250Alearning%252C%2520but%2520neglected%2520stable%2520segment-level%2520supervision%2520and%2520class-aware%250Across-modal%2520alignment.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520strategies%253A%2520%25281%2529%2520an%250Aexponential%2520moving%2520average%2520%2528EMA%2529-guided%2520pseudo%2520supervision%2520framework%2520that%250Agenerates%2520reliable%2520segment-level%2520masks%2520via%2520adaptive%2520thresholds%2520or%2520top-k%250Aselection%252C%2520offering%2520stable%2520temporal%2520guidance%2520beyond%2520video-level%2520labels%253B%2520and%2520%25282%2529%250Aa%2520class-aware%2520cross-modal%2520agreement%2520%2528CMA%2529%2520loss%2520that%2520aligns%2520audio%2520and%2520visual%250Aembeddings%2520at%2520reliable%2520segment-class%2520pairs%252C%2520ensuring%2520consistency%2520across%250Amodalities%2520while%2520preserving%2520temporal%2520structure.%2520Evaluations%2520on%2520LLP%2520and%2520UnAV-100%250Adatasets%2520shows%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%250Aacross%2520multiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teacher-Guided%20Pseudo%20Supervision%20and%20Cross-Modal%20Alignment%20for%0A%20%20Audio-Visual%20Video%20Parsing&entry.906535625=Yaru%20Chen%20and%20Ruohao%20Guo%20and%20Liting%20Gao%20and%20Yang%20Xiang%20and%20Qingyu%20Luo%20and%20Zhenbo%20Li%20and%20Wenwu%20Wang&entry.1292438233=%20%20Weakly-supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20seeks%20to%20detect%20audible%2C%0Avisible%2C%20and%20audio-visual%20events%20without%20temporal%20annotations.%20Previous%20work%0Ahas%20emphasized%20refining%20global%20predictions%20through%20contrastive%20or%20collaborative%0Alearning%2C%20but%20neglected%20stable%20segment-level%20supervision%20and%20class-aware%0Across-modal%20alignment.%20To%20address%20this%2C%20we%20propose%20two%20strategies%3A%20%281%29%20an%0Aexponential%20moving%20average%20%28EMA%29-guided%20pseudo%20supervision%20framework%20that%0Agenerates%20reliable%20segment-level%20masks%20via%20adaptive%20thresholds%20or%20top-k%0Aselection%2C%20offering%20stable%20temporal%20guidance%20beyond%20video-level%20labels%3B%20and%20%282%29%0Aa%20class-aware%20cross-modal%20agreement%20%28CMA%29%20loss%20that%20aligns%20audio%20and%20visual%0Aembeddings%20at%20reliable%20segment-class%20pairs%2C%20ensuring%20consistency%20across%0Amodalities%20while%20preserving%20temporal%20structure.%20Evaluations%20on%20LLP%20and%20UnAV-100%0Adatasets%20shows%20that%20our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%0Aacross%20multiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14097v1&entry.124074799=Read"},
{"title": "LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction\n  for Scalable Neural TSP Solvers", "author": "Junrui Wen and Yifei Li and Bart Selman and Kun He", "abstract": "  Neural solvers have shown significant potential in solving the Traveling\nSalesman Problem (TSP), yet current approaches face significant challenges.\nSupervised learning (SL)-based solvers require large amounts of high-quality\nlabeled data, while reinforcement learning (RL)-based solvers, though less\ndependent on such data, often suffer from inefficiencies. To address these\nlimitations, we propose LocalEscaper, a novel weakly-supervised learning\nframework for large-scale TSP. LocalEscaper effectively combines the advantages\nof both SL and RL, enabling effective training on datasets with low-quality\nlabels. To further enhance solution quality, we introduce a regional\nreconstruction strategy, which is the key technique of this paper and mitigates\nthe local-optima problem common in existing local reconstruction methods.\nExperimental results on both synthetic and real-world datasets demonstrate that\nLocalEscaper outperforms existing neural solvers, achieving remarkable results.\n", "link": "http://arxiv.org/abs/2502.12484v2", "date": "2025-09-17", "relevancy": 2.6589, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocalEscaper%3A%20A%20Weakly-supervised%20Framework%20with%20Regional%20Reconstruction%0A%20%20for%20Scalable%20Neural%20TSP%20Solvers&body=Title%3A%20LocalEscaper%3A%20A%20Weakly-supervised%20Framework%20with%20Regional%20Reconstruction%0A%20%20for%20Scalable%20Neural%20TSP%20Solvers%0AAuthor%3A%20Junrui%20Wen%20and%20Yifei%20Li%20and%20Bart%20Selman%20and%20Kun%20He%0AAbstract%3A%20%20%20Neural%20solvers%20have%20shown%20significant%20potential%20in%20solving%20the%20Traveling%0ASalesman%20Problem%20%28TSP%29%2C%20yet%20current%20approaches%20face%20significant%20challenges.%0ASupervised%20learning%20%28SL%29-based%20solvers%20require%20large%20amounts%20of%20high-quality%0Alabeled%20data%2C%20while%20reinforcement%20learning%20%28RL%29-based%20solvers%2C%20though%20less%0Adependent%20on%20such%20data%2C%20often%20suffer%20from%20inefficiencies.%20To%20address%20these%0Alimitations%2C%20we%20propose%20LocalEscaper%2C%20a%20novel%20weakly-supervised%20learning%0Aframework%20for%20large-scale%20TSP.%20LocalEscaper%20effectively%20combines%20the%20advantages%0Aof%20both%20SL%20and%20RL%2C%20enabling%20effective%20training%20on%20datasets%20with%20low-quality%0Alabels.%20To%20further%20enhance%20solution%20quality%2C%20we%20introduce%20a%20regional%0Areconstruction%20strategy%2C%20which%20is%20the%20key%20technique%20of%20this%20paper%20and%20mitigates%0Athe%20local-optima%20problem%20common%20in%20existing%20local%20reconstruction%20methods.%0AExperimental%20results%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%0ALocalEscaper%20outperforms%20existing%20neural%20solvers%2C%20achieving%20remarkable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalEscaper%253A%2520A%2520Weakly-supervised%2520Framework%2520with%2520Regional%2520Reconstruction%250A%2520%2520for%2520Scalable%2520Neural%2520TSP%2520Solvers%26entry.906535625%3DJunrui%2520Wen%2520and%2520Yifei%2520Li%2520and%2520Bart%2520Selman%2520and%2520Kun%2520He%26entry.1292438233%3D%2520%2520Neural%2520solvers%2520have%2520shown%2520significant%2520potential%2520in%2520solving%2520the%2520Traveling%250ASalesman%2520Problem%2520%2528TSP%2529%252C%2520yet%2520current%2520approaches%2520face%2520significant%2520challenges.%250ASupervised%2520learning%2520%2528SL%2529-based%2520solvers%2520require%2520large%2520amounts%2520of%2520high-quality%250Alabeled%2520data%252C%2520while%2520reinforcement%2520learning%2520%2528RL%2529-based%2520solvers%252C%2520though%2520less%250Adependent%2520on%2520such%2520data%252C%2520often%2520suffer%2520from%2520inefficiencies.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520LocalEscaper%252C%2520a%2520novel%2520weakly-supervised%2520learning%250Aframework%2520for%2520large-scale%2520TSP.%2520LocalEscaper%2520effectively%2520combines%2520the%2520advantages%250Aof%2520both%2520SL%2520and%2520RL%252C%2520enabling%2520effective%2520training%2520on%2520datasets%2520with%2520low-quality%250Alabels.%2520To%2520further%2520enhance%2520solution%2520quality%252C%2520we%2520introduce%2520a%2520regional%250Areconstruction%2520strategy%252C%2520which%2520is%2520the%2520key%2520technique%2520of%2520this%2520paper%2520and%2520mitigates%250Athe%2520local-optima%2520problem%2520common%2520in%2520existing%2520local%2520reconstruction%2520methods.%250AExperimental%2520results%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%250ALocalEscaper%2520outperforms%2520existing%2520neural%2520solvers%252C%2520achieving%2520remarkable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalEscaper%3A%20A%20Weakly-supervised%20Framework%20with%20Regional%20Reconstruction%0A%20%20for%20Scalable%20Neural%20TSP%20Solvers&entry.906535625=Junrui%20Wen%20and%20Yifei%20Li%20and%20Bart%20Selman%20and%20Kun%20He&entry.1292438233=%20%20Neural%20solvers%20have%20shown%20significant%20potential%20in%20solving%20the%20Traveling%0ASalesman%20Problem%20%28TSP%29%2C%20yet%20current%20approaches%20face%20significant%20challenges.%0ASupervised%20learning%20%28SL%29-based%20solvers%20require%20large%20amounts%20of%20high-quality%0Alabeled%20data%2C%20while%20reinforcement%20learning%20%28RL%29-based%20solvers%2C%20though%20less%0Adependent%20on%20such%20data%2C%20often%20suffer%20from%20inefficiencies.%20To%20address%20these%0Alimitations%2C%20we%20propose%20LocalEscaper%2C%20a%20novel%20weakly-supervised%20learning%0Aframework%20for%20large-scale%20TSP.%20LocalEscaper%20effectively%20combines%20the%20advantages%0Aof%20both%20SL%20and%20RL%2C%20enabling%20effective%20training%20on%20datasets%20with%20low-quality%0Alabels.%20To%20further%20enhance%20solution%20quality%2C%20we%20introduce%20a%20regional%0Areconstruction%20strategy%2C%20which%20is%20the%20key%20technique%20of%20this%20paper%20and%20mitigates%0Athe%20local-optima%20problem%20common%20in%20existing%20local%20reconstruction%20methods.%0AExperimental%20results%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%0ALocalEscaper%20outperforms%20existing%20neural%20solvers%2C%20achieving%20remarkable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12484v2&entry.124074799=Read"},
{"title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone\n  Detection in Visually Complex Environments", "author": "Tamara R. Lenhard and Andreas Weinmann and Tobias Koch", "abstract": "  Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.\n", "link": "http://arxiv.org/abs/2509.14012v1", "date": "2025-09-17", "relevancy": 2.6454, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Optimization%20of%20YOLO-FEDER%20FusionNet%20for%20Robust%20Drone%0A%20%20Detection%20in%20Visually%20Complex%20Environments&body=Title%3A%20Performance%20Optimization%20of%20YOLO-FEDER%20FusionNet%20for%20Robust%20Drone%0A%20%20Detection%20in%20Visually%20Complex%20Environments%0AAuthor%3A%20Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Tobias%20Koch%0AAbstract%3A%20%20%20Drone%20detection%20in%20visually%20complex%20environments%20remains%20challenging%20due%20to%0Abackground%20clutter%2C%20small%20object%20scale%2C%20and%20camouflage%20effects.%20While%20generic%0Aobject%20detectors%20like%20YOLO%20exhibit%20strong%20performance%20in%20low-texture%20scenes%2C%0Atheir%20effectiveness%20degrades%20in%20cluttered%20environments%20with%20low%0Aobject-background%20separability.%20To%20address%20these%20limitations%2C%20this%20work%0Apresents%20an%20enhanced%20iteration%20of%20YOLO-FEDER%20FusionNet%20--%20a%20detection%20framework%0Athat%20integrates%20generic%20object%20detection%20with%20camouflage%20object%20detection%0Atechniques.%20Building%20upon%20the%20original%20architecture%2C%20the%20proposed%20iteration%0Aintroduces%20systematic%20advancements%20in%20training%20data%20composition%2C%20feature%20fusion%0Astrategies%2C%20and%20backbone%20design.%20Specifically%2C%20the%20training%20process%20leverages%0Alarge-scale%2C%20photo-realistic%20synthetic%20data%2C%20complemented%20by%20a%20small%20set%20of%0Areal-world%20samples%2C%20to%20enhance%20robustness%20under%20visually%20complex%20conditions.%0AThe%20contribution%20of%20intermediate%20multi-scale%20FEDER%20features%20is%20systematically%0Aevaluated%2C%20and%20detection%20performance%20is%20comprehensively%20benchmarked%20across%0Amultiple%20YOLO-based%20backbone%20configurations.%20Empirical%20results%20indicate%20that%0Aintegrating%20intermediate%20FEDER%20features%2C%20in%20combination%20with%20backbone%20upgrades%2C%0Acontributes%20to%20notable%20performance%20improvements.%20In%20the%20most%20promising%0Aconfiguration%20--%20YOLO-FEDER%20FusionNet%20with%20a%20YOLOv8l%20backbone%20and%20FEDER%0Afeatures%20derived%20from%20the%20DWD%20module%20--%20these%20enhancements%20lead%20to%20a%20FNR%0Areduction%20of%20up%20to%2039.1%20percentage%20points%20and%20a%20mAP%20increase%20of%20up%20to%2062.8%0Apercentage%20points%20at%20an%20IoU%20threshold%20of%200.5%2C%20compared%20to%20the%20initial%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Optimization%2520of%2520YOLO-FEDER%2520FusionNet%2520for%2520Robust%2520Drone%250A%2520%2520Detection%2520in%2520Visually%2520Complex%2520Environments%26entry.906535625%3DTamara%2520R.%2520Lenhard%2520and%2520Andreas%2520Weinmann%2520and%2520Tobias%2520Koch%26entry.1292438233%3D%2520%2520Drone%2520detection%2520in%2520visually%2520complex%2520environments%2520remains%2520challenging%2520due%2520to%250Abackground%2520clutter%252C%2520small%2520object%2520scale%252C%2520and%2520camouflage%2520effects.%2520While%2520generic%250Aobject%2520detectors%2520like%2520YOLO%2520exhibit%2520strong%2520performance%2520in%2520low-texture%2520scenes%252C%250Atheir%2520effectiveness%2520degrades%2520in%2520cluttered%2520environments%2520with%2520low%250Aobject-background%2520separability.%2520To%2520address%2520these%2520limitations%252C%2520this%2520work%250Apresents%2520an%2520enhanced%2520iteration%2520of%2520YOLO-FEDER%2520FusionNet%2520--%2520a%2520detection%2520framework%250Athat%2520integrates%2520generic%2520object%2520detection%2520with%2520camouflage%2520object%2520detection%250Atechniques.%2520Building%2520upon%2520the%2520original%2520architecture%252C%2520the%2520proposed%2520iteration%250Aintroduces%2520systematic%2520advancements%2520in%2520training%2520data%2520composition%252C%2520feature%2520fusion%250Astrategies%252C%2520and%2520backbone%2520design.%2520Specifically%252C%2520the%2520training%2520process%2520leverages%250Alarge-scale%252C%2520photo-realistic%2520synthetic%2520data%252C%2520complemented%2520by%2520a%2520small%2520set%2520of%250Areal-world%2520samples%252C%2520to%2520enhance%2520robustness%2520under%2520visually%2520complex%2520conditions.%250AThe%2520contribution%2520of%2520intermediate%2520multi-scale%2520FEDER%2520features%2520is%2520systematically%250Aevaluated%252C%2520and%2520detection%2520performance%2520is%2520comprehensively%2520benchmarked%2520across%250Amultiple%2520YOLO-based%2520backbone%2520configurations.%2520Empirical%2520results%2520indicate%2520that%250Aintegrating%2520intermediate%2520FEDER%2520features%252C%2520in%2520combination%2520with%2520backbone%2520upgrades%252C%250Acontributes%2520to%2520notable%2520performance%2520improvements.%2520In%2520the%2520most%2520promising%250Aconfiguration%2520--%2520YOLO-FEDER%2520FusionNet%2520with%2520a%2520YOLOv8l%2520backbone%2520and%2520FEDER%250Afeatures%2520derived%2520from%2520the%2520DWD%2520module%2520--%2520these%2520enhancements%2520lead%2520to%2520a%2520FNR%250Areduction%2520of%2520up%2520to%252039.1%2520percentage%2520points%2520and%2520a%2520mAP%2520increase%2520of%2520up%2520to%252062.8%250Apercentage%2520points%2520at%2520an%2520IoU%2520threshold%2520of%25200.5%252C%2520compared%2520to%2520the%2520initial%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Optimization%20of%20YOLO-FEDER%20FusionNet%20for%20Robust%20Drone%0A%20%20Detection%20in%20Visually%20Complex%20Environments&entry.906535625=Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Tobias%20Koch&entry.1292438233=%20%20Drone%20detection%20in%20visually%20complex%20environments%20remains%20challenging%20due%20to%0Abackground%20clutter%2C%20small%20object%20scale%2C%20and%20camouflage%20effects.%20While%20generic%0Aobject%20detectors%20like%20YOLO%20exhibit%20strong%20performance%20in%20low-texture%20scenes%2C%0Atheir%20effectiveness%20degrades%20in%20cluttered%20environments%20with%20low%0Aobject-background%20separability.%20To%20address%20these%20limitations%2C%20this%20work%0Apresents%20an%20enhanced%20iteration%20of%20YOLO-FEDER%20FusionNet%20--%20a%20detection%20framework%0Athat%20integrates%20generic%20object%20detection%20with%20camouflage%20object%20detection%0Atechniques.%20Building%20upon%20the%20original%20architecture%2C%20the%20proposed%20iteration%0Aintroduces%20systematic%20advancements%20in%20training%20data%20composition%2C%20feature%20fusion%0Astrategies%2C%20and%20backbone%20design.%20Specifically%2C%20the%20training%20process%20leverages%0Alarge-scale%2C%20photo-realistic%20synthetic%20data%2C%20complemented%20by%20a%20small%20set%20of%0Areal-world%20samples%2C%20to%20enhance%20robustness%20under%20visually%20complex%20conditions.%0AThe%20contribution%20of%20intermediate%20multi-scale%20FEDER%20features%20is%20systematically%0Aevaluated%2C%20and%20detection%20performance%20is%20comprehensively%20benchmarked%20across%0Amultiple%20YOLO-based%20backbone%20configurations.%20Empirical%20results%20indicate%20that%0Aintegrating%20intermediate%20FEDER%20features%2C%20in%20combination%20with%20backbone%20upgrades%2C%0Acontributes%20to%20notable%20performance%20improvements.%20In%20the%20most%20promising%0Aconfiguration%20--%20YOLO-FEDER%20FusionNet%20with%20a%20YOLOv8l%20backbone%20and%20FEDER%0Afeatures%20derived%20from%20the%20DWD%20module%20--%20these%20enhancements%20lead%20to%20a%20FNR%0Areduction%20of%20up%20to%2039.1%20percentage%20points%20and%20a%20mAP%20increase%20of%20up%20to%2062.8%0Apercentage%20points%20at%20an%20IoU%20threshold%20of%200.5%2C%20compared%20to%20the%20initial%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14012v1&entry.124074799=Read"},
{"title": "GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in\n  Unknown Environments", "author": "Seth Farrell and Chenghao Li and Hongzhan Yu and Hesam Mojtahedi and Sicun Gao and Henrik I. Christensen", "abstract": "  We present a cooperative aerial-ground search-and-rescue (SAR) framework that\npairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)\nto achieve rapid victim localization and obstacle-aware navigation in unknown\nenvironments. We dub this framework Guided Long-horizon Integrated Drone Escort\n(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon\nplanning. In our framework, a goal-searching UAV executes real-time onboard\nvictim detection and georeferencing to nominate goals for the ground platform,\nwhile a terrain-scouting UAV flies ahead of the UGV's planned route to provide\nmid-level traversability updates. The UGV fuses aerial cues with local sensing\nto perform time-efficient A* planning and continuous replanning as information\narrives. Additionally, we present a hardware demonstration (using a GEM e6 golf\ncart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission\nperformance and include simulation ablations to assess the planning stack in\nisolation from detection. Empirical results demonstrate that explicit role\nseparation across UAVs, coupled with terrain scouting and guided planning,\nimproves reach time and navigation safety in time-critical SAR missions.\n", "link": "http://arxiv.org/abs/2509.14210v1", "date": "2025-09-17", "relevancy": 2.6313, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5437}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5236}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLIDE%3A%20A%20Coordinated%20Aerial-Ground%20Framework%20for%20Search%20and%20Rescue%20in%0A%20%20Unknown%20Environments&body=Title%3A%20GLIDE%3A%20A%20Coordinated%20Aerial-Ground%20Framework%20for%20Search%20and%20Rescue%20in%0A%20%20Unknown%20Environments%0AAuthor%3A%20Seth%20Farrell%20and%20Chenghao%20Li%20and%20Hongzhan%20Yu%20and%20Hesam%20Mojtahedi%20and%20Sicun%20Gao%20and%20Henrik%20I.%20Christensen%0AAbstract%3A%20%20%20We%20present%20a%20cooperative%20aerial-ground%20search-and-rescue%20%28SAR%29%20framework%20that%0Apairs%20two%20unmanned%20aerial%20vehicles%20%28UAVs%29%20with%20an%20unmanned%20ground%20vehicle%20%28UGV%29%0Ato%20achieve%20rapid%20victim%20localization%20and%20obstacle-aware%20navigation%20in%20unknown%0Aenvironments.%20We%20dub%20this%20framework%20Guided%20Long-horizon%20Integrated%20Drone%20Escort%0A%28GLIDE%29%2C%20highlighting%20the%20UGV%27s%20reliance%20on%20UAV%20guidance%20for%20long-horizon%0Aplanning.%20In%20our%20framework%2C%20a%20goal-searching%20UAV%20executes%20real-time%20onboard%0Avictim%20detection%20and%20georeferencing%20to%20nominate%20goals%20for%20the%20ground%20platform%2C%0Awhile%20a%20terrain-scouting%20UAV%20flies%20ahead%20of%20the%20UGV%27s%20planned%20route%20to%20provide%0Amid-level%20traversability%20updates.%20The%20UGV%20fuses%20aerial%20cues%20with%20local%20sensing%0Ato%20perform%20time-efficient%20A%2A%20planning%20and%20continuous%20replanning%20as%20information%0Aarrives.%20Additionally%2C%20we%20present%20a%20hardware%20demonstration%20%28using%20a%20GEM%20e6%20golf%0Acart%20as%20the%20UGV%20and%20two%20X500%20UAVs%29%20to%20evaluate%20end-to-end%20SAR%20mission%0Aperformance%20and%20include%20simulation%20ablations%20to%20assess%20the%20planning%20stack%20in%0Aisolation%20from%20detection.%20Empirical%20results%20demonstrate%20that%20explicit%20role%0Aseparation%20across%20UAVs%2C%20coupled%20with%20terrain%20scouting%20and%20guided%20planning%2C%0Aimproves%20reach%20time%20and%20navigation%20safety%20in%20time-critical%20SAR%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLIDE%253A%2520A%2520Coordinated%2520Aerial-Ground%2520Framework%2520for%2520Search%2520and%2520Rescue%2520in%250A%2520%2520Unknown%2520Environments%26entry.906535625%3DSeth%2520Farrell%2520and%2520Chenghao%2520Li%2520and%2520Hongzhan%2520Yu%2520and%2520Hesam%2520Mojtahedi%2520and%2520Sicun%2520Gao%2520and%2520Henrik%2520I.%2520Christensen%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520cooperative%2520aerial-ground%2520search-and-rescue%2520%2528SAR%2529%2520framework%2520that%250Apairs%2520two%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520with%2520an%2520unmanned%2520ground%2520vehicle%2520%2528UGV%2529%250Ato%2520achieve%2520rapid%2520victim%2520localization%2520and%2520obstacle-aware%2520navigation%2520in%2520unknown%250Aenvironments.%2520We%2520dub%2520this%2520framework%2520Guided%2520Long-horizon%2520Integrated%2520Drone%2520Escort%250A%2528GLIDE%2529%252C%2520highlighting%2520the%2520UGV%2527s%2520reliance%2520on%2520UAV%2520guidance%2520for%2520long-horizon%250Aplanning.%2520In%2520our%2520framework%252C%2520a%2520goal-searching%2520UAV%2520executes%2520real-time%2520onboard%250Avictim%2520detection%2520and%2520georeferencing%2520to%2520nominate%2520goals%2520for%2520the%2520ground%2520platform%252C%250Awhile%2520a%2520terrain-scouting%2520UAV%2520flies%2520ahead%2520of%2520the%2520UGV%2527s%2520planned%2520route%2520to%2520provide%250Amid-level%2520traversability%2520updates.%2520The%2520UGV%2520fuses%2520aerial%2520cues%2520with%2520local%2520sensing%250Ato%2520perform%2520time-efficient%2520A%252A%2520planning%2520and%2520continuous%2520replanning%2520as%2520information%250Aarrives.%2520Additionally%252C%2520we%2520present%2520a%2520hardware%2520demonstration%2520%2528using%2520a%2520GEM%2520e6%2520golf%250Acart%2520as%2520the%2520UGV%2520and%2520two%2520X500%2520UAVs%2529%2520to%2520evaluate%2520end-to-end%2520SAR%2520mission%250Aperformance%2520and%2520include%2520simulation%2520ablations%2520to%2520assess%2520the%2520planning%2520stack%2520in%250Aisolation%2520from%2520detection.%2520Empirical%2520results%2520demonstrate%2520that%2520explicit%2520role%250Aseparation%2520across%2520UAVs%252C%2520coupled%2520with%2520terrain%2520scouting%2520and%2520guided%2520planning%252C%250Aimproves%2520reach%2520time%2520and%2520navigation%2520safety%2520in%2520time-critical%2520SAR%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLIDE%3A%20A%20Coordinated%20Aerial-Ground%20Framework%20for%20Search%20and%20Rescue%20in%0A%20%20Unknown%20Environments&entry.906535625=Seth%20Farrell%20and%20Chenghao%20Li%20and%20Hongzhan%20Yu%20and%20Hesam%20Mojtahedi%20and%20Sicun%20Gao%20and%20Henrik%20I.%20Christensen&entry.1292438233=%20%20We%20present%20a%20cooperative%20aerial-ground%20search-and-rescue%20%28SAR%29%20framework%20that%0Apairs%20two%20unmanned%20aerial%20vehicles%20%28UAVs%29%20with%20an%20unmanned%20ground%20vehicle%20%28UGV%29%0Ato%20achieve%20rapid%20victim%20localization%20and%20obstacle-aware%20navigation%20in%20unknown%0Aenvironments.%20We%20dub%20this%20framework%20Guided%20Long-horizon%20Integrated%20Drone%20Escort%0A%28GLIDE%29%2C%20highlighting%20the%20UGV%27s%20reliance%20on%20UAV%20guidance%20for%20long-horizon%0Aplanning.%20In%20our%20framework%2C%20a%20goal-searching%20UAV%20executes%20real-time%20onboard%0Avictim%20detection%20and%20georeferencing%20to%20nominate%20goals%20for%20the%20ground%20platform%2C%0Awhile%20a%20terrain-scouting%20UAV%20flies%20ahead%20of%20the%20UGV%27s%20planned%20route%20to%20provide%0Amid-level%20traversability%20updates.%20The%20UGV%20fuses%20aerial%20cues%20with%20local%20sensing%0Ato%20perform%20time-efficient%20A%2A%20planning%20and%20continuous%20replanning%20as%20information%0Aarrives.%20Additionally%2C%20we%20present%20a%20hardware%20demonstration%20%28using%20a%20GEM%20e6%20golf%0Acart%20as%20the%20UGV%20and%20two%20X500%20UAVs%29%20to%20evaluate%20end-to-end%20SAR%20mission%0Aperformance%20and%20include%20simulation%20ablations%20to%20assess%20the%20planning%20stack%20in%0Aisolation%20from%20detection.%20Empirical%20results%20demonstrate%20that%20explicit%20role%0Aseparation%20across%20UAVs%2C%20coupled%20with%20terrain%20scouting%20and%20guided%20planning%2C%0Aimproves%20reach%20time%20and%20navigation%20safety%20in%20time-critical%20SAR%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14210v1&entry.124074799=Read"},
{"title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate\n  Histopathology Workflows", "author": "Jiabo MA and Wenqiang Li and Jinbang Li and Ziyi Liu and Linshan Wu and Fengtao Zhou and Li Liang and Ronald Cheong Kin Chan and Terence T. W. Wong and Hao Chen", "abstract": "  Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.\n", "link": "http://arxiv.org/abs/2509.14119v1", "date": "2025-09-17", "relevancy": 2.5821, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5205}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Misalignment-Resistant%20Virtual%20Staining%20to%20Accelerate%0A%20%20Histopathology%20Workflows&body=Title%3A%20Generative%20AI%20for%20Misalignment-Resistant%20Virtual%20Staining%20to%20Accelerate%0A%20%20Histopathology%20Workflows%0AAuthor%3A%20Jiabo%20MA%20and%20Wenqiang%20Li%20and%20Jinbang%20Li%20and%20Ziyi%20Liu%20and%20Linshan%20Wu%20and%20Fengtao%20Zhou%20and%20Li%20Liang%20and%20Ronald%20Cheong%20Kin%20Chan%20and%20Terence%20T.%20W.%20Wong%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Accurate%20histopathological%20diagnosis%20often%20requires%20multiple%20differently%0Astained%20tissue%20sections%2C%20a%20process%20that%20is%20time-consuming%2C%20labor-intensive%2C%20and%0Aenvironmentally%20taxing%20due%20to%20the%20use%20of%20multiple%20chemical%20stains.%20Recently%2C%0Avirtual%20staining%20has%20emerged%20as%20a%20promising%20alternative%20that%20is%20faster%2C%0Atissue-conserving%2C%20and%20environmentally%20friendly.%20However%2C%20existing%20virtual%0Astaining%20methods%20face%20significant%20challenges%20in%20clinical%20applications%2C%0Aprimarily%20due%20to%20their%20reliance%20on%20well-aligned%20paired%20data.%20Obtaining%20such%0Adata%20is%20inherently%20difficult%20because%20chemical%20staining%20processes%20can%20distort%0Atissue%20structures%2C%20and%20a%20single%20tissue%20section%20cannot%20undergo%20multiple%20staining%0Aprocedures%20without%20damage%20or%20loss%20of%20information.%20As%20a%20result%2C%20most%20available%0Avirtual%20staining%20datasets%20are%20either%20unpaired%20or%20roughly%20paired%2C%20making%20it%0Adifficult%20for%20existing%20methods%20to%20achieve%20accurate%20pixel-level%20supervision.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20robust%20virtual%20staining%20framework%0Afeaturing%20cascaded%20registration%20mechanisms%20to%20resolve%20spatial%20mismatches%0Abetween%20generated%20outputs%20and%20their%20corresponding%20ground%20truth.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Amodels%20across%20five%20datasets%2C%20achieving%20an%20average%20improvement%20of%203.2%25%20on%0Ainternal%20datasets%20and%2010.1%25%20on%20external%20datasets.%20Moreover%2C%20in%20datasets%20with%0Asubstantial%20misalignment%2C%20our%20approach%20achieves%20a%20remarkable%2023.8%25%20improvement%0Ain%20peak%20signal-to-noise%20ratio%20compared%20to%20baseline%20models.%20The%20exceptional%0Arobustness%20of%20the%20proposed%20method%20across%20diverse%20datasets%20simplifies%20the%20data%0Aacquisition%20process%20for%20virtual%20staining%20and%20offers%20new%20insights%20for%20advancing%0Aits%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Misalignment-Resistant%2520Virtual%2520Staining%2520to%2520Accelerate%250A%2520%2520Histopathology%2520Workflows%26entry.906535625%3DJiabo%2520MA%2520and%2520Wenqiang%2520Li%2520and%2520Jinbang%2520Li%2520and%2520Ziyi%2520Liu%2520and%2520Linshan%2520Wu%2520and%2520Fengtao%2520Zhou%2520and%2520Li%2520Liang%2520and%2520Ronald%2520Cheong%2520Kin%2520Chan%2520and%2520Terence%2520T.%2520W.%2520Wong%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Accurate%2520histopathological%2520diagnosis%2520often%2520requires%2520multiple%2520differently%250Astained%2520tissue%2520sections%252C%2520a%2520process%2520that%2520is%2520time-consuming%252C%2520labor-intensive%252C%2520and%250Aenvironmentally%2520taxing%2520due%2520to%2520the%2520use%2520of%2520multiple%2520chemical%2520stains.%2520Recently%252C%250Avirtual%2520staining%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520that%2520is%2520faster%252C%250Atissue-conserving%252C%2520and%2520environmentally%2520friendly.%2520However%252C%2520existing%2520virtual%250Astaining%2520methods%2520face%2520significant%2520challenges%2520in%2520clinical%2520applications%252C%250Aprimarily%2520due%2520to%2520their%2520reliance%2520on%2520well-aligned%2520paired%2520data.%2520Obtaining%2520such%250Adata%2520is%2520inherently%2520difficult%2520because%2520chemical%2520staining%2520processes%2520can%2520distort%250Atissue%2520structures%252C%2520and%2520a%2520single%2520tissue%2520section%2520cannot%2520undergo%2520multiple%2520staining%250Aprocedures%2520without%2520damage%2520or%2520loss%2520of%2520information.%2520As%2520a%2520result%252C%2520most%2520available%250Avirtual%2520staining%2520datasets%2520are%2520either%2520unpaired%2520or%2520roughly%2520paired%252C%2520making%2520it%250Adifficult%2520for%2520existing%2520methods%2520to%2520achieve%2520accurate%2520pixel-level%2520supervision.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520a%2520robust%2520virtual%2520staining%2520framework%250Afeaturing%2520cascaded%2520registration%2520mechanisms%2520to%2520resolve%2520spatial%2520mismatches%250Abetween%2520generated%2520outputs%2520and%2520their%2520corresponding%2520ground%2520truth.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Amodels%2520across%2520five%2520datasets%252C%2520achieving%2520an%2520average%2520improvement%2520of%25203.2%2525%2520on%250Ainternal%2520datasets%2520and%252010.1%2525%2520on%2520external%2520datasets.%2520Moreover%252C%2520in%2520datasets%2520with%250Asubstantial%2520misalignment%252C%2520our%2520approach%2520achieves%2520a%2520remarkable%252023.8%2525%2520improvement%250Ain%2520peak%2520signal-to-noise%2520ratio%2520compared%2520to%2520baseline%2520models.%2520The%2520exceptional%250Arobustness%2520of%2520the%2520proposed%2520method%2520across%2520diverse%2520datasets%2520simplifies%2520the%2520data%250Aacquisition%2520process%2520for%2520virtual%2520staining%2520and%2520offers%2520new%2520insights%2520for%2520advancing%250Aits%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Misalignment-Resistant%20Virtual%20Staining%20to%20Accelerate%0A%20%20Histopathology%20Workflows&entry.906535625=Jiabo%20MA%20and%20Wenqiang%20Li%20and%20Jinbang%20Li%20and%20Ziyi%20Liu%20and%20Linshan%20Wu%20and%20Fengtao%20Zhou%20and%20Li%20Liang%20and%20Ronald%20Cheong%20Kin%20Chan%20and%20Terence%20T.%20W.%20Wong%20and%20Hao%20Chen&entry.1292438233=%20%20Accurate%20histopathological%20diagnosis%20often%20requires%20multiple%20differently%0Astained%20tissue%20sections%2C%20a%20process%20that%20is%20time-consuming%2C%20labor-intensive%2C%20and%0Aenvironmentally%20taxing%20due%20to%20the%20use%20of%20multiple%20chemical%20stains.%20Recently%2C%0Avirtual%20staining%20has%20emerged%20as%20a%20promising%20alternative%20that%20is%20faster%2C%0Atissue-conserving%2C%20and%20environmentally%20friendly.%20However%2C%20existing%20virtual%0Astaining%20methods%20face%20significant%20challenges%20in%20clinical%20applications%2C%0Aprimarily%20due%20to%20their%20reliance%20on%20well-aligned%20paired%20data.%20Obtaining%20such%0Adata%20is%20inherently%20difficult%20because%20chemical%20staining%20processes%20can%20distort%0Atissue%20structures%2C%20and%20a%20single%20tissue%20section%20cannot%20undergo%20multiple%20staining%0Aprocedures%20without%20damage%20or%20loss%20of%20information.%20As%20a%20result%2C%20most%20available%0Avirtual%20staining%20datasets%20are%20either%20unpaired%20or%20roughly%20paired%2C%20making%20it%0Adifficult%20for%20existing%20methods%20to%20achieve%20accurate%20pixel-level%20supervision.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20a%20robust%20virtual%20staining%20framework%0Afeaturing%20cascaded%20registration%20mechanisms%20to%20resolve%20spatial%20mismatches%0Abetween%20generated%20outputs%20and%20their%20corresponding%20ground%20truth.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Amodels%20across%20five%20datasets%2C%20achieving%20an%20average%20improvement%20of%203.2%25%20on%0Ainternal%20datasets%20and%2010.1%25%20on%20external%20datasets.%20Moreover%2C%20in%20datasets%20with%0Asubstantial%20misalignment%2C%20our%20approach%20achieves%20a%20remarkable%2023.8%25%20improvement%0Ain%20peak%20signal-to-noise%20ratio%20compared%20to%20baseline%20models.%20The%20exceptional%0Arobustness%20of%20the%20proposed%20method%20across%20diverse%20datasets%20simplifies%20the%20data%0Aacquisition%20process%20for%20virtual%20staining%20and%20offers%20new%20insights%20for%20advancing%0Aits%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14119v1&entry.124074799=Read"},
{"title": "NIRVANA: Structured pruning reimagined for large language models\n  compression", "author": "Mengting Ai and Tianxin Wei and Sirui Chen and Jingrui He", "abstract": "  Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.\n", "link": "http://arxiv.org/abs/2509.14230v1", "date": "2025-09-17", "relevancy": 2.5611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NIRVANA%3A%20Structured%20pruning%20reimagined%20for%20large%20language%20models%0A%20%20compression&body=Title%3A%20NIRVANA%3A%20Structured%20pruning%20reimagined%20for%20large%20language%20models%0A%20%20compression%0AAuthor%3A%20Mengting%20Ai%20and%20Tianxin%20Wei%20and%20Sirui%20Chen%20and%20Jingrui%20He%0AAbstract%3A%20%20%20Structured%20pruning%20of%20large%20language%20models%20%28LLMs%29%20offers%20substantial%0Aefficiency%20improvements%20by%20removing%20entire%20hidden%20units%2C%20yet%20current%20approaches%0Aoften%20suffer%20from%20significant%20performance%20degradation%2C%20particularly%20in%0Azero-shot%20settings%2C%20and%20necessitate%20costly%20recovery%20techniques%20such%20as%0Asupervised%20fine-tuning%20%28SFT%29%20or%20adapter%20insertion.%20To%20address%20these%20critical%0Ashortcomings%2C%20we%20introduce%20NIRVANA%2C%20a%20novel%20pruning%20method%20explicitly%20designed%0Ato%20balance%20immediate%20zero-shot%20accuracy%20preservation%20with%20robust%20fine-tuning%0Acapability.%20Leveraging%20a%20first-order%20saliency%20criterion%20derived%20from%20the%20Neural%0ATangent%20Kernel%20under%20Adam%20optimization%20dynamics%2C%20NIRVANA%20provides%20a%0Atheoretically%20grounded%20pruning%20strategy%20that%20respects%20essential%20model%20training%0Abehaviors.%20To%20further%20address%20the%20unique%20challenges%20posed%20by%20structured%0Apruning%2C%20NIRVANA%20incorporates%20an%20adaptive%20sparsity%20allocation%20mechanism%20across%0Alayers%20and%20modules%20%28attention%20vs.%20MLP%29%2C%20which%20adjusts%20pruning%20intensity%20between%0Amodules%20in%20a%20globally%20balanced%20manner.%20Additionally%2C%20to%20mitigate%20the%20high%0Asensitivity%20of%20pruning%20decisions%20to%20calibration%20data%20quality%2C%20we%20propose%20a%0Asimple%20yet%20effective%20KL%20divergence-based%20calibration%20data%20selection%20strategy%2C%0Aensuring%20more%20reliable%20and%20task-agnostic%20pruning%20outcomes.%20Comprehensive%0Aexperiments%20conducted%20on%20Llama3%2C%20Qwen%2C%20and%20T5%20models%20demonstrate%20that%20NIRVANA%0Aoutperforms%20existing%20structured%20pruning%20methods%20under%20equivalent%20sparsity%0Aconstraints%2C%20providing%20a%20theoretically%20sound%20and%20practical%20approach%20to%20LLM%0Acompression.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNIRVANA%253A%2520Structured%2520pruning%2520reimagined%2520for%2520large%2520language%2520models%250A%2520%2520compression%26entry.906535625%3DMengting%2520Ai%2520and%2520Tianxin%2520Wei%2520and%2520Sirui%2520Chen%2520and%2520Jingrui%2520He%26entry.1292438233%3D%2520%2520Structured%2520pruning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520offers%2520substantial%250Aefficiency%2520improvements%2520by%2520removing%2520entire%2520hidden%2520units%252C%2520yet%2520current%2520approaches%250Aoften%2520suffer%2520from%2520significant%2520performance%2520degradation%252C%2520particularly%2520in%250Azero-shot%2520settings%252C%2520and%2520necessitate%2520costly%2520recovery%2520techniques%2520such%2520as%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520or%2520adapter%2520insertion.%2520To%2520address%2520these%2520critical%250Ashortcomings%252C%2520we%2520introduce%2520NIRVANA%252C%2520a%2520novel%2520pruning%2520method%2520explicitly%2520designed%250Ato%2520balance%2520immediate%2520zero-shot%2520accuracy%2520preservation%2520with%2520robust%2520fine-tuning%250Acapability.%2520Leveraging%2520a%2520first-order%2520saliency%2520criterion%2520derived%2520from%2520the%2520Neural%250ATangent%2520Kernel%2520under%2520Adam%2520optimization%2520dynamics%252C%2520NIRVANA%2520provides%2520a%250Atheoretically%2520grounded%2520pruning%2520strategy%2520that%2520respects%2520essential%2520model%2520training%250Abehaviors.%2520To%2520further%2520address%2520the%2520unique%2520challenges%2520posed%2520by%2520structured%250Apruning%252C%2520NIRVANA%2520incorporates%2520an%2520adaptive%2520sparsity%2520allocation%2520mechanism%2520across%250Alayers%2520and%2520modules%2520%2528attention%2520vs.%2520MLP%2529%252C%2520which%2520adjusts%2520pruning%2520intensity%2520between%250Amodules%2520in%2520a%2520globally%2520balanced%2520manner.%2520Additionally%252C%2520to%2520mitigate%2520the%2520high%250Asensitivity%2520of%2520pruning%2520decisions%2520to%2520calibration%2520data%2520quality%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520KL%2520divergence-based%2520calibration%2520data%2520selection%2520strategy%252C%250Aensuring%2520more%2520reliable%2520and%2520task-agnostic%2520pruning%2520outcomes.%2520Comprehensive%250Aexperiments%2520conducted%2520on%2520Llama3%252C%2520Qwen%252C%2520and%2520T5%2520models%2520demonstrate%2520that%2520NIRVANA%250Aoutperforms%2520existing%2520structured%2520pruning%2520methods%2520under%2520equivalent%2520sparsity%250Aconstraints%252C%2520providing%2520a%2520theoretically%2520sound%2520and%2520practical%2520approach%2520to%2520LLM%250Acompression.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIRVANA%3A%20Structured%20pruning%20reimagined%20for%20large%20language%20models%0A%20%20compression&entry.906535625=Mengting%20Ai%20and%20Tianxin%20Wei%20and%20Sirui%20Chen%20and%20Jingrui%20He&entry.1292438233=%20%20Structured%20pruning%20of%20large%20language%20models%20%28LLMs%29%20offers%20substantial%0Aefficiency%20improvements%20by%20removing%20entire%20hidden%20units%2C%20yet%20current%20approaches%0Aoften%20suffer%20from%20significant%20performance%20degradation%2C%20particularly%20in%0Azero-shot%20settings%2C%20and%20necessitate%20costly%20recovery%20techniques%20such%20as%0Asupervised%20fine-tuning%20%28SFT%29%20or%20adapter%20insertion.%20To%20address%20these%20critical%0Ashortcomings%2C%20we%20introduce%20NIRVANA%2C%20a%20novel%20pruning%20method%20explicitly%20designed%0Ato%20balance%20immediate%20zero-shot%20accuracy%20preservation%20with%20robust%20fine-tuning%0Acapability.%20Leveraging%20a%20first-order%20saliency%20criterion%20derived%20from%20the%20Neural%0ATangent%20Kernel%20under%20Adam%20optimization%20dynamics%2C%20NIRVANA%20provides%20a%0Atheoretically%20grounded%20pruning%20strategy%20that%20respects%20essential%20model%20training%0Abehaviors.%20To%20further%20address%20the%20unique%20challenges%20posed%20by%20structured%0Apruning%2C%20NIRVANA%20incorporates%20an%20adaptive%20sparsity%20allocation%20mechanism%20across%0Alayers%20and%20modules%20%28attention%20vs.%20MLP%29%2C%20which%20adjusts%20pruning%20intensity%20between%0Amodules%20in%20a%20globally%20balanced%20manner.%20Additionally%2C%20to%20mitigate%20the%20high%0Asensitivity%20of%20pruning%20decisions%20to%20calibration%20data%20quality%2C%20we%20propose%20a%0Asimple%20yet%20effective%20KL%20divergence-based%20calibration%20data%20selection%20strategy%2C%0Aensuring%20more%20reliable%20and%20task-agnostic%20pruning%20outcomes.%20Comprehensive%0Aexperiments%20conducted%20on%20Llama3%2C%20Qwen%2C%20and%20T5%20models%20demonstrate%20that%20NIRVANA%0Aoutperforms%20existing%20structured%20pruning%20methods%20under%20equivalent%20sparsity%0Aconstraints%2C%20providing%20a%20theoretically%20sound%20and%20practical%20approach%20to%20LLM%0Acompression.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14230v1&entry.124074799=Read"},
{"title": "ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signals", "author": "Yucong Zhang and Juan Liu and Ming Li", "abstract": "  Pre-trained foundation models have demonstrated remarkable success in audio,\nvision and language, yet their potential for general machine signal modeling\nwith arbitrary sampling rates-covering acoustic, vibration, and other\nindustrial sensor data-remains under-explored. In this work, we propose a novel\nfoundation model ECHO that integrates an advanced band-split architecture with\nfrequency positional embeddings, enabling spectral localization across\narbitrary sampling configurations. Moreover, the model incorporates sliding\npatches to support inputs of variable length without padding or cropping,\nproducing a concise embedding that retains both temporal and spectral fidelity\nand naturally extends to streaming scenarios. We evaluate our method on various\nkinds of machine signal datasets, including previous DCASE task 2 challenges\n(2020-2025), and widely-used industrial signal corpora. Experimental results\ndemonstrate consistent state-of-the-art performance in machine signal anomaly\ndetection and fault classification, confirming the effectiveness and\ngeneralization capability of the proposed model. We open-sourced ECHO on\nhttps://github.com/yucongzh/ECHO.\n", "link": "http://arxiv.org/abs/2508.14689v2", "date": "2025-09-17", "relevancy": 2.5611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signals&body=Title%3A%20ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signals%0AAuthor%3A%20Yucong%20Zhang%20and%20Juan%20Liu%20and%20Ming%20Li%0AAbstract%3A%20%20%20Pre-trained%20foundation%20models%20have%20demonstrated%20remarkable%20success%20in%20audio%2C%0Avision%20and%20language%2C%20yet%20their%20potential%20for%20general%20machine%20signal%20modeling%0Awith%20arbitrary%20sampling%20rates-covering%20acoustic%2C%20vibration%2C%20and%20other%0Aindustrial%20sensor%20data-remains%20under-explored.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Afoundation%20model%20ECHO%20that%20integrates%20an%20advanced%20band-split%20architecture%20with%0Afrequency%20positional%20embeddings%2C%20enabling%20spectral%20localization%20across%0Aarbitrary%20sampling%20configurations.%20Moreover%2C%20the%20model%20incorporates%20sliding%0Apatches%20to%20support%20inputs%20of%20variable%20length%20without%20padding%20or%20cropping%2C%0Aproducing%20a%20concise%20embedding%20that%20retains%20both%20temporal%20and%20spectral%20fidelity%0Aand%20naturally%20extends%20to%20streaming%20scenarios.%20We%20evaluate%20our%20method%20on%20various%0Akinds%20of%20machine%20signal%20datasets%2C%20including%20previous%20DCASE%20task%202%20challenges%0A%282020-2025%29%2C%20and%20widely-used%20industrial%20signal%20corpora.%20Experimental%20results%0Ademonstrate%20consistent%20state-of-the-art%20performance%20in%20machine%20signal%20anomaly%0Adetection%20and%20fault%20classification%2C%20confirming%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20the%20proposed%20model.%20We%20open-sourced%20ECHO%20on%0Ahttps%3A//github.com/yucongzh/ECHO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECHO%253A%2520Frequency-aware%2520Hierarchical%2520Encoding%2520for%2520Variable-length%2520Signals%26entry.906535625%3DYucong%2520Zhang%2520and%2520Juan%2520Liu%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520Pre-trained%2520foundation%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520audio%252C%250Avision%2520and%2520language%252C%2520yet%2520their%2520potential%2520for%2520general%2520machine%2520signal%2520modeling%250Awith%2520arbitrary%2520sampling%2520rates-covering%2520acoustic%252C%2520vibration%252C%2520and%2520other%250Aindustrial%2520sensor%2520data-remains%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Afoundation%2520model%2520ECHO%2520that%2520integrates%2520an%2520advanced%2520band-split%2520architecture%2520with%250Afrequency%2520positional%2520embeddings%252C%2520enabling%2520spectral%2520localization%2520across%250Aarbitrary%2520sampling%2520configurations.%2520Moreover%252C%2520the%2520model%2520incorporates%2520sliding%250Apatches%2520to%2520support%2520inputs%2520of%2520variable%2520length%2520without%2520padding%2520or%2520cropping%252C%250Aproducing%2520a%2520concise%2520embedding%2520that%2520retains%2520both%2520temporal%2520and%2520spectral%2520fidelity%250Aand%2520naturally%2520extends%2520to%2520streaming%2520scenarios.%2520We%2520evaluate%2520our%2520method%2520on%2520various%250Akinds%2520of%2520machine%2520signal%2520datasets%252C%2520including%2520previous%2520DCASE%2520task%25202%2520challenges%250A%25282020-2025%2529%252C%2520and%2520widely-used%2520industrial%2520signal%2520corpora.%2520Experimental%2520results%250Ademonstrate%2520consistent%2520state-of-the-art%2520performance%2520in%2520machine%2520signal%2520anomaly%250Adetection%2520and%2520fault%2520classification%252C%2520confirming%2520the%2520effectiveness%2520and%250Ageneralization%2520capability%2520of%2520the%2520proposed%2520model.%2520We%2520open-sourced%2520ECHO%2520on%250Ahttps%253A//github.com/yucongzh/ECHO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECHO%3A%20Frequency-aware%20Hierarchical%20Encoding%20for%20Variable-length%20Signals&entry.906535625=Yucong%20Zhang%20and%20Juan%20Liu%20and%20Ming%20Li&entry.1292438233=%20%20Pre-trained%20foundation%20models%20have%20demonstrated%20remarkable%20success%20in%20audio%2C%0Avision%20and%20language%2C%20yet%20their%20potential%20for%20general%20machine%20signal%20modeling%0Awith%20arbitrary%20sampling%20rates-covering%20acoustic%2C%20vibration%2C%20and%20other%0Aindustrial%20sensor%20data-remains%20under-explored.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Afoundation%20model%20ECHO%20that%20integrates%20an%20advanced%20band-split%20architecture%20with%0Afrequency%20positional%20embeddings%2C%20enabling%20spectral%20localization%20across%0Aarbitrary%20sampling%20configurations.%20Moreover%2C%20the%20model%20incorporates%20sliding%0Apatches%20to%20support%20inputs%20of%20variable%20length%20without%20padding%20or%20cropping%2C%0Aproducing%20a%20concise%20embedding%20that%20retains%20both%20temporal%20and%20spectral%20fidelity%0Aand%20naturally%20extends%20to%20streaming%20scenarios.%20We%20evaluate%20our%20method%20on%20various%0Akinds%20of%20machine%20signal%20datasets%2C%20including%20previous%20DCASE%20task%202%20challenges%0A%282020-2025%29%2C%20and%20widely-used%20industrial%20signal%20corpora.%20Experimental%20results%0Ademonstrate%20consistent%20state-of-the-art%20performance%20in%20machine%20signal%20anomaly%0Adetection%20and%20fault%20classification%2C%20confirming%20the%20effectiveness%20and%0Ageneralization%20capability%20of%20the%20proposed%20model.%20We%20open-sourced%20ECHO%20on%0Ahttps%3A//github.com/yucongzh/ECHO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14689v2&entry.124074799=Read"},
{"title": "SCALP: Superpixels with Contour Adherence using Linear Path", "author": "R\u00e9mi Giraud and Vinh-Thong Ta and Nicolas Papadakis", "abstract": "  Superpixel decomposition methods are generally used as a pre-processing step\nto speed up image processing tasks. They group the pixels of an image into\nhomogeneous regions while trying to respect existing contours. For all\nstate-of-the-art superpixel decomposition methods, a trade-off is made between\n1) computational time, 2) adherence to image contours and 3) regularity and\ncompactness of the decomposition. In this paper, we propose a fast method to\ncompute Superpixels with Contour Adherence using Linear Path (SCALP) in an\niterative clustering framework. The distance computed when trying to associate\na pixel to a superpixel during the clustering is enhanced by considering the\nlinear path to the superpixel barycenter. The proposed framework produces\nregular and compact superpixels that adhere to the image contours. We provide a\ndetailed evaluation of SCALP on the standard Berkeley Segmentation Dataset. The\nobtained results outperform state-of-the-art methods in terms of standard\nsuperpixel and contour detection metrics.\n", "link": "http://arxiv.org/abs/1903.07149v2", "date": "2025-09-17", "relevancy": 2.517, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5044}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCALP%3A%20Superpixels%20with%20Contour%20Adherence%20using%20Linear%20Path&body=Title%3A%20SCALP%3A%20Superpixels%20with%20Contour%20Adherence%20using%20Linear%20Path%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20Superpixel%20decomposition%20methods%20are%20generally%20used%20as%20a%20pre-processing%20step%0Ato%20speed%20up%20image%20processing%20tasks.%20They%20group%20the%20pixels%20of%20an%20image%20into%0Ahomogeneous%20regions%20while%20trying%20to%20respect%20existing%20contours.%20For%20all%0Astate-of-the-art%20superpixel%20decomposition%20methods%2C%20a%20trade-off%20is%20made%20between%0A1%29%20computational%20time%2C%202%29%20adherence%20to%20image%20contours%20and%203%29%20regularity%20and%0Acompactness%20of%20the%20decomposition.%20In%20this%20paper%2C%20we%20propose%20a%20fast%20method%20to%0Acompute%20Superpixels%20with%20Contour%20Adherence%20using%20Linear%20Path%20%28SCALP%29%20in%20an%0Aiterative%20clustering%20framework.%20The%20distance%20computed%20when%20trying%20to%20associate%0Aa%20pixel%20to%20a%20superpixel%20during%20the%20clustering%20is%20enhanced%20by%20considering%20the%0Alinear%20path%20to%20the%20superpixel%20barycenter.%20The%20proposed%20framework%20produces%0Aregular%20and%20compact%20superpixels%20that%20adhere%20to%20the%20image%20contours.%20We%20provide%20a%0Adetailed%20evaluation%20of%20SCALP%20on%20the%20standard%20Berkeley%20Segmentation%20Dataset.%20The%0Aobtained%20results%20outperform%20state-of-the-art%20methods%20in%20terms%20of%20standard%0Asuperpixel%20and%20contour%20detection%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1903.07149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCALP%253A%2520Superpixels%2520with%2520Contour%2520Adherence%2520using%2520Linear%2520Path%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Vinh-Thong%2520Ta%2520and%2520Nicolas%2520Papadakis%26entry.1292438233%3D%2520%2520Superpixel%2520decomposition%2520methods%2520are%2520generally%2520used%2520as%2520a%2520pre-processing%2520step%250Ato%2520speed%2520up%2520image%2520processing%2520tasks.%2520They%2520group%2520the%2520pixels%2520of%2520an%2520image%2520into%250Ahomogeneous%2520regions%2520while%2520trying%2520to%2520respect%2520existing%2520contours.%2520For%2520all%250Astate-of-the-art%2520superpixel%2520decomposition%2520methods%252C%2520a%2520trade-off%2520is%2520made%2520between%250A1%2529%2520computational%2520time%252C%25202%2529%2520adherence%2520to%2520image%2520contours%2520and%25203%2529%2520regularity%2520and%250Acompactness%2520of%2520the%2520decomposition.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520fast%2520method%2520to%250Acompute%2520Superpixels%2520with%2520Contour%2520Adherence%2520using%2520Linear%2520Path%2520%2528SCALP%2529%2520in%2520an%250Aiterative%2520clustering%2520framework.%2520The%2520distance%2520computed%2520when%2520trying%2520to%2520associate%250Aa%2520pixel%2520to%2520a%2520superpixel%2520during%2520the%2520clustering%2520is%2520enhanced%2520by%2520considering%2520the%250Alinear%2520path%2520to%2520the%2520superpixel%2520barycenter.%2520The%2520proposed%2520framework%2520produces%250Aregular%2520and%2520compact%2520superpixels%2520that%2520adhere%2520to%2520the%2520image%2520contours.%2520We%2520provide%2520a%250Adetailed%2520evaluation%2520of%2520SCALP%2520on%2520the%2520standard%2520Berkeley%2520Segmentation%2520Dataset.%2520The%250Aobtained%2520results%2520outperform%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520standard%250Asuperpixel%2520and%2520contour%2520detection%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1903.07149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCALP%3A%20Superpixels%20with%20Contour%20Adherence%20using%20Linear%20Path&entry.906535625=R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20Superpixel%20decomposition%20methods%20are%20generally%20used%20as%20a%20pre-processing%20step%0Ato%20speed%20up%20image%20processing%20tasks.%20They%20group%20the%20pixels%20of%20an%20image%20into%0Ahomogeneous%20regions%20while%20trying%20to%20respect%20existing%20contours.%20For%20all%0Astate-of-the-art%20superpixel%20decomposition%20methods%2C%20a%20trade-off%20is%20made%20between%0A1%29%20computational%20time%2C%202%29%20adherence%20to%20image%20contours%20and%203%29%20regularity%20and%0Acompactness%20of%20the%20decomposition.%20In%20this%20paper%2C%20we%20propose%20a%20fast%20method%20to%0Acompute%20Superpixels%20with%20Contour%20Adherence%20using%20Linear%20Path%20%28SCALP%29%20in%20an%0Aiterative%20clustering%20framework.%20The%20distance%20computed%20when%20trying%20to%20associate%0Aa%20pixel%20to%20a%20superpixel%20during%20the%20clustering%20is%20enhanced%20by%20considering%20the%0Alinear%20path%20to%20the%20superpixel%20barycenter.%20The%20proposed%20framework%20produces%0Aregular%20and%20compact%20superpixels%20that%20adhere%20to%20the%20image%20contours.%20We%20provide%20a%0Adetailed%20evaluation%20of%20SCALP%20on%20the%20standard%20Berkeley%20Segmentation%20Dataset.%20The%0Aobtained%20results%20outperform%20state-of-the-art%20methods%20in%20terms%20of%20standard%0Asuperpixel%20and%20contour%20detection%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/1903.07149v2&entry.124074799=Read"},
{"title": "CoPL: Collaborative Preference Learning for Personalizing LLMs", "author": "Youngbin Choi and Seunghyuk Cho and Minjong Lee and MoonJeong Park and Yesong Ko and Jungseul Ok and Dongwoo Kim", "abstract": "  Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment. The code is available at\nhttps://github.com/ml-postech/CoPL.\n", "link": "http://arxiv.org/abs/2503.01658v2", "date": "2025-09-17", "relevancy": 2.4729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoPL%3A%20Collaborative%20Preference%20Learning%20for%20Personalizing%20LLMs&body=Title%3A%20CoPL%3A%20Collaborative%20Preference%20Learning%20for%20Personalizing%20LLMs%0AAuthor%3A%20Youngbin%20Choi%20and%20Seunghyuk%20Cho%20and%20Minjong%20Lee%20and%20MoonJeong%20Park%20and%20Yesong%20Ko%20and%20Jungseul%20Ok%20and%20Dongwoo%20Kim%0AAbstract%3A%20%20%20Personalizing%20large%20language%20models%20%28LLMs%29%20is%20important%20for%20aligning%20outputs%0Awith%20diverse%20user%20preferences%2C%20yet%20existing%20methods%20struggle%20with%20flexibility%0Aand%20generalization.%20We%20propose%20CoPL%20%28Collaborative%20Preference%20Learning%29%2C%20a%0Agraph-based%20collaborative%20filtering%20framework%20that%20models%20user-response%0Arelationships%20to%20enhance%20preference%20estimation%2C%20particularly%20in%20sparse%0Aannotation%20settings.%20By%20integrating%20a%20mixture%20of%20LoRA%20experts%2C%20CoPL%20efficiently%0Afine-tunes%20LLMs%20while%20dynamically%20balancing%20shared%20and%20user-specific%0Apreferences.%20Additionally%2C%20an%20optimization-free%20adaptation%20strategy%20enables%0Ageneralization%20to%20unseen%20users%20without%20fine-tuning.%20Experiments%20on%0AUltraFeedback-P%20demonstrate%20that%20CoPL%20outperforms%20existing%20personalized%20reward%0Amodels%2C%20effectively%20capturing%20both%20common%20and%20controversial%20preferences%2C%20making%0Ait%20a%20scalable%20solution%20for%20personalized%20LLM%20alignment.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ml-postech/CoPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoPL%253A%2520Collaborative%2520Preference%2520Learning%2520for%2520Personalizing%2520LLMs%26entry.906535625%3DYoungbin%2520Choi%2520and%2520Seunghyuk%2520Cho%2520and%2520Minjong%2520Lee%2520and%2520MoonJeong%2520Park%2520and%2520Yesong%2520Ko%2520and%2520Jungseul%2520Ok%2520and%2520Dongwoo%2520Kim%26entry.1292438233%3D%2520%2520Personalizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520important%2520for%2520aligning%2520outputs%250Awith%2520diverse%2520user%2520preferences%252C%2520yet%2520existing%2520methods%2520struggle%2520with%2520flexibility%250Aand%2520generalization.%2520We%2520propose%2520CoPL%2520%2528Collaborative%2520Preference%2520Learning%2529%252C%2520a%250Agraph-based%2520collaborative%2520filtering%2520framework%2520that%2520models%2520user-response%250Arelationships%2520to%2520enhance%2520preference%2520estimation%252C%2520particularly%2520in%2520sparse%250Aannotation%2520settings.%2520By%2520integrating%2520a%2520mixture%2520of%2520LoRA%2520experts%252C%2520CoPL%2520efficiently%250Afine-tunes%2520LLMs%2520while%2520dynamically%2520balancing%2520shared%2520and%2520user-specific%250Apreferences.%2520Additionally%252C%2520an%2520optimization-free%2520adaptation%2520strategy%2520enables%250Ageneralization%2520to%2520unseen%2520users%2520without%2520fine-tuning.%2520Experiments%2520on%250AUltraFeedback-P%2520demonstrate%2520that%2520CoPL%2520outperforms%2520existing%2520personalized%2520reward%250Amodels%252C%2520effectively%2520capturing%2520both%2520common%2520and%2520controversial%2520preferences%252C%2520making%250Ait%2520a%2520scalable%2520solution%2520for%2520personalized%2520LLM%2520alignment.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ml-postech/CoPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoPL%3A%20Collaborative%20Preference%20Learning%20for%20Personalizing%20LLMs&entry.906535625=Youngbin%20Choi%20and%20Seunghyuk%20Cho%20and%20Minjong%20Lee%20and%20MoonJeong%20Park%20and%20Yesong%20Ko%20and%20Jungseul%20Ok%20and%20Dongwoo%20Kim&entry.1292438233=%20%20Personalizing%20large%20language%20models%20%28LLMs%29%20is%20important%20for%20aligning%20outputs%0Awith%20diverse%20user%20preferences%2C%20yet%20existing%20methods%20struggle%20with%20flexibility%0Aand%20generalization.%20We%20propose%20CoPL%20%28Collaborative%20Preference%20Learning%29%2C%20a%0Agraph-based%20collaborative%20filtering%20framework%20that%20models%20user-response%0Arelationships%20to%20enhance%20preference%20estimation%2C%20particularly%20in%20sparse%0Aannotation%20settings.%20By%20integrating%20a%20mixture%20of%20LoRA%20experts%2C%20CoPL%20efficiently%0Afine-tunes%20LLMs%20while%20dynamically%20balancing%20shared%20and%20user-specific%0Apreferences.%20Additionally%2C%20an%20optimization-free%20adaptation%20strategy%20enables%0Ageneralization%20to%20unseen%20users%20without%20fine-tuning.%20Experiments%20on%0AUltraFeedback-P%20demonstrate%20that%20CoPL%20outperforms%20existing%20personalized%20reward%0Amodels%2C%20effectively%20capturing%20both%20common%20and%20controversial%20preferences%2C%20making%0Ait%20a%20scalable%20solution%20for%20personalized%20LLM%20alignment.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ml-postech/CoPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01658v2&entry.124074799=Read"},
{"title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting\n  for Question-Based Sign Language Translation", "author": "Zekang Liu and Wei Feng and Fanhua Shang and Lianyu Hu and Jichao Feng and Liqing Gao", "abstract": "  Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.\n", "link": "http://arxiv.org/abs/2509.14036v1", "date": "2025-09-17", "relevancy": 2.4169, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL-SSAW%3A%20Self-Supervised%20Learning%20with%20Sigmoid%20Self-Attention%20Weighting%0A%20%20for%20Question-Based%20Sign%20Language%20Translation&body=Title%3A%20SSL-SSAW%3A%20Self-Supervised%20Learning%20with%20Sigmoid%20Self-Attention%20Weighting%0A%20%20for%20Question-Based%20Sign%20Language%20Translation%0AAuthor%3A%20Zekang%20Liu%20and%20Wei%20Feng%20and%20Fanhua%20Shang%20and%20Lianyu%20Hu%20and%20Jichao%20Feng%20and%20Liqing%20Gao%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20bridges%20the%20communication%20gap%20between%20deaf%0Apeople%20and%20hearing%20people%2C%20where%20dialogue%20provides%20crucial%20contextual%20cues%20to%0Aaid%20in%20translation.%20Building%20on%20this%20foundational%20concept%2C%20this%20paper%20proposes%0AQuestion-based%20Sign%20Language%20Translation%20%28QB-SLT%29%2C%20a%20novel%20task%20that%20explores%0Athe%20efficient%20integration%20of%20dialogue.%20Unlike%20gloss%20%28sign%20language%0Atranscription%29%20annotations%2C%20dialogue%20naturally%20occurs%20in%20communication%20and%20is%0Aeasier%20to%20annotate.%20The%20key%20challenge%20lies%20in%20aligning%20multimodality%20features%0Awhile%20leveraging%20the%20context%20of%20the%20question%20to%20improve%20translation.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20cross-modality%20Self-supervised%20Learning%20with%20Sigmoid%0ASelf-attention%20Weighting%20%28SSL-SSAW%29%20fusion%20method%20for%20sign%20language%0Atranslation.%20Specifically%2C%20we%20employ%20contrastive%20learning%20to%20align%0Amultimodality%20features%20in%20QB-SLT%2C%20then%20introduce%20a%20Sigmoid%20Self-attention%0AWeighting%20%28SSAW%29%20module%20for%20adaptive%20feature%20extraction%20from%20question%20and%20sign%0Alanguage%20sequences.%20Additionally%2C%20we%20leverage%20available%20question%20text%20through%0Aself-supervised%20learning%20to%20enhance%20representation%20and%20translation%0Acapabilities.%20We%20evaluated%20our%20approach%20on%20newly%20constructed%20CSL-Daily-QA%20and%0APHOENIX-2014T-QA%20datasets%2C%20where%20SSL-SSAW%20achieved%20SOTA%20performance.%20Notably%2C%0Aeasily%20accessible%20question%20assistance%20can%20achieve%20or%20even%20surpass%20the%0Aperformance%20of%20gloss%20assistance.%20Furthermore%2C%20visualization%20results%20demonstrate%0Athe%20effectiveness%20of%20incorporating%20dialogue%20in%20improving%20translation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL-SSAW%253A%2520Self-Supervised%2520Learning%2520with%2520Sigmoid%2520Self-Attention%2520Weighting%250A%2520%2520for%2520Question-Based%2520Sign%2520Language%2520Translation%26entry.906535625%3DZekang%2520Liu%2520and%2520Wei%2520Feng%2520and%2520Fanhua%2520Shang%2520and%2520Lianyu%2520Hu%2520and%2520Jichao%2520Feng%2520and%2520Liqing%2520Gao%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520bridges%2520the%2520communication%2520gap%2520between%2520deaf%250Apeople%2520and%2520hearing%2520people%252C%2520where%2520dialogue%2520provides%2520crucial%2520contextual%2520cues%2520to%250Aaid%2520in%2520translation.%2520Building%2520on%2520this%2520foundational%2520concept%252C%2520this%2520paper%2520proposes%250AQuestion-based%2520Sign%2520Language%2520Translation%2520%2528QB-SLT%2529%252C%2520a%2520novel%2520task%2520that%2520explores%250Athe%2520efficient%2520integration%2520of%2520dialogue.%2520Unlike%2520gloss%2520%2528sign%2520language%250Atranscription%2529%2520annotations%252C%2520dialogue%2520naturally%2520occurs%2520in%2520communication%2520and%2520is%250Aeasier%2520to%2520annotate.%2520The%2520key%2520challenge%2520lies%2520in%2520aligning%2520multimodality%2520features%250Awhile%2520leveraging%2520the%2520context%2520of%2520the%2520question%2520to%2520improve%2520translation.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520cross-modality%2520Self-supervised%2520Learning%2520with%2520Sigmoid%250ASelf-attention%2520Weighting%2520%2528SSL-SSAW%2529%2520fusion%2520method%2520for%2520sign%2520language%250Atranslation.%2520Specifically%252C%2520we%2520employ%2520contrastive%2520learning%2520to%2520align%250Amultimodality%2520features%2520in%2520QB-SLT%252C%2520then%2520introduce%2520a%2520Sigmoid%2520Self-attention%250AWeighting%2520%2528SSAW%2529%2520module%2520for%2520adaptive%2520feature%2520extraction%2520from%2520question%2520and%2520sign%250Alanguage%2520sequences.%2520Additionally%252C%2520we%2520leverage%2520available%2520question%2520text%2520through%250Aself-supervised%2520learning%2520to%2520enhance%2520representation%2520and%2520translation%250Acapabilities.%2520We%2520evaluated%2520our%2520approach%2520on%2520newly%2520constructed%2520CSL-Daily-QA%2520and%250APHOENIX-2014T-QA%2520datasets%252C%2520where%2520SSL-SSAW%2520achieved%2520SOTA%2520performance.%2520Notably%252C%250Aeasily%2520accessible%2520question%2520assistance%2520can%2520achieve%2520or%2520even%2520surpass%2520the%250Aperformance%2520of%2520gloss%2520assistance.%2520Furthermore%252C%2520visualization%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520incorporating%2520dialogue%2520in%2520improving%2520translation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL-SSAW%3A%20Self-Supervised%20Learning%20with%20Sigmoid%20Self-Attention%20Weighting%0A%20%20for%20Question-Based%20Sign%20Language%20Translation&entry.906535625=Zekang%20Liu%20and%20Wei%20Feng%20and%20Fanhua%20Shang%20and%20Lianyu%20Hu%20and%20Jichao%20Feng%20and%20Liqing%20Gao&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20bridges%20the%20communication%20gap%20between%20deaf%0Apeople%20and%20hearing%20people%2C%20where%20dialogue%20provides%20crucial%20contextual%20cues%20to%0Aaid%20in%20translation.%20Building%20on%20this%20foundational%20concept%2C%20this%20paper%20proposes%0AQuestion-based%20Sign%20Language%20Translation%20%28QB-SLT%29%2C%20a%20novel%20task%20that%20explores%0Athe%20efficient%20integration%20of%20dialogue.%20Unlike%20gloss%20%28sign%20language%0Atranscription%29%20annotations%2C%20dialogue%20naturally%20occurs%20in%20communication%20and%20is%0Aeasier%20to%20annotate.%20The%20key%20challenge%20lies%20in%20aligning%20multimodality%20features%0Awhile%20leveraging%20the%20context%20of%20the%20question%20to%20improve%20translation.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20cross-modality%20Self-supervised%20Learning%20with%20Sigmoid%0ASelf-attention%20Weighting%20%28SSL-SSAW%29%20fusion%20method%20for%20sign%20language%0Atranslation.%20Specifically%2C%20we%20employ%20contrastive%20learning%20to%20align%0Amultimodality%20features%20in%20QB-SLT%2C%20then%20introduce%20a%20Sigmoid%20Self-attention%0AWeighting%20%28SSAW%29%20module%20for%20adaptive%20feature%20extraction%20from%20question%20and%20sign%0Alanguage%20sequences.%20Additionally%2C%20we%20leverage%20available%20question%20text%20through%0Aself-supervised%20learning%20to%20enhance%20representation%20and%20translation%0Acapabilities.%20We%20evaluated%20our%20approach%20on%20newly%20constructed%20CSL-Daily-QA%20and%0APHOENIX-2014T-QA%20datasets%2C%20where%20SSL-SSAW%20achieved%20SOTA%20performance.%20Notably%2C%0Aeasily%20accessible%20question%20assistance%20can%20achieve%20or%20even%20surpass%20the%0Aperformance%20of%20gloss%20assistance.%20Furthermore%2C%20visualization%20results%20demonstrate%0Athe%20effectiveness%20of%20incorporating%20dialogue%20in%20improving%20translation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14036v1&entry.124074799=Read"},
{"title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "author": "Elena Camuffo and Francesco Barbato and Mete Ozay and Simone Milani and Umberto Michieli", "abstract": "  We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.\n", "link": "http://arxiv.org/abs/2509.14001v1", "date": "2025-09-17", "relevancy": 2.415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&body=Title%3A%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%0AAuthor%3A%20Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli%0AAbstract%3A%20%20%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%0Aa%20knowledge%20distillation%20approach%20that%20transfers%20region-level%20multimodal%0Asemantics%20from%20a%20large%20vision-language%20teacher%20%28e.g.%2C%20LLaVa%29%20into%20a%20lightweight%0Avision-only%20object%20detector%20student%20%28e.g.%2C%20YOLO%29.%20A%20translation%20module%20maps%0Astudent%20features%20into%20a%20joint%20space%2C%20where%20the%20training%20of%20the%20student%20and%0Atranslator%20is%20guided%20by%20a%20dual-objective%20loss%20that%20enforces%20both%20local%0Aalignment%20and%20global%20relational%20consistency.%20Unlike%20prior%20approaches%20focused%20on%0Adense%20or%20global%20alignment%2C%20MOCHA%20operates%20at%20the%20object%20level%2C%20enabling%0Aefficient%20transfer%20of%20semantics%20without%20modifying%20the%20teacher%20or%20requiring%0Atextual%20input%20at%20inference.%20We%20validate%20our%20method%20across%20four%20personalized%0Adetection%20benchmarks%20under%20few-shot%20regimes.%20Results%20show%20consistent%20gains%20over%0Abaselines%2C%20with%20a%20%2B10.1%20average%20score%20improvement.%20Despite%20its%20compact%0Aarchitecture%2C%20MOCHA%20reaches%20performance%20on%20par%20with%20larger%20multimodal%20models%2C%0Aproving%20its%20suitability%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOCHA%253A%2520Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%26entry.906535625%3DElena%2520Camuffo%2520and%2520Francesco%2520Barbato%2520and%2520Mete%2520Ozay%2520and%2520Simone%2520Milani%2520and%2520Umberto%2520Michieli%26entry.1292438233%3D%2520%2520We%2520introduce%2520MOCHA%2520%2528Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%2529%252C%250Aa%2520knowledge%2520distillation%2520approach%2520that%2520transfers%2520region-level%2520multimodal%250Asemantics%2520from%2520a%2520large%2520vision-language%2520teacher%2520%2528e.g.%252C%2520LLaVa%2529%2520into%2520a%2520lightweight%250Avision-only%2520object%2520detector%2520student%2520%2528e.g.%252C%2520YOLO%2529.%2520A%2520translation%2520module%2520maps%250Astudent%2520features%2520into%2520a%2520joint%2520space%252C%2520where%2520the%2520training%2520of%2520the%2520student%2520and%250Atranslator%2520is%2520guided%2520by%2520a%2520dual-objective%2520loss%2520that%2520enforces%2520both%2520local%250Aalignment%2520and%2520global%2520relational%2520consistency.%2520Unlike%2520prior%2520approaches%2520focused%2520on%250Adense%2520or%2520global%2520alignment%252C%2520MOCHA%2520operates%2520at%2520the%2520object%2520level%252C%2520enabling%250Aefficient%2520transfer%2520of%2520semantics%2520without%2520modifying%2520the%2520teacher%2520or%2520requiring%250Atextual%2520input%2520at%2520inference.%2520We%2520validate%2520our%2520method%2520across%2520four%2520personalized%250Adetection%2520benchmarks%2520under%2520few-shot%2520regimes.%2520Results%2520show%2520consistent%2520gains%2520over%250Abaselines%252C%2520with%2520a%2520%252B10.1%2520average%2520score%2520improvement.%2520Despite%2520its%2520compact%250Aarchitecture%252C%2520MOCHA%2520reaches%2520performance%2520on%2520par%2520with%2520larger%2520multimodal%2520models%252C%250Aproving%2520its%2520suitability%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&entry.906535625=Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli&entry.1292438233=%20%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%0Aa%20knowledge%20distillation%20approach%20that%20transfers%20region-level%20multimodal%0Asemantics%20from%20a%20large%20vision-language%20teacher%20%28e.g.%2C%20LLaVa%29%20into%20a%20lightweight%0Avision-only%20object%20detector%20student%20%28e.g.%2C%20YOLO%29.%20A%20translation%20module%20maps%0Astudent%20features%20into%20a%20joint%20space%2C%20where%20the%20training%20of%20the%20student%20and%0Atranslator%20is%20guided%20by%20a%20dual-objective%20loss%20that%20enforces%20both%20local%0Aalignment%20and%20global%20relational%20consistency.%20Unlike%20prior%20approaches%20focused%20on%0Adense%20or%20global%20alignment%2C%20MOCHA%20operates%20at%20the%20object%20level%2C%20enabling%0Aefficient%20transfer%20of%20semantics%20without%20modifying%20the%20teacher%20or%20requiring%0Atextual%20input%20at%20inference.%20We%20validate%20our%20method%20across%20four%20personalized%0Adetection%20benchmarks%20under%20few-shot%20regimes.%20Results%20show%20consistent%20gains%20over%0Abaselines%2C%20with%20a%20%2B10.1%20average%20score%20improvement.%20Despite%20its%20compact%0Aarchitecture%2C%20MOCHA%20reaches%20performance%20on%20par%20with%20larger%20multimodal%20models%2C%0Aproving%20its%20suitability%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14001v1&entry.124074799=Read"},
{"title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication", "author": "Gang Cheng and Xin Gao and Li Hu and Siqi Hu and Mingyang Huang and Chaonan Ji and Ju Li and Dechao Meng and Jinwei Qi and Penchong Qiao and Zhen Shen and Yafei Song and Ke Sun and Linrui Tian and Feng Wang and Guangyuan Wang and Qi Wang and Zhongjian Wang and Jiayu Xiao and Sheng Xu and Bang Zhang and Peng Zhang and Xindi Zhang and Zhe Zhang and Jingren Zhou and Lian Zhuo", "abstract": "  We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.\n", "link": "http://arxiv.org/abs/2509.14055v1", "date": "2025-09-17", "relevancy": 2.412, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6583}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5774}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wan-Animate%3A%20Unified%20Character%20Animation%20and%20Replacement%20with%20Holistic%0A%20%20Replication&body=Title%3A%20Wan-Animate%3A%20Unified%20Character%20Animation%20and%20Replacement%20with%20Holistic%0A%20%20Replication%0AAuthor%3A%20Gang%20Cheng%20and%20Xin%20Gao%20and%20Li%20Hu%20and%20Siqi%20Hu%20and%20Mingyang%20Huang%20and%20Chaonan%20Ji%20and%20Ju%20Li%20and%20Dechao%20Meng%20and%20Jinwei%20Qi%20and%20Penchong%20Qiao%20and%20Zhen%20Shen%20and%20Yafei%20Song%20and%20Ke%20Sun%20and%20Linrui%20Tian%20and%20Feng%20Wang%20and%20Guangyuan%20Wang%20and%20Qi%20Wang%20and%20Zhongjian%20Wang%20and%20Jiayu%20Xiao%20and%20Sheng%20Xu%20and%20Bang%20Zhang%20and%20Peng%20Zhang%20and%20Xindi%20Zhang%20and%20Zhe%20Zhang%20and%20Jingren%20Zhou%20and%20Lian%20Zhuo%0AAbstract%3A%20%20%20We%20introduce%20Wan-Animate%2C%20a%20unified%20framework%20for%20character%20animation%20and%0Areplacement.%20Given%20a%20character%20image%20and%20a%20reference%20video%2C%20Wan-Animate%20can%0Aanimate%20the%20character%20by%20precisely%20replicating%20the%20expressions%20and%20movements%20of%0Athe%20character%20in%20the%20video%20to%20generate%20high-fidelity%20character%20videos.%0AAlternatively%2C%20it%20can%20integrate%20the%20animated%20character%20into%20the%20reference%20video%0Ato%20replace%20the%20original%20character%2C%20replicating%20the%20scene%27s%20lighting%20and%20color%0Atone%20to%20achieve%20seamless%20environmental%20integration.%20Wan-Animate%20is%20built%20upon%0Athe%20Wan%20model.%20To%20adapt%20it%20for%20character%20animation%20tasks%2C%20we%20employ%20a%20modified%0Ainput%20paradigm%20to%20differentiate%20between%20reference%20conditions%20and%20regions%20for%0Ageneration.%20This%20design%20unifies%20multiple%20tasks%20into%20a%20common%20symbolic%0Arepresentation.%20We%20use%20spatially-aligned%20skeleton%20signals%20to%20replicate%20body%0Amotion%20and%20implicit%20facial%20features%20extracted%20from%20source%20images%20to%20reenact%0Aexpressions%2C%20enabling%20the%20generation%20of%20character%20videos%20with%20high%0Acontrollability%20and%20expressiveness.%20Furthermore%2C%20to%20enhance%20environmental%0Aintegration%20during%20character%20replacement%2C%20we%20develop%20an%20auxiliary%20Relighting%0ALoRA.%20This%20module%20preserves%20the%20character%27s%20appearance%20consistency%20while%0Aapplying%20the%20appropriate%20environmental%20lighting%20and%20color%20tone.%20Experimental%0Aresults%20demonstrate%20that%20Wan-Animate%20achieves%20state-of-the-art%20performance.%20We%0Aare%20committed%20to%20open-sourcing%20the%20model%20weights%20and%20its%20source%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWan-Animate%253A%2520Unified%2520Character%2520Animation%2520and%2520Replacement%2520with%2520Holistic%250A%2520%2520Replication%26entry.906535625%3DGang%2520Cheng%2520and%2520Xin%2520Gao%2520and%2520Li%2520Hu%2520and%2520Siqi%2520Hu%2520and%2520Mingyang%2520Huang%2520and%2520Chaonan%2520Ji%2520and%2520Ju%2520Li%2520and%2520Dechao%2520Meng%2520and%2520Jinwei%2520Qi%2520and%2520Penchong%2520Qiao%2520and%2520Zhen%2520Shen%2520and%2520Yafei%2520Song%2520and%2520Ke%2520Sun%2520and%2520Linrui%2520Tian%2520and%2520Feng%2520Wang%2520and%2520Guangyuan%2520Wang%2520and%2520Qi%2520Wang%2520and%2520Zhongjian%2520Wang%2520and%2520Jiayu%2520Xiao%2520and%2520Sheng%2520Xu%2520and%2520Bang%2520Zhang%2520and%2520Peng%2520Zhang%2520and%2520Xindi%2520Zhang%2520and%2520Zhe%2520Zhang%2520and%2520Jingren%2520Zhou%2520and%2520Lian%2520Zhuo%26entry.1292438233%3D%2520%2520We%2520introduce%2520Wan-Animate%252C%2520a%2520unified%2520framework%2520for%2520character%2520animation%2520and%250Areplacement.%2520Given%2520a%2520character%2520image%2520and%2520a%2520reference%2520video%252C%2520Wan-Animate%2520can%250Aanimate%2520the%2520character%2520by%2520precisely%2520replicating%2520the%2520expressions%2520and%2520movements%2520of%250Athe%2520character%2520in%2520the%2520video%2520to%2520generate%2520high-fidelity%2520character%2520videos.%250AAlternatively%252C%2520it%2520can%2520integrate%2520the%2520animated%2520character%2520into%2520the%2520reference%2520video%250Ato%2520replace%2520the%2520original%2520character%252C%2520replicating%2520the%2520scene%2527s%2520lighting%2520and%2520color%250Atone%2520to%2520achieve%2520seamless%2520environmental%2520integration.%2520Wan-Animate%2520is%2520built%2520upon%250Athe%2520Wan%2520model.%2520To%2520adapt%2520it%2520for%2520character%2520animation%2520tasks%252C%2520we%2520employ%2520a%2520modified%250Ainput%2520paradigm%2520to%2520differentiate%2520between%2520reference%2520conditions%2520and%2520regions%2520for%250Ageneration.%2520This%2520design%2520unifies%2520multiple%2520tasks%2520into%2520a%2520common%2520symbolic%250Arepresentation.%2520We%2520use%2520spatially-aligned%2520skeleton%2520signals%2520to%2520replicate%2520body%250Amotion%2520and%2520implicit%2520facial%2520features%2520extracted%2520from%2520source%2520images%2520to%2520reenact%250Aexpressions%252C%2520enabling%2520the%2520generation%2520of%2520character%2520videos%2520with%2520high%250Acontrollability%2520and%2520expressiveness.%2520Furthermore%252C%2520to%2520enhance%2520environmental%250Aintegration%2520during%2520character%2520replacement%252C%2520we%2520develop%2520an%2520auxiliary%2520Relighting%250ALoRA.%2520This%2520module%2520preserves%2520the%2520character%2527s%2520appearance%2520consistency%2520while%250Aapplying%2520the%2520appropriate%2520environmental%2520lighting%2520and%2520color%2520tone.%2520Experimental%250Aresults%2520demonstrate%2520that%2520Wan-Animate%2520achieves%2520state-of-the-art%2520performance.%2520We%250Aare%2520committed%2520to%2520open-sourcing%2520the%2520model%2520weights%2520and%2520its%2520source%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wan-Animate%3A%20Unified%20Character%20Animation%20and%20Replacement%20with%20Holistic%0A%20%20Replication&entry.906535625=Gang%20Cheng%20and%20Xin%20Gao%20and%20Li%20Hu%20and%20Siqi%20Hu%20and%20Mingyang%20Huang%20and%20Chaonan%20Ji%20and%20Ju%20Li%20and%20Dechao%20Meng%20and%20Jinwei%20Qi%20and%20Penchong%20Qiao%20and%20Zhen%20Shen%20and%20Yafei%20Song%20and%20Ke%20Sun%20and%20Linrui%20Tian%20and%20Feng%20Wang%20and%20Guangyuan%20Wang%20and%20Qi%20Wang%20and%20Zhongjian%20Wang%20and%20Jiayu%20Xiao%20and%20Sheng%20Xu%20and%20Bang%20Zhang%20and%20Peng%20Zhang%20and%20Xindi%20Zhang%20and%20Zhe%20Zhang%20and%20Jingren%20Zhou%20and%20Lian%20Zhuo&entry.1292438233=%20%20We%20introduce%20Wan-Animate%2C%20a%20unified%20framework%20for%20character%20animation%20and%0Areplacement.%20Given%20a%20character%20image%20and%20a%20reference%20video%2C%20Wan-Animate%20can%0Aanimate%20the%20character%20by%20precisely%20replicating%20the%20expressions%20and%20movements%20of%0Athe%20character%20in%20the%20video%20to%20generate%20high-fidelity%20character%20videos.%0AAlternatively%2C%20it%20can%20integrate%20the%20animated%20character%20into%20the%20reference%20video%0Ato%20replace%20the%20original%20character%2C%20replicating%20the%20scene%27s%20lighting%20and%20color%0Atone%20to%20achieve%20seamless%20environmental%20integration.%20Wan-Animate%20is%20built%20upon%0Athe%20Wan%20model.%20To%20adapt%20it%20for%20character%20animation%20tasks%2C%20we%20employ%20a%20modified%0Ainput%20paradigm%20to%20differentiate%20between%20reference%20conditions%20and%20regions%20for%0Ageneration.%20This%20design%20unifies%20multiple%20tasks%20into%20a%20common%20symbolic%0Arepresentation.%20We%20use%20spatially-aligned%20skeleton%20signals%20to%20replicate%20body%0Amotion%20and%20implicit%20facial%20features%20extracted%20from%20source%20images%20to%20reenact%0Aexpressions%2C%20enabling%20the%20generation%20of%20character%20videos%20with%20high%0Acontrollability%20and%20expressiveness.%20Furthermore%2C%20to%20enhance%20environmental%0Aintegration%20during%20character%20replacement%2C%20we%20develop%20an%20auxiliary%20Relighting%0ALoRA.%20This%20module%20preserves%20the%20character%27s%20appearance%20consistency%20while%0Aapplying%20the%20appropriate%20environmental%20lighting%20and%20color%20tone.%20Experimental%0Aresults%20demonstrate%20that%20Wan-Animate%20achieves%20state-of-the-art%20performance.%20We%0Aare%20committed%20to%20open-sourcing%20the%20model%20weights%20and%20its%20source%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14055v1&entry.124074799=Read"},
{"title": "Bridging Past and Future: Distribution-Aware Alignment for Time Series\n  Forecasting", "author": "Yifan Hu and Jie Yang and Tian Zhou and Peiyuan Liu and Yujin Tang and Rong Jin and Liang Sun", "abstract": "  Representation learning techniques like contrastive learning have long been\nexplored in time series forecasting, mirroring their success in computer vision\nand natural language processing. Yet recent state-of-the-art (SOTA) forecasters\nseldom adopt these representation approaches because they have shown little\nperformance advantage. We challenge this view and demonstrate that explicit\nrepresentation alignment can supply critical information that bridges the\ndistributional gap between input histories and future targets. To this end, we\nintroduce TimeAlign, a lightweight, plug-and-play framework that learns\nauxiliary features via a simple reconstruction task and feeds them back to any\nbase forecaster. Extensive experiments across eight benchmarks verify its\nsuperior performance. Further studies indicate that the gains arises primarily\nfrom correcting frequency mismatches between historical inputs and future\noutputs. We also provide a theoretical justification for the effectiveness of\nTimeAlign in increasing the mutual information between learned representations\nand predicted targets. As it is architecture-agnostic and incurs negligible\noverhead, TimeAlign can serve as a general alignment module for modern deep\nlearning time-series forecasting systems. The code is available at\nhttps://github.com/TROUBADOUR000/TimeAlign.\n", "link": "http://arxiv.org/abs/2509.14181v1", "date": "2025-09-17", "relevancy": 2.3962, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4765}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Yifan%20Hu%20and%20Jie%20Yang%20and%20Tian%20Zhou%20and%20Peiyuan%20Liu%20and%20Yujin%20Tang%20and%20Rong%20Jin%20and%20Liang%20Sun%0AAbstract%3A%20%20%20Representation%20learning%20techniques%20like%20contrastive%20learning%20have%20long%20been%0Aexplored%20in%20time%20series%20forecasting%2C%20mirroring%20their%20success%20in%20computer%20vision%0Aand%20natural%20language%20processing.%20Yet%20recent%20state-of-the-art%20%28SOTA%29%20forecasters%0Aseldom%20adopt%20these%20representation%20approaches%20because%20they%20have%20shown%20little%0Aperformance%20advantage.%20We%20challenge%20this%20view%20and%20demonstrate%20that%20explicit%0Arepresentation%20alignment%20can%20supply%20critical%20information%20that%20bridges%20the%0Adistributional%20gap%20between%20input%20histories%20and%20future%20targets.%20To%20this%20end%2C%20we%0Aintroduce%20TimeAlign%2C%20a%20lightweight%2C%20plug-and-play%20framework%20that%20learns%0Aauxiliary%20features%20via%20a%20simple%20reconstruction%20task%20and%20feeds%20them%20back%20to%20any%0Abase%20forecaster.%20Extensive%20experiments%20across%20eight%20benchmarks%20verify%20its%0Asuperior%20performance.%20Further%20studies%20indicate%20that%20the%20gains%20arises%20primarily%0Afrom%20correcting%20frequency%20mismatches%20between%20historical%20inputs%20and%20future%0Aoutputs.%20We%20also%20provide%20a%20theoretical%20justification%20for%20the%20effectiveness%20of%0ATimeAlign%20in%20increasing%20the%20mutual%20information%20between%20learned%20representations%0Aand%20predicted%20targets.%20As%20it%20is%20architecture-agnostic%20and%20incurs%20negligible%0Aoverhead%2C%20TimeAlign%20can%20serve%20as%20a%20general%20alignment%20module%20for%20modern%20deep%0Alearning%20time-series%20forecasting%20systems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/TROUBADOUR000/TimeAlign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Past%2520and%2520Future%253A%2520Distribution-Aware%2520Alignment%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DYifan%2520Hu%2520and%2520Jie%2520Yang%2520and%2520Tian%2520Zhou%2520and%2520Peiyuan%2520Liu%2520and%2520Yujin%2520Tang%2520and%2520Rong%2520Jin%2520and%2520Liang%2520Sun%26entry.1292438233%3D%2520%2520Representation%2520learning%2520techniques%2520like%2520contrastive%2520learning%2520have%2520long%2520been%250Aexplored%2520in%2520time%2520series%2520forecasting%252C%2520mirroring%2520their%2520success%2520in%2520computer%2520vision%250Aand%2520natural%2520language%2520processing.%2520Yet%2520recent%2520state-of-the-art%2520%2528SOTA%2529%2520forecasters%250Aseldom%2520adopt%2520these%2520representation%2520approaches%2520because%2520they%2520have%2520shown%2520little%250Aperformance%2520advantage.%2520We%2520challenge%2520this%2520view%2520and%2520demonstrate%2520that%2520explicit%250Arepresentation%2520alignment%2520can%2520supply%2520critical%2520information%2520that%2520bridges%2520the%250Adistributional%2520gap%2520between%2520input%2520histories%2520and%2520future%2520targets.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520TimeAlign%252C%2520a%2520lightweight%252C%2520plug-and-play%2520framework%2520that%2520learns%250Aauxiliary%2520features%2520via%2520a%2520simple%2520reconstruction%2520task%2520and%2520feeds%2520them%2520back%2520to%2520any%250Abase%2520forecaster.%2520Extensive%2520experiments%2520across%2520eight%2520benchmarks%2520verify%2520its%250Asuperior%2520performance.%2520Further%2520studies%2520indicate%2520that%2520the%2520gains%2520arises%2520primarily%250Afrom%2520correcting%2520frequency%2520mismatches%2520between%2520historical%2520inputs%2520and%2520future%250Aoutputs.%2520We%2520also%2520provide%2520a%2520theoretical%2520justification%2520for%2520the%2520effectiveness%2520of%250ATimeAlign%2520in%2520increasing%2520the%2520mutual%2520information%2520between%2520learned%2520representations%250Aand%2520predicted%2520targets.%2520As%2520it%2520is%2520architecture-agnostic%2520and%2520incurs%2520negligible%250Aoverhead%252C%2520TimeAlign%2520can%2520serve%2520as%2520a%2520general%2520alignment%2520module%2520for%2520modern%2520deep%250Alearning%2520time-series%2520forecasting%2520systems.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/TROUBADOUR000/TimeAlign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Past%20and%20Future%3A%20Distribution-Aware%20Alignment%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Yifan%20Hu%20and%20Jie%20Yang%20and%20Tian%20Zhou%20and%20Peiyuan%20Liu%20and%20Yujin%20Tang%20and%20Rong%20Jin%20and%20Liang%20Sun&entry.1292438233=%20%20Representation%20learning%20techniques%20like%20contrastive%20learning%20have%20long%20been%0Aexplored%20in%20time%20series%20forecasting%2C%20mirroring%20their%20success%20in%20computer%20vision%0Aand%20natural%20language%20processing.%20Yet%20recent%20state-of-the-art%20%28SOTA%29%20forecasters%0Aseldom%20adopt%20these%20representation%20approaches%20because%20they%20have%20shown%20little%0Aperformance%20advantage.%20We%20challenge%20this%20view%20and%20demonstrate%20that%20explicit%0Arepresentation%20alignment%20can%20supply%20critical%20information%20that%20bridges%20the%0Adistributional%20gap%20between%20input%20histories%20and%20future%20targets.%20To%20this%20end%2C%20we%0Aintroduce%20TimeAlign%2C%20a%20lightweight%2C%20plug-and-play%20framework%20that%20learns%0Aauxiliary%20features%20via%20a%20simple%20reconstruction%20task%20and%20feeds%20them%20back%20to%20any%0Abase%20forecaster.%20Extensive%20experiments%20across%20eight%20benchmarks%20verify%20its%0Asuperior%20performance.%20Further%20studies%20indicate%20that%20the%20gains%20arises%20primarily%0Afrom%20correcting%20frequency%20mismatches%20between%20historical%20inputs%20and%20future%0Aoutputs.%20We%20also%20provide%20a%20theoretical%20justification%20for%20the%20effectiveness%20of%0ATimeAlign%20in%20increasing%20the%20mutual%20information%20between%20learned%20representations%0Aand%20predicted%20targets.%20As%20it%20is%20architecture-agnostic%20and%20incurs%20negligible%0Aoverhead%2C%20TimeAlign%20can%20serve%20as%20a%20general%20alignment%20module%20for%20modern%20deep%0Alearning%20time-series%20forecasting%20systems.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/TROUBADOUR000/TimeAlign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14181v1&entry.124074799=Read"},
{"title": "Language models' activations linearly encode training-order recency", "author": "Dmitrii Krasheninnikov and Richard E. Turner and David Krueger", "abstract": "  We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples for the six training datasets encode the training\norder: when projected into a 2D subspace, these centroids are arranged exactly\nin the order of training and lie on a straight line. Further, we show that\nlinear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities,\ngeneralizing to entities unseen during the probes' own training. The model can\nalso be fine-tuned to explicitly report an unseen entity's training stage (~80%\naccuracy). Interestingly, this temporal signal does not seem attributable to\nsimple differences in activation magnitudes, losses, or model confidence. Our\npaper demonstrates that models are capable of differentiating information by\nits acquisition time, and carries significant implications for how they might\nmanage conflicting data and respond to knowledge modifications.\n", "link": "http://arxiv.org/abs/2509.14223v1", "date": "2025-09-17", "relevancy": 2.3955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20models%27%20activations%20linearly%20encode%20training-order%20recency&body=Title%3A%20Language%20models%27%20activations%20linearly%20encode%20training-order%20recency%0AAuthor%3A%20Dmitrii%20Krasheninnikov%20and%20Richard%20E.%20Turner%20and%20David%20Krueger%0AAbstract%3A%20%20%20We%20show%20that%20language%20models%27%20activations%20linearly%20encode%20when%20information%0Awas%20learned%20during%20training.%20Our%20setup%20involves%20creating%20a%20model%20with%20a%20known%0Atraining%20order%20by%20sequentially%20fine-tuning%20Llama-3.2-1B%20on%20six%20disjoint%20but%0Aotherwise%20similar%20datasets%20about%20named%20entities.%20We%20find%20that%20the%20average%0Aactivations%20of%20test%20samples%20for%20the%20six%20training%20datasets%20encode%20the%20training%0Aorder%3A%20when%20projected%20into%20a%202D%20subspace%2C%20these%20centroids%20are%20arranged%20exactly%0Ain%20the%20order%20of%20training%20and%20lie%20on%20a%20straight%20line.%20Further%2C%20we%20show%20that%0Alinear%20probes%20can%20accurately%20%28~90%25%29%20distinguish%20%22early%22%20vs.%20%22late%22%20entities%2C%0Ageneralizing%20to%20entities%20unseen%20during%20the%20probes%27%20own%20training.%20The%20model%20can%0Aalso%20be%20fine-tuned%20to%20explicitly%20report%20an%20unseen%20entity%27s%20training%20stage%20%28~80%25%0Aaccuracy%29.%20Interestingly%2C%20this%20temporal%20signal%20does%20not%20seem%20attributable%20to%0Asimple%20differences%20in%20activation%20magnitudes%2C%20losses%2C%20or%20model%20confidence.%20Our%0Apaper%20demonstrates%20that%20models%20are%20capable%20of%20differentiating%20information%20by%0Aits%20acquisition%20time%2C%20and%20carries%20significant%20implications%20for%20how%20they%20might%0Amanage%20conflicting%20data%20and%20respond%20to%20knowledge%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520models%2527%2520activations%2520linearly%2520encode%2520training-order%2520recency%26entry.906535625%3DDmitrii%2520Krasheninnikov%2520and%2520Richard%2520E.%2520Turner%2520and%2520David%2520Krueger%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520language%2520models%2527%2520activations%2520linearly%2520encode%2520when%2520information%250Awas%2520learned%2520during%2520training.%2520Our%2520setup%2520involves%2520creating%2520a%2520model%2520with%2520a%2520known%250Atraining%2520order%2520by%2520sequentially%2520fine-tuning%2520Llama-3.2-1B%2520on%2520six%2520disjoint%2520but%250Aotherwise%2520similar%2520datasets%2520about%2520named%2520entities.%2520We%2520find%2520that%2520the%2520average%250Aactivations%2520of%2520test%2520samples%2520for%2520the%2520six%2520training%2520datasets%2520encode%2520the%2520training%250Aorder%253A%2520when%2520projected%2520into%2520a%25202D%2520subspace%252C%2520these%2520centroids%2520are%2520arranged%2520exactly%250Ain%2520the%2520order%2520of%2520training%2520and%2520lie%2520on%2520a%2520straight%2520line.%2520Further%252C%2520we%2520show%2520that%250Alinear%2520probes%2520can%2520accurately%2520%2528~90%2525%2529%2520distinguish%2520%2522early%2522%2520vs.%2520%2522late%2522%2520entities%252C%250Ageneralizing%2520to%2520entities%2520unseen%2520during%2520the%2520probes%2527%2520own%2520training.%2520The%2520model%2520can%250Aalso%2520be%2520fine-tuned%2520to%2520explicitly%2520report%2520an%2520unseen%2520entity%2527s%2520training%2520stage%2520%2528~80%2525%250Aaccuracy%2529.%2520Interestingly%252C%2520this%2520temporal%2520signal%2520does%2520not%2520seem%2520attributable%2520to%250Asimple%2520differences%2520in%2520activation%2520magnitudes%252C%2520losses%252C%2520or%2520model%2520confidence.%2520Our%250Apaper%2520demonstrates%2520that%2520models%2520are%2520capable%2520of%2520differentiating%2520information%2520by%250Aits%2520acquisition%2520time%252C%2520and%2520carries%2520significant%2520implications%2520for%2520how%2520they%2520might%250Amanage%2520conflicting%2520data%2520and%2520respond%2520to%2520knowledge%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20models%27%20activations%20linearly%20encode%20training-order%20recency&entry.906535625=Dmitrii%20Krasheninnikov%20and%20Richard%20E.%20Turner%20and%20David%20Krueger&entry.1292438233=%20%20We%20show%20that%20language%20models%27%20activations%20linearly%20encode%20when%20information%0Awas%20learned%20during%20training.%20Our%20setup%20involves%20creating%20a%20model%20with%20a%20known%0Atraining%20order%20by%20sequentially%20fine-tuning%20Llama-3.2-1B%20on%20six%20disjoint%20but%0Aotherwise%20similar%20datasets%20about%20named%20entities.%20We%20find%20that%20the%20average%0Aactivations%20of%20test%20samples%20for%20the%20six%20training%20datasets%20encode%20the%20training%0Aorder%3A%20when%20projected%20into%20a%202D%20subspace%2C%20these%20centroids%20are%20arranged%20exactly%0Ain%20the%20order%20of%20training%20and%20lie%20on%20a%20straight%20line.%20Further%2C%20we%20show%20that%0Alinear%20probes%20can%20accurately%20%28~90%25%29%20distinguish%20%22early%22%20vs.%20%22late%22%20entities%2C%0Ageneralizing%20to%20entities%20unseen%20during%20the%20probes%27%20own%20training.%20The%20model%20can%0Aalso%20be%20fine-tuned%20to%20explicitly%20report%20an%20unseen%20entity%27s%20training%20stage%20%28~80%25%0Aaccuracy%29.%20Interestingly%2C%20this%20temporal%20signal%20does%20not%20seem%20attributable%20to%0Asimple%20differences%20in%20activation%20magnitudes%2C%20losses%2C%20or%20model%20confidence.%20Our%0Apaper%20demonstrates%20that%20models%20are%20capable%20of%20differentiating%20information%20by%0Aits%20acquisition%20time%2C%20and%20carries%20significant%20implications%20for%20how%20they%20might%0Amanage%20conflicting%20data%20and%20respond%20to%20knowledge%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14223v1&entry.124074799=Read"},
{"title": "Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile\n  Manipulator via Contact-Aware Dynamic Optimization", "author": "Zong Chen and Shaoyang Li and Ben Liu and Min Li and Zhouping Yin and Yiqun Li", "abstract": "  Wheel-legged robots with integrated manipulators hold great promise for\nmobile manipulation in logistics, industrial automation, and human-robot\ncollaboration. However, unified control of such systems remains challenging due\nto the redundancy in degrees of freedom, complex wheel-ground contact dynamics,\nand the need for seamless coordination between locomotion and manipulation. In\nthis work, we present the design and whole-body motion control of an\nomnidirectional wheel-legged quadrupedal robot equipped with a dexterous\nmanipulator. The proposed platform incorporates independently actuated steering\nmodules and hub-driven wheels, enabling agile omnidirectional locomotion with\nhigh maneuverability in structured environments. To address the challenges of\ncontact-rich interaction, we develop a contact-aware whole-body dynamic\noptimization framework that integrates point-contact modeling for manipulation\nwith line-contact modeling for wheel-ground interactions. A warm-start strategy\nis introduced to accelerate online optimization, ensuring real-time feasibility\nfor high-dimensional control. Furthermore, a unified kinematic model tailored\nfor the robot's 4WIS-4WID actuation scheme eliminates the need for mode\nswitching across different locomotion strategies, improving control consistency\nand robustness. Simulation and experimental results validate the effectiveness\nof the proposed framework, demonstrating agile terrain traversal, high-speed\nomnidirectional mobility, and precise manipulation under diverse scenarios,\nunderscoring the system's potential for factory automation, urban logistics,\nand service robotics in semi-structured environments.\n", "link": "http://arxiv.org/abs/2509.14010v1", "date": "2025-09-17", "relevancy": 2.3787, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5722}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole-body%20Motion%20Control%20of%20an%20Omnidirectional%20Wheel-Legged%20Mobile%0A%20%20Manipulator%20via%20Contact-Aware%20Dynamic%20Optimization&body=Title%3A%20Whole-body%20Motion%20Control%20of%20an%20Omnidirectional%20Wheel-Legged%20Mobile%0A%20%20Manipulator%20via%20Contact-Aware%20Dynamic%20Optimization%0AAuthor%3A%20Zong%20Chen%20and%20Shaoyang%20Li%20and%20Ben%20Liu%20and%20Min%20Li%20and%20Zhouping%20Yin%20and%20Yiqun%20Li%0AAbstract%3A%20%20%20Wheel-legged%20robots%20with%20integrated%20manipulators%20hold%20great%20promise%20for%0Amobile%20manipulation%20in%20logistics%2C%20industrial%20automation%2C%20and%20human-robot%0Acollaboration.%20However%2C%20unified%20control%20of%20such%20systems%20remains%20challenging%20due%0Ato%20the%20redundancy%20in%20degrees%20of%20freedom%2C%20complex%20wheel-ground%20contact%20dynamics%2C%0Aand%20the%20need%20for%20seamless%20coordination%20between%20locomotion%20and%20manipulation.%20In%0Athis%20work%2C%20we%20present%20the%20design%20and%20whole-body%20motion%20control%20of%20an%0Aomnidirectional%20wheel-legged%20quadrupedal%20robot%20equipped%20with%20a%20dexterous%0Amanipulator.%20The%20proposed%20platform%20incorporates%20independently%20actuated%20steering%0Amodules%20and%20hub-driven%20wheels%2C%20enabling%20agile%20omnidirectional%20locomotion%20with%0Ahigh%20maneuverability%20in%20structured%20environments.%20To%20address%20the%20challenges%20of%0Acontact-rich%20interaction%2C%20we%20develop%20a%20contact-aware%20whole-body%20dynamic%0Aoptimization%20framework%20that%20integrates%20point-contact%20modeling%20for%20manipulation%0Awith%20line-contact%20modeling%20for%20wheel-ground%20interactions.%20A%20warm-start%20strategy%0Ais%20introduced%20to%20accelerate%20online%20optimization%2C%20ensuring%20real-time%20feasibility%0Afor%20high-dimensional%20control.%20Furthermore%2C%20a%20unified%20kinematic%20model%20tailored%0Afor%20the%20robot%27s%204WIS-4WID%20actuation%20scheme%20eliminates%20the%20need%20for%20mode%0Aswitching%20across%20different%20locomotion%20strategies%2C%20improving%20control%20consistency%0Aand%20robustness.%20Simulation%20and%20experimental%20results%20validate%20the%20effectiveness%0Aof%20the%20proposed%20framework%2C%20demonstrating%20agile%20terrain%20traversal%2C%20high-speed%0Aomnidirectional%20mobility%2C%20and%20precise%20manipulation%20under%20diverse%20scenarios%2C%0Aunderscoring%20the%20system%27s%20potential%20for%20factory%20automation%2C%20urban%20logistics%2C%0Aand%20service%20robotics%20in%20semi-structured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole-body%2520Motion%2520Control%2520of%2520an%2520Omnidirectional%2520Wheel-Legged%2520Mobile%250A%2520%2520Manipulator%2520via%2520Contact-Aware%2520Dynamic%2520Optimization%26entry.906535625%3DZong%2520Chen%2520and%2520Shaoyang%2520Li%2520and%2520Ben%2520Liu%2520and%2520Min%2520Li%2520and%2520Zhouping%2520Yin%2520and%2520Yiqun%2520Li%26entry.1292438233%3D%2520%2520Wheel-legged%2520robots%2520with%2520integrated%2520manipulators%2520hold%2520great%2520promise%2520for%250Amobile%2520manipulation%2520in%2520logistics%252C%2520industrial%2520automation%252C%2520and%2520human-robot%250Acollaboration.%2520However%252C%2520unified%2520control%2520of%2520such%2520systems%2520remains%2520challenging%2520due%250Ato%2520the%2520redundancy%2520in%2520degrees%2520of%2520freedom%252C%2520complex%2520wheel-ground%2520contact%2520dynamics%252C%250Aand%2520the%2520need%2520for%2520seamless%2520coordination%2520between%2520locomotion%2520and%2520manipulation.%2520In%250Athis%2520work%252C%2520we%2520present%2520the%2520design%2520and%2520whole-body%2520motion%2520control%2520of%2520an%250Aomnidirectional%2520wheel-legged%2520quadrupedal%2520robot%2520equipped%2520with%2520a%2520dexterous%250Amanipulator.%2520The%2520proposed%2520platform%2520incorporates%2520independently%2520actuated%2520steering%250Amodules%2520and%2520hub-driven%2520wheels%252C%2520enabling%2520agile%2520omnidirectional%2520locomotion%2520with%250Ahigh%2520maneuverability%2520in%2520structured%2520environments.%2520To%2520address%2520the%2520challenges%2520of%250Acontact-rich%2520interaction%252C%2520we%2520develop%2520a%2520contact-aware%2520whole-body%2520dynamic%250Aoptimization%2520framework%2520that%2520integrates%2520point-contact%2520modeling%2520for%2520manipulation%250Awith%2520line-contact%2520modeling%2520for%2520wheel-ground%2520interactions.%2520A%2520warm-start%2520strategy%250Ais%2520introduced%2520to%2520accelerate%2520online%2520optimization%252C%2520ensuring%2520real-time%2520feasibility%250Afor%2520high-dimensional%2520control.%2520Furthermore%252C%2520a%2520unified%2520kinematic%2520model%2520tailored%250Afor%2520the%2520robot%2527s%25204WIS-4WID%2520actuation%2520scheme%2520eliminates%2520the%2520need%2520for%2520mode%250Aswitching%2520across%2520different%2520locomotion%2520strategies%252C%2520improving%2520control%2520consistency%250Aand%2520robustness.%2520Simulation%2520and%2520experimental%2520results%2520validate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520framework%252C%2520demonstrating%2520agile%2520terrain%2520traversal%252C%2520high-speed%250Aomnidirectional%2520mobility%252C%2520and%2520precise%2520manipulation%2520under%2520diverse%2520scenarios%252C%250Aunderscoring%2520the%2520system%2527s%2520potential%2520for%2520factory%2520automation%252C%2520urban%2520logistics%252C%250Aand%2520service%2520robotics%2520in%2520semi-structured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole-body%20Motion%20Control%20of%20an%20Omnidirectional%20Wheel-Legged%20Mobile%0A%20%20Manipulator%20via%20Contact-Aware%20Dynamic%20Optimization&entry.906535625=Zong%20Chen%20and%20Shaoyang%20Li%20and%20Ben%20Liu%20and%20Min%20Li%20and%20Zhouping%20Yin%20and%20Yiqun%20Li&entry.1292438233=%20%20Wheel-legged%20robots%20with%20integrated%20manipulators%20hold%20great%20promise%20for%0Amobile%20manipulation%20in%20logistics%2C%20industrial%20automation%2C%20and%20human-robot%0Acollaboration.%20However%2C%20unified%20control%20of%20such%20systems%20remains%20challenging%20due%0Ato%20the%20redundancy%20in%20degrees%20of%20freedom%2C%20complex%20wheel-ground%20contact%20dynamics%2C%0Aand%20the%20need%20for%20seamless%20coordination%20between%20locomotion%20and%20manipulation.%20In%0Athis%20work%2C%20we%20present%20the%20design%20and%20whole-body%20motion%20control%20of%20an%0Aomnidirectional%20wheel-legged%20quadrupedal%20robot%20equipped%20with%20a%20dexterous%0Amanipulator.%20The%20proposed%20platform%20incorporates%20independently%20actuated%20steering%0Amodules%20and%20hub-driven%20wheels%2C%20enabling%20agile%20omnidirectional%20locomotion%20with%0Ahigh%20maneuverability%20in%20structured%20environments.%20To%20address%20the%20challenges%20of%0Acontact-rich%20interaction%2C%20we%20develop%20a%20contact-aware%20whole-body%20dynamic%0Aoptimization%20framework%20that%20integrates%20point-contact%20modeling%20for%20manipulation%0Awith%20line-contact%20modeling%20for%20wheel-ground%20interactions.%20A%20warm-start%20strategy%0Ais%20introduced%20to%20accelerate%20online%20optimization%2C%20ensuring%20real-time%20feasibility%0Afor%20high-dimensional%20control.%20Furthermore%2C%20a%20unified%20kinematic%20model%20tailored%0Afor%20the%20robot%27s%204WIS-4WID%20actuation%20scheme%20eliminates%20the%20need%20for%20mode%0Aswitching%20across%20different%20locomotion%20strategies%2C%20improving%20control%20consistency%0Aand%20robustness.%20Simulation%20and%20experimental%20results%20validate%20the%20effectiveness%0Aof%20the%20proposed%20framework%2C%20demonstrating%20agile%20terrain%20traversal%2C%20high-speed%0Aomnidirectional%20mobility%2C%20and%20precise%20manipulation%20under%20diverse%20scenarios%2C%0Aunderscoring%20the%20system%27s%20potential%20for%20factory%20automation%2C%20urban%20logistics%2C%0Aand%20service%20robotics%20in%20semi-structured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14010v1&entry.124074799=Read"},
{"title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View\n  3D Object Detection", "author": "Rongyu Zhang and Jiaming Liu and Xiaoqi Li and Xiaowei Chi and Dan Wang and Li Du and Yuan Du and Shanghang Zhang", "abstract": "  Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.\n", "link": "http://arxiv.org/abs/2509.14151v1", "date": "2025-09-17", "relevancy": 2.3713, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6215}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5889}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEVUDA%2B%2B%3A%20Geometric-aware%20Unsupervised%20Domain%20Adaptation%20for%20Multi-View%0A%20%203D%20Object%20Detection&body=Title%3A%20BEVUDA%2B%2B%3A%20Geometric-aware%20Unsupervised%20Domain%20Adaptation%20for%20Multi-View%0A%20%203D%20Object%20Detection%0AAuthor%3A%20Rongyu%20Zhang%20and%20Jiaming%20Liu%20and%20Xiaoqi%20Li%20and%20Xiaowei%20Chi%20and%20Dan%20Wang%20and%20Li%20Du%20and%20Yuan%20Du%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Vision-centric%20Bird%27s%20Eye%20View%20%28BEV%29%20perception%20holds%20considerable%20promise%0Afor%20autonomous%20driving.%20Recent%20studies%20have%20prioritized%20efficiency%20or%20accuracy%0Aenhancements%2C%20yet%20the%20issue%20of%20domain%20shift%20has%20been%20overlooked%2C%20leading%20to%0Asubstantial%20performance%20degradation%20upon%20transfer.%20We%20identify%20major%20domain%0Agaps%20in%20real-world%20cross-domain%20scenarios%20and%20initiate%20the%20first%20effort%20to%0Aaddress%20the%20Domain%20Adaptation%20%28DA%29%20challenge%20in%20multi-view%203D%20object%20detection%0Afor%20BEV%20perception.%20Given%20the%20complexity%20of%20BEV%20perception%20approaches%20with%0Atheir%20multiple%20components%2C%20domain%20shift%20accumulation%20across%20multi-geometric%0Aspaces%20%28e.g.%2C%202D%2C%203D%20Voxel%2C%20BEV%29%20poses%20a%20significant%20challenge%20for%20BEV%20domain%0Aadaptation.%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20geometric-aware%0Ateacher-student%20framework%2C%20BEVUDA%2B%2B%2C%20to%20diminish%20this%20issue%2C%20comprising%20a%0AReliable%20Depth%20Teacher%20%28RDT%29%20and%20a%20Geometric%20Consistent%20Student%20%28GCS%29%20model.%0ASpecifically%2C%20RDT%20effectively%20blends%20target%20LiDAR%20with%20dependable%20depth%0Apredictions%20to%20generate%20depth-aware%20information%20based%20on%20uncertainty%0Aestimation%2C%20enhancing%20the%20extraction%20of%20Voxel%20and%20BEV%20features%20that%20are%0Aessential%20for%20understanding%20the%20target%20domain.%20To%20collaboratively%20reduce%20the%0Adomain%20shift%2C%20GCS%20maps%20features%20from%20multiple%20spaces%20into%20a%20unified%20geometric%0Aembedding%20space%2C%20thereby%20narrowing%20the%20gap%20in%20data%20distribution%20between%20the%20two%0Adomains.%20Additionally%2C%20we%20introduce%20a%20novel%20Uncertainty-guided%20Exponential%0AMoving%20Average%20%28UEMA%29%20to%20further%20reduce%20error%20accumulation%20due%20to%20domain%20shifts%0Ainformed%20by%20previously%20obtained%20uncertainty%20guidance.%20To%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%2C%20we%20execute%20comprehensive%20experiments%20in%0Afour%20cross-domain%20scenarios%2C%20securing%20state-of-the-art%20performance%20in%20BEV%203D%0Aobject%20detection%20tasks%2C%20e.g.%2C%2012.9%5C%25%20NDS%20and%209.5%5C%25%20mAP%20enhancement%20on%20Day-Night%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEVUDA%252B%252B%253A%2520Geometric-aware%2520Unsupervised%2520Domain%2520Adaptation%2520for%2520Multi-View%250A%2520%25203D%2520Object%2520Detection%26entry.906535625%3DRongyu%2520Zhang%2520and%2520Jiaming%2520Liu%2520and%2520Xiaoqi%2520Li%2520and%2520Xiaowei%2520Chi%2520and%2520Dan%2520Wang%2520and%2520Li%2520Du%2520and%2520Yuan%2520Du%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-centric%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520perception%2520holds%2520considerable%2520promise%250Afor%2520autonomous%2520driving.%2520Recent%2520studies%2520have%2520prioritized%2520efficiency%2520or%2520accuracy%250Aenhancements%252C%2520yet%2520the%2520issue%2520of%2520domain%2520shift%2520has%2520been%2520overlooked%252C%2520leading%2520to%250Asubstantial%2520performance%2520degradation%2520upon%2520transfer.%2520We%2520identify%2520major%2520domain%250Agaps%2520in%2520real-world%2520cross-domain%2520scenarios%2520and%2520initiate%2520the%2520first%2520effort%2520to%250Aaddress%2520the%2520Domain%2520Adaptation%2520%2528DA%2529%2520challenge%2520in%2520multi-view%25203D%2520object%2520detection%250Afor%2520BEV%2520perception.%2520Given%2520the%2520complexity%2520of%2520BEV%2520perception%2520approaches%2520with%250Atheir%2520multiple%2520components%252C%2520domain%2520shift%2520accumulation%2520across%2520multi-geometric%250Aspaces%2520%2528e.g.%252C%25202D%252C%25203D%2520Voxel%252C%2520BEV%2529%2520poses%2520a%2520significant%2520challenge%2520for%2520BEV%2520domain%250Aadaptation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520innovative%2520geometric-aware%250Ateacher-student%2520framework%252C%2520BEVUDA%252B%252B%252C%2520to%2520diminish%2520this%2520issue%252C%2520comprising%2520a%250AReliable%2520Depth%2520Teacher%2520%2528RDT%2529%2520and%2520a%2520Geometric%2520Consistent%2520Student%2520%2528GCS%2529%2520model.%250ASpecifically%252C%2520RDT%2520effectively%2520blends%2520target%2520LiDAR%2520with%2520dependable%2520depth%250Apredictions%2520to%2520generate%2520depth-aware%2520information%2520based%2520on%2520uncertainty%250Aestimation%252C%2520enhancing%2520the%2520extraction%2520of%2520Voxel%2520and%2520BEV%2520features%2520that%2520are%250Aessential%2520for%2520understanding%2520the%2520target%2520domain.%2520To%2520collaboratively%2520reduce%2520the%250Adomain%2520shift%252C%2520GCS%2520maps%2520features%2520from%2520multiple%2520spaces%2520into%2520a%2520unified%2520geometric%250Aembedding%2520space%252C%2520thereby%2520narrowing%2520the%2520gap%2520in%2520data%2520distribution%2520between%2520the%2520two%250Adomains.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520Uncertainty-guided%2520Exponential%250AMoving%2520Average%2520%2528UEMA%2529%2520to%2520further%2520reduce%2520error%2520accumulation%2520due%2520to%2520domain%2520shifts%250Ainformed%2520by%2520previously%2520obtained%2520uncertainty%2520guidance.%2520To%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520method%252C%2520we%2520execute%2520comprehensive%2520experiments%2520in%250Afour%2520cross-domain%2520scenarios%252C%2520securing%2520state-of-the-art%2520performance%2520in%2520BEV%25203D%250Aobject%2520detection%2520tasks%252C%2520e.g.%252C%252012.9%255C%2525%2520NDS%2520and%25209.5%255C%2525%2520mAP%2520enhancement%2520on%2520Day-Night%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVUDA%2B%2B%3A%20Geometric-aware%20Unsupervised%20Domain%20Adaptation%20for%20Multi-View%0A%20%203D%20Object%20Detection&entry.906535625=Rongyu%20Zhang%20and%20Jiaming%20Liu%20and%20Xiaoqi%20Li%20and%20Xiaowei%20Chi%20and%20Dan%20Wang%20and%20Li%20Du%20and%20Yuan%20Du%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Vision-centric%20Bird%27s%20Eye%20View%20%28BEV%29%20perception%20holds%20considerable%20promise%0Afor%20autonomous%20driving.%20Recent%20studies%20have%20prioritized%20efficiency%20or%20accuracy%0Aenhancements%2C%20yet%20the%20issue%20of%20domain%20shift%20has%20been%20overlooked%2C%20leading%20to%0Asubstantial%20performance%20degradation%20upon%20transfer.%20We%20identify%20major%20domain%0Agaps%20in%20real-world%20cross-domain%20scenarios%20and%20initiate%20the%20first%20effort%20to%0Aaddress%20the%20Domain%20Adaptation%20%28DA%29%20challenge%20in%20multi-view%203D%20object%20detection%0Afor%20BEV%20perception.%20Given%20the%20complexity%20of%20BEV%20perception%20approaches%20with%0Atheir%20multiple%20components%2C%20domain%20shift%20accumulation%20across%20multi-geometric%0Aspaces%20%28e.g.%2C%202D%2C%203D%20Voxel%2C%20BEV%29%20poses%20a%20significant%20challenge%20for%20BEV%20domain%0Aadaptation.%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20geometric-aware%0Ateacher-student%20framework%2C%20BEVUDA%2B%2B%2C%20to%20diminish%20this%20issue%2C%20comprising%20a%0AReliable%20Depth%20Teacher%20%28RDT%29%20and%20a%20Geometric%20Consistent%20Student%20%28GCS%29%20model.%0ASpecifically%2C%20RDT%20effectively%20blends%20target%20LiDAR%20with%20dependable%20depth%0Apredictions%20to%20generate%20depth-aware%20information%20based%20on%20uncertainty%0Aestimation%2C%20enhancing%20the%20extraction%20of%20Voxel%20and%20BEV%20features%20that%20are%0Aessential%20for%20understanding%20the%20target%20domain.%20To%20collaboratively%20reduce%20the%0Adomain%20shift%2C%20GCS%20maps%20features%20from%20multiple%20spaces%20into%20a%20unified%20geometric%0Aembedding%20space%2C%20thereby%20narrowing%20the%20gap%20in%20data%20distribution%20between%20the%20two%0Adomains.%20Additionally%2C%20we%20introduce%20a%20novel%20Uncertainty-guided%20Exponential%0AMoving%20Average%20%28UEMA%29%20to%20further%20reduce%20error%20accumulation%20due%20to%20domain%20shifts%0Ainformed%20by%20previously%20obtained%20uncertainty%20guidance.%20To%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20method%2C%20we%20execute%20comprehensive%20experiments%20in%0Afour%20cross-domain%20scenarios%2C%20securing%20state-of-the-art%20performance%20in%20BEV%203D%0Aobject%20detection%20tasks%2C%20e.g.%2C%2012.9%5C%25%20NDS%20and%209.5%5C%25%20mAP%20enhancement%20on%20Day-Night%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14151v1&entry.124074799=Read"},
{"title": "BIM Informed Visual SLAM for Construction Monitoring", "author": "Asier Bikandi and Miguel Fernandez-Cortizas and Muhammad Shaheer and Ali Tourani and Holger Voos and Jose Luis Sanchez-Lopez", "abstract": "  Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring\nconstruction sites, where aligning the evolving as-built state with the\nas-planned design enables early error detection and reduces costly rework.\nLiDAR-based SLAM achieves high geometric precision, but its sensors are\ntypically large and power-demanding, limiting their use on portable platforms.\nVisual SLAM offers a practical alternative with lightweight cameras already\nembedded in most mobile devices. however, visually mapping construction\nenvironments remains challenging: repetitive layouts, occlusions, and\nincomplete or low-texture structures often cause drift in the trajectory map.\nTo mitigate this, we propose an RGB-D SLAM system that incorporates the\nBuilding Information Model (BIM) as structural prior knowledge. Instead of\nrelying solely on visual cues, our system continuously establishes\ncorrespondences between detected wall and their BIM counterparts, which are\nthen introduced as constraints in the back-end optimization. The proposed\nmethod operates in real time and has been validated on real construction sites,\nreducing trajectory error by an average of 23.71% and map RMSE by 7.14%\ncompared to visual SLAM baselines. These results demonstrate that BIM\nconstraints enable reliable alignment of the digital plan with the as-built\nscene, even under partially constructed conditions.\n", "link": "http://arxiv.org/abs/2509.13972v1", "date": "2025-09-17", "relevancy": 2.3623, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5878}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIM%20Informed%20Visual%20SLAM%20for%20Construction%20Monitoring&body=Title%3A%20BIM%20Informed%20Visual%20SLAM%20for%20Construction%20Monitoring%0AAuthor%3A%20Asier%20Bikandi%20and%20Miguel%20Fernandez-Cortizas%20and%20Muhammad%20Shaheer%20and%20Ali%20Tourani%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20key%20tool%20for%20monitoring%0Aconstruction%20sites%2C%20where%20aligning%20the%20evolving%20as-built%20state%20with%20the%0Aas-planned%20design%20enables%20early%20error%20detection%20and%20reduces%20costly%20rework.%0ALiDAR-based%20SLAM%20achieves%20high%20geometric%20precision%2C%20but%20its%20sensors%20are%0Atypically%20large%20and%20power-demanding%2C%20limiting%20their%20use%20on%20portable%20platforms.%0AVisual%20SLAM%20offers%20a%20practical%20alternative%20with%20lightweight%20cameras%20already%0Aembedded%20in%20most%20mobile%20devices.%20however%2C%20visually%20mapping%20construction%0Aenvironments%20remains%20challenging%3A%20repetitive%20layouts%2C%20occlusions%2C%20and%0Aincomplete%20or%20low-texture%20structures%20often%20cause%20drift%20in%20the%20trajectory%20map.%0ATo%20mitigate%20this%2C%20we%20propose%20an%20RGB-D%20SLAM%20system%20that%20incorporates%20the%0ABuilding%20Information%20Model%20%28BIM%29%20as%20structural%20prior%20knowledge.%20Instead%20of%0Arelying%20solely%20on%20visual%20cues%2C%20our%20system%20continuously%20establishes%0Acorrespondences%20between%20detected%20wall%20and%20their%20BIM%20counterparts%2C%20which%20are%0Athen%20introduced%20as%20constraints%20in%20the%20back-end%20optimization.%20The%20proposed%0Amethod%20operates%20in%20real%20time%20and%20has%20been%20validated%20on%20real%20construction%20sites%2C%0Areducing%20trajectory%20error%20by%20an%20average%20of%2023.71%25%20and%20map%20RMSE%20by%207.14%25%0Acompared%20to%20visual%20SLAM%20baselines.%20These%20results%20demonstrate%20that%20BIM%0Aconstraints%20enable%20reliable%20alignment%20of%20the%20digital%20plan%20with%20the%20as-built%0Ascene%2C%20even%20under%20partially%20constructed%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIM%2520Informed%2520Visual%2520SLAM%2520for%2520Construction%2520Monitoring%26entry.906535625%3DAsier%2520Bikandi%2520and%2520Miguel%2520Fernandez-Cortizas%2520and%2520Muhammad%2520Shaheer%2520and%2520Ali%2520Tourani%2520and%2520Holger%2520Voos%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520a%2520key%2520tool%2520for%2520monitoring%250Aconstruction%2520sites%252C%2520where%2520aligning%2520the%2520evolving%2520as-built%2520state%2520with%2520the%250Aas-planned%2520design%2520enables%2520early%2520error%2520detection%2520and%2520reduces%2520costly%2520rework.%250ALiDAR-based%2520SLAM%2520achieves%2520high%2520geometric%2520precision%252C%2520but%2520its%2520sensors%2520are%250Atypically%2520large%2520and%2520power-demanding%252C%2520limiting%2520their%2520use%2520on%2520portable%2520platforms.%250AVisual%2520SLAM%2520offers%2520a%2520practical%2520alternative%2520with%2520lightweight%2520cameras%2520already%250Aembedded%2520in%2520most%2520mobile%2520devices.%2520however%252C%2520visually%2520mapping%2520construction%250Aenvironments%2520remains%2520challenging%253A%2520repetitive%2520layouts%252C%2520occlusions%252C%2520and%250Aincomplete%2520or%2520low-texture%2520structures%2520often%2520cause%2520drift%2520in%2520the%2520trajectory%2520map.%250ATo%2520mitigate%2520this%252C%2520we%2520propose%2520an%2520RGB-D%2520SLAM%2520system%2520that%2520incorporates%2520the%250ABuilding%2520Information%2520Model%2520%2528BIM%2529%2520as%2520structural%2520prior%2520knowledge.%2520Instead%2520of%250Arelying%2520solely%2520on%2520visual%2520cues%252C%2520our%2520system%2520continuously%2520establishes%250Acorrespondences%2520between%2520detected%2520wall%2520and%2520their%2520BIM%2520counterparts%252C%2520which%2520are%250Athen%2520introduced%2520as%2520constraints%2520in%2520the%2520back-end%2520optimization.%2520The%2520proposed%250Amethod%2520operates%2520in%2520real%2520time%2520and%2520has%2520been%2520validated%2520on%2520real%2520construction%2520sites%252C%250Areducing%2520trajectory%2520error%2520by%2520an%2520average%2520of%252023.71%2525%2520and%2520map%2520RMSE%2520by%25207.14%2525%250Acompared%2520to%2520visual%2520SLAM%2520baselines.%2520These%2520results%2520demonstrate%2520that%2520BIM%250Aconstraints%2520enable%2520reliable%2520alignment%2520of%2520the%2520digital%2520plan%2520with%2520the%2520as-built%250Ascene%252C%2520even%2520under%2520partially%2520constructed%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIM%20Informed%20Visual%20SLAM%20for%20Construction%20Monitoring&entry.906535625=Asier%20Bikandi%20and%20Miguel%20Fernandez-Cortizas%20and%20Muhammad%20Shaheer%20and%20Ali%20Tourani%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20key%20tool%20for%20monitoring%0Aconstruction%20sites%2C%20where%20aligning%20the%20evolving%20as-built%20state%20with%20the%0Aas-planned%20design%20enables%20early%20error%20detection%20and%20reduces%20costly%20rework.%0ALiDAR-based%20SLAM%20achieves%20high%20geometric%20precision%2C%20but%20its%20sensors%20are%0Atypically%20large%20and%20power-demanding%2C%20limiting%20their%20use%20on%20portable%20platforms.%0AVisual%20SLAM%20offers%20a%20practical%20alternative%20with%20lightweight%20cameras%20already%0Aembedded%20in%20most%20mobile%20devices.%20however%2C%20visually%20mapping%20construction%0Aenvironments%20remains%20challenging%3A%20repetitive%20layouts%2C%20occlusions%2C%20and%0Aincomplete%20or%20low-texture%20structures%20often%20cause%20drift%20in%20the%20trajectory%20map.%0ATo%20mitigate%20this%2C%20we%20propose%20an%20RGB-D%20SLAM%20system%20that%20incorporates%20the%0ABuilding%20Information%20Model%20%28BIM%29%20as%20structural%20prior%20knowledge.%20Instead%20of%0Arelying%20solely%20on%20visual%20cues%2C%20our%20system%20continuously%20establishes%0Acorrespondences%20between%20detected%20wall%20and%20their%20BIM%20counterparts%2C%20which%20are%0Athen%20introduced%20as%20constraints%20in%20the%20back-end%20optimization.%20The%20proposed%0Amethod%20operates%20in%20real%20time%20and%20has%20been%20validated%20on%20real%20construction%20sites%2C%0Areducing%20trajectory%20error%20by%20an%20average%20of%2023.71%25%20and%20map%20RMSE%20by%207.14%25%0Acompared%20to%20visual%20SLAM%20baselines.%20These%20results%20demonstrate%20that%20BIM%0Aconstraints%20enable%20reliable%20alignment%20of%20the%20digital%20plan%20with%20the%20as-built%0Ascene%2C%20even%20under%20partially%20constructed%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13972v1&entry.124074799=Read"},
{"title": "Exploring the Relationship between Brain Hemisphere States and Frequency\n  Bands through Deep Learning Optimization Techniques", "author": "Robiul Islam and Dmitry I. Ignatov and Karl Kaberg and Roman Nabatchikov", "abstract": "  This study investigates classifier performance across EEG frequency bands\nusing various optimizers and evaluates efficient class prediction for the left\nand right hemispheres. Three neural network architectures - a deep dense\nnetwork, a shallow three-layer network, and a convolutional neural network\n(CNN) - are implemented and compared using the TensorFlow and PyTorch\nframeworks. Results indicate that the Adagrad and RMSprop optimizers\nconsistently perform well across different frequency bands, with Adadelta\nexhibiting robust performance in cross-model evaluations. Specifically, Adagrad\nexcels in the beta band, while RMSprop achieves superior performance in the\ngamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among\nthe models, the CNN demonstrates the second highest accuracy, particularly in\ncapturing spatial features of EEG data. The deep dense network shows\ncompetitive performance in learning complex patterns, whereas the shallow\nthree-layer network, sometimes being less accurate, provides computational\nefficiency. SHAP (Shapley Additive Explanations) plots are employed to identify\nefficient class prediction, revealing nuanced contributions of EEG frequency\nbands to model accuracy. Overall, the study highlights the importance of\noptimizer selection, model architecture, and EEG frequency band analysis in\nenhancing classifier performance and understanding feature importance in\nneuroimaging-based classification tasks.\n", "link": "http://arxiv.org/abs/2509.14078v1", "date": "2025-09-17", "relevancy": 2.3198, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Relationship%20between%20Brain%20Hemisphere%20States%20and%20Frequency%0A%20%20Bands%20through%20Deep%20Learning%20Optimization%20Techniques&body=Title%3A%20Exploring%20the%20Relationship%20between%20Brain%20Hemisphere%20States%20and%20Frequency%0A%20%20Bands%20through%20Deep%20Learning%20Optimization%20Techniques%0AAuthor%3A%20Robiul%20Islam%20and%20Dmitry%20I.%20Ignatov%20and%20Karl%20Kaberg%20and%20Roman%20Nabatchikov%0AAbstract%3A%20%20%20This%20study%20investigates%20classifier%20performance%20across%20EEG%20frequency%20bands%0Ausing%20various%20optimizers%20and%20evaluates%20efficient%20class%20prediction%20for%20the%20left%0Aand%20right%20hemispheres.%20Three%20neural%20network%20architectures%20-%20a%20deep%20dense%0Anetwork%2C%20a%20shallow%20three-layer%20network%2C%20and%20a%20convolutional%20neural%20network%0A%28CNN%29%20-%20are%20implemented%20and%20compared%20using%20the%20TensorFlow%20and%20PyTorch%0Aframeworks.%20Results%20indicate%20that%20the%20Adagrad%20and%20RMSprop%20optimizers%0Aconsistently%20perform%20well%20across%20different%20frequency%20bands%2C%20with%20Adadelta%0Aexhibiting%20robust%20performance%20in%20cross-model%20evaluations.%20Specifically%2C%20Adagrad%0Aexcels%20in%20the%20beta%20band%2C%20while%20RMSprop%20achieves%20superior%20performance%20in%20the%0Agamma%20band.%20Conversely%2C%20SGD%20and%20FTRL%20exhibit%20inconsistent%20performance.%20Among%0Athe%20models%2C%20the%20CNN%20demonstrates%20the%20second%20highest%20accuracy%2C%20particularly%20in%0Acapturing%20spatial%20features%20of%20EEG%20data.%20The%20deep%20dense%20network%20shows%0Acompetitive%20performance%20in%20learning%20complex%20patterns%2C%20whereas%20the%20shallow%0Athree-layer%20network%2C%20sometimes%20being%20less%20accurate%2C%20provides%20computational%0Aefficiency.%20SHAP%20%28Shapley%20Additive%20Explanations%29%20plots%20are%20employed%20to%20identify%0Aefficient%20class%20prediction%2C%20revealing%20nuanced%20contributions%20of%20EEG%20frequency%0Abands%20to%20model%20accuracy.%20Overall%2C%20the%20study%20highlights%20the%20importance%20of%0Aoptimizer%20selection%2C%20model%20architecture%2C%20and%20EEG%20frequency%20band%20analysis%20in%0Aenhancing%20classifier%20performance%20and%20understanding%20feature%20importance%20in%0Aneuroimaging-based%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Relationship%2520between%2520Brain%2520Hemisphere%2520States%2520and%2520Frequency%250A%2520%2520Bands%2520through%2520Deep%2520Learning%2520Optimization%2520Techniques%26entry.906535625%3DRobiul%2520Islam%2520and%2520Dmitry%2520I.%2520Ignatov%2520and%2520Karl%2520Kaberg%2520and%2520Roman%2520Nabatchikov%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520classifier%2520performance%2520across%2520EEG%2520frequency%2520bands%250Ausing%2520various%2520optimizers%2520and%2520evaluates%2520efficient%2520class%2520prediction%2520for%2520the%2520left%250Aand%2520right%2520hemispheres.%2520Three%2520neural%2520network%2520architectures%2520-%2520a%2520deep%2520dense%250Anetwork%252C%2520a%2520shallow%2520three-layer%2520network%252C%2520and%2520a%2520convolutional%2520neural%2520network%250A%2528CNN%2529%2520-%2520are%2520implemented%2520and%2520compared%2520using%2520the%2520TensorFlow%2520and%2520PyTorch%250Aframeworks.%2520Results%2520indicate%2520that%2520the%2520Adagrad%2520and%2520RMSprop%2520optimizers%250Aconsistently%2520perform%2520well%2520across%2520different%2520frequency%2520bands%252C%2520with%2520Adadelta%250Aexhibiting%2520robust%2520performance%2520in%2520cross-model%2520evaluations.%2520Specifically%252C%2520Adagrad%250Aexcels%2520in%2520the%2520beta%2520band%252C%2520while%2520RMSprop%2520achieves%2520superior%2520performance%2520in%2520the%250Agamma%2520band.%2520Conversely%252C%2520SGD%2520and%2520FTRL%2520exhibit%2520inconsistent%2520performance.%2520Among%250Athe%2520models%252C%2520the%2520CNN%2520demonstrates%2520the%2520second%2520highest%2520accuracy%252C%2520particularly%2520in%250Acapturing%2520spatial%2520features%2520of%2520EEG%2520data.%2520The%2520deep%2520dense%2520network%2520shows%250Acompetitive%2520performance%2520in%2520learning%2520complex%2520patterns%252C%2520whereas%2520the%2520shallow%250Athree-layer%2520network%252C%2520sometimes%2520being%2520less%2520accurate%252C%2520provides%2520computational%250Aefficiency.%2520SHAP%2520%2528Shapley%2520Additive%2520Explanations%2529%2520plots%2520are%2520employed%2520to%2520identify%250Aefficient%2520class%2520prediction%252C%2520revealing%2520nuanced%2520contributions%2520of%2520EEG%2520frequency%250Abands%2520to%2520model%2520accuracy.%2520Overall%252C%2520the%2520study%2520highlights%2520the%2520importance%2520of%250Aoptimizer%2520selection%252C%2520model%2520architecture%252C%2520and%2520EEG%2520frequency%2520band%2520analysis%2520in%250Aenhancing%2520classifier%2520performance%2520and%2520understanding%2520feature%2520importance%2520in%250Aneuroimaging-based%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Relationship%20between%20Brain%20Hemisphere%20States%20and%20Frequency%0A%20%20Bands%20through%20Deep%20Learning%20Optimization%20Techniques&entry.906535625=Robiul%20Islam%20and%20Dmitry%20I.%20Ignatov%20and%20Karl%20Kaberg%20and%20Roman%20Nabatchikov&entry.1292438233=%20%20This%20study%20investigates%20classifier%20performance%20across%20EEG%20frequency%20bands%0Ausing%20various%20optimizers%20and%20evaluates%20efficient%20class%20prediction%20for%20the%20left%0Aand%20right%20hemispheres.%20Three%20neural%20network%20architectures%20-%20a%20deep%20dense%0Anetwork%2C%20a%20shallow%20three-layer%20network%2C%20and%20a%20convolutional%20neural%20network%0A%28CNN%29%20-%20are%20implemented%20and%20compared%20using%20the%20TensorFlow%20and%20PyTorch%0Aframeworks.%20Results%20indicate%20that%20the%20Adagrad%20and%20RMSprop%20optimizers%0Aconsistently%20perform%20well%20across%20different%20frequency%20bands%2C%20with%20Adadelta%0Aexhibiting%20robust%20performance%20in%20cross-model%20evaluations.%20Specifically%2C%20Adagrad%0Aexcels%20in%20the%20beta%20band%2C%20while%20RMSprop%20achieves%20superior%20performance%20in%20the%0Agamma%20band.%20Conversely%2C%20SGD%20and%20FTRL%20exhibit%20inconsistent%20performance.%20Among%0Athe%20models%2C%20the%20CNN%20demonstrates%20the%20second%20highest%20accuracy%2C%20particularly%20in%0Acapturing%20spatial%20features%20of%20EEG%20data.%20The%20deep%20dense%20network%20shows%0Acompetitive%20performance%20in%20learning%20complex%20patterns%2C%20whereas%20the%20shallow%0Athree-layer%20network%2C%20sometimes%20being%20less%20accurate%2C%20provides%20computational%0Aefficiency.%20SHAP%20%28Shapley%20Additive%20Explanations%29%20plots%20are%20employed%20to%20identify%0Aefficient%20class%20prediction%2C%20revealing%20nuanced%20contributions%20of%20EEG%20frequency%0Abands%20to%20model%20accuracy.%20Overall%2C%20the%20study%20highlights%20the%20importance%20of%0Aoptimizer%20selection%2C%20model%20architecture%2C%20and%20EEG%20frequency%20band%20analysis%20in%0Aenhancing%20classifier%20performance%20and%20understanding%20feature%20importance%20in%0Aneuroimaging-based%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14078v1&entry.124074799=Read"},
{"title": "Robust Shape Regularity Criteria for Superpixel Evaluation", "author": "R\u00e9mi Giraud and Vinh-Thong Ta and Nicolas Papadakis", "abstract": "  Regular decompositions are necessary for most superpixel-based object\nrecognition or tracking applications. So far in the literature, the regularity\nor compactness of a superpixel shape is mainly measured by its circularity. In\nthis work, we first demonstrate that such measure is not adapted for superpixel\nevaluation, since it does not directly express regularity but circular\nappearance. Then, we propose a new metric that considers several shape\nregularity aspects: convexity, balanced repartition, and contour smoothness.\nFinally, we demonstrate that our measure is robust to scale and noise and\nenables to more relevantly compare superpixel methods.\n", "link": "http://arxiv.org/abs/1903.07146v2", "date": "2025-09-17", "relevancy": 2.3151, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation&body=Title%3A%20Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20Regular%20decompositions%20are%20necessary%20for%20most%20superpixel-based%20object%0Arecognition%20or%20tracking%20applications.%20So%20far%20in%20the%20literature%2C%20the%20regularity%0Aor%20compactness%20of%20a%20superpixel%20shape%20is%20mainly%20measured%20by%20its%20circularity.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20that%20such%20measure%20is%20not%20adapted%20for%20superpixel%0Aevaluation%2C%20since%20it%20does%20not%20directly%20express%20regularity%20but%20circular%0Aappearance.%20Then%2C%20we%20propose%20a%20new%20metric%20that%20considers%20several%20shape%0Aregularity%20aspects%3A%20convexity%2C%20balanced%20repartition%2C%20and%20contour%20smoothness.%0AFinally%2C%20we%20demonstrate%20that%20our%20measure%20is%20robust%20to%20scale%20and%20noise%20and%0Aenables%20to%20more%20relevantly%20compare%20superpixel%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1903.07146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Shape%2520Regularity%2520Criteria%2520for%2520Superpixel%2520Evaluation%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Vinh-Thong%2520Ta%2520and%2520Nicolas%2520Papadakis%26entry.1292438233%3D%2520%2520Regular%2520decompositions%2520are%2520necessary%2520for%2520most%2520superpixel-based%2520object%250Arecognition%2520or%2520tracking%2520applications.%2520So%2520far%2520in%2520the%2520literature%252C%2520the%2520regularity%250Aor%2520compactness%2520of%2520a%2520superpixel%2520shape%2520is%2520mainly%2520measured%2520by%2520its%2520circularity.%2520In%250Athis%2520work%252C%2520we%2520first%2520demonstrate%2520that%2520such%2520measure%2520is%2520not%2520adapted%2520for%2520superpixel%250Aevaluation%252C%2520since%2520it%2520does%2520not%2520directly%2520express%2520regularity%2520but%2520circular%250Aappearance.%2520Then%252C%2520we%2520propose%2520a%2520new%2520metric%2520that%2520considers%2520several%2520shape%250Aregularity%2520aspects%253A%2520convexity%252C%2520balanced%2520repartition%252C%2520and%2520contour%2520smoothness.%250AFinally%252C%2520we%2520demonstrate%2520that%2520our%2520measure%2520is%2520robust%2520to%2520scale%2520and%2520noise%2520and%250Aenables%2520to%2520more%2520relevantly%2520compare%2520superpixel%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1903.07146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation&entry.906535625=R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20Regular%20decompositions%20are%20necessary%20for%20most%20superpixel-based%20object%0Arecognition%20or%20tracking%20applications.%20So%20far%20in%20the%20literature%2C%20the%20regularity%0Aor%20compactness%20of%20a%20superpixel%20shape%20is%20mainly%20measured%20by%20its%20circularity.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20that%20such%20measure%20is%20not%20adapted%20for%20superpixel%0Aevaluation%2C%20since%20it%20does%20not%20directly%20express%20regularity%20but%20circular%0Aappearance.%20Then%2C%20we%20propose%20a%20new%20metric%20that%20considers%20several%20shape%0Aregularity%20aspects%3A%20convexity%2C%20balanced%20repartition%2C%20and%20contour%20smoothness.%0AFinally%2C%20we%20demonstrate%20that%20our%20measure%20is%20robust%20to%20scale%20and%20noise%20and%0Aenables%20to%20more%20relevantly%20compare%20superpixel%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/1903.07146v2&entry.124074799=Read"},
{"title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision", "author": "Luozheng Qin and Jia Gong and Yuqing Sun and Tianjiao Li and Mengping Yang and Xiaomeng Yang and Chao Qu and Zhiyu Tan and Hao Li", "abstract": "  Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/\n", "link": "http://arxiv.org/abs/2508.05606v2", "date": "2025-09-17", "relevancy": 2.3013, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision&body=Title%3A%20Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision%0AAuthor%3A%20Luozheng%20Qin%20and%20Jia%20Gong%20and%20Yuqing%20Sun%20and%20Tianjiao%20Li%20and%20Mengping%20Yang%20and%20Xiaomeng%20Yang%20and%20Chao%20Qu%20and%20Zhiyu%20Tan%20and%20Hao%20Li%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%20adopted%20to%20enhance%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20decomposing%20complex%20tasks%20into%20simpler%2C%20sequential%0Asubtasks.%20However%2C%20extending%20CoT%20to%20vision-language%20reasoning%20tasks%20remains%0Achallenging%2C%20as%20it%20often%20requires%20interpreting%20transitions%20of%20visual%20states%20to%0Asupport%20reasoning.%20Existing%20methods%20often%20struggle%20with%20this%20due%20to%20limited%0Acapacity%20of%20modeling%20visual%20state%20transitions%20or%20incoherent%20visual%20trajectories%0Acaused%20by%20fragmented%20architectures.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20Uni-CoT%2C%20a%20Unified%20Chain-of-Thought%0Aframework%20that%20enables%20coherent%20and%20grounded%20multimodal%20reasoning%20within%20a%0Asingle%20unified%20model.%20The%20key%20idea%20is%20to%20leverage%20a%20model%20capable%20of%20both%20image%0Aunderstanding%20and%20generation%20to%20reason%20over%20visual%20content%20and%20model%20evolving%0Avisual%20states.%20However%2C%20empowering%20a%20unified%20model%20to%20achieve%20that%20is%0Anon-trivial%2C%20given%20the%20high%20computational%20cost%20and%20the%20burden%20of%20training.%20To%0Aaddress%20this%2C%20Uni-CoT%20introduces%20a%20novel%20two-level%20reasoning%20paradigm%3A%20A%0AMacro-Level%20CoT%20for%20high-level%20task%20planning%20and%20A%20Micro-Level%20CoT%20for%20subtask%0Aexecution.%20This%20design%20significantly%20reduces%20the%20computational%20overhead.%0AFurthermore%2C%20we%20introduce%20a%20structured%20training%20paradigm%20that%20combines%0Ainterleaved%20image-text%20supervision%20for%20macro-level%20CoT%20with%20multi-task%0Aobjectives%20for%20micro-level%20CoT.%20Together%2C%20these%20innovations%20allow%20Uni-CoT%20to%0Aperform%20scalable%20and%20coherent%20multi-modal%20reasoning.%20Furthermore%2C%20thanks%20to%20our%0Adesign%2C%20all%20experiments%20can%20be%20efficiently%20completed%20using%20only%208%20A100%20GPUs%0Awith%2080GB%20VRAM%20each.%20Experimental%20results%20on%20reasoning-driven%20image%20generation%0Abenchmark%20%28WISE%29%20and%20editing%20benchmarks%20%28RISE%20and%20KRIS%29%20indicates%20that%20Uni-CoT%0Ademonstrates%20SOTA%20performance%20and%20strong%20generalization%2C%20establishing%20Uni-CoT%0Aas%20a%20promising%20solution%20for%20multi-modal%20reasoning.%20Project%20Page%20and%20Code%3A%0Ahttps%3A//sais-fuxi.github.io/projects/uni-cot/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-cot%253A%2520Towards%2520Unified%2520Chain-of-Thought%2520Reasoning%2520Across%2520Text%2520and%250A%2520%2520Vision%26entry.906535625%3DLuozheng%2520Qin%2520and%2520Jia%2520Gong%2520and%2520Yuqing%2520Sun%2520and%2520Tianjiao%2520Li%2520and%2520Mengping%2520Yang%2520and%2520Xiaomeng%2520Yang%2520and%2520Chao%2520Qu%2520and%2520Zhiyu%2520Tan%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520has%2520been%2520widely%2520adopted%2520to%2520enhance%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520by%2520decomposing%2520complex%2520tasks%2520into%2520simpler%252C%2520sequential%250Asubtasks.%2520However%252C%2520extending%2520CoT%2520to%2520vision-language%2520reasoning%2520tasks%2520remains%250Achallenging%252C%2520as%2520it%2520often%2520requires%2520interpreting%2520transitions%2520of%2520visual%2520states%2520to%250Asupport%2520reasoning.%2520Existing%2520methods%2520often%2520struggle%2520with%2520this%2520due%2520to%2520limited%250Acapacity%2520of%2520modeling%2520visual%2520state%2520transitions%2520or%2520incoherent%2520visual%2520trajectories%250Acaused%2520by%2520fragmented%2520architectures.%250A%2520%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Uni-CoT%252C%2520a%2520Unified%2520Chain-of-Thought%250Aframework%2520that%2520enables%2520coherent%2520and%2520grounded%2520multimodal%2520reasoning%2520within%2520a%250Asingle%2520unified%2520model.%2520The%2520key%2520idea%2520is%2520to%2520leverage%2520a%2520model%2520capable%2520of%2520both%2520image%250Aunderstanding%2520and%2520generation%2520to%2520reason%2520over%2520visual%2520content%2520and%2520model%2520evolving%250Avisual%2520states.%2520However%252C%2520empowering%2520a%2520unified%2520model%2520to%2520achieve%2520that%2520is%250Anon-trivial%252C%2520given%2520the%2520high%2520computational%2520cost%2520and%2520the%2520burden%2520of%2520training.%2520To%250Aaddress%2520this%252C%2520Uni-CoT%2520introduces%2520a%2520novel%2520two-level%2520reasoning%2520paradigm%253A%2520A%250AMacro-Level%2520CoT%2520for%2520high-level%2520task%2520planning%2520and%2520A%2520Micro-Level%2520CoT%2520for%2520subtask%250Aexecution.%2520This%2520design%2520significantly%2520reduces%2520the%2520computational%2520overhead.%250AFurthermore%252C%2520we%2520introduce%2520a%2520structured%2520training%2520paradigm%2520that%2520combines%250Ainterleaved%2520image-text%2520supervision%2520for%2520macro-level%2520CoT%2520with%2520multi-task%250Aobjectives%2520for%2520micro-level%2520CoT.%2520Together%252C%2520these%2520innovations%2520allow%2520Uni-CoT%2520to%250Aperform%2520scalable%2520and%2520coherent%2520multi-modal%2520reasoning.%2520Furthermore%252C%2520thanks%2520to%2520our%250Adesign%252C%2520all%2520experiments%2520can%2520be%2520efficiently%2520completed%2520using%2520only%25208%2520A100%2520GPUs%250Awith%252080GB%2520VRAM%2520each.%2520Experimental%2520results%2520on%2520reasoning-driven%2520image%2520generation%250Abenchmark%2520%2528WISE%2529%2520and%2520editing%2520benchmarks%2520%2528RISE%2520and%2520KRIS%2529%2520indicates%2520that%2520Uni-CoT%250Ademonstrates%2520SOTA%2520performance%2520and%2520strong%2520generalization%252C%2520establishing%2520Uni-CoT%250Aas%2520a%2520promising%2520solution%2520for%2520multi-modal%2520reasoning.%2520Project%2520Page%2520and%2520Code%253A%250Ahttps%253A//sais-fuxi.github.io/projects/uni-cot/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-cot%3A%20Towards%20Unified%20Chain-of-Thought%20Reasoning%20Across%20Text%20and%0A%20%20Vision&entry.906535625=Luozheng%20Qin%20and%20Jia%20Gong%20and%20Yuqing%20Sun%20and%20Tianjiao%20Li%20and%20Mengping%20Yang%20and%20Xiaomeng%20Yang%20and%20Chao%20Qu%20and%20Zhiyu%20Tan%20and%20Hao%20Li&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%20adopted%20to%20enhance%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20decomposing%20complex%20tasks%20into%20simpler%2C%20sequential%0Asubtasks.%20However%2C%20extending%20CoT%20to%20vision-language%20reasoning%20tasks%20remains%0Achallenging%2C%20as%20it%20often%20requires%20interpreting%20transitions%20of%20visual%20states%20to%0Asupport%20reasoning.%20Existing%20methods%20often%20struggle%20with%20this%20due%20to%20limited%0Acapacity%20of%20modeling%20visual%20state%20transitions%20or%20incoherent%20visual%20trajectories%0Acaused%20by%20fragmented%20architectures.%0A%20%20To%20overcome%20these%20limitations%2C%20we%20propose%20Uni-CoT%2C%20a%20Unified%20Chain-of-Thought%0Aframework%20that%20enables%20coherent%20and%20grounded%20multimodal%20reasoning%20within%20a%0Asingle%20unified%20model.%20The%20key%20idea%20is%20to%20leverage%20a%20model%20capable%20of%20both%20image%0Aunderstanding%20and%20generation%20to%20reason%20over%20visual%20content%20and%20model%20evolving%0Avisual%20states.%20However%2C%20empowering%20a%20unified%20model%20to%20achieve%20that%20is%0Anon-trivial%2C%20given%20the%20high%20computational%20cost%20and%20the%20burden%20of%20training.%20To%0Aaddress%20this%2C%20Uni-CoT%20introduces%20a%20novel%20two-level%20reasoning%20paradigm%3A%20A%0AMacro-Level%20CoT%20for%20high-level%20task%20planning%20and%20A%20Micro-Level%20CoT%20for%20subtask%0Aexecution.%20This%20design%20significantly%20reduces%20the%20computational%20overhead.%0AFurthermore%2C%20we%20introduce%20a%20structured%20training%20paradigm%20that%20combines%0Ainterleaved%20image-text%20supervision%20for%20macro-level%20CoT%20with%20multi-task%0Aobjectives%20for%20micro-level%20CoT.%20Together%2C%20these%20innovations%20allow%20Uni-CoT%20to%0Aperform%20scalable%20and%20coherent%20multi-modal%20reasoning.%20Furthermore%2C%20thanks%20to%20our%0Adesign%2C%20all%20experiments%20can%20be%20efficiently%20completed%20using%20only%208%20A100%20GPUs%0Awith%2080GB%20VRAM%20each.%20Experimental%20results%20on%20reasoning-driven%20image%20generation%0Abenchmark%20%28WISE%29%20and%20editing%20benchmarks%20%28RISE%20and%20KRIS%29%20indicates%20that%20Uni-CoT%0Ademonstrates%20SOTA%20performance%20and%20strong%20generalization%2C%20establishing%20Uni-CoT%0Aas%20a%20promising%20solution%20for%20multi-modal%20reasoning.%20Project%20Page%20and%20Code%3A%0Ahttps%3A//sais-fuxi.github.io/projects/uni-cot/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05606v2&entry.124074799=Read"},
{"title": "Prompt2Auto: From Motion Prompt to Automated Control via\n  Geometry-Invariant One-Shot Gaussian Process Learning", "author": "Zewen Yang and Xiaobing Dai and Dongfa Zhang and Yu Li and Ziyang Meng and Bingkun Huang and Hamid Sadeghian and Sami Haddadin", "abstract": "  Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io\n", "link": "http://arxiv.org/abs/2509.14040v1", "date": "2025-09-17", "relevancy": 2.2966, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5836}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt2Auto%3A%20From%20Motion%20Prompt%20to%20Automated%20Control%20via%0A%20%20Geometry-Invariant%20One-Shot%20Gaussian%20Process%20Learning&body=Title%3A%20Prompt2Auto%3A%20From%20Motion%20Prompt%20to%20Automated%20Control%20via%0A%20%20Geometry-Invariant%20One-Shot%20Gaussian%20Process%20Learning%0AAuthor%3A%20Zewen%20Yang%20and%20Xiaobing%20Dai%20and%20Dongfa%20Zhang%20and%20Yu%20Li%20and%20Ziyang%20Meng%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Learning%20from%20demonstration%20allows%20robots%20to%20acquire%20complex%20skills%20from%0Ahuman%20demonstrations%2C%20but%20conventional%20approaches%20often%20require%20large%20datasets%0Aand%20fail%20to%20generalize%20across%20coordinate%20transformations.%20In%20this%20paper%2C%20we%0Apropose%20Prompt2Auto%2C%20a%20geometry-invariant%20one-shot%20Gaussian%20process%20%28GeoGP%29%0Alearning%20framework%20that%20enables%20robots%20to%20perform%20human-guided%20automated%0Acontrol%20from%20a%20single%20motion%20prompt.%20A%20dataset-construction%20strategy%20based%20on%0Acoordinate%20transformations%20is%20introduced%20that%20enforces%20invariance%20to%0Atranslation%2C%20rotation%2C%20and%20scaling%2C%20while%20supporting%20multi-step%20predictions.%0AMoreover%2C%20GeoGP%20is%20robust%20to%20variations%20in%20the%20user%27s%20motion%20prompt%20and%0Asupports%20multi-skill%20autonomy.%20We%20validate%20the%20proposed%20approach%20through%0Anumerical%20simulations%20with%20the%20designed%20user%20graphical%20interface%20and%20two%0Areal-world%20robotic%20experiments%2C%20which%20demonstrate%20that%20the%20proposed%20method%20is%0Aeffective%2C%20generalizes%20across%20tasks%2C%20and%20significantly%20reduces%20the%0Ademonstration%20burden.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//prompt2auto.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt2Auto%253A%2520From%2520Motion%2520Prompt%2520to%2520Automated%2520Control%2520via%250A%2520%2520Geometry-Invariant%2520One-Shot%2520Gaussian%2520Process%2520Learning%26entry.906535625%3DZewen%2520Yang%2520and%2520Xiaobing%2520Dai%2520and%2520Dongfa%2520Zhang%2520and%2520Yu%2520Li%2520and%2520Ziyang%2520Meng%2520and%2520Bingkun%2520Huang%2520and%2520Hamid%2520Sadeghian%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Learning%2520from%2520demonstration%2520allows%2520robots%2520to%2520acquire%2520complex%2520skills%2520from%250Ahuman%2520demonstrations%252C%2520but%2520conventional%2520approaches%2520often%2520require%2520large%2520datasets%250Aand%2520fail%2520to%2520generalize%2520across%2520coordinate%2520transformations.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Prompt2Auto%252C%2520a%2520geometry-invariant%2520one-shot%2520Gaussian%2520process%2520%2528GeoGP%2529%250Alearning%2520framework%2520that%2520enables%2520robots%2520to%2520perform%2520human-guided%2520automated%250Acontrol%2520from%2520a%2520single%2520motion%2520prompt.%2520A%2520dataset-construction%2520strategy%2520based%2520on%250Acoordinate%2520transformations%2520is%2520introduced%2520that%2520enforces%2520invariance%2520to%250Atranslation%252C%2520rotation%252C%2520and%2520scaling%252C%2520while%2520supporting%2520multi-step%2520predictions.%250AMoreover%252C%2520GeoGP%2520is%2520robust%2520to%2520variations%2520in%2520the%2520user%2527s%2520motion%2520prompt%2520and%250Asupports%2520multi-skill%2520autonomy.%2520We%2520validate%2520the%2520proposed%2520approach%2520through%250Anumerical%2520simulations%2520with%2520the%2520designed%2520user%2520graphical%2520interface%2520and%2520two%250Areal-world%2520robotic%2520experiments%252C%2520which%2520demonstrate%2520that%2520the%2520proposed%2520method%2520is%250Aeffective%252C%2520generalizes%2520across%2520tasks%252C%2520and%2520significantly%2520reduces%2520the%250Ademonstration%2520burden.%2520Project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//prompt2auto.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt2Auto%3A%20From%20Motion%20Prompt%20to%20Automated%20Control%20via%0A%20%20Geometry-Invariant%20One-Shot%20Gaussian%20Process%20Learning&entry.906535625=Zewen%20Yang%20and%20Xiaobing%20Dai%20and%20Dongfa%20Zhang%20and%20Yu%20Li%20and%20Ziyang%20Meng%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin&entry.1292438233=%20%20Learning%20from%20demonstration%20allows%20robots%20to%20acquire%20complex%20skills%20from%0Ahuman%20demonstrations%2C%20but%20conventional%20approaches%20often%20require%20large%20datasets%0Aand%20fail%20to%20generalize%20across%20coordinate%20transformations.%20In%20this%20paper%2C%20we%0Apropose%20Prompt2Auto%2C%20a%20geometry-invariant%20one-shot%20Gaussian%20process%20%28GeoGP%29%0Alearning%20framework%20that%20enables%20robots%20to%20perform%20human-guided%20automated%0Acontrol%20from%20a%20single%20motion%20prompt.%20A%20dataset-construction%20strategy%20based%20on%0Acoordinate%20transformations%20is%20introduced%20that%20enforces%20invariance%20to%0Atranslation%2C%20rotation%2C%20and%20scaling%2C%20while%20supporting%20multi-step%20predictions.%0AMoreover%2C%20GeoGP%20is%20robust%20to%20variations%20in%20the%20user%27s%20motion%20prompt%20and%0Asupports%20multi-skill%20autonomy.%20We%20validate%20the%20proposed%20approach%20through%0Anumerical%20simulations%20with%20the%20designed%20user%20graphical%20interface%20and%20two%0Areal-world%20robotic%20experiments%2C%20which%20demonstrate%20that%20the%20proposed%20method%20is%0Aeffective%2C%20generalizes%20across%20tasks%2C%20and%20significantly%20reduces%20the%0Ademonstration%20burden.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//prompt2auto.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14040v1&entry.124074799=Read"},
{"title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High\n  Resolutions", "author": "Michal Szczepanski and Martyna Poreba and Karim Haroun", "abstract": "  Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.\n", "link": "http://arxiv.org/abs/2509.14165v1", "date": "2025-09-17", "relevancy": 2.2855, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6586}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Do%20Tokens%20Go%3F%20Understanding%20Pruning%20Behaviors%20in%20STEP%20at%20High%0A%20%20Resolutions&body=Title%3A%20Where%20Do%20Tokens%20Go%3F%20Understanding%20Pruning%20Behaviors%20in%20STEP%20at%20High%0A%20%20Resolutions%0AAuthor%3A%20Michal%20Szczepanski%20and%20Martyna%20Poreba%20and%20Karim%20Haroun%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20state-of-the-art%20performance%20in%20semantic%0Asegmentation%20but%20are%20hindered%20by%20high%20computational%20and%20memory%20costs.%20To%0Aaddress%20this%2C%20we%20propose%20STEP%20%28SuperToken%20and%20Early-Pruning%29%2C%20a%20hybrid%0Atoken-reduction%20framework%20that%20combines%20dynamic%20patch%20merging%20and%20token%20pruning%0Ato%20enhance%20efficiency%20without%20significantly%20compromising%20accuracy.%20At%20the%20core%0Aof%20STEP%20is%20dCTS%2C%20a%20lightweight%20CNN-based%20policy%20network%20that%20enables%20flexible%0Amerging%20into%20superpatches.%20Encoder%20blocks%20integrate%20also%20early-exits%20to%20remove%0Ahigh-confident%20supertokens%2C%20lowering%20computational%20load.%20We%20evaluate%20our%20method%0Aon%20high-resolution%20semantic%20segmentation%20benchmarks%2C%20including%20images%20up%20to%0A1024%20x%201024%2C%20and%20show%20that%20when%20dCTS%20is%20applied%20alone%2C%20the%20token%20count%20can%20be%0Areduced%20by%20a%20factor%20of%202.5%20compared%20to%20the%20standard%2016%20x%2016%20pixel%20patching%0Ascheme.%20This%20yields%20a%202.6x%20reduction%20in%20computational%20cost%20and%20a%203.4x%20increase%0Ain%20throughput%20when%20using%20ViT-Large%20as%20the%20backbone.%20Applying%20the%20full%20STEP%0Aframework%20further%20improves%20efficiency%2C%20reaching%20up%20to%20a%204x%20reduction%20in%0Acomputational%20complexity%20and%20a%201.7x%20gain%20in%20inference%20speed%2C%20with%20a%20maximum%0Aaccuracy%20drop%20of%20no%20more%20than%202.0%25.%20With%20the%20proposed%20STEP%20configurations%2C%20up%0Ato%2040%25%20of%20tokens%20can%20be%20confidently%20predicted%20and%20halted%20before%20reaching%20the%0Afinal%20encoder%20layer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Do%2520Tokens%2520Go%253F%2520Understanding%2520Pruning%2520Behaviors%2520in%2520STEP%2520at%2520High%250A%2520%2520Resolutions%26entry.906535625%3DMichal%2520Szczepanski%2520and%2520Martyna%2520Poreba%2520and%2520Karim%2520Haroun%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520achieve%2520state-of-the-art%2520performance%2520in%2520semantic%250Asegmentation%2520but%2520are%2520hindered%2520by%2520high%2520computational%2520and%2520memory%2520costs.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520STEP%2520%2528SuperToken%2520and%2520Early-Pruning%2529%252C%2520a%2520hybrid%250Atoken-reduction%2520framework%2520that%2520combines%2520dynamic%2520patch%2520merging%2520and%2520token%2520pruning%250Ato%2520enhance%2520efficiency%2520without%2520significantly%2520compromising%2520accuracy.%2520At%2520the%2520core%250Aof%2520STEP%2520is%2520dCTS%252C%2520a%2520lightweight%2520CNN-based%2520policy%2520network%2520that%2520enables%2520flexible%250Amerging%2520into%2520superpatches.%2520Encoder%2520blocks%2520integrate%2520also%2520early-exits%2520to%2520remove%250Ahigh-confident%2520supertokens%252C%2520lowering%2520computational%2520load.%2520We%2520evaluate%2520our%2520method%250Aon%2520high-resolution%2520semantic%2520segmentation%2520benchmarks%252C%2520including%2520images%2520up%2520to%250A1024%2520x%25201024%252C%2520and%2520show%2520that%2520when%2520dCTS%2520is%2520applied%2520alone%252C%2520the%2520token%2520count%2520can%2520be%250Areduced%2520by%2520a%2520factor%2520of%25202.5%2520compared%2520to%2520the%2520standard%252016%2520x%252016%2520pixel%2520patching%250Ascheme.%2520This%2520yields%2520a%25202.6x%2520reduction%2520in%2520computational%2520cost%2520and%2520a%25203.4x%2520increase%250Ain%2520throughput%2520when%2520using%2520ViT-Large%2520as%2520the%2520backbone.%2520Applying%2520the%2520full%2520STEP%250Aframework%2520further%2520improves%2520efficiency%252C%2520reaching%2520up%2520to%2520a%25204x%2520reduction%2520in%250Acomputational%2520complexity%2520and%2520a%25201.7x%2520gain%2520in%2520inference%2520speed%252C%2520with%2520a%2520maximum%250Aaccuracy%2520drop%2520of%2520no%2520more%2520than%25202.0%2525.%2520With%2520the%2520proposed%2520STEP%2520configurations%252C%2520up%250Ato%252040%2525%2520of%2520tokens%2520can%2520be%2520confidently%2520predicted%2520and%2520halted%2520before%2520reaching%2520the%250Afinal%2520encoder%2520layer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Do%20Tokens%20Go%3F%20Understanding%20Pruning%20Behaviors%20in%20STEP%20at%20High%0A%20%20Resolutions&entry.906535625=Michal%20Szczepanski%20and%20Martyna%20Poreba%20and%20Karim%20Haroun&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20achieve%20state-of-the-art%20performance%20in%20semantic%0Asegmentation%20but%20are%20hindered%20by%20high%20computational%20and%20memory%20costs.%20To%0Aaddress%20this%2C%20we%20propose%20STEP%20%28SuperToken%20and%20Early-Pruning%29%2C%20a%20hybrid%0Atoken-reduction%20framework%20that%20combines%20dynamic%20patch%20merging%20and%20token%20pruning%0Ato%20enhance%20efficiency%20without%20significantly%20compromising%20accuracy.%20At%20the%20core%0Aof%20STEP%20is%20dCTS%2C%20a%20lightweight%20CNN-based%20policy%20network%20that%20enables%20flexible%0Amerging%20into%20superpatches.%20Encoder%20blocks%20integrate%20also%20early-exits%20to%20remove%0Ahigh-confident%20supertokens%2C%20lowering%20computational%20load.%20We%20evaluate%20our%20method%0Aon%20high-resolution%20semantic%20segmentation%20benchmarks%2C%20including%20images%20up%20to%0A1024%20x%201024%2C%20and%20show%20that%20when%20dCTS%20is%20applied%20alone%2C%20the%20token%20count%20can%20be%0Areduced%20by%20a%20factor%20of%202.5%20compared%20to%20the%20standard%2016%20x%2016%20pixel%20patching%0Ascheme.%20This%20yields%20a%202.6x%20reduction%20in%20computational%20cost%20and%20a%203.4x%20increase%0Ain%20throughput%20when%20using%20ViT-Large%20as%20the%20backbone.%20Applying%20the%20full%20STEP%0Aframework%20further%20improves%20efficiency%2C%20reaching%20up%20to%20a%204x%20reduction%20in%0Acomputational%20complexity%20and%20a%201.7x%20gain%20in%20inference%20speed%2C%20with%20a%20maximum%0Aaccuracy%20drop%20of%20no%20more%20than%202.0%25.%20With%20the%20proposed%20STEP%20configurations%2C%20up%0Ato%2040%25%20of%20tokens%20can%20be%20confidently%20predicted%20and%20halted%20before%20reaching%20the%0Afinal%20encoder%20layer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14165v1&entry.124074799=Read"},
{"title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation\n  Models at Scale", "author": "Hasan Abed Al Kader Hammoud and Mohammad Zbeeb and Bernard Ghanem", "abstract": "  We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.\n", "link": "http://arxiv.org/abs/2509.14008v1", "date": "2025-09-17", "relevancy": 2.2776, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hala%20Technical%20Report%3A%20Building%20Arabic-Centric%20Instruction%20%26%20Translation%0A%20%20Models%20at%20Scale&body=Title%3A%20Hala%20Technical%20Report%3A%20Building%20Arabic-Centric%20Instruction%20%26%20Translation%0A%20%20Models%20at%20Scale%0AAuthor%3A%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Mohammad%20Zbeeb%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20We%20present%20Hala%2C%20a%20family%20of%20Arabic-centric%20instruction%20and%20translation%0Amodels%20built%20with%20our%20translate-and-tune%20pipeline.%20We%20first%20compress%20a%20strong%0AAR%24%5Cleftrightarrow%24EN%20teacher%20to%20FP8%20%28yielding%20%24%5Csim%242%24%5Ctimes%24%20higher%0Athroughput%20with%20no%20quality%20loss%29%20and%20use%20it%20to%20create%20high-fidelity%20bilingual%0Asupervision.%20A%20lightweight%20language%20model%20LFM2-1.2B%20is%20then%20fine-tuned%20on%20this%0Adata%20and%20used%20to%20translate%20high-quality%20English%20instruction%20sets%20into%20Arabic%2C%0Aproducing%20a%20million-scale%20corpus%20tailored%20to%20instruction%20following.%20We%20train%0AHala%20models%20at%20350M%2C%20700M%2C%201.2B%2C%20and%209B%20parameters%2C%20and%20apply%20slerp%20merging%20to%0Abalance%20Arabic%20specialization%20with%20base-model%20strengths.%20On%20Arabic-centric%0Abenchmarks%2C%20Hala%20achieves%20state-of-the-art%20results%20within%20both%20the%20%22nano%22%0A%28%24%5Cleq%242B%29%20and%20%22small%22%20%287-9B%29%20categories%2C%20outperforming%20their%20bases.%20We%20release%0Amodels%2C%20data%2C%20evaluation%2C%20and%20recipes%20to%20accelerate%20research%20in%20Arabic%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHala%2520Technical%2520Report%253A%2520Building%2520Arabic-Centric%2520Instruction%2520%2526%2520Translation%250A%2520%2520Models%2520at%2520Scale%26entry.906535625%3DHasan%2520Abed%2520Al%2520Kader%2520Hammoud%2520and%2520Mohammad%2520Zbeeb%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520We%2520present%2520Hala%252C%2520a%2520family%2520of%2520Arabic-centric%2520instruction%2520and%2520translation%250Amodels%2520built%2520with%2520our%2520translate-and-tune%2520pipeline.%2520We%2520first%2520compress%2520a%2520strong%250AAR%2524%255Cleftrightarrow%2524EN%2520teacher%2520to%2520FP8%2520%2528yielding%2520%2524%255Csim%25242%2524%255Ctimes%2524%2520higher%250Athroughput%2520with%2520no%2520quality%2520loss%2529%2520and%2520use%2520it%2520to%2520create%2520high-fidelity%2520bilingual%250Asupervision.%2520A%2520lightweight%2520language%2520model%2520LFM2-1.2B%2520is%2520then%2520fine-tuned%2520on%2520this%250Adata%2520and%2520used%2520to%2520translate%2520high-quality%2520English%2520instruction%2520sets%2520into%2520Arabic%252C%250Aproducing%2520a%2520million-scale%2520corpus%2520tailored%2520to%2520instruction%2520following.%2520We%2520train%250AHala%2520models%2520at%2520350M%252C%2520700M%252C%25201.2B%252C%2520and%25209B%2520parameters%252C%2520and%2520apply%2520slerp%2520merging%2520to%250Abalance%2520Arabic%2520specialization%2520with%2520base-model%2520strengths.%2520On%2520Arabic-centric%250Abenchmarks%252C%2520Hala%2520achieves%2520state-of-the-art%2520results%2520within%2520both%2520the%2520%2522nano%2522%250A%2528%2524%255Cleq%25242B%2529%2520and%2520%2522small%2522%2520%25287-9B%2529%2520categories%252C%2520outperforming%2520their%2520bases.%2520We%2520release%250Amodels%252C%2520data%252C%2520evaluation%252C%2520and%2520recipes%2520to%2520accelerate%2520research%2520in%2520Arabic%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hala%20Technical%20Report%3A%20Building%20Arabic-Centric%20Instruction%20%26%20Translation%0A%20%20Models%20at%20Scale&entry.906535625=Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Mohammad%20Zbeeb%20and%20Bernard%20Ghanem&entry.1292438233=%20%20We%20present%20Hala%2C%20a%20family%20of%20Arabic-centric%20instruction%20and%20translation%0Amodels%20built%20with%20our%20translate-and-tune%20pipeline.%20We%20first%20compress%20a%20strong%0AAR%24%5Cleftrightarrow%24EN%20teacher%20to%20FP8%20%28yielding%20%24%5Csim%242%24%5Ctimes%24%20higher%0Athroughput%20with%20no%20quality%20loss%29%20and%20use%20it%20to%20create%20high-fidelity%20bilingual%0Asupervision.%20A%20lightweight%20language%20model%20LFM2-1.2B%20is%20then%20fine-tuned%20on%20this%0Adata%20and%20used%20to%20translate%20high-quality%20English%20instruction%20sets%20into%20Arabic%2C%0Aproducing%20a%20million-scale%20corpus%20tailored%20to%20instruction%20following.%20We%20train%0AHala%20models%20at%20350M%2C%20700M%2C%201.2B%2C%20and%209B%20parameters%2C%20and%20apply%20slerp%20merging%20to%0Abalance%20Arabic%20specialization%20with%20base-model%20strengths.%20On%20Arabic-centric%0Abenchmarks%2C%20Hala%20achieves%20state-of-the-art%20results%20within%20both%20the%20%22nano%22%0A%28%24%5Cleq%242B%29%20and%20%22small%22%20%287-9B%29%20categories%2C%20outperforming%20their%20bases.%20We%20release%0Amodels%2C%20data%2C%20evaluation%2C%20and%20recipes%20to%20accelerate%20research%20in%20Arabic%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14008v1&entry.124074799=Read"},
{"title": "Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound", "author": "Junwon Lee and Jaekwon Im and Dabin Kim and Juhan Nam", "abstract": "  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor alignment and controllability, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an\nintuitive condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope closely related to audio semantics, acts as a\ntemporal event feature to guide audio generation from video. The\nannotation-free self-supervised learning framework consists of two stages,\nVideo2RMS and RMS2Sound, incorporating novel ideas including RMS discretization\nand RMS-ControlNet with a pretrained text-to-audio model. Our extensive\nevaluation shows that Video-Foley achieves state-of-the-art performance in\naudio-visual alignment and controllability for sound timing, intensity, timbre,\nand nuance. Source code, model weights and demos are available on our companion\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n", "link": "http://arxiv.org/abs/2408.11915v3", "date": "2025-09-17", "relevancy": 2.229, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.579}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5533}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Foley%3A%20Two-Stage%20Video-To-Sound%20Generation%20via%20Temporal%20Event%0A%20%20Condition%20For%20Foley%20Sound&body=Title%3A%20Video-Foley%3A%20Two-Stage%20Video-To-Sound%20Generation%20via%20Temporal%20Event%0A%20%20Condition%20For%20Foley%20Sound%0AAuthor%3A%20Junwon%20Lee%20and%20Jaekwon%20Im%20and%20Dabin%20Kim%20and%20Juhan%20Nam%0AAbstract%3A%20%20%20Foley%20sound%20synthesis%20is%20crucial%20for%20multimedia%20production%2C%20enhancing%20user%0Aexperience%20by%20synchronizing%20audio%20and%20video%20both%20temporally%20and%20semantically.%0ARecent%20studies%20on%20automating%20this%20labor-intensive%20process%20through%0Avideo-to-sound%20generation%20face%20significant%20challenges.%20Systems%20lacking%20explicit%0Atemporal%20features%20suffer%20from%20poor%20alignment%20and%20controllability%2C%20while%0Atimestamp-based%20models%20require%20costly%20and%20subjective%20human%20annotation.%20We%0Apropose%20Video-Foley%2C%20a%20video-to-sound%20system%20using%20Root%20Mean%20Square%20%28RMS%29%20as%20an%0Aintuitive%20condition%20with%20semantic%20timbre%20prompts%20%28audio%20or%20text%29.%20RMS%2C%20a%0Aframe-level%20intensity%20envelope%20closely%20related%20to%20audio%20semantics%2C%20acts%20as%20a%0Atemporal%20event%20feature%20to%20guide%20audio%20generation%20from%20video.%20The%0Aannotation-free%20self-supervised%20learning%20framework%20consists%20of%20two%20stages%2C%0AVideo2RMS%20and%20RMS2Sound%2C%20incorporating%20novel%20ideas%20including%20RMS%20discretization%0Aand%20RMS-ControlNet%20with%20a%20pretrained%20text-to-audio%20model.%20Our%20extensive%0Aevaluation%20shows%20that%20Video-Foley%20achieves%20state-of-the-art%20performance%20in%0Aaudio-visual%20alignment%20and%20controllability%20for%20sound%20timing%2C%20intensity%2C%20timbre%2C%0Aand%20nuance.%20Source%20code%2C%20model%20weights%20and%20demos%20are%20available%20on%20our%20companion%0Awebsite.%20%28https%3A//jnwnlee.github.io/video-foley-demo%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Foley%253A%2520Two-Stage%2520Video-To-Sound%2520Generation%2520via%2520Temporal%2520Event%250A%2520%2520Condition%2520For%2520Foley%2520Sound%26entry.906535625%3DJunwon%2520Lee%2520and%2520Jaekwon%2520Im%2520and%2520Dabin%2520Kim%2520and%2520Juhan%2520Nam%26entry.1292438233%3D%2520%2520Foley%2520sound%2520synthesis%2520is%2520crucial%2520for%2520multimedia%2520production%252C%2520enhancing%2520user%250Aexperience%2520by%2520synchronizing%2520audio%2520and%2520video%2520both%2520temporally%2520and%2520semantically.%250ARecent%2520studies%2520on%2520automating%2520this%2520labor-intensive%2520process%2520through%250Avideo-to-sound%2520generation%2520face%2520significant%2520challenges.%2520Systems%2520lacking%2520explicit%250Atemporal%2520features%2520suffer%2520from%2520poor%2520alignment%2520and%2520controllability%252C%2520while%250Atimestamp-based%2520models%2520require%2520costly%2520and%2520subjective%2520human%2520annotation.%2520We%250Apropose%2520Video-Foley%252C%2520a%2520video-to-sound%2520system%2520using%2520Root%2520Mean%2520Square%2520%2528RMS%2529%2520as%2520an%250Aintuitive%2520condition%2520with%2520semantic%2520timbre%2520prompts%2520%2528audio%2520or%2520text%2529.%2520RMS%252C%2520a%250Aframe-level%2520intensity%2520envelope%2520closely%2520related%2520to%2520audio%2520semantics%252C%2520acts%2520as%2520a%250Atemporal%2520event%2520feature%2520to%2520guide%2520audio%2520generation%2520from%2520video.%2520The%250Aannotation-free%2520self-supervised%2520learning%2520framework%2520consists%2520of%2520two%2520stages%252C%250AVideo2RMS%2520and%2520RMS2Sound%252C%2520incorporating%2520novel%2520ideas%2520including%2520RMS%2520discretization%250Aand%2520RMS-ControlNet%2520with%2520a%2520pretrained%2520text-to-audio%2520model.%2520Our%2520extensive%250Aevaluation%2520shows%2520that%2520Video-Foley%2520achieves%2520state-of-the-art%2520performance%2520in%250Aaudio-visual%2520alignment%2520and%2520controllability%2520for%2520sound%2520timing%252C%2520intensity%252C%2520timbre%252C%250Aand%2520nuance.%2520Source%2520code%252C%2520model%2520weights%2520and%2520demos%2520are%2520available%2520on%2520our%2520companion%250Awebsite.%2520%2528https%253A//jnwnlee.github.io/video-foley-demo%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Foley%3A%20Two-Stage%20Video-To-Sound%20Generation%20via%20Temporal%20Event%0A%20%20Condition%20For%20Foley%20Sound&entry.906535625=Junwon%20Lee%20and%20Jaekwon%20Im%20and%20Dabin%20Kim%20and%20Juhan%20Nam&entry.1292438233=%20%20Foley%20sound%20synthesis%20is%20crucial%20for%20multimedia%20production%2C%20enhancing%20user%0Aexperience%20by%20synchronizing%20audio%20and%20video%20both%20temporally%20and%20semantically.%0ARecent%20studies%20on%20automating%20this%20labor-intensive%20process%20through%0Avideo-to-sound%20generation%20face%20significant%20challenges.%20Systems%20lacking%20explicit%0Atemporal%20features%20suffer%20from%20poor%20alignment%20and%20controllability%2C%20while%0Atimestamp-based%20models%20require%20costly%20and%20subjective%20human%20annotation.%20We%0Apropose%20Video-Foley%2C%20a%20video-to-sound%20system%20using%20Root%20Mean%20Square%20%28RMS%29%20as%20an%0Aintuitive%20condition%20with%20semantic%20timbre%20prompts%20%28audio%20or%20text%29.%20RMS%2C%20a%0Aframe-level%20intensity%20envelope%20closely%20related%20to%20audio%20semantics%2C%20acts%20as%20a%0Atemporal%20event%20feature%20to%20guide%20audio%20generation%20from%20video.%20The%0Aannotation-free%20self-supervised%20learning%20framework%20consists%20of%20two%20stages%2C%0AVideo2RMS%20and%20RMS2Sound%2C%20incorporating%20novel%20ideas%20including%20RMS%20discretization%0Aand%20RMS-ControlNet%20with%20a%20pretrained%20text-to-audio%20model.%20Our%20extensive%0Aevaluation%20shows%20that%20Video-Foley%20achieves%20state-of-the-art%20performance%20in%0Aaudio-visual%20alignment%20and%20controllability%20for%20sound%20timing%2C%20intensity%2C%20timbre%2C%0Aand%20nuance.%20Source%20code%2C%20model%20weights%20and%20demos%20are%20available%20on%20our%20companion%0Awebsite.%20%28https%3A//jnwnlee.github.io/video-foley-demo%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11915v3&entry.124074799=Read"},
{"title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with\n  Decentralized Diffusion Policies", "author": "Dayi Dong and Maulik Bhatt and Seoyeon Choi and Negar Mehr", "abstract": "  As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2509.14159v1", "date": "2025-09-17", "relevancy": 2.2222, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.562}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIMIC-D%3A%20Multi-modal%20Imitation%20for%20MultI-agent%20Coordination%20with%0A%20%20Decentralized%20Diffusion%20Policies&body=Title%3A%20MIMIC-D%3A%20Multi-modal%20Imitation%20for%20MultI-agent%20Coordination%20with%0A%20%20Decentralized%20Diffusion%20Policies%0AAuthor%3A%20Dayi%20Dong%20and%20Maulik%20Bhatt%20and%20Seoyeon%20Choi%20and%20Negar%20Mehr%0AAbstract%3A%20%20%20As%20robots%20become%20more%20integrated%20in%20society%2C%20their%20ability%20to%20coordinate%20with%0Aother%20robots%20and%20humans%20on%20multi-modal%20tasks%20%28those%20with%20multiple%20valid%0Asolutions%29%20is%20crucial.%20We%20propose%20to%20learn%20such%20behaviors%20from%20expert%0Ademonstrations%20via%20imitation%20learning%20%28IL%29.%20However%2C%20when%20expert%20demonstrations%0Aare%20multi-modal%2C%20standard%20IL%20approaches%20can%20struggle%20to%20capture%20the%20diverse%0Astrategies%2C%20hindering%20effective%20coordination.%20Diffusion%20models%20are%20known%20to%20be%0Aeffective%20at%20handling%20complex%20multi-modal%20trajectory%20distributions%20in%0Asingle-agent%20systems.%20Diffusion%20models%20have%20also%20excelled%20in%20multi-agent%0Ascenarios%20where%20multi-modality%20is%20more%20common%20and%20crucial%20to%20learning%0Acoordinated%20behaviors.%20Typically%2C%20diffusion-based%20approaches%20require%20a%0Acentralized%20planner%20or%20explicit%20communication%20among%20agents%2C%20but%20this%20assumption%0Acan%20fail%20in%20real-world%20scenarios%20where%20robots%20must%20operate%20independently%20or%0Awith%20agents%20like%20humans%20that%20they%20cannot%20directly%20communicate%20with.%20Therefore%2C%0Awe%20propose%20MIMIC-D%2C%20a%20Centralized%20Training%2C%20Decentralized%20Execution%20%28CTDE%29%0Aparadigm%20for%20multi-modal%20multi-agent%20imitation%20learning%20using%20diffusion%0Apolicies.%20Agents%20are%20trained%20jointly%20with%20full%20information%2C%20but%20execute%0Apolicies%20using%20only%20local%20information%20to%20achieve%20implicit%20coordination.%20We%0Ademonstrate%20in%20both%20simulation%20and%20hardware%20experiments%20that%20our%20method%0Arecovers%20multi-modal%20coordination%20behavior%20among%20agents%20in%20a%20variety%20of%20tasks%0Aand%20environments%2C%20while%20improving%20upon%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIMIC-D%253A%2520Multi-modal%2520Imitation%2520for%2520MultI-agent%2520Coordination%2520with%250A%2520%2520Decentralized%2520Diffusion%2520Policies%26entry.906535625%3DDayi%2520Dong%2520and%2520Maulik%2520Bhatt%2520and%2520Seoyeon%2520Choi%2520and%2520Negar%2520Mehr%26entry.1292438233%3D%2520%2520As%2520robots%2520become%2520more%2520integrated%2520in%2520society%252C%2520their%2520ability%2520to%2520coordinate%2520with%250Aother%2520robots%2520and%2520humans%2520on%2520multi-modal%2520tasks%2520%2528those%2520with%2520multiple%2520valid%250Asolutions%2529%2520is%2520crucial.%2520We%2520propose%2520to%2520learn%2520such%2520behaviors%2520from%2520expert%250Ademonstrations%2520via%2520imitation%2520learning%2520%2528IL%2529.%2520However%252C%2520when%2520expert%2520demonstrations%250Aare%2520multi-modal%252C%2520standard%2520IL%2520approaches%2520can%2520struggle%2520to%2520capture%2520the%2520diverse%250Astrategies%252C%2520hindering%2520effective%2520coordination.%2520Diffusion%2520models%2520are%2520known%2520to%2520be%250Aeffective%2520at%2520handling%2520complex%2520multi-modal%2520trajectory%2520distributions%2520in%250Asingle-agent%2520systems.%2520Diffusion%2520models%2520have%2520also%2520excelled%2520in%2520multi-agent%250Ascenarios%2520where%2520multi-modality%2520is%2520more%2520common%2520and%2520crucial%2520to%2520learning%250Acoordinated%2520behaviors.%2520Typically%252C%2520diffusion-based%2520approaches%2520require%2520a%250Acentralized%2520planner%2520or%2520explicit%2520communication%2520among%2520agents%252C%2520but%2520this%2520assumption%250Acan%2520fail%2520in%2520real-world%2520scenarios%2520where%2520robots%2520must%2520operate%2520independently%2520or%250Awith%2520agents%2520like%2520humans%2520that%2520they%2520cannot%2520directly%2520communicate%2520with.%2520Therefore%252C%250Awe%2520propose%2520MIMIC-D%252C%2520a%2520Centralized%2520Training%252C%2520Decentralized%2520Execution%2520%2528CTDE%2529%250Aparadigm%2520for%2520multi-modal%2520multi-agent%2520imitation%2520learning%2520using%2520diffusion%250Apolicies.%2520Agents%2520are%2520trained%2520jointly%2520with%2520full%2520information%252C%2520but%2520execute%250Apolicies%2520using%2520only%2520local%2520information%2520to%2520achieve%2520implicit%2520coordination.%2520We%250Ademonstrate%2520in%2520both%2520simulation%2520and%2520hardware%2520experiments%2520that%2520our%2520method%250Arecovers%2520multi-modal%2520coordination%2520behavior%2520among%2520agents%2520in%2520a%2520variety%2520of%2520tasks%250Aand%2520environments%252C%2520while%2520improving%2520upon%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIMIC-D%3A%20Multi-modal%20Imitation%20for%20MultI-agent%20Coordination%20with%0A%20%20Decentralized%20Diffusion%20Policies&entry.906535625=Dayi%20Dong%20and%20Maulik%20Bhatt%20and%20Seoyeon%20Choi%20and%20Negar%20Mehr&entry.1292438233=%20%20As%20robots%20become%20more%20integrated%20in%20society%2C%20their%20ability%20to%20coordinate%20with%0Aother%20robots%20and%20humans%20on%20multi-modal%20tasks%20%28those%20with%20multiple%20valid%0Asolutions%29%20is%20crucial.%20We%20propose%20to%20learn%20such%20behaviors%20from%20expert%0Ademonstrations%20via%20imitation%20learning%20%28IL%29.%20However%2C%20when%20expert%20demonstrations%0Aare%20multi-modal%2C%20standard%20IL%20approaches%20can%20struggle%20to%20capture%20the%20diverse%0Astrategies%2C%20hindering%20effective%20coordination.%20Diffusion%20models%20are%20known%20to%20be%0Aeffective%20at%20handling%20complex%20multi-modal%20trajectory%20distributions%20in%0Asingle-agent%20systems.%20Diffusion%20models%20have%20also%20excelled%20in%20multi-agent%0Ascenarios%20where%20multi-modality%20is%20more%20common%20and%20crucial%20to%20learning%0Acoordinated%20behaviors.%20Typically%2C%20diffusion-based%20approaches%20require%20a%0Acentralized%20planner%20or%20explicit%20communication%20among%20agents%2C%20but%20this%20assumption%0Acan%20fail%20in%20real-world%20scenarios%20where%20robots%20must%20operate%20independently%20or%0Awith%20agents%20like%20humans%20that%20they%20cannot%20directly%20communicate%20with.%20Therefore%2C%0Awe%20propose%20MIMIC-D%2C%20a%20Centralized%20Training%2C%20Decentralized%20Execution%20%28CTDE%29%0Aparadigm%20for%20multi-modal%20multi-agent%20imitation%20learning%20using%20diffusion%0Apolicies.%20Agents%20are%20trained%20jointly%20with%20full%20information%2C%20but%20execute%0Apolicies%20using%20only%20local%20information%20to%20achieve%20implicit%20coordination.%20We%0Ademonstrate%20in%20both%20simulation%20and%20hardware%20experiments%20that%20our%20method%0Arecovers%20multi-modal%20coordination%20behavior%20among%20agents%20in%20a%20variety%20of%20tasks%0Aand%20environments%2C%20while%20improving%20upon%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14159v1&entry.124074799=Read"},
{"title": "PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease\n  Similarity Prediction", "author": "Ranga Baminiwatte and Kazi Jewel Rana and Aaron J. Masino", "abstract": "  Understanding disease similarity is critical for advancing diagnostics, drug\ndiscovery, and personalized treatment strategies. We present PhenoGnet, a novel\ngraph-based contrastive learning framework designed to predict disease\nsimilarity by integrating gene functional interaction networks with the Human\nPhenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view\nmodel that separately encodes gene and phenotype graphs using Graph\nConvolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross\nview model implemented as a shared weight multilayer perceptron (MLP) that\naligns gene and phenotype embeddings through contrastive learning. The model is\ntrained using known gene phenotype associations as positive pairs and randomly\nsampled unrelated pairs as negatives. Diseases are represented by the mean\nembeddings of their associated genes and/or phenotypes, and pairwise similarity\nis computed via cosine similarity. Evaluation on a curated benchmark of 1,100\nsimilar and 866 dissimilar disease pairs demonstrates strong performance, with\ngene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,\noutperforming existing state of the art methods. Notably, PhenoGnet captures\nlatent biological relationships beyond direct overlap, offering a scalable and\ninterpretable solution for disease similarity prediction. These results\nunderscore its potential for enabling downstream applications in rare disease\nresearch and precision medicine.\n", "link": "http://arxiv.org/abs/2509.14037v1", "date": "2025-09-17", "relevancy": 2.2185, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4492}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.444}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhenoGnet%3A%20A%20Graph-Based%20Contrastive%20Learning%20Framework%20for%20Disease%0A%20%20Similarity%20Prediction&body=Title%3A%20PhenoGnet%3A%20A%20Graph-Based%20Contrastive%20Learning%20Framework%20for%20Disease%0A%20%20Similarity%20Prediction%0AAuthor%3A%20Ranga%20Baminiwatte%20and%20Kazi%20Jewel%20Rana%20and%20Aaron%20J.%20Masino%0AAbstract%3A%20%20%20Understanding%20disease%20similarity%20is%20critical%20for%20advancing%20diagnostics%2C%20drug%0Adiscovery%2C%20and%20personalized%20treatment%20strategies.%20We%20present%20PhenoGnet%2C%20a%20novel%0Agraph-based%20contrastive%20learning%20framework%20designed%20to%20predict%20disease%0Asimilarity%20by%20integrating%20gene%20functional%20interaction%20networks%20with%20the%20Human%0APhenotype%20Ontology%20%28HPO%29.%20PhenoGnet%20comprises%20two%20key%20components%3A%20an%20intra-view%0Amodel%20that%20separately%20encodes%20gene%20and%20phenotype%20graphs%20using%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Graph%20Attention%20Networks%20%28GATs%29%2C%20and%20a%20cross%0Aview%20model%20implemented%20as%20a%20shared%20weight%20multilayer%20perceptron%20%28MLP%29%20that%0Aaligns%20gene%20and%20phenotype%20embeddings%20through%20contrastive%20learning.%20The%20model%20is%0Atrained%20using%20known%20gene%20phenotype%20associations%20as%20positive%20pairs%20and%20randomly%0Asampled%20unrelated%20pairs%20as%20negatives.%20Diseases%20are%20represented%20by%20the%20mean%0Aembeddings%20of%20their%20associated%20genes%20and/or%20phenotypes%2C%20and%20pairwise%20similarity%0Ais%20computed%20via%20cosine%20similarity.%20Evaluation%20on%20a%20curated%20benchmark%20of%201%2C100%0Asimilar%20and%20866%20dissimilar%20disease%20pairs%20demonstrates%20strong%20performance%2C%20with%0Agene%20based%20embeddings%20achieving%20an%20AUCPR%20of%200.9012%20and%20AUROC%20of%200.8764%2C%0Aoutperforming%20existing%20state%20of%20the%20art%20methods.%20Notably%2C%20PhenoGnet%20captures%0Alatent%20biological%20relationships%20beyond%20direct%20overlap%2C%20offering%20a%20scalable%20and%0Ainterpretable%20solution%20for%20disease%20similarity%20prediction.%20These%20results%0Aunderscore%20its%20potential%20for%20enabling%20downstream%20applications%20in%20rare%20disease%0Aresearch%20and%20precision%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhenoGnet%253A%2520A%2520Graph-Based%2520Contrastive%2520Learning%2520Framework%2520for%2520Disease%250A%2520%2520Similarity%2520Prediction%26entry.906535625%3DRanga%2520Baminiwatte%2520and%2520Kazi%2520Jewel%2520Rana%2520and%2520Aaron%2520J.%2520Masino%26entry.1292438233%3D%2520%2520Understanding%2520disease%2520similarity%2520is%2520critical%2520for%2520advancing%2520diagnostics%252C%2520drug%250Adiscovery%252C%2520and%2520personalized%2520treatment%2520strategies.%2520We%2520present%2520PhenoGnet%252C%2520a%2520novel%250Agraph-based%2520contrastive%2520learning%2520framework%2520designed%2520to%2520predict%2520disease%250Asimilarity%2520by%2520integrating%2520gene%2520functional%2520interaction%2520networks%2520with%2520the%2520Human%250APhenotype%2520Ontology%2520%2528HPO%2529.%2520PhenoGnet%2520comprises%2520two%2520key%2520components%253A%2520an%2520intra-view%250Amodel%2520that%2520separately%2520encodes%2520gene%2520and%2520phenotype%2520graphs%2520using%2520Graph%250AConvolutional%2520Networks%2520%2528GCNs%2529%2520and%2520Graph%2520Attention%2520Networks%2520%2528GATs%2529%252C%2520and%2520a%2520cross%250Aview%2520model%2520implemented%2520as%2520a%2520shared%2520weight%2520multilayer%2520perceptron%2520%2528MLP%2529%2520that%250Aaligns%2520gene%2520and%2520phenotype%2520embeddings%2520through%2520contrastive%2520learning.%2520The%2520model%2520is%250Atrained%2520using%2520known%2520gene%2520phenotype%2520associations%2520as%2520positive%2520pairs%2520and%2520randomly%250Asampled%2520unrelated%2520pairs%2520as%2520negatives.%2520Diseases%2520are%2520represented%2520by%2520the%2520mean%250Aembeddings%2520of%2520their%2520associated%2520genes%2520and/or%2520phenotypes%252C%2520and%2520pairwise%2520similarity%250Ais%2520computed%2520via%2520cosine%2520similarity.%2520Evaluation%2520on%2520a%2520curated%2520benchmark%2520of%25201%252C100%250Asimilar%2520and%2520866%2520dissimilar%2520disease%2520pairs%2520demonstrates%2520strong%2520performance%252C%2520with%250Agene%2520based%2520embeddings%2520achieving%2520an%2520AUCPR%2520of%25200.9012%2520and%2520AUROC%2520of%25200.8764%252C%250Aoutperforming%2520existing%2520state%2520of%2520the%2520art%2520methods.%2520Notably%252C%2520PhenoGnet%2520captures%250Alatent%2520biological%2520relationships%2520beyond%2520direct%2520overlap%252C%2520offering%2520a%2520scalable%2520and%250Ainterpretable%2520solution%2520for%2520disease%2520similarity%2520prediction.%2520These%2520results%250Aunderscore%2520its%2520potential%2520for%2520enabling%2520downstream%2520applications%2520in%2520rare%2520disease%250Aresearch%2520and%2520precision%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhenoGnet%3A%20A%20Graph-Based%20Contrastive%20Learning%20Framework%20for%20Disease%0A%20%20Similarity%20Prediction&entry.906535625=Ranga%20Baminiwatte%20and%20Kazi%20Jewel%20Rana%20and%20Aaron%20J.%20Masino&entry.1292438233=%20%20Understanding%20disease%20similarity%20is%20critical%20for%20advancing%20diagnostics%2C%20drug%0Adiscovery%2C%20and%20personalized%20treatment%20strategies.%20We%20present%20PhenoGnet%2C%20a%20novel%0Agraph-based%20contrastive%20learning%20framework%20designed%20to%20predict%20disease%0Asimilarity%20by%20integrating%20gene%20functional%20interaction%20networks%20with%20the%20Human%0APhenotype%20Ontology%20%28HPO%29.%20PhenoGnet%20comprises%20two%20key%20components%3A%20an%20intra-view%0Amodel%20that%20separately%20encodes%20gene%20and%20phenotype%20graphs%20using%20Graph%0AConvolutional%20Networks%20%28GCNs%29%20and%20Graph%20Attention%20Networks%20%28GATs%29%2C%20and%20a%20cross%0Aview%20model%20implemented%20as%20a%20shared%20weight%20multilayer%20perceptron%20%28MLP%29%20that%0Aaligns%20gene%20and%20phenotype%20embeddings%20through%20contrastive%20learning.%20The%20model%20is%0Atrained%20using%20known%20gene%20phenotype%20associations%20as%20positive%20pairs%20and%20randomly%0Asampled%20unrelated%20pairs%20as%20negatives.%20Diseases%20are%20represented%20by%20the%20mean%0Aembeddings%20of%20their%20associated%20genes%20and/or%20phenotypes%2C%20and%20pairwise%20similarity%0Ais%20computed%20via%20cosine%20similarity.%20Evaluation%20on%20a%20curated%20benchmark%20of%201%2C100%0Asimilar%20and%20866%20dissimilar%20disease%20pairs%20demonstrates%20strong%20performance%2C%20with%0Agene%20based%20embeddings%20achieving%20an%20AUCPR%20of%200.9012%20and%20AUROC%20of%200.8764%2C%0Aoutperforming%20existing%20state%20of%20the%20art%20methods.%20Notably%2C%20PhenoGnet%20captures%0Alatent%20biological%20relationships%20beyond%20direct%20overlap%2C%20offering%20a%20scalable%20and%0Ainterpretable%20solution%20for%20disease%20similarity%20prediction.%20These%20results%0Aunderscore%20its%20potential%20for%20enabling%20downstream%20applications%20in%20rare%20disease%0Aresearch%20and%20precision%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14037v1&entry.124074799=Read"},
{"title": "On the Rate of Gaussian Approximation for Linear Regression Problems", "author": "Marat Khusainov and Marina Sheshukova and Alain Durmus and Sergey Samsonov", "abstract": "  In this paper, we consider the problem of Gaussian approximation for the\nonline linear regression task. We derive the corresponding rates for the\nsetting of a constant learning rate and study the explicit dependence of the\nconvergence rate upon the problem dimension $d$ and quantities related to the\ndesign matrix. When the number of iterations $n$ is known in advance, our\nresults yield the rate of normal approximation of order $\\sqrt{\\log{n}/n}$,\nprovided that the sample size $n$ is large enough.\n", "link": "http://arxiv.org/abs/2509.14039v1", "date": "2025-09-17", "relevancy": 2.1917, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.45}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4463}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Rate%20of%20Gaussian%20Approximation%20for%20Linear%20Regression%20Problems&body=Title%3A%20On%20the%20Rate%20of%20Gaussian%20Approximation%20for%20Linear%20Regression%20Problems%0AAuthor%3A%20Marat%20Khusainov%20and%20Marina%20Sheshukova%20and%20Alain%20Durmus%20and%20Sergey%20Samsonov%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20Gaussian%20approximation%20for%20the%0Aonline%20linear%20regression%20task.%20We%20derive%20the%20corresponding%20rates%20for%20the%0Asetting%20of%20a%20constant%20learning%20rate%20and%20study%20the%20explicit%20dependence%20of%20the%0Aconvergence%20rate%20upon%20the%20problem%20dimension%20%24d%24%20and%20quantities%20related%20to%20the%0Adesign%20matrix.%20When%20the%20number%20of%20iterations%20%24n%24%20is%20known%20in%20advance%2C%20our%0Aresults%20yield%20the%20rate%20of%20normal%20approximation%20of%20order%20%24%5Csqrt%7B%5Clog%7Bn%7D/n%7D%24%2C%0Aprovided%20that%20the%20sample%20size%20%24n%24%20is%20large%20enough.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Rate%2520of%2520Gaussian%2520Approximation%2520for%2520Linear%2520Regression%2520Problems%26entry.906535625%3DMarat%2520Khusainov%2520and%2520Marina%2520Sheshukova%2520and%2520Alain%2520Durmus%2520and%2520Sergey%2520Samsonov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520of%2520Gaussian%2520approximation%2520for%2520the%250Aonline%2520linear%2520regression%2520task.%2520We%2520derive%2520the%2520corresponding%2520rates%2520for%2520the%250Asetting%2520of%2520a%2520constant%2520learning%2520rate%2520and%2520study%2520the%2520explicit%2520dependence%2520of%2520the%250Aconvergence%2520rate%2520upon%2520the%2520problem%2520dimension%2520%2524d%2524%2520and%2520quantities%2520related%2520to%2520the%250Adesign%2520matrix.%2520When%2520the%2520number%2520of%2520iterations%2520%2524n%2524%2520is%2520known%2520in%2520advance%252C%2520our%250Aresults%2520yield%2520the%2520rate%2520of%2520normal%2520approximation%2520of%2520order%2520%2524%255Csqrt%257B%255Clog%257Bn%257D/n%257D%2524%252C%250Aprovided%2520that%2520the%2520sample%2520size%2520%2524n%2524%2520is%2520large%2520enough.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Rate%20of%20Gaussian%20Approximation%20for%20Linear%20Regression%20Problems&entry.906535625=Marat%20Khusainov%20and%20Marina%20Sheshukova%20and%20Alain%20Durmus%20and%20Sergey%20Samsonov&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20the%20problem%20of%20Gaussian%20approximation%20for%20the%0Aonline%20linear%20regression%20task.%20We%20derive%20the%20corresponding%20rates%20for%20the%0Asetting%20of%20a%20constant%20learning%20rate%20and%20study%20the%20explicit%20dependence%20of%20the%0Aconvergence%20rate%20upon%20the%20problem%20dimension%20%24d%24%20and%20quantities%20related%20to%20the%0Adesign%20matrix.%20When%20the%20number%20of%20iterations%20%24n%24%20is%20known%20in%20advance%2C%20our%0Aresults%20yield%20the%20rate%20of%20normal%20approximation%20of%20order%20%24%5Csqrt%7B%5Clog%7Bn%7D/n%7D%24%2C%0Aprovided%20that%20the%20sample%20size%20%24n%24%20is%20large%20enough.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14039v1&entry.124074799=Read"},
{"title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook", "author": "Peng Xu and Shengwu Xiong and Jiajun Zhang and Yaxiong Chen and Bowen Zhou and Chen Change Loy and David A. Clifton and Kyoung Mu Lee and Luc Van Gool and Ruiming He and Ruilin Yao and Xinwei Long and Jirui Huang and Kai Tian and Sa Yang and Yihua Shao and Jin Feng and Yue Zhong and Jiakai Zhou and Cheng Tang and Tianyu Zou and Yifang Zhang and Junming Liang and Guoyou Li and Zhaoxiang Wang and Qiang Zhou and Yichen Zhao and Shili Xiong and Hyeongjin Nam and Jaerin Lee and Jaeyoung Chung and JoonKyu Park and Junghun Oh and Kanggeon Lee and Wooseok Lee and Juneyoung Ro and Turghun Osman and Can Hu and Chaoyang Liao and Cheng Chen and Chengcheng Han and Chenhao Qiu and Chong Peng and Cong Xu and Dailin Li and Feiyu Wang and Feng Gao and Guibo Zhu and Guopeng Tang and Haibo Lu and Han Fang and Han Qi and Hanxiao Wu and Haobo Cheng and Hongbo Sun and Hongyao Chen and Huayong Hu and Hui Li and Jiaheng Ma and Jiang Yu and Jianing Wang and Jie Yang and Jing He and Jinglin Zhou and Jingxuan Li and Josef Kittler and Lihao Zheng and Linnan Zhao and Mengxi Jia and Muyang Yan and Nguyen Thanh Thien and Pu Luo and Qi Li and Shien Song and Shijie Dong and Shuai Shao and Shutao Li and Taofeng Xue and Tianyang Xu and Tianyi Gao and Tingting Li and Wei Zhang and Weiyang Su and Xiaodong Dong and Xiao-Jun Wu and Xiaopeng Zhou and Xin Chen and Xin Wei and Xinyi You and Xudong Kang and Xujie Zhou and Xusheng Liu and Yanan Wang and Yanbin Huang and Yang Liu and Yang Yang and Yanglin Deng and Yashu Kang and Ye Yuan and Yi Wen and Yicen Tian and Yilin Tao and Yin Tang and Yipeng Lin and Yiqing Wang and Yiting Xi and Yongkang Yu and Yumei Li and Yuxin Qin and Yuying Chen and Yuzhe Cen and Zhaofan Zou and Zhaohong Liu and Zhehao Shen and Zhenglin Du and Zhengyang Li and Zhenni Huang and Zhenwei Shao and Zhilong Song and Zhiyong Feng and Zhiyu Wang and Zhou Yu and Ziang Li and Zihan Zhai and Zijian Zhang and Ziyang Peng and Ziyun Xiao and Zongshu Li", "abstract": "  This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.\n", "link": "http://arxiv.org/abs/2509.14142v1", "date": "2025-09-17", "relevancy": 2.1739, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning%3A%20Datasets%2C%20Methods%2C%0A%20%20Results%2C%20Discussion%2C%20and%20Outlook&body=Title%3A%20MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning%3A%20Datasets%2C%20Methods%2C%0A%20%20Results%2C%20Discussion%2C%20and%20Outlook%0AAuthor%3A%20Peng%20Xu%20and%20Shengwu%20Xiong%20and%20Jiajun%20Zhang%20and%20Yaxiong%20Chen%20and%20Bowen%20Zhou%20and%20Chen%20Change%20Loy%20and%20David%20A.%20Clifton%20and%20Kyoung%20Mu%20Lee%20and%20Luc%20Van%20Gool%20and%20Ruiming%20He%20and%20Ruilin%20Yao%20and%20Xinwei%20Long%20and%20Jirui%20Huang%20and%20Kai%20Tian%20and%20Sa%20Yang%20and%20Yihua%20Shao%20and%20Jin%20Feng%20and%20Yue%20Zhong%20and%20Jiakai%20Zhou%20and%20Cheng%20Tang%20and%20Tianyu%20Zou%20and%20Yifang%20Zhang%20and%20Junming%20Liang%20and%20Guoyou%20Li%20and%20Zhaoxiang%20Wang%20and%20Qiang%20Zhou%20and%20Yichen%20Zhao%20and%20Shili%20Xiong%20and%20Hyeongjin%20Nam%20and%20Jaerin%20Lee%20and%20Jaeyoung%20Chung%20and%20JoonKyu%20Park%20and%20Junghun%20Oh%20and%20Kanggeon%20Lee%20and%20Wooseok%20Lee%20and%20Juneyoung%20Ro%20and%20Turghun%20Osman%20and%20Can%20Hu%20and%20Chaoyang%20Liao%20and%20Cheng%20Chen%20and%20Chengcheng%20Han%20and%20Chenhao%20Qiu%20and%20Chong%20Peng%20and%20Cong%20Xu%20and%20Dailin%20Li%20and%20Feiyu%20Wang%20and%20Feng%20Gao%20and%20Guibo%20Zhu%20and%20Guopeng%20Tang%20and%20Haibo%20Lu%20and%20Han%20Fang%20and%20Han%20Qi%20and%20Hanxiao%20Wu%20and%20Haobo%20Cheng%20and%20Hongbo%20Sun%20and%20Hongyao%20Chen%20and%20Huayong%20Hu%20and%20Hui%20Li%20and%20Jiaheng%20Ma%20and%20Jiang%20Yu%20and%20Jianing%20Wang%20and%20Jie%20Yang%20and%20Jing%20He%20and%20Jinglin%20Zhou%20and%20Jingxuan%20Li%20and%20Josef%20Kittler%20and%20Lihao%20Zheng%20and%20Linnan%20Zhao%20and%20Mengxi%20Jia%20and%20Muyang%20Yan%20and%20Nguyen%20Thanh%20Thien%20and%20Pu%20Luo%20and%20Qi%20Li%20and%20Shien%20Song%20and%20Shijie%20Dong%20and%20Shuai%20Shao%20and%20Shutao%20Li%20and%20Taofeng%20Xue%20and%20Tianyang%20Xu%20and%20Tianyi%20Gao%20and%20Tingting%20Li%20and%20Wei%20Zhang%20and%20Weiyang%20Su%20and%20Xiaodong%20Dong%20and%20Xiao-Jun%20Wu%20and%20Xiaopeng%20Zhou%20and%20Xin%20Chen%20and%20Xin%20Wei%20and%20Xinyi%20You%20and%20Xudong%20Kang%20and%20Xujie%20Zhou%20and%20Xusheng%20Liu%20and%20Yanan%20Wang%20and%20Yanbin%20Huang%20and%20Yang%20Liu%20and%20Yang%20Yang%20and%20Yanglin%20Deng%20and%20Yashu%20Kang%20and%20Ye%20Yuan%20and%20Yi%20Wen%20and%20Yicen%20Tian%20and%20Yilin%20Tao%20and%20Yin%20Tang%20and%20Yipeng%20Lin%20and%20Yiqing%20Wang%20and%20Yiting%20Xi%20and%20Yongkang%20Yu%20and%20Yumei%20Li%20and%20Yuxin%20Qin%20and%20Yuying%20Chen%20and%20Yuzhe%20Cen%20and%20Zhaofan%20Zou%20and%20Zhaohong%20Liu%20and%20Zhehao%20Shen%20and%20Zhenglin%20Du%20and%20Zhengyang%20Li%20and%20Zhenni%20Huang%20and%20Zhenwei%20Shao%20and%20Zhilong%20Song%20and%20Zhiyong%20Feng%20and%20Zhiyu%20Wang%20and%20Zhou%20Yu%20and%20Ziang%20Li%20and%20Zihan%20Zhai%20and%20Zijian%20Zhang%20and%20Ziyang%20Peng%20and%20Ziyun%20Xiao%20and%20Zongshu%20Li%0AAbstract%3A%20%20%20This%20paper%20reviews%20the%20MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning.%20We%20aim%0Ato%20bring%20together%20different%20approaches%20in%20multimodal%20machine%20learning%20and%20LLMs%0Avia%20a%20large%20benchmark.%20We%20hope%20it%20better%20allows%20researchers%20to%20follow%20the%0Astate-of-the-art%20in%20this%20very%20dynamic%20area.%20Meanwhile%2C%20a%20growing%20number%20of%0Atestbeds%20have%20boosted%20the%20evolution%20of%20general-purpose%20large%20language%20models.%0AThus%2C%20this%20year%27s%20MARS2%20focuses%20on%20real-world%20and%20specialized%20scenarios%20to%0Abroaden%20the%20multimodal%20reasoning%20applications%20of%20MLLMs.%20Our%20organizing%20team%0Areleased%20two%20tailored%20datasets%20Lens%20and%20AdsQA%20as%20test%20sets%2C%20which%20support%0Ageneral%20reasoning%20in%2012%20daily%20scenarios%20and%20domain-specific%20reasoning%20in%0Aadvertisement%20videos%2C%20respectively.%20We%20evaluated%2040%2B%20baselines%20that%20include%0Aboth%20generalist%20MLLMs%20and%20task-specific%20models%2C%20and%20opened%20up%20three%20competition%0Atracks%2C%20i.e.%2C%20Visual%20Grounding%20in%20Real-world%20Scenarios%20%28VG-RS%29%2C%20Visual%20Question%0AAnswering%20with%20Spatial%20Awareness%20%28VQA-SA%29%2C%20and%20Visual%20Reasoning%20in%20Creative%0AAdvertisement%20Videos%20%28VR-Ads%29.%20Finally%2C%2076%20teams%20from%20the%20renowned%20academic%20and%0Aindustrial%20institutions%20have%20registered%20and%2040%2B%20valid%20submissions%20%28out%20of%0A1200%2B%29%20have%20been%20included%20in%20our%20ranking%20lists.%20Our%20datasets%2C%20code%20sets%20%2840%2B%0Abaselines%20and%2015%2B%20participants%27%20methods%29%2C%20and%20rankings%20are%20publicly%20available%0Aon%20the%20MARS2%20workshop%20website%20and%20our%20GitHub%20organization%20page%0Ahttps%3A//github.com/mars2workshop/%2C%20where%20our%20updates%20and%20announcements%20of%0Aupcoming%20events%20will%20be%20continuously%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARS2%25202025%2520Challenge%2520on%2520Multimodal%2520Reasoning%253A%2520Datasets%252C%2520Methods%252C%250A%2520%2520Results%252C%2520Discussion%252C%2520and%2520Outlook%26entry.906535625%3DPeng%2520Xu%2520and%2520Shengwu%2520Xiong%2520and%2520Jiajun%2520Zhang%2520and%2520Yaxiong%2520Chen%2520and%2520Bowen%2520Zhou%2520and%2520Chen%2520Change%2520Loy%2520and%2520David%2520A.%2520Clifton%2520and%2520Kyoung%2520Mu%2520Lee%2520and%2520Luc%2520Van%2520Gool%2520and%2520Ruiming%2520He%2520and%2520Ruilin%2520Yao%2520and%2520Xinwei%2520Long%2520and%2520Jirui%2520Huang%2520and%2520Kai%2520Tian%2520and%2520Sa%2520Yang%2520and%2520Yihua%2520Shao%2520and%2520Jin%2520Feng%2520and%2520Yue%2520Zhong%2520and%2520Jiakai%2520Zhou%2520and%2520Cheng%2520Tang%2520and%2520Tianyu%2520Zou%2520and%2520Yifang%2520Zhang%2520and%2520Junming%2520Liang%2520and%2520Guoyou%2520Li%2520and%2520Zhaoxiang%2520Wang%2520and%2520Qiang%2520Zhou%2520and%2520Yichen%2520Zhao%2520and%2520Shili%2520Xiong%2520and%2520Hyeongjin%2520Nam%2520and%2520Jaerin%2520Lee%2520and%2520Jaeyoung%2520Chung%2520and%2520JoonKyu%2520Park%2520and%2520Junghun%2520Oh%2520and%2520Kanggeon%2520Lee%2520and%2520Wooseok%2520Lee%2520and%2520Juneyoung%2520Ro%2520and%2520Turghun%2520Osman%2520and%2520Can%2520Hu%2520and%2520Chaoyang%2520Liao%2520and%2520Cheng%2520Chen%2520and%2520Chengcheng%2520Han%2520and%2520Chenhao%2520Qiu%2520and%2520Chong%2520Peng%2520and%2520Cong%2520Xu%2520and%2520Dailin%2520Li%2520and%2520Feiyu%2520Wang%2520and%2520Feng%2520Gao%2520and%2520Guibo%2520Zhu%2520and%2520Guopeng%2520Tang%2520and%2520Haibo%2520Lu%2520and%2520Han%2520Fang%2520and%2520Han%2520Qi%2520and%2520Hanxiao%2520Wu%2520and%2520Haobo%2520Cheng%2520and%2520Hongbo%2520Sun%2520and%2520Hongyao%2520Chen%2520and%2520Huayong%2520Hu%2520and%2520Hui%2520Li%2520and%2520Jiaheng%2520Ma%2520and%2520Jiang%2520Yu%2520and%2520Jianing%2520Wang%2520and%2520Jie%2520Yang%2520and%2520Jing%2520He%2520and%2520Jinglin%2520Zhou%2520and%2520Jingxuan%2520Li%2520and%2520Josef%2520Kittler%2520and%2520Lihao%2520Zheng%2520and%2520Linnan%2520Zhao%2520and%2520Mengxi%2520Jia%2520and%2520Muyang%2520Yan%2520and%2520Nguyen%2520Thanh%2520Thien%2520and%2520Pu%2520Luo%2520and%2520Qi%2520Li%2520and%2520Shien%2520Song%2520and%2520Shijie%2520Dong%2520and%2520Shuai%2520Shao%2520and%2520Shutao%2520Li%2520and%2520Taofeng%2520Xue%2520and%2520Tianyang%2520Xu%2520and%2520Tianyi%2520Gao%2520and%2520Tingting%2520Li%2520and%2520Wei%2520Zhang%2520and%2520Weiyang%2520Su%2520and%2520Xiaodong%2520Dong%2520and%2520Xiao-Jun%2520Wu%2520and%2520Xiaopeng%2520Zhou%2520and%2520Xin%2520Chen%2520and%2520Xin%2520Wei%2520and%2520Xinyi%2520You%2520and%2520Xudong%2520Kang%2520and%2520Xujie%2520Zhou%2520and%2520Xusheng%2520Liu%2520and%2520Yanan%2520Wang%2520and%2520Yanbin%2520Huang%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Yang%2520and%2520Yanglin%2520Deng%2520and%2520Yashu%2520Kang%2520and%2520Ye%2520Yuan%2520and%2520Yi%2520Wen%2520and%2520Yicen%2520Tian%2520and%2520Yilin%2520Tao%2520and%2520Yin%2520Tang%2520and%2520Yipeng%2520Lin%2520and%2520Yiqing%2520Wang%2520and%2520Yiting%2520Xi%2520and%2520Yongkang%2520Yu%2520and%2520Yumei%2520Li%2520and%2520Yuxin%2520Qin%2520and%2520Yuying%2520Chen%2520and%2520Yuzhe%2520Cen%2520and%2520Zhaofan%2520Zou%2520and%2520Zhaohong%2520Liu%2520and%2520Zhehao%2520Shen%2520and%2520Zhenglin%2520Du%2520and%2520Zhengyang%2520Li%2520and%2520Zhenni%2520Huang%2520and%2520Zhenwei%2520Shao%2520and%2520Zhilong%2520Song%2520and%2520Zhiyong%2520Feng%2520and%2520Zhiyu%2520Wang%2520and%2520Zhou%2520Yu%2520and%2520Ziang%2520Li%2520and%2520Zihan%2520Zhai%2520and%2520Zijian%2520Zhang%2520and%2520Ziyang%2520Peng%2520and%2520Ziyun%2520Xiao%2520and%2520Zongshu%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520reviews%2520the%2520MARS2%25202025%2520Challenge%2520on%2520Multimodal%2520Reasoning.%2520We%2520aim%250Ato%2520bring%2520together%2520different%2520approaches%2520in%2520multimodal%2520machine%2520learning%2520and%2520LLMs%250Avia%2520a%2520large%2520benchmark.%2520We%2520hope%2520it%2520better%2520allows%2520researchers%2520to%2520follow%2520the%250Astate-of-the-art%2520in%2520this%2520very%2520dynamic%2520area.%2520Meanwhile%252C%2520a%2520growing%2520number%2520of%250Atestbeds%2520have%2520boosted%2520the%2520evolution%2520of%2520general-purpose%2520large%2520language%2520models.%250AThus%252C%2520this%2520year%2527s%2520MARS2%2520focuses%2520on%2520real-world%2520and%2520specialized%2520scenarios%2520to%250Abroaden%2520the%2520multimodal%2520reasoning%2520applications%2520of%2520MLLMs.%2520Our%2520organizing%2520team%250Areleased%2520two%2520tailored%2520datasets%2520Lens%2520and%2520AdsQA%2520as%2520test%2520sets%252C%2520which%2520support%250Ageneral%2520reasoning%2520in%252012%2520daily%2520scenarios%2520and%2520domain-specific%2520reasoning%2520in%250Aadvertisement%2520videos%252C%2520respectively.%2520We%2520evaluated%252040%252B%2520baselines%2520that%2520include%250Aboth%2520generalist%2520MLLMs%2520and%2520task-specific%2520models%252C%2520and%2520opened%2520up%2520three%2520competition%250Atracks%252C%2520i.e.%252C%2520Visual%2520Grounding%2520in%2520Real-world%2520Scenarios%2520%2528VG-RS%2529%252C%2520Visual%2520Question%250AAnswering%2520with%2520Spatial%2520Awareness%2520%2528VQA-SA%2529%252C%2520and%2520Visual%2520Reasoning%2520in%2520Creative%250AAdvertisement%2520Videos%2520%2528VR-Ads%2529.%2520Finally%252C%252076%2520teams%2520from%2520the%2520renowned%2520academic%2520and%250Aindustrial%2520institutions%2520have%2520registered%2520and%252040%252B%2520valid%2520submissions%2520%2528out%2520of%250A1200%252B%2529%2520have%2520been%2520included%2520in%2520our%2520ranking%2520lists.%2520Our%2520datasets%252C%2520code%2520sets%2520%252840%252B%250Abaselines%2520and%252015%252B%2520participants%2527%2520methods%2529%252C%2520and%2520rankings%2520are%2520publicly%2520available%250Aon%2520the%2520MARS2%2520workshop%2520website%2520and%2520our%2520GitHub%2520organization%2520page%250Ahttps%253A//github.com/mars2workshop/%252C%2520where%2520our%2520updates%2520and%2520announcements%2520of%250Aupcoming%2520events%2520will%2520be%2520continuously%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning%3A%20Datasets%2C%20Methods%2C%0A%20%20Results%2C%20Discussion%2C%20and%20Outlook&entry.906535625=Peng%20Xu%20and%20Shengwu%20Xiong%20and%20Jiajun%20Zhang%20and%20Yaxiong%20Chen%20and%20Bowen%20Zhou%20and%20Chen%20Change%20Loy%20and%20David%20A.%20Clifton%20and%20Kyoung%20Mu%20Lee%20and%20Luc%20Van%20Gool%20and%20Ruiming%20He%20and%20Ruilin%20Yao%20and%20Xinwei%20Long%20and%20Jirui%20Huang%20and%20Kai%20Tian%20and%20Sa%20Yang%20and%20Yihua%20Shao%20and%20Jin%20Feng%20and%20Yue%20Zhong%20and%20Jiakai%20Zhou%20and%20Cheng%20Tang%20and%20Tianyu%20Zou%20and%20Yifang%20Zhang%20and%20Junming%20Liang%20and%20Guoyou%20Li%20and%20Zhaoxiang%20Wang%20and%20Qiang%20Zhou%20and%20Yichen%20Zhao%20and%20Shili%20Xiong%20and%20Hyeongjin%20Nam%20and%20Jaerin%20Lee%20and%20Jaeyoung%20Chung%20and%20JoonKyu%20Park%20and%20Junghun%20Oh%20and%20Kanggeon%20Lee%20and%20Wooseok%20Lee%20and%20Juneyoung%20Ro%20and%20Turghun%20Osman%20and%20Can%20Hu%20and%20Chaoyang%20Liao%20and%20Cheng%20Chen%20and%20Chengcheng%20Han%20and%20Chenhao%20Qiu%20and%20Chong%20Peng%20and%20Cong%20Xu%20and%20Dailin%20Li%20and%20Feiyu%20Wang%20and%20Feng%20Gao%20and%20Guibo%20Zhu%20and%20Guopeng%20Tang%20and%20Haibo%20Lu%20and%20Han%20Fang%20and%20Han%20Qi%20and%20Hanxiao%20Wu%20and%20Haobo%20Cheng%20and%20Hongbo%20Sun%20and%20Hongyao%20Chen%20and%20Huayong%20Hu%20and%20Hui%20Li%20and%20Jiaheng%20Ma%20and%20Jiang%20Yu%20and%20Jianing%20Wang%20and%20Jie%20Yang%20and%20Jing%20He%20and%20Jinglin%20Zhou%20and%20Jingxuan%20Li%20and%20Josef%20Kittler%20and%20Lihao%20Zheng%20and%20Linnan%20Zhao%20and%20Mengxi%20Jia%20and%20Muyang%20Yan%20and%20Nguyen%20Thanh%20Thien%20and%20Pu%20Luo%20and%20Qi%20Li%20and%20Shien%20Song%20and%20Shijie%20Dong%20and%20Shuai%20Shao%20and%20Shutao%20Li%20and%20Taofeng%20Xue%20and%20Tianyang%20Xu%20and%20Tianyi%20Gao%20and%20Tingting%20Li%20and%20Wei%20Zhang%20and%20Weiyang%20Su%20and%20Xiaodong%20Dong%20and%20Xiao-Jun%20Wu%20and%20Xiaopeng%20Zhou%20and%20Xin%20Chen%20and%20Xin%20Wei%20and%20Xinyi%20You%20and%20Xudong%20Kang%20and%20Xujie%20Zhou%20and%20Xusheng%20Liu%20and%20Yanan%20Wang%20and%20Yanbin%20Huang%20and%20Yang%20Liu%20and%20Yang%20Yang%20and%20Yanglin%20Deng%20and%20Yashu%20Kang%20and%20Ye%20Yuan%20and%20Yi%20Wen%20and%20Yicen%20Tian%20and%20Yilin%20Tao%20and%20Yin%20Tang%20and%20Yipeng%20Lin%20and%20Yiqing%20Wang%20and%20Yiting%20Xi%20and%20Yongkang%20Yu%20and%20Yumei%20Li%20and%20Yuxin%20Qin%20and%20Yuying%20Chen%20and%20Yuzhe%20Cen%20and%20Zhaofan%20Zou%20and%20Zhaohong%20Liu%20and%20Zhehao%20Shen%20and%20Zhenglin%20Du%20and%20Zhengyang%20Li%20and%20Zhenni%20Huang%20and%20Zhenwei%20Shao%20and%20Zhilong%20Song%20and%20Zhiyong%20Feng%20and%20Zhiyu%20Wang%20and%20Zhou%20Yu%20and%20Ziang%20Li%20and%20Zihan%20Zhai%20and%20Zijian%20Zhang%20and%20Ziyang%20Peng%20and%20Ziyun%20Xiao%20and%20Zongshu%20Li&entry.1292438233=%20%20This%20paper%20reviews%20the%20MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning.%20We%20aim%0Ato%20bring%20together%20different%20approaches%20in%20multimodal%20machine%20learning%20and%20LLMs%0Avia%20a%20large%20benchmark.%20We%20hope%20it%20better%20allows%20researchers%20to%20follow%20the%0Astate-of-the-art%20in%20this%20very%20dynamic%20area.%20Meanwhile%2C%20a%20growing%20number%20of%0Atestbeds%20have%20boosted%20the%20evolution%20of%20general-purpose%20large%20language%20models.%0AThus%2C%20this%20year%27s%20MARS2%20focuses%20on%20real-world%20and%20specialized%20scenarios%20to%0Abroaden%20the%20multimodal%20reasoning%20applications%20of%20MLLMs.%20Our%20organizing%20team%0Areleased%20two%20tailored%20datasets%20Lens%20and%20AdsQA%20as%20test%20sets%2C%20which%20support%0Ageneral%20reasoning%20in%2012%20daily%20scenarios%20and%20domain-specific%20reasoning%20in%0Aadvertisement%20videos%2C%20respectively.%20We%20evaluated%2040%2B%20baselines%20that%20include%0Aboth%20generalist%20MLLMs%20and%20task-specific%20models%2C%20and%20opened%20up%20three%20competition%0Atracks%2C%20i.e.%2C%20Visual%20Grounding%20in%20Real-world%20Scenarios%20%28VG-RS%29%2C%20Visual%20Question%0AAnswering%20with%20Spatial%20Awareness%20%28VQA-SA%29%2C%20and%20Visual%20Reasoning%20in%20Creative%0AAdvertisement%20Videos%20%28VR-Ads%29.%20Finally%2C%2076%20teams%20from%20the%20renowned%20academic%20and%0Aindustrial%20institutions%20have%20registered%20and%2040%2B%20valid%20submissions%20%28out%20of%0A1200%2B%29%20have%20been%20included%20in%20our%20ranking%20lists.%20Our%20datasets%2C%20code%20sets%20%2840%2B%0Abaselines%20and%2015%2B%20participants%27%20methods%29%2C%20and%20rankings%20are%20publicly%20available%0Aon%20the%20MARS2%20workshop%20website%20and%20our%20GitHub%20organization%20page%0Ahttps%3A//github.com/mars2workshop/%2C%20where%20our%20updates%20and%20announcements%20of%0Aupcoming%20events%20will%20be%20continuously%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14142v1&entry.124074799=Read"},
{"title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft\n  Mixture-of-Experts", "author": "Leonard Hackel and Tom Burgert and Beg\u00fcm Demir", "abstract": "  Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.\n", "link": "http://arxiv.org/abs/2509.14104v1", "date": "2025-09-17", "relevancy": 2.154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSMoE%3A%20An%20Efficient%20Remote%20Sensing%20Foundation%20Model%20with%20Soft%0A%20%20Mixture-of-Experts&body=Title%3A%20CSMoE%3A%20An%20Efficient%20Remote%20Sensing%20Foundation%20Model%20with%20Soft%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20has%20attracted%20great%0Aattention%20for%20remote%20sensing%20%28RS%29%20foundation%20model%20%28FM%29%20development%2C%20enabling%0Aimproved%20representation%20learning%20across%20diverse%20sensors%20and%20downstream%20tasks.%0AHowever%2C%20existing%20RS%20FMs%20often%20either%20suffer%20from%20substantial%20computational%0Acomplexity%20during%20both%20training%20and%20inference%20or%20exhibit%20limited%0Arepresentational%20capacity.%20These%20issues%20restrict%20their%20practical%20applicability%0Ain%20RS.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20adaptation%20for%20enhancing%20the%0Aefficiency%20of%20RS%20FMs%20by%20integrating%20the%20Soft%20mixture-of-experts%20%28MoE%29%20mechanism%0Ainto%20the%20FM.%20The%20integration%20of%20Soft%20MoEs%20into%20the%20FM%20allows%20modality-specific%0Aexpert%20specialization%20alongside%20shared%20cross-sensor%20representation%20learning.%20To%0Ademonstrate%20the%20effectiveness%20of%20our%20adaptation%2C%20we%20apply%20it%20on%20the%0ACross-Sensor%20Masked%20Autoencoder%20%28CSMAE%29%20model%2C%20resulting%20in%20the%20Cross-Sensor%0AMixture-of-Experts%20%28CSMoE%29%20model.%20In%20addition%2C%20we%20introduce%20a%20thematic-climatic%0Adescriptor-driven%20sampling%20strategy%20for%20the%20construction%20of%20a%20representative%0Aand%20diverse%20training%20set%20to%20train%20our%20CSMoE%20model.%20Extensive%20experiments%20on%0Ascene%20classification%2C%20semantic%20segmentation%2C%20and%20content-based%20image%20retrieval%0Ademonstrate%20that%20our%20adaptation%20yields%20a%20reduction%20in%20computational%0Arequirements%20while%20maintaining%20or%20improving%20representational%20performance.%0ACompared%20to%20state-of-the-art%20RS%20FMs%2C%20CSMoE%20achieves%20a%20superior%20trade-off%0Abetween%20representational%20capacity%2C%20accuracy%2C%20and%20computational%20efficiency.%20On%0Aaverage%2C%20CSMoE%20achieves%20more%20than%20twice%20the%20computational%20efficiency%20of%0Aexisting%20RS%20FMs%2C%20while%20maintaining%20competitive%20performance%20across%20all%0Aexperiments.%20These%20results%20show%20the%20effectiveness%20of%20the%20proposed%20adaptation%0Afor%20creating%20computationally%20efficient%20RS%20FMs.%20The%20code%20for%20the%20model%2C%20the%0Atraining%20set%20creation%2C%20and%20the%20model%20weights%20will%20be%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/csmoe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSMoE%253A%2520An%2520Efficient%2520Remote%2520Sensing%2520Foundation%2520Model%2520with%2520Soft%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DLeonard%2520Hackel%2520and%2520Tom%2520Burgert%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520through%2520masked%2520autoencoders%2520has%2520attracted%2520great%250Aattention%2520for%2520remote%2520sensing%2520%2528RS%2529%2520foundation%2520model%2520%2528FM%2529%2520development%252C%2520enabling%250Aimproved%2520representation%2520learning%2520across%2520diverse%2520sensors%2520and%2520downstream%2520tasks.%250AHowever%252C%2520existing%2520RS%2520FMs%2520often%2520either%2520suffer%2520from%2520substantial%2520computational%250Acomplexity%2520during%2520both%2520training%2520and%2520inference%2520or%2520exhibit%2520limited%250Arepresentational%2520capacity.%2520These%2520issues%2520restrict%2520their%2520practical%2520applicability%250Ain%2520RS.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520an%2520adaptation%2520for%2520enhancing%2520the%250Aefficiency%2520of%2520RS%2520FMs%2520by%2520integrating%2520the%2520Soft%2520mixture-of-experts%2520%2528MoE%2529%2520mechanism%250Ainto%2520the%2520FM.%2520The%2520integration%2520of%2520Soft%2520MoEs%2520into%2520the%2520FM%2520allows%2520modality-specific%250Aexpert%2520specialization%2520alongside%2520shared%2520cross-sensor%2520representation%2520learning.%2520To%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520adaptation%252C%2520we%2520apply%2520it%2520on%2520the%250ACross-Sensor%2520Masked%2520Autoencoder%2520%2528CSMAE%2529%2520model%252C%2520resulting%2520in%2520the%2520Cross-Sensor%250AMixture-of-Experts%2520%2528CSMoE%2529%2520model.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520thematic-climatic%250Adescriptor-driven%2520sampling%2520strategy%2520for%2520the%2520construction%2520of%2520a%2520representative%250Aand%2520diverse%2520training%2520set%2520to%2520train%2520our%2520CSMoE%2520model.%2520Extensive%2520experiments%2520on%250Ascene%2520classification%252C%2520semantic%2520segmentation%252C%2520and%2520content-based%2520image%2520retrieval%250Ademonstrate%2520that%2520our%2520adaptation%2520yields%2520a%2520reduction%2520in%2520computational%250Arequirements%2520while%2520maintaining%2520or%2520improving%2520representational%2520performance.%250ACompared%2520to%2520state-of-the-art%2520RS%2520FMs%252C%2520CSMoE%2520achieves%2520a%2520superior%2520trade-off%250Abetween%2520representational%2520capacity%252C%2520accuracy%252C%2520and%2520computational%2520efficiency.%2520On%250Aaverage%252C%2520CSMoE%2520achieves%2520more%2520than%2520twice%2520the%2520computational%2520efficiency%2520of%250Aexisting%2520RS%2520FMs%252C%2520while%2520maintaining%2520competitive%2520performance%2520across%2520all%250Aexperiments.%2520These%2520results%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520adaptation%250Afor%2520creating%2520computationally%2520efficient%2520RS%2520FMs.%2520The%2520code%2520for%2520the%2520model%252C%2520the%250Atraining%2520set%2520creation%252C%2520and%2520the%2520model%2520weights%2520will%2520be%2520available%2520at%250Ahttps%253A//git.tu-berlin.de/rsim/csmoe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSMoE%3A%20An%20Efficient%20Remote%20Sensing%20Foundation%20Model%20with%20Soft%0A%20%20Mixture-of-Experts&entry.906535625=Leonard%20Hackel%20and%20Tom%20Burgert%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20has%20attracted%20great%0Aattention%20for%20remote%20sensing%20%28RS%29%20foundation%20model%20%28FM%29%20development%2C%20enabling%0Aimproved%20representation%20learning%20across%20diverse%20sensors%20and%20downstream%20tasks.%0AHowever%2C%20existing%20RS%20FMs%20often%20either%20suffer%20from%20substantial%20computational%0Acomplexity%20during%20both%20training%20and%20inference%20or%20exhibit%20limited%0Arepresentational%20capacity.%20These%20issues%20restrict%20their%20practical%20applicability%0Ain%20RS.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20adaptation%20for%20enhancing%20the%0Aefficiency%20of%20RS%20FMs%20by%20integrating%20the%20Soft%20mixture-of-experts%20%28MoE%29%20mechanism%0Ainto%20the%20FM.%20The%20integration%20of%20Soft%20MoEs%20into%20the%20FM%20allows%20modality-specific%0Aexpert%20specialization%20alongside%20shared%20cross-sensor%20representation%20learning.%20To%0Ademonstrate%20the%20effectiveness%20of%20our%20adaptation%2C%20we%20apply%20it%20on%20the%0ACross-Sensor%20Masked%20Autoencoder%20%28CSMAE%29%20model%2C%20resulting%20in%20the%20Cross-Sensor%0AMixture-of-Experts%20%28CSMoE%29%20model.%20In%20addition%2C%20we%20introduce%20a%20thematic-climatic%0Adescriptor-driven%20sampling%20strategy%20for%20the%20construction%20of%20a%20representative%0Aand%20diverse%20training%20set%20to%20train%20our%20CSMoE%20model.%20Extensive%20experiments%20on%0Ascene%20classification%2C%20semantic%20segmentation%2C%20and%20content-based%20image%20retrieval%0Ademonstrate%20that%20our%20adaptation%20yields%20a%20reduction%20in%20computational%0Arequirements%20while%20maintaining%20or%20improving%20representational%20performance.%0ACompared%20to%20state-of-the-art%20RS%20FMs%2C%20CSMoE%20achieves%20a%20superior%20trade-off%0Abetween%20representational%20capacity%2C%20accuracy%2C%20and%20computational%20efficiency.%20On%0Aaverage%2C%20CSMoE%20achieves%20more%20than%20twice%20the%20computational%20efficiency%20of%0Aexisting%20RS%20FMs%2C%20while%20maintaining%20competitive%20performance%20across%20all%0Aexperiments.%20These%20results%20show%20the%20effectiveness%20of%20the%20proposed%20adaptation%0Afor%20creating%20computationally%20efficient%20RS%20FMs.%20The%20code%20for%20the%20model%2C%20the%0Atraining%20set%20creation%2C%20and%20the%20model%20weights%20will%20be%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/csmoe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14104v1&entry.124074799=Read"},
{"title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent\n  Reinforcement Learning", "author": "Ziyuan Chen and Zhenghui Zhao and Zhangye Han and Miancan Liu and Xianhang Ye and Yiqing Li and Hongbo Min and Jinkui Ren and Xiantao Zhang and Guitao Cao", "abstract": "  With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps.\n", "link": "http://arxiv.org/abs/2509.14172v1", "date": "2025-09-17", "relevancy": 2.1251, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5474}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5304}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TGPO%3A%20Tree-Guided%20Preference%20Optimization%20for%20Robust%20Web%20Agent%0A%20%20Reinforcement%20Learning&body=Title%3A%20TGPO%3A%20Tree-Guided%20Preference%20Optimization%20for%20Robust%20Web%20Agent%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ziyuan%20Chen%20and%20Zhenghui%20Zhao%20and%20Zhangye%20Han%20and%20Miancan%20Liu%20and%20Xianhang%20Ye%20and%20Yiqing%20Li%20and%20Hongbo%20Min%20and%20Jinkui%20Ren%20and%20Xiantao%20Zhang%20and%20Guitao%20Cao%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20and%20vision-language%0Amodels%2C%20employing%20large%20models%20as%20Web%20Agents%20has%20become%20essential%20for%20automated%0Aweb%20interaction.%20However%2C%20training%20Web%20Agents%20with%20reinforcement%20learning%20faces%0Acritical%20challenges%20including%20credit%20assignment%20misallocation%2C%20prohibitively%0Ahigh%20annotation%20costs%2C%20and%20reward%20sparsity.%20To%20address%20these%20issues%2C%20we%20propose%0ATree-Guided%20Preference%20Optimization%20%28TGPO%29%2C%20an%20offline%20reinforcement%20learning%0Aframework%20that%20proposes%20a%20tree-structured%20trajectory%20representation%20merging%0Asemantically%20identical%20states%20across%20trajectories%20to%20eliminate%20label%20conflicts.%0AOur%20framework%20incorporates%20a%20Process%20Reward%20Model%20that%20automatically%20generates%0Afine-grained%20rewards%20through%20subgoal%20progress%2C%20redundancy%20detection%2C%20and%20action%0Averification.%20Additionally%2C%20a%20dynamic%20weighting%20mechanism%20prioritizes%0Ahigh-impact%20decision%20points%20during%20training.%20Experiments%20on%20Online-Mind2Web%20and%0Aour%20self-constructed%20C-WebShop%20datasets%20demonstrate%20that%20TGPO%20significantly%0Aoutperforms%20existing%20methods%2C%20achieving%20higher%20success%20rates%20with%20fewer%0Aredundant%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTGPO%253A%2520Tree-Guided%2520Preference%2520Optimization%2520for%2520Robust%2520Web%2520Agent%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZiyuan%2520Chen%2520and%2520Zhenghui%2520Zhao%2520and%2520Zhangye%2520Han%2520and%2520Miancan%2520Liu%2520and%2520Xianhang%2520Ye%2520and%2520Yiqing%2520Li%2520and%2520Hongbo%2520Min%2520and%2520Jinkui%2520Ren%2520and%2520Xiantao%2520Zhang%2520and%2520Guitao%2520Cao%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520and%2520vision-language%250Amodels%252C%2520employing%2520large%2520models%2520as%2520Web%2520Agents%2520has%2520become%2520essential%2520for%2520automated%250Aweb%2520interaction.%2520However%252C%2520training%2520Web%2520Agents%2520with%2520reinforcement%2520learning%2520faces%250Acritical%2520challenges%2520including%2520credit%2520assignment%2520misallocation%252C%2520prohibitively%250Ahigh%2520annotation%2520costs%252C%2520and%2520reward%2520sparsity.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250ATree-Guided%2520Preference%2520Optimization%2520%2528TGPO%2529%252C%2520an%2520offline%2520reinforcement%2520learning%250Aframework%2520that%2520proposes%2520a%2520tree-structured%2520trajectory%2520representation%2520merging%250Asemantically%2520identical%2520states%2520across%2520trajectories%2520to%2520eliminate%2520label%2520conflicts.%250AOur%2520framework%2520incorporates%2520a%2520Process%2520Reward%2520Model%2520that%2520automatically%2520generates%250Afine-grained%2520rewards%2520through%2520subgoal%2520progress%252C%2520redundancy%2520detection%252C%2520and%2520action%250Averification.%2520Additionally%252C%2520a%2520dynamic%2520weighting%2520mechanism%2520prioritizes%250Ahigh-impact%2520decision%2520points%2520during%2520training.%2520Experiments%2520on%2520Online-Mind2Web%2520and%250Aour%2520self-constructed%2520C-WebShop%2520datasets%2520demonstrate%2520that%2520TGPO%2520significantly%250Aoutperforms%2520existing%2520methods%252C%2520achieving%2520higher%2520success%2520rates%2520with%2520fewer%250Aredundant%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TGPO%3A%20Tree-Guided%20Preference%20Optimization%20for%20Robust%20Web%20Agent%0A%20%20Reinforcement%20Learning&entry.906535625=Ziyuan%20Chen%20and%20Zhenghui%20Zhao%20and%20Zhangye%20Han%20and%20Miancan%20Liu%20and%20Xianhang%20Ye%20and%20Yiqing%20Li%20and%20Hongbo%20Min%20and%20Jinkui%20Ren%20and%20Xiantao%20Zhang%20and%20Guitao%20Cao&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20and%20vision-language%0Amodels%2C%20employing%20large%20models%20as%20Web%20Agents%20has%20become%20essential%20for%20automated%0Aweb%20interaction.%20However%2C%20training%20Web%20Agents%20with%20reinforcement%20learning%20faces%0Acritical%20challenges%20including%20credit%20assignment%20misallocation%2C%20prohibitively%0Ahigh%20annotation%20costs%2C%20and%20reward%20sparsity.%20To%20address%20these%20issues%2C%20we%20propose%0ATree-Guided%20Preference%20Optimization%20%28TGPO%29%2C%20an%20offline%20reinforcement%20learning%0Aframework%20that%20proposes%20a%20tree-structured%20trajectory%20representation%20merging%0Asemantically%20identical%20states%20across%20trajectories%20to%20eliminate%20label%20conflicts.%0AOur%20framework%20incorporates%20a%20Process%20Reward%20Model%20that%20automatically%20generates%0Afine-grained%20rewards%20through%20subgoal%20progress%2C%20redundancy%20detection%2C%20and%20action%0Averification.%20Additionally%2C%20a%20dynamic%20weighting%20mechanism%20prioritizes%0Ahigh-impact%20decision%20points%20during%20training.%20Experiments%20on%20Online-Mind2Web%20and%0Aour%20self-constructed%20C-WebShop%20datasets%20demonstrate%20that%20TGPO%20significantly%0Aoutperforms%20existing%20methods%2C%20achieving%20higher%20success%20rates%20with%20fewer%0Aredundant%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14172v1&entry.124074799=Read"},
{"title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments", "author": "Alejandro Hern\u00e1ndez-Cano and Alexander H\u00e4gele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank \u010eurech and Ido Hakimi and Juan Garc\u00eda Giraldo and Mete Ismayilzada and Negar Foroutan and Skander Moalla and Tiancheng Chen and Vinko Sabol\u010dec and Yixuan Xu and Michael Aerni and Badr AlKhamissi and Ines Altemir Marinas and Mohammad Hossein Amani and Matin Ansaripour and Ilia Badanin and Harold Benoit and Emanuela Boros and Nicholas Browning and Fabian B\u00f6sch and Maximilian B\u00f6ther and Niklas Canova and Camille Challier and Clement Charmillot and Jonathan Coles and Jan Deriu and Arnout Devos and Lukas Drescher and Daniil Dzenhaliou and Maud Ehrmann and Dongyang Fan and Simin Fan and Silin Gao and Miguel Gila and Mar\u00eda Grandury and Diba Hashemi and Alexander Hoyle and Jiaming Jiang and Mark Klein and Andrei Kucharavy and Anastasiia Kucherenko and Frederike L\u00fcbeck and Roman Machacek and Theofilos Manitaras and Andreas Marfurt and Kyle Matoba and Simon Matrenok and Henrique Mendonc\u00e7a and Fawzi Roberto Mohamed and Syrielle Montariol and Luca Mouchel and Sven Najem-Meyer and Jingwei Ni and Gennaro Oliva and Matteo Pagliardini and Elia Palme and Andrei Panferov and L\u00e9o Paoletti and Marco Passerini and Ivan Pavlov and Auguste Poiroux and Kaustubh Ponkshe and Nathan Ranchin and Javi Rando and Mathieu Sauser and Jakhongir Saydaliev and Muhammad Ali Sayfiddinov and Marian Schneider and Stefano Schuppli and Marco Scialanga and Andrei Semenov and Kumar Shridhar and Raghav Singhal and Anna Sotnikova and Alexander Sternfeld and Ayush Kumar Tarun and Paul Teiletche and Jannis Vamvas and Xiaozhe Yao and Hao Zhao Alexander Ilic and Ana Klimovic and Andreas Krause and Caglar Gulcehre and David Rosenthal and Elliott Ash and Florian Tram\u00e8r and Joost VandeVondele and Livio Veraldi and Martin Rajman and Thomas Schulthess and Torsten Hoefler and Antoine Bosselut and Martin Jaggi and Imanol Schlag", "abstract": "  We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.\n", "link": "http://arxiv.org/abs/2509.14233v1", "date": "2025-09-17", "relevancy": 2.1002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apertus%3A%20Democratizing%20Open%20and%20Compliant%20LLMs%20for%20Global%20Language%0A%20%20Environments&body=Title%3A%20Apertus%3A%20Democratizing%20Open%20and%20Compliant%20LLMs%20for%20Global%20Language%0A%20%20Environments%0AAuthor%3A%20Alejandro%20Hern%C3%A1ndez-Cano%20and%20Alexander%20H%C3%A4gele%20and%20Allen%20Hao%20Huang%20and%20Angelika%20Romanou%20and%20Antoni-Joan%20Solergibert%20and%20Barna%20Pasztor%20and%20Bettina%20Messmer%20and%20Dhia%20Garbaya%20and%20Eduard%20Frank%20%C4%8Eurech%20and%20Ido%20Hakimi%20and%20Juan%20Garc%C3%ADa%20Giraldo%20and%20Mete%20Ismayilzada%20and%20Negar%20Foroutan%20and%20Skander%20Moalla%20and%20Tiancheng%20Chen%20and%20Vinko%20Sabol%C4%8Dec%20and%20Yixuan%20Xu%20and%20Michael%20Aerni%20and%20Badr%20AlKhamissi%20and%20Ines%20Altemir%20Marinas%20and%20Mohammad%20Hossein%20Amani%20and%20Matin%20Ansaripour%20and%20Ilia%20Badanin%20and%20Harold%20Benoit%20and%20Emanuela%20Boros%20and%20Nicholas%20Browning%20and%20Fabian%20B%C3%B6sch%20and%20Maximilian%20B%C3%B6ther%20and%20Niklas%20Canova%20and%20Camille%20Challier%20and%20Clement%20Charmillot%20and%20Jonathan%20Coles%20and%20Jan%20Deriu%20and%20Arnout%20Devos%20and%20Lukas%20Drescher%20and%20Daniil%20Dzenhaliou%20and%20Maud%20Ehrmann%20and%20Dongyang%20Fan%20and%20Simin%20Fan%20and%20Silin%20Gao%20and%20Miguel%20Gila%20and%20Mar%C3%ADa%20Grandury%20and%20Diba%20Hashemi%20and%20Alexander%20Hoyle%20and%20Jiaming%20Jiang%20and%20Mark%20Klein%20and%20Andrei%20Kucharavy%20and%20Anastasiia%20Kucherenko%20and%20Frederike%20L%C3%BCbeck%20and%20Roman%20Machacek%20and%20Theofilos%20Manitaras%20and%20Andreas%20Marfurt%20and%20Kyle%20Matoba%20and%20Simon%20Matrenok%20and%20Henrique%20Mendonc%C3%A7a%20and%20Fawzi%20Roberto%20Mohamed%20and%20Syrielle%20Montariol%20and%20Luca%20Mouchel%20and%20Sven%20Najem-Meyer%20and%20Jingwei%20Ni%20and%20Gennaro%20Oliva%20and%20Matteo%20Pagliardini%20and%20Elia%20Palme%20and%20Andrei%20Panferov%20and%20L%C3%A9o%20Paoletti%20and%20Marco%20Passerini%20and%20Ivan%20Pavlov%20and%20Auguste%20Poiroux%20and%20Kaustubh%20Ponkshe%20and%20Nathan%20Ranchin%20and%20Javi%20Rando%20and%20Mathieu%20Sauser%20and%20Jakhongir%20Saydaliev%20and%20Muhammad%20Ali%20Sayfiddinov%20and%20Marian%20Schneider%20and%20Stefano%20Schuppli%20and%20Marco%20Scialanga%20and%20Andrei%20Semenov%20and%20Kumar%20Shridhar%20and%20Raghav%20Singhal%20and%20Anna%20Sotnikova%20and%20Alexander%20Sternfeld%20and%20Ayush%20Kumar%20Tarun%20and%20Paul%20Teiletche%20and%20Jannis%20Vamvas%20and%20Xiaozhe%20Yao%20and%20Hao%20Zhao%20Alexander%20Ilic%20and%20Ana%20Klimovic%20and%20Andreas%20Krause%20and%20Caglar%20Gulcehre%20and%20David%20Rosenthal%20and%20Elliott%20Ash%20and%20Florian%20Tram%C3%A8r%20and%20Joost%20VandeVondele%20and%20Livio%20Veraldi%20and%20Martin%20Rajman%20and%20Thomas%20Schulthess%20and%20Torsten%20Hoefler%20and%20Antoine%20Bosselut%20and%20Martin%20Jaggi%20and%20Imanol%20Schlag%0AAbstract%3A%20%20%20We%20present%20Apertus%2C%20a%20fully%20open%20suite%20of%20large%20language%20models%20%28LLMs%29%0Adesigned%20to%20address%20two%20systemic%20shortcomings%20in%20today%27s%20open%20model%20ecosystem%3A%0Adata%20compliance%20and%20multilingual%20representation.%20Unlike%20many%20prior%20models%20that%0Arelease%20weights%20without%20reproducible%20data%20pipelines%20or%20regard%20for%20content-owner%0Arights%2C%20Apertus%20models%20are%20pretrained%20exclusively%20on%20openly%20available%20data%2C%0Aretroactively%20respecting%20robots.txt%20exclusions%20and%20filtering%20for%0Anon-permissive%2C%20toxic%2C%20and%20personally%20identifiable%20content.%20To%20mitigate%20risks%0Aof%20memorization%2C%20we%20adopt%20the%20Goldfish%20objective%20during%20pretraining%2C%20strongly%0Asuppressing%20verbatim%20recall%20of%20data%20while%20retaining%20downstream%20task%0Aperformance.%20The%20Apertus%20models%20also%20expand%20multilingual%20coverage%2C%20training%20on%0A15T%20tokens%20from%20over%201800%20languages%2C%20with%20~40%25%20of%20pretraining%20data%20allocated%20to%0Anon-English%20content.%20Released%20at%208B%20and%2070B%20scales%2C%20Apertus%20approaches%0Astate-of-the-art%20results%20among%20fully%20open%20models%20on%20multilingual%20benchmarks%2C%0Arivalling%20or%20surpassing%20open-weight%20counterparts.%20Beyond%20model%20weights%2C%20we%0Arelease%20all%20scientific%20artifacts%20from%20our%20development%20cycle%20with%20a%20permissive%0Alicense%2C%20including%20data%20preparation%20scripts%2C%20checkpoints%2C%20evaluation%20suites%2C%0Aand%20training%20code%2C%20enabling%20transparent%20audit%20and%20extension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApertus%253A%2520Democratizing%2520Open%2520and%2520Compliant%2520LLMs%2520for%2520Global%2520Language%250A%2520%2520Environments%26entry.906535625%3DAlejandro%2520Hern%25C3%25A1ndez-Cano%2520and%2520Alexander%2520H%25C3%25A4gele%2520and%2520Allen%2520Hao%2520Huang%2520and%2520Angelika%2520Romanou%2520and%2520Antoni-Joan%2520Solergibert%2520and%2520Barna%2520Pasztor%2520and%2520Bettina%2520Messmer%2520and%2520Dhia%2520Garbaya%2520and%2520Eduard%2520Frank%2520%25C4%258Eurech%2520and%2520Ido%2520Hakimi%2520and%2520Juan%2520Garc%25C3%25ADa%2520Giraldo%2520and%2520Mete%2520Ismayilzada%2520and%2520Negar%2520Foroutan%2520and%2520Skander%2520Moalla%2520and%2520Tiancheng%2520Chen%2520and%2520Vinko%2520Sabol%25C4%258Dec%2520and%2520Yixuan%2520Xu%2520and%2520Michael%2520Aerni%2520and%2520Badr%2520AlKhamissi%2520and%2520Ines%2520Altemir%2520Marinas%2520and%2520Mohammad%2520Hossein%2520Amani%2520and%2520Matin%2520Ansaripour%2520and%2520Ilia%2520Badanin%2520and%2520Harold%2520Benoit%2520and%2520Emanuela%2520Boros%2520and%2520Nicholas%2520Browning%2520and%2520Fabian%2520B%25C3%25B6sch%2520and%2520Maximilian%2520B%25C3%25B6ther%2520and%2520Niklas%2520Canova%2520and%2520Camille%2520Challier%2520and%2520Clement%2520Charmillot%2520and%2520Jonathan%2520Coles%2520and%2520Jan%2520Deriu%2520and%2520Arnout%2520Devos%2520and%2520Lukas%2520Drescher%2520and%2520Daniil%2520Dzenhaliou%2520and%2520Maud%2520Ehrmann%2520and%2520Dongyang%2520Fan%2520and%2520Simin%2520Fan%2520and%2520Silin%2520Gao%2520and%2520Miguel%2520Gila%2520and%2520Mar%25C3%25ADa%2520Grandury%2520and%2520Diba%2520Hashemi%2520and%2520Alexander%2520Hoyle%2520and%2520Jiaming%2520Jiang%2520and%2520Mark%2520Klein%2520and%2520Andrei%2520Kucharavy%2520and%2520Anastasiia%2520Kucherenko%2520and%2520Frederike%2520L%25C3%25BCbeck%2520and%2520Roman%2520Machacek%2520and%2520Theofilos%2520Manitaras%2520and%2520Andreas%2520Marfurt%2520and%2520Kyle%2520Matoba%2520and%2520Simon%2520Matrenok%2520and%2520Henrique%2520Mendonc%25C3%25A7a%2520and%2520Fawzi%2520Roberto%2520Mohamed%2520and%2520Syrielle%2520Montariol%2520and%2520Luca%2520Mouchel%2520and%2520Sven%2520Najem-Meyer%2520and%2520Jingwei%2520Ni%2520and%2520Gennaro%2520Oliva%2520and%2520Matteo%2520Pagliardini%2520and%2520Elia%2520Palme%2520and%2520Andrei%2520Panferov%2520and%2520L%25C3%25A9o%2520Paoletti%2520and%2520Marco%2520Passerini%2520and%2520Ivan%2520Pavlov%2520and%2520Auguste%2520Poiroux%2520and%2520Kaustubh%2520Ponkshe%2520and%2520Nathan%2520Ranchin%2520and%2520Javi%2520Rando%2520and%2520Mathieu%2520Sauser%2520and%2520Jakhongir%2520Saydaliev%2520and%2520Muhammad%2520Ali%2520Sayfiddinov%2520and%2520Marian%2520Schneider%2520and%2520Stefano%2520Schuppli%2520and%2520Marco%2520Scialanga%2520and%2520Andrei%2520Semenov%2520and%2520Kumar%2520Shridhar%2520and%2520Raghav%2520Singhal%2520and%2520Anna%2520Sotnikova%2520and%2520Alexander%2520Sternfeld%2520and%2520Ayush%2520Kumar%2520Tarun%2520and%2520Paul%2520Teiletche%2520and%2520Jannis%2520Vamvas%2520and%2520Xiaozhe%2520Yao%2520and%2520Hao%2520Zhao%2520Alexander%2520Ilic%2520and%2520Ana%2520Klimovic%2520and%2520Andreas%2520Krause%2520and%2520Caglar%2520Gulcehre%2520and%2520David%2520Rosenthal%2520and%2520Elliott%2520Ash%2520and%2520Florian%2520Tram%25C3%25A8r%2520and%2520Joost%2520VandeVondele%2520and%2520Livio%2520Veraldi%2520and%2520Martin%2520Rajman%2520and%2520Thomas%2520Schulthess%2520and%2520Torsten%2520Hoefler%2520and%2520Antoine%2520Bosselut%2520and%2520Martin%2520Jaggi%2520and%2520Imanol%2520Schlag%26entry.1292438233%3D%2520%2520We%2520present%2520Apertus%252C%2520a%2520fully%2520open%2520suite%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Adesigned%2520to%2520address%2520two%2520systemic%2520shortcomings%2520in%2520today%2527s%2520open%2520model%2520ecosystem%253A%250Adata%2520compliance%2520and%2520multilingual%2520representation.%2520Unlike%2520many%2520prior%2520models%2520that%250Arelease%2520weights%2520without%2520reproducible%2520data%2520pipelines%2520or%2520regard%2520for%2520content-owner%250Arights%252C%2520Apertus%2520models%2520are%2520pretrained%2520exclusively%2520on%2520openly%2520available%2520data%252C%250Aretroactively%2520respecting%2520robots.txt%2520exclusions%2520and%2520filtering%2520for%250Anon-permissive%252C%2520toxic%252C%2520and%2520personally%2520identifiable%2520content.%2520To%2520mitigate%2520risks%250Aof%2520memorization%252C%2520we%2520adopt%2520the%2520Goldfish%2520objective%2520during%2520pretraining%252C%2520strongly%250Asuppressing%2520verbatim%2520recall%2520of%2520data%2520while%2520retaining%2520downstream%2520task%250Aperformance.%2520The%2520Apertus%2520models%2520also%2520expand%2520multilingual%2520coverage%252C%2520training%2520on%250A15T%2520tokens%2520from%2520over%25201800%2520languages%252C%2520with%2520~40%2525%2520of%2520pretraining%2520data%2520allocated%2520to%250Anon-English%2520content.%2520Released%2520at%25208B%2520and%252070B%2520scales%252C%2520Apertus%2520approaches%250Astate-of-the-art%2520results%2520among%2520fully%2520open%2520models%2520on%2520multilingual%2520benchmarks%252C%250Arivalling%2520or%2520surpassing%2520open-weight%2520counterparts.%2520Beyond%2520model%2520weights%252C%2520we%250Arelease%2520all%2520scientific%2520artifacts%2520from%2520our%2520development%2520cycle%2520with%2520a%2520permissive%250Alicense%252C%2520including%2520data%2520preparation%2520scripts%252C%2520checkpoints%252C%2520evaluation%2520suites%252C%250Aand%2520training%2520code%252C%2520enabling%2520transparent%2520audit%2520and%2520extension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apertus%3A%20Democratizing%20Open%20and%20Compliant%20LLMs%20for%20Global%20Language%0A%20%20Environments&entry.906535625=Alejandro%20Hern%C3%A1ndez-Cano%20and%20Alexander%20H%C3%A4gele%20and%20Allen%20Hao%20Huang%20and%20Angelika%20Romanou%20and%20Antoni-Joan%20Solergibert%20and%20Barna%20Pasztor%20and%20Bettina%20Messmer%20and%20Dhia%20Garbaya%20and%20Eduard%20Frank%20%C4%8Eurech%20and%20Ido%20Hakimi%20and%20Juan%20Garc%C3%ADa%20Giraldo%20and%20Mete%20Ismayilzada%20and%20Negar%20Foroutan%20and%20Skander%20Moalla%20and%20Tiancheng%20Chen%20and%20Vinko%20Sabol%C4%8Dec%20and%20Yixuan%20Xu%20and%20Michael%20Aerni%20and%20Badr%20AlKhamissi%20and%20Ines%20Altemir%20Marinas%20and%20Mohammad%20Hossein%20Amani%20and%20Matin%20Ansaripour%20and%20Ilia%20Badanin%20and%20Harold%20Benoit%20and%20Emanuela%20Boros%20and%20Nicholas%20Browning%20and%20Fabian%20B%C3%B6sch%20and%20Maximilian%20B%C3%B6ther%20and%20Niklas%20Canova%20and%20Camille%20Challier%20and%20Clement%20Charmillot%20and%20Jonathan%20Coles%20and%20Jan%20Deriu%20and%20Arnout%20Devos%20and%20Lukas%20Drescher%20and%20Daniil%20Dzenhaliou%20and%20Maud%20Ehrmann%20and%20Dongyang%20Fan%20and%20Simin%20Fan%20and%20Silin%20Gao%20and%20Miguel%20Gila%20and%20Mar%C3%ADa%20Grandury%20and%20Diba%20Hashemi%20and%20Alexander%20Hoyle%20and%20Jiaming%20Jiang%20and%20Mark%20Klein%20and%20Andrei%20Kucharavy%20and%20Anastasiia%20Kucherenko%20and%20Frederike%20L%C3%BCbeck%20and%20Roman%20Machacek%20and%20Theofilos%20Manitaras%20and%20Andreas%20Marfurt%20and%20Kyle%20Matoba%20and%20Simon%20Matrenok%20and%20Henrique%20Mendonc%C3%A7a%20and%20Fawzi%20Roberto%20Mohamed%20and%20Syrielle%20Montariol%20and%20Luca%20Mouchel%20and%20Sven%20Najem-Meyer%20and%20Jingwei%20Ni%20and%20Gennaro%20Oliva%20and%20Matteo%20Pagliardini%20and%20Elia%20Palme%20and%20Andrei%20Panferov%20and%20L%C3%A9o%20Paoletti%20and%20Marco%20Passerini%20and%20Ivan%20Pavlov%20and%20Auguste%20Poiroux%20and%20Kaustubh%20Ponkshe%20and%20Nathan%20Ranchin%20and%20Javi%20Rando%20and%20Mathieu%20Sauser%20and%20Jakhongir%20Saydaliev%20and%20Muhammad%20Ali%20Sayfiddinov%20and%20Marian%20Schneider%20and%20Stefano%20Schuppli%20and%20Marco%20Scialanga%20and%20Andrei%20Semenov%20and%20Kumar%20Shridhar%20and%20Raghav%20Singhal%20and%20Anna%20Sotnikova%20and%20Alexander%20Sternfeld%20and%20Ayush%20Kumar%20Tarun%20and%20Paul%20Teiletche%20and%20Jannis%20Vamvas%20and%20Xiaozhe%20Yao%20and%20Hao%20Zhao%20Alexander%20Ilic%20and%20Ana%20Klimovic%20and%20Andreas%20Krause%20and%20Caglar%20Gulcehre%20and%20David%20Rosenthal%20and%20Elliott%20Ash%20and%20Florian%20Tram%C3%A8r%20and%20Joost%20VandeVondele%20and%20Livio%20Veraldi%20and%20Martin%20Rajman%20and%20Thomas%20Schulthess%20and%20Torsten%20Hoefler%20and%20Antoine%20Bosselut%20and%20Martin%20Jaggi%20and%20Imanol%20Schlag&entry.1292438233=%20%20We%20present%20Apertus%2C%20a%20fully%20open%20suite%20of%20large%20language%20models%20%28LLMs%29%0Adesigned%20to%20address%20two%20systemic%20shortcomings%20in%20today%27s%20open%20model%20ecosystem%3A%0Adata%20compliance%20and%20multilingual%20representation.%20Unlike%20many%20prior%20models%20that%0Arelease%20weights%20without%20reproducible%20data%20pipelines%20or%20regard%20for%20content-owner%0Arights%2C%20Apertus%20models%20are%20pretrained%20exclusively%20on%20openly%20available%20data%2C%0Aretroactively%20respecting%20robots.txt%20exclusions%20and%20filtering%20for%0Anon-permissive%2C%20toxic%2C%20and%20personally%20identifiable%20content.%20To%20mitigate%20risks%0Aof%20memorization%2C%20we%20adopt%20the%20Goldfish%20objective%20during%20pretraining%2C%20strongly%0Asuppressing%20verbatim%20recall%20of%20data%20while%20retaining%20downstream%20task%0Aperformance.%20The%20Apertus%20models%20also%20expand%20multilingual%20coverage%2C%20training%20on%0A15T%20tokens%20from%20over%201800%20languages%2C%20with%20~40%25%20of%20pretraining%20data%20allocated%20to%0Anon-English%20content.%20Released%20at%208B%20and%2070B%20scales%2C%20Apertus%20approaches%0Astate-of-the-art%20results%20among%20fully%20open%20models%20on%20multilingual%20benchmarks%2C%0Arivalling%20or%20surpassing%20open-weight%20counterparts.%20Beyond%20model%20weights%2C%20we%0Arelease%20all%20scientific%20artifacts%20from%20our%20development%20cycle%20with%20a%20permissive%0Alicense%2C%20including%20data%20preparation%20scripts%2C%20checkpoints%2C%20evaluation%20suites%2C%0Aand%20training%20code%2C%20enabling%20transparent%20audit%20and%20extension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14233v1&entry.124074799=Read"},
{"title": "You Are What You Train: Effects of Data Composition on Training\n  Context-aware Machine Translation Models", "author": "Pawe\u0142 M\u0105ka and Yusuf Can Semerci and Jan Scholtes and Gerasimos Spanakis", "abstract": "  Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.\n", "link": "http://arxiv.org/abs/2509.14031v1", "date": "2025-09-17", "relevancy": 2.081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Are%20What%20You%20Train%3A%20Effects%20of%20Data%20Composition%20on%20Training%0A%20%20Context-aware%20Machine%20Translation%20Models&body=Title%3A%20You%20Are%20What%20You%20Train%3A%20Effects%20of%20Data%20Composition%20on%20Training%0A%20%20Context-aware%20Machine%20Translation%20Models%0AAuthor%3A%20Pawe%C5%82%20M%C4%85ka%20and%20Yusuf%20Can%20Semerci%20and%20Jan%20Scholtes%20and%20Gerasimos%20Spanakis%0AAbstract%3A%20%20%20Achieving%20human-level%20translations%20requires%20leveraging%20context%20to%20ensure%0Acoherence%20and%20handle%20complex%20phenomena%20like%20pronoun%20disambiguation.%20Sparsity%20of%0Acontextually%20rich%20examples%20in%20the%20standard%20training%20data%20has%20been%20hypothesized%0Aas%20the%20reason%20for%20the%20difficulty%20of%20context%20utilization.%20In%20this%20work%2C%20we%0Asystematically%20validate%20this%20claim%20in%20both%20single-%20and%20multilingual%20settings%20by%0Aconstructing%20training%20datasets%20with%20a%20controlled%20proportions%20of%20contextually%0Arelevant%20examples.%20We%20demonstrate%20a%20strong%20association%20between%20training%20data%0Asparsity%20and%20model%20performance%20confirming%20sparsity%20as%20a%20key%20bottleneck.%0AImportantly%2C%20we%20reveal%20that%20improvements%20in%20one%20contextual%20phenomenon%20do%20no%0Ageneralize%20to%20others.%20While%20we%20observe%20some%20cross-lingual%20transfer%2C%20it%20is%20not%0Asignificantly%20higher%20between%20languages%20within%20the%20same%20sub-family.%20Finally%2C%20we%0Apropose%20and%20empirically%20evaluate%20two%20training%20strategies%20designed%20to%20leverage%0Athe%20available%20data.%20These%20strategies%20improve%20context%20utilization%2C%20resulting%20in%0Aaccuracy%20gains%20of%20up%20to%206%20and%208%20percentage%20points%20on%20the%20ctxPro%20evaluation%20in%0Asingle-%20and%20multilingual%20settings%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Are%2520What%2520You%2520Train%253A%2520Effects%2520of%2520Data%2520Composition%2520on%2520Training%250A%2520%2520Context-aware%2520Machine%2520Translation%2520Models%26entry.906535625%3DPawe%25C5%2582%2520M%25C4%2585ka%2520and%2520Yusuf%2520Can%2520Semerci%2520and%2520Jan%2520Scholtes%2520and%2520Gerasimos%2520Spanakis%26entry.1292438233%3D%2520%2520Achieving%2520human-level%2520translations%2520requires%2520leveraging%2520context%2520to%2520ensure%250Acoherence%2520and%2520handle%2520complex%2520phenomena%2520like%2520pronoun%2520disambiguation.%2520Sparsity%2520of%250Acontextually%2520rich%2520examples%2520in%2520the%2520standard%2520training%2520data%2520has%2520been%2520hypothesized%250Aas%2520the%2520reason%2520for%2520the%2520difficulty%2520of%2520context%2520utilization.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520validate%2520this%2520claim%2520in%2520both%2520single-%2520and%2520multilingual%2520settings%2520by%250Aconstructing%2520training%2520datasets%2520with%2520a%2520controlled%2520proportions%2520of%2520contextually%250Arelevant%2520examples.%2520We%2520demonstrate%2520a%2520strong%2520association%2520between%2520training%2520data%250Asparsity%2520and%2520model%2520performance%2520confirming%2520sparsity%2520as%2520a%2520key%2520bottleneck.%250AImportantly%252C%2520we%2520reveal%2520that%2520improvements%2520in%2520one%2520contextual%2520phenomenon%2520do%2520no%250Ageneralize%2520to%2520others.%2520While%2520we%2520observe%2520some%2520cross-lingual%2520transfer%252C%2520it%2520is%2520not%250Asignificantly%2520higher%2520between%2520languages%2520within%2520the%2520same%2520sub-family.%2520Finally%252C%2520we%250Apropose%2520and%2520empirically%2520evaluate%2520two%2520training%2520strategies%2520designed%2520to%2520leverage%250Athe%2520available%2520data.%2520These%2520strategies%2520improve%2520context%2520utilization%252C%2520resulting%2520in%250Aaccuracy%2520gains%2520of%2520up%2520to%25206%2520and%25208%2520percentage%2520points%2520on%2520the%2520ctxPro%2520evaluation%2520in%250Asingle-%2520and%2520multilingual%2520settings%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Are%20What%20You%20Train%3A%20Effects%20of%20Data%20Composition%20on%20Training%0A%20%20Context-aware%20Machine%20Translation%20Models&entry.906535625=Pawe%C5%82%20M%C4%85ka%20and%20Yusuf%20Can%20Semerci%20and%20Jan%20Scholtes%20and%20Gerasimos%20Spanakis&entry.1292438233=%20%20Achieving%20human-level%20translations%20requires%20leveraging%20context%20to%20ensure%0Acoherence%20and%20handle%20complex%20phenomena%20like%20pronoun%20disambiguation.%20Sparsity%20of%0Acontextually%20rich%20examples%20in%20the%20standard%20training%20data%20has%20been%20hypothesized%0Aas%20the%20reason%20for%20the%20difficulty%20of%20context%20utilization.%20In%20this%20work%2C%20we%0Asystematically%20validate%20this%20claim%20in%20both%20single-%20and%20multilingual%20settings%20by%0Aconstructing%20training%20datasets%20with%20a%20controlled%20proportions%20of%20contextually%0Arelevant%20examples.%20We%20demonstrate%20a%20strong%20association%20between%20training%20data%0Asparsity%20and%20model%20performance%20confirming%20sparsity%20as%20a%20key%20bottleneck.%0AImportantly%2C%20we%20reveal%20that%20improvements%20in%20one%20contextual%20phenomenon%20do%20no%0Ageneralize%20to%20others.%20While%20we%20observe%20some%20cross-lingual%20transfer%2C%20it%20is%20not%0Asignificantly%20higher%20between%20languages%20within%20the%20same%20sub-family.%20Finally%2C%20we%0Apropose%20and%20empirically%20evaluate%20two%20training%20strategies%20designed%20to%20leverage%0Athe%20available%20data.%20These%20strategies%20improve%20context%20utilization%2C%20resulting%20in%0Aaccuracy%20gains%20of%20up%20to%206%20and%208%20percentage%20points%20on%20the%20ctxPro%20evaluation%20in%0Asingle-%20and%20multilingual%20settings%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14031v1&entry.124074799=Read"},
{"title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling\n  Perspective of Safety Decision Boundary", "author": "Licheng Pan and Yongqi Tong and Xin Zhang and Xiaolu Zhang and Jun Zhou and Zhixuan Chu", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries--a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models' safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios. We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets are available at\nhttps://github.com/Master-PLC/RASS.\n", "link": "http://arxiv.org/abs/2505.18325v3", "date": "2025-09-17", "relevancy": 2.0738, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Mitigating%20Overrefusal%20in%20LLMs%20from%20an%20Unveiling%0A%20%20Perspective%20of%20Safety%20Decision%20Boundary&body=Title%3A%20Understanding%20and%20Mitigating%20Overrefusal%20in%20LLMs%20from%20an%20Unveiling%0A%20%20Perspective%20of%20Safety%20Decision%20Boundary%0AAuthor%3A%20Licheng%20Pan%20and%20Yongqi%20Tong%20and%20Xin%20Zhang%20and%20Xiaolu%20Zhang%20and%20Jun%20Zhou%20and%20Zhixuan%20Chu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Aa%20wide%20range%20of%20tasks%2C%20yet%20they%20often%20refuse%20to%20answer%20legitimate%20queries--a%0Aphenomenon%20known%20as%20overrefusal.%20Overrefusal%20typically%20stems%20from%0Aover-conservative%20safety%20alignment%2C%20causing%20models%20to%20treat%20many%20reasonable%0Aprompts%20as%20potentially%20risky.%20To%20systematically%20understand%20this%20issue%2C%20we%20probe%0Aand%20leverage%20the%20models%27%20safety%20decision%20boundaries%20to%20analyze%20and%20mitigate%0Aoverrefusal.%20Our%20findings%20reveal%20that%20overrefusal%20is%20closely%20tied%20to%0Amisalignment%20at%20these%20boundary%20regions%2C%20where%20models%20struggle%20to%20distinguish%0Asubtle%20differences%20between%20benign%20and%20harmful%20content.%20Building%20on%20these%0Ainsights%2C%20we%20present%20RASS%2C%20an%20automated%20framework%20for%20prompt%20generation%20and%0Aselection%20that%20strategically%20targets%20overrefusal%20prompts%20near%20the%20safety%0Aboundary.%20By%20harnessing%20steering%20vectors%20in%20the%20representation%20space%2C%20RASS%0Aefficiently%20identifies%20and%20curates%20boundary-aligned%20prompts%2C%20enabling%20more%0Aeffective%20and%20targeted%20mitigation%20of%20overrefusal.%20This%20approach%20not%20only%0Aprovides%20a%20more%20precise%20and%20interpretable%20view%20of%20model%20safety%20decisions%20but%0Aalso%20seamlessly%20extends%20to%20multilingual%20scenarios.%20We%20have%20explored%20the%20safety%0Adecision%20boundaries%20of%20various%20LLMs%20and%20construct%20the%20MORBench%20evaluation%20set%0Ato%20facilitate%20robust%20assessment%20of%20model%20safety%20and%20helpfulness%20across%20multiple%0Alanguages.%20Code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/Master-PLC/RASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18325v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Mitigating%2520Overrefusal%2520in%2520LLMs%2520from%2520an%2520Unveiling%250A%2520%2520Perspective%2520of%2520Safety%2520Decision%2520Boundary%26entry.906535625%3DLicheng%2520Pan%2520and%2520Yongqi%2520Tong%2520and%2520Xin%2520Zhang%2520and%2520Xiaolu%2520Zhang%2520and%2520Jun%2520Zhou%2520and%2520Zhixuan%2520Chu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Aa%2520wide%2520range%2520of%2520tasks%252C%2520yet%2520they%2520often%2520refuse%2520to%2520answer%2520legitimate%2520queries--a%250Aphenomenon%2520known%2520as%2520overrefusal.%2520Overrefusal%2520typically%2520stems%2520from%250Aover-conservative%2520safety%2520alignment%252C%2520causing%2520models%2520to%2520treat%2520many%2520reasonable%250Aprompts%2520as%2520potentially%2520risky.%2520To%2520systematically%2520understand%2520this%2520issue%252C%2520we%2520probe%250Aand%2520leverage%2520the%2520models%2527%2520safety%2520decision%2520boundaries%2520to%2520analyze%2520and%2520mitigate%250Aoverrefusal.%2520Our%2520findings%2520reveal%2520that%2520overrefusal%2520is%2520closely%2520tied%2520to%250Amisalignment%2520at%2520these%2520boundary%2520regions%252C%2520where%2520models%2520struggle%2520to%2520distinguish%250Asubtle%2520differences%2520between%2520benign%2520and%2520harmful%2520content.%2520Building%2520on%2520these%250Ainsights%252C%2520we%2520present%2520RASS%252C%2520an%2520automated%2520framework%2520for%2520prompt%2520generation%2520and%250Aselection%2520that%2520strategically%2520targets%2520overrefusal%2520prompts%2520near%2520the%2520safety%250Aboundary.%2520By%2520harnessing%2520steering%2520vectors%2520in%2520the%2520representation%2520space%252C%2520RASS%250Aefficiently%2520identifies%2520and%2520curates%2520boundary-aligned%2520prompts%252C%2520enabling%2520more%250Aeffective%2520and%2520targeted%2520mitigation%2520of%2520overrefusal.%2520This%2520approach%2520not%2520only%250Aprovides%2520a%2520more%2520precise%2520and%2520interpretable%2520view%2520of%2520model%2520safety%2520decisions%2520but%250Aalso%2520seamlessly%2520extends%2520to%2520multilingual%2520scenarios.%2520We%2520have%2520explored%2520the%2520safety%250Adecision%2520boundaries%2520of%2520various%2520LLMs%2520and%2520construct%2520the%2520MORBench%2520evaluation%2520set%250Ato%2520facilitate%2520robust%2520assessment%2520of%2520model%2520safety%2520and%2520helpfulness%2520across%2520multiple%250Alanguages.%2520Code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/Master-PLC/RASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18325v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Mitigating%20Overrefusal%20in%20LLMs%20from%20an%20Unveiling%0A%20%20Perspective%20of%20Safety%20Decision%20Boundary&entry.906535625=Licheng%20Pan%20and%20Yongqi%20Tong%20and%20Xin%20Zhang%20and%20Xiaolu%20Zhang%20and%20Jun%20Zhou%20and%20Zhixuan%20Chu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Aa%20wide%20range%20of%20tasks%2C%20yet%20they%20often%20refuse%20to%20answer%20legitimate%20queries--a%0Aphenomenon%20known%20as%20overrefusal.%20Overrefusal%20typically%20stems%20from%0Aover-conservative%20safety%20alignment%2C%20causing%20models%20to%20treat%20many%20reasonable%0Aprompts%20as%20potentially%20risky.%20To%20systematically%20understand%20this%20issue%2C%20we%20probe%0Aand%20leverage%20the%20models%27%20safety%20decision%20boundaries%20to%20analyze%20and%20mitigate%0Aoverrefusal.%20Our%20findings%20reveal%20that%20overrefusal%20is%20closely%20tied%20to%0Amisalignment%20at%20these%20boundary%20regions%2C%20where%20models%20struggle%20to%20distinguish%0Asubtle%20differences%20between%20benign%20and%20harmful%20content.%20Building%20on%20these%0Ainsights%2C%20we%20present%20RASS%2C%20an%20automated%20framework%20for%20prompt%20generation%20and%0Aselection%20that%20strategically%20targets%20overrefusal%20prompts%20near%20the%20safety%0Aboundary.%20By%20harnessing%20steering%20vectors%20in%20the%20representation%20space%2C%20RASS%0Aefficiently%20identifies%20and%20curates%20boundary-aligned%20prompts%2C%20enabling%20more%0Aeffective%20and%20targeted%20mitigation%20of%20overrefusal.%20This%20approach%20not%20only%0Aprovides%20a%20more%20precise%20and%20interpretable%20view%20of%20model%20safety%20decisions%20but%0Aalso%20seamlessly%20extends%20to%20multilingual%20scenarios.%20We%20have%20explored%20the%20safety%0Adecision%20boundaries%20of%20various%20LLMs%20and%20construct%20the%20MORBench%20evaluation%20set%0Ato%20facilitate%20robust%20assessment%20of%20model%20safety%20and%20helpfulness%20across%20multiple%0Alanguages.%20Code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/Master-PLC/RASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18325v3&entry.124074799=Read"},
{"title": "SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous\n  Parking via End-to-End Offline Reinforcement Learning", "author": "Zewei Yang and Zengqi Peng and Jun Ma", "abstract": "  Autonomous parking is a critical component for achieving safe and efficient\nurban autonomous driving. However, unstructured environments and dynamic\ninteractions pose significant challenges to autonomous parking tasks. To\naddress this problem, we propose SEG-Parking, a novel end-to-end offline\nreinforcement learning (RL) framework to achieve interaction-aware autonomous\nparking. Notably, a specialized parking dataset is constructed for parking\nscenarios, which include those without interference from the opposite vehicle\n(OV) and complex ones involving interactions with the OV. Based on this\ndataset, a goal-conditioned state encoder is pretrained to map the fused\nperception information into the latent space. Then, an offline RL policy is\noptimized with a conservative regularizer that penalizes out-of-distribution\nactions. Extensive closed-loop experiments are conducted in the high-fidelity\nCARLA simulator. Comparative results demonstrate the superior performance of\nour framework with the highest success rate and robust generalization to\nout-of-distribution parking scenarios. The related dataset and source code will\nbe made publicly available after the paper is accepted.\n", "link": "http://arxiv.org/abs/2509.13956v1", "date": "2025-09-17", "relevancy": 2.0642, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEG-Parking%3A%20Towards%20Safe%2C%20Efficient%2C%20and%20Generalizable%20Autonomous%0A%20%20Parking%20via%20End-to-End%20Offline%20Reinforcement%20Learning&body=Title%3A%20SEG-Parking%3A%20Towards%20Safe%2C%20Efficient%2C%20and%20Generalizable%20Autonomous%0A%20%20Parking%20via%20End-to-End%20Offline%20Reinforcement%20Learning%0AAuthor%3A%20Zewei%20Yang%20and%20Zengqi%20Peng%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Autonomous%20parking%20is%20a%20critical%20component%20for%20achieving%20safe%20and%20efficient%0Aurban%20autonomous%20driving.%20However%2C%20unstructured%20environments%20and%20dynamic%0Ainteractions%20pose%20significant%20challenges%20to%20autonomous%20parking%20tasks.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20SEG-Parking%2C%20a%20novel%20end-to-end%20offline%0Areinforcement%20learning%20%28RL%29%20framework%20to%20achieve%20interaction-aware%20autonomous%0Aparking.%20Notably%2C%20a%20specialized%20parking%20dataset%20is%20constructed%20for%20parking%0Ascenarios%2C%20which%20include%20those%20without%20interference%20from%20the%20opposite%20vehicle%0A%28OV%29%20and%20complex%20ones%20involving%20interactions%20with%20the%20OV.%20Based%20on%20this%0Adataset%2C%20a%20goal-conditioned%20state%20encoder%20is%20pretrained%20to%20map%20the%20fused%0Aperception%20information%20into%20the%20latent%20space.%20Then%2C%20an%20offline%20RL%20policy%20is%0Aoptimized%20with%20a%20conservative%20regularizer%20that%20penalizes%20out-of-distribution%0Aactions.%20Extensive%20closed-loop%20experiments%20are%20conducted%20in%20the%20high-fidelity%0ACARLA%20simulator.%20Comparative%20results%20demonstrate%20the%20superior%20performance%20of%0Aour%20framework%20with%20the%20highest%20success%20rate%20and%20robust%20generalization%20to%0Aout-of-distribution%20parking%20scenarios.%20The%20related%20dataset%20and%20source%20code%20will%0Abe%20made%20publicly%20available%20after%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEG-Parking%253A%2520Towards%2520Safe%252C%2520Efficient%252C%2520and%2520Generalizable%2520Autonomous%250A%2520%2520Parking%2520via%2520End-to-End%2520Offline%2520Reinforcement%2520Learning%26entry.906535625%3DZewei%2520Yang%2520and%2520Zengqi%2520Peng%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Autonomous%2520parking%2520is%2520a%2520critical%2520component%2520for%2520achieving%2520safe%2520and%2520efficient%250Aurban%2520autonomous%2520driving.%2520However%252C%2520unstructured%2520environments%2520and%2520dynamic%250Ainteractions%2520pose%2520significant%2520challenges%2520to%2520autonomous%2520parking%2520tasks.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520SEG-Parking%252C%2520a%2520novel%2520end-to-end%2520offline%250Areinforcement%2520learning%2520%2528RL%2529%2520framework%2520to%2520achieve%2520interaction-aware%2520autonomous%250Aparking.%2520Notably%252C%2520a%2520specialized%2520parking%2520dataset%2520is%2520constructed%2520for%2520parking%250Ascenarios%252C%2520which%2520include%2520those%2520without%2520interference%2520from%2520the%2520opposite%2520vehicle%250A%2528OV%2529%2520and%2520complex%2520ones%2520involving%2520interactions%2520with%2520the%2520OV.%2520Based%2520on%2520this%250Adataset%252C%2520a%2520goal-conditioned%2520state%2520encoder%2520is%2520pretrained%2520to%2520map%2520the%2520fused%250Aperception%2520information%2520into%2520the%2520latent%2520space.%2520Then%252C%2520an%2520offline%2520RL%2520policy%2520is%250Aoptimized%2520with%2520a%2520conservative%2520regularizer%2520that%2520penalizes%2520out-of-distribution%250Aactions.%2520Extensive%2520closed-loop%2520experiments%2520are%2520conducted%2520in%2520the%2520high-fidelity%250ACARLA%2520simulator.%2520Comparative%2520results%2520demonstrate%2520the%2520superior%2520performance%2520of%250Aour%2520framework%2520with%2520the%2520highest%2520success%2520rate%2520and%2520robust%2520generalization%2520to%250Aout-of-distribution%2520parking%2520scenarios.%2520The%2520related%2520dataset%2520and%2520source%2520code%2520will%250Abe%2520made%2520publicly%2520available%2520after%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEG-Parking%3A%20Towards%20Safe%2C%20Efficient%2C%20and%20Generalizable%20Autonomous%0A%20%20Parking%20via%20End-to-End%20Offline%20Reinforcement%20Learning&entry.906535625=Zewei%20Yang%20and%20Zengqi%20Peng%20and%20Jun%20Ma&entry.1292438233=%20%20Autonomous%20parking%20is%20a%20critical%20component%20for%20achieving%20safe%20and%20efficient%0Aurban%20autonomous%20driving.%20However%2C%20unstructured%20environments%20and%20dynamic%0Ainteractions%20pose%20significant%20challenges%20to%20autonomous%20parking%20tasks.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20SEG-Parking%2C%20a%20novel%20end-to-end%20offline%0Areinforcement%20learning%20%28RL%29%20framework%20to%20achieve%20interaction-aware%20autonomous%0Aparking.%20Notably%2C%20a%20specialized%20parking%20dataset%20is%20constructed%20for%20parking%0Ascenarios%2C%20which%20include%20those%20without%20interference%20from%20the%20opposite%20vehicle%0A%28OV%29%20and%20complex%20ones%20involving%20interactions%20with%20the%20OV.%20Based%20on%20this%0Adataset%2C%20a%20goal-conditioned%20state%20encoder%20is%20pretrained%20to%20map%20the%20fused%0Aperception%20information%20into%20the%20latent%20space.%20Then%2C%20an%20offline%20RL%20policy%20is%0Aoptimized%20with%20a%20conservative%20regularizer%20that%20penalizes%20out-of-distribution%0Aactions.%20Extensive%20closed-loop%20experiments%20are%20conducted%20in%20the%20high-fidelity%0ACARLA%20simulator.%20Comparative%20results%20demonstrate%20the%20superior%20performance%20of%0Aour%20framework%20with%20the%20highest%20success%20rate%20and%20robust%20generalization%20to%0Aout-of-distribution%20parking%20scenarios.%20The%20related%20dataset%20and%20source%20code%20will%0Abe%20made%20publicly%20available%20after%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13956v1&entry.124074799=Read"},
{"title": "Superpixel-based Color Transfer", "author": "R\u00e9mi Giraud and Vinh-Thong Ta and Nicolas Papadakis", "abstract": "  In this work, we propose a fast superpixel-based color transfer method (SCT)\nbetween two images. Superpixels enable to decrease the image dimension and to\nextract a reduced set of color candidates. We propose to use a fast approximate\nnearest neighbor matching algorithm in which we enforce the match diversity by\nlimiting the selection of the same superpixels. A fusion framework is designed\nto transfer the matched colors, and we demonstrate the improvement obtained\nover exact matching results. Finally, we show that SCT is visually competitive\ncompared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/1903.06010v2", "date": "2025-09-17", "relevancy": 2.0508, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.524}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5196}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superpixel-based%20Color%20Transfer&body=Title%3A%20Superpixel-based%20Color%20Transfer%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20fast%20superpixel-based%20color%20transfer%20method%20%28SCT%29%0Abetween%20two%20images.%20Superpixels%20enable%20to%20decrease%20the%20image%20dimension%20and%20to%0Aextract%20a%20reduced%20set%20of%20color%20candidates.%20We%20propose%20to%20use%20a%20fast%20approximate%0Anearest%20neighbor%20matching%20algorithm%20in%20which%20we%20enforce%20the%20match%20diversity%20by%0Alimiting%20the%20selection%20of%20the%20same%20superpixels.%20A%20fusion%20framework%20is%20designed%0Ato%20transfer%20the%20matched%20colors%2C%20and%20we%20demonstrate%20the%20improvement%20obtained%0Aover%20exact%20matching%20results.%20Finally%2C%20we%20show%20that%20SCT%20is%20visually%20competitive%0Acompared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1903.06010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperpixel-based%2520Color%2520Transfer%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Vinh-Thong%2520Ta%2520and%2520Nicolas%2520Papadakis%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520fast%2520superpixel-based%2520color%2520transfer%2520method%2520%2528SCT%2529%250Abetween%2520two%2520images.%2520Superpixels%2520enable%2520to%2520decrease%2520the%2520image%2520dimension%2520and%2520to%250Aextract%2520a%2520reduced%2520set%2520of%2520color%2520candidates.%2520We%2520propose%2520to%2520use%2520a%2520fast%2520approximate%250Anearest%2520neighbor%2520matching%2520algorithm%2520in%2520which%2520we%2520enforce%2520the%2520match%2520diversity%2520by%250Alimiting%2520the%2520selection%2520of%2520the%2520same%2520superpixels.%2520A%2520fusion%2520framework%2520is%2520designed%250Ato%2520transfer%2520the%2520matched%2520colors%252C%2520and%2520we%2520demonstrate%2520the%2520improvement%2520obtained%250Aover%2520exact%2520matching%2520results.%2520Finally%252C%2520we%2520show%2520that%2520SCT%2520is%2520visually%2520competitive%250Acompared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1903.06010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superpixel-based%20Color%20Transfer&entry.906535625=R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20fast%20superpixel-based%20color%20transfer%20method%20%28SCT%29%0Abetween%20two%20images.%20Superpixels%20enable%20to%20decrease%20the%20image%20dimension%20and%20to%0Aextract%20a%20reduced%20set%20of%20color%20candidates.%20We%20propose%20to%20use%20a%20fast%20approximate%0Anearest%20neighbor%20matching%20algorithm%20in%20which%20we%20enforce%20the%20match%20diversity%20by%0Alimiting%20the%20selection%20of%20the%20same%20superpixels.%20A%20fusion%20framework%20is%20designed%0Ato%20transfer%20the%20matched%20colors%2C%20and%20we%20demonstrate%20the%20improvement%20obtained%0Aover%20exact%20matching%20results.%20Finally%2C%20we%20show%20that%20SCT%20is%20visually%20competitive%0Acompared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/1903.06010v2&entry.124074799=Read"},
{"title": "Deep Temporal Graph Networks for Real-Time Correction of GNSS\n  Jamming-Induced Deviations", "author": "Ivana Kesi\u0107 and Alja\u017e Blatnik and Carolina Fortuna and Bla\u017e Bertalani\u010d", "abstract": "  Global Navigation Satellite Systems (GNSS) are increasingly disrupted by\nintentional jamming, degrading availability precisely when positioning and\ntiming must remain operational. We address this by reframing jamming mitigation\nas dynamic graph regression and introducing a receiver-centric deep temporal\ngraph network that predicts, and thus corrects, the receivers horizontal\ndeviation in real time. At each 1 Hz epoch, the satellite receiver environment\nis represented as a heterogeneous star graph (receiver center, tracked\nsatellites as leaves) with time varying attributes (e.g., SNR, azimuth,\nelevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM\n(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a\nshort history to output the 2D deviation vector applied for on the fly\ncorrection.\n  We evaluate on datasets from two distinct receivers under three jammer\nprofiles, continuous wave (cw), triple tone (cw3), and wideband FM, each\nexercised at six power levels between -45 and -70 dBm, with 50 repetitions per\nscenario (prejam/jam/recovery). Against strong multivariate time series\nbaselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains\nthe lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm\n(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and\n4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode\ndatasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),\noutperforming Seq2Point, MLP, and CNN. A split study shows superior data\nefficiency: with only 10\\% training data our approach remains well ahead of\nbaselines (20 cm vs. 36-42 cm).\n", "link": "http://arxiv.org/abs/2509.14000v1", "date": "2025-09-17", "relevancy": 2.0311, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5248}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5012}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Temporal%20Graph%20Networks%20for%20Real-Time%20Correction%20of%20GNSS%0A%20%20Jamming-Induced%20Deviations&body=Title%3A%20Deep%20Temporal%20Graph%20Networks%20for%20Real-Time%20Correction%20of%20GNSS%0A%20%20Jamming-Induced%20Deviations%0AAuthor%3A%20Ivana%20Kesi%C4%87%20and%20Alja%C5%BE%20Blatnik%20and%20Carolina%20Fortuna%20and%20Bla%C5%BE%20Bertalani%C4%8D%0AAbstract%3A%20%20%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29%20are%20increasingly%20disrupted%20by%0Aintentional%20jamming%2C%20degrading%20availability%20precisely%20when%20positioning%20and%0Atiming%20must%20remain%20operational.%20We%20address%20this%20by%20reframing%20jamming%20mitigation%0Aas%20dynamic%20graph%20regression%20and%20introducing%20a%20receiver-centric%20deep%20temporal%0Agraph%20network%20that%20predicts%2C%20and%20thus%20corrects%2C%20the%20receivers%20horizontal%0Adeviation%20in%20real%20time.%20At%20each%201%20Hz%20epoch%2C%20the%20satellite%20receiver%20environment%0Ais%20represented%20as%20a%20heterogeneous%20star%20graph%20%28receiver%20center%2C%20tracked%0Asatellites%20as%20leaves%29%20with%20time%20varying%20attributes%20%28e.g.%2C%20SNR%2C%20azimuth%2C%0Aelevation%2C%20latitude/longitude%29.%20A%20single%20layer%20Heterogeneous%20Graph%20ConvLSTM%0A%28HeteroGCLSTM%29%20aggregates%20one%20hop%20spatial%20context%20and%20temporal%20dynamics%20over%20a%0Ashort%20history%20to%20output%20the%202D%20deviation%20vector%20applied%20for%20on%20the%20fly%0Acorrection.%0A%20%20We%20evaluate%20on%20datasets%20from%20two%20distinct%20receivers%20under%20three%20jammer%0Aprofiles%2C%20continuous%20wave%20%28cw%29%2C%20triple%20tone%20%28cw3%29%2C%20and%20wideband%20FM%2C%20each%0Aexercised%20at%20six%20power%20levels%20between%20-45%20and%20-70%20dBm%2C%20with%2050%20repetitions%20per%0Ascenario%20%28prejam/jam/recovery%29.%20Against%20strong%20multivariate%20time%20series%0Abaselines%20%28MLP%2C%20uniform%20CNN%2C%20and%20Seq2Point%20CNN%29%2C%20our%20model%20consistently%20attains%0Athe%20lowest%20mean%20absolute%20error%20%28MAE%29.%20At%20-45%20dBm%2C%20it%20achieves%203.64%20cm%0A%28GP01/cw%29%2C%207.74%20cm%20%28GP01/cw3%29%2C%204.41%20cm%20%28ublox/cw%29%2C%204.84%20cm%20%28ublox/cw3%29%2C%20and%0A4.82%20cm%20%28ublox/FM%29%2C%20improving%20to%201.65-2.08%20cm%20by%20-60%20to%20-70%20dBm.%20On%20mixed%20mode%0Adatasets%20pooling%20all%20powers%2C%20MAE%20is%203.78%20cm%20%28GP01%29%20and%204.25%20cm%20%28ublox10%29%2C%0Aoutperforming%20Seq2Point%2C%20MLP%2C%20and%20CNN.%20A%20split%20study%20shows%20superior%20data%0Aefficiency%3A%20with%20only%2010%5C%25%20training%20data%20our%20approach%20remains%20well%20ahead%20of%0Abaselines%20%2820%20cm%20vs.%2036-42%20cm%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Temporal%2520Graph%2520Networks%2520for%2520Real-Time%2520Correction%2520of%2520GNSS%250A%2520%2520Jamming-Induced%2520Deviations%26entry.906535625%3DIvana%2520Kesi%25C4%2587%2520and%2520Alja%25C5%25BE%2520Blatnik%2520and%2520Carolina%2520Fortuna%2520and%2520Bla%25C5%25BE%2520Bertalani%25C4%258D%26entry.1292438233%3D%2520%2520Global%2520Navigation%2520Satellite%2520Systems%2520%2528GNSS%2529%2520are%2520increasingly%2520disrupted%2520by%250Aintentional%2520jamming%252C%2520degrading%2520availability%2520precisely%2520when%2520positioning%2520and%250Atiming%2520must%2520remain%2520operational.%2520We%2520address%2520this%2520by%2520reframing%2520jamming%2520mitigation%250Aas%2520dynamic%2520graph%2520regression%2520and%2520introducing%2520a%2520receiver-centric%2520deep%2520temporal%250Agraph%2520network%2520that%2520predicts%252C%2520and%2520thus%2520corrects%252C%2520the%2520receivers%2520horizontal%250Adeviation%2520in%2520real%2520time.%2520At%2520each%25201%2520Hz%2520epoch%252C%2520the%2520satellite%2520receiver%2520environment%250Ais%2520represented%2520as%2520a%2520heterogeneous%2520star%2520graph%2520%2528receiver%2520center%252C%2520tracked%250Asatellites%2520as%2520leaves%2529%2520with%2520time%2520varying%2520attributes%2520%2528e.g.%252C%2520SNR%252C%2520azimuth%252C%250Aelevation%252C%2520latitude/longitude%2529.%2520A%2520single%2520layer%2520Heterogeneous%2520Graph%2520ConvLSTM%250A%2528HeteroGCLSTM%2529%2520aggregates%2520one%2520hop%2520spatial%2520context%2520and%2520temporal%2520dynamics%2520over%2520a%250Ashort%2520history%2520to%2520output%2520the%25202D%2520deviation%2520vector%2520applied%2520for%2520on%2520the%2520fly%250Acorrection.%250A%2520%2520We%2520evaluate%2520on%2520datasets%2520from%2520two%2520distinct%2520receivers%2520under%2520three%2520jammer%250Aprofiles%252C%2520continuous%2520wave%2520%2528cw%2529%252C%2520triple%2520tone%2520%2528cw3%2529%252C%2520and%2520wideband%2520FM%252C%2520each%250Aexercised%2520at%2520six%2520power%2520levels%2520between%2520-45%2520and%2520-70%2520dBm%252C%2520with%252050%2520repetitions%2520per%250Ascenario%2520%2528prejam/jam/recovery%2529.%2520Against%2520strong%2520multivariate%2520time%2520series%250Abaselines%2520%2528MLP%252C%2520uniform%2520CNN%252C%2520and%2520Seq2Point%2520CNN%2529%252C%2520our%2520model%2520consistently%2520attains%250Athe%2520lowest%2520mean%2520absolute%2520error%2520%2528MAE%2529.%2520At%2520-45%2520dBm%252C%2520it%2520achieves%25203.64%2520cm%250A%2528GP01/cw%2529%252C%25207.74%2520cm%2520%2528GP01/cw3%2529%252C%25204.41%2520cm%2520%2528ublox/cw%2529%252C%25204.84%2520cm%2520%2528ublox/cw3%2529%252C%2520and%250A4.82%2520cm%2520%2528ublox/FM%2529%252C%2520improving%2520to%25201.65-2.08%2520cm%2520by%2520-60%2520to%2520-70%2520dBm.%2520On%2520mixed%2520mode%250Adatasets%2520pooling%2520all%2520powers%252C%2520MAE%2520is%25203.78%2520cm%2520%2528GP01%2529%2520and%25204.25%2520cm%2520%2528ublox10%2529%252C%250Aoutperforming%2520Seq2Point%252C%2520MLP%252C%2520and%2520CNN.%2520A%2520split%2520study%2520shows%2520superior%2520data%250Aefficiency%253A%2520with%2520only%252010%255C%2525%2520training%2520data%2520our%2520approach%2520remains%2520well%2520ahead%2520of%250Abaselines%2520%252820%2520cm%2520vs.%252036-42%2520cm%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Temporal%20Graph%20Networks%20for%20Real-Time%20Correction%20of%20GNSS%0A%20%20Jamming-Induced%20Deviations&entry.906535625=Ivana%20Kesi%C4%87%20and%20Alja%C5%BE%20Blatnik%20and%20Carolina%20Fortuna%20and%20Bla%C5%BE%20Bertalani%C4%8D&entry.1292438233=%20%20Global%20Navigation%20Satellite%20Systems%20%28GNSS%29%20are%20increasingly%20disrupted%20by%0Aintentional%20jamming%2C%20degrading%20availability%20precisely%20when%20positioning%20and%0Atiming%20must%20remain%20operational.%20We%20address%20this%20by%20reframing%20jamming%20mitigation%0Aas%20dynamic%20graph%20regression%20and%20introducing%20a%20receiver-centric%20deep%20temporal%0Agraph%20network%20that%20predicts%2C%20and%20thus%20corrects%2C%20the%20receivers%20horizontal%0Adeviation%20in%20real%20time.%20At%20each%201%20Hz%20epoch%2C%20the%20satellite%20receiver%20environment%0Ais%20represented%20as%20a%20heterogeneous%20star%20graph%20%28receiver%20center%2C%20tracked%0Asatellites%20as%20leaves%29%20with%20time%20varying%20attributes%20%28e.g.%2C%20SNR%2C%20azimuth%2C%0Aelevation%2C%20latitude/longitude%29.%20A%20single%20layer%20Heterogeneous%20Graph%20ConvLSTM%0A%28HeteroGCLSTM%29%20aggregates%20one%20hop%20spatial%20context%20and%20temporal%20dynamics%20over%20a%0Ashort%20history%20to%20output%20the%202D%20deviation%20vector%20applied%20for%20on%20the%20fly%0Acorrection.%0A%20%20We%20evaluate%20on%20datasets%20from%20two%20distinct%20receivers%20under%20three%20jammer%0Aprofiles%2C%20continuous%20wave%20%28cw%29%2C%20triple%20tone%20%28cw3%29%2C%20and%20wideband%20FM%2C%20each%0Aexercised%20at%20six%20power%20levels%20between%20-45%20and%20-70%20dBm%2C%20with%2050%20repetitions%20per%0Ascenario%20%28prejam/jam/recovery%29.%20Against%20strong%20multivariate%20time%20series%0Abaselines%20%28MLP%2C%20uniform%20CNN%2C%20and%20Seq2Point%20CNN%29%2C%20our%20model%20consistently%20attains%0Athe%20lowest%20mean%20absolute%20error%20%28MAE%29.%20At%20-45%20dBm%2C%20it%20achieves%203.64%20cm%0A%28GP01/cw%29%2C%207.74%20cm%20%28GP01/cw3%29%2C%204.41%20cm%20%28ublox/cw%29%2C%204.84%20cm%20%28ublox/cw3%29%2C%20and%0A4.82%20cm%20%28ublox/FM%29%2C%20improving%20to%201.65-2.08%20cm%20by%20-60%20to%20-70%20dBm.%20On%20mixed%20mode%0Adatasets%20pooling%20all%20powers%2C%20MAE%20is%203.78%20cm%20%28GP01%29%20and%204.25%20cm%20%28ublox10%29%2C%0Aoutperforming%20Seq2Point%2C%20MLP%2C%20and%20CNN.%20A%20split%20study%20shows%20superior%20data%0Aefficiency%3A%20with%20only%2010%5C%25%20training%20data%20our%20approach%20remains%20well%20ahead%20of%0Abaselines%20%2820%20cm%20vs.%2036-42%20cm%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14000v1&entry.124074799=Read"},
{"title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak\n  Boundary Polyp Detection", "author": "Zhe Yee Tan and Ashwaq Qasem", "abstract": "  Colorectal polyp segmentation is critical for early detection of colorectal\ncancer, yet weak and low contrast boundaries significantly limit automated\naccuracy. Existing deep models either blur fine edge details or rely on\nhandcrafted filters that perform poorly under variable imaging conditions. We\npropose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects\ndirectional, parameter free Haar wavelet edge maps into each decoder stage to\nrecalibrate semantic features. The key novelties of MEGANet-W include a\ntwo-level Haar wavelet head for multi-orientation edge extraction; and Wavelet\nEdge Guided Attention (W-EGA) modules that fuse wavelet cues with boundary and\ninput branches. On five public polyp datasets, MEGANet-W consistently\noutperforms existing methods, improving mIoU by up to 2.3% and mDice by 1.2%,\nwhile introducing no additional learnable parameters. This approach improves\nreliability in difficult cases and offers a robust solution for medical image\nsegmentation tasks requiring precise boundary detection.\n", "link": "http://arxiv.org/abs/2507.02668v4", "date": "2025-09-17", "relevancy": 2.0228, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5323}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection&body=Title%3A%20MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection%0AAuthor%3A%20Zhe%20Yee%20Tan%20and%20Ashwaq%20Qasem%0AAbstract%3A%20%20%20Colorectal%20polyp%20segmentation%20is%20critical%20for%20early%20detection%20of%20colorectal%0Acancer%2C%20yet%20weak%20and%20low%20contrast%20boundaries%20significantly%20limit%20automated%0Aaccuracy.%20Existing%20deep%20models%20either%20blur%20fine%20edge%20details%20or%20rely%20on%0Ahandcrafted%20filters%20that%20perform%20poorly%20under%20variable%20imaging%20conditions.%20We%0Apropose%20MEGANet-W%2C%20a%20Wavelet%20Driven%20Edge%20Guided%20Attention%20Network%20that%20injects%0Adirectional%2C%20parameter%20free%20Haar%20wavelet%20edge%20maps%20into%20each%20decoder%20stage%20to%0Arecalibrate%20semantic%20features.%20The%20key%20novelties%20of%20MEGANet-W%20include%20a%0Atwo-level%20Haar%20wavelet%20head%20for%20multi-orientation%20edge%20extraction%3B%20and%20Wavelet%0AEdge%20Guided%20Attention%20%28W-EGA%29%20modules%20that%20fuse%20wavelet%20cues%20with%20boundary%20and%0Ainput%20branches.%20On%20five%20public%20polyp%20datasets%2C%20MEGANet-W%20consistently%0Aoutperforms%20existing%20methods%2C%20improving%20mIoU%20by%20up%20to%202.3%25%20and%20mDice%20by%201.2%25%2C%0Awhile%20introducing%20no%20additional%20learnable%20parameters.%20This%20approach%20improves%0Areliability%20in%20difficult%20cases%20and%20offers%20a%20robust%20solution%20for%20medical%20image%0Asegmentation%20tasks%20requiring%20precise%20boundary%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02668v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGANet-W%253A%2520A%2520Wavelet-Driven%2520Edge-Guided%2520Attention%2520Framework%2520for%2520Weak%250A%2520%2520Boundary%2520Polyp%2520Detection%26entry.906535625%3DZhe%2520Yee%2520Tan%2520and%2520Ashwaq%2520Qasem%26entry.1292438233%3D%2520%2520Colorectal%2520polyp%2520segmentation%2520is%2520critical%2520for%2520early%2520detection%2520of%2520colorectal%250Acancer%252C%2520yet%2520weak%2520and%2520low%2520contrast%2520boundaries%2520significantly%2520limit%2520automated%250Aaccuracy.%2520Existing%2520deep%2520models%2520either%2520blur%2520fine%2520edge%2520details%2520or%2520rely%2520on%250Ahandcrafted%2520filters%2520that%2520perform%2520poorly%2520under%2520variable%2520imaging%2520conditions.%2520We%250Apropose%2520MEGANet-W%252C%2520a%2520Wavelet%2520Driven%2520Edge%2520Guided%2520Attention%2520Network%2520that%2520injects%250Adirectional%252C%2520parameter%2520free%2520Haar%2520wavelet%2520edge%2520maps%2520into%2520each%2520decoder%2520stage%2520to%250Arecalibrate%2520semantic%2520features.%2520The%2520key%2520novelties%2520of%2520MEGANet-W%2520include%2520a%250Atwo-level%2520Haar%2520wavelet%2520head%2520for%2520multi-orientation%2520edge%2520extraction%253B%2520and%2520Wavelet%250AEdge%2520Guided%2520Attention%2520%2528W-EGA%2529%2520modules%2520that%2520fuse%2520wavelet%2520cues%2520with%2520boundary%2520and%250Ainput%2520branches.%2520On%2520five%2520public%2520polyp%2520datasets%252C%2520MEGANet-W%2520consistently%250Aoutperforms%2520existing%2520methods%252C%2520improving%2520mIoU%2520by%2520up%2520to%25202.3%2525%2520and%2520mDice%2520by%25201.2%2525%252C%250Awhile%2520introducing%2520no%2520additional%2520learnable%2520parameters.%2520This%2520approach%2520improves%250Areliability%2520in%2520difficult%2520cases%2520and%2520offers%2520a%2520robust%2520solution%2520for%2520medical%2520image%250Asegmentation%2520tasks%2520requiring%2520precise%2520boundary%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02668v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGANet-W%3A%20A%20Wavelet-Driven%20Edge-Guided%20Attention%20Framework%20for%20Weak%0A%20%20Boundary%20Polyp%20Detection&entry.906535625=Zhe%20Yee%20Tan%20and%20Ashwaq%20Qasem&entry.1292438233=%20%20Colorectal%20polyp%20segmentation%20is%20critical%20for%20early%20detection%20of%20colorectal%0Acancer%2C%20yet%20weak%20and%20low%20contrast%20boundaries%20significantly%20limit%20automated%0Aaccuracy.%20Existing%20deep%20models%20either%20blur%20fine%20edge%20details%20or%20rely%20on%0Ahandcrafted%20filters%20that%20perform%20poorly%20under%20variable%20imaging%20conditions.%20We%0Apropose%20MEGANet-W%2C%20a%20Wavelet%20Driven%20Edge%20Guided%20Attention%20Network%20that%20injects%0Adirectional%2C%20parameter%20free%20Haar%20wavelet%20edge%20maps%20into%20each%20decoder%20stage%20to%0Arecalibrate%20semantic%20features.%20The%20key%20novelties%20of%20MEGANet-W%20include%20a%0Atwo-level%20Haar%20wavelet%20head%20for%20multi-orientation%20edge%20extraction%3B%20and%20Wavelet%0AEdge%20Guided%20Attention%20%28W-EGA%29%20modules%20that%20fuse%20wavelet%20cues%20with%20boundary%20and%0Ainput%20branches.%20On%20five%20public%20polyp%20datasets%2C%20MEGANet-W%20consistently%0Aoutperforms%20existing%20methods%2C%20improving%20mIoU%20by%20up%20to%202.3%25%20and%20mDice%20by%201.2%25%2C%0Awhile%20introducing%20no%20additional%20learnable%20parameters.%20This%20approach%20improves%0Areliability%20in%20difficult%20cases%20and%20offers%20a%20robust%20solution%20for%20medical%20image%0Asegmentation%20tasks%20requiring%20precise%20boundary%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02668v4&entry.124074799=Read"},
{"title": "LLM-ABBA: Understanding time series via symbolic approximation", "author": "Erin Carson and Xinye Chen and Cheng Kang", "abstract": "  The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.\n", "link": "http://arxiv.org/abs/2411.18506v4", "date": "2025-09-17", "relevancy": 2.0192, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5192}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-ABBA%3A%20Understanding%20time%20series%20via%20symbolic%20approximation&body=Title%3A%20LLM-ABBA%3A%20Understanding%20time%20series%20via%20symbolic%20approximation%0AAuthor%3A%20Erin%20Carson%20and%20Xinye%20Chen%20and%20Cheng%20Kang%0AAbstract%3A%20%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20for%20time%20series%20has%20been%0Ademonstrated%20in%20previous%20work.%20Utilizing%20a%20symbolic%20time%20series%20representation%2C%0Aone%20can%20efficiently%20bridge%20the%20gap%20between%20LLMs%20and%20time%20series.%20However%2C%20the%0Aremaining%20challenge%20is%20to%20exploit%20the%20semantic%20information%20hidden%20in%20time%0Aseries%20by%20using%20symbols%20or%20existing%20tokens%20of%20LLMs%2C%20while%20aligning%20the%0Aembedding%20space%20of%20LLMs%20according%20to%20the%20hidden%20information%20of%20time%20series.%20The%0Asymbolic%20time%20series%20approximation%20%28STSA%29%20method%20called%20adaptive%20Brownian%0Abridge-based%20symbolic%20aggregation%20%28ABBA%29%20shows%20outstanding%20efficacy%20in%0Apreserving%20salient%20time%20series%20features%20by%20modeling%20time%20series%20patterns%20in%0Aterms%20of%20amplitude%20and%20period%20while%20using%20existing%20tokens%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%2C%20called%20LLM-ABBA%2C%20that%20integrates%20ABBA%0Ainto%20large%20language%20models%20for%20various%20downstream%20time%20series%20tasks.%20By%0Asymbolizing%20time%20series%2C%20LLM-ABBA%20compares%20favorably%20to%20the%20recent%0Astate-of-the-art%20%28SOTA%29%20in%20UCR%20and%20three%20medical%20time%20series%20classification%0Atasks.%20Meanwhile%2C%20a%20fixed-polygonal%20chain%20trick%20in%20ABBA%20is%20introduced%20to%0A%5Ckc%7Bavoid%20obvious%20drifting%7D%20during%20prediction%20tasks%20by%20significantly%20mitigating%0Athe%20effects%20of%20cumulative%20error%20arising%20from%20misused%20symbols%20during%20the%0Atransition%20from%20symbols%20to%20numerical%20values.%20In%20time%20series%20regression%20tasks%2C%0ALLM-ABBA%20achieves%20the%20new%20SOTA%20on%20Time%20Series%20Extrinsic%20Regression%20%28TSER%29%0Abenchmarks.%20LLM-ABBA%20also%20shows%20competitive%20prediction%20capability%20compared%20to%0Arecent%20SOTA%20time%20series%20prediction%20results.%20We%20believe%20this%20framework%20can%20also%0Aseamlessly%20extend%20to%20other%20time%20series%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18506v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-ABBA%253A%2520Understanding%2520time%2520series%2520via%2520symbolic%2520approximation%26entry.906535625%3DErin%2520Carson%2520and%2520Xinye%2520Chen%2520and%2520Cheng%2520Kang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520time%2520series%2520has%2520been%250Ademonstrated%2520in%2520previous%2520work.%2520Utilizing%2520a%2520symbolic%2520time%2520series%2520representation%252C%250Aone%2520can%2520efficiently%2520bridge%2520the%2520gap%2520between%2520LLMs%2520and%2520time%2520series.%2520However%252C%2520the%250Aremaining%2520challenge%2520is%2520to%2520exploit%2520the%2520semantic%2520information%2520hidden%2520in%2520time%250Aseries%2520by%2520using%2520symbols%2520or%2520existing%2520tokens%2520of%2520LLMs%252C%2520while%2520aligning%2520the%250Aembedding%2520space%2520of%2520LLMs%2520according%2520to%2520the%2520hidden%2520information%2520of%2520time%2520series.%2520The%250Asymbolic%2520time%2520series%2520approximation%2520%2528STSA%2529%2520method%2520called%2520adaptive%2520Brownian%250Abridge-based%2520symbolic%2520aggregation%2520%2528ABBA%2529%2520shows%2520outstanding%2520efficacy%2520in%250Apreserving%2520salient%2520time%2520series%2520features%2520by%2520modeling%2520time%2520series%2520patterns%2520in%250Aterms%2520of%2520amplitude%2520and%2520period%2520while%2520using%2520existing%2520tokens%2520of%2520LLMs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%252C%2520called%2520LLM-ABBA%252C%2520that%2520integrates%2520ABBA%250Ainto%2520large%2520language%2520models%2520for%2520various%2520downstream%2520time%2520series%2520tasks.%2520By%250Asymbolizing%2520time%2520series%252C%2520LLM-ABBA%2520compares%2520favorably%2520to%2520the%2520recent%250Astate-of-the-art%2520%2528SOTA%2529%2520in%2520UCR%2520and%2520three%2520medical%2520time%2520series%2520classification%250Atasks.%2520Meanwhile%252C%2520a%2520fixed-polygonal%2520chain%2520trick%2520in%2520ABBA%2520is%2520introduced%2520to%250A%255Ckc%257Bavoid%2520obvious%2520drifting%257D%2520during%2520prediction%2520tasks%2520by%2520significantly%2520mitigating%250Athe%2520effects%2520of%2520cumulative%2520error%2520arising%2520from%2520misused%2520symbols%2520during%2520the%250Atransition%2520from%2520symbols%2520to%2520numerical%2520values.%2520In%2520time%2520series%2520regression%2520tasks%252C%250ALLM-ABBA%2520achieves%2520the%2520new%2520SOTA%2520on%2520Time%2520Series%2520Extrinsic%2520Regression%2520%2528TSER%2529%250Abenchmarks.%2520LLM-ABBA%2520also%2520shows%2520competitive%2520prediction%2520capability%2520compared%2520to%250Arecent%2520SOTA%2520time%2520series%2520prediction%2520results.%2520We%2520believe%2520this%2520framework%2520can%2520also%250Aseamlessly%2520extend%2520to%2520other%2520time%2520series%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18506v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-ABBA%3A%20Understanding%20time%20series%20via%20symbolic%20approximation&entry.906535625=Erin%20Carson%20and%20Xinye%20Chen%20and%20Cheng%20Kang&entry.1292438233=%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20for%20time%20series%20has%20been%0Ademonstrated%20in%20previous%20work.%20Utilizing%20a%20symbolic%20time%20series%20representation%2C%0Aone%20can%20efficiently%20bridge%20the%20gap%20between%20LLMs%20and%20time%20series.%20However%2C%20the%0Aremaining%20challenge%20is%20to%20exploit%20the%20semantic%20information%20hidden%20in%20time%0Aseries%20by%20using%20symbols%20or%20existing%20tokens%20of%20LLMs%2C%20while%20aligning%20the%0Aembedding%20space%20of%20LLMs%20according%20to%20the%20hidden%20information%20of%20time%20series.%20The%0Asymbolic%20time%20series%20approximation%20%28STSA%29%20method%20called%20adaptive%20Brownian%0Abridge-based%20symbolic%20aggregation%20%28ABBA%29%20shows%20outstanding%20efficacy%20in%0Apreserving%20salient%20time%20series%20features%20by%20modeling%20time%20series%20patterns%20in%0Aterms%20of%20amplitude%20and%20period%20while%20using%20existing%20tokens%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20method%2C%20called%20LLM-ABBA%2C%20that%20integrates%20ABBA%0Ainto%20large%20language%20models%20for%20various%20downstream%20time%20series%20tasks.%20By%0Asymbolizing%20time%20series%2C%20LLM-ABBA%20compares%20favorably%20to%20the%20recent%0Astate-of-the-art%20%28SOTA%29%20in%20UCR%20and%20three%20medical%20time%20series%20classification%0Atasks.%20Meanwhile%2C%20a%20fixed-polygonal%20chain%20trick%20in%20ABBA%20is%20introduced%20to%0A%5Ckc%7Bavoid%20obvious%20drifting%7D%20during%20prediction%20tasks%20by%20significantly%20mitigating%0Athe%20effects%20of%20cumulative%20error%20arising%20from%20misused%20symbols%20during%20the%0Atransition%20from%20symbols%20to%20numerical%20values.%20In%20time%20series%20regression%20tasks%2C%0ALLM-ABBA%20achieves%20the%20new%20SOTA%20on%20Time%20Series%20Extrinsic%20Regression%20%28TSER%29%0Abenchmarks.%20LLM-ABBA%20also%20shows%20competitive%20prediction%20capability%20compared%20to%0Arecent%20SOTA%20time%20series%20prediction%20results.%20We%20believe%20this%20framework%20can%20also%0Aseamlessly%20extend%20to%20other%20time%20series%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18506v4&entry.124074799=Read"},
{"title": "Text-to-Speech for Unseen Speakers via Low-Complexity Discrete\n  Unit-Based Frame Selection", "author": "Ismail Rasim Ulgen and Shreeram Suresh Chandra and Junchen Lu and Berrak Sisman", "abstract": "  Synthesizing the voices of unseen speakers remains a persisting challenge in\nmulti-speaker text-to-speech (TTS). Existing methods model speaker\ncharacteristics through speaker conditioning during training, leading to\nincreased model complexity and limiting reproducibility and accessibility. A\nlow-complexity alternative would broaden the reach of speech synthesis\nresearch, particularly in settings with limited computational and data\nresources. To this end, we propose SelectTTS, a simple and effective\nalternative. SelectTTS selects appropriate frames from the target speaker and\ndecodes them using frame-level self-supervised learning (SSL) features. We\ndemonstrate that this approach can effectively capture speaker characteristics\nfor unseen speakers and achieves performance comparable to state-of-the-art\nmulti-speaker TTS frameworks on both objective and subjective metrics. By\ndirectly selecting frames from the target speaker's speech, SelectTTS enables\ngeneralization to unseen speakers with significantly lower model complexity.\nExperimental results show that the proposed approach achieves performance\ncomparable to state-of-the-art systems such as XTTS-v2 and VALL-E, while\nrequiring over 8x fewer parameters and 270x less training data. Moreover, it\ndemonstrates that frame selection with SSL features offers an efficient path to\nlow-complexity, high-quality multi-speaker TTS.\n", "link": "http://arxiv.org/abs/2408.17432v3", "date": "2025-09-17", "relevancy": 2.0171, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5255}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Speech%20for%20Unseen%20Speakers%20via%20Low-Complexity%20Discrete%0A%20%20Unit-Based%20Frame%20Selection&body=Title%3A%20Text-to-Speech%20for%20Unseen%20Speakers%20via%20Low-Complexity%20Discrete%0A%20%20Unit-Based%20Frame%20Selection%0AAuthor%3A%20Ismail%20Rasim%20Ulgen%20and%20Shreeram%20Suresh%20Chandra%20and%20Junchen%20Lu%20and%20Berrak%20Sisman%0AAbstract%3A%20%20%20Synthesizing%20the%20voices%20of%20unseen%20speakers%20remains%20a%20persisting%20challenge%20in%0Amulti-speaker%20text-to-speech%20%28TTS%29.%20Existing%20methods%20model%20speaker%0Acharacteristics%20through%20speaker%20conditioning%20during%20training%2C%20leading%20to%0Aincreased%20model%20complexity%20and%20limiting%20reproducibility%20and%20accessibility.%20A%0Alow-complexity%20alternative%20would%20broaden%20the%20reach%20of%20speech%20synthesis%0Aresearch%2C%20particularly%20in%20settings%20with%20limited%20computational%20and%20data%0Aresources.%20To%20this%20end%2C%20we%20propose%20SelectTTS%2C%20a%20simple%20and%20effective%0Aalternative.%20SelectTTS%20selects%20appropriate%20frames%20from%20the%20target%20speaker%20and%0Adecodes%20them%20using%20frame-level%20self-supervised%20learning%20%28SSL%29%20features.%20We%0Ademonstrate%20that%20this%20approach%20can%20effectively%20capture%20speaker%20characteristics%0Afor%20unseen%20speakers%20and%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amulti-speaker%20TTS%20frameworks%20on%20both%20objective%20and%20subjective%20metrics.%20By%0Adirectly%20selecting%20frames%20from%20the%20target%20speaker%27s%20speech%2C%20SelectTTS%20enables%0Ageneralization%20to%20unseen%20speakers%20with%20significantly%20lower%20model%20complexity.%0AExperimental%20results%20show%20that%20the%20proposed%20approach%20achieves%20performance%0Acomparable%20to%20state-of-the-art%20systems%20such%20as%20XTTS-v2%20and%20VALL-E%2C%20while%0Arequiring%20over%208x%20fewer%20parameters%20and%20270x%20less%20training%20data.%20Moreover%2C%20it%0Ademonstrates%20that%20frame%20selection%20with%20SSL%20features%20offers%20an%20efficient%20path%20to%0Alow-complexity%2C%20high-quality%20multi-speaker%20TTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Speech%2520for%2520Unseen%2520Speakers%2520via%2520Low-Complexity%2520Discrete%250A%2520%2520Unit-Based%2520Frame%2520Selection%26entry.906535625%3DIsmail%2520Rasim%2520Ulgen%2520and%2520Shreeram%2520Suresh%2520Chandra%2520and%2520Junchen%2520Lu%2520and%2520Berrak%2520Sisman%26entry.1292438233%3D%2520%2520Synthesizing%2520the%2520voices%2520of%2520unseen%2520speakers%2520remains%2520a%2520persisting%2520challenge%2520in%250Amulti-speaker%2520text-to-speech%2520%2528TTS%2529.%2520Existing%2520methods%2520model%2520speaker%250Acharacteristics%2520through%2520speaker%2520conditioning%2520during%2520training%252C%2520leading%2520to%250Aincreased%2520model%2520complexity%2520and%2520limiting%2520reproducibility%2520and%2520accessibility.%2520A%250Alow-complexity%2520alternative%2520would%2520broaden%2520the%2520reach%2520of%2520speech%2520synthesis%250Aresearch%252C%2520particularly%2520in%2520settings%2520with%2520limited%2520computational%2520and%2520data%250Aresources.%2520To%2520this%2520end%252C%2520we%2520propose%2520SelectTTS%252C%2520a%2520simple%2520and%2520effective%250Aalternative.%2520SelectTTS%2520selects%2520appropriate%2520frames%2520from%2520the%2520target%2520speaker%2520and%250Adecodes%2520them%2520using%2520frame-level%2520self-supervised%2520learning%2520%2528SSL%2529%2520features.%2520We%250Ademonstrate%2520that%2520this%2520approach%2520can%2520effectively%2520capture%2520speaker%2520characteristics%250Afor%2520unseen%2520speakers%2520and%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%250Amulti-speaker%2520TTS%2520frameworks%2520on%2520both%2520objective%2520and%2520subjective%2520metrics.%2520By%250Adirectly%2520selecting%2520frames%2520from%2520the%2520target%2520speaker%2527s%2520speech%252C%2520SelectTTS%2520enables%250Ageneralization%2520to%2520unseen%2520speakers%2520with%2520significantly%2520lower%2520model%2520complexity.%250AExperimental%2520results%2520show%2520that%2520the%2520proposed%2520approach%2520achieves%2520performance%250Acomparable%2520to%2520state-of-the-art%2520systems%2520such%2520as%2520XTTS-v2%2520and%2520VALL-E%252C%2520while%250Arequiring%2520over%25208x%2520fewer%2520parameters%2520and%2520270x%2520less%2520training%2520data.%2520Moreover%252C%2520it%250Ademonstrates%2520that%2520frame%2520selection%2520with%2520SSL%2520features%2520offers%2520an%2520efficient%2520path%2520to%250Alow-complexity%252C%2520high-quality%2520multi-speaker%2520TTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Speech%20for%20Unseen%20Speakers%20via%20Low-Complexity%20Discrete%0A%20%20Unit-Based%20Frame%20Selection&entry.906535625=Ismail%20Rasim%20Ulgen%20and%20Shreeram%20Suresh%20Chandra%20and%20Junchen%20Lu%20and%20Berrak%20Sisman&entry.1292438233=%20%20Synthesizing%20the%20voices%20of%20unseen%20speakers%20remains%20a%20persisting%20challenge%20in%0Amulti-speaker%20text-to-speech%20%28TTS%29.%20Existing%20methods%20model%20speaker%0Acharacteristics%20through%20speaker%20conditioning%20during%20training%2C%20leading%20to%0Aincreased%20model%20complexity%20and%20limiting%20reproducibility%20and%20accessibility.%20A%0Alow-complexity%20alternative%20would%20broaden%20the%20reach%20of%20speech%20synthesis%0Aresearch%2C%20particularly%20in%20settings%20with%20limited%20computational%20and%20data%0Aresources.%20To%20this%20end%2C%20we%20propose%20SelectTTS%2C%20a%20simple%20and%20effective%0Aalternative.%20SelectTTS%20selects%20appropriate%20frames%20from%20the%20target%20speaker%20and%0Adecodes%20them%20using%20frame-level%20self-supervised%20learning%20%28SSL%29%20features.%20We%0Ademonstrate%20that%20this%20approach%20can%20effectively%20capture%20speaker%20characteristics%0Afor%20unseen%20speakers%20and%20achieves%20performance%20comparable%20to%20state-of-the-art%0Amulti-speaker%20TTS%20frameworks%20on%20both%20objective%20and%20subjective%20metrics.%20By%0Adirectly%20selecting%20frames%20from%20the%20target%20speaker%27s%20speech%2C%20SelectTTS%20enables%0Ageneralization%20to%20unseen%20speakers%20with%20significantly%20lower%20model%20complexity.%0AExperimental%20results%20show%20that%20the%20proposed%20approach%20achieves%20performance%0Acomparable%20to%20state-of-the-art%20systems%20such%20as%20XTTS-v2%20and%20VALL-E%2C%20while%0Arequiring%20over%208x%20fewer%20parameters%20and%20270x%20less%20training%20data.%20Moreover%2C%20it%0Ademonstrates%20that%20frame%20selection%20with%20SSL%20features%20offers%20an%20efficient%20path%20to%0Alow-complexity%2C%20high-quality%20multi-speaker%20TTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17432v3&entry.124074799=Read"},
{"title": "CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative\n  Aerial Transport of Cable-Suspended Payloads", "author": "Viktor Lorentz and Khaled Wahba and Sayantan Auddy and Marc Toussaint and Wolfgang H\u00f6nig", "abstract": "  Collaborative transportation of cable-suspended payloads by teams of Unmanned\nAerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to\ndifferent payload shapes, and provide built-in compliance, making it attractive\nfor applications ranging from disaster relief to precision logistics. However,\nmulti-UAV coordination under disturbances, nonlinear payload dynamics, and\nslack--taut cable modes remains a challenging control problem. To our\nknowledge, no prior work has addressed these cable mode transitions in the\nmulti-UAV context, instead relying on simplifying rigid-link assumptions. We\npropose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for\nmulti-UAV cable-suspended payload transport. Simulation results demonstrate\nthat the learned policies can outperform classical decentralized controllers in\nterms of disturbance rejection and tracking precision, achieving an 80%\nrecovery rate from harsh conditions compared to 44% for the baseline method. We\nalso achieve successful zero-shot sim-to-real transfer and demonstrate that our\npolicies are highly robust under harsh conditions, including wind, random\nexternal disturbances, and transitions between slack and taut cable dynamics.\nThis work paves the way for autonomous, resilient UAV teams capable of\nexecuting complex payload missions in unstructured environments.\n", "link": "http://arxiv.org/abs/2509.14126v1", "date": "2025-09-17", "relevancy": 1.9976, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5023}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrazyMARL%3A%20Decentralized%20Direct%20Motor%20Control%20Policies%20for%20Cooperative%0A%20%20Aerial%20Transport%20of%20Cable-Suspended%20Payloads&body=Title%3A%20CrazyMARL%3A%20Decentralized%20Direct%20Motor%20Control%20Policies%20for%20Cooperative%0A%20%20Aerial%20Transport%20of%20Cable-Suspended%20Payloads%0AAuthor%3A%20Viktor%20Lorentz%20and%20Khaled%20Wahba%20and%20Sayantan%20Auddy%20and%20Marc%20Toussaint%20and%20Wolfgang%20H%C3%B6nig%0AAbstract%3A%20%20%20Collaborative%20transportation%20of%20cable-suspended%20payloads%20by%20teams%20of%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20has%20the%20potential%20to%20enhance%20payload%20capacity%2C%20adapt%20to%0Adifferent%20payload%20shapes%2C%20and%20provide%20built-in%20compliance%2C%20making%20it%20attractive%0Afor%20applications%20ranging%20from%20disaster%20relief%20to%20precision%20logistics.%20However%2C%0Amulti-UAV%20coordination%20under%20disturbances%2C%20nonlinear%20payload%20dynamics%2C%20and%0Aslack--taut%20cable%20modes%20remains%20a%20challenging%20control%20problem.%20To%20our%0Aknowledge%2C%20no%20prior%20work%20has%20addressed%20these%20cable%20mode%20transitions%20in%20the%0Amulti-UAV%20context%2C%20instead%20relying%20on%20simplifying%20rigid-link%20assumptions.%20We%0Apropose%20CrazyMARL%2C%20a%20decentralized%20Reinforcement%20Learning%20%28RL%29%20framework%20for%0Amulti-UAV%20cable-suspended%20payload%20transport.%20Simulation%20results%20demonstrate%0Athat%20the%20learned%20policies%20can%20outperform%20classical%20decentralized%20controllers%20in%0Aterms%20of%20disturbance%20rejection%20and%20tracking%20precision%2C%20achieving%20an%2080%25%0Arecovery%20rate%20from%20harsh%20conditions%20compared%20to%2044%25%20for%20the%20baseline%20method.%20We%0Aalso%20achieve%20successful%20zero-shot%20sim-to-real%20transfer%20and%20demonstrate%20that%20our%0Apolicies%20are%20highly%20robust%20under%20harsh%20conditions%2C%20including%20wind%2C%20random%0Aexternal%20disturbances%2C%20and%20transitions%20between%20slack%20and%20taut%20cable%20dynamics.%0AThis%20work%20paves%20the%20way%20for%20autonomous%2C%20resilient%20UAV%20teams%20capable%20of%0Aexecuting%20complex%20payload%20missions%20in%20unstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrazyMARL%253A%2520Decentralized%2520Direct%2520Motor%2520Control%2520Policies%2520for%2520Cooperative%250A%2520%2520Aerial%2520Transport%2520of%2520Cable-Suspended%2520Payloads%26entry.906535625%3DViktor%2520Lorentz%2520and%2520Khaled%2520Wahba%2520and%2520Sayantan%2520Auddy%2520and%2520Marc%2520Toussaint%2520and%2520Wolfgang%2520H%25C3%25B6nig%26entry.1292438233%3D%2520%2520Collaborative%2520transportation%2520of%2520cable-suspended%2520payloads%2520by%2520teams%2520of%2520Unmanned%250AAerial%2520Vehicles%2520%2528UAVs%2529%2520has%2520the%2520potential%2520to%2520enhance%2520payload%2520capacity%252C%2520adapt%2520to%250Adifferent%2520payload%2520shapes%252C%2520and%2520provide%2520built-in%2520compliance%252C%2520making%2520it%2520attractive%250Afor%2520applications%2520ranging%2520from%2520disaster%2520relief%2520to%2520precision%2520logistics.%2520However%252C%250Amulti-UAV%2520coordination%2520under%2520disturbances%252C%2520nonlinear%2520payload%2520dynamics%252C%2520and%250Aslack--taut%2520cable%2520modes%2520remains%2520a%2520challenging%2520control%2520problem.%2520To%2520our%250Aknowledge%252C%2520no%2520prior%2520work%2520has%2520addressed%2520these%2520cable%2520mode%2520transitions%2520in%2520the%250Amulti-UAV%2520context%252C%2520instead%2520relying%2520on%2520simplifying%2520rigid-link%2520assumptions.%2520We%250Apropose%2520CrazyMARL%252C%2520a%2520decentralized%2520Reinforcement%2520Learning%2520%2528RL%2529%2520framework%2520for%250Amulti-UAV%2520cable-suspended%2520payload%2520transport.%2520Simulation%2520results%2520demonstrate%250Athat%2520the%2520learned%2520policies%2520can%2520outperform%2520classical%2520decentralized%2520controllers%2520in%250Aterms%2520of%2520disturbance%2520rejection%2520and%2520tracking%2520precision%252C%2520achieving%2520an%252080%2525%250Arecovery%2520rate%2520from%2520harsh%2520conditions%2520compared%2520to%252044%2525%2520for%2520the%2520baseline%2520method.%2520We%250Aalso%2520achieve%2520successful%2520zero-shot%2520sim-to-real%2520transfer%2520and%2520demonstrate%2520that%2520our%250Apolicies%2520are%2520highly%2520robust%2520under%2520harsh%2520conditions%252C%2520including%2520wind%252C%2520random%250Aexternal%2520disturbances%252C%2520and%2520transitions%2520between%2520slack%2520and%2520taut%2520cable%2520dynamics.%250AThis%2520work%2520paves%2520the%2520way%2520for%2520autonomous%252C%2520resilient%2520UAV%2520teams%2520capable%2520of%250Aexecuting%2520complex%2520payload%2520missions%2520in%2520unstructured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrazyMARL%3A%20Decentralized%20Direct%20Motor%20Control%20Policies%20for%20Cooperative%0A%20%20Aerial%20Transport%20of%20Cable-Suspended%20Payloads&entry.906535625=Viktor%20Lorentz%20and%20Khaled%20Wahba%20and%20Sayantan%20Auddy%20and%20Marc%20Toussaint%20and%20Wolfgang%20H%C3%B6nig&entry.1292438233=%20%20Collaborative%20transportation%20of%20cable-suspended%20payloads%20by%20teams%20of%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20has%20the%20potential%20to%20enhance%20payload%20capacity%2C%20adapt%20to%0Adifferent%20payload%20shapes%2C%20and%20provide%20built-in%20compliance%2C%20making%20it%20attractive%0Afor%20applications%20ranging%20from%20disaster%20relief%20to%20precision%20logistics.%20However%2C%0Amulti-UAV%20coordination%20under%20disturbances%2C%20nonlinear%20payload%20dynamics%2C%20and%0Aslack--taut%20cable%20modes%20remains%20a%20challenging%20control%20problem.%20To%20our%0Aknowledge%2C%20no%20prior%20work%20has%20addressed%20these%20cable%20mode%20transitions%20in%20the%0Amulti-UAV%20context%2C%20instead%20relying%20on%20simplifying%20rigid-link%20assumptions.%20We%0Apropose%20CrazyMARL%2C%20a%20decentralized%20Reinforcement%20Learning%20%28RL%29%20framework%20for%0Amulti-UAV%20cable-suspended%20payload%20transport.%20Simulation%20results%20demonstrate%0Athat%20the%20learned%20policies%20can%20outperform%20classical%20decentralized%20controllers%20in%0Aterms%20of%20disturbance%20rejection%20and%20tracking%20precision%2C%20achieving%20an%2080%25%0Arecovery%20rate%20from%20harsh%20conditions%20compared%20to%2044%25%20for%20the%20baseline%20method.%20We%0Aalso%20achieve%20successful%20zero-shot%20sim-to-real%20transfer%20and%20demonstrate%20that%20our%0Apolicies%20are%20highly%20robust%20under%20harsh%20conditions%2C%20including%20wind%2C%20random%0Aexternal%20disturbances%2C%20and%20transitions%20between%20slack%20and%20taut%20cable%20dynamics.%0AThis%20work%20paves%20the%20way%20for%20autonomous%2C%20resilient%20UAV%20teams%20capable%20of%0Aexecuting%20complex%20payload%20missions%20in%20unstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14126v1&entry.124074799=Read"},
{"title": "Spacing Test for Fused Lasso", "author": "Rieko Tasaka and Tatsuya Kimura and Joe Suzuki", "abstract": "  This study addresses the unresolved problem of selecting the regularization\nparameter in the fused lasso. In particular, we extend the framework of the\nSpacing Test proposed by Tibshirani et al. to the fused lasso, providing a\ntheoretical foundation for post-selection inference by characterizing the\nselection event as a polyhedral constraint. Based on the analysis of the\nsolution path of the fused lasso using a LARS-type algorithm, we derive exact\nconditional $p$-values for the selected change-points. Our method broadens the\napplicability of the Spacing Test from the standard lasso to fused penalty\nstructures. Furthermore, through numerical experiments comparing the proposed\nmethod with sequential versions of AIC and BIC as well as cross-validation, we\ndemonstrate that the proposed approach properly controls the type I error while\nachieving high detection power. This work offers a theoretically sound and\ncomputationally practical solution for parameter selection and post-selection\ninference in structured signal estimation problems. Keywords: Fused Lasso,\nRegularization parameter selection, Spacing Test for Lasso, Selective\ninference, Change-point detection\n", "link": "http://arxiv.org/abs/2509.14229v1", "date": "2025-09-17", "relevancy": 1.9968, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4132}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.407}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spacing%20Test%20for%20Fused%20Lasso&body=Title%3A%20Spacing%20Test%20for%20Fused%20Lasso%0AAuthor%3A%20Rieko%20Tasaka%20and%20Tatsuya%20Kimura%20and%20Joe%20Suzuki%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20unresolved%20problem%20of%20selecting%20the%20regularization%0Aparameter%20in%20the%20fused%20lasso.%20In%20particular%2C%20we%20extend%20the%20framework%20of%20the%0ASpacing%20Test%20proposed%20by%20Tibshirani%20et%20al.%20to%20the%20fused%20lasso%2C%20providing%20a%0Atheoretical%20foundation%20for%20post-selection%20inference%20by%20characterizing%20the%0Aselection%20event%20as%20a%20polyhedral%20constraint.%20Based%20on%20the%20analysis%20of%20the%0Asolution%20path%20of%20the%20fused%20lasso%20using%20a%20LARS-type%20algorithm%2C%20we%20derive%20exact%0Aconditional%20%24p%24-values%20for%20the%20selected%20change-points.%20Our%20method%20broadens%20the%0Aapplicability%20of%20the%20Spacing%20Test%20from%20the%20standard%20lasso%20to%20fused%20penalty%0Astructures.%20Furthermore%2C%20through%20numerical%20experiments%20comparing%20the%20proposed%0Amethod%20with%20sequential%20versions%20of%20AIC%20and%20BIC%20as%20well%20as%20cross-validation%2C%20we%0Ademonstrate%20that%20the%20proposed%20approach%20properly%20controls%20the%20type%20I%20error%20while%0Aachieving%20high%20detection%20power.%20This%20work%20offers%20a%20theoretically%20sound%20and%0Acomputationally%20practical%20solution%20for%20parameter%20selection%20and%20post-selection%0Ainference%20in%20structured%20signal%20estimation%20problems.%20Keywords%3A%20Fused%20Lasso%2C%0ARegularization%20parameter%20selection%2C%20Spacing%20Test%20for%20Lasso%2C%20Selective%0Ainference%2C%20Change-point%20detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpacing%2520Test%2520for%2520Fused%2520Lasso%26entry.906535625%3DRieko%2520Tasaka%2520and%2520Tatsuya%2520Kimura%2520and%2520Joe%2520Suzuki%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520unresolved%2520problem%2520of%2520selecting%2520the%2520regularization%250Aparameter%2520in%2520the%2520fused%2520lasso.%2520In%2520particular%252C%2520we%2520extend%2520the%2520framework%2520of%2520the%250ASpacing%2520Test%2520proposed%2520by%2520Tibshirani%2520et%2520al.%2520to%2520the%2520fused%2520lasso%252C%2520providing%2520a%250Atheoretical%2520foundation%2520for%2520post-selection%2520inference%2520by%2520characterizing%2520the%250Aselection%2520event%2520as%2520a%2520polyhedral%2520constraint.%2520Based%2520on%2520the%2520analysis%2520of%2520the%250Asolution%2520path%2520of%2520the%2520fused%2520lasso%2520using%2520a%2520LARS-type%2520algorithm%252C%2520we%2520derive%2520exact%250Aconditional%2520%2524p%2524-values%2520for%2520the%2520selected%2520change-points.%2520Our%2520method%2520broadens%2520the%250Aapplicability%2520of%2520the%2520Spacing%2520Test%2520from%2520the%2520standard%2520lasso%2520to%2520fused%2520penalty%250Astructures.%2520Furthermore%252C%2520through%2520numerical%2520experiments%2520comparing%2520the%2520proposed%250Amethod%2520with%2520sequential%2520versions%2520of%2520AIC%2520and%2520BIC%2520as%2520well%2520as%2520cross-validation%252C%2520we%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520properly%2520controls%2520the%2520type%2520I%2520error%2520while%250Aachieving%2520high%2520detection%2520power.%2520This%2520work%2520offers%2520a%2520theoretically%2520sound%2520and%250Acomputationally%2520practical%2520solution%2520for%2520parameter%2520selection%2520and%2520post-selection%250Ainference%2520in%2520structured%2520signal%2520estimation%2520problems.%2520Keywords%253A%2520Fused%2520Lasso%252C%250ARegularization%2520parameter%2520selection%252C%2520Spacing%2520Test%2520for%2520Lasso%252C%2520Selective%250Ainference%252C%2520Change-point%2520detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spacing%20Test%20for%20Fused%20Lasso&entry.906535625=Rieko%20Tasaka%20and%20Tatsuya%20Kimura%20and%20Joe%20Suzuki&entry.1292438233=%20%20This%20study%20addresses%20the%20unresolved%20problem%20of%20selecting%20the%20regularization%0Aparameter%20in%20the%20fused%20lasso.%20In%20particular%2C%20we%20extend%20the%20framework%20of%20the%0ASpacing%20Test%20proposed%20by%20Tibshirani%20et%20al.%20to%20the%20fused%20lasso%2C%20providing%20a%0Atheoretical%20foundation%20for%20post-selection%20inference%20by%20characterizing%20the%0Aselection%20event%20as%20a%20polyhedral%20constraint.%20Based%20on%20the%20analysis%20of%20the%0Asolution%20path%20of%20the%20fused%20lasso%20using%20a%20LARS-type%20algorithm%2C%20we%20derive%20exact%0Aconditional%20%24p%24-values%20for%20the%20selected%20change-points.%20Our%20method%20broadens%20the%0Aapplicability%20of%20the%20Spacing%20Test%20from%20the%20standard%20lasso%20to%20fused%20penalty%0Astructures.%20Furthermore%2C%20through%20numerical%20experiments%20comparing%20the%20proposed%0Amethod%20with%20sequential%20versions%20of%20AIC%20and%20BIC%20as%20well%20as%20cross-validation%2C%20we%0Ademonstrate%20that%20the%20proposed%20approach%20properly%20controls%20the%20type%20I%20error%20while%0Aachieving%20high%20detection%20power.%20This%20work%20offers%20a%20theoretically%20sound%20and%0Acomputationally%20practical%20solution%20for%20parameter%20selection%20and%20post-selection%0Ainference%20in%20structured%20signal%20estimation%20problems.%20Keywords%3A%20Fused%20Lasso%2C%0ARegularization%20parameter%20selection%2C%20Spacing%20Test%20for%20Lasso%2C%20Selective%0Ainference%2C%20Change-point%20detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14229v1&entry.124074799=Read"},
{"title": "From n-gram to Attention: How Model Architectures Learn and Propagate\n  Bias in Language Modeling", "author": "Mohsinul Kabir and Tasfia Tahsin and Sophia Ananiadou", "abstract": "  Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm.\n", "link": "http://arxiv.org/abs/2505.12381v2", "date": "2025-09-17", "relevancy": 1.9958, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20n-gram%20to%20Attention%3A%20How%20Model%20Architectures%20Learn%20and%20Propagate%0A%20%20Bias%20in%20Language%20Modeling&body=Title%3A%20From%20n-gram%20to%20Attention%3A%20How%20Model%20Architectures%20Learn%20and%20Propagate%0A%20%20Bias%20in%20Language%20Modeling%0AAuthor%3A%20Mohsinul%20Kabir%20and%20Tasfia%20Tahsin%20and%20Sophia%20Ananiadou%0AAbstract%3A%20%20%20Current%20research%20on%20bias%20in%20language%20models%20%28LMs%29%20predominantly%20focuses%20on%0Adata%20quality%2C%20with%20significantly%20less%20attention%20paid%20to%20model%20architecture%20and%0Atemporal%20influences%20of%20data.%20Even%20more%20critically%2C%20few%20studies%20systematically%0Ainvestigate%20the%20origins%20of%20bias.%20We%20propose%20a%20methodology%20grounded%20in%0Acomparative%20behavioral%20theory%20to%20interpret%20the%20complex%20interaction%20between%0Atraining%20data%20and%20model%20architecture%20in%20bias%20propagation%20during%20language%0Amodeling.%20Building%20on%20recent%20work%20that%20relates%20transformers%20to%20n-gram%20LMs%2C%20we%0Aevaluate%20how%20data%2C%20model%20design%20choices%2C%20and%20temporal%20dynamics%20affect%20bias%0Apropagation.%20Our%20findings%20reveal%20that%3A%20%281%29%20n-gram%20LMs%20are%20highly%20sensitive%20to%0Acontext%20window%20size%20in%20bias%20propagation%2C%20while%20transformers%20demonstrate%0Aarchitectural%20robustness%3B%20%282%29%20the%20temporal%20provenance%20of%20training%20data%0Asignificantly%20affects%20bias%3B%20and%20%283%29%20different%20model%20architectures%20respond%0Adifferentially%20to%20controlled%20bias%20injection%2C%20with%20certain%20biases%20%28e.g.%20sexual%0Aorientation%29%20being%20disproportionately%20amplified.%20As%20language%20models%20become%0Aubiquitous%2C%20our%20findings%20highlight%20the%20need%20for%20a%20holistic%20approach%20--%20tracing%0Abias%20to%20its%20origins%20across%20both%20data%20and%20model%20dimensions%2C%20not%20just%20symptoms%2C%0Ato%20mitigate%20harm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520n-gram%2520to%2520Attention%253A%2520How%2520Model%2520Architectures%2520Learn%2520and%2520Propagate%250A%2520%2520Bias%2520in%2520Language%2520Modeling%26entry.906535625%3DMohsinul%2520Kabir%2520and%2520Tasfia%2520Tahsin%2520and%2520Sophia%2520Ananiadou%26entry.1292438233%3D%2520%2520Current%2520research%2520on%2520bias%2520in%2520language%2520models%2520%2528LMs%2529%2520predominantly%2520focuses%2520on%250Adata%2520quality%252C%2520with%2520significantly%2520less%2520attention%2520paid%2520to%2520model%2520architecture%2520and%250Atemporal%2520influences%2520of%2520data.%2520Even%2520more%2520critically%252C%2520few%2520studies%2520systematically%250Ainvestigate%2520the%2520origins%2520of%2520bias.%2520We%2520propose%2520a%2520methodology%2520grounded%2520in%250Acomparative%2520behavioral%2520theory%2520to%2520interpret%2520the%2520complex%2520interaction%2520between%250Atraining%2520data%2520and%2520model%2520architecture%2520in%2520bias%2520propagation%2520during%2520language%250Amodeling.%2520Building%2520on%2520recent%2520work%2520that%2520relates%2520transformers%2520to%2520n-gram%2520LMs%252C%2520we%250Aevaluate%2520how%2520data%252C%2520model%2520design%2520choices%252C%2520and%2520temporal%2520dynamics%2520affect%2520bias%250Apropagation.%2520Our%2520findings%2520reveal%2520that%253A%2520%25281%2529%2520n-gram%2520LMs%2520are%2520highly%2520sensitive%2520to%250Acontext%2520window%2520size%2520in%2520bias%2520propagation%252C%2520while%2520transformers%2520demonstrate%250Aarchitectural%2520robustness%253B%2520%25282%2529%2520the%2520temporal%2520provenance%2520of%2520training%2520data%250Asignificantly%2520affects%2520bias%253B%2520and%2520%25283%2529%2520different%2520model%2520architectures%2520respond%250Adifferentially%2520to%2520controlled%2520bias%2520injection%252C%2520with%2520certain%2520biases%2520%2528e.g.%2520sexual%250Aorientation%2529%2520being%2520disproportionately%2520amplified.%2520As%2520language%2520models%2520become%250Aubiquitous%252C%2520our%2520findings%2520highlight%2520the%2520need%2520for%2520a%2520holistic%2520approach%2520--%2520tracing%250Abias%2520to%2520its%2520origins%2520across%2520both%2520data%2520and%2520model%2520dimensions%252C%2520not%2520just%2520symptoms%252C%250Ato%2520mitigate%2520harm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20n-gram%20to%20Attention%3A%20How%20Model%20Architectures%20Learn%20and%20Propagate%0A%20%20Bias%20in%20Language%20Modeling&entry.906535625=Mohsinul%20Kabir%20and%20Tasfia%20Tahsin%20and%20Sophia%20Ananiadou&entry.1292438233=%20%20Current%20research%20on%20bias%20in%20language%20models%20%28LMs%29%20predominantly%20focuses%20on%0Adata%20quality%2C%20with%20significantly%20less%20attention%20paid%20to%20model%20architecture%20and%0Atemporal%20influences%20of%20data.%20Even%20more%20critically%2C%20few%20studies%20systematically%0Ainvestigate%20the%20origins%20of%20bias.%20We%20propose%20a%20methodology%20grounded%20in%0Acomparative%20behavioral%20theory%20to%20interpret%20the%20complex%20interaction%20between%0Atraining%20data%20and%20model%20architecture%20in%20bias%20propagation%20during%20language%0Amodeling.%20Building%20on%20recent%20work%20that%20relates%20transformers%20to%20n-gram%20LMs%2C%20we%0Aevaluate%20how%20data%2C%20model%20design%20choices%2C%20and%20temporal%20dynamics%20affect%20bias%0Apropagation.%20Our%20findings%20reveal%20that%3A%20%281%29%20n-gram%20LMs%20are%20highly%20sensitive%20to%0Acontext%20window%20size%20in%20bias%20propagation%2C%20while%20transformers%20demonstrate%0Aarchitectural%20robustness%3B%20%282%29%20the%20temporal%20provenance%20of%20training%20data%0Asignificantly%20affects%20bias%3B%20and%20%283%29%20different%20model%20architectures%20respond%0Adifferentially%20to%20controlled%20bias%20injection%2C%20with%20certain%20biases%20%28e.g.%20sexual%0Aorientation%29%20being%20disproportionately%20amplified.%20As%20language%20models%20become%0Aubiquitous%2C%20our%20findings%20highlight%20the%20need%20for%20a%20holistic%20approach%20--%20tracing%0Abias%20to%20its%20origins%20across%20both%20data%20and%20model%20dimensions%2C%20not%20just%20symptoms%2C%0Ato%20mitigate%20harm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12381v2&entry.124074799=Read"},
{"title": "Self-supervised learning on gene expression data", "author": "Kevin Dradjat and Massinissa Hamidi and Pierre Bartet and Blaise Hanczar", "abstract": "  Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.\n", "link": "http://arxiv.org/abs/2507.13912v2", "date": "2025-09-17", "relevancy": 1.9651, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.537}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4653}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20on%20gene%20expression%20data&body=Title%3A%20Self-supervised%20learning%20on%20gene%20expression%20data%0AAuthor%3A%20Kevin%20Dradjat%20and%20Massinissa%20Hamidi%20and%20Pierre%20Bartet%20and%20Blaise%20Hanczar%0AAbstract%3A%20%20%20Predicting%20phenotypes%20from%20gene%20expression%20data%20is%20a%20crucial%20task%20in%0Abiomedical%20research%2C%20enabling%20insights%20into%20disease%20mechanisms%2C%20drug%20responses%2C%0Aand%20personalized%20medicine.%20Traditional%20machine%20learning%20and%20deep%20learning%20rely%0Aon%20supervised%20learning%2C%20which%20requires%20large%20quantities%20of%20labeled%20data%20that%0Aare%20costly%20and%20time-consuming%20to%20obtain%20in%20the%20case%20of%20gene%20expression%20data.%0ASelf-supervised%20learning%20has%20recently%20emerged%20as%20a%20promising%20approach%20to%0Aovercome%20these%20limitations%20by%20extracting%20information%20directly%20from%20the%0Astructure%20of%20unlabeled%20data.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%0Astate-of-the-art%20self-supervised%20learning%20methods%20to%20bulk%20gene%20expression%20data%0Afor%20phenotype%20prediction.%20We%20selected%20three%20self-supervised%20methods%2C%20based%20on%0Adifferent%20approaches%2C%20to%20assess%20their%20ability%20to%20exploit%20the%20inherent%20structure%0Aof%20the%20data%20and%20to%20generate%20qualitative%20representations%20which%20can%20be%20used%20for%0Adownstream%20predictive%20tasks.%20By%20using%20several%20publicly%20available%20gene%0Aexpression%20datasets%2C%20we%20demonstrate%20how%20the%20selected%20methods%20can%20effectively%0Acapture%20complex%20information%20and%20improve%20phenotype%20prediction%20accuracy.%20The%0Aresults%20obtained%20show%20that%20self-supervised%20learning%20methods%20can%20outperform%0Atraditional%20supervised%20models%20besides%20offering%20significant%20advantage%20by%0Areducing%20the%20dependency%20on%20annotated%20data.%20We%20provide%20a%20comprehensive%20analysis%0Aof%20the%20performance%20of%20each%20method%20by%20highlighting%20their%20strengths%20and%0Alimitations.%20We%20also%20provide%20recommendations%20for%20using%20these%20methods%20depending%0Aon%20the%20case%20under%20study.%20Finally%2C%20we%20outline%20future%20research%20directions%20to%0Aenhance%20the%20application%20of%20self-supervised%20learning%20in%20the%20field%20of%20gene%0Aexpression%20data%20analysis.%20This%20study%20is%20the%20first%20work%20that%20deals%20with%20bulk%0ARNA-Seq%20data%20and%20self-supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520learning%2520on%2520gene%2520expression%2520data%26entry.906535625%3DKevin%2520Dradjat%2520and%2520Massinissa%2520Hamidi%2520and%2520Pierre%2520Bartet%2520and%2520Blaise%2520Hanczar%26entry.1292438233%3D%2520%2520Predicting%2520phenotypes%2520from%2520gene%2520expression%2520data%2520is%2520a%2520crucial%2520task%2520in%250Abiomedical%2520research%252C%2520enabling%2520insights%2520into%2520disease%2520mechanisms%252C%2520drug%2520responses%252C%250Aand%2520personalized%2520medicine.%2520Traditional%2520machine%2520learning%2520and%2520deep%2520learning%2520rely%250Aon%2520supervised%2520learning%252C%2520which%2520requires%2520large%2520quantities%2520of%2520labeled%2520data%2520that%250Aare%2520costly%2520and%2520time-consuming%2520to%2520obtain%2520in%2520the%2520case%2520of%2520gene%2520expression%2520data.%250ASelf-supervised%2520learning%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520approach%2520to%250Aovercome%2520these%2520limitations%2520by%2520extracting%2520information%2520directly%2520from%2520the%250Astructure%2520of%2520unlabeled%2520data.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520application%2520of%250Astate-of-the-art%2520self-supervised%2520learning%2520methods%2520to%2520bulk%2520gene%2520expression%2520data%250Afor%2520phenotype%2520prediction.%2520We%2520selected%2520three%2520self-supervised%2520methods%252C%2520based%2520on%250Adifferent%2520approaches%252C%2520to%2520assess%2520their%2520ability%2520to%2520exploit%2520the%2520inherent%2520structure%250Aof%2520the%2520data%2520and%2520to%2520generate%2520qualitative%2520representations%2520which%2520can%2520be%2520used%2520for%250Adownstream%2520predictive%2520tasks.%2520By%2520using%2520several%2520publicly%2520available%2520gene%250Aexpression%2520datasets%252C%2520we%2520demonstrate%2520how%2520the%2520selected%2520methods%2520can%2520effectively%250Acapture%2520complex%2520information%2520and%2520improve%2520phenotype%2520prediction%2520accuracy.%2520The%250Aresults%2520obtained%2520show%2520that%2520self-supervised%2520learning%2520methods%2520can%2520outperform%250Atraditional%2520supervised%2520models%2520besides%2520offering%2520significant%2520advantage%2520by%250Areducing%2520the%2520dependency%2520on%2520annotated%2520data.%2520We%2520provide%2520a%2520comprehensive%2520analysis%250Aof%2520the%2520performance%2520of%2520each%2520method%2520by%2520highlighting%2520their%2520strengths%2520and%250Alimitations.%2520We%2520also%2520provide%2520recommendations%2520for%2520using%2520these%2520methods%2520depending%250Aon%2520the%2520case%2520under%2520study.%2520Finally%252C%2520we%2520outline%2520future%2520research%2520directions%2520to%250Aenhance%2520the%2520application%2520of%2520self-supervised%2520learning%2520in%2520the%2520field%2520of%2520gene%250Aexpression%2520data%2520analysis.%2520This%2520study%2520is%2520the%2520first%2520work%2520that%2520deals%2520with%2520bulk%250ARNA-Seq%2520data%2520and%2520self-supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20on%20gene%20expression%20data&entry.906535625=Kevin%20Dradjat%20and%20Massinissa%20Hamidi%20and%20Pierre%20Bartet%20and%20Blaise%20Hanczar&entry.1292438233=%20%20Predicting%20phenotypes%20from%20gene%20expression%20data%20is%20a%20crucial%20task%20in%0Abiomedical%20research%2C%20enabling%20insights%20into%20disease%20mechanisms%2C%20drug%20responses%2C%0Aand%20personalized%20medicine.%20Traditional%20machine%20learning%20and%20deep%20learning%20rely%0Aon%20supervised%20learning%2C%20which%20requires%20large%20quantities%20of%20labeled%20data%20that%0Aare%20costly%20and%20time-consuming%20to%20obtain%20in%20the%20case%20of%20gene%20expression%20data.%0ASelf-supervised%20learning%20has%20recently%20emerged%20as%20a%20promising%20approach%20to%0Aovercome%20these%20limitations%20by%20extracting%20information%20directly%20from%20the%0Astructure%20of%20unlabeled%20data.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%0Astate-of-the-art%20self-supervised%20learning%20methods%20to%20bulk%20gene%20expression%20data%0Afor%20phenotype%20prediction.%20We%20selected%20three%20self-supervised%20methods%2C%20based%20on%0Adifferent%20approaches%2C%20to%20assess%20their%20ability%20to%20exploit%20the%20inherent%20structure%0Aof%20the%20data%20and%20to%20generate%20qualitative%20representations%20which%20can%20be%20used%20for%0Adownstream%20predictive%20tasks.%20By%20using%20several%20publicly%20available%20gene%0Aexpression%20datasets%2C%20we%20demonstrate%20how%20the%20selected%20methods%20can%20effectively%0Acapture%20complex%20information%20and%20improve%20phenotype%20prediction%20accuracy.%20The%0Aresults%20obtained%20show%20that%20self-supervised%20learning%20methods%20can%20outperform%0Atraditional%20supervised%20models%20besides%20offering%20significant%20advantage%20by%0Areducing%20the%20dependency%20on%20annotated%20data.%20We%20provide%20a%20comprehensive%20analysis%0Aof%20the%20performance%20of%20each%20method%20by%20highlighting%20their%20strengths%20and%0Alimitations.%20We%20also%20provide%20recommendations%20for%20using%20these%20methods%20depending%0Aon%20the%20case%20under%20study.%20Finally%2C%20we%20outline%20future%20research%20directions%20to%0Aenhance%20the%20application%20of%20self-supervised%20learning%20in%20the%20field%20of%20gene%0Aexpression%20data%20analysis.%20This%20study%20is%20the%20first%20work%20that%20deals%20with%20bulk%0ARNA-Seq%20data%20and%20self-supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13912v2&entry.124074799=Read"},
{"title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework", "author": "Kerui Huang and Shuhan Liu and Xing Hu and Tongtong Xu and Lingfeng Bao and Xin Xia", "abstract": "  Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.\n", "link": "http://arxiv.org/abs/2509.14093v1", "date": "2025-09-17", "relevancy": 1.952, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Efficiently%20Through%20Adaptive%20Chain-of-Thought%20Compression%3A%20A%0A%20%20Self-Optimizing%20Framework&body=Title%3A%20Reasoning%20Efficiently%20Through%20Adaptive%20Chain-of-Thought%20Compression%3A%20A%0A%20%20Self-Optimizing%20Framework%0AAuthor%3A%20Kerui%20Huang%20and%20Shuhan%20Liu%20and%20Xing%20Hu%20and%20Tongtong%20Xu%20and%20Lingfeng%20Bao%20and%20Xin%20Xia%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20enhances%20Large%20Language%20Models%20%28LLMs%29%20by%0Aprompting%20intermediate%20steps%2C%20improving%20accuracy%20and%20robustness%20in%20arithmetic%2C%0Alogic%2C%20and%20commonsense%20tasks.%20However%2C%20this%20benefit%20comes%20with%20high%0Acomputational%20costs%3A%20longer%20outputs%20increase%20latency%2C%20memory%20usage%2C%20and%0AKV-cache%20demands.%20These%20issues%20are%20especially%20critical%20in%20software%20engineering%0Atasks%20where%20concise%20and%20deterministic%20outputs%20are%20required.%20To%20investigate%0Athese%20trade-offs%2C%20we%20conduct%20an%20empirical%20study%20based%20on%20code%20generation%0Abenchmarks.%20The%20results%20reveal%20that%20longer%20CoT%20does%20not%20always%20help.%20Excessive%0Areasoning%20often%20causes%20truncation%2C%20accuracy%20drops%2C%20and%20latency%20up%20to%20five%20times%0Ahigher%2C%20with%20failed%20outputs%20consistently%20longer%20than%20successful%20ones.%20These%0Afindings%20challenge%20the%20assumption%20that%20longer%20reasoning%20is%20inherently%20better%0Aand%20highlight%20the%20need%20for%20adaptive%20CoT%20control.%20Motivated%20by%20this%2C%20we%20propose%0ASEER%20%28Self-Enhancing%20Efficient%20Reasoning%29%2C%20an%20adaptive%20framework%20that%0Acompresses%20CoT%20while%20preserving%20accuracy.%20SEER%20combines%20Best-of-N%20sampling%20with%0Atask-aware%20adaptive%20filtering%2C%20dynamically%20adjusting%20thresholds%20based%20on%0Apre-inference%20outputs%20to%20reduce%20verbosity%20and%20computational%20overhead.%20We%20then%0Aevaluate%20SEER%20on%20three%20software%20engineering%20tasks%20and%20one%20math%20task.%20On%0Aaverage%2C%20SEER%20shortens%20CoT%20by%2042.1%25%2C%20improves%20accuracy%20by%20reducing%20truncation%2C%0Aand%20eliminates%20most%20infinite%20loops.%20These%20results%20demonstrate%20SEER%20as%20a%0Apractical%20method%20to%20make%20CoT-enhanced%20LLMs%20more%20efficient%20and%20robust%2C%20even%0Aunder%20resource%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Efficiently%2520Through%2520Adaptive%2520Chain-of-Thought%2520Compression%253A%2520A%250A%2520%2520Self-Optimizing%2520Framework%26entry.906535625%3DKerui%2520Huang%2520and%2520Shuhan%2520Liu%2520and%2520Xing%2520Hu%2520and%2520Tongtong%2520Xu%2520and%2520Lingfeng%2520Bao%2520and%2520Xin%2520Xia%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520enhances%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%250Aprompting%2520intermediate%2520steps%252C%2520improving%2520accuracy%2520and%2520robustness%2520in%2520arithmetic%252C%250Alogic%252C%2520and%2520commonsense%2520tasks.%2520However%252C%2520this%2520benefit%2520comes%2520with%2520high%250Acomputational%2520costs%253A%2520longer%2520outputs%2520increase%2520latency%252C%2520memory%2520usage%252C%2520and%250AKV-cache%2520demands.%2520These%2520issues%2520are%2520especially%2520critical%2520in%2520software%2520engineering%250Atasks%2520where%2520concise%2520and%2520deterministic%2520outputs%2520are%2520required.%2520To%2520investigate%250Athese%2520trade-offs%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520based%2520on%2520code%2520generation%250Abenchmarks.%2520The%2520results%2520reveal%2520that%2520longer%2520CoT%2520does%2520not%2520always%2520help.%2520Excessive%250Areasoning%2520often%2520causes%2520truncation%252C%2520accuracy%2520drops%252C%2520and%2520latency%2520up%2520to%2520five%2520times%250Ahigher%252C%2520with%2520failed%2520outputs%2520consistently%2520longer%2520than%2520successful%2520ones.%2520These%250Afindings%2520challenge%2520the%2520assumption%2520that%2520longer%2520reasoning%2520is%2520inherently%2520better%250Aand%2520highlight%2520the%2520need%2520for%2520adaptive%2520CoT%2520control.%2520Motivated%2520by%2520this%252C%2520we%2520propose%250ASEER%2520%2528Self-Enhancing%2520Efficient%2520Reasoning%2529%252C%2520an%2520adaptive%2520framework%2520that%250Acompresses%2520CoT%2520while%2520preserving%2520accuracy.%2520SEER%2520combines%2520Best-of-N%2520sampling%2520with%250Atask-aware%2520adaptive%2520filtering%252C%2520dynamically%2520adjusting%2520thresholds%2520based%2520on%250Apre-inference%2520outputs%2520to%2520reduce%2520verbosity%2520and%2520computational%2520overhead.%2520We%2520then%250Aevaluate%2520SEER%2520on%2520three%2520software%2520engineering%2520tasks%2520and%2520one%2520math%2520task.%2520On%250Aaverage%252C%2520SEER%2520shortens%2520CoT%2520by%252042.1%2525%252C%2520improves%2520accuracy%2520by%2520reducing%2520truncation%252C%250Aand%2520eliminates%2520most%2520infinite%2520loops.%2520These%2520results%2520demonstrate%2520SEER%2520as%2520a%250Apractical%2520method%2520to%2520make%2520CoT-enhanced%2520LLMs%2520more%2520efficient%2520and%2520robust%252C%2520even%250Aunder%2520resource%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Efficiently%20Through%20Adaptive%20Chain-of-Thought%20Compression%3A%20A%0A%20%20Self-Optimizing%20Framework&entry.906535625=Kerui%20Huang%20and%20Shuhan%20Liu%20and%20Xing%20Hu%20and%20Tongtong%20Xu%20and%20Lingfeng%20Bao%20and%20Xin%20Xia&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20reasoning%20enhances%20Large%20Language%20Models%20%28LLMs%29%20by%0Aprompting%20intermediate%20steps%2C%20improving%20accuracy%20and%20robustness%20in%20arithmetic%2C%0Alogic%2C%20and%20commonsense%20tasks.%20However%2C%20this%20benefit%20comes%20with%20high%0Acomputational%20costs%3A%20longer%20outputs%20increase%20latency%2C%20memory%20usage%2C%20and%0AKV-cache%20demands.%20These%20issues%20are%20especially%20critical%20in%20software%20engineering%0Atasks%20where%20concise%20and%20deterministic%20outputs%20are%20required.%20To%20investigate%0Athese%20trade-offs%2C%20we%20conduct%20an%20empirical%20study%20based%20on%20code%20generation%0Abenchmarks.%20The%20results%20reveal%20that%20longer%20CoT%20does%20not%20always%20help.%20Excessive%0Areasoning%20often%20causes%20truncation%2C%20accuracy%20drops%2C%20and%20latency%20up%20to%20five%20times%0Ahigher%2C%20with%20failed%20outputs%20consistently%20longer%20than%20successful%20ones.%20These%0Afindings%20challenge%20the%20assumption%20that%20longer%20reasoning%20is%20inherently%20better%0Aand%20highlight%20the%20need%20for%20adaptive%20CoT%20control.%20Motivated%20by%20this%2C%20we%20propose%0ASEER%20%28Self-Enhancing%20Efficient%20Reasoning%29%2C%20an%20adaptive%20framework%20that%0Acompresses%20CoT%20while%20preserving%20accuracy.%20SEER%20combines%20Best-of-N%20sampling%20with%0Atask-aware%20adaptive%20filtering%2C%20dynamically%20adjusting%20thresholds%20based%20on%0Apre-inference%20outputs%20to%20reduce%20verbosity%20and%20computational%20overhead.%20We%20then%0Aevaluate%20SEER%20on%20three%20software%20engineering%20tasks%20and%20one%20math%20task.%20On%0Aaverage%2C%20SEER%20shortens%20CoT%20by%2042.1%25%2C%20improves%20accuracy%20by%20reducing%20truncation%2C%0Aand%20eliminates%20most%20infinite%20loops.%20These%20results%20demonstrate%20SEER%20as%20a%0Apractical%20method%20to%20make%20CoT-enhanced%20LLMs%20more%20efficient%20and%20robust%2C%20even%0Aunder%20resource%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14093v1&entry.124074799=Read"},
{"title": "Long-context Reference-based MT Quality Estimation", "author": "Sami Ul Haq and Chinonso Cynthia Osuji and Sheila Castilho and Brian Davis", "abstract": "  In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.\n", "link": "http://arxiv.org/abs/2509.13980v1", "date": "2025-09-17", "relevancy": 1.943, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-context%20Reference-based%20MT%20Quality%20Estimation&body=Title%3A%20Long-context%20Reference-based%20MT%20Quality%20Estimation%0AAuthor%3A%20Sami%20Ul%20Haq%20and%20Chinonso%20Cynthia%20Osuji%20and%20Sheila%20Castilho%20and%20Brian%20Davis%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20submission%20to%20the%20Tenth%20Conference%20on%20Machine%0ATranslation%20%28WMT25%29%20Shared%20Task%20on%20Automated%20Translation%20Quality%20Evaluation.%0A%20%20Our%20systems%20are%20built%20upon%20the%20COMET%20framework%20and%20trained%20to%20predict%0Asegment-level%20Error%20Span%20Annotation%20%28ESA%29%20scores%20using%20augmented%20long-context%0Adata.%0A%20%20To%20construct%20long-context%20training%20data%2C%20we%20concatenate%20in-domain%2C%0Ahuman-annotated%20sentences%20and%20compute%20a%20weighted%20average%20of%20their%20scores.%0A%20%20We%20integrate%20multiple%20human%20judgment%20datasets%20%28MQM%2C%20SQM%2C%20and%20DA%29%20by%0Anormalising%20their%20scales%20and%20train%20multilingual%20regression%20models%20to%20predict%0Aquality%20scores%20from%20the%20source%2C%20hypothesis%2C%20and%20reference%20translations.%0A%20%20Experimental%20results%20show%20that%20incorporating%20long-context%20information%0Aimproves%20correlations%20with%20human%20judgments%20compared%20to%20models%20trained%20only%20on%0Ashort%20segments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-context%2520Reference-based%2520MT%2520Quality%2520Estimation%26entry.906535625%3DSami%2520Ul%2520Haq%2520and%2520Chinonso%2520Cynthia%2520Osuji%2520and%2520Sheila%2520Castilho%2520and%2520Brian%2520Davis%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520submission%2520to%2520the%2520Tenth%2520Conference%2520on%2520Machine%250ATranslation%2520%2528WMT25%2529%2520Shared%2520Task%2520on%2520Automated%2520Translation%2520Quality%2520Evaluation.%250A%2520%2520Our%2520systems%2520are%2520built%2520upon%2520the%2520COMET%2520framework%2520and%2520trained%2520to%2520predict%250Asegment-level%2520Error%2520Span%2520Annotation%2520%2528ESA%2529%2520scores%2520using%2520augmented%2520long-context%250Adata.%250A%2520%2520To%2520construct%2520long-context%2520training%2520data%252C%2520we%2520concatenate%2520in-domain%252C%250Ahuman-annotated%2520sentences%2520and%2520compute%2520a%2520weighted%2520average%2520of%2520their%2520scores.%250A%2520%2520We%2520integrate%2520multiple%2520human%2520judgment%2520datasets%2520%2528MQM%252C%2520SQM%252C%2520and%2520DA%2529%2520by%250Anormalising%2520their%2520scales%2520and%2520train%2520multilingual%2520regression%2520models%2520to%2520predict%250Aquality%2520scores%2520from%2520the%2520source%252C%2520hypothesis%252C%2520and%2520reference%2520translations.%250A%2520%2520Experimental%2520results%2520show%2520that%2520incorporating%2520long-context%2520information%250Aimproves%2520correlations%2520with%2520human%2520judgments%2520compared%2520to%2520models%2520trained%2520only%2520on%250Ashort%2520segments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-context%20Reference-based%20MT%20Quality%20Estimation&entry.906535625=Sami%20Ul%20Haq%20and%20Chinonso%20Cynthia%20Osuji%20and%20Sheila%20Castilho%20and%20Brian%20Davis&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20submission%20to%20the%20Tenth%20Conference%20on%20Machine%0ATranslation%20%28WMT25%29%20Shared%20Task%20on%20Automated%20Translation%20Quality%20Evaluation.%0A%20%20Our%20systems%20are%20built%20upon%20the%20COMET%20framework%20and%20trained%20to%20predict%0Asegment-level%20Error%20Span%20Annotation%20%28ESA%29%20scores%20using%20augmented%20long-context%0Adata.%0A%20%20To%20construct%20long-context%20training%20data%2C%20we%20concatenate%20in-domain%2C%0Ahuman-annotated%20sentences%20and%20compute%20a%20weighted%20average%20of%20their%20scores.%0A%20%20We%20integrate%20multiple%20human%20judgment%20datasets%20%28MQM%2C%20SQM%2C%20and%20DA%29%20by%0Anormalising%20their%20scales%20and%20train%20multilingual%20regression%20models%20to%20predict%0Aquality%20scores%20from%20the%20source%2C%20hypothesis%2C%20and%20reference%20translations.%0A%20%20Experimental%20results%20show%20that%20incorporating%20long-context%20information%0Aimproves%20correlations%20with%20human%20judgments%20compared%20to%20models%20trained%20only%20on%0Ashort%20segments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13980v1&entry.124074799=Read"},
{"title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices", "author": "Jordi Grau-Haro and Ruben Ribes-Serrano and Javier Naranjo-Alcazar and Marta Garcia-Ballesteros and Pedro Zuccarello", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios.\n", "link": "http://arxiv.org/abs/2509.14049v1", "date": "2025-09-17", "relevancy": 1.9428, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Evaluation%20of%20CNN-Based%20Audio%20Tagging%20Models%20on%0A%20%20Resource-Constrained%20Devices&body=Title%3A%20Comprehensive%20Evaluation%20of%20CNN-Based%20Audio%20Tagging%20Models%20on%0A%20%20Resource-Constrained%20Devices%0AAuthor%3A%20Jordi%20Grau-Haro%20and%20Ruben%20Ribes-Serrano%20and%20Javier%20Naranjo-Alcazar%20and%20Marta%20Garcia-Ballesteros%20and%20Pedro%20Zuccarello%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20demonstrated%20exceptional%0Aperformance%20in%20audio%20tagging%20tasks.%20However%2C%20deploying%20these%20models%20on%0Aresource-constrained%20devices%20like%20the%20Raspberry%20Pi%20poses%20challenges%20related%20to%0Acomputational%20efficiency%20and%20thermal%20management.%20In%20this%20paper%2C%20a%20comprehensive%0Aevaluation%20of%20multiple%20convolutional%20neural%20network%20%28CNN%29%20architectures%20for%0Aaudio%20tagging%20on%20the%20Raspberry%20Pi%20is%20conducted%2C%20encompassing%20all%201D%20and%202D%0Amodels%20from%20the%20Pretrained%20Audio%20Neural%20Networks%20%28PANNs%29%20framework%2C%20a%0AConvNeXt-based%20model%20adapted%20for%20audio%20classification%2C%20as%20well%20as%20MobileNetV3%0Aarchitectures.%20In%20addition%2C%20two%20PANNs-derived%20networks%2C%20CNN9%20and%20CNN13%2C%0Arecently%20proposed%2C%20are%20also%20evaluated.%20To%20enhance%20deployment%20efficiency%20and%0Aportability%20across%20diverse%20hardware%20platforms%2C%20all%20models%20are%20converted%20to%20the%0AOpen%20Neural%20Network%20Exchange%20%28ONNX%29%20format.%20Unlike%20previous%20works%20that%20focus%20on%0Aa%20single%20model%2C%20our%20analysis%20encompasses%20a%20broader%20range%20of%20architectures%20and%0Ainvolves%20continuous%2024-hour%20inference%20sessions%20to%20assess%20performance%20stability.%0AOur%20experiments%20reveal%20that%2C%20with%20appropriate%20model%20selection%20and%20optimization%2C%0Ait%20is%20possible%20to%20maintain%20consistent%20inference%20latency%20and%20manage%20thermal%0Abehavior%20effectively%20over%20extended%20periods.%20These%20findings%20provide%20valuable%0Ainsights%20for%20deploying%20audio%20tagging%20models%20in%20real-world%20edge%20computing%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Evaluation%2520of%2520CNN-Based%2520Audio%2520Tagging%2520Models%2520on%250A%2520%2520Resource-Constrained%2520Devices%26entry.906535625%3DJordi%2520Grau-Haro%2520and%2520Ruben%2520Ribes-Serrano%2520and%2520Javier%2520Naranjo-Alcazar%2520and%2520Marta%2520Garcia-Ballesteros%2520and%2520Pedro%2520Zuccarello%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520demonstrated%2520exceptional%250Aperformance%2520in%2520audio%2520tagging%2520tasks.%2520However%252C%2520deploying%2520these%2520models%2520on%250Aresource-constrained%2520devices%2520like%2520the%2520Raspberry%2520Pi%2520poses%2520challenges%2520related%2520to%250Acomputational%2520efficiency%2520and%2520thermal%2520management.%2520In%2520this%2520paper%252C%2520a%2520comprehensive%250Aevaluation%2520of%2520multiple%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520architectures%2520for%250Aaudio%2520tagging%2520on%2520the%2520Raspberry%2520Pi%2520is%2520conducted%252C%2520encompassing%2520all%25201D%2520and%25202D%250Amodels%2520from%2520the%2520Pretrained%2520Audio%2520Neural%2520Networks%2520%2528PANNs%2529%2520framework%252C%2520a%250AConvNeXt-based%2520model%2520adapted%2520for%2520audio%2520classification%252C%2520as%2520well%2520as%2520MobileNetV3%250Aarchitectures.%2520In%2520addition%252C%2520two%2520PANNs-derived%2520networks%252C%2520CNN9%2520and%2520CNN13%252C%250Arecently%2520proposed%252C%2520are%2520also%2520evaluated.%2520To%2520enhance%2520deployment%2520efficiency%2520and%250Aportability%2520across%2520diverse%2520hardware%2520platforms%252C%2520all%2520models%2520are%2520converted%2520to%2520the%250AOpen%2520Neural%2520Network%2520Exchange%2520%2528ONNX%2529%2520format.%2520Unlike%2520previous%2520works%2520that%2520focus%2520on%250Aa%2520single%2520model%252C%2520our%2520analysis%2520encompasses%2520a%2520broader%2520range%2520of%2520architectures%2520and%250Ainvolves%2520continuous%252024-hour%2520inference%2520sessions%2520to%2520assess%2520performance%2520stability.%250AOur%2520experiments%2520reveal%2520that%252C%2520with%2520appropriate%2520model%2520selection%2520and%2520optimization%252C%250Ait%2520is%2520possible%2520to%2520maintain%2520consistent%2520inference%2520latency%2520and%2520manage%2520thermal%250Abehavior%2520effectively%2520over%2520extended%2520periods.%2520These%2520findings%2520provide%2520valuable%250Ainsights%2520for%2520deploying%2520audio%2520tagging%2520models%2520in%2520real-world%2520edge%2520computing%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Evaluation%20of%20CNN-Based%20Audio%20Tagging%20Models%20on%0A%20%20Resource-Constrained%20Devices&entry.906535625=Jordi%20Grau-Haro%20and%20Ruben%20Ribes-Serrano%20and%20Javier%20Naranjo-Alcazar%20and%20Marta%20Garcia-Ballesteros%20and%20Pedro%20Zuccarello&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20demonstrated%20exceptional%0Aperformance%20in%20audio%20tagging%20tasks.%20However%2C%20deploying%20these%20models%20on%0Aresource-constrained%20devices%20like%20the%20Raspberry%20Pi%20poses%20challenges%20related%20to%0Acomputational%20efficiency%20and%20thermal%20management.%20In%20this%20paper%2C%20a%20comprehensive%0Aevaluation%20of%20multiple%20convolutional%20neural%20network%20%28CNN%29%20architectures%20for%0Aaudio%20tagging%20on%20the%20Raspberry%20Pi%20is%20conducted%2C%20encompassing%20all%201D%20and%202D%0Amodels%20from%20the%20Pretrained%20Audio%20Neural%20Networks%20%28PANNs%29%20framework%2C%20a%0AConvNeXt-based%20model%20adapted%20for%20audio%20classification%2C%20as%20well%20as%20MobileNetV3%0Aarchitectures.%20In%20addition%2C%20two%20PANNs-derived%20networks%2C%20CNN9%20and%20CNN13%2C%0Arecently%20proposed%2C%20are%20also%20evaluated.%20To%20enhance%20deployment%20efficiency%20and%0Aportability%20across%20diverse%20hardware%20platforms%2C%20all%20models%20are%20converted%20to%20the%0AOpen%20Neural%20Network%20Exchange%20%28ONNX%29%20format.%20Unlike%20previous%20works%20that%20focus%20on%0Aa%20single%20model%2C%20our%20analysis%20encompasses%20a%20broader%20range%20of%20architectures%20and%0Ainvolves%20continuous%2024-hour%20inference%20sessions%20to%20assess%20performance%20stability.%0AOur%20experiments%20reveal%20that%2C%20with%20appropriate%20model%20selection%20and%20optimization%2C%0Ait%20is%20possible%20to%20maintain%20consistent%20inference%20latency%20and%20manage%20thermal%0Abehavior%20effectively%20over%20extended%20periods.%20These%20findings%20provide%20valuable%0Ainsights%20for%20deploying%20audio%20tagging%20models%20in%20real-world%20edge%20computing%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14049v1&entry.124074799=Read"},
{"title": "Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers\n  and RNNs Trained with a New Loss Function", "author": "Pedro Seber", "abstract": "  O-GlcNAcylation, a subtype of glycosylation, has the potential to be an\nimportant target for therapeutics, but methods to reliably predict\nO-GlcNAcylation sites had not been available until 2023; a 2021 review\ncorrectly noted that published models were insufficient and failed to\ngeneralize. Moreover, many are no longer usable. In 2023, a considerably better\nrecurrent neural network (RNN) model was published. This article creates\nimproved models by using a new loss function, which we call the weighted focal\ndifferentiable MCC. RNN models trained with this new loss display superior\nperformance to models trained using the weighted cross-entropy loss; this new\nfunction can also be used to fine-tune trained models. An RNN trained with this\nloss achieves state-of-the-art performance in O-GlcNAcylation site prediction\nwith an F$_1$ score of 38.88% and an MCC of 38.20% on an independent test set\nfrom the largest dataset available.\n", "link": "http://arxiv.org/abs/2402.17131v3", "date": "2025-09-17", "relevancy": 1.9379, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4962}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4764}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20O-GlcNAcylation%20Sites%20in%20Mammalian%20Proteins%20with%20Transformers%0A%20%20and%20RNNs%20Trained%20with%20a%20New%20Loss%20Function&body=Title%3A%20Predicting%20O-GlcNAcylation%20Sites%20in%20Mammalian%20Proteins%20with%20Transformers%0A%20%20and%20RNNs%20Trained%20with%20a%20New%20Loss%20Function%0AAuthor%3A%20Pedro%20Seber%0AAbstract%3A%20%20%20O-GlcNAcylation%2C%20a%20subtype%20of%20glycosylation%2C%20has%20the%20potential%20to%20be%20an%0Aimportant%20target%20for%20therapeutics%2C%20but%20methods%20to%20reliably%20predict%0AO-GlcNAcylation%20sites%20had%20not%20been%20available%20until%202023%3B%20a%202021%20review%0Acorrectly%20noted%20that%20published%20models%20were%20insufficient%20and%20failed%20to%0Ageneralize.%20Moreover%2C%20many%20are%20no%20longer%20usable.%20In%202023%2C%20a%20considerably%20better%0Arecurrent%20neural%20network%20%28RNN%29%20model%20was%20published.%20This%20article%20creates%0Aimproved%20models%20by%20using%20a%20new%20loss%20function%2C%20which%20we%20call%20the%20weighted%20focal%0Adifferentiable%20MCC.%20RNN%20models%20trained%20with%20this%20new%20loss%20display%20superior%0Aperformance%20to%20models%20trained%20using%20the%20weighted%20cross-entropy%20loss%3B%20this%20new%0Afunction%20can%20also%20be%20used%20to%20fine-tune%20trained%20models.%20An%20RNN%20trained%20with%20this%0Aloss%20achieves%20state-of-the-art%20performance%20in%20O-GlcNAcylation%20site%20prediction%0Awith%20an%20F%24_1%24%20score%20of%2038.88%25%20and%20an%20MCC%20of%2038.20%25%20on%20an%20independent%20test%20set%0Afrom%20the%20largest%20dataset%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17131v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520O-GlcNAcylation%2520Sites%2520in%2520Mammalian%2520Proteins%2520with%2520Transformers%250A%2520%2520and%2520RNNs%2520Trained%2520with%2520a%2520New%2520Loss%2520Function%26entry.906535625%3DPedro%2520Seber%26entry.1292438233%3D%2520%2520O-GlcNAcylation%252C%2520a%2520subtype%2520of%2520glycosylation%252C%2520has%2520the%2520potential%2520to%2520be%2520an%250Aimportant%2520target%2520for%2520therapeutics%252C%2520but%2520methods%2520to%2520reliably%2520predict%250AO-GlcNAcylation%2520sites%2520had%2520not%2520been%2520available%2520until%25202023%253B%2520a%25202021%2520review%250Acorrectly%2520noted%2520that%2520published%2520models%2520were%2520insufficient%2520and%2520failed%2520to%250Ageneralize.%2520Moreover%252C%2520many%2520are%2520no%2520longer%2520usable.%2520In%25202023%252C%2520a%2520considerably%2520better%250Arecurrent%2520neural%2520network%2520%2528RNN%2529%2520model%2520was%2520published.%2520This%2520article%2520creates%250Aimproved%2520models%2520by%2520using%2520a%2520new%2520loss%2520function%252C%2520which%2520we%2520call%2520the%2520weighted%2520focal%250Adifferentiable%2520MCC.%2520RNN%2520models%2520trained%2520with%2520this%2520new%2520loss%2520display%2520superior%250Aperformance%2520to%2520models%2520trained%2520using%2520the%2520weighted%2520cross-entropy%2520loss%253B%2520this%2520new%250Afunction%2520can%2520also%2520be%2520used%2520to%2520fine-tune%2520trained%2520models.%2520An%2520RNN%2520trained%2520with%2520this%250Aloss%2520achieves%2520state-of-the-art%2520performance%2520in%2520O-GlcNAcylation%2520site%2520prediction%250Awith%2520an%2520F%2524_1%2524%2520score%2520of%252038.88%2525%2520and%2520an%2520MCC%2520of%252038.20%2525%2520on%2520an%2520independent%2520test%2520set%250Afrom%2520the%2520largest%2520dataset%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17131v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20O-GlcNAcylation%20Sites%20in%20Mammalian%20Proteins%20with%20Transformers%0A%20%20and%20RNNs%20Trained%20with%20a%20New%20Loss%20Function&entry.906535625=Pedro%20Seber&entry.1292438233=%20%20O-GlcNAcylation%2C%20a%20subtype%20of%20glycosylation%2C%20has%20the%20potential%20to%20be%20an%0Aimportant%20target%20for%20therapeutics%2C%20but%20methods%20to%20reliably%20predict%0AO-GlcNAcylation%20sites%20had%20not%20been%20available%20until%202023%3B%20a%202021%20review%0Acorrectly%20noted%20that%20published%20models%20were%20insufficient%20and%20failed%20to%0Ageneralize.%20Moreover%2C%20many%20are%20no%20longer%20usable.%20In%202023%2C%20a%20considerably%20better%0Arecurrent%20neural%20network%20%28RNN%29%20model%20was%20published.%20This%20article%20creates%0Aimproved%20models%20by%20using%20a%20new%20loss%20function%2C%20which%20we%20call%20the%20weighted%20focal%0Adifferentiable%20MCC.%20RNN%20models%20trained%20with%20this%20new%20loss%20display%20superior%0Aperformance%20to%20models%20trained%20using%20the%20weighted%20cross-entropy%20loss%3B%20this%20new%0Afunction%20can%20also%20be%20used%20to%20fine-tune%20trained%20models.%20An%20RNN%20trained%20with%20this%0Aloss%20achieves%20state-of-the-art%20performance%20in%20O-GlcNAcylation%20site%20prediction%0Awith%20an%20F%24_1%24%20score%20of%2038.88%25%20and%20an%20MCC%20of%2038.20%25%20on%20an%20independent%20test%20set%0Afrom%20the%20largest%20dataset%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17131v3&entry.124074799=Read"},
{"title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song\n  Translation", "author": "Woohyun Cho and Youngmin Kim and Sunghyun Lee and Youngjae Yu", "abstract": "  Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.\n", "link": "http://arxiv.org/abs/2505.18614v3", "date": "2025-09-17", "relevancy": 1.9286, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5043}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4725}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAVL%3A%20A%20Multilingual%20Audio-Video%20Lyrics%20Dataset%20for%20Animated%20Song%0A%20%20Translation&body=Title%3A%20MAVL%3A%20A%20Multilingual%20Audio-Video%20Lyrics%20Dataset%20for%20Animated%20Song%0A%20%20Translation%0AAuthor%3A%20Woohyun%20Cho%20and%20Youngmin%20Kim%20and%20Sunghyun%20Lee%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20Lyrics%20translation%20requires%20both%20accurate%20semantic%20transfer%20and%20preservation%0Aof%20musical%20rhythm%2C%20syllabic%20structure%2C%20and%20poetic%20style.%20In%20animated%20musicals%2C%0Athe%20challenge%20intensifies%20due%20to%20alignment%20with%20visual%20and%20auditory%20cues.%20We%0Aintroduce%20Multilingual%20Audio-Video%20Lyrics%20Benchmark%20for%20Animated%20Song%0ATranslation%20%28MAVL%29%2C%20the%20first%20multilingual%2C%20multimodal%20benchmark%20for%20singable%0Alyrics%20translation.%20By%20integrating%20text%2C%20audio%2C%20and%20video%2C%20MAVL%20enables%20richer%0Aand%20more%20expressive%20translations%20than%20text-only%20approaches.%20Building%20on%20this%2C%0Awe%20propose%20Syllable-Constrained%20Audio-Video%20LLM%20with%20Chain-of-Thought%0ASylAVL-CoT%2C%20which%20leverages%20audio-video%20cues%20and%20enforces%20syllabic%20constraints%0Ato%20produce%20natural-sounding%20lyrics.%20Experimental%20results%20demonstrate%20that%0ASylAVL-CoT%20significantly%20outperforms%20text-based%20models%20in%20singability%20and%0Acontextual%20accuracy%2C%20emphasizing%20the%20value%20of%20multimodal%2C%20multilingual%0Aapproaches%20for%20lyrics%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAVL%253A%2520A%2520Multilingual%2520Audio-Video%2520Lyrics%2520Dataset%2520for%2520Animated%2520Song%250A%2520%2520Translation%26entry.906535625%3DWoohyun%2520Cho%2520and%2520Youngmin%2520Kim%2520and%2520Sunghyun%2520Lee%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520Lyrics%2520translation%2520requires%2520both%2520accurate%2520semantic%2520transfer%2520and%2520preservation%250Aof%2520musical%2520rhythm%252C%2520syllabic%2520structure%252C%2520and%2520poetic%2520style.%2520In%2520animated%2520musicals%252C%250Athe%2520challenge%2520intensifies%2520due%2520to%2520alignment%2520with%2520visual%2520and%2520auditory%2520cues.%2520We%250Aintroduce%2520Multilingual%2520Audio-Video%2520Lyrics%2520Benchmark%2520for%2520Animated%2520Song%250ATranslation%2520%2528MAVL%2529%252C%2520the%2520first%2520multilingual%252C%2520multimodal%2520benchmark%2520for%2520singable%250Alyrics%2520translation.%2520By%2520integrating%2520text%252C%2520audio%252C%2520and%2520video%252C%2520MAVL%2520enables%2520richer%250Aand%2520more%2520expressive%2520translations%2520than%2520text-only%2520approaches.%2520Building%2520on%2520this%252C%250Awe%2520propose%2520Syllable-Constrained%2520Audio-Video%2520LLM%2520with%2520Chain-of-Thought%250ASylAVL-CoT%252C%2520which%2520leverages%2520audio-video%2520cues%2520and%2520enforces%2520syllabic%2520constraints%250Ato%2520produce%2520natural-sounding%2520lyrics.%2520Experimental%2520results%2520demonstrate%2520that%250ASylAVL-CoT%2520significantly%2520outperforms%2520text-based%2520models%2520in%2520singability%2520and%250Acontextual%2520accuracy%252C%2520emphasizing%2520the%2520value%2520of%2520multimodal%252C%2520multilingual%250Aapproaches%2520for%2520lyrics%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAVL%3A%20A%20Multilingual%20Audio-Video%20Lyrics%20Dataset%20for%20Animated%20Song%0A%20%20Translation&entry.906535625=Woohyun%20Cho%20and%20Youngmin%20Kim%20and%20Sunghyun%20Lee%20and%20Youngjae%20Yu&entry.1292438233=%20%20Lyrics%20translation%20requires%20both%20accurate%20semantic%20transfer%20and%20preservation%0Aof%20musical%20rhythm%2C%20syllabic%20structure%2C%20and%20poetic%20style.%20In%20animated%20musicals%2C%0Athe%20challenge%20intensifies%20due%20to%20alignment%20with%20visual%20and%20auditory%20cues.%20We%0Aintroduce%20Multilingual%20Audio-Video%20Lyrics%20Benchmark%20for%20Animated%20Song%0ATranslation%20%28MAVL%29%2C%20the%20first%20multilingual%2C%20multimodal%20benchmark%20for%20singable%0Alyrics%20translation.%20By%20integrating%20text%2C%20audio%2C%20and%20video%2C%20MAVL%20enables%20richer%0Aand%20more%20expressive%20translations%20than%20text-only%20approaches.%20Building%20on%20this%2C%0Awe%20propose%20Syllable-Constrained%20Audio-Video%20LLM%20with%20Chain-of-Thought%0ASylAVL-CoT%2C%20which%20leverages%20audio-video%20cues%20and%20enforces%20syllabic%20constraints%0Ato%20produce%20natural-sounding%20lyrics.%20Experimental%20results%20demonstrate%20that%0ASylAVL-CoT%20significantly%20outperforms%20text-based%20models%20in%20singability%20and%0Acontextual%20accuracy%2C%20emphasizing%20the%20value%20of%20multimodal%2C%20multilingual%0Aapproaches%20for%20lyrics%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18614v3&entry.124074799=Read"},
{"title": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models", "author": "Amin Abbasishahkoo and Mahboubeh Dadkhah and Lionel Briand and Dayi Lin", "abstract": "  Deep Neural Networks (DNNs) face challenges during deployment due to\ncovariate shift, i.e., data distribution shifts between development and\ndeployment contexts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach tailored for DNN models that have been\nfine-tuned to address covariate shift, to select tests from unlabeled inputs.\nMetaSel assumes that fine-tuned and pre-trained models share related data\ndistributions and exhibit similar behaviors for many inputs. However, their\nbehaviors diverge within the input subspace where fine-tuning alters decision\nboundaries, making those inputs more prone to misclassification. Unlike general\napproaches that rely solely on the DNN model and its input set, MetaSel\nleverages information from both the fine-tuned and pre-trained models and their\nbehavioral differences to estimate misclassification probability for unlabeled\ntest inputs, enabling more effective test selection. Our extensive empirical\nevaluation, comparing MetaSel against 11 state-of-the-art approaches and\ninvolving 68 fine-tuned models across weak, medium, and strong distribution\nshifts, demonstrates that MetaSel consistently delivers significant\nimprovements in Test Relative Coverage (TRC) over existing baselines,\nparticularly under highly constrained labeling budgets. MetaSel shows average\nTRC improvements of 28.46% to 56.18% over the most frequent second-best\nbaselines while maintaining a high TRC median and low variability. Our results\nconfirm MetaSel's practicality, robustness, and cost-effectiveness for test\nselection in the context of fine-tuned models.\n", "link": "http://arxiv.org/abs/2503.17534v4", "date": "2025-09-17", "relevancy": 1.9266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4991}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4988}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaSel%3A%20A%20Test%20Selection%20Approach%20for%20Fine-tuned%20DNN%20Models&body=Title%3A%20MetaSel%3A%20A%20Test%20Selection%20Approach%20for%20Fine-tuned%20DNN%20Models%0AAuthor%3A%20Amin%20Abbasishahkoo%20and%20Mahboubeh%20Dadkhah%20and%20Lionel%20Briand%20and%20Dayi%20Lin%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20face%20challenges%20during%20deployment%20due%20to%0Acovariate%20shift%2C%20i.e.%2C%20data%20distribution%20shifts%20between%20development%20and%0Adeployment%20contexts.%20Fine-tuning%20adapts%20pre-trained%20models%20to%20new%20contexts%0Arequiring%20smaller%20labeled%20sets.%20However%2C%20testing%20fine-tuned%20models%20under%0Aconstrained%20labeling%20budgets%20remains%20a%20critical%20challenge.%20This%20paper%0Aintroduces%20MetaSel%2C%20a%20new%20approach%20tailored%20for%20DNN%20models%20that%20have%20been%0Afine-tuned%20to%20address%20covariate%20shift%2C%20to%20select%20tests%20from%20unlabeled%20inputs.%0AMetaSel%20assumes%20that%20fine-tuned%20and%20pre-trained%20models%20share%20related%20data%0Adistributions%20and%20exhibit%20similar%20behaviors%20for%20many%20inputs.%20However%2C%20their%0Abehaviors%20diverge%20within%20the%20input%20subspace%20where%20fine-tuning%20alters%20decision%0Aboundaries%2C%20making%20those%20inputs%20more%20prone%20to%20misclassification.%20Unlike%20general%0Aapproaches%20that%20rely%20solely%20on%20the%20DNN%20model%20and%20its%20input%20set%2C%20MetaSel%0Aleverages%20information%20from%20both%20the%20fine-tuned%20and%20pre-trained%20models%20and%20their%0Abehavioral%20differences%20to%20estimate%20misclassification%20probability%20for%20unlabeled%0Atest%20inputs%2C%20enabling%20more%20effective%20test%20selection.%20Our%20extensive%20empirical%0Aevaluation%2C%20comparing%20MetaSel%20against%2011%20state-of-the-art%20approaches%20and%0Ainvolving%2068%20fine-tuned%20models%20across%20weak%2C%20medium%2C%20and%20strong%20distribution%0Ashifts%2C%20demonstrates%20that%20MetaSel%20consistently%20delivers%20significant%0Aimprovements%20in%20Test%20Relative%20Coverage%20%28TRC%29%20over%20existing%20baselines%2C%0Aparticularly%20under%20highly%20constrained%20labeling%20budgets.%20MetaSel%20shows%20average%0ATRC%20improvements%20of%2028.46%25%20to%2056.18%25%20over%20the%20most%20frequent%20second-best%0Abaselines%20while%20maintaining%20a%20high%20TRC%20median%20and%20low%20variability.%20Our%20results%0Aconfirm%20MetaSel%27s%20practicality%2C%20robustness%2C%20and%20cost-effectiveness%20for%20test%0Aselection%20in%20the%20context%20of%20fine-tuned%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17534v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaSel%253A%2520A%2520Test%2520Selection%2520Approach%2520for%2520Fine-tuned%2520DNN%2520Models%26entry.906535625%3DAmin%2520Abbasishahkoo%2520and%2520Mahboubeh%2520Dadkhah%2520and%2520Lionel%2520Briand%2520and%2520Dayi%2520Lin%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520face%2520challenges%2520during%2520deployment%2520due%2520to%250Acovariate%2520shift%252C%2520i.e.%252C%2520data%2520distribution%2520shifts%2520between%2520development%2520and%250Adeployment%2520contexts.%2520Fine-tuning%2520adapts%2520pre-trained%2520models%2520to%2520new%2520contexts%250Arequiring%2520smaller%2520labeled%2520sets.%2520However%252C%2520testing%2520fine-tuned%2520models%2520under%250Aconstrained%2520labeling%2520budgets%2520remains%2520a%2520critical%2520challenge.%2520This%2520paper%250Aintroduces%2520MetaSel%252C%2520a%2520new%2520approach%2520tailored%2520for%2520DNN%2520models%2520that%2520have%2520been%250Afine-tuned%2520to%2520address%2520covariate%2520shift%252C%2520to%2520select%2520tests%2520from%2520unlabeled%2520inputs.%250AMetaSel%2520assumes%2520that%2520fine-tuned%2520and%2520pre-trained%2520models%2520share%2520related%2520data%250Adistributions%2520and%2520exhibit%2520similar%2520behaviors%2520for%2520many%2520inputs.%2520However%252C%2520their%250Abehaviors%2520diverge%2520within%2520the%2520input%2520subspace%2520where%2520fine-tuning%2520alters%2520decision%250Aboundaries%252C%2520making%2520those%2520inputs%2520more%2520prone%2520to%2520misclassification.%2520Unlike%2520general%250Aapproaches%2520that%2520rely%2520solely%2520on%2520the%2520DNN%2520model%2520and%2520its%2520input%2520set%252C%2520MetaSel%250Aleverages%2520information%2520from%2520both%2520the%2520fine-tuned%2520and%2520pre-trained%2520models%2520and%2520their%250Abehavioral%2520differences%2520to%2520estimate%2520misclassification%2520probability%2520for%2520unlabeled%250Atest%2520inputs%252C%2520enabling%2520more%2520effective%2520test%2520selection.%2520Our%2520extensive%2520empirical%250Aevaluation%252C%2520comparing%2520MetaSel%2520against%252011%2520state-of-the-art%2520approaches%2520and%250Ainvolving%252068%2520fine-tuned%2520models%2520across%2520weak%252C%2520medium%252C%2520and%2520strong%2520distribution%250Ashifts%252C%2520demonstrates%2520that%2520MetaSel%2520consistently%2520delivers%2520significant%250Aimprovements%2520in%2520Test%2520Relative%2520Coverage%2520%2528TRC%2529%2520over%2520existing%2520baselines%252C%250Aparticularly%2520under%2520highly%2520constrained%2520labeling%2520budgets.%2520MetaSel%2520shows%2520average%250ATRC%2520improvements%2520of%252028.46%2525%2520to%252056.18%2525%2520over%2520the%2520most%2520frequent%2520second-best%250Abaselines%2520while%2520maintaining%2520a%2520high%2520TRC%2520median%2520and%2520low%2520variability.%2520Our%2520results%250Aconfirm%2520MetaSel%2527s%2520practicality%252C%2520robustness%252C%2520and%2520cost-effectiveness%2520for%2520test%250Aselection%2520in%2520the%2520context%2520of%2520fine-tuned%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17534v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaSel%3A%20A%20Test%20Selection%20Approach%20for%20Fine-tuned%20DNN%20Models&entry.906535625=Amin%20Abbasishahkoo%20and%20Mahboubeh%20Dadkhah%20and%20Lionel%20Briand%20and%20Dayi%20Lin&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20face%20challenges%20during%20deployment%20due%20to%0Acovariate%20shift%2C%20i.e.%2C%20data%20distribution%20shifts%20between%20development%20and%0Adeployment%20contexts.%20Fine-tuning%20adapts%20pre-trained%20models%20to%20new%20contexts%0Arequiring%20smaller%20labeled%20sets.%20However%2C%20testing%20fine-tuned%20models%20under%0Aconstrained%20labeling%20budgets%20remains%20a%20critical%20challenge.%20This%20paper%0Aintroduces%20MetaSel%2C%20a%20new%20approach%20tailored%20for%20DNN%20models%20that%20have%20been%0Afine-tuned%20to%20address%20covariate%20shift%2C%20to%20select%20tests%20from%20unlabeled%20inputs.%0AMetaSel%20assumes%20that%20fine-tuned%20and%20pre-trained%20models%20share%20related%20data%0Adistributions%20and%20exhibit%20similar%20behaviors%20for%20many%20inputs.%20However%2C%20their%0Abehaviors%20diverge%20within%20the%20input%20subspace%20where%20fine-tuning%20alters%20decision%0Aboundaries%2C%20making%20those%20inputs%20more%20prone%20to%20misclassification.%20Unlike%20general%0Aapproaches%20that%20rely%20solely%20on%20the%20DNN%20model%20and%20its%20input%20set%2C%20MetaSel%0Aleverages%20information%20from%20both%20the%20fine-tuned%20and%20pre-trained%20models%20and%20their%0Abehavioral%20differences%20to%20estimate%20misclassification%20probability%20for%20unlabeled%0Atest%20inputs%2C%20enabling%20more%20effective%20test%20selection.%20Our%20extensive%20empirical%0Aevaluation%2C%20comparing%20MetaSel%20against%2011%20state-of-the-art%20approaches%20and%0Ainvolving%2068%20fine-tuned%20models%20across%20weak%2C%20medium%2C%20and%20strong%20distribution%0Ashifts%2C%20demonstrates%20that%20MetaSel%20consistently%20delivers%20significant%0Aimprovements%20in%20Test%20Relative%20Coverage%20%28TRC%29%20over%20existing%20baselines%2C%0Aparticularly%20under%20highly%20constrained%20labeling%20budgets.%20MetaSel%20shows%20average%0ATRC%20improvements%20of%2028.46%25%20to%2056.18%25%20over%20the%20most%20frequent%20second-best%0Abaselines%20while%20maintaining%20a%20high%20TRC%20median%20and%20low%20variability.%20Our%20results%0Aconfirm%20MetaSel%27s%20practicality%2C%20robustness%2C%20and%20cost-effectiveness%20for%20test%0Aselection%20in%20the%20context%20of%20fine-tuned%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17534v4&entry.124074799=Read"},
{"title": "Enabling Local Neural Operators to perform Equation-Free System-Level\n  Analysis", "author": "Gianluca Fabiani and Hannes Vandecasteele and Somdatta Goswami and Constantinos Siettos and Ioannis G. Kevrekidis", "abstract": "  Neural Operators (NOs) provide a powerful framework for computations\ninvolving physical laws that can be modelled by (integro-) partial differential\nequations (PDEs), directly learning maps between infinite-dimensional function\nspaces that bypass both the explicit equation identification and their\nsubsequent numerical solving. Still, NOs have so far primarily been employed to\nexplore the dynamical behavior as surrogates of brute-force temporal\nsimulations/predictions. Their potential for systematic rigorous numerical\nsystem-level tasks, such as fixed-point, stability, and bifurcation analysis -\ncrucial for predicting irreversible transitions in real-world phenomena -\nremains largely unexplored. Toward this aim, inspired by the Equation-Free\nmultiscale framework, we propose and implement a framework that integrates\n(local) NOs with advanced iterative numerical methods in the Krylov subspace,\nso as to perform efficient system-level stability and bifurcation analysis of\nlarge-scale dynamical systems. Beyond fixed point, stability, and bifurcation\nanalysis enabled by local in time NOs, we also demonstrate the usefulness of\nlocal in space as well as in space-time (\"patch\") NOs in accelerating the\ncomputer-aided analysis of spatiotemporal dynamics. We illustrate our framework\nvia three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes\nmultiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE,\nwhich features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN)\nmodel, consisting of two coupled PDEs that exhibit both Hopf and saddle-node\nbifurcations.\n", "link": "http://arxiv.org/abs/2505.02308v2", "date": "2025-09-17", "relevancy": 1.9121, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Local%20Neural%20Operators%20to%20perform%20Equation-Free%20System-Level%0A%20%20Analysis&body=Title%3A%20Enabling%20Local%20Neural%20Operators%20to%20perform%20Equation-Free%20System-Level%0A%20%20Analysis%0AAuthor%3A%20Gianluca%20Fabiani%20and%20Hannes%20Vandecasteele%20and%20Somdatta%20Goswami%20and%20Constantinos%20Siettos%20and%20Ioannis%20G.%20Kevrekidis%0AAbstract%3A%20%20%20Neural%20Operators%20%28NOs%29%20provide%20a%20powerful%20framework%20for%20computations%0Ainvolving%20physical%20laws%20that%20can%20be%20modelled%20by%20%28integro-%29%20partial%20differential%0Aequations%20%28PDEs%29%2C%20directly%20learning%20maps%20between%20infinite-dimensional%20function%0Aspaces%20that%20bypass%20both%20the%20explicit%20equation%20identification%20and%20their%0Asubsequent%20numerical%20solving.%20Still%2C%20NOs%20have%20so%20far%20primarily%20been%20employed%20to%0Aexplore%20the%20dynamical%20behavior%20as%20surrogates%20of%20brute-force%20temporal%0Asimulations/predictions.%20Their%20potential%20for%20systematic%20rigorous%20numerical%0Asystem-level%20tasks%2C%20such%20as%20fixed-point%2C%20stability%2C%20and%20bifurcation%20analysis%20-%0Acrucial%20for%20predicting%20irreversible%20transitions%20in%20real-world%20phenomena%20-%0Aremains%20largely%20unexplored.%20Toward%20this%20aim%2C%20inspired%20by%20the%20Equation-Free%0Amultiscale%20framework%2C%20we%20propose%20and%20implement%20a%20framework%20that%20integrates%0A%28local%29%20NOs%20with%20advanced%20iterative%20numerical%20methods%20in%20the%20Krylov%20subspace%2C%0Aso%20as%20to%20perform%20efficient%20system-level%20stability%20and%20bifurcation%20analysis%20of%0Alarge-scale%20dynamical%20systems.%20Beyond%20fixed%20point%2C%20stability%2C%20and%20bifurcation%0Aanalysis%20enabled%20by%20local%20in%20time%20NOs%2C%20we%20also%20demonstrate%20the%20usefulness%20of%0Alocal%20in%20space%20as%20well%20as%20in%20space-time%20%28%22patch%22%29%20NOs%20in%20accelerating%20the%0Acomputer-aided%20analysis%20of%20spatiotemporal%20dynamics.%20We%20illustrate%20our%20framework%0Avia%20three%20nonlinear%20PDE%20benchmarks%3A%20the%201D%20Allen-Cahn%20equation%2C%20which%20undergoes%0Amultiple%20concatenated%20pitchfork%20bifurcations%3B%20the%20Liouville-Bratu-Gelfand%20PDE%2C%0Awhich%20features%20a%20saddle-node%20tipping%20point%3B%20and%20the%20FitzHugh-Nagumo%20%28FHN%29%0Amodel%2C%20consisting%20of%20two%20coupled%20PDEs%20that%20exhibit%20both%20Hopf%20and%20saddle-node%0Abifurcations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Local%2520Neural%2520Operators%2520to%2520perform%2520Equation-Free%2520System-Level%250A%2520%2520Analysis%26entry.906535625%3DGianluca%2520Fabiani%2520and%2520Hannes%2520Vandecasteele%2520and%2520Somdatta%2520Goswami%2520and%2520Constantinos%2520Siettos%2520and%2520Ioannis%2520G.%2520Kevrekidis%26entry.1292438233%3D%2520%2520Neural%2520Operators%2520%2528NOs%2529%2520provide%2520a%2520powerful%2520framework%2520for%2520computations%250Ainvolving%2520physical%2520laws%2520that%2520can%2520be%2520modelled%2520by%2520%2528integro-%2529%2520partial%2520differential%250Aequations%2520%2528PDEs%2529%252C%2520directly%2520learning%2520maps%2520between%2520infinite-dimensional%2520function%250Aspaces%2520that%2520bypass%2520both%2520the%2520explicit%2520equation%2520identification%2520and%2520their%250Asubsequent%2520numerical%2520solving.%2520Still%252C%2520NOs%2520have%2520so%2520far%2520primarily%2520been%2520employed%2520to%250Aexplore%2520the%2520dynamical%2520behavior%2520as%2520surrogates%2520of%2520brute-force%2520temporal%250Asimulations/predictions.%2520Their%2520potential%2520for%2520systematic%2520rigorous%2520numerical%250Asystem-level%2520tasks%252C%2520such%2520as%2520fixed-point%252C%2520stability%252C%2520and%2520bifurcation%2520analysis%2520-%250Acrucial%2520for%2520predicting%2520irreversible%2520transitions%2520in%2520real-world%2520phenomena%2520-%250Aremains%2520largely%2520unexplored.%2520Toward%2520this%2520aim%252C%2520inspired%2520by%2520the%2520Equation-Free%250Amultiscale%2520framework%252C%2520we%2520propose%2520and%2520implement%2520a%2520framework%2520that%2520integrates%250A%2528local%2529%2520NOs%2520with%2520advanced%2520iterative%2520numerical%2520methods%2520in%2520the%2520Krylov%2520subspace%252C%250Aso%2520as%2520to%2520perform%2520efficient%2520system-level%2520stability%2520and%2520bifurcation%2520analysis%2520of%250Alarge-scale%2520dynamical%2520systems.%2520Beyond%2520fixed%2520point%252C%2520stability%252C%2520and%2520bifurcation%250Aanalysis%2520enabled%2520by%2520local%2520in%2520time%2520NOs%252C%2520we%2520also%2520demonstrate%2520the%2520usefulness%2520of%250Alocal%2520in%2520space%2520as%2520well%2520as%2520in%2520space-time%2520%2528%2522patch%2522%2529%2520NOs%2520in%2520accelerating%2520the%250Acomputer-aided%2520analysis%2520of%2520spatiotemporal%2520dynamics.%2520We%2520illustrate%2520our%2520framework%250Avia%2520three%2520nonlinear%2520PDE%2520benchmarks%253A%2520the%25201D%2520Allen-Cahn%2520equation%252C%2520which%2520undergoes%250Amultiple%2520concatenated%2520pitchfork%2520bifurcations%253B%2520the%2520Liouville-Bratu-Gelfand%2520PDE%252C%250Awhich%2520features%2520a%2520saddle-node%2520tipping%2520point%253B%2520and%2520the%2520FitzHugh-Nagumo%2520%2528FHN%2529%250Amodel%252C%2520consisting%2520of%2520two%2520coupled%2520PDEs%2520that%2520exhibit%2520both%2520Hopf%2520and%2520saddle-node%250Abifurcations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Local%20Neural%20Operators%20to%20perform%20Equation-Free%20System-Level%0A%20%20Analysis&entry.906535625=Gianluca%20Fabiani%20and%20Hannes%20Vandecasteele%20and%20Somdatta%20Goswami%20and%20Constantinos%20Siettos%20and%20Ioannis%20G.%20Kevrekidis&entry.1292438233=%20%20Neural%20Operators%20%28NOs%29%20provide%20a%20powerful%20framework%20for%20computations%0Ainvolving%20physical%20laws%20that%20can%20be%20modelled%20by%20%28integro-%29%20partial%20differential%0Aequations%20%28PDEs%29%2C%20directly%20learning%20maps%20between%20infinite-dimensional%20function%0Aspaces%20that%20bypass%20both%20the%20explicit%20equation%20identification%20and%20their%0Asubsequent%20numerical%20solving.%20Still%2C%20NOs%20have%20so%20far%20primarily%20been%20employed%20to%0Aexplore%20the%20dynamical%20behavior%20as%20surrogates%20of%20brute-force%20temporal%0Asimulations/predictions.%20Their%20potential%20for%20systematic%20rigorous%20numerical%0Asystem-level%20tasks%2C%20such%20as%20fixed-point%2C%20stability%2C%20and%20bifurcation%20analysis%20-%0Acrucial%20for%20predicting%20irreversible%20transitions%20in%20real-world%20phenomena%20-%0Aremains%20largely%20unexplored.%20Toward%20this%20aim%2C%20inspired%20by%20the%20Equation-Free%0Amultiscale%20framework%2C%20we%20propose%20and%20implement%20a%20framework%20that%20integrates%0A%28local%29%20NOs%20with%20advanced%20iterative%20numerical%20methods%20in%20the%20Krylov%20subspace%2C%0Aso%20as%20to%20perform%20efficient%20system-level%20stability%20and%20bifurcation%20analysis%20of%0Alarge-scale%20dynamical%20systems.%20Beyond%20fixed%20point%2C%20stability%2C%20and%20bifurcation%0Aanalysis%20enabled%20by%20local%20in%20time%20NOs%2C%20we%20also%20demonstrate%20the%20usefulness%20of%0Alocal%20in%20space%20as%20well%20as%20in%20space-time%20%28%22patch%22%29%20NOs%20in%20accelerating%20the%0Acomputer-aided%20analysis%20of%20spatiotemporal%20dynamics.%20We%20illustrate%20our%20framework%0Avia%20three%20nonlinear%20PDE%20benchmarks%3A%20the%201D%20Allen-Cahn%20equation%2C%20which%20undergoes%0Amultiple%20concatenated%20pitchfork%20bifurcations%3B%20the%20Liouville-Bratu-Gelfand%20PDE%2C%0Awhich%20features%20a%20saddle-node%20tipping%20point%3B%20and%20the%20FitzHugh-Nagumo%20%28FHN%29%0Amodel%2C%20consisting%20of%20two%20coupled%20PDEs%20that%20exhibit%20both%20Hopf%20and%20saddle-node%0Abifurcations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02308v2&entry.124074799=Read"},
{"title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd\n  Multi-modal Embeddings", "author": "Suhang You and Carla Pitarch-Abaigar and Sanket Kachole and Sumedh Sonawane and Juhyung Ha and Anish Sudarshan Gada and David Crandall and Rakesh Shiradkar and Spyridon Bakas", "abstract": "  Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7103 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.\n", "link": "http://arxiv.org/abs/2509.14051v1", "date": "2025-09-17", "relevancy": 1.9016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROFUSEme%3A%20PROstate%20Cancer%20Biochemical%20Recurrence%20Prediction%20via%20FUSEd%0A%20%20Multi-modal%20Embeddings&body=Title%3A%20PROFUSEme%3A%20PROstate%20Cancer%20Biochemical%20Recurrence%20Prediction%20via%20FUSEd%0A%20%20Multi-modal%20Embeddings%0AAuthor%3A%20Suhang%20You%20and%20Carla%20Pitarch-Abaigar%20and%20Sanket%20Kachole%20and%20Sumedh%20Sonawane%20and%20Juhyung%20Ha%20and%20Anish%20Sudarshan%20Gada%20and%20David%20Crandall%20and%20Rakesh%20Shiradkar%20and%20Spyridon%20Bakas%0AAbstract%3A%20%20%20Almost%2030%25%20of%20prostate%20cancer%20%28PCa%29%20patients%20undergoing%20radical%20prostatectomy%0A%28RP%29%20experience%20biochemical%20recurrence%20%28BCR%29%2C%20characterized%20by%20increased%0Aprostate%20specific%20antigen%20%28PSA%29%20and%20associated%20with%20increased%20mortality.%0AAccurate%20early%20prediction%20of%20BCR%2C%20at%20the%20time%20of%20RP%2C%20would%20contribute%20to%20prompt%0Aadaptive%20clinical%20decision-making%20and%20improved%20patient%20outcomes.%20In%20this%20work%2C%0Awe%20propose%20prostate%20cancer%20BCR%20prediction%20via%20fused%20multi-modal%20embeddings%0A%28PROFUSEme%29%2C%20which%20learns%20cross-modal%20interactions%20of%20clinical%2C%20radiology%2C%20and%0Apathology%20data%2C%20following%20an%20intermediate%20fusion%20configuration%20in%20combination%0Awith%20Cox%20Proportional%20Hazard%20regressors.%20Quantitative%20evaluation%20of%20our%0Aproposed%20approach%20reveals%20superior%20performance%2C%20when%20compared%20with%20late%20fusion%0Aconfigurations%2C%20yielding%20a%20mean%20C-index%20of%200.861%20%28%24%5Csigma%3D0.112%24%29%20on%20the%0Ainternal%205-fold%20nested%20cross-validation%20framework%2C%20and%20a%20C-index%20of%200.7103%20on%0Athe%20hold%20out%20data%20of%20CHIMERA%202025%20challenge%20validation%20leaderboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROFUSEme%253A%2520PROstate%2520Cancer%2520Biochemical%2520Recurrence%2520Prediction%2520via%2520FUSEd%250A%2520%2520Multi-modal%2520Embeddings%26entry.906535625%3DSuhang%2520You%2520and%2520Carla%2520Pitarch-Abaigar%2520and%2520Sanket%2520Kachole%2520and%2520Sumedh%2520Sonawane%2520and%2520Juhyung%2520Ha%2520and%2520Anish%2520Sudarshan%2520Gada%2520and%2520David%2520Crandall%2520and%2520Rakesh%2520Shiradkar%2520and%2520Spyridon%2520Bakas%26entry.1292438233%3D%2520%2520Almost%252030%2525%2520of%2520prostate%2520cancer%2520%2528PCa%2529%2520patients%2520undergoing%2520radical%2520prostatectomy%250A%2528RP%2529%2520experience%2520biochemical%2520recurrence%2520%2528BCR%2529%252C%2520characterized%2520by%2520increased%250Aprostate%2520specific%2520antigen%2520%2528PSA%2529%2520and%2520associated%2520with%2520increased%2520mortality.%250AAccurate%2520early%2520prediction%2520of%2520BCR%252C%2520at%2520the%2520time%2520of%2520RP%252C%2520would%2520contribute%2520to%2520prompt%250Aadaptive%2520clinical%2520decision-making%2520and%2520improved%2520patient%2520outcomes.%2520In%2520this%2520work%252C%250Awe%2520propose%2520prostate%2520cancer%2520BCR%2520prediction%2520via%2520fused%2520multi-modal%2520embeddings%250A%2528PROFUSEme%2529%252C%2520which%2520learns%2520cross-modal%2520interactions%2520of%2520clinical%252C%2520radiology%252C%2520and%250Apathology%2520data%252C%2520following%2520an%2520intermediate%2520fusion%2520configuration%2520in%2520combination%250Awith%2520Cox%2520Proportional%2520Hazard%2520regressors.%2520Quantitative%2520evaluation%2520of%2520our%250Aproposed%2520approach%2520reveals%2520superior%2520performance%252C%2520when%2520compared%2520with%2520late%2520fusion%250Aconfigurations%252C%2520yielding%2520a%2520mean%2520C-index%2520of%25200.861%2520%2528%2524%255Csigma%253D0.112%2524%2529%2520on%2520the%250Ainternal%25205-fold%2520nested%2520cross-validation%2520framework%252C%2520and%2520a%2520C-index%2520of%25200.7103%2520on%250Athe%2520hold%2520out%2520data%2520of%2520CHIMERA%25202025%2520challenge%2520validation%2520leaderboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROFUSEme%3A%20PROstate%20Cancer%20Biochemical%20Recurrence%20Prediction%20via%20FUSEd%0A%20%20Multi-modal%20Embeddings&entry.906535625=Suhang%20You%20and%20Carla%20Pitarch-Abaigar%20and%20Sanket%20Kachole%20and%20Sumedh%20Sonawane%20and%20Juhyung%20Ha%20and%20Anish%20Sudarshan%20Gada%20and%20David%20Crandall%20and%20Rakesh%20Shiradkar%20and%20Spyridon%20Bakas&entry.1292438233=%20%20Almost%2030%25%20of%20prostate%20cancer%20%28PCa%29%20patients%20undergoing%20radical%20prostatectomy%0A%28RP%29%20experience%20biochemical%20recurrence%20%28BCR%29%2C%20characterized%20by%20increased%0Aprostate%20specific%20antigen%20%28PSA%29%20and%20associated%20with%20increased%20mortality.%0AAccurate%20early%20prediction%20of%20BCR%2C%20at%20the%20time%20of%20RP%2C%20would%20contribute%20to%20prompt%0Aadaptive%20clinical%20decision-making%20and%20improved%20patient%20outcomes.%20In%20this%20work%2C%0Awe%20propose%20prostate%20cancer%20BCR%20prediction%20via%20fused%20multi-modal%20embeddings%0A%28PROFUSEme%29%2C%20which%20learns%20cross-modal%20interactions%20of%20clinical%2C%20radiology%2C%20and%0Apathology%20data%2C%20following%20an%20intermediate%20fusion%20configuration%20in%20combination%0Awith%20Cox%20Proportional%20Hazard%20regressors.%20Quantitative%20evaluation%20of%20our%0Aproposed%20approach%20reveals%20superior%20performance%2C%20when%20compared%20with%20late%20fusion%0Aconfigurations%2C%20yielding%20a%20mean%20C-index%20of%200.861%20%28%24%5Csigma%3D0.112%24%29%20on%20the%0Ainternal%205-fold%20nested%20cross-validation%20framework%2C%20and%20a%20C-index%20of%200.7103%20on%0Athe%20hold%20out%20data%20of%20CHIMERA%202025%20challenge%20validation%20leaderboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14051v1&entry.124074799=Read"},
{"title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause\n  Frequencies", "author": "Terrance Liu and Shuyi Wang and Daniel Preotiuc-Pietro and Yash Chandarana and Chirag Gupta", "abstract": "  While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.\n", "link": "http://arxiv.org/abs/2505.23804v2", "date": "2025-09-17", "relevancy": 1.8904, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrating%20LLMs%20for%20Text-to-SQL%20Parsing%20by%20Leveraging%20Sub-clause%0A%20%20Frequencies&body=Title%3A%20Calibrating%20LLMs%20for%20Text-to-SQL%20Parsing%20by%20Leveraging%20Sub-clause%0A%20%20Frequencies%0AAuthor%3A%20Terrance%20Liu%20and%20Shuyi%20Wang%20and%20Daniel%20Preotiuc-Pietro%20and%20Yash%20Chandarana%20and%20Chirag%20Gupta%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20on%20text-to-SQL%0Aparsing%2C%20they%20sometimes%20exhibit%20unexpected%20failures%20in%20which%20they%20are%0Aconfidently%20incorrect.%20Building%20trustworthy%20text-to-SQL%20systems%20thus%20requires%0Aeliciting%20reliable%20uncertainty%20measures%20from%20the%20LLM.%20In%20this%20paper%2C%20we%20study%0Athe%20problem%20of%20providing%20a%20calibrated%20confidence%20score%20that%20conveys%20the%0Alikelihood%20of%20an%20output%20query%20being%20correct.%20Our%20work%20is%20the%20first%20to%20establish%0Aa%20benchmark%20for%20post-hoc%20calibration%20of%20LLM-based%20text-to-SQL%20parsing.%20In%0Aparticular%2C%20we%20show%20that%20Platt%20scaling%2C%20a%20canonical%20method%20for%20calibration%2C%0Aprovides%20substantial%20improvements%20over%20directly%20using%20raw%20model%20output%0Aprobabilities%20as%20confidence%20scores.%20Furthermore%2C%20we%20propose%20a%20method%20for%0Atext-to-SQL%20calibration%20that%20leverages%20the%20structured%20nature%20of%20SQL%20queries%20to%0Aprovide%20more%20granular%20signals%20of%20correctness%2C%20named%20%22sub-clause%20frequency%22%0A%28SCF%29%20scores.%20Using%20multivariate%20Platt%20scaling%20%28MPS%29%2C%20our%20extension%20of%20the%0Acanonical%20Platt%20scaling%20technique%2C%20we%20combine%20individual%20SCF%20scores%20into%20an%0Aoverall%20accurate%20and%20calibrated%20score.%20Empirical%20evaluation%20on%20two%20popular%0Atext-to-SQL%20datasets%20shows%20that%20our%20approach%20of%20combining%20MPS%20and%20SCF%20yields%0Afurther%20improvements%20in%20calibration%20and%20the%20related%20task%20of%20error%20detection%0Aover%20traditional%20Platt%20scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23804v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrating%2520LLMs%2520for%2520Text-to-SQL%2520Parsing%2520by%2520Leveraging%2520Sub-clause%250A%2520%2520Frequencies%26entry.906535625%3DTerrance%2520Liu%2520and%2520Shuyi%2520Wang%2520and%2520Daniel%2520Preotiuc-Pietro%2520and%2520Yash%2520Chandarana%2520and%2520Chirag%2520Gupta%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520strong%2520performance%2520on%2520text-to-SQL%250Aparsing%252C%2520they%2520sometimes%2520exhibit%2520unexpected%2520failures%2520in%2520which%2520they%2520are%250Aconfidently%2520incorrect.%2520Building%2520trustworthy%2520text-to-SQL%2520systems%2520thus%2520requires%250Aeliciting%2520reliable%2520uncertainty%2520measures%2520from%2520the%2520LLM.%2520In%2520this%2520paper%252C%2520we%2520study%250Athe%2520problem%2520of%2520providing%2520a%2520calibrated%2520confidence%2520score%2520that%2520conveys%2520the%250Alikelihood%2520of%2520an%2520output%2520query%2520being%2520correct.%2520Our%2520work%2520is%2520the%2520first%2520to%2520establish%250Aa%2520benchmark%2520for%2520post-hoc%2520calibration%2520of%2520LLM-based%2520text-to-SQL%2520parsing.%2520In%250Aparticular%252C%2520we%2520show%2520that%2520Platt%2520scaling%252C%2520a%2520canonical%2520method%2520for%2520calibration%252C%250Aprovides%2520substantial%2520improvements%2520over%2520directly%2520using%2520raw%2520model%2520output%250Aprobabilities%2520as%2520confidence%2520scores.%2520Furthermore%252C%2520we%2520propose%2520a%2520method%2520for%250Atext-to-SQL%2520calibration%2520that%2520leverages%2520the%2520structured%2520nature%2520of%2520SQL%2520queries%2520to%250Aprovide%2520more%2520granular%2520signals%2520of%2520correctness%252C%2520named%2520%2522sub-clause%2520frequency%2522%250A%2528SCF%2529%2520scores.%2520Using%2520multivariate%2520Platt%2520scaling%2520%2528MPS%2529%252C%2520our%2520extension%2520of%2520the%250Acanonical%2520Platt%2520scaling%2520technique%252C%2520we%2520combine%2520individual%2520SCF%2520scores%2520into%2520an%250Aoverall%2520accurate%2520and%2520calibrated%2520score.%2520Empirical%2520evaluation%2520on%2520two%2520popular%250Atext-to-SQL%2520datasets%2520shows%2520that%2520our%2520approach%2520of%2520combining%2520MPS%2520and%2520SCF%2520yields%250Afurther%2520improvements%2520in%2520calibration%2520and%2520the%2520related%2520task%2520of%2520error%2520detection%250Aover%2520traditional%2520Platt%2520scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23804v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20LLMs%20for%20Text-to-SQL%20Parsing%20by%20Leveraging%20Sub-clause%0A%20%20Frequencies&entry.906535625=Terrance%20Liu%20and%20Shuyi%20Wang%20and%20Daniel%20Preotiuc-Pietro%20and%20Yash%20Chandarana%20and%20Chirag%20Gupta&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20achieve%20strong%20performance%20on%20text-to-SQL%0Aparsing%2C%20they%20sometimes%20exhibit%20unexpected%20failures%20in%20which%20they%20are%0Aconfidently%20incorrect.%20Building%20trustworthy%20text-to-SQL%20systems%20thus%20requires%0Aeliciting%20reliable%20uncertainty%20measures%20from%20the%20LLM.%20In%20this%20paper%2C%20we%20study%0Athe%20problem%20of%20providing%20a%20calibrated%20confidence%20score%20that%20conveys%20the%0Alikelihood%20of%20an%20output%20query%20being%20correct.%20Our%20work%20is%20the%20first%20to%20establish%0Aa%20benchmark%20for%20post-hoc%20calibration%20of%20LLM-based%20text-to-SQL%20parsing.%20In%0Aparticular%2C%20we%20show%20that%20Platt%20scaling%2C%20a%20canonical%20method%20for%20calibration%2C%0Aprovides%20substantial%20improvements%20over%20directly%20using%20raw%20model%20output%0Aprobabilities%20as%20confidence%20scores.%20Furthermore%2C%20we%20propose%20a%20method%20for%0Atext-to-SQL%20calibration%20that%20leverages%20the%20structured%20nature%20of%20SQL%20queries%20to%0Aprovide%20more%20granular%20signals%20of%20correctness%2C%20named%20%22sub-clause%20frequency%22%0A%28SCF%29%20scores.%20Using%20multivariate%20Platt%20scaling%20%28MPS%29%2C%20our%20extension%20of%20the%0Acanonical%20Platt%20scaling%20technique%2C%20we%20combine%20individual%20SCF%20scores%20into%20an%0Aoverall%20accurate%20and%20calibrated%20score.%20Empirical%20evaluation%20on%20two%20popular%0Atext-to-SQL%20datasets%20shows%20that%20our%20approach%20of%20combining%20MPS%20and%20SCF%20yields%0Afurther%20improvements%20in%20calibration%20and%20the%20related%20task%20of%20error%20detection%0Aover%20traditional%20Platt%20scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23804v2&entry.124074799=Read"},
{"title": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions", "author": "Xiyu Zhou and Ruiyin Li and Peng Liang and Beiqi Zhang and Mojtaba Shahin and Zengyang Li and Chen Yang", "abstract": "  Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading. To\nfurther understand the trustworthiness and applicability of LLM-generated DR in\npractice, we conducted semi-structured interviews with six practitioners. Based\non the experimental and interview results, we discussed the pros and cons of\nthe three prompting strategies, the strengths and limitations of LLM-generated\nDR, and the implications for the practical use of LLM-generated DR.\n", "link": "http://arxiv.org/abs/2504.20781v2", "date": "2025-09-17", "relevancy": 1.8828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20LLMs%20in%20Generating%20Design%20Rationale%20for%20Software%20Architecture%0A%20%20Decisions&body=Title%3A%20Using%20LLMs%20in%20Generating%20Design%20Rationale%20for%20Software%20Architecture%0A%20%20Decisions%0AAuthor%3A%20Xiyu%20Zhou%20and%20Ruiyin%20Li%20and%20Peng%20Liang%20and%20Beiqi%20Zhang%20and%20Mojtaba%20Shahin%20and%20Zengyang%20Li%20and%20Chen%20Yang%0AAbstract%3A%20%20%20Design%20Rationale%20%28DR%29%20for%20software%20architecture%20decisions%20refers%20to%20the%0Areasoning%20underlying%20architectural%20choices%2C%20which%20provides%20valuable%20insights%0Ainto%20the%20different%20phases%20of%20the%20architecting%20process%20throughout%20software%0Adevelopment.%20However%2C%20in%20practice%2C%20DR%20is%20often%20inadequately%20documented%20due%20to%20a%0Alack%20of%20motivation%20and%20effort%20from%20developers.%20With%20the%20recent%20advancements%20in%0ALarge%20Language%20Models%20%28LLMs%29%2C%20their%20capabilities%20in%20text%20comprehension%2C%0Areasoning%2C%20and%20generation%20may%20enable%20the%20generation%20and%20recovery%20of%20DR%20for%0Aarchitecture%20decisions.%20In%20this%20study%2C%20we%20evaluated%20the%20performance%20of%20LLMs%20in%0Agenerating%20DR%20for%20architecture%20decisions.%20First%2C%20we%20collected%2050%20Stack%20Overflow%0A%28SO%29%20posts%2C%2025%20GitHub%20issues%2C%20and%2025%20GitHub%20discussions%20related%20to%20architecture%0Adecisions%20to%20construct%20a%20dataset%20of%20100%20architecture-related%20problems.%20Then%2C%20we%0Aselected%20five%20LLMs%20to%20generate%20DR%20for%20the%20architecture%20decisions%20with%20three%0Aprompting%20strategies%2C%20including%20zero-shot%2C%20chain%20of%20thought%20%28CoT%29%2C%20and%0ALLM-based%20agents.%20With%20the%20DR%20provided%20by%20human%20experts%20as%20ground%20truth%2C%20the%0APrecision%20of%20LLM-generated%20DR%20with%20the%20three%20prompting%20strategies%20ranges%20from%0A0.267%20to%200.278%2C%20Recall%20from%200.627%20to%200.715%2C%20and%20F1-score%20from%200.351%20to%200.389.%0AAdditionally%2C%2064.45%25%20to%2069.42%25%20of%20the%20arguments%20of%20DR%20not%20mentioned%20by%20human%0Aexperts%20are%20also%20helpful%2C%204.12%25%20to%204.87%25%20of%20the%20arguments%20have%20uncertain%0Acorrectness%2C%20and%201.59%25%20to%203.24%25%20of%20the%20arguments%20are%20potentially%20misleading.%20To%0Afurther%20understand%20the%20trustworthiness%20and%20applicability%20of%20LLM-generated%20DR%20in%0Apractice%2C%20we%20conducted%20semi-structured%20interviews%20with%20six%20practitioners.%20Based%0Aon%20the%20experimental%20and%20interview%20results%2C%20we%20discussed%20the%20pros%20and%20cons%20of%0Athe%20three%20prompting%20strategies%2C%20the%20strengths%20and%20limitations%20of%20LLM-generated%0ADR%2C%20and%20the%20implications%20for%20the%20practical%20use%20of%20LLM-generated%20DR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20781v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520LLMs%2520in%2520Generating%2520Design%2520Rationale%2520for%2520Software%2520Architecture%250A%2520%2520Decisions%26entry.906535625%3DXiyu%2520Zhou%2520and%2520Ruiyin%2520Li%2520and%2520Peng%2520Liang%2520and%2520Beiqi%2520Zhang%2520and%2520Mojtaba%2520Shahin%2520and%2520Zengyang%2520Li%2520and%2520Chen%2520Yang%26entry.1292438233%3D%2520%2520Design%2520Rationale%2520%2528DR%2529%2520for%2520software%2520architecture%2520decisions%2520refers%2520to%2520the%250Areasoning%2520underlying%2520architectural%2520choices%252C%2520which%2520provides%2520valuable%2520insights%250Ainto%2520the%2520different%2520phases%2520of%2520the%2520architecting%2520process%2520throughout%2520software%250Adevelopment.%2520However%252C%2520in%2520practice%252C%2520DR%2520is%2520often%2520inadequately%2520documented%2520due%2520to%2520a%250Alack%2520of%2520motivation%2520and%2520effort%2520from%2520developers.%2520With%2520the%2520recent%2520advancements%2520in%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520their%2520capabilities%2520in%2520text%2520comprehension%252C%250Areasoning%252C%2520and%2520generation%2520may%2520enable%2520the%2520generation%2520and%2520recovery%2520of%2520DR%2520for%250Aarchitecture%2520decisions.%2520In%2520this%2520study%252C%2520we%2520evaluated%2520the%2520performance%2520of%2520LLMs%2520in%250Agenerating%2520DR%2520for%2520architecture%2520decisions.%2520First%252C%2520we%2520collected%252050%2520Stack%2520Overflow%250A%2528SO%2529%2520posts%252C%252025%2520GitHub%2520issues%252C%2520and%252025%2520GitHub%2520discussions%2520related%2520to%2520architecture%250Adecisions%2520to%2520construct%2520a%2520dataset%2520of%2520100%2520architecture-related%2520problems.%2520Then%252C%2520we%250Aselected%2520five%2520LLMs%2520to%2520generate%2520DR%2520for%2520the%2520architecture%2520decisions%2520with%2520three%250Aprompting%2520strategies%252C%2520including%2520zero-shot%252C%2520chain%2520of%2520thought%2520%2528CoT%2529%252C%2520and%250ALLM-based%2520agents.%2520With%2520the%2520DR%2520provided%2520by%2520human%2520experts%2520as%2520ground%2520truth%252C%2520the%250APrecision%2520of%2520LLM-generated%2520DR%2520with%2520the%2520three%2520prompting%2520strategies%2520ranges%2520from%250A0.267%2520to%25200.278%252C%2520Recall%2520from%25200.627%2520to%25200.715%252C%2520and%2520F1-score%2520from%25200.351%2520to%25200.389.%250AAdditionally%252C%252064.45%2525%2520to%252069.42%2525%2520of%2520the%2520arguments%2520of%2520DR%2520not%2520mentioned%2520by%2520human%250Aexperts%2520are%2520also%2520helpful%252C%25204.12%2525%2520to%25204.87%2525%2520of%2520the%2520arguments%2520have%2520uncertain%250Acorrectness%252C%2520and%25201.59%2525%2520to%25203.24%2525%2520of%2520the%2520arguments%2520are%2520potentially%2520misleading.%2520To%250Afurther%2520understand%2520the%2520trustworthiness%2520and%2520applicability%2520of%2520LLM-generated%2520DR%2520in%250Apractice%252C%2520we%2520conducted%2520semi-structured%2520interviews%2520with%2520six%2520practitioners.%2520Based%250Aon%2520the%2520experimental%2520and%2520interview%2520results%252C%2520we%2520discussed%2520the%2520pros%2520and%2520cons%2520of%250Athe%2520three%2520prompting%2520strategies%252C%2520the%2520strengths%2520and%2520limitations%2520of%2520LLM-generated%250ADR%252C%2520and%2520the%2520implications%2520for%2520the%2520practical%2520use%2520of%2520LLM-generated%2520DR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20781v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20LLMs%20in%20Generating%20Design%20Rationale%20for%20Software%20Architecture%0A%20%20Decisions&entry.906535625=Xiyu%20Zhou%20and%20Ruiyin%20Li%20and%20Peng%20Liang%20and%20Beiqi%20Zhang%20and%20Mojtaba%20Shahin%20and%20Zengyang%20Li%20and%20Chen%20Yang&entry.1292438233=%20%20Design%20Rationale%20%28DR%29%20for%20software%20architecture%20decisions%20refers%20to%20the%0Areasoning%20underlying%20architectural%20choices%2C%20which%20provides%20valuable%20insights%0Ainto%20the%20different%20phases%20of%20the%20architecting%20process%20throughout%20software%0Adevelopment.%20However%2C%20in%20practice%2C%20DR%20is%20often%20inadequately%20documented%20due%20to%20a%0Alack%20of%20motivation%20and%20effort%20from%20developers.%20With%20the%20recent%20advancements%20in%0ALarge%20Language%20Models%20%28LLMs%29%2C%20their%20capabilities%20in%20text%20comprehension%2C%0Areasoning%2C%20and%20generation%20may%20enable%20the%20generation%20and%20recovery%20of%20DR%20for%0Aarchitecture%20decisions.%20In%20this%20study%2C%20we%20evaluated%20the%20performance%20of%20LLMs%20in%0Agenerating%20DR%20for%20architecture%20decisions.%20First%2C%20we%20collected%2050%20Stack%20Overflow%0A%28SO%29%20posts%2C%2025%20GitHub%20issues%2C%20and%2025%20GitHub%20discussions%20related%20to%20architecture%0Adecisions%20to%20construct%20a%20dataset%20of%20100%20architecture-related%20problems.%20Then%2C%20we%0Aselected%20five%20LLMs%20to%20generate%20DR%20for%20the%20architecture%20decisions%20with%20three%0Aprompting%20strategies%2C%20including%20zero-shot%2C%20chain%20of%20thought%20%28CoT%29%2C%20and%0ALLM-based%20agents.%20With%20the%20DR%20provided%20by%20human%20experts%20as%20ground%20truth%2C%20the%0APrecision%20of%20LLM-generated%20DR%20with%20the%20three%20prompting%20strategies%20ranges%20from%0A0.267%20to%200.278%2C%20Recall%20from%200.627%20to%200.715%2C%20and%20F1-score%20from%200.351%20to%200.389.%0AAdditionally%2C%2064.45%25%20to%2069.42%25%20of%20the%20arguments%20of%20DR%20not%20mentioned%20by%20human%0Aexperts%20are%20also%20helpful%2C%204.12%25%20to%204.87%25%20of%20the%20arguments%20have%20uncertain%0Acorrectness%2C%20and%201.59%25%20to%203.24%25%20of%20the%20arguments%20are%20potentially%20misleading.%20To%0Afurther%20understand%20the%20trustworthiness%20and%20applicability%20of%20LLM-generated%20DR%20in%0Apractice%2C%20we%20conducted%20semi-structured%20interviews%20with%20six%20practitioners.%20Based%0Aon%20the%20experimental%20and%20interview%20results%2C%20we%20discussed%20the%20pros%20and%20cons%20of%0Athe%20three%20prompting%20strategies%2C%20the%20strengths%20and%20limitations%20of%20LLM-generated%0ADR%2C%20and%20the%20implications%20for%20the%20practical%20use%20of%20LLM-generated%20DR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20781v2&entry.124074799=Read"},
{"title": "Soft Graph Transformer for MIMO Detection", "author": "Jiadong Hong and Lei Liu and Xinyu Bian and Wenjie Wang and Zhaoyang Zhang", "abstract": "  We propose the Soft Graph Transformer (SGT), a soft-input-soft-output neural\narchitecture designed for MIMO detection. While Maximum Likelihood (ML)\ndetection achieves optimal accuracy, its exponential complexity makes it\ninfeasible in large systems, and conventional message-passing algorithms rely\non asymptotic assumptions that often fail in finite dimensions. Recent\nTransformer-based detectors show strong performance but typically overlook the\nMIMO factor graph structure and cannot exploit prior soft information. SGT\naddresses these limitations by combining self-attention, which encodes\ncontextual dependencies within symbol and constraint subgraphs, with\ngraph-aware cross-attention, which performs structured message passing across\nsubgraphs. Its soft-input interface allows the integration of auxiliary priors,\nproducing effective soft outputs while maintaining computational efficiency.\nExperiments demonstrate that SGT achieves near-ML performance and offers a\nflexible and interpretable framework for receiver systems that leverage soft\npriors.\n", "link": "http://arxiv.org/abs/2509.12694v2", "date": "2025-09-17", "relevancy": 1.8756, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4764}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4666}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Graph%20Transformer%20for%20MIMO%20Detection&body=Title%3A%20Soft%20Graph%20Transformer%20for%20MIMO%20Detection%0AAuthor%3A%20Jiadong%20Hong%20and%20Lei%20Liu%20and%20Xinyu%20Bian%20and%20Wenjie%20Wang%20and%20Zhaoyang%20Zhang%0AAbstract%3A%20%20%20We%20propose%20the%20Soft%20Graph%20Transformer%20%28SGT%29%2C%20a%20soft-input-soft-output%20neural%0Aarchitecture%20designed%20for%20MIMO%20detection.%20While%20Maximum%20Likelihood%20%28ML%29%0Adetection%20achieves%20optimal%20accuracy%2C%20its%20exponential%20complexity%20makes%20it%0Ainfeasible%20in%20large%20systems%2C%20and%20conventional%20message-passing%20algorithms%20rely%0Aon%20asymptotic%20assumptions%20that%20often%20fail%20in%20finite%20dimensions.%20Recent%0ATransformer-based%20detectors%20show%20strong%20performance%20but%20typically%20overlook%20the%0AMIMO%20factor%20graph%20structure%20and%20cannot%20exploit%20prior%20soft%20information.%20SGT%0Aaddresses%20these%20limitations%20by%20combining%20self-attention%2C%20which%20encodes%0Acontextual%20dependencies%20within%20symbol%20and%20constraint%20subgraphs%2C%20with%0Agraph-aware%20cross-attention%2C%20which%20performs%20structured%20message%20passing%20across%0Asubgraphs.%20Its%20soft-input%20interface%20allows%20the%20integration%20of%20auxiliary%20priors%2C%0Aproducing%20effective%20soft%20outputs%20while%20maintaining%20computational%20efficiency.%0AExperiments%20demonstrate%20that%20SGT%20achieves%20near-ML%20performance%20and%20offers%20a%0Aflexible%20and%20interpretable%20framework%20for%20receiver%20systems%20that%20leverage%20soft%0Apriors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Graph%2520Transformer%2520for%2520MIMO%2520Detection%26entry.906535625%3DJiadong%2520Hong%2520and%2520Lei%2520Liu%2520and%2520Xinyu%2520Bian%2520and%2520Wenjie%2520Wang%2520and%2520Zhaoyang%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520Soft%2520Graph%2520Transformer%2520%2528SGT%2529%252C%2520a%2520soft-input-soft-output%2520neural%250Aarchitecture%2520designed%2520for%2520MIMO%2520detection.%2520While%2520Maximum%2520Likelihood%2520%2528ML%2529%250Adetection%2520achieves%2520optimal%2520accuracy%252C%2520its%2520exponential%2520complexity%2520makes%2520it%250Ainfeasible%2520in%2520large%2520systems%252C%2520and%2520conventional%2520message-passing%2520algorithms%2520rely%250Aon%2520asymptotic%2520assumptions%2520that%2520often%2520fail%2520in%2520finite%2520dimensions.%2520Recent%250ATransformer-based%2520detectors%2520show%2520strong%2520performance%2520but%2520typically%2520overlook%2520the%250AMIMO%2520factor%2520graph%2520structure%2520and%2520cannot%2520exploit%2520prior%2520soft%2520information.%2520SGT%250Aaddresses%2520these%2520limitations%2520by%2520combining%2520self-attention%252C%2520which%2520encodes%250Acontextual%2520dependencies%2520within%2520symbol%2520and%2520constraint%2520subgraphs%252C%2520with%250Agraph-aware%2520cross-attention%252C%2520which%2520performs%2520structured%2520message%2520passing%2520across%250Asubgraphs.%2520Its%2520soft-input%2520interface%2520allows%2520the%2520integration%2520of%2520auxiliary%2520priors%252C%250Aproducing%2520effective%2520soft%2520outputs%2520while%2520maintaining%2520computational%2520efficiency.%250AExperiments%2520demonstrate%2520that%2520SGT%2520achieves%2520near-ML%2520performance%2520and%2520offers%2520a%250Aflexible%2520and%2520interpretable%2520framework%2520for%2520receiver%2520systems%2520that%2520leverage%2520soft%250Apriors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Graph%20Transformer%20for%20MIMO%20Detection&entry.906535625=Jiadong%20Hong%20and%20Lei%20Liu%20and%20Xinyu%20Bian%20and%20Wenjie%20Wang%20and%20Zhaoyang%20Zhang&entry.1292438233=%20%20We%20propose%20the%20Soft%20Graph%20Transformer%20%28SGT%29%2C%20a%20soft-input-soft-output%20neural%0Aarchitecture%20designed%20for%20MIMO%20detection.%20While%20Maximum%20Likelihood%20%28ML%29%0Adetection%20achieves%20optimal%20accuracy%2C%20its%20exponential%20complexity%20makes%20it%0Ainfeasible%20in%20large%20systems%2C%20and%20conventional%20message-passing%20algorithms%20rely%0Aon%20asymptotic%20assumptions%20that%20often%20fail%20in%20finite%20dimensions.%20Recent%0ATransformer-based%20detectors%20show%20strong%20performance%20but%20typically%20overlook%20the%0AMIMO%20factor%20graph%20structure%20and%20cannot%20exploit%20prior%20soft%20information.%20SGT%0Aaddresses%20these%20limitations%20by%20combining%20self-attention%2C%20which%20encodes%0Acontextual%20dependencies%20within%20symbol%20and%20constraint%20subgraphs%2C%20with%0Agraph-aware%20cross-attention%2C%20which%20performs%20structured%20message%20passing%20across%0Asubgraphs.%20Its%20soft-input%20interface%20allows%20the%20integration%20of%20auxiliary%20priors%2C%0Aproducing%20effective%20soft%20outputs%20while%20maintaining%20computational%20efficiency.%0AExperiments%20demonstrate%20that%20SGT%20achieves%20near-ML%20performance%20and%20offers%20a%0Aflexible%20and%20interpretable%20framework%20for%20receiver%20systems%20that%20leverage%20soft%0Apriors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12694v2&entry.124074799=Read"},
{"title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "author": "Colin Hong and Xu Guo and Anand Chaanan Singh and Esha Choukse and Dmitrii Ustiugov", "abstract": "  Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.\n", "link": "http://arxiv.org/abs/2509.13990v1", "date": "2025-09-17", "relevancy": 1.8754, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.469}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4688}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slim-SC%3A%20Thought%20Pruning%20for%20Efficient%20Scaling%20with%20Self-Consistency&body=Title%3A%20Slim-SC%3A%20Thought%20Pruning%20for%20Efficient%20Scaling%20with%20Self-Consistency%0AAuthor%3A%20Colin%20Hong%20and%20Xu%20Guo%20and%20Anand%20Chaanan%20Singh%20and%20Esha%20Choukse%20and%20Dmitrii%20Ustiugov%0AAbstract%3A%20%20%20Recently%2C%20Test-Time%20Scaling%20%28TTS%29%20has%20gained%20increasing%20attention%20for%0Aimproving%20LLM%20reasoning%20performance%20at%20test%20time%20without%20retraining%20the%20model.%0AA%20notable%20TTS%20technique%20is%20Self-Consistency%20%28SC%29%2C%20which%20generates%20multiple%0Areasoning%20chains%20in%20parallel%20and%20selects%20the%20final%20answer%20via%20majority%20voting.%0AWhile%20effective%2C%20the%20order-of-magnitude%20computational%20overhead%20limits%20its%20broad%0Adeployment.%20Prior%20attempts%20to%20accelerate%20SC%20mainly%20rely%20on%20model-based%0Aconfidence%20scores%20or%20heuristics%20with%20limited%20empirical%20support.%20For%20the%20first%0Atime%2C%20we%20theoretically%20and%20empirically%20analyze%20the%20inefficiencies%20of%20SC%20and%0Areveal%20actionable%20opportunities%20for%20improvement.%20Building%20on%20these%20insights%2C%20we%0Apropose%20Slim-SC%2C%20a%20step-wise%20pruning%20strategy%20that%20identifies%20and%20removes%0Aredundant%20chains%20using%20inter-chain%20similarity%20at%20the%20thought%20level.%20Experiments%0Aon%20three%20STEM%20reasoning%20datasets%20and%20two%20recent%20LLM%20architectures%20show%20that%0ASlim-SC%20reduces%20inference%20latency%20and%20KVC%20usage%20by%20up%20to%2045%25%20and%2026%25%2C%0Arespectively%2C%20with%20R1-Distill%2C%20while%20maintaining%20or%20improving%20accuracy%2C%20thus%0Aoffering%20a%20simple%20yet%20efficient%20TTS%20alternative%20for%20SC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlim-SC%253A%2520Thought%2520Pruning%2520for%2520Efficient%2520Scaling%2520with%2520Self-Consistency%26entry.906535625%3DColin%2520Hong%2520and%2520Xu%2520Guo%2520and%2520Anand%2520Chaanan%2520Singh%2520and%2520Esha%2520Choukse%2520and%2520Dmitrii%2520Ustiugov%26entry.1292438233%3D%2520%2520Recently%252C%2520Test-Time%2520Scaling%2520%2528TTS%2529%2520has%2520gained%2520increasing%2520attention%2520for%250Aimproving%2520LLM%2520reasoning%2520performance%2520at%2520test%2520time%2520without%2520retraining%2520the%2520model.%250AA%2520notable%2520TTS%2520technique%2520is%2520Self-Consistency%2520%2528SC%2529%252C%2520which%2520generates%2520multiple%250Areasoning%2520chains%2520in%2520parallel%2520and%2520selects%2520the%2520final%2520answer%2520via%2520majority%2520voting.%250AWhile%2520effective%252C%2520the%2520order-of-magnitude%2520computational%2520overhead%2520limits%2520its%2520broad%250Adeployment.%2520Prior%2520attempts%2520to%2520accelerate%2520SC%2520mainly%2520rely%2520on%2520model-based%250Aconfidence%2520scores%2520or%2520heuristics%2520with%2520limited%2520empirical%2520support.%2520For%2520the%2520first%250Atime%252C%2520we%2520theoretically%2520and%2520empirically%2520analyze%2520the%2520inefficiencies%2520of%2520SC%2520and%250Areveal%2520actionable%2520opportunities%2520for%2520improvement.%2520Building%2520on%2520these%2520insights%252C%2520we%250Apropose%2520Slim-SC%252C%2520a%2520step-wise%2520pruning%2520strategy%2520that%2520identifies%2520and%2520removes%250Aredundant%2520chains%2520using%2520inter-chain%2520similarity%2520at%2520the%2520thought%2520level.%2520Experiments%250Aon%2520three%2520STEM%2520reasoning%2520datasets%2520and%2520two%2520recent%2520LLM%2520architectures%2520show%2520that%250ASlim-SC%2520reduces%2520inference%2520latency%2520and%2520KVC%2520usage%2520by%2520up%2520to%252045%2525%2520and%252026%2525%252C%250Arespectively%252C%2520with%2520R1-Distill%252C%2520while%2520maintaining%2520or%2520improving%2520accuracy%252C%2520thus%250Aoffering%2520a%2520simple%2520yet%2520efficient%2520TTS%2520alternative%2520for%2520SC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slim-SC%3A%20Thought%20Pruning%20for%20Efficient%20Scaling%20with%20Self-Consistency&entry.906535625=Colin%20Hong%20and%20Xu%20Guo%20and%20Anand%20Chaanan%20Singh%20and%20Esha%20Choukse%20and%20Dmitrii%20Ustiugov&entry.1292438233=%20%20Recently%2C%20Test-Time%20Scaling%20%28TTS%29%20has%20gained%20increasing%20attention%20for%0Aimproving%20LLM%20reasoning%20performance%20at%20test%20time%20without%20retraining%20the%20model.%0AA%20notable%20TTS%20technique%20is%20Self-Consistency%20%28SC%29%2C%20which%20generates%20multiple%0Areasoning%20chains%20in%20parallel%20and%20selects%20the%20final%20answer%20via%20majority%20voting.%0AWhile%20effective%2C%20the%20order-of-magnitude%20computational%20overhead%20limits%20its%20broad%0Adeployment.%20Prior%20attempts%20to%20accelerate%20SC%20mainly%20rely%20on%20model-based%0Aconfidence%20scores%20or%20heuristics%20with%20limited%20empirical%20support.%20For%20the%20first%0Atime%2C%20we%20theoretically%20and%20empirically%20analyze%20the%20inefficiencies%20of%20SC%20and%0Areveal%20actionable%20opportunities%20for%20improvement.%20Building%20on%20these%20insights%2C%20we%0Apropose%20Slim-SC%2C%20a%20step-wise%20pruning%20strategy%20that%20identifies%20and%20removes%0Aredundant%20chains%20using%20inter-chain%20similarity%20at%20the%20thought%20level.%20Experiments%0Aon%20three%20STEM%20reasoning%20datasets%20and%20two%20recent%20LLM%20architectures%20show%20that%0ASlim-SC%20reduces%20inference%20latency%20and%20KVC%20usage%20by%20up%20to%2045%25%20and%2026%25%2C%0Arespectively%2C%20with%20R1-Distill%2C%20while%20maintaining%20or%20improving%20accuracy%2C%20thus%0Aoffering%20a%20simple%20yet%20efficient%20TTS%20alternative%20for%20SC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13990v1&entry.124074799=Read"},
{"title": "Building the Self-Improvement Loop: Error Detection and Correction in\n  Goal-Oriented Semantic Communications", "author": "Peizheng Li and Xinyi Lin and Adnan Aijaz", "abstract": "  Error detection and correction are essential for ensuring robust and reliable\noperation in modern communication systems, particularly in complex transmission\nenvironments. However, discussions on these topics have largely been overlooked\nin semantic communication (SemCom), which focuses on transmitting meaning\nrather than symbols, leading to significant improvements in communication\nefficiency. Despite these advantages, semantic errors -- stemming from\ndiscrepancies between transmitted and received meanings -- present a major\nchallenge to system reliability. This paper addresses this gap by proposing a\ncomprehensive framework for detecting and correcting semantic errors in SemCom\nsystems. We formally define semantic error, detection, and correction\nmechanisms, and identify key sources of semantic errors. To address these\nchallenges, we develop a Gaussian process (GP)-based method for latent space\nmonitoring to detect errors, alongside a human-in-the-loop reinforcement\nlearning (HITL-RL) approach to optimize semantic model configurations using\nuser feedback. Experimental results validate the effectiveness of the proposed\nmethods in mitigating semantic errors under various conditions, including\nadversarial attacks, input feature changes, physical channel variations, and\nuser preference shifts. This work lays the foundation for more reliable and\nadaptive SemCom systems with robust semantic error management techniques.\n", "link": "http://arxiv.org/abs/2411.01544v2", "date": "2025-09-17", "relevancy": 1.868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20the%20Self-Improvement%20Loop%3A%20Error%20Detection%20and%20Correction%20in%0A%20%20Goal-Oriented%20Semantic%20Communications&body=Title%3A%20Building%20the%20Self-Improvement%20Loop%3A%20Error%20Detection%20and%20Correction%20in%0A%20%20Goal-Oriented%20Semantic%20Communications%0AAuthor%3A%20Peizheng%20Li%20and%20Xinyi%20Lin%20and%20Adnan%20Aijaz%0AAbstract%3A%20%20%20Error%20detection%20and%20correction%20are%20essential%20for%20ensuring%20robust%20and%20reliable%0Aoperation%20in%20modern%20communication%20systems%2C%20particularly%20in%20complex%20transmission%0Aenvironments.%20However%2C%20discussions%20on%20these%20topics%20have%20largely%20been%20overlooked%0Ain%20semantic%20communication%20%28SemCom%29%2C%20which%20focuses%20on%20transmitting%20meaning%0Arather%20than%20symbols%2C%20leading%20to%20significant%20improvements%20in%20communication%0Aefficiency.%20Despite%20these%20advantages%2C%20semantic%20errors%20--%20stemming%20from%0Adiscrepancies%20between%20transmitted%20and%20received%20meanings%20--%20present%20a%20major%0Achallenge%20to%20system%20reliability.%20This%20paper%20addresses%20this%20gap%20by%20proposing%20a%0Acomprehensive%20framework%20for%20detecting%20and%20correcting%20semantic%20errors%20in%20SemCom%0Asystems.%20We%20formally%20define%20semantic%20error%2C%20detection%2C%20and%20correction%0Amechanisms%2C%20and%20identify%20key%20sources%20of%20semantic%20errors.%20To%20address%20these%0Achallenges%2C%20we%20develop%20a%20Gaussian%20process%20%28GP%29-based%20method%20for%20latent%20space%0Amonitoring%20to%20detect%20errors%2C%20alongside%20a%20human-in-the-loop%20reinforcement%0Alearning%20%28HITL-RL%29%20approach%20to%20optimize%20semantic%20model%20configurations%20using%0Auser%20feedback.%20Experimental%20results%20validate%20the%20effectiveness%20of%20the%20proposed%0Amethods%20in%20mitigating%20semantic%20errors%20under%20various%20conditions%2C%20including%0Aadversarial%20attacks%2C%20input%20feature%20changes%2C%20physical%20channel%20variations%2C%20and%0Auser%20preference%20shifts.%20This%20work%20lays%20the%20foundation%20for%20more%20reliable%20and%0Aadaptive%20SemCom%20systems%20with%20robust%20semantic%20error%20management%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520the%2520Self-Improvement%2520Loop%253A%2520Error%2520Detection%2520and%2520Correction%2520in%250A%2520%2520Goal-Oriented%2520Semantic%2520Communications%26entry.906535625%3DPeizheng%2520Li%2520and%2520Xinyi%2520Lin%2520and%2520Adnan%2520Aijaz%26entry.1292438233%3D%2520%2520Error%2520detection%2520and%2520correction%2520are%2520essential%2520for%2520ensuring%2520robust%2520and%2520reliable%250Aoperation%2520in%2520modern%2520communication%2520systems%252C%2520particularly%2520in%2520complex%2520transmission%250Aenvironments.%2520However%252C%2520discussions%2520on%2520these%2520topics%2520have%2520largely%2520been%2520overlooked%250Ain%2520semantic%2520communication%2520%2528SemCom%2529%252C%2520which%2520focuses%2520on%2520transmitting%2520meaning%250Arather%2520than%2520symbols%252C%2520leading%2520to%2520significant%2520improvements%2520in%2520communication%250Aefficiency.%2520Despite%2520these%2520advantages%252C%2520semantic%2520errors%2520--%2520stemming%2520from%250Adiscrepancies%2520between%2520transmitted%2520and%2520received%2520meanings%2520--%2520present%2520a%2520major%250Achallenge%2520to%2520system%2520reliability.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520proposing%2520a%250Acomprehensive%2520framework%2520for%2520detecting%2520and%2520correcting%2520semantic%2520errors%2520in%2520SemCom%250Asystems.%2520We%2520formally%2520define%2520semantic%2520error%252C%2520detection%252C%2520and%2520correction%250Amechanisms%252C%2520and%2520identify%2520key%2520sources%2520of%2520semantic%2520errors.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520develop%2520a%2520Gaussian%2520process%2520%2528GP%2529-based%2520method%2520for%2520latent%2520space%250Amonitoring%2520to%2520detect%2520errors%252C%2520alongside%2520a%2520human-in-the-loop%2520reinforcement%250Alearning%2520%2528HITL-RL%2529%2520approach%2520to%2520optimize%2520semantic%2520model%2520configurations%2520using%250Auser%2520feedback.%2520Experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethods%2520in%2520mitigating%2520semantic%2520errors%2520under%2520various%2520conditions%252C%2520including%250Aadversarial%2520attacks%252C%2520input%2520feature%2520changes%252C%2520physical%2520channel%2520variations%252C%2520and%250Auser%2520preference%2520shifts.%2520This%2520work%2520lays%2520the%2520foundation%2520for%2520more%2520reliable%2520and%250Aadaptive%2520SemCom%2520systems%2520with%2520robust%2520semantic%2520error%2520management%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20the%20Self-Improvement%20Loop%3A%20Error%20Detection%20and%20Correction%20in%0A%20%20Goal-Oriented%20Semantic%20Communications&entry.906535625=Peizheng%20Li%20and%20Xinyi%20Lin%20and%20Adnan%20Aijaz&entry.1292438233=%20%20Error%20detection%20and%20correction%20are%20essential%20for%20ensuring%20robust%20and%20reliable%0Aoperation%20in%20modern%20communication%20systems%2C%20particularly%20in%20complex%20transmission%0Aenvironments.%20However%2C%20discussions%20on%20these%20topics%20have%20largely%20been%20overlooked%0Ain%20semantic%20communication%20%28SemCom%29%2C%20which%20focuses%20on%20transmitting%20meaning%0Arather%20than%20symbols%2C%20leading%20to%20significant%20improvements%20in%20communication%0Aefficiency.%20Despite%20these%20advantages%2C%20semantic%20errors%20--%20stemming%20from%0Adiscrepancies%20between%20transmitted%20and%20received%20meanings%20--%20present%20a%20major%0Achallenge%20to%20system%20reliability.%20This%20paper%20addresses%20this%20gap%20by%20proposing%20a%0Acomprehensive%20framework%20for%20detecting%20and%20correcting%20semantic%20errors%20in%20SemCom%0Asystems.%20We%20formally%20define%20semantic%20error%2C%20detection%2C%20and%20correction%0Amechanisms%2C%20and%20identify%20key%20sources%20of%20semantic%20errors.%20To%20address%20these%0Achallenges%2C%20we%20develop%20a%20Gaussian%20process%20%28GP%29-based%20method%20for%20latent%20space%0Amonitoring%20to%20detect%20errors%2C%20alongside%20a%20human-in-the-loop%20reinforcement%0Alearning%20%28HITL-RL%29%20approach%20to%20optimize%20semantic%20model%20configurations%20using%0Auser%20feedback.%20Experimental%20results%20validate%20the%20effectiveness%20of%20the%20proposed%0Amethods%20in%20mitigating%20semantic%20errors%20under%20various%20conditions%2C%20including%0Aadversarial%20attacks%2C%20input%20feature%20changes%2C%20physical%20channel%20variations%2C%20and%0Auser%20preference%20shifts.%20This%20work%20lays%20the%20foundation%20for%20more%20reliable%20and%0Aadaptive%20SemCom%20systems%20with%20robust%20semantic%20error%20management%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01544v2&entry.124074799=Read"},
{"title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits", "author": "Ziming Wei and Zichen Kong and Yuan Wang and David Z. Pan and Xiyuan Tang", "abstract": "  Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.\n", "link": "http://arxiv.org/abs/2509.14169v1", "date": "2025-09-17", "relevancy": 1.8671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4642}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoSizing%3A%20An%20LLM-aided%20Framework%20of%20Topology-based%20Understanding%20and%0A%20%20Sizing%20for%20AMS%20Circuits&body=Title%3A%20TopoSizing%3A%20An%20LLM-aided%20Framework%20of%20Topology-based%20Understanding%20and%0A%20%20Sizing%20for%20AMS%20Circuits%0AAuthor%3A%20Ziming%20Wei%20and%20Zichen%20Kong%20and%20Yuan%20Wang%20and%20David%20Z.%20Pan%20and%20Xiyuan%20Tang%0AAbstract%3A%20%20%20Analog%20and%20mixed-signal%20circuit%20design%20remains%20challenging%20due%20to%20the%0Ashortage%20of%20high-quality%20data%20and%20the%20difficulty%20of%20embedding%20domain%20knowledge%0Ainto%20automated%20flows.%20Traditional%20black-box%20optimization%20achieves%20sampling%0Aefficiency%20but%20lacks%20circuit%20understanding%2C%20which%20often%20causes%20evaluations%20to%0Abe%20wasted%20in%20low-value%20regions%20of%20the%20design%20space.%20In%20contrast%2C%20learning-based%0Amethods%20embed%20structural%20knowledge%20but%20are%20case-specific%20and%20costly%20to%20retrain.%0ARecent%20attempts%20with%20large%20language%20models%20show%20potential%2C%20yet%20they%20often%20rely%0Aon%20manual%20intervention%2C%20limiting%20generality%20and%20transparency.%20We%20propose%0ATopoSizing%2C%20an%20end-to-end%20framework%20that%20performs%20robust%20circuit%20understanding%0Adirectly%20from%20raw%20netlists%20and%20translates%20this%20knowledge%20into%20optimization%0Agains.%20Our%20approach%20first%20applies%20graph%20algorithms%20to%20organize%20circuits%20into%20a%0Ahierarchical%20device-module-stage%20representation.%20LLM%20agents%20then%20execute%20an%0Aiterative%20hypothesis-verification-refinement%20loop%20with%20built-in%20consistency%0Achecks%2C%20producing%20explicit%20annotations.%20Verified%20insights%20are%20integrated%20into%0ABayesian%20optimization%20through%20LLM-guided%20initial%20sampling%20and%0Astagnation-triggered%20trust-region%20updates%2C%20improving%20efficiency%20while%0Apreserving%20feasibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoSizing%253A%2520An%2520LLM-aided%2520Framework%2520of%2520Topology-based%2520Understanding%2520and%250A%2520%2520Sizing%2520for%2520AMS%2520Circuits%26entry.906535625%3DZiming%2520Wei%2520and%2520Zichen%2520Kong%2520and%2520Yuan%2520Wang%2520and%2520David%2520Z.%2520Pan%2520and%2520Xiyuan%2520Tang%26entry.1292438233%3D%2520%2520Analog%2520and%2520mixed-signal%2520circuit%2520design%2520remains%2520challenging%2520due%2520to%2520the%250Ashortage%2520of%2520high-quality%2520data%2520and%2520the%2520difficulty%2520of%2520embedding%2520domain%2520knowledge%250Ainto%2520automated%2520flows.%2520Traditional%2520black-box%2520optimization%2520achieves%2520sampling%250Aefficiency%2520but%2520lacks%2520circuit%2520understanding%252C%2520which%2520often%2520causes%2520evaluations%2520to%250Abe%2520wasted%2520in%2520low-value%2520regions%2520of%2520the%2520design%2520space.%2520In%2520contrast%252C%2520learning-based%250Amethods%2520embed%2520structural%2520knowledge%2520but%2520are%2520case-specific%2520and%2520costly%2520to%2520retrain.%250ARecent%2520attempts%2520with%2520large%2520language%2520models%2520show%2520potential%252C%2520yet%2520they%2520often%2520rely%250Aon%2520manual%2520intervention%252C%2520limiting%2520generality%2520and%2520transparency.%2520We%2520propose%250ATopoSizing%252C%2520an%2520end-to-end%2520framework%2520that%2520performs%2520robust%2520circuit%2520understanding%250Adirectly%2520from%2520raw%2520netlists%2520and%2520translates%2520this%2520knowledge%2520into%2520optimization%250Agains.%2520Our%2520approach%2520first%2520applies%2520graph%2520algorithms%2520to%2520organize%2520circuits%2520into%2520a%250Ahierarchical%2520device-module-stage%2520representation.%2520LLM%2520agents%2520then%2520execute%2520an%250Aiterative%2520hypothesis-verification-refinement%2520loop%2520with%2520built-in%2520consistency%250Achecks%252C%2520producing%2520explicit%2520annotations.%2520Verified%2520insights%2520are%2520integrated%2520into%250ABayesian%2520optimization%2520through%2520LLM-guided%2520initial%2520sampling%2520and%250Astagnation-triggered%2520trust-region%2520updates%252C%2520improving%2520efficiency%2520while%250Apreserving%2520feasibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoSizing%3A%20An%20LLM-aided%20Framework%20of%20Topology-based%20Understanding%20and%0A%20%20Sizing%20for%20AMS%20Circuits&entry.906535625=Ziming%20Wei%20and%20Zichen%20Kong%20and%20Yuan%20Wang%20and%20David%20Z.%20Pan%20and%20Xiyuan%20Tang&entry.1292438233=%20%20Analog%20and%20mixed-signal%20circuit%20design%20remains%20challenging%20due%20to%20the%0Ashortage%20of%20high-quality%20data%20and%20the%20difficulty%20of%20embedding%20domain%20knowledge%0Ainto%20automated%20flows.%20Traditional%20black-box%20optimization%20achieves%20sampling%0Aefficiency%20but%20lacks%20circuit%20understanding%2C%20which%20often%20causes%20evaluations%20to%0Abe%20wasted%20in%20low-value%20regions%20of%20the%20design%20space.%20In%20contrast%2C%20learning-based%0Amethods%20embed%20structural%20knowledge%20but%20are%20case-specific%20and%20costly%20to%20retrain.%0ARecent%20attempts%20with%20large%20language%20models%20show%20potential%2C%20yet%20they%20often%20rely%0Aon%20manual%20intervention%2C%20limiting%20generality%20and%20transparency.%20We%20propose%0ATopoSizing%2C%20an%20end-to-end%20framework%20that%20performs%20robust%20circuit%20understanding%0Adirectly%20from%20raw%20netlists%20and%20translates%20this%20knowledge%20into%20optimization%0Agains.%20Our%20approach%20first%20applies%20graph%20algorithms%20to%20organize%20circuits%20into%20a%0Ahierarchical%20device-module-stage%20representation.%20LLM%20agents%20then%20execute%20an%0Aiterative%20hypothesis-verification-refinement%20loop%20with%20built-in%20consistency%0Achecks%2C%20producing%20explicit%20annotations.%20Verified%20insights%20are%20integrated%20into%0ABayesian%20optimization%20through%20LLM-guided%20initial%20sampling%20and%0Astagnation-triggered%20trust-region%20updates%2C%20improving%20efficiency%20while%0Apreserving%20feasibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14169v1&entry.124074799=Read"},
{"title": "Brain Inspired Probabilistic Occupancy Grid Mapping with Vector Symbolic\n  Architectures", "author": "Shay Snyder and Andrew Capodieci and David Gorsich and Maryam Parsa", "abstract": "  Real-time robotic systems require advanced perception, computation, and\naction capability. However, the main bottleneck in current autonomous systems\nis the trade-off between computational capability, energy efficiency and model\ndeterminism. World modeling, a key objective of many robotic systems, commonly\nuses occupancy grid mapping (OGM) as the first step towards building an\nend-to-end robotic system with perception, planning, autonomous maneuvering,\nand decision making capabilities. OGM divides the environment into discrete\ncells and assigns probability values to attributes such as occupancy and\ntraversability. Existing methods fall into two categories: traditional methods\nand neural methods. Traditional methods rely on dense statistical calculations,\nwhile neural methods employ deep learning for probabilistic information\nprocessing. In this study, we propose a vector symbolic architecture-based OGM\nsystem (VSA-OGM) that retains the interpretability and stability of traditional\nmethods with the improved computational efficiency of neural methods. Our\napproach, validated across multiple datasets, achieves similar accuracy to\ncovariant traditional methods while reducing latency by approximately 45x and\nmemory by 400x. Compared to invariant traditional methods, we see similar\naccuracy values while reducing latency by 5.5x. Moreover, we achieve up to 6x\nlatency reductions compared to neural methods while eliminating the need for\ndomain-specific model training. This work demonstrates the potential of vector\nsymbolic architectures as a practical foundation for real-time probabilistic\nmapping in autonomous systems operating under strict computational and latency\nconstraints.\n", "link": "http://arxiv.org/abs/2408.09066v4", "date": "2025-09-17", "relevancy": 1.865, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6518}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%20Vector%20Symbolic%0A%20%20Architectures&body=Title%3A%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%20Vector%20Symbolic%0A%20%20Architectures%0AAuthor%3A%20Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa%0AAbstract%3A%20%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20In%20this%20study%2C%20we%20propose%20a%20vector%20symbolic%20architecture-based%20OGM%0Asystem%20%28VSA-OGM%29%20that%20retains%20the%20interpretability%20and%20stability%20of%20traditional%0Amethods%20with%20the%20improved%20computational%20efficiency%20of%20neural%20methods.%20Our%0Aapproach%2C%20validated%20across%20multiple%20datasets%2C%20achieves%20similar%20accuracy%20to%0Acovariant%20traditional%20methods%20while%20reducing%20latency%20by%20approximately%2045x%20and%0Amemory%20by%20400x.%20Compared%20to%20invariant%20traditional%20methods%2C%20we%20see%20similar%0Aaccuracy%20values%20while%20reducing%20latency%20by%205.5x.%20Moreover%2C%20we%20achieve%20up%20to%206x%0Alatency%20reductions%20compared%20to%20neural%20methods%20while%20eliminating%20the%20need%20for%0Adomain-specific%20model%20training.%20This%20work%20demonstrates%20the%20potential%20of%20vector%0Asymbolic%20architectures%20as%20a%20practical%20foundation%20for%20real-time%20probabilistic%0Amapping%20in%20autonomous%20systems%20operating%20under%20strict%20computational%20and%20latency%0Aconstraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09066v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Inspired%2520Probabilistic%2520Occupancy%2520Grid%2520Mapping%2520with%2520Vector%2520Symbolic%250A%2520%2520Architectures%26entry.906535625%3DShay%2520Snyder%2520and%2520Andrew%2520Capodieci%2520and%2520David%2520Gorsich%2520and%2520Maryam%2520Parsa%26entry.1292438233%3D%2520%2520Real-time%2520robotic%2520systems%2520require%2520advanced%2520perception%252C%2520computation%252C%2520and%250Aaction%2520capability.%2520However%252C%2520the%2520main%2520bottleneck%2520in%2520current%2520autonomous%2520systems%250Ais%2520the%2520trade-off%2520between%2520computational%2520capability%252C%2520energy%2520efficiency%2520and%2520model%250Adeterminism.%2520World%2520modeling%252C%2520a%2520key%2520objective%2520of%2520many%2520robotic%2520systems%252C%2520commonly%250Auses%2520occupancy%2520grid%2520mapping%2520%2528OGM%2529%2520as%2520the%2520first%2520step%2520towards%2520building%2520an%250Aend-to-end%2520robotic%2520system%2520with%2520perception%252C%2520planning%252C%2520autonomous%2520maneuvering%252C%250Aand%2520decision%2520making%2520capabilities.%2520OGM%2520divides%2520the%2520environment%2520into%2520discrete%250Acells%2520and%2520assigns%2520probability%2520values%2520to%2520attributes%2520such%2520as%2520occupancy%2520and%250Atraversability.%2520Existing%2520methods%2520fall%2520into%2520two%2520categories%253A%2520traditional%2520methods%250Aand%2520neural%2520methods.%2520Traditional%2520methods%2520rely%2520on%2520dense%2520statistical%2520calculations%252C%250Awhile%2520neural%2520methods%2520employ%2520deep%2520learning%2520for%2520probabilistic%2520information%250Aprocessing.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520vector%2520symbolic%2520architecture-based%2520OGM%250Asystem%2520%2528VSA-OGM%2529%2520that%2520retains%2520the%2520interpretability%2520and%2520stability%2520of%2520traditional%250Amethods%2520with%2520the%2520improved%2520computational%2520efficiency%2520of%2520neural%2520methods.%2520Our%250Aapproach%252C%2520validated%2520across%2520multiple%2520datasets%252C%2520achieves%2520similar%2520accuracy%2520to%250Acovariant%2520traditional%2520methods%2520while%2520reducing%2520latency%2520by%2520approximately%252045x%2520and%250Amemory%2520by%2520400x.%2520Compared%2520to%2520invariant%2520traditional%2520methods%252C%2520we%2520see%2520similar%250Aaccuracy%2520values%2520while%2520reducing%2520latency%2520by%25205.5x.%2520Moreover%252C%2520we%2520achieve%2520up%2520to%25206x%250Alatency%2520reductions%2520compared%2520to%2520neural%2520methods%2520while%2520eliminating%2520the%2520need%2520for%250Adomain-specific%2520model%2520training.%2520This%2520work%2520demonstrates%2520the%2520potential%2520of%2520vector%250Asymbolic%2520architectures%2520as%2520a%2520practical%2520foundation%2520for%2520real-time%2520probabilistic%250Amapping%2520in%2520autonomous%2520systems%2520operating%2520under%2520strict%2520computational%2520and%2520latency%250Aconstraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09066v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%20Vector%20Symbolic%0A%20%20Architectures&entry.906535625=Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa&entry.1292438233=%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20In%20this%20study%2C%20we%20propose%20a%20vector%20symbolic%20architecture-based%20OGM%0Asystem%20%28VSA-OGM%29%20that%20retains%20the%20interpretability%20and%20stability%20of%20traditional%0Amethods%20with%20the%20improved%20computational%20efficiency%20of%20neural%20methods.%20Our%0Aapproach%2C%20validated%20across%20multiple%20datasets%2C%20achieves%20similar%20accuracy%20to%0Acovariant%20traditional%20methods%20while%20reducing%20latency%20by%20approximately%2045x%20and%0Amemory%20by%20400x.%20Compared%20to%20invariant%20traditional%20methods%2C%20we%20see%20similar%0Aaccuracy%20values%20while%20reducing%20latency%20by%205.5x.%20Moreover%2C%20we%20achieve%20up%20to%206x%0Alatency%20reductions%20compared%20to%20neural%20methods%20while%20eliminating%20the%20need%20for%0Adomain-specific%20model%20training.%20This%20work%20demonstrates%20the%20potential%20of%20vector%0Asymbolic%20architectures%20as%20a%20practical%20foundation%20for%20real-time%20probabilistic%0Amapping%20in%20autonomous%20systems%20operating%20under%20strict%20computational%20and%20latency%0Aconstraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09066v4&entry.124074799=Read"},
{"title": "A Compositional Kernel Model for Feature Learning", "author": "Feng Ruan and Keli Liu and Michael Jordan", "abstract": "  We study a compositional variant of kernel ridge regression in which the\npredictor is applied to a coordinate-wise reweighting of the inputs. Formulated\nas a variational problem, this model provides a simple testbed for feature\nlearning in compositional architectures. From the perspective of variable\nselection, we show how relevant variables are recovered while noise variables\nare eliminated. We establish guarantees showing that both global minimizers and\nstationary points discard noise coordinates when the noise variables are\nGaussian distributed. A central finding is that $\\ell_1$-type kernels, such as\nthe Laplace kernel, succeed in recovering features contributing to nonlinear\neffects at stationary points, whereas Gaussian kernels recover only linear\nones.\n", "link": "http://arxiv.org/abs/2509.14158v1", "date": "2025-09-17", "relevancy": 1.8328, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4754}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4514}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Compositional%20Kernel%20Model%20for%20Feature%20Learning&body=Title%3A%20A%20Compositional%20Kernel%20Model%20for%20Feature%20Learning%0AAuthor%3A%20Feng%20Ruan%20and%20Keli%20Liu%20and%20Michael%20Jordan%0AAbstract%3A%20%20%20We%20study%20a%20compositional%20variant%20of%20kernel%20ridge%20regression%20in%20which%20the%0Apredictor%20is%20applied%20to%20a%20coordinate-wise%20reweighting%20of%20the%20inputs.%20Formulated%0Aas%20a%20variational%20problem%2C%20this%20model%20provides%20a%20simple%20testbed%20for%20feature%0Alearning%20in%20compositional%20architectures.%20From%20the%20perspective%20of%20variable%0Aselection%2C%20we%20show%20how%20relevant%20variables%20are%20recovered%20while%20noise%20variables%0Aare%20eliminated.%20We%20establish%20guarantees%20showing%20that%20both%20global%20minimizers%20and%0Astationary%20points%20discard%20noise%20coordinates%20when%20the%20noise%20variables%20are%0AGaussian%20distributed.%20A%20central%20finding%20is%20that%20%24%5Cell_1%24-type%20kernels%2C%20such%20as%0Athe%20Laplace%20kernel%2C%20succeed%20in%20recovering%20features%20contributing%20to%20nonlinear%0Aeffects%20at%20stationary%20points%2C%20whereas%20Gaussian%20kernels%20recover%20only%20linear%0Aones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Compositional%2520Kernel%2520Model%2520for%2520Feature%2520Learning%26entry.906535625%3DFeng%2520Ruan%2520and%2520Keli%2520Liu%2520and%2520Michael%2520Jordan%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520compositional%2520variant%2520of%2520kernel%2520ridge%2520regression%2520in%2520which%2520the%250Apredictor%2520is%2520applied%2520to%2520a%2520coordinate-wise%2520reweighting%2520of%2520the%2520inputs.%2520Formulated%250Aas%2520a%2520variational%2520problem%252C%2520this%2520model%2520provides%2520a%2520simple%2520testbed%2520for%2520feature%250Alearning%2520in%2520compositional%2520architectures.%2520From%2520the%2520perspective%2520of%2520variable%250Aselection%252C%2520we%2520show%2520how%2520relevant%2520variables%2520are%2520recovered%2520while%2520noise%2520variables%250Aare%2520eliminated.%2520We%2520establish%2520guarantees%2520showing%2520that%2520both%2520global%2520minimizers%2520and%250Astationary%2520points%2520discard%2520noise%2520coordinates%2520when%2520the%2520noise%2520variables%2520are%250AGaussian%2520distributed.%2520A%2520central%2520finding%2520is%2520that%2520%2524%255Cell_1%2524-type%2520kernels%252C%2520such%2520as%250Athe%2520Laplace%2520kernel%252C%2520succeed%2520in%2520recovering%2520features%2520contributing%2520to%2520nonlinear%250Aeffects%2520at%2520stationary%2520points%252C%2520whereas%2520Gaussian%2520kernels%2520recover%2520only%2520linear%250Aones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Compositional%20Kernel%20Model%20for%20Feature%20Learning&entry.906535625=Feng%20Ruan%20and%20Keli%20Liu%20and%20Michael%20Jordan&entry.1292438233=%20%20We%20study%20a%20compositional%20variant%20of%20kernel%20ridge%20regression%20in%20which%20the%0Apredictor%20is%20applied%20to%20a%20coordinate-wise%20reweighting%20of%20the%20inputs.%20Formulated%0Aas%20a%20variational%20problem%2C%20this%20model%20provides%20a%20simple%20testbed%20for%20feature%0Alearning%20in%20compositional%20architectures.%20From%20the%20perspective%20of%20variable%0Aselection%2C%20we%20show%20how%20relevant%20variables%20are%20recovered%20while%20noise%20variables%0Aare%20eliminated.%20We%20establish%20guarantees%20showing%20that%20both%20global%20minimizers%20and%0Astationary%20points%20discard%20noise%20coordinates%20when%20the%20noise%20variables%20are%0AGaussian%20distributed.%20A%20central%20finding%20is%20that%20%24%5Cell_1%24-type%20kernels%2C%20such%20as%0Athe%20Laplace%20kernel%2C%20succeed%20in%20recovering%20features%20contributing%20to%20nonlinear%0Aeffects%20at%20stationary%20points%2C%20whereas%20Gaussian%20kernels%20recover%20only%20linear%0Aones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14158v1&entry.124074799=Read"},
{"title": "CyberLLMInstruct: A Pseudo-malicious Dataset Revealing\n  Safety-performance Trade-offs in Cyber Security LLM Fine-tuning", "author": "Adel ElZemity and Budi Arief and Shujun Li", "abstract": "  The integration of large language models (LLMs) into cyber security\napplications presents both opportunities and critical safety risks. We\nintroduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious\ninstruction-response pairs spanning cyber security tasks including malware\nanalysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive\nevaluation using seven open-source LLMs reveals a critical trade-off: while\nfine-tuning improves cyber security task performance (achieving up to 92.50%\naccuracy on CyberMetric), it severely compromises safety resilience across all\ntested models and attack vectors (e.g., Llama 3.1 8B's security score against\nprompt injection drops from 0.95 to 0.15). The dataset incorporates diverse\nsources including CTF challenges, academic papers, industry reports, and CVE\ndatabases to ensure comprehensive coverage of cyber security domains. Our\nfindings highlight the unique challenges of securing LLMs in adversarial\ndomains and establish the critical need for developing fine-tuning\nmethodologies that balance performance gains with safety preservation in\nsecurity-sensitive domains.\n", "link": "http://arxiv.org/abs/2503.09334v3", "date": "2025-09-17", "relevancy": 1.8242, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CyberLLMInstruct%3A%20A%20Pseudo-malicious%20Dataset%20Revealing%0A%20%20Safety-performance%20Trade-offs%20in%20Cyber%20Security%20LLM%20Fine-tuning&body=Title%3A%20CyberLLMInstruct%3A%20A%20Pseudo-malicious%20Dataset%20Revealing%0A%20%20Safety-performance%20Trade-offs%20in%20Cyber%20Security%20LLM%20Fine-tuning%0AAuthor%3A%20Adel%20ElZemity%20and%20Budi%20Arief%20and%20Shujun%20Li%0AAbstract%3A%20%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20cyber%20security%0Aapplications%20presents%20both%20opportunities%20and%20critical%20safety%20risks.%20We%0Aintroduce%20CyberLLMInstruct%2C%20a%20dataset%20of%2054%2C928%20pseudo-malicious%0Ainstruction-response%20pairs%20spanning%20cyber%20security%20tasks%20including%20malware%0Aanalysis%2C%20phishing%20simulations%2C%20and%20zero-day%20vulnerabilities.%20Our%20comprehensive%0Aevaluation%20using%20seven%20open-source%20LLMs%20reveals%20a%20critical%20trade-off%3A%20while%0Afine-tuning%20improves%20cyber%20security%20task%20performance%20%28achieving%20up%20to%2092.50%25%0Aaccuracy%20on%20CyberMetric%29%2C%20it%20severely%20compromises%20safety%20resilience%20across%20all%0Atested%20models%20and%20attack%20vectors%20%28e.g.%2C%20Llama%203.1%208B%27s%20security%20score%20against%0Aprompt%20injection%20drops%20from%200.95%20to%200.15%29.%20The%20dataset%20incorporates%20diverse%0Asources%20including%20CTF%20challenges%2C%20academic%20papers%2C%20industry%20reports%2C%20and%20CVE%0Adatabases%20to%20ensure%20comprehensive%20coverage%20of%20cyber%20security%20domains.%20Our%0Afindings%20highlight%20the%20unique%20challenges%20of%20securing%20LLMs%20in%20adversarial%0Adomains%20and%20establish%20the%20critical%20need%20for%20developing%20fine-tuning%0Amethodologies%20that%20balance%20performance%20gains%20with%20safety%20preservation%20in%0Asecurity-sensitive%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyberLLMInstruct%253A%2520A%2520Pseudo-malicious%2520Dataset%2520Revealing%250A%2520%2520Safety-performance%2520Trade-offs%2520in%2520Cyber%2520Security%2520LLM%2520Fine-tuning%26entry.906535625%3DAdel%2520ElZemity%2520and%2520Budi%2520Arief%2520and%2520Shujun%2520Li%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520cyber%2520security%250Aapplications%2520presents%2520both%2520opportunities%2520and%2520critical%2520safety%2520risks.%2520We%250Aintroduce%2520CyberLLMInstruct%252C%2520a%2520dataset%2520of%252054%252C928%2520pseudo-malicious%250Ainstruction-response%2520pairs%2520spanning%2520cyber%2520security%2520tasks%2520including%2520malware%250Aanalysis%252C%2520phishing%2520simulations%252C%2520and%2520zero-day%2520vulnerabilities.%2520Our%2520comprehensive%250Aevaluation%2520using%2520seven%2520open-source%2520LLMs%2520reveals%2520a%2520critical%2520trade-off%253A%2520while%250Afine-tuning%2520improves%2520cyber%2520security%2520task%2520performance%2520%2528achieving%2520up%2520to%252092.50%2525%250Aaccuracy%2520on%2520CyberMetric%2529%252C%2520it%2520severely%2520compromises%2520safety%2520resilience%2520across%2520all%250Atested%2520models%2520and%2520attack%2520vectors%2520%2528e.g.%252C%2520Llama%25203.1%25208B%2527s%2520security%2520score%2520against%250Aprompt%2520injection%2520drops%2520from%25200.95%2520to%25200.15%2529.%2520The%2520dataset%2520incorporates%2520diverse%250Asources%2520including%2520CTF%2520challenges%252C%2520academic%2520papers%252C%2520industry%2520reports%252C%2520and%2520CVE%250Adatabases%2520to%2520ensure%2520comprehensive%2520coverage%2520of%2520cyber%2520security%2520domains.%2520Our%250Afindings%2520highlight%2520the%2520unique%2520challenges%2520of%2520securing%2520LLMs%2520in%2520adversarial%250Adomains%2520and%2520establish%2520the%2520critical%2520need%2520for%2520developing%2520fine-tuning%250Amethodologies%2520that%2520balance%2520performance%2520gains%2520with%2520safety%2520preservation%2520in%250Asecurity-sensitive%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CyberLLMInstruct%3A%20A%20Pseudo-malicious%20Dataset%20Revealing%0A%20%20Safety-performance%20Trade-offs%20in%20Cyber%20Security%20LLM%20Fine-tuning&entry.906535625=Adel%20ElZemity%20and%20Budi%20Arief%20and%20Shujun%20Li&entry.1292438233=%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20cyber%20security%0Aapplications%20presents%20both%20opportunities%20and%20critical%20safety%20risks.%20We%0Aintroduce%20CyberLLMInstruct%2C%20a%20dataset%20of%2054%2C928%20pseudo-malicious%0Ainstruction-response%20pairs%20spanning%20cyber%20security%20tasks%20including%20malware%0Aanalysis%2C%20phishing%20simulations%2C%20and%20zero-day%20vulnerabilities.%20Our%20comprehensive%0Aevaluation%20using%20seven%20open-source%20LLMs%20reveals%20a%20critical%20trade-off%3A%20while%0Afine-tuning%20improves%20cyber%20security%20task%20performance%20%28achieving%20up%20to%2092.50%25%0Aaccuracy%20on%20CyberMetric%29%2C%20it%20severely%20compromises%20safety%20resilience%20across%20all%0Atested%20models%20and%20attack%20vectors%20%28e.g.%2C%20Llama%203.1%208B%27s%20security%20score%20against%0Aprompt%20injection%20drops%20from%200.95%20to%200.15%29.%20The%20dataset%20incorporates%20diverse%0Asources%20including%20CTF%20challenges%2C%20academic%20papers%2C%20industry%20reports%2C%20and%20CVE%0Adatabases%20to%20ensure%20comprehensive%20coverage%20of%20cyber%20security%20domains.%20Our%0Afindings%20highlight%20the%20unique%20challenges%20of%20securing%20LLMs%20in%20adversarial%0Adomains%20and%20establish%20the%20critical%20need%20for%20developing%20fine-tuning%0Amethodologies%20that%20balance%20performance%20gains%20with%20safety%20preservation%20in%0Asecurity-sensitive%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09334v3&entry.124074799=Read"},
{"title": "FlightDiffusion: Revolutionising Autonomous Drone Training with\n  Diffusion Models Generating FPV Video", "author": "Valerii Serpiva and Artem Lykov and Faryal Batool and Vladislav Kozlovskiy and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "  We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.\n", "link": "http://arxiv.org/abs/2509.14082v1", "date": "2025-09-17", "relevancy": 1.8046, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6057}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6027}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlightDiffusion%3A%20Revolutionising%20Autonomous%20Drone%20Training%20with%0A%20%20Diffusion%20Models%20Generating%20FPV%20Video&body=Title%3A%20FlightDiffusion%3A%20Revolutionising%20Autonomous%20Drone%20Training%20with%0A%20%20Diffusion%20Models%20Generating%20FPV%20Video%0AAuthor%3A%20Valerii%20Serpiva%20and%20Artem%20Lykov%20and%20Faryal%20Batool%20and%20Vladislav%20Kozlovskiy%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20We%20present%20FlightDiffusion%2C%20a%20diffusion-model-based%20framework%20for%20training%0Aautonomous%20drones%20from%20first-person%20view%20%28FPV%29%20video.%20Our%20model%20generates%0Arealistic%20video%20sequences%20from%20a%20single%20frame%2C%20enriched%20with%20corresponding%0Aaction%20spaces%20to%20enable%20reasoning-driven%20navigation%20in%20dynamic%20environments.%0ABeyond%20direct%20policy%20learning%2C%20FlightDiffusion%20leverages%20its%20generative%0Acapabilities%20to%20synthesize%20diverse%20FPV%20trajectories%20and%20state-action%20pairs%2C%0Afacilitating%20the%20creation%20of%20large-scale%20training%20datasets%20without%20the%20high%0Acost%20of%20real-world%20data%20collection.%20Our%20evaluation%20demonstrates%20that%20the%0Agenerated%20trajectories%20are%20physically%20plausible%20and%20executable%2C%20with%20a%20mean%0Aposition%20error%20of%200.25%20m%20%28RMSE%200.28%20m%29%20and%20a%20mean%20orientation%20error%20of%200.19%20rad%0A%28RMSE%200.24%20rad%29.%20This%20approach%20enables%20improved%20policy%20learning%20and%20dataset%0Ascalability%2C%20leading%20to%20superior%20performance%20in%20downstream%20navigation%20tasks.%0AResults%20in%20simulated%20environments%20highlight%20enhanced%20robustness%2C%20smoother%0Atrajectory%20planning%2C%20and%20adaptability%20to%20unseen%20conditions.%20An%20ANOVA%20revealed%0Ano%20statistically%20significant%20difference%20between%20performance%20in%20simulation%20and%0Areality%20%28F%281%2C%2016%29%20%3D%200.394%2C%20p%20%3D%200.541%29%2C%20with%20success%20rates%20of%20M%20%3D%200.628%20%28SD%20%3D%0A0.162%29%20and%20M%20%3D%200.617%20%28SD%20%3D%200.177%29%2C%20respectively%2C%20indicating%20strong%20sim-to-real%0Atransfer.%20The%20generated%20datasets%20provide%20a%20valuable%20resource%20for%20future%20UAV%0Aresearch.%20This%20work%20introduces%20diffusion-based%20reasoning%20as%20a%20promising%0Aparadigm%20for%20unifying%20navigation%2C%20action%20generation%2C%20and%20data%20synthesis%20in%0Aaerial%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlightDiffusion%253A%2520Revolutionising%2520Autonomous%2520Drone%2520Training%2520with%250A%2520%2520Diffusion%2520Models%2520Generating%2520FPV%2520Video%26entry.906535625%3DValerii%2520Serpiva%2520and%2520Artem%2520Lykov%2520and%2520Faryal%2520Batool%2520and%2520Vladislav%2520Kozlovskiy%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520We%2520present%2520FlightDiffusion%252C%2520a%2520diffusion-model-based%2520framework%2520for%2520training%250Aautonomous%2520drones%2520from%2520first-person%2520view%2520%2528FPV%2529%2520video.%2520Our%2520model%2520generates%250Arealistic%2520video%2520sequences%2520from%2520a%2520single%2520frame%252C%2520enriched%2520with%2520corresponding%250Aaction%2520spaces%2520to%2520enable%2520reasoning-driven%2520navigation%2520in%2520dynamic%2520environments.%250ABeyond%2520direct%2520policy%2520learning%252C%2520FlightDiffusion%2520leverages%2520its%2520generative%250Acapabilities%2520to%2520synthesize%2520diverse%2520FPV%2520trajectories%2520and%2520state-action%2520pairs%252C%250Afacilitating%2520the%2520creation%2520of%2520large-scale%2520training%2520datasets%2520without%2520the%2520high%250Acost%2520of%2520real-world%2520data%2520collection.%2520Our%2520evaluation%2520demonstrates%2520that%2520the%250Agenerated%2520trajectories%2520are%2520physically%2520plausible%2520and%2520executable%252C%2520with%2520a%2520mean%250Aposition%2520error%2520of%25200.25%2520m%2520%2528RMSE%25200.28%2520m%2529%2520and%2520a%2520mean%2520orientation%2520error%2520of%25200.19%2520rad%250A%2528RMSE%25200.24%2520rad%2529.%2520This%2520approach%2520enables%2520improved%2520policy%2520learning%2520and%2520dataset%250Ascalability%252C%2520leading%2520to%2520superior%2520performance%2520in%2520downstream%2520navigation%2520tasks.%250AResults%2520in%2520simulated%2520environments%2520highlight%2520enhanced%2520robustness%252C%2520smoother%250Atrajectory%2520planning%252C%2520and%2520adaptability%2520to%2520unseen%2520conditions.%2520An%2520ANOVA%2520revealed%250Ano%2520statistically%2520significant%2520difference%2520between%2520performance%2520in%2520simulation%2520and%250Areality%2520%2528F%25281%252C%252016%2529%2520%253D%25200.394%252C%2520p%2520%253D%25200.541%2529%252C%2520with%2520success%2520rates%2520of%2520M%2520%253D%25200.628%2520%2528SD%2520%253D%250A0.162%2529%2520and%2520M%2520%253D%25200.617%2520%2528SD%2520%253D%25200.177%2529%252C%2520respectively%252C%2520indicating%2520strong%2520sim-to-real%250Atransfer.%2520The%2520generated%2520datasets%2520provide%2520a%2520valuable%2520resource%2520for%2520future%2520UAV%250Aresearch.%2520This%2520work%2520introduces%2520diffusion-based%2520reasoning%2520as%2520a%2520promising%250Aparadigm%2520for%2520unifying%2520navigation%252C%2520action%2520generation%252C%2520and%2520data%2520synthesis%2520in%250Aaerial%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightDiffusion%3A%20Revolutionising%20Autonomous%20Drone%20Training%20with%0A%20%20Diffusion%20Models%20Generating%20FPV%20Video&entry.906535625=Valerii%20Serpiva%20and%20Artem%20Lykov%20and%20Faryal%20Batool%20and%20Vladislav%20Kozlovskiy%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20We%20present%20FlightDiffusion%2C%20a%20diffusion-model-based%20framework%20for%20training%0Aautonomous%20drones%20from%20first-person%20view%20%28FPV%29%20video.%20Our%20model%20generates%0Arealistic%20video%20sequences%20from%20a%20single%20frame%2C%20enriched%20with%20corresponding%0Aaction%20spaces%20to%20enable%20reasoning-driven%20navigation%20in%20dynamic%20environments.%0ABeyond%20direct%20policy%20learning%2C%20FlightDiffusion%20leverages%20its%20generative%0Acapabilities%20to%20synthesize%20diverse%20FPV%20trajectories%20and%20state-action%20pairs%2C%0Afacilitating%20the%20creation%20of%20large-scale%20training%20datasets%20without%20the%20high%0Acost%20of%20real-world%20data%20collection.%20Our%20evaluation%20demonstrates%20that%20the%0Agenerated%20trajectories%20are%20physically%20plausible%20and%20executable%2C%20with%20a%20mean%0Aposition%20error%20of%200.25%20m%20%28RMSE%200.28%20m%29%20and%20a%20mean%20orientation%20error%20of%200.19%20rad%0A%28RMSE%200.24%20rad%29.%20This%20approach%20enables%20improved%20policy%20learning%20and%20dataset%0Ascalability%2C%20leading%20to%20superior%20performance%20in%20downstream%20navigation%20tasks.%0AResults%20in%20simulated%20environments%20highlight%20enhanced%20robustness%2C%20smoother%0Atrajectory%20planning%2C%20and%20adaptability%20to%20unseen%20conditions.%20An%20ANOVA%20revealed%0Ano%20statistically%20significant%20difference%20between%20performance%20in%20simulation%20and%0Areality%20%28F%281%2C%2016%29%20%3D%200.394%2C%20p%20%3D%200.541%29%2C%20with%20success%20rates%20of%20M%20%3D%200.628%20%28SD%20%3D%0A0.162%29%20and%20M%20%3D%200.617%20%28SD%20%3D%200.177%29%2C%20respectively%2C%20indicating%20strong%20sim-to-real%0Atransfer.%20The%20generated%20datasets%20provide%20a%20valuable%20resource%20for%20future%20UAV%0Aresearch.%20This%20work%20introduces%20diffusion-based%20reasoning%20as%20a%20promising%0Aparadigm%20for%20unifying%20navigation%2C%20action%20generation%2C%20and%20data%20synthesis%20in%0Aaerial%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14082v1&entry.124074799=Read"},
{"title": "Artificial neural networks ensemble methodology to predict significant\n  wave height", "author": "Felipe Crivellaro Minuzzi and Leandro Farina", "abstract": "  The forecast of wave variables are important for several applications that\ndepend on a better description of the ocean state. Due to the chaotic behaviour\nof the differential equations which model this problem, a well know strategy to\novercome the difficulties is basically to run several simulations, by for\ninstance, varying the initial condition, and averaging the result of each of\nthese, creating an ensemble. Moreover, in the last few years, considering the\namount of available data and the computational power increase, machine learning\nalgorithms have been applied as surrogate to traditional numerical models,\nyielding comparative or better results. In this work, we present a methodology\nto create an ensemble of different artificial neural networks architectures,\nnamely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict\nsignificant wave height on six different locations in the Brazilian coast. The\nnetworks are trained using NOAA's numerical reforecast data and target the\nresidual between observational data and the numerical model output. A new\nstrategy to create the training and target datasets is demonstrated. Results\nshow that our framework is capable of producing high efficient forecast, with\nan average accuracy of $80\\%$, that can achieve up to $88\\%$ in the best case\nscenario, which means $5\\%$ reduction in error metrics if compared to NOAA's\nnumerical model, and a increasingly reduction of computational cost.\n", "link": "http://arxiv.org/abs/2509.14020v1", "date": "2025-09-17", "relevancy": 1.7973, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20neural%20networks%20ensemble%20methodology%20to%20predict%20significant%0A%20%20wave%20height&body=Title%3A%20Artificial%20neural%20networks%20ensemble%20methodology%20to%20predict%20significant%0A%20%20wave%20height%0AAuthor%3A%20Felipe%20Crivellaro%20Minuzzi%20and%20Leandro%20Farina%0AAbstract%3A%20%20%20The%20forecast%20of%20wave%20variables%20are%20important%20for%20several%20applications%20that%0Adepend%20on%20a%20better%20description%20of%20the%20ocean%20state.%20Due%20to%20the%20chaotic%20behaviour%0Aof%20the%20differential%20equations%20which%20model%20this%20problem%2C%20a%20well%20know%20strategy%20to%0Aovercome%20the%20difficulties%20is%20basically%20to%20run%20several%20simulations%2C%20by%20for%0Ainstance%2C%20varying%20the%20initial%20condition%2C%20and%20averaging%20the%20result%20of%20each%20of%0Athese%2C%20creating%20an%20ensemble.%20Moreover%2C%20in%20the%20last%20few%20years%2C%20considering%20the%0Aamount%20of%20available%20data%20and%20the%20computational%20power%20increase%2C%20machine%20learning%0Aalgorithms%20have%20been%20applied%20as%20surrogate%20to%20traditional%20numerical%20models%2C%0Ayielding%20comparative%20or%20better%20results.%20In%20this%20work%2C%20we%20present%20a%20methodology%0Ato%20create%20an%20ensemble%20of%20different%20artificial%20neural%20networks%20architectures%2C%0Anamely%2C%20MLP%2C%20RNN%2C%20LSTM%2C%20CNN%20and%20a%20hybrid%20CNN-LSTM%2C%20which%20aims%20to%20predict%0Asignificant%20wave%20height%20on%20six%20different%20locations%20in%20the%20Brazilian%20coast.%20The%0Anetworks%20are%20trained%20using%20NOAA%27s%20numerical%20reforecast%20data%20and%20target%20the%0Aresidual%20between%20observational%20data%20and%20the%20numerical%20model%20output.%20A%20new%0Astrategy%20to%20create%20the%20training%20and%20target%20datasets%20is%20demonstrated.%20Results%0Ashow%20that%20our%20framework%20is%20capable%20of%20producing%20high%20efficient%20forecast%2C%20with%0Aan%20average%20accuracy%20of%20%2480%5C%25%24%2C%20that%20can%20achieve%20up%20to%20%2488%5C%25%24%20in%20the%20best%20case%0Ascenario%2C%20which%20means%20%245%5C%25%24%20reduction%20in%20error%20metrics%20if%20compared%20to%20NOAA%27s%0Anumerical%20model%2C%20and%20a%20increasingly%20reduction%20of%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520neural%2520networks%2520ensemble%2520methodology%2520to%2520predict%2520significant%250A%2520%2520wave%2520height%26entry.906535625%3DFelipe%2520Crivellaro%2520Minuzzi%2520and%2520Leandro%2520Farina%26entry.1292438233%3D%2520%2520The%2520forecast%2520of%2520wave%2520variables%2520are%2520important%2520for%2520several%2520applications%2520that%250Adepend%2520on%2520a%2520better%2520description%2520of%2520the%2520ocean%2520state.%2520Due%2520to%2520the%2520chaotic%2520behaviour%250Aof%2520the%2520differential%2520equations%2520which%2520model%2520this%2520problem%252C%2520a%2520well%2520know%2520strategy%2520to%250Aovercome%2520the%2520difficulties%2520is%2520basically%2520to%2520run%2520several%2520simulations%252C%2520by%2520for%250Ainstance%252C%2520varying%2520the%2520initial%2520condition%252C%2520and%2520averaging%2520the%2520result%2520of%2520each%2520of%250Athese%252C%2520creating%2520an%2520ensemble.%2520Moreover%252C%2520in%2520the%2520last%2520few%2520years%252C%2520considering%2520the%250Aamount%2520of%2520available%2520data%2520and%2520the%2520computational%2520power%2520increase%252C%2520machine%2520learning%250Aalgorithms%2520have%2520been%2520applied%2520as%2520surrogate%2520to%2520traditional%2520numerical%2520models%252C%250Ayielding%2520comparative%2520or%2520better%2520results.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520methodology%250Ato%2520create%2520an%2520ensemble%2520of%2520different%2520artificial%2520neural%2520networks%2520architectures%252C%250Anamely%252C%2520MLP%252C%2520RNN%252C%2520LSTM%252C%2520CNN%2520and%2520a%2520hybrid%2520CNN-LSTM%252C%2520which%2520aims%2520to%2520predict%250Asignificant%2520wave%2520height%2520on%2520six%2520different%2520locations%2520in%2520the%2520Brazilian%2520coast.%2520The%250Anetworks%2520are%2520trained%2520using%2520NOAA%2527s%2520numerical%2520reforecast%2520data%2520and%2520target%2520the%250Aresidual%2520between%2520observational%2520data%2520and%2520the%2520numerical%2520model%2520output.%2520A%2520new%250Astrategy%2520to%2520create%2520the%2520training%2520and%2520target%2520datasets%2520is%2520demonstrated.%2520Results%250Ashow%2520that%2520our%2520framework%2520is%2520capable%2520of%2520producing%2520high%2520efficient%2520forecast%252C%2520with%250Aan%2520average%2520accuracy%2520of%2520%252480%255C%2525%2524%252C%2520that%2520can%2520achieve%2520up%2520to%2520%252488%255C%2525%2524%2520in%2520the%2520best%2520case%250Ascenario%252C%2520which%2520means%2520%25245%255C%2525%2524%2520reduction%2520in%2520error%2520metrics%2520if%2520compared%2520to%2520NOAA%2527s%250Anumerical%2520model%252C%2520and%2520a%2520increasingly%2520reduction%2520of%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20neural%20networks%20ensemble%20methodology%20to%20predict%20significant%0A%20%20wave%20height&entry.906535625=Felipe%20Crivellaro%20Minuzzi%20and%20Leandro%20Farina&entry.1292438233=%20%20The%20forecast%20of%20wave%20variables%20are%20important%20for%20several%20applications%20that%0Adepend%20on%20a%20better%20description%20of%20the%20ocean%20state.%20Due%20to%20the%20chaotic%20behaviour%0Aof%20the%20differential%20equations%20which%20model%20this%20problem%2C%20a%20well%20know%20strategy%20to%0Aovercome%20the%20difficulties%20is%20basically%20to%20run%20several%20simulations%2C%20by%20for%0Ainstance%2C%20varying%20the%20initial%20condition%2C%20and%20averaging%20the%20result%20of%20each%20of%0Athese%2C%20creating%20an%20ensemble.%20Moreover%2C%20in%20the%20last%20few%20years%2C%20considering%20the%0Aamount%20of%20available%20data%20and%20the%20computational%20power%20increase%2C%20machine%20learning%0Aalgorithms%20have%20been%20applied%20as%20surrogate%20to%20traditional%20numerical%20models%2C%0Ayielding%20comparative%20or%20better%20results.%20In%20this%20work%2C%20we%20present%20a%20methodology%0Ato%20create%20an%20ensemble%20of%20different%20artificial%20neural%20networks%20architectures%2C%0Anamely%2C%20MLP%2C%20RNN%2C%20LSTM%2C%20CNN%20and%20a%20hybrid%20CNN-LSTM%2C%20which%20aims%20to%20predict%0Asignificant%20wave%20height%20on%20six%20different%20locations%20in%20the%20Brazilian%20coast.%20The%0Anetworks%20are%20trained%20using%20NOAA%27s%20numerical%20reforecast%20data%20and%20target%20the%0Aresidual%20between%20observational%20data%20and%20the%20numerical%20model%20output.%20A%20new%0Astrategy%20to%20create%20the%20training%20and%20target%20datasets%20is%20demonstrated.%20Results%0Ashow%20that%20our%20framework%20is%20capable%20of%20producing%20high%20efficient%20forecast%2C%20with%0Aan%20average%20accuracy%20of%20%2480%5C%25%24%2C%20that%20can%20achieve%20up%20to%20%2488%5C%25%24%20in%20the%20best%20case%0Ascenario%2C%20which%20means%20%245%5C%25%24%20reduction%20in%20error%20metrics%20if%20compared%20to%20NOAA%27s%0Anumerical%20model%2C%20and%20a%20increasingly%20reduction%20of%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14020v1&entry.124074799=Read"},
{"title": "Classification Filtering", "author": "Ilker Bayram", "abstract": "  We consider a streaming signal in which each sample is linked to a latent\nclass. We assume that multiple classifiers are available, each providing class\nprobabilities with varying degrees of accuracy. These classifiers are employed\nfollowing a straightforward and fixed policy. In this setting, we consider the\nproblem of fusing the output of the classifiers while incorporating the\ntemporal aspect to improve classification accuracy. We propose a state-space\nmodel and develop a filter tailored for realtime execution. We demonstrate the\neffectiveness of the proposed filter in an activity classification application\nbased on inertial measurement unit (IMU) data from a wearable device.\n", "link": "http://arxiv.org/abs/2509.13975v1", "date": "2025-09-17", "relevancy": 1.7926, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4578}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20Filtering&body=Title%3A%20Classification%20Filtering%0AAuthor%3A%20Ilker%20Bayram%0AAbstract%3A%20%20%20We%20consider%20a%20streaming%20signal%20in%20which%20each%20sample%20is%20linked%20to%20a%20latent%0Aclass.%20We%20assume%20that%20multiple%20classifiers%20are%20available%2C%20each%20providing%20class%0Aprobabilities%20with%20varying%20degrees%20of%20accuracy.%20These%20classifiers%20are%20employed%0Afollowing%20a%20straightforward%20and%20fixed%20policy.%20In%20this%20setting%2C%20we%20consider%20the%0Aproblem%20of%20fusing%20the%20output%20of%20the%20classifiers%20while%20incorporating%20the%0Atemporal%20aspect%20to%20improve%20classification%20accuracy.%20We%20propose%20a%20state-space%0Amodel%20and%20develop%20a%20filter%20tailored%20for%20realtime%20execution.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20filter%20in%20an%20activity%20classification%20application%0Abased%20on%20inertial%20measurement%20unit%20%28IMU%29%20data%20from%20a%20wearable%20device.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520Filtering%26entry.906535625%3DIlker%2520Bayram%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520streaming%2520signal%2520in%2520which%2520each%2520sample%2520is%2520linked%2520to%2520a%2520latent%250Aclass.%2520We%2520assume%2520that%2520multiple%2520classifiers%2520are%2520available%252C%2520each%2520providing%2520class%250Aprobabilities%2520with%2520varying%2520degrees%2520of%2520accuracy.%2520These%2520classifiers%2520are%2520employed%250Afollowing%2520a%2520straightforward%2520and%2520fixed%2520policy.%2520In%2520this%2520setting%252C%2520we%2520consider%2520the%250Aproblem%2520of%2520fusing%2520the%2520output%2520of%2520the%2520classifiers%2520while%2520incorporating%2520the%250Atemporal%2520aspect%2520to%2520improve%2520classification%2520accuracy.%2520We%2520propose%2520a%2520state-space%250Amodel%2520and%2520develop%2520a%2520filter%2520tailored%2520for%2520realtime%2520execution.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520filter%2520in%2520an%2520activity%2520classification%2520application%250Abased%2520on%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520data%2520from%2520a%2520wearable%2520device.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Filtering&entry.906535625=Ilker%20Bayram&entry.1292438233=%20%20We%20consider%20a%20streaming%20signal%20in%20which%20each%20sample%20is%20linked%20to%20a%20latent%0Aclass.%20We%20assume%20that%20multiple%20classifiers%20are%20available%2C%20each%20providing%20class%0Aprobabilities%20with%20varying%20degrees%20of%20accuracy.%20These%20classifiers%20are%20employed%0Afollowing%20a%20straightforward%20and%20fixed%20policy.%20In%20this%20setting%2C%20we%20consider%20the%0Aproblem%20of%20fusing%20the%20output%20of%20the%20classifiers%20while%20incorporating%20the%0Atemporal%20aspect%20to%20improve%20classification%20accuracy.%20We%20propose%20a%20state-space%0Amodel%20and%20develop%20a%20filter%20tailored%20for%20realtime%20execution.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20filter%20in%20an%20activity%20classification%20application%0Abased%20on%20inertial%20measurement%20unit%20%28IMU%29%20data%20from%20a%20wearable%20device.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13975v1&entry.124074799=Read"},
{"title": "CoVariance Filters and Neural Networks over Hilbert Spaces", "author": "Claudio Battiloro and Andrea Cavallo and Elvin Isufi", "abstract": "  CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical\ncovariance matrix of signals defined over finite-dimensional Hilbert spaces,\nmotivated by robustness and transferability properties. Yet, little is known\nabout how these arguments extend to infinite-dimensional Hilbert spaces. In\nthis work, we take a first step by introducing a novel convolutional learning\nframework for signals defined over infinite-dimensional Hilbert spaces,\ncentered on the (empirical) covariance operator. We constructively define\nHilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)\nas stacks of HVF filterbanks with nonlinear activations. We propose a\nprincipled discretization procedure, and we prove that empirical HVFs can\nrecover the Functional PCA (FPCA) of the filtered signals. We then describe the\nversatility of our framework with examples ranging from multivariate\nreal-valued functions to reproducing kernel Hilbert spaces. Finally, we\nvalidate HVNs on both synthetic and real-world time-series classification\ntasks, showing robust performance compared to MLP and FPCA-based classifiers.\n", "link": "http://arxiv.org/abs/2509.13178v2", "date": "2025-09-17", "relevancy": 1.7922, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4605}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoVariance%20Filters%20and%20Neural%20Networks%20over%20Hilbert%20Spaces&body=Title%3A%20CoVariance%20Filters%20and%20Neural%20Networks%20over%20Hilbert%20Spaces%0AAuthor%3A%20Claudio%20Battiloro%20and%20Andrea%20Cavallo%20and%20Elvin%20Isufi%0AAbstract%3A%20%20%20CoVariance%20Neural%20Networks%20%28VNNs%29%20perform%20graph%20convolutions%20on%20the%20empirical%0Acovariance%20matrix%20of%20signals%20defined%20over%20finite-dimensional%20Hilbert%20spaces%2C%0Amotivated%20by%20robustness%20and%20transferability%20properties.%20Yet%2C%20little%20is%20known%0Aabout%20how%20these%20arguments%20extend%20to%20infinite-dimensional%20Hilbert%20spaces.%20In%0Athis%20work%2C%20we%20take%20a%20first%20step%20by%20introducing%20a%20novel%20convolutional%20learning%0Aframework%20for%20signals%20defined%20over%20infinite-dimensional%20Hilbert%20spaces%2C%0Acentered%20on%20the%20%28empirical%29%20covariance%20operator.%20We%20constructively%20define%0AHilbert%20coVariance%20Filters%20%28HVFs%29%20and%20design%20Hilbert%20coVariance%20Networks%20%28HVNs%29%0Aas%20stacks%20of%20HVF%20filterbanks%20with%20nonlinear%20activations.%20We%20propose%20a%0Aprincipled%20discretization%20procedure%2C%20and%20we%20prove%20that%20empirical%20HVFs%20can%0Arecover%20the%20Functional%20PCA%20%28FPCA%29%20of%20the%20filtered%20signals.%20We%20then%20describe%20the%0Aversatility%20of%20our%20framework%20with%20examples%20ranging%20from%20multivariate%0Areal-valued%20functions%20to%20reproducing%20kernel%20Hilbert%20spaces.%20Finally%2C%20we%0Avalidate%20HVNs%20on%20both%20synthetic%20and%20real-world%20time-series%20classification%0Atasks%2C%20showing%20robust%20performance%20compared%20to%20MLP%20and%20FPCA-based%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoVariance%2520Filters%2520and%2520Neural%2520Networks%2520over%2520Hilbert%2520Spaces%26entry.906535625%3DClaudio%2520Battiloro%2520and%2520Andrea%2520Cavallo%2520and%2520Elvin%2520Isufi%26entry.1292438233%3D%2520%2520CoVariance%2520Neural%2520Networks%2520%2528VNNs%2529%2520perform%2520graph%2520convolutions%2520on%2520the%2520empirical%250Acovariance%2520matrix%2520of%2520signals%2520defined%2520over%2520finite-dimensional%2520Hilbert%2520spaces%252C%250Amotivated%2520by%2520robustness%2520and%2520transferability%2520properties.%2520Yet%252C%2520little%2520is%2520known%250Aabout%2520how%2520these%2520arguments%2520extend%2520to%2520infinite-dimensional%2520Hilbert%2520spaces.%2520In%250Athis%2520work%252C%2520we%2520take%2520a%2520first%2520step%2520by%2520introducing%2520a%2520novel%2520convolutional%2520learning%250Aframework%2520for%2520signals%2520defined%2520over%2520infinite-dimensional%2520Hilbert%2520spaces%252C%250Acentered%2520on%2520the%2520%2528empirical%2529%2520covariance%2520operator.%2520We%2520constructively%2520define%250AHilbert%2520coVariance%2520Filters%2520%2528HVFs%2529%2520and%2520design%2520Hilbert%2520coVariance%2520Networks%2520%2528HVNs%2529%250Aas%2520stacks%2520of%2520HVF%2520filterbanks%2520with%2520nonlinear%2520activations.%2520We%2520propose%2520a%250Aprincipled%2520discretization%2520procedure%252C%2520and%2520we%2520prove%2520that%2520empirical%2520HVFs%2520can%250Arecover%2520the%2520Functional%2520PCA%2520%2528FPCA%2529%2520of%2520the%2520filtered%2520signals.%2520We%2520then%2520describe%2520the%250Aversatility%2520of%2520our%2520framework%2520with%2520examples%2520ranging%2520from%2520multivariate%250Areal-valued%2520functions%2520to%2520reproducing%2520kernel%2520Hilbert%2520spaces.%2520Finally%252C%2520we%250Avalidate%2520HVNs%2520on%2520both%2520synthetic%2520and%2520real-world%2520time-series%2520classification%250Atasks%252C%2520showing%2520robust%2520performance%2520compared%2520to%2520MLP%2520and%2520FPCA-based%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoVariance%20Filters%20and%20Neural%20Networks%20over%20Hilbert%20Spaces&entry.906535625=Claudio%20Battiloro%20and%20Andrea%20Cavallo%20and%20Elvin%20Isufi&entry.1292438233=%20%20CoVariance%20Neural%20Networks%20%28VNNs%29%20perform%20graph%20convolutions%20on%20the%20empirical%0Acovariance%20matrix%20of%20signals%20defined%20over%20finite-dimensional%20Hilbert%20spaces%2C%0Amotivated%20by%20robustness%20and%20transferability%20properties.%20Yet%2C%20little%20is%20known%0Aabout%20how%20these%20arguments%20extend%20to%20infinite-dimensional%20Hilbert%20spaces.%20In%0Athis%20work%2C%20we%20take%20a%20first%20step%20by%20introducing%20a%20novel%20convolutional%20learning%0Aframework%20for%20signals%20defined%20over%20infinite-dimensional%20Hilbert%20spaces%2C%0Acentered%20on%20the%20%28empirical%29%20covariance%20operator.%20We%20constructively%20define%0AHilbert%20coVariance%20Filters%20%28HVFs%29%20and%20design%20Hilbert%20coVariance%20Networks%20%28HVNs%29%0Aas%20stacks%20of%20HVF%20filterbanks%20with%20nonlinear%20activations.%20We%20propose%20a%0Aprincipled%20discretization%20procedure%2C%20and%20we%20prove%20that%20empirical%20HVFs%20can%0Arecover%20the%20Functional%20PCA%20%28FPCA%29%20of%20the%20filtered%20signals.%20We%20then%20describe%20the%0Aversatility%20of%20our%20framework%20with%20examples%20ranging%20from%20multivariate%0Areal-valued%20functions%20to%20reproducing%20kernel%20Hilbert%20spaces.%20Finally%2C%20we%0Avalidate%20HVNs%20on%20both%20synthetic%20and%20real-world%20time-series%20classification%0Atasks%2C%20showing%20robust%20performance%20compared%20to%20MLP%20and%20FPCA-based%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13178v2&entry.124074799=Read"},
{"title": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations", "author": "Mahjabin Nahar and Eun-Ju Lee and Jin Won Park and Dongwon Lee", "abstract": "  While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or 'hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N=560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.\n", "link": "http://arxiv.org/abs/2504.01153v4", "date": "2025-09-17", "relevancy": 1.7585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4415}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Catch%20Me%20if%20You%20Search%3A%20When%20Contextual%20Web%20Search%20Results%20Affect%20the%0A%20%20Detection%20of%20Hallucinations&body=Title%3A%20Catch%20Me%20if%20You%20Search%3A%20When%20Contextual%20Web%20Search%20Results%20Affect%20the%0A%20%20Detection%20of%20Hallucinations%0AAuthor%3A%20Mahjabin%20Nahar%20and%20Eun-Ju%20Lee%20and%20Jin%20Won%20Park%20and%20Dongwon%20Lee%0AAbstract%3A%20%20%20While%20we%20increasingly%20rely%20on%20large%20language%20models%20%28LLMs%29%20for%20various%20tasks%2C%0Athese%20models%20are%20known%20to%20produce%20inaccurate%20content%20or%20%27hallucinations%27%20with%0Apotentially%20disastrous%20consequences.%20The%20recent%20integration%20of%20web%20search%0Aresults%20into%20LLMs%20prompts%20the%20question%20of%20whether%20people%20utilize%20them%20to%20verify%0Athe%20generated%20content%2C%20thereby%20accurately%20detecting%20hallucinations.%20An%20online%0Aexperiment%20%28N%3D560%29%20investigated%20how%20the%20provision%20of%20search%20results%2C%20either%0Astatic%20%28i.e.%2C%20fixed%20search%20results%20provided%20by%20LLM%29%20or%20dynamic%20%28i.e.%2C%0Aparticipant-led%20searches%29%2C%20affects%20participants%27%20perceived%20accuracy%20of%0ALLM-generated%20content%20%28i.e.%2C%20genuine%2C%20minor%20hallucination%2C%20major%0Ahallucination%29%2C%20self-confidence%20in%20accuracy%20ratings%2C%20as%20well%20as%20their%20overall%0Aevaluation%20of%20the%20LLM%2C%20as%20compared%20to%20the%20control%20condition%20%28i.e.%2C%20no%20search%0Aresults%29.%20Results%20showed%20that%20participants%20in%20both%20static%20and%20dynamic%0Aconditions%20%28vs.%20control%29%20rated%20hallucinated%20content%20to%20be%20less%20accurate%20and%0Aperceived%20the%20LLM%20more%20negatively.%20However%2C%20those%20in%20the%20dynamic%20condition%0Arated%20genuine%20content%20as%20more%20accurate%20and%20demonstrated%20greater%20overall%0Aself-confidence%20in%20their%20assessments%20than%20those%20in%20the%20static%20search%20or%20control%0Aconditions.%20We%20highlighted%20practical%20implications%20of%20incorporating%20web%20search%0Afunctionality%20into%20LLMs%20in%20real-world%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.01153v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatch%2520Me%2520if%2520You%2520Search%253A%2520When%2520Contextual%2520Web%2520Search%2520Results%2520Affect%2520the%250A%2520%2520Detection%2520of%2520Hallucinations%26entry.906535625%3DMahjabin%2520Nahar%2520and%2520Eun-Ju%2520Lee%2520and%2520Jin%2520Won%2520Park%2520and%2520Dongwon%2520Lee%26entry.1292438233%3D%2520%2520While%2520we%2520increasingly%2520rely%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520various%2520tasks%252C%250Athese%2520models%2520are%2520known%2520to%2520produce%2520inaccurate%2520content%2520or%2520%2527hallucinations%2527%2520with%250Apotentially%2520disastrous%2520consequences.%2520The%2520recent%2520integration%2520of%2520web%2520search%250Aresults%2520into%2520LLMs%2520prompts%2520the%2520question%2520of%2520whether%2520people%2520utilize%2520them%2520to%2520verify%250Athe%2520generated%2520content%252C%2520thereby%2520accurately%2520detecting%2520hallucinations.%2520An%2520online%250Aexperiment%2520%2528N%253D560%2529%2520investigated%2520how%2520the%2520provision%2520of%2520search%2520results%252C%2520either%250Astatic%2520%2528i.e.%252C%2520fixed%2520search%2520results%2520provided%2520by%2520LLM%2529%2520or%2520dynamic%2520%2528i.e.%252C%250Aparticipant-led%2520searches%2529%252C%2520affects%2520participants%2527%2520perceived%2520accuracy%2520of%250ALLM-generated%2520content%2520%2528i.e.%252C%2520genuine%252C%2520minor%2520hallucination%252C%2520major%250Ahallucination%2529%252C%2520self-confidence%2520in%2520accuracy%2520ratings%252C%2520as%2520well%2520as%2520their%2520overall%250Aevaluation%2520of%2520the%2520LLM%252C%2520as%2520compared%2520to%2520the%2520control%2520condition%2520%2528i.e.%252C%2520no%2520search%250Aresults%2529.%2520Results%2520showed%2520that%2520participants%2520in%2520both%2520static%2520and%2520dynamic%250Aconditions%2520%2528vs.%2520control%2529%2520rated%2520hallucinated%2520content%2520to%2520be%2520less%2520accurate%2520and%250Aperceived%2520the%2520LLM%2520more%2520negatively.%2520However%252C%2520those%2520in%2520the%2520dynamic%2520condition%250Arated%2520genuine%2520content%2520as%2520more%2520accurate%2520and%2520demonstrated%2520greater%2520overall%250Aself-confidence%2520in%2520their%2520assessments%2520than%2520those%2520in%2520the%2520static%2520search%2520or%2520control%250Aconditions.%2520We%2520highlighted%2520practical%2520implications%2520of%2520incorporating%2520web%2520search%250Afunctionality%2520into%2520LLMs%2520in%2520real-world%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01153v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Catch%20Me%20if%20You%20Search%3A%20When%20Contextual%20Web%20Search%20Results%20Affect%20the%0A%20%20Detection%20of%20Hallucinations&entry.906535625=Mahjabin%20Nahar%20and%20Eun-Ju%20Lee%20and%20Jin%20Won%20Park%20and%20Dongwon%20Lee&entry.1292438233=%20%20While%20we%20increasingly%20rely%20on%20large%20language%20models%20%28LLMs%29%20for%20various%20tasks%2C%0Athese%20models%20are%20known%20to%20produce%20inaccurate%20content%20or%20%27hallucinations%27%20with%0Apotentially%20disastrous%20consequences.%20The%20recent%20integration%20of%20web%20search%0Aresults%20into%20LLMs%20prompts%20the%20question%20of%20whether%20people%20utilize%20them%20to%20verify%0Athe%20generated%20content%2C%20thereby%20accurately%20detecting%20hallucinations.%20An%20online%0Aexperiment%20%28N%3D560%29%20investigated%20how%20the%20provision%20of%20search%20results%2C%20either%0Astatic%20%28i.e.%2C%20fixed%20search%20results%20provided%20by%20LLM%29%20or%20dynamic%20%28i.e.%2C%0Aparticipant-led%20searches%29%2C%20affects%20participants%27%20perceived%20accuracy%20of%0ALLM-generated%20content%20%28i.e.%2C%20genuine%2C%20minor%20hallucination%2C%20major%0Ahallucination%29%2C%20self-confidence%20in%20accuracy%20ratings%2C%20as%20well%20as%20their%20overall%0Aevaluation%20of%20the%20LLM%2C%20as%20compared%20to%20the%20control%20condition%20%28i.e.%2C%20no%20search%0Aresults%29.%20Results%20showed%20that%20participants%20in%20both%20static%20and%20dynamic%0Aconditions%20%28vs.%20control%29%20rated%20hallucinated%20content%20to%20be%20less%20accurate%20and%0Aperceived%20the%20LLM%20more%20negatively.%20However%2C%20those%20in%20the%20dynamic%20condition%0Arated%20genuine%20content%20as%20more%20accurate%20and%20demonstrated%20greater%20overall%0Aself-confidence%20in%20their%20assessments%20than%20those%20in%20the%20static%20search%20or%20control%0Aconditions.%20We%20highlighted%20practical%20implications%20of%20incorporating%20web%20search%0Afunctionality%20into%20LLMs%20in%20real-world%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.01153v4&entry.124074799=Read"},
{"title": "Quantum Reinforcement Learning-Guided Diffusion Model for Image\n  Synthesis via Hybrid Quantum-Classical Generative Model Architectures", "author": "Chi-Sheng Chen and En-Jui Kuo", "abstract": "  Diffusion models typically employ static or heuristic classifier-free\nguidance (CFG) schedules, which often fail to adapt across timesteps and noise\nconditions. In this work, we introduce a quantum reinforcement learning (QRL)\ncontroller that dynamically adjusts CFG at each denoising step. The controller\nadopts a hybrid quantum--classical actor--critic architecture: a shallow\nvariational quantum circuit (VQC) with ring entanglement generates policy\nfeatures, which are mapped by a compact multilayer perceptron (MLP) into\nGaussian actions over $\\Delta$CFG, while a classical critic estimates value\nfunctions. The policy is optimized using Proximal Policy Optimization (PPO)\nwith Generalized Advantage Estimation (GAE), guided by a reward that balances\nclassification confidence, perceptual improvement, and action regularization.\nExperiments on CIFAR-10 demonstrate that our QRL policy improves perceptual\nquality (LPIPS, PSNR, SSIM) while reducing parameter count compared to\nclassical RL actors and fixed schedules. Ablation studies on qubit number and\ncircuit depth reveal trade-offs between accuracy and efficiency, and extended\nevaluations confirm robust generation under long diffusion schedules.\n", "link": "http://arxiv.org/abs/2509.14163v1", "date": "2025-09-17", "relevancy": 1.7571, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6026}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5971}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Reinforcement%20Learning-Guided%20Diffusion%20Model%20for%20Image%0A%20%20Synthesis%20via%20Hybrid%20Quantum-Classical%20Generative%20Model%20Architectures&body=Title%3A%20Quantum%20Reinforcement%20Learning-Guided%20Diffusion%20Model%20for%20Image%0A%20%20Synthesis%20via%20Hybrid%20Quantum-Classical%20Generative%20Model%20Architectures%0AAuthor%3A%20Chi-Sheng%20Chen%20and%20En-Jui%20Kuo%0AAbstract%3A%20%20%20Diffusion%20models%20typically%20employ%20static%20or%20heuristic%20classifier-free%0Aguidance%20%28CFG%29%20schedules%2C%20which%20often%20fail%20to%20adapt%20across%20timesteps%20and%20noise%0Aconditions.%20In%20this%20work%2C%20we%20introduce%20a%20quantum%20reinforcement%20learning%20%28QRL%29%0Acontroller%20that%20dynamically%20adjusts%20CFG%20at%20each%20denoising%20step.%20The%20controller%0Aadopts%20a%20hybrid%20quantum--classical%20actor--critic%20architecture%3A%20a%20shallow%0Avariational%20quantum%20circuit%20%28VQC%29%20with%20ring%20entanglement%20generates%20policy%0Afeatures%2C%20which%20are%20mapped%20by%20a%20compact%20multilayer%20perceptron%20%28MLP%29%20into%0AGaussian%20actions%20over%20%24%5CDelta%24CFG%2C%20while%20a%20classical%20critic%20estimates%20value%0Afunctions.%20The%20policy%20is%20optimized%20using%20Proximal%20Policy%20Optimization%20%28PPO%29%0Awith%20Generalized%20Advantage%20Estimation%20%28GAE%29%2C%20guided%20by%20a%20reward%20that%20balances%0Aclassification%20confidence%2C%20perceptual%20improvement%2C%20and%20action%20regularization.%0AExperiments%20on%20CIFAR-10%20demonstrate%20that%20our%20QRL%20policy%20improves%20perceptual%0Aquality%20%28LPIPS%2C%20PSNR%2C%20SSIM%29%20while%20reducing%20parameter%20count%20compared%20to%0Aclassical%20RL%20actors%20and%20fixed%20schedules.%20Ablation%20studies%20on%20qubit%20number%20and%0Acircuit%20depth%20reveal%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20and%20extended%0Aevaluations%20confirm%20robust%20generation%20under%20long%20diffusion%20schedules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Reinforcement%2520Learning-Guided%2520Diffusion%2520Model%2520for%2520Image%250A%2520%2520Synthesis%2520via%2520Hybrid%2520Quantum-Classical%2520Generative%2520Model%2520Architectures%26entry.906535625%3DChi-Sheng%2520Chen%2520and%2520En-Jui%2520Kuo%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520typically%2520employ%2520static%2520or%2520heuristic%2520classifier-free%250Aguidance%2520%2528CFG%2529%2520schedules%252C%2520which%2520often%2520fail%2520to%2520adapt%2520across%2520timesteps%2520and%2520noise%250Aconditions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520quantum%2520reinforcement%2520learning%2520%2528QRL%2529%250Acontroller%2520that%2520dynamically%2520adjusts%2520CFG%2520at%2520each%2520denoising%2520step.%2520The%2520controller%250Aadopts%2520a%2520hybrid%2520quantum--classical%2520actor--critic%2520architecture%253A%2520a%2520shallow%250Avariational%2520quantum%2520circuit%2520%2528VQC%2529%2520with%2520ring%2520entanglement%2520generates%2520policy%250Afeatures%252C%2520which%2520are%2520mapped%2520by%2520a%2520compact%2520multilayer%2520perceptron%2520%2528MLP%2529%2520into%250AGaussian%2520actions%2520over%2520%2524%255CDelta%2524CFG%252C%2520while%2520a%2520classical%2520critic%2520estimates%2520value%250Afunctions.%2520The%2520policy%2520is%2520optimized%2520using%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%250Awith%2520Generalized%2520Advantage%2520Estimation%2520%2528GAE%2529%252C%2520guided%2520by%2520a%2520reward%2520that%2520balances%250Aclassification%2520confidence%252C%2520perceptual%2520improvement%252C%2520and%2520action%2520regularization.%250AExperiments%2520on%2520CIFAR-10%2520demonstrate%2520that%2520our%2520QRL%2520policy%2520improves%2520perceptual%250Aquality%2520%2528LPIPS%252C%2520PSNR%252C%2520SSIM%2529%2520while%2520reducing%2520parameter%2520count%2520compared%2520to%250Aclassical%2520RL%2520actors%2520and%2520fixed%2520schedules.%2520Ablation%2520studies%2520on%2520qubit%2520number%2520and%250Acircuit%2520depth%2520reveal%2520trade-offs%2520between%2520accuracy%2520and%2520efficiency%252C%2520and%2520extended%250Aevaluations%2520confirm%2520robust%2520generation%2520under%2520long%2520diffusion%2520schedules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Reinforcement%20Learning-Guided%20Diffusion%20Model%20for%20Image%0A%20%20Synthesis%20via%20Hybrid%20Quantum-Classical%20Generative%20Model%20Architectures&entry.906535625=Chi-Sheng%20Chen%20and%20En-Jui%20Kuo&entry.1292438233=%20%20Diffusion%20models%20typically%20employ%20static%20or%20heuristic%20classifier-free%0Aguidance%20%28CFG%29%20schedules%2C%20which%20often%20fail%20to%20adapt%20across%20timesteps%20and%20noise%0Aconditions.%20In%20this%20work%2C%20we%20introduce%20a%20quantum%20reinforcement%20learning%20%28QRL%29%0Acontroller%20that%20dynamically%20adjusts%20CFG%20at%20each%20denoising%20step.%20The%20controller%0Aadopts%20a%20hybrid%20quantum--classical%20actor--critic%20architecture%3A%20a%20shallow%0Avariational%20quantum%20circuit%20%28VQC%29%20with%20ring%20entanglement%20generates%20policy%0Afeatures%2C%20which%20are%20mapped%20by%20a%20compact%20multilayer%20perceptron%20%28MLP%29%20into%0AGaussian%20actions%20over%20%24%5CDelta%24CFG%2C%20while%20a%20classical%20critic%20estimates%20value%0Afunctions.%20The%20policy%20is%20optimized%20using%20Proximal%20Policy%20Optimization%20%28PPO%29%0Awith%20Generalized%20Advantage%20Estimation%20%28GAE%29%2C%20guided%20by%20a%20reward%20that%20balances%0Aclassification%20confidence%2C%20perceptual%20improvement%2C%20and%20action%20regularization.%0AExperiments%20on%20CIFAR-10%20demonstrate%20that%20our%20QRL%20policy%20improves%20perceptual%0Aquality%20%28LPIPS%2C%20PSNR%2C%20SSIM%29%20while%20reducing%20parameter%20count%20compared%20to%0Aclassical%20RL%20actors%20and%20fixed%20schedules.%20Ablation%20studies%20on%20qubit%20number%20and%0Acircuit%20depth%20reveal%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20and%20extended%0Aevaluations%20confirm%20robust%20generation%20under%20long%20diffusion%20schedules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14163v1&entry.124074799=Read"},
{"title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in\n  Hindi-English Code-Mixing", "author": "Rajvee Sheth and Himanshu Beniwal and Mayank Singh", "abstract": "  We introduce COMI-LINGUA, the largest manually annotated Hindi-English\ncode-mixed dataset, comprising 125K+ high-quality instances across five core\nNLP tasks: Matrix Language Identification, Token-level Language Identification,\nPart-Of-Speech Tagging, Named Entity Recognition, and Machine Translation. Each\ninstance is annotated by three bilingual annotators, yielding over 376K expert\nannotations with strong inter-annotator agreement (Fleiss' Kappa $\\geq$ 0.81).\nThe rigorously preprocessed and filtered dataset covers both Devanagari and\nRoman scripts and spans diverse domains, ensuring real-world linguistic\ncoverage. Evaluation reveals that closed-source LLMs significantly outperform\ntraditional tools and open-source models in zero-shot settings. Notably,\none-shot prompting consistently boosts performance across tasks, especially in\nstructure-sensitive predictions like POS and NER. Fine-tuning state-of-the-art\nLLMs on COMI-LINGUA demonstrates substantial improvements, achieving up to\n95.25 F1 in NER, 98.77 F1 in MLI, and competitive MT performance, setting new\nbenchmarks for Hinglish code-mixed text. COMI-LINGUA is publicly available at\nthis URL: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.\n", "link": "http://arxiv.org/abs/2503.21670v3", "date": "2025-09-17", "relevancy": 1.7493, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4456}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMI-LINGUA%3A%20Expert%20Annotated%20Large-Scale%20Dataset%20for%20Multitask%20NLP%20in%0A%20%20Hindi-English%20Code-Mixing&body=Title%3A%20COMI-LINGUA%3A%20Expert%20Annotated%20Large-Scale%20Dataset%20for%20Multitask%20NLP%20in%0A%20%20Hindi-English%20Code-Mixing%0AAuthor%3A%20Rajvee%20Sheth%20and%20Himanshu%20Beniwal%20and%20Mayank%20Singh%0AAbstract%3A%20%20%20We%20introduce%20COMI-LINGUA%2C%20the%20largest%20manually%20annotated%20Hindi-English%0Acode-mixed%20dataset%2C%20comprising%20125K%2B%20high-quality%20instances%20across%20five%20core%0ANLP%20tasks%3A%20Matrix%20Language%20Identification%2C%20Token-level%20Language%20Identification%2C%0APart-Of-Speech%20Tagging%2C%20Named%20Entity%20Recognition%2C%20and%20Machine%20Translation.%20Each%0Ainstance%20is%20annotated%20by%20three%20bilingual%20annotators%2C%20yielding%20over%20376K%20expert%0Aannotations%20with%20strong%20inter-annotator%20agreement%20%28Fleiss%27%20Kappa%20%24%5Cgeq%24%200.81%29.%0AThe%20rigorously%20preprocessed%20and%20filtered%20dataset%20covers%20both%20Devanagari%20and%0ARoman%20scripts%20and%20spans%20diverse%20domains%2C%20ensuring%20real-world%20linguistic%0Acoverage.%20Evaluation%20reveals%20that%20closed-source%20LLMs%20significantly%20outperform%0Atraditional%20tools%20and%20open-source%20models%20in%20zero-shot%20settings.%20Notably%2C%0Aone-shot%20prompting%20consistently%20boosts%20performance%20across%20tasks%2C%20especially%20in%0Astructure-sensitive%20predictions%20like%20POS%20and%20NER.%20Fine-tuning%20state-of-the-art%0ALLMs%20on%20COMI-LINGUA%20demonstrates%20substantial%20improvements%2C%20achieving%20up%20to%0A95.25%20F1%20in%20NER%2C%2098.77%20F1%20in%20MLI%2C%20and%20competitive%20MT%20performance%2C%20setting%20new%0Abenchmarks%20for%20Hinglish%20code-mixed%20text.%20COMI-LINGUA%20is%20publicly%20available%20at%0Athis%20URL%3A%20https%3A//huggingface.co/datasets/LingoIITGN/COMI-LINGUA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMI-LINGUA%253A%2520Expert%2520Annotated%2520Large-Scale%2520Dataset%2520for%2520Multitask%2520NLP%2520in%250A%2520%2520Hindi-English%2520Code-Mixing%26entry.906535625%3DRajvee%2520Sheth%2520and%2520Himanshu%2520Beniwal%2520and%2520Mayank%2520Singh%26entry.1292438233%3D%2520%2520We%2520introduce%2520COMI-LINGUA%252C%2520the%2520largest%2520manually%2520annotated%2520Hindi-English%250Acode-mixed%2520dataset%252C%2520comprising%2520125K%252B%2520high-quality%2520instances%2520across%2520five%2520core%250ANLP%2520tasks%253A%2520Matrix%2520Language%2520Identification%252C%2520Token-level%2520Language%2520Identification%252C%250APart-Of-Speech%2520Tagging%252C%2520Named%2520Entity%2520Recognition%252C%2520and%2520Machine%2520Translation.%2520Each%250Ainstance%2520is%2520annotated%2520by%2520three%2520bilingual%2520annotators%252C%2520yielding%2520over%2520376K%2520expert%250Aannotations%2520with%2520strong%2520inter-annotator%2520agreement%2520%2528Fleiss%2527%2520Kappa%2520%2524%255Cgeq%2524%25200.81%2529.%250AThe%2520rigorously%2520preprocessed%2520and%2520filtered%2520dataset%2520covers%2520both%2520Devanagari%2520and%250ARoman%2520scripts%2520and%2520spans%2520diverse%2520domains%252C%2520ensuring%2520real-world%2520linguistic%250Acoverage.%2520Evaluation%2520reveals%2520that%2520closed-source%2520LLMs%2520significantly%2520outperform%250Atraditional%2520tools%2520and%2520open-source%2520models%2520in%2520zero-shot%2520settings.%2520Notably%252C%250Aone-shot%2520prompting%2520consistently%2520boosts%2520performance%2520across%2520tasks%252C%2520especially%2520in%250Astructure-sensitive%2520predictions%2520like%2520POS%2520and%2520NER.%2520Fine-tuning%2520state-of-the-art%250ALLMs%2520on%2520COMI-LINGUA%2520demonstrates%2520substantial%2520improvements%252C%2520achieving%2520up%2520to%250A95.25%2520F1%2520in%2520NER%252C%252098.77%2520F1%2520in%2520MLI%252C%2520and%2520competitive%2520MT%2520performance%252C%2520setting%2520new%250Abenchmarks%2520for%2520Hinglish%2520code-mixed%2520text.%2520COMI-LINGUA%2520is%2520publicly%2520available%2520at%250Athis%2520URL%253A%2520https%253A//huggingface.co/datasets/LingoIITGN/COMI-LINGUA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMI-LINGUA%3A%20Expert%20Annotated%20Large-Scale%20Dataset%20for%20Multitask%20NLP%20in%0A%20%20Hindi-English%20Code-Mixing&entry.906535625=Rajvee%20Sheth%20and%20Himanshu%20Beniwal%20and%20Mayank%20Singh&entry.1292438233=%20%20We%20introduce%20COMI-LINGUA%2C%20the%20largest%20manually%20annotated%20Hindi-English%0Acode-mixed%20dataset%2C%20comprising%20125K%2B%20high-quality%20instances%20across%20five%20core%0ANLP%20tasks%3A%20Matrix%20Language%20Identification%2C%20Token-level%20Language%20Identification%2C%0APart-Of-Speech%20Tagging%2C%20Named%20Entity%20Recognition%2C%20and%20Machine%20Translation.%20Each%0Ainstance%20is%20annotated%20by%20three%20bilingual%20annotators%2C%20yielding%20over%20376K%20expert%0Aannotations%20with%20strong%20inter-annotator%20agreement%20%28Fleiss%27%20Kappa%20%24%5Cgeq%24%200.81%29.%0AThe%20rigorously%20preprocessed%20and%20filtered%20dataset%20covers%20both%20Devanagari%20and%0ARoman%20scripts%20and%20spans%20diverse%20domains%2C%20ensuring%20real-world%20linguistic%0Acoverage.%20Evaluation%20reveals%20that%20closed-source%20LLMs%20significantly%20outperform%0Atraditional%20tools%20and%20open-source%20models%20in%20zero-shot%20settings.%20Notably%2C%0Aone-shot%20prompting%20consistently%20boosts%20performance%20across%20tasks%2C%20especially%20in%0Astructure-sensitive%20predictions%20like%20POS%20and%20NER.%20Fine-tuning%20state-of-the-art%0ALLMs%20on%20COMI-LINGUA%20demonstrates%20substantial%20improvements%2C%20achieving%20up%20to%0A95.25%20F1%20in%20NER%2C%2098.77%20F1%20in%20MLI%2C%20and%20competitive%20MT%20performance%2C%20setting%20new%0Abenchmarks%20for%20Hinglish%20code-mixed%20text.%20COMI-LINGUA%20is%20publicly%20available%20at%0Athis%20URL%3A%20https%3A//huggingface.co/datasets/LingoIITGN/COMI-LINGUA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21670v3&entry.124074799=Read"},
{"title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with\n  Texture-Geometry Dual Guidance", "author": "Zefan Qu and Zhenwei Wang and Haoyuan Wang and Ke Xu and Gerhard Hancke and Rynson W. H. Lau", "abstract": "  Creating 3D assets that follow the texture and geometry style of existing\nones is often desirable or even inevitable in practical applications like video\ngaming and virtual reality. While impressive progress has been made in\ngenerating 3D objects from text or images, creating style-controllable 3D\nassets remains a complex and challenging problem. In this work, we propose\nStyleSculptor, a novel training-free approach for generating style-guided 3D\nassets from a content image and one or more style images. Unlike previous\nworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,\nenabling fine-grained 3D style control that captures the texture, geometry, or\nboth styles of user-provided style images. At the core of StyleSculptor is a\nnovel Style Disentangled Attention (SD-Attn) module, which establishes a\ndynamic interaction between the input content image and style image for\nstyle-guided 3D asset generation via a cross-3D attention mechanism, enabling\nstable feature fusion and effective style-guided generation. To alleviate\nsemantic content leakage, we also introduce a style-disentangled feature\nselection strategy within the SD-Attn module, which leverages the variance of\n3D feature patches to disentangle style- and content-significant channels,\nallowing selective feature injection within the attention framework. With\nSD-Attn, the network can dynamically compute texture-, geometry-, or\nboth-guided features to steer the 3D generation process. Built upon this, we\nfurther propose the Style Guided Control (SGC) mechanism, which enables\nexclusive geometry- or texture-only stylization, as well as adjustable style\nintensity control. Extensive experiments demonstrate that StyleSculptor\noutperforms existing baseline methods in producing high-fidelity 3D assets.\n", "link": "http://arxiv.org/abs/2509.13301v2", "date": "2025-09-17", "relevancy": 1.7302, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5795}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5759}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance&body=Title%3A%20StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance%0AAuthor%3A%20Zefan%20Qu%20and%20Zhenwei%20Wang%20and%20Haoyuan%20Wang%20and%20Ke%20Xu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Creating%203D%20assets%20that%20follow%20the%20texture%20and%20geometry%20style%20of%20existing%0Aones%20is%20often%20desirable%20or%20even%20inevitable%20in%20practical%20applications%20like%20video%0Agaming%20and%20virtual%20reality.%20While%20impressive%20progress%20has%20been%20made%20in%0Agenerating%203D%20objects%20from%20text%20or%20images%2C%20creating%20style-controllable%203D%0Aassets%20remains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20propose%0AStyleSculptor%2C%20a%20novel%20training-free%20approach%20for%20generating%20style-guided%203D%0Aassets%20from%20a%20content%20image%20and%20one%20or%20more%20style%20images.%20Unlike%20previous%0Aworks%2C%20StyleSculptor%20achieves%20style-guided%203D%20generation%20in%20a%20zero-shot%20manner%2C%0Aenabling%20fine-grained%203D%20style%20control%20that%20captures%20the%20texture%2C%20geometry%2C%20or%0Aboth%20styles%20of%20user-provided%20style%20images.%20At%20the%20core%20of%20StyleSculptor%20is%20a%0Anovel%20Style%20Disentangled%20Attention%20%28SD-Attn%29%20module%2C%20which%20establishes%20a%0Adynamic%20interaction%20between%20the%20input%20content%20image%20and%20style%20image%20for%0Astyle-guided%203D%20asset%20generation%20via%20a%20cross-3D%20attention%20mechanism%2C%20enabling%0Astable%20feature%20fusion%20and%20effective%20style-guided%20generation.%20To%20alleviate%0Asemantic%20content%20leakage%2C%20we%20also%20introduce%20a%20style-disentangled%20feature%0Aselection%20strategy%20within%20the%20SD-Attn%20module%2C%20which%20leverages%20the%20variance%20of%0A3D%20feature%20patches%20to%20disentangle%20style-%20and%20content-significant%20channels%2C%0Aallowing%20selective%20feature%20injection%20within%20the%20attention%20framework.%20With%0ASD-Attn%2C%20the%20network%20can%20dynamically%20compute%20texture-%2C%20geometry-%2C%20or%0Aboth-guided%20features%20to%20steer%20the%203D%20generation%20process.%20Built%20upon%20this%2C%20we%0Afurther%20propose%20the%20Style%20Guided%20Control%20%28SGC%29%20mechanism%2C%20which%20enables%0Aexclusive%20geometry-%20or%20texture-only%20stylization%2C%20as%20well%20as%20adjustable%20style%0Aintensity%20control.%20Extensive%20experiments%20demonstrate%20that%20StyleSculptor%0Aoutperforms%20existing%20baseline%20methods%20in%20producing%20high-fidelity%203D%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleSculptor%253A%2520Zero-Shot%2520Style-Controllable%25203D%2520Asset%2520Generation%2520with%250A%2520%2520Texture-Geometry%2520Dual%2520Guidance%26entry.906535625%3DZefan%2520Qu%2520and%2520Zhenwei%2520Wang%2520and%2520Haoyuan%2520Wang%2520and%2520Ke%2520Xu%2520and%2520Gerhard%2520Hancke%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Creating%25203D%2520assets%2520that%2520follow%2520the%2520texture%2520and%2520geometry%2520style%2520of%2520existing%250Aones%2520is%2520often%2520desirable%2520or%2520even%2520inevitable%2520in%2520practical%2520applications%2520like%2520video%250Agaming%2520and%2520virtual%2520reality.%2520While%2520impressive%2520progress%2520has%2520been%2520made%2520in%250Agenerating%25203D%2520objects%2520from%2520text%2520or%2520images%252C%2520creating%2520style-controllable%25203D%250Aassets%2520remains%2520a%2520complex%2520and%2520challenging%2520problem.%2520In%2520this%2520work%252C%2520we%2520propose%250AStyleSculptor%252C%2520a%2520novel%2520training-free%2520approach%2520for%2520generating%2520style-guided%25203D%250Aassets%2520from%2520a%2520content%2520image%2520and%2520one%2520or%2520more%2520style%2520images.%2520Unlike%2520previous%250Aworks%252C%2520StyleSculptor%2520achieves%2520style-guided%25203D%2520generation%2520in%2520a%2520zero-shot%2520manner%252C%250Aenabling%2520fine-grained%25203D%2520style%2520control%2520that%2520captures%2520the%2520texture%252C%2520geometry%252C%2520or%250Aboth%2520styles%2520of%2520user-provided%2520style%2520images.%2520At%2520the%2520core%2520of%2520StyleSculptor%2520is%2520a%250Anovel%2520Style%2520Disentangled%2520Attention%2520%2528SD-Attn%2529%2520module%252C%2520which%2520establishes%2520a%250Adynamic%2520interaction%2520between%2520the%2520input%2520content%2520image%2520and%2520style%2520image%2520for%250Astyle-guided%25203D%2520asset%2520generation%2520via%2520a%2520cross-3D%2520attention%2520mechanism%252C%2520enabling%250Astable%2520feature%2520fusion%2520and%2520effective%2520style-guided%2520generation.%2520To%2520alleviate%250Asemantic%2520content%2520leakage%252C%2520we%2520also%2520introduce%2520a%2520style-disentangled%2520feature%250Aselection%2520strategy%2520within%2520the%2520SD-Attn%2520module%252C%2520which%2520leverages%2520the%2520variance%2520of%250A3D%2520feature%2520patches%2520to%2520disentangle%2520style-%2520and%2520content-significant%2520channels%252C%250Aallowing%2520selective%2520feature%2520injection%2520within%2520the%2520attention%2520framework.%2520With%250ASD-Attn%252C%2520the%2520network%2520can%2520dynamically%2520compute%2520texture-%252C%2520geometry-%252C%2520or%250Aboth-guided%2520features%2520to%2520steer%2520the%25203D%2520generation%2520process.%2520Built%2520upon%2520this%252C%2520we%250Afurther%2520propose%2520the%2520Style%2520Guided%2520Control%2520%2528SGC%2529%2520mechanism%252C%2520which%2520enables%250Aexclusive%2520geometry-%2520or%2520texture-only%2520stylization%252C%2520as%2520well%2520as%2520adjustable%2520style%250Aintensity%2520control.%2520Extensive%2520experiments%2520demonstrate%2520that%2520StyleSculptor%250Aoutperforms%2520existing%2520baseline%2520methods%2520in%2520producing%2520high-fidelity%25203D%2520assets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance&entry.906535625=Zefan%20Qu%20and%20Zhenwei%20Wang%20and%20Haoyuan%20Wang%20and%20Ke%20Xu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Creating%203D%20assets%20that%20follow%20the%20texture%20and%20geometry%20style%20of%20existing%0Aones%20is%20often%20desirable%20or%20even%20inevitable%20in%20practical%20applications%20like%20video%0Agaming%20and%20virtual%20reality.%20While%20impressive%20progress%20has%20been%20made%20in%0Agenerating%203D%20objects%20from%20text%20or%20images%2C%20creating%20style-controllable%203D%0Aassets%20remains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20propose%0AStyleSculptor%2C%20a%20novel%20training-free%20approach%20for%20generating%20style-guided%203D%0Aassets%20from%20a%20content%20image%20and%20one%20or%20more%20style%20images.%20Unlike%20previous%0Aworks%2C%20StyleSculptor%20achieves%20style-guided%203D%20generation%20in%20a%20zero-shot%20manner%2C%0Aenabling%20fine-grained%203D%20style%20control%20that%20captures%20the%20texture%2C%20geometry%2C%20or%0Aboth%20styles%20of%20user-provided%20style%20images.%20At%20the%20core%20of%20StyleSculptor%20is%20a%0Anovel%20Style%20Disentangled%20Attention%20%28SD-Attn%29%20module%2C%20which%20establishes%20a%0Adynamic%20interaction%20between%20the%20input%20content%20image%20and%20style%20image%20for%0Astyle-guided%203D%20asset%20generation%20via%20a%20cross-3D%20attention%20mechanism%2C%20enabling%0Astable%20feature%20fusion%20and%20effective%20style-guided%20generation.%20To%20alleviate%0Asemantic%20content%20leakage%2C%20we%20also%20introduce%20a%20style-disentangled%20feature%0Aselection%20strategy%20within%20the%20SD-Attn%20module%2C%20which%20leverages%20the%20variance%20of%0A3D%20feature%20patches%20to%20disentangle%20style-%20and%20content-significant%20channels%2C%0Aallowing%20selective%20feature%20injection%20within%20the%20attention%20framework.%20With%0ASD-Attn%2C%20the%20network%20can%20dynamically%20compute%20texture-%2C%20geometry-%2C%20or%0Aboth-guided%20features%20to%20steer%20the%203D%20generation%20process.%20Built%20upon%20this%2C%20we%0Afurther%20propose%20the%20Style%20Guided%20Control%20%28SGC%29%20mechanism%2C%20which%20enables%0Aexclusive%20geometry-%20or%20texture-only%20stylization%2C%20as%20well%20as%20adjustable%20style%0Aintensity%20control.%20Extensive%20experiments%20demonstrate%20that%20StyleSculptor%0Aoutperforms%20existing%20baseline%20methods%20in%20producing%20high-fidelity%203D%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13301v2&entry.124074799=Read"},
{"title": "Well-Conditioned Polynomial Representations for Mathematical Handwriting\n  Recognition", "author": "Robert M. Corless and Deepak Singh Kalhan and Stephen M. Watt", "abstract": "  Previous work has made use of a parameterized plane curve polynomial\nrepresentation for mathematical handwriting, with the polynomials represented\nin a Legendre or Legendre-Sobolev graded basis. This provides a compact\ngeometric representation for the digital ink. Preliminary results have also\nbeen shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the\ntrade-offs between basis choice and polynomial degree to achieve accurate\nmodeling with a low computational cost. To do this, we consider the condition\nnumber for polynomial evaluation in these bases and bound how the various inner\nproducts give norms for the variations between symbols.\n", "link": "http://arxiv.org/abs/2509.10815v2", "date": "2025-09-17", "relevancy": 1.7297, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4338}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4321}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Well-Conditioned%20Polynomial%20Representations%20for%20Mathematical%20Handwriting%0A%20%20Recognition&body=Title%3A%20Well-Conditioned%20Polynomial%20Representations%20for%20Mathematical%20Handwriting%0A%20%20Recognition%0AAuthor%3A%20Robert%20M.%20Corless%20and%20Deepak%20Singh%20Kalhan%20and%20Stephen%20M.%20Watt%0AAbstract%3A%20%20%20Previous%20work%20has%20made%20use%20of%20a%20parameterized%20plane%20curve%20polynomial%0Arepresentation%20for%20mathematical%20handwriting%2C%20with%20the%20polynomials%20represented%0Ain%20a%20Legendre%20or%20Legendre-Sobolev%20graded%20basis.%20This%20provides%20a%20compact%0Ageometric%20representation%20for%20the%20digital%20ink.%20Preliminary%20results%20have%20also%0Abeen%20shown%20for%20Chebyshev%20and%20Chebyshev-Sobolev%20bases.%20This%20article%20explores%20the%0Atrade-offs%20between%20basis%20choice%20and%20polynomial%20degree%20to%20achieve%20accurate%0Amodeling%20with%20a%20low%20computational%20cost.%20To%20do%20this%2C%20we%20consider%20the%20condition%0Anumber%20for%20polynomial%20evaluation%20in%20these%20bases%20and%20bound%20how%20the%20various%20inner%0Aproducts%20give%20norms%20for%20the%20variations%20between%20symbols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWell-Conditioned%2520Polynomial%2520Representations%2520for%2520Mathematical%2520Handwriting%250A%2520%2520Recognition%26entry.906535625%3DRobert%2520M.%2520Corless%2520and%2520Deepak%2520Singh%2520Kalhan%2520and%2520Stephen%2520M.%2520Watt%26entry.1292438233%3D%2520%2520Previous%2520work%2520has%2520made%2520use%2520of%2520a%2520parameterized%2520plane%2520curve%2520polynomial%250Arepresentation%2520for%2520mathematical%2520handwriting%252C%2520with%2520the%2520polynomials%2520represented%250Ain%2520a%2520Legendre%2520or%2520Legendre-Sobolev%2520graded%2520basis.%2520This%2520provides%2520a%2520compact%250Ageometric%2520representation%2520for%2520the%2520digital%2520ink.%2520Preliminary%2520results%2520have%2520also%250Abeen%2520shown%2520for%2520Chebyshev%2520and%2520Chebyshev-Sobolev%2520bases.%2520This%2520article%2520explores%2520the%250Atrade-offs%2520between%2520basis%2520choice%2520and%2520polynomial%2520degree%2520to%2520achieve%2520accurate%250Amodeling%2520with%2520a%2520low%2520computational%2520cost.%2520To%2520do%2520this%252C%2520we%2520consider%2520the%2520condition%250Anumber%2520for%2520polynomial%2520evaluation%2520in%2520these%2520bases%2520and%2520bound%2520how%2520the%2520various%2520inner%250Aproducts%2520give%2520norms%2520for%2520the%2520variations%2520between%2520symbols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Well-Conditioned%20Polynomial%20Representations%20for%20Mathematical%20Handwriting%0A%20%20Recognition&entry.906535625=Robert%20M.%20Corless%20and%20Deepak%20Singh%20Kalhan%20and%20Stephen%20M.%20Watt&entry.1292438233=%20%20Previous%20work%20has%20made%20use%20of%20a%20parameterized%20plane%20curve%20polynomial%0Arepresentation%20for%20mathematical%20handwriting%2C%20with%20the%20polynomials%20represented%0Ain%20a%20Legendre%20or%20Legendre-Sobolev%20graded%20basis.%20This%20provides%20a%20compact%0Ageometric%20representation%20for%20the%20digital%20ink.%20Preliminary%20results%20have%20also%0Abeen%20shown%20for%20Chebyshev%20and%20Chebyshev-Sobolev%20bases.%20This%20article%20explores%20the%0Atrade-offs%20between%20basis%20choice%20and%20polynomial%20degree%20to%20achieve%20accurate%0Amodeling%20with%20a%20low%20computational%20cost.%20To%20do%20this%2C%20we%20consider%20the%20condition%0Anumber%20for%20polynomial%20evaluation%20in%20these%20bases%20and%20bound%20how%20the%20various%20inner%0Aproducts%20give%20norms%20for%20the%20variations%20between%20symbols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10815v2&entry.124074799=Read"},
{"title": "Multi-robot Multi-source Localization in Complex Flows with\n  Physics-Preserving Environment Models", "author": "Benjamin Shaffer and Victoria Edwards and Brooks Kinch and Nathaniel Trask and M. Ani Hsieh", "abstract": "  Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.\n", "link": "http://arxiv.org/abs/2509.14228v1", "date": "2025-09-17", "relevancy": 1.7169, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6152}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.574}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-robot%20Multi-source%20Localization%20in%20Complex%20Flows%20with%0A%20%20Physics-Preserving%20Environment%20Models&body=Title%3A%20Multi-robot%20Multi-source%20Localization%20in%20Complex%20Flows%20with%0A%20%20Physics-Preserving%20Environment%20Models%0AAuthor%3A%20Benjamin%20Shaffer%20and%20Victoria%20Edwards%20and%20Brooks%20Kinch%20and%20Nathaniel%20Trask%20and%20M.%20Ani%20Hsieh%0AAbstract%3A%20%20%20Source%20localization%20in%20a%20complex%20flow%20poses%20a%20significant%20challenge%20for%0Amulti-robot%20teams%20tasked%20with%20localizing%20the%20source%20of%20chemical%20leaks%20or%0Atracking%20the%20dispersion%20of%20an%20oil%20spill.%20The%20flow%20dynamics%20can%20be%20time-varying%0Aand%20chaotic%2C%20resulting%20in%20sporadic%20and%20intermittent%20sensor%20readings%2C%20and%0Acomplex%20environmental%20geometries%20further%20complicate%20a%20team%27s%20ability%20to%20model%0Aand%20predict%20the%20dispersion.%20To%20accurately%20account%20for%20the%20physical%20processes%0Athat%20drive%20the%20dispersion%20dynamics%2C%20robots%20must%20have%20access%20to%20computationally%0Aintensive%20numerical%20models%2C%20which%20can%20be%20difficult%20when%20onboard%20computation%20is%0Alimited.%20We%20present%20a%20distributed%20mobile%20sensing%20framework%20for%20source%0Alocalization%20in%20which%20each%20robot%20carries%20a%20machine-learned%2C%20finite%20element%0Amodel%20of%20its%20environment%20to%20guide%20information-based%20sampling.%20The%20models%20are%0Aused%20to%20evaluate%20an%20approximate%20mutual%20information%20criterion%20to%20drive%20an%0Ainfotaxis%20control%20strategy%2C%20which%20selects%20sensing%20regions%20that%20are%20expected%20to%0Amaximize%20informativeness%20for%20the%20source%20localization%20objective.%20Our%20approach%0Aachieves%20faster%20error%20reduction%20compared%20to%20baseline%20sensing%20strategies%20and%0Aresults%20in%20more%20accurate%20source%20localization%20compared%20to%20baseline%20machine%0Alearning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-robot%2520Multi-source%2520Localization%2520in%2520Complex%2520Flows%2520with%250A%2520%2520Physics-Preserving%2520Environment%2520Models%26entry.906535625%3DBenjamin%2520Shaffer%2520and%2520Victoria%2520Edwards%2520and%2520Brooks%2520Kinch%2520and%2520Nathaniel%2520Trask%2520and%2520M.%2520Ani%2520Hsieh%26entry.1292438233%3D%2520%2520Source%2520localization%2520in%2520a%2520complex%2520flow%2520poses%2520a%2520significant%2520challenge%2520for%250Amulti-robot%2520teams%2520tasked%2520with%2520localizing%2520the%2520source%2520of%2520chemical%2520leaks%2520or%250Atracking%2520the%2520dispersion%2520of%2520an%2520oil%2520spill.%2520The%2520flow%2520dynamics%2520can%2520be%2520time-varying%250Aand%2520chaotic%252C%2520resulting%2520in%2520sporadic%2520and%2520intermittent%2520sensor%2520readings%252C%2520and%250Acomplex%2520environmental%2520geometries%2520further%2520complicate%2520a%2520team%2527s%2520ability%2520to%2520model%250Aand%2520predict%2520the%2520dispersion.%2520To%2520accurately%2520account%2520for%2520the%2520physical%2520processes%250Athat%2520drive%2520the%2520dispersion%2520dynamics%252C%2520robots%2520must%2520have%2520access%2520to%2520computationally%250Aintensive%2520numerical%2520models%252C%2520which%2520can%2520be%2520difficult%2520when%2520onboard%2520computation%2520is%250Alimited.%2520We%2520present%2520a%2520distributed%2520mobile%2520sensing%2520framework%2520for%2520source%250Alocalization%2520in%2520which%2520each%2520robot%2520carries%2520a%2520machine-learned%252C%2520finite%2520element%250Amodel%2520of%2520its%2520environment%2520to%2520guide%2520information-based%2520sampling.%2520The%2520models%2520are%250Aused%2520to%2520evaluate%2520an%2520approximate%2520mutual%2520information%2520criterion%2520to%2520drive%2520an%250Ainfotaxis%2520control%2520strategy%252C%2520which%2520selects%2520sensing%2520regions%2520that%2520are%2520expected%2520to%250Amaximize%2520informativeness%2520for%2520the%2520source%2520localization%2520objective.%2520Our%2520approach%250Aachieves%2520faster%2520error%2520reduction%2520compared%2520to%2520baseline%2520sensing%2520strategies%2520and%250Aresults%2520in%2520more%2520accurate%2520source%2520localization%2520compared%2520to%2520baseline%2520machine%250Alearning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-robot%20Multi-source%20Localization%20in%20Complex%20Flows%20with%0A%20%20Physics-Preserving%20Environment%20Models&entry.906535625=Benjamin%20Shaffer%20and%20Victoria%20Edwards%20and%20Brooks%20Kinch%20and%20Nathaniel%20Trask%20and%20M.%20Ani%20Hsieh&entry.1292438233=%20%20Source%20localization%20in%20a%20complex%20flow%20poses%20a%20significant%20challenge%20for%0Amulti-robot%20teams%20tasked%20with%20localizing%20the%20source%20of%20chemical%20leaks%20or%0Atracking%20the%20dispersion%20of%20an%20oil%20spill.%20The%20flow%20dynamics%20can%20be%20time-varying%0Aand%20chaotic%2C%20resulting%20in%20sporadic%20and%20intermittent%20sensor%20readings%2C%20and%0Acomplex%20environmental%20geometries%20further%20complicate%20a%20team%27s%20ability%20to%20model%0Aand%20predict%20the%20dispersion.%20To%20accurately%20account%20for%20the%20physical%20processes%0Athat%20drive%20the%20dispersion%20dynamics%2C%20robots%20must%20have%20access%20to%20computationally%0Aintensive%20numerical%20models%2C%20which%20can%20be%20difficult%20when%20onboard%20computation%20is%0Alimited.%20We%20present%20a%20distributed%20mobile%20sensing%20framework%20for%20source%0Alocalization%20in%20which%20each%20robot%20carries%20a%20machine-learned%2C%20finite%20element%0Amodel%20of%20its%20environment%20to%20guide%20information-based%20sampling.%20The%20models%20are%0Aused%20to%20evaluate%20an%20approximate%20mutual%20information%20criterion%20to%20drive%20an%0Ainfotaxis%20control%20strategy%2C%20which%20selects%20sensing%20regions%20that%20are%20expected%20to%0Amaximize%20informativeness%20for%20the%20source%20localization%20objective.%20Our%20approach%0Aachieves%20faster%20error%20reduction%20compared%20to%20baseline%20sensing%20strategies%20and%0Aresults%20in%20more%20accurate%20source%20localization%20compared%20to%20baseline%20machine%0Alearning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14228v1&entry.124074799=Read"},
{"title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by\n  Visual Semantic Enhancement", "author": "Jun Du and Weiwei Xing and Ming Li and Fei Richard Yu", "abstract": "  Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.\n", "link": "http://arxiv.org/abs/2509.14060v1", "date": "2025-09-17", "relevancy": 1.6966, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5885}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VSE-MOT%3A%20Multi-Object%20Tracking%20in%20Low-Quality%20Video%20Scenes%20Guided%20by%0A%20%20Visual%20Semantic%20Enhancement&body=Title%3A%20VSE-MOT%3A%20Multi-Object%20Tracking%20in%20Low-Quality%20Video%20Scenes%20Guided%20by%0A%20%20Visual%20Semantic%20Enhancement%0AAuthor%3A%20Jun%20Du%20and%20Weiwei%20Xing%20and%20Ming%20Li%20and%20Fei%20Richard%20Yu%0AAbstract%3A%20%20%20Current%20multi-object%20tracking%20%28MOT%29%20algorithms%20typically%20overlook%20issues%0Ainherent%20in%20low-quality%20videos%2C%20leading%20to%20significant%20degradation%20in%20tracking%0Aperformance%20when%20confronted%20with%20real-world%20image%20deterioration.%20Therefore%2C%0Aadvancing%20the%20application%20of%20MOT%20algorithms%20in%20real-world%20low-quality%20video%0Ascenarios%20represents%20a%20critical%20and%20meaningful%20endeavor.%20To%20address%20the%0Achallenges%20posed%20by%20low-quality%20scenarios%2C%20inspired%20by%20vision-language%20models%2C%0Athis%20paper%20proposes%20a%20Visual%20Semantic%20Enhancement-guided%20Multi-Object%20Tracking%0Aframework%20%28VSE-MOT%29.%20Specifically%2C%20we%20first%20design%20a%20tri-branch%20architecture%0Athat%20leverages%20a%20vision-language%20model%20to%20extract%20global%20visual%20semantic%0Ainformation%20from%20images%20and%20fuse%20it%20with%20query%20vectors.%20Subsequently%2C%20to%0Afurther%20enhance%20the%20utilization%20of%20visual%20semantic%20information%2C%20we%20introduce%0Athe%20Multi-Object%20Tracking%20Adapter%20%28MOT-Adapter%29%20and%20the%20Visual%20Semantic%20Fusion%0AModule%20%28VSFM%29.%20The%20MOT-Adapter%20adapts%20the%20extracted%20global%20visual%20semantic%0Ainformation%20to%20suit%20multi-object%20tracking%20tasks%2C%20while%20the%20VSFM%20improves%20the%0Aefficacy%20of%20feature%20fusion.%20Through%20extensive%20experiments%2C%20we%20validate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20method%20in%20real-world%20low-quality%0Avideo%20scenarios.%20Its%20tracking%20performance%20metrics%20outperform%20those%20of%20existing%0Amethods%20by%20approximately%208%25%20to%2020%25%2C%20while%20maintaining%20robust%20performance%20in%0Aconventional%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVSE-MOT%253A%2520Multi-Object%2520Tracking%2520in%2520Low-Quality%2520Video%2520Scenes%2520Guided%2520by%250A%2520%2520Visual%2520Semantic%2520Enhancement%26entry.906535625%3DJun%2520Du%2520and%2520Weiwei%2520Xing%2520and%2520Ming%2520Li%2520and%2520Fei%2520Richard%2520Yu%26entry.1292438233%3D%2520%2520Current%2520multi-object%2520tracking%2520%2528MOT%2529%2520algorithms%2520typically%2520overlook%2520issues%250Ainherent%2520in%2520low-quality%2520videos%252C%2520leading%2520to%2520significant%2520degradation%2520in%2520tracking%250Aperformance%2520when%2520confronted%2520with%2520real-world%2520image%2520deterioration.%2520Therefore%252C%250Aadvancing%2520the%2520application%2520of%2520MOT%2520algorithms%2520in%2520real-world%2520low-quality%2520video%250Ascenarios%2520represents%2520a%2520critical%2520and%2520meaningful%2520endeavor.%2520To%2520address%2520the%250Achallenges%2520posed%2520by%2520low-quality%2520scenarios%252C%2520inspired%2520by%2520vision-language%2520models%252C%250Athis%2520paper%2520proposes%2520a%2520Visual%2520Semantic%2520Enhancement-guided%2520Multi-Object%2520Tracking%250Aframework%2520%2528VSE-MOT%2529.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520tri-branch%2520architecture%250Athat%2520leverages%2520a%2520vision-language%2520model%2520to%2520extract%2520global%2520visual%2520semantic%250Ainformation%2520from%2520images%2520and%2520fuse%2520it%2520with%2520query%2520vectors.%2520Subsequently%252C%2520to%250Afurther%2520enhance%2520the%2520utilization%2520of%2520visual%2520semantic%2520information%252C%2520we%2520introduce%250Athe%2520Multi-Object%2520Tracking%2520Adapter%2520%2528MOT-Adapter%2529%2520and%2520the%2520Visual%2520Semantic%2520Fusion%250AModule%2520%2528VSFM%2529.%2520The%2520MOT-Adapter%2520adapts%2520the%2520extracted%2520global%2520visual%2520semantic%250Ainformation%2520to%2520suit%2520multi-object%2520tracking%2520tasks%252C%2520while%2520the%2520VSFM%2520improves%2520the%250Aefficacy%2520of%2520feature%2520fusion.%2520Through%2520extensive%2520experiments%252C%2520we%2520validate%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520method%2520in%2520real-world%2520low-quality%250Avideo%2520scenarios.%2520Its%2520tracking%2520performance%2520metrics%2520outperform%2520those%2520of%2520existing%250Amethods%2520by%2520approximately%25208%2525%2520to%252020%2525%252C%2520while%2520maintaining%2520robust%2520performance%2520in%250Aconventional%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VSE-MOT%3A%20Multi-Object%20Tracking%20in%20Low-Quality%20Video%20Scenes%20Guided%20by%0A%20%20Visual%20Semantic%20Enhancement&entry.906535625=Jun%20Du%20and%20Weiwei%20Xing%20and%20Ming%20Li%20and%20Fei%20Richard%20Yu&entry.1292438233=%20%20Current%20multi-object%20tracking%20%28MOT%29%20algorithms%20typically%20overlook%20issues%0Ainherent%20in%20low-quality%20videos%2C%20leading%20to%20significant%20degradation%20in%20tracking%0Aperformance%20when%20confronted%20with%20real-world%20image%20deterioration.%20Therefore%2C%0Aadvancing%20the%20application%20of%20MOT%20algorithms%20in%20real-world%20low-quality%20video%0Ascenarios%20represents%20a%20critical%20and%20meaningful%20endeavor.%20To%20address%20the%0Achallenges%20posed%20by%20low-quality%20scenarios%2C%20inspired%20by%20vision-language%20models%2C%0Athis%20paper%20proposes%20a%20Visual%20Semantic%20Enhancement-guided%20Multi-Object%20Tracking%0Aframework%20%28VSE-MOT%29.%20Specifically%2C%20we%20first%20design%20a%20tri-branch%20architecture%0Athat%20leverages%20a%20vision-language%20model%20to%20extract%20global%20visual%20semantic%0Ainformation%20from%20images%20and%20fuse%20it%20with%20query%20vectors.%20Subsequently%2C%20to%0Afurther%20enhance%20the%20utilization%20of%20visual%20semantic%20information%2C%20we%20introduce%0Athe%20Multi-Object%20Tracking%20Adapter%20%28MOT-Adapter%29%20and%20the%20Visual%20Semantic%20Fusion%0AModule%20%28VSFM%29.%20The%20MOT-Adapter%20adapts%20the%20extracted%20global%20visual%20semantic%0Ainformation%20to%20suit%20multi-object%20tracking%20tasks%2C%20while%20the%20VSFM%20improves%20the%0Aefficacy%20of%20feature%20fusion.%20Through%20extensive%20experiments%2C%20we%20validate%20the%0Aeffectiveness%20and%20superiority%20of%20the%20proposed%20method%20in%20real-world%20low-quality%0Avideo%20scenarios.%20Its%20tracking%20performance%20metrics%20outperform%20those%20of%20existing%0Amethods%20by%20approximately%208%25%20to%2020%25%2C%20while%20maintaining%20robust%20performance%20in%0Aconventional%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14060v1&entry.124074799=Read"},
{"title": "RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing", "author": "Liting Gao and Yi Yuan and Yaru Chen and Yuelan Cheng and Zhenbo Li and Juan Wen and Shubin Zhang and Wenwu Wang", "abstract": "  Diffusion models have shown remarkable progress in text-to-audio generation.\nHowever, text-guided audio editing remains in its early stages. This task\nfocuses on modifying the target content within an audio signal while preserving\nthe rest, thus demanding precise localization and faithful editing according to\nthe text prompt. Existing training-based and zero-shot methods that rely on\nfull-caption or costly optimization often struggle with complex editing or lack\npracticality. In this work, we propose a novel end-to-end efficient rectified\nflow matching-based diffusion framework for audio editing, and construct a\ndataset featuring overlapping multi-event audio to support training and\nbenchmarking in complex scenarios. Experiments show that our model achieves\nfaithful semantic alignment without requiring auxiliary captions or masks,\nwhile maintaining competitive editing quality across metrics.\n", "link": "http://arxiv.org/abs/2509.14003v1", "date": "2025-09-17", "relevancy": 1.6934, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6253}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5491}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RFM-Editing%3A%20Rectified%20Flow%20Matching%20for%20Text-guided%20Audio%20Editing&body=Title%3A%20RFM-Editing%3A%20Rectified%20Flow%20Matching%20for%20Text-guided%20Audio%20Editing%0AAuthor%3A%20Liting%20Gao%20and%20Yi%20Yuan%20and%20Yaru%20Chen%20and%20Yuelan%20Cheng%20and%20Zhenbo%20Li%20and%20Juan%20Wen%20and%20Shubin%20Zhang%20and%20Wenwu%20Wang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20remarkable%20progress%20in%20text-to-audio%20generation.%0AHowever%2C%20text-guided%20audio%20editing%20remains%20in%20its%20early%20stages.%20This%20task%0Afocuses%20on%20modifying%20the%20target%20content%20within%20an%20audio%20signal%20while%20preserving%0Athe%20rest%2C%20thus%20demanding%20precise%20localization%20and%20faithful%20editing%20according%20to%0Athe%20text%20prompt.%20Existing%20training-based%20and%20zero-shot%20methods%20that%20rely%20on%0Afull-caption%20or%20costly%20optimization%20often%20struggle%20with%20complex%20editing%20or%20lack%0Apracticality.%20In%20this%20work%2C%20we%20propose%20a%20novel%20end-to-end%20efficient%20rectified%0Aflow%20matching-based%20diffusion%20framework%20for%20audio%20editing%2C%20and%20construct%20a%0Adataset%20featuring%20overlapping%20multi-event%20audio%20to%20support%20training%20and%0Abenchmarking%20in%20complex%20scenarios.%20Experiments%20show%20that%20our%20model%20achieves%0Afaithful%20semantic%20alignment%20without%20requiring%20auxiliary%20captions%20or%20masks%2C%0Awhile%20maintaining%20competitive%20editing%20quality%20across%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRFM-Editing%253A%2520Rectified%2520Flow%2520Matching%2520for%2520Text-guided%2520Audio%2520Editing%26entry.906535625%3DLiting%2520Gao%2520and%2520Yi%2520Yuan%2520and%2520Yaru%2520Chen%2520and%2520Yuelan%2520Cheng%2520and%2520Zhenbo%2520Li%2520and%2520Juan%2520Wen%2520and%2520Shubin%2520Zhang%2520and%2520Wenwu%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520remarkable%2520progress%2520in%2520text-to-audio%2520generation.%250AHowever%252C%2520text-guided%2520audio%2520editing%2520remains%2520in%2520its%2520early%2520stages.%2520This%2520task%250Afocuses%2520on%2520modifying%2520the%2520target%2520content%2520within%2520an%2520audio%2520signal%2520while%2520preserving%250Athe%2520rest%252C%2520thus%2520demanding%2520precise%2520localization%2520and%2520faithful%2520editing%2520according%2520to%250Athe%2520text%2520prompt.%2520Existing%2520training-based%2520and%2520zero-shot%2520methods%2520that%2520rely%2520on%250Afull-caption%2520or%2520costly%2520optimization%2520often%2520struggle%2520with%2520complex%2520editing%2520or%2520lack%250Apracticality.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520efficient%2520rectified%250Aflow%2520matching-based%2520diffusion%2520framework%2520for%2520audio%2520editing%252C%2520and%2520construct%2520a%250Adataset%2520featuring%2520overlapping%2520multi-event%2520audio%2520to%2520support%2520training%2520and%250Abenchmarking%2520in%2520complex%2520scenarios.%2520Experiments%2520show%2520that%2520our%2520model%2520achieves%250Afaithful%2520semantic%2520alignment%2520without%2520requiring%2520auxiliary%2520captions%2520or%2520masks%252C%250Awhile%2520maintaining%2520competitive%2520editing%2520quality%2520across%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RFM-Editing%3A%20Rectified%20Flow%20Matching%20for%20Text-guided%20Audio%20Editing&entry.906535625=Liting%20Gao%20and%20Yi%20Yuan%20and%20Yaru%20Chen%20and%20Yuelan%20Cheng%20and%20Zhenbo%20Li%20and%20Juan%20Wen%20and%20Shubin%20Zhang%20and%20Wenwu%20Wang&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20remarkable%20progress%20in%20text-to-audio%20generation.%0AHowever%2C%20text-guided%20audio%20editing%20remains%20in%20its%20early%20stages.%20This%20task%0Afocuses%20on%20modifying%20the%20target%20content%20within%20an%20audio%20signal%20while%20preserving%0Athe%20rest%2C%20thus%20demanding%20precise%20localization%20and%20faithful%20editing%20according%20to%0Athe%20text%20prompt.%20Existing%20training-based%20and%20zero-shot%20methods%20that%20rely%20on%0Afull-caption%20or%20costly%20optimization%20often%20struggle%20with%20complex%20editing%20or%20lack%0Apracticality.%20In%20this%20work%2C%20we%20propose%20a%20novel%20end-to-end%20efficient%20rectified%0Aflow%20matching-based%20diffusion%20framework%20for%20audio%20editing%2C%20and%20construct%20a%0Adataset%20featuring%20overlapping%20multi-event%20audio%20to%20support%20training%20and%0Abenchmarking%20in%20complex%20scenarios.%20Experiments%20show%20that%20our%20model%20achieves%0Afaithful%20semantic%20alignment%20without%20requiring%20auxiliary%20captions%20or%20masks%2C%0Awhile%20maintaining%20competitive%20editing%20quality%20across%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14003v1&entry.124074799=Read"},
{"title": "Post-Hoc Split-Point Self-Consistency Verification for Efficient,\n  Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep\n  Learning", "author": "Zhizhong Zhao and Ke Chen", "abstract": "  Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet\nexisting methods are either computationally intensive, such as Bayesian or\nensemble methods, or provide only partial, task-specific estimates, such as\nsingle-forward-pass techniques. In this paper, we propose a post-hoc\nsingle-forward-pass framework that jointly captures aleatoric and epistemic\nuncertainty without modifying or retraining pretrained models. Our method\napplies \\emph{Split-Point Analysis} (SPA) to decompose predictive residuals\ninto upper and lower subsets, computing \\emph{Mean Absolute Residuals} (MARs)\non each side. We prove that, under ideal conditions, the total MAR equals the\nharmonic mean of subset MARs; deviations define a novel \\emph{Self-consistency\nDiscrepancy Score} (SDS) for fine-grained epistemic estimation across\nregression and classification. For regression, side-specific quantile\nregression yields prediction intervals with improved empirical coverage, which\nare further calibrated via SDS. For classification, when calibration data are\navailable, we apply SPA-based calibration identities to adjust the softmax\noutputs and then compute predictive entropy on these calibrated probabilities.\nExtensive experiments on diverse regression and classification benchmarks\ndemonstrate that our framework matches or exceeds several state-of-the-art UQ\nmethods while incurring minimal overhead.\n  Our source code is available at https://github.com/zzz0527/SPC-UQ.\n", "link": "http://arxiv.org/abs/2509.13262v2", "date": "2025-09-17", "relevancy": 1.6869, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5768}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5639}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-Hoc%20Split-Point%20Self-Consistency%20Verification%20for%20Efficient%2C%0A%20%20Unified%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20in%20Deep%0A%20%20Learning&body=Title%3A%20Post-Hoc%20Split-Point%20Self-Consistency%20Verification%20for%20Efficient%2C%0A%20%20Unified%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20in%20Deep%0A%20%20Learning%0AAuthor%3A%20Zhizhong%20Zhao%20and%20Ke%20Chen%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20is%20vital%20for%20trustworthy%20deep%20learning%2C%20yet%0Aexisting%20methods%20are%20either%20computationally%20intensive%2C%20such%20as%20Bayesian%20or%0Aensemble%20methods%2C%20or%20provide%20only%20partial%2C%20task-specific%20estimates%2C%20such%20as%0Asingle-forward-pass%20techniques.%20In%20this%20paper%2C%20we%20propose%20a%20post-hoc%0Asingle-forward-pass%20framework%20that%20jointly%20captures%20aleatoric%20and%20epistemic%0Auncertainty%20without%20modifying%20or%20retraining%20pretrained%20models.%20Our%20method%0Aapplies%20%5Cemph%7BSplit-Point%20Analysis%7D%20%28SPA%29%20to%20decompose%20predictive%20residuals%0Ainto%20upper%20and%20lower%20subsets%2C%20computing%20%5Cemph%7BMean%20Absolute%20Residuals%7D%20%28MARs%29%0Aon%20each%20side.%20We%20prove%20that%2C%20under%20ideal%20conditions%2C%20the%20total%20MAR%20equals%20the%0Aharmonic%20mean%20of%20subset%20MARs%3B%20deviations%20define%20a%20novel%20%5Cemph%7BSelf-consistency%0ADiscrepancy%20Score%7D%20%28SDS%29%20for%20fine-grained%20epistemic%20estimation%20across%0Aregression%20and%20classification.%20For%20regression%2C%20side-specific%20quantile%0Aregression%20yields%20prediction%20intervals%20with%20improved%20empirical%20coverage%2C%20which%0Aare%20further%20calibrated%20via%20SDS.%20For%20classification%2C%20when%20calibration%20data%20are%0Aavailable%2C%20we%20apply%20SPA-based%20calibration%20identities%20to%20adjust%20the%20softmax%0Aoutputs%20and%20then%20compute%20predictive%20entropy%20on%20these%20calibrated%20probabilities.%0AExtensive%20experiments%20on%20diverse%20regression%20and%20classification%20benchmarks%0Ademonstrate%20that%20our%20framework%20matches%20or%20exceeds%20several%20state-of-the-art%20UQ%0Amethods%20while%20incurring%20minimal%20overhead.%0A%20%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/zzz0527/SPC-UQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-Hoc%2520Split-Point%2520Self-Consistency%2520Verification%2520for%2520Efficient%252C%250A%2520%2520Unified%2520Quantification%2520of%2520Aleatoric%2520and%2520Epistemic%2520Uncertainty%2520in%2520Deep%250A%2520%2520Learning%26entry.906535625%3DZhizhong%2520Zhao%2520and%2520Ke%2520Chen%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520is%2520vital%2520for%2520trustworthy%2520deep%2520learning%252C%2520yet%250Aexisting%2520methods%2520are%2520either%2520computationally%2520intensive%252C%2520such%2520as%2520Bayesian%2520or%250Aensemble%2520methods%252C%2520or%2520provide%2520only%2520partial%252C%2520task-specific%2520estimates%252C%2520such%2520as%250Asingle-forward-pass%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520post-hoc%250Asingle-forward-pass%2520framework%2520that%2520jointly%2520captures%2520aleatoric%2520and%2520epistemic%250Auncertainty%2520without%2520modifying%2520or%2520retraining%2520pretrained%2520models.%2520Our%2520method%250Aapplies%2520%255Cemph%257BSplit-Point%2520Analysis%257D%2520%2528SPA%2529%2520to%2520decompose%2520predictive%2520residuals%250Ainto%2520upper%2520and%2520lower%2520subsets%252C%2520computing%2520%255Cemph%257BMean%2520Absolute%2520Residuals%257D%2520%2528MARs%2529%250Aon%2520each%2520side.%2520We%2520prove%2520that%252C%2520under%2520ideal%2520conditions%252C%2520the%2520total%2520MAR%2520equals%2520the%250Aharmonic%2520mean%2520of%2520subset%2520MARs%253B%2520deviations%2520define%2520a%2520novel%2520%255Cemph%257BSelf-consistency%250ADiscrepancy%2520Score%257D%2520%2528SDS%2529%2520for%2520fine-grained%2520epistemic%2520estimation%2520across%250Aregression%2520and%2520classification.%2520For%2520regression%252C%2520side-specific%2520quantile%250Aregression%2520yields%2520prediction%2520intervals%2520with%2520improved%2520empirical%2520coverage%252C%2520which%250Aare%2520further%2520calibrated%2520via%2520SDS.%2520For%2520classification%252C%2520when%2520calibration%2520data%2520are%250Aavailable%252C%2520we%2520apply%2520SPA-based%2520calibration%2520identities%2520to%2520adjust%2520the%2520softmax%250Aoutputs%2520and%2520then%2520compute%2520predictive%2520entropy%2520on%2520these%2520calibrated%2520probabilities.%250AExtensive%2520experiments%2520on%2520diverse%2520regression%2520and%2520classification%2520benchmarks%250Ademonstrate%2520that%2520our%2520framework%2520matches%2520or%2520exceeds%2520several%2520state-of-the-art%2520UQ%250Amethods%2520while%2520incurring%2520minimal%2520overhead.%250A%2520%2520Our%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/zzz0527/SPC-UQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Hoc%20Split-Point%20Self-Consistency%20Verification%20for%20Efficient%2C%0A%20%20Unified%20Quantification%20of%20Aleatoric%20and%20Epistemic%20Uncertainty%20in%20Deep%0A%20%20Learning&entry.906535625=Zhizhong%20Zhao%20and%20Ke%20Chen&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20is%20vital%20for%20trustworthy%20deep%20learning%2C%20yet%0Aexisting%20methods%20are%20either%20computationally%20intensive%2C%20such%20as%20Bayesian%20or%0Aensemble%20methods%2C%20or%20provide%20only%20partial%2C%20task-specific%20estimates%2C%20such%20as%0Asingle-forward-pass%20techniques.%20In%20this%20paper%2C%20we%20propose%20a%20post-hoc%0Asingle-forward-pass%20framework%20that%20jointly%20captures%20aleatoric%20and%20epistemic%0Auncertainty%20without%20modifying%20or%20retraining%20pretrained%20models.%20Our%20method%0Aapplies%20%5Cemph%7BSplit-Point%20Analysis%7D%20%28SPA%29%20to%20decompose%20predictive%20residuals%0Ainto%20upper%20and%20lower%20subsets%2C%20computing%20%5Cemph%7BMean%20Absolute%20Residuals%7D%20%28MARs%29%0Aon%20each%20side.%20We%20prove%20that%2C%20under%20ideal%20conditions%2C%20the%20total%20MAR%20equals%20the%0Aharmonic%20mean%20of%20subset%20MARs%3B%20deviations%20define%20a%20novel%20%5Cemph%7BSelf-consistency%0ADiscrepancy%20Score%7D%20%28SDS%29%20for%20fine-grained%20epistemic%20estimation%20across%0Aregression%20and%20classification.%20For%20regression%2C%20side-specific%20quantile%0Aregression%20yields%20prediction%20intervals%20with%20improved%20empirical%20coverage%2C%20which%0Aare%20further%20calibrated%20via%20SDS.%20For%20classification%2C%20when%20calibration%20data%20are%0Aavailable%2C%20we%20apply%20SPA-based%20calibration%20identities%20to%20adjust%20the%20softmax%0Aoutputs%20and%20then%20compute%20predictive%20entropy%20on%20these%20calibrated%20probabilities.%0AExtensive%20experiments%20on%20diverse%20regression%20and%20classification%20benchmarks%0Ademonstrate%20that%20our%20framework%20matches%20or%20exceeds%20several%20state-of-the-art%20UQ%0Amethods%20while%20incurring%20minimal%20overhead.%0A%20%20Our%20source%20code%20is%20available%20at%20https%3A//github.com/zzz0527/SPC-UQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13262v2&entry.124074799=Read"},
{"title": "Legal Knowledge Graph Foundations, Part I: URI-Addressable Abstract\n  Works (LRMoo F1 to schema.org)", "author": "Hudson de Martim", "abstract": "  Building upon a formal, event-centric model for the diachronic evolution of\nlegal norms grounded in the IFLA Library Reference Model (LRMoo), this paper\naddresses the essential first step of publishing this model's foundational\nentity-the abstract legal Work (F1)-on the Semantic Web. We propose a detailed,\nproperty-by-property mapping of the LRMoo F1 Work to the widely adopted\nschema.org/Legislation vocabulary. Using Brazilian federal legislation from the\nNormas.leg.br portal as a practical case study, we demonstrate how to create\ninteroperable, machine-readable descriptions via JSON-LD, focusing on stable\nURN identifiers, core metadata, and norm relationships. This structured mapping\nestablishes a stable, URI-addressable anchor for each legal norm, creating a\nverifiable \"ground truth\". It provides the essential, interoperable foundation\nupon which subsequent layers of the model, such as temporal versions\n(Expressions) and internal components, can be built. By bridging formal\nontology with web-native standards, this work paves the way for building\ndeterministic and reliable Legal Knowledge Graphs (LKGs), overcoming the\nlimitations of purely probabilistic models.\n", "link": "http://arxiv.org/abs/2508.00827v3", "date": "2025-09-17", "relevancy": 1.684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Legal%20Knowledge%20Graph%20Foundations%2C%20Part%20I%3A%20URI-Addressable%20Abstract%0A%20%20Works%20%28LRMoo%20F1%20to%20schema.org%29&body=Title%3A%20Legal%20Knowledge%20Graph%20Foundations%2C%20Part%20I%3A%20URI-Addressable%20Abstract%0A%20%20Works%20%28LRMoo%20F1%20to%20schema.org%29%0AAuthor%3A%20Hudson%20de%20Martim%0AAbstract%3A%20%20%20Building%20upon%20a%20formal%2C%20event-centric%20model%20for%20the%20diachronic%20evolution%20of%0Alegal%20norms%20grounded%20in%20the%20IFLA%20Library%20Reference%20Model%20%28LRMoo%29%2C%20this%20paper%0Aaddresses%20the%20essential%20first%20step%20of%20publishing%20this%20model%27s%20foundational%0Aentity-the%20abstract%20legal%20Work%20%28F1%29-on%20the%20Semantic%20Web.%20We%20propose%20a%20detailed%2C%0Aproperty-by-property%20mapping%20of%20the%20LRMoo%20F1%20Work%20to%20the%20widely%20adopted%0Aschema.org/Legislation%20vocabulary.%20Using%20Brazilian%20federal%20legislation%20from%20the%0ANormas.leg.br%20portal%20as%20a%20practical%20case%20study%2C%20we%20demonstrate%20how%20to%20create%0Ainteroperable%2C%20machine-readable%20descriptions%20via%20JSON-LD%2C%20focusing%20on%20stable%0AURN%20identifiers%2C%20core%20metadata%2C%20and%20norm%20relationships.%20This%20structured%20mapping%0Aestablishes%20a%20stable%2C%20URI-addressable%20anchor%20for%20each%20legal%20norm%2C%20creating%20a%0Averifiable%20%22ground%20truth%22.%20It%20provides%20the%20essential%2C%20interoperable%20foundation%0Aupon%20which%20subsequent%20layers%20of%20the%20model%2C%20such%20as%20temporal%20versions%0A%28Expressions%29%20and%20internal%20components%2C%20can%20be%20built.%20By%20bridging%20formal%0Aontology%20with%20web-native%20standards%2C%20this%20work%20paves%20the%20way%20for%20building%0Adeterministic%20and%20reliable%20Legal%20Knowledge%20Graphs%20%28LKGs%29%2C%20overcoming%20the%0Alimitations%20of%20purely%20probabilistic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00827v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLegal%2520Knowledge%2520Graph%2520Foundations%252C%2520Part%2520I%253A%2520URI-Addressable%2520Abstract%250A%2520%2520Works%2520%2528LRMoo%2520F1%2520to%2520schema.org%2529%26entry.906535625%3DHudson%2520de%2520Martim%26entry.1292438233%3D%2520%2520Building%2520upon%2520a%2520formal%252C%2520event-centric%2520model%2520for%2520the%2520diachronic%2520evolution%2520of%250Alegal%2520norms%2520grounded%2520in%2520the%2520IFLA%2520Library%2520Reference%2520Model%2520%2528LRMoo%2529%252C%2520this%2520paper%250Aaddresses%2520the%2520essential%2520first%2520step%2520of%2520publishing%2520this%2520model%2527s%2520foundational%250Aentity-the%2520abstract%2520legal%2520Work%2520%2528F1%2529-on%2520the%2520Semantic%2520Web.%2520We%2520propose%2520a%2520detailed%252C%250Aproperty-by-property%2520mapping%2520of%2520the%2520LRMoo%2520F1%2520Work%2520to%2520the%2520widely%2520adopted%250Aschema.org/Legislation%2520vocabulary.%2520Using%2520Brazilian%2520federal%2520legislation%2520from%2520the%250ANormas.leg.br%2520portal%2520as%2520a%2520practical%2520case%2520study%252C%2520we%2520demonstrate%2520how%2520to%2520create%250Ainteroperable%252C%2520machine-readable%2520descriptions%2520via%2520JSON-LD%252C%2520focusing%2520on%2520stable%250AURN%2520identifiers%252C%2520core%2520metadata%252C%2520and%2520norm%2520relationships.%2520This%2520structured%2520mapping%250Aestablishes%2520a%2520stable%252C%2520URI-addressable%2520anchor%2520for%2520each%2520legal%2520norm%252C%2520creating%2520a%250Averifiable%2520%2522ground%2520truth%2522.%2520It%2520provides%2520the%2520essential%252C%2520interoperable%2520foundation%250Aupon%2520which%2520subsequent%2520layers%2520of%2520the%2520model%252C%2520such%2520as%2520temporal%2520versions%250A%2528Expressions%2529%2520and%2520internal%2520components%252C%2520can%2520be%2520built.%2520By%2520bridging%2520formal%250Aontology%2520with%2520web-native%2520standards%252C%2520this%2520work%2520paves%2520the%2520way%2520for%2520building%250Adeterministic%2520and%2520reliable%2520Legal%2520Knowledge%2520Graphs%2520%2528LKGs%2529%252C%2520overcoming%2520the%250Alimitations%2520of%2520purely%2520probabilistic%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00827v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Legal%20Knowledge%20Graph%20Foundations%2C%20Part%20I%3A%20URI-Addressable%20Abstract%0A%20%20Works%20%28LRMoo%20F1%20to%20schema.org%29&entry.906535625=Hudson%20de%20Martim&entry.1292438233=%20%20Building%20upon%20a%20formal%2C%20event-centric%20model%20for%20the%20diachronic%20evolution%20of%0Alegal%20norms%20grounded%20in%20the%20IFLA%20Library%20Reference%20Model%20%28LRMoo%29%2C%20this%20paper%0Aaddresses%20the%20essential%20first%20step%20of%20publishing%20this%20model%27s%20foundational%0Aentity-the%20abstract%20legal%20Work%20%28F1%29-on%20the%20Semantic%20Web.%20We%20propose%20a%20detailed%2C%0Aproperty-by-property%20mapping%20of%20the%20LRMoo%20F1%20Work%20to%20the%20widely%20adopted%0Aschema.org/Legislation%20vocabulary.%20Using%20Brazilian%20federal%20legislation%20from%20the%0ANormas.leg.br%20portal%20as%20a%20practical%20case%20study%2C%20we%20demonstrate%20how%20to%20create%0Ainteroperable%2C%20machine-readable%20descriptions%20via%20JSON-LD%2C%20focusing%20on%20stable%0AURN%20identifiers%2C%20core%20metadata%2C%20and%20norm%20relationships.%20This%20structured%20mapping%0Aestablishes%20a%20stable%2C%20URI-addressable%20anchor%20for%20each%20legal%20norm%2C%20creating%20a%0Averifiable%20%22ground%20truth%22.%20It%20provides%20the%20essential%2C%20interoperable%20foundation%0Aupon%20which%20subsequent%20layers%20of%20the%20model%2C%20such%20as%20temporal%20versions%0A%28Expressions%29%20and%20internal%20components%2C%20can%20be%20built.%20By%20bridging%20formal%0Aontology%20with%20web-native%20standards%2C%20this%20work%20paves%20the%20way%20for%20building%0Adeterministic%20and%20reliable%20Legal%20Knowledge%20Graphs%20%28LKGs%29%2C%20overcoming%20the%0Alimitations%20of%20purely%20probabilistic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00827v3&entry.124074799=Read"},
{"title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic\n  Grasping", "author": "Zijian An and Ran Yang and Yiming Feng and Lifeng Zhou", "abstract": "  Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.\n", "link": "http://arxiv.org/abs/2509.14143v1", "date": "2025-09-17", "relevancy": 1.6493, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5721}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5591}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAW%3A%20A%20Vision-Language-Action%20Framework%20for%20Weight-Aware%20Robotic%0A%20%20Grasping&body=Title%3A%20CLAW%3A%20A%20Vision-Language-Action%20Framework%20for%20Weight-Aware%20Robotic%0A%20%20Grasping%0AAuthor%3A%20Zijian%20An%20and%20Ran%20Yang%20and%20Yiming%20Feng%20and%20Lifeng%20Zhou%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20models%20have%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20robotic%20control%2C%20enabling%20end-to-end%20policies%20that%20ground%20natural%0Alanguage%20instructions%20into%20visuomotor%20actions.%20However%2C%20current%20VLAs%20often%0Astruggle%20to%20satisfy%20precise%20task%20constraints%2C%20such%20as%20stopping%20based%20on%20numeric%0Athresholds%2C%20since%20their%20observation-to-action%20mappings%20are%20implicitly%20shaped%20by%0Atraining%20data%20and%20lack%20explicit%20mechanisms%20for%20condition%20monitoring.%20In%20this%0Awork%2C%20we%20propose%20CLAW%20%28CLIP-Language-Action%20for%20Weight%29%2C%20a%20framework%20that%0Adecouples%20condition%20evaluation%20from%20action%20generation.%20CLAW%20leverages%20a%0Afine-tuned%20CLIP%20model%20as%20a%20lightweight%20prompt%20generator%2C%20which%20continuously%0Amonitors%20the%20digital%20readout%20of%20a%20scale%20and%20produces%20discrete%20directives%20based%0Aon%20task-specific%20weight%20thresholds.%20These%20prompts%20are%20then%20consumed%20by%20%24%5Cpi_0%24%2C%0Aa%20flow-based%20VLA%20policy%2C%20which%20integrates%20the%20prompts%20with%20multi-view%20camera%0Aobservations%20to%20produce%20continuous%20robot%20actions.%20This%20design%20enables%20CLAW%20to%0Acombine%20symbolic%20weight%20reasoning%20with%20high-frequency%20visuomotor%20control.%20We%0Avalidate%20CLAW%20on%20three%20experimental%20setups%3A%20single-object%20grasping%20and%0Amixed-object%20tasks%20requiring%20dual-arm%20manipulation.%20Across%20all%20conditions%2C%20CLAW%0Areliably%20executes%20weight-aware%20behaviors%20and%20outperforms%20both%20raw-%24%5Cpi_0%24%20and%0Afine-tuned%20%24%5Cpi_0%24%20models.%20We%20have%20uploaded%20the%20videos%20as%20supplementary%0Amaterials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAW%253A%2520A%2520Vision-Language-Action%2520Framework%2520for%2520Weight-Aware%2520Robotic%250A%2520%2520Grasping%26entry.906535625%3DZijian%2520An%2520and%2520Ran%2520Yang%2520and%2520Yiming%2520Feng%2520and%2520Lifeng%2520Zhou%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520promising%250Aparadigm%2520for%2520robotic%2520control%252C%2520enabling%2520end-to-end%2520policies%2520that%2520ground%2520natural%250Alanguage%2520instructions%2520into%2520visuomotor%2520actions.%2520However%252C%2520current%2520VLAs%2520often%250Astruggle%2520to%2520satisfy%2520precise%2520task%2520constraints%252C%2520such%2520as%2520stopping%2520based%2520on%2520numeric%250Athresholds%252C%2520since%2520their%2520observation-to-action%2520mappings%2520are%2520implicitly%2520shaped%2520by%250Atraining%2520data%2520and%2520lack%2520explicit%2520mechanisms%2520for%2520condition%2520monitoring.%2520In%2520this%250Awork%252C%2520we%2520propose%2520CLAW%2520%2528CLIP-Language-Action%2520for%2520Weight%2529%252C%2520a%2520framework%2520that%250Adecouples%2520condition%2520evaluation%2520from%2520action%2520generation.%2520CLAW%2520leverages%2520a%250Afine-tuned%2520CLIP%2520model%2520as%2520a%2520lightweight%2520prompt%2520generator%252C%2520which%2520continuously%250Amonitors%2520the%2520digital%2520readout%2520of%2520a%2520scale%2520and%2520produces%2520discrete%2520directives%2520based%250Aon%2520task-specific%2520weight%2520thresholds.%2520These%2520prompts%2520are%2520then%2520consumed%2520by%2520%2524%255Cpi_0%2524%252C%250Aa%2520flow-based%2520VLA%2520policy%252C%2520which%2520integrates%2520the%2520prompts%2520with%2520multi-view%2520camera%250Aobservations%2520to%2520produce%2520continuous%2520robot%2520actions.%2520This%2520design%2520enables%2520CLAW%2520to%250Acombine%2520symbolic%2520weight%2520reasoning%2520with%2520high-frequency%2520visuomotor%2520control.%2520We%250Avalidate%2520CLAW%2520on%2520three%2520experimental%2520setups%253A%2520single-object%2520grasping%2520and%250Amixed-object%2520tasks%2520requiring%2520dual-arm%2520manipulation.%2520Across%2520all%2520conditions%252C%2520CLAW%250Areliably%2520executes%2520weight-aware%2520behaviors%2520and%2520outperforms%2520both%2520raw-%2524%255Cpi_0%2524%2520and%250Afine-tuned%2520%2524%255Cpi_0%2524%2520models.%2520We%2520have%2520uploaded%2520the%2520videos%2520as%2520supplementary%250Amaterials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAW%3A%20A%20Vision-Language-Action%20Framework%20for%20Weight-Aware%20Robotic%0A%20%20Grasping&entry.906535625=Zijian%20An%20and%20Ran%20Yang%20and%20Yiming%20Feng%20and%20Lifeng%20Zhou&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20models%20have%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20robotic%20control%2C%20enabling%20end-to-end%20policies%20that%20ground%20natural%0Alanguage%20instructions%20into%20visuomotor%20actions.%20However%2C%20current%20VLAs%20often%0Astruggle%20to%20satisfy%20precise%20task%20constraints%2C%20such%20as%20stopping%20based%20on%20numeric%0Athresholds%2C%20since%20their%20observation-to-action%20mappings%20are%20implicitly%20shaped%20by%0Atraining%20data%20and%20lack%20explicit%20mechanisms%20for%20condition%20monitoring.%20In%20this%0Awork%2C%20we%20propose%20CLAW%20%28CLIP-Language-Action%20for%20Weight%29%2C%20a%20framework%20that%0Adecouples%20condition%20evaluation%20from%20action%20generation.%20CLAW%20leverages%20a%0Afine-tuned%20CLIP%20model%20as%20a%20lightweight%20prompt%20generator%2C%20which%20continuously%0Amonitors%20the%20digital%20readout%20of%20a%20scale%20and%20produces%20discrete%20directives%20based%0Aon%20task-specific%20weight%20thresholds.%20These%20prompts%20are%20then%20consumed%20by%20%24%5Cpi_0%24%2C%0Aa%20flow-based%20VLA%20policy%2C%20which%20integrates%20the%20prompts%20with%20multi-view%20camera%0Aobservations%20to%20produce%20continuous%20robot%20actions.%20This%20design%20enables%20CLAW%20to%0Acombine%20symbolic%20weight%20reasoning%20with%20high-frequency%20visuomotor%20control.%20We%0Avalidate%20CLAW%20on%20three%20experimental%20setups%3A%20single-object%20grasping%20and%0Amixed-object%20tasks%20requiring%20dual-arm%20manipulation.%20Across%20all%20conditions%2C%20CLAW%0Areliably%20executes%20weight-aware%20behaviors%20and%20outperforms%20both%20raw-%24%5Cpi_0%24%20and%0Afine-tuned%20%24%5Cpi_0%24%20models.%20We%20have%20uploaded%20the%20videos%20as%20supplementary%0Amaterials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14143v1&entry.124074799=Read"},
{"title": "Online Bayesian Risk-Averse Reinforcement Learning", "author": "Yuhao Wang and Enlu Zhou", "abstract": "  In this paper, we study the Bayesian risk-averse formulation in reinforcement\nlearning (RL). To address the epistemic uncertainty due to a lack of data, we\nadopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the\nparameter uncertainty of the unknown underlying model. We derive the asymptotic\nnormality that characterizes the difference between the Bayesian risk value\nfunction and the original value function under the true unknown distribution.\nThe results indicate that the Bayesian risk-averse approach tends to\npessimistically underestimate the original value function. This discrepancy\nincreases with stronger risk aversion and decreases as more data become\navailable. We then utilize this adaptive property in the setting of online RL\nas well as online contextual multi-arm bandits (CMAB), a special case of online\nRL. We provide two procedures using posterior sampling for both the general RL\nproblem and the CMAB problem. We establish a sub-linear regret bound, with the\nregret defined as the conventional regret for both the RL and CMAB settings.\nAdditionally, we establish a sub-linear regret bound for the CMAB setting with\nthe regret defined as the Bayesian risk regret. Finally, we conduct numerical\nexperiments to demonstrate the effectiveness of the proposed algorithm in\naddressing epistemic uncertainty and verifying the theoretical properties.\n", "link": "http://arxiv.org/abs/2509.14077v1", "date": "2025-09-17", "relevancy": 1.4633, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4653}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Bayesian%20Risk-Averse%20Reinforcement%20Learning&body=Title%3A%20Online%20Bayesian%20Risk-Averse%20Reinforcement%20Learning%0AAuthor%3A%20Yuhao%20Wang%20and%20Enlu%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20Bayesian%20risk-averse%20formulation%20in%20reinforcement%0Alearning%20%28RL%29.%20To%20address%20the%20epistemic%20uncertainty%20due%20to%20a%20lack%20of%20data%2C%20we%0Aadopt%20the%20Bayesian%20Risk%20Markov%20Decision%20Process%20%28BRMDP%29%20to%20account%20for%20the%0Aparameter%20uncertainty%20of%20the%20unknown%20underlying%20model.%20We%20derive%20the%20asymptotic%0Anormality%20that%20characterizes%20the%20difference%20between%20the%20Bayesian%20risk%20value%0Afunction%20and%20the%20original%20value%20function%20under%20the%20true%20unknown%20distribution.%0AThe%20results%20indicate%20that%20the%20Bayesian%20risk-averse%20approach%20tends%20to%0Apessimistically%20underestimate%20the%20original%20value%20function.%20This%20discrepancy%0Aincreases%20with%20stronger%20risk%20aversion%20and%20decreases%20as%20more%20data%20become%0Aavailable.%20We%20then%20utilize%20this%20adaptive%20property%20in%20the%20setting%20of%20online%20RL%0Aas%20well%20as%20online%20contextual%20multi-arm%20bandits%20%28CMAB%29%2C%20a%20special%20case%20of%20online%0ARL.%20We%20provide%20two%20procedures%20using%20posterior%20sampling%20for%20both%20the%20general%20RL%0Aproblem%20and%20the%20CMAB%20problem.%20We%20establish%20a%20sub-linear%20regret%20bound%2C%20with%20the%0Aregret%20defined%20as%20the%20conventional%20regret%20for%20both%20the%20RL%20and%20CMAB%20settings.%0AAdditionally%2C%20we%20establish%20a%20sub-linear%20regret%20bound%20for%20the%20CMAB%20setting%20with%0Athe%20regret%20defined%20as%20the%20Bayesian%20risk%20regret.%20Finally%2C%20we%20conduct%20numerical%0Aexperiments%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20algorithm%20in%0Aaddressing%20epistemic%20uncertainty%20and%20verifying%20the%20theoretical%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Bayesian%2520Risk-Averse%2520Reinforcement%2520Learning%26entry.906535625%3DYuhao%2520Wang%2520and%2520Enlu%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520Bayesian%2520risk-averse%2520formulation%2520in%2520reinforcement%250Alearning%2520%2528RL%2529.%2520To%2520address%2520the%2520epistemic%2520uncertainty%2520due%2520to%2520a%2520lack%2520of%2520data%252C%2520we%250Aadopt%2520the%2520Bayesian%2520Risk%2520Markov%2520Decision%2520Process%2520%2528BRMDP%2529%2520to%2520account%2520for%2520the%250Aparameter%2520uncertainty%2520of%2520the%2520unknown%2520underlying%2520model.%2520We%2520derive%2520the%2520asymptotic%250Anormality%2520that%2520characterizes%2520the%2520difference%2520between%2520the%2520Bayesian%2520risk%2520value%250Afunction%2520and%2520the%2520original%2520value%2520function%2520under%2520the%2520true%2520unknown%2520distribution.%250AThe%2520results%2520indicate%2520that%2520the%2520Bayesian%2520risk-averse%2520approach%2520tends%2520to%250Apessimistically%2520underestimate%2520the%2520original%2520value%2520function.%2520This%2520discrepancy%250Aincreases%2520with%2520stronger%2520risk%2520aversion%2520and%2520decreases%2520as%2520more%2520data%2520become%250Aavailable.%2520We%2520then%2520utilize%2520this%2520adaptive%2520property%2520in%2520the%2520setting%2520of%2520online%2520RL%250Aas%2520well%2520as%2520online%2520contextual%2520multi-arm%2520bandits%2520%2528CMAB%2529%252C%2520a%2520special%2520case%2520of%2520online%250ARL.%2520We%2520provide%2520two%2520procedures%2520using%2520posterior%2520sampling%2520for%2520both%2520the%2520general%2520RL%250Aproblem%2520and%2520the%2520CMAB%2520problem.%2520We%2520establish%2520a%2520sub-linear%2520regret%2520bound%252C%2520with%2520the%250Aregret%2520defined%2520as%2520the%2520conventional%2520regret%2520for%2520both%2520the%2520RL%2520and%2520CMAB%2520settings.%250AAdditionally%252C%2520we%2520establish%2520a%2520sub-linear%2520regret%2520bound%2520for%2520the%2520CMAB%2520setting%2520with%250Athe%2520regret%2520defined%2520as%2520the%2520Bayesian%2520risk%2520regret.%2520Finally%252C%2520we%2520conduct%2520numerical%250Aexperiments%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520algorithm%2520in%250Aaddressing%2520epistemic%2520uncertainty%2520and%2520verifying%2520the%2520theoretical%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Bayesian%20Risk-Averse%20Reinforcement%20Learning&entry.906535625=Yuhao%20Wang%20and%20Enlu%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20Bayesian%20risk-averse%20formulation%20in%20reinforcement%0Alearning%20%28RL%29.%20To%20address%20the%20epistemic%20uncertainty%20due%20to%20a%20lack%20of%20data%2C%20we%0Aadopt%20the%20Bayesian%20Risk%20Markov%20Decision%20Process%20%28BRMDP%29%20to%20account%20for%20the%0Aparameter%20uncertainty%20of%20the%20unknown%20underlying%20model.%20We%20derive%20the%20asymptotic%0Anormality%20that%20characterizes%20the%20difference%20between%20the%20Bayesian%20risk%20value%0Afunction%20and%20the%20original%20value%20function%20under%20the%20true%20unknown%20distribution.%0AThe%20results%20indicate%20that%20the%20Bayesian%20risk-averse%20approach%20tends%20to%0Apessimistically%20underestimate%20the%20original%20value%20function.%20This%20discrepancy%0Aincreases%20with%20stronger%20risk%20aversion%20and%20decreases%20as%20more%20data%20become%0Aavailable.%20We%20then%20utilize%20this%20adaptive%20property%20in%20the%20setting%20of%20online%20RL%0Aas%20well%20as%20online%20contextual%20multi-arm%20bandits%20%28CMAB%29%2C%20a%20special%20case%20of%20online%0ARL.%20We%20provide%20two%20procedures%20using%20posterior%20sampling%20for%20both%20the%20general%20RL%0Aproblem%20and%20the%20CMAB%20problem.%20We%20establish%20a%20sub-linear%20regret%20bound%2C%20with%20the%0Aregret%20defined%20as%20the%20conventional%20regret%20for%20both%20the%20RL%20and%20CMAB%20settings.%0AAdditionally%2C%20we%20establish%20a%20sub-linear%20regret%20bound%20for%20the%20CMAB%20setting%20with%0Athe%20regret%20defined%20as%20the%20Bayesian%20risk%20regret.%20Finally%2C%20we%20conduct%20numerical%0Aexperiments%20to%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20algorithm%20in%0Aaddressing%20epistemic%20uncertainty%20and%20verifying%20the%20theoretical%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14077v1&entry.124074799=Read"},
{"title": "Learning AC Power Flow Solutions using a Data-Dependent Variational\n  Quantum Circuit", "author": "Thinh Viet Le and Md Obaidur Rahman and Vassilis Kekatos", "abstract": "  Interconnection studies require solving numerous instances of the AC load or\npower flow (AC PF) problem to simulate diverse scenarios as power systems\nnavigate the ongoing energy transition. To expedite such studies, this work\nleverages recent advances in quantum computing to find or predict AC PF\nsolutions using a variational quantum circuit (VQC). VQCs are trainable models\nthat run on modern-day noisy intermediate-scale quantum (NISQ) hardware to\naccomplish elaborate optimization and machine learning (ML) tasks. Our first\ncontribution is to pose a single instance of the AC PF as a nonlinear\nleast-squares fit over the VQC trainable parameters (weights) and solve it\nusing a hybrid classical/quantum computing approach. The second contribution is\nto feed PF specifications as features into a data-embedded VQC and train the\nresultant quantum ML (QML) model to predict general PF solutions. The third\ncontribution is to develop a novel protocol to efficiently measure AC-PF\nquantum observables by exploiting the graph structure of a power network.\nPreliminary numerical tests indicate that the proposed VQC models attain\nenhanced prediction performance over a deep neural network despite using much\nfewer weights. The proposed quantum AC-PF framework sets the foundations for\naddressing more elaborate grid tasks via quantum computing.\n", "link": "http://arxiv.org/abs/2509.03495v2", "date": "2025-09-17", "relevancy": 1.3645, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4887}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.456}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20AC%20Power%20Flow%20Solutions%20using%20a%20Data-Dependent%20Variational%0A%20%20Quantum%20Circuit&body=Title%3A%20Learning%20AC%20Power%20Flow%20Solutions%20using%20a%20Data-Dependent%20Variational%0A%20%20Quantum%20Circuit%0AAuthor%3A%20Thinh%20Viet%20Le%20and%20Md%20Obaidur%20Rahman%20and%20Vassilis%20Kekatos%0AAbstract%3A%20%20%20Interconnection%20studies%20require%20solving%20numerous%20instances%20of%20the%20AC%20load%20or%0Apower%20flow%20%28AC%20PF%29%20problem%20to%20simulate%20diverse%20scenarios%20as%20power%20systems%0Anavigate%20the%20ongoing%20energy%20transition.%20To%20expedite%20such%20studies%2C%20this%20work%0Aleverages%20recent%20advances%20in%20quantum%20computing%20to%20find%20or%20predict%20AC%20PF%0Asolutions%20using%20a%20variational%20quantum%20circuit%20%28VQC%29.%20VQCs%20are%20trainable%20models%0Athat%20run%20on%20modern-day%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20hardware%20to%0Aaccomplish%20elaborate%20optimization%20and%20machine%20learning%20%28ML%29%20tasks.%20Our%20first%0Acontribution%20is%20to%20pose%20a%20single%20instance%20of%20the%20AC%20PF%20as%20a%20nonlinear%0Aleast-squares%20fit%20over%20the%20VQC%20trainable%20parameters%20%28weights%29%20and%20solve%20it%0Ausing%20a%20hybrid%20classical/quantum%20computing%20approach.%20The%20second%20contribution%20is%0Ato%20feed%20PF%20specifications%20as%20features%20into%20a%20data-embedded%20VQC%20and%20train%20the%0Aresultant%20quantum%20ML%20%28QML%29%20model%20to%20predict%20general%20PF%20solutions.%20The%20third%0Acontribution%20is%20to%20develop%20a%20novel%20protocol%20to%20efficiently%20measure%20AC-PF%0Aquantum%20observables%20by%20exploiting%20the%20graph%20structure%20of%20a%20power%20network.%0APreliminary%20numerical%20tests%20indicate%20that%20the%20proposed%20VQC%20models%20attain%0Aenhanced%20prediction%20performance%20over%20a%20deep%20neural%20network%20despite%20using%20much%0Afewer%20weights.%20The%20proposed%20quantum%20AC-PF%20framework%20sets%20the%20foundations%20for%0Aaddressing%20more%20elaborate%20grid%20tasks%20via%20quantum%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520AC%2520Power%2520Flow%2520Solutions%2520using%2520a%2520Data-Dependent%2520Variational%250A%2520%2520Quantum%2520Circuit%26entry.906535625%3DThinh%2520Viet%2520Le%2520and%2520Md%2520Obaidur%2520Rahman%2520and%2520Vassilis%2520Kekatos%26entry.1292438233%3D%2520%2520Interconnection%2520studies%2520require%2520solving%2520numerous%2520instances%2520of%2520the%2520AC%2520load%2520or%250Apower%2520flow%2520%2528AC%2520PF%2529%2520problem%2520to%2520simulate%2520diverse%2520scenarios%2520as%2520power%2520systems%250Anavigate%2520the%2520ongoing%2520energy%2520transition.%2520To%2520expedite%2520such%2520studies%252C%2520this%2520work%250Aleverages%2520recent%2520advances%2520in%2520quantum%2520computing%2520to%2520find%2520or%2520predict%2520AC%2520PF%250Asolutions%2520using%2520a%2520variational%2520quantum%2520circuit%2520%2528VQC%2529.%2520VQCs%2520are%2520trainable%2520models%250Athat%2520run%2520on%2520modern-day%2520noisy%2520intermediate-scale%2520quantum%2520%2528NISQ%2529%2520hardware%2520to%250Aaccomplish%2520elaborate%2520optimization%2520and%2520machine%2520learning%2520%2528ML%2529%2520tasks.%2520Our%2520first%250Acontribution%2520is%2520to%2520pose%2520a%2520single%2520instance%2520of%2520the%2520AC%2520PF%2520as%2520a%2520nonlinear%250Aleast-squares%2520fit%2520over%2520the%2520VQC%2520trainable%2520parameters%2520%2528weights%2529%2520and%2520solve%2520it%250Ausing%2520a%2520hybrid%2520classical/quantum%2520computing%2520approach.%2520The%2520second%2520contribution%2520is%250Ato%2520feed%2520PF%2520specifications%2520as%2520features%2520into%2520a%2520data-embedded%2520VQC%2520and%2520train%2520the%250Aresultant%2520quantum%2520ML%2520%2528QML%2529%2520model%2520to%2520predict%2520general%2520PF%2520solutions.%2520The%2520third%250Acontribution%2520is%2520to%2520develop%2520a%2520novel%2520protocol%2520to%2520efficiently%2520measure%2520AC-PF%250Aquantum%2520observables%2520by%2520exploiting%2520the%2520graph%2520structure%2520of%2520a%2520power%2520network.%250APreliminary%2520numerical%2520tests%2520indicate%2520that%2520the%2520proposed%2520VQC%2520models%2520attain%250Aenhanced%2520prediction%2520performance%2520over%2520a%2520deep%2520neural%2520network%2520despite%2520using%2520much%250Afewer%2520weights.%2520The%2520proposed%2520quantum%2520AC-PF%2520framework%2520sets%2520the%2520foundations%2520for%250Aaddressing%2520more%2520elaborate%2520grid%2520tasks%2520via%2520quantum%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20AC%20Power%20Flow%20Solutions%20using%20a%20Data-Dependent%20Variational%0A%20%20Quantum%20Circuit&entry.906535625=Thinh%20Viet%20Le%20and%20Md%20Obaidur%20Rahman%20and%20Vassilis%20Kekatos&entry.1292438233=%20%20Interconnection%20studies%20require%20solving%20numerous%20instances%20of%20the%20AC%20load%20or%0Apower%20flow%20%28AC%20PF%29%20problem%20to%20simulate%20diverse%20scenarios%20as%20power%20systems%0Anavigate%20the%20ongoing%20energy%20transition.%20To%20expedite%20such%20studies%2C%20this%20work%0Aleverages%20recent%20advances%20in%20quantum%20computing%20to%20find%20or%20predict%20AC%20PF%0Asolutions%20using%20a%20variational%20quantum%20circuit%20%28VQC%29.%20VQCs%20are%20trainable%20models%0Athat%20run%20on%20modern-day%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20hardware%20to%0Aaccomplish%20elaborate%20optimization%20and%20machine%20learning%20%28ML%29%20tasks.%20Our%20first%0Acontribution%20is%20to%20pose%20a%20single%20instance%20of%20the%20AC%20PF%20as%20a%20nonlinear%0Aleast-squares%20fit%20over%20the%20VQC%20trainable%20parameters%20%28weights%29%20and%20solve%20it%0Ausing%20a%20hybrid%20classical/quantum%20computing%20approach.%20The%20second%20contribution%20is%0Ato%20feed%20PF%20specifications%20as%20features%20into%20a%20data-embedded%20VQC%20and%20train%20the%0Aresultant%20quantum%20ML%20%28QML%29%20model%20to%20predict%20general%20PF%20solutions.%20The%20third%0Acontribution%20is%20to%20develop%20a%20novel%20protocol%20to%20efficiently%20measure%20AC-PF%0Aquantum%20observables%20by%20exploiting%20the%20graph%20structure%20of%20a%20power%20network.%0APreliminary%20numerical%20tests%20indicate%20that%20the%20proposed%20VQC%20models%20attain%0Aenhanced%20prediction%20performance%20over%20a%20deep%20neural%20network%20despite%20using%20much%0Afewer%20weights.%20The%20proposed%20quantum%20AC-PF%20framework%20sets%20the%20foundations%20for%0Aaddressing%20more%20elaborate%20grid%20tasks%20via%20quantum%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03495v2&entry.124074799=Read"},
{"title": "Exploring Major Transitions in the Evolution of Biological Cognition\n  With Artificial Neural Networks", "author": "Konstantinos Voudouris and Andrew Barron and Marta Halina and Colin Klein and Matishalin Patel", "abstract": "  Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.\n", "link": "http://arxiv.org/abs/2509.13968v1", "date": "2025-09-17", "relevancy": 1.3886, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4635}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Major%20Transitions%20in%20the%20Evolution%20of%20Biological%20Cognition%0A%20%20With%20Artificial%20Neural%20Networks&body=Title%3A%20Exploring%20Major%20Transitions%20in%20the%20Evolution%20of%20Biological%20Cognition%0A%20%20With%20Artificial%20Neural%20Networks%0AAuthor%3A%20Konstantinos%20Voudouris%20and%20Andrew%20Barron%20and%20Marta%20Halina%20and%20Colin%20Klein%20and%20Matishalin%20Patel%0AAbstract%3A%20%20%20Transitional%20accounts%20of%20evolution%20emphasise%20a%20few%20changes%20that%20shape%20what%20is%0Aevolvable%2C%20with%20dramatic%20consequences%20for%20derived%20lineages.%20More%20recently%20it%0Ahas%20been%20proposed%20that%20cognition%20might%20also%20have%20evolved%20via%20a%20series%20of%20major%0Atransitions%20that%20manipulate%20the%20structure%20of%20biological%20neural%20networks%2C%0Afundamentally%20changing%20the%20flow%20of%20information.%20We%20used%20idealised%20models%20of%0Ainformation%20flow%2C%20artificial%20neural%20networks%20%28ANNs%29%2C%20to%20evaluate%20whether%0Achanges%20in%20information%20flow%20in%20a%20network%20can%20yield%20a%20transitional%20change%20in%0Acognitive%20performance.%20We%20compared%20networks%20with%20feed-forward%2C%20recurrent%20and%0Alaminated%20topologies%2C%20and%20tested%20their%20performance%20learning%20artificial%20grammars%0Athat%20differed%20in%20complexity%2C%20controlling%20for%20network%20size%20and%20resources.%20We%0Adocumented%20a%20qualitative%20expansion%20in%20the%20types%20of%20input%20that%20recurrent%0Anetworks%20can%20process%20compared%20to%20feed-forward%20networks%2C%20and%20a%20related%0Aqualitative%20increase%20in%20performance%20for%20learning%20the%20most%20complex%20grammars.%20We%0Aalso%20noted%20how%20the%20difficulty%20in%20training%20recurrent%20networks%20poses%20a%20form%20of%0Atransition%20barrier%20and%20contingent%20irreversibility%20--%20other%20key%20features%20of%0Aevolutionary%20transitions.%20Not%20all%20changes%20in%20network%20topology%20confer%20a%0Aperformance%20advantage%20in%20this%20task%20set.%20Laminated%20networks%20did%20not%20outperform%0Anon-laminated%20networks%20in%20grammar%20learning.%20Overall%2C%20our%20findings%20show%20how%20some%0Achanges%20in%20information%20flow%20can%20yield%20transitions%20in%20cognitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Major%2520Transitions%2520in%2520the%2520Evolution%2520of%2520Biological%2520Cognition%250A%2520%2520With%2520Artificial%2520Neural%2520Networks%26entry.906535625%3DKonstantinos%2520Voudouris%2520and%2520Andrew%2520Barron%2520and%2520Marta%2520Halina%2520and%2520Colin%2520Klein%2520and%2520Matishalin%2520Patel%26entry.1292438233%3D%2520%2520Transitional%2520accounts%2520of%2520evolution%2520emphasise%2520a%2520few%2520changes%2520that%2520shape%2520what%2520is%250Aevolvable%252C%2520with%2520dramatic%2520consequences%2520for%2520derived%2520lineages.%2520More%2520recently%2520it%250Ahas%2520been%2520proposed%2520that%2520cognition%2520might%2520also%2520have%2520evolved%2520via%2520a%2520series%2520of%2520major%250Atransitions%2520that%2520manipulate%2520the%2520structure%2520of%2520biological%2520neural%2520networks%252C%250Afundamentally%2520changing%2520the%2520flow%2520of%2520information.%2520We%2520used%2520idealised%2520models%2520of%250Ainformation%2520flow%252C%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%252C%2520to%2520evaluate%2520whether%250Achanges%2520in%2520information%2520flow%2520in%2520a%2520network%2520can%2520yield%2520a%2520transitional%2520change%2520in%250Acognitive%2520performance.%2520We%2520compared%2520networks%2520with%2520feed-forward%252C%2520recurrent%2520and%250Alaminated%2520topologies%252C%2520and%2520tested%2520their%2520performance%2520learning%2520artificial%2520grammars%250Athat%2520differed%2520in%2520complexity%252C%2520controlling%2520for%2520network%2520size%2520and%2520resources.%2520We%250Adocumented%2520a%2520qualitative%2520expansion%2520in%2520the%2520types%2520of%2520input%2520that%2520recurrent%250Anetworks%2520can%2520process%2520compared%2520to%2520feed-forward%2520networks%252C%2520and%2520a%2520related%250Aqualitative%2520increase%2520in%2520performance%2520for%2520learning%2520the%2520most%2520complex%2520grammars.%2520We%250Aalso%2520noted%2520how%2520the%2520difficulty%2520in%2520training%2520recurrent%2520networks%2520poses%2520a%2520form%2520of%250Atransition%2520barrier%2520and%2520contingent%2520irreversibility%2520--%2520other%2520key%2520features%2520of%250Aevolutionary%2520transitions.%2520Not%2520all%2520changes%2520in%2520network%2520topology%2520confer%2520a%250Aperformance%2520advantage%2520in%2520this%2520task%2520set.%2520Laminated%2520networks%2520did%2520not%2520outperform%250Anon-laminated%2520networks%2520in%2520grammar%2520learning.%2520Overall%252C%2520our%2520findings%2520show%2520how%2520some%250Achanges%2520in%2520information%2520flow%2520can%2520yield%2520transitions%2520in%2520cognitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Major%20Transitions%20in%20the%20Evolution%20of%20Biological%20Cognition%0A%20%20With%20Artificial%20Neural%20Networks&entry.906535625=Konstantinos%20Voudouris%20and%20Andrew%20Barron%20and%20Marta%20Halina%20and%20Colin%20Klein%20and%20Matishalin%20Patel&entry.1292438233=%20%20Transitional%20accounts%20of%20evolution%20emphasise%20a%20few%20changes%20that%20shape%20what%20is%0Aevolvable%2C%20with%20dramatic%20consequences%20for%20derived%20lineages.%20More%20recently%20it%0Ahas%20been%20proposed%20that%20cognition%20might%20also%20have%20evolved%20via%20a%20series%20of%20major%0Atransitions%20that%20manipulate%20the%20structure%20of%20biological%20neural%20networks%2C%0Afundamentally%20changing%20the%20flow%20of%20information.%20We%20used%20idealised%20models%20of%0Ainformation%20flow%2C%20artificial%20neural%20networks%20%28ANNs%29%2C%20to%20evaluate%20whether%0Achanges%20in%20information%20flow%20in%20a%20network%20can%20yield%20a%20transitional%20change%20in%0Acognitive%20performance.%20We%20compared%20networks%20with%20feed-forward%2C%20recurrent%20and%0Alaminated%20topologies%2C%20and%20tested%20their%20performance%20learning%20artificial%20grammars%0Athat%20differed%20in%20complexity%2C%20controlling%20for%20network%20size%20and%20resources.%20We%0Adocumented%20a%20qualitative%20expansion%20in%20the%20types%20of%20input%20that%20recurrent%0Anetworks%20can%20process%20compared%20to%20feed-forward%20networks%2C%20and%20a%20related%0Aqualitative%20increase%20in%20performance%20for%20learning%20the%20most%20complex%20grammars.%20We%0Aalso%20noted%20how%20the%20difficulty%20in%20training%20recurrent%20networks%20poses%20a%20form%20of%0Atransition%20barrier%20and%20contingent%20irreversibility%20--%20other%20key%20features%20of%0Aevolutionary%20transitions.%20Not%20all%20changes%20in%20network%20topology%20confer%20a%0Aperformance%20advantage%20in%20this%20task%20set.%20Laminated%20networks%20did%20not%20outperform%0Anon-laminated%20networks%20in%20grammar%20learning.%20Overall%2C%20our%20findings%20show%20how%20some%0Achanges%20in%20information%20flow%20can%20yield%20transitions%20in%20cognitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13968v1&entry.124074799=Read"},
{"title": "Breaking the Cycle of Incarceration With Targeted Mental Health\n  Outreach: A Case Study in Machine Learning for Public Policy", "author": "Kit T. Rodolfa and Erika Salomon and Jin Yao and Steve Yoder and Robert Sullivan and Kevin McGuire and Allie Dickinson and Rob MacDougall and Brian Seidler and Christina Sung and Claire Herdeman and Rayid Ghani", "abstract": "  Many incarcerated individuals face significant and complex challenges,\nincluding mental illness, substance dependence, and homelessness, yet jails and\nprisons are often poorly equipped to address these needs. With little support\nfrom the existing criminal justice system, these needs can remain untreated and\nworsen, often leading to further offenses and a cycle of incarceration with\nadverse outcomes both for the individual and for public safety, with\nparticularly large impacts on communities of color that continue to widen the\nalready extensive racial disparities in criminal justice outcomes. Responding\nto these failures, a growing number of criminal justice stakeholders are\nseeking to break this cycle through innovative approaches such as\ncommunity-driven and alternative approaches to policing, mentoring, community\nbuilding, restorative justice, pretrial diversion, holistic defense, and social\nservice connections. Here we report on a collaboration between Johnson County,\nKansas, and Carnegie Mellon University to perform targeted, proactive mental\nhealth outreach in an effort to reduce reincarceration rates.\n  This paper describes the data used, our predictive modeling approach and\nresults, as well as the design and analysis of a field trial conducted to\nconfirm our model's predictive power, evaluate the impact of this targeted\noutreach, and understand at what level of reincarceration risk outreach might\nbe most effective. Through this trial, we find that our model is highly\npredictive of new jail bookings, with more than half of individuals in the\ntrial's highest-risk group returning to jail in the following year. Outreach\nwas most effective among these highest-risk individuals, with impacts on mental\nhealth utilization, EMS dispatches, and criminal justice involvement.\n", "link": "http://arxiv.org/abs/2509.14129v1", "date": "2025-09-17", "relevancy": 0.8001, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4144}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3982}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Cycle%20of%20Incarceration%20With%20Targeted%20Mental%20Health%0A%20%20Outreach%3A%20A%20Case%20Study%20in%20Machine%20Learning%20for%20Public%20Policy&body=Title%3A%20Breaking%20the%20Cycle%20of%20Incarceration%20With%20Targeted%20Mental%20Health%0A%20%20Outreach%3A%20A%20Case%20Study%20in%20Machine%20Learning%20for%20Public%20Policy%0AAuthor%3A%20Kit%20T.%20Rodolfa%20and%20Erika%20Salomon%20and%20Jin%20Yao%20and%20Steve%20Yoder%20and%20Robert%20Sullivan%20and%20Kevin%20McGuire%20and%20Allie%20Dickinson%20and%20Rob%20MacDougall%20and%20Brian%20Seidler%20and%20Christina%20Sung%20and%20Claire%20Herdeman%20and%20Rayid%20Ghani%0AAbstract%3A%20%20%20Many%20incarcerated%20individuals%20face%20significant%20and%20complex%20challenges%2C%0Aincluding%20mental%20illness%2C%20substance%20dependence%2C%20and%20homelessness%2C%20yet%20jails%20and%0Aprisons%20are%20often%20poorly%20equipped%20to%20address%20these%20needs.%20With%20little%20support%0Afrom%20the%20existing%20criminal%20justice%20system%2C%20these%20needs%20can%20remain%20untreated%20and%0Aworsen%2C%20often%20leading%20to%20further%20offenses%20and%20a%20cycle%20of%20incarceration%20with%0Aadverse%20outcomes%20both%20for%20the%20individual%20and%20for%20public%20safety%2C%20with%0Aparticularly%20large%20impacts%20on%20communities%20of%20color%20that%20continue%20to%20widen%20the%0Aalready%20extensive%20racial%20disparities%20in%20criminal%20justice%20outcomes.%20Responding%0Ato%20these%20failures%2C%20a%20growing%20number%20of%20criminal%20justice%20stakeholders%20are%0Aseeking%20to%20break%20this%20cycle%20through%20innovative%20approaches%20such%20as%0Acommunity-driven%20and%20alternative%20approaches%20to%20policing%2C%20mentoring%2C%20community%0Abuilding%2C%20restorative%20justice%2C%20pretrial%20diversion%2C%20holistic%20defense%2C%20and%20social%0Aservice%20connections.%20Here%20we%20report%20on%20a%20collaboration%20between%20Johnson%20County%2C%0AKansas%2C%20and%20Carnegie%20Mellon%20University%20to%20perform%20targeted%2C%20proactive%20mental%0Ahealth%20outreach%20in%20an%20effort%20to%20reduce%20reincarceration%20rates.%0A%20%20This%20paper%20describes%20the%20data%20used%2C%20our%20predictive%20modeling%20approach%20and%0Aresults%2C%20as%20well%20as%20the%20design%20and%20analysis%20of%20a%20field%20trial%20conducted%20to%0Aconfirm%20our%20model%27s%20predictive%20power%2C%20evaluate%20the%20impact%20of%20this%20targeted%0Aoutreach%2C%20and%20understand%20at%20what%20level%20of%20reincarceration%20risk%20outreach%20might%0Abe%20most%20effective.%20Through%20this%20trial%2C%20we%20find%20that%20our%20model%20is%20highly%0Apredictive%20of%20new%20jail%20bookings%2C%20with%20more%20than%20half%20of%20individuals%20in%20the%0Atrial%27s%20highest-risk%20group%20returning%20to%20jail%20in%20the%20following%20year.%20Outreach%0Awas%20most%20effective%20among%20these%20highest-risk%20individuals%2C%20with%20impacts%20on%20mental%0Ahealth%20utilization%2C%20EMS%20dispatches%2C%20and%20criminal%20justice%20involvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Cycle%2520of%2520Incarceration%2520With%2520Targeted%2520Mental%2520Health%250A%2520%2520Outreach%253A%2520A%2520Case%2520Study%2520in%2520Machine%2520Learning%2520for%2520Public%2520Policy%26entry.906535625%3DKit%2520T.%2520Rodolfa%2520and%2520Erika%2520Salomon%2520and%2520Jin%2520Yao%2520and%2520Steve%2520Yoder%2520and%2520Robert%2520Sullivan%2520and%2520Kevin%2520McGuire%2520and%2520Allie%2520Dickinson%2520and%2520Rob%2520MacDougall%2520and%2520Brian%2520Seidler%2520and%2520Christina%2520Sung%2520and%2520Claire%2520Herdeman%2520and%2520Rayid%2520Ghani%26entry.1292438233%3D%2520%2520Many%2520incarcerated%2520individuals%2520face%2520significant%2520and%2520complex%2520challenges%252C%250Aincluding%2520mental%2520illness%252C%2520substance%2520dependence%252C%2520and%2520homelessness%252C%2520yet%2520jails%2520and%250Aprisons%2520are%2520often%2520poorly%2520equipped%2520to%2520address%2520these%2520needs.%2520With%2520little%2520support%250Afrom%2520the%2520existing%2520criminal%2520justice%2520system%252C%2520these%2520needs%2520can%2520remain%2520untreated%2520and%250Aworsen%252C%2520often%2520leading%2520to%2520further%2520offenses%2520and%2520a%2520cycle%2520of%2520incarceration%2520with%250Aadverse%2520outcomes%2520both%2520for%2520the%2520individual%2520and%2520for%2520public%2520safety%252C%2520with%250Aparticularly%2520large%2520impacts%2520on%2520communities%2520of%2520color%2520that%2520continue%2520to%2520widen%2520the%250Aalready%2520extensive%2520racial%2520disparities%2520in%2520criminal%2520justice%2520outcomes.%2520Responding%250Ato%2520these%2520failures%252C%2520a%2520growing%2520number%2520of%2520criminal%2520justice%2520stakeholders%2520are%250Aseeking%2520to%2520break%2520this%2520cycle%2520through%2520innovative%2520approaches%2520such%2520as%250Acommunity-driven%2520and%2520alternative%2520approaches%2520to%2520policing%252C%2520mentoring%252C%2520community%250Abuilding%252C%2520restorative%2520justice%252C%2520pretrial%2520diversion%252C%2520holistic%2520defense%252C%2520and%2520social%250Aservice%2520connections.%2520Here%2520we%2520report%2520on%2520a%2520collaboration%2520between%2520Johnson%2520County%252C%250AKansas%252C%2520and%2520Carnegie%2520Mellon%2520University%2520to%2520perform%2520targeted%252C%2520proactive%2520mental%250Ahealth%2520outreach%2520in%2520an%2520effort%2520to%2520reduce%2520reincarceration%2520rates.%250A%2520%2520This%2520paper%2520describes%2520the%2520data%2520used%252C%2520our%2520predictive%2520modeling%2520approach%2520and%250Aresults%252C%2520as%2520well%2520as%2520the%2520design%2520and%2520analysis%2520of%2520a%2520field%2520trial%2520conducted%2520to%250Aconfirm%2520our%2520model%2527s%2520predictive%2520power%252C%2520evaluate%2520the%2520impact%2520of%2520this%2520targeted%250Aoutreach%252C%2520and%2520understand%2520at%2520what%2520level%2520of%2520reincarceration%2520risk%2520outreach%2520might%250Abe%2520most%2520effective.%2520Through%2520this%2520trial%252C%2520we%2520find%2520that%2520our%2520model%2520is%2520highly%250Apredictive%2520of%2520new%2520jail%2520bookings%252C%2520with%2520more%2520than%2520half%2520of%2520individuals%2520in%2520the%250Atrial%2527s%2520highest-risk%2520group%2520returning%2520to%2520jail%2520in%2520the%2520following%2520year.%2520Outreach%250Awas%2520most%2520effective%2520among%2520these%2520highest-risk%2520individuals%252C%2520with%2520impacts%2520on%2520mental%250Ahealth%2520utilization%252C%2520EMS%2520dispatches%252C%2520and%2520criminal%2520justice%2520involvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Cycle%20of%20Incarceration%20With%20Targeted%20Mental%20Health%0A%20%20Outreach%3A%20A%20Case%20Study%20in%20Machine%20Learning%20for%20Public%20Policy&entry.906535625=Kit%20T.%20Rodolfa%20and%20Erika%20Salomon%20and%20Jin%20Yao%20and%20Steve%20Yoder%20and%20Robert%20Sullivan%20and%20Kevin%20McGuire%20and%20Allie%20Dickinson%20and%20Rob%20MacDougall%20and%20Brian%20Seidler%20and%20Christina%20Sung%20and%20Claire%20Herdeman%20and%20Rayid%20Ghani&entry.1292438233=%20%20Many%20incarcerated%20individuals%20face%20significant%20and%20complex%20challenges%2C%0Aincluding%20mental%20illness%2C%20substance%20dependence%2C%20and%20homelessness%2C%20yet%20jails%20and%0Aprisons%20are%20often%20poorly%20equipped%20to%20address%20these%20needs.%20With%20little%20support%0Afrom%20the%20existing%20criminal%20justice%20system%2C%20these%20needs%20can%20remain%20untreated%20and%0Aworsen%2C%20often%20leading%20to%20further%20offenses%20and%20a%20cycle%20of%20incarceration%20with%0Aadverse%20outcomes%20both%20for%20the%20individual%20and%20for%20public%20safety%2C%20with%0Aparticularly%20large%20impacts%20on%20communities%20of%20color%20that%20continue%20to%20widen%20the%0Aalready%20extensive%20racial%20disparities%20in%20criminal%20justice%20outcomes.%20Responding%0Ato%20these%20failures%2C%20a%20growing%20number%20of%20criminal%20justice%20stakeholders%20are%0Aseeking%20to%20break%20this%20cycle%20through%20innovative%20approaches%20such%20as%0Acommunity-driven%20and%20alternative%20approaches%20to%20policing%2C%20mentoring%2C%20community%0Abuilding%2C%20restorative%20justice%2C%20pretrial%20diversion%2C%20holistic%20defense%2C%20and%20social%0Aservice%20connections.%20Here%20we%20report%20on%20a%20collaboration%20between%20Johnson%20County%2C%0AKansas%2C%20and%20Carnegie%20Mellon%20University%20to%20perform%20targeted%2C%20proactive%20mental%0Ahealth%20outreach%20in%20an%20effort%20to%20reduce%20reincarceration%20rates.%0A%20%20This%20paper%20describes%20the%20data%20used%2C%20our%20predictive%20modeling%20approach%20and%0Aresults%2C%20as%20well%20as%20the%20design%20and%20analysis%20of%20a%20field%20trial%20conducted%20to%0Aconfirm%20our%20model%27s%20predictive%20power%2C%20evaluate%20the%20impact%20of%20this%20targeted%0Aoutreach%2C%20and%20understand%20at%20what%20level%20of%20reincarceration%20risk%20outreach%20might%0Abe%20most%20effective.%20Through%20this%20trial%2C%20we%20find%20that%20our%20model%20is%20highly%0Apredictive%20of%20new%20jail%20bookings%2C%20with%20more%20than%20half%20of%20individuals%20in%20the%0Atrial%27s%20highest-risk%20group%20returning%20to%20jail%20in%20the%20following%20year.%20Outreach%0Awas%20most%20effective%20among%20these%20highest-risk%20individuals%2C%20with%20impacts%20on%20mental%0Ahealth%20utilization%2C%20EMS%20dispatches%2C%20and%20criminal%20justice%20involvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14129v1&entry.124074799=Read"},
{"title": "Synthesis and Perceptual Scaling of High Resolution Naturalistic Images\n  Using Stable Diffusion", "author": "Leonardo Pettini and Carsten Bogler and Christian Doeller and John-Dylan Haynes", "abstract": "  Naturalistic scenes are of key interest for visual perception, but\ncontrolling their perceptual and semantic properties is challenging. Previous\nwork on naturalistic scenes has frequently focused on collections of discrete\nimages with considerable physical differences between stimuli. However, it is\noften desirable to assess representations of naturalistic images that vary\nalong a continuum. Traditionally, perceptually continuous variations of\nnaturalistic stimuli have been obtained by morphing a source image into a\ntarget image. This produces transitions driven mainly by low-level physical\nfeatures and can result in semantically ambiguous outcomes. More recently,\ngenerative adversarial networks (GANs) have been used to generate continuous\nperceptual variations within a stimulus category. Here we extend and generalize\nthis approach using a different machine learning approach, a text-to-image\ndiffusion model (Stable Diffusion XL), to generate a freely customizable\nstimulus set of photorealistic images that are characterized by gradual\ntransitions, with each image representing a unique exemplar within a prompted\ncategory. We demonstrate the approach by generating a set of 108 object scenes\nfrom 6 categories. For each object scene, we generate 10 variants that are\nordered along a perceptual continuum. This ordering was first estimated using a\nmachine learning model of perceptual similarity (LPIPS) and then subsequently\nvalidated with a large online sample of human participants. In a subsequent\nexperiment we show that this ordering is also predictive of confusability of\nstimuli in a working memory experiment. Our image set is suited for studies\ninvestigating the graded encoding of naturalistic stimuli in visual perception,\nattention, and memory.\n", "link": "http://arxiv.org/abs/2410.13034v2", "date": "2025-09-17", "relevancy": 1.2312, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6297}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6154}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesis%20and%20Perceptual%20Scaling%20of%20High%20Resolution%20Naturalistic%20Images%0A%20%20Using%20Stable%20Diffusion&body=Title%3A%20Synthesis%20and%20Perceptual%20Scaling%20of%20High%20Resolution%20Naturalistic%20Images%0A%20%20Using%20Stable%20Diffusion%0AAuthor%3A%20Leonardo%20Pettini%20and%20Carsten%20Bogler%20and%20Christian%20Doeller%20and%20John-Dylan%20Haynes%0AAbstract%3A%20%20%20Naturalistic%20scenes%20are%20of%20key%20interest%20for%20visual%20perception%2C%20but%0Acontrolling%20their%20perceptual%20and%20semantic%20properties%20is%20challenging.%20Previous%0Awork%20on%20naturalistic%20scenes%20has%20frequently%20focused%20on%20collections%20of%20discrete%0Aimages%20with%20considerable%20physical%20differences%20between%20stimuli.%20However%2C%20it%20is%0Aoften%20desirable%20to%20assess%20representations%20of%20naturalistic%20images%20that%20vary%0Aalong%20a%20continuum.%20Traditionally%2C%20perceptually%20continuous%20variations%20of%0Anaturalistic%20stimuli%20have%20been%20obtained%20by%20morphing%20a%20source%20image%20into%20a%0Atarget%20image.%20This%20produces%20transitions%20driven%20mainly%20by%20low-level%20physical%0Afeatures%20and%20can%20result%20in%20semantically%20ambiguous%20outcomes.%20More%20recently%2C%0Agenerative%20adversarial%20networks%20%28GANs%29%20have%20been%20used%20to%20generate%20continuous%0Aperceptual%20variations%20within%20a%20stimulus%20category.%20Here%20we%20extend%20and%20generalize%0Athis%20approach%20using%20a%20different%20machine%20learning%20approach%2C%20a%20text-to-image%0Adiffusion%20model%20%28Stable%20Diffusion%20XL%29%2C%20to%20generate%20a%20freely%20customizable%0Astimulus%20set%20of%20photorealistic%20images%20that%20are%20characterized%20by%20gradual%0Atransitions%2C%20with%20each%20image%20representing%20a%20unique%20exemplar%20within%20a%20prompted%0Acategory.%20We%20demonstrate%20the%20approach%20by%20generating%20a%20set%20of%20108%20object%20scenes%0Afrom%206%20categories.%20For%20each%20object%20scene%2C%20we%20generate%2010%20variants%20that%20are%0Aordered%20along%20a%20perceptual%20continuum.%20This%20ordering%20was%20first%20estimated%20using%20a%0Amachine%20learning%20model%20of%20perceptual%20similarity%20%28LPIPS%29%20and%20then%20subsequently%0Avalidated%20with%20a%20large%20online%20sample%20of%20human%20participants.%20In%20a%20subsequent%0Aexperiment%20we%20show%20that%20this%20ordering%20is%20also%20predictive%20of%20confusability%20of%0Astimuli%20in%20a%20working%20memory%20experiment.%20Our%20image%20set%20is%20suited%20for%20studies%0Ainvestigating%20the%20graded%20encoding%20of%20naturalistic%20stimuli%20in%20visual%20perception%2C%0Aattention%2C%20and%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesis%2520and%2520Perceptual%2520Scaling%2520of%2520High%2520Resolution%2520Naturalistic%2520Images%250A%2520%2520Using%2520Stable%2520Diffusion%26entry.906535625%3DLeonardo%2520Pettini%2520and%2520Carsten%2520Bogler%2520and%2520Christian%2520Doeller%2520and%2520John-Dylan%2520Haynes%26entry.1292438233%3D%2520%2520Naturalistic%2520scenes%2520are%2520of%2520key%2520interest%2520for%2520visual%2520perception%252C%2520but%250Acontrolling%2520their%2520perceptual%2520and%2520semantic%2520properties%2520is%2520challenging.%2520Previous%250Awork%2520on%2520naturalistic%2520scenes%2520has%2520frequently%2520focused%2520on%2520collections%2520of%2520discrete%250Aimages%2520with%2520considerable%2520physical%2520differences%2520between%2520stimuli.%2520However%252C%2520it%2520is%250Aoften%2520desirable%2520to%2520assess%2520representations%2520of%2520naturalistic%2520images%2520that%2520vary%250Aalong%2520a%2520continuum.%2520Traditionally%252C%2520perceptually%2520continuous%2520variations%2520of%250Anaturalistic%2520stimuli%2520have%2520been%2520obtained%2520by%2520morphing%2520a%2520source%2520image%2520into%2520a%250Atarget%2520image.%2520This%2520produces%2520transitions%2520driven%2520mainly%2520by%2520low-level%2520physical%250Afeatures%2520and%2520can%2520result%2520in%2520semantically%2520ambiguous%2520outcomes.%2520More%2520recently%252C%250Agenerative%2520adversarial%2520networks%2520%2528GANs%2529%2520have%2520been%2520used%2520to%2520generate%2520continuous%250Aperceptual%2520variations%2520within%2520a%2520stimulus%2520category.%2520Here%2520we%2520extend%2520and%2520generalize%250Athis%2520approach%2520using%2520a%2520different%2520machine%2520learning%2520approach%252C%2520a%2520text-to-image%250Adiffusion%2520model%2520%2528Stable%2520Diffusion%2520XL%2529%252C%2520to%2520generate%2520a%2520freely%2520customizable%250Astimulus%2520set%2520of%2520photorealistic%2520images%2520that%2520are%2520characterized%2520by%2520gradual%250Atransitions%252C%2520with%2520each%2520image%2520representing%2520a%2520unique%2520exemplar%2520within%2520a%2520prompted%250Acategory.%2520We%2520demonstrate%2520the%2520approach%2520by%2520generating%2520a%2520set%2520of%2520108%2520object%2520scenes%250Afrom%25206%2520categories.%2520For%2520each%2520object%2520scene%252C%2520we%2520generate%252010%2520variants%2520that%2520are%250Aordered%2520along%2520a%2520perceptual%2520continuum.%2520This%2520ordering%2520was%2520first%2520estimated%2520using%2520a%250Amachine%2520learning%2520model%2520of%2520perceptual%2520similarity%2520%2528LPIPS%2529%2520and%2520then%2520subsequently%250Avalidated%2520with%2520a%2520large%2520online%2520sample%2520of%2520human%2520participants.%2520In%2520a%2520subsequent%250Aexperiment%2520we%2520show%2520that%2520this%2520ordering%2520is%2520also%2520predictive%2520of%2520confusability%2520of%250Astimuli%2520in%2520a%2520working%2520memory%2520experiment.%2520Our%2520image%2520set%2520is%2520suited%2520for%2520studies%250Ainvestigating%2520the%2520graded%2520encoding%2520of%2520naturalistic%2520stimuli%2520in%2520visual%2520perception%252C%250Aattention%252C%2520and%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesis%20and%20Perceptual%20Scaling%20of%20High%20Resolution%20Naturalistic%20Images%0A%20%20Using%20Stable%20Diffusion&entry.906535625=Leonardo%20Pettini%20and%20Carsten%20Bogler%20and%20Christian%20Doeller%20and%20John-Dylan%20Haynes&entry.1292438233=%20%20Naturalistic%20scenes%20are%20of%20key%20interest%20for%20visual%20perception%2C%20but%0Acontrolling%20their%20perceptual%20and%20semantic%20properties%20is%20challenging.%20Previous%0Awork%20on%20naturalistic%20scenes%20has%20frequently%20focused%20on%20collections%20of%20discrete%0Aimages%20with%20considerable%20physical%20differences%20between%20stimuli.%20However%2C%20it%20is%0Aoften%20desirable%20to%20assess%20representations%20of%20naturalistic%20images%20that%20vary%0Aalong%20a%20continuum.%20Traditionally%2C%20perceptually%20continuous%20variations%20of%0Anaturalistic%20stimuli%20have%20been%20obtained%20by%20morphing%20a%20source%20image%20into%20a%0Atarget%20image.%20This%20produces%20transitions%20driven%20mainly%20by%20low-level%20physical%0Afeatures%20and%20can%20result%20in%20semantically%20ambiguous%20outcomes.%20More%20recently%2C%0Agenerative%20adversarial%20networks%20%28GANs%29%20have%20been%20used%20to%20generate%20continuous%0Aperceptual%20variations%20within%20a%20stimulus%20category.%20Here%20we%20extend%20and%20generalize%0Athis%20approach%20using%20a%20different%20machine%20learning%20approach%2C%20a%20text-to-image%0Adiffusion%20model%20%28Stable%20Diffusion%20XL%29%2C%20to%20generate%20a%20freely%20customizable%0Astimulus%20set%20of%20photorealistic%20images%20that%20are%20characterized%20by%20gradual%0Atransitions%2C%20with%20each%20image%20representing%20a%20unique%20exemplar%20within%20a%20prompted%0Acategory.%20We%20demonstrate%20the%20approach%20by%20generating%20a%20set%20of%20108%20object%20scenes%0Afrom%206%20categories.%20For%20each%20object%20scene%2C%20we%20generate%2010%20variants%20that%20are%0Aordered%20along%20a%20perceptual%20continuum.%20This%20ordering%20was%20first%20estimated%20using%20a%0Amachine%20learning%20model%20of%20perceptual%20similarity%20%28LPIPS%29%20and%20then%20subsequently%0Avalidated%20with%20a%20large%20online%20sample%20of%20human%20participants.%20In%20a%20subsequent%0Aexperiment%20we%20show%20that%20this%20ordering%20is%20also%20predictive%20of%20confusability%20of%0Astimuli%20in%20a%20working%20memory%20experiment.%20Our%20image%20set%20is%20suited%20for%20studies%0Ainvestigating%20the%20graded%20encoding%20of%20naturalistic%20stimuli%20in%20visual%20perception%2C%0Aattention%2C%20and%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13034v2&entry.124074799=Read"},
{"title": "Efficient Tactile Perception with Soft Electrical Impedance Tomography\n  and Pre-trained Transformer", "author": "Huazhi Dong and Ronald B. Liu and Sihao Teng and Delin Hu and  Peisan and  E and Francesco Giorgio-Serchi and Yunjie Yang", "abstract": "  Tactile sensing is fundamental to robotic systems, enabling interactions\nthrough physical contact in multiple tasks. Despite its importance, achieving\nhigh-resolution, large-area tactile sensing remains challenging. Electrical\nImpedance Tomography (EIT) has emerged as a promising approach for large-area,\ndistributed tactile sensing with minimal electrode requirements which can lend\nitself to addressing complex contact problems in robotics. However, existing\nEIT-based tactile reconstruction methods often suffer from high computational\ncosts or depend on extensive annotated simulation datasets, hindering its\nviability in real-world settings. To address this shortcoming, here we propose\na Pre-trained Transformer for EIT-based Tactile Reconstruction (PTET), a\nlearning-based framework that bridges the simulation-to-reality gap by\nleveraging self-supervised pretraining on simulation data and fine-tuning with\nlimited real-world data. In simulations, PTET requires 99.44 percent fewer\nannotated samples than equivalent state-of-the-art approaches (2,500 vs.\n450,000 samples) while achieving reconstruction performance improvements of up\nto 43.57 percent under identical data conditions. Fine-tuning with real-world\ndata further enables PTET to overcome discrepancies between simulated and\nexperimental datasets, achieving superior reconstruction and detail recovery in\npractical scenarios. The improved reconstruction accuracy, data efficiency, and\nrobustness in real-world tasks establish it as a scalable and practical\nsolution for tactile sensing systems in robotics, especially for object\nhandling and adaptive grasping under varying pressure conditions.\n", "link": "http://arxiv.org/abs/2506.02824v2", "date": "2025-09-17", "relevancy": 1.5488, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5257}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Tactile%20Perception%20with%20Soft%20Electrical%20Impedance%20Tomography%0A%20%20and%20Pre-trained%20Transformer&body=Title%3A%20Efficient%20Tactile%20Perception%20with%20Soft%20Electrical%20Impedance%20Tomography%0A%20%20and%20Pre-trained%20Transformer%0AAuthor%3A%20Huazhi%20Dong%20and%20Ronald%20B.%20Liu%20and%20Sihao%20Teng%20and%20Delin%20Hu%20and%20%20Peisan%20and%20%20E%20and%20Francesco%20Giorgio-Serchi%20and%20Yunjie%20Yang%0AAbstract%3A%20%20%20Tactile%20sensing%20is%20fundamental%20to%20robotic%20systems%2C%20enabling%20interactions%0Athrough%20physical%20contact%20in%20multiple%20tasks.%20Despite%20its%20importance%2C%20achieving%0Ahigh-resolution%2C%20large-area%20tactile%20sensing%20remains%20challenging.%20Electrical%0AImpedance%20Tomography%20%28EIT%29%20has%20emerged%20as%20a%20promising%20approach%20for%20large-area%2C%0Adistributed%20tactile%20sensing%20with%20minimal%20electrode%20requirements%20which%20can%20lend%0Aitself%20to%20addressing%20complex%20contact%20problems%20in%20robotics.%20However%2C%20existing%0AEIT-based%20tactile%20reconstruction%20methods%20often%20suffer%20from%20high%20computational%0Acosts%20or%20depend%20on%20extensive%20annotated%20simulation%20datasets%2C%20hindering%20its%0Aviability%20in%20real-world%20settings.%20To%20address%20this%20shortcoming%2C%20here%20we%20propose%0Aa%20Pre-trained%20Transformer%20for%20EIT-based%20Tactile%20Reconstruction%20%28PTET%29%2C%20a%0Alearning-based%20framework%20that%20bridges%20the%20simulation-to-reality%20gap%20by%0Aleveraging%20self-supervised%20pretraining%20on%20simulation%20data%20and%20fine-tuning%20with%0Alimited%20real-world%20data.%20In%20simulations%2C%20PTET%20requires%2099.44%20percent%20fewer%0Aannotated%20samples%20than%20equivalent%20state-of-the-art%20approaches%20%282%2C500%20vs.%0A450%2C000%20samples%29%20while%20achieving%20reconstruction%20performance%20improvements%20of%20up%0Ato%2043.57%20percent%20under%20identical%20data%20conditions.%20Fine-tuning%20with%20real-world%0Adata%20further%20enables%20PTET%20to%20overcome%20discrepancies%20between%20simulated%20and%0Aexperimental%20datasets%2C%20achieving%20superior%20reconstruction%20and%20detail%20recovery%20in%0Apractical%20scenarios.%20The%20improved%20reconstruction%20accuracy%2C%20data%20efficiency%2C%20and%0Arobustness%20in%20real-world%20tasks%20establish%20it%20as%20a%20scalable%20and%20practical%0Asolution%20for%20tactile%20sensing%20systems%20in%20robotics%2C%20especially%20for%20object%0Ahandling%20and%20adaptive%20grasping%20under%20varying%20pressure%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Tactile%2520Perception%2520with%2520Soft%2520Electrical%2520Impedance%2520Tomography%250A%2520%2520and%2520Pre-trained%2520Transformer%26entry.906535625%3DHuazhi%2520Dong%2520and%2520Ronald%2520B.%2520Liu%2520and%2520Sihao%2520Teng%2520and%2520Delin%2520Hu%2520and%2520%2520Peisan%2520and%2520%2520E%2520and%2520Francesco%2520Giorgio-Serchi%2520and%2520Yunjie%2520Yang%26entry.1292438233%3D%2520%2520Tactile%2520sensing%2520is%2520fundamental%2520to%2520robotic%2520systems%252C%2520enabling%2520interactions%250Athrough%2520physical%2520contact%2520in%2520multiple%2520tasks.%2520Despite%2520its%2520importance%252C%2520achieving%250Ahigh-resolution%252C%2520large-area%2520tactile%2520sensing%2520remains%2520challenging.%2520Electrical%250AImpedance%2520Tomography%2520%2528EIT%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520large-area%252C%250Adistributed%2520tactile%2520sensing%2520with%2520minimal%2520electrode%2520requirements%2520which%2520can%2520lend%250Aitself%2520to%2520addressing%2520complex%2520contact%2520problems%2520in%2520robotics.%2520However%252C%2520existing%250AEIT-based%2520tactile%2520reconstruction%2520methods%2520often%2520suffer%2520from%2520high%2520computational%250Acosts%2520or%2520depend%2520on%2520extensive%2520annotated%2520simulation%2520datasets%252C%2520hindering%2520its%250Aviability%2520in%2520real-world%2520settings.%2520To%2520address%2520this%2520shortcoming%252C%2520here%2520we%2520propose%250Aa%2520Pre-trained%2520Transformer%2520for%2520EIT-based%2520Tactile%2520Reconstruction%2520%2528PTET%2529%252C%2520a%250Alearning-based%2520framework%2520that%2520bridges%2520the%2520simulation-to-reality%2520gap%2520by%250Aleveraging%2520self-supervised%2520pretraining%2520on%2520simulation%2520data%2520and%2520fine-tuning%2520with%250Alimited%2520real-world%2520data.%2520In%2520simulations%252C%2520PTET%2520requires%252099.44%2520percent%2520fewer%250Aannotated%2520samples%2520than%2520equivalent%2520state-of-the-art%2520approaches%2520%25282%252C500%2520vs.%250A450%252C000%2520samples%2529%2520while%2520achieving%2520reconstruction%2520performance%2520improvements%2520of%2520up%250Ato%252043.57%2520percent%2520under%2520identical%2520data%2520conditions.%2520Fine-tuning%2520with%2520real-world%250Adata%2520further%2520enables%2520PTET%2520to%2520overcome%2520discrepancies%2520between%2520simulated%2520and%250Aexperimental%2520datasets%252C%2520achieving%2520superior%2520reconstruction%2520and%2520detail%2520recovery%2520in%250Apractical%2520scenarios.%2520The%2520improved%2520reconstruction%2520accuracy%252C%2520data%2520efficiency%252C%2520and%250Arobustness%2520in%2520real-world%2520tasks%2520establish%2520it%2520as%2520a%2520scalable%2520and%2520practical%250Asolution%2520for%2520tactile%2520sensing%2520systems%2520in%2520robotics%252C%2520especially%2520for%2520object%250Ahandling%2520and%2520adaptive%2520grasping%2520under%2520varying%2520pressure%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Tactile%20Perception%20with%20Soft%20Electrical%20Impedance%20Tomography%0A%20%20and%20Pre-trained%20Transformer&entry.906535625=Huazhi%20Dong%20and%20Ronald%20B.%20Liu%20and%20Sihao%20Teng%20and%20Delin%20Hu%20and%20%20Peisan%20and%20%20E%20and%20Francesco%20Giorgio-Serchi%20and%20Yunjie%20Yang&entry.1292438233=%20%20Tactile%20sensing%20is%20fundamental%20to%20robotic%20systems%2C%20enabling%20interactions%0Athrough%20physical%20contact%20in%20multiple%20tasks.%20Despite%20its%20importance%2C%20achieving%0Ahigh-resolution%2C%20large-area%20tactile%20sensing%20remains%20challenging.%20Electrical%0AImpedance%20Tomography%20%28EIT%29%20has%20emerged%20as%20a%20promising%20approach%20for%20large-area%2C%0Adistributed%20tactile%20sensing%20with%20minimal%20electrode%20requirements%20which%20can%20lend%0Aitself%20to%20addressing%20complex%20contact%20problems%20in%20robotics.%20However%2C%20existing%0AEIT-based%20tactile%20reconstruction%20methods%20often%20suffer%20from%20high%20computational%0Acosts%20or%20depend%20on%20extensive%20annotated%20simulation%20datasets%2C%20hindering%20its%0Aviability%20in%20real-world%20settings.%20To%20address%20this%20shortcoming%2C%20here%20we%20propose%0Aa%20Pre-trained%20Transformer%20for%20EIT-based%20Tactile%20Reconstruction%20%28PTET%29%2C%20a%0Alearning-based%20framework%20that%20bridges%20the%20simulation-to-reality%20gap%20by%0Aleveraging%20self-supervised%20pretraining%20on%20simulation%20data%20and%20fine-tuning%20with%0Alimited%20real-world%20data.%20In%20simulations%2C%20PTET%20requires%2099.44%20percent%20fewer%0Aannotated%20samples%20than%20equivalent%20state-of-the-art%20approaches%20%282%2C500%20vs.%0A450%2C000%20samples%29%20while%20achieving%20reconstruction%20performance%20improvements%20of%20up%0Ato%2043.57%20percent%20under%20identical%20data%20conditions.%20Fine-tuning%20with%20real-world%0Adata%20further%20enables%20PTET%20to%20overcome%20discrepancies%20between%20simulated%20and%0Aexperimental%20datasets%2C%20achieving%20superior%20reconstruction%20and%20detail%20recovery%20in%0Apractical%20scenarios.%20The%20improved%20reconstruction%20accuracy%2C%20data%20efficiency%2C%20and%0Arobustness%20in%20real-world%20tasks%20establish%20it%20as%20a%20scalable%20and%20practical%0Asolution%20for%20tactile%20sensing%20systems%20in%20robotics%2C%20especially%20for%20object%0Ahandling%20and%20adaptive%20grasping%20under%20varying%20pressure%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02824v2&entry.124074799=Read"},
{"title": "Learning Multimodal Attention for Manipulating Deformable Objects with\n  Changing States", "author": "Namiko Saito and Mayu Tatsumi and Ayuna Kubo and Kanata Suzuki and Hiroshi Ito and Shigeki Sugano and Tetsuya Ogata", "abstract": "  To support humans in their daily lives, robots are required to autonomously\nlearn, adapt to objects and environments, and perform the appropriate actions.\nWe tackled on the task of cooking scrambled eggs using real ingredients, in\nwhich the robot needs to perceive the states of the egg and adjust stirring\nmovement in real time, while the egg is heated and the state changes\ncontinuously. In previous works, handling changing objects was found to be\nchallenging because sensory information includes dynamical, both important or\nnoisy information, and the modality which should be focused on changes every\ntime, making it difficult to realize both perception and motion generation in\nreal time. We propose a predictive recurrent neural network with an attention\nmechanism that can weigh the sensor input, distinguishing how important and\nreliable each modality is, that realize quick and efficient perception and\nmotion generation. The model is trained with learning from the demonstration,\nand allows the robot to acquire human-like skills. We validated the proposed\ntechnique using the robot, Dry-AIREC, and with our learning model, it could\nperform cooking eggs with unknown ingredients. The robot could change the\nmethod of stirring and direction depending on the status of the egg, as in the\nbeginning it stirs in the whole pot, then subsequently, after the egg started\nbeing heated, it starts flipping and splitting motion targeting specific areas,\nalthough we did not explicitly indicate them.\n", "link": "http://arxiv.org/abs/2309.14837v2", "date": "2025-09-17", "relevancy": 1.6813, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5832}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Multimodal%20Attention%20for%20Manipulating%20Deformable%20Objects%20with%0A%20%20Changing%20States&body=Title%3A%20Learning%20Multimodal%20Attention%20for%20Manipulating%20Deformable%20Objects%20with%0A%20%20Changing%20States%0AAuthor%3A%20Namiko%20Saito%20and%20Mayu%20Tatsumi%20and%20Ayuna%20Kubo%20and%20Kanata%20Suzuki%20and%20Hiroshi%20Ito%20and%20Shigeki%20Sugano%20and%20Tetsuya%20Ogata%0AAbstract%3A%20%20%20To%20support%20humans%20in%20their%20daily%20lives%2C%20robots%20are%20required%20to%20autonomously%0Alearn%2C%20adapt%20to%20objects%20and%20environments%2C%20and%20perform%20the%20appropriate%20actions.%0AWe%20tackled%20on%20the%20task%20of%20cooking%20scrambled%20eggs%20using%20real%20ingredients%2C%20in%0Awhich%20the%20robot%20needs%20to%20perceive%20the%20states%20of%20the%20egg%20and%20adjust%20stirring%0Amovement%20in%20real%20time%2C%20while%20the%20egg%20is%20heated%20and%20the%20state%20changes%0Acontinuously.%20In%20previous%20works%2C%20handling%20changing%20objects%20was%20found%20to%20be%0Achallenging%20because%20sensory%20information%20includes%20dynamical%2C%20both%20important%20or%0Anoisy%20information%2C%20and%20the%20modality%20which%20should%20be%20focused%20on%20changes%20every%0Atime%2C%20making%20it%20difficult%20to%20realize%20both%20perception%20and%20motion%20generation%20in%0Areal%20time.%20We%20propose%20a%20predictive%20recurrent%20neural%20network%20with%20an%20attention%0Amechanism%20that%20can%20weigh%20the%20sensor%20input%2C%20distinguishing%20how%20important%20and%0Areliable%20each%20modality%20is%2C%20that%20realize%20quick%20and%20efficient%20perception%20and%0Amotion%20generation.%20The%20model%20is%20trained%20with%20learning%20from%20the%20demonstration%2C%0Aand%20allows%20the%20robot%20to%20acquire%20human-like%20skills.%20We%20validated%20the%20proposed%0Atechnique%20using%20the%20robot%2C%20Dry-AIREC%2C%20and%20with%20our%20learning%20model%2C%20it%20could%0Aperform%20cooking%20eggs%20with%20unknown%20ingredients.%20The%20robot%20could%20change%20the%0Amethod%20of%20stirring%20and%20direction%20depending%20on%20the%20status%20of%20the%20egg%2C%20as%20in%20the%0Abeginning%20it%20stirs%20in%20the%20whole%20pot%2C%20then%20subsequently%2C%20after%20the%20egg%20started%0Abeing%20heated%2C%20it%20starts%20flipping%20and%20splitting%20motion%20targeting%20specific%20areas%2C%0Aalthough%20we%20did%20not%20explicitly%20indicate%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Multimodal%2520Attention%2520for%2520Manipulating%2520Deformable%2520Objects%2520with%250A%2520%2520Changing%2520States%26entry.906535625%3DNamiko%2520Saito%2520and%2520Mayu%2520Tatsumi%2520and%2520Ayuna%2520Kubo%2520and%2520Kanata%2520Suzuki%2520and%2520Hiroshi%2520Ito%2520and%2520Shigeki%2520Sugano%2520and%2520Tetsuya%2520Ogata%26entry.1292438233%3D%2520%2520To%2520support%2520humans%2520in%2520their%2520daily%2520lives%252C%2520robots%2520are%2520required%2520to%2520autonomously%250Alearn%252C%2520adapt%2520to%2520objects%2520and%2520environments%252C%2520and%2520perform%2520the%2520appropriate%2520actions.%250AWe%2520tackled%2520on%2520the%2520task%2520of%2520cooking%2520scrambled%2520eggs%2520using%2520real%2520ingredients%252C%2520in%250Awhich%2520the%2520robot%2520needs%2520to%2520perceive%2520the%2520states%2520of%2520the%2520egg%2520and%2520adjust%2520stirring%250Amovement%2520in%2520real%2520time%252C%2520while%2520the%2520egg%2520is%2520heated%2520and%2520the%2520state%2520changes%250Acontinuously.%2520In%2520previous%2520works%252C%2520handling%2520changing%2520objects%2520was%2520found%2520to%2520be%250Achallenging%2520because%2520sensory%2520information%2520includes%2520dynamical%252C%2520both%2520important%2520or%250Anoisy%2520information%252C%2520and%2520the%2520modality%2520which%2520should%2520be%2520focused%2520on%2520changes%2520every%250Atime%252C%2520making%2520it%2520difficult%2520to%2520realize%2520both%2520perception%2520and%2520motion%2520generation%2520in%250Areal%2520time.%2520We%2520propose%2520a%2520predictive%2520recurrent%2520neural%2520network%2520with%2520an%2520attention%250Amechanism%2520that%2520can%2520weigh%2520the%2520sensor%2520input%252C%2520distinguishing%2520how%2520important%2520and%250Areliable%2520each%2520modality%2520is%252C%2520that%2520realize%2520quick%2520and%2520efficient%2520perception%2520and%250Amotion%2520generation.%2520The%2520model%2520is%2520trained%2520with%2520learning%2520from%2520the%2520demonstration%252C%250Aand%2520allows%2520the%2520robot%2520to%2520acquire%2520human-like%2520skills.%2520We%2520validated%2520the%2520proposed%250Atechnique%2520using%2520the%2520robot%252C%2520Dry-AIREC%252C%2520and%2520with%2520our%2520learning%2520model%252C%2520it%2520could%250Aperform%2520cooking%2520eggs%2520with%2520unknown%2520ingredients.%2520The%2520robot%2520could%2520change%2520the%250Amethod%2520of%2520stirring%2520and%2520direction%2520depending%2520on%2520the%2520status%2520of%2520the%2520egg%252C%2520as%2520in%2520the%250Abeginning%2520it%2520stirs%2520in%2520the%2520whole%2520pot%252C%2520then%2520subsequently%252C%2520after%2520the%2520egg%2520started%250Abeing%2520heated%252C%2520it%2520starts%2520flipping%2520and%2520splitting%2520motion%2520targeting%2520specific%2520areas%252C%250Aalthough%2520we%2520did%2520not%2520explicitly%2520indicate%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Multimodal%20Attention%20for%20Manipulating%20Deformable%20Objects%20with%0A%20%20Changing%20States&entry.906535625=Namiko%20Saito%20and%20Mayu%20Tatsumi%20and%20Ayuna%20Kubo%20and%20Kanata%20Suzuki%20and%20Hiroshi%20Ito%20and%20Shigeki%20Sugano%20and%20Tetsuya%20Ogata&entry.1292438233=%20%20To%20support%20humans%20in%20their%20daily%20lives%2C%20robots%20are%20required%20to%20autonomously%0Alearn%2C%20adapt%20to%20objects%20and%20environments%2C%20and%20perform%20the%20appropriate%20actions.%0AWe%20tackled%20on%20the%20task%20of%20cooking%20scrambled%20eggs%20using%20real%20ingredients%2C%20in%0Awhich%20the%20robot%20needs%20to%20perceive%20the%20states%20of%20the%20egg%20and%20adjust%20stirring%0Amovement%20in%20real%20time%2C%20while%20the%20egg%20is%20heated%20and%20the%20state%20changes%0Acontinuously.%20In%20previous%20works%2C%20handling%20changing%20objects%20was%20found%20to%20be%0Achallenging%20because%20sensory%20information%20includes%20dynamical%2C%20both%20important%20or%0Anoisy%20information%2C%20and%20the%20modality%20which%20should%20be%20focused%20on%20changes%20every%0Atime%2C%20making%20it%20difficult%20to%20realize%20both%20perception%20and%20motion%20generation%20in%0Areal%20time.%20We%20propose%20a%20predictive%20recurrent%20neural%20network%20with%20an%20attention%0Amechanism%20that%20can%20weigh%20the%20sensor%20input%2C%20distinguishing%20how%20important%20and%0Areliable%20each%20modality%20is%2C%20that%20realize%20quick%20and%20efficient%20perception%20and%0Amotion%20generation.%20The%20model%20is%20trained%20with%20learning%20from%20the%20demonstration%2C%0Aand%20allows%20the%20robot%20to%20acquire%20human-like%20skills.%20We%20validated%20the%20proposed%0Atechnique%20using%20the%20robot%2C%20Dry-AIREC%2C%20and%20with%20our%20learning%20model%2C%20it%20could%0Aperform%20cooking%20eggs%20with%20unknown%20ingredients.%20The%20robot%20could%20change%20the%0Amethod%20of%20stirring%20and%20direction%20depending%20on%20the%20status%20of%20the%20egg%2C%20as%20in%20the%0Abeginning%20it%20stirs%20in%20the%20whole%20pot%2C%20then%20subsequently%2C%20after%20the%20egg%20started%0Abeing%20heated%2C%20it%20starts%20flipping%20and%20splitting%20motion%20targeting%20specific%20areas%2C%0Aalthough%20we%20did%20not%20explicitly%20indicate%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14837v2&entry.124074799=Read"},
{"title": "ModalSurv: A Multimodal Deep Survival Framework for Prostate and Bladder\n  Cancer", "author": "Noorul Wahab and Ethar Alzaid and Jiaqi Lv and Adam Shephard and Shan E Ahmed Raza", "abstract": "  Accurate prediction of time-to-event outcomes is a central challenge in\noncology, with significant implications for treatment planning and patient\nmanagement. In this work, we present ModaliSurv, a multimodal deep survival\nmodel utilising DeepHit with a projection layer and inter-modality\ncross-attention, which integrates heterogeneous patient data, including\nclinical, MRI, RNA-seq and whole-slide pathology features. The model is\ndesigned to capture complementary prognostic signals across modalities and\nestimate individualised time-to-biochemical recurrence in prostate cancer and\ntime-to-cancer recurrence in bladder cancer. Our approach was evaluated in the\ncontext of the CHIMERA Grand Challenge, across two of the three provided tasks.\nFor Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed\nframework achieved a concordance index (C-index) of 0.843 on 5-folds\ncross-validation and 0.818 on CHIMERA development set, demonstrating robust\ndiscriminatory ability. For Task 3 (bladder cancer recurrence prediction), the\nmodel obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on\ndevelopment set, highlighting its adaptability and potential for clinical\ntranslation. These results suggest that leveraging multimodal integration with\ndeep survival learning provides a promising pathway toward personalised risk\nstratification in prostate and bladder cancer. Beyond the challenge setting,\nour framework is broadly applicable to survival prediction tasks involving\nheterogeneous biomedical data.\n", "link": "http://arxiv.org/abs/2509.05037v3", "date": "2025-09-17", "relevancy": 1.6167, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ModalSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostate%20and%20Bladder%0A%20%20Cancer&body=Title%3A%20ModalSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostate%20and%20Bladder%0A%20%20Cancer%0AAuthor%3A%20Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20time-to-event%20outcomes%20is%20a%20central%20challenge%20in%0Aoncology%2C%20with%20significant%20implications%20for%20treatment%20planning%20and%20patient%0Amanagement.%20In%20this%20work%2C%20we%20present%20ModaliSurv%2C%20a%20multimodal%20deep%20survival%0Amodel%20utilising%20DeepHit%20with%20a%20projection%20layer%20and%20inter-modality%0Across-attention%2C%20which%20integrates%20heterogeneous%20patient%20data%2C%20including%0Aclinical%2C%20MRI%2C%20RNA-seq%20and%20whole-slide%20pathology%20features.%20The%20model%20is%0Adesigned%20to%20capture%20complementary%20prognostic%20signals%20across%20modalities%20and%0Aestimate%20individualised%20time-to-biochemical%20recurrence%20in%20prostate%20cancer%20and%0Atime-to-cancer%20recurrence%20in%20bladder%20cancer.%20Our%20approach%20was%20evaluated%20in%20the%0Acontext%20of%20the%20CHIMERA%20Grand%20Challenge%2C%20across%20two%20of%20the%20three%20provided%20tasks.%0AFor%20Task%201%20%28prostate%20cancer%20bio-chemical%20recurrence%20prediction%29%2C%20the%20proposed%0Aframework%20achieved%20a%20concordance%20index%20%28C-index%29%20of%200.843%20on%205-folds%0Across-validation%20and%200.818%20on%20CHIMERA%20development%20set%2C%20demonstrating%20robust%0Adiscriminatory%20ability.%20For%20Task%203%20%28bladder%20cancer%20recurrence%20prediction%29%2C%20the%0Amodel%20obtained%20a%20C-index%20of%200.662%20on%205-folds%20cross-validation%20and%200.457%20on%0Adevelopment%20set%2C%20highlighting%20its%20adaptability%20and%20potential%20for%20clinical%0Atranslation.%20These%20results%20suggest%20that%20leveraging%20multimodal%20integration%20with%0Adeep%20survival%20learning%20provides%20a%20promising%20pathway%20toward%20personalised%20risk%0Astratification%20in%20prostate%20and%20bladder%20cancer.%20Beyond%20the%20challenge%20setting%2C%0Aour%20framework%20is%20broadly%20applicable%20to%20survival%20prediction%20tasks%20involving%0Aheterogeneous%20biomedical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05037v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModalSurv%253A%2520A%2520Multimodal%2520Deep%2520Survival%2520Framework%2520for%2520Prostate%2520and%2520Bladder%250A%2520%2520Cancer%26entry.906535625%3DNoorul%2520Wahab%2520and%2520Ethar%2520Alzaid%2520and%2520Jiaqi%2520Lv%2520and%2520Adam%2520Shephard%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520time-to-event%2520outcomes%2520is%2520a%2520central%2520challenge%2520in%250Aoncology%252C%2520with%2520significant%2520implications%2520for%2520treatment%2520planning%2520and%2520patient%250Amanagement.%2520In%2520this%2520work%252C%2520we%2520present%2520ModaliSurv%252C%2520a%2520multimodal%2520deep%2520survival%250Amodel%2520utilising%2520DeepHit%2520with%2520a%2520projection%2520layer%2520and%2520inter-modality%250Across-attention%252C%2520which%2520integrates%2520heterogeneous%2520patient%2520data%252C%2520including%250Aclinical%252C%2520MRI%252C%2520RNA-seq%2520and%2520whole-slide%2520pathology%2520features.%2520The%2520model%2520is%250Adesigned%2520to%2520capture%2520complementary%2520prognostic%2520signals%2520across%2520modalities%2520and%250Aestimate%2520individualised%2520time-to-biochemical%2520recurrence%2520in%2520prostate%2520cancer%2520and%250Atime-to-cancer%2520recurrence%2520in%2520bladder%2520cancer.%2520Our%2520approach%2520was%2520evaluated%2520in%2520the%250Acontext%2520of%2520the%2520CHIMERA%2520Grand%2520Challenge%252C%2520across%2520two%2520of%2520the%2520three%2520provided%2520tasks.%250AFor%2520Task%25201%2520%2528prostate%2520cancer%2520bio-chemical%2520recurrence%2520prediction%2529%252C%2520the%2520proposed%250Aframework%2520achieved%2520a%2520concordance%2520index%2520%2528C-index%2529%2520of%25200.843%2520on%25205-folds%250Across-validation%2520and%25200.818%2520on%2520CHIMERA%2520development%2520set%252C%2520demonstrating%2520robust%250Adiscriminatory%2520ability.%2520For%2520Task%25203%2520%2528bladder%2520cancer%2520recurrence%2520prediction%2529%252C%2520the%250Amodel%2520obtained%2520a%2520C-index%2520of%25200.662%2520on%25205-folds%2520cross-validation%2520and%25200.457%2520on%250Adevelopment%2520set%252C%2520highlighting%2520its%2520adaptability%2520and%2520potential%2520for%2520clinical%250Atranslation.%2520These%2520results%2520suggest%2520that%2520leveraging%2520multimodal%2520integration%2520with%250Adeep%2520survival%2520learning%2520provides%2520a%2520promising%2520pathway%2520toward%2520personalised%2520risk%250Astratification%2520in%2520prostate%2520and%2520bladder%2520cancer.%2520Beyond%2520the%2520challenge%2520setting%252C%250Aour%2520framework%2520is%2520broadly%2520applicable%2520to%2520survival%2520prediction%2520tasks%2520involving%250Aheterogeneous%2520biomedical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05037v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModalSurv%3A%20A%20Multimodal%20Deep%20Survival%20Framework%20for%20Prostate%20and%20Bladder%0A%20%20Cancer&entry.906535625=Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Accurate%20prediction%20of%20time-to-event%20outcomes%20is%20a%20central%20challenge%20in%0Aoncology%2C%20with%20significant%20implications%20for%20treatment%20planning%20and%20patient%0Amanagement.%20In%20this%20work%2C%20we%20present%20ModaliSurv%2C%20a%20multimodal%20deep%20survival%0Amodel%20utilising%20DeepHit%20with%20a%20projection%20layer%20and%20inter-modality%0Across-attention%2C%20which%20integrates%20heterogeneous%20patient%20data%2C%20including%0Aclinical%2C%20MRI%2C%20RNA-seq%20and%20whole-slide%20pathology%20features.%20The%20model%20is%0Adesigned%20to%20capture%20complementary%20prognostic%20signals%20across%20modalities%20and%0Aestimate%20individualised%20time-to-biochemical%20recurrence%20in%20prostate%20cancer%20and%0Atime-to-cancer%20recurrence%20in%20bladder%20cancer.%20Our%20approach%20was%20evaluated%20in%20the%0Acontext%20of%20the%20CHIMERA%20Grand%20Challenge%2C%20across%20two%20of%20the%20three%20provided%20tasks.%0AFor%20Task%201%20%28prostate%20cancer%20bio-chemical%20recurrence%20prediction%29%2C%20the%20proposed%0Aframework%20achieved%20a%20concordance%20index%20%28C-index%29%20of%200.843%20on%205-folds%0Across-validation%20and%200.818%20on%20CHIMERA%20development%20set%2C%20demonstrating%20robust%0Adiscriminatory%20ability.%20For%20Task%203%20%28bladder%20cancer%20recurrence%20prediction%29%2C%20the%0Amodel%20obtained%20a%20C-index%20of%200.662%20on%205-folds%20cross-validation%20and%200.457%20on%0Adevelopment%20set%2C%20highlighting%20its%20adaptability%20and%20potential%20for%20clinical%0Atranslation.%20These%20results%20suggest%20that%20leveraging%20multimodal%20integration%20with%0Adeep%20survival%20learning%20provides%20a%20promising%20pathway%20toward%20personalised%20risk%0Astratification%20in%20prostate%20and%20bladder%20cancer.%20Beyond%20the%20challenge%20setting%2C%0Aour%20framework%20is%20broadly%20applicable%20to%20survival%20prediction%20tasks%20involving%0Aheterogeneous%20biomedical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05037v3&entry.124074799=Read"},
{"title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient\n  Sequence Modeling", "author": "M\u00f3nika Farsang and Ramin Hasani and Daniela Rus and Radu Grosu", "abstract": "  We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes\nlong sequences as fast as today's linear state-space layers. By forcing the\nJacobian matrix to be diagonal, the full sequence can be solved in parallel,\ngiving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$\nsequential depth, for input-sequence length $T$ and a state dimension $D$.\nMoreover, LrcSSM offers a formal gradient-stability guarantee that other\ninput-varying systems such as Liquid-S4 and Mamba do not provide. Importantly,\nthe diagonal Jacobian structure of our model results in no performance loss\ncompared to the original model with dense Jacobian, and the approach can be\ngeneralized to other non-linear recurrent models, demonstrating broader\napplicability. On a suite of long-range forecasting tasks, we demonstrate that\nLrcSSM outperforms Transformers, LRU, S5, and Mamba.\n", "link": "http://arxiv.org/abs/2505.21717v4", "date": "2025-09-17", "relevancy": 1.544, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5485}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5255}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Up%20Liquid-Resistance%20Liquid-Capacitance%20Networks%20for%20Efficient%0A%20%20Sequence%20Modeling&body=Title%3A%20Scaling%20Up%20Liquid-Resistance%20Liquid-Capacitance%20Networks%20for%20Efficient%0A%20%20Sequence%20Modeling%0AAuthor%3A%20M%C3%B3nika%20Farsang%20and%20Ramin%20Hasani%20and%20Daniela%20Rus%20and%20Radu%20Grosu%0AAbstract%3A%20%20%20We%20present%20LrcSSM%2C%20a%20%24%5Ctextit%7Bnon-linear%7D%24%20recurrent%20model%20that%20processes%0Along%20sequences%20as%20fast%20as%20today%27s%20linear%20state-space%20layers.%20By%20forcing%20the%0AJacobian%20matrix%20to%20be%20diagonal%2C%20the%20full%20sequence%20can%20be%20solved%20in%20parallel%2C%0Agiving%20%24%5Cmathcal%7BO%7D%28TD%29%24%20time%20and%20memory%20and%20only%20%24%5Cmathcal%7BO%7D%28%5Clog%20T%29%24%0Asequential%20depth%2C%20for%20input-sequence%20length%20%24T%24%20and%20a%20state%20dimension%20%24D%24.%0AMoreover%2C%20LrcSSM%20offers%20a%20formal%20gradient-stability%20guarantee%20that%20other%0Ainput-varying%20systems%20such%20as%20Liquid-S4%20and%20Mamba%20do%20not%20provide.%20Importantly%2C%0Athe%20diagonal%20Jacobian%20structure%20of%20our%20model%20results%20in%20no%20performance%20loss%0Acompared%20to%20the%20original%20model%20with%20dense%20Jacobian%2C%20and%20the%20approach%20can%20be%0Ageneralized%20to%20other%20non-linear%20recurrent%20models%2C%20demonstrating%20broader%0Aapplicability.%20On%20a%20suite%20of%20long-range%20forecasting%20tasks%2C%20we%20demonstrate%20that%0ALrcSSM%20outperforms%20Transformers%2C%20LRU%2C%20S5%2C%20and%20Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21717v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Up%2520Liquid-Resistance%2520Liquid-Capacitance%2520Networks%2520for%2520Efficient%250A%2520%2520Sequence%2520Modeling%26entry.906535625%3DM%25C3%25B3nika%2520Farsang%2520and%2520Ramin%2520Hasani%2520and%2520Daniela%2520Rus%2520and%2520Radu%2520Grosu%26entry.1292438233%3D%2520%2520We%2520present%2520LrcSSM%252C%2520a%2520%2524%255Ctextit%257Bnon-linear%257D%2524%2520recurrent%2520model%2520that%2520processes%250Along%2520sequences%2520as%2520fast%2520as%2520today%2527s%2520linear%2520state-space%2520layers.%2520By%2520forcing%2520the%250AJacobian%2520matrix%2520to%2520be%2520diagonal%252C%2520the%2520full%2520sequence%2520can%2520be%2520solved%2520in%2520parallel%252C%250Agiving%2520%2524%255Cmathcal%257BO%257D%2528TD%2529%2524%2520time%2520and%2520memory%2520and%2520only%2520%2524%255Cmathcal%257BO%257D%2528%255Clog%2520T%2529%2524%250Asequential%2520depth%252C%2520for%2520input-sequence%2520length%2520%2524T%2524%2520and%2520a%2520state%2520dimension%2520%2524D%2524.%250AMoreover%252C%2520LrcSSM%2520offers%2520a%2520formal%2520gradient-stability%2520guarantee%2520that%2520other%250Ainput-varying%2520systems%2520such%2520as%2520Liquid-S4%2520and%2520Mamba%2520do%2520not%2520provide.%2520Importantly%252C%250Athe%2520diagonal%2520Jacobian%2520structure%2520of%2520our%2520model%2520results%2520in%2520no%2520performance%2520loss%250Acompared%2520to%2520the%2520original%2520model%2520with%2520dense%2520Jacobian%252C%2520and%2520the%2520approach%2520can%2520be%250Ageneralized%2520to%2520other%2520non-linear%2520recurrent%2520models%252C%2520demonstrating%2520broader%250Aapplicability.%2520On%2520a%2520suite%2520of%2520long-range%2520forecasting%2520tasks%252C%2520we%2520demonstrate%2520that%250ALrcSSM%2520outperforms%2520Transformers%252C%2520LRU%252C%2520S5%252C%2520and%2520Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21717v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Up%20Liquid-Resistance%20Liquid-Capacitance%20Networks%20for%20Efficient%0A%20%20Sequence%20Modeling&entry.906535625=M%C3%B3nika%20Farsang%20and%20Ramin%20Hasani%20and%20Daniela%20Rus%20and%20Radu%20Grosu&entry.1292438233=%20%20We%20present%20LrcSSM%2C%20a%20%24%5Ctextit%7Bnon-linear%7D%24%20recurrent%20model%20that%20processes%0Along%20sequences%20as%20fast%20as%20today%27s%20linear%20state-space%20layers.%20By%20forcing%20the%0AJacobian%20matrix%20to%20be%20diagonal%2C%20the%20full%20sequence%20can%20be%20solved%20in%20parallel%2C%0Agiving%20%24%5Cmathcal%7BO%7D%28TD%29%24%20time%20and%20memory%20and%20only%20%24%5Cmathcal%7BO%7D%28%5Clog%20T%29%24%0Asequential%20depth%2C%20for%20input-sequence%20length%20%24T%24%20and%20a%20state%20dimension%20%24D%24.%0AMoreover%2C%20LrcSSM%20offers%20a%20formal%20gradient-stability%20guarantee%20that%20other%0Ainput-varying%20systems%20such%20as%20Liquid-S4%20and%20Mamba%20do%20not%20provide.%20Importantly%2C%0Athe%20diagonal%20Jacobian%20structure%20of%20our%20model%20results%20in%20no%20performance%20loss%0Acompared%20to%20the%20original%20model%20with%20dense%20Jacobian%2C%20and%20the%20approach%20can%20be%0Ageneralized%20to%20other%20non-linear%20recurrent%20models%2C%20demonstrating%20broader%0Aapplicability.%20On%20a%20suite%20of%20long-range%20forecasting%20tasks%2C%20we%20demonstrate%20that%0ALrcSSM%20outperforms%20Transformers%2C%20LRU%2C%20S5%2C%20and%20Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21717v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


