<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240430.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Naturally Supervised 3D Visual Grounding with Language-Regularized\n  Concept Learners", "author": "Chun Feng and Joy Hsu and Weiyu Liu and Jiajun Wu", "abstract": "  3D visual grounding is a challenging task that often requires direct and\ndense supervision, notably the semantic label for each object in the scene. In\nthis paper, we instead study the naturally supervised setting that learns from\nonly 3D scene and QA pairs, where prior works underperform. We propose the\nLanguage-Regularized Concept Learner (LARC), which uses constraints from\nlanguage as regularization to significantly improve the accuracy of\nneuro-symbolic concept learners in the naturally supervised setting. Our\napproach is based on two core insights: the first is that language constraints\n(e.g., a word's relation to another) can serve as effective regularization for\nstructured representations in neuro-symbolic models; the second is that we can\nquery large language models to distill such constraints from language\nproperties. We show that LARC improves performance of prior works in naturally\nsupervised 3D visual grounding, and demonstrates a wide range of 3D visual\nreasoning capabilities-from zero-shot composition, to data efficiency and\ntransferability. Our method represents a promising step towards regularizing\nstructured visual reasoning frameworks with language-based priors, for learning\nin settings without dense supervision.\n", "link": "http://arxiv.org/abs/2404.19696v1", "date": "2024-04-30", "relevancy": 2.8938, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.605}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5576}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Naturally%20Supervised%203D%20Visual%20Grounding%20with%20Language-Regularized%0A%20%20Concept%20Learners&body=Title%3A%20Naturally%20Supervised%203D%20Visual%20Grounding%20with%20Language-Regularized%0A%20%20Concept%20Learners%0AAuthor%3A%20Chun%20Feng%20and%20Joy%20Hsu%20and%20Weiyu%20Liu%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%203D%20visual%20grounding%20is%20a%20challenging%20task%20that%20often%20requires%20direct%20and%0Adense%20supervision%2C%20notably%20the%20semantic%20label%20for%20each%20object%20in%20the%20scene.%20In%0Athis%20paper%2C%20we%20instead%20study%20the%20naturally%20supervised%20setting%20that%20learns%20from%0Aonly%203D%20scene%20and%20QA%20pairs%2C%20where%20prior%20works%20underperform.%20We%20propose%20the%0ALanguage-Regularized%20Concept%20Learner%20%28LARC%29%2C%20which%20uses%20constraints%20from%0Alanguage%20as%20regularization%20to%20significantly%20improve%20the%20accuracy%20of%0Aneuro-symbolic%20concept%20learners%20in%20the%20naturally%20supervised%20setting.%20Our%0Aapproach%20is%20based%20on%20two%20core%20insights%3A%20the%20first%20is%20that%20language%20constraints%0A%28e.g.%2C%20a%20word%27s%20relation%20to%20another%29%20can%20serve%20as%20effective%20regularization%20for%0Astructured%20representations%20in%20neuro-symbolic%20models%3B%20the%20second%20is%20that%20we%20can%0Aquery%20large%20language%20models%20to%20distill%20such%20constraints%20from%20language%0Aproperties.%20We%20show%20that%20LARC%20improves%20performance%20of%20prior%20works%20in%20naturally%0Asupervised%203D%20visual%20grounding%2C%20and%20demonstrates%20a%20wide%20range%20of%203D%20visual%0Areasoning%20capabilities-from%20zero-shot%20composition%2C%20to%20data%20efficiency%20and%0Atransferability.%20Our%20method%20represents%20a%20promising%20step%20towards%20regularizing%0Astructured%20visual%20reasoning%20frameworks%20with%20language-based%20priors%2C%20for%20learning%0Ain%20settings%20without%20dense%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Naturally%20Supervised%203D%20Visual%20Grounding%20with%20Language-Regularized%0A%20%20Concept%20Learners&entry.906535625=Chun%20Feng%20and%20Joy%20Hsu%20and%20Weiyu%20Liu%20and%20Jiajun%20Wu&entry.1292438233=%20%203D%20visual%20grounding%20is%20a%20challenging%20task%20that%20often%20requires%20direct%20and%0Adense%20supervision%2C%20notably%20the%20semantic%20label%20for%20each%20object%20in%20the%20scene.%20In%0Athis%20paper%2C%20we%20instead%20study%20the%20naturally%20supervised%20setting%20that%20learns%20from%0Aonly%203D%20scene%20and%20QA%20pairs%2C%20where%20prior%20works%20underperform.%20We%20propose%20the%0ALanguage-Regularized%20Concept%20Learner%20%28LARC%29%2C%20which%20uses%20constraints%20from%0Alanguage%20as%20regularization%20to%20significantly%20improve%20the%20accuracy%20of%0Aneuro-symbolic%20concept%20learners%20in%20the%20naturally%20supervised%20setting.%20Our%0Aapproach%20is%20based%20on%20two%20core%20insights%3A%20the%20first%20is%20that%20language%20constraints%0A%28e.g.%2C%20a%20word%27s%20relation%20to%20another%29%20can%20serve%20as%20effective%20regularization%20for%0Astructured%20representations%20in%20neuro-symbolic%20models%3B%20the%20second%20is%20that%20we%20can%0Aquery%20large%20language%20models%20to%20distill%20such%20constraints%20from%20language%0Aproperties.%20We%20show%20that%20LARC%20improves%20performance%20of%20prior%20works%20in%20naturally%0Asupervised%203D%20visual%20grounding%2C%20and%20demonstrates%20a%20wide%20range%20of%203D%20visual%0Areasoning%20capabilities-from%20zero-shot%20composition%2C%20to%20data%20efficiency%20and%0Atransferability.%20Our%20method%20represents%20a%20promising%20step%20towards%20regularizing%0Astructured%20visual%20reasoning%20frameworks%20with%20language-based%20priors%2C%20for%20learning%0Ain%20settings%20without%20dense%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19696v1&entry.124074799=Read"},
{"title": "ESP-Zero: Unsupervised enhancement of zero-shot classification for\n  Extremely Sparse Point cloud", "author": "Jiayi Han and Zidi Cao and Weibo Zheng and Xiangguo Zhou and Xiangjian He and Yuanfang Zhang and Daisen Wei", "abstract": "  In recent years, zero-shot learning has attracted the focus of many\nresearchers, due to its flexibility and generality. Many approaches have been\nproposed to achieve the zero-shot classification of the point clouds for 3D\nobject understanding, following the schema of CLIP. However, in the real world,\nthe point clouds could be extremely sparse, dramatically limiting the\neffectiveness of the 3D point cloud encoders, and resulting in the misalignment\nof point cloud features and text embeddings. To the point cloud encoders to fit\nthe extremely sparse point clouds without re-running the pre-training procedure\nwhich could be time-consuming and expensive, in this work, we propose an\nunsupervised model adaptation approach to enhance the point cloud encoder for\nthe extremely sparse point clouds. We propose a novel fused-cross attention\nlayer that expands the pre-trained self-attention layer with additional\nlearnable tokens and attention blocks, which effectively modifies the point\ncloud features while maintaining the alignment between point cloud features and\ntext embeddings. We also propose a complementary learning-based\nself-distillation schema that encourages the modified features to be pulled\napart from the irrelevant text embeddings without overfitting the feature space\nto the observed text embeddings. Extensive experiments demonstrate that the\nproposed approach effectively increases the zero-shot capability on extremely\nsparse point clouds, and overwhelms other state-of-the-art model adaptation\napproaches.\n", "link": "http://arxiv.org/abs/2404.19639v1", "date": "2024-04-30", "relevancy": 2.7786, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5917}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ESP-Zero%3A%20Unsupervised%20enhancement%20of%20zero-shot%20classification%20for%0A%20%20Extremely%20Sparse%20Point%20cloud&body=Title%3A%20ESP-Zero%3A%20Unsupervised%20enhancement%20of%20zero-shot%20classification%20for%0A%20%20Extremely%20Sparse%20Point%20cloud%0AAuthor%3A%20Jiayi%20Han%20and%20Zidi%20Cao%20and%20Weibo%20Zheng%20and%20Xiangguo%20Zhou%20and%20Xiangjian%20He%20and%20Yuanfang%20Zhang%20and%20Daisen%20Wei%0AAbstract%3A%20%20%20In%20recent%20years%2C%20zero-shot%20learning%20has%20attracted%20the%20focus%20of%20many%0Aresearchers%2C%20due%20to%20its%20flexibility%20and%20generality.%20Many%20approaches%20have%20been%0Aproposed%20to%20achieve%20the%20zero-shot%20classification%20of%20the%20point%20clouds%20for%203D%0Aobject%20understanding%2C%20following%20the%20schema%20of%20CLIP.%20However%2C%20in%20the%20real%20world%2C%0Athe%20point%20clouds%20could%20be%20extremely%20sparse%2C%20dramatically%20limiting%20the%0Aeffectiveness%20of%20the%203D%20point%20cloud%20encoders%2C%20and%20resulting%20in%20the%20misalignment%0Aof%20point%20cloud%20features%20and%20text%20embeddings.%20To%20the%20point%20cloud%20encoders%20to%20fit%0Athe%20extremely%20sparse%20point%20clouds%20without%20re-running%20the%20pre-training%20procedure%0Awhich%20could%20be%20time-consuming%20and%20expensive%2C%20in%20this%20work%2C%20we%20propose%20an%0Aunsupervised%20model%20adaptation%20approach%20to%20enhance%20the%20point%20cloud%20encoder%20for%0Athe%20extremely%20sparse%20point%20clouds.%20We%20propose%20a%20novel%20fused-cross%20attention%0Alayer%20that%20expands%20the%20pre-trained%20self-attention%20layer%20with%20additional%0Alearnable%20tokens%20and%20attention%20blocks%2C%20which%20effectively%20modifies%20the%20point%0Acloud%20features%20while%20maintaining%20the%20alignment%20between%20point%20cloud%20features%20and%0Atext%20embeddings.%20We%20also%20propose%20a%20complementary%20learning-based%0Aself-distillation%20schema%20that%20encourages%20the%20modified%20features%20to%20be%20pulled%0Aapart%20from%20the%20irrelevant%20text%20embeddings%20without%20overfitting%20the%20feature%20space%0Ato%20the%20observed%20text%20embeddings.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20approach%20effectively%20increases%20the%20zero-shot%20capability%20on%20extremely%0Asparse%20point%20clouds%2C%20and%20overwhelms%20other%20state-of-the-art%20model%20adaptation%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESP-Zero%3A%20Unsupervised%20enhancement%20of%20zero-shot%20classification%20for%0A%20%20Extremely%20Sparse%20Point%20cloud&entry.906535625=Jiayi%20Han%20and%20Zidi%20Cao%20and%20Weibo%20Zheng%20and%20Xiangguo%20Zhou%20and%20Xiangjian%20He%20and%20Yuanfang%20Zhang%20and%20Daisen%20Wei&entry.1292438233=%20%20In%20recent%20years%2C%20zero-shot%20learning%20has%20attracted%20the%20focus%20of%20many%0Aresearchers%2C%20due%20to%20its%20flexibility%20and%20generality.%20Many%20approaches%20have%20been%0Aproposed%20to%20achieve%20the%20zero-shot%20classification%20of%20the%20point%20clouds%20for%203D%0Aobject%20understanding%2C%20following%20the%20schema%20of%20CLIP.%20However%2C%20in%20the%20real%20world%2C%0Athe%20point%20clouds%20could%20be%20extremely%20sparse%2C%20dramatically%20limiting%20the%0Aeffectiveness%20of%20the%203D%20point%20cloud%20encoders%2C%20and%20resulting%20in%20the%20misalignment%0Aof%20point%20cloud%20features%20and%20text%20embeddings.%20To%20the%20point%20cloud%20encoders%20to%20fit%0Athe%20extremely%20sparse%20point%20clouds%20without%20re-running%20the%20pre-training%20procedure%0Awhich%20could%20be%20time-consuming%20and%20expensive%2C%20in%20this%20work%2C%20we%20propose%20an%0Aunsupervised%20model%20adaptation%20approach%20to%20enhance%20the%20point%20cloud%20encoder%20for%0Athe%20extremely%20sparse%20point%20clouds.%20We%20propose%20a%20novel%20fused-cross%20attention%0Alayer%20that%20expands%20the%20pre-trained%20self-attention%20layer%20with%20additional%0Alearnable%20tokens%20and%20attention%20blocks%2C%20which%20effectively%20modifies%20the%20point%0Acloud%20features%20while%20maintaining%20the%20alignment%20between%20point%20cloud%20features%20and%0Atext%20embeddings.%20We%20also%20propose%20a%20complementary%20learning-based%0Aself-distillation%20schema%20that%20encourages%20the%20modified%20features%20to%20be%20pulled%0Aapart%20from%20the%20irrelevant%20text%20embeddings%20without%20overfitting%20the%20feature%20space%0Ato%20the%20observed%20text%20embeddings.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20approach%20effectively%20increases%20the%20zero-shot%20capability%20on%20extremely%0Asparse%20point%20clouds%2C%20and%20overwhelms%20other%20state-of-the-art%20model%20adaptation%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19639v1&entry.124074799=Read"},
{"title": "Learning Separable Hidden Unit Contributions for Speaker-Adaptive\n  Lip-Reading", "author": "Songtao Luo and Shuang Yang and Shiguang Shan and Xilin Chen", "abstract": "  In this paper, we propose a novel method for speaker adaptation in lip\nreading, motivated by two observations. Firstly, a speaker's own\ncharacteristics can always be portrayed well by his/her few facial images or\neven a single image with shallow networks, while the fine-grained dynamic\nfeatures associated with speech content expressed by the talking face always\nneed deep sequential networks to represent accurately. Therefore, we treat the\nshallow and deep layers differently for speaker adaptive lip reading. Secondly,\nwe observe that a speaker's unique characteristics ( e.g. prominent oral cavity\nand mandible) have varied effects on lip reading performance for different\nwords and pronunciations, necessitating adaptive enhancement or suppression of\nthe features for robust lip reading. Based on these two observations, we\npropose to take advantage of the speaker's own characteristics to automatically\nlearn separable hidden unit contributions with different targets for shallow\nlayers and deep layers respectively. For shallow layers where features related\nto the speaker's characteristics are stronger than the speech content related\nfeatures, we introduce speaker-adaptive features to learn for enhancing the\nspeech content features. For deep layers where both the speaker's features and\nthe speech content features are all expressed well, we introduce the\nspeaker-adaptive features to learn for suppressing the speech content\nirrelevant noise for robust lip reading. Our approach consistently outperforms\nexisting methods, as confirmed by comprehensive analysis and comparison across\ndifferent settings. Besides the evaluation on the popular LRW-ID and GRID\ndatasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to\nfurther assess the performance in an extreme setting where just a few speakers\nare available but the speech content covers a large and diversified range.\n", "link": "http://arxiv.org/abs/2310.05058v3", "date": "2024-04-30", "relevancy": 2.6452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5079}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Separable%20Hidden%20Unit%20Contributions%20for%20Speaker-Adaptive%0A%20%20Lip-Reading&body=Title%3A%20Learning%20Separable%20Hidden%20Unit%20Contributions%20for%20Speaker-Adaptive%0A%20%20Lip-Reading%0AAuthor%3A%20Songtao%20Luo%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20speaker%20adaptation%20in%20lip%0Areading%2C%20motivated%20by%20two%20observations.%20Firstly%2C%20a%20speaker%27s%20own%0Acharacteristics%20can%20always%20be%20portrayed%20well%20by%20his/her%20few%20facial%20images%20or%0Aeven%20a%20single%20image%20with%20shallow%20networks%2C%20while%20the%20fine-grained%20dynamic%0Afeatures%20associated%20with%20speech%20content%20expressed%20by%20the%20talking%20face%20always%0Aneed%20deep%20sequential%20networks%20to%20represent%20accurately.%20Therefore%2C%20we%20treat%20the%0Ashallow%20and%20deep%20layers%20differently%20for%20speaker%20adaptive%20lip%20reading.%20Secondly%2C%0Awe%20observe%20that%20a%20speaker%27s%20unique%20characteristics%20%28%20e.g.%20prominent%20oral%20cavity%0Aand%20mandible%29%20have%20varied%20effects%20on%20lip%20reading%20performance%20for%20different%0Awords%20and%20pronunciations%2C%20necessitating%20adaptive%20enhancement%20or%20suppression%20of%0Athe%20features%20for%20robust%20lip%20reading.%20Based%20on%20these%20two%20observations%2C%20we%0Apropose%20to%20take%20advantage%20of%20the%20speaker%27s%20own%20characteristics%20to%20automatically%0Alearn%20separable%20hidden%20unit%20contributions%20with%20different%20targets%20for%20shallow%0Alayers%20and%20deep%20layers%20respectively.%20For%20shallow%20layers%20where%20features%20related%0Ato%20the%20speaker%27s%20characteristics%20are%20stronger%20than%20the%20speech%20content%20related%0Afeatures%2C%20we%20introduce%20speaker-adaptive%20features%20to%20learn%20for%20enhancing%20the%0Aspeech%20content%20features.%20For%20deep%20layers%20where%20both%20the%20speaker%27s%20features%20and%0Athe%20speech%20content%20features%20are%20all%20expressed%20well%2C%20we%20introduce%20the%0Aspeaker-adaptive%20features%20to%20learn%20for%20suppressing%20the%20speech%20content%0Airrelevant%20noise%20for%20robust%20lip%20reading.%20Our%20approach%20consistently%20outperforms%0Aexisting%20methods%2C%20as%20confirmed%20by%20comprehensive%20analysis%20and%20comparison%20across%0Adifferent%20settings.%20Besides%20the%20evaluation%20on%20the%20popular%20LRW-ID%20and%20GRID%0Adatasets%2C%20we%20also%20release%20a%20new%20dataset%20for%20evaluation%2C%20CAS-VSR-S68h%2C%20to%0Afurther%20assess%20the%20performance%20in%20an%20extreme%20setting%20where%20just%20a%20few%20speakers%0Aare%20available%20but%20the%20speech%20content%20covers%20a%20large%20and%20diversified%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05058v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Separable%20Hidden%20Unit%20Contributions%20for%20Speaker-Adaptive%0A%20%20Lip-Reading&entry.906535625=Songtao%20Luo%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20speaker%20adaptation%20in%20lip%0Areading%2C%20motivated%20by%20two%20observations.%20Firstly%2C%20a%20speaker%27s%20own%0Acharacteristics%20can%20always%20be%20portrayed%20well%20by%20his/her%20few%20facial%20images%20or%0Aeven%20a%20single%20image%20with%20shallow%20networks%2C%20while%20the%20fine-grained%20dynamic%0Afeatures%20associated%20with%20speech%20content%20expressed%20by%20the%20talking%20face%20always%0Aneed%20deep%20sequential%20networks%20to%20represent%20accurately.%20Therefore%2C%20we%20treat%20the%0Ashallow%20and%20deep%20layers%20differently%20for%20speaker%20adaptive%20lip%20reading.%20Secondly%2C%0Awe%20observe%20that%20a%20speaker%27s%20unique%20characteristics%20%28%20e.g.%20prominent%20oral%20cavity%0Aand%20mandible%29%20have%20varied%20effects%20on%20lip%20reading%20performance%20for%20different%0Awords%20and%20pronunciations%2C%20necessitating%20adaptive%20enhancement%20or%20suppression%20of%0Athe%20features%20for%20robust%20lip%20reading.%20Based%20on%20these%20two%20observations%2C%20we%0Apropose%20to%20take%20advantage%20of%20the%20speaker%27s%20own%20characteristics%20to%20automatically%0Alearn%20separable%20hidden%20unit%20contributions%20with%20different%20targets%20for%20shallow%0Alayers%20and%20deep%20layers%20respectively.%20For%20shallow%20layers%20where%20features%20related%0Ato%20the%20speaker%27s%20characteristics%20are%20stronger%20than%20the%20speech%20content%20related%0Afeatures%2C%20we%20introduce%20speaker-adaptive%20features%20to%20learn%20for%20enhancing%20the%0Aspeech%20content%20features.%20For%20deep%20layers%20where%20both%20the%20speaker%27s%20features%20and%0Athe%20speech%20content%20features%20are%20all%20expressed%20well%2C%20we%20introduce%20the%0Aspeaker-adaptive%20features%20to%20learn%20for%20suppressing%20the%20speech%20content%0Airrelevant%20noise%20for%20robust%20lip%20reading.%20Our%20approach%20consistently%20outperforms%0Aexisting%20methods%2C%20as%20confirmed%20by%20comprehensive%20analysis%20and%20comparison%20across%0Adifferent%20settings.%20Besides%20the%20evaluation%20on%20the%20popular%20LRW-ID%20and%20GRID%0Adatasets%2C%20we%20also%20release%20a%20new%20dataset%20for%20evaluation%2C%20CAS-VSR-S68h%2C%20to%0Afurther%20assess%20the%20performance%20in%20an%20extreme%20setting%20where%20just%20a%20few%20speakers%0Aare%20available%20but%20the%20speech%20content%20covers%20a%20large%20and%20diversified%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05058v3&entry.124074799=Read"},
{"title": "Just Say the Name: Online Continual Learning with Category Names Only\n  via Data Generation", "author": "Minhyuk Seo and Diganta Misra and Seongwon Cho and Minjae Lee and Jonghyun Choi", "abstract": "  In real-world scenarios, extensive manual annotation for continual learning\nis impractical due to prohibitive costs. Although prior arts, influenced by\nlarge-scale webly supervised training, suggest leveraging web-scraped data in\ncontinual learning, this poses challenges such as data imbalance, usage\nrestrictions, and privacy concerns. Addressing the risks of continual webly\nsupervised training, we present an online continual learning framework -\nGenerative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a\nset of generators G along with the learner. When encountering new concepts\n(i.e., classes), G-NoCL employs the novel sample complexity-guided data\nensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to\noptimally sample training data from generated data. Through extensive\nexperimentation, we demonstrate superior performance of DISCOBER in G-NoCL\nonline CL benchmarks, covering both In-Distribution (ID) and\nOut-of-Distribution (OOD) generalization evaluations, compared to naive\ngenerator-ensembling, web-supervised, and manually annotated data.\n", "link": "http://arxiv.org/abs/2403.10853v2", "date": "2024-04-30", "relevancy": 2.5659, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5616}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4991}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4789}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Just%20Say%20the%20Name%3A%20Online%20Continual%20Learning%20with%20Category%20Names%20Only%0A%20%20via%20Data%20Generation&body=Title%3A%20Just%20Say%20the%20Name%3A%20Online%20Continual%20Learning%20with%20Category%20Names%20Only%0A%20%20via%20Data%20Generation%0AAuthor%3A%20Minhyuk%20Seo%20and%20Diganta%20Misra%20and%20Seongwon%20Cho%20and%20Minjae%20Lee%20and%20Jonghyun%20Choi%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20extensive%20manual%20annotation%20for%20continual%20learning%0Ais%20impractical%20due%20to%20prohibitive%20costs.%20Although%20prior%20arts%2C%20influenced%20by%0Alarge-scale%20webly%20supervised%20training%2C%20suggest%20leveraging%20web-scraped%20data%20in%0Acontinual%20learning%2C%20this%20poses%20challenges%20such%20as%20data%20imbalance%2C%20usage%0Arestrictions%2C%20and%20privacy%20concerns.%20Addressing%20the%20risks%20of%20continual%20webly%0Asupervised%20training%2C%20we%20present%20an%20online%20continual%20learning%20framework%20-%0AGenerative%20Name%20only%20Continual%20Learning%20%28G-NoCL%29.%20The%20proposed%20G-NoCL%20uses%20a%0Aset%20of%20generators%20G%20along%20with%20the%20learner.%20When%20encountering%20new%20concepts%0A%28i.e.%2C%20classes%29%2C%20G-NoCL%20employs%20the%20novel%20sample%20complexity-guided%20data%0Aensembling%20technique%20DIverSity%20and%20COmplexity%20enhancing%20ensemBlER%20%28DISCOBER%29%20to%0Aoptimally%20sample%20training%20data%20from%20generated%20data.%20Through%20extensive%0Aexperimentation%2C%20we%20demonstrate%20superior%20performance%20of%20DISCOBER%20in%20G-NoCL%0Aonline%20CL%20benchmarks%2C%20covering%20both%20In-Distribution%20%28ID%29%20and%0AOut-of-Distribution%20%28OOD%29%20generalization%20evaluations%2C%20compared%20to%20naive%0Agenerator-ensembling%2C%20web-supervised%2C%20and%20manually%20annotated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10853v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Just%20Say%20the%20Name%3A%20Online%20Continual%20Learning%20with%20Category%20Names%20Only%0A%20%20via%20Data%20Generation&entry.906535625=Minhyuk%20Seo%20and%20Diganta%20Misra%20and%20Seongwon%20Cho%20and%20Minjae%20Lee%20and%20Jonghyun%20Choi&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20extensive%20manual%20annotation%20for%20continual%20learning%0Ais%20impractical%20due%20to%20prohibitive%20costs.%20Although%20prior%20arts%2C%20influenced%20by%0Alarge-scale%20webly%20supervised%20training%2C%20suggest%20leveraging%20web-scraped%20data%20in%0Acontinual%20learning%2C%20this%20poses%20challenges%20such%20as%20data%20imbalance%2C%20usage%0Arestrictions%2C%20and%20privacy%20concerns.%20Addressing%20the%20risks%20of%20continual%20webly%0Asupervised%20training%2C%20we%20present%20an%20online%20continual%20learning%20framework%20-%0AGenerative%20Name%20only%20Continual%20Learning%20%28G-NoCL%29.%20The%20proposed%20G-NoCL%20uses%20a%0Aset%20of%20generators%20G%20along%20with%20the%20learner.%20When%20encountering%20new%20concepts%0A%28i.e.%2C%20classes%29%2C%20G-NoCL%20employs%20the%20novel%20sample%20complexity-guided%20data%0Aensembling%20technique%20DIverSity%20and%20COmplexity%20enhancing%20ensemBlER%20%28DISCOBER%29%20to%0Aoptimally%20sample%20training%20data%20from%20generated%20data.%20Through%20extensive%0Aexperimentation%2C%20we%20demonstrate%20superior%20performance%20of%20DISCOBER%20in%20G-NoCL%0Aonline%20CL%20benchmarks%2C%20covering%20both%20In-Distribution%20%28ID%29%20and%0AOut-of-Distribution%20%28OOD%29%20generalization%20evaluations%2C%20compared%20to%20naive%0Agenerator-ensembling%2C%20web-supervised%2C%20and%20manually%20annotated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10853v2&entry.124074799=Read"},
{"title": "VimTS: A Unified Video and Image Text Spotter for Enhancing the\n  Cross-domain Generalization", "author": "Yuliang Liu and Mingxin Huang and Hao Yan and Linger Deng and Weijia Wu and Hao Lu and Chunhua Shen and Lianwen Jin and Xiang Bai", "abstract": "  Text spotting, a task involving the extraction of textual information from\nimage or video sequences, faces challenges in cross-domain adaption, such as\nimage-to-image and image-to-video generalization. In this paper, we introduce a\nnew method, termed VimTS, which enhances the generalization ability of the\nmodel by achieving better synergy among different tasks. Typically, we propose\na Prompt Queries Generation Module and a Tasks-aware Adapter to effectively\nconvert the original single-task model into a multi-task model suitable for\nboth image and video scenarios with minimal additional parameters. The Prompt\nQueries Generation Module facilitates explicit interaction between different\ntasks, while the Tasks-aware Adapter helps the model dynamically learn suitable\nfeatures for each task. Additionally, to further enable the model to learn\ntemporal information at a lower cost, we propose a synthetic video text dataset\n(VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.\nNotably, our method outperforms the state-of-the-art method by an average of\n2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and\nTT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses\nthe previous end-to-end video spotting method in ICDAR2015 video and DSText v2\nby an average of 5.5% on the MOTA metric, using only image-level data. We\nfurther demonstrate that existing Large Multimodal Models exhibit limitations\nin generating cross-domain scene text spotting, in contrast to our VimTS model\nwhich requires significantly fewer parameters and data. The code and datasets\nwill be made available at the https://VimTextSpotter.github.io.\n", "link": "http://arxiv.org/abs/2404.19652v1", "date": "2024-04-30", "relevancy": 2.5349, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6394}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6349}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6303}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VimTS%3A%20A%20Unified%20Video%20and%20Image%20Text%20Spotter%20for%20Enhancing%20the%0A%20%20Cross-domain%20Generalization&body=Title%3A%20VimTS%3A%20A%20Unified%20Video%20and%20Image%20Text%20Spotter%20for%20Enhancing%20the%0A%20%20Cross-domain%20Generalization%0AAuthor%3A%20Yuliang%20Liu%20and%20Mingxin%20Huang%20and%20Hao%20Yan%20and%20Linger%20Deng%20and%20Weijia%20Wu%20and%20Hao%20Lu%20and%20Chunhua%20Shen%20and%20Lianwen%20Jin%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Text%20spotting%2C%20a%20task%20involving%20the%20extraction%20of%20textual%20information%20from%0Aimage%20or%20video%20sequences%2C%20faces%20challenges%20in%20cross-domain%20adaption%2C%20such%20as%0Aimage-to-image%20and%20image-to-video%20generalization.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20method%2C%20termed%20VimTS%2C%20which%20enhances%20the%20generalization%20ability%20of%20the%0Amodel%20by%20achieving%20better%20synergy%20among%20different%20tasks.%20Typically%2C%20we%20propose%0Aa%20Prompt%20Queries%20Generation%20Module%20and%20a%20Tasks-aware%20Adapter%20to%20effectively%0Aconvert%20the%20original%20single-task%20model%20into%20a%20multi-task%20model%20suitable%20for%0Aboth%20image%20and%20video%20scenarios%20with%20minimal%20additional%20parameters.%20The%20Prompt%0AQueries%20Generation%20Module%20facilitates%20explicit%20interaction%20between%20different%0Atasks%2C%20while%20the%20Tasks-aware%20Adapter%20helps%20the%20model%20dynamically%20learn%20suitable%0Afeatures%20for%20each%20task.%20Additionally%2C%20to%20further%20enable%20the%20model%20to%20learn%0Atemporal%20information%20at%20a%20lower%20cost%2C%20we%20propose%20a%20synthetic%20video%20text%20dataset%0A%28VTD-368k%29%20by%20leveraging%20the%20Content%20Deformation%20Fields%20%28CoDeF%29%20algorithm.%0ANotably%2C%20our%20method%20outperforms%20the%20state-of-the-art%20method%20by%20an%20average%20of%0A2.6%25%20in%20six%20cross-domain%20benchmarks%20such%20as%20TT-to-IC15%2C%20CTW1500-to-TT%2C%20and%0ATT-to-CTW1500.%20For%20video-level%20cross-domain%20adaption%2C%20our%20method%20even%20surpasses%0Athe%20previous%20end-to-end%20video%20spotting%20method%20in%20ICDAR2015%20video%20and%20DSText%20v2%0Aby%20an%20average%20of%205.5%25%20on%20the%20MOTA%20metric%2C%20using%20only%20image-level%20data.%20We%0Afurther%20demonstrate%20that%20existing%20Large%20Multimodal%20Models%20exhibit%20limitations%0Ain%20generating%20cross-domain%20scene%20text%20spotting%2C%20in%20contrast%20to%20our%20VimTS%20model%0Awhich%20requires%20significantly%20fewer%20parameters%20and%20data.%20The%20code%20and%20datasets%0Awill%20be%20made%20available%20at%20the%20https%3A//VimTextSpotter.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19652v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VimTS%3A%20A%20Unified%20Video%20and%20Image%20Text%20Spotter%20for%20Enhancing%20the%0A%20%20Cross-domain%20Generalization&entry.906535625=Yuliang%20Liu%20and%20Mingxin%20Huang%20and%20Hao%20Yan%20and%20Linger%20Deng%20and%20Weijia%20Wu%20and%20Hao%20Lu%20and%20Chunhua%20Shen%20and%20Lianwen%20Jin%20and%20Xiang%20Bai&entry.1292438233=%20%20Text%20spotting%2C%20a%20task%20involving%20the%20extraction%20of%20textual%20information%20from%0Aimage%20or%20video%20sequences%2C%20faces%20challenges%20in%20cross-domain%20adaption%2C%20such%20as%0Aimage-to-image%20and%20image-to-video%20generalization.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20method%2C%20termed%20VimTS%2C%20which%20enhances%20the%20generalization%20ability%20of%20the%0Amodel%20by%20achieving%20better%20synergy%20among%20different%20tasks.%20Typically%2C%20we%20propose%0Aa%20Prompt%20Queries%20Generation%20Module%20and%20a%20Tasks-aware%20Adapter%20to%20effectively%0Aconvert%20the%20original%20single-task%20model%20into%20a%20multi-task%20model%20suitable%20for%0Aboth%20image%20and%20video%20scenarios%20with%20minimal%20additional%20parameters.%20The%20Prompt%0AQueries%20Generation%20Module%20facilitates%20explicit%20interaction%20between%20different%0Atasks%2C%20while%20the%20Tasks-aware%20Adapter%20helps%20the%20model%20dynamically%20learn%20suitable%0Afeatures%20for%20each%20task.%20Additionally%2C%20to%20further%20enable%20the%20model%20to%20learn%0Atemporal%20information%20at%20a%20lower%20cost%2C%20we%20propose%20a%20synthetic%20video%20text%20dataset%0A%28VTD-368k%29%20by%20leveraging%20the%20Content%20Deformation%20Fields%20%28CoDeF%29%20algorithm.%0ANotably%2C%20our%20method%20outperforms%20the%20state-of-the-art%20method%20by%20an%20average%20of%0A2.6%25%20in%20six%20cross-domain%20benchmarks%20such%20as%20TT-to-IC15%2C%20CTW1500-to-TT%2C%20and%0ATT-to-CTW1500.%20For%20video-level%20cross-domain%20adaption%2C%20our%20method%20even%20surpasses%0Athe%20previous%20end-to-end%20video%20spotting%20method%20in%20ICDAR2015%20video%20and%20DSText%20v2%0Aby%20an%20average%20of%205.5%25%20on%20the%20MOTA%20metric%2C%20using%20only%20image-level%20data.%20We%0Afurther%20demonstrate%20that%20existing%20Large%20Multimodal%20Models%20exhibit%20limitations%0Ain%20generating%20cross-domain%20scene%20text%20spotting%2C%20in%20contrast%20to%20our%20VimTS%20model%0Awhich%20requires%20significantly%20fewer%20parameters%20and%20data.%20The%20code%20and%20datasets%0Awill%20be%20made%20available%20at%20the%20https%3A//VimTextSpotter.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19652v1&entry.124074799=Read"},
{"title": "MambaPupil: Bidirectional Selective Recurrent model for Event-based Eye\n  tracking", "author": "Zhong Wang and Zengyu Wan and Han Han and Bohao Liao and Yuliang Wu and Wei Zhai and Yang Cao and Zheng-jun Zha", "abstract": "  Event-based eye tracking has shown great promise with the high temporal\nresolution and low redundancy provided by the event camera. However, the\ndiversity and abruptness of eye movement patterns, including blinking,\nfixating, saccades, and smooth pursuit, pose significant challenges for eye\nlocalization. To achieve a stable event-based eye-tracking system, this paper\nproposes a bidirectional long-term sequence modeling and time-varying state\nselection mechanism to fully utilize contextual temporal information in\nresponse to the variability of eye movements. Specifically, the MambaPupil\nnetwork is proposed, which consists of the multi-layer convolutional encoder to\nextract features from the event representations, a bidirectional Gated\nRecurrent Unit (GRU), and a Linear Time-Varying State Space Module (LTV-SSM),\nto selectively capture contextual correlation from the forward and backward\ntemporal relationship. Furthermore, the Bina-rep is utilized as a compact event\nrepresentation, and the tailor-made data augmentation, called as Event-Cutout,\nis proposed to enhance the model's robustness by applying spatial random\nmasking to the event image. The evaluation on the ThreeET-plus benchmark shows\nthe superior performance of the MambaPupil, which secured the 1st place in\nCVPR'2024 AIS Event-based Eye Tracking challenge.\n", "link": "http://arxiv.org/abs/2404.12083v2", "date": "2024-04-30", "relevancy": 2.5285, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4923}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaPupil%3A%20Bidirectional%20Selective%20Recurrent%20model%20for%20Event-based%20Eye%0A%20%20tracking&body=Title%3A%20MambaPupil%3A%20Bidirectional%20Selective%20Recurrent%20model%20for%20Event-based%20Eye%0A%20%20tracking%0AAuthor%3A%20Zhong%20Wang%20and%20Zengyu%20Wan%20and%20Han%20Han%20and%20Bohao%20Liao%20and%20Yuliang%20Wu%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-jun%20Zha%0AAbstract%3A%20%20%20Event-based%20eye%20tracking%20has%20shown%20great%20promise%20with%20the%20high%20temporal%0Aresolution%20and%20low%20redundancy%20provided%20by%20the%20event%20camera.%20However%2C%20the%0Adiversity%20and%20abruptness%20of%20eye%20movement%20patterns%2C%20including%20blinking%2C%0Afixating%2C%20saccades%2C%20and%20smooth%20pursuit%2C%20pose%20significant%20challenges%20for%20eye%0Alocalization.%20To%20achieve%20a%20stable%20event-based%20eye-tracking%20system%2C%20this%20paper%0Aproposes%20a%20bidirectional%20long-term%20sequence%20modeling%20and%20time-varying%20state%0Aselection%20mechanism%20to%20fully%20utilize%20contextual%20temporal%20information%20in%0Aresponse%20to%20the%20variability%20of%20eye%20movements.%20Specifically%2C%20the%20MambaPupil%0Anetwork%20is%20proposed%2C%20which%20consists%20of%20the%20multi-layer%20convolutional%20encoder%20to%0Aextract%20features%20from%20the%20event%20representations%2C%20a%20bidirectional%20Gated%0ARecurrent%20Unit%20%28GRU%29%2C%20and%20a%20Linear%20Time-Varying%20State%20Space%20Module%20%28LTV-SSM%29%2C%0Ato%20selectively%20capture%20contextual%20correlation%20from%20the%20forward%20and%20backward%0Atemporal%20relationship.%20Furthermore%2C%20the%20Bina-rep%20is%20utilized%20as%20a%20compact%20event%0Arepresentation%2C%20and%20the%20tailor-made%20data%20augmentation%2C%20called%20as%20Event-Cutout%2C%0Ais%20proposed%20to%20enhance%20the%20model%27s%20robustness%20by%20applying%20spatial%20random%0Amasking%20to%20the%20event%20image.%20The%20evaluation%20on%20the%20ThreeET-plus%20benchmark%20shows%0Athe%20superior%20performance%20of%20the%20MambaPupil%2C%20which%20secured%20the%201st%20place%20in%0ACVPR%272024%20AIS%20Event-based%20Eye%20Tracking%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12083v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaPupil%3A%20Bidirectional%20Selective%20Recurrent%20model%20for%20Event-based%20Eye%0A%20%20tracking&entry.906535625=Zhong%20Wang%20and%20Zengyu%20Wan%20and%20Han%20Han%20and%20Bohao%20Liao%20and%20Yuliang%20Wu%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Zheng-jun%20Zha&entry.1292438233=%20%20Event-based%20eye%20tracking%20has%20shown%20great%20promise%20with%20the%20high%20temporal%0Aresolution%20and%20low%20redundancy%20provided%20by%20the%20event%20camera.%20However%2C%20the%0Adiversity%20and%20abruptness%20of%20eye%20movement%20patterns%2C%20including%20blinking%2C%0Afixating%2C%20saccades%2C%20and%20smooth%20pursuit%2C%20pose%20significant%20challenges%20for%20eye%0Alocalization.%20To%20achieve%20a%20stable%20event-based%20eye-tracking%20system%2C%20this%20paper%0Aproposes%20a%20bidirectional%20long-term%20sequence%20modeling%20and%20time-varying%20state%0Aselection%20mechanism%20to%20fully%20utilize%20contextual%20temporal%20information%20in%0Aresponse%20to%20the%20variability%20of%20eye%20movements.%20Specifically%2C%20the%20MambaPupil%0Anetwork%20is%20proposed%2C%20which%20consists%20of%20the%20multi-layer%20convolutional%20encoder%20to%0Aextract%20features%20from%20the%20event%20representations%2C%20a%20bidirectional%20Gated%0ARecurrent%20Unit%20%28GRU%29%2C%20and%20a%20Linear%20Time-Varying%20State%20Space%20Module%20%28LTV-SSM%29%2C%0Ato%20selectively%20capture%20contextual%20correlation%20from%20the%20forward%20and%20backward%0Atemporal%20relationship.%20Furthermore%2C%20the%20Bina-rep%20is%20utilized%20as%20a%20compact%20event%0Arepresentation%2C%20and%20the%20tailor-made%20data%20augmentation%2C%20called%20as%20Event-Cutout%2C%0Ais%20proposed%20to%20enhance%20the%20model%27s%20robustness%20by%20applying%20spatial%20random%0Amasking%20to%20the%20event%20image.%20The%20evaluation%20on%20the%20ThreeET-plus%20benchmark%20shows%0Athe%20superior%20performance%20of%20the%20MambaPupil%2C%20which%20secured%20the%201st%20place%20in%0ACVPR%272024%20AIS%20Event-based%20Eye%20Tracking%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12083v2&entry.124074799=Read"},
{"title": "InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation", "author": "Chanran Kim and Jeongin Lee and Shichang Joung and Bongmo Kim and Yeul-Min Baek", "abstract": "  In the field of personalized image generation, the ability to create images\npreserving concepts has significantly improved. Creating an image that\nnaturally integrates multiple concepts in a cohesive and visually appealing\ncomposition can indeed be challenging. This paper introduces \"InstantFamily,\"\nan approach that employs a novel masked cross-attention mechanism and a\nmultimodal embedding stack to achieve zero-shot multi-ID image generation. Our\nmethod effectively preserves ID as it utilizes global and local features from a\npre-trained face recognition model integrated with text conditions.\nAdditionally, our masked cross-attention mechanism enables the precise control\nof multi-ID and composition in the generated images. We demonstrate the\neffectiveness of InstantFamily through experiments showing its dominance in\ngenerating images with multi-ID, while resolving well-known multi-ID generation\nproblems. Additionally, our model achieves state-of-the-art performance in both\nsingle-ID and multi-ID preservation. Furthermore, our model exhibits remarkable\nscalability with a greater number of ID preservation than it was originally\ntrained with.\n", "link": "http://arxiv.org/abs/2404.19427v1", "date": "2024-04-30", "relevancy": 2.5226, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.608}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5772}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstantFamily%3A%20Masked%20Attention%20for%20Zero-shot%20Multi-ID%20Image%20Generation&body=Title%3A%20InstantFamily%3A%20Masked%20Attention%20for%20Zero-shot%20Multi-ID%20Image%20Generation%0AAuthor%3A%20Chanran%20Kim%20and%20Jeongin%20Lee%20and%20Shichang%20Joung%20and%20Bongmo%20Kim%20and%20Yeul-Min%20Baek%0AAbstract%3A%20%20%20In%20the%20field%20of%20personalized%20image%20generation%2C%20the%20ability%20to%20create%20images%0Apreserving%20concepts%20has%20significantly%20improved.%20Creating%20an%20image%20that%0Anaturally%20integrates%20multiple%20concepts%20in%20a%20cohesive%20and%20visually%20appealing%0Acomposition%20can%20indeed%20be%20challenging.%20This%20paper%20introduces%20%22InstantFamily%2C%22%0Aan%20approach%20that%20employs%20a%20novel%20masked%20cross-attention%20mechanism%20and%20a%0Amultimodal%20embedding%20stack%20to%20achieve%20zero-shot%20multi-ID%20image%20generation.%20Our%0Amethod%20effectively%20preserves%20ID%20as%20it%20utilizes%20global%20and%20local%20features%20from%20a%0Apre-trained%20face%20recognition%20model%20integrated%20with%20text%20conditions.%0AAdditionally%2C%20our%20masked%20cross-attention%20mechanism%20enables%20the%20precise%20control%0Aof%20multi-ID%20and%20composition%20in%20the%20generated%20images.%20We%20demonstrate%20the%0Aeffectiveness%20of%20InstantFamily%20through%20experiments%20showing%20its%20dominance%20in%0Agenerating%20images%20with%20multi-ID%2C%20while%20resolving%20well-known%20multi-ID%20generation%0Aproblems.%20Additionally%2C%20our%20model%20achieves%20state-of-the-art%20performance%20in%20both%0Asingle-ID%20and%20multi-ID%20preservation.%20Furthermore%2C%20our%20model%20exhibits%20remarkable%0Ascalability%20with%20a%20greater%20number%20of%20ID%20preservation%20than%20it%20was%20originally%0Atrained%20with.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19427v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantFamily%3A%20Masked%20Attention%20for%20Zero-shot%20Multi-ID%20Image%20Generation&entry.906535625=Chanran%20Kim%20and%20Jeongin%20Lee%20and%20Shichang%20Joung%20and%20Bongmo%20Kim%20and%20Yeul-Min%20Baek&entry.1292438233=%20%20In%20the%20field%20of%20personalized%20image%20generation%2C%20the%20ability%20to%20create%20images%0Apreserving%20concepts%20has%20significantly%20improved.%20Creating%20an%20image%20that%0Anaturally%20integrates%20multiple%20concepts%20in%20a%20cohesive%20and%20visually%20appealing%0Acomposition%20can%20indeed%20be%20challenging.%20This%20paper%20introduces%20%22InstantFamily%2C%22%0Aan%20approach%20that%20employs%20a%20novel%20masked%20cross-attention%20mechanism%20and%20a%0Amultimodal%20embedding%20stack%20to%20achieve%20zero-shot%20multi-ID%20image%20generation.%20Our%0Amethod%20effectively%20preserves%20ID%20as%20it%20utilizes%20global%20and%20local%20features%20from%20a%0Apre-trained%20face%20recognition%20model%20integrated%20with%20text%20conditions.%0AAdditionally%2C%20our%20masked%20cross-attention%20mechanism%20enables%20the%20precise%20control%0Aof%20multi-ID%20and%20composition%20in%20the%20generated%20images.%20We%20demonstrate%20the%0Aeffectiveness%20of%20InstantFamily%20through%20experiments%20showing%20its%20dominance%20in%0Agenerating%20images%20with%20multi-ID%2C%20while%20resolving%20well-known%20multi-ID%20generation%0Aproblems.%20Additionally%2C%20our%20model%20achieves%20state-of-the-art%20performance%20in%20both%0Asingle-ID%20and%20multi-ID%20preservation.%20Furthermore%2C%20our%20model%20exhibits%20remarkable%0Ascalability%20with%20a%20greater%20number%20of%20ID%20preservation%20than%20it%20was%20originally%0Atrained%20with.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19427v1&entry.124074799=Read"},
{"title": "SwipeGANSpace: Swipe-to-Compare Image Generation via Efficient Latent\n  Space Exploration", "author": "Yuto Nakashima and Mingzhe Yang and Yukino Baba", "abstract": "  Generating preferred images using generative adversarial networks (GANs) is\nchallenging owing to the high-dimensional nature of latent space. In this\nstudy, we propose a novel approach that uses simple user-swipe interactions to\ngenerate preferred images for users. To effectively explore the latent space\nwith only swipe interactions, we apply principal component analysis to the\nlatent space of the StyleGAN, creating meaningful subspaces. We use a\nmulti-armed bandit algorithm to decide the dimensions to explore, focusing on\nthe preferences of the user. Experiments show that our method is more efficient\nin generating preferred images than the baseline methods. Furthermore, changes\nin preferred images during image generation or the display of entirely\ndifferent image styles were observed to provide new inspirations, subsequently\naltering user preferences. This highlights the dynamic nature of user\npreferences, which our proposed approach recognizes and enhances.\n", "link": "http://arxiv.org/abs/2404.19693v1", "date": "2024-04-30", "relevancy": 2.5189, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5108}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4864}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SwipeGANSpace%3A%20Swipe-to-Compare%20Image%20Generation%20via%20Efficient%20Latent%0A%20%20Space%20Exploration&body=Title%3A%20SwipeGANSpace%3A%20Swipe-to-Compare%20Image%20Generation%20via%20Efficient%20Latent%0A%20%20Space%20Exploration%0AAuthor%3A%20Yuto%20Nakashima%20and%20Mingzhe%20Yang%20and%20Yukino%20Baba%0AAbstract%3A%20%20%20Generating%20preferred%20images%20using%20generative%20adversarial%20networks%20%28GANs%29%20is%0Achallenging%20owing%20to%20the%20high-dimensional%20nature%20of%20latent%20space.%20In%20this%0Astudy%2C%20we%20propose%20a%20novel%20approach%20that%20uses%20simple%20user-swipe%20interactions%20to%0Agenerate%20preferred%20images%20for%20users.%20To%20effectively%20explore%20the%20latent%20space%0Awith%20only%20swipe%20interactions%2C%20we%20apply%20principal%20component%20analysis%20to%20the%0Alatent%20space%20of%20the%20StyleGAN%2C%20creating%20meaningful%20subspaces.%20We%20use%20a%0Amulti-armed%20bandit%20algorithm%20to%20decide%20the%20dimensions%20to%20explore%2C%20focusing%20on%0Athe%20preferences%20of%20the%20user.%20Experiments%20show%20that%20our%20method%20is%20more%20efficient%0Ain%20generating%20preferred%20images%20than%20the%20baseline%20methods.%20Furthermore%2C%20changes%0Ain%20preferred%20images%20during%20image%20generation%20or%20the%20display%20of%20entirely%0Adifferent%20image%20styles%20were%20observed%20to%20provide%20new%20inspirations%2C%20subsequently%0Aaltering%20user%20preferences.%20This%20highlights%20the%20dynamic%20nature%20of%20user%0Apreferences%2C%20which%20our%20proposed%20approach%20recognizes%20and%20enhances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19693v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwipeGANSpace%3A%20Swipe-to-Compare%20Image%20Generation%20via%20Efficient%20Latent%0A%20%20Space%20Exploration&entry.906535625=Yuto%20Nakashima%20and%20Mingzhe%20Yang%20and%20Yukino%20Baba&entry.1292438233=%20%20Generating%20preferred%20images%20using%20generative%20adversarial%20networks%20%28GANs%29%20is%0Achallenging%20owing%20to%20the%20high-dimensional%20nature%20of%20latent%20space.%20In%20this%0Astudy%2C%20we%20propose%20a%20novel%20approach%20that%20uses%20simple%20user-swipe%20interactions%20to%0Agenerate%20preferred%20images%20for%20users.%20To%20effectively%20explore%20the%20latent%20space%0Awith%20only%20swipe%20interactions%2C%20we%20apply%20principal%20component%20analysis%20to%20the%0Alatent%20space%20of%20the%20StyleGAN%2C%20creating%20meaningful%20subspaces.%20We%20use%20a%0Amulti-armed%20bandit%20algorithm%20to%20decide%20the%20dimensions%20to%20explore%2C%20focusing%20on%0Athe%20preferences%20of%20the%20user.%20Experiments%20show%20that%20our%20method%20is%20more%20efficient%0Ain%20generating%20preferred%20images%20than%20the%20baseline%20methods.%20Furthermore%2C%20changes%0Ain%20preferred%20images%20during%20image%20generation%20or%20the%20display%20of%20entirely%0Adifferent%20image%20styles%20were%20observed%20to%20provide%20new%20inspirations%2C%20subsequently%0Aaltering%20user%20preferences.%20This%20highlights%20the%20dynamic%20nature%20of%20user%0Apreferences%2C%20which%20our%20proposed%20approach%20recognizes%20and%20enhances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19693v1&entry.124074799=Read"},
{"title": "Neural Dynamic Data Valuation", "author": "Zhangyong Liang and Huanhuan Gao and Ji Zhang", "abstract": "  Data constitute the foundational component of the data economy and its\nmarketplaces. Efficient and fair data valuation has emerged as a topic of\nsignificant interest.\\ Many approaches based on marginal contribution have\nshown promising results in various downstream tasks. However, they are well\nknown to be computationally expensive as they require training a large number\nof utility functions, which are used to evaluate the usefulness or value of a\ngiven dataset for a specific purpose. As a result, it has been recognized as\ninfeasible to apply these methods to a data marketplace involving large-scale\ndatasets. Consequently, a critical issue arises: how can the re-training of the\nutility function be avoided? To address this issue, we propose a novel data\nvaluation method from the perspective of optimal control, named the neural\ndynamic data valuation (NDDV). Our method has solid theoretical interpretations\nto accurately identify the data valuation via the sensitivity of the data\noptimal control state. In addition, we implement a data re-weighting strategy\nto capture the unique features of data points, ensuring fairness through the\ninteraction between data points and the mean-field states. Notably, our method\nrequires only training once to estimate the value of all data points,\nsignificantly improving the computational efficiency. We conduct comprehensive\nexperiments using different datasets and tasks. The results demonstrate that\nthe proposed NDDV method outperforms the existing state-of-the-art data\nvaluation methods in accurately identifying data points with either high or low\nvalues and is more computationally efficient.\n", "link": "http://arxiv.org/abs/2404.19557v1", "date": "2024-04-30", "relevancy": 2.4937, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5409}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Dynamic%20Data%20Valuation&body=Title%3A%20Neural%20Dynamic%20Data%20Valuation%0AAuthor%3A%20Zhangyong%20Liang%20and%20Huanhuan%20Gao%20and%20Ji%20Zhang%0AAbstract%3A%20%20%20Data%20constitute%20the%20foundational%20component%20of%20the%20data%20economy%20and%20its%0Amarketplaces.%20Efficient%20and%20fair%20data%20valuation%20has%20emerged%20as%20a%20topic%20of%0Asignificant%20interest.%5C%20Many%20approaches%20based%20on%20marginal%20contribution%20have%0Ashown%20promising%20results%20in%20various%20downstream%20tasks.%20However%2C%20they%20are%20well%0Aknown%20to%20be%20computationally%20expensive%20as%20they%20require%20training%20a%20large%20number%0Aof%20utility%20functions%2C%20which%20are%20used%20to%20evaluate%20the%20usefulness%20or%20value%20of%20a%0Agiven%20dataset%20for%20a%20specific%20purpose.%20As%20a%20result%2C%20it%20has%20been%20recognized%20as%0Ainfeasible%20to%20apply%20these%20methods%20to%20a%20data%20marketplace%20involving%20large-scale%0Adatasets.%20Consequently%2C%20a%20critical%20issue%20arises%3A%20how%20can%20the%20re-training%20of%20the%0Autility%20function%20be%20avoided%3F%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20data%0Avaluation%20method%20from%20the%20perspective%20of%20optimal%20control%2C%20named%20the%20neural%0Adynamic%20data%20valuation%20%28NDDV%29.%20Our%20method%20has%20solid%20theoretical%20interpretations%0Ato%20accurately%20identify%20the%20data%20valuation%20via%20the%20sensitivity%20of%20the%20data%0Aoptimal%20control%20state.%20In%20addition%2C%20we%20implement%20a%20data%20re-weighting%20strategy%0Ato%20capture%20the%20unique%20features%20of%20data%20points%2C%20ensuring%20fairness%20through%20the%0Ainteraction%20between%20data%20points%20and%20the%20mean-field%20states.%20Notably%2C%20our%20method%0Arequires%20only%20training%20once%20to%20estimate%20the%20value%20of%20all%20data%20points%2C%0Asignificantly%20improving%20the%20computational%20efficiency.%20We%20conduct%20comprehensive%0Aexperiments%20using%20different%20datasets%20and%20tasks.%20The%20results%20demonstrate%20that%0Athe%20proposed%20NDDV%20method%20outperforms%20the%20existing%20state-of-the-art%20data%0Avaluation%20methods%20in%20accurately%20identifying%20data%20points%20with%20either%20high%20or%20low%0Avalues%20and%20is%20more%20computationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19557v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Dynamic%20Data%20Valuation&entry.906535625=Zhangyong%20Liang%20and%20Huanhuan%20Gao%20and%20Ji%20Zhang&entry.1292438233=%20%20Data%20constitute%20the%20foundational%20component%20of%20the%20data%20economy%20and%20its%0Amarketplaces.%20Efficient%20and%20fair%20data%20valuation%20has%20emerged%20as%20a%20topic%20of%0Asignificant%20interest.%5C%20Many%20approaches%20based%20on%20marginal%20contribution%20have%0Ashown%20promising%20results%20in%20various%20downstream%20tasks.%20However%2C%20they%20are%20well%0Aknown%20to%20be%20computationally%20expensive%20as%20they%20require%20training%20a%20large%20number%0Aof%20utility%20functions%2C%20which%20are%20used%20to%20evaluate%20the%20usefulness%20or%20value%20of%20a%0Agiven%20dataset%20for%20a%20specific%20purpose.%20As%20a%20result%2C%20it%20has%20been%20recognized%20as%0Ainfeasible%20to%20apply%20these%20methods%20to%20a%20data%20marketplace%20involving%20large-scale%0Adatasets.%20Consequently%2C%20a%20critical%20issue%20arises%3A%20how%20can%20the%20re-training%20of%20the%0Autility%20function%20be%20avoided%3F%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20data%0Avaluation%20method%20from%20the%20perspective%20of%20optimal%20control%2C%20named%20the%20neural%0Adynamic%20data%20valuation%20%28NDDV%29.%20Our%20method%20has%20solid%20theoretical%20interpretations%0Ato%20accurately%20identify%20the%20data%20valuation%20via%20the%20sensitivity%20of%20the%20data%0Aoptimal%20control%20state.%20In%20addition%2C%20we%20implement%20a%20data%20re-weighting%20strategy%0Ato%20capture%20the%20unique%20features%20of%20data%20points%2C%20ensuring%20fairness%20through%20the%0Ainteraction%20between%20data%20points%20and%20the%20mean-field%20states.%20Notably%2C%20our%20method%0Arequires%20only%20training%20once%20to%20estimate%20the%20value%20of%20all%20data%20points%2C%0Asignificantly%20improving%20the%20computational%20efficiency.%20We%20conduct%20comprehensive%0Aexperiments%20using%20different%20datasets%20and%20tasks.%20The%20results%20demonstrate%20that%0Athe%20proposed%20NDDV%20method%20outperforms%20the%20existing%20state-of-the-art%20data%0Avaluation%20methods%20in%20accurately%20identifying%20data%20points%20with%20either%20high%20or%20low%0Avalues%20and%20is%20more%20computationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19557v1&entry.124074799=Read"},
{"title": "Tabular Data Contrastive Learning via Class-Conditioned and\n  Feature-Correlation Based Augmentation", "author": "Wei Cui and Rasa Hosseinzadeh and Junwei Ma and Tongzi Wu and Yi Sui and Keyvan Golestan", "abstract": "  Contrastive learning is a model pre-training technique by first creating\nsimilar views of the original data, and then encouraging the data and its\ncorresponding views to be close in the embedding space. Contrastive learning\nhas witnessed success in image and natural language data, thanks to the\ndomain-specific augmentation techniques that are both intuitive and effective.\nNonetheless, in tabular domain, the predominant augmentation technique for\ncreating views is through corrupting tabular entries via swapping values, which\nis not as sound or effective. We propose a simple yet powerful improvement to\nthis augmentation technique: corrupting tabular data conditioned on class\nidentity. Specifically, when corrupting a specific tabular entry from an anchor\nrow, instead of randomly sampling a value in the same feature column from the\nentire table uniformly, we only sample from rows that are identified to be\nwithin the same class as the anchor row. We assume the semi-supervised learning\nsetting, and adopt the pseudo labeling technique for obtaining class identities\nover all table rows. We also explore the novel idea of selecting features to be\ncorrupted based on feature correlation structures. Extensive experiments show\nthat the proposed approach consistently outperforms the conventional corruption\nmethod for tabular data classification tasks. Our code is available at\nhttps://github.com/willtop/Tabular-Class-Conditioned-SSL.\n", "link": "http://arxiv.org/abs/2404.17489v2", "date": "2024-04-30", "relevancy": 2.4213, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation&body=Title%3A%20Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation%0AAuthor%3A%20Wei%20Cui%20and%20Rasa%20Hosseinzadeh%20and%20Junwei%20Ma%20and%20Tongzi%20Wu%20and%20Yi%20Sui%20and%20Keyvan%20Golestan%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20model%20pre-training%20technique%20by%20first%20creating%0Asimilar%20views%20of%20the%20original%20data%2C%20and%20then%20encouraging%20the%20data%20and%20its%0Acorresponding%20views%20to%20be%20close%20in%20the%20embedding%20space.%20Contrastive%20learning%0Ahas%20witnessed%20success%20in%20image%20and%20natural%20language%20data%2C%20thanks%20to%20the%0Adomain-specific%20augmentation%20techniques%20that%20are%20both%20intuitive%20and%20effective.%0ANonetheless%2C%20in%20tabular%20domain%2C%20the%20predominant%20augmentation%20technique%20for%0Acreating%20views%20is%20through%20corrupting%20tabular%20entries%20via%20swapping%20values%2C%20which%0Ais%20not%20as%20sound%20or%20effective.%20We%20propose%20a%20simple%20yet%20powerful%20improvement%20to%0Athis%20augmentation%20technique%3A%20corrupting%20tabular%20data%20conditioned%20on%20class%0Aidentity.%20Specifically%2C%20when%20corrupting%20a%20specific%20tabular%20entry%20from%20an%20anchor%0Arow%2C%20instead%20of%20randomly%20sampling%20a%20value%20in%20the%20same%20feature%20column%20from%20the%0Aentire%20table%20uniformly%2C%20we%20only%20sample%20from%20rows%20that%20are%20identified%20to%20be%0Awithin%20the%20same%20class%20as%20the%20anchor%20row.%20We%20assume%20the%20semi-supervised%20learning%0Asetting%2C%20and%20adopt%20the%20pseudo%20labeling%20technique%20for%20obtaining%20class%20identities%0Aover%20all%20table%20rows.%20We%20also%20explore%20the%20novel%20idea%20of%20selecting%20features%20to%20be%0Acorrupted%20based%20on%20feature%20correlation%20structures.%20Extensive%20experiments%20show%0Athat%20the%20proposed%20approach%20consistently%20outperforms%20the%20conventional%20corruption%0Amethod%20for%20tabular%20data%20classification%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/willtop/Tabular-Class-Conditioned-SSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17489v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20Data%20Contrastive%20Learning%20via%20Class-Conditioned%20and%0A%20%20Feature-Correlation%20Based%20Augmentation&entry.906535625=Wei%20Cui%20and%20Rasa%20Hosseinzadeh%20and%20Junwei%20Ma%20and%20Tongzi%20Wu%20and%20Yi%20Sui%20and%20Keyvan%20Golestan&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20model%20pre-training%20technique%20by%20first%20creating%0Asimilar%20views%20of%20the%20original%20data%2C%20and%20then%20encouraging%20the%20data%20and%20its%0Acorresponding%20views%20to%20be%20close%20in%20the%20embedding%20space.%20Contrastive%20learning%0Ahas%20witnessed%20success%20in%20image%20and%20natural%20language%20data%2C%20thanks%20to%20the%0Adomain-specific%20augmentation%20techniques%20that%20are%20both%20intuitive%20and%20effective.%0ANonetheless%2C%20in%20tabular%20domain%2C%20the%20predominant%20augmentation%20technique%20for%0Acreating%20views%20is%20through%20corrupting%20tabular%20entries%20via%20swapping%20values%2C%20which%0Ais%20not%20as%20sound%20or%20effective.%20We%20propose%20a%20simple%20yet%20powerful%20improvement%20to%0Athis%20augmentation%20technique%3A%20corrupting%20tabular%20data%20conditioned%20on%20class%0Aidentity.%20Specifically%2C%20when%20corrupting%20a%20specific%20tabular%20entry%20from%20an%20anchor%0Arow%2C%20instead%20of%20randomly%20sampling%20a%20value%20in%20the%20same%20feature%20column%20from%20the%0Aentire%20table%20uniformly%2C%20we%20only%20sample%20from%20rows%20that%20are%20identified%20to%20be%0Awithin%20the%20same%20class%20as%20the%20anchor%20row.%20We%20assume%20the%20semi-supervised%20learning%0Asetting%2C%20and%20adopt%20the%20pseudo%20labeling%20technique%20for%20obtaining%20class%20identities%0Aover%20all%20table%20rows.%20We%20also%20explore%20the%20novel%20idea%20of%20selecting%20features%20to%20be%0Acorrupted%20based%20on%20feature%20correlation%20structures.%20Extensive%20experiments%20show%0Athat%20the%20proposed%20approach%20consistently%20outperforms%20the%20conventional%20corruption%0Amethod%20for%20tabular%20data%20classification%20tasks.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/willtop/Tabular-Class-Conditioned-SSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17489v2&entry.124074799=Read"},
{"title": "AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples", "author": "Antonio Emanuele Cin\u00e0 and J\u00e9r\u00f4me Rony and Maura Pintor and Luca Demetrio and Ambra Demontis and Battista Biggio and Ismail Ben Ayed and Fabio Roli", "abstract": "  Adversarial examples are typically optimized with gradient-based attacks.\nWhile novel attacks are continuously proposed, each is shown to outperform its\npredecessors using different experimental setups, hyperparameter settings, and\nnumber of forward and backward calls to the target models. This provides\noverly-optimistic and even biased evaluations that may unfairly favor one\nparticular attack over the others. In this work, we aim to overcome these\nlimitations by proposing AttackBench, i.e., the first evaluation framework that\nenables a fair comparison among different attacks. To this end, we first\npropose a categorization of gradient-based attacks, identifying their main\ncomponents and differences. We then introduce our framework, which evaluates\ntheir effectiveness and efficiency. We measure these characteristics by (i)\ndefining an optimality metric that quantifies how close an attack is to the\noptimal solution, and (ii) limiting the number of forward and backward queries\nto the model, such that all attacks are compared within a given maximum query\nbudget. Our extensive experimental analysis compares more than 100 attack\nimplementations with a total of over 800 different configurations against\nCIFAR-10 and ImageNet models, highlighting that only very few attacks\noutperform all the competing approaches. Within this analysis, we shed light on\nseveral implementation issues that prevent many attacks from finding better\nsolutions or running at all. We release AttackBench as a publicly available\nbenchmark, aiming to continuously update it to include and evaluate novel\ngradient-based attacks for optimizing adversarial examples.\n", "link": "http://arxiv.org/abs/2404.19460v1", "date": "2024-04-30", "relevancy": 2.4201, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4965}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AttackBench%3A%20Evaluating%20Gradient-based%20Attacks%20for%20Adversarial%20Examples&body=Title%3A%20AttackBench%3A%20Evaluating%20Gradient-based%20Attacks%20for%20Adversarial%20Examples%0AAuthor%3A%20Antonio%20Emanuele%20Cin%C3%A0%20and%20J%C3%A9r%C3%B4me%20Rony%20and%20Maura%20Pintor%20and%20Luca%20Demetrio%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Ismail%20Ben%20Ayed%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Adversarial%20examples%20are%20typically%20optimized%20with%20gradient-based%20attacks.%0AWhile%20novel%20attacks%20are%20continuously%20proposed%2C%20each%20is%20shown%20to%20outperform%20its%0Apredecessors%20using%20different%20experimental%20setups%2C%20hyperparameter%20settings%2C%20and%0Anumber%20of%20forward%20and%20backward%20calls%20to%20the%20target%20models.%20This%20provides%0Aoverly-optimistic%20and%20even%20biased%20evaluations%20that%20may%20unfairly%20favor%20one%0Aparticular%20attack%20over%20the%20others.%20In%20this%20work%2C%20we%20aim%20to%20overcome%20these%0Alimitations%20by%20proposing%20AttackBench%2C%20i.e.%2C%20the%20first%20evaluation%20framework%20that%0Aenables%20a%20fair%20comparison%20among%20different%20attacks.%20To%20this%20end%2C%20we%20first%0Apropose%20a%20categorization%20of%20gradient-based%20attacks%2C%20identifying%20their%20main%0Acomponents%20and%20differences.%20We%20then%20introduce%20our%20framework%2C%20which%20evaluates%0Atheir%20effectiveness%20and%20efficiency.%20We%20measure%20these%20characteristics%20by%20%28i%29%0Adefining%20an%20optimality%20metric%20that%20quantifies%20how%20close%20an%20attack%20is%20to%20the%0Aoptimal%20solution%2C%20and%20%28ii%29%20limiting%20the%20number%20of%20forward%20and%20backward%20queries%0Ato%20the%20model%2C%20such%20that%20all%20attacks%20are%20compared%20within%20a%20given%20maximum%20query%0Abudget.%20Our%20extensive%20experimental%20analysis%20compares%20more%20than%20100%20attack%0Aimplementations%20with%20a%20total%20of%20over%20800%20different%20configurations%20against%0ACIFAR-10%20and%20ImageNet%20models%2C%20highlighting%20that%20only%20very%20few%20attacks%0Aoutperform%20all%20the%20competing%20approaches.%20Within%20this%20analysis%2C%20we%20shed%20light%20on%0Aseveral%20implementation%20issues%20that%20prevent%20many%20attacks%20from%20finding%20better%0Asolutions%20or%20running%20at%20all.%20We%20release%20AttackBench%20as%20a%20publicly%20available%0Abenchmark%2C%20aiming%20to%20continuously%20update%20it%20to%20include%20and%20evaluate%20novel%0Agradient-based%20attacks%20for%20optimizing%20adversarial%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19460v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttackBench%3A%20Evaluating%20Gradient-based%20Attacks%20for%20Adversarial%20Examples&entry.906535625=Antonio%20Emanuele%20Cin%C3%A0%20and%20J%C3%A9r%C3%B4me%20Rony%20and%20Maura%20Pintor%20and%20Luca%20Demetrio%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Ismail%20Ben%20Ayed%20and%20Fabio%20Roli&entry.1292438233=%20%20Adversarial%20examples%20are%20typically%20optimized%20with%20gradient-based%20attacks.%0AWhile%20novel%20attacks%20are%20continuously%20proposed%2C%20each%20is%20shown%20to%20outperform%20its%0Apredecessors%20using%20different%20experimental%20setups%2C%20hyperparameter%20settings%2C%20and%0Anumber%20of%20forward%20and%20backward%20calls%20to%20the%20target%20models.%20This%20provides%0Aoverly-optimistic%20and%20even%20biased%20evaluations%20that%20may%20unfairly%20favor%20one%0Aparticular%20attack%20over%20the%20others.%20In%20this%20work%2C%20we%20aim%20to%20overcome%20these%0Alimitations%20by%20proposing%20AttackBench%2C%20i.e.%2C%20the%20first%20evaluation%20framework%20that%0Aenables%20a%20fair%20comparison%20among%20different%20attacks.%20To%20this%20end%2C%20we%20first%0Apropose%20a%20categorization%20of%20gradient-based%20attacks%2C%20identifying%20their%20main%0Acomponents%20and%20differences.%20We%20then%20introduce%20our%20framework%2C%20which%20evaluates%0Atheir%20effectiveness%20and%20efficiency.%20We%20measure%20these%20characteristics%20by%20%28i%29%0Adefining%20an%20optimality%20metric%20that%20quantifies%20how%20close%20an%20attack%20is%20to%20the%0Aoptimal%20solution%2C%20and%20%28ii%29%20limiting%20the%20number%20of%20forward%20and%20backward%20queries%0Ato%20the%20model%2C%20such%20that%20all%20attacks%20are%20compared%20within%20a%20given%20maximum%20query%0Abudget.%20Our%20extensive%20experimental%20analysis%20compares%20more%20than%20100%20attack%0Aimplementations%20with%20a%20total%20of%20over%20800%20different%20configurations%20against%0ACIFAR-10%20and%20ImageNet%20models%2C%20highlighting%20that%20only%20very%20few%20attacks%0Aoutperform%20all%20the%20competing%20approaches.%20Within%20this%20analysis%2C%20we%20shed%20light%20on%0Aseveral%20implementation%20issues%20that%20prevent%20many%20attacks%20from%20finding%20better%0Asolutions%20or%20running%20at%20all.%20We%20release%20AttackBench%20as%20a%20publicly%20available%0Abenchmark%2C%20aiming%20to%20continuously%20update%20it%20to%20include%20and%20evaluate%20novel%0Agradient-based%20attacks%20for%20optimizing%20adversarial%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19460v1&entry.124074799=Read"},
{"title": "One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal\n  Multi-scale and Action Label Features", "author": "Trung Thanh Nguyen and Yasutomo Kawanishi and Takahiro Komamizu and Ichiro Ide", "abstract": "  Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced\nvideo analysis approach that expands Closed-vocabulary Temporal Action\nDetection (Closed-vocab TAD) capabilities. Closed-vocab TAD is typically\nconfined to localizing and classifying actions based on a predefined set of\ncategories. In contrast, Open-vocab TAD goes further and is not limited to\nthese predefined categories. This is particularly useful in real-world\nscenarios where the variety of actions in videos can be vast and not always\npredictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage\napproach, which involves generating action proposals and then identifying those\nactions. However, errors made during the first stage can adversely affect the\nsubsequent action identification accuracy. Additionally, existing studies face\nchallenges in handling actions of different durations owing to the use of fixed\ntemporal processing methods. Therefore, we propose a 1-stage approach\nconsisting of two primary modules: Multi-scale Video Analysis (MVA) and\nVideo-Text Alignment (VTA). The MVA module captures actions at varying temporal\nresolutions, overcoming the challenge of detecting actions with diverse\ndurations. The VTA module leverages the synergy between visual and textual\nmodalities to precisely align video segments with corresponding action labels,\na critical step for accurate action identification in Open-vocab scenarios.\nEvaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed\nthat the proposed method achieved superior results compared to the other\nmethods in both Open-vocab and Closed-vocab settings. This serves as a strong\ndemonstration of the effectiveness of the proposed method in the TAD task.\n", "link": "http://arxiv.org/abs/2404.19542v1", "date": "2024-04-30", "relevancy": 2.4132, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6073}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6067}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5847}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-Stage%20Open-Vocabulary%20Temporal%20Action%20Detection%20Leveraging%20Temporal%0A%20%20Multi-scale%20and%20Action%20Label%20Features&body=Title%3A%20One-Stage%20Open-Vocabulary%20Temporal%20Action%20Detection%20Leveraging%20Temporal%0A%20%20Multi-scale%20and%20Action%20Label%20Features%0AAuthor%3A%20Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide%0AAbstract%3A%20%20%20Open-vocabulary%20Temporal%20Action%20Detection%20%28Open-vocab%20TAD%29%20is%20an%20advanced%0Avideo%20analysis%20approach%20that%20expands%20Closed-vocabulary%20Temporal%20Action%0ADetection%20%28Closed-vocab%20TAD%29%20capabilities.%20Closed-vocab%20TAD%20is%20typically%0Aconfined%20to%20localizing%20and%20classifying%20actions%20based%20on%20a%20predefined%20set%20of%0Acategories.%20In%20contrast%2C%20Open-vocab%20TAD%20goes%20further%20and%20is%20not%20limited%20to%0Athese%20predefined%20categories.%20This%20is%20particularly%20useful%20in%20real-world%0Ascenarios%20where%20the%20variety%20of%20actions%20in%20videos%20can%20be%20vast%20and%20not%20always%0Apredictable.%20The%20prevalent%20methods%20in%20Open-vocab%20TAD%20typically%20employ%20a%202-stage%0Aapproach%2C%20which%20involves%20generating%20action%20proposals%20and%20then%20identifying%20those%0Aactions.%20However%2C%20errors%20made%20during%20the%20first%20stage%20can%20adversely%20affect%20the%0Asubsequent%20action%20identification%20accuracy.%20Additionally%2C%20existing%20studies%20face%0Achallenges%20in%20handling%20actions%20of%20different%20durations%20owing%20to%20the%20use%20of%20fixed%0Atemporal%20processing%20methods.%20Therefore%2C%20we%20propose%20a%201-stage%20approach%0Aconsisting%20of%20two%20primary%20modules%3A%20Multi-scale%20Video%20Analysis%20%28MVA%29%20and%0AVideo-Text%20Alignment%20%28VTA%29.%20The%20MVA%20module%20captures%20actions%20at%20varying%20temporal%0Aresolutions%2C%20overcoming%20the%20challenge%20of%20detecting%20actions%20with%20diverse%0Adurations.%20The%20VTA%20module%20leverages%20the%20synergy%20between%20visual%20and%20textual%0Amodalities%20to%20precisely%20align%20video%20segments%20with%20corresponding%20action%20labels%2C%0Aa%20critical%20step%20for%20accurate%20action%20identification%20in%20Open-vocab%20scenarios.%0AEvaluations%20on%20widely%20recognized%20datasets%20THUMOS14%20and%20ActivityNet-1.3%2C%20showed%0Athat%20the%20proposed%20method%20achieved%20superior%20results%20compared%20to%20the%20other%0Amethods%20in%20both%20Open-vocab%20and%20Closed-vocab%20settings.%20This%20serves%20as%20a%20strong%0Ademonstration%20of%20the%20effectiveness%20of%20the%20proposed%20method%20in%20the%20TAD%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19542v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Stage%20Open-Vocabulary%20Temporal%20Action%20Detection%20Leveraging%20Temporal%0A%20%20Multi-scale%20and%20Action%20Label%20Features&entry.906535625=Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide&entry.1292438233=%20%20Open-vocabulary%20Temporal%20Action%20Detection%20%28Open-vocab%20TAD%29%20is%20an%20advanced%0Avideo%20analysis%20approach%20that%20expands%20Closed-vocabulary%20Temporal%20Action%0ADetection%20%28Closed-vocab%20TAD%29%20capabilities.%20Closed-vocab%20TAD%20is%20typically%0Aconfined%20to%20localizing%20and%20classifying%20actions%20based%20on%20a%20predefined%20set%20of%0Acategories.%20In%20contrast%2C%20Open-vocab%20TAD%20goes%20further%20and%20is%20not%20limited%20to%0Athese%20predefined%20categories.%20This%20is%20particularly%20useful%20in%20real-world%0Ascenarios%20where%20the%20variety%20of%20actions%20in%20videos%20can%20be%20vast%20and%20not%20always%0Apredictable.%20The%20prevalent%20methods%20in%20Open-vocab%20TAD%20typically%20employ%20a%202-stage%0Aapproach%2C%20which%20involves%20generating%20action%20proposals%20and%20then%20identifying%20those%0Aactions.%20However%2C%20errors%20made%20during%20the%20first%20stage%20can%20adversely%20affect%20the%0Asubsequent%20action%20identification%20accuracy.%20Additionally%2C%20existing%20studies%20face%0Achallenges%20in%20handling%20actions%20of%20different%20durations%20owing%20to%20the%20use%20of%20fixed%0Atemporal%20processing%20methods.%20Therefore%2C%20we%20propose%20a%201-stage%20approach%0Aconsisting%20of%20two%20primary%20modules%3A%20Multi-scale%20Video%20Analysis%20%28MVA%29%20and%0AVideo-Text%20Alignment%20%28VTA%29.%20The%20MVA%20module%20captures%20actions%20at%20varying%20temporal%0Aresolutions%2C%20overcoming%20the%20challenge%20of%20detecting%20actions%20with%20diverse%0Adurations.%20The%20VTA%20module%20leverages%20the%20synergy%20between%20visual%20and%20textual%0Amodalities%20to%20precisely%20align%20video%20segments%20with%20corresponding%20action%20labels%2C%0Aa%20critical%20step%20for%20accurate%20action%20identification%20in%20Open-vocab%20scenarios.%0AEvaluations%20on%20widely%20recognized%20datasets%20THUMOS14%20and%20ActivityNet-1.3%2C%20showed%0Athat%20the%20proposed%20method%20achieved%20superior%20results%20compared%20to%20the%20other%0Amethods%20in%20both%20Open-vocab%20and%20Closed-vocab%20settings.%20This%20serves%20as%20a%20strong%0Ademonstration%20of%20the%20effectiveness%20of%20the%20proposed%20method%20in%20the%20TAD%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19542v1&entry.124074799=Read"},
{"title": "Spot The Odd One Out: Regularized Complete Cycle Consistent Anomaly\n  Detector GAN", "author": "Zahra Dehghanian and Saeed Saravani and Maryam Amirmazlaghani and Mohammad Rahmati", "abstract": "  This study presents an adversarial method for anomaly detection in real-world\napplications, leveraging the power of generative adversarial neural networks\n(GANs) through cycle consistency in reconstruction error. Previous methods\nsuffer from the high variance between class-wise accuracy which leads to not\nbeing applicable for all types of anomalies. The proposed method named RCALAD\ntries to solve this problem by introducing a novel discriminator to the\nstructure, which results in a more efficient training process. Additionally,\nRCALAD employs a supplementary distribution in the input space to steer\nreconstructions toward the normal data distribution, effectively separating\nanomalous samples from their reconstructions and facilitating more accurate\nanomaly detection. To further enhance the performance of the model, two novel\nanomaly scores are introduced. The proposed model has been thoroughly evaluated\nthrough extensive experiments on six various datasets, yielding results that\ndemonstrate its superiority over existing state-of-the-art models. The code is\nreadily available to the research community at\nhttps://github.com/zahraDehghanian97/RCALAD.\n", "link": "http://arxiv.org/abs/2304.07769v3", "date": "2024-04-30", "relevancy": 2.3697, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4585}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spot%20The%20Odd%20One%20Out%3A%20Regularized%20Complete%20Cycle%20Consistent%20Anomaly%0A%20%20Detector%20GAN&body=Title%3A%20Spot%20The%20Odd%20One%20Out%3A%20Regularized%20Complete%20Cycle%20Consistent%20Anomaly%0A%20%20Detector%20GAN%0AAuthor%3A%20Zahra%20Dehghanian%20and%20Saeed%20Saravani%20and%20Maryam%20Amirmazlaghani%20and%20Mohammad%20Rahmati%0AAbstract%3A%20%20%20This%20study%20presents%20an%20adversarial%20method%20for%20anomaly%20detection%20in%20real-world%0Aapplications%2C%20leveraging%20the%20power%20of%20generative%20adversarial%20neural%20networks%0A%28GANs%29%20through%20cycle%20consistency%20in%20reconstruction%20error.%20Previous%20methods%0Asuffer%20from%20the%20high%20variance%20between%20class-wise%20accuracy%20which%20leads%20to%20not%0Abeing%20applicable%20for%20all%20types%20of%20anomalies.%20The%20proposed%20method%20named%20RCALAD%0Atries%20to%20solve%20this%20problem%20by%20introducing%20a%20novel%20discriminator%20to%20the%0Astructure%2C%20which%20results%20in%20a%20more%20efficient%20training%20process.%20Additionally%2C%0ARCALAD%20employs%20a%20supplementary%20distribution%20in%20the%20input%20space%20to%20steer%0Areconstructions%20toward%20the%20normal%20data%20distribution%2C%20effectively%20separating%0Aanomalous%20samples%20from%20their%20reconstructions%20and%20facilitating%20more%20accurate%0Aanomaly%20detection.%20To%20further%20enhance%20the%20performance%20of%20the%20model%2C%20two%20novel%0Aanomaly%20scores%20are%20introduced.%20The%20proposed%20model%20has%20been%20thoroughly%20evaluated%0Athrough%20extensive%20experiments%20on%20six%20various%20datasets%2C%20yielding%20results%20that%0Ademonstrate%20its%20superiority%20over%20existing%20state-of-the-art%20models.%20The%20code%20is%0Areadily%20available%20to%20the%20research%20community%20at%0Ahttps%3A//github.com/zahraDehghanian97/RCALAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07769v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spot%20The%20Odd%20One%20Out%3A%20Regularized%20Complete%20Cycle%20Consistent%20Anomaly%0A%20%20Detector%20GAN&entry.906535625=Zahra%20Dehghanian%20and%20Saeed%20Saravani%20and%20Maryam%20Amirmazlaghani%20and%20Mohammad%20Rahmati&entry.1292438233=%20%20This%20study%20presents%20an%20adversarial%20method%20for%20anomaly%20detection%20in%20real-world%0Aapplications%2C%20leveraging%20the%20power%20of%20generative%20adversarial%20neural%20networks%0A%28GANs%29%20through%20cycle%20consistency%20in%20reconstruction%20error.%20Previous%20methods%0Asuffer%20from%20the%20high%20variance%20between%20class-wise%20accuracy%20which%20leads%20to%20not%0Abeing%20applicable%20for%20all%20types%20of%20anomalies.%20The%20proposed%20method%20named%20RCALAD%0Atries%20to%20solve%20this%20problem%20by%20introducing%20a%20novel%20discriminator%20to%20the%0Astructure%2C%20which%20results%20in%20a%20more%20efficient%20training%20process.%20Additionally%2C%0ARCALAD%20employs%20a%20supplementary%20distribution%20in%20the%20input%20space%20to%20steer%0Areconstructions%20toward%20the%20normal%20data%20distribution%2C%20effectively%20separating%0Aanomalous%20samples%20from%20their%20reconstructions%20and%20facilitating%20more%20accurate%0Aanomaly%20detection.%20To%20further%20enhance%20the%20performance%20of%20the%20model%2C%20two%20novel%0Aanomaly%20scores%20are%20introduced.%20The%20proposed%20model%20has%20been%20thoroughly%20evaluated%0Athrough%20extensive%20experiments%20on%20six%20various%20datasets%2C%20yielding%20results%20that%0Ademonstrate%20its%20superiority%20over%20existing%20state-of-the-art%20models.%20The%20code%20is%0Areadily%20available%20to%20the%20research%20community%20at%0Ahttps%3A//github.com/zahraDehghanian97/RCALAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07769v3&entry.124074799=Read"},
{"title": "MoST: Multi-modality Scene Tokenization for Motion Prediction", "author": "Norman Mu and Jingwei Ji and Zhenpei Yang and Nate Harada and Haotian Tang and Kan Chen and Charles R. Qi and Runzhou Ge and Kratarth Goel and Zoey Yang and Scott Ettinger and Rami Al-Rfou and Dragomir Anguelov and Yin Zhou", "abstract": "  Many existing motion prediction approaches rely on symbolic perception\noutputs to generate agent trajectories, such as bounding boxes, road graph\ninformation and traffic lights. This symbolic representation is a high-level\nabstraction of the real world, which may render the motion prediction model\nvulnerable to perception errors (e.g., failures in detecting open-vocabulary\nobstacles) while missing salient information from the scene context (e.g., poor\nroad conditions). An alternative paradigm is end-to-end learning from raw\nsensors. However, this approach suffers from the lack of interpretability and\nrequires significantly more training resources. In this work, we propose\ntokenizing the visual world into a compact set of scene elements and then\nleveraging pre-trained image foundation models and LiDAR neural networks to\nencode all the scene elements in an open-vocabulary manner. The image\nfoundation model enables our scene tokens to encode the general knowledge of\nthe open world while the LiDAR neural network encodes geometry information. Our\nproposed representation can efficiently encode the multi-frame multi-modality\nobservations with a few hundred tokens and is compatible with most\ntransformer-based architectures. To evaluate our method, we have augmented\nWaymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open\nMotion Dataset show that our approach leads to significant performance\nimprovements over the state-of-the-art.\n", "link": "http://arxiv.org/abs/2404.19531v1", "date": "2024-04-30", "relevancy": 2.3657, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5688}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoST%3A%20Multi-modality%20Scene%20Tokenization%20for%20Motion%20Prediction&body=Title%3A%20MoST%3A%20Multi-modality%20Scene%20Tokenization%20for%20Motion%20Prediction%0AAuthor%3A%20Norman%20Mu%20and%20Jingwei%20Ji%20and%20Zhenpei%20Yang%20and%20Nate%20Harada%20and%20Haotian%20Tang%20and%20Kan%20Chen%20and%20Charles%20R.%20Qi%20and%20Runzhou%20Ge%20and%20Kratarth%20Goel%20and%20Zoey%20Yang%20and%20Scott%20Ettinger%20and%20Rami%20Al-Rfou%20and%20Dragomir%20Anguelov%20and%20Yin%20Zhou%0AAbstract%3A%20%20%20Many%20existing%20motion%20prediction%20approaches%20rely%20on%20symbolic%20perception%0Aoutputs%20to%20generate%20agent%20trajectories%2C%20such%20as%20bounding%20boxes%2C%20road%20graph%0Ainformation%20and%20traffic%20lights.%20This%20symbolic%20representation%20is%20a%20high-level%0Aabstraction%20of%20the%20real%20world%2C%20which%20may%20render%20the%20motion%20prediction%20model%0Avulnerable%20to%20perception%20errors%20%28e.g.%2C%20failures%20in%20detecting%20open-vocabulary%0Aobstacles%29%20while%20missing%20salient%20information%20from%20the%20scene%20context%20%28e.g.%2C%20poor%0Aroad%20conditions%29.%20An%20alternative%20paradigm%20is%20end-to-end%20learning%20from%20raw%0Asensors.%20However%2C%20this%20approach%20suffers%20from%20the%20lack%20of%20interpretability%20and%0Arequires%20significantly%20more%20training%20resources.%20In%20this%20work%2C%20we%20propose%0Atokenizing%20the%20visual%20world%20into%20a%20compact%20set%20of%20scene%20elements%20and%20then%0Aleveraging%20pre-trained%20image%20foundation%20models%20and%20LiDAR%20neural%20networks%20to%0Aencode%20all%20the%20scene%20elements%20in%20an%20open-vocabulary%20manner.%20The%20image%0Afoundation%20model%20enables%20our%20scene%20tokens%20to%20encode%20the%20general%20knowledge%20of%0Athe%20open%20world%20while%20the%20LiDAR%20neural%20network%20encodes%20geometry%20information.%20Our%0Aproposed%20representation%20can%20efficiently%20encode%20the%20multi-frame%20multi-modality%0Aobservations%20with%20a%20few%20hundred%20tokens%20and%20is%20compatible%20with%20most%0Atransformer-based%20architectures.%20To%20evaluate%20our%20method%2C%20we%20have%20augmented%0AWaymo%20Open%20Motion%20Dataset%20with%20camera%20embeddings.%20Experiments%20over%20Waymo%20Open%0AMotion%20Dataset%20show%20that%20our%20approach%20leads%20to%20significant%20performance%0Aimprovements%20over%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19531v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoST%3A%20Multi-modality%20Scene%20Tokenization%20for%20Motion%20Prediction&entry.906535625=Norman%20Mu%20and%20Jingwei%20Ji%20and%20Zhenpei%20Yang%20and%20Nate%20Harada%20and%20Haotian%20Tang%20and%20Kan%20Chen%20and%20Charles%20R.%20Qi%20and%20Runzhou%20Ge%20and%20Kratarth%20Goel%20and%20Zoey%20Yang%20and%20Scott%20Ettinger%20and%20Rami%20Al-Rfou%20and%20Dragomir%20Anguelov%20and%20Yin%20Zhou&entry.1292438233=%20%20Many%20existing%20motion%20prediction%20approaches%20rely%20on%20symbolic%20perception%0Aoutputs%20to%20generate%20agent%20trajectories%2C%20such%20as%20bounding%20boxes%2C%20road%20graph%0Ainformation%20and%20traffic%20lights.%20This%20symbolic%20representation%20is%20a%20high-level%0Aabstraction%20of%20the%20real%20world%2C%20which%20may%20render%20the%20motion%20prediction%20model%0Avulnerable%20to%20perception%20errors%20%28e.g.%2C%20failures%20in%20detecting%20open-vocabulary%0Aobstacles%29%20while%20missing%20salient%20information%20from%20the%20scene%20context%20%28e.g.%2C%20poor%0Aroad%20conditions%29.%20An%20alternative%20paradigm%20is%20end-to-end%20learning%20from%20raw%0Asensors.%20However%2C%20this%20approach%20suffers%20from%20the%20lack%20of%20interpretability%20and%0Arequires%20significantly%20more%20training%20resources.%20In%20this%20work%2C%20we%20propose%0Atokenizing%20the%20visual%20world%20into%20a%20compact%20set%20of%20scene%20elements%20and%20then%0Aleveraging%20pre-trained%20image%20foundation%20models%20and%20LiDAR%20neural%20networks%20to%0Aencode%20all%20the%20scene%20elements%20in%20an%20open-vocabulary%20manner.%20The%20image%0Afoundation%20model%20enables%20our%20scene%20tokens%20to%20encode%20the%20general%20knowledge%20of%0Athe%20open%20world%20while%20the%20LiDAR%20neural%20network%20encodes%20geometry%20information.%20Our%0Aproposed%20representation%20can%20efficiently%20encode%20the%20multi-frame%20multi-modality%0Aobservations%20with%20a%20few%20hundred%20tokens%20and%20is%20compatible%20with%20most%0Atransformer-based%20architectures.%20To%20evaluate%20our%20method%2C%20we%20have%20augmented%0AWaymo%20Open%20Motion%20Dataset%20with%20camera%20embeddings.%20Experiments%20over%20Waymo%20Open%0AMotion%20Dataset%20show%20that%20our%20approach%20leads%20to%20significant%20performance%0Aimprovements%20over%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19531v1&entry.124074799=Read"},
{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "author": "Zhexi Peng and Tianjia Shao and Yong Liu and Jingke Zhou and Yin Yang and Jingdong Wang and Kun Zhou", "abstract": "  We propose RTG-SLAM, a real-time 3D reconstruction system with an RGBD camera\nfor large-scale environments using Gaussian splatting. RTG-SLAM features a\ncompact Gaussian representation and a highly efficient on-the-fly Gaussian\noptimization scheme. We force each Gaussian to be either opaque or nearly\ntransparent, with the opaque ones fitting the surface and dominant colors, and\ntransparent ones fitting residual colors. By rendering depth in a different way\nfrom color rendering, we let a single opaque Gaussian well fit a local surface\nregion without the need of multiple overlapping Gaussians, hence largely\nreducing the memory and computation cost. For on-the-fly Gaussian optimization,\nwe explicitly add Gaussians for three types of pixels per frame: newly\nobserved, with large color errors and with large depth errors. We also\ncategorize all Gaussians into stable and unstable ones, where the stable\nGaussians are expected to well fit previously observed RGBD images and\notherwise unstable. We only optimize the unstable Gaussians and only render the\npixels occupied by unstable Gaussians. In this way, both the number of\nGaussians to be optimized and pixels to be rendered are largely reduced, and\nthe optimization can be done in real time. We show real-time reconstructions of\na variety of real large scenes. Compared with the state-of-the-art NeRF-based\nRGBD SLAM, our system achieves comparable high-quality reconstruction but with\naround twice the speed and half the memory cost, and shows superior performance\nin the realism of novel view synthesis and camera tracking accuracy.\n", "link": "http://arxiv.org/abs/2404.19706v1", "date": "2024-04-30", "relevancy": 2.3596, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5321}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&body=Title%3A%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting%0AAuthor%3A%20Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20propose%20RTG-SLAM%2C%20a%20real-time%203D%20reconstruction%20system%20with%20an%20RGBD%20camera%0Afor%20large-scale%20environments%20using%20Gaussian%20splatting.%20RTG-SLAM%20features%20a%0Acompact%20Gaussian%20representation%20and%20a%20highly%20efficient%20on-the-fly%20Gaussian%0Aoptimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%20either%20opaque%20or%20nearly%0Atransparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%20and%20dominant%20colors%2C%20and%0Atransparent%20ones%20fitting%20residual%20colors.%20By%20rendering%20depth%20in%20a%20different%20way%0Afrom%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%20well%20fit%20a%20local%20surface%0Aregion%20without%20the%20need%20of%20multiple%20overlapping%20Gaussians%2C%20hence%20largely%0Areducing%20the%20memory%20and%20computation%20cost.%20For%20on-the-fly%20Gaussian%20optimization%2C%0Awe%20explicitly%20add%20Gaussians%20for%20three%20types%20of%20pixels%20per%20frame%3A%20newly%0Aobserved%2C%20with%20large%20color%20errors%20and%20with%20large%20depth%20errors.%20We%20also%0Acategorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%20where%20the%20stable%0AGaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%20images%20and%0Aotherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%20render%20the%0Apixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%20of%0AGaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20real%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%0ARGBD%20SLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19706v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&entry.906535625=Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20propose%20RTG-SLAM%2C%20a%20real-time%203D%20reconstruction%20system%20with%20an%20RGBD%20camera%0Afor%20large-scale%20environments%20using%20Gaussian%20splatting.%20RTG-SLAM%20features%20a%0Acompact%20Gaussian%20representation%20and%20a%20highly%20efficient%20on-the-fly%20Gaussian%0Aoptimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%20either%20opaque%20or%20nearly%0Atransparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%20and%20dominant%20colors%2C%20and%0Atransparent%20ones%20fitting%20residual%20colors.%20By%20rendering%20depth%20in%20a%20different%20way%0Afrom%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%20well%20fit%20a%20local%20surface%0Aregion%20without%20the%20need%20of%20multiple%20overlapping%20Gaussians%2C%20hence%20largely%0Areducing%20the%20memory%20and%20computation%20cost.%20For%20on-the-fly%20Gaussian%20optimization%2C%0Awe%20explicitly%20add%20Gaussians%20for%20three%20types%20of%20pixels%20per%20frame%3A%20newly%0Aobserved%2C%20with%20large%20color%20errors%20and%20with%20large%20depth%20errors.%20We%20also%0Acategorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%20where%20the%20stable%0AGaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%20images%20and%0Aotherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%20render%20the%0Apixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%20of%0AGaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20real%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%0ARGBD%20SLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19706v1&entry.124074799=Read"},
{"title": "TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image\n  Generation with Diffusion Models", "author": "Teng Zhou and Yongchuan Tang", "abstract": "  Diffusion models have emerged as effective tools for generating diverse and\nhigh-quality content. However, their capability in high-resolution image\ngeneration, particularly for panoramic images, still faces challenges such as\nvisible seams and incoherent transitions. In this paper, we propose\nTwinDiffusion, an optimized framework designed to address these challenges\nthrough two key innovations: Crop Fusion for quality enhancement and Cross\nSampling for efficiency optimization. We introduce a training-free optimizing\nstage to refine the similarity of the adjacent image areas, as well as an\ninterleaving sampling strategy to yield dynamic patches during the cropping\nprocess. A comprehensive evaluation is conducted to compare TwinDiffusion with\nthe existing methods, considering factors including coherence, fidelity,\ncompatibility, and efficiency. The results demonstrate the superior performance\nof our approach in generating seamless and coherent panoramas, setting a new\nstandard in quality and efficiency for panoramic image generation.\n", "link": "http://arxiv.org/abs/2404.19475v1", "date": "2024-04-30", "relevancy": 2.317, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6192}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5742}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5683}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models&body=Title%3A%20TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models%0AAuthor%3A%20Teng%20Zhou%20and%20Yongchuan%20Tang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20effective%20tools%20for%20generating%20diverse%20and%0Ahigh-quality%20content.%20However%2C%20their%20capability%20in%20high-resolution%20image%0Ageneration%2C%20particularly%20for%20panoramic%20images%2C%20still%20faces%20challenges%20such%20as%0Avisible%20seams%20and%20incoherent%20transitions.%20In%20this%20paper%2C%20we%20propose%0ATwinDiffusion%2C%20an%20optimized%20framework%20designed%20to%20address%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Crop%20Fusion%20for%20quality%20enhancement%20and%20Cross%0ASampling%20for%20efficiency%20optimization.%20We%20introduce%20a%20training-free%20optimizing%0Astage%20to%20refine%20the%20similarity%20of%20the%20adjacent%20image%20areas%2C%20as%20well%20as%20an%0Ainterleaving%20sampling%20strategy%20to%20yield%20dynamic%20patches%20during%20the%20cropping%0Aprocess.%20A%20comprehensive%20evaluation%20is%20conducted%20to%20compare%20TwinDiffusion%20with%0Athe%20existing%20methods%2C%20considering%20factors%20including%20coherence%2C%20fidelity%2C%0Acompatibility%2C%20and%20efficiency.%20The%20results%20demonstrate%20the%20superior%20performance%0Aof%20our%20approach%20in%20generating%20seamless%20and%20coherent%20panoramas%2C%20setting%20a%20new%0Astandard%20in%20quality%20and%20efficiency%20for%20panoramic%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19475v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwinDiffusion%3A%20Enhancing%20Coherence%20and%20Efficiency%20in%20Panoramic%20Image%0A%20%20Generation%20with%20Diffusion%20Models&entry.906535625=Teng%20Zhou%20and%20Yongchuan%20Tang&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20effective%20tools%20for%20generating%20diverse%20and%0Ahigh-quality%20content.%20However%2C%20their%20capability%20in%20high-resolution%20image%0Ageneration%2C%20particularly%20for%20panoramic%20images%2C%20still%20faces%20challenges%20such%20as%0Avisible%20seams%20and%20incoherent%20transitions.%20In%20this%20paper%2C%20we%20propose%0ATwinDiffusion%2C%20an%20optimized%20framework%20designed%20to%20address%20these%20challenges%0Athrough%20two%20key%20innovations%3A%20Crop%20Fusion%20for%20quality%20enhancement%20and%20Cross%0ASampling%20for%20efficiency%20optimization.%20We%20introduce%20a%20training-free%20optimizing%0Astage%20to%20refine%20the%20similarity%20of%20the%20adjacent%20image%20areas%2C%20as%20well%20as%20an%0Ainterleaving%20sampling%20strategy%20to%20yield%20dynamic%20patches%20during%20the%20cropping%0Aprocess.%20A%20comprehensive%20evaluation%20is%20conducted%20to%20compare%20TwinDiffusion%20with%0Athe%20existing%20methods%2C%20considering%20factors%20including%20coherence%2C%20fidelity%2C%0Acompatibility%2C%20and%20efficiency.%20The%20results%20demonstrate%20the%20superior%20performance%0Aof%20our%20approach%20in%20generating%20seamless%20and%20coherent%20panoramas%2C%20setting%20a%20new%0Astandard%20in%20quality%20and%20efficiency%20for%20panoramic%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19475v1&entry.124074799=Read"},
{"title": "MicroDreamer: Zero-shot 3D Generation in $\\sim$20 Seconds by Score-based\n  Iterative Reconstruction", "author": "Luxi Chen and Zhengyi Wang and Chongxuan Li and Tingting Gao and Hang Su and Jun Zhu", "abstract": "  Optimization-based approaches, such as score distillation sampling (SDS),\nshow promise in zero-shot 3D generation but suffer from low efficiency,\nprimarily due to the high number of function evaluations (NFEs) required for\neach sample. In this paper, we introduce score-based iterative reconstruction\n(SIR), an efficient and general algorithm for 3D generation with a multi-view\nscore-based diffusion model. Given the images produced by the diffusion model,\nSIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single\noptimization in SDS, mimicking the 3D reconstruction process. With other\nimprovements including optimization in the pixel space, we present an efficient\napproach called MicroDreamer that generally applies to various 3D\nrepresentations and 3D generation tasks. In particular, retaining a comparable\nperformance, MicroDreamer is 5-20 times faster than SDS in generating neural\nradiance field and takes about 20 seconds to generate meshes from 3D Gaussian\nsplitting on a single A100 GPU, halving the time of the fastest zero-shot\nbaseline, DreamGaussian. Our code is available at\nhttps://github.com/ML-GSAI/MicroDreamer.\n", "link": "http://arxiv.org/abs/2404.19525v1", "date": "2024-04-30", "relevancy": 2.3164, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6073}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5905}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5564}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MicroDreamer%3A%20Zero-shot%203D%20Generation%20in%20%24%5Csim%2420%20Seconds%20by%20Score-based%0A%20%20Iterative%20Reconstruction&body=Title%3A%20MicroDreamer%3A%20Zero-shot%203D%20Generation%20in%20%24%5Csim%2420%20Seconds%20by%20Score-based%0A%20%20Iterative%20Reconstruction%0AAuthor%3A%20Luxi%20Chen%20and%20Zhengyi%20Wang%20and%20Chongxuan%20Li%20and%20Tingting%20Gao%20and%20Hang%20Su%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20Optimization-based%20approaches%2C%20such%20as%20score%20distillation%20sampling%20%28SDS%29%2C%0Ashow%20promise%20in%20zero-shot%203D%20generation%20but%20suffer%20from%20low%20efficiency%2C%0Aprimarily%20due%20to%20the%20high%20number%20of%20function%20evaluations%20%28NFEs%29%20required%20for%0Aeach%20sample.%20In%20this%20paper%2C%20we%20introduce%20score-based%20iterative%20reconstruction%0A%28SIR%29%2C%20an%20efficient%20and%20general%20algorithm%20for%203D%20generation%20with%20a%20multi-view%0Ascore-based%20diffusion%20model.%20Given%20the%20images%20produced%20by%20the%20diffusion%20model%2C%0ASIR%20reduces%20NFEs%20by%20repeatedly%20optimizing%203D%20parameters%2C%20unlike%20the%20single%0Aoptimization%20in%20SDS%2C%20mimicking%20the%203D%20reconstruction%20process.%20With%20other%0Aimprovements%20including%20optimization%20in%20the%20pixel%20space%2C%20we%20present%20an%20efficient%0Aapproach%20called%20MicroDreamer%20that%20generally%20applies%20to%20various%203D%0Arepresentations%20and%203D%20generation%20tasks.%20In%20particular%2C%20retaining%20a%20comparable%0Aperformance%2C%20MicroDreamer%20is%205-20%20times%20faster%20than%20SDS%20in%20generating%20neural%0Aradiance%20field%20and%20takes%20about%2020%20seconds%20to%20generate%20meshes%20from%203D%20Gaussian%0Asplitting%20on%20a%20single%20A100%20GPU%2C%20halving%20the%20time%20of%20the%20fastest%20zero-shot%0Abaseline%2C%20DreamGaussian.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ML-GSAI/MicroDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19525v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroDreamer%3A%20Zero-shot%203D%20Generation%20in%20%24%5Csim%2420%20Seconds%20by%20Score-based%0A%20%20Iterative%20Reconstruction&entry.906535625=Luxi%20Chen%20and%20Zhengyi%20Wang%20and%20Chongxuan%20Li%20and%20Tingting%20Gao%20and%20Hang%20Su%20and%20Jun%20Zhu&entry.1292438233=%20%20Optimization-based%20approaches%2C%20such%20as%20score%20distillation%20sampling%20%28SDS%29%2C%0Ashow%20promise%20in%20zero-shot%203D%20generation%20but%20suffer%20from%20low%20efficiency%2C%0Aprimarily%20due%20to%20the%20high%20number%20of%20function%20evaluations%20%28NFEs%29%20required%20for%0Aeach%20sample.%20In%20this%20paper%2C%20we%20introduce%20score-based%20iterative%20reconstruction%0A%28SIR%29%2C%20an%20efficient%20and%20general%20algorithm%20for%203D%20generation%20with%20a%20multi-view%0Ascore-based%20diffusion%20model.%20Given%20the%20images%20produced%20by%20the%20diffusion%20model%2C%0ASIR%20reduces%20NFEs%20by%20repeatedly%20optimizing%203D%20parameters%2C%20unlike%20the%20single%0Aoptimization%20in%20SDS%2C%20mimicking%20the%203D%20reconstruction%20process.%20With%20other%0Aimprovements%20including%20optimization%20in%20the%20pixel%20space%2C%20we%20present%20an%20efficient%0Aapproach%20called%20MicroDreamer%20that%20generally%20applies%20to%20various%203D%0Arepresentations%20and%203D%20generation%20tasks.%20In%20particular%2C%20retaining%20a%20comparable%0Aperformance%2C%20MicroDreamer%20is%205-20%20times%20faster%20than%20SDS%20in%20generating%20neural%0Aradiance%20field%20and%20takes%20about%2020%20seconds%20to%20generate%20meshes%20from%203D%20Gaussian%0Asplitting%20on%20a%20single%20A100%20GPU%2C%20halving%20the%20time%20of%20the%20fastest%20zero-shot%0Abaseline%2C%20DreamGaussian.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ML-GSAI/MicroDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19525v1&entry.124074799=Read"},
{"title": "Let's Focus: Focused Backdoor Attack against Federated Transfer Learning", "author": "Marco Arazzi and Stefanos Koffas and Antonino Nocera and Stjepan Picek", "abstract": "  Federated Transfer Learning (FTL) is the most general variation of Federated\nLearning. According to this distributed paradigm, a feature learning pre-step\nis commonly carried out by only one party, typically the server, on publicly\nshared data. After that, the Federated Learning phase takes place to train a\nclassifier collaboratively using the learned feature extractor. Each involved\nclient contributes by locally training only the classification layers on a\nprivate training set. The peculiarity of an FTL scenario makes it hard to\nunderstand whether poisoning attacks can be developed to craft an effective\nbackdoor. State-of-the-art attack strategies assume the possibility of shifting\nthe model attention toward relevant features introduced by a forged trigger\ninjected in the input data by some untrusted clients. Of course, this is not\nfeasible in FTL, as the learned features are fixed once the server performs the\npre-training step. Consequently, in this paper, we investigate this intriguing\nFederated Learning scenario to identify and exploit a vulnerability obtained by\ncombining eXplainable AI (XAI) and dataset distillation. In particular, the\nproposed attack can be carried out by one of the clients during the Federated\nLearning phase of FTL by identifying the optimal local for the trigger through\nXAI and encapsulating compressed information of the backdoor class. Due to its\nbehavior, we refer to our approach as a focused backdoor approach (FB-FTL for\nshort) and test its performance by explicitly referencing an image\nclassification scenario. With an average 80% attack success rate, obtained\nresults show the effectiveness of our attack also against existing defenses for\nFederated Learning.\n", "link": "http://arxiv.org/abs/2404.19420v1", "date": "2024-04-30", "relevancy": 2.2981, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4469}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Focus%3A%20Focused%20Backdoor%20Attack%20against%20Federated%20Transfer%20Learning&body=Title%3A%20Let%27s%20Focus%3A%20Focused%20Backdoor%20Attack%20against%20Federated%20Transfer%20Learning%0AAuthor%3A%20Marco%20Arazzi%20and%20Stefanos%20Koffas%20and%20Antonino%20Nocera%20and%20Stjepan%20Picek%0AAbstract%3A%20%20%20Federated%20Transfer%20Learning%20%28FTL%29%20is%20the%20most%20general%20variation%20of%20Federated%0ALearning.%20According%20to%20this%20distributed%20paradigm%2C%20a%20feature%20learning%20pre-step%0Ais%20commonly%20carried%20out%20by%20only%20one%20party%2C%20typically%20the%20server%2C%20on%20publicly%0Ashared%20data.%20After%20that%2C%20the%20Federated%20Learning%20phase%20takes%20place%20to%20train%20a%0Aclassifier%20collaboratively%20using%20the%20learned%20feature%20extractor.%20Each%20involved%0Aclient%20contributes%20by%20locally%20training%20only%20the%20classification%20layers%20on%20a%0Aprivate%20training%20set.%20The%20peculiarity%20of%20an%20FTL%20scenario%20makes%20it%20hard%20to%0Aunderstand%20whether%20poisoning%20attacks%20can%20be%20developed%20to%20craft%20an%20effective%0Abackdoor.%20State-of-the-art%20attack%20strategies%20assume%20the%20possibility%20of%20shifting%0Athe%20model%20attention%20toward%20relevant%20features%20introduced%20by%20a%20forged%20trigger%0Ainjected%20in%20the%20input%20data%20by%20some%20untrusted%20clients.%20Of%20course%2C%20this%20is%20not%0Afeasible%20in%20FTL%2C%20as%20the%20learned%20features%20are%20fixed%20once%20the%20server%20performs%20the%0Apre-training%20step.%20Consequently%2C%20in%20this%20paper%2C%20we%20investigate%20this%20intriguing%0AFederated%20Learning%20scenario%20to%20identify%20and%20exploit%20a%20vulnerability%20obtained%20by%0Acombining%20eXplainable%20AI%20%28XAI%29%20and%20dataset%20distillation.%20In%20particular%2C%20the%0Aproposed%20attack%20can%20be%20carried%20out%20by%20one%20of%20the%20clients%20during%20the%20Federated%0ALearning%20phase%20of%20FTL%20by%20identifying%20the%20optimal%20local%20for%20the%20trigger%20through%0AXAI%20and%20encapsulating%20compressed%20information%20of%20the%20backdoor%20class.%20Due%20to%20its%0Abehavior%2C%20we%20refer%20to%20our%20approach%20as%20a%20focused%20backdoor%20approach%20%28FB-FTL%20for%0Ashort%29%20and%20test%20its%20performance%20by%20explicitly%20referencing%20an%20image%0Aclassification%20scenario.%20With%20an%20average%2080%25%20attack%20success%20rate%2C%20obtained%0Aresults%20show%20the%20effectiveness%20of%20our%20attack%20also%20against%20existing%20defenses%20for%0AFederated%20Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19420v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Focus%3A%20Focused%20Backdoor%20Attack%20against%20Federated%20Transfer%20Learning&entry.906535625=Marco%20Arazzi%20and%20Stefanos%20Koffas%20and%20Antonino%20Nocera%20and%20Stjepan%20Picek&entry.1292438233=%20%20Federated%20Transfer%20Learning%20%28FTL%29%20is%20the%20most%20general%20variation%20of%20Federated%0ALearning.%20According%20to%20this%20distributed%20paradigm%2C%20a%20feature%20learning%20pre-step%0Ais%20commonly%20carried%20out%20by%20only%20one%20party%2C%20typically%20the%20server%2C%20on%20publicly%0Ashared%20data.%20After%20that%2C%20the%20Federated%20Learning%20phase%20takes%20place%20to%20train%20a%0Aclassifier%20collaboratively%20using%20the%20learned%20feature%20extractor.%20Each%20involved%0Aclient%20contributes%20by%20locally%20training%20only%20the%20classification%20layers%20on%20a%0Aprivate%20training%20set.%20The%20peculiarity%20of%20an%20FTL%20scenario%20makes%20it%20hard%20to%0Aunderstand%20whether%20poisoning%20attacks%20can%20be%20developed%20to%20craft%20an%20effective%0Abackdoor.%20State-of-the-art%20attack%20strategies%20assume%20the%20possibility%20of%20shifting%0Athe%20model%20attention%20toward%20relevant%20features%20introduced%20by%20a%20forged%20trigger%0Ainjected%20in%20the%20input%20data%20by%20some%20untrusted%20clients.%20Of%20course%2C%20this%20is%20not%0Afeasible%20in%20FTL%2C%20as%20the%20learned%20features%20are%20fixed%20once%20the%20server%20performs%20the%0Apre-training%20step.%20Consequently%2C%20in%20this%20paper%2C%20we%20investigate%20this%20intriguing%0AFederated%20Learning%20scenario%20to%20identify%20and%20exploit%20a%20vulnerability%20obtained%20by%0Acombining%20eXplainable%20AI%20%28XAI%29%20and%20dataset%20distillation.%20In%20particular%2C%20the%0Aproposed%20attack%20can%20be%20carried%20out%20by%20one%20of%20the%20clients%20during%20the%20Federated%0ALearning%20phase%20of%20FTL%20by%20identifying%20the%20optimal%20local%20for%20the%20trigger%20through%0AXAI%20and%20encapsulating%20compressed%20information%20of%20the%20backdoor%20class.%20Due%20to%20its%0Abehavior%2C%20we%20refer%20to%20our%20approach%20as%20a%20focused%20backdoor%20approach%20%28FB-FTL%20for%0Ashort%29%20and%20test%20its%20performance%20by%20explicitly%20referencing%20an%20image%0Aclassification%20scenario.%20With%20an%20average%2080%25%20attack%20success%20rate%2C%20obtained%0Aresults%20show%20the%20effectiveness%20of%20our%20attack%20also%20against%20existing%20defenses%20for%0AFederated%20Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19420v1&entry.124074799=Read"},
{"title": "SeaTurtleID2022: A long-span dataset for reliable sea turtle\n  re-identification", "author": "Luk\u00e1\u0161 Adam and Vojt\u011bch \u010cerm\u00e1k and Kostas Papafitsoros and Luk\u00e1\u0161 Picek", "abstract": "  This paper introduces the first public large-scale, long-span dataset with\nsea turtle photographs captured in the wild -- SeaTurtleID2022\n(https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022). The dataset\ncontains 8729 photographs of 438 unique individuals collected within 13 years,\nmaking it the longest-spanned dataset for animal re-identification. All\nphotographs include various annotations, e.g., identity, encounter timestamp,\nand body parts segmentation masks. Instead of standard \"random\" splits, the\ndataset allows for two realistic and ecologically motivated splits: (i) a\ntime-aware closed-set with training, validation, and test data from different\ndays/years, and (ii) a time-aware open-set with new unknown individuals in test\nand validation sets. We show that time-aware splits are essential for\nbenchmarking re-identification methods, as random splits lead to performance\noverestimation. Furthermore, a baseline instance segmentation and\nre-identification performance over various body parts is provided. Finally, an\nend-to-end system for sea turtle re-identification is proposed and evaluated.\nThe proposed system based on Hybrid Task Cascade for head instance segmentation\nand ArcFace-trained feature-extractor achieved an accuracy of 86.8%.\n", "link": "http://arxiv.org/abs/2311.05524v2", "date": "2024-04-30", "relevancy": 2.2974, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&body=Title%3A%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification%0AAuthor%3A%20Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%20SeaTurtleID2022%0A%28https%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%29.%20The%20dataset%0Acontains%208729%20photographs%20of%20438%20unique%20individuals%20collected%20within%2013%20years%2C%0Amaking%20it%20the%20longest-spanned%20dataset%20for%20animal%20re-identification.%20All%0Aphotographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%20encounter%20timestamp%2C%0Aand%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%20%22random%22%20splits%2C%20the%0Adataset%20allows%20for%20two%20realistic%20and%20ecologically%20motivated%20splits%3A%20%28i%29%20a%0Atime-aware%20closed-set%20with%20training%2C%20validation%2C%20and%20test%20data%20from%20different%0Adays/years%2C%20and%20%28ii%29%20a%20time-aware%20open-set%20with%20new%20unknown%20individuals%20in%20test%0Aand%20validation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%0Abenchmarking%20re-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%0Aoverestimation.%20Furthermore%2C%20a%20baseline%20instance%20segmentation%20and%0Are-identification%20performance%20over%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%0Aend-to-end%20system%20for%20sea%20turtle%20re-identification%20is%20proposed%20and%20evaluated.%0AThe%20proposed%20system%20based%20on%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%0Aand%20ArcFace-trained%20feature-extractor%20achieved%20an%20accuracy%20of%2086.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05524v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&entry.906535625=Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek&entry.1292438233=%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%20SeaTurtleID2022%0A%28https%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%29.%20The%20dataset%0Acontains%208729%20photographs%20of%20438%20unique%20individuals%20collected%20within%2013%20years%2C%0Amaking%20it%20the%20longest-spanned%20dataset%20for%20animal%20re-identification.%20All%0Aphotographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%20encounter%20timestamp%2C%0Aand%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%20%22random%22%20splits%2C%20the%0Adataset%20allows%20for%20two%20realistic%20and%20ecologically%20motivated%20splits%3A%20%28i%29%20a%0Atime-aware%20closed-set%20with%20training%2C%20validation%2C%20and%20test%20data%20from%20different%0Adays/years%2C%20and%20%28ii%29%20a%20time-aware%20open-set%20with%20new%20unknown%20individuals%20in%20test%0Aand%20validation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%0Abenchmarking%20re-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%0Aoverestimation.%20Furthermore%2C%20a%20baseline%20instance%20segmentation%20and%0Are-identification%20performance%20over%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%0Aend-to-end%20system%20for%20sea%20turtle%20re-identification%20is%20proposed%20and%20evaluated.%0AThe%20proposed%20system%20based%20on%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%0Aand%20ArcFace-trained%20feature-extractor%20achieved%20an%20accuracy%20of%2086.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05524v2&entry.124074799=Read"},
{"title": "Visible-Infrared Person Re-Identification via Patch-Mixed Cross-Modality\n  Learning", "author": "Zhihao Qian and Yutian Lin and Bo Du", "abstract": "  Visible-infrared person re-identification (VI-ReID) aims to retrieve images\nof the same pedestrian from different modalities, where the challenges lie in\nthe significant modality discrepancy. To alleviate the modality gap, recent\nmethods generate intermediate images by GANs, grayscaling, or mixup strategies.\nHowever, these methods could introduce extra data distribution, and the\nsemantic correspondence between the two modalities is not well learned. In this\npaper, we propose a Patch-Mixed Cross-Modality framework (PMCM), where two\nimages of the same person from two modalities are split into patches and\nstitched into a new one for model learning. A part-alignment loss is introduced\nto regularize representation learning, and a patch-mixed modality learning loss\nis proposed to align between the modalities. In this way, the model learns to\nrecognize a person through patches of different styles, thereby the modality\nsemantic correspondence can be inferred. In addition, with the flexible image\ngeneration strategy, the patch-mixed images freely adjust the ratio of\ndifferent modality patches, which could further alleviate the modality\nimbalance problem. On two VI-ReID datasets, we report new state-of-the-art\nperformance with the proposed method.\n", "link": "http://arxiv.org/abs/2302.08212v2", "date": "2024-04-30", "relevancy": 2.2944, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5978}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5663}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5523}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visible-Infrared%20Person%20Re-Identification%20via%20Patch-Mixed%20Cross-Modality%0A%20%20Learning&body=Title%3A%20Visible-Infrared%20Person%20Re-Identification%20via%20Patch-Mixed%20Cross-Modality%0A%20%20Learning%0AAuthor%3A%20Zhihao%20Qian%20and%20Yutian%20Lin%20and%20Bo%20Du%0AAbstract%3A%20%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20aims%20to%20retrieve%20images%0Aof%20the%20same%20pedestrian%20from%20different%20modalities%2C%20where%20the%20challenges%20lie%20in%0Athe%20significant%20modality%20discrepancy.%20To%20alleviate%20the%20modality%20gap%2C%20recent%0Amethods%20generate%20intermediate%20images%20by%20GANs%2C%20grayscaling%2C%20or%20mixup%20strategies.%0AHowever%2C%20these%20methods%20could%20introduce%20extra%20data%20distribution%2C%20and%20the%0Asemantic%20correspondence%20between%20the%20two%20modalities%20is%20not%20well%20learned.%20In%20this%0Apaper%2C%20we%20propose%20a%20Patch-Mixed%20Cross-Modality%20framework%20%28PMCM%29%2C%20where%20two%0Aimages%20of%20the%20same%20person%20from%20two%20modalities%20are%20split%20into%20patches%20and%0Astitched%20into%20a%20new%20one%20for%20model%20learning.%20A%20part-alignment%20loss%20is%20introduced%0Ato%20regularize%20representation%20learning%2C%20and%20a%20patch-mixed%20modality%20learning%20loss%0Ais%20proposed%20to%20align%20between%20the%20modalities.%20In%20this%20way%2C%20the%20model%20learns%20to%0Arecognize%20a%20person%20through%20patches%20of%20different%20styles%2C%20thereby%20the%20modality%0Asemantic%20correspondence%20can%20be%20inferred.%20In%20addition%2C%20with%20the%20flexible%20image%0Ageneration%20strategy%2C%20the%20patch-mixed%20images%20freely%20adjust%20the%20ratio%20of%0Adifferent%20modality%20patches%2C%20which%20could%20further%20alleviate%20the%20modality%0Aimbalance%20problem.%20On%20two%20VI-ReID%20datasets%2C%20we%20report%20new%20state-of-the-art%0Aperformance%20with%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08212v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visible-Infrared%20Person%20Re-Identification%20via%20Patch-Mixed%20Cross-Modality%0A%20%20Learning&entry.906535625=Zhihao%20Qian%20and%20Yutian%20Lin%20and%20Bo%20Du&entry.1292438233=%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20aims%20to%20retrieve%20images%0Aof%20the%20same%20pedestrian%20from%20different%20modalities%2C%20where%20the%20challenges%20lie%20in%0Athe%20significant%20modality%20discrepancy.%20To%20alleviate%20the%20modality%20gap%2C%20recent%0Amethods%20generate%20intermediate%20images%20by%20GANs%2C%20grayscaling%2C%20or%20mixup%20strategies.%0AHowever%2C%20these%20methods%20could%20introduce%20extra%20data%20distribution%2C%20and%20the%0Asemantic%20correspondence%20between%20the%20two%20modalities%20is%20not%20well%20learned.%20In%20this%0Apaper%2C%20we%20propose%20a%20Patch-Mixed%20Cross-Modality%20framework%20%28PMCM%29%2C%20where%20two%0Aimages%20of%20the%20same%20person%20from%20two%20modalities%20are%20split%20into%20patches%20and%0Astitched%20into%20a%20new%20one%20for%20model%20learning.%20A%20part-alignment%20loss%20is%20introduced%0Ato%20regularize%20representation%20learning%2C%20and%20a%20patch-mixed%20modality%20learning%20loss%0Ais%20proposed%20to%20align%20between%20the%20modalities.%20In%20this%20way%2C%20the%20model%20learns%20to%0Arecognize%20a%20person%20through%20patches%20of%20different%20styles%2C%20thereby%20the%20modality%0Asemantic%20correspondence%20can%20be%20inferred.%20In%20addition%2C%20with%20the%20flexible%20image%0Ageneration%20strategy%2C%20the%20patch-mixed%20images%20freely%20adjust%20the%20ratio%20of%0Adifferent%20modality%20patches%2C%20which%20could%20further%20alleviate%20the%20modality%0Aimbalance%20problem.%20On%20two%20VI-ReID%20datasets%2C%20we%20report%20new%20state-of-the-art%0Aperformance%20with%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08212v2&entry.124074799=Read"},
{"title": "auto-sktime: Automated Time Series Forecasting", "author": "Marc-Andr\u00e9 Z\u00f6ller and Marius Lindauer and Marco F. Huber", "abstract": "  In today's data-driven landscape, time series forecasting is pivotal in\ndecision-making across various sectors. Yet, the proliferation of more diverse\ntime series data, coupled with the expanding landscape of available forecasting\nmethods, poses significant challenges for forecasters. To meet the growing\ndemand for efficient forecasting, we introduce auto-sktime, a novel framework\nfor automated time series forecasting. The proposed framework uses the power of\nautomated machine learning (AutoML) techniques to automate the creation of the\nentire forecasting pipeline. The framework employs Bayesian optimization, to\nautomatically construct pipelines from statistical, machine learning (ML) and\ndeep neural network (DNN) models. Furthermore, we propose three essential\nimprovements to adapt AutoML to time series data. First, pipeline templates to\naccount for the different supported forecasting models. Second, a novel\nwarm-starting technique to start the optimization from prior optimization runs.\nThird, we adapt multi-fidelity optimizations to make them applicable to a\nsearch space containing statistical, ML and DNN models. Experimental results on\n64 diverse real-world time series datasets demonstrate the effectiveness and\nefficiency of the framework, outperforming traditional methods while requiring\nminimal human involvement.\n", "link": "http://arxiv.org/abs/2312.08528v3", "date": "2024-04-30", "relevancy": 2.2783, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4749}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4465}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20auto-sktime%3A%20Automated%20Time%20Series%20Forecasting&body=Title%3A%20auto-sktime%3A%20Automated%20Time%20Series%20Forecasting%0AAuthor%3A%20Marc-Andr%C3%A9%20Z%C3%B6ller%20and%20Marius%20Lindauer%20and%20Marco%20F.%20Huber%0AAbstract%3A%20%20%20In%20today%27s%20data-driven%20landscape%2C%20time%20series%20forecasting%20is%20pivotal%20in%0Adecision-making%20across%20various%20sectors.%20Yet%2C%20the%20proliferation%20of%20more%20diverse%0Atime%20series%20data%2C%20coupled%20with%20the%20expanding%20landscape%20of%20available%20forecasting%0Amethods%2C%20poses%20significant%20challenges%20for%20forecasters.%20To%20meet%20the%20growing%0Ademand%20for%20efficient%20forecasting%2C%20we%20introduce%20auto-sktime%2C%20a%20novel%20framework%0Afor%20automated%20time%20series%20forecasting.%20The%20proposed%20framework%20uses%20the%20power%20of%0Aautomated%20machine%20learning%20%28AutoML%29%20techniques%20to%20automate%20the%20creation%20of%20the%0Aentire%20forecasting%20pipeline.%20The%20framework%20employs%20Bayesian%20optimization%2C%20to%0Aautomatically%20construct%20pipelines%20from%20statistical%2C%20machine%20learning%20%28ML%29%20and%0Adeep%20neural%20network%20%28DNN%29%20models.%20Furthermore%2C%20we%20propose%20three%20essential%0Aimprovements%20to%20adapt%20AutoML%20to%20time%20series%20data.%20First%2C%20pipeline%20templates%20to%0Aaccount%20for%20the%20different%20supported%20forecasting%20models.%20Second%2C%20a%20novel%0Awarm-starting%20technique%20to%20start%20the%20optimization%20from%20prior%20optimization%20runs.%0AThird%2C%20we%20adapt%20multi-fidelity%20optimizations%20to%20make%20them%20applicable%20to%20a%0Asearch%20space%20containing%20statistical%2C%20ML%20and%20DNN%20models.%20Experimental%20results%20on%0A64%20diverse%20real-world%20time%20series%20datasets%20demonstrate%20the%20effectiveness%20and%0Aefficiency%20of%20the%20framework%2C%20outperforming%20traditional%20methods%20while%20requiring%0Aminimal%20human%20involvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08528v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=auto-sktime%3A%20Automated%20Time%20Series%20Forecasting&entry.906535625=Marc-Andr%C3%A9%20Z%C3%B6ller%20and%20Marius%20Lindauer%20and%20Marco%20F.%20Huber&entry.1292438233=%20%20In%20today%27s%20data-driven%20landscape%2C%20time%20series%20forecasting%20is%20pivotal%20in%0Adecision-making%20across%20various%20sectors.%20Yet%2C%20the%20proliferation%20of%20more%20diverse%0Atime%20series%20data%2C%20coupled%20with%20the%20expanding%20landscape%20of%20available%20forecasting%0Amethods%2C%20poses%20significant%20challenges%20for%20forecasters.%20To%20meet%20the%20growing%0Ademand%20for%20efficient%20forecasting%2C%20we%20introduce%20auto-sktime%2C%20a%20novel%20framework%0Afor%20automated%20time%20series%20forecasting.%20The%20proposed%20framework%20uses%20the%20power%20of%0Aautomated%20machine%20learning%20%28AutoML%29%20techniques%20to%20automate%20the%20creation%20of%20the%0Aentire%20forecasting%20pipeline.%20The%20framework%20employs%20Bayesian%20optimization%2C%20to%0Aautomatically%20construct%20pipelines%20from%20statistical%2C%20machine%20learning%20%28ML%29%20and%0Adeep%20neural%20network%20%28DNN%29%20models.%20Furthermore%2C%20we%20propose%20three%20essential%0Aimprovements%20to%20adapt%20AutoML%20to%20time%20series%20data.%20First%2C%20pipeline%20templates%20to%0Aaccount%20for%20the%20different%20supported%20forecasting%20models.%20Second%2C%20a%20novel%0Awarm-starting%20technique%20to%20start%20the%20optimization%20from%20prior%20optimization%20runs.%0AThird%2C%20we%20adapt%20multi-fidelity%20optimizations%20to%20make%20them%20applicable%20to%20a%0Asearch%20space%20containing%20statistical%2C%20ML%20and%20DNN%20models.%20Experimental%20results%20on%0A64%20diverse%20real-world%20time%20series%20datasets%20demonstrate%20the%20effectiveness%20and%0Aefficiency%20of%20the%20framework%2C%20outperforming%20traditional%20methods%20while%20requiring%0Aminimal%20human%20involvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08528v3&entry.124074799=Read"},
{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "author": "Bingwen Zhu and Fanyi Wang and Tianyi Lu and Peng Liu and Jingwen Su and Jinxiu Liu and Yanhao Zhang and Zuxuan Wu and Yu-Gang Jiang and Guo-Jun Qi", "abstract": "  Image-to-video(I2V) generation aims to create a video sequence from a single\nimage, which requires high temporal coherence and visual fidelity with the\nsource image.However, existing approaches suffer from character appearance\ninconsistency and poor preservation of fine details. Moreover, they require a\nlarge amount of video data for training, which can be computationally\ndemanding.To address these limitations,we propose PoseAnimate, a novel\nzero-shot I2V framework for character animation.PoseAnimate contains three key\ncomponents: 1) Pose-Aware Control Module (PACM) incorporates diverse pose\nsignals into conditional embeddings, to preserve character-independent content\nand maintain precise alignment of actions.2) Dual Consistency Attention Module\n(DCAM) enhances temporal consistency, and retains character identity and\nintricate background details.3) Mask-Guided Decoupling Module (MGDM) refines\ndistinct feature perception, improving animation fidelity by decoupling the\ncharacter and background.We also propose a Pose Alignment Transition Algorithm\n(PATA) to ensure smooth action transition.Extensive experiment results\ndemonstrate that our approach outperforms the state-of-the-art training-based\nmethods in terms of character consistency and detail fidelity. Moreover, it\nmaintains a high level of temporal coherence throughout the generated\nanimations.\n", "link": "http://arxiv.org/abs/2404.13680v2", "date": "2024-04-30", "relevancy": 2.2525, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5684}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5635}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5606}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PoseAnimate%3A%20Zero-shot%20high%20fidelity%20pose%20controllable%20character%0A%20%20animation&body=Title%3A%20PoseAnimate%3A%20Zero-shot%20high%20fidelity%20pose%20controllable%20character%0A%20%20animation%0AAuthor%3A%20Bingwen%20Zhu%20and%20Fanyi%20Wang%20and%20Tianyi%20Lu%20and%20Peng%20Liu%20and%20Jingwen%20Su%20and%20Jinxiu%20Liu%20and%20Yanhao%20Zhang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%20and%20Guo-Jun%20Qi%0AAbstract%3A%20%20%20Image-to-video%28I2V%29%20generation%20aims%20to%20create%20a%20video%20sequence%20from%20a%20single%0Aimage%2C%20which%20requires%20high%20temporal%20coherence%20and%20visual%20fidelity%20with%20the%0Asource%20image.However%2C%20existing%20approaches%20suffer%20from%20character%20appearance%0Ainconsistency%20and%20poor%20preservation%20of%20fine%20details.%20Moreover%2C%20they%20require%20a%0Alarge%20amount%20of%20video%20data%20for%20training%2C%20which%20can%20be%20computationally%0Ademanding.To%20address%20these%20limitations%2Cwe%20propose%20PoseAnimate%2C%20a%20novel%0Azero-shot%20I2V%20framework%20for%20character%20animation.PoseAnimate%20contains%20three%20key%0Acomponents%3A%201%29%20Pose-Aware%20Control%20Module%20%28PACM%29%20incorporates%20diverse%20pose%0Asignals%20into%20conditional%20embeddings%2C%20to%20preserve%20character-independent%20content%0Aand%20maintain%20precise%20alignment%20of%20actions.2%29%20Dual%20Consistency%20Attention%20Module%0A%28DCAM%29%20enhances%20temporal%20consistency%2C%20and%20retains%20character%20identity%20and%0Aintricate%20background%20details.3%29%20Mask-Guided%20Decoupling%20Module%20%28MGDM%29%20refines%0Adistinct%20feature%20perception%2C%20improving%20animation%20fidelity%20by%20decoupling%20the%0Acharacter%20and%20background.We%20also%20propose%20a%20Pose%20Alignment%20Transition%20Algorithm%0A%28PATA%29%20to%20ensure%20smooth%20action%20transition.Extensive%20experiment%20results%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20state-of-the-art%20training-based%0Amethods%20in%20terms%20of%20character%20consistency%20and%20detail%20fidelity.%20Moreover%2C%20it%0Amaintains%20a%20high%20level%20of%20temporal%20coherence%20throughout%20the%20generated%0Aanimations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13680v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseAnimate%3A%20Zero-shot%20high%20fidelity%20pose%20controllable%20character%0A%20%20animation&entry.906535625=Bingwen%20Zhu%20and%20Fanyi%20Wang%20and%20Tianyi%20Lu%20and%20Peng%20Liu%20and%20Jingwen%20Su%20and%20Jinxiu%20Liu%20and%20Yanhao%20Zhang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%20and%20Guo-Jun%20Qi&entry.1292438233=%20%20Image-to-video%28I2V%29%20generation%20aims%20to%20create%20a%20video%20sequence%20from%20a%20single%0Aimage%2C%20which%20requires%20high%20temporal%20coherence%20and%20visual%20fidelity%20with%20the%0Asource%20image.However%2C%20existing%20approaches%20suffer%20from%20character%20appearance%0Ainconsistency%20and%20poor%20preservation%20of%20fine%20details.%20Moreover%2C%20they%20require%20a%0Alarge%20amount%20of%20video%20data%20for%20training%2C%20which%20can%20be%20computationally%0Ademanding.To%20address%20these%20limitations%2Cwe%20propose%20PoseAnimate%2C%20a%20novel%0Azero-shot%20I2V%20framework%20for%20character%20animation.PoseAnimate%20contains%20three%20key%0Acomponents%3A%201%29%20Pose-Aware%20Control%20Module%20%28PACM%29%20incorporates%20diverse%20pose%0Asignals%20into%20conditional%20embeddings%2C%20to%20preserve%20character-independent%20content%0Aand%20maintain%20precise%20alignment%20of%20actions.2%29%20Dual%20Consistency%20Attention%20Module%0A%28DCAM%29%20enhances%20temporal%20consistency%2C%20and%20retains%20character%20identity%20and%0Aintricate%20background%20details.3%29%20Mask-Guided%20Decoupling%20Module%20%28MGDM%29%20refines%0Adistinct%20feature%20perception%2C%20improving%20animation%20fidelity%20by%20decoupling%20the%0Acharacter%20and%20background.We%20also%20propose%20a%20Pose%20Alignment%20Transition%20Algorithm%0A%28PATA%29%20to%20ensure%20smooth%20action%20transition.Extensive%20experiment%20results%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20state-of-the-art%20training-based%0Amethods%20in%20terms%20of%20character%20consistency%20and%20detail%20fidelity.%20Moreover%2C%20it%0Amaintains%20a%20high%20level%20of%20temporal%20coherence%20throughout%20the%20generated%0Aanimations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13680v2&entry.124074799=Read"},
{"title": "UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes", "author": "David Rozenberszki and Or Litany and Angela Dai", "abstract": "  3D instance segmentation is fundamental to geometric understanding of the\nworld around us. Existing methods for instance segmentation of 3D scenes rely\non supervision from expensive, manual 3D annotations. We propose UnScene3D, the\nfirst fully unsupervised 3D learning approach for class-agnostic 3D instance\nsegmentation of indoor scans. UnScene3D first generates pseudo masks by\nleveraging self-supervised color and geometry features to find potential object\nregions. We operate on a basis of geometric oversegmentation, enabling\nefficient representation and learning on high-resolution 3D data. The coarse\nproposals are then refined through self-training our model on its predictions.\nOur approach improves over state-of-the-art unsupervised 3D instance\nsegmentation methods by more than 300% Average Precision score, demonstrating\neffective instance segmentation even in challenging, cluttered 3D scenes.\n", "link": "http://arxiv.org/abs/2303.14541v2", "date": "2024-04-30", "relevancy": 2.2377, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5651}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UnScene3D%3A%20Unsupervised%203D%20Instance%20Segmentation%20for%20Indoor%20Scenes&body=Title%3A%20UnScene3D%3A%20Unsupervised%203D%20Instance%20Segmentation%20for%20Indoor%20Scenes%0AAuthor%3A%20David%20Rozenberszki%20and%20Or%20Litany%20and%20Angela%20Dai%0AAbstract%3A%20%20%203D%20instance%20segmentation%20is%20fundamental%20to%20geometric%20understanding%20of%20the%0Aworld%20around%20us.%20Existing%20methods%20for%20instance%20segmentation%20of%203D%20scenes%20rely%0Aon%20supervision%20from%20expensive%2C%20manual%203D%20annotations.%20We%20propose%20UnScene3D%2C%20the%0Afirst%20fully%20unsupervised%203D%20learning%20approach%20for%20class-agnostic%203D%20instance%0Asegmentation%20of%20indoor%20scans.%20UnScene3D%20first%20generates%20pseudo%20masks%20by%0Aleveraging%20self-supervised%20color%20and%20geometry%20features%20to%20find%20potential%20object%0Aregions.%20We%20operate%20on%20a%20basis%20of%20geometric%20oversegmentation%2C%20enabling%0Aefficient%20representation%20and%20learning%20on%20high-resolution%203D%20data.%20The%20coarse%0Aproposals%20are%20then%20refined%20through%20self-training%20our%20model%20on%20its%20predictions.%0AOur%20approach%20improves%20over%20state-of-the-art%20unsupervised%203D%20instance%0Asegmentation%20methods%20by%20more%20than%20300%25%20Average%20Precision%20score%2C%20demonstrating%0Aeffective%20instance%20segmentation%20even%20in%20challenging%2C%20cluttered%203D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14541v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnScene3D%3A%20Unsupervised%203D%20Instance%20Segmentation%20for%20Indoor%20Scenes&entry.906535625=David%20Rozenberszki%20and%20Or%20Litany%20and%20Angela%20Dai&entry.1292438233=%20%203D%20instance%20segmentation%20is%20fundamental%20to%20geometric%20understanding%20of%20the%0Aworld%20around%20us.%20Existing%20methods%20for%20instance%20segmentation%20of%203D%20scenes%20rely%0Aon%20supervision%20from%20expensive%2C%20manual%203D%20annotations.%20We%20propose%20UnScene3D%2C%20the%0Afirst%20fully%20unsupervised%203D%20learning%20approach%20for%20class-agnostic%203D%20instance%0Asegmentation%20of%20indoor%20scans.%20UnScene3D%20first%20generates%20pseudo%20masks%20by%0Aleveraging%20self-supervised%20color%20and%20geometry%20features%20to%20find%20potential%20object%0Aregions.%20We%20operate%20on%20a%20basis%20of%20geometric%20oversegmentation%2C%20enabling%0Aefficient%20representation%20and%20learning%20on%20high-resolution%203D%20data.%20The%20coarse%0Aproposals%20are%20then%20refined%20through%20self-training%20our%20model%20on%20its%20predictions.%0AOur%20approach%20improves%20over%20state-of-the-art%20unsupervised%203D%20instance%0Asegmentation%20methods%20by%20more%20than%20300%25%20Average%20Precision%20score%2C%20demonstrating%0Aeffective%20instance%20segmentation%20even%20in%20challenging%2C%20cluttered%203D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14541v2&entry.124074799=Read"},
{"title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting", "author": "Paul Engstler and Andrea Vedaldi and Iro Laina and Christian Rupprecht", "abstract": "  3D scene generation has quickly become a challenging new research direction,\nfueled by consistent improvements of 2D generative diffusion models. Most prior\nwork in this area generates scenes by iteratively stitching newly generated\nframes with existing geometry. These works often depend on pre-trained\nmonocular depth estimators to lift the generated images into 3D, fusing them\nwith the existing scene representation. These approaches are then often\nevaluated via a text metric, measuring the similarity between the generated\nimages and a given text prompt. In this work, we make two fundamental\ncontributions to the field of 3D scene generation. First, we note that lifting\nimages to 3D with a monocular depth estimation model is suboptimal as it\nignores the geometry of the existing scene. We thus introduce a novel depth\ncompletion model, trained via teacher distillation and self-training to learn\nthe 3D fusion process, resulting in improved geometric coherence of the scene.\nSecond, we introduce a new benchmarking scheme for scene generation methods\nthat is based on ground truth geometry, and thus measures the quality of the\nstructure of the scene.\n", "link": "http://arxiv.org/abs/2404.19758v1", "date": "2024-04-30", "relevancy": 2.217, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5659}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5464}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Invisible%20Stitch%3A%20Generating%20Smooth%203D%20Scenes%20with%20Depth%20Inpainting&body=Title%3A%20Invisible%20Stitch%3A%20Generating%20Smooth%203D%20Scenes%20with%20Depth%20Inpainting%0AAuthor%3A%20Paul%20Engstler%20and%20Andrea%20Vedaldi%20and%20Iro%20Laina%20and%20Christian%20Rupprecht%0AAbstract%3A%20%20%203D%20scene%20generation%20has%20quickly%20become%20a%20challenging%20new%20research%20direction%2C%0Afueled%20by%20consistent%20improvements%20of%202D%20generative%20diffusion%20models.%20Most%20prior%0Awork%20in%20this%20area%20generates%20scenes%20by%20iteratively%20stitching%20newly%20generated%0Aframes%20with%20existing%20geometry.%20These%20works%20often%20depend%20on%20pre-trained%0Amonocular%20depth%20estimators%20to%20lift%20the%20generated%20images%20into%203D%2C%20fusing%20them%0Awith%20the%20existing%20scene%20representation.%20These%20approaches%20are%20then%20often%0Aevaluated%20via%20a%20text%20metric%2C%20measuring%20the%20similarity%20between%20the%20generated%0Aimages%20and%20a%20given%20text%20prompt.%20In%20this%20work%2C%20we%20make%20two%20fundamental%0Acontributions%20to%20the%20field%20of%203D%20scene%20generation.%20First%2C%20we%20note%20that%20lifting%0Aimages%20to%203D%20with%20a%20monocular%20depth%20estimation%20model%20is%20suboptimal%20as%20it%0Aignores%20the%20geometry%20of%20the%20existing%20scene.%20We%20thus%20introduce%20a%20novel%20depth%0Acompletion%20model%2C%20trained%20via%20teacher%20distillation%20and%20self-training%20to%20learn%0Athe%203D%20fusion%20process%2C%20resulting%20in%20improved%20geometric%20coherence%20of%20the%20scene.%0ASecond%2C%20we%20introduce%20a%20new%20benchmarking%20scheme%20for%20scene%20generation%20methods%0Athat%20is%20based%20on%20ground%20truth%20geometry%2C%20and%20thus%20measures%20the%20quality%20of%20the%0Astructure%20of%20the%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19758v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invisible%20Stitch%3A%20Generating%20Smooth%203D%20Scenes%20with%20Depth%20Inpainting&entry.906535625=Paul%20Engstler%20and%20Andrea%20Vedaldi%20and%20Iro%20Laina%20and%20Christian%20Rupprecht&entry.1292438233=%20%203D%20scene%20generation%20has%20quickly%20become%20a%20challenging%20new%20research%20direction%2C%0Afueled%20by%20consistent%20improvements%20of%202D%20generative%20diffusion%20models.%20Most%20prior%0Awork%20in%20this%20area%20generates%20scenes%20by%20iteratively%20stitching%20newly%20generated%0Aframes%20with%20existing%20geometry.%20These%20works%20often%20depend%20on%20pre-trained%0Amonocular%20depth%20estimators%20to%20lift%20the%20generated%20images%20into%203D%2C%20fusing%20them%0Awith%20the%20existing%20scene%20representation.%20These%20approaches%20are%20then%20often%0Aevaluated%20via%20a%20text%20metric%2C%20measuring%20the%20similarity%20between%20the%20generated%0Aimages%20and%20a%20given%20text%20prompt.%20In%20this%20work%2C%20we%20make%20two%20fundamental%0Acontributions%20to%20the%20field%20of%203D%20scene%20generation.%20First%2C%20we%20note%20that%20lifting%0Aimages%20to%203D%20with%20a%20monocular%20depth%20estimation%20model%20is%20suboptimal%20as%20it%0Aignores%20the%20geometry%20of%20the%20existing%20scene.%20We%20thus%20introduce%20a%20novel%20depth%0Acompletion%20model%2C%20trained%20via%20teacher%20distillation%20and%20self-training%20to%20learn%0Athe%203D%20fusion%20process%2C%20resulting%20in%20improved%20geometric%20coherence%20of%20the%20scene.%0ASecond%2C%20we%20introduce%20a%20new%20benchmarking%20scheme%20for%20scene%20generation%20methods%0Athat%20is%20based%20on%20ground%20truth%20geometry%2C%20and%20thus%20measures%20the%20quality%20of%20the%0Astructure%20of%20the%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19758v1&entry.124074799=Read"},
{"title": "Conditioning Generative Latent Optimization for Sparse-View CT Image\n  Reconstruction", "author": "Thomas Braure and Delphine Lazaro and David Hateau and Vincent Brandon and K\u00e9vin Ginsburger", "abstract": "  Computed Tomography (CT) is a prominent example of Imaging Inverse Problem\nhighlighting the unrivaled performances of data-driven methods in degraded\nmeasurements setups like sparse X-ray projections. Although a significant\nproportion of deep learning approaches benefit from large supervised datasets,\nthey cannot generalize to new experimental setups. In contrast, fully\nunsupervised techniques, most notably using score-based generative models, have\nrecently demonstrated similar or better performances compared to supervised\napproaches while being flexible at test time. However, their use cases are\nlimited as they need considerable amounts of training data to have good\ngeneralization properties. Another unsupervised approach taking advantage of\nthe implicit natural bias of deep convolutional networks, Deep Image Prior, has\nrecently been adapted to solve sparse CT by reparameterizing the reconstruction\nproblem. Although this methodology does not require any training dataset, it\nenforces a weaker prior on the reconstructions when compared to data-driven\nmethods. To fill the gap between these two strategies, we propose an\nunsupervised conditional approach to the Generative Latent Optimization\nframework (cGLO). Similarly to DIP, without any training dataset, cGLO benefits\nfrom the structural bias of a decoder network. However, the prior is further\nreinforced as the effect of a likelihood objective shared between multiple\nslices being reconstructed simultaneously through the same decoder network. In\naddition, the parameters of the decoder may be initialized on an unsupervised,\nand eventually very small, training dataset to enhance the reconstruction. The\nresulting approach is tested on full-dose sparse-view CT using multiple\ntraining dataset sizes and varying numbers of viewing angles.\n", "link": "http://arxiv.org/abs/2307.16670v3", "date": "2024-04-30", "relevancy": 2.2117, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5743}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conditioning%20Generative%20Latent%20Optimization%20for%20Sparse-View%20CT%20Image%0A%20%20Reconstruction&body=Title%3A%20Conditioning%20Generative%20Latent%20Optimization%20for%20Sparse-View%20CT%20Image%0A%20%20Reconstruction%0AAuthor%3A%20Thomas%20Braure%20and%20Delphine%20Lazaro%20and%20David%20Hateau%20and%20Vincent%20Brandon%20and%20K%C3%A9vin%20Ginsburger%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20is%20a%20prominent%20example%20of%20Imaging%20Inverse%20Problem%0Ahighlighting%20the%20unrivaled%20performances%20of%20data-driven%20methods%20in%20degraded%0Ameasurements%20setups%20like%20sparse%20X-ray%20projections.%20Although%20a%20significant%0Aproportion%20of%20deep%20learning%20approaches%20benefit%20from%20large%20supervised%20datasets%2C%0Athey%20cannot%20generalize%20to%20new%20experimental%20setups.%20In%20contrast%2C%20fully%0Aunsupervised%20techniques%2C%20most%20notably%20using%20score-based%20generative%20models%2C%20have%0Arecently%20demonstrated%20similar%20or%20better%20performances%20compared%20to%20supervised%0Aapproaches%20while%20being%20flexible%20at%20test%20time.%20However%2C%20their%20use%20cases%20are%0Alimited%20as%20they%20need%20considerable%20amounts%20of%20training%20data%20to%20have%20good%0Ageneralization%20properties.%20Another%20unsupervised%20approach%20taking%20advantage%20of%0Athe%20implicit%20natural%20bias%20of%20deep%20convolutional%20networks%2C%20Deep%20Image%20Prior%2C%20has%0Arecently%20been%20adapted%20to%20solve%20sparse%20CT%20by%20reparameterizing%20the%20reconstruction%0Aproblem.%20Although%20this%20methodology%20does%20not%20require%20any%20training%20dataset%2C%20it%0Aenforces%20a%20weaker%20prior%20on%20the%20reconstructions%20when%20compared%20to%20data-driven%0Amethods.%20To%20fill%20the%20gap%20between%20these%20two%20strategies%2C%20we%20propose%20an%0Aunsupervised%20conditional%20approach%20to%20the%20Generative%20Latent%20Optimization%0Aframework%20%28cGLO%29.%20Similarly%20to%20DIP%2C%20without%20any%20training%20dataset%2C%20cGLO%20benefits%0Afrom%20the%20structural%20bias%20of%20a%20decoder%20network.%20However%2C%20the%20prior%20is%20further%0Areinforced%20as%20the%20effect%20of%20a%20likelihood%20objective%20shared%20between%20multiple%0Aslices%20being%20reconstructed%20simultaneously%20through%20the%20same%20decoder%20network.%20In%0Aaddition%2C%20the%20parameters%20of%20the%20decoder%20may%20be%20initialized%20on%20an%20unsupervised%2C%0Aand%20eventually%20very%20small%2C%20training%20dataset%20to%20enhance%20the%20reconstruction.%20The%0Aresulting%20approach%20is%20tested%20on%20full-dose%20sparse-view%20CT%20using%20multiple%0Atraining%20dataset%20sizes%20and%20varying%20numbers%20of%20viewing%20angles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16670v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditioning%20Generative%20Latent%20Optimization%20for%20Sparse-View%20CT%20Image%0A%20%20Reconstruction&entry.906535625=Thomas%20Braure%20and%20Delphine%20Lazaro%20and%20David%20Hateau%20and%20Vincent%20Brandon%20and%20K%C3%A9vin%20Ginsburger&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20is%20a%20prominent%20example%20of%20Imaging%20Inverse%20Problem%0Ahighlighting%20the%20unrivaled%20performances%20of%20data-driven%20methods%20in%20degraded%0Ameasurements%20setups%20like%20sparse%20X-ray%20projections.%20Although%20a%20significant%0Aproportion%20of%20deep%20learning%20approaches%20benefit%20from%20large%20supervised%20datasets%2C%0Athey%20cannot%20generalize%20to%20new%20experimental%20setups.%20In%20contrast%2C%20fully%0Aunsupervised%20techniques%2C%20most%20notably%20using%20score-based%20generative%20models%2C%20have%0Arecently%20demonstrated%20similar%20or%20better%20performances%20compared%20to%20supervised%0Aapproaches%20while%20being%20flexible%20at%20test%20time.%20However%2C%20their%20use%20cases%20are%0Alimited%20as%20they%20need%20considerable%20amounts%20of%20training%20data%20to%20have%20good%0Ageneralization%20properties.%20Another%20unsupervised%20approach%20taking%20advantage%20of%0Athe%20implicit%20natural%20bias%20of%20deep%20convolutional%20networks%2C%20Deep%20Image%20Prior%2C%20has%0Arecently%20been%20adapted%20to%20solve%20sparse%20CT%20by%20reparameterizing%20the%20reconstruction%0Aproblem.%20Although%20this%20methodology%20does%20not%20require%20any%20training%20dataset%2C%20it%0Aenforces%20a%20weaker%20prior%20on%20the%20reconstructions%20when%20compared%20to%20data-driven%0Amethods.%20To%20fill%20the%20gap%20between%20these%20two%20strategies%2C%20we%20propose%20an%0Aunsupervised%20conditional%20approach%20to%20the%20Generative%20Latent%20Optimization%0Aframework%20%28cGLO%29.%20Similarly%20to%20DIP%2C%20without%20any%20training%20dataset%2C%20cGLO%20benefits%0Afrom%20the%20structural%20bias%20of%20a%20decoder%20network.%20However%2C%20the%20prior%20is%20further%0Areinforced%20as%20the%20effect%20of%20a%20likelihood%20objective%20shared%20between%20multiple%0Aslices%20being%20reconstructed%20simultaneously%20through%20the%20same%20decoder%20network.%20In%0Aaddition%2C%20the%20parameters%20of%20the%20decoder%20may%20be%20initialized%20on%20an%20unsupervised%2C%0Aand%20eventually%20very%20small%2C%20training%20dataset%20to%20enhance%20the%20reconstruction.%20The%0Aresulting%20approach%20is%20tested%20on%20full-dose%20sparse-view%20CT%20using%20multiple%0Atraining%20dataset%20sizes%20and%20varying%20numbers%20of%20viewing%20angles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16670v3&entry.124074799=Read"},
{"title": "Iterative Reasoning Preference Optimization", "author": "Richard Yuanzhe Pang and Weizhe Yuan and Kyunghyun Cho and He He and Sainbayar Sukhbaatar and Jason Weston", "abstract": "  Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy for\nLlama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting\nout of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on\nARC-Challenge, which outperforms other Llama-2-based models not relying on\nadditionally sourced datasets.\n", "link": "http://arxiv.org/abs/2404.19733v1", "date": "2024-04-30", "relevancy": 2.2105, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4448}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4423}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Iterative%20Reasoning%20Preference%20Optimization&body=Title%3A%20Iterative%20Reasoning%20Preference%20Optimization%0AAuthor%3A%20Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%0AAbstract%3A%20%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20for%0ALlama-2-70B-Chat%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20%28and%2088.7%25%20with%20majority%20voting%0Aout%20of%2032%20samples%29%2C%20from%2012.5%25%20to%2020.8%25%20on%20MATH%2C%20and%20from%2077.8%25%20to%2086.7%25%20on%0AARC-Challenge%2C%20which%20outperforms%20other%20Llama-2-based%20models%20not%20relying%20on%0Aadditionally%20sourced%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19733v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Reasoning%20Preference%20Optimization&entry.906535625=Richard%20Yuanzhe%20Pang%20and%20Weizhe%20Yuan%20and%20Kyunghyun%20Cho%20and%20He%20He%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston&entry.1292438233=%20%20Iterative%20preference%20optimization%20methods%20have%20recently%20been%20shown%20to%20perform%0Awell%20for%20general%20instruction%20tuning%20tasks%2C%20but%20typically%20make%20little%0Aimprovement%20on%20reasoning%20tasks%20%28Yuan%20et%20al.%2C%202024%2C%20Chen%20et%20al.%2C%202024%29.%20In%20this%0Awork%20we%20develop%20an%20iterative%20approach%20that%20optimizes%20the%20preference%20between%0Acompeting%20generated%20Chain-of-Thought%20%28CoT%29%20candidates%20by%20optimizing%20for%20winning%0Avs.%20losing%20reasoning%20steps%20that%20lead%20to%20the%20correct%20answer.%20We%20train%20using%20a%0Amodified%20DPO%20loss%20%28Rafailov%20et%20al.%2C%202023%29%20with%20an%20additional%20negative%0Alog-likelihood%20term%2C%20which%20we%20find%20to%20be%20crucial.%20We%20show%20reasoning%20improves%0Aacross%20repeated%20iterations%20of%20this%20scheme.%20While%20only%20relying%20on%20examples%20in%0Athe%20training%20set%2C%20our%20approach%20results%20in%20increasing%20accuracy%20for%0ALlama-2-70B-Chat%20from%2055.6%25%20to%2081.6%25%20on%20GSM8K%20%28and%2088.7%25%20with%20majority%20voting%0Aout%20of%2032%20samples%29%2C%20from%2012.5%25%20to%2020.8%25%20on%20MATH%2C%20and%20from%2077.8%25%20to%2086.7%25%20on%0AARC-Challenge%2C%20which%20outperforms%20other%20Llama-2-based%20models%20not%20relying%20on%0Aadditionally%20sourced%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19733v1&entry.124074799=Read"},
{"title": "Finetuning greedy kernel models by exchange algorithms", "author": "Tizian Wenzel and Armin Iske", "abstract": "  Kernel based approximation offers versatile tools for high-dimensional\napproximation, which can especially be leveraged for surrogate modeling. For\nthis purpose, both \"knot insertion\" and \"knot removal\" approaches aim at\nchoosing a suitable subset of the data, in order to obtain a sparse but\nnevertheless accurate kernel model. In the present work, focussing on kernel\nbased interpolation, we aim at combining these two approaches to further\nimprove the accuracy of kernel models, without increasing the computational\ncomplexity of the final kernel model. For this, we introduce a class of kernel\nexchange algorithms (KEA). The resulting KEA algorithm can be used for\nfinetuning greedy kernel surrogate models, allowing for an reduction of the\nerror up to 86.4% (17.2% on average) in our experiments.\n", "link": "http://arxiv.org/abs/2404.19487v1", "date": "2024-04-30", "relevancy": 2.1984, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4375}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4343}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Finetuning%20greedy%20kernel%20models%20by%20exchange%20algorithms&body=Title%3A%20Finetuning%20greedy%20kernel%20models%20by%20exchange%20algorithms%0AAuthor%3A%20Tizian%20Wenzel%20and%20Armin%20Iske%0AAbstract%3A%20%20%20Kernel%20based%20approximation%20offers%20versatile%20tools%20for%20high-dimensional%0Aapproximation%2C%20which%20can%20especially%20be%20leveraged%20for%20surrogate%20modeling.%20For%0Athis%20purpose%2C%20both%20%22knot%20insertion%22%20and%20%22knot%20removal%22%20approaches%20aim%20at%0Achoosing%20a%20suitable%20subset%20of%20the%20data%2C%20in%20order%20to%20obtain%20a%20sparse%20but%0Anevertheless%20accurate%20kernel%20model.%20In%20the%20present%20work%2C%20focussing%20on%20kernel%0Abased%20interpolation%2C%20we%20aim%20at%20combining%20these%20two%20approaches%20to%20further%0Aimprove%20the%20accuracy%20of%20kernel%20models%2C%20without%20increasing%20the%20computational%0Acomplexity%20of%20the%20final%20kernel%20model.%20For%20this%2C%20we%20introduce%20a%20class%20of%20kernel%0Aexchange%20algorithms%20%28KEA%29.%20The%20resulting%20KEA%20algorithm%20can%20be%20used%20for%0Afinetuning%20greedy%20kernel%20surrogate%20models%2C%20allowing%20for%20an%20reduction%20of%20the%0Aerror%20up%20to%2086.4%25%20%2817.2%25%20on%20average%29%20in%20our%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19487v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finetuning%20greedy%20kernel%20models%20by%20exchange%20algorithms&entry.906535625=Tizian%20Wenzel%20and%20Armin%20Iske&entry.1292438233=%20%20Kernel%20based%20approximation%20offers%20versatile%20tools%20for%20high-dimensional%0Aapproximation%2C%20which%20can%20especially%20be%20leveraged%20for%20surrogate%20modeling.%20For%0Athis%20purpose%2C%20both%20%22knot%20insertion%22%20and%20%22knot%20removal%22%20approaches%20aim%20at%0Achoosing%20a%20suitable%20subset%20of%20the%20data%2C%20in%20order%20to%20obtain%20a%20sparse%20but%0Anevertheless%20accurate%20kernel%20model.%20In%20the%20present%20work%2C%20focussing%20on%20kernel%0Abased%20interpolation%2C%20we%20aim%20at%20combining%20these%20two%20approaches%20to%20further%0Aimprove%20the%20accuracy%20of%20kernel%20models%2C%20without%20increasing%20the%20computational%0Acomplexity%20of%20the%20final%20kernel%20model.%20For%20this%2C%20we%20introduce%20a%20class%20of%20kernel%0Aexchange%20algorithms%20%28KEA%29.%20The%20resulting%20KEA%20algorithm%20can%20be%20used%20for%0Afinetuning%20greedy%20kernel%20surrogate%20models%2C%20allowing%20for%20an%20reduction%20of%20the%0Aerror%20up%20to%2086.4%25%20%2817.2%25%20on%20average%29%20in%20our%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19487v1&entry.124074799=Read"},
{"title": "Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation", "author": "Yunhao Ge and Xiaohui Zeng and Jacob Samuel Huffman and Tsung-Yi Lin and Ming-Yu Liu and Yin Cui", "abstract": "  Existing automatic captioning methods for visual content face challenges such\nas lack of detail, content hallucination, and poor instruction following. In\nthis work, we propose VisualFactChecker (VFC), a flexible training-free\npipeline that generates high-fidelity and detailed captions for both 2D images\nand 3D objects. VFC consists of three steps: 1) proposal, where image-to-text\ncaptioning models propose multiple initial captions; 2) verification, where a\nlarge language model (LLM) utilizes tools such as object detection and VQA\nmodels to fact-check proposed captions; 3) captioning, where an LLM generates\nthe final caption by summarizing caption proposals and the fact check\nverification results. In this step, VFC can flexibly generate captions in\nvarious styles following complex instructions. We conduct comprehensive\ncaptioning evaluations using four metrics: 1) CLIP-Score for image-text\nsimilarity; 2) CLIP-Image-Score for measuring the image-image similarity\nbetween the original and the reconstructed image generated by a text-to-image\nmodel using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V\nfor fine-grained evaluation. Evaluation results show that VFC outperforms\nstate-of-the-art open-sourced captioning methods for 2D images on the COCO\ndataset and 3D assets on the Objaverse dataset. Our study demonstrates that by\ncombining open-source models into a pipeline, we can attain captioning\ncapability comparable to proprietary models such as GPT-4V, despite being over\n10x smaller in model size.\n", "link": "http://arxiv.org/abs/2404.19752v1", "date": "2024-04-30", "relevancy": 2.187, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5585}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5415}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5305}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visual%20Fact%20Checker%3A%20Enabling%20High-Fidelity%20Detailed%20Caption%20Generation&body=Title%3A%20Visual%20Fact%20Checker%3A%20Enabling%20High-Fidelity%20Detailed%20Caption%20Generation%0AAuthor%3A%20Yunhao%20Ge%20and%20Xiaohui%20Zeng%20and%20Jacob%20Samuel%20Huffman%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Yin%20Cui%0AAbstract%3A%20%20%20Existing%20automatic%20captioning%20methods%20for%20visual%20content%20face%20challenges%20such%0Aas%20lack%20of%20detail%2C%20content%20hallucination%2C%20and%20poor%20instruction%20following.%20In%0Athis%20work%2C%20we%20propose%20VisualFactChecker%20%28VFC%29%2C%20a%20flexible%20training-free%0Apipeline%20that%20generates%20high-fidelity%20and%20detailed%20captions%20for%20both%202D%20images%0Aand%203D%20objects.%20VFC%20consists%20of%20three%20steps%3A%201%29%20proposal%2C%20where%20image-to-text%0Acaptioning%20models%20propose%20multiple%20initial%20captions%3B%202%29%20verification%2C%20where%20a%0Alarge%20language%20model%20%28LLM%29%20utilizes%20tools%20such%20as%20object%20detection%20and%20VQA%0Amodels%20to%20fact-check%20proposed%20captions%3B%203%29%20captioning%2C%20where%20an%20LLM%20generates%0Athe%20final%20caption%20by%20summarizing%20caption%20proposals%20and%20the%20fact%20check%0Averification%20results.%20In%20this%20step%2C%20VFC%20can%20flexibly%20generate%20captions%20in%0Avarious%20styles%20following%20complex%20instructions.%20We%20conduct%20comprehensive%0Acaptioning%20evaluations%20using%20four%20metrics%3A%201%29%20CLIP-Score%20for%20image-text%0Asimilarity%3B%202%29%20CLIP-Image-Score%20for%20measuring%20the%20image-image%20similarity%0Abetween%20the%20original%20and%20the%20reconstructed%20image%20generated%20by%20a%20text-to-image%0Amodel%20using%20the%20caption.%203%29%20human%20study%20on%20Amazon%20Mechanical%20Turk%3B%204%29%20GPT-4V%0Afor%20fine-grained%20evaluation.%20Evaluation%20results%20show%20that%20VFC%20outperforms%0Astate-of-the-art%20open-sourced%20captioning%20methods%20for%202D%20images%20on%20the%20COCO%0Adataset%20and%203D%20assets%20on%20the%20Objaverse%20dataset.%20Our%20study%20demonstrates%20that%20by%0Acombining%20open-source%20models%20into%20a%20pipeline%2C%20we%20can%20attain%20captioning%0Acapability%20comparable%20to%20proprietary%20models%20such%20as%20GPT-4V%2C%20despite%20being%20over%0A10x%20smaller%20in%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19752v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Fact%20Checker%3A%20Enabling%20High-Fidelity%20Detailed%20Caption%20Generation&entry.906535625=Yunhao%20Ge%20and%20Xiaohui%20Zeng%20and%20Jacob%20Samuel%20Huffman%20and%20Tsung-Yi%20Lin%20and%20Ming-Yu%20Liu%20and%20Yin%20Cui&entry.1292438233=%20%20Existing%20automatic%20captioning%20methods%20for%20visual%20content%20face%20challenges%20such%0Aas%20lack%20of%20detail%2C%20content%20hallucination%2C%20and%20poor%20instruction%20following.%20In%0Athis%20work%2C%20we%20propose%20VisualFactChecker%20%28VFC%29%2C%20a%20flexible%20training-free%0Apipeline%20that%20generates%20high-fidelity%20and%20detailed%20captions%20for%20both%202D%20images%0Aand%203D%20objects.%20VFC%20consists%20of%20three%20steps%3A%201%29%20proposal%2C%20where%20image-to-text%0Acaptioning%20models%20propose%20multiple%20initial%20captions%3B%202%29%20verification%2C%20where%20a%0Alarge%20language%20model%20%28LLM%29%20utilizes%20tools%20such%20as%20object%20detection%20and%20VQA%0Amodels%20to%20fact-check%20proposed%20captions%3B%203%29%20captioning%2C%20where%20an%20LLM%20generates%0Athe%20final%20caption%20by%20summarizing%20caption%20proposals%20and%20the%20fact%20check%0Averification%20results.%20In%20this%20step%2C%20VFC%20can%20flexibly%20generate%20captions%20in%0Avarious%20styles%20following%20complex%20instructions.%20We%20conduct%20comprehensive%0Acaptioning%20evaluations%20using%20four%20metrics%3A%201%29%20CLIP-Score%20for%20image-text%0Asimilarity%3B%202%29%20CLIP-Image-Score%20for%20measuring%20the%20image-image%20similarity%0Abetween%20the%20original%20and%20the%20reconstructed%20image%20generated%20by%20a%20text-to-image%0Amodel%20using%20the%20caption.%203%29%20human%20study%20on%20Amazon%20Mechanical%20Turk%3B%204%29%20GPT-4V%0Afor%20fine-grained%20evaluation.%20Evaluation%20results%20show%20that%20VFC%20outperforms%0Astate-of-the-art%20open-sourced%20captioning%20methods%20for%202D%20images%20on%20the%20COCO%0Adataset%20and%203D%20assets%20on%20the%20Objaverse%20dataset.%20Our%20study%20demonstrates%20that%20by%0Acombining%20open-source%20models%20into%20a%20pipeline%2C%20we%20can%20attain%20captioning%0Acapability%20comparable%20to%20proprietary%20models%20such%20as%20GPT-4V%2C%20despite%20being%20over%0A10x%20smaller%20in%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19752v1&entry.124074799=Read"},
{"title": "Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot\n  Medical Image Segmentation", "author": "Sidra Aleem and Fangyijie Wang and Mayug Maniparambil and Eric Arazo and Julia Dietlmeier and Guenole Silvestre and Kathleen Curran and Noel E. O'Connor and Suzanne Little", "abstract": "  The Segment Anything Model (SAM) and CLIP are remarkable vision foundation\nmodels (VFMs). SAM, a prompt driven segmentation model, excels in segmentation\ntasks across diverse domains, while CLIP is renowned for its zero shot\nrecognition capabilities. However, their unified potential has not yet been\nexplored in medical image segmentation. To adapt SAM to medical imaging,\nexisting methods primarily rely on tuning strategies that require extensive\ndata or prior prompts tailored to the specific task, making it particularly\nchallenging when only a limited number of data samples are available. This work\npresents an in depth exploration of integrating SAM and CLIP into a unified\nframework for medical image segmentation. Specifically, we propose a simple\nunified framework, SaLIP, for organ segmentation. Initially, SAM is used for\npart based segmentation within the image, followed by CLIP to retrieve the mask\ncorresponding to the region of interest (ROI) from the pool of SAM generated\nmasks. Finally, SAM is prompted by the retrieved ROI to segment a specific\norgan. Thus, SaLIP is training and fine tuning free and does not rely on domain\nexpertise or labeled data for prompt engineering. Our method shows substantial\nenhancements in zero shot segmentation, showcasing notable improvements in DICE\nscores across diverse segmentation tasks like brain (63.46%), lung (50.11%),\nand fetal head (30.82%), when compared to un prompted SAM. Code and text\nprompts are available at: https://github.com/aleemsidra/SaLIP.\n", "link": "http://arxiv.org/abs/2404.06362v2", "date": "2024-04-30", "relevancy": 2.1788, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Sidra%20Aleem%20and%20Fangyijie%20Wang%20and%20Mayug%20Maniparambil%20and%20Eric%20Arazo%20and%20Julia%20Dietlmeier%20and%20Guenole%20Silvestre%20and%20Kathleen%20Curran%20and%20Noel%20E.%20O%27Connor%20and%20Suzanne%20Little%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20CLIP%20are%20remarkable%20vision%20foundation%0Amodels%20%28VFMs%29.%20SAM%2C%20a%20prompt%20driven%20segmentation%20model%2C%20excels%20in%20segmentation%0Atasks%20across%20diverse%20domains%2C%20while%20CLIP%20is%20renowned%20for%20its%20zero%20shot%0Arecognition%20capabilities.%20However%2C%20their%20unified%20potential%20has%20not%20yet%20been%0Aexplored%20in%20medical%20image%20segmentation.%20To%20adapt%20SAM%20to%20medical%20imaging%2C%0Aexisting%20methods%20primarily%20rely%20on%20tuning%20strategies%20that%20require%20extensive%0Adata%20or%20prior%20prompts%20tailored%20to%20the%20specific%20task%2C%20making%20it%20particularly%0Achallenging%20when%20only%20a%20limited%20number%20of%20data%20samples%20are%20available.%20This%20work%0Apresents%20an%20in%20depth%20exploration%20of%20integrating%20SAM%20and%20CLIP%20into%20a%20unified%0Aframework%20for%20medical%20image%20segmentation.%20Specifically%2C%20we%20propose%20a%20simple%0Aunified%20framework%2C%20SaLIP%2C%20for%20organ%20segmentation.%20Initially%2C%20SAM%20is%20used%20for%0Apart%20based%20segmentation%20within%20the%20image%2C%20followed%20by%20CLIP%20to%20retrieve%20the%20mask%0Acorresponding%20to%20the%20region%20of%20interest%20%28ROI%29%20from%20the%20pool%20of%20SAM%20generated%0Amasks.%20Finally%2C%20SAM%20is%20prompted%20by%20the%20retrieved%20ROI%20to%20segment%20a%20specific%0Aorgan.%20Thus%2C%20SaLIP%20is%20training%20and%20fine%20tuning%20free%20and%20does%20not%20rely%20on%20domain%0Aexpertise%20or%20labeled%20data%20for%20prompt%20engineering.%20Our%20method%20shows%20substantial%0Aenhancements%20in%20zero%20shot%20segmentation%2C%20showcasing%20notable%20improvements%20in%20DICE%0Ascores%20across%20diverse%20segmentation%20tasks%20like%20brain%20%2863.46%25%29%2C%20lung%20%2850.11%25%29%2C%0Aand%20fetal%20head%20%2830.82%25%29%2C%20when%20compared%20to%20un%20prompted%20SAM.%20Code%20and%20text%0Aprompts%20are%20available%20at%3A%20https%3A//github.com/aleemsidra/SaLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06362v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Adaptation%20with%20SaLIP%3A%20A%20Cascade%20of%20SAM%20and%20CLIP%20for%20Zero%20shot%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Sidra%20Aleem%20and%20Fangyijie%20Wang%20and%20Mayug%20Maniparambil%20and%20Eric%20Arazo%20and%20Julia%20Dietlmeier%20and%20Guenole%20Silvestre%20and%20Kathleen%20Curran%20and%20Noel%20E.%20O%27Connor%20and%20Suzanne%20Little&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20CLIP%20are%20remarkable%20vision%20foundation%0Amodels%20%28VFMs%29.%20SAM%2C%20a%20prompt%20driven%20segmentation%20model%2C%20excels%20in%20segmentation%0Atasks%20across%20diverse%20domains%2C%20while%20CLIP%20is%20renowned%20for%20its%20zero%20shot%0Arecognition%20capabilities.%20However%2C%20their%20unified%20potential%20has%20not%20yet%20been%0Aexplored%20in%20medical%20image%20segmentation.%20To%20adapt%20SAM%20to%20medical%20imaging%2C%0Aexisting%20methods%20primarily%20rely%20on%20tuning%20strategies%20that%20require%20extensive%0Adata%20or%20prior%20prompts%20tailored%20to%20the%20specific%20task%2C%20making%20it%20particularly%0Achallenging%20when%20only%20a%20limited%20number%20of%20data%20samples%20are%20available.%20This%20work%0Apresents%20an%20in%20depth%20exploration%20of%20integrating%20SAM%20and%20CLIP%20into%20a%20unified%0Aframework%20for%20medical%20image%20segmentation.%20Specifically%2C%20we%20propose%20a%20simple%0Aunified%20framework%2C%20SaLIP%2C%20for%20organ%20segmentation.%20Initially%2C%20SAM%20is%20used%20for%0Apart%20based%20segmentation%20within%20the%20image%2C%20followed%20by%20CLIP%20to%20retrieve%20the%20mask%0Acorresponding%20to%20the%20region%20of%20interest%20%28ROI%29%20from%20the%20pool%20of%20SAM%20generated%0Amasks.%20Finally%2C%20SAM%20is%20prompted%20by%20the%20retrieved%20ROI%20to%20segment%20a%20specific%0Aorgan.%20Thus%2C%20SaLIP%20is%20training%20and%20fine%20tuning%20free%20and%20does%20not%20rely%20on%20domain%0Aexpertise%20or%20labeled%20data%20for%20prompt%20engineering.%20Our%20method%20shows%20substantial%0Aenhancements%20in%20zero%20shot%20segmentation%2C%20showcasing%20notable%20improvements%20in%20DICE%0Ascores%20across%20diverse%20segmentation%20tasks%20like%20brain%20%2863.46%25%29%2C%20lung%20%2850.11%25%29%2C%0Aand%20fetal%20head%20%2830.82%25%29%2C%20when%20compared%20to%20un%20prompted%20SAM.%20Code%20and%20text%0Aprompts%20are%20available%20at%3A%20https%3A//github.com/aleemsidra/SaLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06362v2&entry.124074799=Read"},
{"title": "Lightplane: Highly-Scalable Components for Neural 3D Fields", "author": "Ang Cao and Justin Johnson and Andrea Vedaldi and David Novotny", "abstract": "  Contemporary 3D research, particularly in reconstruction and generation,\nheavily relies on 2D images for inputs or supervision. However, current designs\nfor these 2D-3D mapping are memory-intensive, posing a significant bottleneck\nfor existing methods and hindering new applications. In response, we propose a\npair of highly scalable components for 3D neural fields: Lightplane Render and\nSplatter, which significantly reduce memory usage in 2D-3D mapping. These\ninnovations enable the processing of vastly more and higher resolution images\nwith small memory and computational costs. We demonstrate their utility in\nvarious applications, from benefiting single-scene optimization with\nimage-level losses to realizing a versatile pipeline for dramatically scaling\n3D reconstruction and generation. Code:\n\\url{https://github.com/facebookresearch/lightplane}.\n", "link": "http://arxiv.org/abs/2404.19760v1", "date": "2024-04-30", "relevancy": 2.1663, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lightplane%3A%20Highly-Scalable%20Components%20for%20Neural%203D%20Fields&body=Title%3A%20Lightplane%3A%20Highly-Scalable%20Components%20for%20Neural%203D%20Fields%0AAuthor%3A%20Ang%20Cao%20and%20Justin%20Johnson%20and%20Andrea%20Vedaldi%20and%20David%20Novotny%0AAbstract%3A%20%20%20Contemporary%203D%20research%2C%20particularly%20in%20reconstruction%20and%20generation%2C%0Aheavily%20relies%20on%202D%20images%20for%20inputs%20or%20supervision.%20However%2C%20current%20designs%0Afor%20these%202D-3D%20mapping%20are%20memory-intensive%2C%20posing%20a%20significant%20bottleneck%0Afor%20existing%20methods%20and%20hindering%20new%20applications.%20In%20response%2C%20we%20propose%20a%0Apair%20of%20highly%20scalable%20components%20for%203D%20neural%20fields%3A%20Lightplane%20Render%20and%0ASplatter%2C%20which%20significantly%20reduce%20memory%20usage%20in%202D-3D%20mapping.%20These%0Ainnovations%20enable%20the%20processing%20of%20vastly%20more%20and%20higher%20resolution%20images%0Awith%20small%20memory%20and%20computational%20costs.%20We%20demonstrate%20their%20utility%20in%0Avarious%20applications%2C%20from%20benefiting%20single-scene%20optimization%20with%0Aimage-level%20losses%20to%20realizing%20a%20versatile%20pipeline%20for%20dramatically%20scaling%0A3D%20reconstruction%20and%20generation.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/facebookresearch/lightplane%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightplane%3A%20Highly-Scalable%20Components%20for%20Neural%203D%20Fields&entry.906535625=Ang%20Cao%20and%20Justin%20Johnson%20and%20Andrea%20Vedaldi%20and%20David%20Novotny&entry.1292438233=%20%20Contemporary%203D%20research%2C%20particularly%20in%20reconstruction%20and%20generation%2C%0Aheavily%20relies%20on%202D%20images%20for%20inputs%20or%20supervision.%20However%2C%20current%20designs%0Afor%20these%202D-3D%20mapping%20are%20memory-intensive%2C%20posing%20a%20significant%20bottleneck%0Afor%20existing%20methods%20and%20hindering%20new%20applications.%20In%20response%2C%20we%20propose%20a%0Apair%20of%20highly%20scalable%20components%20for%203D%20neural%20fields%3A%20Lightplane%20Render%20and%0ASplatter%2C%20which%20significantly%20reduce%20memory%20usage%20in%202D-3D%20mapping.%20These%0Ainnovations%20enable%20the%20processing%20of%20vastly%20more%20and%20higher%20resolution%20images%0Awith%20small%20memory%20and%20computational%20costs.%20We%20demonstrate%20their%20utility%20in%0Avarious%20applications%2C%20from%20benefiting%20single-scene%20optimization%20with%0Aimage-level%20losses%20to%20realizing%20a%20versatile%20pipeline%20for%20dramatically%20scaling%0A3D%20reconstruction%20and%20generation.%20Code%3A%0A%5Curl%7Bhttps%3A//github.com/facebookresearch/lightplane%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19760v1&entry.124074799=Read"},
{"title": "Masked Multi-Query Slot Attention for Unsupervised Object Discovery", "author": "Rishav Pramanik and Jos\u00e9-Fabian Villa-V\u00e1squez and Marco Pedersoli", "abstract": "  Unsupervised object discovery is becoming an essential line of research for\ntackling recognition problems that require decomposing an image into entities,\nsuch as semantic segmentation and object detection. Recently, object-centric\nmethods that leverage self-supervision have gained popularity, due to their\nsimplicity and adaptability to different settings and conditions. However,\nthose methods do not exploit effective techniques already employed in modern\nself-supervised approaches. In this work, we consider an object-centric\napproach in which DINO ViT features are reconstructed via a set of queried\nrepresentations called slots. Based on that, we propose a masking scheme on\ninput features that selectively disregards the background regions, inducing our\nmodel to focus more on salient objects during the reconstruction phase.\nMoreover, we extend the slot attention to a multi-query approach, allowing the\nmodel to learn multiple sets of slots, producing more stable masks. During\ntraining, these multiple sets of slots are learned independently while, at test\ntime, these sets are merged through Hungarian matching to obtain the final\nslots. Our experimental results and ablations on the PASCAL-VOC 2012 dataset\nshow the importance of each component and highlight how their combination\nconsistently improves object localization. Our source code is available at:\nhttps://github.com/rishavpramanik/maskedmultiqueryslot\n", "link": "http://arxiv.org/abs/2404.19654v1", "date": "2024-04-30", "relevancy": 2.1545, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Masked%20Multi-Query%20Slot%20Attention%20for%20Unsupervised%20Object%20Discovery&body=Title%3A%20Masked%20Multi-Query%20Slot%20Attention%20for%20Unsupervised%20Object%20Discovery%0AAuthor%3A%20Rishav%20Pramanik%20and%20Jos%C3%A9-Fabian%20Villa-V%C3%A1squez%20and%20Marco%20Pedersoli%0AAbstract%3A%20%20%20Unsupervised%20object%20discovery%20is%20becoming%20an%20essential%20line%20of%20research%20for%0Atackling%20recognition%20problems%20that%20require%20decomposing%20an%20image%20into%20entities%2C%0Asuch%20as%20semantic%20segmentation%20and%20object%20detection.%20Recently%2C%20object-centric%0Amethods%20that%20leverage%20self-supervision%20have%20gained%20popularity%2C%20due%20to%20their%0Asimplicity%20and%20adaptability%20to%20different%20settings%20and%20conditions.%20However%2C%0Athose%20methods%20do%20not%20exploit%20effective%20techniques%20already%20employed%20in%20modern%0Aself-supervised%20approaches.%20In%20this%20work%2C%20we%20consider%20an%20object-centric%0Aapproach%20in%20which%20DINO%20ViT%20features%20are%20reconstructed%20via%20a%20set%20of%20queried%0Arepresentations%20called%20slots.%20Based%20on%20that%2C%20we%20propose%20a%20masking%20scheme%20on%0Ainput%20features%20that%20selectively%20disregards%20the%20background%20regions%2C%20inducing%20our%0Amodel%20to%20focus%20more%20on%20salient%20objects%20during%20the%20reconstruction%20phase.%0AMoreover%2C%20we%20extend%20the%20slot%20attention%20to%20a%20multi-query%20approach%2C%20allowing%20the%0Amodel%20to%20learn%20multiple%20sets%20of%20slots%2C%20producing%20more%20stable%20masks.%20During%0Atraining%2C%20these%20multiple%20sets%20of%20slots%20are%20learned%20independently%20while%2C%20at%20test%0Atime%2C%20these%20sets%20are%20merged%20through%20Hungarian%20matching%20to%20obtain%20the%20final%0Aslots.%20Our%20experimental%20results%20and%20ablations%20on%20the%20PASCAL-VOC%202012%20dataset%0Ashow%20the%20importance%20of%20each%20component%20and%20highlight%20how%20their%20combination%0Aconsistently%20improves%20object%20localization.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/rishavpramanik/maskedmultiqueryslot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19654v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Multi-Query%20Slot%20Attention%20for%20Unsupervised%20Object%20Discovery&entry.906535625=Rishav%20Pramanik%20and%20Jos%C3%A9-Fabian%20Villa-V%C3%A1squez%20and%20Marco%20Pedersoli&entry.1292438233=%20%20Unsupervised%20object%20discovery%20is%20becoming%20an%20essential%20line%20of%20research%20for%0Atackling%20recognition%20problems%20that%20require%20decomposing%20an%20image%20into%20entities%2C%0Asuch%20as%20semantic%20segmentation%20and%20object%20detection.%20Recently%2C%20object-centric%0Amethods%20that%20leverage%20self-supervision%20have%20gained%20popularity%2C%20due%20to%20their%0Asimplicity%20and%20adaptability%20to%20different%20settings%20and%20conditions.%20However%2C%0Athose%20methods%20do%20not%20exploit%20effective%20techniques%20already%20employed%20in%20modern%0Aself-supervised%20approaches.%20In%20this%20work%2C%20we%20consider%20an%20object-centric%0Aapproach%20in%20which%20DINO%20ViT%20features%20are%20reconstructed%20via%20a%20set%20of%20queried%0Arepresentations%20called%20slots.%20Based%20on%20that%2C%20we%20propose%20a%20masking%20scheme%20on%0Ainput%20features%20that%20selectively%20disregards%20the%20background%20regions%2C%20inducing%20our%0Amodel%20to%20focus%20more%20on%20salient%20objects%20during%20the%20reconstruction%20phase.%0AMoreover%2C%20we%20extend%20the%20slot%20attention%20to%20a%20multi-query%20approach%2C%20allowing%20the%0Amodel%20to%20learn%20multiple%20sets%20of%20slots%2C%20producing%20more%20stable%20masks.%20During%0Atraining%2C%20these%20multiple%20sets%20of%20slots%20are%20learned%20independently%20while%2C%20at%20test%0Atime%2C%20these%20sets%20are%20merged%20through%20Hungarian%20matching%20to%20obtain%20the%20final%0Aslots.%20Our%20experimental%20results%20and%20ablations%20on%20the%20PASCAL-VOC%202012%20dataset%0Ashow%20the%20importance%20of%20each%20component%20and%20highlight%20how%20their%20combination%0Aconsistently%20improves%20object%20localization.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/rishavpramanik/maskedmultiqueryslot%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19654v1&entry.124074799=Read"},
{"title": "Automatic Cardiac Pathology Recognition in Echocardiography Images Using\n  Higher Order Dynamic Mode Decomposition and a Vision Transformer for Small\n  Datasets", "author": "Andr\u00e9s Bell-Navas and Nourelhouda Groun and Mar\u00eda Villalba-Orero and Enrique Lara-Pezzi and Jes\u00fas Garicano-Mena and Soledad Le Clainche", "abstract": "  Heart diseases are the main international cause of human defunction.\nAccording to the WHO, nearly 18 million people decease each year because of\nheart diseases. Also considering the increase of medical data, much pressure is\nput on the health industry to develop systems for early and accurate heart\ndisease recognition. In this work, an automatic cardiac pathology recognition\nsystem based on a novel deep learning framework is proposed, which analyses in\nreal-time echocardiography video sequences. The system works in two stages. The\nfirst one transforms the data included in a database of echocardiography\nsequences into a machine-learning-compatible collection of annotated images\nwhich can be used in the training stage of any kind of machine learning-based\nframework, and more specifically with deep learning. This includes the use of\nthe Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the first\ntime to the authors' knowledge, for both data augmentation and feature\nextraction in the medical field. The second stage is focused on building and\ntraining a Vision Transformer (ViT), barely explored in the related literature.\nThe ViT is adapted for an effective training from scratch, even with small\ndatasets. The designed neural network analyses images from an echocardiography\nsequence to predict the heart state. The results obtained show the superiority\nof the proposed system and the efficacy of the HODMD algorithm, even\noutperforming pretrained Convolutional Neural Networks (CNNs), which are so far\nthe method of choice in the literature.\n", "link": "http://arxiv.org/abs/2404.19579v1", "date": "2024-04-30", "relevancy": 2.1225, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5295}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Cardiac%20Pathology%20Recognition%20in%20Echocardiography%20Images%20Using%0A%20%20Higher%20Order%20Dynamic%20Mode%20Decomposition%20and%20a%20Vision%20Transformer%20for%20Small%0A%20%20Datasets&body=Title%3A%20Automatic%20Cardiac%20Pathology%20Recognition%20in%20Echocardiography%20Images%20Using%0A%20%20Higher%20Order%20Dynamic%20Mode%20Decomposition%20and%20a%20Vision%20Transformer%20for%20Small%0A%20%20Datasets%0AAuthor%3A%20Andr%C3%A9s%20Bell-Navas%20and%20Nourelhouda%20Groun%20and%20Mar%C3%ADa%20Villalba-Orero%20and%20Enrique%20Lara-Pezzi%20and%20Jes%C3%BAs%20Garicano-Mena%20and%20Soledad%20Le%20Clainche%0AAbstract%3A%20%20%20Heart%20diseases%20are%20the%20main%20international%20cause%20of%20human%20defunction.%0AAccording%20to%20the%20WHO%2C%20nearly%2018%20million%20people%20decease%20each%20year%20because%20of%0Aheart%20diseases.%20Also%20considering%20the%20increase%20of%20medical%20data%2C%20much%20pressure%20is%0Aput%20on%20the%20health%20industry%20to%20develop%20systems%20for%20early%20and%20accurate%20heart%0Adisease%20recognition.%20In%20this%20work%2C%20an%20automatic%20cardiac%20pathology%20recognition%0Asystem%20based%20on%20a%20novel%20deep%20learning%20framework%20is%20proposed%2C%20which%20analyses%20in%0Areal-time%20echocardiography%20video%20sequences.%20The%20system%20works%20in%20two%20stages.%20The%0Afirst%20one%20transforms%20the%20data%20included%20in%20a%20database%20of%20echocardiography%0Asequences%20into%20a%20machine-learning-compatible%20collection%20of%20annotated%20images%0Awhich%20can%20be%20used%20in%20the%20training%20stage%20of%20any%20kind%20of%20machine%20learning-based%0Aframework%2C%20and%20more%20specifically%20with%20deep%20learning.%20This%20includes%20the%20use%20of%0Athe%20Higher%20Order%20Dynamic%20Mode%20Decomposition%20%28HODMD%29%20algorithm%2C%20for%20the%20first%0Atime%20to%20the%20authors%27%20knowledge%2C%20for%20both%20data%20augmentation%20and%20feature%0Aextraction%20in%20the%20medical%20field.%20The%20second%20stage%20is%20focused%20on%20building%20and%0Atraining%20a%20Vision%20Transformer%20%28ViT%29%2C%20barely%20explored%20in%20the%20related%20literature.%0AThe%20ViT%20is%20adapted%20for%20an%20effective%20training%20from%20scratch%2C%20even%20with%20small%0Adatasets.%20The%20designed%20neural%20network%20analyses%20images%20from%20an%20echocardiography%0Asequence%20to%20predict%20the%20heart%20state.%20The%20results%20obtained%20show%20the%20superiority%0Aof%20the%20proposed%20system%20and%20the%20efficacy%20of%20the%20HODMD%20algorithm%2C%20even%0Aoutperforming%20pretrained%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20which%20are%20so%20far%0Athe%20method%20of%20choice%20in%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Cardiac%20Pathology%20Recognition%20in%20Echocardiography%20Images%20Using%0A%20%20Higher%20Order%20Dynamic%20Mode%20Decomposition%20and%20a%20Vision%20Transformer%20for%20Small%0A%20%20Datasets&entry.906535625=Andr%C3%A9s%20Bell-Navas%20and%20Nourelhouda%20Groun%20and%20Mar%C3%ADa%20Villalba-Orero%20and%20Enrique%20Lara-Pezzi%20and%20Jes%C3%BAs%20Garicano-Mena%20and%20Soledad%20Le%20Clainche&entry.1292438233=%20%20Heart%20diseases%20are%20the%20main%20international%20cause%20of%20human%20defunction.%0AAccording%20to%20the%20WHO%2C%20nearly%2018%20million%20people%20decease%20each%20year%20because%20of%0Aheart%20diseases.%20Also%20considering%20the%20increase%20of%20medical%20data%2C%20much%20pressure%20is%0Aput%20on%20the%20health%20industry%20to%20develop%20systems%20for%20early%20and%20accurate%20heart%0Adisease%20recognition.%20In%20this%20work%2C%20an%20automatic%20cardiac%20pathology%20recognition%0Asystem%20based%20on%20a%20novel%20deep%20learning%20framework%20is%20proposed%2C%20which%20analyses%20in%0Areal-time%20echocardiography%20video%20sequences.%20The%20system%20works%20in%20two%20stages.%20The%0Afirst%20one%20transforms%20the%20data%20included%20in%20a%20database%20of%20echocardiography%0Asequences%20into%20a%20machine-learning-compatible%20collection%20of%20annotated%20images%0Awhich%20can%20be%20used%20in%20the%20training%20stage%20of%20any%20kind%20of%20machine%20learning-based%0Aframework%2C%20and%20more%20specifically%20with%20deep%20learning.%20This%20includes%20the%20use%20of%0Athe%20Higher%20Order%20Dynamic%20Mode%20Decomposition%20%28HODMD%29%20algorithm%2C%20for%20the%20first%0Atime%20to%20the%20authors%27%20knowledge%2C%20for%20both%20data%20augmentation%20and%20feature%0Aextraction%20in%20the%20medical%20field.%20The%20second%20stage%20is%20focused%20on%20building%20and%0Atraining%20a%20Vision%20Transformer%20%28ViT%29%2C%20barely%20explored%20in%20the%20related%20literature.%0AThe%20ViT%20is%20adapted%20for%20an%20effective%20training%20from%20scratch%2C%20even%20with%20small%0Adatasets.%20The%20designed%20neural%20network%20analyses%20images%20from%20an%20echocardiography%0Asequence%20to%20predict%20the%20heart%20state.%20The%20results%20obtained%20show%20the%20superiority%0Aof%20the%20proposed%20system%20and%20the%20efficacy%20of%20the%20HODMD%20algorithm%2C%20even%0Aoutperforming%20pretrained%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20which%20are%20so%20far%0Athe%20method%20of%20choice%20in%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19579v1&entry.124074799=Read"},
{"title": "Provably Robust Conformal Prediction with Improved Efficiency", "author": "Ge Yan and Yaniv Romano and Tsui-Wei Weng", "abstract": "  Conformal prediction is a powerful tool to generate uncertainty sets with\nguaranteed coverage using any predictive model, under the assumption that the\ntraining and test data are i.i.d.. Recently, it has been shown that adversarial\nexamples are able to manipulate conformal methods to construct prediction sets\nwith invalid coverage rates, as the i.i.d. assumption is violated. To address\nthis issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was\nfirst proposed to certify the robustness of conformal prediction methods to\nadversarial noise. However, RSCP has two major limitations: (i) its robustness\nguarantee is flawed when used in practice and (ii) it tends to produce large\nuncertainty sets. To address these limitations, we first propose a novel\nframework called RSCP+ to provide provable robustness guarantee in evaluation,\nwhich fixes the issues in the original RSCP method. Next, we propose two novel\nmethods, Post-Training Transformation (PTT) and Robust Conformal Training\n(RCT), to effectively reduce prediction set size with little computation\noverhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the\nbaseline method only yields trivial predictions including full label set, while\nour methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and\n$16.9\\times$ respectively and provide practical robustness guarantee. Our codes\nare available at\nhttps://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.\n", "link": "http://arxiv.org/abs/2404.19651v1", "date": "2024-04-30", "relevancy": 2.1086, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.544}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5328}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5148}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Provably%20Robust%20Conformal%20Prediction%20with%20Improved%20Efficiency&body=Title%3A%20Provably%20Robust%20Conformal%20Prediction%20with%20Improved%20Efficiency%0AAuthor%3A%20Ge%20Yan%20and%20Yaniv%20Romano%20and%20Tsui-Wei%20Weng%0AAbstract%3A%20%20%20Conformal%20prediction%20is%20a%20powerful%20tool%20to%20generate%20uncertainty%20sets%20with%0Aguaranteed%20coverage%20using%20any%20predictive%20model%2C%20under%20the%20assumption%20that%20the%0Atraining%20and%20test%20data%20are%20i.i.d..%20Recently%2C%20it%20has%20been%20shown%20that%20adversarial%0Aexamples%20are%20able%20to%20manipulate%20conformal%20methods%20to%20construct%20prediction%20sets%0Awith%20invalid%20coverage%20rates%2C%20as%20the%20i.i.d.%20assumption%20is%20violated.%20To%20address%0Athis%20issue%2C%20a%20recent%20work%2C%20Randomized%20Smoothed%20Conformal%20Prediction%20%28RSCP%29%2C%20was%0Afirst%20proposed%20to%20certify%20the%20robustness%20of%20conformal%20prediction%20methods%20to%0Aadversarial%20noise.%20However%2C%20RSCP%20has%20two%20major%20limitations%3A%20%28i%29%20its%20robustness%0Aguarantee%20is%20flawed%20when%20used%20in%20practice%20and%20%28ii%29%20it%20tends%20to%20produce%20large%0Auncertainty%20sets.%20To%20address%20these%20limitations%2C%20we%20first%20propose%20a%20novel%0Aframework%20called%20RSCP%2B%20to%20provide%20provable%20robustness%20guarantee%20in%20evaluation%2C%0Awhich%20fixes%20the%20issues%20in%20the%20original%20RSCP%20method.%20Next%2C%20we%20propose%20two%20novel%0Amethods%2C%20Post-Training%20Transformation%20%28PTT%29%20and%20Robust%20Conformal%20Training%0A%28RCT%29%2C%20to%20effectively%20reduce%20prediction%20set%20size%20with%20little%20computation%0Aoverhead.%20Experimental%20results%20in%20CIFAR10%2C%20CIFAR100%2C%20and%20ImageNet%20suggest%20the%0Abaseline%20method%20only%20yields%20trivial%20predictions%20including%20full%20label%20set%2C%20while%0Aour%20methods%20could%20boost%20the%20efficiency%20by%20up%20to%20%244.36%5Ctimes%24%2C%20%245.46%5Ctimes%24%2C%20and%0A%2416.9%5Ctimes%24%20respectively%20and%20provide%20practical%20robustness%20guarantee.%20Our%20codes%0Aare%20available%20at%0Ahttps%3A//github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19651v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Robust%20Conformal%20Prediction%20with%20Improved%20Efficiency&entry.906535625=Ge%20Yan%20and%20Yaniv%20Romano%20and%20Tsui-Wei%20Weng&entry.1292438233=%20%20Conformal%20prediction%20is%20a%20powerful%20tool%20to%20generate%20uncertainty%20sets%20with%0Aguaranteed%20coverage%20using%20any%20predictive%20model%2C%20under%20the%20assumption%20that%20the%0Atraining%20and%20test%20data%20are%20i.i.d..%20Recently%2C%20it%20has%20been%20shown%20that%20adversarial%0Aexamples%20are%20able%20to%20manipulate%20conformal%20methods%20to%20construct%20prediction%20sets%0Awith%20invalid%20coverage%20rates%2C%20as%20the%20i.i.d.%20assumption%20is%20violated.%20To%20address%0Athis%20issue%2C%20a%20recent%20work%2C%20Randomized%20Smoothed%20Conformal%20Prediction%20%28RSCP%29%2C%20was%0Afirst%20proposed%20to%20certify%20the%20robustness%20of%20conformal%20prediction%20methods%20to%0Aadversarial%20noise.%20However%2C%20RSCP%20has%20two%20major%20limitations%3A%20%28i%29%20its%20robustness%0Aguarantee%20is%20flawed%20when%20used%20in%20practice%20and%20%28ii%29%20it%20tends%20to%20produce%20large%0Auncertainty%20sets.%20To%20address%20these%20limitations%2C%20we%20first%20propose%20a%20novel%0Aframework%20called%20RSCP%2B%20to%20provide%20provable%20robustness%20guarantee%20in%20evaluation%2C%0Awhich%20fixes%20the%20issues%20in%20the%20original%20RSCP%20method.%20Next%2C%20we%20propose%20two%20novel%0Amethods%2C%20Post-Training%20Transformation%20%28PTT%29%20and%20Robust%20Conformal%20Training%0A%28RCT%29%2C%20to%20effectively%20reduce%20prediction%20set%20size%20with%20little%20computation%0Aoverhead.%20Experimental%20results%20in%20CIFAR10%2C%20CIFAR100%2C%20and%20ImageNet%20suggest%20the%0Abaseline%20method%20only%20yields%20trivial%20predictions%20including%20full%20label%20set%2C%20while%0Aour%20methods%20could%20boost%20the%20efficiency%20by%20up%20to%20%244.36%5Ctimes%24%2C%20%245.46%5Ctimes%24%2C%20and%0A%2416.9%5Ctimes%24%20respectively%20and%20provide%20practical%20robustness%20guarantee.%20Our%20codes%0Aare%20available%20at%0Ahttps%3A//github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19651v1&entry.124074799=Read"},
{"title": "Towards Real-world Video Face Restoration: A New Benchmark", "author": "Ziyan Chen and Jingwen He and Xinqi Lin and Yu Qiao and Chao Dong", "abstract": "  Blind face restoration (BFR) on images has significantly progressed over the\nlast several years, while real-world video face restoration (VFR), which is\nmore challenging for more complex face motions such as moving gaze directions\nand facial orientations involved, remains unsolved. Typical BFR methods are\nevaluated on privately synthesized datasets or self-collected real-world\nlow-quality face images, which are limited in their coverage of real-world\nvideo frames. In this work, we introduced new real-world datasets named FOS\nwith a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to\nstudy the applicability of current methods on videos. Compared with existing\ntest datasets, FOS datasets cover more diverse degradations and involve face\nsamples from more complex scenarios, which helps to revisit current face\nrestoration approaches more comprehensively. Given the established datasets, we\nbenchmarked both the state-of-the-art BFR methods and the video super\nresolution (VSR) methods to comprehensively study current approaches,\nidentifying their potential and limitations in VFR tasks. In addition, we\nstudied the effectiveness of the commonly used image quality assessment (IQA)\nmetrics and face IQA (FIQA) metrics by leveraging a subjective user study. With\nextensive experimental results and detailed analysis provided, we gained\ninsights from the successes and failures of both current BFR and VSR methods.\nThese results also pose challenges to current face restoration approaches,\nwhich we hope stimulate future advances in VFR research.\n", "link": "http://arxiv.org/abs/2404.19500v1", "date": "2024-04-30", "relevancy": 2.1081, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5071}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4953}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Real-world%20Video%20Face%20Restoration%3A%20A%20New%20Benchmark&body=Title%3A%20Towards%20Real-world%20Video%20Face%20Restoration%3A%20A%20New%20Benchmark%0AAuthor%3A%20Ziyan%20Chen%20and%20Jingwen%20He%20and%20Xinqi%20Lin%20and%20Yu%20Qiao%20and%20Chao%20Dong%0AAbstract%3A%20%20%20Blind%20face%20restoration%20%28BFR%29%20on%20images%20has%20significantly%20progressed%20over%20the%0Alast%20several%20years%2C%20while%20real-world%20video%20face%20restoration%20%28VFR%29%2C%20which%20is%0Amore%20challenging%20for%20more%20complex%20face%20motions%20such%20as%20moving%20gaze%20directions%0Aand%20facial%20orientations%20involved%2C%20remains%20unsolved.%20Typical%20BFR%20methods%20are%0Aevaluated%20on%20privately%20synthesized%20datasets%20or%20self-collected%20real-world%0Alow-quality%20face%20images%2C%20which%20are%20limited%20in%20their%20coverage%20of%20real-world%0Avideo%20frames.%20In%20this%20work%2C%20we%20introduced%20new%20real-world%20datasets%20named%20FOS%0Awith%20a%20taxonomy%20of%20%22Full%2C%20Occluded%2C%20and%20Side%22%20faces%20from%20mainly%20video%20frames%20to%0Astudy%20the%20applicability%20of%20current%20methods%20on%20videos.%20Compared%20with%20existing%0Atest%20datasets%2C%20FOS%20datasets%20cover%20more%20diverse%20degradations%20and%20involve%20face%0Asamples%20from%20more%20complex%20scenarios%2C%20which%20helps%20to%20revisit%20current%20face%0Arestoration%20approaches%20more%20comprehensively.%20Given%20the%20established%20datasets%2C%20we%0Abenchmarked%20both%20the%20state-of-the-art%20BFR%20methods%20and%20the%20video%20super%0Aresolution%20%28VSR%29%20methods%20to%20comprehensively%20study%20current%20approaches%2C%0Aidentifying%20their%20potential%20and%20limitations%20in%20VFR%20tasks.%20In%20addition%2C%20we%0Astudied%20the%20effectiveness%20of%20the%20commonly%20used%20image%20quality%20assessment%20%28IQA%29%0Ametrics%20and%20face%20IQA%20%28FIQA%29%20metrics%20by%20leveraging%20a%20subjective%20user%20study.%20With%0Aextensive%20experimental%20results%20and%20detailed%20analysis%20provided%2C%20we%20gained%0Ainsights%20from%20the%20successes%20and%20failures%20of%20both%20current%20BFR%20and%20VSR%20methods.%0AThese%20results%20also%20pose%20challenges%20to%20current%20face%20restoration%20approaches%2C%0Awhich%20we%20hope%20stimulate%20future%20advances%20in%20VFR%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19500v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Real-world%20Video%20Face%20Restoration%3A%20A%20New%20Benchmark&entry.906535625=Ziyan%20Chen%20and%20Jingwen%20He%20and%20Xinqi%20Lin%20and%20Yu%20Qiao%20and%20Chao%20Dong&entry.1292438233=%20%20Blind%20face%20restoration%20%28BFR%29%20on%20images%20has%20significantly%20progressed%20over%20the%0Alast%20several%20years%2C%20while%20real-world%20video%20face%20restoration%20%28VFR%29%2C%20which%20is%0Amore%20challenging%20for%20more%20complex%20face%20motions%20such%20as%20moving%20gaze%20directions%0Aand%20facial%20orientations%20involved%2C%20remains%20unsolved.%20Typical%20BFR%20methods%20are%0Aevaluated%20on%20privately%20synthesized%20datasets%20or%20self-collected%20real-world%0Alow-quality%20face%20images%2C%20which%20are%20limited%20in%20their%20coverage%20of%20real-world%0Avideo%20frames.%20In%20this%20work%2C%20we%20introduced%20new%20real-world%20datasets%20named%20FOS%0Awith%20a%20taxonomy%20of%20%22Full%2C%20Occluded%2C%20and%20Side%22%20faces%20from%20mainly%20video%20frames%20to%0Astudy%20the%20applicability%20of%20current%20methods%20on%20videos.%20Compared%20with%20existing%0Atest%20datasets%2C%20FOS%20datasets%20cover%20more%20diverse%20degradations%20and%20involve%20face%0Asamples%20from%20more%20complex%20scenarios%2C%20which%20helps%20to%20revisit%20current%20face%0Arestoration%20approaches%20more%20comprehensively.%20Given%20the%20established%20datasets%2C%20we%0Abenchmarked%20both%20the%20state-of-the-art%20BFR%20methods%20and%20the%20video%20super%0Aresolution%20%28VSR%29%20methods%20to%20comprehensively%20study%20current%20approaches%2C%0Aidentifying%20their%20potential%20and%20limitations%20in%20VFR%20tasks.%20In%20addition%2C%20we%0Astudied%20the%20effectiveness%20of%20the%20commonly%20used%20image%20quality%20assessment%20%28IQA%29%0Ametrics%20and%20face%20IQA%20%28FIQA%29%20metrics%20by%20leveraging%20a%20subjective%20user%20study.%20With%0Aextensive%20experimental%20results%20and%20detailed%20analysis%20provided%2C%20we%20gained%0Ainsights%20from%20the%20successes%20and%20failures%20of%20both%20current%20BFR%20and%20VSR%20methods.%0AThese%20results%20also%20pose%20challenges%20to%20current%20face%20restoration%20approaches%2C%0Awhich%20we%20hope%20stimulate%20future%20advances%20in%20VFR%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19500v1&entry.124074799=Read"},
{"title": "EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision", "author": "Yufeng Yang and Adrian Kneip and Charlotte Frenkel", "abstract": "  Edge vision systems combining sensing and embedded processing promise\nlow-latency, decentralized, and energy-efficient solutions that forgo reliance\non the cloud. As opposed to conventional frame-based vision sensors,\nevent-based cameras deliver a microsecond-scale temporal resolution with sparse\ninformation encoding, thereby outlining new opportunities for edge vision\nsystems. However, mainstream algorithms for frame-based vision, which mostly\nrely on convolutional neural networks (CNNs), can hardly exploit the advantages\nof event-based vision as they are typically optimized for dense matrix-vector\nmultiplications. While event-driven graph neural networks (GNNs) have recently\nemerged as a promising solution for sparse event-based vision, their irregular\nstructure is a challenge that currently hinders the design of efficient\nhardware accelerators. In this paper, we propose EvGNN, the first event-driven\nGNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge\nvision with event-based cameras. It relies on three central ideas: (i) directed\ndynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event\nqueues for the efficient identification of local neighbors within a\nspatiotemporally decoupled search range, and (iii) a novel layer-parallel\nprocessing scheme enabling the low-latency execution of multi-layer GNNs. We\ndeployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it\non the N-CARS dataset for car recognition, demonstrating a classification\naccuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby\nenabling real-time, microsecond-resolution event-based vision at the edge.\n", "link": "http://arxiv.org/abs/2404.19489v1", "date": "2024-04-30", "relevancy": 2.1067, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5427}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5254}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5112}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EvGNN%3A%20An%20Event-driven%20Graph%20Neural%20Network%20Accelerator%20for%20Edge%20Vision&body=Title%3A%20EvGNN%3A%20An%20Event-driven%20Graph%20Neural%20Network%20Accelerator%20for%20Edge%20Vision%0AAuthor%3A%20Yufeng%20Yang%20and%20Adrian%20Kneip%20and%20Charlotte%20Frenkel%0AAbstract%3A%20%20%20Edge%20vision%20systems%20combining%20sensing%20and%20embedded%20processing%20promise%0Alow-latency%2C%20decentralized%2C%20and%20energy-efficient%20solutions%20that%20forgo%20reliance%0Aon%20the%20cloud.%20As%20opposed%20to%20conventional%20frame-based%20vision%20sensors%2C%0Aevent-based%20cameras%20deliver%20a%20microsecond-scale%20temporal%20resolution%20with%20sparse%0Ainformation%20encoding%2C%20thereby%20outlining%20new%20opportunities%20for%20edge%20vision%0Asystems.%20However%2C%20mainstream%20algorithms%20for%20frame-based%20vision%2C%20which%20mostly%0Arely%20on%20convolutional%20neural%20networks%20%28CNNs%29%2C%20can%20hardly%20exploit%20the%20advantages%0Aof%20event-based%20vision%20as%20they%20are%20typically%20optimized%20for%20dense%20matrix-vector%0Amultiplications.%20While%20event-driven%20graph%20neural%20networks%20%28GNNs%29%20have%20recently%0Aemerged%20as%20a%20promising%20solution%20for%20sparse%20event-based%20vision%2C%20their%20irregular%0Astructure%20is%20a%20challenge%20that%20currently%20hinders%20the%20design%20of%20efficient%0Ahardware%20accelerators.%20In%20this%20paper%2C%20we%20propose%20EvGNN%2C%20the%20first%20event-driven%0AGNN%20accelerator%20for%20low-footprint%2C%20ultra-low-latency%2C%20and%20high-accuracy%20edge%0Avision%20with%20event-based%20cameras.%20It%20relies%20on%20three%20central%20ideas%3A%20%28i%29%20directed%0Adynamic%20graphs%20exploiting%20single-hop%20nodes%20with%20edge-free%20storage%2C%20%28ii%29%20event%0Aqueues%20for%20the%20efficient%20identification%20of%20local%20neighbors%20within%20a%0Aspatiotemporally%20decoupled%20search%20range%2C%20and%20%28iii%29%20a%20novel%20layer-parallel%0Aprocessing%20scheme%20enabling%20the%20low-latency%20execution%20of%20multi-layer%20GNNs.%20We%0Adeployed%20EvGNN%20on%20a%20Xilinx%20KV260%20Ultrascale%2B%20MPSoC%20platform%20and%20benchmarked%20it%0Aon%20the%20N-CARS%20dataset%20for%20car%20recognition%2C%20demonstrating%20a%20classification%0Aaccuracy%20of%2087.8%25%20and%20an%20average%20latency%20per%20event%20of%2016%24%5Cmu%24s%2C%20thereby%0Aenabling%20real-time%2C%20microsecond-resolution%20event-based%20vision%20at%20the%20edge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19489v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvGNN%3A%20An%20Event-driven%20Graph%20Neural%20Network%20Accelerator%20for%20Edge%20Vision&entry.906535625=Yufeng%20Yang%20and%20Adrian%20Kneip%20and%20Charlotte%20Frenkel&entry.1292438233=%20%20Edge%20vision%20systems%20combining%20sensing%20and%20embedded%20processing%20promise%0Alow-latency%2C%20decentralized%2C%20and%20energy-efficient%20solutions%20that%20forgo%20reliance%0Aon%20the%20cloud.%20As%20opposed%20to%20conventional%20frame-based%20vision%20sensors%2C%0Aevent-based%20cameras%20deliver%20a%20microsecond-scale%20temporal%20resolution%20with%20sparse%0Ainformation%20encoding%2C%20thereby%20outlining%20new%20opportunities%20for%20edge%20vision%0Asystems.%20However%2C%20mainstream%20algorithms%20for%20frame-based%20vision%2C%20which%20mostly%0Arely%20on%20convolutional%20neural%20networks%20%28CNNs%29%2C%20can%20hardly%20exploit%20the%20advantages%0Aof%20event-based%20vision%20as%20they%20are%20typically%20optimized%20for%20dense%20matrix-vector%0Amultiplications.%20While%20event-driven%20graph%20neural%20networks%20%28GNNs%29%20have%20recently%0Aemerged%20as%20a%20promising%20solution%20for%20sparse%20event-based%20vision%2C%20their%20irregular%0Astructure%20is%20a%20challenge%20that%20currently%20hinders%20the%20design%20of%20efficient%0Ahardware%20accelerators.%20In%20this%20paper%2C%20we%20propose%20EvGNN%2C%20the%20first%20event-driven%0AGNN%20accelerator%20for%20low-footprint%2C%20ultra-low-latency%2C%20and%20high-accuracy%20edge%0Avision%20with%20event-based%20cameras.%20It%20relies%20on%20three%20central%20ideas%3A%20%28i%29%20directed%0Adynamic%20graphs%20exploiting%20single-hop%20nodes%20with%20edge-free%20storage%2C%20%28ii%29%20event%0Aqueues%20for%20the%20efficient%20identification%20of%20local%20neighbors%20within%20a%0Aspatiotemporally%20decoupled%20search%20range%2C%20and%20%28iii%29%20a%20novel%20layer-parallel%0Aprocessing%20scheme%20enabling%20the%20low-latency%20execution%20of%20multi-layer%20GNNs.%20We%0Adeployed%20EvGNN%20on%20a%20Xilinx%20KV260%20Ultrascale%2B%20MPSoC%20platform%20and%20benchmarked%20it%0Aon%20the%20N-CARS%20dataset%20for%20car%20recognition%2C%20demonstrating%20a%20classification%0Aaccuracy%20of%2087.8%25%20and%20an%20average%20latency%20per%20event%20of%2016%24%5Cmu%24s%2C%20thereby%0Aenabling%20real-time%2C%20microsecond-resolution%20event-based%20vision%20at%20the%20edge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19489v1&entry.124074799=Read"},
{"title": "The Machine Vision Iceberg Explained: Advancing Dynamic Testing by\n  Considering Holistic Environmental Relations", "author": "Hubert Padusinski and Christian Steinhauser and Thilo Braun and Lennart Ries and Eric Sax", "abstract": "  Machine Vision (MV) is essential for solving driving automation. This paper\nexamines potential shortcomings in current MV testing strategies for highly\nautomated driving (HAD) systems. We argue for a more comprehensive\nunderstanding of the performance factors that must be considered during the MV\nevaluation process, noting that neglecting these factors can lead to\nsignificant risks. This is not only relevant to MV component testing, but also\nto integration testing. To illustrate this point, we draw an analogy to a ship\nnavigating towards an iceberg to show potential hidden challenges in current MV\ntesting strategies. The main contribution is a novel framework for black-box\ntesting which observes environmental relations. This means it is designed to\nenhance MV assessments by considering the attributes and surroundings of\nrelevant individual objects. The framework provides the identification of seven\ngeneral concerns about the object recognition of MV, which are not addressed\nadequately in established test processes. To detect these deficits based on\ntheir performance factors, we propose the use of a taxonomy called \"granularity\norders\" along with a graphical representation. This allows an identification of\nMV uncertainties across a range of driving scenarios. This approach aims to\nadvance the precision, efficiency, and completeness of testing procedures for\nMV.\n", "link": "http://arxiv.org/abs/2401.14831v3", "date": "2024-04-30", "relevancy": 2.0955, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Machine%20Vision%20Iceberg%20Explained%3A%20Advancing%20Dynamic%20Testing%20by%0A%20%20Considering%20Holistic%20Environmental%20Relations&body=Title%3A%20The%20Machine%20Vision%20Iceberg%20Explained%3A%20Advancing%20Dynamic%20Testing%20by%0A%20%20Considering%20Holistic%20Environmental%20Relations%0AAuthor%3A%20Hubert%20Padusinski%20and%20Christian%20Steinhauser%20and%20Thilo%20Braun%20and%20Lennart%20Ries%20and%20Eric%20Sax%0AAbstract%3A%20%20%20Machine%20Vision%20%28MV%29%20is%20essential%20for%20solving%20driving%20automation.%20This%20paper%0Aexamines%20potential%20shortcomings%20in%20current%20MV%20testing%20strategies%20for%20highly%0Aautomated%20driving%20%28HAD%29%20systems.%20We%20argue%20for%20a%20more%20comprehensive%0Aunderstanding%20of%20the%20performance%20factors%20that%20must%20be%20considered%20during%20the%20MV%0Aevaluation%20process%2C%20noting%20that%20neglecting%20these%20factors%20can%20lead%20to%0Asignificant%20risks.%20This%20is%20not%20only%20relevant%20to%20MV%20component%20testing%2C%20but%20also%0Ato%20integration%20testing.%20To%20illustrate%20this%20point%2C%20we%20draw%20an%20analogy%20to%20a%20ship%0Anavigating%20towards%20an%20iceberg%20to%20show%20potential%20hidden%20challenges%20in%20current%20MV%0Atesting%20strategies.%20The%20main%20contribution%20is%20a%20novel%20framework%20for%20black-box%0Atesting%20which%20observes%20environmental%20relations.%20This%20means%20it%20is%20designed%20to%0Aenhance%20MV%20assessments%20by%20considering%20the%20attributes%20and%20surroundings%20of%0Arelevant%20individual%20objects.%20The%20framework%20provides%20the%20identification%20of%20seven%0Ageneral%20concerns%20about%20the%20object%20recognition%20of%20MV%2C%20which%20are%20not%20addressed%0Aadequately%20in%20established%20test%20processes.%20To%20detect%20these%20deficits%20based%20on%0Atheir%20performance%20factors%2C%20we%20propose%20the%20use%20of%20a%20taxonomy%20called%20%22granularity%0Aorders%22%20along%20with%20a%20graphical%20representation.%20This%20allows%20an%20identification%20of%0AMV%20uncertainties%20across%20a%20range%20of%20driving%20scenarios.%20This%20approach%20aims%20to%0Aadvance%20the%20precision%2C%20efficiency%2C%20and%20completeness%20of%20testing%20procedures%20for%0AMV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14831v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Machine%20Vision%20Iceberg%20Explained%3A%20Advancing%20Dynamic%20Testing%20by%0A%20%20Considering%20Holistic%20Environmental%20Relations&entry.906535625=Hubert%20Padusinski%20and%20Christian%20Steinhauser%20and%20Thilo%20Braun%20and%20Lennart%20Ries%20and%20Eric%20Sax&entry.1292438233=%20%20Machine%20Vision%20%28MV%29%20is%20essential%20for%20solving%20driving%20automation.%20This%20paper%0Aexamines%20potential%20shortcomings%20in%20current%20MV%20testing%20strategies%20for%20highly%0Aautomated%20driving%20%28HAD%29%20systems.%20We%20argue%20for%20a%20more%20comprehensive%0Aunderstanding%20of%20the%20performance%20factors%20that%20must%20be%20considered%20during%20the%20MV%0Aevaluation%20process%2C%20noting%20that%20neglecting%20these%20factors%20can%20lead%20to%0Asignificant%20risks.%20This%20is%20not%20only%20relevant%20to%20MV%20component%20testing%2C%20but%20also%0Ato%20integration%20testing.%20To%20illustrate%20this%20point%2C%20we%20draw%20an%20analogy%20to%20a%20ship%0Anavigating%20towards%20an%20iceberg%20to%20show%20potential%20hidden%20challenges%20in%20current%20MV%0Atesting%20strategies.%20The%20main%20contribution%20is%20a%20novel%20framework%20for%20black-box%0Atesting%20which%20observes%20environmental%20relations.%20This%20means%20it%20is%20designed%20to%0Aenhance%20MV%20assessments%20by%20considering%20the%20attributes%20and%20surroundings%20of%0Arelevant%20individual%20objects.%20The%20framework%20provides%20the%20identification%20of%20seven%0Ageneral%20concerns%20about%20the%20object%20recognition%20of%20MV%2C%20which%20are%20not%20addressed%0Aadequately%20in%20established%20test%20processes.%20To%20detect%20these%20deficits%20based%20on%0Atheir%20performance%20factors%2C%20we%20propose%20the%20use%20of%20a%20taxonomy%20called%20%22granularity%0Aorders%22%20along%20with%20a%20graphical%20representation.%20This%20allows%20an%20identification%20of%0AMV%20uncertainties%20across%20a%20range%20of%20driving%20scenarios.%20This%20approach%20aims%20to%0Aadvance%20the%20precision%2C%20efficiency%2C%20and%20completeness%20of%20testing%20procedures%20for%0AMV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14831v3&entry.124074799=Read"},
{"title": "Sensorized Soft Skin for Dexterous Robotic Hands", "author": "Jana Egli and Benedek Forrai and Thomas Buchner and Jiangtao Su and Xiaodong Chen and Robert K. Katzschmann", "abstract": "  Conventional industrial robots often use two-fingered grippers or suction\ncups to manipulate objects or interact with the world. Because of their\nsimplified design, they are unable to reproduce the dexterity of human hands\nwhen manipulating a wide range of objects. While the control of humanoid hands\nevolved greatly, hardware platforms still lack capabilities, particularly in\ntactile sensing and providing soft contact surfaces. In this work, we present a\nmethod that equips the skeleton of a tendon-driven humanoid hand with a soft\nand sensorized tactile skin. Multi-material 3D printing allows us to\niteratively approach a cast skin design which preserves the robot's dexterity\nin terms of range of motion and speed. We demonstrate that a soft skin enables\nfirmer grasps and piezoresistive sensor integration enhances the hand's tactile\nsensing capabilities.\n", "link": "http://arxiv.org/abs/2404.19448v1", "date": "2024-04-30", "relevancy": 2.0892, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sensorized%20Soft%20Skin%20for%20Dexterous%20Robotic%20Hands&body=Title%3A%20Sensorized%20Soft%20Skin%20for%20Dexterous%20Robotic%20Hands%0AAuthor%3A%20Jana%20Egli%20and%20Benedek%20Forrai%20and%20Thomas%20Buchner%20and%20Jiangtao%20Su%20and%20Xiaodong%20Chen%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20Conventional%20industrial%20robots%20often%20use%20two-fingered%20grippers%20or%20suction%0Acups%20to%20manipulate%20objects%20or%20interact%20with%20the%20world.%20Because%20of%20their%0Asimplified%20design%2C%20they%20are%20unable%20to%20reproduce%20the%20dexterity%20of%20human%20hands%0Awhen%20manipulating%20a%20wide%20range%20of%20objects.%20While%20the%20control%20of%20humanoid%20hands%0Aevolved%20greatly%2C%20hardware%20platforms%20still%20lack%20capabilities%2C%20particularly%20in%0Atactile%20sensing%20and%20providing%20soft%20contact%20surfaces.%20In%20this%20work%2C%20we%20present%20a%0Amethod%20that%20equips%20the%20skeleton%20of%20a%20tendon-driven%20humanoid%20hand%20with%20a%20soft%0Aand%20sensorized%20tactile%20skin.%20Multi-material%203D%20printing%20allows%20us%20to%0Aiteratively%20approach%20a%20cast%20skin%20design%20which%20preserves%20the%20robot%27s%20dexterity%0Ain%20terms%20of%20range%20of%20motion%20and%20speed.%20We%20demonstrate%20that%20a%20soft%20skin%20enables%0Afirmer%20grasps%20and%20piezoresistive%20sensor%20integration%20enhances%20the%20hand%27s%20tactile%0Asensing%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19448v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sensorized%20Soft%20Skin%20for%20Dexterous%20Robotic%20Hands&entry.906535625=Jana%20Egli%20and%20Benedek%20Forrai%20and%20Thomas%20Buchner%20and%20Jiangtao%20Su%20and%20Xiaodong%20Chen%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20Conventional%20industrial%20robots%20often%20use%20two-fingered%20grippers%20or%20suction%0Acups%20to%20manipulate%20objects%20or%20interact%20with%20the%20world.%20Because%20of%20their%0Asimplified%20design%2C%20they%20are%20unable%20to%20reproduce%20the%20dexterity%20of%20human%20hands%0Awhen%20manipulating%20a%20wide%20range%20of%20objects.%20While%20the%20control%20of%20humanoid%20hands%0Aevolved%20greatly%2C%20hardware%20platforms%20still%20lack%20capabilities%2C%20particularly%20in%0Atactile%20sensing%20and%20providing%20soft%20contact%20surfaces.%20In%20this%20work%2C%20we%20present%20a%0Amethod%20that%20equips%20the%20skeleton%20of%20a%20tendon-driven%20humanoid%20hand%20with%20a%20soft%0Aand%20sensorized%20tactile%20skin.%20Multi-material%203D%20printing%20allows%20us%20to%0Aiteratively%20approach%20a%20cast%20skin%20design%20which%20preserves%20the%20robot%27s%20dexterity%0Ain%20terms%20of%20range%20of%20motion%20and%20speed.%20We%20demonstrate%20that%20a%20soft%20skin%20enables%0Afirmer%20grasps%20and%20piezoresistive%20sensor%20integration%20enhances%20the%20hand%27s%20tactile%0Asensing%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19448v1&entry.124074799=Read"},
{"title": "SemiPL: A Semi-supervised Method for Event Sound Source Localization", "author": "Yue Li and Baiqiao Yin and Jinfu Liu and Jiajun Wen and Jiaying Lin and Mengyuan Liu", "abstract": "  In recent years, Event Sound Source Localization has been widely applied in\nvarious fields. Recent works typically relying on the contrastive learning\nframework show impressive performance. However, all work is based on large\nrelatively simple datasets. It's also crucial to understand and analyze human\nbehaviors (actions and interactions of people), voices, and sounds in chaotic\nevents in many applications, e.g., crowd management, and emergency response\nservices. In this paper, we apply the existing model to a more complex dataset,\nexplore the influence of parameters on the model, and propose a semi-supervised\nimprovement method SemiPL. With the increase in data quantity and the influence\nof label quality, self-supervised learning will be an unstoppable trend. The\nexperiment shows that the parameter adjustment will positively affect the\nexisting model. In particular, SSPL achieved an improvement of 12.2% cIoU and\n0.56% AUC in Chaotic World compared to the results provided. The code is\navailable at: https://github.com/ly245422/SSPL\n", "link": "http://arxiv.org/abs/2404.19615v1", "date": "2024-04-30", "relevancy": 2.0805, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.513}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SemiPL%3A%20A%20Semi-supervised%20Method%20for%20Event%20Sound%20Source%20Localization&body=Title%3A%20SemiPL%3A%20A%20Semi-supervised%20Method%20for%20Event%20Sound%20Source%20Localization%0AAuthor%3A%20Yue%20Li%20and%20Baiqiao%20Yin%20and%20Jinfu%20Liu%20and%20Jiajun%20Wen%20and%20Jiaying%20Lin%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Event%20Sound%20Source%20Localization%20has%20been%20widely%20applied%20in%0Avarious%20fields.%20Recent%20works%20typically%20relying%20on%20the%20contrastive%20learning%0Aframework%20show%20impressive%20performance.%20However%2C%20all%20work%20is%20based%20on%20large%0Arelatively%20simple%20datasets.%20It%27s%20also%20crucial%20to%20understand%20and%20analyze%20human%0Abehaviors%20%28actions%20and%20interactions%20of%20people%29%2C%20voices%2C%20and%20sounds%20in%20chaotic%0Aevents%20in%20many%20applications%2C%20e.g.%2C%20crowd%20management%2C%20and%20emergency%20response%0Aservices.%20In%20this%20paper%2C%20we%20apply%20the%20existing%20model%20to%20a%20more%20complex%20dataset%2C%0Aexplore%20the%20influence%20of%20parameters%20on%20the%20model%2C%20and%20propose%20a%20semi-supervised%0Aimprovement%20method%20SemiPL.%20With%20the%20increase%20in%20data%20quantity%20and%20the%20influence%0Aof%20label%20quality%2C%20self-supervised%20learning%20will%20be%20an%20unstoppable%20trend.%20The%0Aexperiment%20shows%20that%20the%20parameter%20adjustment%20will%20positively%20affect%20the%0Aexisting%20model.%20In%20particular%2C%20SSPL%20achieved%20an%20improvement%20of%2012.2%25%20cIoU%20and%0A0.56%25%20AUC%20in%20Chaotic%20World%20compared%20to%20the%20results%20provided.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/ly245422/SSPL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19615v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemiPL%3A%20A%20Semi-supervised%20Method%20for%20Event%20Sound%20Source%20Localization&entry.906535625=Yue%20Li%20and%20Baiqiao%20Yin%20and%20Jinfu%20Liu%20and%20Jiajun%20Wen%20and%20Jiaying%20Lin%20and%20Mengyuan%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20Event%20Sound%20Source%20Localization%20has%20been%20widely%20applied%20in%0Avarious%20fields.%20Recent%20works%20typically%20relying%20on%20the%20contrastive%20learning%0Aframework%20show%20impressive%20performance.%20However%2C%20all%20work%20is%20based%20on%20large%0Arelatively%20simple%20datasets.%20It%27s%20also%20crucial%20to%20understand%20and%20analyze%20human%0Abehaviors%20%28actions%20and%20interactions%20of%20people%29%2C%20voices%2C%20and%20sounds%20in%20chaotic%0Aevents%20in%20many%20applications%2C%20e.g.%2C%20crowd%20management%2C%20and%20emergency%20response%0Aservices.%20In%20this%20paper%2C%20we%20apply%20the%20existing%20model%20to%20a%20more%20complex%20dataset%2C%0Aexplore%20the%20influence%20of%20parameters%20on%20the%20model%2C%20and%20propose%20a%20semi-supervised%0Aimprovement%20method%20SemiPL.%20With%20the%20increase%20in%20data%20quantity%20and%20the%20influence%0Aof%20label%20quality%2C%20self-supervised%20learning%20will%20be%20an%20unstoppable%20trend.%20The%0Aexperiment%20shows%20that%20the%20parameter%20adjustment%20will%20positively%20affect%20the%0Aexisting%20model.%20In%20particular%2C%20SSPL%20achieved%20an%20improvement%20of%2012.2%25%20cIoU%20and%0A0.56%25%20AUC%20in%20Chaotic%20World%20compared%20to%20the%20results%20provided.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/ly245422/SSPL%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19615v1&entry.124074799=Read"},
{"title": "A Framework for Leveraging Human Computation Gaming to Enhance Knowledge\n  Graphs for Accuracy Critical Generative AI Applications", "author": "Steph Buongiorno and Corey Clark", "abstract": "  External knowledge graphs (KGs) can be used to augment large language models\n(LLMs), while simultaneously providing an explainable knowledge base of facts\nthat can be inspected by a human. This approach may be particularly valuable in\ndomains where explainability is critical, like human trafficking data analysis.\nHowever, creating KGs can pose challenges. KGs parsed from documents may\ncomprise explicit connections (those directly stated by a document) but miss\nimplicit connections (those obvious to a human although not directly stated).\nTo address these challenges, this preliminary research introduces the GAME-KG\nframework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge\nGraphs.\" GAME-KG is a federated approach to modifying explicit as well as\nimplicit connections in KGs by using crowdsourced feedback collected through\nvideo games. GAME-KG is shown through two demonstrations: a Unity test scenario\nfrom Dark Shadows, a video game that collects feedback on KGs parsed from US\nDepartment of Justice (DOJ) Press Releases on human trafficking, and a\nfollowing experiment where OpenAI's GPT-4 is prompted to answer questions based\non a modified and unmodified KG. Initial results suggest that GAME-KG can be an\neffective framework for enhancing KGs, while simultaneously providing an\nexplainable set of structured facts verified by humans.\n", "link": "http://arxiv.org/abs/2404.19729v1", "date": "2024-04-30", "relevancy": 2.074, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5438}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5163}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5106}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Leveraging%20Human%20Computation%20Gaming%20to%20Enhance%20Knowledge%0A%20%20Graphs%20for%20Accuracy%20Critical%20Generative%20AI%20Applications&body=Title%3A%20A%20Framework%20for%20Leveraging%20Human%20Computation%20Gaming%20to%20Enhance%20Knowledge%0A%20%20Graphs%20for%20Accuracy%20Critical%20Generative%20AI%20Applications%0AAuthor%3A%20Steph%20Buongiorno%20and%20Corey%20Clark%0AAbstract%3A%20%20%20External%20knowledge%20graphs%20%28KGs%29%20can%20be%20used%20to%20augment%20large%20language%20models%0A%28LLMs%29%2C%20while%20simultaneously%20providing%20an%20explainable%20knowledge%20base%20of%20facts%0Athat%20can%20be%20inspected%20by%20a%20human.%20This%20approach%20may%20be%20particularly%20valuable%20in%0Adomains%20where%20explainability%20is%20critical%2C%20like%20human%20trafficking%20data%20analysis.%0AHowever%2C%20creating%20KGs%20can%20pose%20challenges.%20KGs%20parsed%20from%20documents%20may%0Acomprise%20explicit%20connections%20%28those%20directly%20stated%20by%20a%20document%29%20but%20miss%0Aimplicit%20connections%20%28those%20obvious%20to%20a%20human%20although%20not%20directly%20stated%29.%0ATo%20address%20these%20challenges%2C%20this%20preliminary%20research%20introduces%20the%20GAME-KG%0Aframework%2C%20standing%20for%20%22Gaming%20for%20Augmenting%20Metadata%20and%20Enhancing%20Knowledge%0AGraphs.%22%20GAME-KG%20is%20a%20federated%20approach%20to%20modifying%20explicit%20as%20well%20as%0Aimplicit%20connections%20in%20KGs%20by%20using%20crowdsourced%20feedback%20collected%20through%0Avideo%20games.%20GAME-KG%20is%20shown%20through%20two%20demonstrations%3A%20a%20Unity%20test%20scenario%0Afrom%20Dark%20Shadows%2C%20a%20video%20game%20that%20collects%20feedback%20on%20KGs%20parsed%20from%20US%0ADepartment%20of%20Justice%20%28DOJ%29%20Press%20Releases%20on%20human%20trafficking%2C%20and%20a%0Afollowing%20experiment%20where%20OpenAI%27s%20GPT-4%20is%20prompted%20to%20answer%20questions%20based%0Aon%20a%20modified%20and%20unmodified%20KG.%20Initial%20results%20suggest%20that%20GAME-KG%20can%20be%20an%0Aeffective%20framework%20for%20enhancing%20KGs%2C%20while%20simultaneously%20providing%20an%0Aexplainable%20set%20of%20structured%20facts%20verified%20by%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Leveraging%20Human%20Computation%20Gaming%20to%20Enhance%20Knowledge%0A%20%20Graphs%20for%20Accuracy%20Critical%20Generative%20AI%20Applications&entry.906535625=Steph%20Buongiorno%20and%20Corey%20Clark&entry.1292438233=%20%20External%20knowledge%20graphs%20%28KGs%29%20can%20be%20used%20to%20augment%20large%20language%20models%0A%28LLMs%29%2C%20while%20simultaneously%20providing%20an%20explainable%20knowledge%20base%20of%20facts%0Athat%20can%20be%20inspected%20by%20a%20human.%20This%20approach%20may%20be%20particularly%20valuable%20in%0Adomains%20where%20explainability%20is%20critical%2C%20like%20human%20trafficking%20data%20analysis.%0AHowever%2C%20creating%20KGs%20can%20pose%20challenges.%20KGs%20parsed%20from%20documents%20may%0Acomprise%20explicit%20connections%20%28those%20directly%20stated%20by%20a%20document%29%20but%20miss%0Aimplicit%20connections%20%28those%20obvious%20to%20a%20human%20although%20not%20directly%20stated%29.%0ATo%20address%20these%20challenges%2C%20this%20preliminary%20research%20introduces%20the%20GAME-KG%0Aframework%2C%20standing%20for%20%22Gaming%20for%20Augmenting%20Metadata%20and%20Enhancing%20Knowledge%0AGraphs.%22%20GAME-KG%20is%20a%20federated%20approach%20to%20modifying%20explicit%20as%20well%20as%0Aimplicit%20connections%20in%20KGs%20by%20using%20crowdsourced%20feedback%20collected%20through%0Avideo%20games.%20GAME-KG%20is%20shown%20through%20two%20demonstrations%3A%20a%20Unity%20test%20scenario%0Afrom%20Dark%20Shadows%2C%20a%20video%20game%20that%20collects%20feedback%20on%20KGs%20parsed%20from%20US%0ADepartment%20of%20Justice%20%28DOJ%29%20Press%20Releases%20on%20human%20trafficking%2C%20and%20a%0Afollowing%20experiment%20where%20OpenAI%27s%20GPT-4%20is%20prompted%20to%20answer%20questions%20based%0Aon%20a%20modified%20and%20unmodified%20KG.%20Initial%20results%20suggest%20that%20GAME-KG%20can%20be%20an%0Aeffective%20framework%20for%20enhancing%20KGs%2C%20while%20simultaneously%20providing%20an%0Aexplainable%20set%20of%20structured%20facts%20verified%20by%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19729v1&entry.124074799=Read"},
{"title": "IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training", "author": "Che Liu and Sibo Cheng and Miaojing Shi and Anand Shah and Wenjia Bai and Rossella Arcucci", "abstract": "  In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.\n", "link": "http://arxiv.org/abs/2310.07355v2", "date": "2024-04-30", "relevancy": 2.0738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training&body=Title%3A%20IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training%0AAuthor%3A%20Che%20Liu%20and%20Sibo%20Cheng%20and%20Miaojing%20Shi%20and%20Anand%20Shah%20and%20Wenjia%20Bai%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20In%20the%20field%20of%20medical%20Vision-Language%20Pre-training%20%28VLP%29%2C%20significant%0Aefforts%20have%20been%20devoted%20to%20deriving%20text%20and%20image%20features%20from%20both%0Aclinical%20reports%20and%20associated%20medical%20images.%20However%2C%20most%20existing%20methods%0Amay%20have%20overlooked%20the%20opportunity%20in%20leveraging%20the%20inherent%20hierarchical%0Astructure%20of%20clinical%20reports%2C%20which%20are%20generally%20split%20into%20%60findings%27%20for%0Adescriptive%20content%20and%20%60impressions%27%20for%20conclusive%20observation.%20Instead%20of%0Autilizing%20this%20rich%2C%20structured%20format%2C%20current%20medical%20VLP%20approaches%20often%0Asimplify%20the%20report%20into%20either%20a%20unified%20entity%20or%20fragmented%20tokens.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20clinical%20prior%20guided%20VLP%20framework%20named%20IMITATE%20to%0Alearn%20the%20structure%20information%20from%20medical%20reports%20with%20hierarchical%0Avision-language%20alignment.%20The%20framework%20derives%20multi-level%20visual%20features%0Afrom%20the%20chest%20X-ray%20%28CXR%29%20images%20and%20separately%20aligns%20these%20features%20with%20the%0Adescriptive%20and%20the%20conclusive%20text%20encoded%20in%20the%20hierarchical%20medical%20report.%0AFurthermore%2C%20a%20new%20clinical-informed%20contrastive%20loss%20is%20introduced%20for%0Across-modal%20learning%2C%20which%20accounts%20for%20clinical%20prior%20knowledge%20in%0Aformulating%20sample%20correlations%20in%20contrastive%20learning.%20The%20proposed%20model%2C%0AIMITATE%2C%20outperforms%20baseline%20VLP%20methods%20across%20six%20different%20datasets%2C%0Aspanning%20five%20medical%20imaging%20downstream%20tasks.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20advantages%20of%20integrating%20the%20hierarchical%20structure%20of%0Amedical%20reports%20for%20vision-language%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07355v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMITATE%3A%20Clinical%20Prior%20Guided%20Hierarchical%20Vision-Language%20Pre-training&entry.906535625=Che%20Liu%20and%20Sibo%20Cheng%20and%20Miaojing%20Shi%20and%20Anand%20Shah%20and%20Wenjia%20Bai%20and%20Rossella%20Arcucci&entry.1292438233=%20%20In%20the%20field%20of%20medical%20Vision-Language%20Pre-training%20%28VLP%29%2C%20significant%0Aefforts%20have%20been%20devoted%20to%20deriving%20text%20and%20image%20features%20from%20both%0Aclinical%20reports%20and%20associated%20medical%20images.%20However%2C%20most%20existing%20methods%0Amay%20have%20overlooked%20the%20opportunity%20in%20leveraging%20the%20inherent%20hierarchical%0Astructure%20of%20clinical%20reports%2C%20which%20are%20generally%20split%20into%20%60findings%27%20for%0Adescriptive%20content%20and%20%60impressions%27%20for%20conclusive%20observation.%20Instead%20of%0Autilizing%20this%20rich%2C%20structured%20format%2C%20current%20medical%20VLP%20approaches%20often%0Asimplify%20the%20report%20into%20either%20a%20unified%20entity%20or%20fragmented%20tokens.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20clinical%20prior%20guided%20VLP%20framework%20named%20IMITATE%20to%0Alearn%20the%20structure%20information%20from%20medical%20reports%20with%20hierarchical%0Avision-language%20alignment.%20The%20framework%20derives%20multi-level%20visual%20features%0Afrom%20the%20chest%20X-ray%20%28CXR%29%20images%20and%20separately%20aligns%20these%20features%20with%20the%0Adescriptive%20and%20the%20conclusive%20text%20encoded%20in%20the%20hierarchical%20medical%20report.%0AFurthermore%2C%20a%20new%20clinical-informed%20contrastive%20loss%20is%20introduced%20for%0Across-modal%20learning%2C%20which%20accounts%20for%20clinical%20prior%20knowledge%20in%0Aformulating%20sample%20correlations%20in%20contrastive%20learning.%20The%20proposed%20model%2C%0AIMITATE%2C%20outperforms%20baseline%20VLP%20methods%20across%20six%20different%20datasets%2C%0Aspanning%20five%20medical%20imaging%20downstream%20tasks.%20Comprehensive%20experimental%0Aresults%20highlight%20the%20advantages%20of%20integrating%20the%20hierarchical%20structure%20of%0Amedical%20reports%20for%20vision-language%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07355v2&entry.124074799=Read"},
{"title": "Causal Perception Inspired Representation Learning for Trustworthy Image\n  Quality Assessment", "author": "Lei Wang and Desen Yuan", "abstract": "  Despite great success in modeling visual perception, deep neural network\nbased image quality assessment (IQA) still remains unreliable in real-world\napplications due to its vulnerability to adversarial perturbations and the\ninexplicit black-box structure. In this paper, we propose to build a\ntrustworthy IQA model via Causal Perception inspired Representation Learning\n(CPRL), and a score reflection attack method for IQA model. More specifically,\nwe assume that each image is composed of Causal Perception Representation (CPR)\nand non-causal perception representation (N-CPR). CPR serves as the causation\nof the subjective quality label, which is invariant to the imperceptible\nadversarial perturbations. Inversely, N-CPR presents spurious associations with\nthe subjective quality label, which may significantly change with the\nadversarial perturbations. To extract the CPR from each input image, we develop\na soft ranking based channel-wise activation function to mediate the causally\nsufficient (beneficial for high prediction accuracy) and necessary (beneficial\nfor high robustness) deep features, and based on intervention employ minimax\ngame to optimize. Experiments on four benchmark databases show that the\nproposed CPRL method outperforms many state-of-the-art adversarial defense\nmethods and provides explicit model interpretation.\n", "link": "http://arxiv.org/abs/2404.19567v1", "date": "2024-04-30", "relevancy": 2.0734, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5248}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5165}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5126}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Causal%20Perception%20Inspired%20Representation%20Learning%20for%20Trustworthy%20Image%0A%20%20Quality%20Assessment&body=Title%3A%20Causal%20Perception%20Inspired%20Representation%20Learning%20for%20Trustworthy%20Image%0A%20%20Quality%20Assessment%0AAuthor%3A%20Lei%20Wang%20and%20Desen%20Yuan%0AAbstract%3A%20%20%20Despite%20great%20success%20in%20modeling%20visual%20perception%2C%20deep%20neural%20network%0Abased%20image%20quality%20assessment%20%28IQA%29%20still%20remains%20unreliable%20in%20real-world%0Aapplications%20due%20to%20its%20vulnerability%20to%20adversarial%20perturbations%20and%20the%0Ainexplicit%20black-box%20structure.%20In%20this%20paper%2C%20we%20propose%20to%20build%20a%0Atrustworthy%20IQA%20model%20via%20Causal%20Perception%20inspired%20Representation%20Learning%0A%28CPRL%29%2C%20and%20a%20score%20reflection%20attack%20method%20for%20IQA%20model.%20More%20specifically%2C%0Awe%20assume%20that%20each%20image%20is%20composed%20of%20Causal%20Perception%20Representation%20%28CPR%29%0Aand%20non-causal%20perception%20representation%20%28N-CPR%29.%20CPR%20serves%20as%20the%20causation%0Aof%20the%20subjective%20quality%20label%2C%20which%20is%20invariant%20to%20the%20imperceptible%0Aadversarial%20perturbations.%20Inversely%2C%20N-CPR%20presents%20spurious%20associations%20with%0Athe%20subjective%20quality%20label%2C%20which%20may%20significantly%20change%20with%20the%0Aadversarial%20perturbations.%20To%20extract%20the%20CPR%20from%20each%20input%20image%2C%20we%20develop%0Aa%20soft%20ranking%20based%20channel-wise%20activation%20function%20to%20mediate%20the%20causally%0Asufficient%20%28beneficial%20for%20high%20prediction%20accuracy%29%20and%20necessary%20%28beneficial%0Afor%20high%20robustness%29%20deep%20features%2C%20and%20based%20on%20intervention%20employ%20minimax%0Agame%20to%20optimize.%20Experiments%20on%20four%20benchmark%20databases%20show%20that%20the%0Aproposed%20CPRL%20method%20outperforms%20many%20state-of-the-art%20adversarial%20defense%0Amethods%20and%20provides%20explicit%20model%20interpretation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19567v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Perception%20Inspired%20Representation%20Learning%20for%20Trustworthy%20Image%0A%20%20Quality%20Assessment&entry.906535625=Lei%20Wang%20and%20Desen%20Yuan&entry.1292438233=%20%20Despite%20great%20success%20in%20modeling%20visual%20perception%2C%20deep%20neural%20network%0Abased%20image%20quality%20assessment%20%28IQA%29%20still%20remains%20unreliable%20in%20real-world%0Aapplications%20due%20to%20its%20vulnerability%20to%20adversarial%20perturbations%20and%20the%0Ainexplicit%20black-box%20structure.%20In%20this%20paper%2C%20we%20propose%20to%20build%20a%0Atrustworthy%20IQA%20model%20via%20Causal%20Perception%20inspired%20Representation%20Learning%0A%28CPRL%29%2C%20and%20a%20score%20reflection%20attack%20method%20for%20IQA%20model.%20More%20specifically%2C%0Awe%20assume%20that%20each%20image%20is%20composed%20of%20Causal%20Perception%20Representation%20%28CPR%29%0Aand%20non-causal%20perception%20representation%20%28N-CPR%29.%20CPR%20serves%20as%20the%20causation%0Aof%20the%20subjective%20quality%20label%2C%20which%20is%20invariant%20to%20the%20imperceptible%0Aadversarial%20perturbations.%20Inversely%2C%20N-CPR%20presents%20spurious%20associations%20with%0Athe%20subjective%20quality%20label%2C%20which%20may%20significantly%20change%20with%20the%0Aadversarial%20perturbations.%20To%20extract%20the%20CPR%20from%20each%20input%20image%2C%20we%20develop%0Aa%20soft%20ranking%20based%20channel-wise%20activation%20function%20to%20mediate%20the%20causally%0Asufficient%20%28beneficial%20for%20high%20prediction%20accuracy%29%20and%20necessary%20%28beneficial%0Afor%20high%20robustness%29%20deep%20features%2C%20and%20based%20on%20intervention%20employ%20minimax%0Agame%20to%20optimize.%20Experiments%20on%20four%20benchmark%20databases%20show%20that%20the%0Aproposed%20CPRL%20method%20outperforms%20many%20state-of-the-art%20adversarial%20defense%0Amethods%20and%20provides%20explicit%20model%20interpretation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19567v1&entry.124074799=Read"},
{"title": "Improving Dictionary Learning with Gated Sparse Autoencoders", "author": "Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and J\u00e1nos Kram\u00e1r and Rohin Shah and Neel Nanda", "abstract": "  Recent work has found that sparse autoencoders (SAEs) are an effective\ntechnique for unsupervised discovery of interpretable features in language\nmodels' (LMs) activations, by finding sparse, linear reconstructions of LM\nactivations. We introduce the Gated Sparse Autoencoder (Gated SAE), which\nachieves a Pareto improvement over training with prevailing methods. In SAEs,\nthe L1 penalty used to encourage sparsity introduces many undesirable biases,\nsuch as shrinkage -- systematic underestimation of feature activations. The key\ninsight of Gated SAEs is to separate the functionality of (a) determining which\ndirections to use and (b) estimating the magnitudes of those directions: this\nenables us to apply the L1 penalty only to the former, limiting the scope of\nundesirable side effects. Through training SAEs on LMs of up to 7B parameters\nwe find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage,\nare similarly interpretable, and require half as many firing features to\nachieve comparable reconstruction fidelity.\n", "link": "http://arxiv.org/abs/2404.16014v2", "date": "2024-04-30", "relevancy": 2.0631, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5213}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Dictionary%20Learning%20with%20Gated%20Sparse%20Autoencoders&body=Title%3A%20Improving%20Dictionary%20Learning%20with%20Gated%20Sparse%20Autoencoders%0AAuthor%3A%20Senthooran%20Rajamanoharan%20and%20Arthur%20Conmy%20and%20Lewis%20Smith%20and%20Tom%20Lieberum%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Rohin%20Shah%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Recent%20work%20has%20found%20that%20sparse%20autoencoders%20%28SAEs%29%20are%20an%20effective%0Atechnique%20for%20unsupervised%20discovery%20of%20interpretable%20features%20in%20language%0Amodels%27%20%28LMs%29%20activations%2C%20by%20finding%20sparse%2C%20linear%20reconstructions%20of%20LM%0Aactivations.%20We%20introduce%20the%20Gated%20Sparse%20Autoencoder%20%28Gated%20SAE%29%2C%20which%0Aachieves%20a%20Pareto%20improvement%20over%20training%20with%20prevailing%20methods.%20In%20SAEs%2C%0Athe%20L1%20penalty%20used%20to%20encourage%20sparsity%20introduces%20many%20undesirable%20biases%2C%0Asuch%20as%20shrinkage%20--%20systematic%20underestimation%20of%20feature%20activations.%20The%20key%0Ainsight%20of%20Gated%20SAEs%20is%20to%20separate%20the%20functionality%20of%20%28a%29%20determining%20which%0Adirections%20to%20use%20and%20%28b%29%20estimating%20the%20magnitudes%20of%20those%20directions%3A%20this%0Aenables%20us%20to%20apply%20the%20L1%20penalty%20only%20to%20the%20former%2C%20limiting%20the%20scope%20of%0Aundesirable%20side%20effects.%20Through%20training%20SAEs%20on%20LMs%20of%20up%20to%207B%20parameters%0Awe%20find%20that%2C%20in%20typical%20hyper-parameter%20ranges%2C%20Gated%20SAEs%20solve%20shrinkage%2C%0Aare%20similarly%20interpretable%2C%20and%20require%20half%20as%20many%20firing%20features%20to%0Aachieve%20comparable%20reconstruction%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16014v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Dictionary%20Learning%20with%20Gated%20Sparse%20Autoencoders&entry.906535625=Senthooran%20Rajamanoharan%20and%20Arthur%20Conmy%20and%20Lewis%20Smith%20and%20Tom%20Lieberum%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Rohin%20Shah%20and%20Neel%20Nanda&entry.1292438233=%20%20Recent%20work%20has%20found%20that%20sparse%20autoencoders%20%28SAEs%29%20are%20an%20effective%0Atechnique%20for%20unsupervised%20discovery%20of%20interpretable%20features%20in%20language%0Amodels%27%20%28LMs%29%20activations%2C%20by%20finding%20sparse%2C%20linear%20reconstructions%20of%20LM%0Aactivations.%20We%20introduce%20the%20Gated%20Sparse%20Autoencoder%20%28Gated%20SAE%29%2C%20which%0Aachieves%20a%20Pareto%20improvement%20over%20training%20with%20prevailing%20methods.%20In%20SAEs%2C%0Athe%20L1%20penalty%20used%20to%20encourage%20sparsity%20introduces%20many%20undesirable%20biases%2C%0Asuch%20as%20shrinkage%20--%20systematic%20underestimation%20of%20feature%20activations.%20The%20key%0Ainsight%20of%20Gated%20SAEs%20is%20to%20separate%20the%20functionality%20of%20%28a%29%20determining%20which%0Adirections%20to%20use%20and%20%28b%29%20estimating%20the%20magnitudes%20of%20those%20directions%3A%20this%0Aenables%20us%20to%20apply%20the%20L1%20penalty%20only%20to%20the%20former%2C%20limiting%20the%20scope%20of%0Aundesirable%20side%20effects.%20Through%20training%20SAEs%20on%20LMs%20of%20up%20to%207B%20parameters%0Awe%20find%20that%2C%20in%20typical%20hyper-parameter%20ranges%2C%20Gated%20SAEs%20solve%20shrinkage%2C%0Aare%20similarly%20interpretable%2C%20and%20require%20half%20as%20many%20firing%20features%20to%0Aachieve%20comparable%20reconstruction%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16014v2&entry.124074799=Read"},
{"title": "Lancet: Accelerating Mixture-of-Experts Training via Whole Graph\n  Computation-Communication Overlapping", "author": "Chenyu Jiang and Ye Tian and Zhen Jia and Shuai Zheng and Chuan Wu and Yida Wang", "abstract": "  The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the\nsize of DNN model parameters. However, it faces the challenge of extended\nall-to-all communication latency during the training process. Existing methods\nattempt to mitigate this issue by overlapping all-to-all with expert\ncomputation. Yet, these methods frequently fall short of achieving sufficient\noverlap, consequently restricting the potential for performance enhancements.\nIn our study, we extend the scope of this challenge by considering overlap at\nthe broader training graph level. During the forward pass, we enable non-MoE\ncomputations to overlap with all-to-all through careful partitioning and\npipelining. In the backward pass, we achieve overlap with all-to-all by\nscheduling gradient weight computations. We implement these techniques in\nLancet, a system using compiler-based optimization to automatically enhance MoE\nmodel training. Our extensive evaluation reveals that Lancet significantly\nreduces the time devoted to non-overlapping communication, by as much as 77%.\nMoreover, it achieves a notable end-to-end speedup of up to 1.3 times when\ncompared to the state-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2404.19429v1", "date": "2024-04-30", "relevancy": 2.0581, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.516}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5158}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lancet%3A%20Accelerating%20Mixture-of-Experts%20Training%20via%20Whole%20Graph%0A%20%20Computation-Communication%20Overlapping&body=Title%3A%20Lancet%3A%20Accelerating%20Mixture-of-Experts%20Training%20via%20Whole%20Graph%0A%20%20Computation-Communication%20Overlapping%0AAuthor%3A%20Chenyu%20Jiang%20and%20Ye%20Tian%20and%20Zhen%20Jia%20and%20Shuai%20Zheng%20and%20Chuan%20Wu%20and%20Yida%20Wang%0AAbstract%3A%20%20%20The%20Mixture-of-Expert%20%28MoE%29%20technique%20plays%20a%20crucial%20role%20in%20expanding%20the%0Asize%20of%20DNN%20model%20parameters.%20However%2C%20it%20faces%20the%20challenge%20of%20extended%0Aall-to-all%20communication%20latency%20during%20the%20training%20process.%20Existing%20methods%0Aattempt%20to%20mitigate%20this%20issue%20by%20overlapping%20all-to-all%20with%20expert%0Acomputation.%20Yet%2C%20these%20methods%20frequently%20fall%20short%20of%20achieving%20sufficient%0Aoverlap%2C%20consequently%20restricting%20the%20potential%20for%20performance%20enhancements.%0AIn%20our%20study%2C%20we%20extend%20the%20scope%20of%20this%20challenge%20by%20considering%20overlap%20at%0Athe%20broader%20training%20graph%20level.%20During%20the%20forward%20pass%2C%20we%20enable%20non-MoE%0Acomputations%20to%20overlap%20with%20all-to-all%20through%20careful%20partitioning%20and%0Apipelining.%20In%20the%20backward%20pass%2C%20we%20achieve%20overlap%20with%20all-to-all%20by%0Ascheduling%20gradient%20weight%20computations.%20We%20implement%20these%20techniques%20in%0ALancet%2C%20a%20system%20using%20compiler-based%20optimization%20to%20automatically%20enhance%20MoE%0Amodel%20training.%20Our%20extensive%20evaluation%20reveals%20that%20Lancet%20significantly%0Areduces%20the%20time%20devoted%20to%20non-overlapping%20communication%2C%20by%20as%20much%20as%2077%25.%0AMoreover%2C%20it%20achieves%20a%20notable%20end-to-end%20speedup%20of%20up%20to%201.3%20times%20when%0Acompared%20to%20the%20state-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19429v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lancet%3A%20Accelerating%20Mixture-of-Experts%20Training%20via%20Whole%20Graph%0A%20%20Computation-Communication%20Overlapping&entry.906535625=Chenyu%20Jiang%20and%20Ye%20Tian%20and%20Zhen%20Jia%20and%20Shuai%20Zheng%20and%20Chuan%20Wu%20and%20Yida%20Wang&entry.1292438233=%20%20The%20Mixture-of-Expert%20%28MoE%29%20technique%20plays%20a%20crucial%20role%20in%20expanding%20the%0Asize%20of%20DNN%20model%20parameters.%20However%2C%20it%20faces%20the%20challenge%20of%20extended%0Aall-to-all%20communication%20latency%20during%20the%20training%20process.%20Existing%20methods%0Aattempt%20to%20mitigate%20this%20issue%20by%20overlapping%20all-to-all%20with%20expert%0Acomputation.%20Yet%2C%20these%20methods%20frequently%20fall%20short%20of%20achieving%20sufficient%0Aoverlap%2C%20consequently%20restricting%20the%20potential%20for%20performance%20enhancements.%0AIn%20our%20study%2C%20we%20extend%20the%20scope%20of%20this%20challenge%20by%20considering%20overlap%20at%0Athe%20broader%20training%20graph%20level.%20During%20the%20forward%20pass%2C%20we%20enable%20non-MoE%0Acomputations%20to%20overlap%20with%20all-to-all%20through%20careful%20partitioning%20and%0Apipelining.%20In%20the%20backward%20pass%2C%20we%20achieve%20overlap%20with%20all-to-all%20by%0Ascheduling%20gradient%20weight%20computations.%20We%20implement%20these%20techniques%20in%0ALancet%2C%20a%20system%20using%20compiler-based%20optimization%20to%20automatically%20enhance%20MoE%0Amodel%20training.%20Our%20extensive%20evaluation%20reveals%20that%20Lancet%20significantly%0Areduces%20the%20time%20devoted%20to%20non-overlapping%20communication%2C%20by%20as%20much%20as%2077%25.%0AMoreover%2C%20it%20achieves%20a%20notable%20end-to-end%20speedup%20of%20up%20to%201.3%20times%20when%0Acompared%20to%20the%20state-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19429v1&entry.124074799=Read"},
{"title": "DOCCI: Descriptions of Connected and Contrasting Images", "author": "Yasumasa Onoe and Sunayana Rane and Zachary Berger and Yonatan Bitton and Jaemin Cho and Roopal Garg and Alexander Ku and Zarana Parekh and Jordi Pont-Tuset and Garrett Tanzer and Su Wang and Jason Baldridge", "abstract": "  Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.\n", "link": "http://arxiv.org/abs/2404.19753v1", "date": "2024-04-30", "relevancy": 2.0501, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5151}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DOCCI%3A%20Descriptions%20of%20Connected%20and%20Contrasting%20Images&body=Title%3A%20DOCCI%3A%20Descriptions%20of%20Connected%20and%20Contrasting%20Images%0AAuthor%3A%20Yasumasa%20Onoe%20and%20Sunayana%20Rane%20and%20Zachary%20Berger%20and%20Yonatan%20Bitton%20and%20Jaemin%20Cho%20and%20Roopal%20Garg%20and%20Alexander%20Ku%20and%20Zarana%20Parekh%20and%20Jordi%20Pont-Tuset%20and%20Garrett%20Tanzer%20and%20Su%20Wang%20and%20Jason%20Baldridge%0AAbstract%3A%20%20%20Vision-language%20datasets%20are%20vital%20for%20both%20text-to-image%20%28T2I%29%20and%0Aimage-to-text%20%28I2T%29%20research.%20However%2C%20current%20datasets%20lack%20descriptions%20with%0Afine-grained%20detail%20that%20would%20allow%20for%20richer%20associations%20to%20be%20learned%20by%0Amodels.%20To%20fill%20the%20gap%2C%20we%20introduce%20Descriptions%20of%20Connected%20and%20Contrasting%0AImages%20%28DOCCI%29%2C%20a%20dataset%20with%20long%2C%20human-annotated%20English%20descriptions%20for%0A15k%20images%20that%20were%20taken%2C%20curated%20and%20donated%20by%20a%20single%20researcher%20intent%0Aon%20capturing%20key%20challenges%20such%20as%20spatial%20relations%2C%20counting%2C%20text%0Arendering%2C%20world%20knowledge%2C%20and%20more.%20We%20instruct%20human%20annotators%20to%20create%0Acomprehensive%20descriptions%20for%20each%20image%3B%20these%20average%20136%20words%20in%20length%0Aand%20are%20crafted%20to%20clearly%20distinguish%20each%20image%20from%20those%20that%20are%20related%0Aor%20similar.%20Each%20description%20is%20highly%20compositional%20and%20typically%20encompasses%0Amultiple%20challenges.%20Through%20both%20quantitative%20and%20qualitative%20analyses%2C%20we%0Ademonstrate%20that%20DOCCI%20serves%20as%20an%20effective%20training%20resource%20for%0Aimage-to-text%20generation%20--%20a%20PaLI%205B%20model%20finetuned%20on%20DOCCI%20shows%20equal%20or%0Asuperior%20results%20compared%20to%20highly-performant%20larger%20models%20like%20LLaVA-1.5%207B%0Aand%20InstructBLIP%207B.%20Furthermore%2C%20we%20show%20that%20DOCCI%20is%20a%20useful%20testbed%20for%0Atext-to-image%20generation%2C%20highlighting%20the%20limitations%20of%20current%20text-to-image%0Amodels%20in%20capturing%20long%20descriptions%20and%20fine%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19753v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOCCI%3A%20Descriptions%20of%20Connected%20and%20Contrasting%20Images&entry.906535625=Yasumasa%20Onoe%20and%20Sunayana%20Rane%20and%20Zachary%20Berger%20and%20Yonatan%20Bitton%20and%20Jaemin%20Cho%20and%20Roopal%20Garg%20and%20Alexander%20Ku%20and%20Zarana%20Parekh%20and%20Jordi%20Pont-Tuset%20and%20Garrett%20Tanzer%20and%20Su%20Wang%20and%20Jason%20Baldridge&entry.1292438233=%20%20Vision-language%20datasets%20are%20vital%20for%20both%20text-to-image%20%28T2I%29%20and%0Aimage-to-text%20%28I2T%29%20research.%20However%2C%20current%20datasets%20lack%20descriptions%20with%0Afine-grained%20detail%20that%20would%20allow%20for%20richer%20associations%20to%20be%20learned%20by%0Amodels.%20To%20fill%20the%20gap%2C%20we%20introduce%20Descriptions%20of%20Connected%20and%20Contrasting%0AImages%20%28DOCCI%29%2C%20a%20dataset%20with%20long%2C%20human-annotated%20English%20descriptions%20for%0A15k%20images%20that%20were%20taken%2C%20curated%20and%20donated%20by%20a%20single%20researcher%20intent%0Aon%20capturing%20key%20challenges%20such%20as%20spatial%20relations%2C%20counting%2C%20text%0Arendering%2C%20world%20knowledge%2C%20and%20more.%20We%20instruct%20human%20annotators%20to%20create%0Acomprehensive%20descriptions%20for%20each%20image%3B%20these%20average%20136%20words%20in%20length%0Aand%20are%20crafted%20to%20clearly%20distinguish%20each%20image%20from%20those%20that%20are%20related%0Aor%20similar.%20Each%20description%20is%20highly%20compositional%20and%20typically%20encompasses%0Amultiple%20challenges.%20Through%20both%20quantitative%20and%20qualitative%20analyses%2C%20we%0Ademonstrate%20that%20DOCCI%20serves%20as%20an%20effective%20training%20resource%20for%0Aimage-to-text%20generation%20--%20a%20PaLI%205B%20model%20finetuned%20on%20DOCCI%20shows%20equal%20or%0Asuperior%20results%20compared%20to%20highly-performant%20larger%20models%20like%20LLaVA-1.5%207B%0Aand%20InstructBLIP%207B.%20Furthermore%2C%20we%20show%20that%20DOCCI%20is%20a%20useful%20testbed%20for%0Atext-to-image%20generation%2C%20highlighting%20the%20limitations%20of%20current%20text-to-image%0Amodels%20in%20capturing%20long%20descriptions%20and%20fine%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19753v1&entry.124074799=Read"},
{"title": "CLEAR: Cross-Transformers with Pre-trained Language Model is All you\n  need for Person Attribute Recognition and Retrieval", "author": "Doanh C. Bui and Thinh V. Le and Ba Hung Ngo and Tae Jong Choi", "abstract": "  Person attribute recognition and attribute-based retrieval are two core\nhuman-centric tasks. In the recognition task, the challenge is specifying\nattributes depending on a person's appearance, while the retrieval task\ninvolves searching for matching persons based on attribute queries. There is a\nsignificant relationship between recognition and retrieval tasks. In this\nstudy, we demonstrate that if there is a sufficiently robust network to solve\nperson attribute recognition, it can be adapted to facilitate better\nperformance for the retrieval task. Another issue that needs addressing in the\nretrieval task is the modality gap between attribute queries and persons'\nimages. Therefore, in this paper, we present CLEAR, a unified network designed\nto address both tasks. We introduce a robust cross-transformers network to\nhandle person attribute recognition. Additionally, leveraging a pre-trained\nlanguage model, we construct pseudo-descriptions for attribute queries and\nintroduce an effective training strategy to train only a few additional\nparameters for adapters, facilitating the handling of the retrieval task.\nFinally, the unified CLEAR model is evaluated on five benchmarks: PETA, PA100K,\nMarket-1501, RAPv2, and UPAR-2024. Without bells and whistles, CLEAR achieves\nstate-of-the-art performance or competitive results for both tasks,\nsignificantly outperforming other competitors in terms of person retrieval\nperformance on the widely-used Market-1501 dataset.\n", "link": "http://arxiv.org/abs/2403.06119v2", "date": "2024-04-30", "relevancy": 2.0495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.486}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4814}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLEAR%3A%20Cross-Transformers%20with%20Pre-trained%20Language%20Model%20is%20All%20you%0A%20%20need%20for%20Person%20Attribute%20Recognition%20and%20Retrieval&body=Title%3A%20CLEAR%3A%20Cross-Transformers%20with%20Pre-trained%20Language%20Model%20is%20All%20you%0A%20%20need%20for%20Person%20Attribute%20Recognition%20and%20Retrieval%0AAuthor%3A%20Doanh%20C.%20Bui%20and%20Thinh%20V.%20Le%20and%20Ba%20Hung%20Ngo%20and%20Tae%20Jong%20Choi%0AAbstract%3A%20%20%20Person%20attribute%20recognition%20and%20attribute-based%20retrieval%20are%20two%20core%0Ahuman-centric%20tasks.%20In%20the%20recognition%20task%2C%20the%20challenge%20is%20specifying%0Aattributes%20depending%20on%20a%20person%27s%20appearance%2C%20while%20the%20retrieval%20task%0Ainvolves%20searching%20for%20matching%20persons%20based%20on%20attribute%20queries.%20There%20is%20a%0Asignificant%20relationship%20between%20recognition%20and%20retrieval%20tasks.%20In%20this%0Astudy%2C%20we%20demonstrate%20that%20if%20there%20is%20a%20sufficiently%20robust%20network%20to%20solve%0Aperson%20attribute%20recognition%2C%20it%20can%20be%20adapted%20to%20facilitate%20better%0Aperformance%20for%20the%20retrieval%20task.%20Another%20issue%20that%20needs%20addressing%20in%20the%0Aretrieval%20task%20is%20the%20modality%20gap%20between%20attribute%20queries%20and%20persons%27%0Aimages.%20Therefore%2C%20in%20this%20paper%2C%20we%20present%20CLEAR%2C%20a%20unified%20network%20designed%0Ato%20address%20both%20tasks.%20We%20introduce%20a%20robust%20cross-transformers%20network%20to%0Ahandle%20person%20attribute%20recognition.%20Additionally%2C%20leveraging%20a%20pre-trained%0Alanguage%20model%2C%20we%20construct%20pseudo-descriptions%20for%20attribute%20queries%20and%0Aintroduce%20an%20effective%20training%20strategy%20to%20train%20only%20a%20few%20additional%0Aparameters%20for%20adapters%2C%20facilitating%20the%20handling%20of%20the%20retrieval%20task.%0AFinally%2C%20the%20unified%20CLEAR%20model%20is%20evaluated%20on%20five%20benchmarks%3A%20PETA%2C%20PA100K%2C%0AMarket-1501%2C%20RAPv2%2C%20and%20UPAR-2024.%20Without%20bells%20and%20whistles%2C%20CLEAR%20achieves%0Astate-of-the-art%20performance%20or%20competitive%20results%20for%20both%20tasks%2C%0Asignificantly%20outperforming%20other%20competitors%20in%20terms%20of%20person%20retrieval%0Aperformance%20on%20the%20widely-used%20Market-1501%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06119v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEAR%3A%20Cross-Transformers%20with%20Pre-trained%20Language%20Model%20is%20All%20you%0A%20%20need%20for%20Person%20Attribute%20Recognition%20and%20Retrieval&entry.906535625=Doanh%20C.%20Bui%20and%20Thinh%20V.%20Le%20and%20Ba%20Hung%20Ngo%20and%20Tae%20Jong%20Choi&entry.1292438233=%20%20Person%20attribute%20recognition%20and%20attribute-based%20retrieval%20are%20two%20core%0Ahuman-centric%20tasks.%20In%20the%20recognition%20task%2C%20the%20challenge%20is%20specifying%0Aattributes%20depending%20on%20a%20person%27s%20appearance%2C%20while%20the%20retrieval%20task%0Ainvolves%20searching%20for%20matching%20persons%20based%20on%20attribute%20queries.%20There%20is%20a%0Asignificant%20relationship%20between%20recognition%20and%20retrieval%20tasks.%20In%20this%0Astudy%2C%20we%20demonstrate%20that%20if%20there%20is%20a%20sufficiently%20robust%20network%20to%20solve%0Aperson%20attribute%20recognition%2C%20it%20can%20be%20adapted%20to%20facilitate%20better%0Aperformance%20for%20the%20retrieval%20task.%20Another%20issue%20that%20needs%20addressing%20in%20the%0Aretrieval%20task%20is%20the%20modality%20gap%20between%20attribute%20queries%20and%20persons%27%0Aimages.%20Therefore%2C%20in%20this%20paper%2C%20we%20present%20CLEAR%2C%20a%20unified%20network%20designed%0Ato%20address%20both%20tasks.%20We%20introduce%20a%20robust%20cross-transformers%20network%20to%0Ahandle%20person%20attribute%20recognition.%20Additionally%2C%20leveraging%20a%20pre-trained%0Alanguage%20model%2C%20we%20construct%20pseudo-descriptions%20for%20attribute%20queries%20and%0Aintroduce%20an%20effective%20training%20strategy%20to%20train%20only%20a%20few%20additional%0Aparameters%20for%20adapters%2C%20facilitating%20the%20handling%20of%20the%20retrieval%20task.%0AFinally%2C%20the%20unified%20CLEAR%20model%20is%20evaluated%20on%20five%20benchmarks%3A%20PETA%2C%20PA100K%2C%0AMarket-1501%2C%20RAPv2%2C%20and%20UPAR-2024.%20Without%20bells%20and%20whistles%2C%20CLEAR%20achieves%0Astate-of-the-art%20performance%20or%20competitive%20results%20for%20both%20tasks%2C%0Asignificantly%20outperforming%20other%20competitors%20in%20terms%20of%20person%20retrieval%0Aperformance%20on%20the%20widely-used%20Market-1501%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06119v2&entry.124074799=Read"},
{"title": "Instance-dependent Noisy-label Learning with Graphical Model Based\n  Noise-rate Estimation", "author": "Arpit Garg and Cuong Nguyen and Rafael Felix and Thanh-Toan Do and Gustavo Carneiro", "abstract": "  Deep learning faces a formidable challenge when handling noisy labels, as\nmodels tend to overfit samples affected by label noise. This challenge is\nfurther compounded by the presence of instance-dependent noise (IDN), a\nrealistic form of label noise arising from ambiguous sample information. To\naddress IDN, Label Noise Learning (LNL) incorporates a sample selection stage\nto differentiate clean and noisy-label samples. This stage uses an arbitrary\ncriterion and a pre-defined curriculum that initially selects most samples as\nnoisy and gradually decreases this selection rate during training. Such\ncurriculum is sub-optimal since it does not consider the actual label noise\nrate in the training set. This paper addresses this issue with a new noise-rate\nestimation method that is easily integrated with most state-of-the-art (SOTA)\nLNL methods to produce a more effective curriculum. Synthetic and real-world\nbenchmark results demonstrate that integrating our approach with SOTA LNL\nmethods improves accuracy in most cases.\n", "link": "http://arxiv.org/abs/2305.19486v2", "date": "2024-04-30", "relevancy": 2.0398, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5217}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4992}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Instance-dependent%20Noisy-label%20Learning%20with%20Graphical%20Model%20Based%0A%20%20Noise-rate%20Estimation&body=Title%3A%20Instance-dependent%20Noisy-label%20Learning%20with%20Graphical%20Model%20Based%0A%20%20Noise-rate%20Estimation%0AAuthor%3A%20Arpit%20Garg%20and%20Cuong%20Nguyen%20and%20Rafael%20Felix%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20Deep%20learning%20faces%20a%20formidable%20challenge%20when%20handling%20noisy%20labels%2C%20as%0Amodels%20tend%20to%20overfit%20samples%20affected%20by%20label%20noise.%20This%20challenge%20is%0Afurther%20compounded%20by%20the%20presence%20of%20instance-dependent%20noise%20%28IDN%29%2C%20a%0Arealistic%20form%20of%20label%20noise%20arising%20from%20ambiguous%20sample%20information.%20To%0Aaddress%20IDN%2C%20Label%20Noise%20Learning%20%28LNL%29%20incorporates%20a%20sample%20selection%20stage%0Ato%20differentiate%20clean%20and%20noisy-label%20samples.%20This%20stage%20uses%20an%20arbitrary%0Acriterion%20and%20a%20pre-defined%20curriculum%20that%20initially%20selects%20most%20samples%20as%0Anoisy%20and%20gradually%20decreases%20this%20selection%20rate%20during%20training.%20Such%0Acurriculum%20is%20sub-optimal%20since%20it%20does%20not%20consider%20the%20actual%20label%20noise%0Arate%20in%20the%20training%20set.%20This%20paper%20addresses%20this%20issue%20with%20a%20new%20noise-rate%0Aestimation%20method%20that%20is%20easily%20integrated%20with%20most%20state-of-the-art%20%28SOTA%29%0ALNL%20methods%20to%20produce%20a%20more%20effective%20curriculum.%20Synthetic%20and%20real-world%0Abenchmark%20results%20demonstrate%20that%20integrating%20our%20approach%20with%20SOTA%20LNL%0Amethods%20improves%20accuracy%20in%20most%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19486v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-dependent%20Noisy-label%20Learning%20with%20Graphical%20Model%20Based%0A%20%20Noise-rate%20Estimation&entry.906535625=Arpit%20Garg%20and%20Cuong%20Nguyen%20and%20Rafael%20Felix%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Deep%20learning%20faces%20a%20formidable%20challenge%20when%20handling%20noisy%20labels%2C%20as%0Amodels%20tend%20to%20overfit%20samples%20affected%20by%20label%20noise.%20This%20challenge%20is%0Afurther%20compounded%20by%20the%20presence%20of%20instance-dependent%20noise%20%28IDN%29%2C%20a%0Arealistic%20form%20of%20label%20noise%20arising%20from%20ambiguous%20sample%20information.%20To%0Aaddress%20IDN%2C%20Label%20Noise%20Learning%20%28LNL%29%20incorporates%20a%20sample%20selection%20stage%0Ato%20differentiate%20clean%20and%20noisy-label%20samples.%20This%20stage%20uses%20an%20arbitrary%0Acriterion%20and%20a%20pre-defined%20curriculum%20that%20initially%20selects%20most%20samples%20as%0Anoisy%20and%20gradually%20decreases%20this%20selection%20rate%20during%20training.%20Such%0Acurriculum%20is%20sub-optimal%20since%20it%20does%20not%20consider%20the%20actual%20label%20noise%0Arate%20in%20the%20training%20set.%20This%20paper%20addresses%20this%20issue%20with%20a%20new%20noise-rate%0Aestimation%20method%20that%20is%20easily%20integrated%20with%20most%20state-of-the-art%20%28SOTA%29%0ALNL%20methods%20to%20produce%20a%20more%20effective%20curriculum.%20Synthetic%20and%20real-world%0Abenchmark%20results%20demonstrate%20that%20integrating%20our%20approach%20with%20SOTA%20LNL%0Amethods%20improves%20accuracy%20in%20most%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19486v2&entry.124074799=Read"},
{"title": "PANGeA: Procedural Artificial Narrative using Generative AI for\n  Turn-Based Video Games", "author": "Steph Buongiorno and Lawrence Jake Klinkert and Tanishq Chawla and Zixin Zhuang and Corey Clark", "abstract": "  This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.\n", "link": "http://arxiv.org/abs/2404.19721v1", "date": "2024-04-30", "relevancy": 2.0255, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5486}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5043}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PANGeA%3A%20Procedural%20Artificial%20Narrative%20using%20Generative%20AI%20for%0A%20%20Turn-Based%20Video%20Games&body=Title%3A%20PANGeA%3A%20Procedural%20Artificial%20Narrative%20using%20Generative%20AI%20for%0A%20%20Turn-Based%20Video%20Games%0AAuthor%3A%20Steph%20Buongiorno%20and%20Lawrence%20Jake%20Klinkert%20and%20Tanishq%20Chawla%20and%20Zixin%20Zhuang%20and%20Corey%20Clark%0AAbstract%3A%20%20%20This%20research%20introduces%20Procedural%20Artificial%20Narrative%20using%20Generative%20AI%0A%28PANGeA%29%2C%20a%20structured%20approach%20for%20leveraging%20large%20language%20models%20%28LLMs%29%2C%0Aguided%20by%20a%20game%20designer%27s%20high-level%20criteria%2C%20to%20generate%20narrative%20content%0Afor%20turn-based%20role-playing%20video%20games%20%28RPGs%29.%20Distinct%20from%20prior%0Aapplications%20of%20LLMs%20used%20for%20video%20game%20design%2C%20PANGeA%20innovates%20by%20not%20only%0Agenerating%20game%20level%20data%20%28which%20includes%2C%20but%20is%20not%20limited%20to%2C%20setting%2C%20key%0Aitems%2C%20and%20non-playable%20characters%20%28NPCs%29%29%2C%20but%20by%20also%20fostering%20dynamic%2C%0Afree-form%20interactions%20between%20the%20player%20and%20the%20environment%20that%20align%20with%0Athe%20procedural%20game%20narrative.%20The%20NPCs%20generated%20by%20PANGeA%20are%0Apersonality-biased%20and%20express%20traits%20from%20the%20Big%205%20Personality%20Model%20in%20their%0Agenerated%20responses.%20PANGeA%20addresses%20challenges%20behind%20ingesting%20free-form%0Atext%20input%2C%20which%20can%20prompt%20LLM%20responses%20beyond%20the%20scope%20of%20the%20game%0Anarrative.%20A%20novel%20validation%20system%20that%20uses%20the%20LLM%27s%20intelligence%20evaluates%0Atext%20input%20and%20aligns%20generated%20responses%20with%20the%20unfolding%20narrative.%20Making%0Athese%20interactions%20possible%2C%20PANGeA%20is%20supported%20by%20a%20server%20that%20hosts%20a%0Acustom%20memory%20system%20that%20supplies%20context%20for%20augmenting%20generated%20responses%0Athus%20aligning%20them%20with%20the%20procedural%20narrative.%20For%20its%20broad%20application%2C%0Athe%20server%20has%20a%20REST%20interface%20enabling%20any%20game%20engine%20to%20integrate%20directly%0Awith%20PANGeA%2C%20as%20well%20as%20an%20LLM%20interface%20adaptable%20with%20local%20or%20private%20LLMs.%0APANGeA%27s%20ability%20to%20foster%20dynamic%20narrative%20generation%20by%20aligning%20responses%0Awith%20the%20procedural%20narrative%20is%20demonstrated%20through%20an%20empirical%20study%20and%0Aablation%20test%20of%20two%20versions%20of%20a%20demo%20game.%20These%20are%2C%20a%20custom%2C%0Abrowser-based%20GPT%20and%20a%20Unity%20demo.%20As%20the%20results%20show%2C%20PANGeA%20holds%20potential%0Ato%20assist%20game%20designers%20in%20using%20LLMs%20to%20generate%20narrative-consistent%20content%0Aeven%20when%20provided%20varied%20and%20unpredictable%2C%20free-form%20text%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19721v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PANGeA%3A%20Procedural%20Artificial%20Narrative%20using%20Generative%20AI%20for%0A%20%20Turn-Based%20Video%20Games&entry.906535625=Steph%20Buongiorno%20and%20Lawrence%20Jake%20Klinkert%20and%20Tanishq%20Chawla%20and%20Zixin%20Zhuang%20and%20Corey%20Clark&entry.1292438233=%20%20This%20research%20introduces%20Procedural%20Artificial%20Narrative%20using%20Generative%20AI%0A%28PANGeA%29%2C%20a%20structured%20approach%20for%20leveraging%20large%20language%20models%20%28LLMs%29%2C%0Aguided%20by%20a%20game%20designer%27s%20high-level%20criteria%2C%20to%20generate%20narrative%20content%0Afor%20turn-based%20role-playing%20video%20games%20%28RPGs%29.%20Distinct%20from%20prior%0Aapplications%20of%20LLMs%20used%20for%20video%20game%20design%2C%20PANGeA%20innovates%20by%20not%20only%0Agenerating%20game%20level%20data%20%28which%20includes%2C%20but%20is%20not%20limited%20to%2C%20setting%2C%20key%0Aitems%2C%20and%20non-playable%20characters%20%28NPCs%29%29%2C%20but%20by%20also%20fostering%20dynamic%2C%0Afree-form%20interactions%20between%20the%20player%20and%20the%20environment%20that%20align%20with%0Athe%20procedural%20game%20narrative.%20The%20NPCs%20generated%20by%20PANGeA%20are%0Apersonality-biased%20and%20express%20traits%20from%20the%20Big%205%20Personality%20Model%20in%20their%0Agenerated%20responses.%20PANGeA%20addresses%20challenges%20behind%20ingesting%20free-form%0Atext%20input%2C%20which%20can%20prompt%20LLM%20responses%20beyond%20the%20scope%20of%20the%20game%0Anarrative.%20A%20novel%20validation%20system%20that%20uses%20the%20LLM%27s%20intelligence%20evaluates%0Atext%20input%20and%20aligns%20generated%20responses%20with%20the%20unfolding%20narrative.%20Making%0Athese%20interactions%20possible%2C%20PANGeA%20is%20supported%20by%20a%20server%20that%20hosts%20a%0Acustom%20memory%20system%20that%20supplies%20context%20for%20augmenting%20generated%20responses%0Athus%20aligning%20them%20with%20the%20procedural%20narrative.%20For%20its%20broad%20application%2C%0Athe%20server%20has%20a%20REST%20interface%20enabling%20any%20game%20engine%20to%20integrate%20directly%0Awith%20PANGeA%2C%20as%20well%20as%20an%20LLM%20interface%20adaptable%20with%20local%20or%20private%20LLMs.%0APANGeA%27s%20ability%20to%20foster%20dynamic%20narrative%20generation%20by%20aligning%20responses%0Awith%20the%20procedural%20narrative%20is%20demonstrated%20through%20an%20empirical%20study%20and%0Aablation%20test%20of%20two%20versions%20of%20a%20demo%20game.%20These%20are%2C%20a%20custom%2C%0Abrowser-based%20GPT%20and%20a%20Unity%20demo.%20As%20the%20results%20show%2C%20PANGeA%20holds%20potential%0Ato%20assist%20game%20designers%20in%20using%20LLMs%20to%20generate%20narrative-consistent%20content%0Aeven%20when%20provided%20varied%20and%20unpredictable%2C%20free-form%20text%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19721v1&entry.124074799=Read"},
{"title": "Improving Language Model Reasoning with Self-motivated Learning", "author": "Yunlong Feng and Yang Xu and Libo Qin and Yasheng Wang and Wanxiang Che", "abstract": "  Large-scale high-quality training data is important for improving the\nperformance of models. After trained with data that has rationales (reasoning\nsteps), models gain reasoning capability. However, the dataset with\nhigh-quality rationales is relatively scarce due to the high annotation cost.\nTo address this issue, we propose \\textit{Self-motivated Learning} framework.\nThe framework motivates the model itself to automatically generate rationales\non existing datasets. Based on the inherent rank from correctness across\nmultiple rationales, the model learns to generate better rationales, leading to\nhigher reasoning capability. Specifically, we train a reward model with the\nrank to evaluate the quality of rationales, and improve the performance of\nreasoning through reinforcement learning. Experiment results of Llama2 7B on\nmultiple reasoning datasets show that our method significantly improves the\nreasoning ability of models, even outperforming text-davinci-002 in some\ndatasets.\n", "link": "http://arxiv.org/abs/2404.07017v3", "date": "2024-04-30", "relevancy": 2.0131, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning&body=Title%3A%20Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning%0AAuthor%3A%20Yunlong%20Feng%20and%20Yang%20Xu%20and%20Libo%20Qin%20and%20Yasheng%20Wang%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Large-scale%20high-quality%20training%20data%20is%20important%20for%20improving%20the%0Aperformance%20of%20models.%20After%20trained%20with%20data%20that%20has%20rationales%20%28reasoning%0Asteps%29%2C%20models%20gain%20reasoning%20capability.%20However%2C%20the%20dataset%20with%0Ahigh-quality%20rationales%20is%20relatively%20scarce%20due%20to%20the%20high%20annotation%20cost.%0ATo%20address%20this%20issue%2C%20we%20propose%20%5Ctextit%7BSelf-motivated%20Learning%7D%20framework.%0AThe%20framework%20motivates%20the%20model%20itself%20to%20automatically%20generate%20rationales%0Aon%20existing%20datasets.%20Based%20on%20the%20inherent%20rank%20from%20correctness%20across%0Amultiple%20rationales%2C%20the%20model%20learns%20to%20generate%20better%20rationales%2C%20leading%20to%0Ahigher%20reasoning%20capability.%20Specifically%2C%20we%20train%20a%20reward%20model%20with%20the%0Arank%20to%20evaluate%20the%20quality%20of%20rationales%2C%20and%20improve%20the%20performance%20of%0Areasoning%20through%20reinforcement%20learning.%20Experiment%20results%20of%20Llama2%207B%20on%0Amultiple%20reasoning%20datasets%20show%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%20models%2C%20even%20outperforming%20text-davinci-002%20in%20some%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07017v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning&entry.906535625=Yunlong%20Feng%20and%20Yang%20Xu%20and%20Libo%20Qin%20and%20Yasheng%20Wang%20and%20Wanxiang%20Che&entry.1292438233=%20%20Large-scale%20high-quality%20training%20data%20is%20important%20for%20improving%20the%0Aperformance%20of%20models.%20After%20trained%20with%20data%20that%20has%20rationales%20%28reasoning%0Asteps%29%2C%20models%20gain%20reasoning%20capability.%20However%2C%20the%20dataset%20with%0Ahigh-quality%20rationales%20is%20relatively%20scarce%20due%20to%20the%20high%20annotation%20cost.%0ATo%20address%20this%20issue%2C%20we%20propose%20%5Ctextit%7BSelf-motivated%20Learning%7D%20framework.%0AThe%20framework%20motivates%20the%20model%20itself%20to%20automatically%20generate%20rationales%0Aon%20existing%20datasets.%20Based%20on%20the%20inherent%20rank%20from%20correctness%20across%0Amultiple%20rationales%2C%20the%20model%20learns%20to%20generate%20better%20rationales%2C%20leading%20to%0Ahigher%20reasoning%20capability.%20Specifically%2C%20we%20train%20a%20reward%20model%20with%20the%0Arank%20to%20evaluate%20the%20quality%20of%20rationales%2C%20and%20improve%20the%20performance%20of%0Areasoning%20through%20reinforcement%20learning.%20Experiment%20results%20of%20Llama2%207B%20on%0Amultiple%20reasoning%20datasets%20show%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%20models%2C%20even%20outperforming%20text-davinci-002%20in%20some%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07017v3&entry.124074799=Read"},
{"title": "A General Framework for Interpretable Neural Learning based on Local\n  Information-Theoretic Goal Functions", "author": "Abdullah Makkeh and Marcel Graetz and Andreas C. Schneider and David A. Ehrlich and Viola Priesemann and Michael Wibral", "abstract": "  Despite the impressive performance of biological and artificial networks, an\nintuitive understanding of how their local learning dynamics contribute to\nnetwork-level task solutions remains a challenge to this date. Efforts to bring\nlearning to a more local scale indeed lead to valuable insights, however, a\ngeneral constructive approach to describe local learning goals that is both\ninterpretable and adaptable across diverse tasks is still missing. We have\npreviously formulated a local information processing goal that is highly\nadaptable and interpretable for a model neuron with compartmental structure.\nBuilding on recent advances in Partial Information Decomposition (PID), we here\nderive a corresponding parametric local learning rule, which allows us to\nintroduce 'infomorphic' neural networks. We demonstrate the versatility of\nthese networks to perform tasks from supervised, unsupervised and memory\nlearning. By leveraging the interpretable nature of the PID framework,\ninfomorphic networks represent a valuable tool to advance our understanding of\nthe intricate structure of local learning.\n", "link": "http://arxiv.org/abs/2306.02149v2", "date": "2024-04-30", "relevancy": 1.9921, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5254}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4816}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4708}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Interpretable%20Neural%20Learning%20based%20on%20Local%0A%20%20Information-Theoretic%20Goal%20Functions&body=Title%3A%20A%20General%20Framework%20for%20Interpretable%20Neural%20Learning%20based%20on%20Local%0A%20%20Information-Theoretic%20Goal%20Functions%0AAuthor%3A%20Abdullah%20Makkeh%20and%20Marcel%20Graetz%20and%20Andreas%20C.%20Schneider%20and%20David%20A.%20Ehrlich%20and%20Viola%20Priesemann%20and%20Michael%20Wibral%0AAbstract%3A%20%20%20Despite%20the%20impressive%20performance%20of%20biological%20and%20artificial%20networks%2C%20an%0Aintuitive%20understanding%20of%20how%20their%20local%20learning%20dynamics%20contribute%20to%0Anetwork-level%20task%20solutions%20remains%20a%20challenge%20to%20this%20date.%20Efforts%20to%20bring%0Alearning%20to%20a%20more%20local%20scale%20indeed%20lead%20to%20valuable%20insights%2C%20however%2C%20a%0Ageneral%20constructive%20approach%20to%20describe%20local%20learning%20goals%20that%20is%20both%0Ainterpretable%20and%20adaptable%20across%20diverse%20tasks%20is%20still%20missing.%20We%20have%0Apreviously%20formulated%20a%20local%20information%20processing%20goal%20that%20is%20highly%0Aadaptable%20and%20interpretable%20for%20a%20model%20neuron%20with%20compartmental%20structure.%0ABuilding%20on%20recent%20advances%20in%20Partial%20Information%20Decomposition%20%28PID%29%2C%20we%20here%0Aderive%20a%20corresponding%20parametric%20local%20learning%20rule%2C%20which%20allows%20us%20to%0Aintroduce%20%27infomorphic%27%20neural%20networks.%20We%20demonstrate%20the%20versatility%20of%0Athese%20networks%20to%20perform%20tasks%20from%20supervised%2C%20unsupervised%20and%20memory%0Alearning.%20By%20leveraging%20the%20interpretable%20nature%20of%20the%20PID%20framework%2C%0Ainfomorphic%20networks%20represent%20a%20valuable%20tool%20to%20advance%20our%20understanding%20of%0Athe%20intricate%20structure%20of%20local%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02149v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Interpretable%20Neural%20Learning%20based%20on%20Local%0A%20%20Information-Theoretic%20Goal%20Functions&entry.906535625=Abdullah%20Makkeh%20and%20Marcel%20Graetz%20and%20Andreas%20C.%20Schneider%20and%20David%20A.%20Ehrlich%20and%20Viola%20Priesemann%20and%20Michael%20Wibral&entry.1292438233=%20%20Despite%20the%20impressive%20performance%20of%20biological%20and%20artificial%20networks%2C%20an%0Aintuitive%20understanding%20of%20how%20their%20local%20learning%20dynamics%20contribute%20to%0Anetwork-level%20task%20solutions%20remains%20a%20challenge%20to%20this%20date.%20Efforts%20to%20bring%0Alearning%20to%20a%20more%20local%20scale%20indeed%20lead%20to%20valuable%20insights%2C%20however%2C%20a%0Ageneral%20constructive%20approach%20to%20describe%20local%20learning%20goals%20that%20is%20both%0Ainterpretable%20and%20adaptable%20across%20diverse%20tasks%20is%20still%20missing.%20We%20have%0Apreviously%20formulated%20a%20local%20information%20processing%20goal%20that%20is%20highly%0Aadaptable%20and%20interpretable%20for%20a%20model%20neuron%20with%20compartmental%20structure.%0ABuilding%20on%20recent%20advances%20in%20Partial%20Information%20Decomposition%20%28PID%29%2C%20we%20here%0Aderive%20a%20corresponding%20parametric%20local%20learning%20rule%2C%20which%20allows%20us%20to%0Aintroduce%20%27infomorphic%27%20neural%20networks.%20We%20demonstrate%20the%20versatility%20of%0Athese%20networks%20to%20perform%20tasks%20from%20supervised%2C%20unsupervised%20and%20memory%0Alearning.%20By%20leveraging%20the%20interpretable%20nature%20of%20the%20PID%20framework%2C%0Ainfomorphic%20networks%20represent%20a%20valuable%20tool%20to%20advance%20our%20understanding%20of%0Athe%20intricate%20structure%20of%20local%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02149v2&entry.124074799=Read"},
{"title": "Adversarial Example Soups: Improving Transferability and Stealthiness\n  for Free", "author": "Bo Yang and Hengwei Zhang and Jindong Wang and Yulong Yang and Chenhao Lin and Chao Shen and Zhengyu Zhao", "abstract": "  Transferable adversarial examples cause practical security risks since they\ncan mislead a target model without knowing its internal knowledge. A\nconventional recipe for maximizing transferability is to keep only the optimal\nadversarial example from all those obtained in the optimization pipeline. In\nthis paper, for the first time, we question this convention and demonstrate\nthat those discarded, sub-optimal adversarial examples can be reused to boost\ntransferability. Specifically, we propose ``Adversarial Example Soups'' (AES),\nwith AES-tune for averaging discarded adversarial examples in hyperparameter\ntuning and AES-rand for stability testing. In addition, our AES is inspired by\n``model soups'', which averages weights of multiple fine-tuned models for\nimproved accuracy without increasing inference time. Extensive experiments\nvalidate the global effectiveness of our AES, boosting 10 state-of-the-art\ntransfer attacks and their combinations by up to 13% against 10 diverse\n(defensive) target models. We also show the possibility of generalizing AES to\nother types, e.g., directly averaging multiple in-the-wild adversarial examples\nthat yield comparable success. A promising byproduct of AES is the improved\nstealthiness of adversarial examples since the perturbation variances are\nnaturally reduced.\n", "link": "http://arxiv.org/abs/2402.18370v2", "date": "2024-04-30", "relevancy": 1.9861, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4697}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4519}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Example%20Soups%3A%20Improving%20Transferability%20and%20Stealthiness%0A%20%20for%20Free&body=Title%3A%20Adversarial%20Example%20Soups%3A%20Improving%20Transferability%20and%20Stealthiness%0A%20%20for%20Free%0AAuthor%3A%20Bo%20Yang%20and%20Hengwei%20Zhang%20and%20Jindong%20Wang%20and%20Yulong%20Yang%20and%20Chenhao%20Lin%20and%20Chao%20Shen%20and%20Zhengyu%20Zhao%0AAbstract%3A%20%20%20Transferable%20adversarial%20examples%20cause%20practical%20security%20risks%20since%20they%0Acan%20mislead%20a%20target%20model%20without%20knowing%20its%20internal%20knowledge.%20A%0Aconventional%20recipe%20for%20maximizing%20transferability%20is%20to%20keep%20only%20the%20optimal%0Aadversarial%20example%20from%20all%20those%20obtained%20in%20the%20optimization%20pipeline.%20In%0Athis%20paper%2C%20for%20the%20first%20time%2C%20we%20question%20this%20convention%20and%20demonstrate%0Athat%20those%20discarded%2C%20sub-optimal%20adversarial%20examples%20can%20be%20reused%20to%20boost%0Atransferability.%20Specifically%2C%20we%20propose%20%60%60Adversarial%20Example%20Soups%27%27%20%28AES%29%2C%0Awith%20AES-tune%20for%20averaging%20discarded%20adversarial%20examples%20in%20hyperparameter%0Atuning%20and%20AES-rand%20for%20stability%20testing.%20In%20addition%2C%20our%20AES%20is%20inspired%20by%0A%60%60model%20soups%27%27%2C%20which%20averages%20weights%20of%20multiple%20fine-tuned%20models%20for%0Aimproved%20accuracy%20without%20increasing%20inference%20time.%20Extensive%20experiments%0Avalidate%20the%20global%20effectiveness%20of%20our%20AES%2C%20boosting%2010%20state-of-the-art%0Atransfer%20attacks%20and%20their%20combinations%20by%20up%20to%2013%25%20against%2010%20diverse%0A%28defensive%29%20target%20models.%20We%20also%20show%20the%20possibility%20of%20generalizing%20AES%20to%0Aother%20types%2C%20e.g.%2C%20directly%20averaging%20multiple%20in-the-wild%20adversarial%20examples%0Athat%20yield%20comparable%20success.%20A%20promising%20byproduct%20of%20AES%20is%20the%20improved%0Astealthiness%20of%20adversarial%20examples%20since%20the%20perturbation%20variances%20are%0Anaturally%20reduced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18370v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Example%20Soups%3A%20Improving%20Transferability%20and%20Stealthiness%0A%20%20for%20Free&entry.906535625=Bo%20Yang%20and%20Hengwei%20Zhang%20and%20Jindong%20Wang%20and%20Yulong%20Yang%20and%20Chenhao%20Lin%20and%20Chao%20Shen%20and%20Zhengyu%20Zhao&entry.1292438233=%20%20Transferable%20adversarial%20examples%20cause%20practical%20security%20risks%20since%20they%0Acan%20mislead%20a%20target%20model%20without%20knowing%20its%20internal%20knowledge.%20A%0Aconventional%20recipe%20for%20maximizing%20transferability%20is%20to%20keep%20only%20the%20optimal%0Aadversarial%20example%20from%20all%20those%20obtained%20in%20the%20optimization%20pipeline.%20In%0Athis%20paper%2C%20for%20the%20first%20time%2C%20we%20question%20this%20convention%20and%20demonstrate%0Athat%20those%20discarded%2C%20sub-optimal%20adversarial%20examples%20can%20be%20reused%20to%20boost%0Atransferability.%20Specifically%2C%20we%20propose%20%60%60Adversarial%20Example%20Soups%27%27%20%28AES%29%2C%0Awith%20AES-tune%20for%20averaging%20discarded%20adversarial%20examples%20in%20hyperparameter%0Atuning%20and%20AES-rand%20for%20stability%20testing.%20In%20addition%2C%20our%20AES%20is%20inspired%20by%0A%60%60model%20soups%27%27%2C%20which%20averages%20weights%20of%20multiple%20fine-tuned%20models%20for%0Aimproved%20accuracy%20without%20increasing%20inference%20time.%20Extensive%20experiments%0Avalidate%20the%20global%20effectiveness%20of%20our%20AES%2C%20boosting%2010%20state-of-the-art%0Atransfer%20attacks%20and%20their%20combinations%20by%20up%20to%2013%25%20against%2010%20diverse%0A%28defensive%29%20target%20models.%20We%20also%20show%20the%20possibility%20of%20generalizing%20AES%20to%0Aother%20types%2C%20e.g.%2C%20directly%20averaging%20multiple%20in-the-wild%20adversarial%20examples%0Athat%20yield%20comparable%20success.%20A%20promising%20byproduct%20of%20AES%20is%20the%20improved%0Astealthiness%20of%20adversarial%20examples%20since%20the%20perturbation%20variances%20are%0Anaturally%20reduced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18370v2&entry.124074799=Read"},
{"title": "ShadowMaskFormer: Mask Augmented Patch Embeddings for Shadow Removal", "author": "Zhuohao Li and Guoyang Xie and Guannan Jiang and Zhichao Lu", "abstract": "  Transformer recently emerged as the de facto model for computer vision tasks\nand has also been successfully applied to shadow removal. However, these\nexisting methods heavily rely on intricate modifications to the attention\nmechanisms within the transformer blocks while using a generic patch embedding.\nAs a result, it often leads to complex architectural designs requiring\nadditional computation resources. In this work, we aim to explore the efficacy\nof incorporating shadow information within the early processing stage.\nAccordingly, we propose a transformer-based framework with a novel patch\nembedding that is tailored for shadow removal, dubbed ShadowMaskFormer.\nSpecifically, we present a simple and effective mask-augmented patch embedding\nto integrate shadow information and promote the model's emphasis on acquiring\nknowledge for shadow regions. Extensive experiments conducted on the ISTD,\nISTD+, and SRD benchmark datasets demonstrate the efficacy of our method\nagainst state-of-the-art approaches while using fewer model parameters.\n", "link": "http://arxiv.org/abs/2404.18433v2", "date": "2024-04-30", "relevancy": 1.9839, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5063}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4892}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ShadowMaskFormer%3A%20Mask%20Augmented%20Patch%20Embeddings%20for%20Shadow%20Removal&body=Title%3A%20ShadowMaskFormer%3A%20Mask%20Augmented%20Patch%20Embeddings%20for%20Shadow%20Removal%0AAuthor%3A%20Zhuohao%20Li%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu%0AAbstract%3A%20%20%20Transformer%20recently%20emerged%20as%20the%20de%20facto%20model%20for%20computer%20vision%20tasks%0Aand%20has%20also%20been%20successfully%20applied%20to%20shadow%20removal.%20However%2C%20these%0Aexisting%20methods%20heavily%20rely%20on%20intricate%20modifications%20to%20the%20attention%0Amechanisms%20within%20the%20transformer%20blocks%20while%20using%20a%20generic%20patch%20embedding.%0AAs%20a%20result%2C%20it%20often%20leads%20to%20complex%20architectural%20designs%20requiring%0Aadditional%20computation%20resources.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20efficacy%0Aof%20incorporating%20shadow%20information%20within%20the%20early%20processing%20stage.%0AAccordingly%2C%20we%20propose%20a%20transformer-based%20framework%20with%20a%20novel%20patch%0Aembedding%20that%20is%20tailored%20for%20shadow%20removal%2C%20dubbed%20ShadowMaskFormer.%0ASpecifically%2C%20we%20present%20a%20simple%20and%20effective%20mask-augmented%20patch%20embedding%0Ato%20integrate%20shadow%20information%20and%20promote%20the%20model%27s%20emphasis%20on%20acquiring%0Aknowledge%20for%20shadow%20regions.%20Extensive%20experiments%20conducted%20on%20the%20ISTD%2C%0AISTD%2B%2C%20and%20SRD%20benchmark%20datasets%20demonstrate%20the%20efficacy%20of%20our%20method%0Aagainst%20state-of-the-art%20approaches%20while%20using%20fewer%20model%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18433v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShadowMaskFormer%3A%20Mask%20Augmented%20Patch%20Embeddings%20for%20Shadow%20Removal&entry.906535625=Zhuohao%20Li%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu&entry.1292438233=%20%20Transformer%20recently%20emerged%20as%20the%20de%20facto%20model%20for%20computer%20vision%20tasks%0Aand%20has%20also%20been%20successfully%20applied%20to%20shadow%20removal.%20However%2C%20these%0Aexisting%20methods%20heavily%20rely%20on%20intricate%20modifications%20to%20the%20attention%0Amechanisms%20within%20the%20transformer%20blocks%20while%20using%20a%20generic%20patch%20embedding.%0AAs%20a%20result%2C%20it%20often%20leads%20to%20complex%20architectural%20designs%20requiring%0Aadditional%20computation%20resources.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20efficacy%0Aof%20incorporating%20shadow%20information%20within%20the%20early%20processing%20stage.%0AAccordingly%2C%20we%20propose%20a%20transformer-based%20framework%20with%20a%20novel%20patch%0Aembedding%20that%20is%20tailored%20for%20shadow%20removal%2C%20dubbed%20ShadowMaskFormer.%0ASpecifically%2C%20we%20present%20a%20simple%20and%20effective%20mask-augmented%20patch%20embedding%0Ato%20integrate%20shadow%20information%20and%20promote%20the%20model%27s%20emphasis%20on%20acquiring%0Aknowledge%20for%20shadow%20regions.%20Extensive%20experiments%20conducted%20on%20the%20ISTD%2C%0AISTD%2B%2C%20and%20SRD%20benchmark%20datasets%20demonstrate%20the%20efficacy%20of%20our%20method%0Aagainst%20state-of-the-art%20approaches%20while%20using%20fewer%20model%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18433v2&entry.124074799=Read"},
{"title": "Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule\n  Generation", "author": "Ian Dunn and David Ryan Koes", "abstract": "  Deep generative models that produce novel molecular structures have the\npotential to facilitate chemical discovery. Diffusion models currently achieve\nstate of the art performance for 3D molecule generation. In this work, we\nexplore the use of flow matching, a recently proposed generative modeling\nframework that generalizes diffusion models, for the task of de novo molecule\ngeneration. Flow matching provides flexibility in model design; however, the\nframework is predicated on the assumption of continuously-valued data. 3D de\nnovo molecule generation requires jointly sampling continuous and categorical\nvariables such as atom position and atom type. We extend the flow matching\nframework to categorical data by constructing flows that are constrained to\nexist on a continuous representation of categorical data known as the\nprobability simplex. We call this extension SimplexFlow. We explore the use of\nSimplexFlow for de novo molecule generation. However, we find that, in\npractice, a simpler approach that makes no accommodations for the categorical\nnature of the data yields equivalent or superior performance. As a result of\nthese experiments, we present FlowMol, a flow matching model for 3D de novo\ngenerative model that achieves improved performance over prior flow matching\nmethods, and we raise important questions about the design of prior\ndistributions for achieving strong performance in flow matching models. Code\nand trained models for reproducing this work are available at\nhttps://github.com/dunni3/FlowMol\n", "link": "http://arxiv.org/abs/2404.19739v1", "date": "2024-04-30", "relevancy": 1.9837, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4854}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4796}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mixed%20Continuous%20and%20Categorical%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%0A%20%20Generation&body=Title%3A%20Mixed%20Continuous%20and%20Categorical%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%0A%20%20Generation%0AAuthor%3A%20Ian%20Dunn%20and%20David%20Ryan%20Koes%0AAbstract%3A%20%20%20Deep%20generative%20models%20that%20produce%20novel%20molecular%20structures%20have%20the%0Apotential%20to%20facilitate%20chemical%20discovery.%20Diffusion%20models%20currently%20achieve%0Astate%20of%20the%20art%20performance%20for%203D%20molecule%20generation.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20flow%20matching%2C%20a%20recently%20proposed%20generative%20modeling%0Aframework%20that%20generalizes%20diffusion%20models%2C%20for%20the%20task%20of%20de%20novo%20molecule%0Ageneration.%20Flow%20matching%20provides%20flexibility%20in%20model%20design%3B%20however%2C%20the%0Aframework%20is%20predicated%20on%20the%20assumption%20of%20continuously-valued%20data.%203D%20de%0Anovo%20molecule%20generation%20requires%20jointly%20sampling%20continuous%20and%20categorical%0Avariables%20such%20as%20atom%20position%20and%20atom%20type.%20We%20extend%20the%20flow%20matching%0Aframework%20to%20categorical%20data%20by%20constructing%20flows%20that%20are%20constrained%20to%0Aexist%20on%20a%20continuous%20representation%20of%20categorical%20data%20known%20as%20the%0Aprobability%20simplex.%20We%20call%20this%20extension%20SimplexFlow.%20We%20explore%20the%20use%20of%0ASimplexFlow%20for%20de%20novo%20molecule%20generation.%20However%2C%20we%20find%20that%2C%20in%0Apractice%2C%20a%20simpler%20approach%20that%20makes%20no%20accommodations%20for%20the%20categorical%0Anature%20of%20the%20data%20yields%20equivalent%20or%20superior%20performance.%20As%20a%20result%20of%0Athese%20experiments%2C%20we%20present%20FlowMol%2C%20a%20flow%20matching%20model%20for%203D%20de%20novo%0Agenerative%20model%20that%20achieves%20improved%20performance%20over%20prior%20flow%20matching%0Amethods%2C%20and%20we%20raise%20important%20questions%20about%20the%20design%20of%20prior%0Adistributions%20for%20achieving%20strong%20performance%20in%20flow%20matching%20models.%20Code%0Aand%20trained%20models%20for%20reproducing%20this%20work%20are%20available%20at%0Ahttps%3A//github.com/dunni3/FlowMol%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19739v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed%20Continuous%20and%20Categorical%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%0A%20%20Generation&entry.906535625=Ian%20Dunn%20and%20David%20Ryan%20Koes&entry.1292438233=%20%20Deep%20generative%20models%20that%20produce%20novel%20molecular%20structures%20have%20the%0Apotential%20to%20facilitate%20chemical%20discovery.%20Diffusion%20models%20currently%20achieve%0Astate%20of%20the%20art%20performance%20for%203D%20molecule%20generation.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20flow%20matching%2C%20a%20recently%20proposed%20generative%20modeling%0Aframework%20that%20generalizes%20diffusion%20models%2C%20for%20the%20task%20of%20de%20novo%20molecule%0Ageneration.%20Flow%20matching%20provides%20flexibility%20in%20model%20design%3B%20however%2C%20the%0Aframework%20is%20predicated%20on%20the%20assumption%20of%20continuously-valued%20data.%203D%20de%0Anovo%20molecule%20generation%20requires%20jointly%20sampling%20continuous%20and%20categorical%0Avariables%20such%20as%20atom%20position%20and%20atom%20type.%20We%20extend%20the%20flow%20matching%0Aframework%20to%20categorical%20data%20by%20constructing%20flows%20that%20are%20constrained%20to%0Aexist%20on%20a%20continuous%20representation%20of%20categorical%20data%20known%20as%20the%0Aprobability%20simplex.%20We%20call%20this%20extension%20SimplexFlow.%20We%20explore%20the%20use%20of%0ASimplexFlow%20for%20de%20novo%20molecule%20generation.%20However%2C%20we%20find%20that%2C%20in%0Apractice%2C%20a%20simpler%20approach%20that%20makes%20no%20accommodations%20for%20the%20categorical%0Anature%20of%20the%20data%20yields%20equivalent%20or%20superior%20performance.%20As%20a%20result%20of%0Athese%20experiments%2C%20we%20present%20FlowMol%2C%20a%20flow%20matching%20model%20for%203D%20de%20novo%0Agenerative%20model%20that%20achieves%20improved%20performance%20over%20prior%20flow%20matching%0Amethods%2C%20and%20we%20raise%20important%20questions%20about%20the%20design%20of%20prior%0Adistributions%20for%20achieving%20strong%20performance%20in%20flow%20matching%20models.%20Code%0Aand%20trained%20models%20for%20reproducing%20this%20work%20are%20available%20at%0Ahttps%3A//github.com/dunni3/FlowMol%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19739v1&entry.124074799=Read"},
{"title": "On Training a Neural Network to Explain Binaries", "author": "Alexander Interrante-Grant and Andy Davis and Heather Preslier and Tim Leek", "abstract": "  In this work, we begin to investigate the possibility of training a deep\nneural network on the task of binary code understanding. Specifically, the\nnetwork would take, as input, features derived directly from binaries and\noutput English descriptions of functionality to aid a reverse engineer in\ninvestigating the capabilities of a piece of closed-source software, be it\nmalicious or benign. Given recent success in applying large language models\n(generative AI) to the task of source code summarization, this seems a\npromising direction. However, in our initial survey of the available datasets,\nwe found nothing of sufficiently high quality and volume to train these complex\nmodels. Instead, we build our own dataset derived from a capture of Stack\nOverflow containing 1.1M entries. A major result of our work is a novel dataset\nevaluation method using the correlation between two distances on sample pairs:\none distance in the embedding space of inputs and the other in the embedding\nspace of outputs. Intuitively, if two samples have inputs close in the input\nembedding space, their outputs should also be close in the output embedding\nspace. We found this Embedding Distance Correlation (EDC) test to be highly\ndiagnostic, indicating that our collected dataset and several existing\nopen-source datasets are of low quality as the distances are not well\ncorrelated. We proceed to explore the general applicability of EDC, applying it\nto a number of qualitatively known good datasets and a number of synthetically\nknown bad ones and found it to be a reliable indicator of dataset value.\n", "link": "http://arxiv.org/abs/2404.19631v1", "date": "2024-04-30", "relevancy": 1.9766, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5309}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4645}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Training%20a%20Neural%20Network%20to%20Explain%20Binaries&body=Title%3A%20On%20Training%20a%20Neural%20Network%20to%20Explain%20Binaries%0AAuthor%3A%20Alexander%20Interrante-Grant%20and%20Andy%20Davis%20and%20Heather%20Preslier%20and%20Tim%20Leek%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20begin%20to%20investigate%20the%20possibility%20of%20training%20a%20deep%0Aneural%20network%20on%20the%20task%20of%20binary%20code%20understanding.%20Specifically%2C%20the%0Anetwork%20would%20take%2C%20as%20input%2C%20features%20derived%20directly%20from%20binaries%20and%0Aoutput%20English%20descriptions%20of%20functionality%20to%20aid%20a%20reverse%20engineer%20in%0Ainvestigating%20the%20capabilities%20of%20a%20piece%20of%20closed-source%20software%2C%20be%20it%0Amalicious%20or%20benign.%20Given%20recent%20success%20in%20applying%20large%20language%20models%0A%28generative%20AI%29%20to%20the%20task%20of%20source%20code%20summarization%2C%20this%20seems%20a%0Apromising%20direction.%20However%2C%20in%20our%20initial%20survey%20of%20the%20available%20datasets%2C%0Awe%20found%20nothing%20of%20sufficiently%20high%20quality%20and%20volume%20to%20train%20these%20complex%0Amodels.%20Instead%2C%20we%20build%20our%20own%20dataset%20derived%20from%20a%20capture%20of%20Stack%0AOverflow%20containing%201.1M%20entries.%20A%20major%20result%20of%20our%20work%20is%20a%20novel%20dataset%0Aevaluation%20method%20using%20the%20correlation%20between%20two%20distances%20on%20sample%20pairs%3A%0Aone%20distance%20in%20the%20embedding%20space%20of%20inputs%20and%20the%20other%20in%20the%20embedding%0Aspace%20of%20outputs.%20Intuitively%2C%20if%20two%20samples%20have%20inputs%20close%20in%20the%20input%0Aembedding%20space%2C%20their%20outputs%20should%20also%20be%20close%20in%20the%20output%20embedding%0Aspace.%20We%20found%20this%20Embedding%20Distance%20Correlation%20%28EDC%29%20test%20to%20be%20highly%0Adiagnostic%2C%20indicating%20that%20our%20collected%20dataset%20and%20several%20existing%0Aopen-source%20datasets%20are%20of%20low%20quality%20as%20the%20distances%20are%20not%20well%0Acorrelated.%20We%20proceed%20to%20explore%20the%20general%20applicability%20of%20EDC%2C%20applying%20it%0Ato%20a%20number%20of%20qualitatively%20known%20good%20datasets%20and%20a%20number%20of%20synthetically%0Aknown%20bad%20ones%20and%20found%20it%20to%20be%20a%20reliable%20indicator%20of%20dataset%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19631v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Training%20a%20Neural%20Network%20to%20Explain%20Binaries&entry.906535625=Alexander%20Interrante-Grant%20and%20Andy%20Davis%20and%20Heather%20Preslier%20and%20Tim%20Leek&entry.1292438233=%20%20In%20this%20work%2C%20we%20begin%20to%20investigate%20the%20possibility%20of%20training%20a%20deep%0Aneural%20network%20on%20the%20task%20of%20binary%20code%20understanding.%20Specifically%2C%20the%0Anetwork%20would%20take%2C%20as%20input%2C%20features%20derived%20directly%20from%20binaries%20and%0Aoutput%20English%20descriptions%20of%20functionality%20to%20aid%20a%20reverse%20engineer%20in%0Ainvestigating%20the%20capabilities%20of%20a%20piece%20of%20closed-source%20software%2C%20be%20it%0Amalicious%20or%20benign.%20Given%20recent%20success%20in%20applying%20large%20language%20models%0A%28generative%20AI%29%20to%20the%20task%20of%20source%20code%20summarization%2C%20this%20seems%20a%0Apromising%20direction.%20However%2C%20in%20our%20initial%20survey%20of%20the%20available%20datasets%2C%0Awe%20found%20nothing%20of%20sufficiently%20high%20quality%20and%20volume%20to%20train%20these%20complex%0Amodels.%20Instead%2C%20we%20build%20our%20own%20dataset%20derived%20from%20a%20capture%20of%20Stack%0AOverflow%20containing%201.1M%20entries.%20A%20major%20result%20of%20our%20work%20is%20a%20novel%20dataset%0Aevaluation%20method%20using%20the%20correlation%20between%20two%20distances%20on%20sample%20pairs%3A%0Aone%20distance%20in%20the%20embedding%20space%20of%20inputs%20and%20the%20other%20in%20the%20embedding%0Aspace%20of%20outputs.%20Intuitively%2C%20if%20two%20samples%20have%20inputs%20close%20in%20the%20input%0Aembedding%20space%2C%20their%20outputs%20should%20also%20be%20close%20in%20the%20output%20embedding%0Aspace.%20We%20found%20this%20Embedding%20Distance%20Correlation%20%28EDC%29%20test%20to%20be%20highly%0Adiagnostic%2C%20indicating%20that%20our%20collected%20dataset%20and%20several%20existing%0Aopen-source%20datasets%20are%20of%20low%20quality%20as%20the%20distances%20are%20not%20well%0Acorrelated.%20We%20proceed%20to%20explore%20the%20general%20applicability%20of%20EDC%2C%20applying%20it%0Ato%20a%20number%20of%20qualitatively%20known%20good%20datasets%20and%20a%20number%20of%20synthetically%0Aknown%20bad%20ones%20and%20found%20it%20to%20be%20a%20reliable%20indicator%20of%20dataset%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19631v1&entry.124074799=Read"},
{"title": "Leveraging Label Information for Stealthy Data Stealing in Vertical\n  Federated Learning", "author": "Duanyi Yao and Songze Li and Xueluan Gong and Sizai Hou and Gaoning Pan", "abstract": "  We develop DMAVFL, a novel attack strategy that evades current detection\nmechanisms. The key idea is to integrate a discriminator with auxiliary\nclassifier that takes a full advantage of the label information (which was\ncompletely ignored in previous attacks): on one hand, label information helps\nto better characterize embeddings of samples from distinct classes, yielding an\nimproved reconstruction performance; on the other hand, computing malicious\ngradients with label information better mimics the honest training, making the\nmalicious gradients indistinguishable from the honest ones, and the attack much\nmore stealthy. Our comprehensive experiments demonstrate that DMAVFL\nsignificantly outperforms existing attacks, and successfully circumvents SOTA\ndefenses for malicious attacks. Additional ablation studies and evaluations on\nother defenses further underscore the robustness and effectiveness of DMAVFL.\n", "link": "http://arxiv.org/abs/2404.19582v1", "date": "2024-04-30", "relevancy": 1.9688, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.505}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5015}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4757}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Label%20Information%20for%20Stealthy%20Data%20Stealing%20in%20Vertical%0A%20%20Federated%20Learning&body=Title%3A%20Leveraging%20Label%20Information%20for%20Stealthy%20Data%20Stealing%20in%20Vertical%0A%20%20Federated%20Learning%0AAuthor%3A%20Duanyi%20Yao%20and%20Songze%20Li%20and%20Xueluan%20Gong%20and%20Sizai%20Hou%20and%20Gaoning%20Pan%0AAbstract%3A%20%20%20We%20develop%20DMAVFL%2C%20a%20novel%20attack%20strategy%20that%20evades%20current%20detection%0Amechanisms.%20The%20key%20idea%20is%20to%20integrate%20a%20discriminator%20with%20auxiliary%0Aclassifier%20that%20takes%20a%20full%20advantage%20of%20the%20label%20information%20%28which%20was%0Acompletely%20ignored%20in%20previous%20attacks%29%3A%20on%20one%20hand%2C%20label%20information%20helps%0Ato%20better%20characterize%20embeddings%20of%20samples%20from%20distinct%20classes%2C%20yielding%20an%0Aimproved%20reconstruction%20performance%3B%20on%20the%20other%20hand%2C%20computing%20malicious%0Agradients%20with%20label%20information%20better%20mimics%20the%20honest%20training%2C%20making%20the%0Amalicious%20gradients%20indistinguishable%20from%20the%20honest%20ones%2C%20and%20the%20attack%20much%0Amore%20stealthy.%20Our%20comprehensive%20experiments%20demonstrate%20that%20DMAVFL%0Asignificantly%20outperforms%20existing%20attacks%2C%20and%20successfully%20circumvents%20SOTA%0Adefenses%20for%20malicious%20attacks.%20Additional%20ablation%20studies%20and%20evaluations%20on%0Aother%20defenses%20further%20underscore%20the%20robustness%20and%20effectiveness%20of%20DMAVFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19582v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Label%20Information%20for%20Stealthy%20Data%20Stealing%20in%20Vertical%0A%20%20Federated%20Learning&entry.906535625=Duanyi%20Yao%20and%20Songze%20Li%20and%20Xueluan%20Gong%20and%20Sizai%20Hou%20and%20Gaoning%20Pan&entry.1292438233=%20%20We%20develop%20DMAVFL%2C%20a%20novel%20attack%20strategy%20that%20evades%20current%20detection%0Amechanisms.%20The%20key%20idea%20is%20to%20integrate%20a%20discriminator%20with%20auxiliary%0Aclassifier%20that%20takes%20a%20full%20advantage%20of%20the%20label%20information%20%28which%20was%0Acompletely%20ignored%20in%20previous%20attacks%29%3A%20on%20one%20hand%2C%20label%20information%20helps%0Ato%20better%20characterize%20embeddings%20of%20samples%20from%20distinct%20classes%2C%20yielding%20an%0Aimproved%20reconstruction%20performance%3B%20on%20the%20other%20hand%2C%20computing%20malicious%0Agradients%20with%20label%20information%20better%20mimics%20the%20honest%20training%2C%20making%20the%0Amalicious%20gradients%20indistinguishable%20from%20the%20honest%20ones%2C%20and%20the%20attack%20much%0Amore%20stealthy.%20Our%20comprehensive%20experiments%20demonstrate%20that%20DMAVFL%0Asignificantly%20outperforms%20existing%20attacks%2C%20and%20successfully%20circumvents%20SOTA%0Adefenses%20for%20malicious%20attacks.%20Additional%20ablation%20studies%20and%20evaluations%20on%0Aother%20defenses%20further%20underscore%20the%20robustness%20and%20effectiveness%20of%20DMAVFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19582v1&entry.124074799=Read"},
{"title": "PASS: Peer-Agreement based Sample Selection for training with Noisy\n  Labels", "author": "Arpit Garg and Cuong Nguyen and Rafael Felix and Thanh-Toan Do and Gustavo Carneiro", "abstract": "  The prevalence of noisy-label samples poses a significant challenge in deep\nlearning, inducing overfitting effects. This has, therefore, motivated the\nemergence of learning with noisy-label (LNL) techniques that focus on\nseparating noisy- and clean-label samples to apply different learning\nstrategies to each group of samples. Current methodologies often rely on the\nsmall-loss hypothesis or feature-based selection to separate noisy- and\nclean-label samples, yet our empirical observations reveal their limitations,\nespecially for labels with instance dependent noise (IDN). An important\ncharacteristic of IDN is the difficulty to distinguish the clean-label samples\nthat lie near the decision boundary (i.e., the hard samples) from the\nnoisy-label samples. We, therefore, propose a new noisy-label detection method,\ntermed Peer-Agreement based Sample Selection (PASS), to address this problem.\nUtilising a trio of classifiers, PASS employs consensus-driven peer-based\nagreement of two models to select the samples to train the remaining model.\nPASS is easily integrated into existing LNL models, enabling the improvement of\nthe detection accuracy of noisy- and clean-label samples, which increases the\nclassification accuracy across various LNL benchmarks.\n", "link": "http://arxiv.org/abs/2303.10802v2", "date": "2024-04-30", "relevancy": 1.9652, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4746}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PASS%3A%20Peer-Agreement%20based%20Sample%20Selection%20for%20training%20with%20Noisy%0A%20%20Labels&body=Title%3A%20PASS%3A%20Peer-Agreement%20based%20Sample%20Selection%20for%20training%20with%20Noisy%0A%20%20Labels%0AAuthor%3A%20Arpit%20Garg%20and%20Cuong%20Nguyen%20and%20Rafael%20Felix%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20The%20prevalence%20of%20noisy-label%20samples%20poses%20a%20significant%20challenge%20in%20deep%0Alearning%2C%20inducing%20overfitting%20effects.%20This%20has%2C%20therefore%2C%20motivated%20the%0Aemergence%20of%20learning%20with%20noisy-label%20%28LNL%29%20techniques%20that%20focus%20on%0Aseparating%20noisy-%20and%20clean-label%20samples%20to%20apply%20different%20learning%0Astrategies%20to%20each%20group%20of%20samples.%20Current%20methodologies%20often%20rely%20on%20the%0Asmall-loss%20hypothesis%20or%20feature-based%20selection%20to%20separate%20noisy-%20and%0Aclean-label%20samples%2C%20yet%20our%20empirical%20observations%20reveal%20their%20limitations%2C%0Aespecially%20for%20labels%20with%20instance%20dependent%20noise%20%28IDN%29.%20An%20important%0Acharacteristic%20of%20IDN%20is%20the%20difficulty%20to%20distinguish%20the%20clean-label%20samples%0Athat%20lie%20near%20the%20decision%20boundary%20%28i.e.%2C%20the%20hard%20samples%29%20from%20the%0Anoisy-label%20samples.%20We%2C%20therefore%2C%20propose%20a%20new%20noisy-label%20detection%20method%2C%0Atermed%20Peer-Agreement%20based%20Sample%20Selection%20%28PASS%29%2C%20to%20address%20this%20problem.%0AUtilising%20a%20trio%20of%20classifiers%2C%20PASS%20employs%20consensus-driven%20peer-based%0Aagreement%20of%20two%20models%20to%20select%20the%20samples%20to%20train%20the%20remaining%20model.%0APASS%20is%20easily%20integrated%20into%20existing%20LNL%20models%2C%20enabling%20the%20improvement%20of%0Athe%20detection%20accuracy%20of%20noisy-%20and%20clean-label%20samples%2C%20which%20increases%20the%0Aclassification%20accuracy%20across%20various%20LNL%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10802v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PASS%3A%20Peer-Agreement%20based%20Sample%20Selection%20for%20training%20with%20Noisy%0A%20%20Labels&entry.906535625=Arpit%20Garg%20and%20Cuong%20Nguyen%20and%20Rafael%20Felix%20and%20Thanh-Toan%20Do%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20The%20prevalence%20of%20noisy-label%20samples%20poses%20a%20significant%20challenge%20in%20deep%0Alearning%2C%20inducing%20overfitting%20effects.%20This%20has%2C%20therefore%2C%20motivated%20the%0Aemergence%20of%20learning%20with%20noisy-label%20%28LNL%29%20techniques%20that%20focus%20on%0Aseparating%20noisy-%20and%20clean-label%20samples%20to%20apply%20different%20learning%0Astrategies%20to%20each%20group%20of%20samples.%20Current%20methodologies%20often%20rely%20on%20the%0Asmall-loss%20hypothesis%20or%20feature-based%20selection%20to%20separate%20noisy-%20and%0Aclean-label%20samples%2C%20yet%20our%20empirical%20observations%20reveal%20their%20limitations%2C%0Aespecially%20for%20labels%20with%20instance%20dependent%20noise%20%28IDN%29.%20An%20important%0Acharacteristic%20of%20IDN%20is%20the%20difficulty%20to%20distinguish%20the%20clean-label%20samples%0Athat%20lie%20near%20the%20decision%20boundary%20%28i.e.%2C%20the%20hard%20samples%29%20from%20the%0Anoisy-label%20samples.%20We%2C%20therefore%2C%20propose%20a%20new%20noisy-label%20detection%20method%2C%0Atermed%20Peer-Agreement%20based%20Sample%20Selection%20%28PASS%29%2C%20to%20address%20this%20problem.%0AUtilising%20a%20trio%20of%20classifiers%2C%20PASS%20employs%20consensus-driven%20peer-based%0Aagreement%20of%20two%20models%20to%20select%20the%20samples%20to%20train%20the%20remaining%20model.%0APASS%20is%20easily%20integrated%20into%20existing%20LNL%20models%2C%20enabling%20the%20improvement%20of%0Athe%20detection%20accuracy%20of%20noisy-%20and%20clean-label%20samples%2C%20which%20increases%20the%0Aclassification%20accuracy%20across%20various%20LNL%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10802v2&entry.124074799=Read"},
{"title": "Fairness Without Demographics in Human-Centered Federated Learning", "author": "Roy Shaily and Sharma Harshit and Salekin Asif", "abstract": "  Federated learning (FL) enables collaborative model training while preserving\ndata privacy, making it suitable for decentralized human-centered AI\napplications. However, a significant research gap remains in ensuring fairness\nin these systems. Current fairness strategies in FL require knowledge of\nbias-creating/sensitive attributes, clashing with FL's privacy principles.\nMoreover, in human-centered datasets, sensitive attributes may remain latent.\nTo tackle these challenges, we present a novel bias mitigation approach\ninspired by \"Fairness without Demographics\" in machine learning. The presented\napproach achieves fairness without needing knowledge of sensitive attributes by\nminimizing the top eigenvalue of the Hessian matrix during training, ensuring\nequitable loss landscapes across FL participants. Notably, we introduce a novel\nFL aggregation scheme that promotes participating models based on error rates\nand loss landscape curvature attributes, fostering fairness across the FL\nsystem. This work represents the first approach to attaining \"Fairness without\nDemographics\" in human-centered FL. Through comprehensive evaluation, our\napproach demonstrates effectiveness in balancing fairness and efficacy across\nvarious real-world applications, FL setups, and scenarios involving single and\nmultiple bias-inducing factors, representing a significant advancement in\nhuman-centered FL.\n", "link": "http://arxiv.org/abs/2404.19725v1", "date": "2024-04-30", "relevancy": 1.9538, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4824}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning&body=Title%3A%20Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning%0AAuthor%3A%20Roy%20Shaily%20and%20Sharma%20Harshit%20and%20Salekin%20Asif%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20while%20preserving%0Adata%20privacy%2C%20making%20it%20suitable%20for%20decentralized%20human-centered%20AI%0Aapplications.%20However%2C%20a%20significant%20research%20gap%20remains%20in%20ensuring%20fairness%0Ain%20these%20systems.%20Current%20fairness%20strategies%20in%20FL%20require%20knowledge%20of%0Abias-creating/sensitive%20attributes%2C%20clashing%20with%20FL%27s%20privacy%20principles.%0AMoreover%2C%20in%20human-centered%20datasets%2C%20sensitive%20attributes%20may%20remain%20latent.%0ATo%20tackle%20these%20challenges%2C%20we%20present%20a%20novel%20bias%20mitigation%20approach%0Ainspired%20by%20%22Fairness%20without%20Demographics%22%20in%20machine%20learning.%20The%20presented%0Aapproach%20achieves%20fairness%20without%20needing%20knowledge%20of%20sensitive%20attributes%20by%0Aminimizing%20the%20top%20eigenvalue%20of%20the%20Hessian%20matrix%20during%20training%2C%20ensuring%0Aequitable%20loss%20landscapes%20across%20FL%20participants.%20Notably%2C%20we%20introduce%20a%20novel%0AFL%20aggregation%20scheme%20that%20promotes%20participating%20models%20based%20on%20error%20rates%0Aand%20loss%20landscape%20curvature%20attributes%2C%20fostering%20fairness%20across%20the%20FL%0Asystem.%20This%20work%20represents%20the%20first%20approach%20to%20attaining%20%22Fairness%20without%0ADemographics%22%20in%20human-centered%20FL.%20Through%20comprehensive%20evaluation%2C%20our%0Aapproach%20demonstrates%20effectiveness%20in%20balancing%20fairness%20and%20efficacy%20across%0Avarious%20real-world%20applications%2C%20FL%20setups%2C%20and%20scenarios%20involving%20single%20and%0Amultiple%20bias-inducing%20factors%2C%20representing%20a%20significant%20advancement%20in%0Ahuman-centered%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19725v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Without%20Demographics%20in%20Human-Centered%20Federated%20Learning&entry.906535625=Roy%20Shaily%20and%20Sharma%20Harshit%20and%20Salekin%20Asif&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20while%20preserving%0Adata%20privacy%2C%20making%20it%20suitable%20for%20decentralized%20human-centered%20AI%0Aapplications.%20However%2C%20a%20significant%20research%20gap%20remains%20in%20ensuring%20fairness%0Ain%20these%20systems.%20Current%20fairness%20strategies%20in%20FL%20require%20knowledge%20of%0Abias-creating/sensitive%20attributes%2C%20clashing%20with%20FL%27s%20privacy%20principles.%0AMoreover%2C%20in%20human-centered%20datasets%2C%20sensitive%20attributes%20may%20remain%20latent.%0ATo%20tackle%20these%20challenges%2C%20we%20present%20a%20novel%20bias%20mitigation%20approach%0Ainspired%20by%20%22Fairness%20without%20Demographics%22%20in%20machine%20learning.%20The%20presented%0Aapproach%20achieves%20fairness%20without%20needing%20knowledge%20of%20sensitive%20attributes%20by%0Aminimizing%20the%20top%20eigenvalue%20of%20the%20Hessian%20matrix%20during%20training%2C%20ensuring%0Aequitable%20loss%20landscapes%20across%20FL%20participants.%20Notably%2C%20we%20introduce%20a%20novel%0AFL%20aggregation%20scheme%20that%20promotes%20participating%20models%20based%20on%20error%20rates%0Aand%20loss%20landscape%20curvature%20attributes%2C%20fostering%20fairness%20across%20the%20FL%0Asystem.%20This%20work%20represents%20the%20first%20approach%20to%20attaining%20%22Fairness%20without%0ADemographics%22%20in%20human-centered%20FL.%20Through%20comprehensive%20evaluation%2C%20our%0Aapproach%20demonstrates%20effectiveness%20in%20balancing%20fairness%20and%20efficacy%20across%0Avarious%20real-world%20applications%2C%20FL%20setups%2C%20and%20scenarios%20involving%20single%20and%0Amultiple%20bias-inducing%20factors%2C%20representing%20a%20significant%20advancement%20in%0Ahuman-centered%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19725v1&entry.124074799=Read"},
{"title": "Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving\n  Conditional Human Image Generation", "author": "Anton Pelykh and Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Recent years have seen significant progress in human image generation,\nparticularly with the advancements in diffusion models. However, existing\ndiffusion methods encounter challenges when producing consistent hand anatomy\nand the generated images often lack precise control over the hand pose. To\naddress this limitation, we introduce a novel approach to pose-conditioned\nhuman image generation, dividing the process into two stages: hand generation\nand subsequent body outpainting around the hands. We propose training the hand\ngenerator in a multi-task setting to produce both hand images and their\ncorresponding segmentation masks, and employ the trained model in the first\nstage of generation. An adapted ControlNet model is then used in the second\nstage to outpaint the body around the generated hands, producing the final\nresult. A novel blending technique is introduced to preserve the hand details\nduring the second stage that combines the results of both stages in a coherent\nway. This involves sequential expansion of the outpainted region while fusing\nthe latent representations, to ensure a seamless and cohesive synthesis of the\nfinal image. Experimental evaluations demonstrate the superiority of our\nproposed method over state-of-the-art techniques, in both pose accuracy and\nimage quality, as validated on the HaGRID dataset. Our approach not only\nenhances the quality of the generated hands but also offers improved control\nover hand pose, advancing the capabilities of pose-conditioned human image\ngeneration. The source code of the proposed approach is available at\nhttps://github.com/apelykh/hand-to-diffusion.\n", "link": "http://arxiv.org/abs/2403.10731v2", "date": "2024-04-30", "relevancy": 1.9262, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6481}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6357}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6333}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Giving%20a%20Hand%20to%20Diffusion%20Models%3A%20a%20Two-Stage%20Approach%20to%20Improving%0A%20%20Conditional%20Human%20Image%20Generation&body=Title%3A%20Giving%20a%20Hand%20to%20Diffusion%20Models%3A%20a%20Two-Stage%20Approach%20to%20Improving%0A%20%20Conditional%20Human%20Image%20Generation%0AAuthor%3A%20Anton%20Pelykh%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20significant%20progress%20in%20human%20image%20generation%2C%0Aparticularly%20with%20the%20advancements%20in%20diffusion%20models.%20However%2C%20existing%0Adiffusion%20methods%20encounter%20challenges%20when%20producing%20consistent%20hand%20anatomy%0Aand%20the%20generated%20images%20often%20lack%20precise%20control%20over%20the%20hand%20pose.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20novel%20approach%20to%20pose-conditioned%0Ahuman%20image%20generation%2C%20dividing%20the%20process%20into%20two%20stages%3A%20hand%20generation%0Aand%20subsequent%20body%20outpainting%20around%20the%20hands.%20We%20propose%20training%20the%20hand%0Agenerator%20in%20a%20multi-task%20setting%20to%20produce%20both%20hand%20images%20and%20their%0Acorresponding%20segmentation%20masks%2C%20and%20employ%20the%20trained%20model%20in%20the%20first%0Astage%20of%20generation.%20An%20adapted%20ControlNet%20model%20is%20then%20used%20in%20the%20second%0Astage%20to%20outpaint%20the%20body%20around%20the%20generated%20hands%2C%20producing%20the%20final%0Aresult.%20A%20novel%20blending%20technique%20is%20introduced%20to%20preserve%20the%20hand%20details%0Aduring%20the%20second%20stage%20that%20combines%20the%20results%20of%20both%20stages%20in%20a%20coherent%0Away.%20This%20involves%20sequential%20expansion%20of%20the%20outpainted%20region%20while%20fusing%0Athe%20latent%20representations%2C%20to%20ensure%20a%20seamless%20and%20cohesive%20synthesis%20of%20the%0Afinal%20image.%20Experimental%20evaluations%20demonstrate%20the%20superiority%20of%20our%0Aproposed%20method%20over%20state-of-the-art%20techniques%2C%20in%20both%20pose%20accuracy%20and%0Aimage%20quality%2C%20as%20validated%20on%20the%20HaGRID%20dataset.%20Our%20approach%20not%20only%0Aenhances%20the%20quality%20of%20the%20generated%20hands%20but%20also%20offers%20improved%20control%0Aover%20hand%20pose%2C%20advancing%20the%20capabilities%20of%20pose-conditioned%20human%20image%0Ageneration.%20The%20source%20code%20of%20the%20proposed%20approach%20is%20available%20at%0Ahttps%3A//github.com/apelykh/hand-to-diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10731v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Giving%20a%20Hand%20to%20Diffusion%20Models%3A%20a%20Two-Stage%20Approach%20to%20Improving%0A%20%20Conditional%20Human%20Image%20Generation&entry.906535625=Anton%20Pelykh%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Recent%20years%20have%20seen%20significant%20progress%20in%20human%20image%20generation%2C%0Aparticularly%20with%20the%20advancements%20in%20diffusion%20models.%20However%2C%20existing%0Adiffusion%20methods%20encounter%20challenges%20when%20producing%20consistent%20hand%20anatomy%0Aand%20the%20generated%20images%20often%20lack%20precise%20control%20over%20the%20hand%20pose.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20a%20novel%20approach%20to%20pose-conditioned%0Ahuman%20image%20generation%2C%20dividing%20the%20process%20into%20two%20stages%3A%20hand%20generation%0Aand%20subsequent%20body%20outpainting%20around%20the%20hands.%20We%20propose%20training%20the%20hand%0Agenerator%20in%20a%20multi-task%20setting%20to%20produce%20both%20hand%20images%20and%20their%0Acorresponding%20segmentation%20masks%2C%20and%20employ%20the%20trained%20model%20in%20the%20first%0Astage%20of%20generation.%20An%20adapted%20ControlNet%20model%20is%20then%20used%20in%20the%20second%0Astage%20to%20outpaint%20the%20body%20around%20the%20generated%20hands%2C%20producing%20the%20final%0Aresult.%20A%20novel%20blending%20technique%20is%20introduced%20to%20preserve%20the%20hand%20details%0Aduring%20the%20second%20stage%20that%20combines%20the%20results%20of%20both%20stages%20in%20a%20coherent%0Away.%20This%20involves%20sequential%20expansion%20of%20the%20outpainted%20region%20while%20fusing%0Athe%20latent%20representations%2C%20to%20ensure%20a%20seamless%20and%20cohesive%20synthesis%20of%20the%0Afinal%20image.%20Experimental%20evaluations%20demonstrate%20the%20superiority%20of%20our%0Aproposed%20method%20over%20state-of-the-art%20techniques%2C%20in%20both%20pose%20accuracy%20and%0Aimage%20quality%2C%20as%20validated%20on%20the%20HaGRID%20dataset.%20Our%20approach%20not%20only%0Aenhances%20the%20quality%20of%20the%20generated%20hands%20but%20also%20offers%20improved%20control%0Aover%20hand%20pose%2C%20advancing%20the%20capabilities%20of%20pose-conditioned%20human%20image%0Ageneration.%20The%20source%20code%20of%20the%20proposed%20approach%20is%20available%20at%0Ahttps%3A//github.com/apelykh/hand-to-diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10731v2&entry.124074799=Read"},
{"title": "A rank decomposition for the topological classification of neural\n  representations", "author": "Kosio Beshkov and Gaute T. Einevoll", "abstract": "  Neural networks can be thought of as applying a transformation to an input\ndataset. The way in which they change the topology of such a dataset often\nholds practical significance for many tasks, particularly those demanding\nnon-homeomorphic mappings for optimal solutions, such as classification\nproblems. In this work, we leverage the fact that neural networks are\nequivalent to continuous piecewise-affine maps, whose rank can be used to\npinpoint regions in the input space that undergo non-homeomorphic\ntransformations, leading to alterations in the topological structure of the\ninput dataset. Our approach enables us to make use of the relative homology\nsequence, with which one can study the homology groups of the quotient of a\nmanifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on\nthese spaces.\n  As a proof of principle, we empirically investigate the presence of low-rank\n(topology-changing) affine maps as a function of network width and mean weight.\nWe show that in randomly initialized narrow networks, there will be regions in\nwhich the (co)homology groups of a data manifold can change. As the width\nincreases, the homology groups of the input manifold become more likely to be\npreserved. We end this part of our work by constructing highly non-random wide\nnetworks that do not have this property and relating this non-random regime to\nDale's principle, which is a defining characteristic of biological neural\nnetworks.\n  Finally, we study simple feedforward networks trained on MNIST, as well as on\ntoy classification and regression tasks, and show that networks manipulate the\ntopology of data differently depending on the continuity of the task they are\ntrained on.\n", "link": "http://arxiv.org/abs/2404.19710v1", "date": "2024-04-30", "relevancy": 1.9223, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4742}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20rank%20decomposition%20for%20the%20topological%20classification%20of%20neural%0A%20%20representations&body=Title%3A%20A%20rank%20decomposition%20for%20the%20topological%20classification%20of%20neural%0A%20%20representations%0AAuthor%3A%20Kosio%20Beshkov%20and%20Gaute%20T.%20Einevoll%0AAbstract%3A%20%20%20Neural%20networks%20can%20be%20thought%20of%20as%20applying%20a%20transformation%20to%20an%20input%0Adataset.%20The%20way%20in%20which%20they%20change%20the%20topology%20of%20such%20a%20dataset%20often%0Aholds%20practical%20significance%20for%20many%20tasks%2C%20particularly%20those%20demanding%0Anon-homeomorphic%20mappings%20for%20optimal%20solutions%2C%20such%20as%20classification%0Aproblems.%20In%20this%20work%2C%20we%20leverage%20the%20fact%20that%20neural%20networks%20are%0Aequivalent%20to%20continuous%20piecewise-affine%20maps%2C%20whose%20rank%20can%20be%20used%20to%0Apinpoint%20regions%20in%20the%20input%20space%20that%20undergo%20non-homeomorphic%0Atransformations%2C%20leading%20to%20alterations%20in%20the%20topological%20structure%20of%20the%0Ainput%20dataset.%20Our%20approach%20enables%20us%20to%20make%20use%20of%20the%20relative%20homology%0Asequence%2C%20with%20which%20one%20can%20study%20the%20homology%20groups%20of%20the%20quotient%20of%20a%0Amanifold%20%24%5Cmathcal%7BM%7D%24%20and%20a%20subset%20%24A%24%2C%20assuming%20some%20minimal%20properties%20on%0Athese%20spaces.%0A%20%20As%20a%20proof%20of%20principle%2C%20we%20empirically%20investigate%20the%20presence%20of%20low-rank%0A%28topology-changing%29%20affine%20maps%20as%20a%20function%20of%20network%20width%20and%20mean%20weight.%0AWe%20show%20that%20in%20randomly%20initialized%20narrow%20networks%2C%20there%20will%20be%20regions%20in%0Awhich%20the%20%28co%29homology%20groups%20of%20a%20data%20manifold%20can%20change.%20As%20the%20width%0Aincreases%2C%20the%20homology%20groups%20of%20the%20input%20manifold%20become%20more%20likely%20to%20be%0Apreserved.%20We%20end%20this%20part%20of%20our%20work%20by%20constructing%20highly%20non-random%20wide%0Anetworks%20that%20do%20not%20have%20this%20property%20and%20relating%20this%20non-random%20regime%20to%0ADale%27s%20principle%2C%20which%20is%20a%20defining%20characteristic%20of%20biological%20neural%0Anetworks.%0A%20%20Finally%2C%20we%20study%20simple%20feedforward%20networks%20trained%20on%20MNIST%2C%20as%20well%20as%20on%0Atoy%20classification%20and%20regression%20tasks%2C%20and%20show%20that%20networks%20manipulate%20the%0Atopology%20of%20data%20differently%20depending%20on%20the%20continuity%20of%20the%20task%20they%20are%0Atrained%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19710v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20rank%20decomposition%20for%20the%20topological%20classification%20of%20neural%0A%20%20representations&entry.906535625=Kosio%20Beshkov%20and%20Gaute%20T.%20Einevoll&entry.1292438233=%20%20Neural%20networks%20can%20be%20thought%20of%20as%20applying%20a%20transformation%20to%20an%20input%0Adataset.%20The%20way%20in%20which%20they%20change%20the%20topology%20of%20such%20a%20dataset%20often%0Aholds%20practical%20significance%20for%20many%20tasks%2C%20particularly%20those%20demanding%0Anon-homeomorphic%20mappings%20for%20optimal%20solutions%2C%20such%20as%20classification%0Aproblems.%20In%20this%20work%2C%20we%20leverage%20the%20fact%20that%20neural%20networks%20are%0Aequivalent%20to%20continuous%20piecewise-affine%20maps%2C%20whose%20rank%20can%20be%20used%20to%0Apinpoint%20regions%20in%20the%20input%20space%20that%20undergo%20non-homeomorphic%0Atransformations%2C%20leading%20to%20alterations%20in%20the%20topological%20structure%20of%20the%0Ainput%20dataset.%20Our%20approach%20enables%20us%20to%20make%20use%20of%20the%20relative%20homology%0Asequence%2C%20with%20which%20one%20can%20study%20the%20homology%20groups%20of%20the%20quotient%20of%20a%0Amanifold%20%24%5Cmathcal%7BM%7D%24%20and%20a%20subset%20%24A%24%2C%20assuming%20some%20minimal%20properties%20on%0Athese%20spaces.%0A%20%20As%20a%20proof%20of%20principle%2C%20we%20empirically%20investigate%20the%20presence%20of%20low-rank%0A%28topology-changing%29%20affine%20maps%20as%20a%20function%20of%20network%20width%20and%20mean%20weight.%0AWe%20show%20that%20in%20randomly%20initialized%20narrow%20networks%2C%20there%20will%20be%20regions%20in%0Awhich%20the%20%28co%29homology%20groups%20of%20a%20data%20manifold%20can%20change.%20As%20the%20width%0Aincreases%2C%20the%20homology%20groups%20of%20the%20input%20manifold%20become%20more%20likely%20to%20be%0Apreserved.%20We%20end%20this%20part%20of%20our%20work%20by%20constructing%20highly%20non-random%20wide%0Anetworks%20that%20do%20not%20have%20this%20property%20and%20relating%20this%20non-random%20regime%20to%0ADale%27s%20principle%2C%20which%20is%20a%20defining%20characteristic%20of%20biological%20neural%0Anetworks.%0A%20%20Finally%2C%20we%20study%20simple%20feedforward%20networks%20trained%20on%20MNIST%2C%20as%20well%20as%20on%0Atoy%20classification%20and%20regression%20tasks%2C%20and%20show%20that%20networks%20manipulate%20the%0Atopology%20of%20data%20differently%20depending%20on%20the%20continuity%20of%20the%20task%20they%20are%0Atrained%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19710v1&entry.124074799=Read"},
{"title": "Inductive biases in deep learning models for weather prediction", "author": "Jannik Thuemmel and Matthias Karlbauer and Sebastian Otte and Christiane Zarfl and Georg Martius and Nicole Ludwig and Thomas Scholten and Ulrich Friedrich and Volker Wulfmeyer and Bedartha Goswami and Martin V. Butz", "abstract": "  Deep learning has gained immense popularity in the Earth sciences as it\nenables us to formulate purely data-driven models of complex Earth system\nprocesses. Deep learning-based weather prediction (DLWP) models have made\nsignificant progress in the last few years, achieving forecast skills\ncomparable to established numerical weather prediction models with\ncomparatively lesser computational costs. In order to train accurate, reliable,\nand tractable DLWP models with several millions of parameters, the model design\nneeds to incorporate suitable inductive biases that encode structural\nassumptions about the data and the modelled processes. When chosen\nappropriately, these biases enable faster learning and better generalisation to\nunseen data. Although inductive biases play a crucial role in successful DLWP\nmodels, they are often not stated explicitly and their contribution to model\nperformance remains unclear. Here, we review and analyse the inductive biases\nof state-of-the-art DLWP models with respect to five key design elements: data\nselection, learning objective, loss function, architecture, and optimisation\nmethod. We identify the most important inductive biases and highlight potential\navenues towards more efficient and probabilistic DLWP models.\n", "link": "http://arxiv.org/abs/2304.04664v2", "date": "2024-04-30", "relevancy": 1.9012, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4673}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inductive%20biases%20in%20deep%20learning%20models%20for%20weather%20prediction&body=Title%3A%20Inductive%20biases%20in%20deep%20learning%20models%20for%20weather%20prediction%0AAuthor%3A%20Jannik%20Thuemmel%20and%20Matthias%20Karlbauer%20and%20Sebastian%20Otte%20and%20Christiane%20Zarfl%20and%20Georg%20Martius%20and%20Nicole%20Ludwig%20and%20Thomas%20Scholten%20and%20Ulrich%20Friedrich%20and%20Volker%20Wulfmeyer%20and%20Bedartha%20Goswami%20and%20Martin%20V.%20Butz%0AAbstract%3A%20%20%20Deep%20learning%20has%20gained%20immense%20popularity%20in%20the%20Earth%20sciences%20as%20it%0Aenables%20us%20to%20formulate%20purely%20data-driven%20models%20of%20complex%20Earth%20system%0Aprocesses.%20Deep%20learning-based%20weather%20prediction%20%28DLWP%29%20models%20have%20made%0Asignificant%20progress%20in%20the%20last%20few%20years%2C%20achieving%20forecast%20skills%0Acomparable%20to%20established%20numerical%20weather%20prediction%20models%20with%0Acomparatively%20lesser%20computational%20costs.%20In%20order%20to%20train%20accurate%2C%20reliable%2C%0Aand%20tractable%20DLWP%20models%20with%20several%20millions%20of%20parameters%2C%20the%20model%20design%0Aneeds%20to%20incorporate%20suitable%20inductive%20biases%20that%20encode%20structural%0Aassumptions%20about%20the%20data%20and%20the%20modelled%20processes.%20When%20chosen%0Aappropriately%2C%20these%20biases%20enable%20faster%20learning%20and%20better%20generalisation%20to%0Aunseen%20data.%20Although%20inductive%20biases%20play%20a%20crucial%20role%20in%20successful%20DLWP%0Amodels%2C%20they%20are%20often%20not%20stated%20explicitly%20and%20their%20contribution%20to%20model%0Aperformance%20remains%20unclear.%20Here%2C%20we%20review%20and%20analyse%20the%20inductive%20biases%0Aof%20state-of-the-art%20DLWP%20models%20with%20respect%20to%20five%20key%20design%20elements%3A%20data%0Aselection%2C%20learning%20objective%2C%20loss%20function%2C%20architecture%2C%20and%20optimisation%0Amethod.%20We%20identify%20the%20most%20important%20inductive%20biases%20and%20highlight%20potential%0Aavenues%20towards%20more%20efficient%20and%20probabilistic%20DLWP%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.04664v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inductive%20biases%20in%20deep%20learning%20models%20for%20weather%20prediction&entry.906535625=Jannik%20Thuemmel%20and%20Matthias%20Karlbauer%20and%20Sebastian%20Otte%20and%20Christiane%20Zarfl%20and%20Georg%20Martius%20and%20Nicole%20Ludwig%20and%20Thomas%20Scholten%20and%20Ulrich%20Friedrich%20and%20Volker%20Wulfmeyer%20and%20Bedartha%20Goswami%20and%20Martin%20V.%20Butz&entry.1292438233=%20%20Deep%20learning%20has%20gained%20immense%20popularity%20in%20the%20Earth%20sciences%20as%20it%0Aenables%20us%20to%20formulate%20purely%20data-driven%20models%20of%20complex%20Earth%20system%0Aprocesses.%20Deep%20learning-based%20weather%20prediction%20%28DLWP%29%20models%20have%20made%0Asignificant%20progress%20in%20the%20last%20few%20years%2C%20achieving%20forecast%20skills%0Acomparable%20to%20established%20numerical%20weather%20prediction%20models%20with%0Acomparatively%20lesser%20computational%20costs.%20In%20order%20to%20train%20accurate%2C%20reliable%2C%0Aand%20tractable%20DLWP%20models%20with%20several%20millions%20of%20parameters%2C%20the%20model%20design%0Aneeds%20to%20incorporate%20suitable%20inductive%20biases%20that%20encode%20structural%0Aassumptions%20about%20the%20data%20and%20the%20modelled%20processes.%20When%20chosen%0Aappropriately%2C%20these%20biases%20enable%20faster%20learning%20and%20better%20generalisation%20to%0Aunseen%20data.%20Although%20inductive%20biases%20play%20a%20crucial%20role%20in%20successful%20DLWP%0Amodels%2C%20they%20are%20often%20not%20stated%20explicitly%20and%20their%20contribution%20to%20model%0Aperformance%20remains%20unclear.%20Here%2C%20we%20review%20and%20analyse%20the%20inductive%20biases%0Aof%20state-of-the-art%20DLWP%20models%20with%20respect%20to%20five%20key%20design%20elements%3A%20data%0Aselection%2C%20learning%20objective%2C%20loss%20function%2C%20architecture%2C%20and%20optimisation%0Amethod.%20We%20identify%20the%20most%20important%20inductive%20biases%20and%20highlight%20potential%0Aavenues%20towards%20more%20efficient%20and%20probabilistic%20DLWP%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.04664v2&entry.124074799=Read"},
{"title": "Balancing Spectral, Temporal and Spatial Information for EEG-based\n  Alzheimer's Disease Classification", "author": "Stephan Goerttler and Fei He and Min Wu", "abstract": "  The prospect of future treatment warrants the development of cost-effective\nscreening for Alzheimer's disease (AD). A promising candidate in this regard is\nelectroencephalography (EEG), as it is one of the most economic imaging\nmodalities. Recent efforts in EEG analysis have shifted towards leveraging\nspatial information, employing novel frameworks such as graph signal processing\nor graph neural networks. Here, we investigate the importance of spatial\ninformation relative to spectral or temporal information by varying the\nproportion of each dimension for AD classification. To do so, we systematically\ntest various dimension resolution configurations on two routine EEG datasets.\nOur findings show that spatial information is more important than temporal\ninformation and equally valuable as spectral information. On the larger second\ndataset, substituting spectral with spatial information even led to an increase\nof 1.1% in accuracy, which emphasises the importance of spatial information for\nEEG-based AD classification. We argue that our resolution-based feature\nextraction has the potential to improve AD classification specifically, and\nmultivariate signal classification generally.\n", "link": "http://arxiv.org/abs/2402.13523v2", "date": "2024-04-30", "relevancy": 1.8964, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4855}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4629}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Balancing%20Spectral%2C%20Temporal%20and%20Spatial%20Information%20for%20EEG-based%0A%20%20Alzheimer%27s%20Disease%20Classification&body=Title%3A%20Balancing%20Spectral%2C%20Temporal%20and%20Spatial%20Information%20for%20EEG-based%0A%20%20Alzheimer%27s%20Disease%20Classification%0AAuthor%3A%20Stephan%20Goerttler%20and%20Fei%20He%20and%20Min%20Wu%0AAbstract%3A%20%20%20The%20prospect%20of%20future%20treatment%20warrants%20the%20development%20of%20cost-effective%0Ascreening%20for%20Alzheimer%27s%20disease%20%28AD%29.%20A%20promising%20candidate%20in%20this%20regard%20is%0Aelectroencephalography%20%28EEG%29%2C%20as%20it%20is%20one%20of%20the%20most%20economic%20imaging%0Amodalities.%20Recent%20efforts%20in%20EEG%20analysis%20have%20shifted%20towards%20leveraging%0Aspatial%20information%2C%20employing%20novel%20frameworks%20such%20as%20graph%20signal%20processing%0Aor%20graph%20neural%20networks.%20Here%2C%20we%20investigate%20the%20importance%20of%20spatial%0Ainformation%20relative%20to%20spectral%20or%20temporal%20information%20by%20varying%20the%0Aproportion%20of%20each%20dimension%20for%20AD%20classification.%20To%20do%20so%2C%20we%20systematically%0Atest%20various%20dimension%20resolution%20configurations%20on%20two%20routine%20EEG%20datasets.%0AOur%20findings%20show%20that%20spatial%20information%20is%20more%20important%20than%20temporal%0Ainformation%20and%20equally%20valuable%20as%20spectral%20information.%20On%20the%20larger%20second%0Adataset%2C%20substituting%20spectral%20with%20spatial%20information%20even%20led%20to%20an%20increase%0Aof%201.1%25%20in%20accuracy%2C%20which%20emphasises%20the%20importance%20of%20spatial%20information%20for%0AEEG-based%20AD%20classification.%20We%20argue%20that%20our%20resolution-based%20feature%0Aextraction%20has%20the%20potential%20to%20improve%20AD%20classification%20specifically%2C%20and%0Amultivariate%20signal%20classification%20generally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13523v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Spectral%2C%20Temporal%20and%20Spatial%20Information%20for%20EEG-based%0A%20%20Alzheimer%27s%20Disease%20Classification&entry.906535625=Stephan%20Goerttler%20and%20Fei%20He%20and%20Min%20Wu&entry.1292438233=%20%20The%20prospect%20of%20future%20treatment%20warrants%20the%20development%20of%20cost-effective%0Ascreening%20for%20Alzheimer%27s%20disease%20%28AD%29.%20A%20promising%20candidate%20in%20this%20regard%20is%0Aelectroencephalography%20%28EEG%29%2C%20as%20it%20is%20one%20of%20the%20most%20economic%20imaging%0Amodalities.%20Recent%20efforts%20in%20EEG%20analysis%20have%20shifted%20towards%20leveraging%0Aspatial%20information%2C%20employing%20novel%20frameworks%20such%20as%20graph%20signal%20processing%0Aor%20graph%20neural%20networks.%20Here%2C%20we%20investigate%20the%20importance%20of%20spatial%0Ainformation%20relative%20to%20spectral%20or%20temporal%20information%20by%20varying%20the%0Aproportion%20of%20each%20dimension%20for%20AD%20classification.%20To%20do%20so%2C%20we%20systematically%0Atest%20various%20dimension%20resolution%20configurations%20on%20two%20routine%20EEG%20datasets.%0AOur%20findings%20show%20that%20spatial%20information%20is%20more%20important%20than%20temporal%0Ainformation%20and%20equally%20valuable%20as%20spectral%20information.%20On%20the%20larger%20second%0Adataset%2C%20substituting%20spectral%20with%20spatial%20information%20even%20led%20to%20an%20increase%0Aof%201.1%25%20in%20accuracy%2C%20which%20emphasises%20the%20importance%20of%20spatial%20information%20for%0AEEG-based%20AD%20classification.%20We%20argue%20that%20our%20resolution-based%20feature%0Aextraction%20has%20the%20potential%20to%20improve%20AD%20classification%20specifically%2C%20and%0Amultivariate%20signal%20classification%20generally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13523v2&entry.124074799=Read"},
{"title": "Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines", "author": "Ekaterina Trofimova and Emil Sataev and Andrey E. Ustyuzhanin", "abstract": "  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n", "link": "http://arxiv.org/abs/2403.11585v2", "date": "2024-04-30", "relevancy": 1.8936, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4821}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4693}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Linguacodus%3A%20A%20Synergistic%20Framework%20for%20Transformative%20Code%20Generation%0A%20%20in%20Machine%20Learning%20Pipelines&body=Title%3A%20Linguacodus%3A%20A%20Synergistic%20Framework%20for%20Transformative%20Code%20Generation%0A%20%20in%20Machine%20Learning%20Pipelines%0AAuthor%3A%20Ekaterina%20Trofimova%20and%20Emil%20Sataev%20and%20Andrey%20E.%20Ustyuzhanin%0AAbstract%3A%20%20%20In%20the%20ever-evolving%20landscape%20of%20machine%20learning%2C%20seamless%20translation%20of%0Anatural%20language%20descriptions%20into%20executable%20code%20remains%20a%20formidable%0Achallenge.%20This%20paper%20introduces%20Linguacodus%2C%20an%20innovative%20framework%20designed%0Ato%20tackle%20this%20challenge%20by%20deploying%20a%20dynamic%20pipeline%20that%20iteratively%0Atransforms%20natural%20language%20task%20descriptions%20into%20code%20through%20high-level%0Adata-shaping%20instructions.%20The%20core%20of%20Linguacodus%20is%20a%20fine-tuned%20large%0Alanguage%20model%20%28LLM%29%2C%20empowered%20to%20evaluate%20diverse%20solutions%20for%20various%0Aproblems%20and%20select%20the%20most%20fitting%20one%20for%20a%20given%20task.%20This%20paper%20details%0Athe%20fine-tuning%20process%2C%20and%20sheds%20light%20on%20how%20natural%20language%20descriptions%0Acan%20be%20translated%20into%20functional%20code.%20Linguacodus%20represents%20a%20substantial%0Aleap%20towards%20automated%20code%20generation%2C%20effectively%20bridging%20the%20gap%20between%0Atask%20descriptions%20and%20executable%20code.%20It%20holds%20great%20promise%20for%20advancing%0Amachine%20learning%20applications%20across%20diverse%20domains.%20Additionally%2C%20we%20propose%0Aan%20algorithm%20capable%20of%20transforming%20a%20natural%20description%20of%20an%20ML%20task%20into%0Acode%20with%20minimal%20human%20interaction.%20In%20extensive%20experiments%20on%20a%20vast%20machine%0Alearning%20code%20dataset%20originating%20from%20Kaggle%2C%20we%20showcase%20the%20effectiveness%20of%0ALinguacodus.%20The%20investigations%20highlight%20its%20potential%20applications%20across%0Adiverse%20domains%2C%20emphasizing%20its%20impact%20on%20applied%20machine%20learning%20in%20various%0Ascientific%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11585v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linguacodus%3A%20A%20Synergistic%20Framework%20for%20Transformative%20Code%20Generation%0A%20%20in%20Machine%20Learning%20Pipelines&entry.906535625=Ekaterina%20Trofimova%20and%20Emil%20Sataev%20and%20Andrey%20E.%20Ustyuzhanin&entry.1292438233=%20%20In%20the%20ever-evolving%20landscape%20of%20machine%20learning%2C%20seamless%20translation%20of%0Anatural%20language%20descriptions%20into%20executable%20code%20remains%20a%20formidable%0Achallenge.%20This%20paper%20introduces%20Linguacodus%2C%20an%20innovative%20framework%20designed%0Ato%20tackle%20this%20challenge%20by%20deploying%20a%20dynamic%20pipeline%20that%20iteratively%0Atransforms%20natural%20language%20task%20descriptions%20into%20code%20through%20high-level%0Adata-shaping%20instructions.%20The%20core%20of%20Linguacodus%20is%20a%20fine-tuned%20large%0Alanguage%20model%20%28LLM%29%2C%20empowered%20to%20evaluate%20diverse%20solutions%20for%20various%0Aproblems%20and%20select%20the%20most%20fitting%20one%20for%20a%20given%20task.%20This%20paper%20details%0Athe%20fine-tuning%20process%2C%20and%20sheds%20light%20on%20how%20natural%20language%20descriptions%0Acan%20be%20translated%20into%20functional%20code.%20Linguacodus%20represents%20a%20substantial%0Aleap%20towards%20automated%20code%20generation%2C%20effectively%20bridging%20the%20gap%20between%0Atask%20descriptions%20and%20executable%20code.%20It%20holds%20great%20promise%20for%20advancing%0Amachine%20learning%20applications%20across%20diverse%20domains.%20Additionally%2C%20we%20propose%0Aan%20algorithm%20capable%20of%20transforming%20a%20natural%20description%20of%20an%20ML%20task%20into%0Acode%20with%20minimal%20human%20interaction.%20In%20extensive%20experiments%20on%20a%20vast%20machine%0Alearning%20code%20dataset%20originating%20from%20Kaggle%2C%20we%20showcase%20the%20effectiveness%20of%0ALinguacodus.%20The%20investigations%20highlight%20its%20potential%20applications%20across%0Adiverse%20domains%2C%20emphasizing%20its%20impact%20on%20applied%20machine%20learning%20in%20various%0Ascientific%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11585v2&entry.124074799=Read"},
{"title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious\n  Correlation", "author": "Min Zhang and Haoxuan Li and Fei Wu and Kun Kuang", "abstract": "  Out-of-distribution (OOD) problems in few-shot classification (FSC) occur\nwhen novel classes sampled from testing distributions differ from base classes\ndrawn from training distributions, which considerably degrades the performance\nof deep learning models deployed in real-world applications. Recent studies\nsuggest that the OOD problems in FSC mainly including: (a) cross-domain\nfew-shot classification (CD-FSC) and (b) spurious-correlation few-shot\nclassification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns\ntransferring knowledge from base classes drawn from seen training distributions\nbut recognizes novel classes sampled from unseen testing distributions. In\ncontrast, SC-FSC arises when a classifier relies on non-causal features (or\ncontexts) that happen to be correlated with the labels (or concepts) in base\nclasses but such relationships no longer hold during the model deployment.\nDespite CD-FSC has been extensively studied, SC-FSC remains understudied due to\nlack of the corresponding evaluation benchmarks. To this end, we present Meta\nConcept Context (MetaCoCo), a benchmark with spurious-correlation shifts\ncollected from real-world scenarios. Moreover, to quantify the extent of\nspurious-correlation shifts of the presented MetaCoCo, we further propose a\nmetric by using CLIP as a pre-trained vision-language model. Extensive\nexperiments on the proposed benchmark are performed to evaluate the\nstate-of-the-art methods in FSC, cross-domain shifts, and self-supervised\nlearning. The experimental results show that the performance of the existing\nmethods degrades significantly in the presence of spurious-correlation shifts.\nWe open-source all codes of our benchmark and hope that the proposed MetaCoCo\ncan facilitate future research on spurious-correlation shifts problems in FSC.\nThe code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24.\n", "link": "http://arxiv.org/abs/2404.19644v1", "date": "2024-04-30", "relevancy": 1.8932, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4799}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MetaCoCo%3A%20A%20New%20Few-Shot%20Classification%20Benchmark%20with%20Spurious%0A%20%20Correlation&body=Title%3A%20MetaCoCo%3A%20A%20New%20Few-Shot%20Classification%20Benchmark%20with%20Spurious%0A%20%20Correlation%0AAuthor%3A%20Min%20Zhang%20and%20Haoxuan%20Li%20and%20Fei%20Wu%20and%20Kun%20Kuang%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20problems%20in%20few-shot%20classification%20%28FSC%29%20occur%0Awhen%20novel%20classes%20sampled%20from%20testing%20distributions%20differ%20from%20base%20classes%0Adrawn%20from%20training%20distributions%2C%20which%20considerably%20degrades%20the%20performance%0Aof%20deep%20learning%20models%20deployed%20in%20real-world%20applications.%20Recent%20studies%0Asuggest%20that%20the%20OOD%20problems%20in%20FSC%20mainly%20including%3A%20%28a%29%20cross-domain%0Afew-shot%20classification%20%28CD-FSC%29%20and%20%28b%29%20spurious-correlation%20few-shot%0Aclassification%20%28SC-FSC%29.%20Specifically%2C%20CD-FSC%20occurs%20when%20a%20classifier%20learns%0Atransferring%20knowledge%20from%20base%20classes%20drawn%20from%20seen%20training%20distributions%0Abut%20recognizes%20novel%20classes%20sampled%20from%20unseen%20testing%20distributions.%20In%0Acontrast%2C%20SC-FSC%20arises%20when%20a%20classifier%20relies%20on%20non-causal%20features%20%28or%0Acontexts%29%20that%20happen%20to%20be%20correlated%20with%20the%20labels%20%28or%20concepts%29%20in%20base%0Aclasses%20but%20such%20relationships%20no%20longer%20hold%20during%20the%20model%20deployment.%0ADespite%20CD-FSC%20has%20been%20extensively%20studied%2C%20SC-FSC%20remains%20understudied%20due%20to%0Alack%20of%20the%20corresponding%20evaluation%20benchmarks.%20To%20this%20end%2C%20we%20present%20Meta%0AConcept%20Context%20%28MetaCoCo%29%2C%20a%20benchmark%20with%20spurious-correlation%20shifts%0Acollected%20from%20real-world%20scenarios.%20Moreover%2C%20to%20quantify%20the%20extent%20of%0Aspurious-correlation%20shifts%20of%20the%20presented%20MetaCoCo%2C%20we%20further%20propose%20a%0Ametric%20by%20using%20CLIP%20as%20a%20pre-trained%20vision-language%20model.%20Extensive%0Aexperiments%20on%20the%20proposed%20benchmark%20are%20performed%20to%20evaluate%20the%0Astate-of-the-art%20methods%20in%20FSC%2C%20cross-domain%20shifts%2C%20and%20self-supervised%0Alearning.%20The%20experimental%20results%20show%20that%20the%20performance%20of%20the%20existing%0Amethods%20degrades%20significantly%20in%20the%20presence%20of%20spurious-correlation%20shifts.%0AWe%20open-source%20all%20codes%20of%20our%20benchmark%20and%20hope%20that%20the%20proposed%20MetaCoCo%0Acan%20facilitate%20future%20research%20on%20spurious-correlation%20shifts%20problems%20in%20FSC.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/remiMZ/MetaCoCo-ICLR24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19644v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaCoCo%3A%20A%20New%20Few-Shot%20Classification%20Benchmark%20with%20Spurious%0A%20%20Correlation&entry.906535625=Min%20Zhang%20and%20Haoxuan%20Li%20and%20Fei%20Wu%20and%20Kun%20Kuang&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20problems%20in%20few-shot%20classification%20%28FSC%29%20occur%0Awhen%20novel%20classes%20sampled%20from%20testing%20distributions%20differ%20from%20base%20classes%0Adrawn%20from%20training%20distributions%2C%20which%20considerably%20degrades%20the%20performance%0Aof%20deep%20learning%20models%20deployed%20in%20real-world%20applications.%20Recent%20studies%0Asuggest%20that%20the%20OOD%20problems%20in%20FSC%20mainly%20including%3A%20%28a%29%20cross-domain%0Afew-shot%20classification%20%28CD-FSC%29%20and%20%28b%29%20spurious-correlation%20few-shot%0Aclassification%20%28SC-FSC%29.%20Specifically%2C%20CD-FSC%20occurs%20when%20a%20classifier%20learns%0Atransferring%20knowledge%20from%20base%20classes%20drawn%20from%20seen%20training%20distributions%0Abut%20recognizes%20novel%20classes%20sampled%20from%20unseen%20testing%20distributions.%20In%0Acontrast%2C%20SC-FSC%20arises%20when%20a%20classifier%20relies%20on%20non-causal%20features%20%28or%0Acontexts%29%20that%20happen%20to%20be%20correlated%20with%20the%20labels%20%28or%20concepts%29%20in%20base%0Aclasses%20but%20such%20relationships%20no%20longer%20hold%20during%20the%20model%20deployment.%0ADespite%20CD-FSC%20has%20been%20extensively%20studied%2C%20SC-FSC%20remains%20understudied%20due%20to%0Alack%20of%20the%20corresponding%20evaluation%20benchmarks.%20To%20this%20end%2C%20we%20present%20Meta%0AConcept%20Context%20%28MetaCoCo%29%2C%20a%20benchmark%20with%20spurious-correlation%20shifts%0Acollected%20from%20real-world%20scenarios.%20Moreover%2C%20to%20quantify%20the%20extent%20of%0Aspurious-correlation%20shifts%20of%20the%20presented%20MetaCoCo%2C%20we%20further%20propose%20a%0Ametric%20by%20using%20CLIP%20as%20a%20pre-trained%20vision-language%20model.%20Extensive%0Aexperiments%20on%20the%20proposed%20benchmark%20are%20performed%20to%20evaluate%20the%0Astate-of-the-art%20methods%20in%20FSC%2C%20cross-domain%20shifts%2C%20and%20self-supervised%0Alearning.%20The%20experimental%20results%20show%20that%20the%20performance%20of%20the%20existing%0Amethods%20degrades%20significantly%20in%20the%20presence%20of%20spurious-correlation%20shifts.%0AWe%20open-source%20all%20codes%20of%20our%20benchmark%20and%20hope%20that%20the%20proposed%20MetaCoCo%0Acan%20facilitate%20future%20research%20on%20spurious-correlation%20shifts%20problems%20in%20FSC.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/remiMZ/MetaCoCo-ICLR24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19644v1&entry.124074799=Read"},
{"title": "Imitation Learning: A Survey of Learning Methods, Environments and\n  Metrics", "author": "Nathan Gavenski and Odinaldo Rodrigues and Michael Luck", "abstract": "  Imitation learning is an approach in which an agent learns how to execute a\ntask by trying to mimic how one or more teachers perform it. This learning\napproach offers a compromise between the time it takes to learn a new task and\nthe effort needed to collect teacher samples for the agent. It achieves this by\nbalancing learning from the teacher, who has some information on how to perform\nthe task, and deviating from their examples when necessary, such as states not\npresent in the teacher samples. Consequently, the field of imitation learning\nhas received much attention from researchers in recent years, resulting in many\nnew methods and applications. However, with this increase in published work and\npast surveys focusing mainly on methodology, a lack of standardisation became\nmore prominent in the field. This non-standardisation is evident in the use of\nenvironments, which appear in no more than two works, and evaluation processes,\nsuch as qualitative analysis, that have become rare in current literature. In\nthis survey, we systematically review current imitation learning literature and\npresent our findings by (i) classifying imitation learning techniques,\nenvironments and metrics by introducing novel taxonomies; (ii) reflecting on\nmain problems from the literature; and (iii) presenting challenges and future\ndirections for researchers.\n", "link": "http://arxiv.org/abs/2404.19456v1", "date": "2024-04-30", "relevancy": 1.8873, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4967}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4598}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4517}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%3A%20A%20Survey%20of%20Learning%20Methods%2C%20Environments%20and%0A%20%20Metrics&body=Title%3A%20Imitation%20Learning%3A%20A%20Survey%20of%20Learning%20Methods%2C%20Environments%20and%0A%20%20Metrics%0AAuthor%3A%20Nathan%20Gavenski%20and%20Odinaldo%20Rodrigues%20and%20Michael%20Luck%0AAbstract%3A%20%20%20Imitation%20learning%20is%20an%20approach%20in%20which%20an%20agent%20learns%20how%20to%20execute%20a%0Atask%20by%20trying%20to%20mimic%20how%20one%20or%20more%20teachers%20perform%20it.%20This%20learning%0Aapproach%20offers%20a%20compromise%20between%20the%20time%20it%20takes%20to%20learn%20a%20new%20task%20and%0Athe%20effort%20needed%20to%20collect%20teacher%20samples%20for%20the%20agent.%20It%20achieves%20this%20by%0Abalancing%20learning%20from%20the%20teacher%2C%20who%20has%20some%20information%20on%20how%20to%20perform%0Athe%20task%2C%20and%20deviating%20from%20their%20examples%20when%20necessary%2C%20such%20as%20states%20not%0Apresent%20in%20the%20teacher%20samples.%20Consequently%2C%20the%20field%20of%20imitation%20learning%0Ahas%20received%20much%20attention%20from%20researchers%20in%20recent%20years%2C%20resulting%20in%20many%0Anew%20methods%20and%20applications.%20However%2C%20with%20this%20increase%20in%20published%20work%20and%0Apast%20surveys%20focusing%20mainly%20on%20methodology%2C%20a%20lack%20of%20standardisation%20became%0Amore%20prominent%20in%20the%20field.%20This%20non-standardisation%20is%20evident%20in%20the%20use%20of%0Aenvironments%2C%20which%20appear%20in%20no%20more%20than%20two%20works%2C%20and%20evaluation%20processes%2C%0Asuch%20as%20qualitative%20analysis%2C%20that%20have%20become%20rare%20in%20current%20literature.%20In%0Athis%20survey%2C%20we%20systematically%20review%20current%20imitation%20learning%20literature%20and%0Apresent%20our%20findings%20by%20%28i%29%20classifying%20imitation%20learning%20techniques%2C%0Aenvironments%20and%20metrics%20by%20introducing%20novel%20taxonomies%3B%20%28ii%29%20reflecting%20on%0Amain%20problems%20from%20the%20literature%3B%20and%20%28iii%29%20presenting%20challenges%20and%20future%0Adirections%20for%20researchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19456v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%3A%20A%20Survey%20of%20Learning%20Methods%2C%20Environments%20and%0A%20%20Metrics&entry.906535625=Nathan%20Gavenski%20and%20Odinaldo%20Rodrigues%20and%20Michael%20Luck&entry.1292438233=%20%20Imitation%20learning%20is%20an%20approach%20in%20which%20an%20agent%20learns%20how%20to%20execute%20a%0Atask%20by%20trying%20to%20mimic%20how%20one%20or%20more%20teachers%20perform%20it.%20This%20learning%0Aapproach%20offers%20a%20compromise%20between%20the%20time%20it%20takes%20to%20learn%20a%20new%20task%20and%0Athe%20effort%20needed%20to%20collect%20teacher%20samples%20for%20the%20agent.%20It%20achieves%20this%20by%0Abalancing%20learning%20from%20the%20teacher%2C%20who%20has%20some%20information%20on%20how%20to%20perform%0Athe%20task%2C%20and%20deviating%20from%20their%20examples%20when%20necessary%2C%20such%20as%20states%20not%0Apresent%20in%20the%20teacher%20samples.%20Consequently%2C%20the%20field%20of%20imitation%20learning%0Ahas%20received%20much%20attention%20from%20researchers%20in%20recent%20years%2C%20resulting%20in%20many%0Anew%20methods%20and%20applications.%20However%2C%20with%20this%20increase%20in%20published%20work%20and%0Apast%20surveys%20focusing%20mainly%20on%20methodology%2C%20a%20lack%20of%20standardisation%20became%0Amore%20prominent%20in%20the%20field.%20This%20non-standardisation%20is%20evident%20in%20the%20use%20of%0Aenvironments%2C%20which%20appear%20in%20no%20more%20than%20two%20works%2C%20and%20evaluation%20processes%2C%0Asuch%20as%20qualitative%20analysis%2C%20that%20have%20become%20rare%20in%20current%20literature.%20In%0Athis%20survey%2C%20we%20systematically%20review%20current%20imitation%20learning%20literature%20and%0Apresent%20our%20findings%20by%20%28i%29%20classifying%20imitation%20learning%20techniques%2C%0Aenvironments%20and%20metrics%20by%20introducing%20novel%20taxonomies%3B%20%28ii%29%20reflecting%20on%0Amain%20problems%20from%20the%20literature%3B%20and%20%28iii%29%20presenting%20challenges%20and%20future%0Adirections%20for%20researchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19456v1&entry.124074799=Read"},
{"title": "CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition", "author": "Qixuan Zheng and Ming Zhang and Hong Yan", "abstract": "  To achieve greater accuracy, hypergraph matching algorithms require\nexponential increases in computational resources. Recent kd-tree-based\napproximate nearest neighbor (ANN) methods, despite the sparsity of their\ncompatibility tensor, still require exhaustive calculations for large-scale\ngraph matching. This work utilizes CUR tensor decomposition and introduces a\nnovel cascaded second and third-order hypergraph matching framework (CURSOR)\nfor efficient hypergraph matching. A CUR-based second-order graph matching\nalgorithm is used to provide a rough match, and then the core of CURSOR, a\nfiber-CUR-based tensor generation method, directly calculates entries of the\ncompatibility tensor by leveraging the initial second-order match result. This\nsignificantly decreases the time complexity and tensor density. A probability\nrelaxation labeling (PRL)-based matching algorithm, especially suitable for\nsparse tensors, is developed. Experiment results on large-scale synthetic\ndatasets and widely-adopted benchmark sets demonstrate the superiority of\nCURSOR over existing methods. The tensor generation method in CURSOR can be\nintegrated seamlessly into existing hypergraph matching methods to improve\ntheir performance and lower their computational costs.\n", "link": "http://arxiv.org/abs/2402.16594v4", "date": "2024-04-30", "relevancy": 1.8857, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4828}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4687}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CURSOR%3A%20Scalable%20Mixed-Order%20Hypergraph%20Matching%20with%20CUR%20Decomposition&body=Title%3A%20CURSOR%3A%20Scalable%20Mixed-Order%20Hypergraph%20Matching%20with%20CUR%20Decomposition%0AAuthor%3A%20Qixuan%20Zheng%20and%20Ming%20Zhang%20and%20Hong%20Yan%0AAbstract%3A%20%20%20To%20achieve%20greater%20accuracy%2C%20hypergraph%20matching%20algorithms%20require%0Aexponential%20increases%20in%20computational%20resources.%20Recent%20kd-tree-based%0Aapproximate%20nearest%20neighbor%20%28ANN%29%20methods%2C%20despite%20the%20sparsity%20of%20their%0Acompatibility%20tensor%2C%20still%20require%20exhaustive%20calculations%20for%20large-scale%0Agraph%20matching.%20This%20work%20utilizes%20CUR%20tensor%20decomposition%20and%20introduces%20a%0Anovel%20cascaded%20second%20and%20third-order%20hypergraph%20matching%20framework%20%28CURSOR%29%0Afor%20efficient%20hypergraph%20matching.%20A%20CUR-based%20second-order%20graph%20matching%0Aalgorithm%20is%20used%20to%20provide%20a%20rough%20match%2C%20and%20then%20the%20core%20of%20CURSOR%2C%20a%0Afiber-CUR-based%20tensor%20generation%20method%2C%20directly%20calculates%20entries%20of%20the%0Acompatibility%20tensor%20by%20leveraging%20the%20initial%20second-order%20match%20result.%20This%0Asignificantly%20decreases%20the%20time%20complexity%20and%20tensor%20density.%20A%20probability%0Arelaxation%20labeling%20%28PRL%29-based%20matching%20algorithm%2C%20especially%20suitable%20for%0Asparse%20tensors%2C%20is%20developed.%20Experiment%20results%20on%20large-scale%20synthetic%0Adatasets%20and%20widely-adopted%20benchmark%20sets%20demonstrate%20the%20superiority%20of%0ACURSOR%20over%20existing%20methods.%20The%20tensor%20generation%20method%20in%20CURSOR%20can%20be%0Aintegrated%20seamlessly%20into%20existing%20hypergraph%20matching%20methods%20to%20improve%0Atheir%20performance%20and%20lower%20their%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16594v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURSOR%3A%20Scalable%20Mixed-Order%20Hypergraph%20Matching%20with%20CUR%20Decomposition&entry.906535625=Qixuan%20Zheng%20and%20Ming%20Zhang%20and%20Hong%20Yan&entry.1292438233=%20%20To%20achieve%20greater%20accuracy%2C%20hypergraph%20matching%20algorithms%20require%0Aexponential%20increases%20in%20computational%20resources.%20Recent%20kd-tree-based%0Aapproximate%20nearest%20neighbor%20%28ANN%29%20methods%2C%20despite%20the%20sparsity%20of%20their%0Acompatibility%20tensor%2C%20still%20require%20exhaustive%20calculations%20for%20large-scale%0Agraph%20matching.%20This%20work%20utilizes%20CUR%20tensor%20decomposition%20and%20introduces%20a%0Anovel%20cascaded%20second%20and%20third-order%20hypergraph%20matching%20framework%20%28CURSOR%29%0Afor%20efficient%20hypergraph%20matching.%20A%20CUR-based%20second-order%20graph%20matching%0Aalgorithm%20is%20used%20to%20provide%20a%20rough%20match%2C%20and%20then%20the%20core%20of%20CURSOR%2C%20a%0Afiber-CUR-based%20tensor%20generation%20method%2C%20directly%20calculates%20entries%20of%20the%0Acompatibility%20tensor%20by%20leveraging%20the%20initial%20second-order%20match%20result.%20This%0Asignificantly%20decreases%20the%20time%20complexity%20and%20tensor%20density.%20A%20probability%0Arelaxation%20labeling%20%28PRL%29-based%20matching%20algorithm%2C%20especially%20suitable%20for%0Asparse%20tensors%2C%20is%20developed.%20Experiment%20results%20on%20large-scale%20synthetic%0Adatasets%20and%20widely-adopted%20benchmark%20sets%20demonstrate%20the%20superiority%20of%0ACURSOR%20over%20existing%20methods.%20The%20tensor%20generation%20method%20in%20CURSOR%20can%20be%0Aintegrated%20seamlessly%20into%20existing%20hypergraph%20matching%20methods%20to%20improve%0Atheir%20performance%20and%20lower%20their%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16594v4&entry.124074799=Read"},
{"title": "Debiased Collaborative Filtering with Kernel-Based Causal Balancing", "author": "Haoxuan Li and Chunyuan Zheng and Yanghao Xiao and Peng Wu and Zhi Geng and Xu Chen and Peng Cui", "abstract": "  Debiased collaborative filtering aims to learn an unbiased prediction model\nby removing different biases in observational datasets. To solve this problem,\none of the simple and effective methods is based on the propensity score, which\nadjusts the observational sample distribution to the target one by reweighting\nobserved instances. Ideally, propensity scores should be learned with causal\nbalancing constraints. However, existing methods usually ignore such\nconstraints or implement them with unreasonable approximations, which may\naffect the accuracy of the learned propensity scores. To bridge this gap, in\nthis paper, we first analyze the gaps between the causal balancing requirements\nand existing methods such as learning the propensity with cross-entropy loss or\nmanually selecting functions to balance. Inspired by these gaps, we propose to\napproximate the balancing functions in reproducing kernel Hilbert space and\ndemonstrate that, based on the universal property and representer theorem of\nkernel functions, the causal balancing constraints can be better satisfied.\nMeanwhile, we propose an algorithm that adaptively balances the kernel function\nand theoretically analyze the generalization error bound of our methods. We\nconduct extensive experiments to demonstrate the effectiveness of our methods,\nand to promote this research direction, we have released our project at\nhttps://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.\n", "link": "http://arxiv.org/abs/2404.19596v1", "date": "2024-04-30", "relevancy": 1.8525, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4573}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4518}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Debiased%20Collaborative%20Filtering%20with%20Kernel-Based%20Causal%20Balancing&body=Title%3A%20Debiased%20Collaborative%20Filtering%20with%20Kernel-Based%20Causal%20Balancing%0AAuthor%3A%20Haoxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Yanghao%20Xiao%20and%20Peng%20Wu%20and%20Zhi%20Geng%20and%20Xu%20Chen%20and%20Peng%20Cui%0AAbstract%3A%20%20%20Debiased%20collaborative%20filtering%20aims%20to%20learn%20an%20unbiased%20prediction%20model%0Aby%20removing%20different%20biases%20in%20observational%20datasets.%20To%20solve%20this%20problem%2C%0Aone%20of%20the%20simple%20and%20effective%20methods%20is%20based%20on%20the%20propensity%20score%2C%20which%0Aadjusts%20the%20observational%20sample%20distribution%20to%20the%20target%20one%20by%20reweighting%0Aobserved%20instances.%20Ideally%2C%20propensity%20scores%20should%20be%20learned%20with%20causal%0Abalancing%20constraints.%20However%2C%20existing%20methods%20usually%20ignore%20such%0Aconstraints%20or%20implement%20them%20with%20unreasonable%20approximations%2C%20which%20may%0Aaffect%20the%20accuracy%20of%20the%20learned%20propensity%20scores.%20To%20bridge%20this%20gap%2C%20in%0Athis%20paper%2C%20we%20first%20analyze%20the%20gaps%20between%20the%20causal%20balancing%20requirements%0Aand%20existing%20methods%20such%20as%20learning%20the%20propensity%20with%20cross-entropy%20loss%20or%0Amanually%20selecting%20functions%20to%20balance.%20Inspired%20by%20these%20gaps%2C%20we%20propose%20to%0Aapproximate%20the%20balancing%20functions%20in%20reproducing%20kernel%20Hilbert%20space%20and%0Ademonstrate%20that%2C%20based%20on%20the%20universal%20property%20and%20representer%20theorem%20of%0Akernel%20functions%2C%20the%20causal%20balancing%20constraints%20can%20be%20better%20satisfied.%0AMeanwhile%2C%20we%20propose%20an%20algorithm%20that%20adaptively%20balances%20the%20kernel%20function%0Aand%20theoretically%20analyze%20the%20generalization%20error%20bound%20of%20our%20methods.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20methods%2C%0Aand%20to%20promote%20this%20research%20direction%2C%20we%20have%20released%20our%20project%20at%0Ahttps%3A//github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19596v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiased%20Collaborative%20Filtering%20with%20Kernel-Based%20Causal%20Balancing&entry.906535625=Haoxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Yanghao%20Xiao%20and%20Peng%20Wu%20and%20Zhi%20Geng%20and%20Xu%20Chen%20and%20Peng%20Cui&entry.1292438233=%20%20Debiased%20collaborative%20filtering%20aims%20to%20learn%20an%20unbiased%20prediction%20model%0Aby%20removing%20different%20biases%20in%20observational%20datasets.%20To%20solve%20this%20problem%2C%0Aone%20of%20the%20simple%20and%20effective%20methods%20is%20based%20on%20the%20propensity%20score%2C%20which%0Aadjusts%20the%20observational%20sample%20distribution%20to%20the%20target%20one%20by%20reweighting%0Aobserved%20instances.%20Ideally%2C%20propensity%20scores%20should%20be%20learned%20with%20causal%0Abalancing%20constraints.%20However%2C%20existing%20methods%20usually%20ignore%20such%0Aconstraints%20or%20implement%20them%20with%20unreasonable%20approximations%2C%20which%20may%0Aaffect%20the%20accuracy%20of%20the%20learned%20propensity%20scores.%20To%20bridge%20this%20gap%2C%20in%0Athis%20paper%2C%20we%20first%20analyze%20the%20gaps%20between%20the%20causal%20balancing%20requirements%0Aand%20existing%20methods%20such%20as%20learning%20the%20propensity%20with%20cross-entropy%20loss%20or%0Amanually%20selecting%20functions%20to%20balance.%20Inspired%20by%20these%20gaps%2C%20we%20propose%20to%0Aapproximate%20the%20balancing%20functions%20in%20reproducing%20kernel%20Hilbert%20space%20and%0Ademonstrate%20that%2C%20based%20on%20the%20universal%20property%20and%20representer%20theorem%20of%0Akernel%20functions%2C%20the%20causal%20balancing%20constraints%20can%20be%20better%20satisfied.%0AMeanwhile%2C%20we%20propose%20an%20algorithm%20that%20adaptively%20balances%20the%20kernel%20function%0Aand%20theoretically%20analyze%20the%20generalization%20error%20bound%20of%20our%20methods.%20We%0Aconduct%20extensive%20experiments%20to%20demonstrate%20the%20effectiveness%20of%20our%20methods%2C%0Aand%20to%20promote%20this%20research%20direction%2C%20we%20have%20released%20our%20project%20at%0Ahttps%3A//github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19596v1&entry.124074799=Read"},
{"title": "SimAC: A Simple Anti-Customization Method for Protecting Face Privacy\n  against Text-to-Image Synthesis of Diffusion Models", "author": "Feifei Wang and Zhentao Tan and Tianyi Wei and Yue Wu and Qidong Huang", "abstract": "  Despite the success of diffusion-based customization methods on visual\ncontent creation, increasing concerns have been raised about such techniques\nfrom both privacy and political perspectives. To tackle this issue, several\nanti-customization methods have been proposed in very recent months,\npredominantly grounded in adversarial attacks. Unfortunately, most of these\nmethods adopt straightforward designs, such as end-to-end optimization with a\nfocus on adversarially maximizing the original training loss, thereby\nneglecting nuanced internal properties intrinsic to the diffusion model, and\neven leading to ineffective optimization in some diffusion time steps.In this\npaper, we strive to bridge this gap by undertaking a comprehensive exploration\nof these inherent properties, to boost the performance of current\nanti-customization approaches. Two aspects of properties are investigated: 1)\nWe examine the relationship between time step selection and the model's\nperception in the frequency domain of images and find that lower time steps can\ngive much more contributions to adversarial noises. This inspires us to propose\nan adaptive greedy search for optimal time steps that seamlessly integrates\nwith existing anti-customization methods. 2) We scrutinize the roles of\nfeatures at different layers during denoising and devise a sophisticated\nfeature-based optimization framework for anti-customization.Experiments on\nfacial benchmarks demonstrate that our approach significantly increases\nidentity disruption, thereby protecting user privacy and copyright. Our code is\navailable at: https://github.com/somuchtome/SimAC.\n", "link": "http://arxiv.org/abs/2312.07865v2", "date": "2024-04-30", "relevancy": 1.8486, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6432}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6118}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6003}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SimAC%3A%20A%20Simple%20Anti-Customization%20Method%20for%20Protecting%20Face%20Privacy%0A%20%20against%20Text-to-Image%20Synthesis%20of%20Diffusion%20Models&body=Title%3A%20SimAC%3A%20A%20Simple%20Anti-Customization%20Method%20for%20Protecting%20Face%20Privacy%0A%20%20against%20Text-to-Image%20Synthesis%20of%20Diffusion%20Models%0AAuthor%3A%20Feifei%20Wang%20and%20Zhentao%20Tan%20and%20Tianyi%20Wei%20and%20Yue%20Wu%20and%20Qidong%20Huang%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20diffusion-based%20customization%20methods%20on%20visual%0Acontent%20creation%2C%20increasing%20concerns%20have%20been%20raised%20about%20such%20techniques%0Afrom%20both%20privacy%20and%20political%20perspectives.%20To%20tackle%20this%20issue%2C%20several%0Aanti-customization%20methods%20have%20been%20proposed%20in%20very%20recent%20months%2C%0Apredominantly%20grounded%20in%20adversarial%20attacks.%20Unfortunately%2C%20most%20of%20these%0Amethods%20adopt%20straightforward%20designs%2C%20such%20as%20end-to-end%20optimization%20with%20a%0Afocus%20on%20adversarially%20maximizing%20the%20original%20training%20loss%2C%20thereby%0Aneglecting%20nuanced%20internal%20properties%20intrinsic%20to%20the%20diffusion%20model%2C%20and%0Aeven%20leading%20to%20ineffective%20optimization%20in%20some%20diffusion%20time%20steps.In%20this%0Apaper%2C%20we%20strive%20to%20bridge%20this%20gap%20by%20undertaking%20a%20comprehensive%20exploration%0Aof%20these%20inherent%20properties%2C%20to%20boost%20the%20performance%20of%20current%0Aanti-customization%20approaches.%20Two%20aspects%20of%20properties%20are%20investigated%3A%201%29%0AWe%20examine%20the%20relationship%20between%20time%20step%20selection%20and%20the%20model%27s%0Aperception%20in%20the%20frequency%20domain%20of%20images%20and%20find%20that%20lower%20time%20steps%20can%0Agive%20much%20more%20contributions%20to%20adversarial%20noises.%20This%20inspires%20us%20to%20propose%0Aan%20adaptive%20greedy%20search%20for%20optimal%20time%20steps%20that%20seamlessly%20integrates%0Awith%20existing%20anti-customization%20methods.%202%29%20We%20scrutinize%20the%20roles%20of%0Afeatures%20at%20different%20layers%20during%20denoising%20and%20devise%20a%20sophisticated%0Afeature-based%20optimization%20framework%20for%20anti-customization.Experiments%20on%0Afacial%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20increases%0Aidentity%20disruption%2C%20thereby%20protecting%20user%20privacy%20and%20copyright.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/somuchtome/SimAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07865v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimAC%3A%20A%20Simple%20Anti-Customization%20Method%20for%20Protecting%20Face%20Privacy%0A%20%20against%20Text-to-Image%20Synthesis%20of%20Diffusion%20Models&entry.906535625=Feifei%20Wang%20and%20Zhentao%20Tan%20and%20Tianyi%20Wei%20and%20Yue%20Wu%20and%20Qidong%20Huang&entry.1292438233=%20%20Despite%20the%20success%20of%20diffusion-based%20customization%20methods%20on%20visual%0Acontent%20creation%2C%20increasing%20concerns%20have%20been%20raised%20about%20such%20techniques%0Afrom%20both%20privacy%20and%20political%20perspectives.%20To%20tackle%20this%20issue%2C%20several%0Aanti-customization%20methods%20have%20been%20proposed%20in%20very%20recent%20months%2C%0Apredominantly%20grounded%20in%20adversarial%20attacks.%20Unfortunately%2C%20most%20of%20these%0Amethods%20adopt%20straightforward%20designs%2C%20such%20as%20end-to-end%20optimization%20with%20a%0Afocus%20on%20adversarially%20maximizing%20the%20original%20training%20loss%2C%20thereby%0Aneglecting%20nuanced%20internal%20properties%20intrinsic%20to%20the%20diffusion%20model%2C%20and%0Aeven%20leading%20to%20ineffective%20optimization%20in%20some%20diffusion%20time%20steps.In%20this%0Apaper%2C%20we%20strive%20to%20bridge%20this%20gap%20by%20undertaking%20a%20comprehensive%20exploration%0Aof%20these%20inherent%20properties%2C%20to%20boost%20the%20performance%20of%20current%0Aanti-customization%20approaches.%20Two%20aspects%20of%20properties%20are%20investigated%3A%201%29%0AWe%20examine%20the%20relationship%20between%20time%20step%20selection%20and%20the%20model%27s%0Aperception%20in%20the%20frequency%20domain%20of%20images%20and%20find%20that%20lower%20time%20steps%20can%0Agive%20much%20more%20contributions%20to%20adversarial%20noises.%20This%20inspires%20us%20to%20propose%0Aan%20adaptive%20greedy%20search%20for%20optimal%20time%20steps%20that%20seamlessly%20integrates%0Awith%20existing%20anti-customization%20methods.%202%29%20We%20scrutinize%20the%20roles%20of%0Afeatures%20at%20different%20layers%20during%20denoising%20and%20devise%20a%20sophisticated%0Afeature-based%20optimization%20framework%20for%20anti-customization.Experiments%20on%0Afacial%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20increases%0Aidentity%20disruption%2C%20thereby%20protecting%20user%20privacy%20and%20copyright.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/somuchtome/SimAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07865v2&entry.124074799=Read"},
{"title": "The lazy (NTK) and rich ($\u03bc$P) regimes: a gentle tutorial", "author": "Dhruva Karkada", "abstract": "  A central theme of the modern machine learning paradigm is that larger neural\nnetworks achieve better performance on a variety of metrics. Theoretical\nanalyses of these overparameterized models have recently centered around\nstudying very wide neural networks. In this tutorial, we provide a nonrigorous\nbut illustrative derivation of the following fact: in order to train wide\nnetworks effectively, there is only one degree of freedom in choosing\nhyperparameters such as the learning rate and the size of the initial weights.\nThis degree of freedom controls the richness of training behavior: at minimum,\nthe wide network trains lazily like a kernel machine, and at maximum, it\nexhibits feature learning in the so-called $\\mu$P regime. In this paper, we\nexplain this richness scale, synthesize recent research results into a coherent\nwhole, offer new perspectives and intuitions, and provide empirical evidence\nsupporting our claims. In doing so, we hope to encourage further study of the\nrichness scale, as it may be key to developing a scientific theory of feature\nlearning in practical deep neural networks.\n", "link": "http://arxiv.org/abs/2404.19719v1", "date": "2024-04-30", "relevancy": 1.8288, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4385}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20lazy%20%28NTK%29%20and%20rich%20%28%24%CE%BC%24P%29%20regimes%3A%20a%20gentle%20tutorial&body=Title%3A%20The%20lazy%20%28NTK%29%20and%20rich%20%28%24%CE%BC%24P%29%20regimes%3A%20a%20gentle%20tutorial%0AAuthor%3A%20Dhruva%20Karkada%0AAbstract%3A%20%20%20A%20central%20theme%20of%20the%20modern%20machine%20learning%20paradigm%20is%20that%20larger%20neural%0Anetworks%20achieve%20better%20performance%20on%20a%20variety%20of%20metrics.%20Theoretical%0Aanalyses%20of%20these%20overparameterized%20models%20have%20recently%20centered%20around%0Astudying%20very%20wide%20neural%20networks.%20In%20this%20tutorial%2C%20we%20provide%20a%20nonrigorous%0Abut%20illustrative%20derivation%20of%20the%20following%20fact%3A%20in%20order%20to%20train%20wide%0Anetworks%20effectively%2C%20there%20is%20only%20one%20degree%20of%20freedom%20in%20choosing%0Ahyperparameters%20such%20as%20the%20learning%20rate%20and%20the%20size%20of%20the%20initial%20weights.%0AThis%20degree%20of%20freedom%20controls%20the%20richness%20of%20training%20behavior%3A%20at%20minimum%2C%0Athe%20wide%20network%20trains%20lazily%20like%20a%20kernel%20machine%2C%20and%20at%20maximum%2C%20it%0Aexhibits%20feature%20learning%20in%20the%20so-called%20%24%5Cmu%24P%20regime.%20In%20this%20paper%2C%20we%0Aexplain%20this%20richness%20scale%2C%20synthesize%20recent%20research%20results%20into%20a%20coherent%0Awhole%2C%20offer%20new%20perspectives%20and%20intuitions%2C%20and%20provide%20empirical%20evidence%0Asupporting%20our%20claims.%20In%20doing%20so%2C%20we%20hope%20to%20encourage%20further%20study%20of%20the%0Arichness%20scale%2C%20as%20it%20may%20be%20key%20to%20developing%20a%20scientific%20theory%20of%20feature%0Alearning%20in%20practical%20deep%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19719v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20lazy%20%28NTK%29%20and%20rich%20%28%24%CE%BC%24P%29%20regimes%3A%20a%20gentle%20tutorial&entry.906535625=Dhruva%20Karkada&entry.1292438233=%20%20A%20central%20theme%20of%20the%20modern%20machine%20learning%20paradigm%20is%20that%20larger%20neural%0Anetworks%20achieve%20better%20performance%20on%20a%20variety%20of%20metrics.%20Theoretical%0Aanalyses%20of%20these%20overparameterized%20models%20have%20recently%20centered%20around%0Astudying%20very%20wide%20neural%20networks.%20In%20this%20tutorial%2C%20we%20provide%20a%20nonrigorous%0Abut%20illustrative%20derivation%20of%20the%20following%20fact%3A%20in%20order%20to%20train%20wide%0Anetworks%20effectively%2C%20there%20is%20only%20one%20degree%20of%20freedom%20in%20choosing%0Ahyperparameters%20such%20as%20the%20learning%20rate%20and%20the%20size%20of%20the%20initial%20weights.%0AThis%20degree%20of%20freedom%20controls%20the%20richness%20of%20training%20behavior%3A%20at%20minimum%2C%0Athe%20wide%20network%20trains%20lazily%20like%20a%20kernel%20machine%2C%20and%20at%20maximum%2C%20it%0Aexhibits%20feature%20learning%20in%20the%20so-called%20%24%5Cmu%24P%20regime.%20In%20this%20paper%2C%20we%0Aexplain%20this%20richness%20scale%2C%20synthesize%20recent%20research%20results%20into%20a%20coherent%0Awhole%2C%20offer%20new%20perspectives%20and%20intuitions%2C%20and%20provide%20empirical%20evidence%0Asupporting%20our%20claims.%20In%20doing%20so%2C%20we%20hope%20to%20encourage%20further%20study%20of%20the%0Arichness%20scale%2C%20as%20it%20may%20be%20key%20to%20developing%20a%20scientific%20theory%20of%20feature%0Alearning%20in%20practical%20deep%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19719v1&entry.124074799=Read"},
{"title": "GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting", "author": "Kai Zhang and Sai Bi and Hao Tan and Yuanbo Xiangli and Nanxuan Zhao and Kalyan Sunkavalli and Zexiang Xu", "abstract": "  We propose GS-LRM, a scalable large reconstruction model that can predict\nhigh-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23\nseconds on single A100 GPU. Our model features a very simple transformer-based\narchitecture; we patchify input posed images, pass the concatenated multi-view\nimage tokens through a sequence of transformer blocks, and decode final\nper-pixel Gaussian parameters directly from these tokens for differentiable\nrendering. In contrast to previous LRMs that can only reconstruct objects, by\npredicting per-pixel Gaussians, GS-LRM naturally handles scenes with large\nvariations in scale and complexity. We show that our model can work on both\nobject and scene captures by training it on Objaverse and RealEstate10K\nrespectively. In both scenarios, the models outperform state-of-the-art\nbaselines by a wide margin. We also demonstrate applications of our model in\ndownstream 3D generation tasks. Our project webpage is available at:\nhttps://sai-bi.github.io/project/gs-lrm/ .\n", "link": "http://arxiv.org/abs/2404.19702v1", "date": "2024-04-30", "relevancy": 1.8114, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.647}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.552}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GS-LRM%3A%20Large%20Reconstruction%20Model%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GS-LRM%3A%20Large%20Reconstruction%20Model%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Hao%20Tan%20and%20Yuanbo%20Xiangli%20and%20Nanxuan%20Zhao%20and%20Kalyan%20Sunkavalli%20and%20Zexiang%20Xu%0AAbstract%3A%20%20%20We%20propose%20GS-LRM%2C%20a%20scalable%20large%20reconstruction%20model%20that%20can%20predict%0Ahigh-quality%203D%20Gaussian%20primitives%20from%202-4%20posed%20sparse%20images%20in%200.23%0Aseconds%20on%20single%20A100%20GPU.%20Our%20model%20features%20a%20very%20simple%20transformer-based%0Aarchitecture%3B%20we%20patchify%20input%20posed%20images%2C%20pass%20the%20concatenated%20multi-view%0Aimage%20tokens%20through%20a%20sequence%20of%20transformer%20blocks%2C%20and%20decode%20final%0Aper-pixel%20Gaussian%20parameters%20directly%20from%20these%20tokens%20for%20differentiable%0Arendering.%20In%20contrast%20to%20previous%20LRMs%20that%20can%20only%20reconstruct%20objects%2C%20by%0Apredicting%20per-pixel%20Gaussians%2C%20GS-LRM%20naturally%20handles%20scenes%20with%20large%0Avariations%20in%20scale%20and%20complexity.%20We%20show%20that%20our%20model%20can%20work%20on%20both%0Aobject%20and%20scene%20captures%20by%20training%20it%20on%20Objaverse%20and%20RealEstate10K%0Arespectively.%20In%20both%20scenarios%2C%20the%20models%20outperform%20state-of-the-art%0Abaselines%20by%20a%20wide%20margin.%20We%20also%20demonstrate%20applications%20of%20our%20model%20in%0Adownstream%203D%20generation%20tasks.%20Our%20project%20webpage%20is%20available%20at%3A%0Ahttps%3A//sai-bi.github.io/project/gs-lrm/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19702v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-LRM%3A%20Large%20Reconstruction%20Model%20for%203D%20Gaussian%20Splatting&entry.906535625=Kai%20Zhang%20and%20Sai%20Bi%20and%20Hao%20Tan%20and%20Yuanbo%20Xiangli%20and%20Nanxuan%20Zhao%20and%20Kalyan%20Sunkavalli%20and%20Zexiang%20Xu&entry.1292438233=%20%20We%20propose%20GS-LRM%2C%20a%20scalable%20large%20reconstruction%20model%20that%20can%20predict%0Ahigh-quality%203D%20Gaussian%20primitives%20from%202-4%20posed%20sparse%20images%20in%200.23%0Aseconds%20on%20single%20A100%20GPU.%20Our%20model%20features%20a%20very%20simple%20transformer-based%0Aarchitecture%3B%20we%20patchify%20input%20posed%20images%2C%20pass%20the%20concatenated%20multi-view%0Aimage%20tokens%20through%20a%20sequence%20of%20transformer%20blocks%2C%20and%20decode%20final%0Aper-pixel%20Gaussian%20parameters%20directly%20from%20these%20tokens%20for%20differentiable%0Arendering.%20In%20contrast%20to%20previous%20LRMs%20that%20can%20only%20reconstruct%20objects%2C%20by%0Apredicting%20per-pixel%20Gaussians%2C%20GS-LRM%20naturally%20handles%20scenes%20with%20large%0Avariations%20in%20scale%20and%20complexity.%20We%20show%20that%20our%20model%20can%20work%20on%20both%0Aobject%20and%20scene%20captures%20by%20training%20it%20on%20Objaverse%20and%20RealEstate10K%0Arespectively.%20In%20both%20scenarios%2C%20the%20models%20outperform%20state-of-the-art%0Abaselines%20by%20a%20wide%20margin.%20We%20also%20demonstrate%20applications%20of%20our%20model%20in%0Adownstream%203D%20generation%20tasks.%20Our%20project%20webpage%20is%20available%20at%3A%0Ahttps%3A//sai-bi.github.io/project/gs-lrm/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19702v1&entry.124074799=Read"},
{"title": "War Elephants: Rethinking Combat AI and Human Oversight", "author": "Philip Feldman and Aaron Dant and Harry Dreany", "abstract": "  This paper explores the changes that pervasive AI is having on the nature of\ncombat. We look beyond the substitution of AI for experts to an approach where\ncomplementary human and machine abilities are blended. Using historical and\nmodern examples, we show how autonomous weapons systems can be effectively\nmanaged by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\"\nBy basing our approach on the principles of complementation, we provide for a\nflexible and dynamic approach to managing lethal autonomous systems. We\nconclude by presenting a path to achieving an integrated vision of\nmachine-speed combat where the battlefield AI is operated by AI Operators that\nwatch for patterns of behavior within battlefield to assess the performance of\nlethal autonomous systems. This approach enables the development of combat\nsystems that are likely to be more ethical, operate at machine speed, and are\ncapable of responding to a broader range of dynamic battlefield conditions than\nany purely autonomous AI system could support.\n", "link": "http://arxiv.org/abs/2404.19573v1", "date": "2024-04-30", "relevancy": 1.8072, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4535}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.452}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4469}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20War%20Elephants%3A%20Rethinking%20Combat%20AI%20and%20Human%20Oversight&body=Title%3A%20War%20Elephants%3A%20Rethinking%20Combat%20AI%20and%20Human%20Oversight%0AAuthor%3A%20Philip%20Feldman%20and%20Aaron%20Dant%20and%20Harry%20Dreany%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20changes%20that%20pervasive%20AI%20is%20having%20on%20the%20nature%20of%0Acombat.%20We%20look%20beyond%20the%20substitution%20of%20AI%20for%20experts%20to%20an%20approach%20where%0Acomplementary%20human%20and%20machine%20abilities%20are%20blended.%20Using%20historical%20and%0Amodern%20examples%2C%20we%20show%20how%20autonomous%20weapons%20systems%20can%20be%20effectively%0Amanaged%20by%20teams%20of%20human%20%22AI%20Operators%22%20combined%20with%20AI/ML%20%22Proxy%20Operators.%22%0ABy%20basing%20our%20approach%20on%20the%20principles%20of%20complementation%2C%20we%20provide%20for%20a%0Aflexible%20and%20dynamic%20approach%20to%20managing%20lethal%20autonomous%20systems.%20We%0Aconclude%20by%20presenting%20a%20path%20to%20achieving%20an%20integrated%20vision%20of%0Amachine-speed%20combat%20where%20the%20battlefield%20AI%20is%20operated%20by%20AI%20Operators%20that%0Awatch%20for%20patterns%20of%20behavior%20within%20battlefield%20to%20assess%20the%20performance%20of%0Alethal%20autonomous%20systems.%20This%20approach%20enables%20the%20development%20of%20combat%0Asystems%20that%20are%20likely%20to%20be%20more%20ethical%2C%20operate%20at%20machine%20speed%2C%20and%20are%0Acapable%20of%20responding%20to%20a%20broader%20range%20of%20dynamic%20battlefield%20conditions%20than%0Aany%20purely%20autonomous%20AI%20system%20could%20support.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19573v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=War%20Elephants%3A%20Rethinking%20Combat%20AI%20and%20Human%20Oversight&entry.906535625=Philip%20Feldman%20and%20Aaron%20Dant%20and%20Harry%20Dreany&entry.1292438233=%20%20This%20paper%20explores%20the%20changes%20that%20pervasive%20AI%20is%20having%20on%20the%20nature%20of%0Acombat.%20We%20look%20beyond%20the%20substitution%20of%20AI%20for%20experts%20to%20an%20approach%20where%0Acomplementary%20human%20and%20machine%20abilities%20are%20blended.%20Using%20historical%20and%0Amodern%20examples%2C%20we%20show%20how%20autonomous%20weapons%20systems%20can%20be%20effectively%0Amanaged%20by%20teams%20of%20human%20%22AI%20Operators%22%20combined%20with%20AI/ML%20%22Proxy%20Operators.%22%0ABy%20basing%20our%20approach%20on%20the%20principles%20of%20complementation%2C%20we%20provide%20for%20a%0Aflexible%20and%20dynamic%20approach%20to%20managing%20lethal%20autonomous%20systems.%20We%0Aconclude%20by%20presenting%20a%20path%20to%20achieving%20an%20integrated%20vision%20of%0Amachine-speed%20combat%20where%20the%20battlefield%20AI%20is%20operated%20by%20AI%20Operators%20that%0Awatch%20for%20patterns%20of%20behavior%20within%20battlefield%20to%20assess%20the%20performance%20of%0Alethal%20autonomous%20systems.%20This%20approach%20enables%20the%20development%20of%20combat%0Asystems%20that%20are%20likely%20to%20be%20more%20ethical%2C%20operate%20at%20machine%20speed%2C%20and%20are%0Acapable%20of%20responding%20to%20a%20broader%20range%20of%20dynamic%20battlefield%20conditions%20than%0Aany%20purely%20autonomous%20AI%20system%20could%20support.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19573v1&entry.124074799=Read"},
{"title": "Harmonic LLMs are Trustworthy", "author": "Nicholas S. Kersting and Mohammad Rahman and Suchismitha Vedala and Yang Wang", "abstract": "  We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time, based upon the local\ndeviation from harmoniticity, denoted as $\\gamma$. To the best of our knowledge\nthis is the first completely model-agnostic and unsupervised method of\nmeasuring the robustness of any given response from an LLM, based upon the\nmodel itself conforming to a purely mathematical standard. We conduct human\nannotation experiments to show the positive correlation of $\\gamma$ with false\nor misleading answers, and demonstrate that following the gradient of $\\gamma$\nin stochastic gradient ascent efficiently exposes adversarial prompts.\nMeasuring $\\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT,\nClaude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to\nestimate the liklihood of wrong or hallucinatory answers automatically and\nquantitatively rank the reliability of these models in various objective\ndomains (Web QA, TruthfulQA, and Programming QA). Across all models and domains\ntested, human ratings confirm that $\\gamma \\to 0$ indicates trustworthiness,\nand the low-$\\gamma$ leaders among these models are GPT-4, ChatGPT, and\nSmaug-72B.\n", "link": "http://arxiv.org/abs/2404.19708v1", "date": "2024-04-30", "relevancy": 1.7925, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.493}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4432}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harmonic%20LLMs%20are%20Trustworthy&body=Title%3A%20Harmonic%20LLMs%20are%20Trustworthy%0AAuthor%3A%20Nicholas%20S.%20Kersting%20and%20Mohammad%20Rahman%20and%20Suchismitha%20Vedala%20and%20Yang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20an%20intuitive%20method%20to%20test%20the%20robustness%20%28stability%20and%0Aexplainability%29%20of%20any%20black-box%20LLM%20in%20real-time%2C%20based%20upon%20the%20local%0Adeviation%20from%20harmoniticity%2C%20denoted%20as%20%24%5Cgamma%24.%20To%20the%20best%20of%20our%20knowledge%0Athis%20is%20the%20first%20completely%20model-agnostic%20and%20unsupervised%20method%20of%0Ameasuring%20the%20robustness%20of%20any%20given%20response%20from%20an%20LLM%2C%20based%20upon%20the%0Amodel%20itself%20conforming%20to%20a%20purely%20mathematical%20standard.%20We%20conduct%20human%0Aannotation%20experiments%20to%20show%20the%20positive%20correlation%20of%20%24%5Cgamma%24%20with%20false%0Aor%20misleading%20answers%2C%20and%20demonstrate%20that%20following%20the%20gradient%20of%20%24%5Cgamma%24%0Ain%20stochastic%20gradient%20ascent%20efficiently%20exposes%20adversarial%20prompts.%0AMeasuring%20%24%5Cgamma%24%20across%20thousands%20of%20queries%20in%20popular%20LLMs%20%28GPT-4%2C%20ChatGPT%2C%0AClaude-2.1%2C%20Mixtral-8x7B%2C%20Smaug-72B%2C%20Llama2-7B%2C%20and%20MPT-7B%29%20allows%20us%20to%0Aestimate%20the%20liklihood%20of%20wrong%20or%20hallucinatory%20answers%20automatically%20and%0Aquantitatively%20rank%20the%20reliability%20of%20these%20models%20in%20various%20objective%0Adomains%20%28Web%20QA%2C%20TruthfulQA%2C%20and%20Programming%20QA%29.%20Across%20all%20models%20and%20domains%0Atested%2C%20human%20ratings%20confirm%20that%20%24%5Cgamma%20%5Cto%200%24%20indicates%20trustworthiness%2C%0Aand%20the%20low-%24%5Cgamma%24%20leaders%20among%20these%20models%20are%20GPT-4%2C%20ChatGPT%2C%20and%0ASmaug-72B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19708v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonic%20LLMs%20are%20Trustworthy&entry.906535625=Nicholas%20S.%20Kersting%20and%20Mohammad%20Rahman%20and%20Suchismitha%20Vedala%20and%20Yang%20Wang&entry.1292438233=%20%20We%20introduce%20an%20intuitive%20method%20to%20test%20the%20robustness%20%28stability%20and%0Aexplainability%29%20of%20any%20black-box%20LLM%20in%20real-time%2C%20based%20upon%20the%20local%0Adeviation%20from%20harmoniticity%2C%20denoted%20as%20%24%5Cgamma%24.%20To%20the%20best%20of%20our%20knowledge%0Athis%20is%20the%20first%20completely%20model-agnostic%20and%20unsupervised%20method%20of%0Ameasuring%20the%20robustness%20of%20any%20given%20response%20from%20an%20LLM%2C%20based%20upon%20the%0Amodel%20itself%20conforming%20to%20a%20purely%20mathematical%20standard.%20We%20conduct%20human%0Aannotation%20experiments%20to%20show%20the%20positive%20correlation%20of%20%24%5Cgamma%24%20with%20false%0Aor%20misleading%20answers%2C%20and%20demonstrate%20that%20following%20the%20gradient%20of%20%24%5Cgamma%24%0Ain%20stochastic%20gradient%20ascent%20efficiently%20exposes%20adversarial%20prompts.%0AMeasuring%20%24%5Cgamma%24%20across%20thousands%20of%20queries%20in%20popular%20LLMs%20%28GPT-4%2C%20ChatGPT%2C%0AClaude-2.1%2C%20Mixtral-8x7B%2C%20Smaug-72B%2C%20Llama2-7B%2C%20and%20MPT-7B%29%20allows%20us%20to%0Aestimate%20the%20liklihood%20of%20wrong%20or%20hallucinatory%20answers%20automatically%20and%0Aquantitatively%20rank%20the%20reliability%20of%20these%20models%20in%20various%20objective%0Adomains%20%28Web%20QA%2C%20TruthfulQA%2C%20and%20Programming%20QA%29.%20Across%20all%20models%20and%20domains%0Atested%2C%20human%20ratings%20confirm%20that%20%24%5Cgamma%20%5Cto%200%24%20indicates%20trustworthiness%2C%0Aand%20the%20low-%24%5Cgamma%24%20leaders%20among%20these%20models%20are%20GPT-4%2C%20ChatGPT%2C%20and%0ASmaug-72B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19708v1&entry.124074799=Read"},
{"title": "KAN: Kolmogorov-Arnold Networks", "author": "Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Solja\u010di\u0107 and Thomas Y. Hou and Max Tegmark", "abstract": "  Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.\n", "link": "http://arxiv.org/abs/2404.19756v1", "date": "2024-04-30", "relevancy": 1.7839, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4144}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KAN%3A%20Kolmogorov-Arnold%20Networks&body=Title%3A%20KAN%3A%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19756v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN%3A%20Kolmogorov-Arnold%20Networks&entry.906535625=Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark&entry.1292438233=%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19756v1&entry.124074799=Read"},
{"title": "MIPI 2024 Challenge on Nighttime Flare Removal: Methods and Results", "author": "Yuekun Dai and Dafeng Zhang and Xiaoming Li and Zongsheng Yue and Chongyi Li and Shangchen Zhou and Ruicheng Feng and Peiqing Yang and Zhezhu Jin and Guanqun Liu and Chen Change Loy and Lize Zhang and Shuai Liu and Chaoyu Feng and Luyang Wang and Shuan Chen and Guangqi Shao and Xiaotao Wang and Lei Lei and Qirui Yang and Qihua Cheng and Zhiqiang Xu and Yihao Liu and Huanjing Yue and Jingyu Yang and Florin-Alexandru Vasluianu and Zongwei Wu and George Ciubotariu and Radu Timofte and Zhao Zhang and Suiyi Zhao and Bo Wang and Zhichao Zuo and Yanyan Wei and Kuppa Sai Sri Teja and Jayakar Reddy A and Girish Rongali and Kaushik Mitra and Zhihao Ma and Yongxu Liu and Wanying Zhang and Wei Shang and Yuhong He and Long Peng and Zhongxin Yu and Shaofei Luo and Jian Wang and Yuqi Miao and Baiang Li and Gang Wei and Rakshank Verma and Ritik Maheshwari and Rahul Tekchandani and Praful Hambarde and Satya Narayan Tazi and Santosh Kumar Vipparthi and Subrahmanyam Murala and Haopeng Zhang and Yingli Hou and Mingde Yao and Levin M S and Aniruth Sundararajan and Hari Kumar A", "abstract": "  The increasing demand for computational photography and imaging on mobile\nplatforms has led to the widespread development and integration of advanced\nimage sensors with novel algorithms in camera systems. However, the scarcity of\nhigh-quality data for research and the rare opportunity for in-depth exchange\nof views from industry and academia constrain the development of mobile\nintelligent photography and imaging (MIPI). Building on the achievements of the\nprevious MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third\nMIPI challenge including three tracks focusing on novel image sensors and\nimaging algorithms. In this paper, we summarize and review the Nighttime Flare\nRemoval track on MIPI 2024. In total, 170 participants were successfully\nregistered, and 14 teams submitted results in the final testing phase. The\ndeveloped solutions in this challenge achieved state-of-the-art performance on\nNighttime Flare Removal. More details of this challenge and the link to the\ndataset can be found at https://mipi-challenge.org/MIPI2024/.\n", "link": "http://arxiv.org/abs/2404.19534v1", "date": "2024-04-30", "relevancy": 1.7796, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4498}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4412}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MIPI%202024%20Challenge%20on%20Nighttime%20Flare%20Removal%3A%20Methods%20and%20Results&body=Title%3A%20MIPI%202024%20Challenge%20on%20Nighttime%20Flare%20Removal%3A%20Methods%20and%20Results%0AAuthor%3A%20Yuekun%20Dai%20and%20Dafeng%20Zhang%20and%20Xiaoming%20Li%20and%20Zongsheng%20Yue%20and%20Chongyi%20Li%20and%20Shangchen%20Zhou%20and%20Ruicheng%20Feng%20and%20Peiqing%20Yang%20and%20Zhezhu%20Jin%20and%20Guanqun%20Liu%20and%20Chen%20Change%20Loy%20and%20Lize%20Zhang%20and%20Shuai%20Liu%20and%20Chaoyu%20Feng%20and%20Luyang%20Wang%20and%20Shuan%20Chen%20and%20Guangqi%20Shao%20and%20Xiaotao%20Wang%20and%20Lei%20Lei%20and%20Qirui%20Yang%20and%20Qihua%20Cheng%20and%20Zhiqiang%20Xu%20and%20Yihao%20Liu%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%20and%20Florin-Alexandru%20Vasluianu%20and%20Zongwei%20Wu%20and%20George%20Ciubotariu%20and%20Radu%20Timofte%20and%20Zhao%20Zhang%20and%20Suiyi%20Zhao%20and%20Bo%20Wang%20and%20Zhichao%20Zuo%20and%20Yanyan%20Wei%20and%20Kuppa%20Sai%20Sri%20Teja%20and%20Jayakar%20Reddy%20A%20and%20Girish%20Rongali%20and%20Kaushik%20Mitra%20and%20Zhihao%20Ma%20and%20Yongxu%20Liu%20and%20Wanying%20Zhang%20and%20Wei%20Shang%20and%20Yuhong%20He%20and%20Long%20Peng%20and%20Zhongxin%20Yu%20and%20Shaofei%20Luo%20and%20Jian%20Wang%20and%20Yuqi%20Miao%20and%20Baiang%20Li%20and%20Gang%20Wei%20and%20Rakshank%20Verma%20and%20Ritik%20Maheshwari%20and%20Rahul%20Tekchandani%20and%20Praful%20Hambarde%20and%20Satya%20Narayan%20Tazi%20and%20Santosh%20Kumar%20Vipparthi%20and%20Subrahmanyam%20Murala%20and%20Haopeng%20Zhang%20and%20Yingli%20Hou%20and%20Mingde%20Yao%20and%20Levin%20M%20S%20and%20Aniruth%20Sundararajan%20and%20Hari%20Kumar%20A%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20computational%20photography%20and%20imaging%20on%20mobile%0Aplatforms%20has%20led%20to%20the%20widespread%20development%20and%20integration%20of%20advanced%0Aimage%20sensors%20with%20novel%20algorithms%20in%20camera%20systems.%20However%2C%20the%20scarcity%20of%0Ahigh-quality%20data%20for%20research%20and%20the%20rare%20opportunity%20for%20in-depth%20exchange%0Aof%20views%20from%20industry%20and%20academia%20constrain%20the%20development%20of%20mobile%0Aintelligent%20photography%20and%20imaging%20%28MIPI%29.%20Building%20on%20the%20achievements%20of%20the%0Aprevious%20MIPI%20Workshops%20held%20at%20ECCV%202022%20and%20CVPR%202023%2C%20we%20introduce%20our%20third%0AMIPI%20challenge%20including%20three%20tracks%20focusing%20on%20novel%20image%20sensors%20and%0Aimaging%20algorithms.%20In%20this%20paper%2C%20we%20summarize%20and%20review%20the%20Nighttime%20Flare%0ARemoval%20track%20on%20MIPI%202024.%20In%20total%2C%20170%20participants%20were%20successfully%0Aregistered%2C%20and%2014%20teams%20submitted%20results%20in%20the%20final%20testing%20phase.%20The%0Adeveloped%20solutions%20in%20this%20challenge%20achieved%20state-of-the-art%20performance%20on%0ANighttime%20Flare%20Removal.%20More%20details%20of%20this%20challenge%20and%20the%20link%20to%20the%0Adataset%20can%20be%20found%20at%20https%3A//mipi-challenge.org/MIPI2024/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19534v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIPI%202024%20Challenge%20on%20Nighttime%20Flare%20Removal%3A%20Methods%20and%20Results&entry.906535625=Yuekun%20Dai%20and%20Dafeng%20Zhang%20and%20Xiaoming%20Li%20and%20Zongsheng%20Yue%20and%20Chongyi%20Li%20and%20Shangchen%20Zhou%20and%20Ruicheng%20Feng%20and%20Peiqing%20Yang%20and%20Zhezhu%20Jin%20and%20Guanqun%20Liu%20and%20Chen%20Change%20Loy%20and%20Lize%20Zhang%20and%20Shuai%20Liu%20and%20Chaoyu%20Feng%20and%20Luyang%20Wang%20and%20Shuan%20Chen%20and%20Guangqi%20Shao%20and%20Xiaotao%20Wang%20and%20Lei%20Lei%20and%20Qirui%20Yang%20and%20Qihua%20Cheng%20and%20Zhiqiang%20Xu%20and%20Yihao%20Liu%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%20and%20Florin-Alexandru%20Vasluianu%20and%20Zongwei%20Wu%20and%20George%20Ciubotariu%20and%20Radu%20Timofte%20and%20Zhao%20Zhang%20and%20Suiyi%20Zhao%20and%20Bo%20Wang%20and%20Zhichao%20Zuo%20and%20Yanyan%20Wei%20and%20Kuppa%20Sai%20Sri%20Teja%20and%20Jayakar%20Reddy%20A%20and%20Girish%20Rongali%20and%20Kaushik%20Mitra%20and%20Zhihao%20Ma%20and%20Yongxu%20Liu%20and%20Wanying%20Zhang%20and%20Wei%20Shang%20and%20Yuhong%20He%20and%20Long%20Peng%20and%20Zhongxin%20Yu%20and%20Shaofei%20Luo%20and%20Jian%20Wang%20and%20Yuqi%20Miao%20and%20Baiang%20Li%20and%20Gang%20Wei%20and%20Rakshank%20Verma%20and%20Ritik%20Maheshwari%20and%20Rahul%20Tekchandani%20and%20Praful%20Hambarde%20and%20Satya%20Narayan%20Tazi%20and%20Santosh%20Kumar%20Vipparthi%20and%20Subrahmanyam%20Murala%20and%20Haopeng%20Zhang%20and%20Yingli%20Hou%20and%20Mingde%20Yao%20and%20Levin%20M%20S%20and%20Aniruth%20Sundararajan%20and%20Hari%20Kumar%20A&entry.1292438233=%20%20The%20increasing%20demand%20for%20computational%20photography%20and%20imaging%20on%20mobile%0Aplatforms%20has%20led%20to%20the%20widespread%20development%20and%20integration%20of%20advanced%0Aimage%20sensors%20with%20novel%20algorithms%20in%20camera%20systems.%20However%2C%20the%20scarcity%20of%0Ahigh-quality%20data%20for%20research%20and%20the%20rare%20opportunity%20for%20in-depth%20exchange%0Aof%20views%20from%20industry%20and%20academia%20constrain%20the%20development%20of%20mobile%0Aintelligent%20photography%20and%20imaging%20%28MIPI%29.%20Building%20on%20the%20achievements%20of%20the%0Aprevious%20MIPI%20Workshops%20held%20at%20ECCV%202022%20and%20CVPR%202023%2C%20we%20introduce%20our%20third%0AMIPI%20challenge%20including%20three%20tracks%20focusing%20on%20novel%20image%20sensors%20and%0Aimaging%20algorithms.%20In%20this%20paper%2C%20we%20summarize%20and%20review%20the%20Nighttime%20Flare%0ARemoval%20track%20on%20MIPI%202024.%20In%20total%2C%20170%20participants%20were%20successfully%0Aregistered%2C%20and%2014%20teams%20submitted%20results%20in%20the%20final%20testing%20phase.%20The%0Adeveloped%20solutions%20in%20this%20challenge%20achieved%20state-of-the-art%20performance%20on%0ANighttime%20Flare%20Removal.%20More%20details%20of%20this%20challenge%20and%20the%20link%20to%20the%0Adataset%20can%20be%20found%20at%20https%3A//mipi-challenge.org/MIPI2024/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19534v1&entry.124074799=Read"},
{"title": "Orthonormal Expansions for Translation-Invariant Kernels", "author": "Filip Tronarp and Toni Karvonen", "abstract": "  We present a general Fourier analytic technique for constructing orthonormal\nbasis expansions of translation-invariant kernels from orthonormal bases of\n$\\mathscr{L}_2(\\mathbb{R})$. This allows us to derive explicit expansions on\nthe real line for (i) Mat\\'ern kernels of all half-integer orders in terms of\nassociated Laguerre functions, (ii) the Cauchy kernel in terms of rational\nfunctions, and (iii) the Gaussian kernel in terms of Hermite functions.\n", "link": "http://arxiv.org/abs/2206.08648v4", "date": "2024-04-30", "relevancy": 1.7668, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3574}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Orthonormal%20Expansions%20for%20Translation-Invariant%20Kernels&body=Title%3A%20Orthonormal%20Expansions%20for%20Translation-Invariant%20Kernels%0AAuthor%3A%20Filip%20Tronarp%20and%20Toni%20Karvonen%0AAbstract%3A%20%20%20We%20present%20a%20general%20Fourier%20analytic%20technique%20for%20constructing%20orthonormal%0Abasis%20expansions%20of%20translation-invariant%20kernels%20from%20orthonormal%20bases%20of%0A%24%5Cmathscr%7BL%7D_2%28%5Cmathbb%7BR%7D%29%24.%20This%20allows%20us%20to%20derive%20explicit%20expansions%20on%0Athe%20real%20line%20for%20%28i%29%20Mat%5C%27ern%20kernels%20of%20all%20half-integer%20orders%20in%20terms%20of%0Aassociated%20Laguerre%20functions%2C%20%28ii%29%20the%20Cauchy%20kernel%20in%20terms%20of%20rational%0Afunctions%2C%20and%20%28iii%29%20the%20Gaussian%20kernel%20in%20terms%20of%20Hermite%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.08648v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orthonormal%20Expansions%20for%20Translation-Invariant%20Kernels&entry.906535625=Filip%20Tronarp%20and%20Toni%20Karvonen&entry.1292438233=%20%20We%20present%20a%20general%20Fourier%20analytic%20technique%20for%20constructing%20orthonormal%0Abasis%20expansions%20of%20translation-invariant%20kernels%20from%20orthonormal%20bases%20of%0A%24%5Cmathscr%7BL%7D_2%28%5Cmathbb%7BR%7D%29%24.%20This%20allows%20us%20to%20derive%20explicit%20expansions%20on%0Athe%20real%20line%20for%20%28i%29%20Mat%5C%27ern%20kernels%20of%20all%20half-integer%20orders%20in%20terms%20of%0Aassociated%20Laguerre%20functions%2C%20%28ii%29%20the%20Cauchy%20kernel%20in%20terms%20of%20rational%0Afunctions%2C%20and%20%28iii%29%20the%20Gaussian%20kernel%20in%20terms%20of%20Hermite%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.08648v4&entry.124074799=Read"},
{"title": "Enhancing Robotic Adaptability: Integrating Unsupervised Trajectory\n  Segmentation and Conditional ProMPs for Dynamic Learning Environments", "author": "Tianci Gao", "abstract": "  We propose a novel framework for enhancing robotic adaptability and learning\nefficiency, which integrates unsupervised trajectory segmentation with adaptive\nprobabilistic movement primitives (ProMPs). By employing a cutting-edge deep\nlearning architecture that combines autoencoders and Recurrent Neural Networks\n(RNNs), our approach autonomously pinpoints critical transitional points in\ncontinuous, unlabeled motion data, thus significantly reducing dependence on\nextensively labeled datasets. This innovative method dynamically adjusts motion\ntrajectories using conditional variables, significantly enhancing the\nflexibility and accuracy of robotic actions under dynamic conditions while also\nreducing the computational overhead associated with traditional robotic\nprogramming methods. Our experimental validation demonstrates superior learning\nefficiency and adaptability compared to existing techniques, paving the way for\nadvanced applications in industrial and service robotics.\n", "link": "http://arxiv.org/abs/2404.19412v1", "date": "2024-04-30", "relevancy": 1.7548, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6293}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6168}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Robotic%20Adaptability%3A%20Integrating%20Unsupervised%20Trajectory%0A%20%20Segmentation%20and%20Conditional%20ProMPs%20for%20Dynamic%20Learning%20Environments&body=Title%3A%20Enhancing%20Robotic%20Adaptability%3A%20Integrating%20Unsupervised%20Trajectory%0A%20%20Segmentation%20and%20Conditional%20ProMPs%20for%20Dynamic%20Learning%20Environments%0AAuthor%3A%20Tianci%20Gao%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20for%20enhancing%20robotic%20adaptability%20and%20learning%0Aefficiency%2C%20which%20integrates%20unsupervised%20trajectory%20segmentation%20with%20adaptive%0Aprobabilistic%20movement%20primitives%20%28ProMPs%29.%20By%20employing%20a%20cutting-edge%20deep%0Alearning%20architecture%20that%20combines%20autoencoders%20and%20Recurrent%20Neural%20Networks%0A%28RNNs%29%2C%20our%20approach%20autonomously%20pinpoints%20critical%20transitional%20points%20in%0Acontinuous%2C%20unlabeled%20motion%20data%2C%20thus%20significantly%20reducing%20dependence%20on%0Aextensively%20labeled%20datasets.%20This%20innovative%20method%20dynamically%20adjusts%20motion%0Atrajectories%20using%20conditional%20variables%2C%20significantly%20enhancing%20the%0Aflexibility%20and%20accuracy%20of%20robotic%20actions%20under%20dynamic%20conditions%20while%20also%0Areducing%20the%20computational%20overhead%20associated%20with%20traditional%20robotic%0Aprogramming%20methods.%20Our%20experimental%20validation%20demonstrates%20superior%20learning%0Aefficiency%20and%20adaptability%20compared%20to%20existing%20techniques%2C%20paving%20the%20way%20for%0Aadvanced%20applications%20in%20industrial%20and%20service%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Robotic%20Adaptability%3A%20Integrating%20Unsupervised%20Trajectory%0A%20%20Segmentation%20and%20Conditional%20ProMPs%20for%20Dynamic%20Learning%20Environments&entry.906535625=Tianci%20Gao&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20for%20enhancing%20robotic%20adaptability%20and%20learning%0Aefficiency%2C%20which%20integrates%20unsupervised%20trajectory%20segmentation%20with%20adaptive%0Aprobabilistic%20movement%20primitives%20%28ProMPs%29.%20By%20employing%20a%20cutting-edge%20deep%0Alearning%20architecture%20that%20combines%20autoencoders%20and%20Recurrent%20Neural%20Networks%0A%28RNNs%29%2C%20our%20approach%20autonomously%20pinpoints%20critical%20transitional%20points%20in%0Acontinuous%2C%20unlabeled%20motion%20data%2C%20thus%20significantly%20reducing%20dependence%20on%0Aextensively%20labeled%20datasets.%20This%20innovative%20method%20dynamically%20adjusts%20motion%0Atrajectories%20using%20conditional%20variables%2C%20significantly%20enhancing%20the%0Aflexibility%20and%20accuracy%20of%20robotic%20actions%20under%20dynamic%20conditions%20while%20also%0Areducing%20the%20computational%20overhead%20associated%20with%20traditional%20robotic%0Aprogramming%20methods.%20Our%20experimental%20validation%20demonstrates%20superior%20learning%0Aefficiency%20and%20adaptability%20compared%20to%20existing%20techniques%2C%20paving%20the%20way%20for%0Aadvanced%20applications%20in%20industrial%20and%20service%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19412v1&entry.124074799=Read"},
{"title": "Risk-aware Meta-level Decision Making for Exploration Under Uncertainty", "author": "Joshua Ott and Sung-Kyun Kim and Amanda Bouman and Oriana Peltzer and Mamoru Sobue and Harrison Delecki and Mykel J. Kochenderfer and Joel Burdick and Ali-akbar Agha-mohammadi", "abstract": "  Robotic exploration of unknown environments is fundamentally a problem of\ndecision making under uncertainty where the robot must account for uncertainty\nin sensor measurements, localization, action execution, as well as many other\nfactors. For large-scale exploration applications, autonomous systems must\novercome the challenges of sequentially deciding which areas of the environment\nare valuable to explore while safely evaluating the risks associated with\nobstacles and hazardous terrain. In this work, we propose a risk-aware\nmeta-level decision making framework to balance the tradeoffs associated with\nlocal and global exploration. Meta-level decision making builds upon classical\nhierarchical coverage planners by switching between local and global policies\nwith the overall objective of selecting the policy that is most likely to\nmaximize reward in a stochastic environment. We use information about the\nenvironment history, traversability risk, and kinodynamic constraints to reason\nabout the probability of successful policy execution to switch between local\nand global policies. We have validated our solution in both simulation and on a\nvariety of large-scale real world hardware tests. Our results show that by\nbalancing local and global exploration we are able to significantly explore\nlarge-scale environments more efficiently.\n", "link": "http://arxiv.org/abs/2209.05580v3", "date": "2024-04-30", "relevancy": 1.7329, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6325}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5583}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Risk-aware%20Meta-level%20Decision%20Making%20for%20Exploration%20Under%20Uncertainty&body=Title%3A%20Risk-aware%20Meta-level%20Decision%20Making%20for%20Exploration%20Under%20Uncertainty%0AAuthor%3A%20Joshua%20Ott%20and%20Sung-Kyun%20Kim%20and%20Amanda%20Bouman%20and%20Oriana%20Peltzer%20and%20Mamoru%20Sobue%20and%20Harrison%20Delecki%20and%20Mykel%20J.%20Kochenderfer%20and%20Joel%20Burdick%20and%20Ali-akbar%20Agha-mohammadi%0AAbstract%3A%20%20%20Robotic%20exploration%20of%20unknown%20environments%20is%20fundamentally%20a%20problem%20of%0Adecision%20making%20under%20uncertainty%20where%20the%20robot%20must%20account%20for%20uncertainty%0Ain%20sensor%20measurements%2C%20localization%2C%20action%20execution%2C%20as%20well%20as%20many%20other%0Afactors.%20For%20large-scale%20exploration%20applications%2C%20autonomous%20systems%20must%0Aovercome%20the%20challenges%20of%20sequentially%20deciding%20which%20areas%20of%20the%20environment%0Aare%20valuable%20to%20explore%20while%20safely%20evaluating%20the%20risks%20associated%20with%0Aobstacles%20and%20hazardous%20terrain.%20In%20this%20work%2C%20we%20propose%20a%20risk-aware%0Ameta-level%20decision%20making%20framework%20to%20balance%20the%20tradeoffs%20associated%20with%0Alocal%20and%20global%20exploration.%20Meta-level%20decision%20making%20builds%20upon%20classical%0Ahierarchical%20coverage%20planners%20by%20switching%20between%20local%20and%20global%20policies%0Awith%20the%20overall%20objective%20of%20selecting%20the%20policy%20that%20is%20most%20likely%20to%0Amaximize%20reward%20in%20a%20stochastic%20environment.%20We%20use%20information%20about%20the%0Aenvironment%20history%2C%20traversability%20risk%2C%20and%20kinodynamic%20constraints%20to%20reason%0Aabout%20the%20probability%20of%20successful%20policy%20execution%20to%20switch%20between%20local%0Aand%20global%20policies.%20We%20have%20validated%20our%20solution%20in%20both%20simulation%20and%20on%20a%0Avariety%20of%20large-scale%20real%20world%20hardware%20tests.%20Our%20results%20show%20that%20by%0Abalancing%20local%20and%20global%20exploration%20we%20are%20able%20to%20significantly%20explore%0Alarge-scale%20environments%20more%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.05580v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-aware%20Meta-level%20Decision%20Making%20for%20Exploration%20Under%20Uncertainty&entry.906535625=Joshua%20Ott%20and%20Sung-Kyun%20Kim%20and%20Amanda%20Bouman%20and%20Oriana%20Peltzer%20and%20Mamoru%20Sobue%20and%20Harrison%20Delecki%20and%20Mykel%20J.%20Kochenderfer%20and%20Joel%20Burdick%20and%20Ali-akbar%20Agha-mohammadi&entry.1292438233=%20%20Robotic%20exploration%20of%20unknown%20environments%20is%20fundamentally%20a%20problem%20of%0Adecision%20making%20under%20uncertainty%20where%20the%20robot%20must%20account%20for%20uncertainty%0Ain%20sensor%20measurements%2C%20localization%2C%20action%20execution%2C%20as%20well%20as%20many%20other%0Afactors.%20For%20large-scale%20exploration%20applications%2C%20autonomous%20systems%20must%0Aovercome%20the%20challenges%20of%20sequentially%20deciding%20which%20areas%20of%20the%20environment%0Aare%20valuable%20to%20explore%20while%20safely%20evaluating%20the%20risks%20associated%20with%0Aobstacles%20and%20hazardous%20terrain.%20In%20this%20work%2C%20we%20propose%20a%20risk-aware%0Ameta-level%20decision%20making%20framework%20to%20balance%20the%20tradeoffs%20associated%20with%0Alocal%20and%20global%20exploration.%20Meta-level%20decision%20making%20builds%20upon%20classical%0Ahierarchical%20coverage%20planners%20by%20switching%20between%20local%20and%20global%20policies%0Awith%20the%20overall%20objective%20of%20selecting%20the%20policy%20that%20is%20most%20likely%20to%0Amaximize%20reward%20in%20a%20stochastic%20environment.%20We%20use%20information%20about%20the%0Aenvironment%20history%2C%20traversability%20risk%2C%20and%20kinodynamic%20constraints%20to%20reason%0Aabout%20the%20probability%20of%20successful%20policy%20execution%20to%20switch%20between%20local%0Aand%20global%20policies.%20We%20have%20validated%20our%20solution%20in%20both%20simulation%20and%20on%20a%0Avariety%20of%20large-scale%20real%20world%20hardware%20tests.%20Our%20results%20show%20that%20by%0Abalancing%20local%20and%20global%20exploration%20we%20are%20able%20to%20significantly%20explore%0Alarge-scale%20environments%20more%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.05580v3&entry.124074799=Read"},
{"title": "AnomalyXFusion: Multi-modal Anomaly Synthesis with Diffusion", "author": "Jie Hu and Yawen Huang and Yilin Lu and Guoyang Xie and Guannan Jiang and Yefeng Zheng", "abstract": "  Anomaly synthesis is one of the effective methods to augment abnormal samples\nfor training. However, current anomaly synthesis methods predominantly rely on\ntexture information as input, which limits the fidelity of synthesized abnormal\nsamples. Because texture information is insufficient to correctly depict the\npattern of anomalies, especially for logical anomalies. To surmount this\nobstacle, we present the AnomalyXFusion framework, designed to harness\nmulti-modality information to enhance the quality of synthesized abnormal\nsamples. The AnomalyXFusion framework comprises two distinct yet synergistic\nmodules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion\n(DDF) module. The MIF module refines modality alignment by aggregating and\nintegrating various modality features into a unified embedding space, termed\nX-embedding, which includes image, text, and mask features. Concurrently, the\nDDF module facilitates controlled generation through an adaptive adjustment of\nX-embedding conditioned on the diffusion steps. In addition, to reveal the\nmulti-modality representational power of AnomalyXFusion, we propose a new\ndataset, called MVTec Caption. More precisely, MVTec Caption extends 2.2k\naccurate image-mask-text annotations for the MVTec AD and LOCO datasets.\nComprehensive evaluations demonstrate the effectiveness of AnomalyXFusion,\nespecially regarding the fidelity and diversity for logical anomalies. Project\npage: http:github.com/hujiecpp/MVTec-Caption\n", "link": "http://arxiv.org/abs/2404.19444v1", "date": "2024-04-30", "relevancy": 1.6919, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.571}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5625}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5607}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnomalyXFusion%3A%20Multi-modal%20Anomaly%20Synthesis%20with%20Diffusion&body=Title%3A%20AnomalyXFusion%3A%20Multi-modal%20Anomaly%20Synthesis%20with%20Diffusion%0AAuthor%3A%20Jie%20Hu%20and%20Yawen%20Huang%20and%20Yilin%20Lu%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Anomaly%20synthesis%20is%20one%20of%20the%20effective%20methods%20to%20augment%20abnormal%20samples%0Afor%20training.%20However%2C%20current%20anomaly%20synthesis%20methods%20predominantly%20rely%20on%0Atexture%20information%20as%20input%2C%20which%20limits%20the%20fidelity%20of%20synthesized%20abnormal%0Asamples.%20Because%20texture%20information%20is%20insufficient%20to%20correctly%20depict%20the%0Apattern%20of%20anomalies%2C%20especially%20for%20logical%20anomalies.%20To%20surmount%20this%0Aobstacle%2C%20we%20present%20the%20AnomalyXFusion%20framework%2C%20designed%20to%20harness%0Amulti-modality%20information%20to%20enhance%20the%20quality%20of%20synthesized%20abnormal%0Asamples.%20The%20AnomalyXFusion%20framework%20comprises%20two%20distinct%20yet%20synergistic%0Amodules%3A%20the%20Multi-modal%20In-Fusion%20%28MIF%29%20module%20and%20the%20Dynamic%20Dif-Fusion%0A%28DDF%29%20module.%20The%20MIF%20module%20refines%20modality%20alignment%20by%20aggregating%20and%0Aintegrating%20various%20modality%20features%20into%20a%20unified%20embedding%20space%2C%20termed%0AX-embedding%2C%20which%20includes%20image%2C%20text%2C%20and%20mask%20features.%20Concurrently%2C%20the%0ADDF%20module%20facilitates%20controlled%20generation%20through%20an%20adaptive%20adjustment%20of%0AX-embedding%20conditioned%20on%20the%20diffusion%20steps.%20In%20addition%2C%20to%20reveal%20the%0Amulti-modality%20representational%20power%20of%20AnomalyXFusion%2C%20we%20propose%20a%20new%0Adataset%2C%20called%20MVTec%20Caption.%20More%20precisely%2C%20MVTec%20Caption%20extends%202.2k%0Aaccurate%20image-mask-text%20annotations%20for%20the%20MVTec%20AD%20and%20LOCO%20datasets.%0AComprehensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20AnomalyXFusion%2C%0Aespecially%20regarding%20the%20fidelity%20and%20diversity%20for%20logical%20anomalies.%20Project%0Apage%3A%20http%3Agithub.com/hujiecpp/MVTec-Caption%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19444v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyXFusion%3A%20Multi-modal%20Anomaly%20Synthesis%20with%20Diffusion&entry.906535625=Jie%20Hu%20and%20Yawen%20Huang%20and%20Yilin%20Lu%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Anomaly%20synthesis%20is%20one%20of%20the%20effective%20methods%20to%20augment%20abnormal%20samples%0Afor%20training.%20However%2C%20current%20anomaly%20synthesis%20methods%20predominantly%20rely%20on%0Atexture%20information%20as%20input%2C%20which%20limits%20the%20fidelity%20of%20synthesized%20abnormal%0Asamples.%20Because%20texture%20information%20is%20insufficient%20to%20correctly%20depict%20the%0Apattern%20of%20anomalies%2C%20especially%20for%20logical%20anomalies.%20To%20surmount%20this%0Aobstacle%2C%20we%20present%20the%20AnomalyXFusion%20framework%2C%20designed%20to%20harness%0Amulti-modality%20information%20to%20enhance%20the%20quality%20of%20synthesized%20abnormal%0Asamples.%20The%20AnomalyXFusion%20framework%20comprises%20two%20distinct%20yet%20synergistic%0Amodules%3A%20the%20Multi-modal%20In-Fusion%20%28MIF%29%20module%20and%20the%20Dynamic%20Dif-Fusion%0A%28DDF%29%20module.%20The%20MIF%20module%20refines%20modality%20alignment%20by%20aggregating%20and%0Aintegrating%20various%20modality%20features%20into%20a%20unified%20embedding%20space%2C%20termed%0AX-embedding%2C%20which%20includes%20image%2C%20text%2C%20and%20mask%20features.%20Concurrently%2C%20the%0ADDF%20module%20facilitates%20controlled%20generation%20through%20an%20adaptive%20adjustment%20of%0AX-embedding%20conditioned%20on%20the%20diffusion%20steps.%20In%20addition%2C%20to%20reveal%20the%0Amulti-modality%20representational%20power%20of%20AnomalyXFusion%2C%20we%20propose%20a%20new%0Adataset%2C%20called%20MVTec%20Caption.%20More%20precisely%2C%20MVTec%20Caption%20extends%202.2k%0Aaccurate%20image-mask-text%20annotations%20for%20the%20MVTec%20AD%20and%20LOCO%20datasets.%0AComprehensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20AnomalyXFusion%2C%0Aespecially%20regarding%20the%20fidelity%20and%20diversity%20for%20logical%20anomalies.%20Project%0Apage%3A%20http%3Agithub.com/hujiecpp/MVTec-Caption%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19444v1&entry.124074799=Read"},
{"title": "Integrating Visuo-tactile Sensing with Haptic Feedback for Teleoperated\n  Robot Manipulation", "author": "Noah Becker and Erik Gattung and Kay Hansel and Tim Schneider and Yaonan Zhu and Yasuhisa Hasegawa and Jan Peters", "abstract": "  Telerobotics enables humans to overcome spatial constraints and allows them\nto physically interact with the environment in remote locations. However, the\nsensory feedback provided by the system to the operator is often purely visual,\nlimiting the operator's dexterity in manipulation tasks. In this work, we\naddress this issue by equipping the robot's end-effector with high-resolution\nvisuotactile GelSight sensors. Using low-cost MANUS-Gloves, we provide the\noperator with haptic feedback about forces acting at the points of contact in\nthe form of vibration signals. We propose two different methods for estimating\nthese forces; one based on estimating the movement of markers on the sensor\nsurface and one deep-learning approach. Additionally, we integrate our system\ninto a virtual-reality teleoperation pipeline in which a human operator\ncontrols both arms of a Tiago robot while receiving visual and haptic feedback.\nWe believe that integrating haptic feedback is a crucial step for dexterous\nmanipulation in teleoperated robotic systems.\n", "link": "http://arxiv.org/abs/2404.19585v1", "date": "2024-04-30", "relevancy": 1.6908, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5839}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5259}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Integrating%20Visuo-tactile%20Sensing%20with%20Haptic%20Feedback%20for%20Teleoperated%0A%20%20Robot%20Manipulation&body=Title%3A%20Integrating%20Visuo-tactile%20Sensing%20with%20Haptic%20Feedback%20for%20Teleoperated%0A%20%20Robot%20Manipulation%0AAuthor%3A%20Noah%20Becker%20and%20Erik%20Gattung%20and%20Kay%20Hansel%20and%20Tim%20Schneider%20and%20Yaonan%20Zhu%20and%20Yasuhisa%20Hasegawa%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Telerobotics%20enables%20humans%20to%20overcome%20spatial%20constraints%20and%20allows%20them%0Ato%20physically%20interact%20with%20the%20environment%20in%20remote%20locations.%20However%2C%20the%0Asensory%20feedback%20provided%20by%20the%20system%20to%20the%20operator%20is%20often%20purely%20visual%2C%0Alimiting%20the%20operator%27s%20dexterity%20in%20manipulation%20tasks.%20In%20this%20work%2C%20we%0Aaddress%20this%20issue%20by%20equipping%20the%20robot%27s%20end-effector%20with%20high-resolution%0Avisuotactile%20GelSight%20sensors.%20Using%20low-cost%20MANUS-Gloves%2C%20we%20provide%20the%0Aoperator%20with%20haptic%20feedback%20about%20forces%20acting%20at%20the%20points%20of%20contact%20in%0Athe%20form%20of%20vibration%20signals.%20We%20propose%20two%20different%20methods%20for%20estimating%0Athese%20forces%3B%20one%20based%20on%20estimating%20the%20movement%20of%20markers%20on%20the%20sensor%0Asurface%20and%20one%20deep-learning%20approach.%20Additionally%2C%20we%20integrate%20our%20system%0Ainto%20a%20virtual-reality%20teleoperation%20pipeline%20in%20which%20a%20human%20operator%0Acontrols%20both%20arms%20of%20a%20Tiago%20robot%20while%20receiving%20visual%20and%20haptic%20feedback.%0AWe%20believe%20that%20integrating%20haptic%20feedback%20is%20a%20crucial%20step%20for%20dexterous%0Amanipulation%20in%20teleoperated%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19585v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Visuo-tactile%20Sensing%20with%20Haptic%20Feedback%20for%20Teleoperated%0A%20%20Robot%20Manipulation&entry.906535625=Noah%20Becker%20and%20Erik%20Gattung%20and%20Kay%20Hansel%20and%20Tim%20Schneider%20and%20Yaonan%20Zhu%20and%20Yasuhisa%20Hasegawa%20and%20Jan%20Peters&entry.1292438233=%20%20Telerobotics%20enables%20humans%20to%20overcome%20spatial%20constraints%20and%20allows%20them%0Ato%20physically%20interact%20with%20the%20environment%20in%20remote%20locations.%20However%2C%20the%0Asensory%20feedback%20provided%20by%20the%20system%20to%20the%20operator%20is%20often%20purely%20visual%2C%0Alimiting%20the%20operator%27s%20dexterity%20in%20manipulation%20tasks.%20In%20this%20work%2C%20we%0Aaddress%20this%20issue%20by%20equipping%20the%20robot%27s%20end-effector%20with%20high-resolution%0Avisuotactile%20GelSight%20sensors.%20Using%20low-cost%20MANUS-Gloves%2C%20we%20provide%20the%0Aoperator%20with%20haptic%20feedback%20about%20forces%20acting%20at%20the%20points%20of%20contact%20in%0Athe%20form%20of%20vibration%20signals.%20We%20propose%20two%20different%20methods%20for%20estimating%0Athese%20forces%3B%20one%20based%20on%20estimating%20the%20movement%20of%20markers%20on%20the%20sensor%0Asurface%20and%20one%20deep-learning%20approach.%20Additionally%2C%20we%20integrate%20our%20system%0Ainto%20a%20virtual-reality%20teleoperation%20pipeline%20in%20which%20a%20human%20operator%0Acontrols%20both%20arms%20of%20a%20Tiago%20robot%20while%20receiving%20visual%20and%20haptic%20feedback.%0AWe%20believe%20that%20integrating%20haptic%20feedback%20is%20a%20crucial%20step%20for%20dexterous%0Amanipulation%20in%20teleoperated%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19585v1&entry.124074799=Read"},
{"title": "ProgDTD: Progressive Learned Image Compression with Double-Tail-Drop\n  Training", "author": "Ali Hojjat and Janek Haberer and Olaf Landsiedel", "abstract": "  Progressive compression allows images to start loading as low-resolution\nversions, becoming clearer as more data is received. This increases user\nexperience when, for example, network connections are slow. Today, most\napproaches for image compression, both classical and learned ones, are designed\nto be non-progressive. This paper introduces ProgDTD, a training method that\ntransforms learned, non-progressive image compression approaches into\nprogressive ones. The design of ProgDTD is based on the observation that the\ninformation stored within the bottleneck of a compression model commonly varies\nin importance. To create a progressive compression model, ProgDTD modifies the\ntraining steps to enforce the model to store the data in the bottleneck sorted\nby priority. We achieve progressive compression by transmitting the data in\norder of its sorted index. ProgDTD is designed for CNN-based learned image\ncompression models, does not need additional parameters, and has a customizable\nrange of progressiveness. For evaluation, we apply ProgDTDto the hyperprior\nmodel, one of the most common structures in learned image compression. Our\nexperimental results show that ProgDTD performs comparably to its\nnon-progressive counterparts and other state-of-the-art progressive models in\nterms of MS-SSIM and accuracy.\n", "link": "http://arxiv.org/abs/2305.02145v2", "date": "2024-04-30", "relevancy": 1.69, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5873}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5645}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5365}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ProgDTD%3A%20Progressive%20Learned%20Image%20Compression%20with%20Double-Tail-Drop%0A%20%20Training&body=Title%3A%20ProgDTD%3A%20Progressive%20Learned%20Image%20Compression%20with%20Double-Tail-Drop%0A%20%20Training%0AAuthor%3A%20Ali%20Hojjat%20and%20Janek%20Haberer%20and%20Olaf%20Landsiedel%0AAbstract%3A%20%20%20Progressive%20compression%20allows%20images%20to%20start%20loading%20as%20low-resolution%0Aversions%2C%20becoming%20clearer%20as%20more%20data%20is%20received.%20This%20increases%20user%0Aexperience%20when%2C%20for%20example%2C%20network%20connections%20are%20slow.%20Today%2C%20most%0Aapproaches%20for%20image%20compression%2C%20both%20classical%20and%20learned%20ones%2C%20are%20designed%0Ato%20be%20non-progressive.%20This%20paper%20introduces%20ProgDTD%2C%20a%20training%20method%20that%0Atransforms%20learned%2C%20non-progressive%20image%20compression%20approaches%20into%0Aprogressive%20ones.%20The%20design%20of%20ProgDTD%20is%20based%20on%20the%20observation%20that%20the%0Ainformation%20stored%20within%20the%20bottleneck%20of%20a%20compression%20model%20commonly%20varies%0Ain%20importance.%20To%20create%20a%20progressive%20compression%20model%2C%20ProgDTD%20modifies%20the%0Atraining%20steps%20to%20enforce%20the%20model%20to%20store%20the%20data%20in%20the%20bottleneck%20sorted%0Aby%20priority.%20We%20achieve%20progressive%20compression%20by%20transmitting%20the%20data%20in%0Aorder%20of%20its%20sorted%20index.%20ProgDTD%20is%20designed%20for%20CNN-based%20learned%20image%0Acompression%20models%2C%20does%20not%20need%20additional%20parameters%2C%20and%20has%20a%20customizable%0Arange%20of%20progressiveness.%20For%20evaluation%2C%20we%20apply%20ProgDTDto%20the%20hyperprior%0Amodel%2C%20one%20of%20the%20most%20common%20structures%20in%20learned%20image%20compression.%20Our%0Aexperimental%20results%20show%20that%20ProgDTD%20performs%20comparably%20to%20its%0Anon-progressive%20counterparts%20and%20other%20state-of-the-art%20progressive%20models%20in%0Aterms%20of%20MS-SSIM%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.02145v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProgDTD%3A%20Progressive%20Learned%20Image%20Compression%20with%20Double-Tail-Drop%0A%20%20Training&entry.906535625=Ali%20Hojjat%20and%20Janek%20Haberer%20and%20Olaf%20Landsiedel&entry.1292438233=%20%20Progressive%20compression%20allows%20images%20to%20start%20loading%20as%20low-resolution%0Aversions%2C%20becoming%20clearer%20as%20more%20data%20is%20received.%20This%20increases%20user%0Aexperience%20when%2C%20for%20example%2C%20network%20connections%20are%20slow.%20Today%2C%20most%0Aapproaches%20for%20image%20compression%2C%20both%20classical%20and%20learned%20ones%2C%20are%20designed%0Ato%20be%20non-progressive.%20This%20paper%20introduces%20ProgDTD%2C%20a%20training%20method%20that%0Atransforms%20learned%2C%20non-progressive%20image%20compression%20approaches%20into%0Aprogressive%20ones.%20The%20design%20of%20ProgDTD%20is%20based%20on%20the%20observation%20that%20the%0Ainformation%20stored%20within%20the%20bottleneck%20of%20a%20compression%20model%20commonly%20varies%0Ain%20importance.%20To%20create%20a%20progressive%20compression%20model%2C%20ProgDTD%20modifies%20the%0Atraining%20steps%20to%20enforce%20the%20model%20to%20store%20the%20data%20in%20the%20bottleneck%20sorted%0Aby%20priority.%20We%20achieve%20progressive%20compression%20by%20transmitting%20the%20data%20in%0Aorder%20of%20its%20sorted%20index.%20ProgDTD%20is%20designed%20for%20CNN-based%20learned%20image%0Acompression%20models%2C%20does%20not%20need%20additional%20parameters%2C%20and%20has%20a%20customizable%0Arange%20of%20progressiveness.%20For%20evaluation%2C%20we%20apply%20ProgDTDto%20the%20hyperprior%0Amodel%2C%20one%20of%20the%20most%20common%20structures%20in%20learned%20image%20compression.%20Our%0Aexperimental%20results%20show%20that%20ProgDTD%20performs%20comparably%20to%20its%0Anon-progressive%20counterparts%20and%20other%20state-of-the-art%20progressive%20models%20in%0Aterms%20of%20MS-SSIM%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.02145v2&entry.124074799=Read"},
{"title": "Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse\n  Inertial Sensors and Ultra-Wideband Ranging", "author": "Rayan Armani and Changlin Qian and Jiaxi Jiang and Christian Holz", "abstract": "  While camera-based capture systems remain the gold standard for recording\nhuman motion, learning-based tracking systems based on sparse wearable sensors\nare gaining popularity. Most commonly, they use inertial sensors, whose\npropensity for drift and jitter have so far limited tracking accuracy. In this\npaper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation\nmethod that constrains drift and jitter in inertial tracking via inter-sensor\ndistances. We estimate these distances across sparse sensor setups using a\nlightweight embedded tracker that augments inexpensive off-the-shelf 6D\ninertial measurement units with ultra-wideband radio-based\nranging$-$dynamically and without the need for stationary reference anchors.\nOur method then fuses these inter-sensor distances with the 3D states estimated\nfrom each sensor Our graph-based machine learning model processes the 3D states\nand distances to estimate a person's 3D full body pose and translation. To\ntrain our model, we synthesize inertial measurements and distance estimates\nfrom the motion capture database AMASS. For evaluation, we contribute a novel\nmotion dataset of 10 participants who performed 25 motion types, captured by 6\nwearable IMU+UWB trackers and an optical motion capture system, totaling 200\nminutes of synchronized sensor data (UIP-DB). Our extensive experiments show\nstate-of-the-art performance for our method over PIP and TIP, reducing position\nerror from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$\nto $0.055km/s^3$ (a reduction of $97\\%$).\n", "link": "http://arxiv.org/abs/2404.19541v1", "date": "2024-04-30", "relevancy": 1.6863, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5495}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ultra%20Inertial%20Poser%3A%20Scalable%20Motion%20Capture%20and%20Tracking%20from%20Sparse%0A%20%20Inertial%20Sensors%20and%20Ultra-Wideband%20Ranging&body=Title%3A%20Ultra%20Inertial%20Poser%3A%20Scalable%20Motion%20Capture%20and%20Tracking%20from%20Sparse%0A%20%20Inertial%20Sensors%20and%20Ultra-Wideband%20Ranging%0AAuthor%3A%20Rayan%20Armani%20and%20Changlin%20Qian%20and%20Jiaxi%20Jiang%20and%20Christian%20Holz%0AAbstract%3A%20%20%20While%20camera-based%20capture%20systems%20remain%20the%20gold%20standard%20for%20recording%0Ahuman%20motion%2C%20learning-based%20tracking%20systems%20based%20on%20sparse%20wearable%20sensors%0Aare%20gaining%20popularity.%20Most%20commonly%2C%20they%20use%20inertial%20sensors%2C%20whose%0Apropensity%20for%20drift%20and%20jitter%20have%20so%20far%20limited%20tracking%20accuracy.%20In%20this%0Apaper%2C%20we%20propose%20Ultra%20Inertial%20Poser%2C%20a%20novel%203D%20full%20body%20pose%20estimation%0Amethod%20that%20constrains%20drift%20and%20jitter%20in%20inertial%20tracking%20via%20inter-sensor%0Adistances.%20We%20estimate%20these%20distances%20across%20sparse%20sensor%20setups%20using%20a%0Alightweight%20embedded%20tracker%20that%20augments%20inexpensive%20off-the-shelf%206D%0Ainertial%20measurement%20units%20with%20ultra-wideband%20radio-based%0Aranging%24-%24dynamically%20and%20without%20the%20need%20for%20stationary%20reference%20anchors.%0AOur%20method%20then%20fuses%20these%20inter-sensor%20distances%20with%20the%203D%20states%20estimated%0Afrom%20each%20sensor%20Our%20graph-based%20machine%20learning%20model%20processes%20the%203D%20states%0Aand%20distances%20to%20estimate%20a%20person%27s%203D%20full%20body%20pose%20and%20translation.%20To%0Atrain%20our%20model%2C%20we%20synthesize%20inertial%20measurements%20and%20distance%20estimates%0Afrom%20the%20motion%20capture%20database%20AMASS.%20For%20evaluation%2C%20we%20contribute%20a%20novel%0Amotion%20dataset%20of%2010%20participants%20who%20performed%2025%20motion%20types%2C%20captured%20by%206%0Awearable%20IMU%2BUWB%20trackers%20and%20an%20optical%20motion%20capture%20system%2C%20totaling%20200%0Aminutes%20of%20synchronized%20sensor%20data%20%28UIP-DB%29.%20Our%20extensive%20experiments%20show%0Astate-of-the-art%20performance%20for%20our%20method%20over%20PIP%20and%20TIP%2C%20reducing%20position%0Aerror%20from%20%2413.62%24%20to%20%2410.65cm%24%20%28%2422%5C%25%24%20better%29%20and%20lowering%20jitter%20from%20%241.56%24%0Ato%20%240.055km/s%5E3%24%20%28a%20reduction%20of%20%2497%5C%25%24%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19541v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra%20Inertial%20Poser%3A%20Scalable%20Motion%20Capture%20and%20Tracking%20from%20Sparse%0A%20%20Inertial%20Sensors%20and%20Ultra-Wideband%20Ranging&entry.906535625=Rayan%20Armani%20and%20Changlin%20Qian%20and%20Jiaxi%20Jiang%20and%20Christian%20Holz&entry.1292438233=%20%20While%20camera-based%20capture%20systems%20remain%20the%20gold%20standard%20for%20recording%0Ahuman%20motion%2C%20learning-based%20tracking%20systems%20based%20on%20sparse%20wearable%20sensors%0Aare%20gaining%20popularity.%20Most%20commonly%2C%20they%20use%20inertial%20sensors%2C%20whose%0Apropensity%20for%20drift%20and%20jitter%20have%20so%20far%20limited%20tracking%20accuracy.%20In%20this%0Apaper%2C%20we%20propose%20Ultra%20Inertial%20Poser%2C%20a%20novel%203D%20full%20body%20pose%20estimation%0Amethod%20that%20constrains%20drift%20and%20jitter%20in%20inertial%20tracking%20via%20inter-sensor%0Adistances.%20We%20estimate%20these%20distances%20across%20sparse%20sensor%20setups%20using%20a%0Alightweight%20embedded%20tracker%20that%20augments%20inexpensive%20off-the-shelf%206D%0Ainertial%20measurement%20units%20with%20ultra-wideband%20radio-based%0Aranging%24-%24dynamically%20and%20without%20the%20need%20for%20stationary%20reference%20anchors.%0AOur%20method%20then%20fuses%20these%20inter-sensor%20distances%20with%20the%203D%20states%20estimated%0Afrom%20each%20sensor%20Our%20graph-based%20machine%20learning%20model%20processes%20the%203D%20states%0Aand%20distances%20to%20estimate%20a%20person%27s%203D%20full%20body%20pose%20and%20translation.%20To%0Atrain%20our%20model%2C%20we%20synthesize%20inertial%20measurements%20and%20distance%20estimates%0Afrom%20the%20motion%20capture%20database%20AMASS.%20For%20evaluation%2C%20we%20contribute%20a%20novel%0Amotion%20dataset%20of%2010%20participants%20who%20performed%2025%20motion%20types%2C%20captured%20by%206%0Awearable%20IMU%2BUWB%20trackers%20and%20an%20optical%20motion%20capture%20system%2C%20totaling%20200%0Aminutes%20of%20synchronized%20sensor%20data%20%28UIP-DB%29.%20Our%20extensive%20experiments%20show%0Astate-of-the-art%20performance%20for%20our%20method%20over%20PIP%20and%20TIP%2C%20reducing%20position%0Aerror%20from%20%2413.62%24%20to%20%2410.65cm%24%20%28%2422%5C%25%24%20better%29%20and%20lowering%20jitter%20from%20%241.56%24%0Ato%20%240.055km/s%5E3%24%20%28a%20reduction%20of%20%2497%5C%25%24%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19541v1&entry.124074799=Read"},
{"title": "Reactive Temporal Logic-based Planning and Control for Interactive\n  Robotic Tasks", "author": "Farhad Nawaz and Shaoting Peng and Lars Lindemann and Nadia Figueroa and Nikolai Matni", "abstract": "  Robots interacting with humans must be safe, reactive and adapt online to\nunforeseen environmental and task changes. Achieving these requirements\nconcurrently is a challenge as interactive planners lack formal safety\nguarantees, while safe motion planners lack flexibility to adapt. To tackle\nthis, we propose a modular control architecture that generates both safe and\nreactive motion plans for human-robot interaction by integrating temporal\nlogic-based discrete task level plans with continuous Dynamical System\n(DS)-based motion plans. We formulate a reactive temporal logic formula that\nenables users to define task specifications through structured language, and\npropose a planning algorithm at the task level that generates a sequence of\ndesired robot behaviors while being adaptive to environmental changes. At the\nmotion level, we incorporate control Lyapunov functions and control barrier\nfunctions to compute stable and safe continuous motion plans for two types of\nrobot behaviors: (i) complex, possibly periodic motions given by autonomous DS\nand (ii) time-critical tasks specified by Signal Temporal Logic~(STL). Our\nmethodology is demonstrated on the Franka robot arm performing wiping tasks on\na whiteboard and a mannequin that is compliant to human interactions and\nadaptive to environmental changes.\n", "link": "http://arxiv.org/abs/2404.19594v1", "date": "2024-04-30", "relevancy": 1.6857, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5307}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reactive%20Temporal%20Logic-based%20Planning%20and%20Control%20for%20Interactive%0A%20%20Robotic%20Tasks&body=Title%3A%20Reactive%20Temporal%20Logic-based%20Planning%20and%20Control%20for%20Interactive%0A%20%20Robotic%20Tasks%0AAuthor%3A%20Farhad%20Nawaz%20and%20Shaoting%20Peng%20and%20Lars%20Lindemann%20and%20Nadia%20Figueroa%20and%20Nikolai%20Matni%0AAbstract%3A%20%20%20Robots%20interacting%20with%20humans%20must%20be%20safe%2C%20reactive%20and%20adapt%20online%20to%0Aunforeseen%20environmental%20and%20task%20changes.%20Achieving%20these%20requirements%0Aconcurrently%20is%20a%20challenge%20as%20interactive%20planners%20lack%20formal%20safety%0Aguarantees%2C%20while%20safe%20motion%20planners%20lack%20flexibility%20to%20adapt.%20To%20tackle%0Athis%2C%20we%20propose%20a%20modular%20control%20architecture%20that%20generates%20both%20safe%20and%0Areactive%20motion%20plans%20for%20human-robot%20interaction%20by%20integrating%20temporal%0Alogic-based%20discrete%20task%20level%20plans%20with%20continuous%20Dynamical%20System%0A%28DS%29-based%20motion%20plans.%20We%20formulate%20a%20reactive%20temporal%20logic%20formula%20that%0Aenables%20users%20to%20define%20task%20specifications%20through%20structured%20language%2C%20and%0Apropose%20a%20planning%20algorithm%20at%20the%20task%20level%20that%20generates%20a%20sequence%20of%0Adesired%20robot%20behaviors%20while%20being%20adaptive%20to%20environmental%20changes.%20At%20the%0Amotion%20level%2C%20we%20incorporate%20control%20Lyapunov%20functions%20and%20control%20barrier%0Afunctions%20to%20compute%20stable%20and%20safe%20continuous%20motion%20plans%20for%20two%20types%20of%0Arobot%20behaviors%3A%20%28i%29%20complex%2C%20possibly%20periodic%20motions%20given%20by%20autonomous%20DS%0Aand%20%28ii%29%20time-critical%20tasks%20specified%20by%20Signal%20Temporal%20Logic~%28STL%29.%20Our%0Amethodology%20is%20demonstrated%20on%20the%20Franka%20robot%20arm%20performing%20wiping%20tasks%20on%0Aa%20whiteboard%20and%20a%20mannequin%20that%20is%20compliant%20to%20human%20interactions%20and%0Aadaptive%20to%20environmental%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19594v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reactive%20Temporal%20Logic-based%20Planning%20and%20Control%20for%20Interactive%0A%20%20Robotic%20Tasks&entry.906535625=Farhad%20Nawaz%20and%20Shaoting%20Peng%20and%20Lars%20Lindemann%20and%20Nadia%20Figueroa%20and%20Nikolai%20Matni&entry.1292438233=%20%20Robots%20interacting%20with%20humans%20must%20be%20safe%2C%20reactive%20and%20adapt%20online%20to%0Aunforeseen%20environmental%20and%20task%20changes.%20Achieving%20these%20requirements%0Aconcurrently%20is%20a%20challenge%20as%20interactive%20planners%20lack%20formal%20safety%0Aguarantees%2C%20while%20safe%20motion%20planners%20lack%20flexibility%20to%20adapt.%20To%20tackle%0Athis%2C%20we%20propose%20a%20modular%20control%20architecture%20that%20generates%20both%20safe%20and%0Areactive%20motion%20plans%20for%20human-robot%20interaction%20by%20integrating%20temporal%0Alogic-based%20discrete%20task%20level%20plans%20with%20continuous%20Dynamical%20System%0A%28DS%29-based%20motion%20plans.%20We%20formulate%20a%20reactive%20temporal%20logic%20formula%20that%0Aenables%20users%20to%20define%20task%20specifications%20through%20structured%20language%2C%20and%0Apropose%20a%20planning%20algorithm%20at%20the%20task%20level%20that%20generates%20a%20sequence%20of%0Adesired%20robot%20behaviors%20while%20being%20adaptive%20to%20environmental%20changes.%20At%20the%0Amotion%20level%2C%20we%20incorporate%20control%20Lyapunov%20functions%20and%20control%20barrier%0Afunctions%20to%20compute%20stable%20and%20safe%20continuous%20motion%20plans%20for%20two%20types%20of%0Arobot%20behaviors%3A%20%28i%29%20complex%2C%20possibly%20periodic%20motions%20given%20by%20autonomous%20DS%0Aand%20%28ii%29%20time-critical%20tasks%20specified%20by%20Signal%20Temporal%20Logic~%28STL%29.%20Our%0Amethodology%20is%20demonstrated%20on%20the%20Franka%20robot%20arm%20performing%20wiping%20tasks%20on%0Aa%20whiteboard%20and%20a%20mannequin%20that%20is%20compliant%20to%20human%20interactions%20and%0Aadaptive%20to%20environmental%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19594v1&entry.124074799=Read"},
{"title": "E-Valuating Classifier Two-Sample Tests", "author": "Teodora Pandeva and Tim Bakker and Christian A. Naesseth and Patrick Forr\u00e9", "abstract": "  We introduce a powerful deep classifier two-sample test for high-dimensional\ndata based on E-values, called E-value Classifier Two-Sample Test (E-C2ST). Our\ntest combines ideas from existing work on split likelihood ratio tests and\npredictive independence tests. The resulting E-values are suitable for\nanytime-valid sequential two-sample tests. This feature allows for more\neffective use of data in constructing test statistics. Through simulations and\nreal data applications, we empirically demonstrate that E-C2ST achieves\nenhanced statistical power by partitioning datasets into multiple batches\nbeyond the conventional two-split (training and testing) approach of standard\nclassifier two-sample tests. This strategy increases the power of the test\nwhile keeping the type I error well below the desired significance level.\n", "link": "http://arxiv.org/abs/2210.13027v2", "date": "2024-04-30", "relevancy": 1.6777, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4318}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4199}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3871}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20E-Valuating%20Classifier%20Two-Sample%20Tests&body=Title%3A%20E-Valuating%20Classifier%20Two-Sample%20Tests%0AAuthor%3A%20Teodora%20Pandeva%20and%20Tim%20Bakker%20and%20Christian%20A.%20Naesseth%20and%20Patrick%20Forr%C3%A9%0AAbstract%3A%20%20%20We%20introduce%20a%20powerful%20deep%20classifier%20two-sample%20test%20for%20high-dimensional%0Adata%20based%20on%20E-values%2C%20called%20E-value%20Classifier%20Two-Sample%20Test%20%28E-C2ST%29.%20Our%0Atest%20combines%20ideas%20from%20existing%20work%20on%20split%20likelihood%20ratio%20tests%20and%0Apredictive%20independence%20tests.%20The%20resulting%20E-values%20are%20suitable%20for%0Aanytime-valid%20sequential%20two-sample%20tests.%20This%20feature%20allows%20for%20more%0Aeffective%20use%20of%20data%20in%20constructing%20test%20statistics.%20Through%20simulations%20and%0Areal%20data%20applications%2C%20we%20empirically%20demonstrate%20that%20E-C2ST%20achieves%0Aenhanced%20statistical%20power%20by%20partitioning%20datasets%20into%20multiple%20batches%0Abeyond%20the%20conventional%20two-split%20%28training%20and%20testing%29%20approach%20of%20standard%0Aclassifier%20two-sample%20tests.%20This%20strategy%20increases%20the%20power%20of%20the%20test%0Awhile%20keeping%20the%20type%20I%20error%20well%20below%20the%20desired%20significance%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.13027v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E-Valuating%20Classifier%20Two-Sample%20Tests&entry.906535625=Teodora%20Pandeva%20and%20Tim%20Bakker%20and%20Christian%20A.%20Naesseth%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20We%20introduce%20a%20powerful%20deep%20classifier%20two-sample%20test%20for%20high-dimensional%0Adata%20based%20on%20E-values%2C%20called%20E-value%20Classifier%20Two-Sample%20Test%20%28E-C2ST%29.%20Our%0Atest%20combines%20ideas%20from%20existing%20work%20on%20split%20likelihood%20ratio%20tests%20and%0Apredictive%20independence%20tests.%20The%20resulting%20E-values%20are%20suitable%20for%0Aanytime-valid%20sequential%20two-sample%20tests.%20This%20feature%20allows%20for%20more%0Aeffective%20use%20of%20data%20in%20constructing%20test%20statistics.%20Through%20simulations%20and%0Areal%20data%20applications%2C%20we%20empirically%20demonstrate%20that%20E-C2ST%20achieves%0Aenhanced%20statistical%20power%20by%20partitioning%20datasets%20into%20multiple%20batches%0Abeyond%20the%20conventional%20two-split%20%28training%20and%20testing%29%20approach%20of%20standard%0Aclassifier%20two-sample%20tests.%20This%20strategy%20increases%20the%20power%20of%20the%20test%0Awhile%20keeping%20the%20type%20I%20error%20well%20below%20the%20desired%20significance%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.13027v2&entry.124074799=Read"},
{"title": "How good are Large Language Models on African Languages?", "author": "Jessica Ojo and Kelechi Ogueji and Pontus Stenetorp and David Ifeoluwa Adelani", "abstract": "  Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on tasks and languages\nthey are not trained on. However, their performance on African languages is\nlargely understudied relative to high-resource languages. We present an\nanalysis of four popular large language models (mT0, Aya, LLaMa 2, and GPT-4)\non six tasks (topic classification, sentiment classification, machine\ntranslation, summarization, question answering, and named entity recognition)\nacross 60 African languages, spanning different language families and\ngeographical regions. Our results suggest that all LLMs produce lower\nperformance for African languages, and there is a large gap in performance\ncompared to high-resource languages (such as English) for most tasks. We find\nthat GPT-4 has an average to good performance on classification tasks, yet its\nperformance on generative tasks such as machine translation and summarization\nis significantly lacking. Surprisingly, we find that mT0 had the best overall\nperformance for cross-lingual QA, better than the state-of-the-art supervised\nmodel (i.e. fine-tuned mT5) and GPT-4 on African languages. Similarly, we find\nthe recent Aya model to have comparable result to mT0 in almost all tasks\nexcept for topic classification where it outperform mT0. Overall, LLaMa 2\nshowed the worst performance, which we believe is due to its English and\ncode-centric~(around 98%) pre-training corpus. Our findings confirms that\nperformance on African languages continues to remain a hurdle for the current\nLLMs, underscoring the need for additional efforts to close this gap.\n", "link": "http://arxiv.org/abs/2311.07978v2", "date": "2024-04-30", "relevancy": 1.671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4356}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.42}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20good%20are%20Large%20Language%20Models%20on%20African%20Languages%3F&body=Title%3A%20How%20good%20are%20Large%20Language%20Models%20on%20African%20Languages%3F%0AAuthor%3A%20Jessica%20Ojo%20and%20Kelechi%20Ogueji%20and%20Pontus%20Stenetorp%20and%20David%20Ifeoluwa%20Adelani%0AAbstract%3A%20%20%20Recent%20advancements%20in%20natural%20language%20processing%20have%20led%20to%20the%0Aproliferation%20of%20large%20language%20models%20%28LLMs%29.%20These%20models%20have%20been%20shown%20to%0Ayield%20good%20performance%2C%20using%20in-context%20learning%2C%20even%20on%20tasks%20and%20languages%0Athey%20are%20not%20trained%20on.%20However%2C%20their%20performance%20on%20African%20languages%20is%0Alargely%20understudied%20relative%20to%20high-resource%20languages.%20We%20present%20an%0Aanalysis%20of%20four%20popular%20large%20language%20models%20%28mT0%2C%20Aya%2C%20LLaMa%202%2C%20and%20GPT-4%29%0Aon%20six%20tasks%20%28topic%20classification%2C%20sentiment%20classification%2C%20machine%0Atranslation%2C%20summarization%2C%20question%20answering%2C%20and%20named%20entity%20recognition%29%0Aacross%2060%20African%20languages%2C%20spanning%20different%20language%20families%20and%0Ageographical%20regions.%20Our%20results%20suggest%20that%20all%20LLMs%20produce%20lower%0Aperformance%20for%20African%20languages%2C%20and%20there%20is%20a%20large%20gap%20in%20performance%0Acompared%20to%20high-resource%20languages%20%28such%20as%20English%29%20for%20most%20tasks.%20We%20find%0Athat%20GPT-4%20has%20an%20average%20to%20good%20performance%20on%20classification%20tasks%2C%20yet%20its%0Aperformance%20on%20generative%20tasks%20such%20as%20machine%20translation%20and%20summarization%0Ais%20significantly%20lacking.%20Surprisingly%2C%20we%20find%20that%20mT0%20had%20the%20best%20overall%0Aperformance%20for%20cross-lingual%20QA%2C%20better%20than%20the%20state-of-the-art%20supervised%0Amodel%20%28i.e.%20fine-tuned%20mT5%29%20and%20GPT-4%20on%20African%20languages.%20Similarly%2C%20we%20find%0Athe%20recent%20Aya%20model%20to%20have%20comparable%20result%20to%20mT0%20in%20almost%20all%20tasks%0Aexcept%20for%20topic%20classification%20where%20it%20outperform%20mT0.%20Overall%2C%20LLaMa%202%0Ashowed%20the%20worst%20performance%2C%20which%20we%20believe%20is%20due%20to%20its%20English%20and%0Acode-centric~%28around%2098%25%29%20pre-training%20corpus.%20Our%20findings%20confirms%20that%0Aperformance%20on%20African%20languages%20continues%20to%20remain%20a%20hurdle%20for%20the%20current%0ALLMs%2C%20underscoring%20the%20need%20for%20additional%20efforts%20to%20close%20this%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07978v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20good%20are%20Large%20Language%20Models%20on%20African%20Languages%3F&entry.906535625=Jessica%20Ojo%20and%20Kelechi%20Ogueji%20and%20Pontus%20Stenetorp%20and%20David%20Ifeoluwa%20Adelani&entry.1292438233=%20%20Recent%20advancements%20in%20natural%20language%20processing%20have%20led%20to%20the%0Aproliferation%20of%20large%20language%20models%20%28LLMs%29.%20These%20models%20have%20been%20shown%20to%0Ayield%20good%20performance%2C%20using%20in-context%20learning%2C%20even%20on%20tasks%20and%20languages%0Athey%20are%20not%20trained%20on.%20However%2C%20their%20performance%20on%20African%20languages%20is%0Alargely%20understudied%20relative%20to%20high-resource%20languages.%20We%20present%20an%0Aanalysis%20of%20four%20popular%20large%20language%20models%20%28mT0%2C%20Aya%2C%20LLaMa%202%2C%20and%20GPT-4%29%0Aon%20six%20tasks%20%28topic%20classification%2C%20sentiment%20classification%2C%20machine%0Atranslation%2C%20summarization%2C%20question%20answering%2C%20and%20named%20entity%20recognition%29%0Aacross%2060%20African%20languages%2C%20spanning%20different%20language%20families%20and%0Ageographical%20regions.%20Our%20results%20suggest%20that%20all%20LLMs%20produce%20lower%0Aperformance%20for%20African%20languages%2C%20and%20there%20is%20a%20large%20gap%20in%20performance%0Acompared%20to%20high-resource%20languages%20%28such%20as%20English%29%20for%20most%20tasks.%20We%20find%0Athat%20GPT-4%20has%20an%20average%20to%20good%20performance%20on%20classification%20tasks%2C%20yet%20its%0Aperformance%20on%20generative%20tasks%20such%20as%20machine%20translation%20and%20summarization%0Ais%20significantly%20lacking.%20Surprisingly%2C%20we%20find%20that%20mT0%20had%20the%20best%20overall%0Aperformance%20for%20cross-lingual%20QA%2C%20better%20than%20the%20state-of-the-art%20supervised%0Amodel%20%28i.e.%20fine-tuned%20mT5%29%20and%20GPT-4%20on%20African%20languages.%20Similarly%2C%20we%20find%0Athe%20recent%20Aya%20model%20to%20have%20comparable%20result%20to%20mT0%20in%20almost%20all%20tasks%0Aexcept%20for%20topic%20classification%20where%20it%20outperform%20mT0.%20Overall%2C%20LLaMa%202%0Ashowed%20the%20worst%20performance%2C%20which%20we%20believe%20is%20due%20to%20its%20English%20and%0Acode-centric~%28around%2098%25%29%20pre-training%20corpus.%20Our%20findings%20confirms%20that%0Aperformance%20on%20African%20languages%20continues%20to%20remain%20a%20hurdle%20for%20the%20current%0ALLMs%2C%20underscoring%20the%20need%20for%20additional%20efforts%20to%20close%20this%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07978v2&entry.124074799=Read"},
{"title": "Bridging Intelligence and Instinct: A New Control Paradigm for\n  Autonomous Robots", "author": "Shimian Zhang and Qiuhong Lu", "abstract": "  As the advent of artificial general intelligence (AGI) progresses at a\nbreathtaking pace, the application of large language models (LLMs) as AI Agents\nin robotics remains in its nascent stage. A significant concern that hampers\nthe seamless integration of these AI Agents into robotics is the\nunpredictability of the content they generate, a phenomena known as\n``hallucination''. Drawing inspiration from biological neural systems, we\npropose a novel, layered architecture for autonomous robotics, bridging AI\nagent intelligence and robot instinct. In this context, we define Robot\nInstinct as the innate or learned set of responses and priorities in an\nautonomous robotic system that ensures survival-essential tasks, such as safety\nassurance and obstacle avoidance, are carried out in a timely and effective\nmanner. This paradigm harmoniously combines the intelligence of LLMs with the\ninstinct of robotic behaviors, contributing to a more safe and versatile\nautonomous robotic system. As a case study, we illustrate this paradigm within\nthe context of a mobile robot, demonstrating its potential to significantly\nenhance autonomous robotics and enabling a future where robots can operate\nindependently and safely across diverse environments.\n", "link": "http://arxiv.org/abs/2307.10690v2", "date": "2024-04-30", "relevancy": 1.6709, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5249}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20Intelligence%20and%20Instinct%3A%20A%20New%20Control%20Paradigm%20for%0A%20%20Autonomous%20Robots&body=Title%3A%20Bridging%20Intelligence%20and%20Instinct%3A%20A%20New%20Control%20Paradigm%20for%0A%20%20Autonomous%20Robots%0AAuthor%3A%20Shimian%20Zhang%20and%20Qiuhong%20Lu%0AAbstract%3A%20%20%20As%20the%20advent%20of%20artificial%20general%20intelligence%20%28AGI%29%20progresses%20at%20a%0Abreathtaking%20pace%2C%20the%20application%20of%20large%20language%20models%20%28LLMs%29%20as%20AI%20Agents%0Ain%20robotics%20remains%20in%20its%20nascent%20stage.%20A%20significant%20concern%20that%20hampers%0Athe%20seamless%20integration%20of%20these%20AI%20Agents%20into%20robotics%20is%20the%0Aunpredictability%20of%20the%20content%20they%20generate%2C%20a%20phenomena%20known%20as%0A%60%60hallucination%27%27.%20Drawing%20inspiration%20from%20biological%20neural%20systems%2C%20we%0Apropose%20a%20novel%2C%20layered%20architecture%20for%20autonomous%20robotics%2C%20bridging%20AI%0Aagent%20intelligence%20and%20robot%20instinct.%20In%20this%20context%2C%20we%20define%20Robot%0AInstinct%20as%20the%20innate%20or%20learned%20set%20of%20responses%20and%20priorities%20in%20an%0Aautonomous%20robotic%20system%20that%20ensures%20survival-essential%20tasks%2C%20such%20as%20safety%0Aassurance%20and%20obstacle%20avoidance%2C%20are%20carried%20out%20in%20a%20timely%20and%20effective%0Amanner.%20This%20paradigm%20harmoniously%20combines%20the%20intelligence%20of%20LLMs%20with%20the%0Ainstinct%20of%20robotic%20behaviors%2C%20contributing%20to%20a%20more%20safe%20and%20versatile%0Aautonomous%20robotic%20system.%20As%20a%20case%20study%2C%20we%20illustrate%20this%20paradigm%20within%0Athe%20context%20of%20a%20mobile%20robot%2C%20demonstrating%20its%20potential%20to%20significantly%0Aenhance%20autonomous%20robotics%20and%20enabling%20a%20future%20where%20robots%20can%20operate%0Aindependently%20and%20safely%20across%20diverse%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10690v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Intelligence%20and%20Instinct%3A%20A%20New%20Control%20Paradigm%20for%0A%20%20Autonomous%20Robots&entry.906535625=Shimian%20Zhang%20and%20Qiuhong%20Lu&entry.1292438233=%20%20As%20the%20advent%20of%20artificial%20general%20intelligence%20%28AGI%29%20progresses%20at%20a%0Abreathtaking%20pace%2C%20the%20application%20of%20large%20language%20models%20%28LLMs%29%20as%20AI%20Agents%0Ain%20robotics%20remains%20in%20its%20nascent%20stage.%20A%20significant%20concern%20that%20hampers%0Athe%20seamless%20integration%20of%20these%20AI%20Agents%20into%20robotics%20is%20the%0Aunpredictability%20of%20the%20content%20they%20generate%2C%20a%20phenomena%20known%20as%0A%60%60hallucination%27%27.%20Drawing%20inspiration%20from%20biological%20neural%20systems%2C%20we%0Apropose%20a%20novel%2C%20layered%20architecture%20for%20autonomous%20robotics%2C%20bridging%20AI%0Aagent%20intelligence%20and%20robot%20instinct.%20In%20this%20context%2C%20we%20define%20Robot%0AInstinct%20as%20the%20innate%20or%20learned%20set%20of%20responses%20and%20priorities%20in%20an%0Aautonomous%20robotic%20system%20that%20ensures%20survival-essential%20tasks%2C%20such%20as%20safety%0Aassurance%20and%20obstacle%20avoidance%2C%20are%20carried%20out%20in%20a%20timely%20and%20effective%0Amanner.%20This%20paradigm%20harmoniously%20combines%20the%20intelligence%20of%20LLMs%20with%20the%0Ainstinct%20of%20robotic%20behaviors%2C%20contributing%20to%20a%20more%20safe%20and%20versatile%0Aautonomous%20robotic%20system.%20As%20a%20case%20study%2C%20we%20illustrate%20this%20paradigm%20within%0Athe%20context%20of%20a%20mobile%20robot%2C%20demonstrating%20its%20potential%20to%20significantly%0Aenhance%20autonomous%20robotics%20and%20enabling%20a%20future%20where%20robots%20can%20operate%0Aindependently%20and%20safely%20across%20diverse%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10690v2&entry.124074799=Read"},
{"title": "Fast and Accurate Unknown Object Instance Segmentation through\n  Error-Informed Refinement", "author": "Seunghyeok Back and Sangbeom Lee and Kangmin Kim and Joosoon Lee and Sungho Shin and Jemo Maeng and Kyoobin Lee", "abstract": "  Accurate perception of unknown objects is essential for autonomous robots,\nparticularly when manipulating novel items in unstructured environments.\nHowever, existing unknown object instance segmentation (UOIS) methods often\nhave over-segmentation and under-segmentation problems, resulting in inaccurate\ninstance boundaries and failures in subsequent robotic tasks such as grasping\nand placement. To address this challenge, this article introduces INSTA-BEER, a\nfast and accurate model-agnostic refinement method that enhances the UOIS\nperformance. The model adopts an error-informed refinement approach, which\nfirst predicts pixel-wise errors in the initial segmentation and then refines\nthe segmentation guided by these error estimates. We introduce the quad-metric\nboundary error, which quantifies pixel-wise true positives, true negatives,\nfalse positives, and false negatives at the boundaries of object instances,\neffectively capturing both fine-grained and instance-level segmentation errors.\nAdditionally, the Error Guidance Fusion (EGF) module explicitly integrates\nerror information into the refinement process, further improving segmentation\nquality. In comprehensive evaluations conducted on three widely used benchmark\ndatasets, INSTA-BEER outperformed state-of-the-art models in both accuracy and\ninference time. Moreover, a real-world robotic experiment demonstrated the\npractical applicability of our method in improving the performance of target\nobject grasping tasks in cluttered environments.\n", "link": "http://arxiv.org/abs/2306.16132v2", "date": "2024-04-30", "relevancy": 1.6669, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Accurate%20Unknown%20Object%20Instance%20Segmentation%20through%0A%20%20Error-Informed%20Refinement&body=Title%3A%20Fast%20and%20Accurate%20Unknown%20Object%20Instance%20Segmentation%20through%0A%20%20Error-Informed%20Refinement%0AAuthor%3A%20Seunghyeok%20Back%20and%20Sangbeom%20Lee%20and%20Kangmin%20Kim%20and%20Joosoon%20Lee%20and%20Sungho%20Shin%20and%20Jemo%20Maeng%20and%20Kyoobin%20Lee%0AAbstract%3A%20%20%20Accurate%20perception%20of%20unknown%20objects%20is%20essential%20for%20autonomous%20robots%2C%0Aparticularly%20when%20manipulating%20novel%20items%20in%20unstructured%20environments.%0AHowever%2C%20existing%20unknown%20object%20instance%20segmentation%20%28UOIS%29%20methods%20often%0Ahave%20over-segmentation%20and%20under-segmentation%20problems%2C%20resulting%20in%20inaccurate%0Ainstance%20boundaries%20and%20failures%20in%20subsequent%20robotic%20tasks%20such%20as%20grasping%0Aand%20placement.%20To%20address%20this%20challenge%2C%20this%20article%20introduces%20INSTA-BEER%2C%20a%0Afast%20and%20accurate%20model-agnostic%20refinement%20method%20that%20enhances%20the%20UOIS%0Aperformance.%20The%20model%20adopts%20an%20error-informed%20refinement%20approach%2C%20which%0Afirst%20predicts%20pixel-wise%20errors%20in%20the%20initial%20segmentation%20and%20then%20refines%0Athe%20segmentation%20guided%20by%20these%20error%20estimates.%20We%20introduce%20the%20quad-metric%0Aboundary%20error%2C%20which%20quantifies%20pixel-wise%20true%20positives%2C%20true%20negatives%2C%0Afalse%20positives%2C%20and%20false%20negatives%20at%20the%20boundaries%20of%20object%20instances%2C%0Aeffectively%20capturing%20both%20fine-grained%20and%20instance-level%20segmentation%20errors.%0AAdditionally%2C%20the%20Error%20Guidance%20Fusion%20%28EGF%29%20module%20explicitly%20integrates%0Aerror%20information%20into%20the%20refinement%20process%2C%20further%20improving%20segmentation%0Aquality.%20In%20comprehensive%20evaluations%20conducted%20on%20three%20widely%20used%20benchmark%0Adatasets%2C%20INSTA-BEER%20outperformed%20state-of-the-art%20models%20in%20both%20accuracy%20and%0Ainference%20time.%20Moreover%2C%20a%20real-world%20robotic%20experiment%20demonstrated%20the%0Apractical%20applicability%20of%20our%20method%20in%20improving%20the%20performance%20of%20target%0Aobject%20grasping%20tasks%20in%20cluttered%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16132v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Accurate%20Unknown%20Object%20Instance%20Segmentation%20through%0A%20%20Error-Informed%20Refinement&entry.906535625=Seunghyeok%20Back%20and%20Sangbeom%20Lee%20and%20Kangmin%20Kim%20and%20Joosoon%20Lee%20and%20Sungho%20Shin%20and%20Jemo%20Maeng%20and%20Kyoobin%20Lee&entry.1292438233=%20%20Accurate%20perception%20of%20unknown%20objects%20is%20essential%20for%20autonomous%20robots%2C%0Aparticularly%20when%20manipulating%20novel%20items%20in%20unstructured%20environments.%0AHowever%2C%20existing%20unknown%20object%20instance%20segmentation%20%28UOIS%29%20methods%20often%0Ahave%20over-segmentation%20and%20under-segmentation%20problems%2C%20resulting%20in%20inaccurate%0Ainstance%20boundaries%20and%20failures%20in%20subsequent%20robotic%20tasks%20such%20as%20grasping%0Aand%20placement.%20To%20address%20this%20challenge%2C%20this%20article%20introduces%20INSTA-BEER%2C%20a%0Afast%20and%20accurate%20model-agnostic%20refinement%20method%20that%20enhances%20the%20UOIS%0Aperformance.%20The%20model%20adopts%20an%20error-informed%20refinement%20approach%2C%20which%0Afirst%20predicts%20pixel-wise%20errors%20in%20the%20initial%20segmentation%20and%20then%20refines%0Athe%20segmentation%20guided%20by%20these%20error%20estimates.%20We%20introduce%20the%20quad-metric%0Aboundary%20error%2C%20which%20quantifies%20pixel-wise%20true%20positives%2C%20true%20negatives%2C%0Afalse%20positives%2C%20and%20false%20negatives%20at%20the%20boundaries%20of%20object%20instances%2C%0Aeffectively%20capturing%20both%20fine-grained%20and%20instance-level%20segmentation%20errors.%0AAdditionally%2C%20the%20Error%20Guidance%20Fusion%20%28EGF%29%20module%20explicitly%20integrates%0Aerror%20information%20into%20the%20refinement%20process%2C%20further%20improving%20segmentation%0Aquality.%20In%20comprehensive%20evaluations%20conducted%20on%20three%20widely%20used%20benchmark%0Adatasets%2C%20INSTA-BEER%20outperformed%20state-of-the-art%20models%20in%20both%20accuracy%20and%0Ainference%20time.%20Moreover%2C%20a%20real-world%20robotic%20experiment%20demonstrated%20the%0Apractical%20applicability%20of%20our%20method%20in%20improving%20the%20performance%20of%20target%0Aobject%20grasping%20tasks%20in%20cluttered%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16132v2&entry.124074799=Read"},
{"title": "PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios", "author": "Jingbo Wang and Zhengyi Luo and Ye Yuan and Yixuan Li and Bo Dai", "abstract": "  We address the challenge of content diversity and controllability in\npedestrian simulation for driving scenarios. Recent pedestrian animation\nframeworks have a significant limitation wherein they primarily focus on either\nfollowing trajectory [46] or the content of the reference video [57],\nconsequently overlooking the potential diversity of human motion within such\nscenarios. This limitation restricts the ability to generate pedestrian\nbehaviors that exhibit a wider range of variations and realistic motions and\ntherefore restricts its usage to provide rich motion content for other\ncomponents in the driving simulation system, e.g., suddenly changed motion to\nwhich the autonomous vehicle should respond. In our approach, we strive to\nsurpass the limitation by showcasing diverse human motions obtained from\nvarious sources, such as generated human motions, in addition to following the\ngiven trajectory. The fundamental contribution of our framework lies in\ncombining the motion tracking task with trajectory following, which enables the\ntracking of specific motion parts (e.g., upper body) while simultaneously\nfollowing the given trajectory by a single policy. This way, we significantly\nenhance both the diversity of simulated human motion within the given scenario\nand the controllability of the content, including language-based control. Our\nframework facilitates the generation of a wide range of human motions,\ncontributing to greater realism and adaptability in pedestrian simulations for\ndriving scenarios. More information is on our project page\nhttps://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .\n", "link": "http://arxiv.org/abs/2404.19722v1", "date": "2024-04-30", "relevancy": 1.6392, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5485}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5316}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PACER%2B%3A%20On-Demand%20Pedestrian%20Animation%20Controller%20in%20Driving%20Scenarios&body=Title%3A%20PACER%2B%3A%20On-Demand%20Pedestrian%20Animation%20Controller%20in%20Driving%20Scenarios%0AAuthor%3A%20Jingbo%20Wang%20and%20Zhengyi%20Luo%20and%20Ye%20Yuan%20and%20Yixuan%20Li%20and%20Bo%20Dai%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20content%20diversity%20and%20controllability%20in%0Apedestrian%20simulation%20for%20driving%20scenarios.%20Recent%20pedestrian%20animation%0Aframeworks%20have%20a%20significant%20limitation%20wherein%20they%20primarily%20focus%20on%20either%0Afollowing%20trajectory%20%5B46%5D%20or%20the%20content%20of%20the%20reference%20video%20%5B57%5D%2C%0Aconsequently%20overlooking%20the%20potential%20diversity%20of%20human%20motion%20within%20such%0Ascenarios.%20This%20limitation%20restricts%20the%20ability%20to%20generate%20pedestrian%0Abehaviors%20that%20exhibit%20a%20wider%20range%20of%20variations%20and%20realistic%20motions%20and%0Atherefore%20restricts%20its%20usage%20to%20provide%20rich%20motion%20content%20for%20other%0Acomponents%20in%20the%20driving%20simulation%20system%2C%20e.g.%2C%20suddenly%20changed%20motion%20to%0Awhich%20the%20autonomous%20vehicle%20should%20respond.%20In%20our%20approach%2C%20we%20strive%20to%0Asurpass%20the%20limitation%20by%20showcasing%20diverse%20human%20motions%20obtained%20from%0Avarious%20sources%2C%20such%20as%20generated%20human%20motions%2C%20in%20addition%20to%20following%20the%0Agiven%20trajectory.%20The%20fundamental%20contribution%20of%20our%20framework%20lies%20in%0Acombining%20the%20motion%20tracking%20task%20with%20trajectory%20following%2C%20which%20enables%20the%0Atracking%20of%20specific%20motion%20parts%20%28e.g.%2C%20upper%20body%29%20while%20simultaneously%0Afollowing%20the%20given%20trajectory%20by%20a%20single%20policy.%20This%20way%2C%20we%20significantly%0Aenhance%20both%20the%20diversity%20of%20simulated%20human%20motion%20within%20the%20given%20scenario%0Aand%20the%20controllability%20of%20the%20content%2C%20including%20language-based%20control.%20Our%0Aframework%20facilitates%20the%20generation%20of%20a%20wide%20range%20of%20human%20motions%2C%0Acontributing%20to%20greater%20realism%20and%20adaptability%20in%20pedestrian%20simulations%20for%0Adriving%20scenarios.%20More%20information%20is%20on%20our%20project%20page%0Ahttps%3A//wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACER%2B%3A%20On-Demand%20Pedestrian%20Animation%20Controller%20in%20Driving%20Scenarios&entry.906535625=Jingbo%20Wang%20and%20Zhengyi%20Luo%20and%20Ye%20Yuan%20and%20Yixuan%20Li%20and%20Bo%20Dai&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20content%20diversity%20and%20controllability%20in%0Apedestrian%20simulation%20for%20driving%20scenarios.%20Recent%20pedestrian%20animation%0Aframeworks%20have%20a%20significant%20limitation%20wherein%20they%20primarily%20focus%20on%20either%0Afollowing%20trajectory%20%5B46%5D%20or%20the%20content%20of%20the%20reference%20video%20%5B57%5D%2C%0Aconsequently%20overlooking%20the%20potential%20diversity%20of%20human%20motion%20within%20such%0Ascenarios.%20This%20limitation%20restricts%20the%20ability%20to%20generate%20pedestrian%0Abehaviors%20that%20exhibit%20a%20wider%20range%20of%20variations%20and%20realistic%20motions%20and%0Atherefore%20restricts%20its%20usage%20to%20provide%20rich%20motion%20content%20for%20other%0Acomponents%20in%20the%20driving%20simulation%20system%2C%20e.g.%2C%20suddenly%20changed%20motion%20to%0Awhich%20the%20autonomous%20vehicle%20should%20respond.%20In%20our%20approach%2C%20we%20strive%20to%0Asurpass%20the%20limitation%20by%20showcasing%20diverse%20human%20motions%20obtained%20from%0Avarious%20sources%2C%20such%20as%20generated%20human%20motions%2C%20in%20addition%20to%20following%20the%0Agiven%20trajectory.%20The%20fundamental%20contribution%20of%20our%20framework%20lies%20in%0Acombining%20the%20motion%20tracking%20task%20with%20trajectory%20following%2C%20which%20enables%20the%0Atracking%20of%20specific%20motion%20parts%20%28e.g.%2C%20upper%20body%29%20while%20simultaneously%0Afollowing%20the%20given%20trajectory%20by%20a%20single%20policy.%20This%20way%2C%20we%20significantly%0Aenhance%20both%20the%20diversity%20of%20simulated%20human%20motion%20within%20the%20given%20scenario%0Aand%20the%20controllability%20of%20the%20content%2C%20including%20language-based%20control.%20Our%0Aframework%20facilitates%20the%20generation%20of%20a%20wide%20range%20of%20human%20motions%2C%0Acontributing%20to%20greater%20realism%20and%20adaptability%20in%20pedestrian%20simulations%20for%0Adriving%20scenarios.%20More%20information%20is%20on%20our%20project%20page%0Ahttps%3A//wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19722v1&entry.124074799=Read"},
{"title": "SIR-RL: Reinforcement Learning for Optimized Policy Control during\n  Epidemiological Outbreaks in Emerging Market and Developing Economies", "author": "Maeghal Jain and Ziya Uddin and Wubshet Ibrahim", "abstract": "  The outbreak of COVID-19 has highlighted the intricate interplay between\npublic health and economic stability on a global scale. This study proposes a\nnovel reinforcement learning framework designed to optimize health and economic\noutcomes during pandemics. The framework leverages the SIR model, integrating\nboth lockdown measures (via a stringency index) and vaccination strategies to\nsimulate disease dynamics. The stringency index, indicative of the severity of\nlockdown measures, influences both the spread of the disease and the economic\nhealth of a country. Developing nations, which bear a disproportionate economic\nburden under stringent lockdowns, are the primary focus of our study. By\nimplementing reinforcement learning, we aim to optimize governmental responses\nand strike a balance between the competing costs associated with public health\nand economic stability. This approach also enhances transparency in\ngovernmental decision-making by establishing a well-defined reward function for\nthe reinforcement learning agent. In essence, this study introduces an\ninnovative and ethical strategy to navigate the challenge of balancing public\nhealth and economic stability amidst infectious disease outbreaks.\n", "link": "http://arxiv.org/abs/2404.08423v2", "date": "2024-04-30", "relevancy": 1.6369, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4386}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3879}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SIR-RL%3A%20Reinforcement%20Learning%20for%20Optimized%20Policy%20Control%20during%0A%20%20Epidemiological%20Outbreaks%20in%20Emerging%20Market%20and%20Developing%20Economies&body=Title%3A%20SIR-RL%3A%20Reinforcement%20Learning%20for%20Optimized%20Policy%20Control%20during%0A%20%20Epidemiological%20Outbreaks%20in%20Emerging%20Market%20and%20Developing%20Economies%0AAuthor%3A%20Maeghal%20Jain%20and%20Ziya%20Uddin%20and%20Wubshet%20Ibrahim%0AAbstract%3A%20%20%20The%20outbreak%20of%20COVID-19%20has%20highlighted%20the%20intricate%20interplay%20between%0Apublic%20health%20and%20economic%20stability%20on%20a%20global%20scale.%20This%20study%20proposes%20a%0Anovel%20reinforcement%20learning%20framework%20designed%20to%20optimize%20health%20and%20economic%0Aoutcomes%20during%20pandemics.%20The%20framework%20leverages%20the%20SIR%20model%2C%20integrating%0Aboth%20lockdown%20measures%20%28via%20a%20stringency%20index%29%20and%20vaccination%20strategies%20to%0Asimulate%20disease%20dynamics.%20The%20stringency%20index%2C%20indicative%20of%20the%20severity%20of%0Alockdown%20measures%2C%20influences%20both%20the%20spread%20of%20the%20disease%20and%20the%20economic%0Ahealth%20of%20a%20country.%20Developing%20nations%2C%20which%20bear%20a%20disproportionate%20economic%0Aburden%20under%20stringent%20lockdowns%2C%20are%20the%20primary%20focus%20of%20our%20study.%20By%0Aimplementing%20reinforcement%20learning%2C%20we%20aim%20to%20optimize%20governmental%20responses%0Aand%20strike%20a%20balance%20between%20the%20competing%20costs%20associated%20with%20public%20health%0Aand%20economic%20stability.%20This%20approach%20also%20enhances%20transparency%20in%0Agovernmental%20decision-making%20by%20establishing%20a%20well-defined%20reward%20function%20for%0Athe%20reinforcement%20learning%20agent.%20In%20essence%2C%20this%20study%20introduces%20an%0Ainnovative%20and%20ethical%20strategy%20to%20navigate%20the%20challenge%20of%20balancing%20public%0Ahealth%20and%20economic%20stability%20amidst%20infectious%20disease%20outbreaks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08423v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIR-RL%3A%20Reinforcement%20Learning%20for%20Optimized%20Policy%20Control%20during%0A%20%20Epidemiological%20Outbreaks%20in%20Emerging%20Market%20and%20Developing%20Economies&entry.906535625=Maeghal%20Jain%20and%20Ziya%20Uddin%20and%20Wubshet%20Ibrahim&entry.1292438233=%20%20The%20outbreak%20of%20COVID-19%20has%20highlighted%20the%20intricate%20interplay%20between%0Apublic%20health%20and%20economic%20stability%20on%20a%20global%20scale.%20This%20study%20proposes%20a%0Anovel%20reinforcement%20learning%20framework%20designed%20to%20optimize%20health%20and%20economic%0Aoutcomes%20during%20pandemics.%20The%20framework%20leverages%20the%20SIR%20model%2C%20integrating%0Aboth%20lockdown%20measures%20%28via%20a%20stringency%20index%29%20and%20vaccination%20strategies%20to%0Asimulate%20disease%20dynamics.%20The%20stringency%20index%2C%20indicative%20of%20the%20severity%20of%0Alockdown%20measures%2C%20influences%20both%20the%20spread%20of%20the%20disease%20and%20the%20economic%0Ahealth%20of%20a%20country.%20Developing%20nations%2C%20which%20bear%20a%20disproportionate%20economic%0Aburden%20under%20stringent%20lockdowns%2C%20are%20the%20primary%20focus%20of%20our%20study.%20By%0Aimplementing%20reinforcement%20learning%2C%20we%20aim%20to%20optimize%20governmental%20responses%0Aand%20strike%20a%20balance%20between%20the%20competing%20costs%20associated%20with%20public%20health%0Aand%20economic%20stability.%20This%20approach%20also%20enhances%20transparency%20in%0Agovernmental%20decision-making%20by%20establishing%20a%20well-defined%20reward%20function%20for%0Athe%20reinforcement%20learning%20agent.%20In%20essence%2C%20this%20study%20introduces%20an%0Ainnovative%20and%20ethical%20strategy%20to%20navigate%20the%20challenge%20of%20balancing%20public%0Ahealth%20and%20economic%20stability%20amidst%20infectious%20disease%20outbreaks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08423v2&entry.124074799=Read"},
{"title": "Multi-Prompt with Depth Partitioned Cross-Modal Learning", "author": "Yingjie Tian and Yiqi Wang and Xianda Guo and Zheng Zhu and Long Chen", "abstract": "  In recent years, soft prompt learning methods have been proposed to fine-tune\nlarge-scale vision-language pre-trained models for various downstream tasks.\nThese methods typically combine learnable textual tokens with class tokens as\ninput for models with frozen parameters. However, they often employ a single\nprompt to describe class contexts, failing to capture categories' diverse\nattributes adequately. This study introduces the Partitioned Multi-modal Prompt\n(PMPO), a multi-modal prompting technique that extends the soft prompt from a\nsingle learnable prompt to multiple prompts. Our method divides the visual\nencoder depths and connects learnable prompts to the separated visual depths,\nenabling different prompts to capture the hierarchical contextual depths of\nvisual representations. Furthermore, to maximize the advantages of multi-prompt\nlearning, we incorporate prior information from manually designed templates and\nlearnable multi-prompts, thus improving the generalization capabilities of our\napproach. We evaluate the effectiveness of our approach on three challenging\ntasks: new class generalization, cross-dataset evaluation, and domain\ngeneralization. For instance, our method achieves a $79.28$ harmonic mean,\naveraged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),\ndemonstrating significant competitiveness compared to state-of-the-art\nprompting methods.\n", "link": "http://arxiv.org/abs/2305.06221v4", "date": "2024-04-30", "relevancy": 1.6323, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Prompt%20with%20Depth%20Partitioned%20Cross-Modal%20Learning&body=Title%3A%20Multi-Prompt%20with%20Depth%20Partitioned%20Cross-Modal%20Learning%0AAuthor%3A%20Yingjie%20Tian%20and%20Yiqi%20Wang%20and%20Xianda%20Guo%20and%20Zheng%20Zhu%20and%20Long%20Chen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20soft%20prompt%20learning%20methods%20have%20been%20proposed%20to%20fine-tune%0Alarge-scale%20vision-language%20pre-trained%20models%20for%20various%20downstream%20tasks.%0AThese%20methods%20typically%20combine%20learnable%20textual%20tokens%20with%20class%20tokens%20as%0Ainput%20for%20models%20with%20frozen%20parameters.%20However%2C%20they%20often%20employ%20a%20single%0Aprompt%20to%20describe%20class%20contexts%2C%20failing%20to%20capture%20categories%27%20diverse%0Aattributes%20adequately.%20This%20study%20introduces%20the%20Partitioned%20Multi-modal%20Prompt%0A%28PMPO%29%2C%20a%20multi-modal%20prompting%20technique%20that%20extends%20the%20soft%20prompt%20from%20a%0Asingle%20learnable%20prompt%20to%20multiple%20prompts.%20Our%20method%20divides%20the%20visual%0Aencoder%20depths%20and%20connects%20learnable%20prompts%20to%20the%20separated%20visual%20depths%2C%0Aenabling%20different%20prompts%20to%20capture%20the%20hierarchical%20contextual%20depths%20of%0Avisual%20representations.%20Furthermore%2C%20to%20maximize%20the%20advantages%20of%20multi-prompt%0Alearning%2C%20we%20incorporate%20prior%20information%20from%20manually%20designed%20templates%20and%0Alearnable%20multi-prompts%2C%20thus%20improving%20the%20generalization%20capabilities%20of%20our%0Aapproach.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%20on%20three%20challenging%0Atasks%3A%20new%20class%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%0Ageneralization.%20For%20instance%2C%20our%20method%20achieves%20a%20%2479.28%24%20harmonic%20mean%2C%0Aaveraged%20over%2011%20diverse%20image%20recognition%20datasets%20%28%24%2B7.62%24%20compared%20to%20CoOp%29%2C%0Ademonstrating%20significant%20competitiveness%20compared%20to%20state-of-the-art%0Aprompting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.06221v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Prompt%20with%20Depth%20Partitioned%20Cross-Modal%20Learning&entry.906535625=Yingjie%20Tian%20and%20Yiqi%20Wang%20and%20Xianda%20Guo%20and%20Zheng%20Zhu%20and%20Long%20Chen&entry.1292438233=%20%20In%20recent%20years%2C%20soft%20prompt%20learning%20methods%20have%20been%20proposed%20to%20fine-tune%0Alarge-scale%20vision-language%20pre-trained%20models%20for%20various%20downstream%20tasks.%0AThese%20methods%20typically%20combine%20learnable%20textual%20tokens%20with%20class%20tokens%20as%0Ainput%20for%20models%20with%20frozen%20parameters.%20However%2C%20they%20often%20employ%20a%20single%0Aprompt%20to%20describe%20class%20contexts%2C%20failing%20to%20capture%20categories%27%20diverse%0Aattributes%20adequately.%20This%20study%20introduces%20the%20Partitioned%20Multi-modal%20Prompt%0A%28PMPO%29%2C%20a%20multi-modal%20prompting%20technique%20that%20extends%20the%20soft%20prompt%20from%20a%0Asingle%20learnable%20prompt%20to%20multiple%20prompts.%20Our%20method%20divides%20the%20visual%0Aencoder%20depths%20and%20connects%20learnable%20prompts%20to%20the%20separated%20visual%20depths%2C%0Aenabling%20different%20prompts%20to%20capture%20the%20hierarchical%20contextual%20depths%20of%0Avisual%20representations.%20Furthermore%2C%20to%20maximize%20the%20advantages%20of%20multi-prompt%0Alearning%2C%20we%20incorporate%20prior%20information%20from%20manually%20designed%20templates%20and%0Alearnable%20multi-prompts%2C%20thus%20improving%20the%20generalization%20capabilities%20of%20our%0Aapproach.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%20on%20three%20challenging%0Atasks%3A%20new%20class%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%0Ageneralization.%20For%20instance%2C%20our%20method%20achieves%20a%20%2479.28%24%20harmonic%20mean%2C%0Aaveraged%20over%2011%20diverse%20image%20recognition%20datasets%20%28%24%2B7.62%24%20compared%20to%20CoOp%29%2C%0Ademonstrating%20significant%20competitiveness%20compared%20to%20state-of-the-art%0Aprompting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.06221v4&entry.124074799=Read"},
{"title": "Towards Generalist Robot Learning from Internet Video: A Survey", "author": "Robert McCarthy and Daniel C. H. Tan and Dominik Schmidt and Fernando Acero and Nathan Herr and Yilun Du and Thomas G. Thuruthel and Zhibin Li", "abstract": "  This survey presents an overview of methods for learning from video (LfV) in\nthe context of reinforcement learning (RL) and robotics. We focus on methods\ncapable of scaling to large internet video datasets and, in the process,\nextracting foundational knowledge about the world's dynamics and physical human\nbehaviour. Such methods hold great promise for developing general-purpose\nrobots.\n  We open with an overview of fundamental concepts relevant to the\nLfV-for-robotics setting. This includes a discussion of the exciting benefits\nLfV methods can offer (e.g., improved generalization beyond the available robot\ndata) and commentary on key LfV challenges (e.g., challenges related to missing\ninformation in video and LfV distribution shifts). Our literature review begins\nwith an analysis of video foundation model techniques that can extract\nknowledge from large, heterogeneous video datasets. Next, we review methods\nthat specifically leverage video data for robot learning. Here, we categorise\nwork according to which RL knowledge modality benefits from the use of video\ndata. We additionally highlight techniques for mitigating LfV challenges,\nincluding reviewing action representations that address the issue of missing\naction labels in video.\n  Finally, we examine LfV datasets and benchmarks, before concluding the survey\nby discussing challenges and opportunities in LfV. Here, we advocate for\nscalable approaches that can leverage the full range of available data and that\ntarget the key benefits of LfV. Overall, we hope this survey will serve as a\ncomprehensive reference for the emerging field of LfV, catalysing further\nresearch in the area, and ultimately facilitating progress towards obtaining\ngeneral-purpose robots.\n", "link": "http://arxiv.org/abs/2404.19664v1", "date": "2024-04-30", "relevancy": 1.6199, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5648}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5393}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5303}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&body=Title%3A%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey%0AAuthor%3A%20Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20This%20survey%20presents%20an%20overview%20of%20methods%20for%20learning%20from%20video%20%28LfV%29%20in%0Athe%20context%20of%20reinforcement%20learning%20%28RL%29%20and%20robotics.%20We%20focus%20on%20methods%0Acapable%20of%20scaling%20to%20large%20internet%20video%20datasets%20and%2C%20in%20the%20process%2C%0Aextracting%20foundational%20knowledge%20about%20the%20world%27s%20dynamics%20and%20physical%20human%0Abehaviour.%20Such%20methods%20hold%20great%20promise%20for%20developing%20general-purpose%0Arobots.%0A%20%20We%20open%20with%20an%20overview%20of%20fundamental%20concepts%20relevant%20to%20the%0ALfV-for-robotics%20setting.%20This%20includes%20a%20discussion%20of%20the%20exciting%20benefits%0ALfV%20methods%20can%20offer%20%28e.g.%2C%20improved%20generalization%20beyond%20the%20available%20robot%0Adata%29%20and%20commentary%20on%20key%20LfV%20challenges%20%28e.g.%2C%20challenges%20related%20to%20missing%0Ainformation%20in%20video%20and%20LfV%20distribution%20shifts%29.%20Our%20literature%20review%20begins%0Awith%20an%20analysis%20of%20video%20foundation%20model%20techniques%20that%20can%20extract%0Aknowledge%20from%20large%2C%20heterogeneous%20video%20datasets.%20Next%2C%20we%20review%20methods%0Athat%20specifically%20leverage%20video%20data%20for%20robot%20learning.%20Here%2C%20we%20categorise%0Awork%20according%20to%20which%20RL%20knowledge%20modality%20benefits%20from%20the%20use%20of%20video%0Adata.%20We%20additionally%20highlight%20techniques%20for%20mitigating%20LfV%20challenges%2C%0Aincluding%20reviewing%20action%20representations%20that%20address%20the%20issue%20of%20missing%0Aaction%20labels%20in%20video.%0A%20%20Finally%2C%20we%20examine%20LfV%20datasets%20and%20benchmarks%2C%20before%20concluding%20the%20survey%0Aby%20discussing%20challenges%20and%20opportunities%20in%20LfV.%20Here%2C%20we%20advocate%20for%0Ascalable%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%20data%20and%20that%0Atarget%20the%20key%20benefits%20of%20LfV.%20Overall%2C%20we%20hope%20this%20survey%20will%20serve%20as%20a%0Acomprehensive%20reference%20for%20the%20emerging%20field%20of%20LfV%2C%20catalysing%20further%0Aresearch%20in%20the%20area%2C%20and%20ultimately%20facilitating%20progress%20towards%20obtaining%0Ageneral-purpose%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19664v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&entry.906535625=Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li&entry.1292438233=%20%20This%20survey%20presents%20an%20overview%20of%20methods%20for%20learning%20from%20video%20%28LfV%29%20in%0Athe%20context%20of%20reinforcement%20learning%20%28RL%29%20and%20robotics.%20We%20focus%20on%20methods%0Acapable%20of%20scaling%20to%20large%20internet%20video%20datasets%20and%2C%20in%20the%20process%2C%0Aextracting%20foundational%20knowledge%20about%20the%20world%27s%20dynamics%20and%20physical%20human%0Abehaviour.%20Such%20methods%20hold%20great%20promise%20for%20developing%20general-purpose%0Arobots.%0A%20%20We%20open%20with%20an%20overview%20of%20fundamental%20concepts%20relevant%20to%20the%0ALfV-for-robotics%20setting.%20This%20includes%20a%20discussion%20of%20the%20exciting%20benefits%0ALfV%20methods%20can%20offer%20%28e.g.%2C%20improved%20generalization%20beyond%20the%20available%20robot%0Adata%29%20and%20commentary%20on%20key%20LfV%20challenges%20%28e.g.%2C%20challenges%20related%20to%20missing%0Ainformation%20in%20video%20and%20LfV%20distribution%20shifts%29.%20Our%20literature%20review%20begins%0Awith%20an%20analysis%20of%20video%20foundation%20model%20techniques%20that%20can%20extract%0Aknowledge%20from%20large%2C%20heterogeneous%20video%20datasets.%20Next%2C%20we%20review%20methods%0Athat%20specifically%20leverage%20video%20data%20for%20robot%20learning.%20Here%2C%20we%20categorise%0Awork%20according%20to%20which%20RL%20knowledge%20modality%20benefits%20from%20the%20use%20of%20video%0Adata.%20We%20additionally%20highlight%20techniques%20for%20mitigating%20LfV%20challenges%2C%0Aincluding%20reviewing%20action%20representations%20that%20address%20the%20issue%20of%20missing%0Aaction%20labels%20in%20video.%0A%20%20Finally%2C%20we%20examine%20LfV%20datasets%20and%20benchmarks%2C%20before%20concluding%20the%20survey%0Aby%20discussing%20challenges%20and%20opportunities%20in%20LfV.%20Here%2C%20we%20advocate%20for%0Ascalable%20approaches%20that%20can%20leverage%20the%20full%20range%20of%20available%20data%20and%20that%0Atarget%20the%20key%20benefits%20of%20LfV.%20Overall%2C%20we%20hope%20this%20survey%20will%20serve%20as%20a%0Acomprehensive%20reference%20for%20the%20emerging%20field%20of%20LfV%2C%20catalysing%20further%0Aresearch%20in%20the%20area%2C%20and%20ultimately%20facilitating%20progress%20towards%20obtaining%0Ageneral-purpose%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19664v1&entry.124074799=Read"},
{"title": "Enhancing Lip Reading with Multi-Scale Video and Multi-Encoder", "author": "He Wang and Pengcheng Guo and Xucheng Wan and Huan Zhou and Lei Xie", "abstract": "  Automatic lip-reading (ALR) aims to automatically transcribe spoken content\nfrom a speaker's silent lip motion captured in video. Current mainstream\nlip-reading approaches only use a single visual encoder to model input videos\nof a single scale. In this paper, we propose to enhance lip-reading by\nincorporating multi-scale video data and multi-encoder. Specifically, we first\npropose a novel multi-scale lip motion extraction algorithm based on the size\nof the speaker's face and an Enhanced ResNet3D visual front-end (VFE) to\nextract lip features at different scales. For the multi-encoder, in addition to\nthe mainstream Transformer and Conformer, we also incorporate the recently\nproposed Branchformer and E-Branchformer as visual encoders. In the\nexperiments, we explore the influence of different video data scales and\nencoders on ALR system performance and fuse the texts transcribed by all ALR\nsystems using recognizer output voting error reduction (ROVER). Finally, our\nproposed approach placed second in the ICME 2024 ChatCLR Challenge Task 2, with\na 21.52% reduction in character error rate (CER) compared to the official\nbaseline on the evaluation set.\n", "link": "http://arxiv.org/abs/2404.05466v2", "date": "2024-04-30", "relevancy": 1.6057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5038}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5024}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder&body=Title%3A%20Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder%0AAuthor%3A%20He%20Wang%20and%20Pengcheng%20Guo%20and%20Xucheng%20Wan%20and%20Huan%20Zhou%20and%20Lei%20Xie%0AAbstract%3A%20%20%20Automatic%20lip-reading%20%28ALR%29%20aims%20to%20automatically%20transcribe%20spoken%20content%0Afrom%20a%20speaker%27s%20silent%20lip%20motion%20captured%20in%20video.%20Current%20mainstream%0Alip-reading%20approaches%20only%20use%20a%20single%20visual%20encoder%20to%20model%20input%20videos%0Aof%20a%20single%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20enhance%20lip-reading%20by%0Aincorporating%20multi-scale%20video%20data%20and%20multi-encoder.%20Specifically%2C%20we%20first%0Apropose%20a%20novel%20multi-scale%20lip%20motion%20extraction%20algorithm%20based%20on%20the%20size%0Aof%20the%20speaker%27s%20face%20and%20an%20Enhanced%20ResNet3D%20visual%20front-end%20%28VFE%29%20to%0Aextract%20lip%20features%20at%20different%20scales.%20For%20the%20multi-encoder%2C%20in%20addition%20to%0Athe%20mainstream%20Transformer%20and%20Conformer%2C%20we%20also%20incorporate%20the%20recently%0Aproposed%20Branchformer%20and%20E-Branchformer%20as%20visual%20encoders.%20In%20the%0Aexperiments%2C%20we%20explore%20the%20influence%20of%20different%20video%20data%20scales%20and%0Aencoders%20on%20ALR%20system%20performance%20and%20fuse%20the%20texts%20transcribed%20by%20all%20ALR%0Asystems%20using%20recognizer%20output%20voting%20error%20reduction%20%28ROVER%29.%20Finally%2C%20our%0Aproposed%20approach%20placed%20second%20in%20the%20ICME%202024%20ChatCLR%20Challenge%20Task%202%2C%20with%0Aa%2021.52%25%20reduction%20in%20character%20error%20rate%20%28CER%29%20compared%20to%20the%20official%0Abaseline%20on%20the%20evaluation%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05466v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Lip%20Reading%20with%20Multi-Scale%20Video%20and%20Multi-Encoder&entry.906535625=He%20Wang%20and%20Pengcheng%20Guo%20and%20Xucheng%20Wan%20and%20Huan%20Zhou%20and%20Lei%20Xie&entry.1292438233=%20%20Automatic%20lip-reading%20%28ALR%29%20aims%20to%20automatically%20transcribe%20spoken%20content%0Afrom%20a%20speaker%27s%20silent%20lip%20motion%20captured%20in%20video.%20Current%20mainstream%0Alip-reading%20approaches%20only%20use%20a%20single%20visual%20encoder%20to%20model%20input%20videos%0Aof%20a%20single%20scale.%20In%20this%20paper%2C%20we%20propose%20to%20enhance%20lip-reading%20by%0Aincorporating%20multi-scale%20video%20data%20and%20multi-encoder.%20Specifically%2C%20we%20first%0Apropose%20a%20novel%20multi-scale%20lip%20motion%20extraction%20algorithm%20based%20on%20the%20size%0Aof%20the%20speaker%27s%20face%20and%20an%20Enhanced%20ResNet3D%20visual%20front-end%20%28VFE%29%20to%0Aextract%20lip%20features%20at%20different%20scales.%20For%20the%20multi-encoder%2C%20in%20addition%20to%0Athe%20mainstream%20Transformer%20and%20Conformer%2C%20we%20also%20incorporate%20the%20recently%0Aproposed%20Branchformer%20and%20E-Branchformer%20as%20visual%20encoders.%20In%20the%0Aexperiments%2C%20we%20explore%20the%20influence%20of%20different%20video%20data%20scales%20and%0Aencoders%20on%20ALR%20system%20performance%20and%20fuse%20the%20texts%20transcribed%20by%20all%20ALR%0Asystems%20using%20recognizer%20output%20voting%20error%20reduction%20%28ROVER%29.%20Finally%2C%20our%0Aproposed%20approach%20placed%20second%20in%20the%20ICME%202024%20ChatCLR%20Challenge%20Task%202%2C%20with%0Aa%2021.52%25%20reduction%20in%20character%20error%20rate%20%28CER%29%20compared%20to%20the%20official%0Abaseline%20on%20the%20evaluation%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05466v2&entry.124074799=Read"},
{"title": "Sim-to-real transfer of active suspension control using deep\n  reinforcement learning", "author": "Viktor Wiberg and Erik Wallin and Arvid F\u00e4lldin and Tobias Semberg and Morgan Rossander and Eddie Wadbro and Martin Servin", "abstract": "  We explore sim-to-real transfer of deep reinforcement learning controllers\nfor a heavy vehicle with active suspensions designed for traversing rough\nterrain. While related research primarily focuses on lightweight robots with\nelectric motors and fast actuation, this study uses a forestry vehicle with a\ncomplex hydraulic driveline and slow actuation. We simulate the vehicle using\nmultibody dynamics and apply system identification to find an appropriate set\nof simulation parameters. We then train policies in simulation using various\ntechniques to mitigate the sim-to-real gap, including domain randomization,\naction delays, and a reward penalty to encourage smooth control. In reality,\nthe policies trained with action delays and a penalty for erratic actions\nperform nearly at the same level as in simulation. In experiments on level\nground, the motion trajectories closely overlap when turning to either side, as\nwell as in a route tracking scenario. When faced with a ramp that requires\nactive use of the suspensions, the simulated and real motions are in close\nalignment. This shows that the actuator model together with system\nidentification yields a sufficiently accurate model of the actuators. We\nobserve that policies trained without the additional action penalty exhibit\nfast switching or bang-bang control. These present smooth motions and high\nperformance in simulation but transfer poorly to reality. We find that policies\nmake marginal use of the local height map for perception, showing no\nindications of predictive planning. However, the strong transfer capabilities\nentail that further development concerning perception and performance can be\nlargely confined to simulation.\n", "link": "http://arxiv.org/abs/2306.11171v3", "date": "2024-04-30", "relevancy": 1.6018, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5583}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5452}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4813}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sim-to-real%20transfer%20of%20active%20suspension%20control%20using%20deep%0A%20%20reinforcement%20learning&body=Title%3A%20Sim-to-real%20transfer%20of%20active%20suspension%20control%20using%20deep%0A%20%20reinforcement%20learning%0AAuthor%3A%20Viktor%20Wiberg%20and%20Erik%20Wallin%20and%20Arvid%20F%C3%A4lldin%20and%20Tobias%20Semberg%20and%20Morgan%20Rossander%20and%20Eddie%20Wadbro%20and%20Martin%20Servin%0AAbstract%3A%20%20%20We%20explore%20sim-to-real%20transfer%20of%20deep%20reinforcement%20learning%20controllers%0Afor%20a%20heavy%20vehicle%20with%20active%20suspensions%20designed%20for%20traversing%20rough%0Aterrain.%20While%20related%20research%20primarily%20focuses%20on%20lightweight%20robots%20with%0Aelectric%20motors%20and%20fast%20actuation%2C%20this%20study%20uses%20a%20forestry%20vehicle%20with%20a%0Acomplex%20hydraulic%20driveline%20and%20slow%20actuation.%20We%20simulate%20the%20vehicle%20using%0Amultibody%20dynamics%20and%20apply%20system%20identification%20to%20find%20an%20appropriate%20set%0Aof%20simulation%20parameters.%20We%20then%20train%20policies%20in%20simulation%20using%20various%0Atechniques%20to%20mitigate%20the%20sim-to-real%20gap%2C%20including%20domain%20randomization%2C%0Aaction%20delays%2C%20and%20a%20reward%20penalty%20to%20encourage%20smooth%20control.%20In%20reality%2C%0Athe%20policies%20trained%20with%20action%20delays%20and%20a%20penalty%20for%20erratic%20actions%0Aperform%20nearly%20at%20the%20same%20level%20as%20in%20simulation.%20In%20experiments%20on%20level%0Aground%2C%20the%20motion%20trajectories%20closely%20overlap%20when%20turning%20to%20either%20side%2C%20as%0Awell%20as%20in%20a%20route%20tracking%20scenario.%20When%20faced%20with%20a%20ramp%20that%20requires%0Aactive%20use%20of%20the%20suspensions%2C%20the%20simulated%20and%20real%20motions%20are%20in%20close%0Aalignment.%20This%20shows%20that%20the%20actuator%20model%20together%20with%20system%0Aidentification%20yields%20a%20sufficiently%20accurate%20model%20of%20the%20actuators.%20We%0Aobserve%20that%20policies%20trained%20without%20the%20additional%20action%20penalty%20exhibit%0Afast%20switching%20or%20bang-bang%20control.%20These%20present%20smooth%20motions%20and%20high%0Aperformance%20in%20simulation%20but%20transfer%20poorly%20to%20reality.%20We%20find%20that%20policies%0Amake%20marginal%20use%20of%20the%20local%20height%20map%20for%20perception%2C%20showing%20no%0Aindications%20of%20predictive%20planning.%20However%2C%20the%20strong%20transfer%20capabilities%0Aentail%20that%20further%20development%20concerning%20perception%20and%20performance%20can%20be%0Alargely%20confined%20to%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11171v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim-to-real%20transfer%20of%20active%20suspension%20control%20using%20deep%0A%20%20reinforcement%20learning&entry.906535625=Viktor%20Wiberg%20and%20Erik%20Wallin%20and%20Arvid%20F%C3%A4lldin%20and%20Tobias%20Semberg%20and%20Morgan%20Rossander%20and%20Eddie%20Wadbro%20and%20Martin%20Servin&entry.1292438233=%20%20We%20explore%20sim-to-real%20transfer%20of%20deep%20reinforcement%20learning%20controllers%0Afor%20a%20heavy%20vehicle%20with%20active%20suspensions%20designed%20for%20traversing%20rough%0Aterrain.%20While%20related%20research%20primarily%20focuses%20on%20lightweight%20robots%20with%0Aelectric%20motors%20and%20fast%20actuation%2C%20this%20study%20uses%20a%20forestry%20vehicle%20with%20a%0Acomplex%20hydraulic%20driveline%20and%20slow%20actuation.%20We%20simulate%20the%20vehicle%20using%0Amultibody%20dynamics%20and%20apply%20system%20identification%20to%20find%20an%20appropriate%20set%0Aof%20simulation%20parameters.%20We%20then%20train%20policies%20in%20simulation%20using%20various%0Atechniques%20to%20mitigate%20the%20sim-to-real%20gap%2C%20including%20domain%20randomization%2C%0Aaction%20delays%2C%20and%20a%20reward%20penalty%20to%20encourage%20smooth%20control.%20In%20reality%2C%0Athe%20policies%20trained%20with%20action%20delays%20and%20a%20penalty%20for%20erratic%20actions%0Aperform%20nearly%20at%20the%20same%20level%20as%20in%20simulation.%20In%20experiments%20on%20level%0Aground%2C%20the%20motion%20trajectories%20closely%20overlap%20when%20turning%20to%20either%20side%2C%20as%0Awell%20as%20in%20a%20route%20tracking%20scenario.%20When%20faced%20with%20a%20ramp%20that%20requires%0Aactive%20use%20of%20the%20suspensions%2C%20the%20simulated%20and%20real%20motions%20are%20in%20close%0Aalignment.%20This%20shows%20that%20the%20actuator%20model%20together%20with%20system%0Aidentification%20yields%20a%20sufficiently%20accurate%20model%20of%20the%20actuators.%20We%0Aobserve%20that%20policies%20trained%20without%20the%20additional%20action%20penalty%20exhibit%0Afast%20switching%20or%20bang-bang%20control.%20These%20present%20smooth%20motions%20and%20high%0Aperformance%20in%20simulation%20but%20transfer%20poorly%20to%20reality.%20We%20find%20that%20policies%0Amake%20marginal%20use%20of%20the%20local%20height%20map%20for%20perception%2C%20showing%20no%0Aindications%20of%20predictive%20planning.%20However%2C%20the%20strong%20transfer%20capabilities%0Aentail%20that%20further%20development%20concerning%20perception%20and%20performance%20can%20be%0Alargely%20confined%20to%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11171v3&entry.124074799=Read"},
{"title": "Conditional validity of heteroskedastic conformal regression", "author": "Nicolas Dewolf and Bernard De Baets and Willem Waegeman", "abstract": "  Conformal prediction, and split conformal prediction as a specific\nimplementation, offer a distribution-free approach to estimating prediction\nintervals with statistical guarantees. Recent work has shown that split\nconformal prediction can produce state-of-the-art prediction intervals when\nfocusing on marginal coverage, i.e. on a calibration dataset the method\nproduces on average prediction intervals that contain the ground truth with a\npredefined coverage level. However, such intervals are often not adaptive,\nwhich can be problematic for regression problems with heteroskedastic noise.\nThis paper tries to shed new light on how prediction intervals can be\nconstructed, using methods such as normalized and Mondrian conformal\nprediction, in such a way that they adapt to the heteroskedasticity of the\nunderlying process. Theoretical and experimental results are presented in which\nthese methods are compared in a systematic way. In particular, it is shown how\nthe conditional validity of a chosen conformal predictor can be related to\n(implicit) assumptions about the data-generating distribution.\n", "link": "http://arxiv.org/abs/2309.08313v2", "date": "2024-04-30", "relevancy": 1.3336, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4408}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4151}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Conditional%20validity%20of%20heteroskedastic%20conformal%20regression&body=Title%3A%20Conditional%20validity%20of%20heteroskedastic%20conformal%20regression%0AAuthor%3A%20Nicolas%20Dewolf%20and%20Bernard%20De%20Baets%20and%20Willem%20Waegeman%0AAbstract%3A%20%20%20Conformal%20prediction%2C%20and%20split%20conformal%20prediction%20as%20a%20specific%0Aimplementation%2C%20offer%20a%20distribution-free%20approach%20to%20estimating%20prediction%0Aintervals%20with%20statistical%20guarantees.%20Recent%20work%20has%20shown%20that%20split%0Aconformal%20prediction%20can%20produce%20state-of-the-art%20prediction%20intervals%20when%0Afocusing%20on%20marginal%20coverage%2C%20i.e.%20on%20a%20calibration%20dataset%20the%20method%0Aproduces%20on%20average%20prediction%20intervals%20that%20contain%20the%20ground%20truth%20with%20a%0Apredefined%20coverage%20level.%20However%2C%20such%20intervals%20are%20often%20not%20adaptive%2C%0Awhich%20can%20be%20problematic%20for%20regression%20problems%20with%20heteroskedastic%20noise.%0AThis%20paper%20tries%20to%20shed%20new%20light%20on%20how%20prediction%20intervals%20can%20be%0Aconstructed%2C%20using%20methods%20such%20as%20normalized%20and%20Mondrian%20conformal%0Aprediction%2C%20in%20such%20a%20way%20that%20they%20adapt%20to%20the%20heteroskedasticity%20of%20the%0Aunderlying%20process.%20Theoretical%20and%20experimental%20results%20are%20presented%20in%20which%0Athese%20methods%20are%20compared%20in%20a%20systematic%20way.%20In%20particular%2C%20it%20is%20shown%20how%0Athe%20conditional%20validity%20of%20a%20chosen%20conformal%20predictor%20can%20be%20related%20to%0A%28implicit%29%20assumptions%20about%20the%20data-generating%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08313v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20validity%20of%20heteroskedastic%20conformal%20regression&entry.906535625=Nicolas%20Dewolf%20and%20Bernard%20De%20Baets%20and%20Willem%20Waegeman&entry.1292438233=%20%20Conformal%20prediction%2C%20and%20split%20conformal%20prediction%20as%20a%20specific%0Aimplementation%2C%20offer%20a%20distribution-free%20approach%20to%20estimating%20prediction%0Aintervals%20with%20statistical%20guarantees.%20Recent%20work%20has%20shown%20that%20split%0Aconformal%20prediction%20can%20produce%20state-of-the-art%20prediction%20intervals%20when%0Afocusing%20on%20marginal%20coverage%2C%20i.e.%20on%20a%20calibration%20dataset%20the%20method%0Aproduces%20on%20average%20prediction%20intervals%20that%20contain%20the%20ground%20truth%20with%20a%0Apredefined%20coverage%20level.%20However%2C%20such%20intervals%20are%20often%20not%20adaptive%2C%0Awhich%20can%20be%20problematic%20for%20regression%20problems%20with%20heteroskedastic%20noise.%0AThis%20paper%20tries%20to%20shed%20new%20light%20on%20how%20prediction%20intervals%20can%20be%0Aconstructed%2C%20using%20methods%20such%20as%20normalized%20and%20Mondrian%20conformal%0Aprediction%2C%20in%20such%20a%20way%20that%20they%20adapt%20to%20the%20heteroskedasticity%20of%20the%0Aunderlying%20process.%20Theoretical%20and%20experimental%20results%20are%20presented%20in%20which%0Athese%20methods%20are%20compared%20in%20a%20systematic%20way.%20In%20particular%2C%20it%20is%20shown%20how%0Athe%20conditional%20validity%20of%20a%20chosen%20conformal%20predictor%20can%20be%20related%20to%0A%28implicit%29%20assumptions%20about%20the%20data-generating%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08313v2&entry.124074799=Read"},
{"title": "Approximation Theory, Computing, and Deep Learning on the Wasserstein\n  Space", "author": "Massimo Fornasier and Pascal Heid and Giacomo Enrico Sodini", "abstract": "  The challenge of approximating functions in infinite-dimensional spaces from\nfinite samples is widely regarded as formidable. In this study, we delve into\nthe challenging problem of the numerical approximation of Sobolev-smooth\nfunctions defined on probability spaces. Our particular focus centers on the\nWasserstein distance function, which serves as a relevant example. In contrast\nto the existing body of literature focused on approximating efficiently\npointwise evaluations, we chart a new course to define functional approximants\nby adopting three machine learning-based approaches: 1. Solving a finite number\nof optimal transport problems and computing the corresponding Wasserstein\npotentials. 2. Employing empirical risk minimization with Tikhonov\nregularization in Wasserstein Sobolev spaces. 3. Addressing the problem through\nthe saddle point formulation that characterizes the weak form of the Tikhonov\nfunctional's Euler-Lagrange equation. As a theoretical contribution, we furnish\nexplicit and quantitative bounds on generalization errors for each of these\nsolutions. In the proofs, we leverage the theory of metric Sobolev spaces and\nwe combine it with techniques of optimal transport, variational calculus, and\nlarge deviation bounds. In our numerical implementation, we harness\nappropriately designed neural networks to serve as basis functions. These\nnetworks undergo training using diverse methodologies. This approach allows us\nto obtain approximating functions that can be rapidly evaluated after training.\nConsequently, our constructive solutions significantly enhance at equal\naccuracy the evaluation speed, surpassing that of state-of-the-art methods by\nseveral orders of magnitude.\n", "link": "http://arxiv.org/abs/2310.19548v3", "date": "2024-04-30", "relevancy": 1.4483, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4891}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4775}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Approximation%20Theory%2C%20Computing%2C%20and%20Deep%20Learning%20on%20the%20Wasserstein%0A%20%20Space&body=Title%3A%20Approximation%20Theory%2C%20Computing%2C%20and%20Deep%20Learning%20on%20the%20Wasserstein%0A%20%20Space%0AAuthor%3A%20Massimo%20Fornasier%20and%20Pascal%20Heid%20and%20Giacomo%20Enrico%20Sodini%0AAbstract%3A%20%20%20The%20challenge%20of%20approximating%20functions%20in%20infinite-dimensional%20spaces%20from%0Afinite%20samples%20is%20widely%20regarded%20as%20formidable.%20In%20this%20study%2C%20we%20delve%20into%0Athe%20challenging%20problem%20of%20the%20numerical%20approximation%20of%20Sobolev-smooth%0Afunctions%20defined%20on%20probability%20spaces.%20Our%20particular%20focus%20centers%20on%20the%0AWasserstein%20distance%20function%2C%20which%20serves%20as%20a%20relevant%20example.%20In%20contrast%0Ato%20the%20existing%20body%20of%20literature%20focused%20on%20approximating%20efficiently%0Apointwise%20evaluations%2C%20we%20chart%20a%20new%20course%20to%20define%20functional%20approximants%0Aby%20adopting%20three%20machine%20learning-based%20approaches%3A%201.%20Solving%20a%20finite%20number%0Aof%20optimal%20transport%20problems%20and%20computing%20the%20corresponding%20Wasserstein%0Apotentials.%202.%20Employing%20empirical%20risk%20minimization%20with%20Tikhonov%0Aregularization%20in%20Wasserstein%20Sobolev%20spaces.%203.%20Addressing%20the%20problem%20through%0Athe%20saddle%20point%20formulation%20that%20characterizes%20the%20weak%20form%20of%20the%20Tikhonov%0Afunctional%27s%20Euler-Lagrange%20equation.%20As%20a%20theoretical%20contribution%2C%20we%20furnish%0Aexplicit%20and%20quantitative%20bounds%20on%20generalization%20errors%20for%20each%20of%20these%0Asolutions.%20In%20the%20proofs%2C%20we%20leverage%20the%20theory%20of%20metric%20Sobolev%20spaces%20and%0Awe%20combine%20it%20with%20techniques%20of%20optimal%20transport%2C%20variational%20calculus%2C%20and%0Alarge%20deviation%20bounds.%20In%20our%20numerical%20implementation%2C%20we%20harness%0Aappropriately%20designed%20neural%20networks%20to%20serve%20as%20basis%20functions.%20These%0Anetworks%20undergo%20training%20using%20diverse%20methodologies.%20This%20approach%20allows%20us%0Ato%20obtain%20approximating%20functions%20that%20can%20be%20rapidly%20evaluated%20after%20training.%0AConsequently%2C%20our%20constructive%20solutions%20significantly%20enhance%20at%20equal%0Aaccuracy%20the%20evaluation%20speed%2C%20surpassing%20that%20of%20state-of-the-art%20methods%20by%0Aseveral%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19548v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximation%20Theory%2C%20Computing%2C%20and%20Deep%20Learning%20on%20the%20Wasserstein%0A%20%20Space&entry.906535625=Massimo%20Fornasier%20and%20Pascal%20Heid%20and%20Giacomo%20Enrico%20Sodini&entry.1292438233=%20%20The%20challenge%20of%20approximating%20functions%20in%20infinite-dimensional%20spaces%20from%0Afinite%20samples%20is%20widely%20regarded%20as%20formidable.%20In%20this%20study%2C%20we%20delve%20into%0Athe%20challenging%20problem%20of%20the%20numerical%20approximation%20of%20Sobolev-smooth%0Afunctions%20defined%20on%20probability%20spaces.%20Our%20particular%20focus%20centers%20on%20the%0AWasserstein%20distance%20function%2C%20which%20serves%20as%20a%20relevant%20example.%20In%20contrast%0Ato%20the%20existing%20body%20of%20literature%20focused%20on%20approximating%20efficiently%0Apointwise%20evaluations%2C%20we%20chart%20a%20new%20course%20to%20define%20functional%20approximants%0Aby%20adopting%20three%20machine%20learning-based%20approaches%3A%201.%20Solving%20a%20finite%20number%0Aof%20optimal%20transport%20problems%20and%20computing%20the%20corresponding%20Wasserstein%0Apotentials.%202.%20Employing%20empirical%20risk%20minimization%20with%20Tikhonov%0Aregularization%20in%20Wasserstein%20Sobolev%20spaces.%203.%20Addressing%20the%20problem%20through%0Athe%20saddle%20point%20formulation%20that%20characterizes%20the%20weak%20form%20of%20the%20Tikhonov%0Afunctional%27s%20Euler-Lagrange%20equation.%20As%20a%20theoretical%20contribution%2C%20we%20furnish%0Aexplicit%20and%20quantitative%20bounds%20on%20generalization%20errors%20for%20each%20of%20these%0Asolutions.%20In%20the%20proofs%2C%20we%20leverage%20the%20theory%20of%20metric%20Sobolev%20spaces%20and%0Awe%20combine%20it%20with%20techniques%20of%20optimal%20transport%2C%20variational%20calculus%2C%20and%0Alarge%20deviation%20bounds.%20In%20our%20numerical%20implementation%2C%20we%20harness%0Aappropriately%20designed%20neural%20networks%20to%20serve%20as%20basis%20functions.%20These%0Anetworks%20undergo%20training%20using%20diverse%20methodologies.%20This%20approach%20allows%20us%0Ato%20obtain%20approximating%20functions%20that%20can%20be%20rapidly%20evaluated%20after%20training.%0AConsequently%2C%20our%20constructive%20solutions%20significantly%20enhance%20at%20equal%0Aaccuracy%20the%20evaluation%20speed%2C%20surpassing%20that%20of%20state-of-the-art%20methods%20by%0Aseveral%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19548v3&entry.124074799=Read"},
{"title": "Bayesian Functional Connectivity and Graph Convolutional Network for\n  Working Memory Load Classification", "author": "Harshini Gangapuram and Vidya Manian", "abstract": "  Brain responses related to working memory originate from distinct brain areas\nand oscillate at different frequencies. EEG signals with high temporal\ncorrelation can effectively capture these responses. Therefore, estimating the\nfunctional connectivity of EEG for working memory protocols in different\nfrequency bands plays a significant role in analyzing the brain dynamics with\nincreasing memory and cognitive loads, which remains largely unexplored. The\npresent study introduces a Bayesian structure learning algorithm to learn the\nfunctional connectivity of EEG in sensor space. Next, the functional\nconnectivity graphs are taken as input to the graph convolutional network to\nclassify the working memory loads. The intrasubject (subject-specific)\nclassification performed on 154 subjects for six different verbal working\nmemory loads produced the highest classification accuracy of 96% and average\nclassification accuracy of 89%, outperforming state-of-the-art classification\nmodels proposed in the literature. Furthermore, the proposed Bayesian structure\nlearning algorithm is compared with state-of-the-art functional connectivity\nestimation methods through intersubject and intrasubject statistical analysis\nof variance. The results also show that the alpha and theta bands have better\nclassification accuracy than the beta band.\n", "link": "http://arxiv.org/abs/2404.19467v1", "date": "2024-04-30", "relevancy": 1.3289, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4977}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4353}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4075}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Functional%20Connectivity%20and%20Graph%20Convolutional%20Network%20for%0A%20%20Working%20Memory%20Load%20Classification&body=Title%3A%20Bayesian%20Functional%20Connectivity%20and%20Graph%20Convolutional%20Network%20for%0A%20%20Working%20Memory%20Load%20Classification%0AAuthor%3A%20Harshini%20Gangapuram%20and%20Vidya%20Manian%0AAbstract%3A%20%20%20Brain%20responses%20related%20to%20working%20memory%20originate%20from%20distinct%20brain%20areas%0Aand%20oscillate%20at%20different%20frequencies.%20EEG%20signals%20with%20high%20temporal%0Acorrelation%20can%20effectively%20capture%20these%20responses.%20Therefore%2C%20estimating%20the%0Afunctional%20connectivity%20of%20EEG%20for%20working%20memory%20protocols%20in%20different%0Afrequency%20bands%20plays%20a%20significant%20role%20in%20analyzing%20the%20brain%20dynamics%20with%0Aincreasing%20memory%20and%20cognitive%20loads%2C%20which%20remains%20largely%20unexplored.%20The%0Apresent%20study%20introduces%20a%20Bayesian%20structure%20learning%20algorithm%20to%20learn%20the%0Afunctional%20connectivity%20of%20EEG%20in%20sensor%20space.%20Next%2C%20the%20functional%0Aconnectivity%20graphs%20are%20taken%20as%20input%20to%20the%20graph%20convolutional%20network%20to%0Aclassify%20the%20working%20memory%20loads.%20The%20intrasubject%20%28subject-specific%29%0Aclassification%20performed%20on%20154%20subjects%20for%20six%20different%20verbal%20working%0Amemory%20loads%20produced%20the%20highest%20classification%20accuracy%20of%2096%25%20and%20average%0Aclassification%20accuracy%20of%2089%25%2C%20outperforming%20state-of-the-art%20classification%0Amodels%20proposed%20in%20the%20literature.%20Furthermore%2C%20the%20proposed%20Bayesian%20structure%0Alearning%20algorithm%20is%20compared%20with%20state-of-the-art%20functional%20connectivity%0Aestimation%20methods%20through%20intersubject%20and%20intrasubject%20statistical%20analysis%0Aof%20variance.%20The%20results%20also%20show%20that%20the%20alpha%20and%20theta%20bands%20have%20better%0Aclassification%20accuracy%20than%20the%20beta%20band.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19467v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Functional%20Connectivity%20and%20Graph%20Convolutional%20Network%20for%0A%20%20Working%20Memory%20Load%20Classification&entry.906535625=Harshini%20Gangapuram%20and%20Vidya%20Manian&entry.1292438233=%20%20Brain%20responses%20related%20to%20working%20memory%20originate%20from%20distinct%20brain%20areas%0Aand%20oscillate%20at%20different%20frequencies.%20EEG%20signals%20with%20high%20temporal%0Acorrelation%20can%20effectively%20capture%20these%20responses.%20Therefore%2C%20estimating%20the%0Afunctional%20connectivity%20of%20EEG%20for%20working%20memory%20protocols%20in%20different%0Afrequency%20bands%20plays%20a%20significant%20role%20in%20analyzing%20the%20brain%20dynamics%20with%0Aincreasing%20memory%20and%20cognitive%20loads%2C%20which%20remains%20largely%20unexplored.%20The%0Apresent%20study%20introduces%20a%20Bayesian%20structure%20learning%20algorithm%20to%20learn%20the%0Afunctional%20connectivity%20of%20EEG%20in%20sensor%20space.%20Next%2C%20the%20functional%0Aconnectivity%20graphs%20are%20taken%20as%20input%20to%20the%20graph%20convolutional%20network%20to%0Aclassify%20the%20working%20memory%20loads.%20The%20intrasubject%20%28subject-specific%29%0Aclassification%20performed%20on%20154%20subjects%20for%20six%20different%20verbal%20working%0Amemory%20loads%20produced%20the%20highest%20classification%20accuracy%20of%2096%25%20and%20average%0Aclassification%20accuracy%20of%2089%25%2C%20outperforming%20state-of-the-art%20classification%0Amodels%20proposed%20in%20the%20literature.%20Furthermore%2C%20the%20proposed%20Bayesian%20structure%0Alearning%20algorithm%20is%20compared%20with%20state-of-the-art%20functional%20connectivity%0Aestimation%20methods%20through%20intersubject%20and%20intrasubject%20statistical%20analysis%0Aof%20variance.%20The%20results%20also%20show%20that%20the%20alpha%20and%20theta%20bands%20have%20better%0Aclassification%20accuracy%20than%20the%20beta%20band.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19467v1&entry.124074799=Read"},
{"title": "Fake it to make it: Using synthetic data to remedy the data shortage in\n  joint multimodal speech-and-gesture synthesis", "author": "Shivam Mehta and Anna Deichler and Jim O'Regan and Birger Mo\u00ebll and Jonas Beskow and Gustav Eje Henter and Simon Alexanderson", "abstract": "  Although humans engaged in face-to-face conversation simultaneously\ncommunicate both verbally and non-verbally, methods for joint and unified\nsynthesis of speech audio and co-speech 3D gesture motion from text are a new\nand emerging field. These technologies hold great promise for more human-like,\nefficient, expressive, and robust synthetic communication, but are currently\nheld back by the lack of suitably large datasets, as existing methods are\ntrained on parallel data from all constituent modalities. Inspired by\nstudent-teacher methods, we propose a straightforward solution to the data\nshortage, by simply synthesising additional training material. Specifically, we\nuse unimodal synthesis models trained on large datasets to create multimodal\n(but synthetic) parallel training data, and then pre-train a joint synthesis\nmodel on that material. In addition, we propose a new synthesis architecture\nthat adds better and more controllable prosody modelling to the\nstate-of-the-art method in the field. Our results confirm that pre-training on\nlarge amounts of synthetic data improves the quality of both the speech and the\nmotion synthesised by the multimodal model, with the proposed architecture\nyielding further benefits when pre-trained on the synthetic data. See\nhttps://shivammehta25.github.io/MAGI/ for example output.\n", "link": "http://arxiv.org/abs/2404.19622v1", "date": "2024-04-30", "relevancy": 1.5876, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5237}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fake%20it%20to%20make%20it%3A%20Using%20synthetic%20data%20to%20remedy%20the%20data%20shortage%20in%0A%20%20joint%20multimodal%20speech-and-gesture%20synthesis&body=Title%3A%20Fake%20it%20to%20make%20it%3A%20Using%20synthetic%20data%20to%20remedy%20the%20data%20shortage%20in%0A%20%20joint%20multimodal%20speech-and-gesture%20synthesis%0AAuthor%3A%20Shivam%20Mehta%20and%20Anna%20Deichler%20and%20Jim%20O%27Regan%20and%20Birger%20Mo%C3%ABll%20and%20Jonas%20Beskow%20and%20Gustav%20Eje%20Henter%20and%20Simon%20Alexanderson%0AAbstract%3A%20%20%20Although%20humans%20engaged%20in%20face-to-face%20conversation%20simultaneously%0Acommunicate%20both%20verbally%20and%20non-verbally%2C%20methods%20for%20joint%20and%20unified%0Asynthesis%20of%20speech%20audio%20and%20co-speech%203D%20gesture%20motion%20from%20text%20are%20a%20new%0Aand%20emerging%20field.%20These%20technologies%20hold%20great%20promise%20for%20more%20human-like%2C%0Aefficient%2C%20expressive%2C%20and%20robust%20synthetic%20communication%2C%20but%20are%20currently%0Aheld%20back%20by%20the%20lack%20of%20suitably%20large%20datasets%2C%20as%20existing%20methods%20are%0Atrained%20on%20parallel%20data%20from%20all%20constituent%20modalities.%20Inspired%20by%0Astudent-teacher%20methods%2C%20we%20propose%20a%20straightforward%20solution%20to%20the%20data%0Ashortage%2C%20by%20simply%20synthesising%20additional%20training%20material.%20Specifically%2C%20we%0Ause%20unimodal%20synthesis%20models%20trained%20on%20large%20datasets%20to%20create%20multimodal%0A%28but%20synthetic%29%20parallel%20training%20data%2C%20and%20then%20pre-train%20a%20joint%20synthesis%0Amodel%20on%20that%20material.%20In%20addition%2C%20we%20propose%20a%20new%20synthesis%20architecture%0Athat%20adds%20better%20and%20more%20controllable%20prosody%20modelling%20to%20the%0Astate-of-the-art%20method%20in%20the%20field.%20Our%20results%20confirm%20that%20pre-training%20on%0Alarge%20amounts%20of%20synthetic%20data%20improves%20the%20quality%20of%20both%20the%20speech%20and%20the%0Amotion%20synthesised%20by%20the%20multimodal%20model%2C%20with%20the%20proposed%20architecture%0Ayielding%20further%20benefits%20when%20pre-trained%20on%20the%20synthetic%20data.%20See%0Ahttps%3A//shivammehta25.github.io/MAGI/%20for%20example%20output.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19622v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fake%20it%20to%20make%20it%3A%20Using%20synthetic%20data%20to%20remedy%20the%20data%20shortage%20in%0A%20%20joint%20multimodal%20speech-and-gesture%20synthesis&entry.906535625=Shivam%20Mehta%20and%20Anna%20Deichler%20and%20Jim%20O%27Regan%20and%20Birger%20Mo%C3%ABll%20and%20Jonas%20Beskow%20and%20Gustav%20Eje%20Henter%20and%20Simon%20Alexanderson&entry.1292438233=%20%20Although%20humans%20engaged%20in%20face-to-face%20conversation%20simultaneously%0Acommunicate%20both%20verbally%20and%20non-verbally%2C%20methods%20for%20joint%20and%20unified%0Asynthesis%20of%20speech%20audio%20and%20co-speech%203D%20gesture%20motion%20from%20text%20are%20a%20new%0Aand%20emerging%20field.%20These%20technologies%20hold%20great%20promise%20for%20more%20human-like%2C%0Aefficient%2C%20expressive%2C%20and%20robust%20synthetic%20communication%2C%20but%20are%20currently%0Aheld%20back%20by%20the%20lack%20of%20suitably%20large%20datasets%2C%20as%20existing%20methods%20are%0Atrained%20on%20parallel%20data%20from%20all%20constituent%20modalities.%20Inspired%20by%0Astudent-teacher%20methods%2C%20we%20propose%20a%20straightforward%20solution%20to%20the%20data%0Ashortage%2C%20by%20simply%20synthesising%20additional%20training%20material.%20Specifically%2C%20we%0Ause%20unimodal%20synthesis%20models%20trained%20on%20large%20datasets%20to%20create%20multimodal%0A%28but%20synthetic%29%20parallel%20training%20data%2C%20and%20then%20pre-train%20a%20joint%20synthesis%0Amodel%20on%20that%20material.%20In%20addition%2C%20we%20propose%20a%20new%20synthesis%20architecture%0Athat%20adds%20better%20and%20more%20controllable%20prosody%20modelling%20to%20the%0Astate-of-the-art%20method%20in%20the%20field.%20Our%20results%20confirm%20that%20pre-training%20on%0Alarge%20amounts%20of%20synthetic%20data%20improves%20the%20quality%20of%20both%20the%20speech%20and%20the%0Amotion%20synthesised%20by%20the%20multimodal%20model%2C%20with%20the%20proposed%20architecture%0Ayielding%20further%20benefits%20when%20pre-trained%20on%20the%20synthetic%20data.%20See%0Ahttps%3A//shivammehta25.github.io/MAGI/%20for%20example%20output.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19622v1&entry.124074799=Read"},
{"title": "Beyond MOS: Subjective Image Quality Score Preprocessing Method Based on\n  Perceptual Similarity", "author": "Lei Wang and Desen Yuan", "abstract": "  Image quality assessment often relies on raw opinion scores provided by\nsubjects in subjective experiments, which can be noisy and unreliable. To\naddress this issue, postprocessing procedures such as ITU-R BT.500, ITU-T\nP.910, and ITU-T P.913 have been standardized to clean up the original opinion\nscores. These methods use annotator-based statistical priors, but they do not\ntake into account extensive information about the image itself, which limits\ntheir performance in less annotated scenarios. Generally speaking, image\nquality datasets usually contain similar scenes or distortions, and it is\ninevitable for subjects to compare images to score a reasonable score when\nscoring. Therefore, In this paper, we proposed Subjective Image Quality Score\nPreprocessing Method perceptual similarity Subjective Preprocessing (PSP),\nwhich exploit the perceptual similarity between images to alleviate subjective\nbias in less annotated scenarios. Specifically, we model subjective scoring as\na conditional probability model based on perceptual similarity with previously\nscored images, called subconscious reference scoring. The reference images are\nstored by a neighbor dictionary, which is obtained by a normalized vector\ndot-product based nearest neighbor search of the images' perceptual depth\nfeatures. Then the preprocessed score is updated by the exponential moving\naverage (EMA) of the subconscious reference scoring, called similarity\nregularized EMA. Our experiments on multiple datasets (LIVE, TID2013, CID2013)\nshow that this method can effectively remove the bias of the subjective scores.\nAdditionally, Experiments prove that the Preprocesed dataset can improve the\nperformance of downstream IQA tasks very well.\n", "link": "http://arxiv.org/abs/2404.19666v1", "date": "2024-04-30", "relevancy": 0.9549, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4871}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4751}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4702}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Beyond%20MOS%3A%20Subjective%20Image%20Quality%20Score%20Preprocessing%20Method%20Based%20on%0A%20%20Perceptual%20Similarity&body=Title%3A%20Beyond%20MOS%3A%20Subjective%20Image%20Quality%20Score%20Preprocessing%20Method%20Based%20on%0A%20%20Perceptual%20Similarity%0AAuthor%3A%20Lei%20Wang%20and%20Desen%20Yuan%0AAbstract%3A%20%20%20Image%20quality%20assessment%20often%20relies%20on%20raw%20opinion%20scores%20provided%20by%0Asubjects%20in%20subjective%20experiments%2C%20which%20can%20be%20noisy%20and%20unreliable.%20To%0Aaddress%20this%20issue%2C%20postprocessing%20procedures%20such%20as%20ITU-R%20BT.500%2C%20ITU-T%0AP.910%2C%20and%20ITU-T%20P.913%20have%20been%20standardized%20to%20clean%20up%20the%20original%20opinion%0Ascores.%20These%20methods%20use%20annotator-based%20statistical%20priors%2C%20but%20they%20do%20not%0Atake%20into%20account%20extensive%20information%20about%20the%20image%20itself%2C%20which%20limits%0Atheir%20performance%20in%20less%20annotated%20scenarios.%20Generally%20speaking%2C%20image%0Aquality%20datasets%20usually%20contain%20similar%20scenes%20or%20distortions%2C%20and%20it%20is%0Ainevitable%20for%20subjects%20to%20compare%20images%20to%20score%20a%20reasonable%20score%20when%0Ascoring.%20Therefore%2C%20In%20this%20paper%2C%20we%20proposed%20Subjective%20Image%20Quality%20Score%0APreprocessing%20Method%20perceptual%20similarity%20Subjective%20Preprocessing%20%28PSP%29%2C%0Awhich%20exploit%20the%20perceptual%20similarity%20between%20images%20to%20alleviate%20subjective%0Abias%20in%20less%20annotated%20scenarios.%20Specifically%2C%20we%20model%20subjective%20scoring%20as%0Aa%20conditional%20probability%20model%20based%20on%20perceptual%20similarity%20with%20previously%0Ascored%20images%2C%20called%20subconscious%20reference%20scoring.%20The%20reference%20images%20are%0Astored%20by%20a%20neighbor%20dictionary%2C%20which%20is%20obtained%20by%20a%20normalized%20vector%0Adot-product%20based%20nearest%20neighbor%20search%20of%20the%20images%27%20perceptual%20depth%0Afeatures.%20Then%20the%20preprocessed%20score%20is%20updated%20by%20the%20exponential%20moving%0Aaverage%20%28EMA%29%20of%20the%20subconscious%20reference%20scoring%2C%20called%20similarity%0Aregularized%20EMA.%20Our%20experiments%20on%20multiple%20datasets%20%28LIVE%2C%20TID2013%2C%20CID2013%29%0Ashow%20that%20this%20method%20can%20effectively%20remove%20the%20bias%20of%20the%20subjective%20scores.%0AAdditionally%2C%20Experiments%20prove%20that%20the%20Preprocesed%20dataset%20can%20improve%20the%0Aperformance%20of%20downstream%20IQA%20tasks%20very%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19666v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20MOS%3A%20Subjective%20Image%20Quality%20Score%20Preprocessing%20Method%20Based%20on%0A%20%20Perceptual%20Similarity&entry.906535625=Lei%20Wang%20and%20Desen%20Yuan&entry.1292438233=%20%20Image%20quality%20assessment%20often%20relies%20on%20raw%20opinion%20scores%20provided%20by%0Asubjects%20in%20subjective%20experiments%2C%20which%20can%20be%20noisy%20and%20unreliable.%20To%0Aaddress%20this%20issue%2C%20postprocessing%20procedures%20such%20as%20ITU-R%20BT.500%2C%20ITU-T%0AP.910%2C%20and%20ITU-T%20P.913%20have%20been%20standardized%20to%20clean%20up%20the%20original%20opinion%0Ascores.%20These%20methods%20use%20annotator-based%20statistical%20priors%2C%20but%20they%20do%20not%0Atake%20into%20account%20extensive%20information%20about%20the%20image%20itself%2C%20which%20limits%0Atheir%20performance%20in%20less%20annotated%20scenarios.%20Generally%20speaking%2C%20image%0Aquality%20datasets%20usually%20contain%20similar%20scenes%20or%20distortions%2C%20and%20it%20is%0Ainevitable%20for%20subjects%20to%20compare%20images%20to%20score%20a%20reasonable%20score%20when%0Ascoring.%20Therefore%2C%20In%20this%20paper%2C%20we%20proposed%20Subjective%20Image%20Quality%20Score%0APreprocessing%20Method%20perceptual%20similarity%20Subjective%20Preprocessing%20%28PSP%29%2C%0Awhich%20exploit%20the%20perceptual%20similarity%20between%20images%20to%20alleviate%20subjective%0Abias%20in%20less%20annotated%20scenarios.%20Specifically%2C%20we%20model%20subjective%20scoring%20as%0Aa%20conditional%20probability%20model%20based%20on%20perceptual%20similarity%20with%20previously%0Ascored%20images%2C%20called%20subconscious%20reference%20scoring.%20The%20reference%20images%20are%0Astored%20by%20a%20neighbor%20dictionary%2C%20which%20is%20obtained%20by%20a%20normalized%20vector%0Adot-product%20based%20nearest%20neighbor%20search%20of%20the%20images%27%20perceptual%20depth%0Afeatures.%20Then%20the%20preprocessed%20score%20is%20updated%20by%20the%20exponential%20moving%0Aaverage%20%28EMA%29%20of%20the%20subconscious%20reference%20scoring%2C%20called%20similarity%0Aregularized%20EMA.%20Our%20experiments%20on%20multiple%20datasets%20%28LIVE%2C%20TID2013%2C%20CID2013%29%0Ashow%20that%20this%20method%20can%20effectively%20remove%20the%20bias%20of%20the%20subjective%20scores.%0AAdditionally%2C%20Experiments%20prove%20that%20the%20Preprocesed%20dataset%20can%20improve%20the%0Aperformance%20of%20downstream%20IQA%20tasks%20very%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19666v1&entry.124074799=Read"},
{"title": "FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning\n  Leveraging Weight Decomposition", "author": "Yuxuan Yan and Shunpu Tang and Zhiguo Shi and Qianqian Yang", "abstract": "  Pre-trained Language Models (PLMs) have shown excellent performance on\nvarious downstream tasks after fine-tuning. Nevertheless, the escalating\nconcerns surrounding user privacy have posed significant challenges to\ncentralized training reliant on extensive data collection. Federated learning,\nwhich only requires training on the clients and aggregates weights on the\nserver without sharing data, has emerged as a solution. However, the\nsubstantial parameter size of PLMs places a significant burden on the\ncomputational resources of client devices, while also leading to costly\ncommunication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) into\nfederated learning can effectively address this problem. However, we observe\nthat the non-IID data in federated learning leads to a gap in performance\nbetween the PEFT method and full parameter fine-tuning(FFT). To overcome this,\nwe propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method in\nfederated learning. FeDeRA uses the same adapter module as LoRA. However, the\ndifference lies in FeDeRA's initialization of the adapter module by performing\nSingular Value Decomposition (SVD) on the pre-trained matrix and selecting its\nprincipal components. We conducted extensive experiments, using RoBERTa and\nDeBERTaV3, on six datasets, comparing the methods including FFT and the other\nthree different PEFT methods. FeDeRA outperforms all other PEFT methods and is\ncomparable to or even surpasses the performance of FFT method. We also deployed\nfederated learning on Jetson AGX Orin and compared the time required by\ndifferent methods to achieve the target accuracy on specific tasks. Compared to\nFFT, FeDeRA reduces the training time by 95.9\\%, 97.9\\%, 96.9\\% and 97.3\\%,\n96.5\\%, 96.5\\% respectively on three tasks using RoBERTa and DeBERTaV3. The\noverall experiments indicate that FeDeRA achieves good performance while also\nmaintaining efficiency.\n", "link": "http://arxiv.org/abs/2404.18848v2", "date": "2024-04-30", "relevancy": 1.4621, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4889}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4841}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FeDeRA%3AEfficient%20Fine-tuning%20of%20Language%20Models%20in%20Federated%20Learning%0A%20%20Leveraging%20Weight%20Decomposition&body=Title%3A%20FeDeRA%3AEfficient%20Fine-tuning%20of%20Language%20Models%20in%20Federated%20Learning%0A%20%20Leveraging%20Weight%20Decomposition%0AAuthor%3A%20Yuxuan%20Yan%20and%20Shunpu%20Tang%20and%20Zhiguo%20Shi%20and%20Qianqian%20Yang%0AAbstract%3A%20%20%20Pre-trained%20Language%20Models%20%28PLMs%29%20have%20shown%20excellent%20performance%20on%0Avarious%20downstream%20tasks%20after%20fine-tuning.%20Nevertheless%2C%20the%20escalating%0Aconcerns%20surrounding%20user%20privacy%20have%20posed%20significant%20challenges%20to%0Acentralized%20training%20reliant%20on%20extensive%20data%20collection.%20Federated%20learning%2C%0Awhich%20only%20requires%20training%20on%20the%20clients%20and%20aggregates%20weights%20on%20the%0Aserver%20without%20sharing%20data%2C%20has%20emerged%20as%20a%20solution.%20However%2C%20the%0Asubstantial%20parameter%20size%20of%20PLMs%20places%20a%20significant%20burden%20on%20the%0Acomputational%20resources%20of%20client%20devices%2C%20while%20also%20leading%20to%20costly%0Acommunication%20expenses.%20Introducing%20Parameter-Efficient%20Fine-Tuning%28PEFT%29%20into%0Afederated%20learning%20can%20effectively%20address%20this%20problem.%20However%2C%20we%20observe%0Athat%20the%20non-IID%20data%20in%20federated%20learning%20leads%20to%20a%20gap%20in%20performance%0Abetween%20the%20PEFT%20method%20and%20full%20parameter%20fine-tuning%28FFT%29.%20To%20overcome%20this%2C%0Awe%20propose%20FeDeRA%2C%20an%20improvement%20over%20the%20Low-Rank%20Adaption%28LoRA%29%20method%20in%0Afederated%20learning.%20FeDeRA%20uses%20the%20same%20adapter%20module%20as%20LoRA.%20However%2C%20the%0Adifference%20lies%20in%20FeDeRA%27s%20initialization%20of%20the%20adapter%20module%20by%20performing%0ASingular%20Value%20Decomposition%20%28SVD%29%20on%20the%20pre-trained%20matrix%20and%20selecting%20its%0Aprincipal%20components.%20We%20conducted%20extensive%20experiments%2C%20using%20RoBERTa%20and%0ADeBERTaV3%2C%20on%20six%20datasets%2C%20comparing%20the%20methods%20including%20FFT%20and%20the%20other%0Athree%20different%20PEFT%20methods.%20FeDeRA%20outperforms%20all%20other%20PEFT%20methods%20and%20is%0Acomparable%20to%20or%20even%20surpasses%20the%20performance%20of%20FFT%20method.%20We%20also%20deployed%0Afederated%20learning%20on%20Jetson%20AGX%20Orin%20and%20compared%20the%20time%20required%20by%0Adifferent%20methods%20to%20achieve%20the%20target%20accuracy%20on%20specific%20tasks.%20Compared%20to%0AFFT%2C%20FeDeRA%20reduces%20the%20training%20time%20by%2095.9%5C%25%2C%2097.9%5C%25%2C%2096.9%5C%25%20and%2097.3%5C%25%2C%0A96.5%5C%25%2C%2096.5%5C%25%20respectively%20on%20three%20tasks%20using%20RoBERTa%20and%20DeBERTaV3.%20The%0Aoverall%20experiments%20indicate%20that%20FeDeRA%20achieves%20good%20performance%20while%20also%0Amaintaining%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18848v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeDeRA%3AEfficient%20Fine-tuning%20of%20Language%20Models%20in%20Federated%20Learning%0A%20%20Leveraging%20Weight%20Decomposition&entry.906535625=Yuxuan%20Yan%20and%20Shunpu%20Tang%20and%20Zhiguo%20Shi%20and%20Qianqian%20Yang&entry.1292438233=%20%20Pre-trained%20Language%20Models%20%28PLMs%29%20have%20shown%20excellent%20performance%20on%0Avarious%20downstream%20tasks%20after%20fine-tuning.%20Nevertheless%2C%20the%20escalating%0Aconcerns%20surrounding%20user%20privacy%20have%20posed%20significant%20challenges%20to%0Acentralized%20training%20reliant%20on%20extensive%20data%20collection.%20Federated%20learning%2C%0Awhich%20only%20requires%20training%20on%20the%20clients%20and%20aggregates%20weights%20on%20the%0Aserver%20without%20sharing%20data%2C%20has%20emerged%20as%20a%20solution.%20However%2C%20the%0Asubstantial%20parameter%20size%20of%20PLMs%20places%20a%20significant%20burden%20on%20the%0Acomputational%20resources%20of%20client%20devices%2C%20while%20also%20leading%20to%20costly%0Acommunication%20expenses.%20Introducing%20Parameter-Efficient%20Fine-Tuning%28PEFT%29%20into%0Afederated%20learning%20can%20effectively%20address%20this%20problem.%20However%2C%20we%20observe%0Athat%20the%20non-IID%20data%20in%20federated%20learning%20leads%20to%20a%20gap%20in%20performance%0Abetween%20the%20PEFT%20method%20and%20full%20parameter%20fine-tuning%28FFT%29.%20To%20overcome%20this%2C%0Awe%20propose%20FeDeRA%2C%20an%20improvement%20over%20the%20Low-Rank%20Adaption%28LoRA%29%20method%20in%0Afederated%20learning.%20FeDeRA%20uses%20the%20same%20adapter%20module%20as%20LoRA.%20However%2C%20the%0Adifference%20lies%20in%20FeDeRA%27s%20initialization%20of%20the%20adapter%20module%20by%20performing%0ASingular%20Value%20Decomposition%20%28SVD%29%20on%20the%20pre-trained%20matrix%20and%20selecting%20its%0Aprincipal%20components.%20We%20conducted%20extensive%20experiments%2C%20using%20RoBERTa%20and%0ADeBERTaV3%2C%20on%20six%20datasets%2C%20comparing%20the%20methods%20including%20FFT%20and%20the%20other%0Athree%20different%20PEFT%20methods.%20FeDeRA%20outperforms%20all%20other%20PEFT%20methods%20and%20is%0Acomparable%20to%20or%20even%20surpasses%20the%20performance%20of%20FFT%20method.%20We%20also%20deployed%0Afederated%20learning%20on%20Jetson%20AGX%20Orin%20and%20compared%20the%20time%20required%20by%0Adifferent%20methods%20to%20achieve%20the%20target%20accuracy%20on%20specific%20tasks.%20Compared%20to%0AFFT%2C%20FeDeRA%20reduces%20the%20training%20time%20by%2095.9%5C%25%2C%2097.9%5C%25%2C%2096.9%5C%25%20and%2097.3%5C%25%2C%0A96.5%5C%25%2C%2096.5%5C%25%20respectively%20on%20three%20tasks%20using%20RoBERTa%20and%20DeBERTaV3.%20The%0Aoverall%20experiments%20indicate%20that%20FeDeRA%20achieves%20good%20performance%20while%20also%0Amaintaining%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18848v2&entry.124074799=Read"},
{"title": "Solving Long-run Average Reward Robust MDPs via Stochastic Games", "author": "Krishnendu Chatterjee and Ehsan Kafshdar Goharshady and Mehrdad Karrabi and Petr Novotn\u00fd and \u0110or\u0111e \u017dikeli\u0107", "abstract": "  Markov decision processes (MDPs) provide a standard framework for sequential\ndecision making under uncertainty. However, MDPs do not take uncertainty in\ntransition probabilities into account. Robust Markov decision processes (RMDPs)\naddress this shortcoming of MDPs by assigning to each transition an uncertainty\nset rather than a single probability value. In this work, we consider polytopic\nRMDPs in which all uncertainty sets are polytopes and study the problem of\nsolving long-run average reward polytopic RMDPs. We present a novel perspective\non this problem and show that it can be reduced to solving long-run average\nreward turn-based stochastic games with finite state and action spaces. This\nreduction allows us to derive several important consequences that were hitherto\nnot known to hold for polytopic RMDPs. First, we derive new computational\ncomplexity bounds for solving long-run average reward polytopic RMDPs, showing\nfor the first time that the threshold decision problem for them is in $NP \\cap\ncoNP$ and that they admit a randomized algorithm with sub-exponential expected\nruntime. Second, we present Robust Polytopic Policy Iteration (RPPI), a novel\npolicy iteration algorithm for solving long-run average reward polytopic RMDPs.\nOur experimental evaluation shows that RPPI is much more efficient in solving\nlong-run average reward polytopic RMDPs compared to state-of-the-art methods\nbased on value iteration.\n", "link": "http://arxiv.org/abs/2312.13912v2", "date": "2024-04-30", "relevancy": 1.3576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4793}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Solving%20Long-run%20Average%20Reward%20Robust%20MDPs%20via%20Stochastic%20Games&body=Title%3A%20Solving%20Long-run%20Average%20Reward%20Robust%20MDPs%20via%20Stochastic%20Games%0AAuthor%3A%20Krishnendu%20Chatterjee%20and%20Ehsan%20Kafshdar%20Goharshady%20and%20Mehrdad%20Karrabi%20and%20Petr%20Novotn%C3%BD%20and%20%C4%90or%C4%91e%20%C5%BDikeli%C4%87%0AAbstract%3A%20%20%20Markov%20decision%20processes%20%28MDPs%29%20provide%20a%20standard%20framework%20for%20sequential%0Adecision%20making%20under%20uncertainty.%20However%2C%20MDPs%20do%20not%20take%20uncertainty%20in%0Atransition%20probabilities%20into%20account.%20Robust%20Markov%20decision%20processes%20%28RMDPs%29%0Aaddress%20this%20shortcoming%20of%20MDPs%20by%20assigning%20to%20each%20transition%20an%20uncertainty%0Aset%20rather%20than%20a%20single%20probability%20value.%20In%20this%20work%2C%20we%20consider%20polytopic%0ARMDPs%20in%20which%20all%20uncertainty%20sets%20are%20polytopes%20and%20study%20the%20problem%20of%0Asolving%20long-run%20average%20reward%20polytopic%20RMDPs.%20We%20present%20a%20novel%20perspective%0Aon%20this%20problem%20and%20show%20that%20it%20can%20be%20reduced%20to%20solving%20long-run%20average%0Areward%20turn-based%20stochastic%20games%20with%20finite%20state%20and%20action%20spaces.%20This%0Areduction%20allows%20us%20to%20derive%20several%20important%20consequences%20that%20were%20hitherto%0Anot%20known%20to%20hold%20for%20polytopic%20RMDPs.%20First%2C%20we%20derive%20new%20computational%0Acomplexity%20bounds%20for%20solving%20long-run%20average%20reward%20polytopic%20RMDPs%2C%20showing%0Afor%20the%20first%20time%20that%20the%20threshold%20decision%20problem%20for%20them%20is%20in%20%24NP%20%5Ccap%0AcoNP%24%20and%20that%20they%20admit%20a%20randomized%20algorithm%20with%20sub-exponential%20expected%0Aruntime.%20Second%2C%20we%20present%20Robust%20Polytopic%20Policy%20Iteration%20%28RPPI%29%2C%20a%20novel%0Apolicy%20iteration%20algorithm%20for%20solving%20long-run%20average%20reward%20polytopic%20RMDPs.%0AOur%20experimental%20evaluation%20shows%20that%20RPPI%20is%20much%20more%20efficient%20in%20solving%0Along-run%20average%20reward%20polytopic%20RMDPs%20compared%20to%20state-of-the-art%20methods%0Abased%20on%20value%20iteration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13912v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Long-run%20Average%20Reward%20Robust%20MDPs%20via%20Stochastic%20Games&entry.906535625=Krishnendu%20Chatterjee%20and%20Ehsan%20Kafshdar%20Goharshady%20and%20Mehrdad%20Karrabi%20and%20Petr%20Novotn%C3%BD%20and%20%C4%90or%C4%91e%20%C5%BDikeli%C4%87&entry.1292438233=%20%20Markov%20decision%20processes%20%28MDPs%29%20provide%20a%20standard%20framework%20for%20sequential%0Adecision%20making%20under%20uncertainty.%20However%2C%20MDPs%20do%20not%20take%20uncertainty%20in%0Atransition%20probabilities%20into%20account.%20Robust%20Markov%20decision%20processes%20%28RMDPs%29%0Aaddress%20this%20shortcoming%20of%20MDPs%20by%20assigning%20to%20each%20transition%20an%20uncertainty%0Aset%20rather%20than%20a%20single%20probability%20value.%20In%20this%20work%2C%20we%20consider%20polytopic%0ARMDPs%20in%20which%20all%20uncertainty%20sets%20are%20polytopes%20and%20study%20the%20problem%20of%0Asolving%20long-run%20average%20reward%20polytopic%20RMDPs.%20We%20present%20a%20novel%20perspective%0Aon%20this%20problem%20and%20show%20that%20it%20can%20be%20reduced%20to%20solving%20long-run%20average%0Areward%20turn-based%20stochastic%20games%20with%20finite%20state%20and%20action%20spaces.%20This%0Areduction%20allows%20us%20to%20derive%20several%20important%20consequences%20that%20were%20hitherto%0Anot%20known%20to%20hold%20for%20polytopic%20RMDPs.%20First%2C%20we%20derive%20new%20computational%0Acomplexity%20bounds%20for%20solving%20long-run%20average%20reward%20polytopic%20RMDPs%2C%20showing%0Afor%20the%20first%20time%20that%20the%20threshold%20decision%20problem%20for%20them%20is%20in%20%24NP%20%5Ccap%0AcoNP%24%20and%20that%20they%20admit%20a%20randomized%20algorithm%20with%20sub-exponential%20expected%0Aruntime.%20Second%2C%20we%20present%20Robust%20Polytopic%20Policy%20Iteration%20%28RPPI%29%2C%20a%20novel%0Apolicy%20iteration%20algorithm%20for%20solving%20long-run%20average%20reward%20polytopic%20RMDPs.%0AOur%20experimental%20evaluation%20shows%20that%20RPPI%20is%20much%20more%20efficient%20in%20solving%0Along-run%20average%20reward%20polytopic%20RMDPs%20compared%20to%20state-of-the-art%20methods%0Abased%20on%20value%20iteration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13912v2&entry.124074799=Read"},
{"title": "Be Aware of the Neighborhood Effect: Modeling Selection Bias under\n  Interference", "author": "Haoxuan Li and Chunyuan Zheng and Sihao Ding and Peng Wu and Zhi Geng and Fuli Feng and Xiangnan He", "abstract": "  Selection bias in recommender system arises from the recommendation process\nof system filtering and the interactive process of user selection. Many\nprevious studies have focused on addressing selection bias to achieve unbiased\nlearning of the prediction model, but ignore the fact that potential outcomes\nfor a given user-item pair may vary with the treatments assigned to other\nuser-item pairs, named neighborhood effect. To fill the gap, this paper\nformally formulates the neighborhood effect as an interference problem from the\nperspective of causal inference and introduces a treatment representation to\ncapture the neighborhood effect. On this basis, we propose a novel ideal loss\nthat can be used to deal with selection bias in the presence of neighborhood\neffect. We further develop two new estimators for estimating the proposed ideal\nloss. We theoretically establish the connection between the proposed and\nprevious debiasing methods ignoring the neighborhood effect, showing that the\nproposed methods can achieve unbiased learning when both selection bias and\nneighborhood effect are present, while the existing methods are biased.\nExtensive semi-synthetic and real-world experiments are conducted to\ndemonstrate the effectiveness of the proposed methods.\n", "link": "http://arxiv.org/abs/2404.19620v1", "date": "2024-04-30", "relevancy": 1.4646, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4947}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4633}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Be%20Aware%20of%20the%20Neighborhood%20Effect%3A%20Modeling%20Selection%20Bias%20under%0A%20%20Interference&body=Title%3A%20Be%20Aware%20of%20the%20Neighborhood%20Effect%3A%20Modeling%20Selection%20Bias%20under%0A%20%20Interference%0AAuthor%3A%20Haoxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Sihao%20Ding%20and%20Peng%20Wu%20and%20Zhi%20Geng%20and%20Fuli%20Feng%20and%20Xiangnan%20He%0AAbstract%3A%20%20%20Selection%20bias%20in%20recommender%20system%20arises%20from%20the%20recommendation%20process%0Aof%20system%20filtering%20and%20the%20interactive%20process%20of%20user%20selection.%20Many%0Aprevious%20studies%20have%20focused%20on%20addressing%20selection%20bias%20to%20achieve%20unbiased%0Alearning%20of%20the%20prediction%20model%2C%20but%20ignore%20the%20fact%20that%20potential%20outcomes%0Afor%20a%20given%20user-item%20pair%20may%20vary%20with%20the%20treatments%20assigned%20to%20other%0Auser-item%20pairs%2C%20named%20neighborhood%20effect.%20To%20fill%20the%20gap%2C%20this%20paper%0Aformally%20formulates%20the%20neighborhood%20effect%20as%20an%20interference%20problem%20from%20the%0Aperspective%20of%20causal%20inference%20and%20introduces%20a%20treatment%20representation%20to%0Acapture%20the%20neighborhood%20effect.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20ideal%20loss%0Athat%20can%20be%20used%20to%20deal%20with%20selection%20bias%20in%20the%20presence%20of%20neighborhood%0Aeffect.%20We%20further%20develop%20two%20new%20estimators%20for%20estimating%20the%20proposed%20ideal%0Aloss.%20We%20theoretically%20establish%20the%20connection%20between%20the%20proposed%20and%0Aprevious%20debiasing%20methods%20ignoring%20the%20neighborhood%20effect%2C%20showing%20that%20the%0Aproposed%20methods%20can%20achieve%20unbiased%20learning%20when%20both%20selection%20bias%20and%0Aneighborhood%20effect%20are%20present%2C%20while%20the%20existing%20methods%20are%20biased.%0AExtensive%20semi-synthetic%20and%20real-world%20experiments%20are%20conducted%20to%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be%20Aware%20of%20the%20Neighborhood%20Effect%3A%20Modeling%20Selection%20Bias%20under%0A%20%20Interference&entry.906535625=Haoxuan%20Li%20and%20Chunyuan%20Zheng%20and%20Sihao%20Ding%20and%20Peng%20Wu%20and%20Zhi%20Geng%20and%20Fuli%20Feng%20and%20Xiangnan%20He&entry.1292438233=%20%20Selection%20bias%20in%20recommender%20system%20arises%20from%20the%20recommendation%20process%0Aof%20system%20filtering%20and%20the%20interactive%20process%20of%20user%20selection.%20Many%0Aprevious%20studies%20have%20focused%20on%20addressing%20selection%20bias%20to%20achieve%20unbiased%0Alearning%20of%20the%20prediction%20model%2C%20but%20ignore%20the%20fact%20that%20potential%20outcomes%0Afor%20a%20given%20user-item%20pair%20may%20vary%20with%20the%20treatments%20assigned%20to%20other%0Auser-item%20pairs%2C%20named%20neighborhood%20effect.%20To%20fill%20the%20gap%2C%20this%20paper%0Aformally%20formulates%20the%20neighborhood%20effect%20as%20an%20interference%20problem%20from%20the%0Aperspective%20of%20causal%20inference%20and%20introduces%20a%20treatment%20representation%20to%0Acapture%20the%20neighborhood%20effect.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20ideal%20loss%0Athat%20can%20be%20used%20to%20deal%20with%20selection%20bias%20in%20the%20presence%20of%20neighborhood%0Aeffect.%20We%20further%20develop%20two%20new%20estimators%20for%20estimating%20the%20proposed%20ideal%0Aloss.%20We%20theoretically%20establish%20the%20connection%20between%20the%20proposed%20and%0Aprevious%20debiasing%20methods%20ignoring%20the%20neighborhood%20effect%2C%20showing%20that%20the%0Aproposed%20methods%20can%20achieve%20unbiased%20learning%20when%20both%20selection%20bias%20and%0Aneighborhood%20effect%20are%20present%2C%20while%20the%20existing%20methods%20are%20biased.%0AExtensive%20semi-synthetic%20and%20real-world%20experiments%20are%20conducted%20to%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19620v1&entry.124074799=Read"},
{"title": "Towards a Systems Theory of Algorithms", "author": "Florian D\u00f6rfler and Zhiyu He and Giuseppe Belgioioso and Saverio Bolognani and John Lygeros and Michael Muehlebach", "abstract": "  Traditionally, numerical algorithms are seen as isolated pieces of code\nconfined to an {\\em in silico} existence. However, this perspective is not\nappropriate for many modern computational approaches in control, learning, or\noptimization, wherein {\\em in vivo} algorithms interact with their environment.\nExamples of such {\\em open algorithms} include various real-time\noptimization-based control strategies, reinforcement learning, decision-making\narchitectures, online optimization, and many more. Further, even {\\em closed}\nalgorithms in learning or optimization are increasingly abstracted in block\ndiagrams with interacting dynamic modules and pipelines. In this opinion paper,\nwe state our vision on a to-be-cultivated {\\em systems theory of algorithms}\nand argue in favor of viewing algorithms as open dynamical systems interacting\nwith other algorithms, physical systems, humans, or databases. Remarkably, the\nmanifold tools developed under the umbrella of systems theory are well suited\nfor addressing a range of challenges in the algorithmic domain. We survey\nvarious instances where the principles of algorithmic systems theory are being\ndeveloped and outline pertinent modeling, analysis, and design challenges.\n", "link": "http://arxiv.org/abs/2401.14029v2", "date": "2024-04-30", "relevancy": 1.2234, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4193}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.395}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Systems%20Theory%20of%20Algorithms&body=Title%3A%20Towards%20a%20Systems%20Theory%20of%20Algorithms%0AAuthor%3A%20Florian%20D%C3%B6rfler%20and%20Zhiyu%20He%20and%20Giuseppe%20Belgioioso%20and%20Saverio%20Bolognani%20and%20John%20Lygeros%20and%20Michael%20Muehlebach%0AAbstract%3A%20%20%20Traditionally%2C%20numerical%20algorithms%20are%20seen%20as%20isolated%20pieces%20of%20code%0Aconfined%20to%20an%20%7B%5Cem%20in%20silico%7D%20existence.%20However%2C%20this%20perspective%20is%20not%0Aappropriate%20for%20many%20modern%20computational%20approaches%20in%20control%2C%20learning%2C%20or%0Aoptimization%2C%20wherein%20%7B%5Cem%20in%20vivo%7D%20algorithms%20interact%20with%20their%20environment.%0AExamples%20of%20such%20%7B%5Cem%20open%20algorithms%7D%20include%20various%20real-time%0Aoptimization-based%20control%20strategies%2C%20reinforcement%20learning%2C%20decision-making%0Aarchitectures%2C%20online%20optimization%2C%20and%20many%20more.%20Further%2C%20even%20%7B%5Cem%20closed%7D%0Aalgorithms%20in%20learning%20or%20optimization%20are%20increasingly%20abstracted%20in%20block%0Adiagrams%20with%20interacting%20dynamic%20modules%20and%20pipelines.%20In%20this%20opinion%20paper%2C%0Awe%20state%20our%20vision%20on%20a%20to-be-cultivated%20%7B%5Cem%20systems%20theory%20of%20algorithms%7D%0Aand%20argue%20in%20favor%20of%20viewing%20algorithms%20as%20open%20dynamical%20systems%20interacting%0Awith%20other%20algorithms%2C%20physical%20systems%2C%20humans%2C%20or%20databases.%20Remarkably%2C%20the%0Amanifold%20tools%20developed%20under%20the%20umbrella%20of%20systems%20theory%20are%20well%20suited%0Afor%20addressing%20a%20range%20of%20challenges%20in%20the%20algorithmic%20domain.%20We%20survey%0Avarious%20instances%20where%20the%20principles%20of%20algorithmic%20systems%20theory%20are%20being%0Adeveloped%20and%20outline%20pertinent%20modeling%2C%20analysis%2C%20and%20design%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14029v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Systems%20Theory%20of%20Algorithms&entry.906535625=Florian%20D%C3%B6rfler%20and%20Zhiyu%20He%20and%20Giuseppe%20Belgioioso%20and%20Saverio%20Bolognani%20and%20John%20Lygeros%20and%20Michael%20Muehlebach&entry.1292438233=%20%20Traditionally%2C%20numerical%20algorithms%20are%20seen%20as%20isolated%20pieces%20of%20code%0Aconfined%20to%20an%20%7B%5Cem%20in%20silico%7D%20existence.%20However%2C%20this%20perspective%20is%20not%0Aappropriate%20for%20many%20modern%20computational%20approaches%20in%20control%2C%20learning%2C%20or%0Aoptimization%2C%20wherein%20%7B%5Cem%20in%20vivo%7D%20algorithms%20interact%20with%20their%20environment.%0AExamples%20of%20such%20%7B%5Cem%20open%20algorithms%7D%20include%20various%20real-time%0Aoptimization-based%20control%20strategies%2C%20reinforcement%20learning%2C%20decision-making%0Aarchitectures%2C%20online%20optimization%2C%20and%20many%20more.%20Further%2C%20even%20%7B%5Cem%20closed%7D%0Aalgorithms%20in%20learning%20or%20optimization%20are%20increasingly%20abstracted%20in%20block%0Adiagrams%20with%20interacting%20dynamic%20modules%20and%20pipelines.%20In%20this%20opinion%20paper%2C%0Awe%20state%20our%20vision%20on%20a%20to-be-cultivated%20%7B%5Cem%20systems%20theory%20of%20algorithms%7D%0Aand%20argue%20in%20favor%20of%20viewing%20algorithms%20as%20open%20dynamical%20systems%20interacting%0Awith%20other%20algorithms%2C%20physical%20systems%2C%20humans%2C%20or%20databases.%20Remarkably%2C%20the%0Amanifold%20tools%20developed%20under%20the%20umbrella%20of%20systems%20theory%20are%20well%20suited%0Afor%20addressing%20a%20range%20of%20challenges%20in%20the%20algorithmic%20domain.%20We%20survey%0Avarious%20instances%20where%20the%20principles%20of%20algorithmic%20systems%20theory%20are%20being%0Adeveloped%20and%20outline%20pertinent%20modeling%2C%20analysis%2C%20and%20design%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14029v2&entry.124074799=Read"},
{"title": "PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for\n  Privacy Policy Compliance Verification", "author": "Leon Garza and Lavanya Elluri and Anantaa Kotal and Aritran Piplai and Deepti Gupta and Anupam Joshi", "abstract": "  Data protection and privacy is becoming increasingly crucial in the digital\nera. Numerous companies depend on third-party vendors and service providers to\ncarry out critical functions within their operations, encompassing tasks such\nas data handling and storage. However, this reliance introduces potential\nvulnerabilities, as these vendors' security measures and practices may not\nalways align with the standards expected by regulatory bodies. Businesses are\nrequired, often under the penalty of law, to ensure compliance with the\nevolving regulatory rules. Interpreting and implementing these regulations pose\nchallenges due to their complexity. Regulatory documents are extensive,\ndemanding significant effort for interpretation, while vendor-drafted privacy\npolicies often lack the detail required for full legal compliance, leading to\nambiguity. To ensure a concise interpretation of the regulatory requirements\nand compliance of organizational privacy policy with said regulations, we\npropose a Large Language Model (LLM) and Semantic Web based approach for\nprivacy compliance. In this paper, we develop the novel Privacy Policy\nCompliance Verification Knowledge Graph, PrivComp-KG. It is designed to\nefficiently store and retrieve comprehensive information concerning privacy\npolicies, regulatory frameworks, and domain-specific knowledge pertaining to\nthe legal landscape of privacy. Using Retrieval Augmented Generation, we\nidentify the relevant sections in a privacy policy with corresponding\nregulatory rules. This information about individual privacy policies is\npopulated into the PrivComp-KG. Combining this with the domain context and\nrules, the PrivComp-KG can be queried to check for compliance with privacy\npolicies by each vendor against relevant policy regulations. We demonstrate the\nrelevance of the PrivComp-KG, by verifying compliance of privacy policy\ndocuments for various organizations.\n", "link": "http://arxiv.org/abs/2404.19744v1", "date": "2024-04-30", "relevancy": 1.3484, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4407}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4393}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PrivComp-KG%20%3A%20Leveraging%20Knowledge%20Graph%20and%20Large%20Language%20Models%20for%0A%20%20Privacy%20Policy%20Compliance%20Verification&body=Title%3A%20PrivComp-KG%20%3A%20Leveraging%20Knowledge%20Graph%20and%20Large%20Language%20Models%20for%0A%20%20Privacy%20Policy%20Compliance%20Verification%0AAuthor%3A%20Leon%20Garza%20and%20Lavanya%20Elluri%20and%20Anantaa%20Kotal%20and%20Aritran%20Piplai%20and%20Deepti%20Gupta%20and%20Anupam%20Joshi%0AAbstract%3A%20%20%20Data%20protection%20and%20privacy%20is%20becoming%20increasingly%20crucial%20in%20the%20digital%0Aera.%20Numerous%20companies%20depend%20on%20third-party%20vendors%20and%20service%20providers%20to%0Acarry%20out%20critical%20functions%20within%20their%20operations%2C%20encompassing%20tasks%20such%0Aas%20data%20handling%20and%20storage.%20However%2C%20this%20reliance%20introduces%20potential%0Avulnerabilities%2C%20as%20these%20vendors%27%20security%20measures%20and%20practices%20may%20not%0Aalways%20align%20with%20the%20standards%20expected%20by%20regulatory%20bodies.%20Businesses%20are%0Arequired%2C%20often%20under%20the%20penalty%20of%20law%2C%20to%20ensure%20compliance%20with%20the%0Aevolving%20regulatory%20rules.%20Interpreting%20and%20implementing%20these%20regulations%20pose%0Achallenges%20due%20to%20their%20complexity.%20Regulatory%20documents%20are%20extensive%2C%0Ademanding%20significant%20effort%20for%20interpretation%2C%20while%20vendor-drafted%20privacy%0Apolicies%20often%20lack%20the%20detail%20required%20for%20full%20legal%20compliance%2C%20leading%20to%0Aambiguity.%20To%20ensure%20a%20concise%20interpretation%20of%20the%20regulatory%20requirements%0Aand%20compliance%20of%20organizational%20privacy%20policy%20with%20said%20regulations%2C%20we%0Apropose%20a%20Large%20Language%20Model%20%28LLM%29%20and%20Semantic%20Web%20based%20approach%20for%0Aprivacy%20compliance.%20In%20this%20paper%2C%20we%20develop%20the%20novel%20Privacy%20Policy%0ACompliance%20Verification%20Knowledge%20Graph%2C%20PrivComp-KG.%20It%20is%20designed%20to%0Aefficiently%20store%20and%20retrieve%20comprehensive%20information%20concerning%20privacy%0Apolicies%2C%20regulatory%20frameworks%2C%20and%20domain-specific%20knowledge%20pertaining%20to%0Athe%20legal%20landscape%20of%20privacy.%20Using%20Retrieval%20Augmented%20Generation%2C%20we%0Aidentify%20the%20relevant%20sections%20in%20a%20privacy%20policy%20with%20corresponding%0Aregulatory%20rules.%20This%20information%20about%20individual%20privacy%20policies%20is%0Apopulated%20into%20the%20PrivComp-KG.%20Combining%20this%20with%20the%20domain%20context%20and%0Arules%2C%20the%20PrivComp-KG%20can%20be%20queried%20to%20check%20for%20compliance%20with%20privacy%0Apolicies%20by%20each%20vendor%20against%20relevant%20policy%20regulations.%20We%20demonstrate%20the%0Arelevance%20of%20the%20PrivComp-KG%2C%20by%20verifying%20compliance%20of%20privacy%20policy%0Adocuments%20for%20various%20organizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19744v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrivComp-KG%20%3A%20Leveraging%20Knowledge%20Graph%20and%20Large%20Language%20Models%20for%0A%20%20Privacy%20Policy%20Compliance%20Verification&entry.906535625=Leon%20Garza%20and%20Lavanya%20Elluri%20and%20Anantaa%20Kotal%20and%20Aritran%20Piplai%20and%20Deepti%20Gupta%20and%20Anupam%20Joshi&entry.1292438233=%20%20Data%20protection%20and%20privacy%20is%20becoming%20increasingly%20crucial%20in%20the%20digital%0Aera.%20Numerous%20companies%20depend%20on%20third-party%20vendors%20and%20service%20providers%20to%0Acarry%20out%20critical%20functions%20within%20their%20operations%2C%20encompassing%20tasks%20such%0Aas%20data%20handling%20and%20storage.%20However%2C%20this%20reliance%20introduces%20potential%0Avulnerabilities%2C%20as%20these%20vendors%27%20security%20measures%20and%20practices%20may%20not%0Aalways%20align%20with%20the%20standards%20expected%20by%20regulatory%20bodies.%20Businesses%20are%0Arequired%2C%20often%20under%20the%20penalty%20of%20law%2C%20to%20ensure%20compliance%20with%20the%0Aevolving%20regulatory%20rules.%20Interpreting%20and%20implementing%20these%20regulations%20pose%0Achallenges%20due%20to%20their%20complexity.%20Regulatory%20documents%20are%20extensive%2C%0Ademanding%20significant%20effort%20for%20interpretation%2C%20while%20vendor-drafted%20privacy%0Apolicies%20often%20lack%20the%20detail%20required%20for%20full%20legal%20compliance%2C%20leading%20to%0Aambiguity.%20To%20ensure%20a%20concise%20interpretation%20of%20the%20regulatory%20requirements%0Aand%20compliance%20of%20organizational%20privacy%20policy%20with%20said%20regulations%2C%20we%0Apropose%20a%20Large%20Language%20Model%20%28LLM%29%20and%20Semantic%20Web%20based%20approach%20for%0Aprivacy%20compliance.%20In%20this%20paper%2C%20we%20develop%20the%20novel%20Privacy%20Policy%0ACompliance%20Verification%20Knowledge%20Graph%2C%20PrivComp-KG.%20It%20is%20designed%20to%0Aefficiently%20store%20and%20retrieve%20comprehensive%20information%20concerning%20privacy%0Apolicies%2C%20regulatory%20frameworks%2C%20and%20domain-specific%20knowledge%20pertaining%20to%0Athe%20legal%20landscape%20of%20privacy.%20Using%20Retrieval%20Augmented%20Generation%2C%20we%0Aidentify%20the%20relevant%20sections%20in%20a%20privacy%20policy%20with%20corresponding%0Aregulatory%20rules.%20This%20information%20about%20individual%20privacy%20policies%20is%0Apopulated%20into%20the%20PrivComp-KG.%20Combining%20this%20with%20the%20domain%20context%20and%0Arules%2C%20the%20PrivComp-KG%20can%20be%20queried%20to%20check%20for%20compliance%20with%20privacy%0Apolicies%20by%20each%20vendor%20against%20relevant%20policy%20regulations.%20We%20demonstrate%20the%0Arelevance%20of%20the%20PrivComp-KG%2C%20by%20verifying%20compliance%20of%20privacy%20policy%0Adocuments%20for%20various%20organizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19744v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


