<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240416.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization\n  in Unseen Environments", "author": "Niklas Gard and Anna Hilsmann and Peter Eisert", "abstract": "  In this paper, we present SPVLoc, a global indoor localization method that\naccurately determines the six-dimensional (6D) camera pose of a query image and\nrequires minimal scene-specific prior knowledge and no scene-specific training.\nOur approach employs a novel matching procedure to localize the perspective\ncamera's viewport, given as an RGB image, within a set of panoramic semantic\nlayout representations of the indoor environment. The panoramas are rendered\nfrom an untextured 3D reference model, which only comprises approximate\nstructural information about room shapes, along with door and window\nannotations. We demonstrate that a straightforward convolutional network\nstructure can successfully achieve image-to-panorama and ultimately\nimage-to-model matching. Through a viewport classification score, we rank\nreference panoramas and select the best match for the query image. Then, a 6D\nrelative pose is estimated between the chosen panorama and query image. Our\nexperiments demonstrate that this approach not only efficiently bridges the\ndomain gap but also generalizes well to previously unseen scenes that are not\npart of the training data. Moreover, it achieves superior localization accuracy\ncompared to the state of the art methods and also estimates more degrees of\nfreedom of the camera pose. We will make our source code publicly available at\nhttps://github.com/fraunhoferhhi/spvloc .\n", "link": "http://arxiv.org/abs/2404.10527v1", "date": "2024-04-16", "relevancy": 3.0018, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6491}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6181}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments&body=Title%3A%20SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments%0AAuthor%3A%20Niklas%20Gard%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20SPVLoc%2C%20a%20global%20indoor%20localization%20method%20that%0Aaccurately%20determines%20the%20six-dimensional%20%286D%29%20camera%20pose%20of%20a%20query%20image%20and%0Arequires%20minimal%20scene-specific%20prior%20knowledge%20and%20no%20scene-specific%20training.%0AOur%20approach%20employs%20a%20novel%20matching%20procedure%20to%20localize%20the%20perspective%0Acamera%27s%20viewport%2C%20given%20as%20an%20RGB%20image%2C%20within%20a%20set%20of%20panoramic%20semantic%0Alayout%20representations%20of%20the%20indoor%20environment.%20The%20panoramas%20are%20rendered%0Afrom%20an%20untextured%203D%20reference%20model%2C%20which%20only%20comprises%20approximate%0Astructural%20information%20about%20room%20shapes%2C%20along%20with%20door%20and%20window%0Aannotations.%20We%20demonstrate%20that%20a%20straightforward%20convolutional%20network%0Astructure%20can%20successfully%20achieve%20image-to-panorama%20and%20ultimately%0Aimage-to-model%20matching.%20Through%20a%20viewport%20classification%20score%2C%20we%20rank%0Areference%20panoramas%20and%20select%20the%20best%20match%20for%20the%20query%20image.%20Then%2C%20a%206D%0Arelative%20pose%20is%20estimated%20between%20the%20chosen%20panorama%20and%20query%20image.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20not%20only%20efficiently%20bridges%20the%0Adomain%20gap%20but%20also%20generalizes%20well%20to%20previously%20unseen%20scenes%20that%20are%20not%0Apart%20of%20the%20training%20data.%20Moreover%2C%20it%20achieves%20superior%20localization%20accuracy%0Acompared%20to%20the%20state%20of%20the%20art%20methods%20and%20also%20estimates%20more%20degrees%20of%0Afreedom%20of%20the%20camera%20pose.%20We%20will%20make%20our%20source%20code%20publicly%20available%20at%0Ahttps%3A//github.com/fraunhoferhhi/spvloc%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10527v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPVLoc%3A%20Semantic%20Panoramic%20Viewport%20Matching%20for%206D%20Camera%20Localization%0A%20%20in%20Unseen%20Environments&entry.906535625=Niklas%20Gard%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20SPVLoc%2C%20a%20global%20indoor%20localization%20method%20that%0Aaccurately%20determines%20the%20six-dimensional%20%286D%29%20camera%20pose%20of%20a%20query%20image%20and%0Arequires%20minimal%20scene-specific%20prior%20knowledge%20and%20no%20scene-specific%20training.%0AOur%20approach%20employs%20a%20novel%20matching%20procedure%20to%20localize%20the%20perspective%0Acamera%27s%20viewport%2C%20given%20as%20an%20RGB%20image%2C%20within%20a%20set%20of%20panoramic%20semantic%0Alayout%20representations%20of%20the%20indoor%20environment.%20The%20panoramas%20are%20rendered%0Afrom%20an%20untextured%203D%20reference%20model%2C%20which%20only%20comprises%20approximate%0Astructural%20information%20about%20room%20shapes%2C%20along%20with%20door%20and%20window%0Aannotations.%20We%20demonstrate%20that%20a%20straightforward%20convolutional%20network%0Astructure%20can%20successfully%20achieve%20image-to-panorama%20and%20ultimately%0Aimage-to-model%20matching.%20Through%20a%20viewport%20classification%20score%2C%20we%20rank%0Areference%20panoramas%20and%20select%20the%20best%20match%20for%20the%20query%20image.%20Then%2C%20a%206D%0Arelative%20pose%20is%20estimated%20between%20the%20chosen%20panorama%20and%20query%20image.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20not%20only%20efficiently%20bridges%20the%0Adomain%20gap%20but%20also%20generalizes%20well%20to%20previously%20unseen%20scenes%20that%20are%20not%0Apart%20of%20the%20training%20data.%20Moreover%2C%20it%20achieves%20superior%20localization%20accuracy%0Acompared%20to%20the%20state%20of%20the%20art%20methods%20and%20also%20estimates%20more%20degrees%20of%0Afreedom%20of%20the%20camera%20pose.%20We%20will%20make%20our%20source%20code%20publicly%20available%20at%0Ahttps%3A//github.com/fraunhoferhhi/spvloc%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10527v1&entry.124074799=Read"},
{"title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM", "author": "Nikhil Keetha and Jay Karhade and Krishna Murthy Jatavallabhula and Gengshan Yang and Sebastian Scherer and Deva Ramanan and Jonathon Luiten", "abstract": "  Dense simultaneous localization and mapping (SLAM) is crucial for robotics\nand augmented reality applications. However, current methods are often hampered\nby the non-volumetric or implicit way they represent a scene. This work\nintroduces SplaTAM, an approach that, for the first time, leverages explicit\nvolumetric representations, i.e., 3D Gaussians, to enable high-fidelity\nreconstruction from a single unposed RGB-D camera, surpassing the capabilities\nof existing methods. SplaTAM employs a simple online tracking and mapping\nsystem tailored to the underlying Gaussian representation. It utilizes a\nsilhouette mask to elegantly capture the presence of scene density. This\ncombination enables several benefits over prior representations, including fast\nrendering and dense optimization, quickly determining if areas have been\npreviously mapped, and structured map expansion by adding more Gaussians.\nExtensive experiments show that SplaTAM achieves up to 2x superior performance\nin camera pose estimation, map construction, and novel-view synthesis over\nexisting methods, paving the way for more immersive high-fidelity SLAM\napplications.\n", "link": "http://arxiv.org/abs/2312.02126v3", "date": "2024-04-16", "relevancy": 2.976, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6095}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SplaTAM%3A%20Splat%2C%20Track%20%26%20Map%203D%20Gaussians%20for%20Dense%20RGB-D%20SLAM&body=Title%3A%20SplaTAM%3A%20Splat%2C%20Track%20%26%20Map%203D%20Gaussians%20for%20Dense%20RGB-D%20SLAM%0AAuthor%3A%20Nikhil%20Keetha%20and%20Jay%20Karhade%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Gengshan%20Yang%20and%20Sebastian%20Scherer%20and%20Deva%20Ramanan%20and%20Jonathon%20Luiten%0AAbstract%3A%20%20%20Dense%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20crucial%20for%20robotics%0Aand%20augmented%20reality%20applications.%20However%2C%20current%20methods%20are%20often%20hampered%0Aby%20the%20non-volumetric%20or%20implicit%20way%20they%20represent%20a%20scene.%20This%20work%0Aintroduces%20SplaTAM%2C%20an%20approach%20that%2C%20for%20the%20first%20time%2C%20leverages%20explicit%0Avolumetric%20representations%2C%20i.e.%2C%203D%20Gaussians%2C%20to%20enable%20high-fidelity%0Areconstruction%20from%20a%20single%20unposed%20RGB-D%20camera%2C%20surpassing%20the%20capabilities%0Aof%20existing%20methods.%20SplaTAM%20employs%20a%20simple%20online%20tracking%20and%20mapping%0Asystem%20tailored%20to%20the%20underlying%20Gaussian%20representation.%20It%20utilizes%20a%0Asilhouette%20mask%20to%20elegantly%20capture%20the%20presence%20of%20scene%20density.%20This%0Acombination%20enables%20several%20benefits%20over%20prior%20representations%2C%20including%20fast%0Arendering%20and%20dense%20optimization%2C%20quickly%20determining%20if%20areas%20have%20been%0Apreviously%20mapped%2C%20and%20structured%20map%20expansion%20by%20adding%20more%20Gaussians.%0AExtensive%20experiments%20show%20that%20SplaTAM%20achieves%20up%20to%202x%20superior%20performance%0Ain%20camera%20pose%20estimation%2C%20map%20construction%2C%20and%20novel-view%20synthesis%20over%0Aexisting%20methods%2C%20paving%20the%20way%20for%20more%20immersive%20high-fidelity%20SLAM%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02126v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplaTAM%3A%20Splat%2C%20Track%20%26%20Map%203D%20Gaussians%20for%20Dense%20RGB-D%20SLAM&entry.906535625=Nikhil%20Keetha%20and%20Jay%20Karhade%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Gengshan%20Yang%20and%20Sebastian%20Scherer%20and%20Deva%20Ramanan%20and%20Jonathon%20Luiten&entry.1292438233=%20%20Dense%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20crucial%20for%20robotics%0Aand%20augmented%20reality%20applications.%20However%2C%20current%20methods%20are%20often%20hampered%0Aby%20the%20non-volumetric%20or%20implicit%20way%20they%20represent%20a%20scene.%20This%20work%0Aintroduces%20SplaTAM%2C%20an%20approach%20that%2C%20for%20the%20first%20time%2C%20leverages%20explicit%0Avolumetric%20representations%2C%20i.e.%2C%203D%20Gaussians%2C%20to%20enable%20high-fidelity%0Areconstruction%20from%20a%20single%20unposed%20RGB-D%20camera%2C%20surpassing%20the%20capabilities%0Aof%20existing%20methods.%20SplaTAM%20employs%20a%20simple%20online%20tracking%20and%20mapping%0Asystem%20tailored%20to%20the%20underlying%20Gaussian%20representation.%20It%20utilizes%20a%0Asilhouette%20mask%20to%20elegantly%20capture%20the%20presence%20of%20scene%20density.%20This%0Acombination%20enables%20several%20benefits%20over%20prior%20representations%2C%20including%20fast%0Arendering%20and%20dense%20optimization%2C%20quickly%20determining%20if%20areas%20have%20been%0Apreviously%20mapped%2C%20and%20structured%20map%20expansion%20by%20adding%20more%20Gaussians.%0AExtensive%20experiments%20show%20that%20SplaTAM%20achieves%20up%20to%202x%20superior%20performance%0Ain%20camera%20pose%20estimation%2C%20map%20construction%2C%20and%20novel-view%20synthesis%20over%0Aexisting%20methods%2C%20paving%20the%20way%20for%20more%20immersive%20high-fidelity%20SLAM%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02126v3&entry.124074799=Read"},
{"title": "Leveraging Image Matching Toward End-to-End Relative Camera Pose\n  Regression", "author": "Fadi Khatib and Yuval Margalit and Meirav Galun and Ronen Basri", "abstract": "  This paper proposes a generalizable, end-to-end deep learning-based method\nfor relative pose regression between two images. Given two images of the same\nscene captured from different viewpoints, our method predicts the relative\nrotation and translation (including direction and scale) between the two\nrespective cameras. Inspired by the classical pipeline, our method leverages\nImage Matching (IM) as a pre-trained task for relative pose regression.\nSpecifically, we use LoFTR, an architecture that utilizes an attention-based\nnetwork pre-trained on Scannet, to extract semi-dense feature maps, which are\nthen warped and fed into a pose regression network. Notably, we use a loss\nfunction that utilizes separate terms to account for the translation direction\nand scale. We believe such a separation is important because translation\ndirection is determined by point correspondences while the scale is inferred\nfrom prior on shape sizes. Our ablations further support this choice. We\nevaluate our method on several datasets and show that it outperforms previous\nend-to-end methods. The method also generalizes well to unseen datasets.\n", "link": "http://arxiv.org/abs/2211.14950v2", "date": "2024-04-16", "relevancy": 2.9728, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Image%20Matching%20Toward%20End-to-End%20Relative%20Camera%20Pose%0A%20%20Regression&body=Title%3A%20Leveraging%20Image%20Matching%20Toward%20End-to-End%20Relative%20Camera%20Pose%0A%20%20Regression%0AAuthor%3A%20Fadi%20Khatib%20and%20Yuval%20Margalit%20and%20Meirav%20Galun%20and%20Ronen%20Basri%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20generalizable%2C%20end-to-end%20deep%20learning-based%20method%0Afor%20relative%20pose%20regression%20between%20two%20images.%20Given%20two%20images%20of%20the%20same%0Ascene%20captured%20from%20different%20viewpoints%2C%20our%20method%20predicts%20the%20relative%0Arotation%20and%20translation%20%28including%20direction%20and%20scale%29%20between%20the%20two%0Arespective%20cameras.%20Inspired%20by%20the%20classical%20pipeline%2C%20our%20method%20leverages%0AImage%20Matching%20%28IM%29%20as%20a%20pre-trained%20task%20for%20relative%20pose%20regression.%0ASpecifically%2C%20we%20use%20LoFTR%2C%20an%20architecture%20that%20utilizes%20an%20attention-based%0Anetwork%20pre-trained%20on%20Scannet%2C%20to%20extract%20semi-dense%20feature%20maps%2C%20which%20are%0Athen%20warped%20and%20fed%20into%20a%20pose%20regression%20network.%20Notably%2C%20we%20use%20a%20loss%0Afunction%20that%20utilizes%20separate%20terms%20to%20account%20for%20the%20translation%20direction%0Aand%20scale.%20We%20believe%20such%20a%20separation%20is%20important%20because%20translation%0Adirection%20is%20determined%20by%20point%20correspondences%20while%20the%20scale%20is%20inferred%0Afrom%20prior%20on%20shape%20sizes.%20Our%20ablations%20further%20support%20this%20choice.%20We%0Aevaluate%20our%20method%20on%20several%20datasets%20and%20show%20that%20it%20outperforms%20previous%0Aend-to-end%20methods.%20The%20method%20also%20generalizes%20well%20to%20unseen%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14950v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Image%20Matching%20Toward%20End-to-End%20Relative%20Camera%20Pose%0A%20%20Regression&entry.906535625=Fadi%20Khatib%20and%20Yuval%20Margalit%20and%20Meirav%20Galun%20and%20Ronen%20Basri&entry.1292438233=%20%20This%20paper%20proposes%20a%20generalizable%2C%20end-to-end%20deep%20learning-based%20method%0Afor%20relative%20pose%20regression%20between%20two%20images.%20Given%20two%20images%20of%20the%20same%0Ascene%20captured%20from%20different%20viewpoints%2C%20our%20method%20predicts%20the%20relative%0Arotation%20and%20translation%20%28including%20direction%20and%20scale%29%20between%20the%20two%0Arespective%20cameras.%20Inspired%20by%20the%20classical%20pipeline%2C%20our%20method%20leverages%0AImage%20Matching%20%28IM%29%20as%20a%20pre-trained%20task%20for%20relative%20pose%20regression.%0ASpecifically%2C%20we%20use%20LoFTR%2C%20an%20architecture%20that%20utilizes%20an%20attention-based%0Anetwork%20pre-trained%20on%20Scannet%2C%20to%20extract%20semi-dense%20feature%20maps%2C%20which%20are%0Athen%20warped%20and%20fed%20into%20a%20pose%20regression%20network.%20Notably%2C%20we%20use%20a%20loss%0Afunction%20that%20utilizes%20separate%20terms%20to%20account%20for%20the%20translation%20direction%0Aand%20scale.%20We%20believe%20such%20a%20separation%20is%20important%20because%20translation%0Adirection%20is%20determined%20by%20point%20correspondences%20while%20the%20scale%20is%20inferred%0Afrom%20prior%20on%20shape%20sizes.%20Our%20ablations%20further%20support%20this%20choice.%20We%0Aevaluate%20our%20method%20on%20several%20datasets%20and%20show%20that%20it%20outperforms%20previous%0Aend-to-end%20methods.%20The%20method%20also%20generalizes%20well%20to%20unseen%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14950v2&entry.124074799=Read"},
{"title": "RemoteCLIP: A Vision Language Foundation Model for Remote Sensing", "author": "Fan Liu and Delong Chen and Zhangqingyun Guan and Xiaocong Zhou and Jiale Zhu and Qiaolin Ye and Liyong Fu and Jun Zhou", "abstract": "  General-purpose foundation models have led to recent breakthroughs in\nartificial intelligence. In remote sensing, self-supervised learning (SSL) and\nMasked Image Modeling (MIM) have been adopted to build foundation models.\nHowever, these models primarily learn low-level features and require annotated\ndata for fine-tuning. Moreover, they are inapplicable for retrieval and\nzero-shot applications due to the lack of language understanding. To address\nthese limitations, we propose RemoteCLIP, the first vision-language foundation\nmodel for remote sensing that aims to learn robust visual features with rich\nsemantics and aligned text embeddings for seamless downstream application. To\naddress the scarcity of pre-training data, we leverage data scaling which\nconverts heterogeneous annotations into a unified image-caption data format\nbased on Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion. By further\nincorporating UAV imagery, we produce a 12 $\\times$ larger pretraining dataset\nthan the combination of all available datasets. RemoteCLIP can be applied to a\nvariety of downstream tasks, including zero-shot image classification, linear\nprobing, $\\textit{k}$-NN classification, few-shot classification, image-text\nretrieval, and object counting in remote sensing images. Evaluation on 16\ndatasets, including a newly introduced RemoteCount benchmark to test the object\ncounting ability, shows that RemoteCLIP consistently outperforms baseline\nfoundation models across different model scales. Impressively, RemoteCLIP beats\nthe state-of-the-art method by 9.14% mean recall on the RSITMD dataset and\n8.92% on the RSICD dataset. For zero-shot classification, our RemoteCLIP\noutperforms the CLIP baseline by up to 6.39% average accuracy on 12 downstream\ndatasets. Project website: https://github.com/ChenDelong1999/RemoteCLIP\n", "link": "http://arxiv.org/abs/2306.11029v4", "date": "2024-04-16", "relevancy": 2.8078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5401}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RemoteCLIP%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Remote%20Sensing&body=Title%3A%20RemoteCLIP%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Remote%20Sensing%0AAuthor%3A%20Fan%20Liu%20and%20Delong%20Chen%20and%20Zhangqingyun%20Guan%20and%20Xiaocong%20Zhou%20and%20Jiale%20Zhu%20and%20Qiaolin%20Ye%20and%20Liyong%20Fu%20and%20Jun%20Zhou%0AAbstract%3A%20%20%20General-purpose%20foundation%20models%20have%20led%20to%20recent%20breakthroughs%20in%0Aartificial%20intelligence.%20In%20remote%20sensing%2C%20self-supervised%20learning%20%28SSL%29%20and%0AMasked%20Image%20Modeling%20%28MIM%29%20have%20been%20adopted%20to%20build%20foundation%20models.%0AHowever%2C%20these%20models%20primarily%20learn%20low-level%20features%20and%20require%20annotated%0Adata%20for%20fine-tuning.%20Moreover%2C%20they%20are%20inapplicable%20for%20retrieval%20and%0Azero-shot%20applications%20due%20to%20the%20lack%20of%20language%20understanding.%20To%20address%0Athese%20limitations%2C%20we%20propose%20RemoteCLIP%2C%20the%20first%20vision-language%20foundation%0Amodel%20for%20remote%20sensing%20that%20aims%20to%20learn%20robust%20visual%20features%20with%20rich%0Asemantics%20and%20aligned%20text%20embeddings%20for%20seamless%20downstream%20application.%20To%0Aaddress%20the%20scarcity%20of%20pre-training%20data%2C%20we%20leverage%20data%20scaling%20which%0Aconverts%20heterogeneous%20annotations%20into%20a%20unified%20image-caption%20data%20format%0Abased%20on%20Box-to-Caption%20%28B2C%29%20and%20Mask-to-Box%20%28M2B%29%20conversion.%20By%20further%0Aincorporating%20UAV%20imagery%2C%20we%20produce%20a%2012%20%24%5Ctimes%24%20larger%20pretraining%20dataset%0Athan%20the%20combination%20of%20all%20available%20datasets.%20RemoteCLIP%20can%20be%20applied%20to%20a%0Avariety%20of%20downstream%20tasks%2C%20including%20zero-shot%20image%20classification%2C%20linear%0Aprobing%2C%20%24%5Ctextit%7Bk%7D%24-NN%20classification%2C%20few-shot%20classification%2C%20image-text%0Aretrieval%2C%20and%20object%20counting%20in%20remote%20sensing%20images.%20Evaluation%20on%2016%0Adatasets%2C%20including%20a%20newly%20introduced%20RemoteCount%20benchmark%20to%20test%20the%20object%0Acounting%20ability%2C%20shows%20that%20RemoteCLIP%20consistently%20outperforms%20baseline%0Afoundation%20models%20across%20different%20model%20scales.%20Impressively%2C%20RemoteCLIP%20beats%0Athe%20state-of-the-art%20method%20by%209.14%25%20mean%20recall%20on%20the%20RSITMD%20dataset%20and%0A8.92%25%20on%20the%20RSICD%20dataset.%20For%20zero-shot%20classification%2C%20our%20RemoteCLIP%0Aoutperforms%20the%20CLIP%20baseline%20by%20up%20to%206.39%25%20average%20accuracy%20on%2012%20downstream%0Adatasets.%20Project%20website%3A%20https%3A//github.com/ChenDelong1999/RemoteCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11029v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RemoteCLIP%3A%20A%20Vision%20Language%20Foundation%20Model%20for%20Remote%20Sensing&entry.906535625=Fan%20Liu%20and%20Delong%20Chen%20and%20Zhangqingyun%20Guan%20and%20Xiaocong%20Zhou%20and%20Jiale%20Zhu%20and%20Qiaolin%20Ye%20and%20Liyong%20Fu%20and%20Jun%20Zhou&entry.1292438233=%20%20General-purpose%20foundation%20models%20have%20led%20to%20recent%20breakthroughs%20in%0Aartificial%20intelligence.%20In%20remote%20sensing%2C%20self-supervised%20learning%20%28SSL%29%20and%0AMasked%20Image%20Modeling%20%28MIM%29%20have%20been%20adopted%20to%20build%20foundation%20models.%0AHowever%2C%20these%20models%20primarily%20learn%20low-level%20features%20and%20require%20annotated%0Adata%20for%20fine-tuning.%20Moreover%2C%20they%20are%20inapplicable%20for%20retrieval%20and%0Azero-shot%20applications%20due%20to%20the%20lack%20of%20language%20understanding.%20To%20address%0Athese%20limitations%2C%20we%20propose%20RemoteCLIP%2C%20the%20first%20vision-language%20foundation%0Amodel%20for%20remote%20sensing%20that%20aims%20to%20learn%20robust%20visual%20features%20with%20rich%0Asemantics%20and%20aligned%20text%20embeddings%20for%20seamless%20downstream%20application.%20To%0Aaddress%20the%20scarcity%20of%20pre-training%20data%2C%20we%20leverage%20data%20scaling%20which%0Aconverts%20heterogeneous%20annotations%20into%20a%20unified%20image-caption%20data%20format%0Abased%20on%20Box-to-Caption%20%28B2C%29%20and%20Mask-to-Box%20%28M2B%29%20conversion.%20By%20further%0Aincorporating%20UAV%20imagery%2C%20we%20produce%20a%2012%20%24%5Ctimes%24%20larger%20pretraining%20dataset%0Athan%20the%20combination%20of%20all%20available%20datasets.%20RemoteCLIP%20can%20be%20applied%20to%20a%0Avariety%20of%20downstream%20tasks%2C%20including%20zero-shot%20image%20classification%2C%20linear%0Aprobing%2C%20%24%5Ctextit%7Bk%7D%24-NN%20classification%2C%20few-shot%20classification%2C%20image-text%0Aretrieval%2C%20and%20object%20counting%20in%20remote%20sensing%20images.%20Evaluation%20on%2016%0Adatasets%2C%20including%20a%20newly%20introduced%20RemoteCount%20benchmark%20to%20test%20the%20object%0Acounting%20ability%2C%20shows%20that%20RemoteCLIP%20consistently%20outperforms%20baseline%0Afoundation%20models%20across%20different%20model%20scales.%20Impressively%2C%20RemoteCLIP%20beats%0Athe%20state-of-the-art%20method%20by%209.14%25%20mean%20recall%20on%20the%20RSITMD%20dataset%20and%0A8.92%25%20on%20the%20RSICD%20dataset.%20For%20zero-shot%20classification%2C%20our%20RemoteCLIP%0Aoutperforms%20the%20CLIP%20baseline%20by%20up%20to%206.39%25%20average%20accuracy%20on%2012%20downstream%0Adatasets.%20Project%20website%3A%20https%3A//github.com/ChenDelong1999/RemoteCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11029v4&entry.124074799=Read"},
{"title": "VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification", "author": "Baolu Li and Ping Liu and Lan Fu and Jinlong Li and Jianwu Fang and Zhigang Xu and Hongkai Yu", "abstract": "  Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.\n", "link": "http://arxiv.org/abs/2311.16278v2", "date": "2024-04-16", "relevancy": 2.7626, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5711}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5506}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification&body=Title%3A%20VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification%0AAuthor%3A%20Baolu%20Li%20and%20Ping%20Liu%20and%20Lan%20Fu%20and%20Jinlong%20Li%20and%20Jianwu%20Fang%20and%20Zhigang%20Xu%20and%20Hongkai%20Yu%0AAbstract%3A%20%20%20Vehicle%20Re-identification%20%28Re-ID%29%20has%20been%20broadly%20studied%20in%20the%20last%0Adecade%3B%20however%2C%20the%20different%20camera%20view%20angle%20leading%20to%20confused%0Adiscrimination%20in%20the%20feature%20subspace%20for%20the%20vehicles%20of%20various%20poses%2C%20is%0Astill%20challenging%20for%20the%20Vehicle%20Re-ID%20models%20in%20the%20real%20world.%20To%20promote%0Athe%20Vehicle%20Re-ID%20models%2C%20this%20paper%20proposes%20to%20synthesize%20a%20large%20number%20of%0Avehicle%20images%20in%20the%20target%20pose%2C%20whose%20idea%20is%20to%20project%20the%20vehicles%20of%0Adiverse%20poses%20into%20the%20unified%20target%20pose%20so%20as%20to%20enhance%20feature%0Adiscrimination.%20Considering%20that%20the%20paired%20data%20of%20the%20same%20vehicles%20in%0Adifferent%20traffic%20surveillance%20cameras%20might%20be%20not%20available%20in%20the%20real%0Aworld%2C%20we%20propose%20the%20first%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20method%0Afor%20Vehicle%20Re-ID%2C%20named%20as%20VehicleGAN%20in%20this%20paper%2C%20which%20works%20for%20both%0Asupervised%20and%20unsupervised%20settings%20without%20the%20knowledge%20of%20geometric%203D%0Amodels.%20Because%20of%20the%20feature%20distribution%20difference%20between%20real%20and%0Asynthetic%20data%2C%20simply%20training%20a%20traditional%20metric%20learning%20based%20Re-ID%20model%0Awith%20data-level%20fusion%20%28i.e.%2C%20data%20augmentation%29%20is%20not%20satisfactory%2C%20therefore%0Awe%20propose%20a%20new%20Joint%20Metric%20Learning%20%28JML%29%20via%20effective%20feature-level%20fusion%0Afrom%20both%20real%20and%20synthetic%20data.%20Intensive%20experimental%20results%20on%20the%20public%0AVeRi-776%20and%20VehicleID%20datasets%20prove%20the%20accuracy%20and%20effectiveness%20of%20our%0Aproposed%20VehicleGAN%20and%20JML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16278v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification&entry.906535625=Baolu%20Li%20and%20Ping%20Liu%20and%20Lan%20Fu%20and%20Jinlong%20Li%20and%20Jianwu%20Fang%20and%20Zhigang%20Xu%20and%20Hongkai%20Yu&entry.1292438233=%20%20Vehicle%20Re-identification%20%28Re-ID%29%20has%20been%20broadly%20studied%20in%20the%20last%0Adecade%3B%20however%2C%20the%20different%20camera%20view%20angle%20leading%20to%20confused%0Adiscrimination%20in%20the%20feature%20subspace%20for%20the%20vehicles%20of%20various%20poses%2C%20is%0Astill%20challenging%20for%20the%20Vehicle%20Re-ID%20models%20in%20the%20real%20world.%20To%20promote%0Athe%20Vehicle%20Re-ID%20models%2C%20this%20paper%20proposes%20to%20synthesize%20a%20large%20number%20of%0Avehicle%20images%20in%20the%20target%20pose%2C%20whose%20idea%20is%20to%20project%20the%20vehicles%20of%0Adiverse%20poses%20into%20the%20unified%20target%20pose%20so%20as%20to%20enhance%20feature%0Adiscrimination.%20Considering%20that%20the%20paired%20data%20of%20the%20same%20vehicles%20in%0Adifferent%20traffic%20surveillance%20cameras%20might%20be%20not%20available%20in%20the%20real%0Aworld%2C%20we%20propose%20the%20first%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20method%0Afor%20Vehicle%20Re-ID%2C%20named%20as%20VehicleGAN%20in%20this%20paper%2C%20which%20works%20for%20both%0Asupervised%20and%20unsupervised%20settings%20without%20the%20knowledge%20of%20geometric%203D%0Amodels.%20Because%20of%20the%20feature%20distribution%20difference%20between%20real%20and%0Asynthetic%20data%2C%20simply%20training%20a%20traditional%20metric%20learning%20based%20Re-ID%20model%0Awith%20data-level%20fusion%20%28i.e.%2C%20data%20augmentation%29%20is%20not%20satisfactory%2C%20therefore%0Awe%20propose%20a%20new%20Joint%20Metric%20Learning%20%28JML%29%20via%20effective%20feature-level%20fusion%0Afrom%20both%20real%20and%20synthetic%20data.%20Intensive%20experimental%20results%20on%20the%20public%0AVeRi-776%20and%20VehicleID%20datasets%20prove%20the%20accuracy%20and%20effectiveness%20of%20our%0Aproposed%20VehicleGAN%20and%20JML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16278v2&entry.124074799=Read"},
{"title": "Exploring selective image matching methods for zero-shot and few-sample\n  unsupervised domain adaptation of urban canopy prediction", "author": "John Francis and Stephen Law", "abstract": "  We explore simple methods for adapting a trained multi-task UNet which\npredicts canopy cover and height to a new geographic setting using remotely\nsensed data without the need of training a domain-adaptive classifier and\nextensive fine-tuning. Extending previous research, we followed a selective\nalignment process to identify similar images in the two geographical domains\nand then tested an array of data-based unsupervised domain adaptation\napproaches in a zero-shot setting as well as with a small amount of\nfine-tuning. We find that the selective aligned data-based image matching\nmethods produce promising results in a zero-shot setting, and even more so with\na small amount of fine-tuning. These methods outperform both an untransformed\nbaseline and a popular data-based image-to-image translation model. The best\nperforming methods were pixel distribution adaptation and fourier domain\nadaptation on the canopy cover and height tasks respectively.\n", "link": "http://arxiv.org/abs/2404.10626v1", "date": "2024-04-16", "relevancy": 2.7441, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20selective%20image%20matching%20methods%20for%20zero-shot%20and%20few-sample%0A%20%20unsupervised%20domain%20adaptation%20of%20urban%20canopy%20prediction&body=Title%3A%20Exploring%20selective%20image%20matching%20methods%20for%20zero-shot%20and%20few-sample%0A%20%20unsupervised%20domain%20adaptation%20of%20urban%20canopy%20prediction%0AAuthor%3A%20John%20Francis%20and%20Stephen%20Law%0AAbstract%3A%20%20%20We%20explore%20simple%20methods%20for%20adapting%20a%20trained%20multi-task%20UNet%20which%0Apredicts%20canopy%20cover%20and%20height%20to%20a%20new%20geographic%20setting%20using%20remotely%0Asensed%20data%20without%20the%20need%20of%20training%20a%20domain-adaptive%20classifier%20and%0Aextensive%20fine-tuning.%20Extending%20previous%20research%2C%20we%20followed%20a%20selective%0Aalignment%20process%20to%20identify%20similar%20images%20in%20the%20two%20geographical%20domains%0Aand%20then%20tested%20an%20array%20of%20data-based%20unsupervised%20domain%20adaptation%0Aapproaches%20in%20a%20zero-shot%20setting%20as%20well%20as%20with%20a%20small%20amount%20of%0Afine-tuning.%20We%20find%20that%20the%20selective%20aligned%20data-based%20image%20matching%0Amethods%20produce%20promising%20results%20in%20a%20zero-shot%20setting%2C%20and%20even%20more%20so%20with%0Aa%20small%20amount%20of%20fine-tuning.%20These%20methods%20outperform%20both%20an%20untransformed%0Abaseline%20and%20a%20popular%20data-based%20image-to-image%20translation%20model.%20The%20best%0Aperforming%20methods%20were%20pixel%20distribution%20adaptation%20and%20fourier%20domain%0Aadaptation%20on%20the%20canopy%20cover%20and%20height%20tasks%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10626v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20selective%20image%20matching%20methods%20for%20zero-shot%20and%20few-sample%0A%20%20unsupervised%20domain%20adaptation%20of%20urban%20canopy%20prediction&entry.906535625=John%20Francis%20and%20Stephen%20Law&entry.1292438233=%20%20We%20explore%20simple%20methods%20for%20adapting%20a%20trained%20multi-task%20UNet%20which%0Apredicts%20canopy%20cover%20and%20height%20to%20a%20new%20geographic%20setting%20using%20remotely%0Asensed%20data%20without%20the%20need%20of%20training%20a%20domain-adaptive%20classifier%20and%0Aextensive%20fine-tuning.%20Extending%20previous%20research%2C%20we%20followed%20a%20selective%0Aalignment%20process%20to%20identify%20similar%20images%20in%20the%20two%20geographical%20domains%0Aand%20then%20tested%20an%20array%20of%20data-based%20unsupervised%20domain%20adaptation%0Aapproaches%20in%20a%20zero-shot%20setting%20as%20well%20as%20with%20a%20small%20amount%20of%0Afine-tuning.%20We%20find%20that%20the%20selective%20aligned%20data-based%20image%20matching%0Amethods%20produce%20promising%20results%20in%20a%20zero-shot%20setting%2C%20and%20even%20more%20so%20with%0Aa%20small%20amount%20of%20fine-tuning.%20These%20methods%20outperform%20both%20an%20untransformed%0Abaseline%20and%20a%20popular%20data-based%20image-to-image%20translation%20model.%20The%20best%0Aperforming%20methods%20were%20pixel%20distribution%20adaptation%20and%20fourier%20domain%0Aadaptation%20on%20the%20canopy%20cover%20and%20height%20tasks%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10626v1&entry.124074799=Read"},
{"title": "Self-Supervised Visual Preference Alignment", "author": "Ke Zhu and Liang Zhao and Zheng Ge and Xiangyu Zhang", "abstract": "  This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code will be available.\n", "link": "http://arxiv.org/abs/2404.10501v1", "date": "2024-04-16", "relevancy": 2.7289, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5719}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Visual%20Preference%20Alignment&body=Title%3A%20Self-Supervised%20Visual%20Preference%20Alignment%0AAuthor%3A%20Ke%20Zhu%20and%20Liang%20Zhao%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20This%20paper%20makes%20the%20first%20attempt%20towards%20unsupervised%20preference%20alignment%0Ain%20Vision-Language%20Models%20%28VLMs%29.%20We%20generate%20chosen%20and%20rejected%20responses%0Awith%20regard%20to%20the%20original%20and%20augmented%20image%20pairs%2C%20and%20conduct%20preference%0Aalignment%20with%20direct%20preference%20optimization.%20It%20is%20based%20on%20a%20core%20idea%3A%0Aproperly%20designed%20augmentation%20to%20the%20image%20input%20will%20induce%20VLM%20to%20generate%0Afalse%20but%20hard%20negative%20responses%2C%20which%20helps%20the%20model%20to%20learn%20from%20and%0Aproduce%20more%20robust%20and%20powerful%20answers.%20The%20whole%20pipeline%20no%20longer%20hinges%0Aon%20supervision%20from%20GPT4%20or%20human%20involvement%20during%20alignment%2C%20and%20is%20highly%0Aefficient%20with%20few%20lines%20of%20code.%20With%20only%208k%20randomly%20sampled%20unsupervised%0Adata%2C%20it%20achieves%2090%5C%25%20relative%20score%20to%20GPT-4%20on%20complex%20reasoning%20in%0ALLaVA-Bench%2C%20and%20improves%20LLaVA-7B/13B%20by%206.7%5C%25/5.6%5C%25%20score%20on%20complex%0Amulti-modal%20benchmark%20MM-Vet.%20Visualizations%20shows%20its%20improved%20ability%20to%0Aalign%20with%20user-intentions.%20A%20series%20of%20ablations%20are%20firmly%20conducted%20to%0Areveal%20the%20latent%20mechanism%20of%20the%20approach%2C%20which%20also%20indicates%20its%20potential%0Atowards%20further%20scaling.%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10501v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Visual%20Preference%20Alignment&entry.906535625=Ke%20Zhu%20and%20Liang%20Zhao%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20This%20paper%20makes%20the%20first%20attempt%20towards%20unsupervised%20preference%20alignment%0Ain%20Vision-Language%20Models%20%28VLMs%29.%20We%20generate%20chosen%20and%20rejected%20responses%0Awith%20regard%20to%20the%20original%20and%20augmented%20image%20pairs%2C%20and%20conduct%20preference%0Aalignment%20with%20direct%20preference%20optimization.%20It%20is%20based%20on%20a%20core%20idea%3A%0Aproperly%20designed%20augmentation%20to%20the%20image%20input%20will%20induce%20VLM%20to%20generate%0Afalse%20but%20hard%20negative%20responses%2C%20which%20helps%20the%20model%20to%20learn%20from%20and%0Aproduce%20more%20robust%20and%20powerful%20answers.%20The%20whole%20pipeline%20no%20longer%20hinges%0Aon%20supervision%20from%20GPT4%20or%20human%20involvement%20during%20alignment%2C%20and%20is%20highly%0Aefficient%20with%20few%20lines%20of%20code.%20With%20only%208k%20randomly%20sampled%20unsupervised%0Adata%2C%20it%20achieves%2090%5C%25%20relative%20score%20to%20GPT-4%20on%20complex%20reasoning%20in%0ALLaVA-Bench%2C%20and%20improves%20LLaVA-7B/13B%20by%206.7%5C%25/5.6%5C%25%20score%20on%20complex%0Amulti-modal%20benchmark%20MM-Vet.%20Visualizations%20shows%20its%20improved%20ability%20to%0Aalign%20with%20user-intentions.%20A%20series%20of%20ablations%20are%20firmly%20conducted%20to%0Areveal%20the%20latent%20mechanism%20of%20the%20approach%2C%20which%20also%20indicates%20its%20potential%0Atowards%20further%20scaling.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10501v1&entry.124074799=Read"},
{"title": "Continual Offline Reinforcement Learning via Diffusion-based Dual\n  Generative Replay", "author": "Jinmei Liu and Wenbin Li and Xiangyu Yue and Shilin Zhang and Chunlin Chen and Zhi Wang", "abstract": "  We study continual offline reinforcement learning, a practical paradigm that\nfacilitates forward transfer and mitigates catastrophic forgetting to tackle\nsequential offline tasks. We propose a dual generative replay framework that\nretains previous knowledge by concurrent replay of generated pseudo-data.\nFirst, we decouple the continual learning policy into a diffusion-based\ngenerative behavior model and a multi-head action evaluation model, allowing\nthe policy to inherit distributional expressivity for encompassing a\nprogressive range of diverse behaviors. Second, we train a task-conditioned\ndiffusion model to mimic state distributions of past tasks. Generated states\nare paired with corresponding responses from the behavior generator to\nrepresent old tasks with high-fidelity replayed samples. Finally, by\ninterleaving pseudo samples with real ones of the new task, we continually\nupdate the state and behavior generators to model progressively diverse\nbehaviors, and regularize the multi-head critic via behavior cloning to\nmitigate forgetting. Experiments demonstrate that our method achieves better\nforward transfer with less forgetting, and closely approximates the results of\nusing previous ground-truth data due to its high-fidelity replay of the sample\nspace. Our code is available at\n\\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}.\n", "link": "http://arxiv.org/abs/2404.10662v1", "date": "2024-04-16", "relevancy": 2.6945, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.543}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5428}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5309}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Offline%20Reinforcement%20Learning%20via%20Diffusion-based%20Dual%0A%20%20Generative%20Replay&body=Title%3A%20Continual%20Offline%20Reinforcement%20Learning%20via%20Diffusion-based%20Dual%0A%20%20Generative%20Replay%0AAuthor%3A%20Jinmei%20Liu%20and%20Wenbin%20Li%20and%20Xiangyu%20Yue%20and%20Shilin%20Zhang%20and%20Chunlin%20Chen%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20We%20study%20continual%20offline%20reinforcement%20learning%2C%20a%20practical%20paradigm%20that%0Afacilitates%20forward%20transfer%20and%20mitigates%20catastrophic%20forgetting%20to%20tackle%0Asequential%20offline%20tasks.%20We%20propose%20a%20dual%20generative%20replay%20framework%20that%0Aretains%20previous%20knowledge%20by%20concurrent%20replay%20of%20generated%20pseudo-data.%0AFirst%2C%20we%20decouple%20the%20continual%20learning%20policy%20into%20a%20diffusion-based%0Agenerative%20behavior%20model%20and%20a%20multi-head%20action%20evaluation%20model%2C%20allowing%0Athe%20policy%20to%20inherit%20distributional%20expressivity%20for%20encompassing%20a%0Aprogressive%20range%20of%20diverse%20behaviors.%20Second%2C%20we%20train%20a%20task-conditioned%0Adiffusion%20model%20to%20mimic%20state%20distributions%20of%20past%20tasks.%20Generated%20states%0Aare%20paired%20with%20corresponding%20responses%20from%20the%20behavior%20generator%20to%0Arepresent%20old%20tasks%20with%20high-fidelity%20replayed%20samples.%20Finally%2C%20by%0Ainterleaving%20pseudo%20samples%20with%20real%20ones%20of%20the%20new%20task%2C%20we%20continually%0Aupdate%20the%20state%20and%20behavior%20generators%20to%20model%20progressively%20diverse%0Abehaviors%2C%20and%20regularize%20the%20multi-head%20critic%20via%20behavior%20cloning%20to%0Amitigate%20forgetting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20better%0Aforward%20transfer%20with%20less%20forgetting%2C%20and%20closely%20approximates%20the%20results%20of%0Ausing%20previous%20ground-truth%20data%20due%20to%20its%20high-fidelity%20replay%20of%20the%20sample%0Aspace.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/NJU-RL/CuGRO%7D%7Bhttps%3A//github.com/NJU-RL/CuGRO%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Offline%20Reinforcement%20Learning%20via%20Diffusion-based%20Dual%0A%20%20Generative%20Replay&entry.906535625=Jinmei%20Liu%20and%20Wenbin%20Li%20and%20Xiangyu%20Yue%20and%20Shilin%20Zhang%20and%20Chunlin%20Chen%20and%20Zhi%20Wang&entry.1292438233=%20%20We%20study%20continual%20offline%20reinforcement%20learning%2C%20a%20practical%20paradigm%20that%0Afacilitates%20forward%20transfer%20and%20mitigates%20catastrophic%20forgetting%20to%20tackle%0Asequential%20offline%20tasks.%20We%20propose%20a%20dual%20generative%20replay%20framework%20that%0Aretains%20previous%20knowledge%20by%20concurrent%20replay%20of%20generated%20pseudo-data.%0AFirst%2C%20we%20decouple%20the%20continual%20learning%20policy%20into%20a%20diffusion-based%0Agenerative%20behavior%20model%20and%20a%20multi-head%20action%20evaluation%20model%2C%20allowing%0Athe%20policy%20to%20inherit%20distributional%20expressivity%20for%20encompassing%20a%0Aprogressive%20range%20of%20diverse%20behaviors.%20Second%2C%20we%20train%20a%20task-conditioned%0Adiffusion%20model%20to%20mimic%20state%20distributions%20of%20past%20tasks.%20Generated%20states%0Aare%20paired%20with%20corresponding%20responses%20from%20the%20behavior%20generator%20to%0Arepresent%20old%20tasks%20with%20high-fidelity%20replayed%20samples.%20Finally%2C%20by%0Ainterleaving%20pseudo%20samples%20with%20real%20ones%20of%20the%20new%20task%2C%20we%20continually%0Aupdate%20the%20state%20and%20behavior%20generators%20to%20model%20progressively%20diverse%0Abehaviors%2C%20and%20regularize%20the%20multi-head%20critic%20via%20behavior%20cloning%20to%0Amitigate%20forgetting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20better%0Aforward%20transfer%20with%20less%20forgetting%2C%20and%20closely%20approximates%20the%20results%20of%0Ausing%20previous%20ground-truth%20data%20due%20to%20its%20high-fidelity%20replay%20of%20the%20sample%0Aspace.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/NJU-RL/CuGRO%7D%7Bhttps%3A//github.com/NJU-RL/CuGRO%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10662v1&entry.124074799=Read"},
{"title": "AV-GAN: Attention-Based Varifocal Generative Adversarial Network for\n  Uneven Medical Image Translation", "author": "Zexin Li and Yiyang Lin and Zijie Fang and Shuyan Li and Xiu Li", "abstract": "  Different types of staining highlight different structures in organs, thereby\nassisting in diagnosis. However, due to the impossibility of repeated staining,\nwe cannot obtain different types of stained slides of the same tissue area.\nTranslating the slide that is easy to obtain (e.g., H&E) to slides of staining\ntypes difficult to obtain (e.g., MT, PAS) is a promising way to solve this\nproblem. However, some regions are closely connected to other regions, and to\nmaintain this connection, they often have complex structures and are difficult\nto translate, which may lead to wrong translations. In this paper, we propose\nthe Attention-Based Varifocal Generative Adversarial Network (AV-GAN), which\nsolves multiple problems in pathologic image translation tasks, such as uneven\ntranslation difficulty in different regions, mutual interference of multiple\nresolution information, and nuclear deformation. Specifically, we develop an\nAttention-Based Key Region Selection Module, which can attend to regions with\nhigher translation difficulty. We then develop a Varifocal Module to translate\nthese regions at multiple resolutions. Experimental results show that our\nproposed AV-GAN outperforms existing image translation methods with two virtual\nkidney tissue staining tasks and improves FID values by 15.9 and 4.16\nrespectively in the H&E-MT and H&E-PAS tasks.\n", "link": "http://arxiv.org/abs/2404.10714v1", "date": "2024-04-16", "relevancy": 2.661, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5417}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5414}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5135}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AV-GAN%3A%20Attention-Based%20Varifocal%20Generative%20Adversarial%20Network%20for%0A%20%20Uneven%20Medical%20Image%20Translation&body=Title%3A%20AV-GAN%3A%20Attention-Based%20Varifocal%20Generative%20Adversarial%20Network%20for%0A%20%20Uneven%20Medical%20Image%20Translation%0AAuthor%3A%20Zexin%20Li%20and%20Yiyang%20Lin%20and%20Zijie%20Fang%20and%20Shuyan%20Li%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Different%20types%20of%20staining%20highlight%20different%20structures%20in%20organs%2C%20thereby%0Aassisting%20in%20diagnosis.%20However%2C%20due%20to%20the%20impossibility%20of%20repeated%20staining%2C%0Awe%20cannot%20obtain%20different%20types%20of%20stained%20slides%20of%20the%20same%20tissue%20area.%0ATranslating%20the%20slide%20that%20is%20easy%20to%20obtain%20%28e.g.%2C%20H%26E%29%20to%20slides%20of%20staining%0Atypes%20difficult%20to%20obtain%20%28e.g.%2C%20MT%2C%20PAS%29%20is%20a%20promising%20way%20to%20solve%20this%0Aproblem.%20However%2C%20some%20regions%20are%20closely%20connected%20to%20other%20regions%2C%20and%20to%0Amaintain%20this%20connection%2C%20they%20often%20have%20complex%20structures%20and%20are%20difficult%0Ato%20translate%2C%20which%20may%20lead%20to%20wrong%20translations.%20In%20this%20paper%2C%20we%20propose%0Athe%20Attention-Based%20Varifocal%20Generative%20Adversarial%20Network%20%28AV-GAN%29%2C%20which%0Asolves%20multiple%20problems%20in%20pathologic%20image%20translation%20tasks%2C%20such%20as%20uneven%0Atranslation%20difficulty%20in%20different%20regions%2C%20mutual%20interference%20of%20multiple%0Aresolution%20information%2C%20and%20nuclear%20deformation.%20Specifically%2C%20we%20develop%20an%0AAttention-Based%20Key%20Region%20Selection%20Module%2C%20which%20can%20attend%20to%20regions%20with%0Ahigher%20translation%20difficulty.%20We%20then%20develop%20a%20Varifocal%20Module%20to%20translate%0Athese%20regions%20at%20multiple%20resolutions.%20Experimental%20results%20show%20that%20our%0Aproposed%20AV-GAN%20outperforms%20existing%20image%20translation%20methods%20with%20two%20virtual%0Akidney%20tissue%20staining%20tasks%20and%20improves%20FID%20values%20by%2015.9%20and%204.16%0Arespectively%20in%20the%20H%26E-MT%20and%20H%26E-PAS%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10714v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AV-GAN%3A%20Attention-Based%20Varifocal%20Generative%20Adversarial%20Network%20for%0A%20%20Uneven%20Medical%20Image%20Translation&entry.906535625=Zexin%20Li%20and%20Yiyang%20Lin%20and%20Zijie%20Fang%20and%20Shuyan%20Li%20and%20Xiu%20Li&entry.1292438233=%20%20Different%20types%20of%20staining%20highlight%20different%20structures%20in%20organs%2C%20thereby%0Aassisting%20in%20diagnosis.%20However%2C%20due%20to%20the%20impossibility%20of%20repeated%20staining%2C%0Awe%20cannot%20obtain%20different%20types%20of%20stained%20slides%20of%20the%20same%20tissue%20area.%0ATranslating%20the%20slide%20that%20is%20easy%20to%20obtain%20%28e.g.%2C%20H%26E%29%20to%20slides%20of%20staining%0Atypes%20difficult%20to%20obtain%20%28e.g.%2C%20MT%2C%20PAS%29%20is%20a%20promising%20way%20to%20solve%20this%0Aproblem.%20However%2C%20some%20regions%20are%20closely%20connected%20to%20other%20regions%2C%20and%20to%0Amaintain%20this%20connection%2C%20they%20often%20have%20complex%20structures%20and%20are%20difficult%0Ato%20translate%2C%20which%20may%20lead%20to%20wrong%20translations.%20In%20this%20paper%2C%20we%20propose%0Athe%20Attention-Based%20Varifocal%20Generative%20Adversarial%20Network%20%28AV-GAN%29%2C%20which%0Asolves%20multiple%20problems%20in%20pathologic%20image%20translation%20tasks%2C%20such%20as%20uneven%0Atranslation%20difficulty%20in%20different%20regions%2C%20mutual%20interference%20of%20multiple%0Aresolution%20information%2C%20and%20nuclear%20deformation.%20Specifically%2C%20we%20develop%20an%0AAttention-Based%20Key%20Region%20Selection%20Module%2C%20which%20can%20attend%20to%20regions%20with%0Ahigher%20translation%20difficulty.%20We%20then%20develop%20a%20Varifocal%20Module%20to%20translate%0Athese%20regions%20at%20multiple%20resolutions.%20Experimental%20results%20show%20that%20our%0Aproposed%20AV-GAN%20outperforms%20existing%20image%20translation%20methods%20with%20two%20virtual%0Akidney%20tissue%20staining%20tasks%20and%20improves%20FID%20values%20by%2015.9%20and%204.16%0Arespectively%20in%20the%20H%26E-MT%20and%20H%26E-PAS%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10714v1&entry.124074799=Read"},
{"title": "A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds", "author": "Raphael Sulzer and Renaud Marlet and Bruno Vallet and Loic Landrieu", "abstract": "  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors like\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can necessitate tedious\nhyperparameter tuning. Conversely, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of these handcrafted and learned priors on the\nprecision and robustness of surface reconstruction techniques. We evaluate\nvarious time-tested and contemporary methods in a standardized manner. When\nboth trained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce superior surfaces compared to their\ntraditional counterparts$\\unicode{x2013}$even in scenarios involving novel\nshape categories. However, traditional methods demonstrate greater resilience\nto the diverse array of point cloud anomalies commonly found in real-world 3D\nacquisitions. For the benefit of the research community, we make our code and\ndatasets available, inviting further enhancements to learning-based surface\nreconstruction. This can be accessed at\nhttps://github.com/raphaelsulzer/dsr-benchmark .\n", "link": "http://arxiv.org/abs/2301.13656v3", "date": "2024-04-16", "relevancy": 2.6504, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5427}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5052}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds&body=Title%3A%20A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds%0AAuthor%3A%20Raphael%20Sulzer%20and%20Renaud%20Marlet%20and%20Bruno%20Vallet%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20We%20present%20a%20comprehensive%20survey%20and%20benchmark%20of%20both%20traditional%20and%0Alearning-based%20methods%20for%20surface%20reconstruction%20from%20point%20clouds.%20This%20task%0Ais%20particularly%20challenging%20for%20real-world%20acquisitions%20due%20to%20factors%20like%0Anoise%2C%20outliers%2C%20non-uniform%20sampling%2C%20and%20missing%20data.%20Traditional%20approaches%0Aoften%20simplify%20the%20problem%20by%20imposing%20handcrafted%20priors%20on%20either%20the%20input%0Apoint%20clouds%20or%20the%20resulting%20surface%2C%20a%20process%20that%20can%20necessitate%20tedious%0Ahyperparameter%20tuning.%20Conversely%2C%20deep%20learning%20models%20have%20the%20capability%20to%0Adirectly%20learn%20the%20properties%20of%20input%20point%20clouds%20and%20desired%20surfaces%20from%0Adata.%20We%20study%20the%20influence%20of%20these%20handcrafted%20and%20learned%20priors%20on%20the%0Aprecision%20and%20robustness%20of%20surface%20reconstruction%20techniques.%20We%20evaluate%0Avarious%20time-tested%20and%20contemporary%20methods%20in%20a%20standardized%20manner.%20When%0Aboth%20trained%20and%20evaluated%20on%20point%20clouds%20with%20identical%20characteristics%2C%20the%0Alearning-based%20models%20consistently%20produce%20superior%20surfaces%20compared%20to%20their%0Atraditional%20counterparts%24%5Cunicode%7Bx2013%7D%24even%20in%20scenarios%20involving%20novel%0Ashape%20categories.%20However%2C%20traditional%20methods%20demonstrate%20greater%20resilience%0Ato%20the%20diverse%20array%20of%20point%20cloud%20anomalies%20commonly%20found%20in%20real-world%203D%0Aacquisitions.%20For%20the%20benefit%20of%20the%20research%20community%2C%20we%20make%20our%20code%20and%0Adatasets%20available%2C%20inviting%20further%20enhancements%20to%20learning-based%20surface%0Areconstruction.%20This%20can%20be%20accessed%20at%0Ahttps%3A//github.com/raphaelsulzer/dsr-benchmark%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13656v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20and%20Benchmark%20of%20Automatic%20Surface%20Reconstruction%20from%20Point%0A%20%20Clouds&entry.906535625=Raphael%20Sulzer%20and%20Renaud%20Marlet%20and%20Bruno%20Vallet%20and%20Loic%20Landrieu&entry.1292438233=%20%20We%20present%20a%20comprehensive%20survey%20and%20benchmark%20of%20both%20traditional%20and%0Alearning-based%20methods%20for%20surface%20reconstruction%20from%20point%20clouds.%20This%20task%0Ais%20particularly%20challenging%20for%20real-world%20acquisitions%20due%20to%20factors%20like%0Anoise%2C%20outliers%2C%20non-uniform%20sampling%2C%20and%20missing%20data.%20Traditional%20approaches%0Aoften%20simplify%20the%20problem%20by%20imposing%20handcrafted%20priors%20on%20either%20the%20input%0Apoint%20clouds%20or%20the%20resulting%20surface%2C%20a%20process%20that%20can%20necessitate%20tedious%0Ahyperparameter%20tuning.%20Conversely%2C%20deep%20learning%20models%20have%20the%20capability%20to%0Adirectly%20learn%20the%20properties%20of%20input%20point%20clouds%20and%20desired%20surfaces%20from%0Adata.%20We%20study%20the%20influence%20of%20these%20handcrafted%20and%20learned%20priors%20on%20the%0Aprecision%20and%20robustness%20of%20surface%20reconstruction%20techniques.%20We%20evaluate%0Avarious%20time-tested%20and%20contemporary%20methods%20in%20a%20standardized%20manner.%20When%0Aboth%20trained%20and%20evaluated%20on%20point%20clouds%20with%20identical%20characteristics%2C%20the%0Alearning-based%20models%20consistently%20produce%20superior%20surfaces%20compared%20to%20their%0Atraditional%20counterparts%24%5Cunicode%7Bx2013%7D%24even%20in%20scenarios%20involving%20novel%0Ashape%20categories.%20However%2C%20traditional%20methods%20demonstrate%20greater%20resilience%0Ato%20the%20diverse%20array%20of%20point%20cloud%20anomalies%20commonly%20found%20in%20real-world%203D%0Aacquisitions.%20For%20the%20benefit%20of%20the%20research%20community%2C%20we%20make%20our%20code%20and%0Adatasets%20available%2C%20inviting%20further%20enhancements%20to%20learning-based%20surface%0Areconstruction.%20This%20can%20be%20accessed%20at%0Ahttps%3A//github.com/raphaelsulzer/dsr-benchmark%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13656v3&entry.124074799=Read"},
{"title": "GPT-4V-AD: Exploring Grounding Potential of VQA-oriented GPT-4V for\n  Zero-shot Anomaly Detection", "author": "Jiangning Zhang and Haoyang He and Xuhai Chen and Zhucun Xue and Yabiao Wang and Chengjie Wang and Lei Xie and Yong Liu", "abstract": "  Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding\ncapabilities, making it possible to handle certain tasks through the Visual\nQuestion Answering (VQA) paradigm. This paper explores the potential of\nVQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and\nis the first to conduct qualitative and quantitative evaluations on the popular\nMVTec AD and VisA datasets. Considering that this task requires both\nimage-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three\ncomponents: \\textbf{\\textit{1)}} Granular Region Division, \\textbf{\\textit{2)}}\nPrompt Designing, \\textbf{\\textit{3)}} Text2Segmentation for easy quantitative\nevaluation, and have made some different attempts for comparative analysis. The\nresults show that GPT-4V can achieve certain results in the zero-shot AD task\nthrough a VQA paradigm, such as achieving image-level 77.1/88.0 and pixel-level\n68.0/76.6 AU-ROCs on MVTec AD and VisA datasets, respectively. However, its\nperformance still has a certain gap compared to the state-of-the-art zero-shot\nmethod, \\eg, WinCLIP and CLIP-AD, and further researches are needed. This study\nprovides a baseline reference for the research of VQA-oriented LMM in the\nzero-shot AD task, and we also post several possible future works. Code is\navailable at \\url{https://github.com/zhangzjn/GPT-4V-AD}.\n", "link": "http://arxiv.org/abs/2311.02612v2", "date": "2024-04-16", "relevancy": 2.6403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5295}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5117}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GPT-4V-AD%3A%20Exploring%20Grounding%20Potential%20of%20VQA-oriented%20GPT-4V%20for%0A%20%20Zero-shot%20Anomaly%20Detection&body=Title%3A%20GPT-4V-AD%3A%20Exploring%20Grounding%20Potential%20of%20VQA-oriented%20GPT-4V%20for%0A%20%20Zero-shot%20Anomaly%20Detection%0AAuthor%3A%20Jiangning%20Zhang%20and%20Haoyang%20He%20and%20Xuhai%20Chen%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lei%20Xie%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Large%20Multimodal%20Model%20%28LMM%29%20GPT-4V%28ision%29%20endows%20GPT-4%20with%20visual%20grounding%0Acapabilities%2C%20making%20it%20possible%20to%20handle%20certain%20tasks%20through%20the%20Visual%0AQuestion%20Answering%20%28VQA%29%20paradigm.%20This%20paper%20explores%20the%20potential%20of%0AVQA-oriented%20GPT-4V%20in%20the%20recently%20popular%20visual%20Anomaly%20Detection%20%28AD%29%20and%0Ais%20the%20first%20to%20conduct%20qualitative%20and%20quantitative%20evaluations%20on%20the%20popular%0AMVTec%20AD%20and%20VisA%20datasets.%20Considering%20that%20this%20task%20requires%20both%0Aimage-/pixel-level%20evaluations%2C%20the%20proposed%20GPT-4V-AD%20framework%20contains%20three%0Acomponents%3A%20%5Ctextbf%7B%5Ctextit%7B1%29%7D%7D%20Granular%20Region%20Division%2C%20%5Ctextbf%7B%5Ctextit%7B2%29%7D%7D%0APrompt%20Designing%2C%20%5Ctextbf%7B%5Ctextit%7B3%29%7D%7D%20Text2Segmentation%20for%20easy%20quantitative%0Aevaluation%2C%20and%20have%20made%20some%20different%20attempts%20for%20comparative%20analysis.%20The%0Aresults%20show%20that%20GPT-4V%20can%20achieve%20certain%20results%20in%20the%20zero-shot%20AD%20task%0Athrough%20a%20VQA%20paradigm%2C%20such%20as%20achieving%20image-level%2077.1/88.0%20and%20pixel-level%0A68.0/76.6%20AU-ROCs%20on%20MVTec%20AD%20and%20VisA%20datasets%2C%20respectively.%20However%2C%20its%0Aperformance%20still%20has%20a%20certain%20gap%20compared%20to%20the%20state-of-the-art%20zero-shot%0Amethod%2C%20%5Ceg%2C%20WinCLIP%20and%20CLIP-AD%2C%20and%20further%20researches%20are%20needed.%20This%20study%0Aprovides%20a%20baseline%20reference%20for%20the%20research%20of%20VQA-oriented%20LMM%20in%20the%0Azero-shot%20AD%20task%2C%20and%20we%20also%20post%20several%20possible%20future%20works.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/zhangzjn/GPT-4V-AD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT-4V-AD%3A%20Exploring%20Grounding%20Potential%20of%20VQA-oriented%20GPT-4V%20for%0A%20%20Zero-shot%20Anomaly%20Detection&entry.906535625=Jiangning%20Zhang%20and%20Haoyang%20He%20and%20Xuhai%20Chen%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lei%20Xie%20and%20Yong%20Liu&entry.1292438233=%20%20Large%20Multimodal%20Model%20%28LMM%29%20GPT-4V%28ision%29%20endows%20GPT-4%20with%20visual%20grounding%0Acapabilities%2C%20making%20it%20possible%20to%20handle%20certain%20tasks%20through%20the%20Visual%0AQuestion%20Answering%20%28VQA%29%20paradigm.%20This%20paper%20explores%20the%20potential%20of%0AVQA-oriented%20GPT-4V%20in%20the%20recently%20popular%20visual%20Anomaly%20Detection%20%28AD%29%20and%0Ais%20the%20first%20to%20conduct%20qualitative%20and%20quantitative%20evaluations%20on%20the%20popular%0AMVTec%20AD%20and%20VisA%20datasets.%20Considering%20that%20this%20task%20requires%20both%0Aimage-/pixel-level%20evaluations%2C%20the%20proposed%20GPT-4V-AD%20framework%20contains%20three%0Acomponents%3A%20%5Ctextbf%7B%5Ctextit%7B1%29%7D%7D%20Granular%20Region%20Division%2C%20%5Ctextbf%7B%5Ctextit%7B2%29%7D%7D%0APrompt%20Designing%2C%20%5Ctextbf%7B%5Ctextit%7B3%29%7D%7D%20Text2Segmentation%20for%20easy%20quantitative%0Aevaluation%2C%20and%20have%20made%20some%20different%20attempts%20for%20comparative%20analysis.%20The%0Aresults%20show%20that%20GPT-4V%20can%20achieve%20certain%20results%20in%20the%20zero-shot%20AD%20task%0Athrough%20a%20VQA%20paradigm%2C%20such%20as%20achieving%20image-level%2077.1/88.0%20and%20pixel-level%0A68.0/76.6%20AU-ROCs%20on%20MVTec%20AD%20and%20VisA%20datasets%2C%20respectively.%20However%2C%20its%0Aperformance%20still%20has%20a%20certain%20gap%20compared%20to%20the%20state-of-the-art%20zero-shot%0Amethod%2C%20%5Ceg%2C%20WinCLIP%20and%20CLIP-AD%2C%20and%20further%20researches%20are%20needed.%20This%20study%0Aprovides%20a%20baseline%20reference%20for%20the%20research%20of%20VQA-oriented%20LMM%20in%20the%0Azero-shot%20AD%20task%2C%20and%20we%20also%20post%20several%20possible%20future%20works.%20Code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/zhangzjn/GPT-4V-AD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02612v2&entry.124074799=Read"},
{"title": "BDAN: Mitigating Temporal Difference Across Electrodes in Cross-Subject\n  Motor Imagery Classification via Generative Bridging Domain", "author": "Zhige Chen and Rui Yang and Mengjie Huang and Chengxuan Qin and Zidong Wang", "abstract": "  Because of \"the non-repeatability of the experiment settings and conditions\"\nand \"the variability of brain patterns among subjects\", the data distributions\nacross sessions and electrodes are different in cross-subject motor imagery\n(MI) studies, eventually reducing the performance of the classification model.\nSystematically summarised based on the existing studies, a novel\ntemporal-electrode data distribution problem is investigated under both\nintra-subject and inter-subject scenarios in this paper. Based on the presented\nissue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to\nminimise the data distribution difference across sessions in the aspect of the\nelectrode, thus improving and enhancing model performance. In the proposed\nBDAN, deep features of all the EEG data are extracted via a specially designed\nspatial feature extractor. With the obtained spatio-temporal features, a\nspecial generative bridging domain is established, bridging the data from all\nthe subjects across sessions. The difference across sessions and electrodes is\nthen minimized using the customized bridging loss functions, and the known\nknowledge is automatically transferred through the constructed bridging domain.\nTo show the effectiveness of the proposed BDAN, comparison experiments and\nablation studies are conducted on a public EEG dataset. The overall comparison\nresults demonstrate the superior performance of the proposed BDAN compared with\nthe other advanced deep learning and domain adaptation methods.\n", "link": "http://arxiv.org/abs/2404.10494v1", "date": "2024-04-16", "relevancy": 2.6164, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5229}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BDAN%3A%20Mitigating%20Temporal%20Difference%20Across%20Electrodes%20in%20Cross-Subject%0A%20%20Motor%20Imagery%20Classification%20via%20Generative%20Bridging%20Domain&body=Title%3A%20BDAN%3A%20Mitigating%20Temporal%20Difference%20Across%20Electrodes%20in%20Cross-Subject%0A%20%20Motor%20Imagery%20Classification%20via%20Generative%20Bridging%20Domain%0AAuthor%3A%20Zhige%20Chen%20and%20Rui%20Yang%20and%20Mengjie%20Huang%20and%20Chengxuan%20Qin%20and%20Zidong%20Wang%0AAbstract%3A%20%20%20Because%20of%20%22the%20non-repeatability%20of%20the%20experiment%20settings%20and%20conditions%22%0Aand%20%22the%20variability%20of%20brain%20patterns%20among%20subjects%22%2C%20the%20data%20distributions%0Aacross%20sessions%20and%20electrodes%20are%20different%20in%20cross-subject%20motor%20imagery%0A%28MI%29%20studies%2C%20eventually%20reducing%20the%20performance%20of%20the%20classification%20model.%0ASystematically%20summarised%20based%20on%20the%20existing%20studies%2C%20a%20novel%0Atemporal-electrode%20data%20distribution%20problem%20is%20investigated%20under%20both%0Aintra-subject%20and%20inter-subject%20scenarios%20in%20this%20paper.%20Based%20on%20the%20presented%0Aissue%2C%20a%20novel%20bridging%20domain%20adaptation%20network%20%28BDAN%29%20is%20proposed%2C%20aiming%20to%0Aminimise%20the%20data%20distribution%20difference%20across%20sessions%20in%20the%20aspect%20of%20the%0Aelectrode%2C%20thus%20improving%20and%20enhancing%20model%20performance.%20In%20the%20proposed%0ABDAN%2C%20deep%20features%20of%20all%20the%20EEG%20data%20are%20extracted%20via%20a%20specially%20designed%0Aspatial%20feature%20extractor.%20With%20the%20obtained%20spatio-temporal%20features%2C%20a%0Aspecial%20generative%20bridging%20domain%20is%20established%2C%20bridging%20the%20data%20from%20all%0Athe%20subjects%20across%20sessions.%20The%20difference%20across%20sessions%20and%20electrodes%20is%0Athen%20minimized%20using%20the%20customized%20bridging%20loss%20functions%2C%20and%20the%20known%0Aknowledge%20is%20automatically%20transferred%20through%20the%20constructed%20bridging%20domain.%0ATo%20show%20the%20effectiveness%20of%20the%20proposed%20BDAN%2C%20comparison%20experiments%20and%0Aablation%20studies%20are%20conducted%20on%20a%20public%20EEG%20dataset.%20The%20overall%20comparison%0Aresults%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20BDAN%20compared%20with%0Athe%20other%20advanced%20deep%20learning%20and%20domain%20adaptation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10494v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BDAN%3A%20Mitigating%20Temporal%20Difference%20Across%20Electrodes%20in%20Cross-Subject%0A%20%20Motor%20Imagery%20Classification%20via%20Generative%20Bridging%20Domain&entry.906535625=Zhige%20Chen%20and%20Rui%20Yang%20and%20Mengjie%20Huang%20and%20Chengxuan%20Qin%20and%20Zidong%20Wang&entry.1292438233=%20%20Because%20of%20%22the%20non-repeatability%20of%20the%20experiment%20settings%20and%20conditions%22%0Aand%20%22the%20variability%20of%20brain%20patterns%20among%20subjects%22%2C%20the%20data%20distributions%0Aacross%20sessions%20and%20electrodes%20are%20different%20in%20cross-subject%20motor%20imagery%0A%28MI%29%20studies%2C%20eventually%20reducing%20the%20performance%20of%20the%20classification%20model.%0ASystematically%20summarised%20based%20on%20the%20existing%20studies%2C%20a%20novel%0Atemporal-electrode%20data%20distribution%20problem%20is%20investigated%20under%20both%0Aintra-subject%20and%20inter-subject%20scenarios%20in%20this%20paper.%20Based%20on%20the%20presented%0Aissue%2C%20a%20novel%20bridging%20domain%20adaptation%20network%20%28BDAN%29%20is%20proposed%2C%20aiming%20to%0Aminimise%20the%20data%20distribution%20difference%20across%20sessions%20in%20the%20aspect%20of%20the%0Aelectrode%2C%20thus%20improving%20and%20enhancing%20model%20performance.%20In%20the%20proposed%0ABDAN%2C%20deep%20features%20of%20all%20the%20EEG%20data%20are%20extracted%20via%20a%20specially%20designed%0Aspatial%20feature%20extractor.%20With%20the%20obtained%20spatio-temporal%20features%2C%20a%0Aspecial%20generative%20bridging%20domain%20is%20established%2C%20bridging%20the%20data%20from%20all%0Athe%20subjects%20across%20sessions.%20The%20difference%20across%20sessions%20and%20electrodes%20is%0Athen%20minimized%20using%20the%20customized%20bridging%20loss%20functions%2C%20and%20the%20known%0Aknowledge%20is%20automatically%20transferred%20through%20the%20constructed%20bridging%20domain.%0ATo%20show%20the%20effectiveness%20of%20the%20proposed%20BDAN%2C%20comparison%20experiments%20and%0Aablation%20studies%20are%20conducted%20on%20a%20public%20EEG%20dataset.%20The%20overall%20comparison%0Aresults%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%20BDAN%20compared%20with%0Athe%20other%20advanced%20deep%20learning%20and%20domain%20adaptation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10494v1&entry.124074799=Read"},
{"title": "GazeHTA: End-to-end Gaze Target Detection with Head-Target Association", "author": "Zhi-Yi Lin and Jouh Yeong Chew and Jan van Gemert and Xucong Zhang", "abstract": "  We propose an end-to-end approach for gaze target detection: predicting a\nhead-target connection between individuals and the target image regions they\nare looking at. Most of the existing methods use independent components such as\noff-the-shelf head detectors or have problems in establishing associations\nbetween heads and gaze targets. In contrast, we investigate an end-to-end\nmulti-person Gaze target detection framework with Heads and Targets Association\n(GazeHTA), which predicts multiple head-target instances based solely on input\nscene image. GazeHTA addresses challenges in gaze target detection by (1)\nleveraging a pre-trained diffusion model to extract scene features for rich\nsemantic understanding, (2) re-injecting a head feature to enhance the head\npriors for improved head understanding, and (3) learning a connection map as\nthe explicit visual associations between heads and gaze targets. Our extensive\nexperimental results demonstrate that GazeHTA outperforms state-of-the-art gaze\ntarget detection methods and two adapted diffusion-based baselines on two\nstandard datasets.\n", "link": "http://arxiv.org/abs/2404.10718v1", "date": "2024-04-16", "relevancy": 2.6081, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association&body=Title%3A%20GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association%0AAuthor%3A%20Zhi-Yi%20Lin%20and%20Jouh%20Yeong%20Chew%20and%20Jan%20van%20Gemert%20and%20Xucong%20Zhang%0AAbstract%3A%20%20%20We%20propose%20an%20end-to-end%20approach%20for%20gaze%20target%20detection%3A%20predicting%20a%0Ahead-target%20connection%20between%20individuals%20and%20the%20target%20image%20regions%20they%0Aare%20looking%20at.%20Most%20of%20the%20existing%20methods%20use%20independent%20components%20such%20as%0Aoff-the-shelf%20head%20detectors%20or%20have%20problems%20in%20establishing%20associations%0Abetween%20heads%20and%20gaze%20targets.%20In%20contrast%2C%20we%20investigate%20an%20end-to-end%0Amulti-person%20Gaze%20target%20detection%20framework%20with%20Heads%20and%20Targets%20Association%0A%28GazeHTA%29%2C%20which%20predicts%20multiple%20head-target%20instances%20based%20solely%20on%20input%0Ascene%20image.%20GazeHTA%20addresses%20challenges%20in%20gaze%20target%20detection%20by%20%281%29%0Aleveraging%20a%20pre-trained%20diffusion%20model%20to%20extract%20scene%20features%20for%20rich%0Asemantic%20understanding%2C%20%282%29%20re-injecting%20a%20head%20feature%20to%20enhance%20the%20head%0Apriors%20for%20improved%20head%20understanding%2C%20and%20%283%29%20learning%20a%20connection%20map%20as%0Athe%20explicit%20visual%20associations%20between%20heads%20and%20gaze%20targets.%20Our%20extensive%0Aexperimental%20results%20demonstrate%20that%20GazeHTA%20outperforms%20state-of-the-art%20gaze%0Atarget%20detection%20methods%20and%20two%20adapted%20diffusion-based%20baselines%20on%20two%0Astandard%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10718v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association&entry.906535625=Zhi-Yi%20Lin%20and%20Jouh%20Yeong%20Chew%20and%20Jan%20van%20Gemert%20and%20Xucong%20Zhang&entry.1292438233=%20%20We%20propose%20an%20end-to-end%20approach%20for%20gaze%20target%20detection%3A%20predicting%20a%0Ahead-target%20connection%20between%20individuals%20and%20the%20target%20image%20regions%20they%0Aare%20looking%20at.%20Most%20of%20the%20existing%20methods%20use%20independent%20components%20such%20as%0Aoff-the-shelf%20head%20detectors%20or%20have%20problems%20in%20establishing%20associations%0Abetween%20heads%20and%20gaze%20targets.%20In%20contrast%2C%20we%20investigate%20an%20end-to-end%0Amulti-person%20Gaze%20target%20detection%20framework%20with%20Heads%20and%20Targets%20Association%0A%28GazeHTA%29%2C%20which%20predicts%20multiple%20head-target%20instances%20based%20solely%20on%20input%0Ascene%20image.%20GazeHTA%20addresses%20challenges%20in%20gaze%20target%20detection%20by%20%281%29%0Aleveraging%20a%20pre-trained%20diffusion%20model%20to%20extract%20scene%20features%20for%20rich%0Asemantic%20understanding%2C%20%282%29%20re-injecting%20a%20head%20feature%20to%20enhance%20the%20head%0Apriors%20for%20improved%20head%20understanding%2C%20and%20%283%29%20learning%20a%20connection%20map%20as%0Athe%20explicit%20visual%20associations%20between%20heads%20and%20gaze%20targets.%20Our%20extensive%0Aexperimental%20results%20demonstrate%20that%20GazeHTA%20outperforms%20state-of-the-art%20gaze%0Atarget%20detection%20methods%20and%20two%20adapted%20diffusion-based%20baselines%20on%20two%0Astandard%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10718v1&entry.124074799=Read"},
{"title": "Watch Your Step: Optimal Retrieval for Continual Learning at Scale", "author": "Truman Hickok and Dhireesha Kudithipudi", "abstract": "  One of the most widely used approaches in continual learning is referred to\nas replay. Replay methods support interleaved learning by storing past\nexperiences in a replay buffer. Although there are methods for selectively\nconstructing the buffer and reprocessing its contents, there is limited\nexploration of the problem of selectively retrieving samples from the buffer.\nCurrent solutions have been tested in limited settings and, more importantly,\nin isolation. Existing work has also not explored the impact of duplicate\nreplays on performance. In this work, we propose a framework for evaluating\nselective retrieval strategies, categorized by simple, independent class- and\nsample-selective primitives. We evaluated several combinations of existing\nstrategies for selective retrieval and present their performances. Furthermore,\nwe propose a set of strategies to prevent duplicate replays and explore whether\nnew samples with low loss values can be learned without replay. In an effort to\nmatch our problem setting to a realistic continual learning pipeline, we\nrestrict our experiments to a setting involving a large, pre-trained, open\nvocabulary object detection model, which is fully fine-tuned on a sequence of\n15 datasets.\n", "link": "http://arxiv.org/abs/2404.10758v1", "date": "2024-04-16", "relevancy": 2.5988, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5084}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Watch%20Your%20Step%3A%20Optimal%20Retrieval%20for%20Continual%20Learning%20at%20Scale&body=Title%3A%20Watch%20Your%20Step%3A%20Optimal%20Retrieval%20for%20Continual%20Learning%20at%20Scale%0AAuthor%3A%20Truman%20Hickok%20and%20Dhireesha%20Kudithipudi%0AAbstract%3A%20%20%20One%20of%20the%20most%20widely%20used%20approaches%20in%20continual%20learning%20is%20referred%20to%0Aas%20replay.%20Replay%20methods%20support%20interleaved%20learning%20by%20storing%20past%0Aexperiences%20in%20a%20replay%20buffer.%20Although%20there%20are%20methods%20for%20selectively%0Aconstructing%20the%20buffer%20and%20reprocessing%20its%20contents%2C%20there%20is%20limited%0Aexploration%20of%20the%20problem%20of%20selectively%20retrieving%20samples%20from%20the%20buffer.%0ACurrent%20solutions%20have%20been%20tested%20in%20limited%20settings%20and%2C%20more%20importantly%2C%0Ain%20isolation.%20Existing%20work%20has%20also%20not%20explored%20the%20impact%20of%20duplicate%0Areplays%20on%20performance.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20evaluating%0Aselective%20retrieval%20strategies%2C%20categorized%20by%20simple%2C%20independent%20class-%20and%0Asample-selective%20primitives.%20We%20evaluated%20several%20combinations%20of%20existing%0Astrategies%20for%20selective%20retrieval%20and%20present%20their%20performances.%20Furthermore%2C%0Awe%20propose%20a%20set%20of%20strategies%20to%20prevent%20duplicate%20replays%20and%20explore%20whether%0Anew%20samples%20with%20low%20loss%20values%20can%20be%20learned%20without%20replay.%20In%20an%20effort%20to%0Amatch%20our%20problem%20setting%20to%20a%20realistic%20continual%20learning%20pipeline%2C%20we%0Arestrict%20our%20experiments%20to%20a%20setting%20involving%20a%20large%2C%20pre-trained%2C%20open%0Avocabulary%20object%20detection%20model%2C%20which%20is%20fully%20fine-tuned%20on%20a%20sequence%20of%0A15%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10758v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watch%20Your%20Step%3A%20Optimal%20Retrieval%20for%20Continual%20Learning%20at%20Scale&entry.906535625=Truman%20Hickok%20and%20Dhireesha%20Kudithipudi&entry.1292438233=%20%20One%20of%20the%20most%20widely%20used%20approaches%20in%20continual%20learning%20is%20referred%20to%0Aas%20replay.%20Replay%20methods%20support%20interleaved%20learning%20by%20storing%20past%0Aexperiences%20in%20a%20replay%20buffer.%20Although%20there%20are%20methods%20for%20selectively%0Aconstructing%20the%20buffer%20and%20reprocessing%20its%20contents%2C%20there%20is%20limited%0Aexploration%20of%20the%20problem%20of%20selectively%20retrieving%20samples%20from%20the%20buffer.%0ACurrent%20solutions%20have%20been%20tested%20in%20limited%20settings%20and%2C%20more%20importantly%2C%0Ain%20isolation.%20Existing%20work%20has%20also%20not%20explored%20the%20impact%20of%20duplicate%0Areplays%20on%20performance.%20In%20this%20work%2C%20we%20propose%20a%20framework%20for%20evaluating%0Aselective%20retrieval%20strategies%2C%20categorized%20by%20simple%2C%20independent%20class-%20and%0Asample-selective%20primitives.%20We%20evaluated%20several%20combinations%20of%20existing%0Astrategies%20for%20selective%20retrieval%20and%20present%20their%20performances.%20Furthermore%2C%0Awe%20propose%20a%20set%20of%20strategies%20to%20prevent%20duplicate%20replays%20and%20explore%20whether%0Anew%20samples%20with%20low%20loss%20values%20can%20be%20learned%20without%20replay.%20In%20an%20effort%20to%0Amatch%20our%20problem%20setting%20to%20a%20realistic%20continual%20learning%20pipeline%2C%20we%0Arestrict%20our%20experiments%20to%20a%20setting%20involving%20a%20large%2C%20pre-trained%2C%20open%0Avocabulary%20object%20detection%20model%2C%20which%20is%20fully%20fine-tuned%20on%20a%20sequence%20of%0A15%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10758v1&entry.124074799=Read"},
{"title": "Ghost-dil-NetVLAD: A Lightweight Neural Network for Visual Place\n  Recognition", "author": "Qingyuan Gong and Yu Liu and Liqiang Zhang and Renhe Liu", "abstract": "  Visual place recognition (VPR) is a challenging task with the unbalance\nbetween enormous computational cost and high recognition performance. Thanks to\nthe practical feature extraction ability of the lightweight convolution neural\nnetworks (CNNs) and the train-ability of the vector of locally aggregated\ndescriptors (VLAD) layer, we propose a lightweight weakly supervised end-to-end\nneural network consisting of a front-ended perception model called GhostCNN and\na learnable VLAD layer as a back-end. GhostCNN is based on Ghost modules that\nare lightweight CNN-based architectures. They can generate redundant feature\nmaps using linear operations instead of the traditional convolution process,\nmaking a good trade-off between computation resources and recognition accuracy.\nTo enhance our proposed lightweight model further, we add dilated convolutions\nto the Ghost module to get features containing more spatial semantic\ninformation, improving accuracy. Finally, rich experiments conducted on a\ncommonly used public benchmark and our private dataset validate that the\nproposed neural network reduces the FLOPs and parameters of VGG16-NetVLAD by\n99.04% and 80.16%, respectively. Besides, both models achieve similar accuracy.\n", "link": "http://arxiv.org/abs/2112.11679v2", "date": "2024-04-16", "relevancy": 2.5981, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5289}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5213}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5086}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ghost-dil-NetVLAD%3A%20A%20Lightweight%20Neural%20Network%20for%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20Ghost-dil-NetVLAD%3A%20A%20Lightweight%20Neural%20Network%20for%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Qingyuan%20Gong%20and%20Yu%20Liu%20and%20Liqiang%20Zhang%20and%20Renhe%20Liu%0AAbstract%3A%20%20%20Visual%20place%20recognition%20%28VPR%29%20is%20a%20challenging%20task%20with%20the%20unbalance%0Abetween%20enormous%20computational%20cost%20and%20high%20recognition%20performance.%20Thanks%20to%0Athe%20practical%20feature%20extraction%20ability%20of%20the%20lightweight%20convolution%20neural%0Anetworks%20%28CNNs%29%20and%20the%20train-ability%20of%20the%20vector%20of%20locally%20aggregated%0Adescriptors%20%28VLAD%29%20layer%2C%20we%20propose%20a%20lightweight%20weakly%20supervised%20end-to-end%0Aneural%20network%20consisting%20of%20a%20front-ended%20perception%20model%20called%20GhostCNN%20and%0Aa%20learnable%20VLAD%20layer%20as%20a%20back-end.%20GhostCNN%20is%20based%20on%20Ghost%20modules%20that%0Aare%20lightweight%20CNN-based%20architectures.%20They%20can%20generate%20redundant%20feature%0Amaps%20using%20linear%20operations%20instead%20of%20the%20traditional%20convolution%20process%2C%0Amaking%20a%20good%20trade-off%20between%20computation%20resources%20and%20recognition%20accuracy.%0ATo%20enhance%20our%20proposed%20lightweight%20model%20further%2C%20we%20add%20dilated%20convolutions%0Ato%20the%20Ghost%20module%20to%20get%20features%20containing%20more%20spatial%20semantic%0Ainformation%2C%20improving%20accuracy.%20Finally%2C%20rich%20experiments%20conducted%20on%20a%0Acommonly%20used%20public%20benchmark%20and%20our%20private%20dataset%20validate%20that%20the%0Aproposed%20neural%20network%20reduces%20the%20FLOPs%20and%20parameters%20of%20VGG16-NetVLAD%20by%0A99.04%25%20and%2080.16%25%2C%20respectively.%20Besides%2C%20both%20models%20achieve%20similar%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.11679v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ghost-dil-NetVLAD%3A%20A%20Lightweight%20Neural%20Network%20for%20Visual%20Place%0A%20%20Recognition&entry.906535625=Qingyuan%20Gong%20and%20Yu%20Liu%20and%20Liqiang%20Zhang%20and%20Renhe%20Liu&entry.1292438233=%20%20Visual%20place%20recognition%20%28VPR%29%20is%20a%20challenging%20task%20with%20the%20unbalance%0Abetween%20enormous%20computational%20cost%20and%20high%20recognition%20performance.%20Thanks%20to%0Athe%20practical%20feature%20extraction%20ability%20of%20the%20lightweight%20convolution%20neural%0Anetworks%20%28CNNs%29%20and%20the%20train-ability%20of%20the%20vector%20of%20locally%20aggregated%0Adescriptors%20%28VLAD%29%20layer%2C%20we%20propose%20a%20lightweight%20weakly%20supervised%20end-to-end%0Aneural%20network%20consisting%20of%20a%20front-ended%20perception%20model%20called%20GhostCNN%20and%0Aa%20learnable%20VLAD%20layer%20as%20a%20back-end.%20GhostCNN%20is%20based%20on%20Ghost%20modules%20that%0Aare%20lightweight%20CNN-based%20architectures.%20They%20can%20generate%20redundant%20feature%0Amaps%20using%20linear%20operations%20instead%20of%20the%20traditional%20convolution%20process%2C%0Amaking%20a%20good%20trade-off%20between%20computation%20resources%20and%20recognition%20accuracy.%0ATo%20enhance%20our%20proposed%20lightweight%20model%20further%2C%20we%20add%20dilated%20convolutions%0Ato%20the%20Ghost%20module%20to%20get%20features%20containing%20more%20spatial%20semantic%0Ainformation%2C%20improving%20accuracy.%20Finally%2C%20rich%20experiments%20conducted%20on%20a%0Acommonly%20used%20public%20benchmark%20and%20our%20private%20dataset%20validate%20that%20the%0Aproposed%20neural%20network%20reduces%20the%20FLOPs%20and%20parameters%20of%20VGG16-NetVLAD%20by%0A99.04%25%20and%2080.16%25%2C%20respectively.%20Besides%2C%20both%20models%20achieve%20similar%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.11679v2&entry.124074799=Read"},
{"title": "The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose\n  Refinement", "author": "Gabriele Trivigno and Carlo Masone and Barbara Caputo and Torsten Sattler", "abstract": "  Pose refinement is an interesting and practically relevant research\ndirection. Pose refinement can be used to (1) obtain a more accurate pose\nestimate from an initial prior (e.g., from retrieval), (2) as pre-processing,\ni.e., to provide a better starting point to a more expensive pose estimator,\n(3) as post-processing of a more accurate localizer. Existing approaches focus\non learning features / scene representations for the pose refinement task. This\ninvolves training an implicit scene representation or learning features while\noptimizing a camera pose-based loss. A natural question is whether training\nspecific features / representations is truly necessary or whether similar\nresults can be already achieved with more generic features. In this work, we\npresent a simple approach that combines pre-trained features with a particle\nfilter and a renderable representation of the scene. Despite its simplicity, it\nachieves state-of-the-art results, demonstrating that one can easily build a\npose refiner without the need for specific training. The code is at\nhttps://github.com/ga1i13o/mcloc_poseref\n", "link": "http://arxiv.org/abs/2404.10438v1", "date": "2024-04-16", "relevancy": 2.5962, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Unreasonable%20Effectiveness%20of%20Pre-Trained%20Features%20for%20Camera%20Pose%0A%20%20Refinement&body=Title%3A%20The%20Unreasonable%20Effectiveness%20of%20Pre-Trained%20Features%20for%20Camera%20Pose%0A%20%20Refinement%0AAuthor%3A%20Gabriele%20Trivigno%20and%20Carlo%20Masone%20and%20Barbara%20Caputo%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Pose%20refinement%20is%20an%20interesting%20and%20practically%20relevant%20research%0Adirection.%20Pose%20refinement%20can%20be%20used%20to%20%281%29%20obtain%20a%20more%20accurate%20pose%0Aestimate%20from%20an%20initial%20prior%20%28e.g.%2C%20from%20retrieval%29%2C%20%282%29%20as%20pre-processing%2C%0Ai.e.%2C%20to%20provide%20a%20better%20starting%20point%20to%20a%20more%20expensive%20pose%20estimator%2C%0A%283%29%20as%20post-processing%20of%20a%20more%20accurate%20localizer.%20Existing%20approaches%20focus%0Aon%20learning%20features%20/%20scene%20representations%20for%20the%20pose%20refinement%20task.%20This%0Ainvolves%20training%20an%20implicit%20scene%20representation%20or%20learning%20features%20while%0Aoptimizing%20a%20camera%20pose-based%20loss.%20A%20natural%20question%20is%20whether%20training%0Aspecific%20features%20/%20representations%20is%20truly%20necessary%20or%20whether%20similar%0Aresults%20can%20be%20already%20achieved%20with%20more%20generic%20features.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20approach%20that%20combines%20pre-trained%20features%20with%20a%20particle%0Afilter%20and%20a%20renderable%20representation%20of%20the%20scene.%20Despite%20its%20simplicity%2C%20it%0Aachieves%20state-of-the-art%20results%2C%20demonstrating%20that%20one%20can%20easily%20build%20a%0Apose%20refiner%20without%20the%20need%20for%20specific%20training.%20The%20code%20is%20at%0Ahttps%3A//github.com/ga1i13o/mcloc_poseref%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10438v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unreasonable%20Effectiveness%20of%20Pre-Trained%20Features%20for%20Camera%20Pose%0A%20%20Refinement&entry.906535625=Gabriele%20Trivigno%20and%20Carlo%20Masone%20and%20Barbara%20Caputo%20and%20Torsten%20Sattler&entry.1292438233=%20%20Pose%20refinement%20is%20an%20interesting%20and%20practically%20relevant%20research%0Adirection.%20Pose%20refinement%20can%20be%20used%20to%20%281%29%20obtain%20a%20more%20accurate%20pose%0Aestimate%20from%20an%20initial%20prior%20%28e.g.%2C%20from%20retrieval%29%2C%20%282%29%20as%20pre-processing%2C%0Ai.e.%2C%20to%20provide%20a%20better%20starting%20point%20to%20a%20more%20expensive%20pose%20estimator%2C%0A%283%29%20as%20post-processing%20of%20a%20more%20accurate%20localizer.%20Existing%20approaches%20focus%0Aon%20learning%20features%20/%20scene%20representations%20for%20the%20pose%20refinement%20task.%20This%0Ainvolves%20training%20an%20implicit%20scene%20representation%20or%20learning%20features%20while%0Aoptimizing%20a%20camera%20pose-based%20loss.%20A%20natural%20question%20is%20whether%20training%0Aspecific%20features%20/%20representations%20is%20truly%20necessary%20or%20whether%20similar%0Aresults%20can%20be%20already%20achieved%20with%20more%20generic%20features.%20In%20this%20work%2C%20we%0Apresent%20a%20simple%20approach%20that%20combines%20pre-trained%20features%20with%20a%20particle%0Afilter%20and%20a%20renderable%20representation%20of%20the%20scene.%20Despite%20its%20simplicity%2C%20it%0Aachieves%20state-of-the-art%20results%2C%20demonstrating%20that%20one%20can%20easily%20build%20a%0Apose%20refiner%20without%20the%20need%20for%20specific%20training.%20The%20code%20is%20at%0Ahttps%3A//github.com/ga1i13o/mcloc_poseref%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10438v1&entry.124074799=Read"},
{"title": "LoopAnimate: Loopable Salient Object Animation", "author": "Fanyi Wang and Peng Liu and Haotian Hu and Dan Meng and Jingwen Su and Jinjin Xu and Yanhao Zhang and Xiaoming Ren and Zhiwang Zhang", "abstract": "  Research on diffusion model-based video generation has advanced rapidly.\nHowever, limitations in object fidelity and generation length hinder its\npractical applications. Additionally, specific domains like animated wallpapers\nrequire seamless looping, where the first and last frames of the video match\nseamlessly. To address these challenges, this paper proposes LoopAnimate, a\nnovel method for generating videos with consistent start and end frames. To\nenhance object fidelity, we introduce a framework that decouples multi-level\nimage appearance and textual semantic information. Building upon an\nimage-to-image diffusion model, our approach incorporates both pixel-level and\nfeature-level information from the input image, injecting image appearance and\ntextual semantic embeddings at different positions of the diffusion model.\nExisting UNet-based video generation models require to input the entire videos\nduring training to encode temporal and positional information at once. However,\ndue to limitations in GPU memory, the number of frames is typically restricted\nto 16. To address this, this paper proposes a three-stage training strategy\nwith progressively increasing frame numbers and reducing fine-tuning modules.\nAdditionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend\nthe capacity for encoding temporal and positional information up to 36 frames.\nThe proposed LoopAnimate, which for the first time extends the single-pass\ngeneration length of UNet-based video generation models to 35 frames while\nmaintaining high-quality video generation. Experiments demonstrate that\nLoopAnimate achieves state-of-the-art performance in both objective metrics,\nsuch as fidelity and temporal consistency, and subjective evaluation results.\n", "link": "http://arxiv.org/abs/2404.09172v2", "date": "2024-04-16", "relevancy": 2.5172, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.649}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6338}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6169}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LoopAnimate%3A%20Loopable%20Salient%20Object%20Animation&body=Title%3A%20LoopAnimate%3A%20Loopable%20Salient%20Object%20Animation%0AAuthor%3A%20Fanyi%20Wang%20and%20Peng%20Liu%20and%20Haotian%20Hu%20and%20Dan%20Meng%20and%20Jingwen%20Su%20and%20Jinjin%20Xu%20and%20Yanhao%20Zhang%20and%20Xiaoming%20Ren%20and%20Zhiwang%20Zhang%0AAbstract%3A%20%20%20Research%20on%20diffusion%20model-based%20video%20generation%20has%20advanced%20rapidly.%0AHowever%2C%20limitations%20in%20object%20fidelity%20and%20generation%20length%20hinder%20its%0Apractical%20applications.%20Additionally%2C%20specific%20domains%20like%20animated%20wallpapers%0Arequire%20seamless%20looping%2C%20where%20the%20first%20and%20last%20frames%20of%20the%20video%20match%0Aseamlessly.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20LoopAnimate%2C%20a%0Anovel%20method%20for%20generating%20videos%20with%20consistent%20start%20and%20end%20frames.%20To%0Aenhance%20object%20fidelity%2C%20we%20introduce%20a%20framework%20that%20decouples%20multi-level%0Aimage%20appearance%20and%20textual%20semantic%20information.%20Building%20upon%20an%0Aimage-to-image%20diffusion%20model%2C%20our%20approach%20incorporates%20both%20pixel-level%20and%0Afeature-level%20information%20from%20the%20input%20image%2C%20injecting%20image%20appearance%20and%0Atextual%20semantic%20embeddings%20at%20different%20positions%20of%20the%20diffusion%20model.%0AExisting%20UNet-based%20video%20generation%20models%20require%20to%20input%20the%20entire%20videos%0Aduring%20training%20to%20encode%20temporal%20and%20positional%20information%20at%20once.%20However%2C%0Adue%20to%20limitations%20in%20GPU%20memory%2C%20the%20number%20of%20frames%20is%20typically%20restricted%0Ato%2016.%20To%20address%20this%2C%20this%20paper%20proposes%20a%20three-stage%20training%20strategy%0Awith%20progressively%20increasing%20frame%20numbers%20and%20reducing%20fine-tuning%20modules.%0AAdditionally%2C%20we%20introduce%20the%20Temporal%20E%20nhanced%20Motion%20Module%28TEMM%29%20to%20extend%0Athe%20capacity%20for%20encoding%20temporal%20and%20positional%20information%20up%20to%2036%20frames.%0AThe%20proposed%20LoopAnimate%2C%20which%20for%20the%20first%20time%20extends%20the%20single-pass%0Ageneration%20length%20of%20UNet-based%20video%20generation%20models%20to%2035%20frames%20while%0Amaintaining%20high-quality%20video%20generation.%20Experiments%20demonstrate%20that%0ALoopAnimate%20achieves%20state-of-the-art%20performance%20in%20both%20objective%20metrics%2C%0Asuch%20as%20fidelity%20and%20temporal%20consistency%2C%20and%20subjective%20evaluation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09172v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoopAnimate%3A%20Loopable%20Salient%20Object%20Animation&entry.906535625=Fanyi%20Wang%20and%20Peng%20Liu%20and%20Haotian%20Hu%20and%20Dan%20Meng%20and%20Jingwen%20Su%20and%20Jinjin%20Xu%20and%20Yanhao%20Zhang%20and%20Xiaoming%20Ren%20and%20Zhiwang%20Zhang&entry.1292438233=%20%20Research%20on%20diffusion%20model-based%20video%20generation%20has%20advanced%20rapidly.%0AHowever%2C%20limitations%20in%20object%20fidelity%20and%20generation%20length%20hinder%20its%0Apractical%20applications.%20Additionally%2C%20specific%20domains%20like%20animated%20wallpapers%0Arequire%20seamless%20looping%2C%20where%20the%20first%20and%20last%20frames%20of%20the%20video%20match%0Aseamlessly.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20LoopAnimate%2C%20a%0Anovel%20method%20for%20generating%20videos%20with%20consistent%20start%20and%20end%20frames.%20To%0Aenhance%20object%20fidelity%2C%20we%20introduce%20a%20framework%20that%20decouples%20multi-level%0Aimage%20appearance%20and%20textual%20semantic%20information.%20Building%20upon%20an%0Aimage-to-image%20diffusion%20model%2C%20our%20approach%20incorporates%20both%20pixel-level%20and%0Afeature-level%20information%20from%20the%20input%20image%2C%20injecting%20image%20appearance%20and%0Atextual%20semantic%20embeddings%20at%20different%20positions%20of%20the%20diffusion%20model.%0AExisting%20UNet-based%20video%20generation%20models%20require%20to%20input%20the%20entire%20videos%0Aduring%20training%20to%20encode%20temporal%20and%20positional%20information%20at%20once.%20However%2C%0Adue%20to%20limitations%20in%20GPU%20memory%2C%20the%20number%20of%20frames%20is%20typically%20restricted%0Ato%2016.%20To%20address%20this%2C%20this%20paper%20proposes%20a%20three-stage%20training%20strategy%0Awith%20progressively%20increasing%20frame%20numbers%20and%20reducing%20fine-tuning%20modules.%0AAdditionally%2C%20we%20introduce%20the%20Temporal%20E%20nhanced%20Motion%20Module%28TEMM%29%20to%20extend%0Athe%20capacity%20for%20encoding%20temporal%20and%20positional%20information%20up%20to%2036%20frames.%0AThe%20proposed%20LoopAnimate%2C%20which%20for%20the%20first%20time%20extends%20the%20single-pass%0Ageneration%20length%20of%20UNet-based%20video%20generation%20models%20to%2035%20frames%20while%0Amaintaining%20high-quality%20video%20generation.%20Experiments%20demonstrate%20that%0ALoopAnimate%20achieves%20state-of-the-art%20performance%20in%20both%20objective%20metrics%2C%0Asuch%20as%20fidelity%20and%20temporal%20consistency%2C%20and%20subjective%20evaluation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09172v2&entry.124074799=Read"},
{"title": "Pixel to Elevation: Learning to Predict Elevation Maps at Long Range\n  using Images for Autonomous Offroad Navigation", "author": "Chanyoung Chung and Georgios Georgakis and Patrick Spieler and Curtis Padgett and Shehryar Khattak", "abstract": "  Understanding terrain topology at long-range is crucial for the success of\noff-road robotic missions, especially when navigating at high-speeds. LiDAR\nsensors, which are currently heavily relied upon for geometric mapping, provide\nsparse measurements when mapping at greater distances. To address this\nchallenge, we present a novel learning-based approach capable of predicting\nterrain elevation maps at long-range using only onboard egocentric images in\nreal-time. Our proposed method is comprised of three main elements. First, a\ntransformer-based encoder is introduced that learns cross-view associations\nbetween the egocentric views and prior bird-eye-view elevation map predictions.\nSecond, an orientation-aware positional encoding is proposed to incorporate the\n3D vehicle pose information over complex unstructured terrain with multi-view\nvisual image features. Lastly, a history-augmented learn-able map embedding is\nproposed to achieve better temporal consistency between elevation map\npredictions to facilitate the downstream navigational tasks. We experimentally\nvalidate the applicability of our proposed approach for autonomous offroad\nrobotic navigation in complex and unstructured terrain using real-world offroad\ndriving data. Furthermore, the method is qualitatively and quantitatively\ncompared against the current state-of-the-art methods. Extensive field\nexperiments demonstrate that our method surpasses baseline models in accurately\npredicting terrain elevation while effectively capturing the overall terrain\ntopology at long-ranges. Finally, ablation studies are conducted to highlight\nand understand the effect of key components of the proposed approach and\nvalidate their suitability to improve offroad robotic navigation capabilities.\n", "link": "http://arxiv.org/abs/2401.17484v2", "date": "2024-04-16", "relevancy": 2.4766, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6114}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pixel%20to%20Elevation%3A%20Learning%20to%20Predict%20Elevation%20Maps%20at%20Long%20Range%0A%20%20using%20Images%20for%20Autonomous%20Offroad%20Navigation&body=Title%3A%20Pixel%20to%20Elevation%3A%20Learning%20to%20Predict%20Elevation%20Maps%20at%20Long%20Range%0A%20%20using%20Images%20for%20Autonomous%20Offroad%20Navigation%0AAuthor%3A%20Chanyoung%20Chung%20and%20Georgios%20Georgakis%20and%20Patrick%20Spieler%20and%20Curtis%20Padgett%20and%20Shehryar%20Khattak%0AAbstract%3A%20%20%20Understanding%20terrain%20topology%20at%20long-range%20is%20crucial%20for%20the%20success%20of%0Aoff-road%20robotic%20missions%2C%20especially%20when%20navigating%20at%20high-speeds.%20LiDAR%0Asensors%2C%20which%20are%20currently%20heavily%20relied%20upon%20for%20geometric%20mapping%2C%20provide%0Asparse%20measurements%20when%20mapping%20at%20greater%20distances.%20To%20address%20this%0Achallenge%2C%20we%20present%20a%20novel%20learning-based%20approach%20capable%20of%20predicting%0Aterrain%20elevation%20maps%20at%20long-range%20using%20only%20onboard%20egocentric%20images%20in%0Areal-time.%20Our%20proposed%20method%20is%20comprised%20of%20three%20main%20elements.%20First%2C%20a%0Atransformer-based%20encoder%20is%20introduced%20that%20learns%20cross-view%20associations%0Abetween%20the%20egocentric%20views%20and%20prior%20bird-eye-view%20elevation%20map%20predictions.%0ASecond%2C%20an%20orientation-aware%20positional%20encoding%20is%20proposed%20to%20incorporate%20the%0A3D%20vehicle%20pose%20information%20over%20complex%20unstructured%20terrain%20with%20multi-view%0Avisual%20image%20features.%20Lastly%2C%20a%20history-augmented%20learn-able%20map%20embedding%20is%0Aproposed%20to%20achieve%20better%20temporal%20consistency%20between%20elevation%20map%0Apredictions%20to%20facilitate%20the%20downstream%20navigational%20tasks.%20We%20experimentally%0Avalidate%20the%20applicability%20of%20our%20proposed%20approach%20for%20autonomous%20offroad%0Arobotic%20navigation%20in%20complex%20and%20unstructured%20terrain%20using%20real-world%20offroad%0Adriving%20data.%20Furthermore%2C%20the%20method%20is%20qualitatively%20and%20quantitatively%0Acompared%20against%20the%20current%20state-of-the-art%20methods.%20Extensive%20field%0Aexperiments%20demonstrate%20that%20our%20method%20surpasses%20baseline%20models%20in%20accurately%0Apredicting%20terrain%20elevation%20while%20effectively%20capturing%20the%20overall%20terrain%0Atopology%20at%20long-ranges.%20Finally%2C%20ablation%20studies%20are%20conducted%20to%20highlight%0Aand%20understand%20the%20effect%20of%20key%20components%20of%20the%20proposed%20approach%20and%0Avalidate%20their%20suitability%20to%20improve%20offroad%20robotic%20navigation%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17484v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel%20to%20Elevation%3A%20Learning%20to%20Predict%20Elevation%20Maps%20at%20Long%20Range%0A%20%20using%20Images%20for%20Autonomous%20Offroad%20Navigation&entry.906535625=Chanyoung%20Chung%20and%20Georgios%20Georgakis%20and%20Patrick%20Spieler%20and%20Curtis%20Padgett%20and%20Shehryar%20Khattak&entry.1292438233=%20%20Understanding%20terrain%20topology%20at%20long-range%20is%20crucial%20for%20the%20success%20of%0Aoff-road%20robotic%20missions%2C%20especially%20when%20navigating%20at%20high-speeds.%20LiDAR%0Asensors%2C%20which%20are%20currently%20heavily%20relied%20upon%20for%20geometric%20mapping%2C%20provide%0Asparse%20measurements%20when%20mapping%20at%20greater%20distances.%20To%20address%20this%0Achallenge%2C%20we%20present%20a%20novel%20learning-based%20approach%20capable%20of%20predicting%0Aterrain%20elevation%20maps%20at%20long-range%20using%20only%20onboard%20egocentric%20images%20in%0Areal-time.%20Our%20proposed%20method%20is%20comprised%20of%20three%20main%20elements.%20First%2C%20a%0Atransformer-based%20encoder%20is%20introduced%20that%20learns%20cross-view%20associations%0Abetween%20the%20egocentric%20views%20and%20prior%20bird-eye-view%20elevation%20map%20predictions.%0ASecond%2C%20an%20orientation-aware%20positional%20encoding%20is%20proposed%20to%20incorporate%20the%0A3D%20vehicle%20pose%20information%20over%20complex%20unstructured%20terrain%20with%20multi-view%0Avisual%20image%20features.%20Lastly%2C%20a%20history-augmented%20learn-able%20map%20embedding%20is%0Aproposed%20to%20achieve%20better%20temporal%20consistency%20between%20elevation%20map%0Apredictions%20to%20facilitate%20the%20downstream%20navigational%20tasks.%20We%20experimentally%0Avalidate%20the%20applicability%20of%20our%20proposed%20approach%20for%20autonomous%20offroad%0Arobotic%20navigation%20in%20complex%20and%20unstructured%20terrain%20using%20real-world%20offroad%0Adriving%20data.%20Furthermore%2C%20the%20method%20is%20qualitatively%20and%20quantitatively%0Acompared%20against%20the%20current%20state-of-the-art%20methods.%20Extensive%20field%0Aexperiments%20demonstrate%20that%20our%20method%20surpasses%20baseline%20models%20in%20accurately%0Apredicting%20terrain%20elevation%20while%20effectively%20capturing%20the%20overall%20terrain%0Atopology%20at%20long-ranges.%20Finally%2C%20ablation%20studies%20are%20conducted%20to%20highlight%0Aand%20understand%20the%20effect%20of%20key%20components%20of%20the%20proposed%20approach%20and%0Avalidate%20their%20suitability%20to%20improve%20offroad%20robotic%20navigation%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17484v2&entry.124074799=Read"},
{"title": "1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View\n  Reconstruction", "author": "Hang Du and Yaping Xue and Weidong Dai and Xuejun Yan and Jingjing Wang", "abstract": "  In this report, we present the 1st place solution for ICCV 2023 OmniObject3D\nChallenge: Sparse-View Reconstruction. The challenge aims to evaluate\napproaches for novel view synthesis and surface reconstruction using only a few\nposed images of each object. We utilize Pixel-NeRF as the basic model, and\napply depth supervision as well as coarse-to-fine positional encoding. The\nexperiments demonstrate the effectiveness of our approach in improving\nsparse-view reconstruction quality. We ranked first in the final test with a\nPSNR of 25.44614.\n", "link": "http://arxiv.org/abs/2404.10441v1", "date": "2024-04-16", "relevancy": 2.4479, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4949}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4803}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%201st%20Place%20Solution%20for%20ICCV%202023%20OmniObject3D%20Challenge%3A%20Sparse-View%0A%20%20Reconstruction&body=Title%3A%201st%20Place%20Solution%20for%20ICCV%202023%20OmniObject3D%20Challenge%3A%20Sparse-View%0A%20%20Reconstruction%0AAuthor%3A%20Hang%20Du%20and%20Yaping%20Xue%20and%20Weidong%20Dai%20and%20Xuejun%20Yan%20and%20Jingjing%20Wang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20the%201st%20place%20solution%20for%20ICCV%202023%20OmniObject3D%0AChallenge%3A%20Sparse-View%20Reconstruction.%20The%20challenge%20aims%20to%20evaluate%0Aapproaches%20for%20novel%20view%20synthesis%20and%20surface%20reconstruction%20using%20only%20a%20few%0Aposed%20images%20of%20each%20object.%20We%20utilize%20Pixel-NeRF%20as%20the%20basic%20model%2C%20and%0Aapply%20depth%20supervision%20as%20well%20as%20coarse-to-fine%20positional%20encoding.%20The%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20improving%0Asparse-view%20reconstruction%20quality.%20We%20ranked%20first%20in%20the%20final%20test%20with%20a%0APSNR%20of%2025.44614.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10441v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=1st%20Place%20Solution%20for%20ICCV%202023%20OmniObject3D%20Challenge%3A%20Sparse-View%0A%20%20Reconstruction&entry.906535625=Hang%20Du%20and%20Yaping%20Xue%20and%20Weidong%20Dai%20and%20Xuejun%20Yan%20and%20Jingjing%20Wang&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20the%201st%20place%20solution%20for%20ICCV%202023%20OmniObject3D%0AChallenge%3A%20Sparse-View%20Reconstruction.%20The%20challenge%20aims%20to%20evaluate%0Aapproaches%20for%20novel%20view%20synthesis%20and%20surface%20reconstruction%20using%20only%20a%20few%0Aposed%20images%20of%20each%20object.%20We%20utilize%20Pixel-NeRF%20as%20the%20basic%20model%2C%20and%0Aapply%20depth%20supervision%20as%20well%20as%20coarse-to-fine%20positional%20encoding.%20The%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20improving%0Asparse-view%20reconstruction%20quality.%20We%20ranked%20first%20in%20the%20final%20test%20with%20a%0APSNR%20of%2025.44614.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10441v1&entry.124074799=Read"},
{"title": "SCALE: Self-Correcting Visual Navigation for Mobile Robots via\n  Anti-Novelty Estimation", "author": "Chang Chen and Yuecheng Liu and Yuzheng Zhuang and Sitong Mao and Kaiwen Xue and Shunbo Zhou", "abstract": "  Although visual navigation has been extensively studied using deep\nreinforcement learning, online learning for real-world robots remains a\nchallenging task. Recent work directly learned from offline dataset to achieve\nbroader generalization in the real-world tasks, which, however, faces the\nout-of-distribution (OOD) issue and potential robot localization failures in a\ngiven map for unseen observation. This significantly drops the success rates\nand even induces collision. In this paper, we present a self-correcting visual\nnavigation method, SCALE, that can autonomously prevent the robot from the OOD\nsituations without human intervention. Specifically, we develop an image-goal\nconditioned offline reinforcement learning method based on implicit Q-learning\n(IQL). When facing OOD observation, our novel localization recovery method\ngenerates the potential future trajectories by learning from the navigation\naffordance, and estimates the future novelty via random network distillation\n(RND). A tailored cost function searches for the candidates with the least\nnovelty that can lead the robot to the familiar places. We collect offline data\nand conduct evaluation experiments in three real-world urban scenarios.\nExperiment results show that SCALE outperforms the previous state-of-the-art\nmethods for open-world navigation with a unique capability of localization\nrecovery, significantly reducing the need for human intervention. Code is\navailable at https://github.com/KubeEdge4Robotics/ScaleNav.\n", "link": "http://arxiv.org/abs/2404.10675v1", "date": "2024-04-16", "relevancy": 2.4009, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5942}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5929}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SCALE%3A%20Self-Correcting%20Visual%20Navigation%20for%20Mobile%20Robots%20via%0A%20%20Anti-Novelty%20Estimation&body=Title%3A%20SCALE%3A%20Self-Correcting%20Visual%20Navigation%20for%20Mobile%20Robots%20via%0A%20%20Anti-Novelty%20Estimation%0AAuthor%3A%20Chang%20Chen%20and%20Yuecheng%20Liu%20and%20Yuzheng%20Zhuang%20and%20Sitong%20Mao%20and%20Kaiwen%20Xue%20and%20Shunbo%20Zhou%0AAbstract%3A%20%20%20Although%20visual%20navigation%20has%20been%20extensively%20studied%20using%20deep%0Areinforcement%20learning%2C%20online%20learning%20for%20real-world%20robots%20remains%20a%0Achallenging%20task.%20Recent%20work%20directly%20learned%20from%20offline%20dataset%20to%20achieve%0Abroader%20generalization%20in%20the%20real-world%20tasks%2C%20which%2C%20however%2C%20faces%20the%0Aout-of-distribution%20%28OOD%29%20issue%20and%20potential%20robot%20localization%20failures%20in%20a%0Agiven%20map%20for%20unseen%20observation.%20This%20significantly%20drops%20the%20success%20rates%0Aand%20even%20induces%20collision.%20In%20this%20paper%2C%20we%20present%20a%20self-correcting%20visual%0Anavigation%20method%2C%20SCALE%2C%20that%20can%20autonomously%20prevent%20the%20robot%20from%20the%20OOD%0Asituations%20without%20human%20intervention.%20Specifically%2C%20we%20develop%20an%20image-goal%0Aconditioned%20offline%20reinforcement%20learning%20method%20based%20on%20implicit%20Q-learning%0A%28IQL%29.%20When%20facing%20OOD%20observation%2C%20our%20novel%20localization%20recovery%20method%0Agenerates%20the%20potential%20future%20trajectories%20by%20learning%20from%20the%20navigation%0Aaffordance%2C%20and%20estimates%20the%20future%20novelty%20via%20random%20network%20distillation%0A%28RND%29.%20A%20tailored%20cost%20function%20searches%20for%20the%20candidates%20with%20the%20least%0Anovelty%20that%20can%20lead%20the%20robot%20to%20the%20familiar%20places.%20We%20collect%20offline%20data%0Aand%20conduct%20evaluation%20experiments%20in%20three%20real-world%20urban%20scenarios.%0AExperiment%20results%20show%20that%20SCALE%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20for%20open-world%20navigation%20with%20a%20unique%20capability%20of%20localization%0Arecovery%2C%20significantly%20reducing%20the%20need%20for%20human%20intervention.%20Code%20is%0Aavailable%20at%20https%3A//github.com/KubeEdge4Robotics/ScaleNav.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10675v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCALE%3A%20Self-Correcting%20Visual%20Navigation%20for%20Mobile%20Robots%20via%0A%20%20Anti-Novelty%20Estimation&entry.906535625=Chang%20Chen%20and%20Yuecheng%20Liu%20and%20Yuzheng%20Zhuang%20and%20Sitong%20Mao%20and%20Kaiwen%20Xue%20and%20Shunbo%20Zhou&entry.1292438233=%20%20Although%20visual%20navigation%20has%20been%20extensively%20studied%20using%20deep%0Areinforcement%20learning%2C%20online%20learning%20for%20real-world%20robots%20remains%20a%0Achallenging%20task.%20Recent%20work%20directly%20learned%20from%20offline%20dataset%20to%20achieve%0Abroader%20generalization%20in%20the%20real-world%20tasks%2C%20which%2C%20however%2C%20faces%20the%0Aout-of-distribution%20%28OOD%29%20issue%20and%20potential%20robot%20localization%20failures%20in%20a%0Agiven%20map%20for%20unseen%20observation.%20This%20significantly%20drops%20the%20success%20rates%0Aand%20even%20induces%20collision.%20In%20this%20paper%2C%20we%20present%20a%20self-correcting%20visual%0Anavigation%20method%2C%20SCALE%2C%20that%20can%20autonomously%20prevent%20the%20robot%20from%20the%20OOD%0Asituations%20without%20human%20intervention.%20Specifically%2C%20we%20develop%20an%20image-goal%0Aconditioned%20offline%20reinforcement%20learning%20method%20based%20on%20implicit%20Q-learning%0A%28IQL%29.%20When%20facing%20OOD%20observation%2C%20our%20novel%20localization%20recovery%20method%0Agenerates%20the%20potential%20future%20trajectories%20by%20learning%20from%20the%20navigation%0Aaffordance%2C%20and%20estimates%20the%20future%20novelty%20via%20random%20network%20distillation%0A%28RND%29.%20A%20tailored%20cost%20function%20searches%20for%20the%20candidates%20with%20the%20least%0Anovelty%20that%20can%20lead%20the%20robot%20to%20the%20familiar%20places.%20We%20collect%20offline%20data%0Aand%20conduct%20evaluation%20experiments%20in%20three%20real-world%20urban%20scenarios.%0AExperiment%20results%20show%20that%20SCALE%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20for%20open-world%20navigation%20with%20a%20unique%20capability%20of%20localization%0Arecovery%2C%20significantly%20reducing%20the%20need%20for%20human%20intervention.%20Code%20is%0Aavailable%20at%20https%3A//github.com/KubeEdge4Robotics/ScaleNav.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10675v1&entry.124074799=Read"},
{"title": "Open-Pose 3D Zero-Shot Learning: Benchmark and Challenges", "author": "Weiguang Zhao and Guanyu Yang and Rui Zhang and Chenru Jiang and Chaolong Yang and Yuyao Yan and Amir Hussain and Kaizhu Huang", "abstract": "  With the explosive 3D data growth, the urgency of utilizing zero-shot\nlearning to facilitate data labeling becomes evident. Recently, methods\ntransferring language or language-image pre-training models like Contrastive\nLanguage-Image Pre-training (CLIP) to 3D vision have made significant progress\nin the 3D zero-shot classification task. These methods primarily focus on 3D\nobject classification with an aligned pose; such a setting is, however, rather\nrestrictive, which overlooks the recognition of 3D objects with open poses\ntypically encountered in real-world scenarios, such as an overturned chair or a\nlying teddy bear. To this end, we propose a more realistic and challenging\nscenario named open-pose 3D zero-shot classification, focusing on the\nrecognition of 3D objects regardless of their orientation. First, we revisit\nthe current research on 3D zero-shot classification, and propose two benchmark\ndatasets specifically designed for the open-pose setting. We empirically\nvalidate many of the most popular methods in the proposed open-pose benchmark.\nOur investigations reveal that most current 3D zero-shot classification models\nsuffer from poor performance, indicating a substantial exploration room towards\nthe new direction. Furthermore, we study a concise pipeline with an iterative\nangle refinement mechanism that automatically optimizes one ideal angle to\nclassify these open-pose 3D objects. In particular, to make validation more\ncompelling and not just limited to existing CLIP-based methods, we also pioneer\nthe exploration of knowledge transfer based on Diffusion models. While the\nproposed solutions can serve as a new benchmark for open-pose 3D zero-shot\nclassification, we discuss the complexities and challenges of this scenario\nthat remain for further research development. The code is available publicly at\nhttps://github.com/weiguangzhao/Diff-OP3D.\n", "link": "http://arxiv.org/abs/2312.07039v2", "date": "2024-04-16", "relevancy": 2.3543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6157}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5667}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Open-Pose%203D%20Zero-Shot%20Learning%3A%20Benchmark%20and%20Challenges&body=Title%3A%20Open-Pose%203D%20Zero-Shot%20Learning%3A%20Benchmark%20and%20Challenges%0AAuthor%3A%20Weiguang%20Zhao%20and%20Guanyu%20Yang%20and%20Rui%20Zhang%20and%20Chenru%20Jiang%20and%20Chaolong%20Yang%20and%20Yuyao%20Yan%20and%20Amir%20Hussain%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20With%20the%20explosive%203D%20data%20growth%2C%20the%20urgency%20of%20utilizing%20zero-shot%0Alearning%20to%20facilitate%20data%20labeling%20becomes%20evident.%20Recently%2C%20methods%0Atransferring%20language%20or%20language-image%20pre-training%20models%20like%20Contrastive%0ALanguage-Image%20Pre-training%20%28CLIP%29%20to%203D%20vision%20have%20made%20significant%20progress%0Ain%20the%203D%20zero-shot%20classification%20task.%20These%20methods%20primarily%20focus%20on%203D%0Aobject%20classification%20with%20an%20aligned%20pose%3B%20such%20a%20setting%20is%2C%20however%2C%20rather%0Arestrictive%2C%20which%20overlooks%20the%20recognition%20of%203D%20objects%20with%20open%20poses%0Atypically%20encountered%20in%20real-world%20scenarios%2C%20such%20as%20an%20overturned%20chair%20or%20a%0Alying%20teddy%20bear.%20To%20this%20end%2C%20we%20propose%20a%20more%20realistic%20and%20challenging%0Ascenario%20named%20open-pose%203D%20zero-shot%20classification%2C%20focusing%20on%20the%0Arecognition%20of%203D%20objects%20regardless%20of%20their%20orientation.%20First%2C%20we%20revisit%0Athe%20current%20research%20on%203D%20zero-shot%20classification%2C%20and%20propose%20two%20benchmark%0Adatasets%20specifically%20designed%20for%20the%20open-pose%20setting.%20We%20empirically%0Avalidate%20many%20of%20the%20most%20popular%20methods%20in%20the%20proposed%20open-pose%20benchmark.%0AOur%20investigations%20reveal%20that%20most%20current%203D%20zero-shot%20classification%20models%0Asuffer%20from%20poor%20performance%2C%20indicating%20a%20substantial%20exploration%20room%20towards%0Athe%20new%20direction.%20Furthermore%2C%20we%20study%20a%20concise%20pipeline%20with%20an%20iterative%0Aangle%20refinement%20mechanism%20that%20automatically%20optimizes%20one%20ideal%20angle%20to%0Aclassify%20these%20open-pose%203D%20objects.%20In%20particular%2C%20to%20make%20validation%20more%0Acompelling%20and%20not%20just%20limited%20to%20existing%20CLIP-based%20methods%2C%20we%20also%20pioneer%0Athe%20exploration%20of%20knowledge%20transfer%20based%20on%20Diffusion%20models.%20While%20the%0Aproposed%20solutions%20can%20serve%20as%20a%20new%20benchmark%20for%20open-pose%203D%20zero-shot%0Aclassification%2C%20we%20discuss%20the%20complexities%20and%20challenges%20of%20this%20scenario%0Athat%20remain%20for%20further%20research%20development.%20The%20code%20is%20available%20publicly%20at%0Ahttps%3A//github.com/weiguangzhao/Diff-OP3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07039v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Pose%203D%20Zero-Shot%20Learning%3A%20Benchmark%20and%20Challenges&entry.906535625=Weiguang%20Zhao%20and%20Guanyu%20Yang%20and%20Rui%20Zhang%20and%20Chenru%20Jiang%20and%20Chaolong%20Yang%20and%20Yuyao%20Yan%20and%20Amir%20Hussain%20and%20Kaizhu%20Huang&entry.1292438233=%20%20With%20the%20explosive%203D%20data%20growth%2C%20the%20urgency%20of%20utilizing%20zero-shot%0Alearning%20to%20facilitate%20data%20labeling%20becomes%20evident.%20Recently%2C%20methods%0Atransferring%20language%20or%20language-image%20pre-training%20models%20like%20Contrastive%0ALanguage-Image%20Pre-training%20%28CLIP%29%20to%203D%20vision%20have%20made%20significant%20progress%0Ain%20the%203D%20zero-shot%20classification%20task.%20These%20methods%20primarily%20focus%20on%203D%0Aobject%20classification%20with%20an%20aligned%20pose%3B%20such%20a%20setting%20is%2C%20however%2C%20rather%0Arestrictive%2C%20which%20overlooks%20the%20recognition%20of%203D%20objects%20with%20open%20poses%0Atypically%20encountered%20in%20real-world%20scenarios%2C%20such%20as%20an%20overturned%20chair%20or%20a%0Alying%20teddy%20bear.%20To%20this%20end%2C%20we%20propose%20a%20more%20realistic%20and%20challenging%0Ascenario%20named%20open-pose%203D%20zero-shot%20classification%2C%20focusing%20on%20the%0Arecognition%20of%203D%20objects%20regardless%20of%20their%20orientation.%20First%2C%20we%20revisit%0Athe%20current%20research%20on%203D%20zero-shot%20classification%2C%20and%20propose%20two%20benchmark%0Adatasets%20specifically%20designed%20for%20the%20open-pose%20setting.%20We%20empirically%0Avalidate%20many%20of%20the%20most%20popular%20methods%20in%20the%20proposed%20open-pose%20benchmark.%0AOur%20investigations%20reveal%20that%20most%20current%203D%20zero-shot%20classification%20models%0Asuffer%20from%20poor%20performance%2C%20indicating%20a%20substantial%20exploration%20room%20towards%0Athe%20new%20direction.%20Furthermore%2C%20we%20study%20a%20concise%20pipeline%20with%20an%20iterative%0Aangle%20refinement%20mechanism%20that%20automatically%20optimizes%20one%20ideal%20angle%20to%0Aclassify%20these%20open-pose%203D%20objects.%20In%20particular%2C%20to%20make%20validation%20more%0Acompelling%20and%20not%20just%20limited%20to%20existing%20CLIP-based%20methods%2C%20we%20also%20pioneer%0Athe%20exploration%20of%20knowledge%20transfer%20based%20on%20Diffusion%20models.%20While%20the%0Aproposed%20solutions%20can%20serve%20as%20a%20new%20benchmark%20for%20open-pose%203D%20zero-shot%0Aclassification%2C%20we%20discuss%20the%20complexities%20and%20challenges%20of%20this%20scenario%0Athat%20remain%20for%20further%20research%20development.%20The%20code%20is%20available%20publicly%20at%0Ahttps%3A//github.com/weiguangzhao/Diff-OP3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07039v2&entry.124074799=Read"},
{"title": "HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target\n  Interaction Prediction", "author": "Bin Liu and Siqi Wu and Jin Wang and Xin Deng and Ao Zhou", "abstract": "  The discovery of drug-target interactions (DTIs) plays a crucial role in\npharmaceutical development. The deep learning model achieves more accurate\nresults in DTI prediction due to its ability to extract robust and expressive\nfeatures from drug and target chemical structures. However, existing deep\nlearning methods typically generate drug features via aggregating molecular\natom representations, ignoring the chemical properties carried by motifs, i.e.,\nsubstructures of the molecular graph. The atom-drug double-level molecular\nrepresentation learning can not fully exploit structure information and fails\nto interpret the DTI mechanism from the motif perspective. In addition,\nsequential model-based target feature extraction either fuses limited\ncontextual information or requires expensive computational resources. To tackle\nthe above issues, we propose a hierarchical graph representation learning-based\nDTI prediction method (HiGraphDTI). Specifically, HiGraphDTI learns\nhierarchical drug representations from triple-level molecular graphs to\nthoroughly exploit chemical information embedded in atoms, motifs, and\nmolecules. Then, an attentional feature fusion module incorporates information\nfrom different receptive fields to extract expressive target features.Last, the\nhierarchical attention mechanism identifies crucial molecular segments, which\noffers complementary views for interpreting interaction mechanisms. The\nexperiment results not only demonstrate the superiority of HiGraphDTI to the\nstate-of-the-art methods, but also confirm the practical ability of our model\nin interaction interpretation and new DTI discovery.\n", "link": "http://arxiv.org/abs/2404.10561v1", "date": "2024-04-16", "relevancy": 2.3453, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4809}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4645}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4619}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HiGraphDTI%3A%20Hierarchical%20Graph%20Representation%20Learning%20for%20Drug-Target%0A%20%20Interaction%20Prediction&body=Title%3A%20HiGraphDTI%3A%20Hierarchical%20Graph%20Representation%20Learning%20for%20Drug-Target%0A%20%20Interaction%20Prediction%0AAuthor%3A%20Bin%20Liu%20and%20Siqi%20Wu%20and%20Jin%20Wang%20and%20Xin%20Deng%20and%20Ao%20Zhou%0AAbstract%3A%20%20%20The%20discovery%20of%20drug-target%20interactions%20%28DTIs%29%20plays%20a%20crucial%20role%20in%0Apharmaceutical%20development.%20The%20deep%20learning%20model%20achieves%20more%20accurate%0Aresults%20in%20DTI%20prediction%20due%20to%20its%20ability%20to%20extract%20robust%20and%20expressive%0Afeatures%20from%20drug%20and%20target%20chemical%20structures.%20However%2C%20existing%20deep%0Alearning%20methods%20typically%20generate%20drug%20features%20via%20aggregating%20molecular%0Aatom%20representations%2C%20ignoring%20the%20chemical%20properties%20carried%20by%20motifs%2C%20i.e.%2C%0Asubstructures%20of%20the%20molecular%20graph.%20The%20atom-drug%20double-level%20molecular%0Arepresentation%20learning%20can%20not%20fully%20exploit%20structure%20information%20and%20fails%0Ato%20interpret%20the%20DTI%20mechanism%20from%20the%20motif%20perspective.%20In%20addition%2C%0Asequential%20model-based%20target%20feature%20extraction%20either%20fuses%20limited%0Acontextual%20information%20or%20requires%20expensive%20computational%20resources.%20To%20tackle%0Athe%20above%20issues%2C%20we%20propose%20a%20hierarchical%20graph%20representation%20learning-based%0ADTI%20prediction%20method%20%28HiGraphDTI%29.%20Specifically%2C%20HiGraphDTI%20learns%0Ahierarchical%20drug%20representations%20from%20triple-level%20molecular%20graphs%20to%0Athoroughly%20exploit%20chemical%20information%20embedded%20in%20atoms%2C%20motifs%2C%20and%0Amolecules.%20Then%2C%20an%20attentional%20feature%20fusion%20module%20incorporates%20information%0Afrom%20different%20receptive%20fields%20to%20extract%20expressive%20target%20features.Last%2C%20the%0Ahierarchical%20attention%20mechanism%20identifies%20crucial%20molecular%20segments%2C%20which%0Aoffers%20complementary%20views%20for%20interpreting%20interaction%20mechanisms.%20The%0Aexperiment%20results%20not%20only%20demonstrate%20the%20superiority%20of%20HiGraphDTI%20to%20the%0Astate-of-the-art%20methods%2C%20but%20also%20confirm%20the%20practical%20ability%20of%20our%20model%0Ain%20interaction%20interpretation%20and%20new%20DTI%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10561v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiGraphDTI%3A%20Hierarchical%20Graph%20Representation%20Learning%20for%20Drug-Target%0A%20%20Interaction%20Prediction&entry.906535625=Bin%20Liu%20and%20Siqi%20Wu%20and%20Jin%20Wang%20and%20Xin%20Deng%20and%20Ao%20Zhou&entry.1292438233=%20%20The%20discovery%20of%20drug-target%20interactions%20%28DTIs%29%20plays%20a%20crucial%20role%20in%0Apharmaceutical%20development.%20The%20deep%20learning%20model%20achieves%20more%20accurate%0Aresults%20in%20DTI%20prediction%20due%20to%20its%20ability%20to%20extract%20robust%20and%20expressive%0Afeatures%20from%20drug%20and%20target%20chemical%20structures.%20However%2C%20existing%20deep%0Alearning%20methods%20typically%20generate%20drug%20features%20via%20aggregating%20molecular%0Aatom%20representations%2C%20ignoring%20the%20chemical%20properties%20carried%20by%20motifs%2C%20i.e.%2C%0Asubstructures%20of%20the%20molecular%20graph.%20The%20atom-drug%20double-level%20molecular%0Arepresentation%20learning%20can%20not%20fully%20exploit%20structure%20information%20and%20fails%0Ato%20interpret%20the%20DTI%20mechanism%20from%20the%20motif%20perspective.%20In%20addition%2C%0Asequential%20model-based%20target%20feature%20extraction%20either%20fuses%20limited%0Acontextual%20information%20or%20requires%20expensive%20computational%20resources.%20To%20tackle%0Athe%20above%20issues%2C%20we%20propose%20a%20hierarchical%20graph%20representation%20learning-based%0ADTI%20prediction%20method%20%28HiGraphDTI%29.%20Specifically%2C%20HiGraphDTI%20learns%0Ahierarchical%20drug%20representations%20from%20triple-level%20molecular%20graphs%20to%0Athoroughly%20exploit%20chemical%20information%20embedded%20in%20atoms%2C%20motifs%2C%20and%0Amolecules.%20Then%2C%20an%20attentional%20feature%20fusion%20module%20incorporates%20information%0Afrom%20different%20receptive%20fields%20to%20extract%20expressive%20target%20features.Last%2C%20the%0Ahierarchical%20attention%20mechanism%20identifies%20crucial%20molecular%20segments%2C%20which%0Aoffers%20complementary%20views%20for%20interpreting%20interaction%20mechanisms.%20The%0Aexperiment%20results%20not%20only%20demonstrate%20the%20superiority%20of%20HiGraphDTI%20to%20the%0Astate-of-the-art%20methods%2C%20but%20also%20confirm%20the%20practical%20ability%20of%20our%20model%0Ain%20interaction%20interpretation%20and%20new%20DTI%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10561v1&entry.124074799=Read"},
{"title": "UIVNAV: Underwater Information-driven Vision-based Navigation via\n  Imitation Learning", "author": "Xiaomin Lin and Nare Karapetyan and Kaustubh Joshi and Tianchen Liu and Nikhil Chopra and Miao Yu and Pratap Tokekar and Yiannis Aloimonos", "abstract": "  Autonomous navigation in the underwater environment is challenging due to\nlimited visibility, dynamic changes, and the lack of a cost-efficient accurate\nlocalization system. We introduce UIVNav, a novel end-to-end underwater\nnavigation solution designed to drive robots over Objects of Interest (OOI)\nwhile avoiding obstacles, without relying on localization. UIVNav uses\nimitation learning and is inspired by the navigation strategies used by human\ndivers who do not rely on localization. UIVNav consists of the following\nphases: (1) generating an intermediate representation (IR), and (2) training\nthe navigation policy based on human-labeled IR. By training the navigation\npolicy on IR instead of raw data, the second phase is domain-invariant -- the\nnavigation policy does not need to be retrained if the domain or the OOI\nchanges. We show this by deploying the same navigation policy for surveying two\ndifferent OOIs, oyster and rock reefs, in two different domains, simulation,\nand a real pool. We compared our method with complete coverage and random walk\nmethods which showed that our method is more efficient in gathering information\nfor OOIs while also avoiding obstacles. The results show that UIVNav chooses to\nvisit the areas with larger area sizes of oysters or rocks with no prior\ninformation about the environment or localization. Moreover, a robot using\nUIVNav compared to complete coverage method surveys on average 36% more oysters\nwhen traveling the same distances. We also demonstrate the feasibility of\nreal-time deployment of UIVNavin pool experiments with BlueROV underwater robot\nfor surveying a bed of oyster shells.\n", "link": "http://arxiv.org/abs/2309.08806v2", "date": "2024-04-16", "relevancy": 2.3315, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5977}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.588}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UIVNAV%3A%20Underwater%20Information-driven%20Vision-based%20Navigation%20via%0A%20%20Imitation%20Learning&body=Title%3A%20UIVNAV%3A%20Underwater%20Information-driven%20Vision-based%20Navigation%20via%0A%20%20Imitation%20Learning%0AAuthor%3A%20Xiaomin%20Lin%20and%20Nare%20Karapetyan%20and%20Kaustubh%20Joshi%20and%20Tianchen%20Liu%20and%20Nikhil%20Chopra%20and%20Miao%20Yu%20and%20Pratap%20Tokekar%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20Autonomous%20navigation%20in%20the%20underwater%20environment%20is%20challenging%20due%20to%0Alimited%20visibility%2C%20dynamic%20changes%2C%20and%20the%20lack%20of%20a%20cost-efficient%20accurate%0Alocalization%20system.%20We%20introduce%20UIVNav%2C%20a%20novel%20end-to-end%20underwater%0Anavigation%20solution%20designed%20to%20drive%20robots%20over%20Objects%20of%20Interest%20%28OOI%29%0Awhile%20avoiding%20obstacles%2C%20without%20relying%20on%20localization.%20UIVNav%20uses%0Aimitation%20learning%20and%20is%20inspired%20by%20the%20navigation%20strategies%20used%20by%20human%0Adivers%20who%20do%20not%20rely%20on%20localization.%20UIVNav%20consists%20of%20the%20following%0Aphases%3A%20%281%29%20generating%20an%20intermediate%20representation%20%28IR%29%2C%20and%20%282%29%20training%0Athe%20navigation%20policy%20based%20on%20human-labeled%20IR.%20By%20training%20the%20navigation%0Apolicy%20on%20IR%20instead%20of%20raw%20data%2C%20the%20second%20phase%20is%20domain-invariant%20--%20the%0Anavigation%20policy%20does%20not%20need%20to%20be%20retrained%20if%20the%20domain%20or%20the%20OOI%0Achanges.%20We%20show%20this%20by%20deploying%20the%20same%20navigation%20policy%20for%20surveying%20two%0Adifferent%20OOIs%2C%20oyster%20and%20rock%20reefs%2C%20in%20two%20different%20domains%2C%20simulation%2C%0Aand%20a%20real%20pool.%20We%20compared%20our%20method%20with%20complete%20coverage%20and%20random%20walk%0Amethods%20which%20showed%20that%20our%20method%20is%20more%20efficient%20in%20gathering%20information%0Afor%20OOIs%20while%20also%20avoiding%20obstacles.%20The%20results%20show%20that%20UIVNav%20chooses%20to%0Avisit%20the%20areas%20with%20larger%20area%20sizes%20of%20oysters%20or%20rocks%20with%20no%20prior%0Ainformation%20about%20the%20environment%20or%20localization.%20Moreover%2C%20a%20robot%20using%0AUIVNav%20compared%20to%20complete%20coverage%20method%20surveys%20on%20average%2036%25%20more%20oysters%0Awhen%20traveling%20the%20same%20distances.%20We%20also%20demonstrate%20the%20feasibility%20of%0Areal-time%20deployment%20of%20UIVNavin%20pool%20experiments%20with%20BlueROV%20underwater%20robot%0Afor%20surveying%20a%20bed%20of%20oyster%20shells.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08806v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UIVNAV%3A%20Underwater%20Information-driven%20Vision-based%20Navigation%20via%0A%20%20Imitation%20Learning&entry.906535625=Xiaomin%20Lin%20and%20Nare%20Karapetyan%20and%20Kaustubh%20Joshi%20and%20Tianchen%20Liu%20and%20Nikhil%20Chopra%20and%20Miao%20Yu%20and%20Pratap%20Tokekar%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20Autonomous%20navigation%20in%20the%20underwater%20environment%20is%20challenging%20due%20to%0Alimited%20visibility%2C%20dynamic%20changes%2C%20and%20the%20lack%20of%20a%20cost-efficient%20accurate%0Alocalization%20system.%20We%20introduce%20UIVNav%2C%20a%20novel%20end-to-end%20underwater%0Anavigation%20solution%20designed%20to%20drive%20robots%20over%20Objects%20of%20Interest%20%28OOI%29%0Awhile%20avoiding%20obstacles%2C%20without%20relying%20on%20localization.%20UIVNav%20uses%0Aimitation%20learning%20and%20is%20inspired%20by%20the%20navigation%20strategies%20used%20by%20human%0Adivers%20who%20do%20not%20rely%20on%20localization.%20UIVNav%20consists%20of%20the%20following%0Aphases%3A%20%281%29%20generating%20an%20intermediate%20representation%20%28IR%29%2C%20and%20%282%29%20training%0Athe%20navigation%20policy%20based%20on%20human-labeled%20IR.%20By%20training%20the%20navigation%0Apolicy%20on%20IR%20instead%20of%20raw%20data%2C%20the%20second%20phase%20is%20domain-invariant%20--%20the%0Anavigation%20policy%20does%20not%20need%20to%20be%20retrained%20if%20the%20domain%20or%20the%20OOI%0Achanges.%20We%20show%20this%20by%20deploying%20the%20same%20navigation%20policy%20for%20surveying%20two%0Adifferent%20OOIs%2C%20oyster%20and%20rock%20reefs%2C%20in%20two%20different%20domains%2C%20simulation%2C%0Aand%20a%20real%20pool.%20We%20compared%20our%20method%20with%20complete%20coverage%20and%20random%20walk%0Amethods%20which%20showed%20that%20our%20method%20is%20more%20efficient%20in%20gathering%20information%0Afor%20OOIs%20while%20also%20avoiding%20obstacles.%20The%20results%20show%20that%20UIVNav%20chooses%20to%0Avisit%20the%20areas%20with%20larger%20area%20sizes%20of%20oysters%20or%20rocks%20with%20no%20prior%0Ainformation%20about%20the%20environment%20or%20localization.%20Moreover%2C%20a%20robot%20using%0AUIVNav%20compared%20to%20complete%20coverage%20method%20surveys%20on%20average%2036%25%20more%20oysters%0Awhen%20traveling%20the%20same%20distances.%20We%20also%20demonstrate%20the%20feasibility%20of%0Areal-time%20deployment%20of%20UIVNavin%20pool%20experiments%20with%20BlueROV%20underwater%20robot%0Afor%20surveying%20a%20bed%20of%20oyster%20shells.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08806v2&entry.124074799=Read"},
{"title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning", "author": "Jonathan Lebensold and Maziar Sanjabi and Pietro Astolfi and Adriana Romero-Soriano and Kamalika Chaudhuri and Mike Rabbat and Chuan Guo", "abstract": "  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n", "link": "http://arxiv.org/abs/2403.14421v2", "date": "2024-04-16", "relevancy": 2.3218, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6104}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5701}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&body=Title%3A%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning%0AAuthor%3A%20Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14421v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&entry.906535625=Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14421v2&entry.124074799=Read"},
{"title": "VFLAIR: A Research Library and Benchmark for Vertical Federated Learning", "author": "Tianyuan Zou and Zixuan Gu and Yu He and Hideaki Takahashi and Yang Liu and Ya-Qin Zhang", "abstract": "  Vertical Federated Learning (VFL) has emerged as a collaborative training\nparadigm that allows participants with different features of the same group of\nusers to accomplish cooperative training without exposing their raw data or\nmodel parameters. VFL has gained significant attention for its research\npotential and real-world applications in recent years, but still faces\nsubstantial challenges, such as in defending various kinds of data inference\nand backdoor attacks. Moreover, most of existing VFL projects are\nindustry-facing and not easily used for keeping track of the current research\nprogress. To address this need, we present an extensible and lightweight VFL\nframework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which\nsupports VFL training with a variety of models, datasets and protocols, along\nwith standardized modules for comprehensive evaluations of attacks and defense\nstrategies. We also benchmark 11 attacks and 8 defenses performance under\ndifferent communication and model partition settings and draw concrete insights\nand recommendations on the choice of defense strategies for different practical\nVFL deployment scenarios.\n", "link": "http://arxiv.org/abs/2310.09827v2", "date": "2024-04-16", "relevancy": 2.3218, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4568}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VFLAIR%3A%20A%20Research%20Library%20and%20Benchmark%20for%20Vertical%20Federated%20Learning&body=Title%3A%20VFLAIR%3A%20A%20Research%20Library%20and%20Benchmark%20for%20Vertical%20Federated%20Learning%0AAuthor%3A%20Tianyuan%20Zou%20and%20Zixuan%20Gu%20and%20Yu%20He%20and%20Hideaki%20Takahashi%20and%20Yang%20Liu%20and%20Ya-Qin%20Zhang%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20has%20emerged%20as%20a%20collaborative%20training%0Aparadigm%20that%20allows%20participants%20with%20different%20features%20of%20the%20same%20group%20of%0Ausers%20to%20accomplish%20cooperative%20training%20without%20exposing%20their%20raw%20data%20or%0Amodel%20parameters.%20VFL%20has%20gained%20significant%20attention%20for%20its%20research%0Apotential%20and%20real-world%20applications%20in%20recent%20years%2C%20but%20still%20faces%0Asubstantial%20challenges%2C%20such%20as%20in%20defending%20various%20kinds%20of%20data%20inference%0Aand%20backdoor%20attacks.%20Moreover%2C%20most%20of%20existing%20VFL%20projects%20are%0Aindustry-facing%20and%20not%20easily%20used%20for%20keeping%20track%20of%20the%20current%20research%0Aprogress.%20To%20address%20this%20need%2C%20we%20present%20an%20extensible%20and%20lightweight%20VFL%0Aframework%20VFLAIR%20%28available%20at%20https%3A//github.com/FLAIR-THU/VFLAIR%29%2C%20which%0Asupports%20VFL%20training%20with%20a%20variety%20of%20models%2C%20datasets%20and%20protocols%2C%20along%0Awith%20standardized%20modules%20for%20comprehensive%20evaluations%20of%20attacks%20and%20defense%0Astrategies.%20We%20also%20benchmark%2011%20attacks%20and%208%20defenses%20performance%20under%0Adifferent%20communication%20and%20model%20partition%20settings%20and%20draw%20concrete%20insights%0Aand%20recommendations%20on%20the%20choice%20of%20defense%20strategies%20for%20different%20practical%0AVFL%20deployment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09827v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFLAIR%3A%20A%20Research%20Library%20and%20Benchmark%20for%20Vertical%20Federated%20Learning&entry.906535625=Tianyuan%20Zou%20and%20Zixuan%20Gu%20and%20Yu%20He%20and%20Hideaki%20Takahashi%20and%20Yang%20Liu%20and%20Ya-Qin%20Zhang&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20has%20emerged%20as%20a%20collaborative%20training%0Aparadigm%20that%20allows%20participants%20with%20different%20features%20of%20the%20same%20group%20of%0Ausers%20to%20accomplish%20cooperative%20training%20without%20exposing%20their%20raw%20data%20or%0Amodel%20parameters.%20VFL%20has%20gained%20significant%20attention%20for%20its%20research%0Apotential%20and%20real-world%20applications%20in%20recent%20years%2C%20but%20still%20faces%0Asubstantial%20challenges%2C%20such%20as%20in%20defending%20various%20kinds%20of%20data%20inference%0Aand%20backdoor%20attacks.%20Moreover%2C%20most%20of%20existing%20VFL%20projects%20are%0Aindustry-facing%20and%20not%20easily%20used%20for%20keeping%20track%20of%20the%20current%20research%0Aprogress.%20To%20address%20this%20need%2C%20we%20present%20an%20extensible%20and%20lightweight%20VFL%0Aframework%20VFLAIR%20%28available%20at%20https%3A//github.com/FLAIR-THU/VFLAIR%29%2C%20which%0Asupports%20VFL%20training%20with%20a%20variety%20of%20models%2C%20datasets%20and%20protocols%2C%20along%0Awith%20standardized%20modules%20for%20comprehensive%20evaluations%20of%20attacks%20and%20defense%0Astrategies.%20We%20also%20benchmark%2011%20attacks%20and%208%20defenses%20performance%20under%0Adifferent%20communication%20and%20model%20partition%20settings%20and%20draw%20concrete%20insights%0Aand%20recommendations%20on%20the%20choice%20of%20defense%20strategies%20for%20different%20practical%0AVFL%20deployment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09827v2&entry.124074799=Read"},
{"title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model\n  Distillation", "author": "Ardian Umam and Cheng-Kun Yang and Min-Hung Chen and Jen-Hui Chuang and Yen-Yu Lin", "abstract": "  This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inconsistent 2D predictions by VLMs, and the lack of knowledge\naccumulation across different 3D shapes. PartDistill consists of a teacher\nnetwork that uses a VLM to make 2D predictions and a student network that\nlearns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D segmentation. Moreover, PartDistill\ncan exploit generative models that facilitate effortless 3D shape creation for\ngenerating knowledge sources to be distilled. Through extensive experiments,\nPartDistill boosts the existing methods with substantial margins on widely used\nShapeNetPart and PartNetE datasets, by more than 15% and 12% higher mIoU\nscores, respectively. The code for this work is available at\nhttps://github.com/ardianumam/PartDistill.\n", "link": "http://arxiv.org/abs/2312.04016v2", "date": "2024-04-16", "relevancy": 2.3113, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5391}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PartDistill%3A%203D%20Shape%20Part%20Segmentation%20by%20Vision-Language%20Model%0A%20%20Distillation&body=Title%3A%20PartDistill%3A%203D%20Shape%20Part%20Segmentation%20by%20Vision-Language%20Model%0A%20%20Distillation%0AAuthor%3A%20Ardian%20Umam%20and%20Cheng-Kun%20Yang%20and%20Min-Hung%20Chen%20and%20Jen-Hui%20Chuang%20and%20Yen-Yu%20Lin%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20cross-modal%20distillation%20framework%2C%20PartDistill%2C%20which%0Atransfers%202D%20knowledge%20from%20vision-language%20models%20%28VLMs%29%20to%20facilitate%203D%0Ashape%20part%20segmentation.%20PartDistill%20addresses%20three%20major%20challenges%20in%20this%0Atask%3A%20the%20lack%20of%203D%20segmentation%20in%20invisible%20or%20undetected%20regions%20in%20the%202D%0Aprojections%2C%20inconsistent%202D%20predictions%20by%20VLMs%2C%20and%20the%20lack%20of%20knowledge%0Aaccumulation%20across%20different%203D%20shapes.%20PartDistill%20consists%20of%20a%20teacher%0Anetwork%20that%20uses%20a%20VLM%20to%20make%202D%20predictions%20and%20a%20student%20network%20that%0Alearns%20from%20the%202D%20predictions%20while%20extracting%20geometrical%20features%20from%0Amultiple%203D%20shapes%20to%20carry%20out%203D%20part%20segmentation.%20A%20bi-directional%0Adistillation%2C%20including%20forward%20and%20backward%20distillations%2C%20is%20carried%20out%0Awithin%20the%20framework%2C%20where%20the%20former%20forward%20distills%20the%202D%20predictions%20to%0Athe%20student%20network%2C%20and%20the%20latter%20improves%20the%20quality%20of%20the%202D%20predictions%2C%0Awhich%20subsequently%20enhances%20the%20final%203D%20segmentation.%20Moreover%2C%20PartDistill%0Acan%20exploit%20generative%20models%20that%20facilitate%20effortless%203D%20shape%20creation%20for%0Agenerating%20knowledge%20sources%20to%20be%20distilled.%20Through%20extensive%20experiments%2C%0APartDistill%20boosts%20the%20existing%20methods%20with%20substantial%20margins%20on%20widely%20used%0AShapeNetPart%20and%20PartNetE%20datasets%2C%20by%20more%20than%2015%25%20and%2012%25%20higher%20mIoU%0Ascores%2C%20respectively.%20The%20code%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/ardianumam/PartDistill.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04016v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartDistill%3A%203D%20Shape%20Part%20Segmentation%20by%20Vision-Language%20Model%0A%20%20Distillation&entry.906535625=Ardian%20Umam%20and%20Cheng-Kun%20Yang%20and%20Min-Hung%20Chen%20and%20Jen-Hui%20Chuang%20and%20Yen-Yu%20Lin&entry.1292438233=%20%20This%20paper%20proposes%20a%20cross-modal%20distillation%20framework%2C%20PartDistill%2C%20which%0Atransfers%202D%20knowledge%20from%20vision-language%20models%20%28VLMs%29%20to%20facilitate%203D%0Ashape%20part%20segmentation.%20PartDistill%20addresses%20three%20major%20challenges%20in%20this%0Atask%3A%20the%20lack%20of%203D%20segmentation%20in%20invisible%20or%20undetected%20regions%20in%20the%202D%0Aprojections%2C%20inconsistent%202D%20predictions%20by%20VLMs%2C%20and%20the%20lack%20of%20knowledge%0Aaccumulation%20across%20different%203D%20shapes.%20PartDistill%20consists%20of%20a%20teacher%0Anetwork%20that%20uses%20a%20VLM%20to%20make%202D%20predictions%20and%20a%20student%20network%20that%0Alearns%20from%20the%202D%20predictions%20while%20extracting%20geometrical%20features%20from%0Amultiple%203D%20shapes%20to%20carry%20out%203D%20part%20segmentation.%20A%20bi-directional%0Adistillation%2C%20including%20forward%20and%20backward%20distillations%2C%20is%20carried%20out%0Awithin%20the%20framework%2C%20where%20the%20former%20forward%20distills%20the%202D%20predictions%20to%0Athe%20student%20network%2C%20and%20the%20latter%20improves%20the%20quality%20of%20the%202D%20predictions%2C%0Awhich%20subsequently%20enhances%20the%20final%203D%20segmentation.%20Moreover%2C%20PartDistill%0Acan%20exploit%20generative%20models%20that%20facilitate%20effortless%203D%20shape%20creation%20for%0Agenerating%20knowledge%20sources%20to%20be%20distilled.%20Through%20extensive%20experiments%2C%0APartDistill%20boosts%20the%20existing%20methods%20with%20substantial%20margins%20on%20widely%20used%0AShapeNetPart%20and%20PartNetE%20datasets%2C%20by%20more%20than%2015%25%20and%2012%25%20higher%20mIoU%0Ascores%2C%20respectively.%20The%20code%20for%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/ardianumam/PartDistill.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04016v2&entry.124074799=Read"},
{"title": "Gemma: Open Models Based on Gemini Research and Technology", "author": " Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and L\u00e9onard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am\u00e9lie H\u00e9liou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl\u00e9ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Miku\u0142a and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl\u00e9ment Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy", "abstract": "  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n", "link": "http://arxiv.org/abs/2403.08295v4", "date": "2024-04-16", "relevancy": 2.3049, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&body=Title%3A%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology%0AAuthor%3A%20%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%0AAbstract%3A%20%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08295v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&entry.906535625=%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy&entry.1292438233=%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08295v4&entry.124074799=Read"},
{"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "author": "Pengyu Cheng and Tianhao Hu and Han Xu and Zhisong Zhang and Yong Dai and Lei Han and Nan Du", "abstract": "  We explore the self-play training procedure of large language models (LLMs)\nin a two-player adversarial language game called Adversarial Taboo. In this\ngame, an attacker and a defender communicate with respect to a target word only\nvisible to the attacker. The attacker aims to induce the defender to utter the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players should have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Play in this Adversarial language Game (SPAG). With this goal, we let LLMs\nact as the attacker and play with a copy of itself as the defender on an\nextensive range of target words. Through reinforcement learning on the game\noutcomes, we observe that the LLMs' performance uniformly improves on a broad\nrange of reasoning benchmarks. Furthermore, iteratively adopting this self-play\nprocess can continuously promote LLM's reasoning ability. The code is at\nhttps://github.com/Linear95/SPAG.\n", "link": "http://arxiv.org/abs/2404.10642v1", "date": "2024-04-16", "relevancy": 2.2958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4695}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-playing%20Adversarial%20Language%20Game%20Enhances%20LLM%20Reasoning&body=Title%3A%20Self-playing%20Adversarial%20Language%20Game%20Enhances%20LLM%20Reasoning%0AAuthor%3A%20Pengyu%20Cheng%20and%20Tianhao%20Hu%20and%20Han%20Xu%20and%20Zhisong%20Zhang%20and%20Yong%20Dai%20and%20Lei%20Han%20and%20Nan%20Du%0AAbstract%3A%20%20%20We%20explore%20the%20self-play%20training%20procedure%20of%20large%20language%20models%20%28LLMs%29%0Ain%20a%20two-player%20adversarial%20language%20game%20called%20Adversarial%20Taboo.%20In%20this%0Agame%2C%20an%20attacker%20and%20a%20defender%20communicate%20with%20respect%20to%20a%20target%20word%20only%0Avisible%20to%20the%20attacker.%20The%20attacker%20aims%20to%20induce%20the%20defender%20to%20utter%20the%0Atarget%20word%20unconsciously%2C%20while%20the%20defender%20tries%20to%20infer%20the%20target%20word%0Afrom%20the%20attacker%27s%20utterances.%20To%20win%20the%20game%2C%20both%20players%20should%20have%0Asufficient%20knowledge%20about%20the%20target%20word%20and%20high-level%20reasoning%20ability%20to%0Ainfer%20and%20express%20in%20this%20information-reserved%20conversation.%20Hence%2C%20we%20are%0Acurious%20about%20whether%20LLMs%27%20reasoning%20ability%20can%20be%20further%20enhanced%20by%0ASelf-Play%20in%20this%20Adversarial%20language%20Game%20%28SPAG%29.%20With%20this%20goal%2C%20we%20let%20LLMs%0Aact%20as%20the%20attacker%20and%20play%20with%20a%20copy%20of%20itself%20as%20the%20defender%20on%20an%0Aextensive%20range%20of%20target%20words.%20Through%20reinforcement%20learning%20on%20the%20game%0Aoutcomes%2C%20we%20observe%20that%20the%20LLMs%27%20performance%20uniformly%20improves%20on%20a%20broad%0Arange%20of%20reasoning%20benchmarks.%20Furthermore%2C%20iteratively%20adopting%20this%20self-play%0Aprocess%20can%20continuously%20promote%20LLM%27s%20reasoning%20ability.%20The%20code%20is%20at%0Ahttps%3A//github.com/Linear95/SPAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10642v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-playing%20Adversarial%20Language%20Game%20Enhances%20LLM%20Reasoning&entry.906535625=Pengyu%20Cheng%20and%20Tianhao%20Hu%20and%20Han%20Xu%20and%20Zhisong%20Zhang%20and%20Yong%20Dai%20and%20Lei%20Han%20and%20Nan%20Du&entry.1292438233=%20%20We%20explore%20the%20self-play%20training%20procedure%20of%20large%20language%20models%20%28LLMs%29%0Ain%20a%20two-player%20adversarial%20language%20game%20called%20Adversarial%20Taboo.%20In%20this%0Agame%2C%20an%20attacker%20and%20a%20defender%20communicate%20with%20respect%20to%20a%20target%20word%20only%0Avisible%20to%20the%20attacker.%20The%20attacker%20aims%20to%20induce%20the%20defender%20to%20utter%20the%0Atarget%20word%20unconsciously%2C%20while%20the%20defender%20tries%20to%20infer%20the%20target%20word%0Afrom%20the%20attacker%27s%20utterances.%20To%20win%20the%20game%2C%20both%20players%20should%20have%0Asufficient%20knowledge%20about%20the%20target%20word%20and%20high-level%20reasoning%20ability%20to%0Ainfer%20and%20express%20in%20this%20information-reserved%20conversation.%20Hence%2C%20we%20are%0Acurious%20about%20whether%20LLMs%27%20reasoning%20ability%20can%20be%20further%20enhanced%20by%0ASelf-Play%20in%20this%20Adversarial%20language%20Game%20%28SPAG%29.%20With%20this%20goal%2C%20we%20let%20LLMs%0Aact%20as%20the%20attacker%20and%20play%20with%20a%20copy%20of%20itself%20as%20the%20defender%20on%20an%0Aextensive%20range%20of%20target%20words.%20Through%20reinforcement%20learning%20on%20the%20game%0Aoutcomes%2C%20we%20observe%20that%20the%20LLMs%27%20performance%20uniformly%20improves%20on%20a%20broad%0Arange%20of%20reasoning%20benchmarks.%20Furthermore%2C%20iteratively%20adopting%20this%20self-play%0Aprocess%20can%20continuously%20promote%20LLM%27s%20reasoning%20ability.%20The%20code%20is%20at%0Ahttps%3A//github.com/Linear95/SPAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10642v1&entry.124074799=Read"},
{"title": "Continuous Control Reinforcement Learning: Distributed Distributional\n  DrQ Algorithms", "author": "Zehao Zhou", "abstract": "  Distributed Distributional DrQ is a model-free and off-policy RL algorithm\nfor continuous control tasks based on the state and observation of the agent,\nwhich is an actor-critic method with the data-augmentation and the\ndistributional perspective of critic value function. Aim to learn to control\nthe agent and master some tasks in a high-dimensional continuous space. DrQ-v2\nuses DDPG as the backbone and achieves out-performance in various continuous\ncontrol tasks. Here Distributed Distributional DrQ uses Distributed\nDistributional DDPG as the backbone, and this modification aims to achieve\nbetter performance in some hard continuous control tasks through the better\nexpression ability of distributional value function and distributed actor\npolicies.\n", "link": "http://arxiv.org/abs/2404.10645v1", "date": "2024-04-16", "relevancy": 2.2685, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4831}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4449}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4331}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continuous%20Control%20Reinforcement%20Learning%3A%20Distributed%20Distributional%0A%20%20DrQ%20Algorithms&body=Title%3A%20Continuous%20Control%20Reinforcement%20Learning%3A%20Distributed%20Distributional%0A%20%20DrQ%20Algorithms%0AAuthor%3A%20Zehao%20Zhou%0AAbstract%3A%20%20%20Distributed%20Distributional%20DrQ%20is%20a%20model-free%20and%20off-policy%20RL%20algorithm%0Afor%20continuous%20control%20tasks%20based%20on%20the%20state%20and%20observation%20of%20the%20agent%2C%0Awhich%20is%20an%20actor-critic%20method%20with%20the%20data-augmentation%20and%20the%0Adistributional%20perspective%20of%20critic%20value%20function.%20Aim%20to%20learn%20to%20control%0Athe%20agent%20and%20master%20some%20tasks%20in%20a%20high-dimensional%20continuous%20space.%20DrQ-v2%0Auses%20DDPG%20as%20the%20backbone%20and%20achieves%20out-performance%20in%20various%20continuous%0Acontrol%20tasks.%20Here%20Distributed%20Distributional%20DrQ%20uses%20Distributed%0ADistributional%20DDPG%20as%20the%20backbone%2C%20and%20this%20modification%20aims%20to%20achieve%0Abetter%20performance%20in%20some%20hard%20continuous%20control%20tasks%20through%20the%20better%0Aexpression%20ability%20of%20distributional%20value%20function%20and%20distributed%20actor%0Apolicies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10645v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Control%20Reinforcement%20Learning%3A%20Distributed%20Distributional%0A%20%20DrQ%20Algorithms&entry.906535625=Zehao%20Zhou&entry.1292438233=%20%20Distributed%20Distributional%20DrQ%20is%20a%20model-free%20and%20off-policy%20RL%20algorithm%0Afor%20continuous%20control%20tasks%20based%20on%20the%20state%20and%20observation%20of%20the%20agent%2C%0Awhich%20is%20an%20actor-critic%20method%20with%20the%20data-augmentation%20and%20the%0Adistributional%20perspective%20of%20critic%20value%20function.%20Aim%20to%20learn%20to%20control%0Athe%20agent%20and%20master%20some%20tasks%20in%20a%20high-dimensional%20continuous%20space.%20DrQ-v2%0Auses%20DDPG%20as%20the%20backbone%20and%20achieves%20out-performance%20in%20various%20continuous%0Acontrol%20tasks.%20Here%20Distributed%20Distributional%20DrQ%20uses%20Distributed%0ADistributional%20DDPG%20as%20the%20backbone%2C%20and%20this%20modification%20aims%20to%20achieve%0Abetter%20performance%20in%20some%20hard%20continuous%20control%20tasks%20through%20the%20better%0Aexpression%20ability%20of%20distributional%20value%20function%20and%20distributed%20actor%0Apolicies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10645v1&entry.124074799=Read"},
{"title": "CoBra: Complementary Branch Fusing Class and Semantic Knowledge for\n  Robust Weakly Supervised Semantic Segmentation", "author": "Woojung Han and Seil Kang and Kyobin Choo and Seong Jae Hwang", "abstract": "  Leveraging semantically precise pseudo masks derived from image-level class\nknowledge for segmentation, namely image-level Weakly Supervised Semantic\nSegmentation (WSSS), still remains challenging. While Class Activation Maps\n(CAMs) using CNNs have steadily been contributing to the success of WSSS, the\nresulting activation maps often narrowly focus on class-specific parts (e.g.,\nonly face of human). On the other hand, recent works based on vision\ntransformers (ViT) have shown promising results based on their self-attention\nmechanism to capture the semantic parts but fail in capturing complete\nclass-specific details (e.g., entire body parts of human but also with a dog\nnearby). In this work, we propose Complementary Branch (CoBra), a novel dual\nbranch framework consisting of two distinct architectures which provide\nvaluable complementary knowledge of class (from CNN) and semantic (from ViT) to\neach branch. In particular, we learn Class-Aware Projection (CAP) for the CNN\nbranch and Semantic-Aware Projection (SAP) for the ViT branch to explicitly\nfuse their complementary knowledge and facilitate a new type of extra\npatch-level supervision. Our model, through CoBra, fuses CNN and ViT's\ncomplementary outputs to create robust pseudo masks that integrate both class\nand semantic information effectively. Extensive experiments qualitatively and\nquantitatively investigate how CNN and ViT complement each other on the PASCAL\nVOC 2012 dataset, showing a state-of-the-art WSSS result. This includes not\nonly the masks generated by our model, but also the segmentation results\nderived from utilizing these masks as pseudo labels.\n", "link": "http://arxiv.org/abs/2403.08801v5", "date": "2024-04-16", "relevancy": 2.2214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoBra%3A%20Complementary%20Branch%20Fusing%20Class%20and%20Semantic%20Knowledge%20for%0A%20%20Robust%20Weakly%20Supervised%20Semantic%20Segmentation&body=Title%3A%20CoBra%3A%20Complementary%20Branch%20Fusing%20Class%20and%20Semantic%20Knowledge%20for%0A%20%20Robust%20Weakly%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Woojung%20Han%20and%20Seil%20Kang%20and%20Kyobin%20Choo%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20Leveraging%20semantically%20precise%20pseudo%20masks%20derived%20from%20image-level%20class%0Aknowledge%20for%20segmentation%2C%20namely%20image-level%20Weakly%20Supervised%20Semantic%0ASegmentation%20%28WSSS%29%2C%20still%20remains%20challenging.%20While%20Class%20Activation%20Maps%0A%28CAMs%29%20using%20CNNs%20have%20steadily%20been%20contributing%20to%20the%20success%20of%20WSSS%2C%20the%0Aresulting%20activation%20maps%20often%20narrowly%20focus%20on%20class-specific%20parts%20%28e.g.%2C%0Aonly%20face%20of%20human%29.%20On%20the%20other%20hand%2C%20recent%20works%20based%20on%20vision%0Atransformers%20%28ViT%29%20have%20shown%20promising%20results%20based%20on%20their%20self-attention%0Amechanism%20to%20capture%20the%20semantic%20parts%20but%20fail%20in%20capturing%20complete%0Aclass-specific%20details%20%28e.g.%2C%20entire%20body%20parts%20of%20human%20but%20also%20with%20a%20dog%0Anearby%29.%20In%20this%20work%2C%20we%20propose%20Complementary%20Branch%20%28CoBra%29%2C%20a%20novel%20dual%0Abranch%20framework%20consisting%20of%20two%20distinct%20architectures%20which%20provide%0Avaluable%20complementary%20knowledge%20of%20class%20%28from%20CNN%29%20and%20semantic%20%28from%20ViT%29%20to%0Aeach%20branch.%20In%20particular%2C%20we%20learn%20Class-Aware%20Projection%20%28CAP%29%20for%20the%20CNN%0Abranch%20and%20Semantic-Aware%20Projection%20%28SAP%29%20for%20the%20ViT%20branch%20to%20explicitly%0Afuse%20their%20complementary%20knowledge%20and%20facilitate%20a%20new%20type%20of%20extra%0Apatch-level%20supervision.%20Our%20model%2C%20through%20CoBra%2C%20fuses%20CNN%20and%20ViT%27s%0Acomplementary%20outputs%20to%20create%20robust%20pseudo%20masks%20that%20integrate%20both%20class%0Aand%20semantic%20information%20effectively.%20Extensive%20experiments%20qualitatively%20and%0Aquantitatively%20investigate%20how%20CNN%20and%20ViT%20complement%20each%20other%20on%20the%20PASCAL%0AVOC%202012%20dataset%2C%20showing%20a%20state-of-the-art%20WSSS%20result.%20This%20includes%20not%0Aonly%20the%20masks%20generated%20by%20our%20model%2C%20but%20also%20the%20segmentation%20results%0Aderived%20from%20utilizing%20these%20masks%20as%20pseudo%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08801v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoBra%3A%20Complementary%20Branch%20Fusing%20Class%20and%20Semantic%20Knowledge%20for%0A%20%20Robust%20Weakly%20Supervised%20Semantic%20Segmentation&entry.906535625=Woojung%20Han%20and%20Seil%20Kang%20and%20Kyobin%20Choo%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20Leveraging%20semantically%20precise%20pseudo%20masks%20derived%20from%20image-level%20class%0Aknowledge%20for%20segmentation%2C%20namely%20image-level%20Weakly%20Supervised%20Semantic%0ASegmentation%20%28WSSS%29%2C%20still%20remains%20challenging.%20While%20Class%20Activation%20Maps%0A%28CAMs%29%20using%20CNNs%20have%20steadily%20been%20contributing%20to%20the%20success%20of%20WSSS%2C%20the%0Aresulting%20activation%20maps%20often%20narrowly%20focus%20on%20class-specific%20parts%20%28e.g.%2C%0Aonly%20face%20of%20human%29.%20On%20the%20other%20hand%2C%20recent%20works%20based%20on%20vision%0Atransformers%20%28ViT%29%20have%20shown%20promising%20results%20based%20on%20their%20self-attention%0Amechanism%20to%20capture%20the%20semantic%20parts%20but%20fail%20in%20capturing%20complete%0Aclass-specific%20details%20%28e.g.%2C%20entire%20body%20parts%20of%20human%20but%20also%20with%20a%20dog%0Anearby%29.%20In%20this%20work%2C%20we%20propose%20Complementary%20Branch%20%28CoBra%29%2C%20a%20novel%20dual%0Abranch%20framework%20consisting%20of%20two%20distinct%20architectures%20which%20provide%0Avaluable%20complementary%20knowledge%20of%20class%20%28from%20CNN%29%20and%20semantic%20%28from%20ViT%29%20to%0Aeach%20branch.%20In%20particular%2C%20we%20learn%20Class-Aware%20Projection%20%28CAP%29%20for%20the%20CNN%0Abranch%20and%20Semantic-Aware%20Projection%20%28SAP%29%20for%20the%20ViT%20branch%20to%20explicitly%0Afuse%20their%20complementary%20knowledge%20and%20facilitate%20a%20new%20type%20of%20extra%0Apatch-level%20supervision.%20Our%20model%2C%20through%20CoBra%2C%20fuses%20CNN%20and%20ViT%27s%0Acomplementary%20outputs%20to%20create%20robust%20pseudo%20masks%20that%20integrate%20both%20class%0Aand%20semantic%20information%20effectively.%20Extensive%20experiments%20qualitatively%20and%0Aquantitatively%20investigate%20how%20CNN%20and%20ViT%20complement%20each%20other%20on%20the%20PASCAL%0AVOC%202012%20dataset%2C%20showing%20a%20state-of-the-art%20WSSS%20result.%20This%20includes%20not%0Aonly%20the%20masks%20generated%20by%20our%20model%2C%20but%20also%20the%20segmentation%20results%0Aderived%20from%20utilizing%20these%20masks%20as%20pseudo%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08801v5&entry.124074799=Read"},
{"title": "E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors\n  to New Generators Using Limited Data", "author": "Aref Azizpour and Tai D. Nguyen and Manil Shrestha and Kaidi Xu and Edward Kim and Matthew C. Stamm", "abstract": "  As generative AI progresses rapidly, new synthetic image generators continue\nto emerge at a swift pace. Traditional detection methods face two main\nchallenges in adapting to these generators: the forensic traces of synthetic\nimages from new techniques can vastly differ from those learned during\ntraining, and access to data for these new generators is often limited. To\naddress these issues, we introduce the Ensemble of Expert Embedders (E3), a\nnovel continual learning framework for updating synthetic image detectors. E3\nenables the accurate detection of images from newly emerged generators using\nminimal training data. Our approach does this by first employing transfer\nlearning to develop a suite of expert embedders, each specializing in the\nforensic traces of a specific generator. Then, all embeddings are jointly\nanalyzed by an Expert Knowledge Fusion Network to produce accurate and reliable\ndetection decisions. Our experiments demonstrate that E3 outperforms existing\ncontinual learning methods, including those developed specifically for\nsynthetic image detection.\n", "link": "http://arxiv.org/abs/2404.08814v2", "date": "2024-04-16", "relevancy": 2.216, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5757}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20E3%3A%20Ensemble%20of%20Expert%20Embedders%20for%20Adapting%20Synthetic%20Image%20Detectors%0A%20%20to%20New%20Generators%20Using%20Limited%20Data&body=Title%3A%20E3%3A%20Ensemble%20of%20Expert%20Embedders%20for%20Adapting%20Synthetic%20Image%20Detectors%0A%20%20to%20New%20Generators%20Using%20Limited%20Data%0AAuthor%3A%20Aref%20Azizpour%20and%20Tai%20D.%20Nguyen%20and%20Manil%20Shrestha%20and%20Kaidi%20Xu%20and%20Edward%20Kim%20and%20Matthew%20C.%20Stamm%0AAbstract%3A%20%20%20As%20generative%20AI%20progresses%20rapidly%2C%20new%20synthetic%20image%20generators%20continue%0Ato%20emerge%20at%20a%20swift%20pace.%20Traditional%20detection%20methods%20face%20two%20main%0Achallenges%20in%20adapting%20to%20these%20generators%3A%20the%20forensic%20traces%20of%20synthetic%0Aimages%20from%20new%20techniques%20can%20vastly%20differ%20from%20those%20learned%20during%0Atraining%2C%20and%20access%20to%20data%20for%20these%20new%20generators%20is%20often%20limited.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Ensemble%20of%20Expert%20Embedders%20%28E3%29%2C%20a%0Anovel%20continual%20learning%20framework%20for%20updating%20synthetic%20image%20detectors.%20E3%0Aenables%20the%20accurate%20detection%20of%20images%20from%20newly%20emerged%20generators%20using%0Aminimal%20training%20data.%20Our%20approach%20does%20this%20by%20first%20employing%20transfer%0Alearning%20to%20develop%20a%20suite%20of%20expert%20embedders%2C%20each%20specializing%20in%20the%0Aforensic%20traces%20of%20a%20specific%20generator.%20Then%2C%20all%20embeddings%20are%20jointly%0Aanalyzed%20by%20an%20Expert%20Knowledge%20Fusion%20Network%20to%20produce%20accurate%20and%20reliable%0Adetection%20decisions.%20Our%20experiments%20demonstrate%20that%20E3%20outperforms%20existing%0Acontinual%20learning%20methods%2C%20including%20those%20developed%20specifically%20for%0Asynthetic%20image%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08814v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3%3A%20Ensemble%20of%20Expert%20Embedders%20for%20Adapting%20Synthetic%20Image%20Detectors%0A%20%20to%20New%20Generators%20Using%20Limited%20Data&entry.906535625=Aref%20Azizpour%20and%20Tai%20D.%20Nguyen%20and%20Manil%20Shrestha%20and%20Kaidi%20Xu%20and%20Edward%20Kim%20and%20Matthew%20C.%20Stamm&entry.1292438233=%20%20As%20generative%20AI%20progresses%20rapidly%2C%20new%20synthetic%20image%20generators%20continue%0Ato%20emerge%20at%20a%20swift%20pace.%20Traditional%20detection%20methods%20face%20two%20main%0Achallenges%20in%20adapting%20to%20these%20generators%3A%20the%20forensic%20traces%20of%20synthetic%0Aimages%20from%20new%20techniques%20can%20vastly%20differ%20from%20those%20learned%20during%0Atraining%2C%20and%20access%20to%20data%20for%20these%20new%20generators%20is%20often%20limited.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20the%20Ensemble%20of%20Expert%20Embedders%20%28E3%29%2C%20a%0Anovel%20continual%20learning%20framework%20for%20updating%20synthetic%20image%20detectors.%20E3%0Aenables%20the%20accurate%20detection%20of%20images%20from%20newly%20emerged%20generators%20using%0Aminimal%20training%20data.%20Our%20approach%20does%20this%20by%20first%20employing%20transfer%0Alearning%20to%20develop%20a%20suite%20of%20expert%20embedders%2C%20each%20specializing%20in%20the%0Aforensic%20traces%20of%20a%20specific%20generator.%20Then%2C%20all%20embeddings%20are%20jointly%0Aanalyzed%20by%20an%20Expert%20Knowledge%20Fusion%20Network%20to%20produce%20accurate%20and%20reliable%0Adetection%20decisions.%20Our%20experiments%20demonstrate%20that%20E3%20outperforms%20existing%0Acontinual%20learning%20methods%2C%20including%20those%20developed%20specifically%20for%0Asynthetic%20image%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08814v2&entry.124074799=Read"},
{"title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation", "author": "Yichi Zhang and Ziqiao Ma and Xiaofeng Gao and Suhaila Shakiah and Qiaozi Gao and Joyce Chai", "abstract": "  Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.\n", "link": "http://arxiv.org/abs/2402.16846v2", "date": "2024-04-16", "relevancy": 2.1785, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5545}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GROUNDHOG%3A%20Grounding%20Large%20Language%20Models%20to%20Holistic%20Segmentation&body=Title%3A%20GROUNDHOG%3A%20Grounding%20Large%20Language%20Models%20to%20Holistic%20Segmentation%0AAuthor%3A%20Yichi%20Zhang%20and%20Ziqiao%20Ma%20and%20Xiaofeng%20Gao%20and%20Suhaila%20Shakiah%20and%20Qiaozi%20Gao%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Most%20multimodal%20large%20language%20models%20%28MLLMs%29%20learn%20language-to-object%0Agrounding%20through%20causal%20language%20modeling%20where%20grounded%20objects%20are%20captured%0Aby%20bounding%20boxes%20as%20sequences%20of%20location%20tokens.%20This%20paradigm%20lacks%0Apixel-level%20representations%20that%20are%20important%20for%20fine-grained%20visual%0Aunderstanding%20and%20diagnosis.%20In%20this%20work%2C%20we%20introduce%20GROUNDHOG%2C%20an%20MLLM%0Adeveloped%20by%20grounding%20Large%20Language%20Models%20to%20holistic%20segmentation.%0AGROUNDHOG%20incorporates%20a%20masked%20feature%20extractor%20and%20converts%20extracted%0Afeatures%20into%20visual%20entity%20tokens%20for%20the%20MLLM%20backbone%2C%20which%20then%20connects%0Agroundable%20phrases%20to%20unified%20grounding%20masks%20by%20retrieving%20and%20merging%20the%0Aentity%20masks.%20To%20train%20GROUNDHOG%2C%20we%20carefully%20curated%20M3G2%2C%20a%20grounded%20visual%0Ainstruction%20tuning%20dataset%20with%20Multi-Modal%20Multi-Grained%20Grounding%2C%20by%0Aharvesting%20a%20collection%20of%20segmentation-grounded%20datasets%20with%20rich%0Aannotations.%20Our%20experimental%20results%20show%20that%20GROUNDHOG%20achieves%20superior%0Aperformance%20on%20various%20language%20grounding%20tasks%20without%20task-specific%0Afine-tuning%2C%20and%20significantly%20reduces%20object%20hallucination.%20GROUNDHOG%20also%0Ademonstrates%20better%20grounding%20towards%20complex%20forms%20of%20visual%20input%20and%0Aprovides%20easy-to-understand%20diagnosis%20in%20failure%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16846v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GROUNDHOG%3A%20Grounding%20Large%20Language%20Models%20to%20Holistic%20Segmentation&entry.906535625=Yichi%20Zhang%20and%20Ziqiao%20Ma%20and%20Xiaofeng%20Gao%20and%20Suhaila%20Shakiah%20and%20Qiaozi%20Gao%20and%20Joyce%20Chai&entry.1292438233=%20%20Most%20multimodal%20large%20language%20models%20%28MLLMs%29%20learn%20language-to-object%0Agrounding%20through%20causal%20language%20modeling%20where%20grounded%20objects%20are%20captured%0Aby%20bounding%20boxes%20as%20sequences%20of%20location%20tokens.%20This%20paradigm%20lacks%0Apixel-level%20representations%20that%20are%20important%20for%20fine-grained%20visual%0Aunderstanding%20and%20diagnosis.%20In%20this%20work%2C%20we%20introduce%20GROUNDHOG%2C%20an%20MLLM%0Adeveloped%20by%20grounding%20Large%20Language%20Models%20to%20holistic%20segmentation.%0AGROUNDHOG%20incorporates%20a%20masked%20feature%20extractor%20and%20converts%20extracted%0Afeatures%20into%20visual%20entity%20tokens%20for%20the%20MLLM%20backbone%2C%20which%20then%20connects%0Agroundable%20phrases%20to%20unified%20grounding%20masks%20by%20retrieving%20and%20merging%20the%0Aentity%20masks.%20To%20train%20GROUNDHOG%2C%20we%20carefully%20curated%20M3G2%2C%20a%20grounded%20visual%0Ainstruction%20tuning%20dataset%20with%20Multi-Modal%20Multi-Grained%20Grounding%2C%20by%0Aharvesting%20a%20collection%20of%20segmentation-grounded%20datasets%20with%20rich%0Aannotations.%20Our%20experimental%20results%20show%20that%20GROUNDHOG%20achieves%20superior%0Aperformance%20on%20various%20language%20grounding%20tasks%20without%20task-specific%0Afine-tuning%2C%20and%20significantly%20reduces%20object%20hallucination.%20GROUNDHOG%20also%0Ademonstrates%20better%20grounding%20towards%20complex%20forms%20of%20visual%20input%20and%0Aprovides%20easy-to-understand%20diagnosis%20in%20failure%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16846v2&entry.124074799=Read"},
{"title": "Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases", "author": "Yanze Li and Wenhua Zhang and Kai Chen and Yanxin Liu and Pengxiang Li and Ruiyuan Gao and Lanqing Hong and Meng Tian and Xinhai Zhao and Zhenguo Li and Dit-Yan Yeung and Huchuan Lu and Xu Jia", "abstract": "  Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning\nability to understand images and videos, have received widespread attention in\nthe autonomous driving domain, which significantly advances the development of\ninterpretable end-to-end autonomous driving. However, current evaluations of\nLVLMs primarily focus on the multi-faceted capabilities in common scenarios,\nlacking quantifiable and automated assessment in autonomous driving contexts,\nlet alone severe road corner cases that even the state-of-the-art autonomous\ndriving perception systems struggle to handle. In this paper, we propose\nCODA-LM, a novel vision-language benchmark for self-driving, which provides the\nfirst automatic and quantitative evaluation of LVLMs for interpretable\nautonomous driving including general perception, regional perception, and\ndriving suggestions. CODA-LM utilizes the texts to describe the road images,\nexploiting powerful text-only large language models (LLMs) without image inputs\nto assess the capabilities of LVLMs in autonomous driving scenarios, which\nreveals stronger alignment with human preferences than LVLM judges. Experiments\ndemonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot\ndeal with road corner cases well, suggesting that we are still far from a\nstrong LVLM-powered intelligent driving agent, and we hope our CODA-LM can\nbecome the catalyst to promote future development.\n", "link": "http://arxiv.org/abs/2404.10595v1", "date": "2024-04-16", "relevancy": 2.1722, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5437}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases&body=Title%3A%20Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases%0AAuthor%3A%20Yanze%20Li%20and%20Wenhua%20Zhang%20and%20Kai%20Chen%20and%20Yanxin%20Liu%20and%20Pengxiang%20Li%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20due%20to%20the%20remarkable%20visual%20reasoning%0Aability%20to%20understand%20images%20and%20videos%2C%20have%20received%20widespread%20attention%20in%0Athe%20autonomous%20driving%20domain%2C%20which%20significantly%20advances%20the%20development%20of%0Ainterpretable%20end-to-end%20autonomous%20driving.%20However%2C%20current%20evaluations%20of%0ALVLMs%20primarily%20focus%20on%20the%20multi-faceted%20capabilities%20in%20common%20scenarios%2C%0Alacking%20quantifiable%20and%20automated%20assessment%20in%20autonomous%20driving%20contexts%2C%0Alet%20alone%20severe%20road%20corner%20cases%20that%20even%20the%20state-of-the-art%20autonomous%0Adriving%20perception%20systems%20struggle%20to%20handle.%20In%20this%20paper%2C%20we%20propose%0ACODA-LM%2C%20a%20novel%20vision-language%20benchmark%20for%20self-driving%2C%20which%20provides%20the%0Afirst%20automatic%20and%20quantitative%20evaluation%20of%20LVLMs%20for%20interpretable%0Aautonomous%20driving%20including%20general%20perception%2C%20regional%20perception%2C%20and%0Adriving%20suggestions.%20CODA-LM%20utilizes%20the%20texts%20to%20describe%20the%20road%20images%2C%0Aexploiting%20powerful%20text-only%20large%20language%20models%20%28LLMs%29%20without%20image%20inputs%0Ato%20assess%20the%20capabilities%20of%20LVLMs%20in%20autonomous%20driving%20scenarios%2C%20which%0Areveals%20stronger%20alignment%20with%20human%20preferences%20than%20LVLM%20judges.%20Experiments%0Ademonstrate%20that%20even%20the%20closed-sourced%20commercial%20LVLMs%20like%20GPT-4V%20cannot%0Adeal%20with%20road%20corner%20cases%20well%2C%20suggesting%20that%20we%20are%20still%20far%20from%20a%0Astrong%20LVLM-powered%20intelligent%20driving%20agent%2C%20and%20we%20hope%20our%20CODA-LM%20can%0Abecome%20the%20catalyst%20to%20promote%20future%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10595v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases&entry.906535625=Yanze%20Li%20and%20Wenhua%20Zhang%20and%20Kai%20Chen%20and%20Yanxin%20Liu%20and%20Pengxiang%20Li%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20due%20to%20the%20remarkable%20visual%20reasoning%0Aability%20to%20understand%20images%20and%20videos%2C%20have%20received%20widespread%20attention%20in%0Athe%20autonomous%20driving%20domain%2C%20which%20significantly%20advances%20the%20development%20of%0Ainterpretable%20end-to-end%20autonomous%20driving.%20However%2C%20current%20evaluations%20of%0ALVLMs%20primarily%20focus%20on%20the%20multi-faceted%20capabilities%20in%20common%20scenarios%2C%0Alacking%20quantifiable%20and%20automated%20assessment%20in%20autonomous%20driving%20contexts%2C%0Alet%20alone%20severe%20road%20corner%20cases%20that%20even%20the%20state-of-the-art%20autonomous%0Adriving%20perception%20systems%20struggle%20to%20handle.%20In%20this%20paper%2C%20we%20propose%0ACODA-LM%2C%20a%20novel%20vision-language%20benchmark%20for%20self-driving%2C%20which%20provides%20the%0Afirst%20automatic%20and%20quantitative%20evaluation%20of%20LVLMs%20for%20interpretable%0Aautonomous%20driving%20including%20general%20perception%2C%20regional%20perception%2C%20and%0Adriving%20suggestions.%20CODA-LM%20utilizes%20the%20texts%20to%20describe%20the%20road%20images%2C%0Aexploiting%20powerful%20text-only%20large%20language%20models%20%28LLMs%29%20without%20image%20inputs%0Ato%20assess%20the%20capabilities%20of%20LVLMs%20in%20autonomous%20driving%20scenarios%2C%20which%0Areveals%20stronger%20alignment%20with%20human%20preferences%20than%20LVLM%20judges.%20Experiments%0Ademonstrate%20that%20even%20the%20closed-sourced%20commercial%20LVLMs%20like%20GPT-4V%20cannot%0Adeal%20with%20road%20corner%20cases%20well%2C%20suggesting%20that%20we%20are%20still%20far%20from%20a%0Astrong%20LVLM-powered%20intelligent%20driving%20agent%2C%20and%20we%20hope%20our%20CODA-LM%20can%0Abecome%20the%20catalyst%20to%20promote%20future%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10595v1&entry.124074799=Read"},
{"title": "Pixel-Wise Contrastive Distillation", "author": "Junqiang Huang and Zichao Guo", "abstract": "  We present a simple but effective pixel-level self-supervised distillation\nframework friendly to dense prediction tasks. Our method, called Pixel-Wise\nContrastive Distillation (PCD), distills knowledge by attracting the\ncorresponding pixels from student's and teacher's output feature maps. PCD\nincludes a novel design called SpatialAdaptor which ``reshapes'' a part of the\nteacher network while preserving the distribution of its output features. Our\nablation experiments suggest that this reshaping behavior enables more\ninformative pixel-to-pixel distillation. Moreover, we utilize a plug-in\nmulti-head self-attention module that explicitly relates the pixels of\nstudent's feature maps to enhance the effective receptive field, leading to a\nmore competitive student. PCD \\textbf{outperforms} previous self-supervised\ndistillation methods on various dense prediction tasks. A backbone of\n\\mbox{ResNet-18-FPN} distilled by PCD achieves $37.4$ AP$^\\text{bbox}$ and\n$34.0$ AP$^\\text{mask}$ on COCO dataset using the detector of \\mbox{Mask\nR-CNN}. We hope our study will inspire future research on how to pre-train a\nsmall model friendly to dense prediction tasks in a self-supervised fashion.\n", "link": "http://arxiv.org/abs/2211.00218v3", "date": "2024-04-16", "relevancy": 2.1673, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5455}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5404}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5363}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pixel-Wise%20Contrastive%20Distillation&body=Title%3A%20Pixel-Wise%20Contrastive%20Distillation%0AAuthor%3A%20Junqiang%20Huang%20and%20Zichao%20Guo%0AAbstract%3A%20%20%20We%20present%20a%20simple%20but%20effective%20pixel-level%20self-supervised%20distillation%0Aframework%20friendly%20to%20dense%20prediction%20tasks.%20Our%20method%2C%20called%20Pixel-Wise%0AContrastive%20Distillation%20%28PCD%29%2C%20distills%20knowledge%20by%20attracting%20the%0Acorresponding%20pixels%20from%20student%27s%20and%20teacher%27s%20output%20feature%20maps.%20PCD%0Aincludes%20a%20novel%20design%20called%20SpatialAdaptor%20which%20%60%60reshapes%27%27%20a%20part%20of%20the%0Ateacher%20network%20while%20preserving%20the%20distribution%20of%20its%20output%20features.%20Our%0Aablation%20experiments%20suggest%20that%20this%20reshaping%20behavior%20enables%20more%0Ainformative%20pixel-to-pixel%20distillation.%20Moreover%2C%20we%20utilize%20a%20plug-in%0Amulti-head%20self-attention%20module%20that%20explicitly%20relates%20the%20pixels%20of%0Astudent%27s%20feature%20maps%20to%20enhance%20the%20effective%20receptive%20field%2C%20leading%20to%20a%0Amore%20competitive%20student.%20PCD%20%5Ctextbf%7Boutperforms%7D%20previous%20self-supervised%0Adistillation%20methods%20on%20various%20dense%20prediction%20tasks.%20A%20backbone%20of%0A%5Cmbox%7BResNet-18-FPN%7D%20distilled%20by%20PCD%20achieves%20%2437.4%24%20AP%24%5E%5Ctext%7Bbbox%7D%24%20and%0A%2434.0%24%20AP%24%5E%5Ctext%7Bmask%7D%24%20on%20COCO%20dataset%20using%20the%20detector%20of%20%5Cmbox%7BMask%0AR-CNN%7D.%20We%20hope%20our%20study%20will%20inspire%20future%20research%20on%20how%20to%20pre-train%20a%0Asmall%20model%20friendly%20to%20dense%20prediction%20tasks%20in%20a%20self-supervised%20fashion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.00218v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Wise%20Contrastive%20Distillation&entry.906535625=Junqiang%20Huang%20and%20Zichao%20Guo&entry.1292438233=%20%20We%20present%20a%20simple%20but%20effective%20pixel-level%20self-supervised%20distillation%0Aframework%20friendly%20to%20dense%20prediction%20tasks.%20Our%20method%2C%20called%20Pixel-Wise%0AContrastive%20Distillation%20%28PCD%29%2C%20distills%20knowledge%20by%20attracting%20the%0Acorresponding%20pixels%20from%20student%27s%20and%20teacher%27s%20output%20feature%20maps.%20PCD%0Aincludes%20a%20novel%20design%20called%20SpatialAdaptor%20which%20%60%60reshapes%27%27%20a%20part%20of%20the%0Ateacher%20network%20while%20preserving%20the%20distribution%20of%20its%20output%20features.%20Our%0Aablation%20experiments%20suggest%20that%20this%20reshaping%20behavior%20enables%20more%0Ainformative%20pixel-to-pixel%20distillation.%20Moreover%2C%20we%20utilize%20a%20plug-in%0Amulti-head%20self-attention%20module%20that%20explicitly%20relates%20the%20pixels%20of%0Astudent%27s%20feature%20maps%20to%20enhance%20the%20effective%20receptive%20field%2C%20leading%20to%20a%0Amore%20competitive%20student.%20PCD%20%5Ctextbf%7Boutperforms%7D%20previous%20self-supervised%0Adistillation%20methods%20on%20various%20dense%20prediction%20tasks.%20A%20backbone%20of%0A%5Cmbox%7BResNet-18-FPN%7D%20distilled%20by%20PCD%20achieves%20%2437.4%24%20AP%24%5E%5Ctext%7Bbbox%7D%24%20and%0A%2434.0%24%20AP%24%5E%5Ctext%7Bmask%7D%24%20on%20COCO%20dataset%20using%20the%20detector%20of%20%5Cmbox%7BMask%0AR-CNN%7D.%20We%20hope%20our%20study%20will%20inspire%20future%20research%20on%20how%20to%20pre-train%20a%0Asmall%20model%20friendly%20to%20dense%20prediction%20tasks%20in%20a%20self-supervised%20fashion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.00218v3&entry.124074799=Read"},
{"title": "Graph Neural Networks for Protein-Protein Interactions - A Short Survey", "author": "Mingda Xu and Peisheng Qian and Ziyuan Zhao and Zeng Zeng and Jianguo Chen and Weide Liu and Xulei Yang", "abstract": "  Protein-protein interactions (PPIs) play key roles in a broad range of\nbiological processes. Numerous strategies have been proposed for predicting\nPPIs, and among them, graph-based methods have demonstrated promising outcomes\nowing to the inherent graph structure of PPI networks. This paper reviews\nvarious graph-based methodologies, and discusses their applications in PPI\nprediction. We classify these approaches into two primary groups based on their\nmodel structures. The first category employs Graph Neural Networks (GNN) or\nGraph Convolutional Networks (GCN), while the second category utilizes Graph\nAttention Networks (GAT), Graph Auto-Encoders and Graph-BERT. We highlight the\ndistinctive methodologies of each approach in managing the graph-structured\ndata inherent in PPI networks and anticipate future research directions in this\ndomain.\n", "link": "http://arxiv.org/abs/2404.10450v1", "date": "2024-04-16", "relevancy": 2.1466, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4341}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4302}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4237}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Protein-Protein%20Interactions%20-%20A%20Short%20Survey&body=Title%3A%20Graph%20Neural%20Networks%20for%20Protein-Protein%20Interactions%20-%20A%20Short%20Survey%0AAuthor%3A%20Mingda%20Xu%20and%20Peisheng%20Qian%20and%20Ziyuan%20Zhao%20and%20Zeng%20Zeng%20and%20Jianguo%20Chen%20and%20Weide%20Liu%20and%20Xulei%20Yang%0AAbstract%3A%20%20%20Protein-protein%20interactions%20%28PPIs%29%20play%20key%20roles%20in%20a%20broad%20range%20of%0Abiological%20processes.%20Numerous%20strategies%20have%20been%20proposed%20for%20predicting%0APPIs%2C%20and%20among%20them%2C%20graph-based%20methods%20have%20demonstrated%20promising%20outcomes%0Aowing%20to%20the%20inherent%20graph%20structure%20of%20PPI%20networks.%20This%20paper%20reviews%0Avarious%20graph-based%20methodologies%2C%20and%20discusses%20their%20applications%20in%20PPI%0Aprediction.%20We%20classify%20these%20approaches%20into%20two%20primary%20groups%20based%20on%20their%0Amodel%20structures.%20The%20first%20category%20employs%20Graph%20Neural%20Networks%20%28GNN%29%20or%0AGraph%20Convolutional%20Networks%20%28GCN%29%2C%20while%20the%20second%20category%20utilizes%20Graph%0AAttention%20Networks%20%28GAT%29%2C%20Graph%20Auto-Encoders%20and%20Graph-BERT.%20We%20highlight%20the%0Adistinctive%20methodologies%20of%20each%20approach%20in%20managing%20the%20graph-structured%0Adata%20inherent%20in%20PPI%20networks%20and%20anticipate%20future%20research%20directions%20in%20this%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10450v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Protein-Protein%20Interactions%20-%20A%20Short%20Survey&entry.906535625=Mingda%20Xu%20and%20Peisheng%20Qian%20and%20Ziyuan%20Zhao%20and%20Zeng%20Zeng%20and%20Jianguo%20Chen%20and%20Weide%20Liu%20and%20Xulei%20Yang&entry.1292438233=%20%20Protein-protein%20interactions%20%28PPIs%29%20play%20key%20roles%20in%20a%20broad%20range%20of%0Abiological%20processes.%20Numerous%20strategies%20have%20been%20proposed%20for%20predicting%0APPIs%2C%20and%20among%20them%2C%20graph-based%20methods%20have%20demonstrated%20promising%20outcomes%0Aowing%20to%20the%20inherent%20graph%20structure%20of%20PPI%20networks.%20This%20paper%20reviews%0Avarious%20graph-based%20methodologies%2C%20and%20discusses%20their%20applications%20in%20PPI%0Aprediction.%20We%20classify%20these%20approaches%20into%20two%20primary%20groups%20based%20on%20their%0Amodel%20structures.%20The%20first%20category%20employs%20Graph%20Neural%20Networks%20%28GNN%29%20or%0AGraph%20Convolutional%20Networks%20%28GCN%29%2C%20while%20the%20second%20category%20utilizes%20Graph%0AAttention%20Networks%20%28GAT%29%2C%20Graph%20Auto-Encoders%20and%20Graph-BERT.%20We%20highlight%20the%0Adistinctive%20methodologies%20of%20each%20approach%20in%20managing%20the%20graph-structured%0Adata%20inherent%20in%20PPI%20networks%20and%20anticipate%20future%20research%20directions%20in%20this%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10450v1&entry.124074799=Read"},
{"title": "Learning Feature Inversion for Multi-class Anomaly Detection under\n  General-purpose COCO-AD Benchmark", "author": "Jiangning Zhang and Chengjie Wang and Xiangtai Li and Guanzhong Tian and Zhucun Xue and Yong Liu and Guansong Pang and Dacheng Tao", "abstract": "  Anomaly detection (AD) is often focused on detecting anomaly areas for\nindustrial quality inspection and medical lesion examination. However, due to\nthe specific scenario targets, the data scale for AD is relatively small, and\nevaluation metrics are still deficient compared to classic vision tasks, such\nas object detection and semantic segmentation. To fill these gaps, this work\nfirst constructs a large-scale and general-purpose COCO-AD dataset by extending\nCOCO to the AD field. This enables fair evaluation and sustainable development\nfor different methods on this challenging benchmark. Moreover, current metrics\nsuch as AU-ROC have nearly reached saturation on simple datasets, which\nprevents a comprehensive evaluation of different methods. Inspired by the\nmetrics in the segmentation field, we further propose several more practical\nthreshold-dependent AD-specific metrics, ie, m$F_1$$^{.2}_{.8}$,\nmAcc$^{.2}_{.8}$, mIoU$^{.2}_{.8}$, and mIoU-max. Motivated by GAN inversion's\nhigh-quality reconstruction capability, we propose a simple but more powerful\nInvAD framework to achieve high-quality feature reconstruction. Our method\nimproves the effectiveness of reconstruction-based methods on popular MVTec AD,\nVisA, and our newly proposed COCO-AD datasets under a multi-class unsupervised\nsetting, where only a single detection model is trained to detect anomalies\nfrom different classes. Extensive ablation experiments have demonstrated the\neffectiveness of each component of our InvAD. Full codes and models are\navailable at https://github.com/zhangzjn/ader.\n", "link": "http://arxiv.org/abs/2404.10760v1", "date": "2024-04-16", "relevancy": 2.1382, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Feature%20Inversion%20for%20Multi-class%20Anomaly%20Detection%20under%0A%20%20General-purpose%20COCO-AD%20Benchmark&body=Title%3A%20Learning%20Feature%20Inversion%20for%20Multi-class%20Anomaly%20Detection%20under%0A%20%20General-purpose%20COCO-AD%20Benchmark%0AAuthor%3A%20Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Xiangtai%20Li%20and%20Guanzhong%20Tian%20and%20Zhucun%20Xue%20and%20Yong%20Liu%20and%20Guansong%20Pang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20is%20often%20focused%20on%20detecting%20anomaly%20areas%20for%0Aindustrial%20quality%20inspection%20and%20medical%20lesion%20examination.%20However%2C%20due%20to%0Athe%20specific%20scenario%20targets%2C%20the%20data%20scale%20for%20AD%20is%20relatively%20small%2C%20and%0Aevaluation%20metrics%20are%20still%20deficient%20compared%20to%20classic%20vision%20tasks%2C%20such%0Aas%20object%20detection%20and%20semantic%20segmentation.%20To%20fill%20these%20gaps%2C%20this%20work%0Afirst%20constructs%20a%20large-scale%20and%20general-purpose%20COCO-AD%20dataset%20by%20extending%0ACOCO%20to%20the%20AD%20field.%20This%20enables%20fair%20evaluation%20and%20sustainable%20development%0Afor%20different%20methods%20on%20this%20challenging%20benchmark.%20Moreover%2C%20current%20metrics%0Asuch%20as%20AU-ROC%20have%20nearly%20reached%20saturation%20on%20simple%20datasets%2C%20which%0Aprevents%20a%20comprehensive%20evaluation%20of%20different%20methods.%20Inspired%20by%20the%0Ametrics%20in%20the%20segmentation%20field%2C%20we%20further%20propose%20several%20more%20practical%0Athreshold-dependent%20AD-specific%20metrics%2C%20ie%2C%20m%24F_1%24%24%5E%7B.2%7D_%7B.8%7D%24%2C%0AmAcc%24%5E%7B.2%7D_%7B.8%7D%24%2C%20mIoU%24%5E%7B.2%7D_%7B.8%7D%24%2C%20and%20mIoU-max.%20Motivated%20by%20GAN%20inversion%27s%0Ahigh-quality%20reconstruction%20capability%2C%20we%20propose%20a%20simple%20but%20more%20powerful%0AInvAD%20framework%20to%20achieve%20high-quality%20feature%20reconstruction.%20Our%20method%0Aimproves%20the%20effectiveness%20of%20reconstruction-based%20methods%20on%20popular%20MVTec%20AD%2C%0AVisA%2C%20and%20our%20newly%20proposed%20COCO-AD%20datasets%20under%20a%20multi-class%20unsupervised%0Asetting%2C%20where%20only%20a%20single%20detection%20model%20is%20trained%20to%20detect%20anomalies%0Afrom%20different%20classes.%20Extensive%20ablation%20experiments%20have%20demonstrated%20the%0Aeffectiveness%20of%20each%20component%20of%20our%20InvAD.%20Full%20codes%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/zhangzjn/ader.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Feature%20Inversion%20for%20Multi-class%20Anomaly%20Detection%20under%0A%20%20General-purpose%20COCO-AD%20Benchmark&entry.906535625=Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Xiangtai%20Li%20and%20Guanzhong%20Tian%20and%20Zhucun%20Xue%20and%20Yong%20Liu%20and%20Guansong%20Pang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20is%20often%20focused%20on%20detecting%20anomaly%20areas%20for%0Aindustrial%20quality%20inspection%20and%20medical%20lesion%20examination.%20However%2C%20due%20to%0Athe%20specific%20scenario%20targets%2C%20the%20data%20scale%20for%20AD%20is%20relatively%20small%2C%20and%0Aevaluation%20metrics%20are%20still%20deficient%20compared%20to%20classic%20vision%20tasks%2C%20such%0Aas%20object%20detection%20and%20semantic%20segmentation.%20To%20fill%20these%20gaps%2C%20this%20work%0Afirst%20constructs%20a%20large-scale%20and%20general-purpose%20COCO-AD%20dataset%20by%20extending%0ACOCO%20to%20the%20AD%20field.%20This%20enables%20fair%20evaluation%20and%20sustainable%20development%0Afor%20different%20methods%20on%20this%20challenging%20benchmark.%20Moreover%2C%20current%20metrics%0Asuch%20as%20AU-ROC%20have%20nearly%20reached%20saturation%20on%20simple%20datasets%2C%20which%0Aprevents%20a%20comprehensive%20evaluation%20of%20different%20methods.%20Inspired%20by%20the%0Ametrics%20in%20the%20segmentation%20field%2C%20we%20further%20propose%20several%20more%20practical%0Athreshold-dependent%20AD-specific%20metrics%2C%20ie%2C%20m%24F_1%24%24%5E%7B.2%7D_%7B.8%7D%24%2C%0AmAcc%24%5E%7B.2%7D_%7B.8%7D%24%2C%20mIoU%24%5E%7B.2%7D_%7B.8%7D%24%2C%20and%20mIoU-max.%20Motivated%20by%20GAN%20inversion%27s%0Ahigh-quality%20reconstruction%20capability%2C%20we%20propose%20a%20simple%20but%20more%20powerful%0AInvAD%20framework%20to%20achieve%20high-quality%20feature%20reconstruction.%20Our%20method%0Aimproves%20the%20effectiveness%20of%20reconstruction-based%20methods%20on%20popular%20MVTec%20AD%2C%0AVisA%2C%20and%20our%20newly%20proposed%20COCO-AD%20datasets%20under%20a%20multi-class%20unsupervised%0Asetting%2C%20where%20only%20a%20single%20detection%20model%20is%20trained%20to%20detect%20anomalies%0Afrom%20different%20classes.%20Extensive%20ablation%20experiments%20have%20demonstrated%20the%0Aeffectiveness%20of%20each%20component%20of%20our%20InvAD.%20Full%20codes%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/zhangzjn/ader.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10760v1&entry.124074799=Read"},
{"title": "Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic\n  Distribution Alignment", "author": "Jia Guo and Haonan Han and Shuai Lu and Weihang Zhang and Huiqi Li", "abstract": "  Conventional unsupervised anomaly detection (UAD) methods build separate\nmodels for each object category. Recent studies have proposed to train a\nunified model for multiple classes, namely model-unified UAD. However, such\nmethods still implement the unified model separately on each class during\ninference with respective anomaly decision thresholds, which hinders their\napplication when the image categories are entirely unavailable. In this work,\nwe present a simple yet powerful method to address multi-class anomaly\ndetection without any class information, namely \\textit{absolute-unified} UAD.\nWe target the crux of prior works in this challenging setting: different\nobjects have mismatched anomaly score distributions. We propose Class-Agnostic\nDistribution Alignment (CADA) to align the mismatched score distribution of\neach implicit class without knowing class information, which enables unified\nanomaly detection for all classes and samples. The essence of CADA is to\npredict each class's score distribution of normal samples given any image,\nnormal or anomalous, of this class. As a general component, CADA can activate\nthe potential of nearly all UAD methods under absolute-unified setting. Our\napproach is extensively evaluated under the proposed setting on two popular UAD\nbenchmark datasets, MVTec AD and VisA, where we exceed previous\nstate-of-the-art by a large margin.\n", "link": "http://arxiv.org/abs/2404.00724v2", "date": "2024-04-16", "relevancy": 2.1351, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5571}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5446}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5136}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Absolute-Unified%20Multi-Class%20Anomaly%20Detection%20via%20Class-Agnostic%0A%20%20Distribution%20Alignment&body=Title%3A%20Absolute-Unified%20Multi-Class%20Anomaly%20Detection%20via%20Class-Agnostic%0A%20%20Distribution%20Alignment%0AAuthor%3A%20Jia%20Guo%20and%20Haonan%20Han%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Huiqi%20Li%0AAbstract%3A%20%20%20Conventional%20unsupervised%20anomaly%20detection%20%28UAD%29%20methods%20build%20separate%0Amodels%20for%20each%20object%20category.%20Recent%20studies%20have%20proposed%20to%20train%20a%0Aunified%20model%20for%20multiple%20classes%2C%20namely%20model-unified%20UAD.%20However%2C%20such%0Amethods%20still%20implement%20the%20unified%20model%20separately%20on%20each%20class%20during%0Ainference%20with%20respective%20anomaly%20decision%20thresholds%2C%20which%20hinders%20their%0Aapplication%20when%20the%20image%20categories%20are%20entirely%20unavailable.%20In%20this%20work%2C%0Awe%20present%20a%20simple%20yet%20powerful%20method%20to%20address%20multi-class%20anomaly%0Adetection%20without%20any%20class%20information%2C%20namely%20%5Ctextit%7Babsolute-unified%7D%20UAD.%0AWe%20target%20the%20crux%20of%20prior%20works%20in%20this%20challenging%20setting%3A%20different%0Aobjects%20have%20mismatched%20anomaly%20score%20distributions.%20We%20propose%20Class-Agnostic%0ADistribution%20Alignment%20%28CADA%29%20to%20align%20the%20mismatched%20score%20distribution%20of%0Aeach%20implicit%20class%20without%20knowing%20class%20information%2C%20which%20enables%20unified%0Aanomaly%20detection%20for%20all%20classes%20and%20samples.%20The%20essence%20of%20CADA%20is%20to%0Apredict%20each%20class%27s%20score%20distribution%20of%20normal%20samples%20given%20any%20image%2C%0Anormal%20or%20anomalous%2C%20of%20this%20class.%20As%20a%20general%20component%2C%20CADA%20can%20activate%0Athe%20potential%20of%20nearly%20all%20UAD%20methods%20under%20absolute-unified%20setting.%20Our%0Aapproach%20is%20extensively%20evaluated%20under%20the%20proposed%20setting%20on%20two%20popular%20UAD%0Abenchmark%20datasets%2C%20MVTec%20AD%20and%20VisA%2C%20where%20we%20exceed%20previous%0Astate-of-the-art%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00724v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Absolute-Unified%20Multi-Class%20Anomaly%20Detection%20via%20Class-Agnostic%0A%20%20Distribution%20Alignment&entry.906535625=Jia%20Guo%20and%20Haonan%20Han%20and%20Shuai%20Lu%20and%20Weihang%20Zhang%20and%20Huiqi%20Li&entry.1292438233=%20%20Conventional%20unsupervised%20anomaly%20detection%20%28UAD%29%20methods%20build%20separate%0Amodels%20for%20each%20object%20category.%20Recent%20studies%20have%20proposed%20to%20train%20a%0Aunified%20model%20for%20multiple%20classes%2C%20namely%20model-unified%20UAD.%20However%2C%20such%0Amethods%20still%20implement%20the%20unified%20model%20separately%20on%20each%20class%20during%0Ainference%20with%20respective%20anomaly%20decision%20thresholds%2C%20which%20hinders%20their%0Aapplication%20when%20the%20image%20categories%20are%20entirely%20unavailable.%20In%20this%20work%2C%0Awe%20present%20a%20simple%20yet%20powerful%20method%20to%20address%20multi-class%20anomaly%0Adetection%20without%20any%20class%20information%2C%20namely%20%5Ctextit%7Babsolute-unified%7D%20UAD.%0AWe%20target%20the%20crux%20of%20prior%20works%20in%20this%20challenging%20setting%3A%20different%0Aobjects%20have%20mismatched%20anomaly%20score%20distributions.%20We%20propose%20Class-Agnostic%0ADistribution%20Alignment%20%28CADA%29%20to%20align%20the%20mismatched%20score%20distribution%20of%0Aeach%20implicit%20class%20without%20knowing%20class%20information%2C%20which%20enables%20unified%0Aanomaly%20detection%20for%20all%20classes%20and%20samples.%20The%20essence%20of%20CADA%20is%20to%0Apredict%20each%20class%27s%20score%20distribution%20of%20normal%20samples%20given%20any%20image%2C%0Anormal%20or%20anomalous%2C%20of%20this%20class.%20As%20a%20general%20component%2C%20CADA%20can%20activate%0Athe%20potential%20of%20nearly%20all%20UAD%20methods%20under%20absolute-unified%20setting.%20Our%0Aapproach%20is%20extensively%20evaluated%20under%20the%20proposed%20setting%20on%20two%20popular%20UAD%0Abenchmark%20datasets%2C%20MVTec%20AD%20and%20VisA%2C%20where%20we%20exceed%20previous%0Astate-of-the-art%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00724v2&entry.124074799=Read"},
{"title": "EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with\n  Global Convergence", "author": "Chung-Yiu Yau and Hoi-To Wai and Parameswaran Raman and Soumajyoti Sarkar and Mingyi Hong", "abstract": "  A key challenge in contrastive learning is to generate negative samples from\na large sample set to contrast with positive samples, for learning better\nencoding of the data. These negative samples often follow a softmax\ndistribution which are dynamically updated during the training process.\nHowever, sampling from this distribution is non-trivial due to the high\ncomputational costs in computing the partition function. In this paper, we\npropose an Efficient Markov Chain Monte Carlo negative sampling method for\nContrastive learning (EMC$^2$). We follow the global contrastive learning loss\nas introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive\nMetropolis-Hastings subroutine to generate hardness-aware negative samples in\nan online fashion during the optimization. We prove that EMC$^2$ finds an\n$\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in\n$T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that\nexhibits global convergence (to stationarity) regardless of the choice of batch\nsize while exhibiting low computation and memory cost. Numerical experiments\nvalidate that EMC$^2$ is effective with small batch training and achieves\ncomparable or better performance than baseline algorithms. We report the\nresults for pre-training image encoders on STL-10 and Imagenet-100.\n", "link": "http://arxiv.org/abs/2404.10575v1", "date": "2024-04-16", "relevancy": 2.1301, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5257}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5074}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EMC%24%5E2%24%3A%20Efficient%20MCMC%20Negative%20Sampling%20for%20Contrastive%20Learning%20with%0A%20%20Global%20Convergence&body=Title%3A%20EMC%24%5E2%24%3A%20Efficient%20MCMC%20Negative%20Sampling%20for%20Contrastive%20Learning%20with%0A%20%20Global%20Convergence%0AAuthor%3A%20Chung-Yiu%20Yau%20and%20Hoi-To%20Wai%20and%20Parameswaran%20Raman%20and%20Soumajyoti%20Sarkar%20and%20Mingyi%20Hong%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20contrastive%20learning%20is%20to%20generate%20negative%20samples%20from%0Aa%20large%20sample%20set%20to%20contrast%20with%20positive%20samples%2C%20for%20learning%20better%0Aencoding%20of%20the%20data.%20These%20negative%20samples%20often%20follow%20a%20softmax%0Adistribution%20which%20are%20dynamically%20updated%20during%20the%20training%20process.%0AHowever%2C%20sampling%20from%20this%20distribution%20is%20non-trivial%20due%20to%20the%20high%0Acomputational%20costs%20in%20computing%20the%20partition%20function.%20In%20this%20paper%2C%20we%0Apropose%20an%20Efficient%20Markov%20Chain%20Monte%20Carlo%20negative%20sampling%20method%20for%0AContrastive%20learning%20%28EMC%24%5E2%24%29.%20We%20follow%20the%20global%20contrastive%20learning%20loss%0Aas%20introduced%20in%20SogCLR%2C%20and%20propose%20EMC%24%5E2%24%20which%20utilizes%20an%20adaptive%0AMetropolis-Hastings%20subroutine%20to%20generate%20hardness-aware%20negative%20samples%20in%0Aan%20online%20fashion%20during%20the%20optimization.%20We%20prove%20that%20EMC%24%5E2%24%20finds%20an%0A%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BT%7D%29%24-stationary%20point%20of%20the%20global%20contrastive%20loss%20in%0A%24T%24%20iterations.%20Compared%20to%20prior%20works%2C%20EMC%24%5E2%24%20is%20the%20first%20algorithm%20that%0Aexhibits%20global%20convergence%20%28to%20stationarity%29%20regardless%20of%20the%20choice%20of%20batch%0Asize%20while%20exhibiting%20low%20computation%20and%20memory%20cost.%20Numerical%20experiments%0Avalidate%20that%20EMC%24%5E2%24%20is%20effective%20with%20small%20batch%20training%20and%20achieves%0Acomparable%20or%20better%20performance%20than%20baseline%20algorithms.%20We%20report%20the%0Aresults%20for%20pre-training%20image%20encoders%20on%20STL-10%20and%20Imagenet-100.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10575v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMC%24%5E2%24%3A%20Efficient%20MCMC%20Negative%20Sampling%20for%20Contrastive%20Learning%20with%0A%20%20Global%20Convergence&entry.906535625=Chung-Yiu%20Yau%20and%20Hoi-To%20Wai%20and%20Parameswaran%20Raman%20and%20Soumajyoti%20Sarkar%20and%20Mingyi%20Hong&entry.1292438233=%20%20A%20key%20challenge%20in%20contrastive%20learning%20is%20to%20generate%20negative%20samples%20from%0Aa%20large%20sample%20set%20to%20contrast%20with%20positive%20samples%2C%20for%20learning%20better%0Aencoding%20of%20the%20data.%20These%20negative%20samples%20often%20follow%20a%20softmax%0Adistribution%20which%20are%20dynamically%20updated%20during%20the%20training%20process.%0AHowever%2C%20sampling%20from%20this%20distribution%20is%20non-trivial%20due%20to%20the%20high%0Acomputational%20costs%20in%20computing%20the%20partition%20function.%20In%20this%20paper%2C%20we%0Apropose%20an%20Efficient%20Markov%20Chain%20Monte%20Carlo%20negative%20sampling%20method%20for%0AContrastive%20learning%20%28EMC%24%5E2%24%29.%20We%20follow%20the%20global%20contrastive%20learning%20loss%0Aas%20introduced%20in%20SogCLR%2C%20and%20propose%20EMC%24%5E2%24%20which%20utilizes%20an%20adaptive%0AMetropolis-Hastings%20subroutine%20to%20generate%20hardness-aware%20negative%20samples%20in%0Aan%20online%20fashion%20during%20the%20optimization.%20We%20prove%20that%20EMC%24%5E2%24%20finds%20an%0A%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BT%7D%29%24-stationary%20point%20of%20the%20global%20contrastive%20loss%20in%0A%24T%24%20iterations.%20Compared%20to%20prior%20works%2C%20EMC%24%5E2%24%20is%20the%20first%20algorithm%20that%0Aexhibits%20global%20convergence%20%28to%20stationarity%29%20regardless%20of%20the%20choice%20of%20batch%0Asize%20while%20exhibiting%20low%20computation%20and%20memory%20cost.%20Numerical%20experiments%0Avalidate%20that%20EMC%24%5E2%24%20is%20effective%20with%20small%20batch%20training%20and%20achieves%0Acomparable%20or%20better%20performance%20than%20baseline%20algorithms.%20We%20report%20the%0Aresults%20for%20pre-training%20image%20encoders%20on%20STL-10%20and%20Imagenet-100.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10575v1&entry.124074799=Read"},
{"title": "Mixed Prototype Consistency Learning for Semi-supervised Medical Image\n  Segmentation", "author": "Lijian Li", "abstract": "  Recently, prototype learning has emerged in semi-supervised medical image\nsegmentation and achieved remarkable performance. However, the scarcity of\nlabeled data limits the expressiveness of prototypes in previous methods,\npotentially hindering the complete representation of prototypes for class\nembedding. To address this problem, we propose the Mixed Prototype Consistency\nLearning (MPCL) framework, which includes a Mean Teacher and an auxiliary\nnetwork. The Mean Teacher generates prototypes for labeled and unlabeled data,\nwhile the auxiliary network produces additional prototypes for mixed data\nprocessed by CutMix. Through prototype fusion, mixed prototypes provide extra\nsemantic information to both labeled and unlabeled prototypes. High-quality\nglobal prototypes for each class are formed by fusing two enhanced prototypes,\noptimizing the distribution of hidden embeddings used in consistency learning.\nExtensive experiments on the left atrium and type B aortic dissection datasets\ndemonstrate MPCL's superiority over previous state-of-the-art approaches,\nconfirming the effectiveness of our framework. The code will be released soon.\n", "link": "http://arxiv.org/abs/2404.10717v1", "date": "2024-04-16", "relevancy": 2.1255, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5159}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mixed%20Prototype%20Consistency%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Mixed%20Prototype%20Consistency%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Lijian%20Li%0AAbstract%3A%20%20%20Recently%2C%20prototype%20learning%20has%20emerged%20in%20semi-supervised%20medical%20image%0Asegmentation%20and%20achieved%20remarkable%20performance.%20However%2C%20the%20scarcity%20of%0Alabeled%20data%20limits%20the%20expressiveness%20of%20prototypes%20in%20previous%20methods%2C%0Apotentially%20hindering%20the%20complete%20representation%20of%20prototypes%20for%20class%0Aembedding.%20To%20address%20this%20problem%2C%20we%20propose%20the%20Mixed%20Prototype%20Consistency%0ALearning%20%28MPCL%29%20framework%2C%20which%20includes%20a%20Mean%20Teacher%20and%20an%20auxiliary%0Anetwork.%20The%20Mean%20Teacher%20generates%20prototypes%20for%20labeled%20and%20unlabeled%20data%2C%0Awhile%20the%20auxiliary%20network%20produces%20additional%20prototypes%20for%20mixed%20data%0Aprocessed%20by%20CutMix.%20Through%20prototype%20fusion%2C%20mixed%20prototypes%20provide%20extra%0Asemantic%20information%20to%20both%20labeled%20and%20unlabeled%20prototypes.%20High-quality%0Aglobal%20prototypes%20for%20each%20class%20are%20formed%20by%20fusing%20two%20enhanced%20prototypes%2C%0Aoptimizing%20the%20distribution%20of%20hidden%20embeddings%20used%20in%20consistency%20learning.%0AExtensive%20experiments%20on%20the%20left%20atrium%20and%20type%20B%20aortic%20dissection%20datasets%0Ademonstrate%20MPCL%27s%20superiority%20over%20previous%20state-of-the-art%20approaches%2C%0Aconfirming%20the%20effectiveness%20of%20our%20framework.%20The%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10717v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed%20Prototype%20Consistency%20Learning%20for%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Lijian%20Li&entry.1292438233=%20%20Recently%2C%20prototype%20learning%20has%20emerged%20in%20semi-supervised%20medical%20image%0Asegmentation%20and%20achieved%20remarkable%20performance.%20However%2C%20the%20scarcity%20of%0Alabeled%20data%20limits%20the%20expressiveness%20of%20prototypes%20in%20previous%20methods%2C%0Apotentially%20hindering%20the%20complete%20representation%20of%20prototypes%20for%20class%0Aembedding.%20To%20address%20this%20problem%2C%20we%20propose%20the%20Mixed%20Prototype%20Consistency%0ALearning%20%28MPCL%29%20framework%2C%20which%20includes%20a%20Mean%20Teacher%20and%20an%20auxiliary%0Anetwork.%20The%20Mean%20Teacher%20generates%20prototypes%20for%20labeled%20and%20unlabeled%20data%2C%0Awhile%20the%20auxiliary%20network%20produces%20additional%20prototypes%20for%20mixed%20data%0Aprocessed%20by%20CutMix.%20Through%20prototype%20fusion%2C%20mixed%20prototypes%20provide%20extra%0Asemantic%20information%20to%20both%20labeled%20and%20unlabeled%20prototypes.%20High-quality%0Aglobal%20prototypes%20for%20each%20class%20are%20formed%20by%20fusing%20two%20enhanced%20prototypes%2C%0Aoptimizing%20the%20distribution%20of%20hidden%20embeddings%20used%20in%20consistency%20learning.%0AExtensive%20experiments%20on%20the%20left%20atrium%20and%20type%20B%20aortic%20dissection%20datasets%0Ademonstrate%20MPCL%27s%20superiority%20over%20previous%20state-of-the-art%20approaches%2C%0Aconfirming%20the%20effectiveness%20of%20our%20framework.%20The%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10717v1&entry.124074799=Read"},
{"title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction", "author": "Stanislaw Szymanowicz and Christian Rupprecht and Andrea Vedaldi", "abstract": "  We introduce the \\method, an ultra-efficient approach for monocular 3D object\nreconstruction. Splatter Image is based on Gaussian Splatting, which allows\nfast and high-quality reconstruction of 3D scenes from multiple images. We\napply Gaussian Splatting to monocular reconstruction by learning a neural\nnetwork that, at test time, performs reconstruction in a feed-forward manner,\nat 38 FPS. Our main innovation is the surprisingly straightforward design of\nthis network, which, using 2D operators, maps the input image to one 3D\nGaussian per pixel. The resulting set of Gaussians thus has the form an image,\nthe Splatter Image. We further extend the method take several images as input\nvia cross-view attention. Owning to the speed of the renderer (588 FPS), we use\na single GPU for training while generating entire images at each iteration to\noptimize perceptual metrics like LPIPS. On several synthetic, real,\nmulti-category and large-scale benchmark datasets, we achieve better results in\nterms of PSNR, LPIPS, and other metrics while training and evaluating much\nfaster than prior works. Code, models, demo and more results are available at\nhttps://szymanowiczs.github.io/splatter-image.\n", "link": "http://arxiv.org/abs/2312.13150v2", "date": "2024-04-16", "relevancy": 2.1069, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5135}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Splatter%20Image%3A%20Ultra-Fast%20Single-View%203D%20Reconstruction&body=Title%3A%20Splatter%20Image%3A%20Ultra-Fast%20Single-View%203D%20Reconstruction%0AAuthor%3A%20Stanislaw%20Szymanowicz%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20introduce%20the%20%5Cmethod%2C%20an%20ultra-efficient%20approach%20for%20monocular%203D%20object%0Areconstruction.%20Splatter%20Image%20is%20based%20on%20Gaussian%20Splatting%2C%20which%20allows%0Afast%20and%20high-quality%20reconstruction%20of%203D%20scenes%20from%20multiple%20images.%20We%0Aapply%20Gaussian%20Splatting%20to%20monocular%20reconstruction%20by%20learning%20a%20neural%0Anetwork%20that%2C%20at%20test%20time%2C%20performs%20reconstruction%20in%20a%20feed-forward%20manner%2C%0Aat%2038%20FPS.%20Our%20main%20innovation%20is%20the%20surprisingly%20straightforward%20design%20of%0Athis%20network%2C%20which%2C%20using%202D%20operators%2C%20maps%20the%20input%20image%20to%20one%203D%0AGaussian%20per%20pixel.%20The%20resulting%20set%20of%20Gaussians%20thus%20has%20the%20form%20an%20image%2C%0Athe%20Splatter%20Image.%20We%20further%20extend%20the%20method%20take%20several%20images%20as%20input%0Avia%20cross-view%20attention.%20Owning%20to%20the%20speed%20of%20the%20renderer%20%28588%20FPS%29%2C%20we%20use%0Aa%20single%20GPU%20for%20training%20while%20generating%20entire%20images%20at%20each%20iteration%20to%0Aoptimize%20perceptual%20metrics%20like%20LPIPS.%20On%20several%20synthetic%2C%20real%2C%0Amulti-category%20and%20large-scale%20benchmark%20datasets%2C%20we%20achieve%20better%20results%20in%0Aterms%20of%20PSNR%2C%20LPIPS%2C%20and%20other%20metrics%20while%20training%20and%20evaluating%20much%0Afaster%20than%20prior%20works.%20Code%2C%20models%2C%20demo%20and%20more%20results%20are%20available%20at%0Ahttps%3A//szymanowiczs.github.io/splatter-image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13150v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatter%20Image%3A%20Ultra-Fast%20Single-View%203D%20Reconstruction&entry.906535625=Stanislaw%20Szymanowicz%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20introduce%20the%20%5Cmethod%2C%20an%20ultra-efficient%20approach%20for%20monocular%203D%20object%0Areconstruction.%20Splatter%20Image%20is%20based%20on%20Gaussian%20Splatting%2C%20which%20allows%0Afast%20and%20high-quality%20reconstruction%20of%203D%20scenes%20from%20multiple%20images.%20We%0Aapply%20Gaussian%20Splatting%20to%20monocular%20reconstruction%20by%20learning%20a%20neural%0Anetwork%20that%2C%20at%20test%20time%2C%20performs%20reconstruction%20in%20a%20feed-forward%20manner%2C%0Aat%2038%20FPS.%20Our%20main%20innovation%20is%20the%20surprisingly%20straightforward%20design%20of%0Athis%20network%2C%20which%2C%20using%202D%20operators%2C%20maps%20the%20input%20image%20to%20one%203D%0AGaussian%20per%20pixel.%20The%20resulting%20set%20of%20Gaussians%20thus%20has%20the%20form%20an%20image%2C%0Athe%20Splatter%20Image.%20We%20further%20extend%20the%20method%20take%20several%20images%20as%20input%0Avia%20cross-view%20attention.%20Owning%20to%20the%20speed%20of%20the%20renderer%20%28588%20FPS%29%2C%20we%20use%0Aa%20single%20GPU%20for%20training%20while%20generating%20entire%20images%20at%20each%20iteration%20to%0Aoptimize%20perceptual%20metrics%20like%20LPIPS.%20On%20several%20synthetic%2C%20real%2C%0Amulti-category%20and%20large-scale%20benchmark%20datasets%2C%20we%20achieve%20better%20results%20in%0Aterms%20of%20PSNR%2C%20LPIPS%2C%20and%20other%20metrics%20while%20training%20and%20evaluating%20much%0Afaster%20than%20prior%20works.%20Code%2C%20models%2C%20demo%20and%20more%20results%20are%20available%20at%0Ahttps%3A//szymanowiczs.github.io/splatter-image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13150v2&entry.124074799=Read"},
{"title": "Toward a Realistic Benchmark for Out-of-Distribution Detection", "author": "Pietro Recalcati and Fabio Garcea and Luca Piano and Fabrizio Lamberti and Lia Morra", "abstract": "  Deep neural networks are increasingly used in a wide range of technologies\nand services, but remain highly susceptible to out-of-distribution (OOD)\nsamples, that is, drawn from a different distribution than the original\ntraining set. A common approach to address this issue is to endow deep neural\nnetworks with the ability to detect OOD samples. Several benchmarks have been\nproposed to design and validate OOD detection techniques. However, many of them\nare based on far-OOD samples drawn from very different distributions, and thus\nlack the complexity needed to capture the nuances of real-world scenarios. In\nthis work, we introduce a comprehensive benchmark for OOD detection, based on\nImageNet and Places365, that assigns individual classes as in-distribution or\nout-of-distribution depending on the semantic similarity with the training set.\nSeveral techniques can be used to determine which classes should be considered\nin-distribution, yielding benchmarks with varying properties. Experimental\nresults on different OOD detection techniques show how their measured efficacy\ndepends on the selected benchmark and how confidence-based techniques may\noutperform classifier-based ones on near-OOD samples.\n", "link": "http://arxiv.org/abs/2404.10474v1", "date": "2024-04-16", "relevancy": 2.0948, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5358}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5279}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5099}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Realistic%20Benchmark%20for%20Out-of-Distribution%20Detection&body=Title%3A%20Toward%20a%20Realistic%20Benchmark%20for%20Out-of-Distribution%20Detection%0AAuthor%3A%20Pietro%20Recalcati%20and%20Fabio%20Garcea%20and%20Luca%20Piano%20and%20Fabrizio%20Lamberti%20and%20Lia%20Morra%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20increasingly%20used%20in%20a%20wide%20range%20of%20technologies%0Aand%20services%2C%20but%20remain%20highly%20susceptible%20to%20out-of-distribution%20%28OOD%29%0Asamples%2C%20that%20is%2C%20drawn%20from%20a%20different%20distribution%20than%20the%20original%0Atraining%20set.%20A%20common%20approach%20to%20address%20this%20issue%20is%20to%20endow%20deep%20neural%0Anetworks%20with%20the%20ability%20to%20detect%20OOD%20samples.%20Several%20benchmarks%20have%20been%0Aproposed%20to%20design%20and%20validate%20OOD%20detection%20techniques.%20However%2C%20many%20of%20them%0Aare%20based%20on%20far-OOD%20samples%20drawn%20from%20very%20different%20distributions%2C%20and%20thus%0Alack%20the%20complexity%20needed%20to%20capture%20the%20nuances%20of%20real-world%20scenarios.%20In%0Athis%20work%2C%20we%20introduce%20a%20comprehensive%20benchmark%20for%20OOD%20detection%2C%20based%20on%0AImageNet%20and%20Places365%2C%20that%20assigns%20individual%20classes%20as%20in-distribution%20or%0Aout-of-distribution%20depending%20on%20the%20semantic%20similarity%20with%20the%20training%20set.%0ASeveral%20techniques%20can%20be%20used%20to%20determine%20which%20classes%20should%20be%20considered%0Ain-distribution%2C%20yielding%20benchmarks%20with%20varying%20properties.%20Experimental%0Aresults%20on%20different%20OOD%20detection%20techniques%20show%20how%20their%20measured%20efficacy%0Adepends%20on%20the%20selected%20benchmark%20and%20how%20confidence-based%20techniques%20may%0Aoutperform%20classifier-based%20ones%20on%20near-OOD%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10474v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Realistic%20Benchmark%20for%20Out-of-Distribution%20Detection&entry.906535625=Pietro%20Recalcati%20and%20Fabio%20Garcea%20and%20Luca%20Piano%20and%20Fabrizio%20Lamberti%20and%20Lia%20Morra&entry.1292438233=%20%20Deep%20neural%20networks%20are%20increasingly%20used%20in%20a%20wide%20range%20of%20technologies%0Aand%20services%2C%20but%20remain%20highly%20susceptible%20to%20out-of-distribution%20%28OOD%29%0Asamples%2C%20that%20is%2C%20drawn%20from%20a%20different%20distribution%20than%20the%20original%0Atraining%20set.%20A%20common%20approach%20to%20address%20this%20issue%20is%20to%20endow%20deep%20neural%0Anetworks%20with%20the%20ability%20to%20detect%20OOD%20samples.%20Several%20benchmarks%20have%20been%0Aproposed%20to%20design%20and%20validate%20OOD%20detection%20techniques.%20However%2C%20many%20of%20them%0Aare%20based%20on%20far-OOD%20samples%20drawn%20from%20very%20different%20distributions%2C%20and%20thus%0Alack%20the%20complexity%20needed%20to%20capture%20the%20nuances%20of%20real-world%20scenarios.%20In%0Athis%20work%2C%20we%20introduce%20a%20comprehensive%20benchmark%20for%20OOD%20detection%2C%20based%20on%0AImageNet%20and%20Places365%2C%20that%20assigns%20individual%20classes%20as%20in-distribution%20or%0Aout-of-distribution%20depending%20on%20the%20semantic%20similarity%20with%20the%20training%20set.%0ASeveral%20techniques%20can%20be%20used%20to%20determine%20which%20classes%20should%20be%20considered%0Ain-distribution%2C%20yielding%20benchmarks%20with%20varying%20properties.%20Experimental%0Aresults%20on%20different%20OOD%20detection%20techniques%20show%20how%20their%20measured%20efficacy%0Adepends%20on%20the%20selected%20benchmark%20and%20how%20confidence-based%20techniques%20may%0Aoutperform%20classifier-based%20ones%20on%20near-OOD%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10474v1&entry.124074799=Read"},
{"title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for\n  Real-time Human Novel View Synthesis", "author": "Shunyuan Zheng and Boyao Zhou and Ruizhi Shao and Boning Liu and Shengping Zhang and Liqiang Nie and Yebin Liu", "abstract": "  We present a new approach, termed GPS-Gaussian, for synthesizing novel views\nof a character in a real-time manner. The proposed method enables 2K-resolution\nrendering under a sparse-view camera setting. Unlike the original Gaussian\nSplatting or neural implicit rendering methods that necessitate per-subject\noptimizations, we introduce Gaussian parameter maps defined on the source views\nand regress directly Gaussian Splatting properties for instant novel view\nsynthesis without any fine-tuning or optimization. To this end, we train our\nGaussian parameter regression module on a large amount of human scan data,\njointly with a depth estimation module to lift 2D parameter maps to 3D space.\nThe proposed framework is fully differentiable and experiments on several\ndatasets demonstrate that our method outperforms state-of-the-art methods while\nachieving an exceeding rendering speed.\n", "link": "http://arxiv.org/abs/2312.02155v3", "date": "2024-04-16", "relevancy": 2.0946, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5374}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5172}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GPS-Gaussian%3A%20Generalizable%20Pixel-wise%203D%20Gaussian%20Splatting%20for%0A%20%20Real-time%20Human%20Novel%20View%20Synthesis&body=Title%3A%20GPS-Gaussian%3A%20Generalizable%20Pixel-wise%203D%20Gaussian%20Splatting%20for%0A%20%20Real-time%20Human%20Novel%20View%20Synthesis%0AAuthor%3A%20Shunyuan%20Zheng%20and%20Boyao%20Zhou%20and%20Ruizhi%20Shao%20and%20Boning%20Liu%20and%20Shengping%20Zhang%20and%20Liqiang%20Nie%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%2C%20termed%20GPS-Gaussian%2C%20for%20synthesizing%20novel%20views%0Aof%20a%20character%20in%20a%20real-time%20manner.%20The%20proposed%20method%20enables%202K-resolution%0Arendering%20under%20a%20sparse-view%20camera%20setting.%20Unlike%20the%20original%20Gaussian%0ASplatting%20or%20neural%20implicit%20rendering%20methods%20that%20necessitate%20per-subject%0Aoptimizations%2C%20we%20introduce%20Gaussian%20parameter%20maps%20defined%20on%20the%20source%20views%0Aand%20regress%20directly%20Gaussian%20Splatting%20properties%20for%20instant%20novel%20view%0Asynthesis%20without%20any%20fine-tuning%20or%20optimization.%20To%20this%20end%2C%20we%20train%20our%0AGaussian%20parameter%20regression%20module%20on%20a%20large%20amount%20of%20human%20scan%20data%2C%0Ajointly%20with%20a%20depth%20estimation%20module%20to%20lift%202D%20parameter%20maps%20to%203D%20space.%0AThe%20proposed%20framework%20is%20fully%20differentiable%20and%20experiments%20on%20several%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20while%0Aachieving%20an%20exceeding%20rendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02155v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPS-Gaussian%3A%20Generalizable%20Pixel-wise%203D%20Gaussian%20Splatting%20for%0A%20%20Real-time%20Human%20Novel%20View%20Synthesis&entry.906535625=Shunyuan%20Zheng%20and%20Boyao%20Zhou%20and%20Ruizhi%20Shao%20and%20Boning%20Liu%20and%20Shengping%20Zhang%20and%20Liqiang%20Nie%20and%20Yebin%20Liu&entry.1292438233=%20%20We%20present%20a%20new%20approach%2C%20termed%20GPS-Gaussian%2C%20for%20synthesizing%20novel%20views%0Aof%20a%20character%20in%20a%20real-time%20manner.%20The%20proposed%20method%20enables%202K-resolution%0Arendering%20under%20a%20sparse-view%20camera%20setting.%20Unlike%20the%20original%20Gaussian%0ASplatting%20or%20neural%20implicit%20rendering%20methods%20that%20necessitate%20per-subject%0Aoptimizations%2C%20we%20introduce%20Gaussian%20parameter%20maps%20defined%20on%20the%20source%20views%0Aand%20regress%20directly%20Gaussian%20Splatting%20properties%20for%20instant%20novel%20view%0Asynthesis%20without%20any%20fine-tuning%20or%20optimization.%20To%20this%20end%2C%20we%20train%20our%0AGaussian%20parameter%20regression%20module%20on%20a%20large%20amount%20of%20human%20scan%20data%2C%0Ajointly%20with%20a%20depth%20estimation%20module%20to%20lift%202D%20parameter%20maps%20to%203D%20space.%0AThe%20proposed%20framework%20is%20fully%20differentiable%20and%20experiments%20on%20several%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20while%0Aachieving%20an%20exceeding%20rendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02155v3&entry.124074799=Read"},
{"title": "MOWA: Multiple-in-One Image Warping Model", "author": "Kang Liao and Zongsheng Yue and Zhonghua Wu and Chen Change Loy", "abstract": "  While recent image warping approaches achieved remarkable success on existing\nbenchmarks, they still require training separate models for each specific task\nand cannot generalize well to different camera models or customized\nmanipulations. To address diverse types of warping in practice, we propose a\nMultiple-in-One image WArping model (named MOWA) in this work. Specifically, we\nmitigate the difficulty of multi-task learning by disentangling the motion\nestimation at both the region level and pixel level. To further enable dynamic\ntask-aware image warping, we introduce a lightweight point-based classifier\nthat predicts the task type, serving as prompts to modulate the feature maps\nfor better estimation. To our knowledge, this is the first work that solves\nmultiple practical warping tasks in one single model. Extensive experiments\ndemonstrate that our MOWA, which is trained on six tasks for multiple-in-one\nimage warping, outperforms state-of-the-art task-specific models across most\ntasks. Moreover, MOWA also exhibits promising potential to generalize into\nunseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code\nwill be made publicly available.\n", "link": "http://arxiv.org/abs/2404.10716v1", "date": "2024-04-16", "relevancy": 2.0934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5188}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4997}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model&body=Title%3A%20MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model%0AAuthor%3A%20Kang%20Liao%20and%20Zongsheng%20Yue%20and%20Zhonghua%20Wu%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20While%20recent%20image%20warping%20approaches%20achieved%20remarkable%20success%20on%20existing%0Abenchmarks%2C%20they%20still%20require%20training%20separate%20models%20for%20each%20specific%20task%0Aand%20cannot%20generalize%20well%20to%20different%20camera%20models%20or%20customized%0Amanipulations.%20To%20address%20diverse%20types%20of%20warping%20in%20practice%2C%20we%20propose%20a%0AMultiple-in-One%20image%20WArping%20model%20%28named%20MOWA%29%20in%20this%20work.%20Specifically%2C%20we%0Amitigate%20the%20difficulty%20of%20multi-task%20learning%20by%20disentangling%20the%20motion%0Aestimation%20at%20both%20the%20region%20level%20and%20pixel%20level.%20To%20further%20enable%20dynamic%0Atask-aware%20image%20warping%2C%20we%20introduce%20a%20lightweight%20point-based%20classifier%0Athat%20predicts%20the%20task%20type%2C%20serving%20as%20prompts%20to%20modulate%20the%20feature%20maps%0Afor%20better%20estimation.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%20solves%0Amultiple%20practical%20warping%20tasks%20in%20one%20single%20model.%20Extensive%20experiments%0Ademonstrate%20that%20our%20MOWA%2C%20which%20is%20trained%20on%20six%20tasks%20for%20multiple-in-one%0Aimage%20warping%2C%20outperforms%20state-of-the-art%20task-specific%20models%20across%20most%0Atasks.%20Moreover%2C%20MOWA%20also%20exhibits%20promising%20potential%20to%20generalize%20into%0Aunseen%20scenes%2C%20as%20evidenced%20by%20cross-domain%20and%20zero-shot%20evaluations.%20The%20code%0Awill%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10716v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model&entry.906535625=Kang%20Liao%20and%20Zongsheng%20Yue%20and%20Zhonghua%20Wu%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20While%20recent%20image%20warping%20approaches%20achieved%20remarkable%20success%20on%20existing%0Abenchmarks%2C%20they%20still%20require%20training%20separate%20models%20for%20each%20specific%20task%0Aand%20cannot%20generalize%20well%20to%20different%20camera%20models%20or%20customized%0Amanipulations.%20To%20address%20diverse%20types%20of%20warping%20in%20practice%2C%20we%20propose%20a%0AMultiple-in-One%20image%20WArping%20model%20%28named%20MOWA%29%20in%20this%20work.%20Specifically%2C%20we%0Amitigate%20the%20difficulty%20of%20multi-task%20learning%20by%20disentangling%20the%20motion%0Aestimation%20at%20both%20the%20region%20level%20and%20pixel%20level.%20To%20further%20enable%20dynamic%0Atask-aware%20image%20warping%2C%20we%20introduce%20a%20lightweight%20point-based%20classifier%0Athat%20predicts%20the%20task%20type%2C%20serving%20as%20prompts%20to%20modulate%20the%20feature%20maps%0Afor%20better%20estimation.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%20solves%0Amultiple%20practical%20warping%20tasks%20in%20one%20single%20model.%20Extensive%20experiments%0Ademonstrate%20that%20our%20MOWA%2C%20which%20is%20trained%20on%20six%20tasks%20for%20multiple-in-one%0Aimage%20warping%2C%20outperforms%20state-of-the-art%20task-specific%20models%20across%20most%0Atasks.%20Moreover%2C%20MOWA%20also%20exhibits%20promising%20potential%20to%20generalize%20into%0Aunseen%20scenes%2C%20as%20evidenced%20by%20cross-domain%20and%20zero-shot%20evaluations.%20The%20code%0Awill%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10716v1&entry.124074799=Read"},
{"title": "3D Human Scan With A Moving Event Camera", "author": "Kai Kohyama and Shintaro Shiba and Yoshimitsu Aoki", "abstract": "  Capturing a 3D human body is one of the important tasks in computer vision\nwith a wide range of applications such as virtual reality and sports analysis.\nHowever, conventional frame cameras are limited by their temporal resolution\nand dynamic range, which imposes constraints in real-world application setups.\nEvent cameras have the advantages of high temporal resolution and high dynamic\nrange (HDR), but the development of event-based methods is necessary to handle\ndata with different characteristics. This paper proposes a novel event-based\nmethod for 3D pose estimation and human mesh recovery. Prior work on\nevent-based human mesh recovery require frames (images) as well as event data.\nThe proposed method solely relies on events; it carves 3D voxels by moving the\nevent camera around a stationary body, reconstructs the human pose and mesh by\nattenuated rays, and fit statistical body models, preserving high-frequency\ndetails. The experimental results show that the proposed method outperforms\nconventional frame-based methods in the estimation accuracy of both pose and\nbody mesh. We also demonstrate results in challenging situations where a\nconventional camera has motion blur. This is the first to demonstrate\nevent-only human mesh recovery, and we hope that it is the first step toward\nachieving robust and accurate 3D human body scanning from vision sensors.\nhttps://florpeng.github.io/event-based-human-scan/\n", "link": "http://arxiv.org/abs/2404.08504v2", "date": "2024-04-16", "relevancy": 2.0857, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5405}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera&body=Title%3A%203D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera%0AAuthor%3A%20Kai%20Kohyama%20and%20Shintaro%20Shiba%20and%20Yoshimitsu%20Aoki%0AAbstract%3A%20%20%20Capturing%20a%203D%20human%20body%20is%20one%20of%20the%20important%20tasks%20in%20computer%20vision%0Awith%20a%20wide%20range%20of%20applications%20such%20as%20virtual%20reality%20and%20sports%20analysis.%0AHowever%2C%20conventional%20frame%20cameras%20are%20limited%20by%20their%20temporal%20resolution%0Aand%20dynamic%20range%2C%20which%20imposes%20constraints%20in%20real-world%20application%20setups.%0AEvent%20cameras%20have%20the%20advantages%20of%20high%20temporal%20resolution%20and%20high%20dynamic%0Arange%20%28HDR%29%2C%20but%20the%20development%20of%20event-based%20methods%20is%20necessary%20to%20handle%0Adata%20with%20different%20characteristics.%20This%20paper%20proposes%20a%20novel%20event-based%0Amethod%20for%203D%20pose%20estimation%20and%20human%20mesh%20recovery.%20Prior%20work%20on%0Aevent-based%20human%20mesh%20recovery%20require%20frames%20%28images%29%20as%20well%20as%20event%20data.%0AThe%20proposed%20method%20solely%20relies%20on%20events%3B%20it%20carves%203D%20voxels%20by%20moving%20the%0Aevent%20camera%20around%20a%20stationary%20body%2C%20reconstructs%20the%20human%20pose%20and%20mesh%20by%0Aattenuated%20rays%2C%20and%20fit%20statistical%20body%20models%2C%20preserving%20high-frequency%0Adetails.%20The%20experimental%20results%20show%20that%20the%20proposed%20method%20outperforms%0Aconventional%20frame-based%20methods%20in%20the%20estimation%20accuracy%20of%20both%20pose%20and%0Abody%20mesh.%20We%20also%20demonstrate%20results%20in%20challenging%20situations%20where%20a%0Aconventional%20camera%20has%20motion%20blur.%20This%20is%20the%20first%20to%20demonstrate%0Aevent-only%20human%20mesh%20recovery%2C%20and%20we%20hope%20that%20it%20is%20the%20first%20step%20toward%0Aachieving%20robust%20and%20accurate%203D%20human%20body%20scanning%20from%20vision%20sensors.%0Ahttps%3A//florpeng.github.io/event-based-human-scan/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08504v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Scan%20With%20A%20Moving%20Event%20Camera&entry.906535625=Kai%20Kohyama%20and%20Shintaro%20Shiba%20and%20Yoshimitsu%20Aoki&entry.1292438233=%20%20Capturing%20a%203D%20human%20body%20is%20one%20of%20the%20important%20tasks%20in%20computer%20vision%0Awith%20a%20wide%20range%20of%20applications%20such%20as%20virtual%20reality%20and%20sports%20analysis.%0AHowever%2C%20conventional%20frame%20cameras%20are%20limited%20by%20their%20temporal%20resolution%0Aand%20dynamic%20range%2C%20which%20imposes%20constraints%20in%20real-world%20application%20setups.%0AEvent%20cameras%20have%20the%20advantages%20of%20high%20temporal%20resolution%20and%20high%20dynamic%0Arange%20%28HDR%29%2C%20but%20the%20development%20of%20event-based%20methods%20is%20necessary%20to%20handle%0Adata%20with%20different%20characteristics.%20This%20paper%20proposes%20a%20novel%20event-based%0Amethod%20for%203D%20pose%20estimation%20and%20human%20mesh%20recovery.%20Prior%20work%20on%0Aevent-based%20human%20mesh%20recovery%20require%20frames%20%28images%29%20as%20well%20as%20event%20data.%0AThe%20proposed%20method%20solely%20relies%20on%20events%3B%20it%20carves%203D%20voxels%20by%20moving%20the%0Aevent%20camera%20around%20a%20stationary%20body%2C%20reconstructs%20the%20human%20pose%20and%20mesh%20by%0Aattenuated%20rays%2C%20and%20fit%20statistical%20body%20models%2C%20preserving%20high-frequency%0Adetails.%20The%20experimental%20results%20show%20that%20the%20proposed%20method%20outperforms%0Aconventional%20frame-based%20methods%20in%20the%20estimation%20accuracy%20of%20both%20pose%20and%0Abody%20mesh.%20We%20also%20demonstrate%20results%20in%20challenging%20situations%20where%20a%0Aconventional%20camera%20has%20motion%20blur.%20This%20is%20the%20first%20to%20demonstrate%0Aevent-only%20human%20mesh%20recovery%2C%20and%20we%20hope%20that%20it%20is%20the%20first%20step%20toward%0Aachieving%20robust%20and%20accurate%203D%20human%20body%20scanning%20from%20vision%20sensors.%0Ahttps%3A//florpeng.github.io/event-based-human-scan/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08504v2&entry.124074799=Read"},
{"title": "Network architecture search of X-ray based scientific applications", "author": "Adarsha Balaji and Ramyad Hadidi and Gregory Kollmer and Mohammed E. Fouda and Prasanna Balaprakash", "abstract": "  X-ray and electron diffraction-based microscopy use bragg peak detection and\nptychography to perform 3-D imaging at an atomic resolution. Typically, these\ntechniques are implemented using computationally complex tasks such as a\nPsuedo-Voigt function or solving a complex inverse problem. Recently, the use\nof deep neural networks has improved the existing state-of-the-art approaches.\nHowever, the design and development of the neural network models depends on\ntime and labor intensive tuning of the model by application experts. To that\nend, we propose a hyperparameter (HPS) and neural architecture search (NAS)\napproach to automate the design and optimization of the neural network models\nfor model size, energy consumption and throughput. We demonstrate the improved\nperformance of the auto-tuned models when compared to the manually tuned\nBraggNN and PtychoNN benchmark. We study and demonstrate the importance of the\nexploring the search space of tunable hyperparameters in enhancing the\nperformance of bragg peak detection and ptychographic reconstruction. Our NAS\nand HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection\naccuracy with a 87.57\\% reduction in model size, and (2) PtychoNN achieves a\n16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size\nwhen compared to the baseline PtychoNN model. When inferred on the Orin-AGX\nplatform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and\n9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in\nenergy consumption when compared to their respective baselines, when inferred\nin the Orin-AGX edge platform.\n", "link": "http://arxiv.org/abs/2404.10689v1", "date": "2024-04-16", "relevancy": 2.0653, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5721}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5283}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Network%20architecture%20search%20of%20X-ray%20based%20scientific%20applications&body=Title%3A%20Network%20architecture%20search%20of%20X-ray%20based%20scientific%20applications%0AAuthor%3A%20Adarsha%20Balaji%20and%20Ramyad%20Hadidi%20and%20Gregory%20Kollmer%20and%20Mohammed%20E.%20Fouda%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20X-ray%20and%20electron%20diffraction-based%20microscopy%20use%20bragg%20peak%20detection%20and%0Aptychography%20to%20perform%203-D%20imaging%20at%20an%20atomic%20resolution.%20Typically%2C%20these%0Atechniques%20are%20implemented%20using%20computationally%20complex%20tasks%20such%20as%20a%0APsuedo-Voigt%20function%20or%20solving%20a%20complex%20inverse%20problem.%20Recently%2C%20the%20use%0Aof%20deep%20neural%20networks%20has%20improved%20the%20existing%20state-of-the-art%20approaches.%0AHowever%2C%20the%20design%20and%20development%20of%20the%20neural%20network%20models%20depends%20on%0Atime%20and%20labor%20intensive%20tuning%20of%20the%20model%20by%20application%20experts.%20To%20that%0Aend%2C%20we%20propose%20a%20hyperparameter%20%28HPS%29%20and%20neural%20architecture%20search%20%28NAS%29%0Aapproach%20to%20automate%20the%20design%20and%20optimization%20of%20the%20neural%20network%20models%0Afor%20model%20size%2C%20energy%20consumption%20and%20throughput.%20We%20demonstrate%20the%20improved%0Aperformance%20of%20the%20auto-tuned%20models%20when%20compared%20to%20the%20manually%20tuned%0ABraggNN%20and%20PtychoNN%20benchmark.%20We%20study%20and%20demonstrate%20the%20importance%20of%20the%0Aexploring%20the%20search%20space%20of%20tunable%20hyperparameters%20in%20enhancing%20the%0Aperformance%20of%20bragg%20peak%20detection%20and%20ptychographic%20reconstruction.%20Our%20NAS%0Aand%20HPS%20of%20%281%29%20BraggNN%20achieves%20a%2031.03%5C%25%20improvement%20in%20bragg%20peak%20detection%0Aaccuracy%20with%20a%2087.57%5C%25%20reduction%20in%20model%20size%2C%20and%20%282%29%20PtychoNN%20achieves%20a%0A16.77%5C%25%20improvement%20in%20model%20accuracy%20and%20a%2012.82%5C%25%20reduction%20in%20model%20size%0Awhen%20compared%20to%20the%20baseline%20PtychoNN%20model.%20When%20inferred%20on%20the%20Orin-AGX%0Aplatform%2C%20the%20optimized%20Braggnn%20and%20Ptychonn%20models%20demonstrate%20a%2010.51%5C%25%20and%0A9.47%5C%25%20reduction%20in%20inference%20latency%20and%20a%2044.18%5C%25%20and%2015.34%5C%25%20reduction%20in%0Aenergy%20consumption%20when%20compared%20to%20their%20respective%20baselines%2C%20when%20inferred%0Ain%20the%20Orin-AGX%20edge%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20architecture%20search%20of%20X-ray%20based%20scientific%20applications&entry.906535625=Adarsha%20Balaji%20and%20Ramyad%20Hadidi%20and%20Gregory%20Kollmer%20and%20Mohammed%20E.%20Fouda%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20X-ray%20and%20electron%20diffraction-based%20microscopy%20use%20bragg%20peak%20detection%20and%0Aptychography%20to%20perform%203-D%20imaging%20at%20an%20atomic%20resolution.%20Typically%2C%20these%0Atechniques%20are%20implemented%20using%20computationally%20complex%20tasks%20such%20as%20a%0APsuedo-Voigt%20function%20or%20solving%20a%20complex%20inverse%20problem.%20Recently%2C%20the%20use%0Aof%20deep%20neural%20networks%20has%20improved%20the%20existing%20state-of-the-art%20approaches.%0AHowever%2C%20the%20design%20and%20development%20of%20the%20neural%20network%20models%20depends%20on%0Atime%20and%20labor%20intensive%20tuning%20of%20the%20model%20by%20application%20experts.%20To%20that%0Aend%2C%20we%20propose%20a%20hyperparameter%20%28HPS%29%20and%20neural%20architecture%20search%20%28NAS%29%0Aapproach%20to%20automate%20the%20design%20and%20optimization%20of%20the%20neural%20network%20models%0Afor%20model%20size%2C%20energy%20consumption%20and%20throughput.%20We%20demonstrate%20the%20improved%0Aperformance%20of%20the%20auto-tuned%20models%20when%20compared%20to%20the%20manually%20tuned%0ABraggNN%20and%20PtychoNN%20benchmark.%20We%20study%20and%20demonstrate%20the%20importance%20of%20the%0Aexploring%20the%20search%20space%20of%20tunable%20hyperparameters%20in%20enhancing%20the%0Aperformance%20of%20bragg%20peak%20detection%20and%20ptychographic%20reconstruction.%20Our%20NAS%0Aand%20HPS%20of%20%281%29%20BraggNN%20achieves%20a%2031.03%5C%25%20improvement%20in%20bragg%20peak%20detection%0Aaccuracy%20with%20a%2087.57%5C%25%20reduction%20in%20model%20size%2C%20and%20%282%29%20PtychoNN%20achieves%20a%0A16.77%5C%25%20improvement%20in%20model%20accuracy%20and%20a%2012.82%5C%25%20reduction%20in%20model%20size%0Awhen%20compared%20to%20the%20baseline%20PtychoNN%20model.%20When%20inferred%20on%20the%20Orin-AGX%0Aplatform%2C%20the%20optimized%20Braggnn%20and%20Ptychonn%20models%20demonstrate%20a%2010.51%5C%25%20and%0A9.47%5C%25%20reduction%20in%20inference%20latency%20and%20a%2044.18%5C%25%20and%2015.34%5C%25%20reduction%20in%0Aenergy%20consumption%20when%20compared%20to%20their%20respective%20baselines%2C%20when%20inferred%0Ain%20the%20Orin-AGX%20edge%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10689v1&entry.124074799=Read"},
{"title": "MobileNetV4 - Universal Models for the Mobile Ecosystem", "author": "Danfeng Qin and Chas Leichner and Manolis Delakis and Marco Fornoni and Shixin Luo and Fan Yang and Weijun Wang and Colby Banbury and Chengxi Ye and Berkin Akin and Vaibhav Aggarwal and Tenghui Zhu and Daniele Moro and Andrew Howard", "abstract": "  We present the latest generation of MobileNets, known as MobileNetV4 (MNv4),\nfeaturing universally efficient architecture designs for mobile devices. At its\ncore, we introduce the Universal Inverted Bottleneck (UIB) search block, a\nunified and flexible structure that merges Inverted Bottleneck (IB), ConvNext,\nFeed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant.\nAlongside UIB, we present Mobile MQA, an attention block tailored for mobile\naccelerators, delivering a significant 39% speedup. An optimized neural\narchitecture search (NAS) recipe is also introduced which improves MNv4 search\neffectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe\nresults in a new suite of MNv4 models that are mostly Pareto optimal across\nmobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural\nEngine and Google Pixel EdgeTPU - a characteristic not found in any other\nmodels tested. Finally, to further boost accuracy, we introduce a novel\ndistillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model\ndelivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just\n3.8ms.\n", "link": "http://arxiv.org/abs/2404.10518v1", "date": "2024-04-16", "relevancy": 2.0594, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5398}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4938}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MobileNetV4%20-%20Universal%20Models%20for%20the%20Mobile%20Ecosystem&body=Title%3A%20MobileNetV4%20-%20Universal%20Models%20for%20the%20Mobile%20Ecosystem%0AAuthor%3A%20Danfeng%20Qin%20and%20Chas%20Leichner%20and%20Manolis%20Delakis%20and%20Marco%20Fornoni%20and%20Shixin%20Luo%20and%20Fan%20Yang%20and%20Weijun%20Wang%20and%20Colby%20Banbury%20and%20Chengxi%20Ye%20and%20Berkin%20Akin%20and%20Vaibhav%20Aggarwal%20and%20Tenghui%20Zhu%20and%20Daniele%20Moro%20and%20Andrew%20Howard%0AAbstract%3A%20%20%20We%20present%20the%20latest%20generation%20of%20MobileNets%2C%20known%20as%20MobileNetV4%20%28MNv4%29%2C%0Afeaturing%20universally%20efficient%20architecture%20designs%20for%20mobile%20devices.%20At%20its%0Acore%2C%20we%20introduce%20the%20Universal%20Inverted%20Bottleneck%20%28UIB%29%20search%20block%2C%20a%0Aunified%20and%20flexible%20structure%20that%20merges%20Inverted%20Bottleneck%20%28IB%29%2C%20ConvNext%2C%0AFeed%20Forward%20Network%20%28FFN%29%2C%20and%20a%20novel%20Extra%20Depthwise%20%28ExtraDW%29%20variant.%0AAlongside%20UIB%2C%20we%20present%20Mobile%20MQA%2C%20an%20attention%20block%20tailored%20for%20mobile%0Aaccelerators%2C%20delivering%20a%20significant%2039%25%20speedup.%20An%20optimized%20neural%0Aarchitecture%20search%20%28NAS%29%20recipe%20is%20also%20introduced%20which%20improves%20MNv4%20search%0Aeffectiveness.%20The%20integration%20of%20UIB%2C%20Mobile%20MQA%20and%20the%20refined%20NAS%20recipe%0Aresults%20in%20a%20new%20suite%20of%20MNv4%20models%20that%20are%20mostly%20Pareto%20optimal%20across%0Amobile%20CPUs%2C%20DSPs%2C%20GPUs%2C%20as%20well%20as%20specialized%20accelerators%20like%20Apple%20Neural%0AEngine%20and%20Google%20Pixel%20EdgeTPU%20-%20a%20characteristic%20not%20found%20in%20any%20other%0Amodels%20tested.%20Finally%2C%20to%20further%20boost%20accuracy%2C%20we%20introduce%20a%20novel%0Adistillation%20technique.%20Enhanced%20by%20this%20technique%2C%20our%20MNv4-Hybrid-Large%20model%0Adelivers%2087%25%20ImageNet-1K%20accuracy%2C%20with%20a%20Pixel%208%20EdgeTPU%20runtime%20of%20just%0A3.8ms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10518v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MobileNetV4%20-%20Universal%20Models%20for%20the%20Mobile%20Ecosystem&entry.906535625=Danfeng%20Qin%20and%20Chas%20Leichner%20and%20Manolis%20Delakis%20and%20Marco%20Fornoni%20and%20Shixin%20Luo%20and%20Fan%20Yang%20and%20Weijun%20Wang%20and%20Colby%20Banbury%20and%20Chengxi%20Ye%20and%20Berkin%20Akin%20and%20Vaibhav%20Aggarwal%20and%20Tenghui%20Zhu%20and%20Daniele%20Moro%20and%20Andrew%20Howard&entry.1292438233=%20%20We%20present%20the%20latest%20generation%20of%20MobileNets%2C%20known%20as%20MobileNetV4%20%28MNv4%29%2C%0Afeaturing%20universally%20efficient%20architecture%20designs%20for%20mobile%20devices.%20At%20its%0Acore%2C%20we%20introduce%20the%20Universal%20Inverted%20Bottleneck%20%28UIB%29%20search%20block%2C%20a%0Aunified%20and%20flexible%20structure%20that%20merges%20Inverted%20Bottleneck%20%28IB%29%2C%20ConvNext%2C%0AFeed%20Forward%20Network%20%28FFN%29%2C%20and%20a%20novel%20Extra%20Depthwise%20%28ExtraDW%29%20variant.%0AAlongside%20UIB%2C%20we%20present%20Mobile%20MQA%2C%20an%20attention%20block%20tailored%20for%20mobile%0Aaccelerators%2C%20delivering%20a%20significant%2039%25%20speedup.%20An%20optimized%20neural%0Aarchitecture%20search%20%28NAS%29%20recipe%20is%20also%20introduced%20which%20improves%20MNv4%20search%0Aeffectiveness.%20The%20integration%20of%20UIB%2C%20Mobile%20MQA%20and%20the%20refined%20NAS%20recipe%0Aresults%20in%20a%20new%20suite%20of%20MNv4%20models%20that%20are%20mostly%20Pareto%20optimal%20across%0Amobile%20CPUs%2C%20DSPs%2C%20GPUs%2C%20as%20well%20as%20specialized%20accelerators%20like%20Apple%20Neural%0AEngine%20and%20Google%20Pixel%20EdgeTPU%20-%20a%20characteristic%20not%20found%20in%20any%20other%0Amodels%20tested.%20Finally%2C%20to%20further%20boost%20accuracy%2C%20we%20introduce%20a%20novel%0Adistillation%20technique.%20Enhanced%20by%20this%20technique%2C%20our%20MNv4-Hybrid-Large%20model%0Adelivers%2087%25%20ImageNet-1K%20accuracy%2C%20with%20a%20Pixel%208%20EdgeTPU%20runtime%20of%20just%0A3.8ms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10518v1&entry.124074799=Read"},
{"title": "Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation\n  with Target-private Class Segregation", "author": "Mattia Litrico and Davide Talon and Sebastiano Battiato and Alessio Del Bue and Mario Valerio Giuffrida and Pietro Morerio", "abstract": "  Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from\na labeled source domain to an unlabeled target but usually requires\nsimultaneous access to both source and target data. Moreover, UDA approaches\ncommonly assume that source and target domains share the same labels space.\nYet, these two assumptions are hardly satisfied in real-world scenarios. This\npaper considers the more challenging Source-Free Open-set Domain Adaptation\n(SF-OSDA) setting, where both assumptions are dropped. We propose a novel\napproach for SF-OSDA that exploits the granularity of target-private categories\nby segregating their samples into multiple unknown classes. Starting from an\ninitial clustering-based assignment, our method progressively improves the\nsegregation of target-private samples by refining their pseudo-labels with the\nguide of an uncertainty-based sample selection module. Additionally, we propose\na novel contrastive loss, named NL-InfoNCELoss, that, integrating negative\nlearning into self-supervised contrastive learning, enhances the model\nrobustness to noisy pseudo-labels. Extensive experiments on benchmark datasets\ndemonstrate the superiority of the proposed method over existing approaches,\nestablishing new state-of-the-art performance. Notably, additional analyses\nshow that our method is able to learn the underlying semantics of novel\nclasses, opening the possibility to perform novel class discovery.\n", "link": "http://arxiv.org/abs/2404.10574v1", "date": "2024-04-16", "relevancy": 2.0565, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uncertainty-guided%20Open-Set%20Source-Free%20Unsupervised%20Domain%20Adaptation%0A%20%20with%20Target-private%20Class%20Segregation&body=Title%3A%20Uncertainty-guided%20Open-Set%20Source-Free%20Unsupervised%20Domain%20Adaptation%0A%20%20with%20Target-private%20Class%20Segregation%0AAuthor%3A%20Mattia%20Litrico%20and%20Davide%20Talon%20and%20Sebastiano%20Battiato%20and%20Alessio%20Del%20Bue%20and%20Mario%20Valerio%20Giuffrida%20and%20Pietro%20Morerio%0AAbstract%3A%20%20%20Standard%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20aims%20to%20transfer%20knowledge%20from%0Aa%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20but%20usually%20requires%0Asimultaneous%20access%20to%20both%20source%20and%20target%20data.%20Moreover%2C%20UDA%20approaches%0Acommonly%20assume%20that%20source%20and%20target%20domains%20share%20the%20same%20labels%20space.%0AYet%2C%20these%20two%20assumptions%20are%20hardly%20satisfied%20in%20real-world%20scenarios.%20This%0Apaper%20considers%20the%20more%20challenging%20Source-Free%20Open-set%20Domain%20Adaptation%0A%28SF-OSDA%29%20setting%2C%20where%20both%20assumptions%20are%20dropped.%20We%20propose%20a%20novel%0Aapproach%20for%20SF-OSDA%20that%20exploits%20the%20granularity%20of%20target-private%20categories%0Aby%20segregating%20their%20samples%20into%20multiple%20unknown%20classes.%20Starting%20from%20an%0Ainitial%20clustering-based%20assignment%2C%20our%20method%20progressively%20improves%20the%0Asegregation%20of%20target-private%20samples%20by%20refining%20their%20pseudo-labels%20with%20the%0Aguide%20of%20an%20uncertainty-based%20sample%20selection%20module.%20Additionally%2C%20we%20propose%0Aa%20novel%20contrastive%20loss%2C%20named%20NL-InfoNCELoss%2C%20that%2C%20integrating%20negative%0Alearning%20into%20self-supervised%20contrastive%20learning%2C%20enhances%20the%20model%0Arobustness%20to%20noisy%20pseudo-labels.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20over%20existing%20approaches%2C%0Aestablishing%20new%20state-of-the-art%20performance.%20Notably%2C%20additional%20analyses%0Ashow%20that%20our%20method%20is%20able%20to%20learn%20the%20underlying%20semantics%20of%20novel%0Aclasses%2C%20opening%20the%20possibility%20to%20perform%20novel%20class%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10574v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-guided%20Open-Set%20Source-Free%20Unsupervised%20Domain%20Adaptation%0A%20%20with%20Target-private%20Class%20Segregation&entry.906535625=Mattia%20Litrico%20and%20Davide%20Talon%20and%20Sebastiano%20Battiato%20and%20Alessio%20Del%20Bue%20and%20Mario%20Valerio%20Giuffrida%20and%20Pietro%20Morerio&entry.1292438233=%20%20Standard%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20aims%20to%20transfer%20knowledge%20from%0Aa%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20but%20usually%20requires%0Asimultaneous%20access%20to%20both%20source%20and%20target%20data.%20Moreover%2C%20UDA%20approaches%0Acommonly%20assume%20that%20source%20and%20target%20domains%20share%20the%20same%20labels%20space.%0AYet%2C%20these%20two%20assumptions%20are%20hardly%20satisfied%20in%20real-world%20scenarios.%20This%0Apaper%20considers%20the%20more%20challenging%20Source-Free%20Open-set%20Domain%20Adaptation%0A%28SF-OSDA%29%20setting%2C%20where%20both%20assumptions%20are%20dropped.%20We%20propose%20a%20novel%0Aapproach%20for%20SF-OSDA%20that%20exploits%20the%20granularity%20of%20target-private%20categories%0Aby%20segregating%20their%20samples%20into%20multiple%20unknown%20classes.%20Starting%20from%20an%0Ainitial%20clustering-based%20assignment%2C%20our%20method%20progressively%20improves%20the%0Asegregation%20of%20target-private%20samples%20by%20refining%20their%20pseudo-labels%20with%20the%0Aguide%20of%20an%20uncertainty-based%20sample%20selection%20module.%20Additionally%2C%20we%20propose%0Aa%20novel%20contrastive%20loss%2C%20named%20NL-InfoNCELoss%2C%20that%2C%20integrating%20negative%0Alearning%20into%20self-supervised%20contrastive%20learning%2C%20enhances%20the%20model%0Arobustness%20to%20noisy%20pseudo-labels.%20Extensive%20experiments%20on%20benchmark%20datasets%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20over%20existing%20approaches%2C%0Aestablishing%20new%20state-of-the-art%20performance.%20Notably%2C%20additional%20analyses%0Ashow%20that%20our%20method%20is%20able%20to%20learn%20the%20underlying%20semantics%20of%20novel%0Aclasses%2C%20opening%20the%20possibility%20to%20perform%20novel%20class%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10574v1&entry.124074799=Read"},
{"title": "Tree Bandits for Generative Bayes", "author": "Sean O'Hagan and Jungeum Kim and Veronika Rockova", "abstract": "  In generative models with obscured likelihood, Approximate Bayesian\nComputation (ABC) is often the tool of last resort for inference. However, ABC\ndemands many prior parameter trials to keep only a small fraction that passes\nan acceptance test. To accelerate ABC rejection sampling, this paper develops a\nself-aware framework that learns from past trials and errors. We apply\nrecursive partitioning classifiers on the ABC lookup table to sequentially\nrefine high-likelihood regions into boxes. Each box is regarded as an arm in a\nbinary bandit problem treating ABC acceptance as a reward. Each arm has a\nproclivity for being chosen for the next ABC evaluation, depending on the prior\ndistribution and past rejections. The method places more splits in those areas\nwhere the likelihood resides, shying away from low-probability regions destined\nfor ABC rejections. We provide two versions: (1) ABC-Tree for posterior\nsampling, and (2) ABC-MAP for maximum a posteriori estimation. We demonstrate\naccurate ABC approximability at much lower simulation cost. We justify the use\nof our tree-based bandit algorithms with nearly optimal regret bounds. Finally,\nwe successfully apply our approach to the problem of masked image\nclassification using deep generative models.\n", "link": "http://arxiv.org/abs/2404.10436v1", "date": "2024-04-16", "relevancy": 2.0563, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5262}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5142}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5019}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tree%20Bandits%20for%20Generative%20Bayes&body=Title%3A%20Tree%20Bandits%20for%20Generative%20Bayes%0AAuthor%3A%20Sean%20O%27Hagan%20and%20Jungeum%20Kim%20and%20Veronika%20Rockova%0AAbstract%3A%20%20%20In%20generative%20models%20with%20obscured%20likelihood%2C%20Approximate%20Bayesian%0AComputation%20%28ABC%29%20is%20often%20the%20tool%20of%20last%20resort%20for%20inference.%20However%2C%20ABC%0Ademands%20many%20prior%20parameter%20trials%20to%20keep%20only%20a%20small%20fraction%20that%20passes%0Aan%20acceptance%20test.%20To%20accelerate%20ABC%20rejection%20sampling%2C%20this%20paper%20develops%20a%0Aself-aware%20framework%20that%20learns%20from%20past%20trials%20and%20errors.%20We%20apply%0Arecursive%20partitioning%20classifiers%20on%20the%20ABC%20lookup%20table%20to%20sequentially%0Arefine%20high-likelihood%20regions%20into%20boxes.%20Each%20box%20is%20regarded%20as%20an%20arm%20in%20a%0Abinary%20bandit%20problem%20treating%20ABC%20acceptance%20as%20a%20reward.%20Each%20arm%20has%20a%0Aproclivity%20for%20being%20chosen%20for%20the%20next%20ABC%20evaluation%2C%20depending%20on%20the%20prior%0Adistribution%20and%20past%20rejections.%20The%20method%20places%20more%20splits%20in%20those%20areas%0Awhere%20the%20likelihood%20resides%2C%20shying%20away%20from%20low-probability%20regions%20destined%0Afor%20ABC%20rejections.%20We%20provide%20two%20versions%3A%20%281%29%20ABC-Tree%20for%20posterior%0Asampling%2C%20and%20%282%29%20ABC-MAP%20for%20maximum%20a%20posteriori%20estimation.%20We%20demonstrate%0Aaccurate%20ABC%20approximability%20at%20much%20lower%20simulation%20cost.%20We%20justify%20the%20use%0Aof%20our%20tree-based%20bandit%20algorithms%20with%20nearly%20optimal%20regret%20bounds.%20Finally%2C%0Awe%20successfully%20apply%20our%20approach%20to%20the%20problem%20of%20masked%20image%0Aclassification%20using%20deep%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10436v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20Bandits%20for%20Generative%20Bayes&entry.906535625=Sean%20O%27Hagan%20and%20Jungeum%20Kim%20and%20Veronika%20Rockova&entry.1292438233=%20%20In%20generative%20models%20with%20obscured%20likelihood%2C%20Approximate%20Bayesian%0AComputation%20%28ABC%29%20is%20often%20the%20tool%20of%20last%20resort%20for%20inference.%20However%2C%20ABC%0Ademands%20many%20prior%20parameter%20trials%20to%20keep%20only%20a%20small%20fraction%20that%20passes%0Aan%20acceptance%20test.%20To%20accelerate%20ABC%20rejection%20sampling%2C%20this%20paper%20develops%20a%0Aself-aware%20framework%20that%20learns%20from%20past%20trials%20and%20errors.%20We%20apply%0Arecursive%20partitioning%20classifiers%20on%20the%20ABC%20lookup%20table%20to%20sequentially%0Arefine%20high-likelihood%20regions%20into%20boxes.%20Each%20box%20is%20regarded%20as%20an%20arm%20in%20a%0Abinary%20bandit%20problem%20treating%20ABC%20acceptance%20as%20a%20reward.%20Each%20arm%20has%20a%0Aproclivity%20for%20being%20chosen%20for%20the%20next%20ABC%20evaluation%2C%20depending%20on%20the%20prior%0Adistribution%20and%20past%20rejections.%20The%20method%20places%20more%20splits%20in%20those%20areas%0Awhere%20the%20likelihood%20resides%2C%20shying%20away%20from%20low-probability%20regions%20destined%0Afor%20ABC%20rejections.%20We%20provide%20two%20versions%3A%20%281%29%20ABC-Tree%20for%20posterior%0Asampling%2C%20and%20%282%29%20ABC-MAP%20for%20maximum%20a%20posteriori%20estimation.%20We%20demonstrate%0Aaccurate%20ABC%20approximability%20at%20much%20lower%20simulation%20cost.%20We%20justify%20the%20use%0Aof%20our%20tree-based%20bandit%20algorithms%20with%20nearly%20optimal%20regret%20bounds.%20Finally%2C%0Awe%20successfully%20apply%20our%20approach%20to%20the%20problem%20of%20masked%20image%0Aclassification%20using%20deep%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10436v1&entry.124074799=Read"},
{"title": "Contextrast: Contextual Contrastive Learning for Semantic Segmentation", "author": "Changki Sung and Wanhee Kim and Jungho An and Wooju Lee and Hyungtae Lim and Hyun Myung", "abstract": "  Despite great improvements in semantic segmentation, challenges persist\nbecause of the lack of local/global contexts and the relationship between them.\nIn this paper, we propose Contextrast, a contrastive learning-based semantic\nsegmentation method that allows to capture local/global contexts and comprehend\ntheir relationships. Our proposed method comprises two parts: a) contextual\ncontrastive learning (CCL) and b) boundary-aware negative (BANE) sampling.\nContextual contrastive learning obtains local/global context from multi-scale\nfeature aggregation and inter/intra-relationship of features for better\ndiscrimination capabilities. Meanwhile, BANE sampling selects embedding\nfeatures along the boundaries of incorrectly predicted regions to employ them\nas harder negative samples on our contrastive learning, resolving segmentation\nissues along the boundary region by exploiting fine-grained details. We\ndemonstrate that our Contextrast substantially enhances the performance of\nsemantic segmentation networks, outperforming state-of-the-art contrastive\nlearning approaches on diverse public datasets, e.g. Cityscapes, CamVid,\nPASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational cost\nduring inference.\n", "link": "http://arxiv.org/abs/2404.10633v1", "date": "2024-04-16", "relevancy": 2.0489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5137}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5012}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contextrast%3A%20Contextual%20Contrastive%20Learning%20for%20Semantic%20Segmentation&body=Title%3A%20Contextrast%3A%20Contextual%20Contrastive%20Learning%20for%20Semantic%20Segmentation%0AAuthor%3A%20Changki%20Sung%20and%20Wanhee%20Kim%20and%20Jungho%20An%20and%20Wooju%20Lee%20and%20Hyungtae%20Lim%20and%20Hyun%20Myung%0AAbstract%3A%20%20%20Despite%20great%20improvements%20in%20semantic%20segmentation%2C%20challenges%20persist%0Abecause%20of%20the%20lack%20of%20local/global%20contexts%20and%20the%20relationship%20between%20them.%0AIn%20this%20paper%2C%20we%20propose%20Contextrast%2C%20a%20contrastive%20learning-based%20semantic%0Asegmentation%20method%20that%20allows%20to%20capture%20local/global%20contexts%20and%20comprehend%0Atheir%20relationships.%20Our%20proposed%20method%20comprises%20two%20parts%3A%20a%29%20contextual%0Acontrastive%20learning%20%28CCL%29%20and%20b%29%20boundary-aware%20negative%20%28BANE%29%20sampling.%0AContextual%20contrastive%20learning%20obtains%20local/global%20context%20from%20multi-scale%0Afeature%20aggregation%20and%20inter/intra-relationship%20of%20features%20for%20better%0Adiscrimination%20capabilities.%20Meanwhile%2C%20BANE%20sampling%20selects%20embedding%0Afeatures%20along%20the%20boundaries%20of%20incorrectly%20predicted%20regions%20to%20employ%20them%0Aas%20harder%20negative%20samples%20on%20our%20contrastive%20learning%2C%20resolving%20segmentation%0Aissues%20along%20the%20boundary%20region%20by%20exploiting%20fine-grained%20details.%20We%0Ademonstrate%20that%20our%20Contextrast%20substantially%20enhances%20the%20performance%20of%0Asemantic%20segmentation%20networks%2C%20outperforming%20state-of-the-art%20contrastive%0Alearning%20approaches%20on%20diverse%20public%20datasets%2C%20e.g.%20Cityscapes%2C%20CamVid%2C%0APASCAL-C%2C%20COCO-Stuff%2C%20and%20ADE20K%2C%20without%20an%20increase%20in%20computational%20cost%0Aduring%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10633v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextrast%3A%20Contextual%20Contrastive%20Learning%20for%20Semantic%20Segmentation&entry.906535625=Changki%20Sung%20and%20Wanhee%20Kim%20and%20Jungho%20An%20and%20Wooju%20Lee%20and%20Hyungtae%20Lim%20and%20Hyun%20Myung&entry.1292438233=%20%20Despite%20great%20improvements%20in%20semantic%20segmentation%2C%20challenges%20persist%0Abecause%20of%20the%20lack%20of%20local/global%20contexts%20and%20the%20relationship%20between%20them.%0AIn%20this%20paper%2C%20we%20propose%20Contextrast%2C%20a%20contrastive%20learning-based%20semantic%0Asegmentation%20method%20that%20allows%20to%20capture%20local/global%20contexts%20and%20comprehend%0Atheir%20relationships.%20Our%20proposed%20method%20comprises%20two%20parts%3A%20a%29%20contextual%0Acontrastive%20learning%20%28CCL%29%20and%20b%29%20boundary-aware%20negative%20%28BANE%29%20sampling.%0AContextual%20contrastive%20learning%20obtains%20local/global%20context%20from%20multi-scale%0Afeature%20aggregation%20and%20inter/intra-relationship%20of%20features%20for%20better%0Adiscrimination%20capabilities.%20Meanwhile%2C%20BANE%20sampling%20selects%20embedding%0Afeatures%20along%20the%20boundaries%20of%20incorrectly%20predicted%20regions%20to%20employ%20them%0Aas%20harder%20negative%20samples%20on%20our%20contrastive%20learning%2C%20resolving%20segmentation%0Aissues%20along%20the%20boundary%20region%20by%20exploiting%20fine-grained%20details.%20We%0Ademonstrate%20that%20our%20Contextrast%20substantially%20enhances%20the%20performance%20of%0Asemantic%20segmentation%20networks%2C%20outperforming%20state-of-the-art%20contrastive%0Alearning%20approaches%20on%20diverse%20public%20datasets%2C%20e.g.%20Cityscapes%2C%20CamVid%2C%0APASCAL-C%2C%20COCO-Stuff%2C%20and%20ADE20K%2C%20without%20an%20increase%20in%20computational%20cost%0Aduring%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10633v1&entry.124074799=Read"},
{"title": "ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation", "author": "Iaroslav Melekhov and Anand Umashankar and Hyeong-Jin Kim and Vladislav Serkov and Dusty Argyle", "abstract": "  We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a\nnew outdoor large-scale aerial LiDAR dataset designed specifically for\nadvancing research in point cloud semantic segmentation. As the most extensive\nand diverse collection of its kind to date, the dataset covers a total area of\n10$km^2$ with close to 600 million points and features eleven distinct object\ncategories. To guarantee the dataset's quality and utility, we have thoroughly\ncurated the point labels through an internal team of experts, ensuring accuracy\nand consistency in semantic labeling. The dataset is engineered to move forward\nthe fields of 3D urban modeling, scene understanding, and utility\ninfrastructure management by presenting new challenges and potential\napplications. As a benchmark, we report qualitative and quantitative analysis\nof a voxel-based point cloud segmentation approach based on the Minkowski\nEngine.\n", "link": "http://arxiv.org/abs/2404.10699v1", "date": "2024-04-16", "relevancy": 2.044, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5232}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5143}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5028}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECLAIR%3A%20A%20High-Fidelity%20Aerial%20LiDAR%20Dataset%20for%20Semantic%20Segmentation&body=Title%3A%20ECLAIR%3A%20A%20High-Fidelity%20Aerial%20LiDAR%20Dataset%20for%20Semantic%20Segmentation%0AAuthor%3A%20Iaroslav%20Melekhov%20and%20Anand%20Umashankar%20and%20Hyeong-Jin%20Kim%20and%20Vladislav%20Serkov%20and%20Dusty%20Argyle%0AAbstract%3A%20%20%20We%20introduce%20ECLAIR%20%28Extended%20Classification%20of%20Lidar%20for%20AI%20Recognition%29%2C%20a%0Anew%20outdoor%20large-scale%20aerial%20LiDAR%20dataset%20designed%20specifically%20for%0Aadvancing%20research%20in%20point%20cloud%20semantic%20segmentation.%20As%20the%20most%20extensive%0Aand%20diverse%20collection%20of%20its%20kind%20to%20date%2C%20the%20dataset%20covers%20a%20total%20area%20of%0A10%24km%5E2%24%20with%20close%20to%20600%20million%20points%20and%20features%20eleven%20distinct%20object%0Acategories.%20To%20guarantee%20the%20dataset%27s%20quality%20and%20utility%2C%20we%20have%20thoroughly%0Acurated%20the%20point%20labels%20through%20an%20internal%20team%20of%20experts%2C%20ensuring%20accuracy%0Aand%20consistency%20in%20semantic%20labeling.%20The%20dataset%20is%20engineered%20to%20move%20forward%0Athe%20fields%20of%203D%20urban%20modeling%2C%20scene%20understanding%2C%20and%20utility%0Ainfrastructure%20management%20by%20presenting%20new%20challenges%20and%20potential%0Aapplications.%20As%20a%20benchmark%2C%20we%20report%20qualitative%20and%20quantitative%20analysis%0Aof%20a%20voxel-based%20point%20cloud%20segmentation%20approach%20based%20on%20the%20Minkowski%0AEngine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10699v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECLAIR%3A%20A%20High-Fidelity%20Aerial%20LiDAR%20Dataset%20for%20Semantic%20Segmentation&entry.906535625=Iaroslav%20Melekhov%20and%20Anand%20Umashankar%20and%20Hyeong-Jin%20Kim%20and%20Vladislav%20Serkov%20and%20Dusty%20Argyle&entry.1292438233=%20%20We%20introduce%20ECLAIR%20%28Extended%20Classification%20of%20Lidar%20for%20AI%20Recognition%29%2C%20a%0Anew%20outdoor%20large-scale%20aerial%20LiDAR%20dataset%20designed%20specifically%20for%0Aadvancing%20research%20in%20point%20cloud%20semantic%20segmentation.%20As%20the%20most%20extensive%0Aand%20diverse%20collection%20of%20its%20kind%20to%20date%2C%20the%20dataset%20covers%20a%20total%20area%20of%0A10%24km%5E2%24%20with%20close%20to%20600%20million%20points%20and%20features%20eleven%20distinct%20object%0Acategories.%20To%20guarantee%20the%20dataset%27s%20quality%20and%20utility%2C%20we%20have%20thoroughly%0Acurated%20the%20point%20labels%20through%20an%20internal%20team%20of%20experts%2C%20ensuring%20accuracy%0Aand%20consistency%20in%20semantic%20labeling.%20The%20dataset%20is%20engineered%20to%20move%20forward%0Athe%20fields%20of%203D%20urban%20modeling%2C%20scene%20understanding%2C%20and%20utility%0Ainfrastructure%20management%20by%20presenting%20new%20challenges%20and%20potential%0Aapplications.%20As%20a%20benchmark%2C%20we%20report%20qualitative%20and%20quantitative%20analysis%0Aof%20a%20voxel-based%20point%20cloud%20segmentation%20approach%20based%20on%20the%20Minkowski%0AEngine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10699v1&entry.124074799=Read"},
{"title": "PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape\n  Reconstruction", "author": "Sinisa Stekovic and Stefan Ainetter and Mattia D'Urso and Friedrich Fraundorfer and Vincent Lepetit", "abstract": "  We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D\nobjects from images using interpretable shape programs. In comparison to\ntraditional CAD model retrieval methods, the use of shape programs for 3D\nreconstruction allows for reasoning about the semantic properties of\nreconstructed objects, editing, low memory footprint, etc. However, the\nutilization of shape programs for 3D scene understanding has been largely\nneglected in past works. As our main contribution, we enable gradient-based\noptimization by introducing a module that translates shape programs designed in\nBlender, for example, into efficient PyTorch code. We also provide a method\nthat relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search\n(MCTS) to jointly optimize discrete and continuous parameters of shape programs\nand reconstruct 3D objects for input scenes. In our experiments, we apply our\nalgorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our\nresults against CAD model retrieval-based reconstructions. Our experiments\nindicate that our reconstructions match well the input scenes while enabling\nsemantic reasoning about reconstructed objects.\n", "link": "http://arxiv.org/abs/2404.10620v1", "date": "2024-04-16", "relevancy": 2.0386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4877}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4788}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PyTorchGeoNodes%3A%20Enabling%20Differentiable%20Shape%20Programs%20for%203D%20Shape%0A%20%20Reconstruction&body=Title%3A%20PyTorchGeoNodes%3A%20Enabling%20Differentiable%20Shape%20Programs%20for%203D%20Shape%0A%20%20Reconstruction%0AAuthor%3A%20Sinisa%20Stekovic%20and%20Stefan%20Ainetter%20and%20Mattia%20D%27Urso%20and%20Friedrich%20Fraundorfer%20and%20Vincent%20Lepetit%0AAbstract%3A%20%20%20We%20propose%20PyTorchGeoNodes%2C%20a%20differentiable%20module%20for%20reconstructing%203D%0Aobjects%20from%20images%20using%20interpretable%20shape%20programs.%20In%20comparison%20to%0Atraditional%20CAD%20model%20retrieval%20methods%2C%20the%20use%20of%20shape%20programs%20for%203D%0Areconstruction%20allows%20for%20reasoning%20about%20the%20semantic%20properties%20of%0Areconstructed%20objects%2C%20editing%2C%20low%20memory%20footprint%2C%20etc.%20However%2C%20the%0Autilization%20of%20shape%20programs%20for%203D%20scene%20understanding%20has%20been%20largely%0Aneglected%20in%20past%20works.%20As%20our%20main%20contribution%2C%20we%20enable%20gradient-based%0Aoptimization%20by%20introducing%20a%20module%20that%20translates%20shape%20programs%20designed%20in%0ABlender%2C%20for%20example%2C%20into%20efficient%20PyTorch%20code.%20We%20also%20provide%20a%20method%0Athat%20relies%20on%20PyTorchGeoNodes%20and%20is%20inspired%20by%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29%20to%20jointly%20optimize%20discrete%20and%20continuous%20parameters%20of%20shape%20programs%0Aand%20reconstruct%203D%20objects%20for%20input%20scenes.%20In%20our%20experiments%2C%20we%20apply%20our%0Aalgorithm%20to%20reconstruct%203D%20objects%20in%20the%20ScanNet%20dataset%20and%20evaluate%20our%0Aresults%20against%20CAD%20model%20retrieval-based%20reconstructions.%20Our%20experiments%0Aindicate%20that%20our%20reconstructions%20match%20well%20the%20input%20scenes%20while%20enabling%0Asemantic%20reasoning%20about%20reconstructed%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyTorchGeoNodes%3A%20Enabling%20Differentiable%20Shape%20Programs%20for%203D%20Shape%0A%20%20Reconstruction&entry.906535625=Sinisa%20Stekovic%20and%20Stefan%20Ainetter%20and%20Mattia%20D%27Urso%20and%20Friedrich%20Fraundorfer%20and%20Vincent%20Lepetit&entry.1292438233=%20%20We%20propose%20PyTorchGeoNodes%2C%20a%20differentiable%20module%20for%20reconstructing%203D%0Aobjects%20from%20images%20using%20interpretable%20shape%20programs.%20In%20comparison%20to%0Atraditional%20CAD%20model%20retrieval%20methods%2C%20the%20use%20of%20shape%20programs%20for%203D%0Areconstruction%20allows%20for%20reasoning%20about%20the%20semantic%20properties%20of%0Areconstructed%20objects%2C%20editing%2C%20low%20memory%20footprint%2C%20etc.%20However%2C%20the%0Autilization%20of%20shape%20programs%20for%203D%20scene%20understanding%20has%20been%20largely%0Aneglected%20in%20past%20works.%20As%20our%20main%20contribution%2C%20we%20enable%20gradient-based%0Aoptimization%20by%20introducing%20a%20module%20that%20translates%20shape%20programs%20designed%20in%0ABlender%2C%20for%20example%2C%20into%20efficient%20PyTorch%20code.%20We%20also%20provide%20a%20method%0Athat%20relies%20on%20PyTorchGeoNodes%20and%20is%20inspired%20by%20Monte%20Carlo%20Tree%20Search%0A%28MCTS%29%20to%20jointly%20optimize%20discrete%20and%20continuous%20parameters%20of%20shape%20programs%0Aand%20reconstruct%203D%20objects%20for%20input%20scenes.%20In%20our%20experiments%2C%20we%20apply%20our%0Aalgorithm%20to%20reconstruct%203D%20objects%20in%20the%20ScanNet%20dataset%20and%20evaluate%20our%0Aresults%20against%20CAD%20model%20retrieval-based%20reconstructions.%20Our%20experiments%0Aindicate%20that%20our%20reconstructions%20match%20well%20the%20input%20scenes%20while%20enabling%0Asemantic%20reasoning%20about%20reconstructed%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10620v1&entry.124074799=Read"},
{"title": "AbsGS: Recovering Fine Details for 3D Gaussian Splatting", "author": "Zongxin Ye and Wenyu Li and Sidun Liu and Peng Qiao and Yong Dou", "abstract": "  3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with\ndifferentiable rasterization to achieve high-quality novel view synthesis\nresults while providing advanced real-time rendering performance. However, due\nto the flaw of its adaptive density control strategy in 3D-GS, it frequently\nsuffers from over-reconstruction issue in intricate scenes containing\nhigh-frequency details, leading to blurry rendered images. The underlying\nreason for the flaw has still been under-explored. In this work, we present a\ncomprehensive analysis of the cause of aforementioned artifacts, namely\ngradient collision, which prevents large Gaussians in over-reconstructed\nregions from splitting. To address this issue, we propose the novel\nhomodirectional view-space positional gradient as the criterion for\ndensification. Our strategy efficiently identifies large Gaussians in\nover-reconstructed regions, and recovers fine details by splitting. We evaluate\nour proposed method on various challenging datasets. The experimental results\nindicate that our approach achieves the best rendering quality with reduced or\nsimilar memory consumption. Our method is easy to implement and can be\nincorporated into a wide variety of most recent Gaussian Splatting-based\nmethods. We will open source our codes upon formal publication. Our project\npage is available at: https://ty424.github.io/AbsGS.github.io/\n", "link": "http://arxiv.org/abs/2404.10484v1", "date": "2024-04-16", "relevancy": 2.0204, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5126}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4997}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4997}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AbsGS%3A%20Recovering%20Fine%20Details%20for%203D%20Gaussian%20Splatting&body=Title%3A%20AbsGS%3A%20Recovering%20Fine%20Details%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Sidun%20Liu%20and%20Peng%20Qiao%20and%20Yong%20Dou%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283D-GS%29%20technique%20couples%203D%20Gaussian%20primitives%20with%0Adifferentiable%20rasterization%20to%20achieve%20high-quality%20novel%20view%20synthesis%0Aresults%20while%20providing%20advanced%20real-time%20rendering%20performance.%20However%2C%20due%0Ato%20the%20flaw%20of%20its%20adaptive%20density%20control%20strategy%20in%203D-GS%2C%20it%20frequently%0Asuffers%20from%20over-reconstruction%20issue%20in%20intricate%20scenes%20containing%0Ahigh-frequency%20details%2C%20leading%20to%20blurry%20rendered%20images.%20The%20underlying%0Areason%20for%20the%20flaw%20has%20still%20been%20under-explored.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20the%20cause%20of%20aforementioned%20artifacts%2C%20namely%0Agradient%20collision%2C%20which%20prevents%20large%20Gaussians%20in%20over-reconstructed%0Aregions%20from%20splitting.%20To%20address%20this%20issue%2C%20we%20propose%20the%20novel%0Ahomodirectional%20view-space%20positional%20gradient%20as%20the%20criterion%20for%0Adensification.%20Our%20strategy%20efficiently%20identifies%20large%20Gaussians%20in%0Aover-reconstructed%20regions%2C%20and%20recovers%20fine%20details%20by%20splitting.%20We%20evaluate%0Aour%20proposed%20method%20on%20various%20challenging%20datasets.%20The%20experimental%20results%0Aindicate%20that%20our%20approach%20achieves%20the%20best%20rendering%20quality%20with%20reduced%20or%0Asimilar%20memory%20consumption.%20Our%20method%20is%20easy%20to%20implement%20and%20can%20be%0Aincorporated%20into%20a%20wide%20variety%20of%20most%20recent%20Gaussian%20Splatting-based%0Amethods.%20We%20will%20open%20source%20our%20codes%20upon%20formal%20publication.%20Our%20project%0Apage%20is%20available%20at%3A%20https%3A//ty424.github.io/AbsGS.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10484v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AbsGS%3A%20Recovering%20Fine%20Details%20for%203D%20Gaussian%20Splatting&entry.906535625=Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Sidun%20Liu%20and%20Peng%20Qiao%20and%20Yong%20Dou&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283D-GS%29%20technique%20couples%203D%20Gaussian%20primitives%20with%0Adifferentiable%20rasterization%20to%20achieve%20high-quality%20novel%20view%20synthesis%0Aresults%20while%20providing%20advanced%20real-time%20rendering%20performance.%20However%2C%20due%0Ato%20the%20flaw%20of%20its%20adaptive%20density%20control%20strategy%20in%203D-GS%2C%20it%20frequently%0Asuffers%20from%20over-reconstruction%20issue%20in%20intricate%20scenes%20containing%0Ahigh-frequency%20details%2C%20leading%20to%20blurry%20rendered%20images.%20The%20underlying%0Areason%20for%20the%20flaw%20has%20still%20been%20under-explored.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20analysis%20of%20the%20cause%20of%20aforementioned%20artifacts%2C%20namely%0Agradient%20collision%2C%20which%20prevents%20large%20Gaussians%20in%20over-reconstructed%0Aregions%20from%20splitting.%20To%20address%20this%20issue%2C%20we%20propose%20the%20novel%0Ahomodirectional%20view-space%20positional%20gradient%20as%20the%20criterion%20for%0Adensification.%20Our%20strategy%20efficiently%20identifies%20large%20Gaussians%20in%0Aover-reconstructed%20regions%2C%20and%20recovers%20fine%20details%20by%20splitting.%20We%20evaluate%0Aour%20proposed%20method%20on%20various%20challenging%20datasets.%20The%20experimental%20results%0Aindicate%20that%20our%20approach%20achieves%20the%20best%20rendering%20quality%20with%20reduced%20or%0Asimilar%20memory%20consumption.%20Our%20method%20is%20easy%20to%20implement%20and%20can%20be%0Aincorporated%20into%20a%20wide%20variety%20of%20most%20recent%20Gaussian%20Splatting-based%0Amethods.%20We%20will%20open%20source%20our%20codes%20upon%20formal%20publication.%20Our%20project%0Apage%20is%20available%20at%3A%20https%3A//ty424.github.io/AbsGS.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10484v1&entry.124074799=Read"},
{"title": "Understanding the Learning Dynamics of Alignment with Human Feedback", "author": "Shawn Im and Yixuan Li", "abstract": "  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n", "link": "http://arxiv.org/abs/2403.18742v4", "date": "2024-04-16", "relevancy": 2.0107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&body=Title%3A%20Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback%0AAuthor%3A%20Shawn%20Im%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18742v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Learning%20Dynamics%20of%20Alignment%20with%20Human%20Feedback&entry.906535625=Shawn%20Im%20and%20Yixuan%20Li&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20intentions%20has%20become%20a%0Acritical%20task%20for%20safely%20deploying%20models%20in%20real-world%20systems.%20While%20existing%0Aalignment%20approaches%20have%20seen%20empirical%20success%2C%20theoretically%20understanding%0Ahow%20these%20methods%20affect%20model%20behavior%20remains%20an%20open%20question.%20Our%20work%0Aprovides%20an%20initial%20attempt%20to%20theoretically%20analyze%20the%20learning%20dynamics%20of%0Ahuman%20preference%20alignment.%20We%20formally%20show%20how%20the%20distribution%20of%20preference%0Adatasets%20influences%20the%20rate%20of%20model%20updates%20and%20provide%20rigorous%20guarantees%0Aon%20the%20training%20accuracy.%20Our%20theory%20also%20reveals%20an%20intricate%20phenomenon%20where%0Athe%20optimization%20is%20prone%20to%20prioritizing%20certain%20behaviors%20with%20higher%0Apreference%20distinguishability.%20We%20empirically%20validate%20our%20findings%20on%0Acontemporary%20LLMs%20and%20alignment%20tasks%2C%20reinforcing%20our%20theoretical%20insights%20and%0Ashedding%20light%20on%20considerations%20for%20future%20alignment%20approaches.%20Disclaimer%3A%0AThis%20paper%20contains%20potentially%20offensive%20text%3B%20reader%20discretion%20is%20advised.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18742v4&entry.124074799=Read"},
{"title": "LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via\n  Eulerian Motion Field", "author": "Jiyang Li and Lechao Cheng and Zhangye Wang and Tingting Mu and Jingxuan He", "abstract": "  Cinemagraph is a unique form of visual media that combines elements of still\nphotography and subtle motion to create a captivating experience. However, the\nmajority of videos generated by recent works lack depth information and are\nconfined to the constraints of 2D image space. In this paper, inspired by\nsignificant progress in the field of novel view synthesis (NVS) achieved by 3D\nGaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from\n2D image space to 3D space using 3D Gaussian modeling. To achieve this, we\nfirst employ the 3D-GS method to reconstruct 3D Gaussian point clouds from\nmulti-view images of static scenes,incorporating shape regularization terms to\nprevent blurring or artifacts caused by object deformation. We then adopt an\nautoencoder tailored for 3D Gaussian to project it into feature space. To\nmaintain the local continuity of the scene, we devise SuperGaussian for\nclustering based on the acquired features. By calculating the similarity\nbetween clusters and employing a two-stage estimation method, we derive an\nEulerian motion field to describe velocities across the entire scene. The 3D\nGaussian points then move within the estimated Eulerian motion field. Through\nbidirectional animation techniques, we ultimately generate a 3D Cinemagraph\nthat exhibits natural and seamlessly loopable dynamics. Experiment results\nvalidate the effectiveness of our approach, demonstrating high-quality and\nvisually appealing scene generation. The project is available at\nhttps://pokerlishao.github.io/LoopGaussian/.\n", "link": "http://arxiv.org/abs/2404.08966v2", "date": "2024-04-16", "relevancy": 2.009, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5142}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4911}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LoopGaussian%3A%20Creating%203D%20Cinemagraph%20with%20Multi-view%20Images%20via%0A%20%20Eulerian%20Motion%20Field&body=Title%3A%20LoopGaussian%3A%20Creating%203D%20Cinemagraph%20with%20Multi-view%20Images%20via%0A%20%20Eulerian%20Motion%20Field%0AAuthor%3A%20Jiyang%20Li%20and%20Lechao%20Cheng%20and%20Zhangye%20Wang%20and%20Tingting%20Mu%20and%20Jingxuan%20He%0AAbstract%3A%20%20%20Cinemagraph%20is%20a%20unique%20form%20of%20visual%20media%20that%20combines%20elements%20of%20still%0Aphotography%20and%20subtle%20motion%20to%20create%20a%20captivating%20experience.%20However%2C%20the%0Amajority%20of%20videos%20generated%20by%20recent%20works%20lack%20depth%20information%20and%20are%0Aconfined%20to%20the%20constraints%20of%202D%20image%20space.%20In%20this%20paper%2C%20inspired%20by%0Asignificant%20progress%20in%20the%20field%20of%20novel%20view%20synthesis%20%28NVS%29%20achieved%20by%203D%0AGaussian%20Splatting%20%283D-GS%29%2C%20we%20propose%20LoopGaussian%20to%20elevate%20cinemagraph%20from%0A2D%20image%20space%20to%203D%20space%20using%203D%20Gaussian%20modeling.%20To%20achieve%20this%2C%20we%0Afirst%20employ%20the%203D-GS%20method%20to%20reconstruct%203D%20Gaussian%20point%20clouds%20from%0Amulti-view%20images%20of%20static%20scenes%2Cincorporating%20shape%20regularization%20terms%20to%0Aprevent%20blurring%20or%20artifacts%20caused%20by%20object%20deformation.%20We%20then%20adopt%20an%0Aautoencoder%20tailored%20for%203D%20Gaussian%20to%20project%20it%20into%20feature%20space.%20To%0Amaintain%20the%20local%20continuity%20of%20the%20scene%2C%20we%20devise%20SuperGaussian%20for%0Aclustering%20based%20on%20the%20acquired%20features.%20By%20calculating%20the%20similarity%0Abetween%20clusters%20and%20employing%20a%20two-stage%20estimation%20method%2C%20we%20derive%20an%0AEulerian%20motion%20field%20to%20describe%20velocities%20across%20the%20entire%20scene.%20The%203D%0AGaussian%20points%20then%20move%20within%20the%20estimated%20Eulerian%20motion%20field.%20Through%0Abidirectional%20animation%20techniques%2C%20we%20ultimately%20generate%20a%203D%20Cinemagraph%0Athat%20exhibits%20natural%20and%20seamlessly%20loopable%20dynamics.%20Experiment%20results%0Avalidate%20the%20effectiveness%20of%20our%20approach%2C%20demonstrating%20high-quality%20and%0Avisually%20appealing%20scene%20generation.%20The%20project%20is%20available%20at%0Ahttps%3A//pokerlishao.github.io/LoopGaussian/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08966v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoopGaussian%3A%20Creating%203D%20Cinemagraph%20with%20Multi-view%20Images%20via%0A%20%20Eulerian%20Motion%20Field&entry.906535625=Jiyang%20Li%20and%20Lechao%20Cheng%20and%20Zhangye%20Wang%20and%20Tingting%20Mu%20and%20Jingxuan%20He&entry.1292438233=%20%20Cinemagraph%20is%20a%20unique%20form%20of%20visual%20media%20that%20combines%20elements%20of%20still%0Aphotography%20and%20subtle%20motion%20to%20create%20a%20captivating%20experience.%20However%2C%20the%0Amajority%20of%20videos%20generated%20by%20recent%20works%20lack%20depth%20information%20and%20are%0Aconfined%20to%20the%20constraints%20of%202D%20image%20space.%20In%20this%20paper%2C%20inspired%20by%0Asignificant%20progress%20in%20the%20field%20of%20novel%20view%20synthesis%20%28NVS%29%20achieved%20by%203D%0AGaussian%20Splatting%20%283D-GS%29%2C%20we%20propose%20LoopGaussian%20to%20elevate%20cinemagraph%20from%0A2D%20image%20space%20to%203D%20space%20using%203D%20Gaussian%20modeling.%20To%20achieve%20this%2C%20we%0Afirst%20employ%20the%203D-GS%20method%20to%20reconstruct%203D%20Gaussian%20point%20clouds%20from%0Amulti-view%20images%20of%20static%20scenes%2Cincorporating%20shape%20regularization%20terms%20to%0Aprevent%20blurring%20or%20artifacts%20caused%20by%20object%20deformation.%20We%20then%20adopt%20an%0Aautoencoder%20tailored%20for%203D%20Gaussian%20to%20project%20it%20into%20feature%20space.%20To%0Amaintain%20the%20local%20continuity%20of%20the%20scene%2C%20we%20devise%20SuperGaussian%20for%0Aclustering%20based%20on%20the%20acquired%20features.%20By%20calculating%20the%20similarity%0Abetween%20clusters%20and%20employing%20a%20two-stage%20estimation%20method%2C%20we%20derive%20an%0AEulerian%20motion%20field%20to%20describe%20velocities%20across%20the%20entire%20scene.%20The%203D%0AGaussian%20points%20then%20move%20within%20the%20estimated%20Eulerian%20motion%20field.%20Through%0Abidirectional%20animation%20techniques%2C%20we%20ultimately%20generate%20a%203D%20Cinemagraph%0Athat%20exhibits%20natural%20and%20seamlessly%20loopable%20dynamics.%20Experiment%20results%0Avalidate%20the%20effectiveness%20of%20our%20approach%2C%20demonstrating%20high-quality%20and%0Avisually%20appealing%20scene%20generation.%20The%20project%20is%20available%20at%0Ahttps%3A//pokerlishao.github.io/LoopGaussian/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08966v2&entry.124074799=Read"},
{"title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random\n  Hierarchy Model", "author": "Umberto Tomasini and Matthieu Wyart", "abstract": "  Understanding what makes high-dimensional data learnable is a fundamental\nquestion in machine learning. On the one hand, it is believed that the success\nof deep learning lies in its ability to build a hierarchy of representations\nthat become increasingly more abstract with depth, going from simple features\nlike edges to more complex concepts. On the other hand, learning to be\ninsensitive to invariances of the task, such as smooth transformations for\nimage datasets, has been argued to be important for deep networks and it\nstrongly correlates with their performance. In this work, we aim to explain\nthis correlation and unify these two viewpoints. We show that by introducing\nsparsity to generative hierarchical models of data, the task acquires\ninsensitivity to spatial transformations that are discrete versions of smooth\ntransformations. In particular, we introduce the Sparse Random Hierarchy Model\n(SRHM), where we observe and rationalize that a hierarchical representation\nmirroring the hierarchical model is learnt precisely when such insensitivity is\nlearnt, thereby explaining the strong correlation between the latter and\nperformance. Moreover, we quantify how the sample complexity of CNNs learning\nthe SRHM depends on both the sparsity and hierarchical structure of the task.\n", "link": "http://arxiv.org/abs/2404.10727v1", "date": "2024-04-16", "relevancy": 1.9986, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5218}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4921}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20Deep%20Networks%20Learn%20Sparse%20and%20Hierarchical%20Data%3A%20the%20Sparse%20Random%0A%20%20Hierarchy%20Model&body=Title%3A%20How%20Deep%20Networks%20Learn%20Sparse%20and%20Hierarchical%20Data%3A%20the%20Sparse%20Random%0A%20%20Hierarchy%20Model%0AAuthor%3A%20Umberto%20Tomasini%20and%20Matthieu%20Wyart%0AAbstract%3A%20%20%20Understanding%20what%20makes%20high-dimensional%20data%20learnable%20is%20a%20fundamental%0Aquestion%20in%20machine%20learning.%20On%20the%20one%20hand%2C%20it%20is%20believed%20that%20the%20success%0Aof%20deep%20learning%20lies%20in%20its%20ability%20to%20build%20a%20hierarchy%20of%20representations%0Athat%20become%20increasingly%20more%20abstract%20with%20depth%2C%20going%20from%20simple%20features%0Alike%20edges%20to%20more%20complex%20concepts.%20On%20the%20other%20hand%2C%20learning%20to%20be%0Ainsensitive%20to%20invariances%20of%20the%20task%2C%20such%20as%20smooth%20transformations%20for%0Aimage%20datasets%2C%20has%20been%20argued%20to%20be%20important%20for%20deep%20networks%20and%20it%0Astrongly%20correlates%20with%20their%20performance.%20In%20this%20work%2C%20we%20aim%20to%20explain%0Athis%20correlation%20and%20unify%20these%20two%20viewpoints.%20We%20show%20that%20by%20introducing%0Asparsity%20to%20generative%20hierarchical%20models%20of%20data%2C%20the%20task%20acquires%0Ainsensitivity%20to%20spatial%20transformations%20that%20are%20discrete%20versions%20of%20smooth%0Atransformations.%20In%20particular%2C%20we%20introduce%20the%20Sparse%20Random%20Hierarchy%20Model%0A%28SRHM%29%2C%20where%20we%20observe%20and%20rationalize%20that%20a%20hierarchical%20representation%0Amirroring%20the%20hierarchical%20model%20is%20learnt%20precisely%20when%20such%20insensitivity%20is%0Alearnt%2C%20thereby%20explaining%20the%20strong%20correlation%20between%20the%20latter%20and%0Aperformance.%20Moreover%2C%20we%20quantify%20how%20the%20sample%20complexity%20of%20CNNs%20learning%0Athe%20SRHM%20depends%20on%20both%20the%20sparsity%20and%20hierarchical%20structure%20of%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10727v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Deep%20Networks%20Learn%20Sparse%20and%20Hierarchical%20Data%3A%20the%20Sparse%20Random%0A%20%20Hierarchy%20Model&entry.906535625=Umberto%20Tomasini%20and%20Matthieu%20Wyart&entry.1292438233=%20%20Understanding%20what%20makes%20high-dimensional%20data%20learnable%20is%20a%20fundamental%0Aquestion%20in%20machine%20learning.%20On%20the%20one%20hand%2C%20it%20is%20believed%20that%20the%20success%0Aof%20deep%20learning%20lies%20in%20its%20ability%20to%20build%20a%20hierarchy%20of%20representations%0Athat%20become%20increasingly%20more%20abstract%20with%20depth%2C%20going%20from%20simple%20features%0Alike%20edges%20to%20more%20complex%20concepts.%20On%20the%20other%20hand%2C%20learning%20to%20be%0Ainsensitive%20to%20invariances%20of%20the%20task%2C%20such%20as%20smooth%20transformations%20for%0Aimage%20datasets%2C%20has%20been%20argued%20to%20be%20important%20for%20deep%20networks%20and%20it%0Astrongly%20correlates%20with%20their%20performance.%20In%20this%20work%2C%20we%20aim%20to%20explain%0Athis%20correlation%20and%20unify%20these%20two%20viewpoints.%20We%20show%20that%20by%20introducing%0Asparsity%20to%20generative%20hierarchical%20models%20of%20data%2C%20the%20task%20acquires%0Ainsensitivity%20to%20spatial%20transformations%20that%20are%20discrete%20versions%20of%20smooth%0Atransformations.%20In%20particular%2C%20we%20introduce%20the%20Sparse%20Random%20Hierarchy%20Model%0A%28SRHM%29%2C%20where%20we%20observe%20and%20rationalize%20that%20a%20hierarchical%20representation%0Amirroring%20the%20hierarchical%20model%20is%20learnt%20precisely%20when%20such%20insensitivity%20is%0Alearnt%2C%20thereby%20explaining%20the%20strong%20correlation%20between%20the%20latter%20and%0Aperformance.%20Moreover%2C%20we%20quantify%20how%20the%20sample%20complexity%20of%20CNNs%20learning%0Athe%20SRHM%20depends%20on%20both%20the%20sparsity%20and%20hierarchical%20structure%20of%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10727v1&entry.124074799=Read"},
{"title": "Hardware-aware training of models with synaptic delays for digital\n  event-driven neuromorphic processors", "author": "Alberto Patino-Saucedo and Roy Meijer and Amirreza Yousefzadeh and Manil-Dev Gomony and Federico Corradi and Paul Detteter and Laura Garrido-Regife and Bernabe Linares-Barranco and Manolis Sifalakis", "abstract": "  Configurable synaptic delays are a basic feature in many neuromorphic neural\nnetwork hardware accelerators. However, they have been rarely used in model\nimplementations, despite their promising impact on performance and efficiency\nin tasks that exhibit complex (temporal) dynamics, as it has been unclear how\nto optimize them. In this work, we propose a framework to train and deploy, in\ndigital neuromorphic hardware, highly performing spiking neural network models\n(SNNs) where apart from the synaptic weights, the per-synapse delays are also\nco-optimized. Leveraging spike-based back-propagation-through-time, the\ntraining accounts for both platform constraints, such as synaptic weight\nprecision and the total number of parameters per core, as a function of the\nnetwork size. In addition, a delay pruning technique is used to reduce memory\nfootprint with a low cost in performance. We evaluate trained models in two\nneuromorphic digital hardware platforms: Intel Loihi and Imec Seneca. Loihi\noffers synaptic delay support using the so-called Ring-Buffer hardware\nstructure. Seneca does not provide native hardware support for synaptic delays.\nA second contribution of this paper is therefore a novel area- and\nmemory-efficient hardware structure for acceleration of synaptic delays, which\nwe have integrated in Seneca. The evaluated benchmark involves several models\nfor solving the SHD (Spiking Heidelberg Digits) classification task, where\nminimal accuracy degradation during the transition from software to hardware is\ndemonstrated. To our knowledge, this is the first work showcasing how to train\nand deploy hardware-aware models parameterized with synaptic delays, on\nmulticore neuromorphic hardware accelerators.\n", "link": "http://arxiv.org/abs/2404.10597v1", "date": "2024-04-16", "relevancy": 1.9784, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5466}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4845}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4839}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hardware-aware%20training%20of%20models%20with%20synaptic%20delays%20for%20digital%0A%20%20event-driven%20neuromorphic%20processors&body=Title%3A%20Hardware-aware%20training%20of%20models%20with%20synaptic%20delays%20for%20digital%0A%20%20event-driven%20neuromorphic%20processors%0AAuthor%3A%20Alberto%20Patino-Saucedo%20and%20Roy%20Meijer%20and%20Amirreza%20Yousefzadeh%20and%20Manil-Dev%20Gomony%20and%20Federico%20Corradi%20and%20Paul%20Detteter%20and%20Laura%20Garrido-Regife%20and%20Bernabe%20Linares-Barranco%20and%20Manolis%20Sifalakis%0AAbstract%3A%20%20%20Configurable%20synaptic%20delays%20are%20a%20basic%20feature%20in%20many%20neuromorphic%20neural%0Anetwork%20hardware%20accelerators.%20However%2C%20they%20have%20been%20rarely%20used%20in%20model%0Aimplementations%2C%20despite%20their%20promising%20impact%20on%20performance%20and%20efficiency%0Ain%20tasks%20that%20exhibit%20complex%20%28temporal%29%20dynamics%2C%20as%20it%20has%20been%20unclear%20how%0Ato%20optimize%20them.%20In%20this%20work%2C%20we%20propose%20a%20framework%20to%20train%20and%20deploy%2C%20in%0Adigital%20neuromorphic%20hardware%2C%20highly%20performing%20spiking%20neural%20network%20models%0A%28SNNs%29%20where%20apart%20from%20the%20synaptic%20weights%2C%20the%20per-synapse%20delays%20are%20also%0Aco-optimized.%20Leveraging%20spike-based%20back-propagation-through-time%2C%20the%0Atraining%20accounts%20for%20both%20platform%20constraints%2C%20such%20as%20synaptic%20weight%0Aprecision%20and%20the%20total%20number%20of%20parameters%20per%20core%2C%20as%20a%20function%20of%20the%0Anetwork%20size.%20In%20addition%2C%20a%20delay%20pruning%20technique%20is%20used%20to%20reduce%20memory%0Afootprint%20with%20a%20low%20cost%20in%20performance.%20We%20evaluate%20trained%20models%20in%20two%0Aneuromorphic%20digital%20hardware%20platforms%3A%20Intel%20Loihi%20and%20Imec%20Seneca.%20Loihi%0Aoffers%20synaptic%20delay%20support%20using%20the%20so-called%20Ring-Buffer%20hardware%0Astructure.%20Seneca%20does%20not%20provide%20native%20hardware%20support%20for%20synaptic%20delays.%0AA%20second%20contribution%20of%20this%20paper%20is%20therefore%20a%20novel%20area-%20and%0Amemory-efficient%20hardware%20structure%20for%20acceleration%20of%20synaptic%20delays%2C%20which%0Awe%20have%20integrated%20in%20Seneca.%20The%20evaluated%20benchmark%20involves%20several%20models%0Afor%20solving%20the%20SHD%20%28Spiking%20Heidelberg%20Digits%29%20classification%20task%2C%20where%0Aminimal%20accuracy%20degradation%20during%20the%20transition%20from%20software%20to%20hardware%20is%0Ademonstrated.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20showcasing%20how%20to%20train%0Aand%20deploy%20hardware-aware%20models%20parameterized%20with%20synaptic%20delays%2C%20on%0Amulticore%20neuromorphic%20hardware%20accelerators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10597v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hardware-aware%20training%20of%20models%20with%20synaptic%20delays%20for%20digital%0A%20%20event-driven%20neuromorphic%20processors&entry.906535625=Alberto%20Patino-Saucedo%20and%20Roy%20Meijer%20and%20Amirreza%20Yousefzadeh%20and%20Manil-Dev%20Gomony%20and%20Federico%20Corradi%20and%20Paul%20Detteter%20and%20Laura%20Garrido-Regife%20and%20Bernabe%20Linares-Barranco%20and%20Manolis%20Sifalakis&entry.1292438233=%20%20Configurable%20synaptic%20delays%20are%20a%20basic%20feature%20in%20many%20neuromorphic%20neural%0Anetwork%20hardware%20accelerators.%20However%2C%20they%20have%20been%20rarely%20used%20in%20model%0Aimplementations%2C%20despite%20their%20promising%20impact%20on%20performance%20and%20efficiency%0Ain%20tasks%20that%20exhibit%20complex%20%28temporal%29%20dynamics%2C%20as%20it%20has%20been%20unclear%20how%0Ato%20optimize%20them.%20In%20this%20work%2C%20we%20propose%20a%20framework%20to%20train%20and%20deploy%2C%20in%0Adigital%20neuromorphic%20hardware%2C%20highly%20performing%20spiking%20neural%20network%20models%0A%28SNNs%29%20where%20apart%20from%20the%20synaptic%20weights%2C%20the%20per-synapse%20delays%20are%20also%0Aco-optimized.%20Leveraging%20spike-based%20back-propagation-through-time%2C%20the%0Atraining%20accounts%20for%20both%20platform%20constraints%2C%20such%20as%20synaptic%20weight%0Aprecision%20and%20the%20total%20number%20of%20parameters%20per%20core%2C%20as%20a%20function%20of%20the%0Anetwork%20size.%20In%20addition%2C%20a%20delay%20pruning%20technique%20is%20used%20to%20reduce%20memory%0Afootprint%20with%20a%20low%20cost%20in%20performance.%20We%20evaluate%20trained%20models%20in%20two%0Aneuromorphic%20digital%20hardware%20platforms%3A%20Intel%20Loihi%20and%20Imec%20Seneca.%20Loihi%0Aoffers%20synaptic%20delay%20support%20using%20the%20so-called%20Ring-Buffer%20hardware%0Astructure.%20Seneca%20does%20not%20provide%20native%20hardware%20support%20for%20synaptic%20delays.%0AA%20second%20contribution%20of%20this%20paper%20is%20therefore%20a%20novel%20area-%20and%0Amemory-efficient%20hardware%20structure%20for%20acceleration%20of%20synaptic%20delays%2C%20which%0Awe%20have%20integrated%20in%20Seneca.%20The%20evaluated%20benchmark%20involves%20several%20models%0Afor%20solving%20the%20SHD%20%28Spiking%20Heidelberg%20Digits%29%20classification%20task%2C%20where%0Aminimal%20accuracy%20degradation%20during%20the%20transition%20from%20software%20to%20hardware%20is%0Ademonstrated.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20showcasing%20how%20to%20train%0Aand%20deploy%20hardware-aware%20models%20parameterized%20with%20synaptic%20delays%2C%20on%0Amulticore%20neuromorphic%20hardware%20accelerators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10597v1&entry.124074799=Read"},
{"title": "Label merge-and-split: A graph-colouring approach for memory-efficient\n  brain parcellation", "author": "Aaron Kujawa and Reuben Dorent and Sebastien Ourselin and Tom Vercauteren", "abstract": "  Whole brain parcellation requires inferring hundreds of segmentation labels\nin large image volumes and thus presents significant practical challenges for\ndeep learning approaches. We introduce label merge-and-split, a method that\nfirst greatly reduces the effective number of labels required for\nlearning-based whole brain parcellation and then recovers original labels.\nUsing a greedy graph colouring algorithm, our method automatically groups and\nmerges multiple spatially separate labels prior to model training and\ninference. The merged labels may be semantically unrelated. A deep learning\nmodel is trained to predict merged labels. At inference time, original labels\nare restored using atlas-based influence regions. In our experiments, the\nproposed approach reduces the number of labels by up to 68% while achieving\nsegmentation accuracy comparable to the baseline method without label merging\nand splitting. Moreover, model training and inference times as well as GPU\nmemory requirements were reduced significantly. The proposed method can be\napplied to all semantic segmentation tasks with a large number of spatially\nseparate classes within an atlas-based prior.\n", "link": "http://arxiv.org/abs/2404.10572v1", "date": "2024-04-16", "relevancy": 1.9767, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4844}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4707}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation&body=Title%3A%20Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation%0AAuthor%3A%20Aaron%20Kujawa%20and%20Reuben%20Dorent%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Whole%20brain%20parcellation%20requires%20inferring%20hundreds%20of%20segmentation%20labels%0Ain%20large%20image%20volumes%20and%20thus%20presents%20significant%20practical%20challenges%20for%0Adeep%20learning%20approaches.%20We%20introduce%20label%20merge-and-split%2C%20a%20method%20that%0Afirst%20greatly%20reduces%20the%20effective%20number%20of%20labels%20required%20for%0Alearning-based%20whole%20brain%20parcellation%20and%20then%20recovers%20original%20labels.%0AUsing%20a%20greedy%20graph%20colouring%20algorithm%2C%20our%20method%20automatically%20groups%20and%0Amerges%20multiple%20spatially%20separate%20labels%20prior%20to%20model%20training%20and%0Ainference.%20The%20merged%20labels%20may%20be%20semantically%20unrelated.%20A%20deep%20learning%0Amodel%20is%20trained%20to%20predict%20merged%20labels.%20At%20inference%20time%2C%20original%20labels%0Aare%20restored%20using%20atlas-based%20influence%20regions.%20In%20our%20experiments%2C%20the%0Aproposed%20approach%20reduces%20the%20number%20of%20labels%20by%20up%20to%2068%25%20while%20achieving%0Asegmentation%20accuracy%20comparable%20to%20the%20baseline%20method%20without%20label%20merging%0Aand%20splitting.%20Moreover%2C%20model%20training%20and%20inference%20times%20as%20well%20as%20GPU%0Amemory%20requirements%20were%20reduced%20significantly.%20The%20proposed%20method%20can%20be%0Aapplied%20to%20all%20semantic%20segmentation%20tasks%20with%20a%20large%20number%20of%20spatially%0Aseparate%20classes%20within%20an%20atlas-based%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10572v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation&entry.906535625=Aaron%20Kujawa%20and%20Reuben%20Dorent%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Whole%20brain%20parcellation%20requires%20inferring%20hundreds%20of%20segmentation%20labels%0Ain%20large%20image%20volumes%20and%20thus%20presents%20significant%20practical%20challenges%20for%0Adeep%20learning%20approaches.%20We%20introduce%20label%20merge-and-split%2C%20a%20method%20that%0Afirst%20greatly%20reduces%20the%20effective%20number%20of%20labels%20required%20for%0Alearning-based%20whole%20brain%20parcellation%20and%20then%20recovers%20original%20labels.%0AUsing%20a%20greedy%20graph%20colouring%20algorithm%2C%20our%20method%20automatically%20groups%20and%0Amerges%20multiple%20spatially%20separate%20labels%20prior%20to%20model%20training%20and%0Ainference.%20The%20merged%20labels%20may%20be%20semantically%20unrelated.%20A%20deep%20learning%0Amodel%20is%20trained%20to%20predict%20merged%20labels.%20At%20inference%20time%2C%20original%20labels%0Aare%20restored%20using%20atlas-based%20influence%20regions.%20In%20our%20experiments%2C%20the%0Aproposed%20approach%20reduces%20the%20number%20of%20labels%20by%20up%20to%2068%25%20while%20achieving%0Asegmentation%20accuracy%20comparable%20to%20the%20baseline%20method%20without%20label%20merging%0Aand%20splitting.%20Moreover%2C%20model%20training%20and%20inference%20times%20as%20well%20as%20GPU%0Amemory%20requirements%20were%20reduced%20significantly.%20The%20proposed%20method%20can%20be%0Aapplied%20to%20all%20semantic%20segmentation%20tasks%20with%20a%20large%20number%20of%20spatially%0Aseparate%20classes%20within%20an%20atlas-based%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10572v1&entry.124074799=Read"},
{"title": "Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder", "author": "Priyam Gupta and Peter J. Schmid and Denis Sipp and Taraneh Sayadi and Georgios Rigas", "abstract": "  The Koopman operator presents an attractive approach to achieve global\nlinearization of nonlinear systems, making it a valuable method for simplifying\nthe understanding of complex dynamics. While data-driven methodologies have\nexhibited promise in approximating finite Koopman operators, they grapple with\nvarious challenges, such as the judicious selection of observables,\ndimensionality reduction, and the ability to predict complex system behaviors\naccurately. This study presents a novel approach termed Mori-Zwanzig\nautoencoder (MZ-AE) to robustly approximate the Koopman operator in\nlow-dimensional spaces. The proposed method leverages a nonlinear autoencoder\nto extract key observables for approximating a finite invariant Koopman\nsubspace and integrates a non-Markovian correction mechanism using the\nMori-Zwanzig formalism. Consequently, this approach yields a closed\nrepresentation of dynamics within the latent manifold of the nonlinear\nautoencoder, thereby enhancing the precision and stability of the Koopman\noperator approximation. Demonstrations showcase the technique's ability to\ncapture regime transitions in the flow around a cylinder. It also provides a\nlow dimensional approximation for Kuramoto-Sivashinsky with promising\nshort-term predictability and robust long-term statistical performance. By\nbridging the gap between data-driven techniques and the mathematical\nfoundations of Koopman theory, MZ-AE offers a promising avenue for improved\nunderstanding and prediction of complex nonlinear dynamics.\n", "link": "http://arxiv.org/abs/2310.10745v2", "date": "2024-04-16", "relevancy": 1.975, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5096}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4788}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder&body=Title%3A%20Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder%0AAuthor%3A%20Priyam%20Gupta%20and%20Peter%20J.%20Schmid%20and%20Denis%20Sipp%20and%20Taraneh%20Sayadi%20and%20Georgios%20Rigas%0AAbstract%3A%20%20%20The%20Koopman%20operator%20presents%20an%20attractive%20approach%20to%20achieve%20global%0Alinearization%20of%20nonlinear%20systems%2C%20making%20it%20a%20valuable%20method%20for%20simplifying%0Athe%20understanding%20of%20complex%20dynamics.%20While%20data-driven%20methodologies%20have%0Aexhibited%20promise%20in%20approximating%20finite%20Koopman%20operators%2C%20they%20grapple%20with%0Avarious%20challenges%2C%20such%20as%20the%20judicious%20selection%20of%20observables%2C%0Adimensionality%20reduction%2C%20and%20the%20ability%20to%20predict%20complex%20system%20behaviors%0Aaccurately.%20This%20study%20presents%20a%20novel%20approach%20termed%20Mori-Zwanzig%0Aautoencoder%20%28MZ-AE%29%20to%20robustly%20approximate%20the%20Koopman%20operator%20in%0Alow-dimensional%20spaces.%20The%20proposed%20method%20leverages%20a%20nonlinear%20autoencoder%0Ato%20extract%20key%20observables%20for%20approximating%20a%20finite%20invariant%20Koopman%0Asubspace%20and%20integrates%20a%20non-Markovian%20correction%20mechanism%20using%20the%0AMori-Zwanzig%20formalism.%20Consequently%2C%20this%20approach%20yields%20a%20closed%0Arepresentation%20of%20dynamics%20within%20the%20latent%20manifold%20of%20the%20nonlinear%0Aautoencoder%2C%20thereby%20enhancing%20the%20precision%20and%20stability%20of%20the%20Koopman%0Aoperator%20approximation.%20Demonstrations%20showcase%20the%20technique%27s%20ability%20to%0Acapture%20regime%20transitions%20in%20the%20flow%20around%20a%20cylinder.%20It%20also%20provides%20a%0Alow%20dimensional%20approximation%20for%20Kuramoto-Sivashinsky%20with%20promising%0Ashort-term%20predictability%20and%20robust%20long-term%20statistical%20performance.%20By%0Abridging%20the%20gap%20between%20data-driven%20techniques%20and%20the%20mathematical%0Afoundations%20of%20Koopman%20theory%2C%20MZ-AE%20offers%20a%20promising%20avenue%20for%20improved%0Aunderstanding%20and%20prediction%20of%20complex%20nonlinear%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10745v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder&entry.906535625=Priyam%20Gupta%20and%20Peter%20J.%20Schmid%20and%20Denis%20Sipp%20and%20Taraneh%20Sayadi%20and%20Georgios%20Rigas&entry.1292438233=%20%20The%20Koopman%20operator%20presents%20an%20attractive%20approach%20to%20achieve%20global%0Alinearization%20of%20nonlinear%20systems%2C%20making%20it%20a%20valuable%20method%20for%20simplifying%0Athe%20understanding%20of%20complex%20dynamics.%20While%20data-driven%20methodologies%20have%0Aexhibited%20promise%20in%20approximating%20finite%20Koopman%20operators%2C%20they%20grapple%20with%0Avarious%20challenges%2C%20such%20as%20the%20judicious%20selection%20of%20observables%2C%0Adimensionality%20reduction%2C%20and%20the%20ability%20to%20predict%20complex%20system%20behaviors%0Aaccurately.%20This%20study%20presents%20a%20novel%20approach%20termed%20Mori-Zwanzig%0Aautoencoder%20%28MZ-AE%29%20to%20robustly%20approximate%20the%20Koopman%20operator%20in%0Alow-dimensional%20spaces.%20The%20proposed%20method%20leverages%20a%20nonlinear%20autoencoder%0Ato%20extract%20key%20observables%20for%20approximating%20a%20finite%20invariant%20Koopman%0Asubspace%20and%20integrates%20a%20non-Markovian%20correction%20mechanism%20using%20the%0AMori-Zwanzig%20formalism.%20Consequently%2C%20this%20approach%20yields%20a%20closed%0Arepresentation%20of%20dynamics%20within%20the%20latent%20manifold%20of%20the%20nonlinear%0Aautoencoder%2C%20thereby%20enhancing%20the%20precision%20and%20stability%20of%20the%20Koopman%0Aoperator%20approximation.%20Demonstrations%20showcase%20the%20technique%27s%20ability%20to%0Acapture%20regime%20transitions%20in%20the%20flow%20around%20a%20cylinder.%20It%20also%20provides%20a%0Alow%20dimensional%20approximation%20for%20Kuramoto-Sivashinsky%20with%20promising%0Ashort-term%20predictability%20and%20robust%20long-term%20statistical%20performance.%20By%0Abridging%20the%20gap%20between%20data-driven%20techniques%20and%20the%20mathematical%0Afoundations%20of%20Koopman%20theory%2C%20MZ-AE%20offers%20a%20promising%20avenue%20for%20improved%0Aunderstanding%20and%20prediction%20of%20complex%20nonlinear%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10745v2&entry.124074799=Read"},
{"title": "PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph\n  Construction Methods and Chebyshev Graph Convolutions", "author": "Yash Semlani and Mihir Relan and Krithik Ramesh", "abstract": "  Jet tagging is a classification problem in high-energy physics experiments\nthat aims to identify the collimated sprays of subatomic particles, jets, from\nparticle collisions and tag them to their emitter particle. Advances in jet\ntagging present opportunities for searches of new physics beyond the Standard\nModel. Current approaches use deep learning to uncover hidden patterns in\ncomplex collision data. However, the representation of jets as inputs to a deep\nlearning model have been varied, and often, informative features are withheld\nfrom models. In this study, we propose a graph-based representation of a jet\nthat encodes the most information possible. To learn best from this\nrepresentation, we design Particle Chebyshev Network (PCN), a graph neural\nnetwork (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been\ndemonstrated as an effective alternative to classical graph convolutions in\nGNNs and has yet to be explored in jet tagging. PCN achieves a substantial\nimprovement in accuracy over existing taggers and opens the door to future\nstudies into graph-based representations of jets and ChebConv layers in\nhigh-energy physics experiments. Code is available at\nhttps://github.com/YVSemlani/PCN-Jet-Tagging.\n", "link": "http://arxiv.org/abs/2309.08630v4", "date": "2024-04-16", "relevancy": 1.9736, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PCN%3A%20A%20Deep%20Learning%20Approach%20to%20Jet%20Tagging%20Utilizing%20Novel%20Graph%0A%20%20Construction%20Methods%20and%20Chebyshev%20Graph%20Convolutions&body=Title%3A%20PCN%3A%20A%20Deep%20Learning%20Approach%20to%20Jet%20Tagging%20Utilizing%20Novel%20Graph%0A%20%20Construction%20Methods%20and%20Chebyshev%20Graph%20Convolutions%0AAuthor%3A%20Yash%20Semlani%20and%20Mihir%20Relan%20and%20Krithik%20Ramesh%0AAbstract%3A%20%20%20Jet%20tagging%20is%20a%20classification%20problem%20in%20high-energy%20physics%20experiments%0Athat%20aims%20to%20identify%20the%20collimated%20sprays%20of%20subatomic%20particles%2C%20jets%2C%20from%0Aparticle%20collisions%20and%20tag%20them%20to%20their%20emitter%20particle.%20Advances%20in%20jet%0Atagging%20present%20opportunities%20for%20searches%20of%20new%20physics%20beyond%20the%20Standard%0AModel.%20Current%20approaches%20use%20deep%20learning%20to%20uncover%20hidden%20patterns%20in%0Acomplex%20collision%20data.%20However%2C%20the%20representation%20of%20jets%20as%20inputs%20to%20a%20deep%0Alearning%20model%20have%20been%20varied%2C%20and%20often%2C%20informative%20features%20are%20withheld%0Afrom%20models.%20In%20this%20study%2C%20we%20propose%20a%20graph-based%20representation%20of%20a%20jet%0Athat%20encodes%20the%20most%20information%20possible.%20To%20learn%20best%20from%20this%0Arepresentation%2C%20we%20design%20Particle%20Chebyshev%20Network%20%28PCN%29%2C%20a%20graph%20neural%0Anetwork%20%28GNN%29%20using%20Chebyshev%20graph%20convolutions%20%28ChebConv%29.%20ChebConv%20has%20been%0Ademonstrated%20as%20an%20effective%20alternative%20to%20classical%20graph%20convolutions%20in%0AGNNs%20and%20has%20yet%20to%20be%20explored%20in%20jet%20tagging.%20PCN%20achieves%20a%20substantial%0Aimprovement%20in%20accuracy%20over%20existing%20taggers%20and%20opens%20the%20door%20to%20future%0Astudies%20into%20graph-based%20representations%20of%20jets%20and%20ChebConv%20layers%20in%0Ahigh-energy%20physics%20experiments.%20Code%20is%20available%20at%0Ahttps%3A//github.com/YVSemlani/PCN-Jet-Tagging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08630v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCN%3A%20A%20Deep%20Learning%20Approach%20to%20Jet%20Tagging%20Utilizing%20Novel%20Graph%0A%20%20Construction%20Methods%20and%20Chebyshev%20Graph%20Convolutions&entry.906535625=Yash%20Semlani%20and%20Mihir%20Relan%20and%20Krithik%20Ramesh&entry.1292438233=%20%20Jet%20tagging%20is%20a%20classification%20problem%20in%20high-energy%20physics%20experiments%0Athat%20aims%20to%20identify%20the%20collimated%20sprays%20of%20subatomic%20particles%2C%20jets%2C%20from%0Aparticle%20collisions%20and%20tag%20them%20to%20their%20emitter%20particle.%20Advances%20in%20jet%0Atagging%20present%20opportunities%20for%20searches%20of%20new%20physics%20beyond%20the%20Standard%0AModel.%20Current%20approaches%20use%20deep%20learning%20to%20uncover%20hidden%20patterns%20in%0Acomplex%20collision%20data.%20However%2C%20the%20representation%20of%20jets%20as%20inputs%20to%20a%20deep%0Alearning%20model%20have%20been%20varied%2C%20and%20often%2C%20informative%20features%20are%20withheld%0Afrom%20models.%20In%20this%20study%2C%20we%20propose%20a%20graph-based%20representation%20of%20a%20jet%0Athat%20encodes%20the%20most%20information%20possible.%20To%20learn%20best%20from%20this%0Arepresentation%2C%20we%20design%20Particle%20Chebyshev%20Network%20%28PCN%29%2C%20a%20graph%20neural%0Anetwork%20%28GNN%29%20using%20Chebyshev%20graph%20convolutions%20%28ChebConv%29.%20ChebConv%20has%20been%0Ademonstrated%20as%20an%20effective%20alternative%20to%20classical%20graph%20convolutions%20in%0AGNNs%20and%20has%20yet%20to%20be%20explored%20in%20jet%20tagging.%20PCN%20achieves%20a%20substantial%0Aimprovement%20in%20accuracy%20over%20existing%20taggers%20and%20opens%20the%20door%20to%20future%0Astudies%20into%20graph-based%20representations%20of%20jets%20and%20ChebConv%20layers%20in%0Ahigh-energy%20physics%20experiments.%20Code%20is%20available%20at%0Ahttps%3A//github.com/YVSemlani/PCN-Jet-Tagging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08630v4&entry.124074799=Read"},
{"title": "The Evolution of Learning: Assessing the Transformative Impact of\n  Generative AI on Higher Education", "author": "Stefanie Krause and Bhumi Hitesh Panchal and Nikhil Ubhe", "abstract": "  Generative Artificial Intelligence (GAI) models such as ChatGPT have\nexperienced a surge in popularity, attracting 100 million active users in 2\nmonths and generating an estimated 10 million daily queries. Despite this\nremarkable adoption, there remains a limited understanding to which extent this\ninnovative technology influences higher education. This research paper\ninvestigates the impact of GAI on university students and Higher Education\nInstitutions (HEIs). The study adopts a mixed-methods approach, combining a\ncomprehensive survey with scenario analysis to explore potential benefits,\ndrawbacks, and transformative changes the new technology brings. Using an\nonline survey with 130 participants we assessed students' perspectives and\nattitudes concerning present ChatGPT usage in academics. Results show that\nstudents use the current technology for tasks like assignment writing and exam\npreparation and believe it to be a effective help in achieving academic goals.\nThe scenario analysis afterwards projected potential future scenarios,\nproviding valuable insights into the possibilities and challenges associated\nwith incorporating GAI into higher education. The main motivation is to gain a\ntangible and precise understanding of the potential consequences for HEIs and\nto provide guidance responding to the evolving learning environment. The\nfindings indicate that irresponsible and excessive use of the technology could\nresult in significant challenges. Hence, HEIs must develop stringent policies,\nreevaluate learning objectives, upskill their lecturers, adjust the curriculum\nand reconsider examination approaches.\n", "link": "http://arxiv.org/abs/2404.10551v1", "date": "2024-04-16", "relevancy": 1.9725, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.537}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4728}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4344}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Evolution%20of%20Learning%3A%20Assessing%20the%20Transformative%20Impact%20of%0A%20%20Generative%20AI%20on%20Higher%20Education&body=Title%3A%20The%20Evolution%20of%20Learning%3A%20Assessing%20the%20Transformative%20Impact%20of%0A%20%20Generative%20AI%20on%20Higher%20Education%0AAuthor%3A%20Stefanie%20Krause%20and%20Bhumi%20Hitesh%20Panchal%20and%20Nikhil%20Ubhe%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GAI%29%20models%20such%20as%20ChatGPT%20have%0Aexperienced%20a%20surge%20in%20popularity%2C%20attracting%20100%20million%20active%20users%20in%202%0Amonths%20and%20generating%20an%20estimated%2010%20million%20daily%20queries.%20Despite%20this%0Aremarkable%20adoption%2C%20there%20remains%20a%20limited%20understanding%20to%20which%20extent%20this%0Ainnovative%20technology%20influences%20higher%20education.%20This%20research%20paper%0Ainvestigates%20the%20impact%20of%20GAI%20on%20university%20students%20and%20Higher%20Education%0AInstitutions%20%28HEIs%29.%20The%20study%20adopts%20a%20mixed-methods%20approach%2C%20combining%20a%0Acomprehensive%20survey%20with%20scenario%20analysis%20to%20explore%20potential%20benefits%2C%0Adrawbacks%2C%20and%20transformative%20changes%20the%20new%20technology%20brings.%20Using%20an%0Aonline%20survey%20with%20130%20participants%20we%20assessed%20students%27%20perspectives%20and%0Aattitudes%20concerning%20present%20ChatGPT%20usage%20in%20academics.%20Results%20show%20that%0Astudents%20use%20the%20current%20technology%20for%20tasks%20like%20assignment%20writing%20and%20exam%0Apreparation%20and%20believe%20it%20to%20be%20a%20effective%20help%20in%20achieving%20academic%20goals.%0AThe%20scenario%20analysis%20afterwards%20projected%20potential%20future%20scenarios%2C%0Aproviding%20valuable%20insights%20into%20the%20possibilities%20and%20challenges%20associated%0Awith%20incorporating%20GAI%20into%20higher%20education.%20The%20main%20motivation%20is%20to%20gain%20a%0Atangible%20and%20precise%20understanding%20of%20the%20potential%20consequences%20for%20HEIs%20and%0Ato%20provide%20guidance%20responding%20to%20the%20evolving%20learning%20environment.%20The%0Afindings%20indicate%20that%20irresponsible%20and%20excessive%20use%20of%20the%20technology%20could%0Aresult%20in%20significant%20challenges.%20Hence%2C%20HEIs%20must%20develop%20stringent%20policies%2C%0Areevaluate%20learning%20objectives%2C%20upskill%20their%20lecturers%2C%20adjust%20the%20curriculum%0Aand%20reconsider%20examination%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10551v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Evolution%20of%20Learning%3A%20Assessing%20the%20Transformative%20Impact%20of%0A%20%20Generative%20AI%20on%20Higher%20Education&entry.906535625=Stefanie%20Krause%20and%20Bhumi%20Hitesh%20Panchal%20and%20Nikhil%20Ubhe&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GAI%29%20models%20such%20as%20ChatGPT%20have%0Aexperienced%20a%20surge%20in%20popularity%2C%20attracting%20100%20million%20active%20users%20in%202%0Amonths%20and%20generating%20an%20estimated%2010%20million%20daily%20queries.%20Despite%20this%0Aremarkable%20adoption%2C%20there%20remains%20a%20limited%20understanding%20to%20which%20extent%20this%0Ainnovative%20technology%20influences%20higher%20education.%20This%20research%20paper%0Ainvestigates%20the%20impact%20of%20GAI%20on%20university%20students%20and%20Higher%20Education%0AInstitutions%20%28HEIs%29.%20The%20study%20adopts%20a%20mixed-methods%20approach%2C%20combining%20a%0Acomprehensive%20survey%20with%20scenario%20analysis%20to%20explore%20potential%20benefits%2C%0Adrawbacks%2C%20and%20transformative%20changes%20the%20new%20technology%20brings.%20Using%20an%0Aonline%20survey%20with%20130%20participants%20we%20assessed%20students%27%20perspectives%20and%0Aattitudes%20concerning%20present%20ChatGPT%20usage%20in%20academics.%20Results%20show%20that%0Astudents%20use%20the%20current%20technology%20for%20tasks%20like%20assignment%20writing%20and%20exam%0Apreparation%20and%20believe%20it%20to%20be%20a%20effective%20help%20in%20achieving%20academic%20goals.%0AThe%20scenario%20analysis%20afterwards%20projected%20potential%20future%20scenarios%2C%0Aproviding%20valuable%20insights%20into%20the%20possibilities%20and%20challenges%20associated%0Awith%20incorporating%20GAI%20into%20higher%20education.%20The%20main%20motivation%20is%20to%20gain%20a%0Atangible%20and%20precise%20understanding%20of%20the%20potential%20consequences%20for%20HEIs%20and%0Ato%20provide%20guidance%20responding%20to%20the%20evolving%20learning%20environment.%20The%0Afindings%20indicate%20that%20irresponsible%20and%20excessive%20use%20of%20the%20technology%20could%0Aresult%20in%20significant%20challenges.%20Hence%2C%20HEIs%20must%20develop%20stringent%20policies%2C%0Areevaluate%20learning%20objectives%2C%20upskill%20their%20lecturers%2C%20adjust%20the%20curriculum%0Aand%20reconsider%20examination%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10551v1&entry.124074799=Read"},
{"title": "Semi-supervised Fr\u00e9chet Regression", "author": "Rui Qiu and Zhou Yu and Zhenhua Lin", "abstract": "  This paper explores the field of semi-supervised Fr\\'echet regression, driven\nby the significant costs associated with obtaining non-Euclidean labels.\nMethodologically, we propose two novel methods: semi-supervised NW Fr\\'echet\nregression and semi-supervised kNN Fr\\'echet regression, both based on graph\ndistance acquired from all feature instances. These methods extend the scope of\nexisting semi-supervised Euclidean regression methods. We establish their\nconvergence rates with limited labeled data and large amounts of unlabeled\ndata, taking into account the low-dimensional manifold structure of the feature\nspace. Through comprehensive simulations across diverse settings and\napplications to real data, we demonstrate the superior performance of our\nmethods over their supervised counterparts. This study addresses existing\nresearch gaps and paves the way for further exploration and advancements in the\nfield of semi-supervised Fr\\'echet regression.\n", "link": "http://arxiv.org/abs/2404.10444v1", "date": "2024-04-16", "relevancy": 1.9721, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4753}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%20Fr%C3%A9chet%20Regression&body=Title%3A%20Semi-supervised%20Fr%C3%A9chet%20Regression%0AAuthor%3A%20Rui%20Qiu%20and%20Zhou%20Yu%20and%20Zhenhua%20Lin%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20field%20of%20semi-supervised%20Fr%5C%27echet%20regression%2C%20driven%0Aby%20the%20significant%20costs%20associated%20with%20obtaining%20non-Euclidean%20labels.%0AMethodologically%2C%20we%20propose%20two%20novel%20methods%3A%20semi-supervised%20NW%20Fr%5C%27echet%0Aregression%20and%20semi-supervised%20kNN%20Fr%5C%27echet%20regression%2C%20both%20based%20on%20graph%0Adistance%20acquired%20from%20all%20feature%20instances.%20These%20methods%20extend%20the%20scope%20of%0Aexisting%20semi-supervised%20Euclidean%20regression%20methods.%20We%20establish%20their%0Aconvergence%20rates%20with%20limited%20labeled%20data%20and%20large%20amounts%20of%20unlabeled%0Adata%2C%20taking%20into%20account%20the%20low-dimensional%20manifold%20structure%20of%20the%20feature%0Aspace.%20Through%20comprehensive%20simulations%20across%20diverse%20settings%20and%0Aapplications%20to%20real%20data%2C%20we%20demonstrate%20the%20superior%20performance%20of%20our%0Amethods%20over%20their%20supervised%20counterparts.%20This%20study%20addresses%20existing%0Aresearch%20gaps%20and%20paves%20the%20way%20for%20further%20exploration%20and%20advancements%20in%20the%0Afield%20of%20semi-supervised%20Fr%5C%27echet%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10444v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Fr%C3%A9chet%20Regression&entry.906535625=Rui%20Qiu%20and%20Zhou%20Yu%20and%20Zhenhua%20Lin&entry.1292438233=%20%20This%20paper%20explores%20the%20field%20of%20semi-supervised%20Fr%5C%27echet%20regression%2C%20driven%0Aby%20the%20significant%20costs%20associated%20with%20obtaining%20non-Euclidean%20labels.%0AMethodologically%2C%20we%20propose%20two%20novel%20methods%3A%20semi-supervised%20NW%20Fr%5C%27echet%0Aregression%20and%20semi-supervised%20kNN%20Fr%5C%27echet%20regression%2C%20both%20based%20on%20graph%0Adistance%20acquired%20from%20all%20feature%20instances.%20These%20methods%20extend%20the%20scope%20of%0Aexisting%20semi-supervised%20Euclidean%20regression%20methods.%20We%20establish%20their%0Aconvergence%20rates%20with%20limited%20labeled%20data%20and%20large%20amounts%20of%20unlabeled%0Adata%2C%20taking%20into%20account%20the%20low-dimensional%20manifold%20structure%20of%20the%20feature%0Aspace.%20Through%20comprehensive%20simulations%20across%20diverse%20settings%20and%0Aapplications%20to%20real%20data%2C%20we%20demonstrate%20the%20superior%20performance%20of%20our%0Amethods%20over%20their%20supervised%20counterparts.%20This%20study%20addresses%20existing%0Aresearch%20gaps%20and%20paves%20the%20way%20for%20further%20exploration%20and%20advancements%20in%20the%0Afield%20of%20semi-supervised%20Fr%5C%27echet%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10444v1&entry.124074799=Read"},
{"title": "On Training Data Influence of GPT Models", "author": "Qingyi Liu and Yekun Chai and Shuohuan Wang and Yu Sun and Qiwei Peng and Keze Wang and Hua Wu", "abstract": "  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.\n", "link": "http://arxiv.org/abs/2404.07840v2", "date": "2024-04-16", "relevancy": 1.9675, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5175}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5103}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Training%20Data%20Influence%20of%20GPT%20Models&body=Title%3A%20On%20Training%20Data%20Influence%20of%20GPT%20Models%0AAuthor%3A%20Qingyi%20Liu%20and%20Yekun%20Chai%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Qiwei%20Peng%20and%20Keze%20Wang%20and%20Hua%20Wu%0AAbstract%3A%20%20%20Amidst%20the%20rapid%20advancements%20in%20generative%20language%20models%2C%20the%0Ainvestigation%20of%20how%20training%20data%20shapes%20the%20performance%20of%20GPT%20models%20is%0Astill%20emerging.%20This%20paper%20presents%20GPTfluence%2C%20a%20novel%20approach%20that%20leverages%0Aa%20featurized%20simulation%20to%20assess%20the%20impact%20of%20training%20examples%20on%20the%0Atraining%20dynamics%20of%20GPT%20models.%20Our%20approach%20not%20only%20traces%20the%20influence%20of%0Aindividual%20training%20instances%20on%20performance%20trajectories%2C%20such%20as%20loss%20and%0Aother%20key%20metrics%2C%20on%20targeted%20test%20points%20but%20also%20enables%20a%20comprehensive%0Acomparison%20with%20existing%20methods%20across%20various%20training%20scenarios%20in%20GPT%0Amodels%2C%20ranging%20from%2014%20million%20to%202.8%20billion%20parameters%2C%20across%20a%20range%20of%0Adownstream%20tasks.%20Contrary%20to%20earlier%20methods%20that%20struggle%20with%20generalization%0Ato%20new%20data%2C%20GPTfluence%20introduces%20a%20parameterized%20simulation%20of%20training%0Adynamics%2C%20demonstrating%20robust%20generalization%20capabilities%20to%20unseen%20training%0Adata.%20This%20adaptability%20is%20evident%20across%20both%20fine-tuning%20and%0Ainstruction-tuning%20scenarios%2C%20spanning%20tasks%20in%20natural%20language%20understanding%0Aand%20generation.%20We%20will%20make%20our%20code%20and%20data%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07840v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Training%20Data%20Influence%20of%20GPT%20Models&entry.906535625=Qingyi%20Liu%20and%20Yekun%20Chai%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Qiwei%20Peng%20and%20Keze%20Wang%20and%20Hua%20Wu&entry.1292438233=%20%20Amidst%20the%20rapid%20advancements%20in%20generative%20language%20models%2C%20the%0Ainvestigation%20of%20how%20training%20data%20shapes%20the%20performance%20of%20GPT%20models%20is%0Astill%20emerging.%20This%20paper%20presents%20GPTfluence%2C%20a%20novel%20approach%20that%20leverages%0Aa%20featurized%20simulation%20to%20assess%20the%20impact%20of%20training%20examples%20on%20the%0Atraining%20dynamics%20of%20GPT%20models.%20Our%20approach%20not%20only%20traces%20the%20influence%20of%0Aindividual%20training%20instances%20on%20performance%20trajectories%2C%20such%20as%20loss%20and%0Aother%20key%20metrics%2C%20on%20targeted%20test%20points%20but%20also%20enables%20a%20comprehensive%0Acomparison%20with%20existing%20methods%20across%20various%20training%20scenarios%20in%20GPT%0Amodels%2C%20ranging%20from%2014%20million%20to%202.8%20billion%20parameters%2C%20across%20a%20range%20of%0Adownstream%20tasks.%20Contrary%20to%20earlier%20methods%20that%20struggle%20with%20generalization%0Ato%20new%20data%2C%20GPTfluence%20introduces%20a%20parameterized%20simulation%20of%20training%0Adynamics%2C%20demonstrating%20robust%20generalization%20capabilities%20to%20unseen%20training%0Adata.%20This%20adaptability%20is%20evident%20across%20both%20fine-tuning%20and%0Ainstruction-tuning%20scenarios%2C%20spanning%20tasks%20in%20natural%20language%20understanding%0Aand%20generation.%20We%20will%20make%20our%20code%20and%20data%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07840v2&entry.124074799=Read"},
{"title": "Do Counterfactual Examples Complicate Adversarial Training?", "author": "Eric Yeats and Cameron Darwin and Eduardo Ortega and Frank Liu and Hai Li", "abstract": "  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n", "link": "http://arxiv.org/abs/2404.10588v1", "date": "2024-04-16", "relevancy": 1.9553, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F&body=Title%3A%20Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F%0AAuthor%3A%20Eric%20Yeats%20and%20Cameron%20Darwin%20and%20Eduardo%20Ortega%20and%20Frank%20Liu%20and%20Hai%20Li%0AAbstract%3A%20%20%20We%20leverage%20diffusion%20models%20to%20study%20the%20robustness-performance%20tradeoff%20of%0Arobust%20classifiers.%20Our%20approach%20introduces%20a%20simple%2C%20pretrained%20diffusion%0Amethod%20to%20generate%20low-norm%20counterfactual%20examples%20%28CEs%29%3A%20semantically%20altered%0Adata%20which%20results%20in%20different%20true%20class%20membership.%20We%20report%20that%20the%0Aconfidence%20and%20accuracy%20of%20robust%20models%20on%20their%20clean%20training%20data%20are%0Aassociated%20with%20the%20proximity%20of%20the%20data%20to%20their%20CEs.%20Moreover%2C%20robust%20models%0Aperform%20very%20poorly%20when%20evaluated%20on%20the%20CEs%20directly%2C%20as%20they%20become%0Aincreasingly%20invariant%20to%20the%20low-norm%2C%20semantic%20changes%20brought%20by%20CEs.%20The%0Aresults%20indicate%20a%20significant%20overlap%20between%20non-robust%20and%20semantic%0Afeatures%2C%20countering%20the%20common%20assumption%20that%20non-robust%20features%20are%20not%0Ainterpretable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10588v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F&entry.906535625=Eric%20Yeats%20and%20Cameron%20Darwin%20and%20Eduardo%20Ortega%20and%20Frank%20Liu%20and%20Hai%20Li&entry.1292438233=%20%20We%20leverage%20diffusion%20models%20to%20study%20the%20robustness-performance%20tradeoff%20of%0Arobust%20classifiers.%20Our%20approach%20introduces%20a%20simple%2C%20pretrained%20diffusion%0Amethod%20to%20generate%20low-norm%20counterfactual%20examples%20%28CEs%29%3A%20semantically%20altered%0Adata%20which%20results%20in%20different%20true%20class%20membership.%20We%20report%20that%20the%0Aconfidence%20and%20accuracy%20of%20robust%20models%20on%20their%20clean%20training%20data%20are%0Aassociated%20with%20the%20proximity%20of%20the%20data%20to%20their%20CEs.%20Moreover%2C%20robust%20models%0Aperform%20very%20poorly%20when%20evaluated%20on%20the%20CEs%20directly%2C%20as%20they%20become%0Aincreasingly%20invariant%20to%20the%20low-norm%2C%20semantic%20changes%20brought%20by%20CEs.%20The%0Aresults%20indicate%20a%20significant%20overlap%20between%20non-robust%20and%20semantic%0Afeatures%2C%20countering%20the%20common%20assumption%20that%20non-robust%20features%20are%20not%0Ainterpretable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10588v1&entry.124074799=Read"},
{"title": "ReWiTe: Realistic Wide-angle and Telephoto Dual Camera Fusion Dataset\n  via Beam Splitter Camera Rig", "author": "Chunli Peng and Xuan Dong and Tiantian Cao and Zhengqing Li and Kun Dong and Weixin Li", "abstract": "  The fusion of images from dual camera systems featuring a wide-angle and a\ntelephoto camera has become a hotspot problem recently. By integrating\nsimultaneously captured wide-angle and telephoto images from these systems, the\nresulting fused image achieves a wide field of view (FOV) coupled with\nhigh-definition quality. Existing approaches are mostly deep learning methods,\nand predominantly rely on supervised learning, where the training dataset plays\na pivotal role. However, current datasets typically adopt a data synthesis\napproach generate input pairs of wide-angle and telephoto images alongside\nground-truth images. Notably, the wide-angle inputs are synthesized rather than\ncaptured using real wide-angle cameras, and the ground-truth image is captured\nby wide-angle camera whose quality is substantially lower than that of input\ntelephoto images captured by telephoto cameras. To address these limitations,\nwe introduce a novel hardware setup utilizing a beam splitter to simultaneously\ncapture three images, i.e. input pairs and ground-truth images, from two\nauthentic cellphones equipped with wide-angle and telephoto dual cameras.\nSpecifically, the wide-angle and telephoto images captured by cellphone 2 serve\nas the input pair, while the telephoto image captured by cellphone 1, which is\ncalibrated to match the optical path of the wide-angle image from cellphone 2,\nserves as the ground-truth image, maintaining quality on par with the input\ntelephoto image. Experiments validate the efficacy of our newly introduced\ndataset, named ReWiTe, significantly enhances the performance of various\nexisting methods for real-world wide-angle and telephoto dual image fusion\ntasks.\n", "link": "http://arxiv.org/abs/2404.10584v1", "date": "2024-04-16", "relevancy": 1.947, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4779}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ReWiTe%3A%20Realistic%20Wide-angle%20and%20Telephoto%20Dual%20Camera%20Fusion%20Dataset%0A%20%20via%20Beam%20Splitter%20Camera%20Rig&body=Title%3A%20ReWiTe%3A%20Realistic%20Wide-angle%20and%20Telephoto%20Dual%20Camera%20Fusion%20Dataset%0A%20%20via%20Beam%20Splitter%20Camera%20Rig%0AAuthor%3A%20Chunli%20Peng%20and%20Xuan%20Dong%20and%20Tiantian%20Cao%20and%20Zhengqing%20Li%20and%20Kun%20Dong%20and%20Weixin%20Li%0AAbstract%3A%20%20%20The%20fusion%20of%20images%20from%20dual%20camera%20systems%20featuring%20a%20wide-angle%20and%20a%0Atelephoto%20camera%20has%20become%20a%20hotspot%20problem%20recently.%20By%20integrating%0Asimultaneously%20captured%20wide-angle%20and%20telephoto%20images%20from%20these%20systems%2C%20the%0Aresulting%20fused%20image%20achieves%20a%20wide%20field%20of%20view%20%28FOV%29%20coupled%20with%0Ahigh-definition%20quality.%20Existing%20approaches%20are%20mostly%20deep%20learning%20methods%2C%0Aand%20predominantly%20rely%20on%20supervised%20learning%2C%20where%20the%20training%20dataset%20plays%0Aa%20pivotal%20role.%20However%2C%20current%20datasets%20typically%20adopt%20a%20data%20synthesis%0Aapproach%20generate%20input%20pairs%20of%20wide-angle%20and%20telephoto%20images%20alongside%0Aground-truth%20images.%20Notably%2C%20the%20wide-angle%20inputs%20are%20synthesized%20rather%20than%0Acaptured%20using%20real%20wide-angle%20cameras%2C%20and%20the%20ground-truth%20image%20is%20captured%0Aby%20wide-angle%20camera%20whose%20quality%20is%20substantially%20lower%20than%20that%20of%20input%0Atelephoto%20images%20captured%20by%20telephoto%20cameras.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20a%20novel%20hardware%20setup%20utilizing%20a%20beam%20splitter%20to%20simultaneously%0Acapture%20three%20images%2C%20i.e.%20input%20pairs%20and%20ground-truth%20images%2C%20from%20two%0Aauthentic%20cellphones%20equipped%20with%20wide-angle%20and%20telephoto%20dual%20cameras.%0ASpecifically%2C%20the%20wide-angle%20and%20telephoto%20images%20captured%20by%20cellphone%202%20serve%0Aas%20the%20input%20pair%2C%20while%20the%20telephoto%20image%20captured%20by%20cellphone%201%2C%20which%20is%0Acalibrated%20to%20match%20the%20optical%20path%20of%20the%20wide-angle%20image%20from%20cellphone%202%2C%0Aserves%20as%20the%20ground-truth%20image%2C%20maintaining%20quality%20on%20par%20with%20the%20input%0Atelephoto%20image.%20Experiments%20validate%20the%20efficacy%20of%20our%20newly%20introduced%0Adataset%2C%20named%20ReWiTe%2C%20significantly%20enhances%20the%20performance%20of%20various%0Aexisting%20methods%20for%20real-world%20wide-angle%20and%20telephoto%20dual%20image%20fusion%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReWiTe%3A%20Realistic%20Wide-angle%20and%20Telephoto%20Dual%20Camera%20Fusion%20Dataset%0A%20%20via%20Beam%20Splitter%20Camera%20Rig&entry.906535625=Chunli%20Peng%20and%20Xuan%20Dong%20and%20Tiantian%20Cao%20and%20Zhengqing%20Li%20and%20Kun%20Dong%20and%20Weixin%20Li&entry.1292438233=%20%20The%20fusion%20of%20images%20from%20dual%20camera%20systems%20featuring%20a%20wide-angle%20and%20a%0Atelephoto%20camera%20has%20become%20a%20hotspot%20problem%20recently.%20By%20integrating%0Asimultaneously%20captured%20wide-angle%20and%20telephoto%20images%20from%20these%20systems%2C%20the%0Aresulting%20fused%20image%20achieves%20a%20wide%20field%20of%20view%20%28FOV%29%20coupled%20with%0Ahigh-definition%20quality.%20Existing%20approaches%20are%20mostly%20deep%20learning%20methods%2C%0Aand%20predominantly%20rely%20on%20supervised%20learning%2C%20where%20the%20training%20dataset%20plays%0Aa%20pivotal%20role.%20However%2C%20current%20datasets%20typically%20adopt%20a%20data%20synthesis%0Aapproach%20generate%20input%20pairs%20of%20wide-angle%20and%20telephoto%20images%20alongside%0Aground-truth%20images.%20Notably%2C%20the%20wide-angle%20inputs%20are%20synthesized%20rather%20than%0Acaptured%20using%20real%20wide-angle%20cameras%2C%20and%20the%20ground-truth%20image%20is%20captured%0Aby%20wide-angle%20camera%20whose%20quality%20is%20substantially%20lower%20than%20that%20of%20input%0Atelephoto%20images%20captured%20by%20telephoto%20cameras.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20a%20novel%20hardware%20setup%20utilizing%20a%20beam%20splitter%20to%20simultaneously%0Acapture%20three%20images%2C%20i.e.%20input%20pairs%20and%20ground-truth%20images%2C%20from%20two%0Aauthentic%20cellphones%20equipped%20with%20wide-angle%20and%20telephoto%20dual%20cameras.%0ASpecifically%2C%20the%20wide-angle%20and%20telephoto%20images%20captured%20by%20cellphone%202%20serve%0Aas%20the%20input%20pair%2C%20while%20the%20telephoto%20image%20captured%20by%20cellphone%201%2C%20which%20is%0Acalibrated%20to%20match%20the%20optical%20path%20of%20the%20wide-angle%20image%20from%20cellphone%202%2C%0Aserves%20as%20the%20ground-truth%20image%2C%20maintaining%20quality%20on%20par%20with%20the%20input%0Atelephoto%20image.%20Experiments%20validate%20the%20efficacy%20of%20our%20newly%20introduced%0Adataset%2C%20named%20ReWiTe%2C%20significantly%20enhances%20the%20performance%20of%20various%0Aexisting%20methods%20for%20real-world%20wide-angle%20and%20telephoto%20dual%20image%20fusion%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10584v1&entry.124074799=Read"},
{"title": "A Systematic Review of Low-Rank and Local Low-Rank Matrix Approximation\n  in Big Data Medical Imaging", "author": "Sisipho Hamlomo and Marcellin Atemkeng and Yusuf Brima and Chuneeta Nunhokee and Jeremy Baxter", "abstract": "  The large volume and complexity of medical imaging datasets are bottlenecks\nfor storage, transmission, and processing. To tackle these challenges, the\napplication of low-rank matrix approximation (LRMA) and its derivative, local\nLRMA (LLRMA) has demonstrated potential.\n  A detailed analysis of the literature identifies LRMA and LLRMA methods\napplied to various imaging modalities, and the challenges and limitations\nassociated with existing LRMA and LLRMA methods are addressed.\n  We note a significant shift towards a preference for LLRMA in the medical\nimaging field since 2015, demonstrating its potential and effectiveness in\ncapturing complex structures in medical data compared to LRMA. Acknowledging\nthe limitations of shallow similarity methods used with LLRMA, we suggest\nadvanced semantic image segmentation for similarity measure, explaining in\ndetail how it can measure similar patches and their feasibility.\n  We note that LRMA and LLRMA are mainly applied to unstructured medical data,\nand we propose extending their application to different medical data types,\nincluding structured and semi-structured. This paper also discusses how LRMA\nand LLRMA can be applied to regular data with missing entries and the impact of\ninaccuracies in predicting missing values and their effects. We discuss the\nimpact of patch size and propose the use of random search (RS) to determine the\noptimal patch size. To enhance feasibility, a hybrid approach using Bayesian\noptimization and RS is proposed, which could improve the application of LRMA\nand LLRMA in medical imaging.\n", "link": "http://arxiv.org/abs/2402.14045v2", "date": "2024-04-16", "relevancy": 1.9439, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4838}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4719}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Review%20of%20Low-Rank%20and%20Local%20Low-Rank%20Matrix%20Approximation%0A%20%20in%20Big%20Data%20Medical%20Imaging&body=Title%3A%20A%20Systematic%20Review%20of%20Low-Rank%20and%20Local%20Low-Rank%20Matrix%20Approximation%0A%20%20in%20Big%20Data%20Medical%20Imaging%0AAuthor%3A%20Sisipho%20Hamlomo%20and%20Marcellin%20Atemkeng%20and%20Yusuf%20Brima%20and%20Chuneeta%20Nunhokee%20and%20Jeremy%20Baxter%0AAbstract%3A%20%20%20The%20large%20volume%20and%20complexity%20of%20medical%20imaging%20datasets%20are%20bottlenecks%0Afor%20storage%2C%20transmission%2C%20and%20processing.%20To%20tackle%20these%20challenges%2C%20the%0Aapplication%20of%20low-rank%20matrix%20approximation%20%28LRMA%29%20and%20its%20derivative%2C%20local%0ALRMA%20%28LLRMA%29%20has%20demonstrated%20potential.%0A%20%20A%20detailed%20analysis%20of%20the%20literature%20identifies%20LRMA%20and%20LLRMA%20methods%0Aapplied%20to%20various%20imaging%20modalities%2C%20and%20the%20challenges%20and%20limitations%0Aassociated%20with%20existing%20LRMA%20and%20LLRMA%20methods%20are%20addressed.%0A%20%20We%20note%20a%20significant%20shift%20towards%20a%20preference%20for%20LLRMA%20in%20the%20medical%0Aimaging%20field%20since%202015%2C%20demonstrating%20its%20potential%20and%20effectiveness%20in%0Acapturing%20complex%20structures%20in%20medical%20data%20compared%20to%20LRMA.%20Acknowledging%0Athe%20limitations%20of%20shallow%20similarity%20methods%20used%20with%20LLRMA%2C%20we%20suggest%0Aadvanced%20semantic%20image%20segmentation%20for%20similarity%20measure%2C%20explaining%20in%0Adetail%20how%20it%20can%20measure%20similar%20patches%20and%20their%20feasibility.%0A%20%20We%20note%20that%20LRMA%20and%20LLRMA%20are%20mainly%20applied%20to%20unstructured%20medical%20data%2C%0Aand%20we%20propose%20extending%20their%20application%20to%20different%20medical%20data%20types%2C%0Aincluding%20structured%20and%20semi-structured.%20This%20paper%20also%20discusses%20how%20LRMA%0Aand%20LLRMA%20can%20be%20applied%20to%20regular%20data%20with%20missing%20entries%20and%20the%20impact%20of%0Ainaccuracies%20in%20predicting%20missing%20values%20and%20their%20effects.%20We%20discuss%20the%0Aimpact%20of%20patch%20size%20and%20propose%20the%20use%20of%20random%20search%20%28RS%29%20to%20determine%20the%0Aoptimal%20patch%20size.%20To%20enhance%20feasibility%2C%20a%20hybrid%20approach%20using%20Bayesian%0Aoptimization%20and%20RS%20is%20proposed%2C%20which%20could%20improve%20the%20application%20of%20LRMA%0Aand%20LLRMA%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14045v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Review%20of%20Low-Rank%20and%20Local%20Low-Rank%20Matrix%20Approximation%0A%20%20in%20Big%20Data%20Medical%20Imaging&entry.906535625=Sisipho%20Hamlomo%20and%20Marcellin%20Atemkeng%20and%20Yusuf%20Brima%20and%20Chuneeta%20Nunhokee%20and%20Jeremy%20Baxter&entry.1292438233=%20%20The%20large%20volume%20and%20complexity%20of%20medical%20imaging%20datasets%20are%20bottlenecks%0Afor%20storage%2C%20transmission%2C%20and%20processing.%20To%20tackle%20these%20challenges%2C%20the%0Aapplication%20of%20low-rank%20matrix%20approximation%20%28LRMA%29%20and%20its%20derivative%2C%20local%0ALRMA%20%28LLRMA%29%20has%20demonstrated%20potential.%0A%20%20A%20detailed%20analysis%20of%20the%20literature%20identifies%20LRMA%20and%20LLRMA%20methods%0Aapplied%20to%20various%20imaging%20modalities%2C%20and%20the%20challenges%20and%20limitations%0Aassociated%20with%20existing%20LRMA%20and%20LLRMA%20methods%20are%20addressed.%0A%20%20We%20note%20a%20significant%20shift%20towards%20a%20preference%20for%20LLRMA%20in%20the%20medical%0Aimaging%20field%20since%202015%2C%20demonstrating%20its%20potential%20and%20effectiveness%20in%0Acapturing%20complex%20structures%20in%20medical%20data%20compared%20to%20LRMA.%20Acknowledging%0Athe%20limitations%20of%20shallow%20similarity%20methods%20used%20with%20LLRMA%2C%20we%20suggest%0Aadvanced%20semantic%20image%20segmentation%20for%20similarity%20measure%2C%20explaining%20in%0Adetail%20how%20it%20can%20measure%20similar%20patches%20and%20their%20feasibility.%0A%20%20We%20note%20that%20LRMA%20and%20LLRMA%20are%20mainly%20applied%20to%20unstructured%20medical%20data%2C%0Aand%20we%20propose%20extending%20their%20application%20to%20different%20medical%20data%20types%2C%0Aincluding%20structured%20and%20semi-structured.%20This%20paper%20also%20discusses%20how%20LRMA%0Aand%20LLRMA%20can%20be%20applied%20to%20regular%20data%20with%20missing%20entries%20and%20the%20impact%20of%0Ainaccuracies%20in%20predicting%20missing%20values%20and%20their%20effects.%20We%20discuss%20the%0Aimpact%20of%20patch%20size%20and%20propose%20the%20use%20of%20random%20search%20%28RS%29%20to%20determine%20the%0Aoptimal%20patch%20size.%20To%20enhance%20feasibility%2C%20a%20hybrid%20approach%20using%20Bayesian%0Aoptimization%20and%20RS%20is%20proposed%2C%20which%20could%20improve%20the%20application%20of%20LRMA%0Aand%20LLRMA%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14045v2&entry.124074799=Read"},
{"title": "Laplace-HDC: Understanding the geometry of binary hyperdimensional\n  computing", "author": "Saeid Pourmand and Wyatt D. Whiting and Alireza Aghasi and Nicholas F. Marshall", "abstract": "  This paper studies the geometry of binary hyperdimensional computing (HDC), a\ncomputational scheme in which data are encoded using high-dimensional binary\nvectors. We establish a result about the similarity structure induced by the\nHDC binding operator and show that the Laplace kernel naturally arises in this\nsetting, motivating our new encoding method Laplace-HDC, which improves upon\nprevious methods. We describe how our results indicate limitations of binary\nHDC in encoding spatial information from images and discuss potential\nsolutions, including using Haar convolutional features and the definition of a\ntranslation-equivariant HDC encoding. Several numerical experiments\nhighlighting the improved accuracy of Laplace-HDC in contrast to alternative\nmethods are presented. We also numerically study other aspects of the proposed\nframework such as robustness and the underlying translation-equivariant\nencoding.\n", "link": "http://arxiv.org/abs/2404.10759v1", "date": "2024-04-16", "relevancy": 1.9431, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5047}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4767}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4704}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing&body=Title%3A%20Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing%0AAuthor%3A%20Saeid%20Pourmand%20and%20Wyatt%20D.%20Whiting%20and%20Alireza%20Aghasi%20and%20Nicholas%20F.%20Marshall%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20geometry%20of%20binary%20hyperdimensional%20computing%20%28HDC%29%2C%20a%0Acomputational%20scheme%20in%20which%20data%20are%20encoded%20using%20high-dimensional%20binary%0Avectors.%20We%20establish%20a%20result%20about%20the%20similarity%20structure%20induced%20by%20the%0AHDC%20binding%20operator%20and%20show%20that%20the%20Laplace%20kernel%20naturally%20arises%20in%20this%0Asetting%2C%20motivating%20our%20new%20encoding%20method%20Laplace-HDC%2C%20which%20improves%20upon%0Aprevious%20methods.%20We%20describe%20how%20our%20results%20indicate%20limitations%20of%20binary%0AHDC%20in%20encoding%20spatial%20information%20from%20images%20and%20discuss%20potential%0Asolutions%2C%20including%20using%20Haar%20convolutional%20features%20and%20the%20definition%20of%20a%0Atranslation-equivariant%20HDC%20encoding.%20Several%20numerical%20experiments%0Ahighlighting%20the%20improved%20accuracy%20of%20Laplace-HDC%20in%20contrast%20to%20alternative%0Amethods%20are%20presented.%20We%20also%20numerically%20study%20other%20aspects%20of%20the%20proposed%0Aframework%20such%20as%20robustness%20and%20the%20underlying%20translation-equivariant%0Aencoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10759v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laplace-HDC%3A%20Understanding%20the%20geometry%20of%20binary%20hyperdimensional%0A%20%20computing&entry.906535625=Saeid%20Pourmand%20and%20Wyatt%20D.%20Whiting%20and%20Alireza%20Aghasi%20and%20Nicholas%20F.%20Marshall&entry.1292438233=%20%20This%20paper%20studies%20the%20geometry%20of%20binary%20hyperdimensional%20computing%20%28HDC%29%2C%20a%0Acomputational%20scheme%20in%20which%20data%20are%20encoded%20using%20high-dimensional%20binary%0Avectors.%20We%20establish%20a%20result%20about%20the%20similarity%20structure%20induced%20by%20the%0AHDC%20binding%20operator%20and%20show%20that%20the%20Laplace%20kernel%20naturally%20arises%20in%20this%0Asetting%2C%20motivating%20our%20new%20encoding%20method%20Laplace-HDC%2C%20which%20improves%20upon%0Aprevious%20methods.%20We%20describe%20how%20our%20results%20indicate%20limitations%20of%20binary%0AHDC%20in%20encoding%20spatial%20information%20from%20images%20and%20discuss%20potential%0Asolutions%2C%20including%20using%20Haar%20convolutional%20features%20and%20the%20definition%20of%20a%0Atranslation-equivariant%20HDC%20encoding.%20Several%20numerical%20experiments%0Ahighlighting%20the%20improved%20accuracy%20of%20Laplace-HDC%20in%20contrast%20to%20alternative%0Amethods%20are%20presented.%20We%20also%20numerically%20study%20other%20aspects%20of%20the%20proposed%0Aframework%20such%20as%20robustness%20and%20the%20underlying%20translation-equivariant%0Aencoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10759v1&entry.124074799=Read"},
{"title": "Teaching Chinese Sign Language with Feedback in Mixed Reality", "author": "Hongli Wen and Yang Xu and Lin Li and Xudong Ru", "abstract": "  Traditional sign language teaching methods face challenges such as limited\nfeedback and diverse learning scenarios. Although 2D resources lack real-time\nfeedback, classroom teaching is constrained by a scarcity of teacher. Methods\nbased on VR and AR have relatively primitive interaction feedback mechanisms.\nThis study proposes an innovative teaching model that uses real-time monocular\nvision and mixed reality technology. First, we introduce an improved\nhand-posture reconstruction method to achieve sign language semantic retention\nand real-time feedback. Second, a ternary system evaluation algorithm is\nproposed for a comprehensive assessment, maintaining good consistency with\nexperts in sign language. Furthermore, we use mixed reality technology to\nconstruct a scenario-based 3D sign language classroom and explore the user\nexperience of scenario teaching. Overall, this paper presents a novel teaching\nmethod that provides an immersive learning experience, advanced posture\nreconstruction, and precise feedback, achieving positive feedback on user\nexperience and learning effectiveness.\n", "link": "http://arxiv.org/abs/2404.10490v1", "date": "2024-04-16", "relevancy": 1.9425, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Teaching%20Chinese%20Sign%20Language%20with%20Feedback%20in%20Mixed%20Reality&body=Title%3A%20Teaching%20Chinese%20Sign%20Language%20with%20Feedback%20in%20Mixed%20Reality%0AAuthor%3A%20Hongli%20Wen%20and%20Yang%20Xu%20and%20Lin%20Li%20and%20Xudong%20Ru%0AAbstract%3A%20%20%20Traditional%20sign%20language%20teaching%20methods%20face%20challenges%20such%20as%20limited%0Afeedback%20and%20diverse%20learning%20scenarios.%20Although%202D%20resources%20lack%20real-time%0Afeedback%2C%20classroom%20teaching%20is%20constrained%20by%20a%20scarcity%20of%20teacher.%20Methods%0Abased%20on%20VR%20and%20AR%20have%20relatively%20primitive%20interaction%20feedback%20mechanisms.%0AThis%20study%20proposes%20an%20innovative%20teaching%20model%20that%20uses%20real-time%20monocular%0Avision%20and%20mixed%20reality%20technology.%20First%2C%20we%20introduce%20an%20improved%0Ahand-posture%20reconstruction%20method%20to%20achieve%20sign%20language%20semantic%20retention%0Aand%20real-time%20feedback.%20Second%2C%20a%20ternary%20system%20evaluation%20algorithm%20is%0Aproposed%20for%20a%20comprehensive%20assessment%2C%20maintaining%20good%20consistency%20with%0Aexperts%20in%20sign%20language.%20Furthermore%2C%20we%20use%20mixed%20reality%20technology%20to%0Aconstruct%20a%20scenario-based%203D%20sign%20language%20classroom%20and%20explore%20the%20user%0Aexperience%20of%20scenario%20teaching.%20Overall%2C%20this%20paper%20presents%20a%20novel%20teaching%0Amethod%20that%20provides%20an%20immersive%20learning%20experience%2C%20advanced%20posture%0Areconstruction%2C%20and%20precise%20feedback%2C%20achieving%20positive%20feedback%20on%20user%0Aexperience%20and%20learning%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10490v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Chinese%20Sign%20Language%20with%20Feedback%20in%20Mixed%20Reality&entry.906535625=Hongli%20Wen%20and%20Yang%20Xu%20and%20Lin%20Li%20and%20Xudong%20Ru&entry.1292438233=%20%20Traditional%20sign%20language%20teaching%20methods%20face%20challenges%20such%20as%20limited%0Afeedback%20and%20diverse%20learning%20scenarios.%20Although%202D%20resources%20lack%20real-time%0Afeedback%2C%20classroom%20teaching%20is%20constrained%20by%20a%20scarcity%20of%20teacher.%20Methods%0Abased%20on%20VR%20and%20AR%20have%20relatively%20primitive%20interaction%20feedback%20mechanisms.%0AThis%20study%20proposes%20an%20innovative%20teaching%20model%20that%20uses%20real-time%20monocular%0Avision%20and%20mixed%20reality%20technology.%20First%2C%20we%20introduce%20an%20improved%0Ahand-posture%20reconstruction%20method%20to%20achieve%20sign%20language%20semantic%20retention%0Aand%20real-time%20feedback.%20Second%2C%20a%20ternary%20system%20evaluation%20algorithm%20is%0Aproposed%20for%20a%20comprehensive%20assessment%2C%20maintaining%20good%20consistency%20with%0Aexperts%20in%20sign%20language.%20Furthermore%2C%20we%20use%20mixed%20reality%20technology%20to%0Aconstruct%20a%20scenario-based%203D%20sign%20language%20classroom%20and%20explore%20the%20user%0Aexperience%20of%20scenario%20teaching.%20Overall%2C%20this%20paper%20presents%20a%20novel%20teaching%0Amethod%20that%20provides%20an%20immersive%20learning%20experience%2C%20advanced%20posture%0Areconstruction%2C%20and%20precise%20feedback%2C%20achieving%20positive%20feedback%20on%20user%0Aexperience%20and%20learning%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10490v1&entry.124074799=Read"},
{"title": "LaVy: Vietnamese Multimodal Large Language Model", "author": "Chi Tran and Huong Le Thanh", "abstract": "  Large Language Models (LLMs) and Multimodal Large language models (MLLMs)\nhave taken the world by storm with impressive abilities in complex reasoning\nand linguistic comprehension. Meanwhile there are plethora of works related to\nVietnamese Large Language Models, the lack of high-quality resources in\nmultimodality limits the progress of Vietnamese MLLMs. In this paper, we\npioneer in address this by introducing LaVy, a state-of-the-art Vietnamese\nMLLM, and we also introduce LaVy-Bench benchmark designated for evaluating\nMLLMs's understanding on Vietnamese visual language tasks. Our project is\npublic at https://github.com/baochi0212/LaVy\n", "link": "http://arxiv.org/abs/2404.07922v3", "date": "2024-04-16", "relevancy": 1.9327, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5128}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4588}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaVy%3A%20Vietnamese%20Multimodal%20Large%20Language%20Model&body=Title%3A%20LaVy%3A%20Vietnamese%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Chi%20Tran%20and%20Huong%20Le%20Thanh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20language%20models%20%28MLLMs%29%0Ahave%20taken%20the%20world%20by%20storm%20with%20impressive%20abilities%20in%20complex%20reasoning%0Aand%20linguistic%20comprehension.%20Meanwhile%20there%20are%20plethora%20of%20works%20related%20to%0AVietnamese%20Large%20Language%20Models%2C%20the%20lack%20of%20high-quality%20resources%20in%0Amultimodality%20limits%20the%20progress%20of%20Vietnamese%20MLLMs.%20In%20this%20paper%2C%20we%0Apioneer%20in%20address%20this%20by%20introducing%20LaVy%2C%20a%20state-of-the-art%20Vietnamese%0AMLLM%2C%20and%20we%20also%20introduce%20LaVy-Bench%20benchmark%20designated%20for%20evaluating%0AMLLMs%27s%20understanding%20on%20Vietnamese%20visual%20language%20tasks.%20Our%20project%20is%0Apublic%20at%20https%3A//github.com/baochi0212/LaVy%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07922v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaVy%3A%20Vietnamese%20Multimodal%20Large%20Language%20Model&entry.906535625=Chi%20Tran%20and%20Huong%20Le%20Thanh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20language%20models%20%28MLLMs%29%0Ahave%20taken%20the%20world%20by%20storm%20with%20impressive%20abilities%20in%20complex%20reasoning%0Aand%20linguistic%20comprehension.%20Meanwhile%20there%20are%20plethora%20of%20works%20related%20to%0AVietnamese%20Large%20Language%20Models%2C%20the%20lack%20of%20high-quality%20resources%20in%0Amultimodality%20limits%20the%20progress%20of%20Vietnamese%20MLLMs.%20In%20this%20paper%2C%20we%0Apioneer%20in%20address%20this%20by%20introducing%20LaVy%2C%20a%20state-of-the-art%20Vietnamese%0AMLLM%2C%20and%20we%20also%20introduce%20LaVy-Bench%20benchmark%20designated%20for%20evaluating%0AMLLMs%27s%20understanding%20on%20Vietnamese%20visual%20language%20tasks.%20Our%20project%20is%0Apublic%20at%20https%3A//github.com/baochi0212/LaVy%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07922v3&entry.124074799=Read"},
{"title": "Evolutionary Optimization of 1D-CNN for Non-contact Respiration Pattern\n  Classification", "author": "Md Zobaer Islam and Sabit Ekin and John F. O'Hara and Gary Yen", "abstract": "  In this study, we present a deep learning-based approach for time-series\nrespiration data classification. The dataset contains regular breathing\npatterns as well as various forms of abnormal breathing, obtained through\nnon-contact incoherent light-wave sensing (LWS) technology. Given the\none-dimensional (1D) nature of the data, we employed a 1D convolutional neural\nnetwork (1D-CNN) for classification purposes. Genetic algorithm was employed to\noptimize the 1D-CNN architecture to maximize classification accuracy.\nAddressing the computational complexity associated with training the 1D-CNN\nacross multiple generations, we implemented transfer learning from a\npre-trained model. This approach significantly reduced the computational time\nrequired for training, thereby enhancing the efficiency of the optimization\nprocess. This study contributes valuable insights into the potential\napplications of deep learning methodologies for enhancing respiratory anomaly\ndetection through precise and efficient respiration classification.\n", "link": "http://arxiv.org/abs/2312.13035v2", "date": "2024-04-16", "relevancy": 1.9258, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4891}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4761}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4758}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evolutionary%20Optimization%20of%201D-CNN%20for%20Non-contact%20Respiration%20Pattern%0A%20%20Classification&body=Title%3A%20Evolutionary%20Optimization%20of%201D-CNN%20for%20Non-contact%20Respiration%20Pattern%0A%20%20Classification%0AAuthor%3A%20Md%20Zobaer%20Islam%20and%20Sabit%20Ekin%20and%20John%20F.%20O%27Hara%20and%20Gary%20Yen%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20present%20a%20deep%20learning-based%20approach%20for%20time-series%0Arespiration%20data%20classification.%20The%20dataset%20contains%20regular%20breathing%0Apatterns%20as%20well%20as%20various%20forms%20of%20abnormal%20breathing%2C%20obtained%20through%0Anon-contact%20incoherent%20light-wave%20sensing%20%28LWS%29%20technology.%20Given%20the%0Aone-dimensional%20%281D%29%20nature%20of%20the%20data%2C%20we%20employed%20a%201D%20convolutional%20neural%0Anetwork%20%281D-CNN%29%20for%20classification%20purposes.%20Genetic%20algorithm%20was%20employed%20to%0Aoptimize%20the%201D-CNN%20architecture%20to%20maximize%20classification%20accuracy.%0AAddressing%20the%20computational%20complexity%20associated%20with%20training%20the%201D-CNN%0Aacross%20multiple%20generations%2C%20we%20implemented%20transfer%20learning%20from%20a%0Apre-trained%20model.%20This%20approach%20significantly%20reduced%20the%20computational%20time%0Arequired%20for%20training%2C%20thereby%20enhancing%20the%20efficiency%20of%20the%20optimization%0Aprocess.%20This%20study%20contributes%20valuable%20insights%20into%20the%20potential%0Aapplications%20of%20deep%20learning%20methodologies%20for%20enhancing%20respiratory%20anomaly%0Adetection%20through%20precise%20and%20efficient%20respiration%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13035v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolutionary%20Optimization%20of%201D-CNN%20for%20Non-contact%20Respiration%20Pattern%0A%20%20Classification&entry.906535625=Md%20Zobaer%20Islam%20and%20Sabit%20Ekin%20and%20John%20F.%20O%27Hara%20and%20Gary%20Yen&entry.1292438233=%20%20In%20this%20study%2C%20we%20present%20a%20deep%20learning-based%20approach%20for%20time-series%0Arespiration%20data%20classification.%20The%20dataset%20contains%20regular%20breathing%0Apatterns%20as%20well%20as%20various%20forms%20of%20abnormal%20breathing%2C%20obtained%20through%0Anon-contact%20incoherent%20light-wave%20sensing%20%28LWS%29%20technology.%20Given%20the%0Aone-dimensional%20%281D%29%20nature%20of%20the%20data%2C%20we%20employed%20a%201D%20convolutional%20neural%0Anetwork%20%281D-CNN%29%20for%20classification%20purposes.%20Genetic%20algorithm%20was%20employed%20to%0Aoptimize%20the%201D-CNN%20architecture%20to%20maximize%20classification%20accuracy.%0AAddressing%20the%20computational%20complexity%20associated%20with%20training%20the%201D-CNN%0Aacross%20multiple%20generations%2C%20we%20implemented%20transfer%20learning%20from%20a%0Apre-trained%20model.%20This%20approach%20significantly%20reduced%20the%20computational%20time%0Arequired%20for%20training%2C%20thereby%20enhancing%20the%20efficiency%20of%20the%20optimization%0Aprocess.%20This%20study%20contributes%20valuable%20insights%20into%20the%20potential%0Aapplications%20of%20deep%20learning%20methodologies%20for%20enhancing%20respiratory%20anomaly%0Adetection%20through%20precise%20and%20efficient%20respiration%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13035v2&entry.124074799=Read"},
{"title": "AGHINT: Attribute-Guided Representation Learning on Heterogeneous\n  Information Networks with Transformer", "author": "Jinhui Yuan and Shan Lu and Peibo Duan and Jieyue He", "abstract": "  Recently, heterogeneous graph neural networks (HGNNs) have achieved\nimpressive success in representation learning by capturing long-range\ndependencies and heterogeneity at the node level. However, few existing studies\nhave delved into the utilization of node attributes in heterogeneous\ninformation networks (HINs). In this paper, we investigate the impact of\ninter-node attribute disparities on HGNNs performance within the benchmark\ntask, i.e., node classification, and empirically find that typical models\nexhibit significant performance decline when classifying nodes whose attributes\nmarkedly differ from their neighbors. To alleviate this issue, we propose a\nnovel Attribute-Guided heterogeneous Information Networks representation\nlearning model with Transformer (AGHINT), which allows a more effective\naggregation of neighbor node information under the guidance of attributes.\nSpecifically, AGHINT transcends the constraints of the original graph structure\nby directly integrating higher-order similar neighbor features into the\nlearning process and modifies the message-passing mechanism between nodes based\non their attribute disparities. Extensive experimental results on three\nreal-world heterogeneous graph benchmarks with target node attributes\ndemonstrate that AGHINT outperforms the state-of-the-art.\n", "link": "http://arxiv.org/abs/2404.10443v1", "date": "2024-04-16", "relevancy": 1.918, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4747}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AGHINT%3A%20Attribute-Guided%20Representation%20Learning%20on%20Heterogeneous%0A%20%20Information%20Networks%20with%20Transformer&body=Title%3A%20AGHINT%3A%20Attribute-Guided%20Representation%20Learning%20on%20Heterogeneous%0A%20%20Information%20Networks%20with%20Transformer%0AAuthor%3A%20Jinhui%20Yuan%20and%20Shan%20Lu%20and%20Peibo%20Duan%20and%20Jieyue%20He%0AAbstract%3A%20%20%20Recently%2C%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20achieved%0Aimpressive%20success%20in%20representation%20learning%20by%20capturing%20long-range%0Adependencies%20and%20heterogeneity%20at%20the%20node%20level.%20However%2C%20few%20existing%20studies%0Ahave%20delved%20into%20the%20utilization%20of%20node%20attributes%20in%20heterogeneous%0Ainformation%20networks%20%28HINs%29.%20In%20this%20paper%2C%20we%20investigate%20the%20impact%20of%0Ainter-node%20attribute%20disparities%20on%20HGNNs%20performance%20within%20the%20benchmark%0Atask%2C%20i.e.%2C%20node%20classification%2C%20and%20empirically%20find%20that%20typical%20models%0Aexhibit%20significant%20performance%20decline%20when%20classifying%20nodes%20whose%20attributes%0Amarkedly%20differ%20from%20their%20neighbors.%20To%20alleviate%20this%20issue%2C%20we%20propose%20a%0Anovel%20Attribute-Guided%20heterogeneous%20Information%20Networks%20representation%0Alearning%20model%20with%20Transformer%20%28AGHINT%29%2C%20which%20allows%20a%20more%20effective%0Aaggregation%20of%20neighbor%20node%20information%20under%20the%20guidance%20of%20attributes.%0ASpecifically%2C%20AGHINT%20transcends%20the%20constraints%20of%20the%20original%20graph%20structure%0Aby%20directly%20integrating%20higher-order%20similar%20neighbor%20features%20into%20the%0Alearning%20process%20and%20modifies%20the%20message-passing%20mechanism%20between%20nodes%20based%0Aon%20their%20attribute%20disparities.%20Extensive%20experimental%20results%20on%20three%0Areal-world%20heterogeneous%20graph%20benchmarks%20with%20target%20node%20attributes%0Ademonstrate%20that%20AGHINT%20outperforms%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10443v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGHINT%3A%20Attribute-Guided%20Representation%20Learning%20on%20Heterogeneous%0A%20%20Information%20Networks%20with%20Transformer&entry.906535625=Jinhui%20Yuan%20and%20Shan%20Lu%20and%20Peibo%20Duan%20and%20Jieyue%20He&entry.1292438233=%20%20Recently%2C%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20achieved%0Aimpressive%20success%20in%20representation%20learning%20by%20capturing%20long-range%0Adependencies%20and%20heterogeneity%20at%20the%20node%20level.%20However%2C%20few%20existing%20studies%0Ahave%20delved%20into%20the%20utilization%20of%20node%20attributes%20in%20heterogeneous%0Ainformation%20networks%20%28HINs%29.%20In%20this%20paper%2C%20we%20investigate%20the%20impact%20of%0Ainter-node%20attribute%20disparities%20on%20HGNNs%20performance%20within%20the%20benchmark%0Atask%2C%20i.e.%2C%20node%20classification%2C%20and%20empirically%20find%20that%20typical%20models%0Aexhibit%20significant%20performance%20decline%20when%20classifying%20nodes%20whose%20attributes%0Amarkedly%20differ%20from%20their%20neighbors.%20To%20alleviate%20this%20issue%2C%20we%20propose%20a%0Anovel%20Attribute-Guided%20heterogeneous%20Information%20Networks%20representation%0Alearning%20model%20with%20Transformer%20%28AGHINT%29%2C%20which%20allows%20a%20more%20effective%0Aaggregation%20of%20neighbor%20node%20information%20under%20the%20guidance%20of%20attributes.%0ASpecifically%2C%20AGHINT%20transcends%20the%20constraints%20of%20the%20original%20graph%20structure%0Aby%20directly%20integrating%20higher-order%20similar%20neighbor%20features%20into%20the%0Alearning%20process%20and%20modifies%20the%20message-passing%20mechanism%20between%20nodes%20based%0Aon%20their%20attribute%20disparities.%20Extensive%20experimental%20results%20on%20three%0Areal-world%20heterogeneous%20graph%20benchmarks%20with%20target%20node%20attributes%0Ademonstrate%20that%20AGHINT%20outperforms%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10443v1&entry.124074799=Read"},
{"title": "SparseDM: Toward Sparse Efficient Diffusion Models", "author": "Kafeng Wang and Jianfei Chen and He Li and Zhenpeng Mi and Jun Zhu", "abstract": "  Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods.\n", "link": "http://arxiv.org/abs/2404.10445v1", "date": "2024-04-16", "relevancy": 1.9077, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6767}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6434}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5762}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SparseDM%3A%20Toward%20Sparse%20Efficient%20Diffusion%20Models&body=Title%3A%20SparseDM%3A%20Toward%20Sparse%20Efficient%20Diffusion%20Models%0AAuthor%3A%20Kafeng%20Wang%20and%20Jianfei%20Chen%20and%20He%20Li%20and%20Zhenpeng%20Mi%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20extensively%20used%20in%20data%20generation%20tasks%20and%20are%0Arecognized%20as%20one%20of%20the%20best%20generative%20models.%20However%2C%20their%20time-consuming%0Adeployment%2C%20long%20inference%20time%2C%20and%20requirements%20on%20large%20memory%20limit%20their%0Aapplication%20on%20mobile%20devices.%20In%20this%20paper%2C%20we%20propose%20a%20method%20based%20on%20the%0Aimproved%20Straight-Through%20Estimator%20to%20improve%20the%20deployment%20efficiency%20of%0Adiffusion%20models.%20Specifically%2C%20we%20add%20sparse%20masks%20to%20the%20Convolution%20and%0ALinear%20layers%20in%20a%20pre-trained%20diffusion%20model%2C%20then%20use%20design%20progressive%0Asparsity%20for%20model%20training%20in%20the%20fine-tuning%20stage%2C%20and%20switch%20the%20inference%0Amask%20on%20and%20off%2C%20which%20supports%20a%20flexible%20choice%20of%20sparsity%20during%20inference%0Aaccording%20to%20the%20FID%20and%20MACs%20requirements.%20Experiments%20on%20four%20datasets%0Aconducted%20on%20a%20state-of-the-art%20Transformer-based%20diffusion%20model%20demonstrate%0Athat%20our%20method%20reduces%20MACs%20by%20%2450%5C%25%24%20while%20increasing%20FID%20by%20only%201.5%20on%0Aaverage.%20Under%20other%20MACs%20conditions%2C%20the%20FID%20is%20also%20lower%20than%201%24%5Csim%24137%0Acompared%20to%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10445v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseDM%3A%20Toward%20Sparse%20Efficient%20Diffusion%20Models&entry.906535625=Kafeng%20Wang%20and%20Jianfei%20Chen%20and%20He%20Li%20and%20Zhenpeng%20Mi%20and%20Jun%20Zhu&entry.1292438233=%20%20Diffusion%20models%20have%20been%20extensively%20used%20in%20data%20generation%20tasks%20and%20are%0Arecognized%20as%20one%20of%20the%20best%20generative%20models.%20However%2C%20their%20time-consuming%0Adeployment%2C%20long%20inference%20time%2C%20and%20requirements%20on%20large%20memory%20limit%20their%0Aapplication%20on%20mobile%20devices.%20In%20this%20paper%2C%20we%20propose%20a%20method%20based%20on%20the%0Aimproved%20Straight-Through%20Estimator%20to%20improve%20the%20deployment%20efficiency%20of%0Adiffusion%20models.%20Specifically%2C%20we%20add%20sparse%20masks%20to%20the%20Convolution%20and%0ALinear%20layers%20in%20a%20pre-trained%20diffusion%20model%2C%20then%20use%20design%20progressive%0Asparsity%20for%20model%20training%20in%20the%20fine-tuning%20stage%2C%20and%20switch%20the%20inference%0Amask%20on%20and%20off%2C%20which%20supports%20a%20flexible%20choice%20of%20sparsity%20during%20inference%0Aaccording%20to%20the%20FID%20and%20MACs%20requirements.%20Experiments%20on%20four%20datasets%0Aconducted%20on%20a%20state-of-the-art%20Transformer-based%20diffusion%20model%20demonstrate%0Athat%20our%20method%20reduces%20MACs%20by%20%2450%5C%25%24%20while%20increasing%20FID%20by%20only%201.5%20on%0Aaverage.%20Under%20other%20MACs%20conditions%2C%20the%20FID%20is%20also%20lower%20than%201%24%5Csim%24137%0Acompared%20to%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10445v1&entry.124074799=Read"},
{"title": "HSVI-based Online Minimax Strategies for Partially Observable Stochastic\n  Games with Neural Perception Mechanisms", "author": "Rui Yan and Gabriel Santos and Gethin Norman and David Parker and Marta Kwiatkowska", "abstract": "  We consider a variant of continuous-state partially-observable stochastic\ngames with neural perception mechanisms and an asymmetric information\nstructure. One agent has partial information, with the observation function\nimplemented as a neural network, while the other agent is assumed to have full\nknowledge of the state. We present, for the first time, an efficient online\nmethod to compute an $\\varepsilon$-minimax strategy profile, which requires\nonly one linear program to be solved for each agent at every stage, instead of\na complex estimation of opponent counterfactual values. For the\npartially-informed agent, we propose a continual resolving approach which uses\nlower bounds, pre-computed offline with heuristic search value iteration\n(HSVI), instead of opponent counterfactual values. This inherits the soundness\nof continual resolving at the cost of pre-computing the bound. For the\nfully-informed agent, we propose an inferred-belief strategy, where the agent\nmaintains an inferred belief about the belief of the partially-informed agent\nbased on (offline) upper bounds from HSVI, guaranteeing $\\varepsilon$-distance\nto the value of the game at the initial belief known to both agents.\n", "link": "http://arxiv.org/abs/2404.10679v1", "date": "2024-04-16", "relevancy": 1.9039, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4673}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HSVI-based%20Online%20Minimax%20Strategies%20for%20Partially%20Observable%20Stochastic%0A%20%20Games%20with%20Neural%20Perception%20Mechanisms&body=Title%3A%20HSVI-based%20Online%20Minimax%20Strategies%20for%20Partially%20Observable%20Stochastic%0A%20%20Games%20with%20Neural%20Perception%20Mechanisms%0AAuthor%3A%20Rui%20Yan%20and%20Gabriel%20Santos%20and%20Gethin%20Norman%20and%20David%20Parker%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20We%20consider%20a%20variant%20of%20continuous-state%20partially-observable%20stochastic%0Agames%20with%20neural%20perception%20mechanisms%20and%20an%20asymmetric%20information%0Astructure.%20One%20agent%20has%20partial%20information%2C%20with%20the%20observation%20function%0Aimplemented%20as%20a%20neural%20network%2C%20while%20the%20other%20agent%20is%20assumed%20to%20have%20full%0Aknowledge%20of%20the%20state.%20We%20present%2C%20for%20the%20first%20time%2C%20an%20efficient%20online%0Amethod%20to%20compute%20an%20%24%5Cvarepsilon%24-minimax%20strategy%20profile%2C%20which%20requires%0Aonly%20one%20linear%20program%20to%20be%20solved%20for%20each%20agent%20at%20every%20stage%2C%20instead%20of%0Aa%20complex%20estimation%20of%20opponent%20counterfactual%20values.%20For%20the%0Apartially-informed%20agent%2C%20we%20propose%20a%20continual%20resolving%20approach%20which%20uses%0Alower%20bounds%2C%20pre-computed%20offline%20with%20heuristic%20search%20value%20iteration%0A%28HSVI%29%2C%20instead%20of%20opponent%20counterfactual%20values.%20This%20inherits%20the%20soundness%0Aof%20continual%20resolving%20at%20the%20cost%20of%20pre-computing%20the%20bound.%20For%20the%0Afully-informed%20agent%2C%20we%20propose%20an%20inferred-belief%20strategy%2C%20where%20the%20agent%0Amaintains%20an%20inferred%20belief%20about%20the%20belief%20of%20the%20partially-informed%20agent%0Abased%20on%20%28offline%29%20upper%20bounds%20from%20HSVI%2C%20guaranteeing%20%24%5Cvarepsilon%24-distance%0Ato%20the%20value%20of%20the%20game%20at%20the%20initial%20belief%20known%20to%20both%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10679v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSVI-based%20Online%20Minimax%20Strategies%20for%20Partially%20Observable%20Stochastic%0A%20%20Games%20with%20Neural%20Perception%20Mechanisms&entry.906535625=Rui%20Yan%20and%20Gabriel%20Santos%20and%20Gethin%20Norman%20and%20David%20Parker%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20We%20consider%20a%20variant%20of%20continuous-state%20partially-observable%20stochastic%0Agames%20with%20neural%20perception%20mechanisms%20and%20an%20asymmetric%20information%0Astructure.%20One%20agent%20has%20partial%20information%2C%20with%20the%20observation%20function%0Aimplemented%20as%20a%20neural%20network%2C%20while%20the%20other%20agent%20is%20assumed%20to%20have%20full%0Aknowledge%20of%20the%20state.%20We%20present%2C%20for%20the%20first%20time%2C%20an%20efficient%20online%0Amethod%20to%20compute%20an%20%24%5Cvarepsilon%24-minimax%20strategy%20profile%2C%20which%20requires%0Aonly%20one%20linear%20program%20to%20be%20solved%20for%20each%20agent%20at%20every%20stage%2C%20instead%20of%0Aa%20complex%20estimation%20of%20opponent%20counterfactual%20values.%20For%20the%0Apartially-informed%20agent%2C%20we%20propose%20a%20continual%20resolving%20approach%20which%20uses%0Alower%20bounds%2C%20pre-computed%20offline%20with%20heuristic%20search%20value%20iteration%0A%28HSVI%29%2C%20instead%20of%20opponent%20counterfactual%20values.%20This%20inherits%20the%20soundness%0Aof%20continual%20resolving%20at%20the%20cost%20of%20pre-computing%20the%20bound.%20For%20the%0Afully-informed%20agent%2C%20we%20propose%20an%20inferred-belief%20strategy%2C%20where%20the%20agent%0Amaintains%20an%20inferred%20belief%20about%20the%20belief%20of%20the%20partially-informed%20agent%0Abased%20on%20%28offline%29%20upper%20bounds%20from%20HSVI%2C%20guaranteeing%20%24%5Cvarepsilon%24-distance%0Ato%20the%20value%20of%20the%20game%20at%20the%20initial%20belief%20known%20to%20both%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10679v1&entry.124074799=Read"},
{"title": "Instabilities in Convnets for Raw Audio", "author": "Daniel Haider and Vincent Lostanlen and Martin Ehler and Peter Balazs", "abstract": "  What makes waveform-based deep learning so hard? Despite numerous attempts at\ntraining convolutional neural networks (convnets) for filterbank design, they\noften fail to outperform hand-crafted baselines. These baselines are linear\ntime-invariant systems: as such, they can be approximated by convnets with wide\nreceptive fields. Yet, in practice, gradient-based optimization leads to\nsuboptimal approximations. In our article, we approach this phenomenon from the\nperspective of initialization. We present a theory of large deviations for the\nenergy response of FIR filterbanks with random Gaussian weights. We find that\ndeviations worsen for large filters and locally periodic input signals, which\nare both typical for audio signal processing applications. Numerical\nsimulations align with our theory and suggest that the condition number of a\nconvolutional layer follows a logarithmic scaling law between the number and\nlength of the filters, which is reminiscent of discrete wavelet bases.\n", "link": "http://arxiv.org/abs/2309.05855v3", "date": "2024-04-16", "relevancy": 1.8976, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.523}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4781}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4512}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Instabilities%20in%20Convnets%20for%20Raw%20Audio&body=Title%3A%20Instabilities%20in%20Convnets%20for%20Raw%20Audio%0AAuthor%3A%20Daniel%20Haider%20and%20Vincent%20Lostanlen%20and%20Martin%20Ehler%20and%20Peter%20Balazs%0AAbstract%3A%20%20%20What%20makes%20waveform-based%20deep%20learning%20so%20hard%3F%20Despite%20numerous%20attempts%20at%0Atraining%20convolutional%20neural%20networks%20%28convnets%29%20for%20filterbank%20design%2C%20they%0Aoften%20fail%20to%20outperform%20hand-crafted%20baselines.%20These%20baselines%20are%20linear%0Atime-invariant%20systems%3A%20as%20such%2C%20they%20can%20be%20approximated%20by%20convnets%20with%20wide%0Areceptive%20fields.%20Yet%2C%20in%20practice%2C%20gradient-based%20optimization%20leads%20to%0Asuboptimal%20approximations.%20In%20our%20article%2C%20we%20approach%20this%20phenomenon%20from%20the%0Aperspective%20of%20initialization.%20We%20present%20a%20theory%20of%20large%20deviations%20for%20the%0Aenergy%20response%20of%20FIR%20filterbanks%20with%20random%20Gaussian%20weights.%20We%20find%20that%0Adeviations%20worsen%20for%20large%20filters%20and%20locally%20periodic%20input%20signals%2C%20which%0Aare%20both%20typical%20for%20audio%20signal%20processing%20applications.%20Numerical%0Asimulations%20align%20with%20our%20theory%20and%20suggest%20that%20the%20condition%20number%20of%20a%0Aconvolutional%20layer%20follows%20a%20logarithmic%20scaling%20law%20between%20the%20number%20and%0Alength%20of%20the%20filters%2C%20which%20is%20reminiscent%20of%20discrete%20wavelet%20bases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05855v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instabilities%20in%20Convnets%20for%20Raw%20Audio&entry.906535625=Daniel%20Haider%20and%20Vincent%20Lostanlen%20and%20Martin%20Ehler%20and%20Peter%20Balazs&entry.1292438233=%20%20What%20makes%20waveform-based%20deep%20learning%20so%20hard%3F%20Despite%20numerous%20attempts%20at%0Atraining%20convolutional%20neural%20networks%20%28convnets%29%20for%20filterbank%20design%2C%20they%0Aoften%20fail%20to%20outperform%20hand-crafted%20baselines.%20These%20baselines%20are%20linear%0Atime-invariant%20systems%3A%20as%20such%2C%20they%20can%20be%20approximated%20by%20convnets%20with%20wide%0Areceptive%20fields.%20Yet%2C%20in%20practice%2C%20gradient-based%20optimization%20leads%20to%0Asuboptimal%20approximations.%20In%20our%20article%2C%20we%20approach%20this%20phenomenon%20from%20the%0Aperspective%20of%20initialization.%20We%20present%20a%20theory%20of%20large%20deviations%20for%20the%0Aenergy%20response%20of%20FIR%20filterbanks%20with%20random%20Gaussian%20weights.%20We%20find%20that%0Adeviations%20worsen%20for%20large%20filters%20and%20locally%20periodic%20input%20signals%2C%20which%0Aare%20both%20typical%20for%20audio%20signal%20processing%20applications.%20Numerical%0Asimulations%20align%20with%20our%20theory%20and%20suggest%20that%20the%20condition%20number%20of%20a%0Aconvolutional%20layer%20follows%20a%20logarithmic%20scaling%20law%20between%20the%20number%20and%0Alength%20of%20the%20filters%2C%20which%20is%20reminiscent%20of%20discrete%20wavelet%20bases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05855v3&entry.124074799=Read"},
{"title": "Automatic re-calibration of quantum devices by reinforcement learning", "author": "T. Crosta and L. Reb\u00f3n and F. Vilari\u00f1o and J. M. Matera and M. Bilkis", "abstract": "  During their operation, due to shifts in environmental conditions, devices\nundergo various forms of detuning from their optimal settings. Typically, this\nis addressed through control loops, which monitor variables and the device\nperformance, to maintain settings at their optimal values. Quantum devices are\nparticularly challenging since their functionality relies on precisely tuning\ntheir parameters. At the same time, the detailed modeling of the environmental\nbehavior is often computationally unaffordable, while a direct measure of the\nparameters defining the system state is costly and introduces extra noise in\nthe mechanism. In this study, we investigate the application of reinforcement\nlearning techniques to develop a model-free control loop for continuous\nrecalibration of quantum device parameters. Furthermore, we explore the\nadvantages of incorporating minimal environmental noise models. As an example,\nthe application to numerical simulations of a Kennedy receiver-based\nlong-distance quantum communication protocol is presented.\n", "link": "http://arxiv.org/abs/2404.10726v1", "date": "2024-04-16", "relevancy": 1.8907, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4896}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4577}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20re-calibration%20of%20quantum%20devices%20by%20reinforcement%20learning&body=Title%3A%20Automatic%20re-calibration%20of%20quantum%20devices%20by%20reinforcement%20learning%0AAuthor%3A%20T.%20Crosta%20and%20L.%20Reb%C3%B3n%20and%20F.%20Vilari%C3%B1o%20and%20J.%20M.%20Matera%20and%20M.%20Bilkis%0AAbstract%3A%20%20%20During%20their%20operation%2C%20due%20to%20shifts%20in%20environmental%20conditions%2C%20devices%0Aundergo%20various%20forms%20of%20detuning%20from%20their%20optimal%20settings.%20Typically%2C%20this%0Ais%20addressed%20through%20control%20loops%2C%20which%20monitor%20variables%20and%20the%20device%0Aperformance%2C%20to%20maintain%20settings%20at%20their%20optimal%20values.%20Quantum%20devices%20are%0Aparticularly%20challenging%20since%20their%20functionality%20relies%20on%20precisely%20tuning%0Atheir%20parameters.%20At%20the%20same%20time%2C%20the%20detailed%20modeling%20of%20the%20environmental%0Abehavior%20is%20often%20computationally%20unaffordable%2C%20while%20a%20direct%20measure%20of%20the%0Aparameters%20defining%20the%20system%20state%20is%20costly%20and%20introduces%20extra%20noise%20in%0Athe%20mechanism.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%20reinforcement%0Alearning%20techniques%20to%20develop%20a%20model-free%20control%20loop%20for%20continuous%0Arecalibration%20of%20quantum%20device%20parameters.%20Furthermore%2C%20we%20explore%20the%0Aadvantages%20of%20incorporating%20minimal%20environmental%20noise%20models.%20As%20an%20example%2C%0Athe%20application%20to%20numerical%20simulations%20of%20a%20Kennedy%20receiver-based%0Along-distance%20quantum%20communication%20protocol%20is%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10726v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20re-calibration%20of%20quantum%20devices%20by%20reinforcement%20learning&entry.906535625=T.%20Crosta%20and%20L.%20Reb%C3%B3n%20and%20F.%20Vilari%C3%B1o%20and%20J.%20M.%20Matera%20and%20M.%20Bilkis&entry.1292438233=%20%20During%20their%20operation%2C%20due%20to%20shifts%20in%20environmental%20conditions%2C%20devices%0Aundergo%20various%20forms%20of%20detuning%20from%20their%20optimal%20settings.%20Typically%2C%20this%0Ais%20addressed%20through%20control%20loops%2C%20which%20monitor%20variables%20and%20the%20device%0Aperformance%2C%20to%20maintain%20settings%20at%20their%20optimal%20values.%20Quantum%20devices%20are%0Aparticularly%20challenging%20since%20their%20functionality%20relies%20on%20precisely%20tuning%0Atheir%20parameters.%20At%20the%20same%20time%2C%20the%20detailed%20modeling%20of%20the%20environmental%0Abehavior%20is%20often%20computationally%20unaffordable%2C%20while%20a%20direct%20measure%20of%20the%0Aparameters%20defining%20the%20system%20state%20is%20costly%20and%20introduces%20extra%20noise%20in%0Athe%20mechanism.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%20reinforcement%0Alearning%20techniques%20to%20develop%20a%20model-free%20control%20loop%20for%20continuous%0Arecalibration%20of%20quantum%20device%20parameters.%20Furthermore%2C%20we%20explore%20the%0Aadvantages%20of%20incorporating%20minimal%20environmental%20noise%20models.%20As%20an%20example%2C%0Athe%20application%20to%20numerical%20simulations%20of%20a%20Kennedy%20receiver-based%0Along-distance%20quantum%20communication%20protocol%20is%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10726v1&entry.124074799=Read"},
{"title": "Efficient Conditional Diffusion Model with Probability Flow Sampling for\n  Image Super-resolution", "author": "Yutao Yuan and Chun Yuan", "abstract": "  Image super-resolution is a fundamentally ill-posed problem because multiple\nvalid high-resolution images exist for one low-resolution image.\nSuper-resolution methods based on diffusion probabilistic models can deal with\nthe ill-posed nature by learning the distribution of high-resolution images\nconditioned on low-resolution images, avoiding the problem of blurry images in\nPSNR-oriented methods. However, existing diffusion-based super-resolution\nmethods have high time consumption with the use of iterative sampling, while\nthe quality and consistency of generated images are less than ideal due to\nproblems like color shifting. In this paper, we propose Efficient Conditional\nDiffusion Model with Probability Flow Sampling (ECDP) for image\nsuper-resolution. To reduce the time consumption, we design a continuous-time\nconditional diffusion model for image super-resolution, which enables the use\nof probability flow sampling for efficient generation. Additionally, to improve\nthe consistency of generated images, we propose a hybrid parametrization for\nthe denoiser network, which interpolates between the data-predicting\nparametrization and the noise-predicting parametrization for different noise\nscales. Moreover, we design an image quality loss as a complement to the score\nmatching loss of diffusion models, further improving the consistency and\nquality of super-resolution. Extensive experiments on DIV2K, ImageNet, and\nCelebA demonstrate that our method achieves higher super-resolution quality\nthan existing diffusion-based image super-resolution methods while having lower\ntime consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.\n", "link": "http://arxiv.org/abs/2404.10688v1", "date": "2024-04-16", "relevancy": 1.8873, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.693}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6215}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Conditional%20Diffusion%20Model%20with%20Probability%20Flow%20Sampling%20for%0A%20%20Image%20Super-resolution&body=Title%3A%20Efficient%20Conditional%20Diffusion%20Model%20with%20Probability%20Flow%20Sampling%20for%0A%20%20Image%20Super-resolution%0AAuthor%3A%20Yutao%20Yuan%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Image%20super-resolution%20is%20a%20fundamentally%20ill-posed%20problem%20because%20multiple%0Avalid%20high-resolution%20images%20exist%20for%20one%20low-resolution%20image.%0ASuper-resolution%20methods%20based%20on%20diffusion%20probabilistic%20models%20can%20deal%20with%0Athe%20ill-posed%20nature%20by%20learning%20the%20distribution%20of%20high-resolution%20images%0Aconditioned%20on%20low-resolution%20images%2C%20avoiding%20the%20problem%20of%20blurry%20images%20in%0APSNR-oriented%20methods.%20However%2C%20existing%20diffusion-based%20super-resolution%0Amethods%20have%20high%20time%20consumption%20with%20the%20use%20of%20iterative%20sampling%2C%20while%0Athe%20quality%20and%20consistency%20of%20generated%20images%20are%20less%20than%20ideal%20due%20to%0Aproblems%20like%20color%20shifting.%20In%20this%20paper%2C%20we%20propose%20Efficient%20Conditional%0ADiffusion%20Model%20with%20Probability%20Flow%20Sampling%20%28ECDP%29%20for%20image%0Asuper-resolution.%20To%20reduce%20the%20time%20consumption%2C%20we%20design%20a%20continuous-time%0Aconditional%20diffusion%20model%20for%20image%20super-resolution%2C%20which%20enables%20the%20use%0Aof%20probability%20flow%20sampling%20for%20efficient%20generation.%20Additionally%2C%20to%20improve%0Athe%20consistency%20of%20generated%20images%2C%20we%20propose%20a%20hybrid%20parametrization%20for%0Athe%20denoiser%20network%2C%20which%20interpolates%20between%20the%20data-predicting%0Aparametrization%20and%20the%20noise-predicting%20parametrization%20for%20different%20noise%0Ascales.%20Moreover%2C%20we%20design%20an%20image%20quality%20loss%20as%20a%20complement%20to%20the%20score%0Amatching%20loss%20of%20diffusion%20models%2C%20further%20improving%20the%20consistency%20and%0Aquality%20of%20super-resolution.%20Extensive%20experiments%20on%20DIV2K%2C%20ImageNet%2C%20and%0ACelebA%20demonstrate%20that%20our%20method%20achieves%20higher%20super-resolution%20quality%0Athan%20existing%20diffusion-based%20image%20super-resolution%20methods%20while%20having%20lower%0Atime%20consumption.%20Our%20code%20is%20available%20at%20https%3A//github.com/Yuan-Yutao/ECDP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10688v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Conditional%20Diffusion%20Model%20with%20Probability%20Flow%20Sampling%20for%0A%20%20Image%20Super-resolution&entry.906535625=Yutao%20Yuan%20and%20Chun%20Yuan&entry.1292438233=%20%20Image%20super-resolution%20is%20a%20fundamentally%20ill-posed%20problem%20because%20multiple%0Avalid%20high-resolution%20images%20exist%20for%20one%20low-resolution%20image.%0ASuper-resolution%20methods%20based%20on%20diffusion%20probabilistic%20models%20can%20deal%20with%0Athe%20ill-posed%20nature%20by%20learning%20the%20distribution%20of%20high-resolution%20images%0Aconditioned%20on%20low-resolution%20images%2C%20avoiding%20the%20problem%20of%20blurry%20images%20in%0APSNR-oriented%20methods.%20However%2C%20existing%20diffusion-based%20super-resolution%0Amethods%20have%20high%20time%20consumption%20with%20the%20use%20of%20iterative%20sampling%2C%20while%0Athe%20quality%20and%20consistency%20of%20generated%20images%20are%20less%20than%20ideal%20due%20to%0Aproblems%20like%20color%20shifting.%20In%20this%20paper%2C%20we%20propose%20Efficient%20Conditional%0ADiffusion%20Model%20with%20Probability%20Flow%20Sampling%20%28ECDP%29%20for%20image%0Asuper-resolution.%20To%20reduce%20the%20time%20consumption%2C%20we%20design%20a%20continuous-time%0Aconditional%20diffusion%20model%20for%20image%20super-resolution%2C%20which%20enables%20the%20use%0Aof%20probability%20flow%20sampling%20for%20efficient%20generation.%20Additionally%2C%20to%20improve%0Athe%20consistency%20of%20generated%20images%2C%20we%20propose%20a%20hybrid%20parametrization%20for%0Athe%20denoiser%20network%2C%20which%20interpolates%20between%20the%20data-predicting%0Aparametrization%20and%20the%20noise-predicting%20parametrization%20for%20different%20noise%0Ascales.%20Moreover%2C%20we%20design%20an%20image%20quality%20loss%20as%20a%20complement%20to%20the%20score%0Amatching%20loss%20of%20diffusion%20models%2C%20further%20improving%20the%20consistency%20and%0Aquality%20of%20super-resolution.%20Extensive%20experiments%20on%20DIV2K%2C%20ImageNet%2C%20and%0ACelebA%20demonstrate%20that%20our%20method%20achieves%20higher%20super-resolution%20quality%0Athan%20existing%20diffusion-based%20image%20super-resolution%20methods%20while%20having%20lower%0Atime%20consumption.%20Our%20code%20is%20available%20at%20https%3A//github.com/Yuan-Yutao/ECDP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10688v1&entry.124074799=Read"},
{"title": "Classification of Prostate Cancer in 3D Magnetic Resonance Imaging Data\n  based on Convolutional Neural Networks", "author": "Malte Rippa and Ruben Schulze and Marian Himstedt and Felice Burn", "abstract": "  Prostate cancer is a commonly diagnosed cancerous disease among men\nworld-wide. Even with modern technology such as multi-parametric magnetic\nresonance tomography and guided biopsies, the process for diagnosing prostate\ncancer remains time consuming and requires highly trained professionals. In\nthis paper, different convolutional neural networks (CNN) are evaluated on\ntheir abilities to reliably classify whether an MRI sequence contains malignant\nlesions. Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image\ndata are trained and evaluated. The models are trained using different data\naugmentation techniques, learning rates, and optimizers. The data is taken from\na private dataset, provided by Cantonal Hospital Aarau. The best result was\nachieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC\nROC score of 0.6214.\n", "link": "http://arxiv.org/abs/2404.10548v1", "date": "2024-04-16", "relevancy": 1.8805, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4765}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4711}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4517}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Prostate%20Cancer%20in%203D%20Magnetic%20Resonance%20Imaging%20Data%0A%20%20based%20on%20Convolutional%20Neural%20Networks&body=Title%3A%20Classification%20of%20Prostate%20Cancer%20in%203D%20Magnetic%20Resonance%20Imaging%20Data%0A%20%20based%20on%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Malte%20Rippa%20and%20Ruben%20Schulze%20and%20Marian%20Himstedt%20and%20Felice%20Burn%0AAbstract%3A%20%20%20Prostate%20cancer%20is%20a%20commonly%20diagnosed%20cancerous%20disease%20among%20men%0Aworld-wide.%20Even%20with%20modern%20technology%20such%20as%20multi-parametric%20magnetic%0Aresonance%20tomography%20and%20guided%20biopsies%2C%20the%20process%20for%20diagnosing%20prostate%0Acancer%20remains%20time%20consuming%20and%20requires%20highly%20trained%20professionals.%20In%0Athis%20paper%2C%20different%20convolutional%20neural%20networks%20%28CNN%29%20are%20evaluated%20on%0Atheir%20abilities%20to%20reliably%20classify%20whether%20an%20MRI%20sequence%20contains%20malignant%0Alesions.%20Implementations%20of%20a%20ResNet%2C%20a%20ConvNet%20and%20a%20ConvNeXt%20for%203D%20image%0Adata%20are%20trained%20and%20evaluated.%20The%20models%20are%20trained%20using%20different%20data%0Aaugmentation%20techniques%2C%20learning%20rates%2C%20and%20optimizers.%20The%20data%20is%20taken%20from%0Aa%20private%20dataset%2C%20provided%20by%20Cantonal%20Hospital%20Aarau.%20The%20best%20result%20was%0Aachieved%20by%20a%20ResNet3D%2C%20yielding%20an%20average%20precision%20score%20of%200.4583%20and%20AUC%0AROC%20score%20of%200.6214.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10548v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Prostate%20Cancer%20in%203D%20Magnetic%20Resonance%20Imaging%20Data%0A%20%20based%20on%20Convolutional%20Neural%20Networks&entry.906535625=Malte%20Rippa%20and%20Ruben%20Schulze%20and%20Marian%20Himstedt%20and%20Felice%20Burn&entry.1292438233=%20%20Prostate%20cancer%20is%20a%20commonly%20diagnosed%20cancerous%20disease%20among%20men%0Aworld-wide.%20Even%20with%20modern%20technology%20such%20as%20multi-parametric%20magnetic%0Aresonance%20tomography%20and%20guided%20biopsies%2C%20the%20process%20for%20diagnosing%20prostate%0Acancer%20remains%20time%20consuming%20and%20requires%20highly%20trained%20professionals.%20In%0Athis%20paper%2C%20different%20convolutional%20neural%20networks%20%28CNN%29%20are%20evaluated%20on%0Atheir%20abilities%20to%20reliably%20classify%20whether%20an%20MRI%20sequence%20contains%20malignant%0Alesions.%20Implementations%20of%20a%20ResNet%2C%20a%20ConvNet%20and%20a%20ConvNeXt%20for%203D%20image%0Adata%20are%20trained%20and%20evaluated.%20The%20models%20are%20trained%20using%20different%20data%0Aaugmentation%20techniques%2C%20learning%20rates%2C%20and%20optimizers.%20The%20data%20is%20taken%20from%0Aa%20private%20dataset%2C%20provided%20by%20Cantonal%20Hospital%20Aarau.%20The%20best%20result%20was%0Aachieved%20by%20a%20ResNet3D%2C%20yielding%20an%20average%20precision%20score%20of%200.4583%20and%20AUC%0AROC%20score%20of%200.6214.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10548v1&entry.124074799=Read"},
{"title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive\n  Counterparts for Image-to-Text Generation?", "author": "Yuchi Wang and Shuhuai Ren and Rundong Gao and Linli Yao and Qingyan Guo and Kaikai An and Jianhong Bai and Xu Sun", "abstract": "  Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.\n", "link": "http://arxiv.org/abs/2404.10763v1", "date": "2024-04-16", "relevancy": 1.8747, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6747}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaDiC%3A%20Are%20Diffusion%20Models%20Really%20Inferior%20to%20Autoregressive%0A%20%20Counterparts%20for%20Image-to-Text%20Generation%3F&body=Title%3A%20LaDiC%3A%20Are%20Diffusion%20Models%20Really%20Inferior%20to%20Autoregressive%0A%20%20Counterparts%20for%20Image-to-Text%20Generation%3F%0AAuthor%3A%20Yuchi%20Wang%20and%20Shuhuai%20Ren%20and%20Rundong%20Gao%20and%20Linli%20Yao%20and%20Qingyan%20Guo%20and%20Kaikai%20An%20and%20Jianhong%20Bai%20and%20Xu%20Sun%0AAbstract%3A%20%20%20Diffusion%20models%20have%20exhibited%20remarkable%20capabilities%20in%20text-to-image%0Ageneration.%20However%2C%20their%20performance%20in%20image-to-text%20generation%2C%0Aspecifically%20image%20captioning%2C%20has%20lagged%20behind%20Auto-Regressive%20%28AR%29%20models%2C%0Acasting%20doubt%20on%20their%20applicability%20for%20such%20tasks.%20In%20this%20work%2C%20we%20revisit%0Adiffusion%20models%2C%20highlighting%20their%20capacity%20for%20holistic%20context%20modeling%20and%0Aparallel%20decoding.%20With%20these%20benefits%2C%20diffusion%20models%20can%20alleviate%20the%0Ainherent%20limitations%20of%20AR%20methods%2C%20including%20their%20slow%20inference%20speed%2C%20error%0Apropagation%2C%20and%20unidirectional%20constraints.%20Furthermore%2C%20we%20identify%20the%20prior%0Aunderperformance%20of%20diffusion%20models%20stemming%20from%20the%20absence%20of%20an%20effective%0Alatent%20space%20for%20image-text%20alignment%2C%20and%20the%20discrepancy%20between%20continuous%0Adiffusion%20processes%20and%20discrete%20textual%20data.%20In%20response%2C%20we%20introduce%20a%0Anovel%20architecture%2C%20LaDiC%2C%20which%20utilizes%20a%20split%20BERT%20to%20create%20a%20dedicated%0Alatent%20space%20for%20captions%20and%20integrates%20a%20regularization%20module%20to%20manage%0Avarying%20text%20lengths.%20Our%20framework%20also%20includes%20a%20diffuser%20for%20semantic%0Aimage-to-text%20conversion%20and%20a%20Back%26Refine%20technique%20to%20enhance%20token%0Ainteractivity%20during%20inference.%20LaDiC%20achieves%20state-of-the-art%20performance%20for%0Adiffusion-based%20methods%20on%20the%20MS%20COCO%20dataset%20with%2038.2%20BLEU%404%20and%20126.2%0ACIDEr%2C%20demonstrating%20exceptional%20performance%20without%20pre-training%20or%20ancillary%0Amodules.%20This%20indicates%20strong%20competitiveness%20with%20AR%20models%2C%20revealing%20the%0Apreviously%20untapped%20potential%20of%20diffusion%20models%20in%20image-to-text%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10763v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaDiC%3A%20Are%20Diffusion%20Models%20Really%20Inferior%20to%20Autoregressive%0A%20%20Counterparts%20for%20Image-to-Text%20Generation%3F&entry.906535625=Yuchi%20Wang%20and%20Shuhuai%20Ren%20and%20Rundong%20Gao%20and%20Linli%20Yao%20and%20Qingyan%20Guo%20and%20Kaikai%20An%20and%20Jianhong%20Bai%20and%20Xu%20Sun&entry.1292438233=%20%20Diffusion%20models%20have%20exhibited%20remarkable%20capabilities%20in%20text-to-image%0Ageneration.%20However%2C%20their%20performance%20in%20image-to-text%20generation%2C%0Aspecifically%20image%20captioning%2C%20has%20lagged%20behind%20Auto-Regressive%20%28AR%29%20models%2C%0Acasting%20doubt%20on%20their%20applicability%20for%20such%20tasks.%20In%20this%20work%2C%20we%20revisit%0Adiffusion%20models%2C%20highlighting%20their%20capacity%20for%20holistic%20context%20modeling%20and%0Aparallel%20decoding.%20With%20these%20benefits%2C%20diffusion%20models%20can%20alleviate%20the%0Ainherent%20limitations%20of%20AR%20methods%2C%20including%20their%20slow%20inference%20speed%2C%20error%0Apropagation%2C%20and%20unidirectional%20constraints.%20Furthermore%2C%20we%20identify%20the%20prior%0Aunderperformance%20of%20diffusion%20models%20stemming%20from%20the%20absence%20of%20an%20effective%0Alatent%20space%20for%20image-text%20alignment%2C%20and%20the%20discrepancy%20between%20continuous%0Adiffusion%20processes%20and%20discrete%20textual%20data.%20In%20response%2C%20we%20introduce%20a%0Anovel%20architecture%2C%20LaDiC%2C%20which%20utilizes%20a%20split%20BERT%20to%20create%20a%20dedicated%0Alatent%20space%20for%20captions%20and%20integrates%20a%20regularization%20module%20to%20manage%0Avarying%20text%20lengths.%20Our%20framework%20also%20includes%20a%20diffuser%20for%20semantic%0Aimage-to-text%20conversion%20and%20a%20Back%26Refine%20technique%20to%20enhance%20token%0Ainteractivity%20during%20inference.%20LaDiC%20achieves%20state-of-the-art%20performance%20for%0Adiffusion-based%20methods%20on%20the%20MS%20COCO%20dataset%20with%2038.2%20BLEU%404%20and%20126.2%0ACIDEr%2C%20demonstrating%20exceptional%20performance%20without%20pre-training%20or%20ancillary%0Amodules.%20This%20indicates%20strong%20competitiveness%20with%20AR%20models%2C%20revealing%20the%0Apreviously%20untapped%20potential%20of%20diffusion%20models%20in%20image-to-text%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10763v1&entry.124074799=Read"},
{"title": "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models", "author": "Zeyu Liu and Souvik Kundu and Anni Li and Junrui Wan and Lianghao Jiang and Peter Anthony Beerel", "abstract": "  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n", "link": "http://arxiv.org/abs/2403.13269v3", "date": "2024-04-16", "relevancy": 1.8658, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4716}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AFLoRA%3A%20Adaptive%20Freezing%20of%20Low%20Rank%20Adaptation%20in%20Parameter%20Efficient%0A%20%20Fine-Tuning%20of%20Large%20Models&body=Title%3A%20AFLoRA%3A%20Adaptive%20Freezing%20of%20Low%20Rank%20Adaptation%20in%20Parameter%20Efficient%0A%20%20Fine-Tuning%20of%20Large%20Models%0AAuthor%3A%20Zeyu%20Liu%20and%20Souvik%20Kundu%20and%20Anni%20Li%20and%20Junrui%20Wan%20and%20Lianghao%20Jiang%20and%20Peter%20Anthony%20Beerel%0AAbstract%3A%20%20%20We%20present%20a%20novel%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20method%2C%20dubbed%20as%0AAdaptive%20Freezing%20of%20Low%20Rank%20Adaptation%20%28AFLoRA%29.%20Specifically%2C%20for%20each%0Apre-trained%20frozen%20weight%20tensor%2C%20we%20add%20a%20parallel%20path%20of%20trainable%20low-rank%0Amatrices%2C%20namely%20a%20down-projection%20and%20an%20up-projection%20matrix%2C%20each%20of%20which%0Ais%20followed%20by%20a%20feature%20transformation%20vector.%20Based%20on%20a%20novel%20freezing%0Ascore%2C%20we%20the%20incrementally%20freeze%20these%20projection%20matrices%20during%20fine-tuning%0Ato%20reduce%20the%20computation%20and%20alleviate%20over-fitting.%20Our%20experimental%20results%0Ademonstrate%20that%20we%20can%20achieve%20state-of-the-art%20performance%20with%20an%20average%0Aimprovement%20of%20up%20to%20%240.85%5C%25%24%20as%20evaluated%20on%20GLUE%20benchmark%20while%20yeilding%20up%0Ato%20%249.5%5Ctimes%24%20fewer%20average%20trainable%20parameters.%20While%20compared%20in%20terms%20of%0Aruntime%2C%20AFLoRA%20can%20yield%20up%20to%20%241.86%5Ctimes%24%20improvement%20as%20opposed%20to%20similar%0APEFT%20alternatives.%20Besides%20the%20practical%20utility%20of%20our%20approach%2C%20we%20provide%0Ainsights%20on%20the%20trainability%20requirements%20of%20LoRA%20paths%20at%20different%20modules%0Aand%20the%20freezing%20schedule%20for%20the%20different%20projection%20matrices.%20Code%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13269v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AFLoRA%3A%20Adaptive%20Freezing%20of%20Low%20Rank%20Adaptation%20in%20Parameter%20Efficient%0A%20%20Fine-Tuning%20of%20Large%20Models&entry.906535625=Zeyu%20Liu%20and%20Souvik%20Kundu%20and%20Anni%20Li%20and%20Junrui%20Wan%20and%20Lianghao%20Jiang%20and%20Peter%20Anthony%20Beerel&entry.1292438233=%20%20We%20present%20a%20novel%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20method%2C%20dubbed%20as%0AAdaptive%20Freezing%20of%20Low%20Rank%20Adaptation%20%28AFLoRA%29.%20Specifically%2C%20for%20each%0Apre-trained%20frozen%20weight%20tensor%2C%20we%20add%20a%20parallel%20path%20of%20trainable%20low-rank%0Amatrices%2C%20namely%20a%20down-projection%20and%20an%20up-projection%20matrix%2C%20each%20of%20which%0Ais%20followed%20by%20a%20feature%20transformation%20vector.%20Based%20on%20a%20novel%20freezing%0Ascore%2C%20we%20the%20incrementally%20freeze%20these%20projection%20matrices%20during%20fine-tuning%0Ato%20reduce%20the%20computation%20and%20alleviate%20over-fitting.%20Our%20experimental%20results%0Ademonstrate%20that%20we%20can%20achieve%20state-of-the-art%20performance%20with%20an%20average%0Aimprovement%20of%20up%20to%20%240.85%5C%25%24%20as%20evaluated%20on%20GLUE%20benchmark%20while%20yeilding%20up%0Ato%20%249.5%5Ctimes%24%20fewer%20average%20trainable%20parameters.%20While%20compared%20in%20terms%20of%0Aruntime%2C%20AFLoRA%20can%20yield%20up%20to%20%241.86%5Ctimes%24%20improvement%20as%20opposed%20to%20similar%0APEFT%20alternatives.%20Besides%20the%20practical%20utility%20of%20our%20approach%2C%20we%20provide%0Ainsights%20on%20the%20trainability%20requirements%20of%20LoRA%20paths%20at%20different%20modules%0Aand%20the%20freezing%20schedule%20for%20the%20different%20projection%20matrices.%20Code%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13269v3&entry.124074799=Read"},
{"title": "Explainable concept mappings of MRI: Revealing the mechanisms underlying\n  deep learning-based brain disease classification", "author": "Christian Tinauer and Anna Damulina and Maximilian Sackl and Martin Soellradl and Reduan Achtibat and Maximilian Dreyer and Frederik Pahde and Sebastian Lapuschkin and Reinhold Schmidt and Stefan Ropele and Wojciech Samek and Christian Langkammer", "abstract": "  Motivation. While recent studies show high accuracy in the classification of\nAlzheimer's disease using deep neural networks, the underlying learned concepts\nhave not been investigated.\n  Goals. To systematically identify changes in brain regions through concepts\nlearned by the deep neural network for model validation.\n  Approach. Using quantitative R2* maps we separated Alzheimer's patients\n(n=117) from normal controls (n=219) by using a convolutional neural network\nand systematically investigated the learned concepts using Concept Relevance\nPropagation and compared these results to a conventional region of\ninterest-based analysis.\n  Results. In line with established histological findings and the region of\ninterest-based analyses, highly relevant concepts were primarily found in and\nadjacent to the basal ganglia.\n  Impact. The identification of concepts learned by deep neural networks for\ndisease classification enables validation of the models and could potentially\nimprove reliability.\n", "link": "http://arxiv.org/abs/2404.10433v1", "date": "2024-04-16", "relevancy": 1.8646, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.452}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explainable%20concept%20mappings%20of%20MRI%3A%20Revealing%20the%20mechanisms%20underlying%0A%20%20deep%20learning-based%20brain%20disease%20classification&body=Title%3A%20Explainable%20concept%20mappings%20of%20MRI%3A%20Revealing%20the%20mechanisms%20underlying%0A%20%20deep%20learning-based%20brain%20disease%20classification%0AAuthor%3A%20Christian%20Tinauer%20and%20Anna%20Damulina%20and%20Maximilian%20Sackl%20and%20Martin%20Soellradl%20and%20Reduan%20Achtibat%20and%20Maximilian%20Dreyer%20and%20Frederik%20Pahde%20and%20Sebastian%20Lapuschkin%20and%20Reinhold%20Schmidt%20and%20Stefan%20Ropele%20and%20Wojciech%20Samek%20and%20Christian%20Langkammer%0AAbstract%3A%20%20%20Motivation.%20While%20recent%20studies%20show%20high%20accuracy%20in%20the%20classification%20of%0AAlzheimer%27s%20disease%20using%20deep%20neural%20networks%2C%20the%20underlying%20learned%20concepts%0Ahave%20not%20been%20investigated.%0A%20%20Goals.%20To%20systematically%20identify%20changes%20in%20brain%20regions%20through%20concepts%0Alearned%20by%20the%20deep%20neural%20network%20for%20model%20validation.%0A%20%20Approach.%20Using%20quantitative%20R2%2A%20maps%20we%20separated%20Alzheimer%27s%20patients%0A%28n%3D117%29%20from%20normal%20controls%20%28n%3D219%29%20by%20using%20a%20convolutional%20neural%20network%0Aand%20systematically%20investigated%20the%20learned%20concepts%20using%20Concept%20Relevance%0APropagation%20and%20compared%20these%20results%20to%20a%20conventional%20region%20of%0Ainterest-based%20analysis.%0A%20%20Results.%20In%20line%20with%20established%20histological%20findings%20and%20the%20region%20of%0Ainterest-based%20analyses%2C%20highly%20relevant%20concepts%20were%20primarily%20found%20in%20and%0Aadjacent%20to%20the%20basal%20ganglia.%0A%20%20Impact.%20The%20identification%20of%20concepts%20learned%20by%20deep%20neural%20networks%20for%0Adisease%20classification%20enables%20validation%20of%20the%20models%20and%20could%20potentially%0Aimprove%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10433v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20concept%20mappings%20of%20MRI%3A%20Revealing%20the%20mechanisms%20underlying%0A%20%20deep%20learning-based%20brain%20disease%20classification&entry.906535625=Christian%20Tinauer%20and%20Anna%20Damulina%20and%20Maximilian%20Sackl%20and%20Martin%20Soellradl%20and%20Reduan%20Achtibat%20and%20Maximilian%20Dreyer%20and%20Frederik%20Pahde%20and%20Sebastian%20Lapuschkin%20and%20Reinhold%20Schmidt%20and%20Stefan%20Ropele%20and%20Wojciech%20Samek%20and%20Christian%20Langkammer&entry.1292438233=%20%20Motivation.%20While%20recent%20studies%20show%20high%20accuracy%20in%20the%20classification%20of%0AAlzheimer%27s%20disease%20using%20deep%20neural%20networks%2C%20the%20underlying%20learned%20concepts%0Ahave%20not%20been%20investigated.%0A%20%20Goals.%20To%20systematically%20identify%20changes%20in%20brain%20regions%20through%20concepts%0Alearned%20by%20the%20deep%20neural%20network%20for%20model%20validation.%0A%20%20Approach.%20Using%20quantitative%20R2%2A%20maps%20we%20separated%20Alzheimer%27s%20patients%0A%28n%3D117%29%20from%20normal%20controls%20%28n%3D219%29%20by%20using%20a%20convolutional%20neural%20network%0Aand%20systematically%20investigated%20the%20learned%20concepts%20using%20Concept%20Relevance%0APropagation%20and%20compared%20these%20results%20to%20a%20conventional%20region%20of%0Ainterest-based%20analysis.%0A%20%20Results.%20In%20line%20with%20established%20histological%20findings%20and%20the%20region%20of%0Ainterest-based%20analyses%2C%20highly%20relevant%20concepts%20were%20primarily%20found%20in%20and%0Aadjacent%20to%20the%20basal%20ganglia.%0A%20%20Impact.%20The%20identification%20of%20concepts%20learned%20by%20deep%20neural%20networks%20for%0Adisease%20classification%20enables%20validation%20of%20the%20models%20and%20could%20potentially%0Aimprove%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10433v1&entry.124074799=Read"},
{"title": "Dataset Reset Policy Optimization for RLHF", "author": "Jonathan D. Chang and Wenhao Zhan and Owen Oertell and Kiant\u00e9 Brantley and Dipendra Misra and Jason D. Lee and Wen Sun", "abstract": "  Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.\n", "link": "http://arxiv.org/abs/2404.08495v3", "date": "2024-04-16", "relevancy": 1.8644, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dataset%20Reset%20Policy%20Optimization%20for%20RLHF&body=Title%3A%20Dataset%20Reset%20Policy%20Optimization%20for%20RLHF%0AAuthor%3A%20Jonathan%20D.%20Chang%20and%20Wenhao%20Zhan%20and%20Owen%20Oertell%20and%20Kiant%C3%A9%20Brantley%20and%20Dipendra%20Misra%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20from%20Human%20Preference-based%20feedback%20is%20a%20popular%0Aparadigm%20for%20fine-tuning%20generative%20models%2C%20which%20has%20produced%20impressive%0Amodels%20such%20as%20GPT-4%20and%20Claude3%20Opus.%20This%20framework%20often%20consists%20of%20two%0Asteps%3A%20learning%20a%20reward%20model%20from%20an%20offline%20preference%20dataset%20followed%20by%0Arunning%20online%20RL%20to%20optimize%20the%20learned%20reward%20model.%20In%20this%20work%2C%0Aleveraging%20the%20idea%20of%20reset%2C%20we%20propose%20a%20new%20RLHF%20algorithm%20with%20provable%0Aguarantees.%20Motivated%20by%20the%20fact%20that%20offline%20preference%20dataset%20provides%0Ainformative%20states%20%28i.e.%2C%20data%20that%20is%20preferred%20by%20the%20labelers%29%2C%20our%20new%0Aalgorithm%2C%20Dataset%20Reset%20Policy%20Optimization%20%28DR-PO%29%2C%20integrates%20the%20existing%0Aoffline%20preference%20dataset%20into%20the%20online%20policy%20training%20procedure%20via%0Adataset%20reset%3A%20it%20directly%20resets%20the%20policy%20optimizer%20to%20the%20states%20in%20the%0Aoffline%20dataset%2C%20instead%20of%20always%20starting%20from%20the%20initial%20state%0Adistribution.%20In%20theory%2C%20we%20show%20that%20DR-PO%20learns%20to%20perform%20at%20least%20as%20good%0Aas%20any%20policy%20that%20is%20covered%20by%20the%20offline%20dataset%20under%20general%20function%0Aapproximation%20with%20finite%20sample%20complexity.%20In%20experiments%2C%20we%20demonstrate%0Athat%20on%20both%20the%20TL%3BDR%20summarization%20and%20the%20Anthropic%20Helpful%20Harmful%20%28HH%29%0Adataset%2C%20the%20generation%20from%20DR-PO%20is%20better%20than%20that%20from%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Direction%20Preference%20Optimization%20%28DPO%29%2C%20under%20the%0Ametric%20of%20GPT4%20win-rate.%20Code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/Cornell-RL/drpo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08495v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Reset%20Policy%20Optimization%20for%20RLHF&entry.906535625=Jonathan%20D.%20Chang%20and%20Wenhao%20Zhan%20and%20Owen%20Oertell%20and%20Kiant%C3%A9%20Brantley%20and%20Dipendra%20Misra%20and%20Jason%20D.%20Lee%20and%20Wen%20Sun&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20from%20Human%20Preference-based%20feedback%20is%20a%20popular%0Aparadigm%20for%20fine-tuning%20generative%20models%2C%20which%20has%20produced%20impressive%0Amodels%20such%20as%20GPT-4%20and%20Claude3%20Opus.%20This%20framework%20often%20consists%20of%20two%0Asteps%3A%20learning%20a%20reward%20model%20from%20an%20offline%20preference%20dataset%20followed%20by%0Arunning%20online%20RL%20to%20optimize%20the%20learned%20reward%20model.%20In%20this%20work%2C%0Aleveraging%20the%20idea%20of%20reset%2C%20we%20propose%20a%20new%20RLHF%20algorithm%20with%20provable%0Aguarantees.%20Motivated%20by%20the%20fact%20that%20offline%20preference%20dataset%20provides%0Ainformative%20states%20%28i.e.%2C%20data%20that%20is%20preferred%20by%20the%20labelers%29%2C%20our%20new%0Aalgorithm%2C%20Dataset%20Reset%20Policy%20Optimization%20%28DR-PO%29%2C%20integrates%20the%20existing%0Aoffline%20preference%20dataset%20into%20the%20online%20policy%20training%20procedure%20via%0Adataset%20reset%3A%20it%20directly%20resets%20the%20policy%20optimizer%20to%20the%20states%20in%20the%0Aoffline%20dataset%2C%20instead%20of%20always%20starting%20from%20the%20initial%20state%0Adistribution.%20In%20theory%2C%20we%20show%20that%20DR-PO%20learns%20to%20perform%20at%20least%20as%20good%0Aas%20any%20policy%20that%20is%20covered%20by%20the%20offline%20dataset%20under%20general%20function%0Aapproximation%20with%20finite%20sample%20complexity.%20In%20experiments%2C%20we%20demonstrate%0Athat%20on%20both%20the%20TL%3BDR%20summarization%20and%20the%20Anthropic%20Helpful%20Harmful%20%28HH%29%0Adataset%2C%20the%20generation%20from%20DR-PO%20is%20better%20than%20that%20from%20Proximal%20Policy%0AOptimization%20%28PPO%29%20and%20Direction%20Preference%20Optimization%20%28DPO%29%2C%20under%20the%0Ametric%20of%20GPT4%20win-rate.%20Code%20for%20this%20work%20can%20be%20found%20at%0Ahttps%3A//github.com/Cornell-RL/drpo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08495v3&entry.124074799=Read"},
{"title": "Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in\n  Unbounded Scenes", "author": "Zehao Yu and Torsten Sattler and Andreas Geiger", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view\nsynthesis results, while allowing the rendering of high-resolution images in\nreal-time. However, leveraging 3D Gaussians for surface reconstruction poses\nsignificant challenges due to the explicit and disconnected nature of 3D\nGaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel\napproach for efficient, high-quality, and compact surface reconstruction in\nunbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of\n3D Gaussians, enabling direct geometry extraction from 3D Gaussians by\nidentifying its levelset, without resorting to Poisson reconstruction or TSDF\nfusion as in previous work. We approximate the surface normal of Gaussians as\nthe normal of the ray-Gaussian intersection plane, enabling the application of\nregularization that significantly enhances geometry. Furthermore, we develop an\nefficient geometry extraction method utilizing marching tetrahedra, where the\ntetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's\ncomplexity. Our evaluations reveal that GOF surpasses existing 3DGS-based\nmethods in surface reconstruction and novel view synthesis. Further, it\ncompares favorably to, or even outperforms, neural implicit methods in both\nquality and speed.\n", "link": "http://arxiv.org/abs/2404.10772v1", "date": "2024-04-16", "relevancy": 1.8637, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4811}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4641}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Opacity%20Fields%3A%20Efficient%20and%20Compact%20Surface%20Reconstruction%20in%0A%20%20Unbounded%20Scenes&body=Title%3A%20Gaussian%20Opacity%20Fields%3A%20Efficient%20and%20Compact%20Surface%20Reconstruction%20in%0A%20%20Unbounded%20Scenes%0AAuthor%3A%20Zehao%20Yu%20and%20Torsten%20Sattler%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20novel%20view%0Asynthesis%20results%2C%20while%20allowing%20the%20rendering%20of%20high-resolution%20images%20in%0Areal-time.%20However%2C%20leveraging%203D%20Gaussians%20for%20surface%20reconstruction%20poses%0Asignificant%20challenges%20due%20to%20the%20explicit%20and%20disconnected%20nature%20of%203D%0AGaussians.%20In%20this%20work%2C%20we%20present%20Gaussian%20Opacity%20Fields%20%28GOF%29%2C%20a%20novel%0Aapproach%20for%20efficient%2C%20high-quality%2C%20and%20compact%20surface%20reconstruction%20in%0Aunbounded%20scenes.%20Our%20GOF%20is%20derived%20from%20ray-tracing-based%20volume%20rendering%20of%0A3D%20Gaussians%2C%20enabling%20direct%20geometry%20extraction%20from%203D%20Gaussians%20by%0Aidentifying%20its%20levelset%2C%20without%20resorting%20to%20Poisson%20reconstruction%20or%20TSDF%0Afusion%20as%20in%20previous%20work.%20We%20approximate%20the%20surface%20normal%20of%20Gaussians%20as%0Athe%20normal%20of%20the%20ray-Gaussian%20intersection%20plane%2C%20enabling%20the%20application%20of%0Aregularization%20that%20significantly%20enhances%20geometry.%20Furthermore%2C%20we%20develop%20an%0Aefficient%20geometry%20extraction%20method%20utilizing%20marching%20tetrahedra%2C%20where%20the%0Atetrahedral%20grids%20are%20induced%20from%203D%20Gaussians%20and%20thus%20adapt%20to%20the%20scene%27s%0Acomplexity.%20Our%20evaluations%20reveal%20that%20GOF%20surpasses%20existing%203DGS-based%0Amethods%20in%20surface%20reconstruction%20and%20novel%20view%20synthesis.%20Further%2C%20it%0Acompares%20favorably%20to%2C%20or%20even%20outperforms%2C%20neural%20implicit%20methods%20in%20both%0Aquality%20and%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10772v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Opacity%20Fields%3A%20Efficient%20and%20Compact%20Surface%20Reconstruction%20in%0A%20%20Unbounded%20Scenes&entry.906535625=Zehao%20Yu%20and%20Torsten%20Sattler%20and%20Andreas%20Geiger&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20novel%20view%0Asynthesis%20results%2C%20while%20allowing%20the%20rendering%20of%20high-resolution%20images%20in%0Areal-time.%20However%2C%20leveraging%203D%20Gaussians%20for%20surface%20reconstruction%20poses%0Asignificant%20challenges%20due%20to%20the%20explicit%20and%20disconnected%20nature%20of%203D%0AGaussians.%20In%20this%20work%2C%20we%20present%20Gaussian%20Opacity%20Fields%20%28GOF%29%2C%20a%20novel%0Aapproach%20for%20efficient%2C%20high-quality%2C%20and%20compact%20surface%20reconstruction%20in%0Aunbounded%20scenes.%20Our%20GOF%20is%20derived%20from%20ray-tracing-based%20volume%20rendering%20of%0A3D%20Gaussians%2C%20enabling%20direct%20geometry%20extraction%20from%203D%20Gaussians%20by%0Aidentifying%20its%20levelset%2C%20without%20resorting%20to%20Poisson%20reconstruction%20or%20TSDF%0Afusion%20as%20in%20previous%20work.%20We%20approximate%20the%20surface%20normal%20of%20Gaussians%20as%0Athe%20normal%20of%20the%20ray-Gaussian%20intersection%20plane%2C%20enabling%20the%20application%20of%0Aregularization%20that%20significantly%20enhances%20geometry.%20Furthermore%2C%20we%20develop%20an%0Aefficient%20geometry%20extraction%20method%20utilizing%20marching%20tetrahedra%2C%20where%20the%0Atetrahedral%20grids%20are%20induced%20from%203D%20Gaussians%20and%20thus%20adapt%20to%20the%20scene%27s%0Acomplexity.%20Our%20evaluations%20reveal%20that%20GOF%20surpasses%20existing%203DGS-based%0Amethods%20in%20surface%20reconstruction%20and%20novel%20view%20synthesis.%20Further%2C%20it%0Acompares%20favorably%20to%2C%20or%20even%20outperforms%2C%20neural%20implicit%20methods%20in%20both%0Aquality%20and%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10772v1&entry.124074799=Read"},
{"title": "RetICL: Sequential Retrieval of In-Context Examples with Reinforcement\n  Learning", "author": "Alexander Scarlatos and Andrew Lan", "abstract": "  Recent developments in large pre-trained language models have enabled\nunprecedented performance on a variety of downstream tasks. Achieving best\nperformance with these models often leverages in-context learning, where a\nmodel performs a (possibly new) task given one or more examples. However,\nrecent work has shown that the choice of examples can have a large impact on\ntask performance and that finding an optimal set of examples is non-trivial.\nWhile there are many existing methods for selecting in-context examples, they\ngenerally score examples independently, ignoring the dependency between them\nand the order in which they are provided to the model. In this work, we propose\nRetrieval for In-Context Learning (RetICL), a learnable method for modeling and\noptimally selecting examples sequentially for in-context learning. We frame the\nproblem of sequential example selection as a Markov decision process and train\nan example retriever using reinforcement learning. We evaluate RetICL on math\nword problem solving and scientific question answering tasks and show that it\nconsistently outperforms or matches heuristic and learnable baselines. We also\nuse case studies to show that RetICL implicitly learns representations of\nproblem solving strategies.\n", "link": "http://arxiv.org/abs/2305.14502v2", "date": "2024-04-16", "relevancy": 1.8477, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4806}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RetICL%3A%20Sequential%20Retrieval%20of%20In-Context%20Examples%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20RetICL%3A%20Sequential%20Retrieval%20of%20In-Context%20Examples%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Alexander%20Scarlatos%20and%20Andrew%20Lan%0AAbstract%3A%20%20%20Recent%20developments%20in%20large%20pre-trained%20language%20models%20have%20enabled%0Aunprecedented%20performance%20on%20a%20variety%20of%20downstream%20tasks.%20Achieving%20best%0Aperformance%20with%20these%20models%20often%20leverages%20in-context%20learning%2C%20where%20a%0Amodel%20performs%20a%20%28possibly%20new%29%20task%20given%20one%20or%20more%20examples.%20However%2C%0Arecent%20work%20has%20shown%20that%20the%20choice%20of%20examples%20can%20have%20a%20large%20impact%20on%0Atask%20performance%20and%20that%20finding%20an%20optimal%20set%20of%20examples%20is%20non-trivial.%0AWhile%20there%20are%20many%20existing%20methods%20for%20selecting%20in-context%20examples%2C%20they%0Agenerally%20score%20examples%20independently%2C%20ignoring%20the%20dependency%20between%20them%0Aand%20the%20order%20in%20which%20they%20are%20provided%20to%20the%20model.%20In%20this%20work%2C%20we%20propose%0ARetrieval%20for%20In-Context%20Learning%20%28RetICL%29%2C%20a%20learnable%20method%20for%20modeling%20and%0Aoptimally%20selecting%20examples%20sequentially%20for%20in-context%20learning.%20We%20frame%20the%0Aproblem%20of%20sequential%20example%20selection%20as%20a%20Markov%20decision%20process%20and%20train%0Aan%20example%20retriever%20using%20reinforcement%20learning.%20We%20evaluate%20RetICL%20on%20math%0Aword%20problem%20solving%20and%20scientific%20question%20answering%20tasks%20and%20show%20that%20it%0Aconsistently%20outperforms%20or%20matches%20heuristic%20and%20learnable%20baselines.%20We%20also%0Ause%20case%20studies%20to%20show%20that%20RetICL%20implicitly%20learns%20representations%20of%0Aproblem%20solving%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14502v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RetICL%3A%20Sequential%20Retrieval%20of%20In-Context%20Examples%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Alexander%20Scarlatos%20and%20Andrew%20Lan&entry.1292438233=%20%20Recent%20developments%20in%20large%20pre-trained%20language%20models%20have%20enabled%0Aunprecedented%20performance%20on%20a%20variety%20of%20downstream%20tasks.%20Achieving%20best%0Aperformance%20with%20these%20models%20often%20leverages%20in-context%20learning%2C%20where%20a%0Amodel%20performs%20a%20%28possibly%20new%29%20task%20given%20one%20or%20more%20examples.%20However%2C%0Arecent%20work%20has%20shown%20that%20the%20choice%20of%20examples%20can%20have%20a%20large%20impact%20on%0Atask%20performance%20and%20that%20finding%20an%20optimal%20set%20of%20examples%20is%20non-trivial.%0AWhile%20there%20are%20many%20existing%20methods%20for%20selecting%20in-context%20examples%2C%20they%0Agenerally%20score%20examples%20independently%2C%20ignoring%20the%20dependency%20between%20them%0Aand%20the%20order%20in%20which%20they%20are%20provided%20to%20the%20model.%20In%20this%20work%2C%20we%20propose%0ARetrieval%20for%20In-Context%20Learning%20%28RetICL%29%2C%20a%20learnable%20method%20for%20modeling%20and%0Aoptimally%20selecting%20examples%20sequentially%20for%20in-context%20learning.%20We%20frame%20the%0Aproblem%20of%20sequential%20example%20selection%20as%20a%20Markov%20decision%20process%20and%20train%0Aan%20example%20retriever%20using%20reinforcement%20learning.%20We%20evaluate%20RetICL%20on%20math%0Aword%20problem%20solving%20and%20scientific%20question%20answering%20tasks%20and%20show%20that%20it%0Aconsistently%20outperforms%20or%20matches%20heuristic%20and%20learnable%20baselines.%20We%20also%0Ause%20case%20studies%20to%20show%20that%20RetICL%20implicitly%20learns%20representations%20of%0Aproblem%20solving%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14502v2&entry.124074799=Read"},
{"title": "Intra-operative tumour margin evaluation in breast-conserving surgery\n  with deep learning", "author": "Wei-Chung Shia and Yu-Len Huang and Yi-Chun Chen and Hwa-Koon Wu and Dar-Ren Chen", "abstract": "  A positive margin may result in an increased risk of local recurrences after\nbreast retention surgery for any malignant tumour. In order to reduce the\nnumber of positive margins would offer surgeon real-time intra-operative\ninformation on the presence of positive resection margins. This study aims to\ndesign an intra-operative tumour margin evaluation scheme by using specimen\nmammography in breast-conserving surgery. Total of 30 cases were evaluated and\ncompared with the manually determined contours by experienced physicians and\npathology report. The proposed method utilizes image thresholding to extract\nregions of interest and then performs a deep learning model, i.e. SegNet, to\nsegment tumour tissue. The margin width of normal tissues surrounding it is\nevaluated as the result. The desired size of margin around the tumor was set\nfor 10 mm. The smallest average difference to manual sketched margin (6.53 mm\n+- 5.84). In the all case, the SegNet architecture was utilized to obtain\ntissue specimen boundary and tumor contour, respectively. The simulation\nresults indicated that this technology is helpful in discriminating positive\nfrom negative margins in the intra-operative setting. The aim of proposed\nscheme was a potential procedure in the intra-operative measurement system. The\nexperimental results reveal that deep learning techniques can draw results that\nare consistent with pathology reports.\n", "link": "http://arxiv.org/abs/2404.10600v1", "date": "2024-04-16", "relevancy": 1.8461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4512}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intra-operative%20tumour%20margin%20evaluation%20in%20breast-conserving%20surgery%0A%20%20with%20deep%20learning&body=Title%3A%20Intra-operative%20tumour%20margin%20evaluation%20in%20breast-conserving%20surgery%0A%20%20with%20deep%20learning%0AAuthor%3A%20Wei-Chung%20Shia%20and%20Yu-Len%20Huang%20and%20Yi-Chun%20Chen%20and%20Hwa-Koon%20Wu%20and%20Dar-Ren%20Chen%0AAbstract%3A%20%20%20A%20positive%20margin%20may%20result%20in%20an%20increased%20risk%20of%20local%20recurrences%20after%0Abreast%20retention%20surgery%20for%20any%20malignant%20tumour.%20In%20order%20to%20reduce%20the%0Anumber%20of%20positive%20margins%20would%20offer%20surgeon%20real-time%20intra-operative%0Ainformation%20on%20the%20presence%20of%20positive%20resection%20margins.%20This%20study%20aims%20to%0Adesign%20an%20intra-operative%20tumour%20margin%20evaluation%20scheme%20by%20using%20specimen%0Amammography%20in%20breast-conserving%20surgery.%20Total%20of%2030%20cases%20were%20evaluated%20and%0Acompared%20with%20the%20manually%20determined%20contours%20by%20experienced%20physicians%20and%0Apathology%20report.%20The%20proposed%20method%20utilizes%20image%20thresholding%20to%20extract%0Aregions%20of%20interest%20and%20then%20performs%20a%20deep%20learning%20model%2C%20i.e.%20SegNet%2C%20to%0Asegment%20tumour%20tissue.%20The%20margin%20width%20of%20normal%20tissues%20surrounding%20it%20is%0Aevaluated%20as%20the%20result.%20The%20desired%20size%20of%20margin%20around%20the%20tumor%20was%20set%0Afor%2010%20mm.%20The%20smallest%20average%20difference%20to%20manual%20sketched%20margin%20%286.53%20mm%0A%2B-%205.84%29.%20In%20the%20all%20case%2C%20the%20SegNet%20architecture%20was%20utilized%20to%20obtain%0Atissue%20specimen%20boundary%20and%20tumor%20contour%2C%20respectively.%20The%20simulation%0Aresults%20indicated%20that%20this%20technology%20is%20helpful%20in%20discriminating%20positive%0Afrom%20negative%20margins%20in%20the%20intra-operative%20setting.%20The%20aim%20of%20proposed%0Ascheme%20was%20a%20potential%20procedure%20in%20the%20intra-operative%20measurement%20system.%20The%0Aexperimental%20results%20reveal%20that%20deep%20learning%20techniques%20can%20draw%20results%20that%0Aare%20consistent%20with%20pathology%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intra-operative%20tumour%20margin%20evaluation%20in%20breast-conserving%20surgery%0A%20%20with%20deep%20learning&entry.906535625=Wei-Chung%20Shia%20and%20Yu-Len%20Huang%20and%20Yi-Chun%20Chen%20and%20Hwa-Koon%20Wu%20and%20Dar-Ren%20Chen&entry.1292438233=%20%20A%20positive%20margin%20may%20result%20in%20an%20increased%20risk%20of%20local%20recurrences%20after%0Abreast%20retention%20surgery%20for%20any%20malignant%20tumour.%20In%20order%20to%20reduce%20the%0Anumber%20of%20positive%20margins%20would%20offer%20surgeon%20real-time%20intra-operative%0Ainformation%20on%20the%20presence%20of%20positive%20resection%20margins.%20This%20study%20aims%20to%0Adesign%20an%20intra-operative%20tumour%20margin%20evaluation%20scheme%20by%20using%20specimen%0Amammography%20in%20breast-conserving%20surgery.%20Total%20of%2030%20cases%20were%20evaluated%20and%0Acompared%20with%20the%20manually%20determined%20contours%20by%20experienced%20physicians%20and%0Apathology%20report.%20The%20proposed%20method%20utilizes%20image%20thresholding%20to%20extract%0Aregions%20of%20interest%20and%20then%20performs%20a%20deep%20learning%20model%2C%20i.e.%20SegNet%2C%20to%0Asegment%20tumour%20tissue.%20The%20margin%20width%20of%20normal%20tissues%20surrounding%20it%20is%0Aevaluated%20as%20the%20result.%20The%20desired%20size%20of%20margin%20around%20the%20tumor%20was%20set%0Afor%2010%20mm.%20The%20smallest%20average%20difference%20to%20manual%20sketched%20margin%20%286.53%20mm%0A%2B-%205.84%29.%20In%20the%20all%20case%2C%20the%20SegNet%20architecture%20was%20utilized%20to%20obtain%0Atissue%20specimen%20boundary%20and%20tumor%20contour%2C%20respectively.%20The%20simulation%0Aresults%20indicated%20that%20this%20technology%20is%20helpful%20in%20discriminating%20positive%0Afrom%20negative%20margins%20in%20the%20intra-operative%20setting.%20The%20aim%20of%20proposed%0Ascheme%20was%20a%20potential%20procedure%20in%20the%20intra-operative%20measurement%20system.%20The%0Aexperimental%20results%20reveal%20that%20deep%20learning%20techniques%20can%20draw%20results%20that%0Aare%20consistent%20with%20pathology%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10600v1&entry.124074799=Read"},
{"title": "Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC\n  Challenge", "author": "Chia-Yen Lee and Hsiang-Chin Chien and Ching-Ping Wang and Hong Yen and Kai-Wen Zhen and Hong-Kun Lin", "abstract": "  Colorectal cancer is one of the most common cancers worldwide, so early\npathological examination is very important. However, it is time-consuming and\nlabor-intensive to identify the number and type of cells on H&E images in\nclinical. Therefore, automatic segmentation and classification task and\ncounting the cellular composition of H&E images from pathological sections is\nproposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer\nwith HTC for this challenge, and also applied the known normalization methods\nto generate more augmentation data. Finally, our strategy showed that the\nmulti-scale played a crucial role to identify different scale features and the\naugmentation arose the recognition of model.\n", "link": "http://arxiv.org/abs/2202.13588v3", "date": "2024-04-16", "relevancy": 1.8374, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4754}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4693}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.443}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Multi-scale%20SwinTransformer-HTC%20with%20Data%20augmentation%20in%20CoNIC%0A%20%20Challenge&body=Title%3A%20Using%20Multi-scale%20SwinTransformer-HTC%20with%20Data%20augmentation%20in%20CoNIC%0A%20%20Challenge%0AAuthor%3A%20Chia-Yen%20Lee%20and%20Hsiang-Chin%20Chien%20and%20Ching-Ping%20Wang%20and%20Hong%20Yen%20and%20Kai-Wen%20Zhen%20and%20Hong-Kun%20Lin%0AAbstract%3A%20%20%20Colorectal%20cancer%20is%20one%20of%20the%20most%20common%20cancers%20worldwide%2C%20so%20early%0Apathological%20examination%20is%20very%20important.%20However%2C%20it%20is%20time-consuming%20and%0Alabor-intensive%20to%20identify%20the%20number%20and%20type%20of%20cells%20on%20H%26E%20images%20in%0Aclinical.%20Therefore%2C%20automatic%20segmentation%20and%20classification%20task%20and%0Acounting%20the%20cellular%20composition%20of%20H%26E%20images%20from%20pathological%20sections%20is%0Aproposed%20by%20CoNIC%20Challenge%202022.%20We%20proposed%20a%20multi-scale%20Swin%20transformer%0Awith%20HTC%20for%20this%20challenge%2C%20and%20also%20applied%20the%20known%20normalization%20methods%0Ato%20generate%20more%20augmentation%20data.%20Finally%2C%20our%20strategy%20showed%20that%20the%0Amulti-scale%20played%20a%20crucial%20role%20to%20identify%20different%20scale%20features%20and%20the%0Aaugmentation%20arose%20the%20recognition%20of%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.13588v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Multi-scale%20SwinTransformer-HTC%20with%20Data%20augmentation%20in%20CoNIC%0A%20%20Challenge&entry.906535625=Chia-Yen%20Lee%20and%20Hsiang-Chin%20Chien%20and%20Ching-Ping%20Wang%20and%20Hong%20Yen%20and%20Kai-Wen%20Zhen%20and%20Hong-Kun%20Lin&entry.1292438233=%20%20Colorectal%20cancer%20is%20one%20of%20the%20most%20common%20cancers%20worldwide%2C%20so%20early%0Apathological%20examination%20is%20very%20important.%20However%2C%20it%20is%20time-consuming%20and%0Alabor-intensive%20to%20identify%20the%20number%20and%20type%20of%20cells%20on%20H%26E%20images%20in%0Aclinical.%20Therefore%2C%20automatic%20segmentation%20and%20classification%20task%20and%0Acounting%20the%20cellular%20composition%20of%20H%26E%20images%20from%20pathological%20sections%20is%0Aproposed%20by%20CoNIC%20Challenge%202022.%20We%20proposed%20a%20multi-scale%20Swin%20transformer%0Awith%20HTC%20for%20this%20challenge%2C%20and%20also%20applied%20the%20known%20normalization%20methods%0Ato%20generate%20more%20augmentation%20data.%20Finally%2C%20our%20strategy%20showed%20that%20the%0Amulti-scale%20played%20a%20crucial%20role%20to%20identify%20different%20scale%20features%20and%20the%0Aaugmentation%20arose%20the%20recognition%20of%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.13588v3&entry.124074799=Read"},
{"title": "Settling Constant Regrets in Linear Markov Decision Processes", "author": "Weitong Zhang and Zhiyuan Fan and Jiafan He and Quanquan Gu", "abstract": "  We study the constant regret guarantees in reinforcement learning (RL). Our\nobjective is to design an algorithm that incurs only finite regret over\ninfinite episodes with high probability. We introduce an algorithm,\nCert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where\nboth the transition kernel and the reward function can be approximated by some\nlinear function up to misspecification level $\\zeta$. At the core of\nCert-LSVI-UCB is an innovative certified estimator, which facilitates a\nfine-grained concentration analysis for multi-phase value-targeted regression,\nenabling us to establish an instance-dependent regret bound that is constant\nw.r.t. the number of episodes. Specifically, we demonstrate that for an MDP\ncharacterized by a minimal suboptimality gap $\\Delta$, Cert-LSVI-UCB has a\ncumulative regret of $\\tilde{\\mathcal{O}}(d^3H^5/\\Delta)$ with high\nprobability, provided that the misspecification level $\\zeta$ is below\n$\\tilde{\\mathcal{O}}(\\Delta / (\\sqrt{d}H^2))$. Remarkably, this regret bound\nremains constant relative to the number of episodes $K$. To the best of our\nknowledge, Cert-LSVI-UCB is the first algorithm to achieve a constant,\ninstance-dependent, high-probability regret bound in RL with linear function\napproximation for infinite runs without relying on prior distribution\nassumptions. This not only highlights the robustness of Cert-LSVI-UCB to model\nmisspecification but also introduces novel algorithmic designs and analytical\ntechniques of independent interest.\n", "link": "http://arxiv.org/abs/2404.10745v1", "date": "2024-04-16", "relevancy": 1.8367, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Settling%20Constant%20Regrets%20in%20Linear%20Markov%20Decision%20Processes&body=Title%3A%20Settling%20Constant%20Regrets%20in%20Linear%20Markov%20Decision%20Processes%0AAuthor%3A%20Weitong%20Zhang%20and%20Zhiyuan%20Fan%20and%20Jiafan%20He%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20We%20study%20the%20constant%20regret%20guarantees%20in%20reinforcement%20learning%20%28RL%29.%20Our%0Aobjective%20is%20to%20design%20an%20algorithm%20that%20incurs%20only%20finite%20regret%20over%0Ainfinite%20episodes%20with%20high%20probability.%20We%20introduce%20an%20algorithm%2C%0ACert-LSVI-UCB%2C%20for%20misspecified%20linear%20Markov%20decision%20processes%20%28MDPs%29%20where%0Aboth%20the%20transition%20kernel%20and%20the%20reward%20function%20can%20be%20approximated%20by%20some%0Alinear%20function%20up%20to%20misspecification%20level%20%24%5Czeta%24.%20At%20the%20core%20of%0ACert-LSVI-UCB%20is%20an%20innovative%20certified%20estimator%2C%20which%20facilitates%20a%0Afine-grained%20concentration%20analysis%20for%20multi-phase%20value-targeted%20regression%2C%0Aenabling%20us%20to%20establish%20an%20instance-dependent%20regret%20bound%20that%20is%20constant%0Aw.r.t.%20the%20number%20of%20episodes.%20Specifically%2C%20we%20demonstrate%20that%20for%20an%20MDP%0Acharacterized%20by%20a%20minimal%20suboptimality%20gap%20%24%5CDelta%24%2C%20Cert-LSVI-UCB%20has%20a%0Acumulative%20regret%20of%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E3H%5E5/%5CDelta%29%24%20with%20high%0Aprobability%2C%20provided%20that%20the%20misspecification%20level%20%24%5Czeta%24%20is%20below%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5CDelta%20/%20%28%5Csqrt%7Bd%7DH%5E2%29%29%24.%20Remarkably%2C%20this%20regret%20bound%0Aremains%20constant%20relative%20to%20the%20number%20of%20episodes%20%24K%24.%20To%20the%20best%20of%20our%0Aknowledge%2C%20Cert-LSVI-UCB%20is%20the%20first%20algorithm%20to%20achieve%20a%20constant%2C%0Ainstance-dependent%2C%20high-probability%20regret%20bound%20in%20RL%20with%20linear%20function%0Aapproximation%20for%20infinite%20runs%20without%20relying%20on%20prior%20distribution%0Aassumptions.%20This%20not%20only%20highlights%20the%20robustness%20of%20Cert-LSVI-UCB%20to%20model%0Amisspecification%20but%20also%20introduces%20novel%20algorithmic%20designs%20and%20analytical%0Atechniques%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10745v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Settling%20Constant%20Regrets%20in%20Linear%20Markov%20Decision%20Processes&entry.906535625=Weitong%20Zhang%20and%20Zhiyuan%20Fan%20and%20Jiafan%20He%20and%20Quanquan%20Gu&entry.1292438233=%20%20We%20study%20the%20constant%20regret%20guarantees%20in%20reinforcement%20learning%20%28RL%29.%20Our%0Aobjective%20is%20to%20design%20an%20algorithm%20that%20incurs%20only%20finite%20regret%20over%0Ainfinite%20episodes%20with%20high%20probability.%20We%20introduce%20an%20algorithm%2C%0ACert-LSVI-UCB%2C%20for%20misspecified%20linear%20Markov%20decision%20processes%20%28MDPs%29%20where%0Aboth%20the%20transition%20kernel%20and%20the%20reward%20function%20can%20be%20approximated%20by%20some%0Alinear%20function%20up%20to%20misspecification%20level%20%24%5Czeta%24.%20At%20the%20core%20of%0ACert-LSVI-UCB%20is%20an%20innovative%20certified%20estimator%2C%20which%20facilitates%20a%0Afine-grained%20concentration%20analysis%20for%20multi-phase%20value-targeted%20regression%2C%0Aenabling%20us%20to%20establish%20an%20instance-dependent%20regret%20bound%20that%20is%20constant%0Aw.r.t.%20the%20number%20of%20episodes.%20Specifically%2C%20we%20demonstrate%20that%20for%20an%20MDP%0Acharacterized%20by%20a%20minimal%20suboptimality%20gap%20%24%5CDelta%24%2C%20Cert-LSVI-UCB%20has%20a%0Acumulative%20regret%20of%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E3H%5E5/%5CDelta%29%24%20with%20high%0Aprobability%2C%20provided%20that%20the%20misspecification%20level%20%24%5Czeta%24%20is%20below%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5CDelta%20/%20%28%5Csqrt%7Bd%7DH%5E2%29%29%24.%20Remarkably%2C%20this%20regret%20bound%0Aremains%20constant%20relative%20to%20the%20number%20of%20episodes%20%24K%24.%20To%20the%20best%20of%20our%0Aknowledge%2C%20Cert-LSVI-UCB%20is%20the%20first%20algorithm%20to%20achieve%20a%20constant%2C%0Ainstance-dependent%2C%20high-probability%20regret%20bound%20in%20RL%20with%20linear%20function%0Aapproximation%20for%20infinite%20runs%20without%20relying%20on%20prior%20distribution%0Aassumptions.%20This%20not%20only%20highlights%20the%20robustness%20of%20Cert-LSVI-UCB%20to%20model%0Amisspecification%20but%20also%20introduces%20novel%20algorithmic%20designs%20and%20analytical%0Atechniques%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10745v1&entry.124074799=Read"},
{"title": "Anatomy of Industrial Scale Multilingual ASR", "author": "Francis McCann Ramirez and Luka Chkhetiani and Andrew Ehrenberg and Robert McHardy and Rami Botros and Yash Khare and Andrea Vanzo and Taufiquzzaman Peyash and Gabriel Oexle and Michael Liang and Ilya Sklyar and Enver Fakhan and Ahmed Etefy and Daniel McCrystal and Sam Flamini and Domenic Donato and Takuya Yoshioka", "abstract": "  This paper describes AssemblyAI's industrial-scale automatic speech\nrecognition (ASR) system, designed to meet the requirements of large-scale,\nmultilingual ASR serving various application needs. Our system leverages a\ndiverse training dataset comprising unsupervised (12.5M hours), supervised\n(188k hours), and pseudo-labeled (1.6M hours) data across four languages. We\nprovide a detailed description of our model architecture, consisting of a\nfull-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an\nRNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation\ndemonstrates competitive word error rates (WERs) against larger and more\ncomputationally expensive models, such as Whisper large and Canary-1B.\nFurthermore, our architectural choices yield several key advantages, including\nan improved code-switching capability, a 5x inference speedup compared to an\noptimized Whisper baseline, a 30% reduction in hallucination rate on speech\ndata, and a 90% reduction in ambient noise compared to Whisper, along with\nsignificantly improved time-stamp accuracy. Throughout this work, we adopt a\nsystem-centric approach to analyzing various aspects of fully-fledged ASR\nmodels to gain practically relevant insights useful for real-world services\noperating at scale.\n", "link": "http://arxiv.org/abs/2404.09841v2", "date": "2024-04-16", "relevancy": 1.8352, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4563}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anatomy%20of%20Industrial%20Scale%20Multilingual%20ASR&body=Title%3A%20Anatomy%20of%20Industrial%20Scale%20Multilingual%20ASR%0AAuthor%3A%20Francis%20McCann%20Ramirez%20and%20Luka%20Chkhetiani%20and%20Andrew%20Ehrenberg%20and%20Robert%20McHardy%20and%20Rami%20Botros%20and%20Yash%20Khare%20and%20Andrea%20Vanzo%20and%20Taufiquzzaman%20Peyash%20and%20Gabriel%20Oexle%20and%20Michael%20Liang%20and%20Ilya%20Sklyar%20and%20Enver%20Fakhan%20and%20Ahmed%20Etefy%20and%20Daniel%20McCrystal%20and%20Sam%20Flamini%20and%20Domenic%20Donato%20and%20Takuya%20Yoshioka%0AAbstract%3A%20%20%20This%20paper%20describes%20AssemblyAI%27s%20industrial-scale%20automatic%20speech%0Arecognition%20%28ASR%29%20system%2C%20designed%20to%20meet%20the%20requirements%20of%20large-scale%2C%0Amultilingual%20ASR%20serving%20various%20application%20needs.%20Our%20system%20leverages%20a%0Adiverse%20training%20dataset%20comprising%20unsupervised%20%2812.5M%20hours%29%2C%20supervised%0A%28188k%20hours%29%2C%20and%20pseudo-labeled%20%281.6M%20hours%29%20data%20across%20four%20languages.%20We%0Aprovide%20a%20detailed%20description%20of%20our%20model%20architecture%2C%20consisting%20of%20a%0Afull-context%20600M-parameter%20Conformer%20encoder%20pre-trained%20with%20BEST-RQ%20and%20an%0ARNN-T%20decoder%20fine-tuned%20jointly%20with%20the%20encoder.%20Our%20extensive%20evaluation%0Ademonstrates%20competitive%20word%20error%20rates%20%28WERs%29%20against%20larger%20and%20more%0Acomputationally%20expensive%20models%2C%20such%20as%20Whisper%20large%20and%20Canary-1B.%0AFurthermore%2C%20our%20architectural%20choices%20yield%20several%20key%20advantages%2C%20including%0Aan%20improved%20code-switching%20capability%2C%20a%205x%20inference%20speedup%20compared%20to%20an%0Aoptimized%20Whisper%20baseline%2C%20a%2030%25%20reduction%20in%20hallucination%20rate%20on%20speech%0Adata%2C%20and%20a%2090%25%20reduction%20in%20ambient%20noise%20compared%20to%20Whisper%2C%20along%20with%0Asignificantly%20improved%20time-stamp%20accuracy.%20Throughout%20this%20work%2C%20we%20adopt%20a%0Asystem-centric%20approach%20to%20analyzing%20various%20aspects%20of%20fully-fledged%20ASR%0Amodels%20to%20gain%20practically%20relevant%20insights%20useful%20for%20real-world%20services%0Aoperating%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09841v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomy%20of%20Industrial%20Scale%20Multilingual%20ASR&entry.906535625=Francis%20McCann%20Ramirez%20and%20Luka%20Chkhetiani%20and%20Andrew%20Ehrenberg%20and%20Robert%20McHardy%20and%20Rami%20Botros%20and%20Yash%20Khare%20and%20Andrea%20Vanzo%20and%20Taufiquzzaman%20Peyash%20and%20Gabriel%20Oexle%20and%20Michael%20Liang%20and%20Ilya%20Sklyar%20and%20Enver%20Fakhan%20and%20Ahmed%20Etefy%20and%20Daniel%20McCrystal%20and%20Sam%20Flamini%20and%20Domenic%20Donato%20and%20Takuya%20Yoshioka&entry.1292438233=%20%20This%20paper%20describes%20AssemblyAI%27s%20industrial-scale%20automatic%20speech%0Arecognition%20%28ASR%29%20system%2C%20designed%20to%20meet%20the%20requirements%20of%20large-scale%2C%0Amultilingual%20ASR%20serving%20various%20application%20needs.%20Our%20system%20leverages%20a%0Adiverse%20training%20dataset%20comprising%20unsupervised%20%2812.5M%20hours%29%2C%20supervised%0A%28188k%20hours%29%2C%20and%20pseudo-labeled%20%281.6M%20hours%29%20data%20across%20four%20languages.%20We%0Aprovide%20a%20detailed%20description%20of%20our%20model%20architecture%2C%20consisting%20of%20a%0Afull-context%20600M-parameter%20Conformer%20encoder%20pre-trained%20with%20BEST-RQ%20and%20an%0ARNN-T%20decoder%20fine-tuned%20jointly%20with%20the%20encoder.%20Our%20extensive%20evaluation%0Ademonstrates%20competitive%20word%20error%20rates%20%28WERs%29%20against%20larger%20and%20more%0Acomputationally%20expensive%20models%2C%20such%20as%20Whisper%20large%20and%20Canary-1B.%0AFurthermore%2C%20our%20architectural%20choices%20yield%20several%20key%20advantages%2C%20including%0Aan%20improved%20code-switching%20capability%2C%20a%205x%20inference%20speedup%20compared%20to%20an%0Aoptimized%20Whisper%20baseline%2C%20a%2030%25%20reduction%20in%20hallucination%20rate%20on%20speech%0Adata%2C%20and%20a%2090%25%20reduction%20in%20ambient%20noise%20compared%20to%20Whisper%2C%20along%20with%0Asignificantly%20improved%20time-stamp%20accuracy.%20Throughout%20this%20work%2C%20we%20adopt%20a%0Asystem-centric%20approach%20to%20analyzing%20various%20aspects%20of%20fully-fledged%20ASR%0Amodels%20to%20gain%20practically%20relevant%20insights%20useful%20for%20real-world%20services%0Aoperating%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09841v2&entry.124074799=Read"},
{"title": "Learning-based Position and Stiffness Feedforward Control of\n  Antagonistic Soft Pneumatic Actuators using Gaussian Processes", "author": "Tim-Lukas Habich and Sarah Kleinjohann and Moritz Schappler", "abstract": "  Variable stiffness actuator (VSA) designs are manifold. Conventional\nmodel-based control of these nonlinear systems is associated with high effort\nand design-dependent assumptions. In contrast, machine learning offers a\npromising alternative as models are trained on real measured data and\nnonlinearities are inherently taken into account. Our work presents a\nuniversal, learning-based approach for position and stiffness control of soft\nactuators. After introducing a soft pneumatic VSA, the model is learned with\ninput-output data. For this purpose, a test bench was set up which enables\nautomated measurement of the variable joint stiffness. During control, Gaussian\nprocesses are used to predict pressures for achieving desired position and\nstiffness. The feedforward error is on average 11.5% of the total pressure\nrange and is compensated by feedback control. Experiments with the soft\nactuator show that the learning-based approach allows continuous adjustment of\nposition and stiffness without model knowledge.\n", "link": "http://arxiv.org/abs/2303.01840v2", "date": "2024-04-16", "relevancy": 1.8221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4489}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Position%20and%20Stiffness%20Feedforward%20Control%20of%0A%20%20Antagonistic%20Soft%20Pneumatic%20Actuators%20using%20Gaussian%20Processes&body=Title%3A%20Learning-based%20Position%20and%20Stiffness%20Feedforward%20Control%20of%0A%20%20Antagonistic%20Soft%20Pneumatic%20Actuators%20using%20Gaussian%20Processes%0AAuthor%3A%20Tim-Lukas%20Habich%20and%20Sarah%20Kleinjohann%20and%20Moritz%20Schappler%0AAbstract%3A%20%20%20Variable%20stiffness%20actuator%20%28VSA%29%20designs%20are%20manifold.%20Conventional%0Amodel-based%20control%20of%20these%20nonlinear%20systems%20is%20associated%20with%20high%20effort%0Aand%20design-dependent%20assumptions.%20In%20contrast%2C%20machine%20learning%20offers%20a%0Apromising%20alternative%20as%20models%20are%20trained%20on%20real%20measured%20data%20and%0Anonlinearities%20are%20inherently%20taken%20into%20account.%20Our%20work%20presents%20a%0Auniversal%2C%20learning-based%20approach%20for%20position%20and%20stiffness%20control%20of%20soft%0Aactuators.%20After%20introducing%20a%20soft%20pneumatic%20VSA%2C%20the%20model%20is%20learned%20with%0Ainput-output%20data.%20For%20this%20purpose%2C%20a%20test%20bench%20was%20set%20up%20which%20enables%0Aautomated%20measurement%20of%20the%20variable%20joint%20stiffness.%20During%20control%2C%20Gaussian%0Aprocesses%20are%20used%20to%20predict%20pressures%20for%20achieving%20desired%20position%20and%0Astiffness.%20The%20feedforward%20error%20is%20on%20average%2011.5%25%20of%20the%20total%20pressure%0Arange%20and%20is%20compensated%20by%20feedback%20control.%20Experiments%20with%20the%20soft%0Aactuator%20show%20that%20the%20learning-based%20approach%20allows%20continuous%20adjustment%20of%0Aposition%20and%20stiffness%20without%20model%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01840v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Position%20and%20Stiffness%20Feedforward%20Control%20of%0A%20%20Antagonistic%20Soft%20Pneumatic%20Actuators%20using%20Gaussian%20Processes&entry.906535625=Tim-Lukas%20Habich%20and%20Sarah%20Kleinjohann%20and%20Moritz%20Schappler&entry.1292438233=%20%20Variable%20stiffness%20actuator%20%28VSA%29%20designs%20are%20manifold.%20Conventional%0Amodel-based%20control%20of%20these%20nonlinear%20systems%20is%20associated%20with%20high%20effort%0Aand%20design-dependent%20assumptions.%20In%20contrast%2C%20machine%20learning%20offers%20a%0Apromising%20alternative%20as%20models%20are%20trained%20on%20real%20measured%20data%20and%0Anonlinearities%20are%20inherently%20taken%20into%20account.%20Our%20work%20presents%20a%0Auniversal%2C%20learning-based%20approach%20for%20position%20and%20stiffness%20control%20of%20soft%0Aactuators.%20After%20introducing%20a%20soft%20pneumatic%20VSA%2C%20the%20model%20is%20learned%20with%0Ainput-output%20data.%20For%20this%20purpose%2C%20a%20test%20bench%20was%20set%20up%20which%20enables%0Aautomated%20measurement%20of%20the%20variable%20joint%20stiffness.%20During%20control%2C%20Gaussian%0Aprocesses%20are%20used%20to%20predict%20pressures%20for%20achieving%20desired%20position%20and%0Astiffness.%20The%20feedforward%20error%20is%20on%20average%2011.5%25%20of%20the%20total%20pressure%0Arange%20and%20is%20compensated%20by%20feedback%20control.%20Experiments%20with%20the%20soft%0Aactuator%20show%20that%20the%20learning-based%20approach%20allows%20continuous%20adjustment%20of%0Aposition%20and%20stiffness%20without%20model%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01840v2&entry.124074799=Read"},
{"title": "Noncontact Respiratory Anomaly Detection Using Infrared Light-Wave\n  Sensing", "author": "Md Zobaer Islam and Brenden Martin and Carly Gotcher and Tyler Martinez and John F. O'Hara and Sabit Ekin", "abstract": "  Human respiratory rate and its pattern convey essential information about the\nphysical and psychological states of the subject. Abnormal breathing can\nindicate fatal health issues leading to further diagnosis and treatment.\nWireless light-wave sensing (LWS) using incoherent infrared light shows promise\nin safe, discreet, efficient, and non-invasive human breathing monitoring\nwithout raising privacy concerns. The respiration monitoring system needs to be\ntrained on different types of breathing patterns to identify breathing\nanomalies.The system must also validate the collected data as a breathing\nwaveform, discarding any faulty data caused by external interruption, user\nmovement, or system malfunction. To address these needs, this study simulated\nnormal and different types of abnormal respiration using a robot that mimics\nhuman breathing patterns. Then, time-series respiration data were collected\nusing infrared light-wave sensing technology. Three machine learning\nalgorithms, decision tree, random forest and XGBoost, were applied to detect\nbreathing anomalies and faulty data. Model performances were evaluated through\ncross-validation, assessing classification accuracy, precision and recall\nscores. The random forest model achieved the highest classification accuracy of\n96.75% with data collected at a 0.5m distance. In general, ensemble models like\nrandom forest and XGBoost performed better than a single model in classifying\nthe data collected at multiple distances from the light-wave sensing setup.\n", "link": "http://arxiv.org/abs/2301.03713v4", "date": "2024-04-16", "relevancy": 1.8005, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4977}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4416}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4396}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Noncontact%20Respiratory%20Anomaly%20Detection%20Using%20Infrared%20Light-Wave%0A%20%20Sensing&body=Title%3A%20Noncontact%20Respiratory%20Anomaly%20Detection%20Using%20Infrared%20Light-Wave%0A%20%20Sensing%0AAuthor%3A%20Md%20Zobaer%20Islam%20and%20Brenden%20Martin%20and%20Carly%20Gotcher%20and%20Tyler%20Martinez%20and%20John%20F.%20O%27Hara%20and%20Sabit%20Ekin%0AAbstract%3A%20%20%20Human%20respiratory%20rate%20and%20its%20pattern%20convey%20essential%20information%20about%20the%0Aphysical%20and%20psychological%20states%20of%20the%20subject.%20Abnormal%20breathing%20can%0Aindicate%20fatal%20health%20issues%20leading%20to%20further%20diagnosis%20and%20treatment.%0AWireless%20light-wave%20sensing%20%28LWS%29%20using%20incoherent%20infrared%20light%20shows%20promise%0Ain%20safe%2C%20discreet%2C%20efficient%2C%20and%20non-invasive%20human%20breathing%20monitoring%0Awithout%20raising%20privacy%20concerns.%20The%20respiration%20monitoring%20system%20needs%20to%20be%0Atrained%20on%20different%20types%20of%20breathing%20patterns%20to%20identify%20breathing%0Aanomalies.The%20system%20must%20also%20validate%20the%20collected%20data%20as%20a%20breathing%0Awaveform%2C%20discarding%20any%20faulty%20data%20caused%20by%20external%20interruption%2C%20user%0Amovement%2C%20or%20system%20malfunction.%20To%20address%20these%20needs%2C%20this%20study%20simulated%0Anormal%20and%20different%20types%20of%20abnormal%20respiration%20using%20a%20robot%20that%20mimics%0Ahuman%20breathing%20patterns.%20Then%2C%20time-series%20respiration%20data%20were%20collected%0Ausing%20infrared%20light-wave%20sensing%20technology.%20Three%20machine%20learning%0Aalgorithms%2C%20decision%20tree%2C%20random%20forest%20and%20XGBoost%2C%20were%20applied%20to%20detect%0Abreathing%20anomalies%20and%20faulty%20data.%20Model%20performances%20were%20evaluated%20through%0Across-validation%2C%20assessing%20classification%20accuracy%2C%20precision%20and%20recall%0Ascores.%20The%20random%20forest%20model%20achieved%20the%20highest%20classification%20accuracy%20of%0A96.75%25%20with%20data%20collected%20at%20a%200.5m%20distance.%20In%20general%2C%20ensemble%20models%20like%0Arandom%20forest%20and%20XGBoost%20performed%20better%20than%20a%20single%20model%20in%20classifying%0Athe%20data%20collected%20at%20multiple%20distances%20from%20the%20light-wave%20sensing%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.03713v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noncontact%20Respiratory%20Anomaly%20Detection%20Using%20Infrared%20Light-Wave%0A%20%20Sensing&entry.906535625=Md%20Zobaer%20Islam%20and%20Brenden%20Martin%20and%20Carly%20Gotcher%20and%20Tyler%20Martinez%20and%20John%20F.%20O%27Hara%20and%20Sabit%20Ekin&entry.1292438233=%20%20Human%20respiratory%20rate%20and%20its%20pattern%20convey%20essential%20information%20about%20the%0Aphysical%20and%20psychological%20states%20of%20the%20subject.%20Abnormal%20breathing%20can%0Aindicate%20fatal%20health%20issues%20leading%20to%20further%20diagnosis%20and%20treatment.%0AWireless%20light-wave%20sensing%20%28LWS%29%20using%20incoherent%20infrared%20light%20shows%20promise%0Ain%20safe%2C%20discreet%2C%20efficient%2C%20and%20non-invasive%20human%20breathing%20monitoring%0Awithout%20raising%20privacy%20concerns.%20The%20respiration%20monitoring%20system%20needs%20to%20be%0Atrained%20on%20different%20types%20of%20breathing%20patterns%20to%20identify%20breathing%0Aanomalies.The%20system%20must%20also%20validate%20the%20collected%20data%20as%20a%20breathing%0Awaveform%2C%20discarding%20any%20faulty%20data%20caused%20by%20external%20interruption%2C%20user%0Amovement%2C%20or%20system%20malfunction.%20To%20address%20these%20needs%2C%20this%20study%20simulated%0Anormal%20and%20different%20types%20of%20abnormal%20respiration%20using%20a%20robot%20that%20mimics%0Ahuman%20breathing%20patterns.%20Then%2C%20time-series%20respiration%20data%20were%20collected%0Ausing%20infrared%20light-wave%20sensing%20technology.%20Three%20machine%20learning%0Aalgorithms%2C%20decision%20tree%2C%20random%20forest%20and%20XGBoost%2C%20were%20applied%20to%20detect%0Abreathing%20anomalies%20and%20faulty%20data.%20Model%20performances%20were%20evaluated%20through%0Across-validation%2C%20assessing%20classification%20accuracy%2C%20precision%20and%20recall%0Ascores.%20The%20random%20forest%20model%20achieved%20the%20highest%20classification%20accuracy%20of%0A96.75%25%20with%20data%20collected%20at%20a%200.5m%20distance.%20In%20general%2C%20ensemble%20models%20like%0Arandom%20forest%20and%20XGBoost%20performed%20better%20than%20a%20single%20model%20in%20classifying%0Athe%20data%20collected%20at%20multiple%20distances%20from%20the%20light-wave%20sensing%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.03713v4&entry.124074799=Read"},
{"title": "Sharp error bounds for imbalanced classification: how many examples in\n  the minority class?", "author": "Anass Aghbalou and Fran\u00e7ois Portier and Anne Sabourin", "abstract": "  When dealing with imbalanced classification data, reweighting the loss\nfunction is a standard procedure allowing to equilibrate between the true\npositive and true negative rates within the risk measure. Despite significant\ntheoretical work in this area, existing results do not adequately address a\nmain challenge within the imbalanced classification framework, which is the\nnegligible size of one class in relation to the full sample size and the need\nto rescale the risk function by a probability tending to zero. To address this\ngap, we present two novel contributions in the setting where the rare class\nprobability approaches zero: (1) a non asymptotic fast rate probability bound\nfor constrained balanced empirical risk minimization, and (2) a consistent\nupper bound for balanced nearest neighbors estimates. Our findings provide a\nclearer understanding of the benefits of class-weighting in realistic settings,\nopening new avenues for further research in this field.\n", "link": "http://arxiv.org/abs/2310.14826v2", "date": "2024-04-16", "relevancy": 1.7994, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4297}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4208}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sharp%20error%20bounds%20for%20imbalanced%20classification%3A%20how%20many%20examples%20in%0A%20%20the%20minority%20class%3F&body=Title%3A%20Sharp%20error%20bounds%20for%20imbalanced%20classification%3A%20how%20many%20examples%20in%0A%20%20the%20minority%20class%3F%0AAuthor%3A%20Anass%20Aghbalou%20and%20Fran%C3%A7ois%20Portier%20and%20Anne%20Sabourin%0AAbstract%3A%20%20%20When%20dealing%20with%20imbalanced%20classification%20data%2C%20reweighting%20the%20loss%0Afunction%20is%20a%20standard%20procedure%20allowing%20to%20equilibrate%20between%20the%20true%0Apositive%20and%20true%20negative%20rates%20within%20the%20risk%20measure.%20Despite%20significant%0Atheoretical%20work%20in%20this%20area%2C%20existing%20results%20do%20not%20adequately%20address%20a%0Amain%20challenge%20within%20the%20imbalanced%20classification%20framework%2C%20which%20is%20the%0Anegligible%20size%20of%20one%20class%20in%20relation%20to%20the%20full%20sample%20size%20and%20the%20need%0Ato%20rescale%20the%20risk%20function%20by%20a%20probability%20tending%20to%20zero.%20To%20address%20this%0Agap%2C%20we%20present%20two%20novel%20contributions%20in%20the%20setting%20where%20the%20rare%20class%0Aprobability%20approaches%20zero%3A%20%281%29%20a%20non%20asymptotic%20fast%20rate%20probability%20bound%0Afor%20constrained%20balanced%20empirical%20risk%20minimization%2C%20and%20%282%29%20a%20consistent%0Aupper%20bound%20for%20balanced%20nearest%20neighbors%20estimates.%20Our%20findings%20provide%20a%0Aclearer%20understanding%20of%20the%20benefits%20of%20class-weighting%20in%20realistic%20settings%2C%0Aopening%20new%20avenues%20for%20further%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14826v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20error%20bounds%20for%20imbalanced%20classification%3A%20how%20many%20examples%20in%0A%20%20the%20minority%20class%3F&entry.906535625=Anass%20Aghbalou%20and%20Fran%C3%A7ois%20Portier%20and%20Anne%20Sabourin&entry.1292438233=%20%20When%20dealing%20with%20imbalanced%20classification%20data%2C%20reweighting%20the%20loss%0Afunction%20is%20a%20standard%20procedure%20allowing%20to%20equilibrate%20between%20the%20true%0Apositive%20and%20true%20negative%20rates%20within%20the%20risk%20measure.%20Despite%20significant%0Atheoretical%20work%20in%20this%20area%2C%20existing%20results%20do%20not%20adequately%20address%20a%0Amain%20challenge%20within%20the%20imbalanced%20classification%20framework%2C%20which%20is%20the%0Anegligible%20size%20of%20one%20class%20in%20relation%20to%20the%20full%20sample%20size%20and%20the%20need%0Ato%20rescale%20the%20risk%20function%20by%20a%20probability%20tending%20to%20zero.%20To%20address%20this%0Agap%2C%20we%20present%20two%20novel%20contributions%20in%20the%20setting%20where%20the%20rare%20class%0Aprobability%20approaches%20zero%3A%20%281%29%20a%20non%20asymptotic%20fast%20rate%20probability%20bound%0Afor%20constrained%20balanced%20empirical%20risk%20minimization%2C%20and%20%282%29%20a%20consistent%0Aupper%20bound%20for%20balanced%20nearest%20neighbors%20estimates.%20Our%20findings%20provide%20a%0Aclearer%20understanding%20of%20the%20benefits%20of%20class-weighting%20in%20realistic%20settings%2C%0Aopening%20new%20avenues%20for%20further%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14826v2&entry.124074799=Read"},
{"title": "Dual Modalities of Text: Visual and Textual Generative Pre-training", "author": "Yekun Chai and Qingyi Liu and Jingwu Xiao and Shuohuan Wang and Yu Sun and Hua Wu", "abstract": "  Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.\n", "link": "http://arxiv.org/abs/2404.10710v1", "date": "2024-04-16", "relevancy": 1.7926, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5876}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Modalities%20of%20Text%3A%20Visual%20and%20Textual%20Generative%20Pre-training&body=Title%3A%20Dual%20Modalities%20of%20Text%3A%20Visual%20and%20Textual%20Generative%20Pre-training%0AAuthor%3A%20Yekun%20Chai%20and%20Qingyi%20Liu%20and%20Jingwu%20Xiao%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%0AAbstract%3A%20%20%20Harnessing%20visual%20texts%20represents%20a%20burgeoning%20frontier%20in%20the%20evolution%20of%0Alanguage%20modeling.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20pre-training%20framework%0Afor%20a%20suite%20of%20pixel-based%20autoregressive%20language%20models%2C%20pre-training%20on%20a%0Acorpus%20of%20over%20400%20million%20documents%20rendered%20as%20RGB%20images.%20Our%20approach%20is%0Acharacterized%20by%20a%20dual-modality%20training%20regimen%2C%20engaging%20both%20visual%20data%0Athrough%20next%20patch%20prediction%20with%20a%20regression%20head%20and%20textual%20data%20via%20next%0Atoken%20prediction%20with%20a%20classification%20head.%20This%20study%20is%20particularly%20focused%0Aon%20investigating%20the%20synergistic%20interplay%20between%20visual%20and%20textual%0Amodalities%20of%20language.%20Our%20comprehensive%20evaluation%20across%20a%20diverse%20array%20of%0Abenchmarks%20reveals%20that%20the%20confluence%20of%20visual%20and%20textual%20data%20substantially%0Aaugments%20the%20efficacy%20of%20pixel-based%20language%20models.%20Notably%2C%20our%20findings%0Ashow%20that%20a%20unidirectional%20pixel-based%20model%2C%20devoid%20of%20textual%20data%20during%0Atraining%2C%20can%20match%20the%20performance%20levels%20of%20advanced%20bidirectional%0Apixel-based%20models%20on%20various%20language%20understanding%20benchmarks.%20This%20work%0Ahighlights%20the%20considerable%20untapped%20potential%20of%20integrating%20visual%20and%0Atextual%20information%20for%20language%20modeling%20purposes.%20We%20will%20release%20our%20code%2C%0Adata%2C%20and%20checkpoints%20to%20inspire%20further%20research%20advancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10710v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Modalities%20of%20Text%3A%20Visual%20and%20Textual%20Generative%20Pre-training&entry.906535625=Yekun%20Chai%20and%20Qingyi%20Liu%20and%20Jingwu%20Xiao%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu&entry.1292438233=%20%20Harnessing%20visual%20texts%20represents%20a%20burgeoning%20frontier%20in%20the%20evolution%20of%0Alanguage%20modeling.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20pre-training%20framework%0Afor%20a%20suite%20of%20pixel-based%20autoregressive%20language%20models%2C%20pre-training%20on%20a%0Acorpus%20of%20over%20400%20million%20documents%20rendered%20as%20RGB%20images.%20Our%20approach%20is%0Acharacterized%20by%20a%20dual-modality%20training%20regimen%2C%20engaging%20both%20visual%20data%0Athrough%20next%20patch%20prediction%20with%20a%20regression%20head%20and%20textual%20data%20via%20next%0Atoken%20prediction%20with%20a%20classification%20head.%20This%20study%20is%20particularly%20focused%0Aon%20investigating%20the%20synergistic%20interplay%20between%20visual%20and%20textual%0Amodalities%20of%20language.%20Our%20comprehensive%20evaluation%20across%20a%20diverse%20array%20of%0Abenchmarks%20reveals%20that%20the%20confluence%20of%20visual%20and%20textual%20data%20substantially%0Aaugments%20the%20efficacy%20of%20pixel-based%20language%20models.%20Notably%2C%20our%20findings%0Ashow%20that%20a%20unidirectional%20pixel-based%20model%2C%20devoid%20of%20textual%20data%20during%0Atraining%2C%20can%20match%20the%20performance%20levels%20of%20advanced%20bidirectional%0Apixel-based%20models%20on%20various%20language%20understanding%20benchmarks.%20This%20work%0Ahighlights%20the%20considerable%20untapped%20potential%20of%20integrating%20visual%20and%0Atextual%20information%20for%20language%20modeling%20purposes.%20We%20will%20release%20our%20code%2C%0Adata%2C%20and%20checkpoints%20to%20inspire%20further%20research%20advancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10710v1&entry.124074799=Read"},
{"title": "Interpolation and differentiation of alchemical degrees of freedom in\n  machine learning interatomic potentials", "author": "Juno Nam and Rafael G\u00f3mez-Bombarelli", "abstract": "  Machine learning interatomic potentials (MLIPs) have become a workhorse of\nmodern atomistic simulations, and recently published universal MLIPs,\npre-trained on large datasets, have demonstrated remarkable accuracy and\ngeneralizability. However, the computational cost of MLIPs limits their\napplicability to chemically disordered systems requiring large simulation cells\nor to sample-intensive statistical methods. Here, we report the use of\ncontinuous and differentiable alchemical degrees of freedom in atomistic\nmaterials simulations, exploiting the fact that graph neural network MLIPs\nrepresent discrete elements as real-valued tensors. The proposed method\nintroduces alchemical atoms with corresponding weights into the input graph,\nalongside modifications to the message-passing and readout mechanisms of MLIPs,\nand allows smooth interpolation between the compositional states of materials.\nThe end-to-end differentiability of MLIPs enables efficient calculation of the\ngradient of energy with respect to the compositional weights. Leveraging these\ngradients, we propose methodologies for optimizing the composition of solid\nsolutions towards target macroscopic properties and conducting alchemical free\nenergy simulations to quantify the free energy of vacancy formation and\ncomposition changes. The approach offers an avenue for extending the\ncapabilities of universal MLIPs in the modeling of compositional disorder and\ncharacterizing the phase stabilities of complex materials systems.\n", "link": "http://arxiv.org/abs/2404.10746v1", "date": "2024-04-16", "relevancy": 1.3486, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4318}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Interpolation%20and%20differentiation%20of%20alchemical%20degrees%20of%20freedom%20in%0A%20%20machine%20learning%20interatomic%20potentials&body=Title%3A%20Interpolation%20and%20differentiation%20of%20alchemical%20degrees%20of%20freedom%20in%0A%20%20machine%20learning%20interatomic%20potentials%0AAuthor%3A%20Juno%20Nam%20and%20Rafael%20G%C3%B3mez-Bombarelli%0AAbstract%3A%20%20%20Machine%20learning%20interatomic%20potentials%20%28MLIPs%29%20have%20become%20a%20workhorse%20of%0Amodern%20atomistic%20simulations%2C%20and%20recently%20published%20universal%20MLIPs%2C%0Apre-trained%20on%20large%20datasets%2C%20have%20demonstrated%20remarkable%20accuracy%20and%0Ageneralizability.%20However%2C%20the%20computational%20cost%20of%20MLIPs%20limits%20their%0Aapplicability%20to%20chemically%20disordered%20systems%20requiring%20large%20simulation%20cells%0Aor%20to%20sample-intensive%20statistical%20methods.%20Here%2C%20we%20report%20the%20use%20of%0Acontinuous%20and%20differentiable%20alchemical%20degrees%20of%20freedom%20in%20atomistic%0Amaterials%20simulations%2C%20exploiting%20the%20fact%20that%20graph%20neural%20network%20MLIPs%0Arepresent%20discrete%20elements%20as%20real-valued%20tensors.%20The%20proposed%20method%0Aintroduces%20alchemical%20atoms%20with%20corresponding%20weights%20into%20the%20input%20graph%2C%0Aalongside%20modifications%20to%20the%20message-passing%20and%20readout%20mechanisms%20of%20MLIPs%2C%0Aand%20allows%20smooth%20interpolation%20between%20the%20compositional%20states%20of%20materials.%0AThe%20end-to-end%20differentiability%20of%20MLIPs%20enables%20efficient%20calculation%20of%20the%0Agradient%20of%20energy%20with%20respect%20to%20the%20compositional%20weights.%20Leveraging%20these%0Agradients%2C%20we%20propose%20methodologies%20for%20optimizing%20the%20composition%20of%20solid%0Asolutions%20towards%20target%20macroscopic%20properties%20and%20conducting%20alchemical%20free%0Aenergy%20simulations%20to%20quantify%20the%20free%20energy%20of%20vacancy%20formation%20and%0Acomposition%20changes.%20The%20approach%20offers%20an%20avenue%20for%20extending%20the%0Acapabilities%20of%20universal%20MLIPs%20in%20the%20modeling%20of%20compositional%20disorder%20and%0Acharacterizing%20the%20phase%20stabilities%20of%20complex%20materials%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10746v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpolation%20and%20differentiation%20of%20alchemical%20degrees%20of%20freedom%20in%0A%20%20machine%20learning%20interatomic%20potentials&entry.906535625=Juno%20Nam%20and%20Rafael%20G%C3%B3mez-Bombarelli&entry.1292438233=%20%20Machine%20learning%20interatomic%20potentials%20%28MLIPs%29%20have%20become%20a%20workhorse%20of%0Amodern%20atomistic%20simulations%2C%20and%20recently%20published%20universal%20MLIPs%2C%0Apre-trained%20on%20large%20datasets%2C%20have%20demonstrated%20remarkable%20accuracy%20and%0Ageneralizability.%20However%2C%20the%20computational%20cost%20of%20MLIPs%20limits%20their%0Aapplicability%20to%20chemically%20disordered%20systems%20requiring%20large%20simulation%20cells%0Aor%20to%20sample-intensive%20statistical%20methods.%20Here%2C%20we%20report%20the%20use%20of%0Acontinuous%20and%20differentiable%20alchemical%20degrees%20of%20freedom%20in%20atomistic%0Amaterials%20simulations%2C%20exploiting%20the%20fact%20that%20graph%20neural%20network%20MLIPs%0Arepresent%20discrete%20elements%20as%20real-valued%20tensors.%20The%20proposed%20method%0Aintroduces%20alchemical%20atoms%20with%20corresponding%20weights%20into%20the%20input%20graph%2C%0Aalongside%20modifications%20to%20the%20message-passing%20and%20readout%20mechanisms%20of%20MLIPs%2C%0Aand%20allows%20smooth%20interpolation%20between%20the%20compositional%20states%20of%20materials.%0AThe%20end-to-end%20differentiability%20of%20MLIPs%20enables%20efficient%20calculation%20of%20the%0Agradient%20of%20energy%20with%20respect%20to%20the%20compositional%20weights.%20Leveraging%20these%0Agradients%2C%20we%20propose%20methodologies%20for%20optimizing%20the%20composition%20of%20solid%0Asolutions%20towards%20target%20macroscopic%20properties%20and%20conducting%20alchemical%20free%0Aenergy%20simulations%20to%20quantify%20the%20free%20energy%20of%20vacancy%20formation%20and%0Acomposition%20changes.%20The%20approach%20offers%20an%20avenue%20for%20extending%20the%0Acapabilities%20of%20universal%20MLIPs%20in%20the%20modeling%20of%20compositional%20disorder%20and%0Acharacterizing%20the%20phase%20stabilities%20of%20complex%20materials%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10746v1&entry.124074799=Read"},
{"title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation", "author": "Hongxin Zhang and Zeyuan Wang and Qiushi Lyu and Zheyuan Zhang and Sunli Chen and Tianmin Shu and Yilun Du and Chuang Gan", "abstract": "  In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only partial\negocentric views of the world. To effectively plan in this setting, in contrast\nto learning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video. By leveraging this compositional world model, in\ncombination with Vision Language Models to infer the actions of other agents,\nwe can use a tree search procedure to integrate these modules and facilitate\nonline cooperative planning. To evaluate the efficacy of our methods, we create\ntwo challenging embodied multi-agent long-horizon cooperation tasks using the\nThreeDWorld simulator and conduct experiments with 2-4 agents. The results show\nour compositional world model is effective and the framework enables the\nembodied agents to cooperate efficiently with different agents across various\ntasks and an arbitrary number of agents, showing the promising future of our\nproposed framework. More videos can be found at\nhttps://vis-www.cs.umass.edu/combo/.\n", "link": "http://arxiv.org/abs/2404.10775v1", "date": "2024-04-16", "relevancy": 1.7216, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5838}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5829}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5663}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COMBO%3A%20Compositional%20World%20Models%20for%20Embodied%20Multi-Agent%20Cooperation&body=Title%3A%20COMBO%3A%20Compositional%20World%20Models%20for%20Embodied%20Multi-Agent%20Cooperation%0AAuthor%3A%20Hongxin%20Zhang%20and%20Zeyuan%20Wang%20and%20Qiushi%20Lyu%20and%20Zheyuan%20Zhang%20and%20Sunli%20Chen%20and%20Tianmin%20Shu%20and%20Yilun%20Du%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20embodied%20multi-agent%0Acooperation%2C%20where%20decentralized%20agents%20must%20cooperate%20given%20only%20partial%0Aegocentric%20views%20of%20the%20world.%20To%20effectively%20plan%20in%20this%20setting%2C%20in%20contrast%0Ato%20learning%20world%20dynamics%20in%20a%20single-agent%20scenario%2C%20we%20must%20simulate%20world%0Adynamics%20conditioned%20on%20an%20arbitrary%20number%20of%20agents%27%20actions%20given%20only%0Apartial%20egocentric%20visual%20observations%20of%20the%20world.%20To%20address%20this%20issue%20of%0Apartial%20observability%2C%20we%20first%20train%20generative%20models%20to%20estimate%20the%20overall%0Aworld%20state%20given%20partial%20egocentric%20observations.%20To%20enable%20accurate%0Asimulation%20of%20multiple%20sets%20of%20actions%20on%20this%20world%20state%2C%20we%20then%20propose%20to%0Alearn%20a%20compositional%20world%20model%20for%20multi-agent%20cooperation%20by%20factorizing%0Athe%20naturally%20composable%20joint%20actions%20of%20multiple%20agents%20and%20compositionally%0Agenerating%20the%20video.%20By%20leveraging%20this%20compositional%20world%20model%2C%20in%0Acombination%20with%20Vision%20Language%20Models%20to%20infer%20the%20actions%20of%20other%20agents%2C%0Awe%20can%20use%20a%20tree%20search%20procedure%20to%20integrate%20these%20modules%20and%20facilitate%0Aonline%20cooperative%20planning.%20To%20evaluate%20the%20efficacy%20of%20our%20methods%2C%20we%20create%0Atwo%20challenging%20embodied%20multi-agent%20long-horizon%20cooperation%20tasks%20using%20the%0AThreeDWorld%20simulator%20and%20conduct%20experiments%20with%202-4%20agents.%20The%20results%20show%0Aour%20compositional%20world%20model%20is%20effective%20and%20the%20framework%20enables%20the%0Aembodied%20agents%20to%20cooperate%20efficiently%20with%20different%20agents%20across%20various%0Atasks%20and%20an%20arbitrary%20number%20of%20agents%2C%20showing%20the%20promising%20future%20of%20our%0Aproposed%20framework.%20More%20videos%20can%20be%20found%20at%0Ahttps%3A//vis-www.cs.umass.edu/combo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10775v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMBO%3A%20Compositional%20World%20Models%20for%20Embodied%20Multi-Agent%20Cooperation&entry.906535625=Hongxin%20Zhang%20and%20Zeyuan%20Wang%20and%20Qiushi%20Lyu%20and%20Zheyuan%20Zhang%20and%20Sunli%20Chen%20and%20Tianmin%20Shu%20and%20Yilun%20Du%20and%20Chuang%20Gan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20problem%20of%20embodied%20multi-agent%0Acooperation%2C%20where%20decentralized%20agents%20must%20cooperate%20given%20only%20partial%0Aegocentric%20views%20of%20the%20world.%20To%20effectively%20plan%20in%20this%20setting%2C%20in%20contrast%0Ato%20learning%20world%20dynamics%20in%20a%20single-agent%20scenario%2C%20we%20must%20simulate%20world%0Adynamics%20conditioned%20on%20an%20arbitrary%20number%20of%20agents%27%20actions%20given%20only%0Apartial%20egocentric%20visual%20observations%20of%20the%20world.%20To%20address%20this%20issue%20of%0Apartial%20observability%2C%20we%20first%20train%20generative%20models%20to%20estimate%20the%20overall%0Aworld%20state%20given%20partial%20egocentric%20observations.%20To%20enable%20accurate%0Asimulation%20of%20multiple%20sets%20of%20actions%20on%20this%20world%20state%2C%20we%20then%20propose%20to%0Alearn%20a%20compositional%20world%20model%20for%20multi-agent%20cooperation%20by%20factorizing%0Athe%20naturally%20composable%20joint%20actions%20of%20multiple%20agents%20and%20compositionally%0Agenerating%20the%20video.%20By%20leveraging%20this%20compositional%20world%20model%2C%20in%0Acombination%20with%20Vision%20Language%20Models%20to%20infer%20the%20actions%20of%20other%20agents%2C%0Awe%20can%20use%20a%20tree%20search%20procedure%20to%20integrate%20these%20modules%20and%20facilitate%0Aonline%20cooperative%20planning.%20To%20evaluate%20the%20efficacy%20of%20our%20methods%2C%20we%20create%0Atwo%20challenging%20embodied%20multi-agent%20long-horizon%20cooperation%20tasks%20using%20the%0AThreeDWorld%20simulator%20and%20conduct%20experiments%20with%202-4%20agents.%20The%20results%20show%0Aour%20compositional%20world%20model%20is%20effective%20and%20the%20framework%20enables%20the%0Aembodied%20agents%20to%20cooperate%20efficiently%20with%20different%20agents%20across%20various%0Atasks%20and%20an%20arbitrary%20number%20of%20agents%2C%20showing%20the%20promising%20future%20of%20our%0Aproposed%20framework.%20More%20videos%20can%20be%20found%20at%0Ahttps%3A//vis-www.cs.umass.edu/combo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10775v1&entry.124074799=Read"},
{"title": "Simplex Decomposition for Portfolio Allocation Constraints in\n  Reinforcement Learning", "author": "David Winkel and Niklas Strau\u00df and Matthias Schubert and Thomas Seidl", "abstract": "  Portfolio optimization tasks describe sequential decision problems in which\nthe investor's wealth is distributed across a set of assets. Allocation\nconstraints are used to enforce minimal or maximal investments into particular\nsubsets of assets to control for objectives such as limiting the portfolio's\nexposure to a certain sector due to environmental concerns. Although methods\nfor constrained Reinforcement Learning (CRL) can optimize policies while\nconsidering allocation constraints, it can be observed that these general\nmethods yield suboptimal results. In this paper, we propose a novel approach to\nhandle allocation constraints based on a decomposition of the constraint action\nspace into a set of unconstrained allocation problems. In particular, we\nexamine this approach for the case of two constraints. For example, an investor\nmay wish to invest at least a certain percentage of the portfolio into green\ntechnologies while limiting the investment in the fossil energy sector. We show\nthat the action space of the task is equivalent to the decomposed action space,\nand introduce a new reinforcement learning (RL) approach CAOSD, which is built\non top of the decomposition. The experimental evaluation on real-world\nNasdaq-100 data demonstrates that our approach consistently outperforms\nstate-of-the-art CRL benchmarks for portfolio optimization.\n", "link": "http://arxiv.org/abs/2404.10683v1", "date": "2024-04-16", "relevancy": 1.368, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4475}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4471}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simplex%20Decomposition%20for%20Portfolio%20Allocation%20Constraints%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20Simplex%20Decomposition%20for%20Portfolio%20Allocation%20Constraints%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20David%20Winkel%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert%20and%20Thomas%20Seidl%0AAbstract%3A%20%20%20Portfolio%20optimization%20tasks%20describe%20sequential%20decision%20problems%20in%20which%0Athe%20investor%27s%20wealth%20is%20distributed%20across%20a%20set%20of%20assets.%20Allocation%0Aconstraints%20are%20used%20to%20enforce%20minimal%20or%20maximal%20investments%20into%20particular%0Asubsets%20of%20assets%20to%20control%20for%20objectives%20such%20as%20limiting%20the%20portfolio%27s%0Aexposure%20to%20a%20certain%20sector%20due%20to%20environmental%20concerns.%20Although%20methods%0Afor%20constrained%20Reinforcement%20Learning%20%28CRL%29%20can%20optimize%20policies%20while%0Aconsidering%20allocation%20constraints%2C%20it%20can%20be%20observed%20that%20these%20general%0Amethods%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%0Ahandle%20allocation%20constraints%20based%20on%20a%20decomposition%20of%20the%20constraint%20action%0Aspace%20into%20a%20set%20of%20unconstrained%20allocation%20problems.%20In%20particular%2C%20we%0Aexamine%20this%20approach%20for%20the%20case%20of%20two%20constraints.%20For%20example%2C%20an%20investor%0Amay%20wish%20to%20invest%20at%20least%20a%20certain%20percentage%20of%20the%20portfolio%20into%20green%0Atechnologies%20while%20limiting%20the%20investment%20in%20the%20fossil%20energy%20sector.%20We%20show%0Athat%20the%20action%20space%20of%20the%20task%20is%20equivalent%20to%20the%20decomposed%20action%20space%2C%0Aand%20introduce%20a%20new%20reinforcement%20learning%20%28RL%29%20approach%20CAOSD%2C%20which%20is%20built%0Aon%20top%20of%20the%20decomposition.%20The%20experimental%20evaluation%20on%20real-world%0ANasdaq-100%20data%20demonstrates%20that%20our%20approach%20consistently%20outperforms%0Astate-of-the-art%20CRL%20benchmarks%20for%20portfolio%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10683v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplex%20Decomposition%20for%20Portfolio%20Allocation%20Constraints%20in%0A%20%20Reinforcement%20Learning&entry.906535625=David%20Winkel%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert%20and%20Thomas%20Seidl&entry.1292438233=%20%20Portfolio%20optimization%20tasks%20describe%20sequential%20decision%20problems%20in%20which%0Athe%20investor%27s%20wealth%20is%20distributed%20across%20a%20set%20of%20assets.%20Allocation%0Aconstraints%20are%20used%20to%20enforce%20minimal%20or%20maximal%20investments%20into%20particular%0Asubsets%20of%20assets%20to%20control%20for%20objectives%20such%20as%20limiting%20the%20portfolio%27s%0Aexposure%20to%20a%20certain%20sector%20due%20to%20environmental%20concerns.%20Although%20methods%0Afor%20constrained%20Reinforcement%20Learning%20%28CRL%29%20can%20optimize%20policies%20while%0Aconsidering%20allocation%20constraints%2C%20it%20can%20be%20observed%20that%20these%20general%0Amethods%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%0Ahandle%20allocation%20constraints%20based%20on%20a%20decomposition%20of%20the%20constraint%20action%0Aspace%20into%20a%20set%20of%20unconstrained%20allocation%20problems.%20In%20particular%2C%20we%0Aexamine%20this%20approach%20for%20the%20case%20of%20two%20constraints.%20For%20example%2C%20an%20investor%0Amay%20wish%20to%20invest%20at%20least%20a%20certain%20percentage%20of%20the%20portfolio%20into%20green%0Atechnologies%20while%20limiting%20the%20investment%20in%20the%20fossil%20energy%20sector.%20We%20show%0Athat%20the%20action%20space%20of%20the%20task%20is%20equivalent%20to%20the%20decomposed%20action%20space%2C%0Aand%20introduce%20a%20new%20reinforcement%20learning%20%28RL%29%20approach%20CAOSD%2C%20which%20is%20built%0Aon%20top%20of%20the%20decomposition.%20The%20experimental%20evaluation%20on%20real-world%0ANasdaq-100%20data%20demonstrates%20that%20our%20approach%20consistently%20outperforms%0Astate-of-the-art%20CRL%20benchmarks%20for%20portfolio%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10683v1&entry.124074799=Read"},
{"title": "Deep Generative Data Assimilation in Multimodal Setting", "author": "Yongquan Qu and Juan Nathaniel and Shuolin Li and Pierre Gentine", "abstract": "  Robust integration of physical knowledge and data is key to improve\ncomputational simulations, such as Earth system models. Data assimilation is\ncrucial for achieving this goal because it provides a systematic framework to\ncalibrate model outputs with observations, which can include remote sensing\nimagery and ground station measurements, with uncertainty quantification.\nConventional methods, including Kalman filters and variational approaches,\ninherently rely on simplifying linear and Gaussian assumptions, and can be\ncomputationally expensive. Nevertheless, with the rapid adoption of data-driven\nmethods in many areas of computational sciences, we see the potential of\nemulating traditional data assimilation with deep learning, especially\ngenerative models. In particular, the diffusion-based probabilistic framework\nhas large overlaps with data assimilation principles: both allows for\nconditional generation of samples with a Bayesian inverse framework. These\nmodels have shown remarkable success in text-conditioned image generation or\nimage-controlled video synthesis. Likewise, one can frame data assimilation as\nobservation-conditioned state calibration. In this work, we propose SLAMS:\nScore-based Latent Assimilation in Multimodal Setting. Specifically, we\nassimilate in-situ weather station data and ex-situ satellite imagery to\ncalibrate the vertical temperature profiles, globally. Through extensive\nablation, we demonstrate that SLAMS is robust even in low-resolution, noisy,\nand sparse data settings. To our knowledge, our work is the first to apply deep\ngenerative framework for multimodal data assimilation using real-world\ndatasets; an important step for building robust computational simulators,\nincluding the next-generation Earth system models. Our code is available at:\nhttps://github.com/yongquan-qu/SLAMS\n", "link": "http://arxiv.org/abs/2404.06665v2", "date": "2024-04-16", "relevancy": 1.6427, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Data%20Assimilation%20in%20Multimodal%20Setting&body=Title%3A%20Deep%20Generative%20Data%20Assimilation%20in%20Multimodal%20Setting%0AAuthor%3A%20Yongquan%20Qu%20and%20Juan%20Nathaniel%20and%20Shuolin%20Li%20and%20Pierre%20Gentine%0AAbstract%3A%20%20%20Robust%20integration%20of%20physical%20knowledge%20and%20data%20is%20key%20to%20improve%0Acomputational%20simulations%2C%20such%20as%20Earth%20system%20models.%20Data%20assimilation%20is%0Acrucial%20for%20achieving%20this%20goal%20because%20it%20provides%20a%20systematic%20framework%20to%0Acalibrate%20model%20outputs%20with%20observations%2C%20which%20can%20include%20remote%20sensing%0Aimagery%20and%20ground%20station%20measurements%2C%20with%20uncertainty%20quantification.%0AConventional%20methods%2C%20including%20Kalman%20filters%20and%20variational%20approaches%2C%0Ainherently%20rely%20on%20simplifying%20linear%20and%20Gaussian%20assumptions%2C%20and%20can%20be%0Acomputationally%20expensive.%20Nevertheless%2C%20with%20the%20rapid%20adoption%20of%20data-driven%0Amethods%20in%20many%20areas%20of%20computational%20sciences%2C%20we%20see%20the%20potential%20of%0Aemulating%20traditional%20data%20assimilation%20with%20deep%20learning%2C%20especially%0Agenerative%20models.%20In%20particular%2C%20the%20diffusion-based%20probabilistic%20framework%0Ahas%20large%20overlaps%20with%20data%20assimilation%20principles%3A%20both%20allows%20for%0Aconditional%20generation%20of%20samples%20with%20a%20Bayesian%20inverse%20framework.%20These%0Amodels%20have%20shown%20remarkable%20success%20in%20text-conditioned%20image%20generation%20or%0Aimage-controlled%20video%20synthesis.%20Likewise%2C%20one%20can%20frame%20data%20assimilation%20as%0Aobservation-conditioned%20state%20calibration.%20In%20this%20work%2C%20we%20propose%20SLAMS%3A%0AScore-based%20Latent%20Assimilation%20in%20Multimodal%20Setting.%20Specifically%2C%20we%0Aassimilate%20in-situ%20weather%20station%20data%20and%20ex-situ%20satellite%20imagery%20to%0Acalibrate%20the%20vertical%20temperature%20profiles%2C%20globally.%20Through%20extensive%0Aablation%2C%20we%20demonstrate%20that%20SLAMS%20is%20robust%20even%20in%20low-resolution%2C%20noisy%2C%0Aand%20sparse%20data%20settings.%20To%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20apply%20deep%0Agenerative%20framework%20for%20multimodal%20data%20assimilation%20using%20real-world%0Adatasets%3B%20an%20important%20step%20for%20building%20robust%20computational%20simulators%2C%0Aincluding%20the%20next-generation%20Earth%20system%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/yongquan-qu/SLAMS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06665v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Data%20Assimilation%20in%20Multimodal%20Setting&entry.906535625=Yongquan%20Qu%20and%20Juan%20Nathaniel%20and%20Shuolin%20Li%20and%20Pierre%20Gentine&entry.1292438233=%20%20Robust%20integration%20of%20physical%20knowledge%20and%20data%20is%20key%20to%20improve%0Acomputational%20simulations%2C%20such%20as%20Earth%20system%20models.%20Data%20assimilation%20is%0Acrucial%20for%20achieving%20this%20goal%20because%20it%20provides%20a%20systematic%20framework%20to%0Acalibrate%20model%20outputs%20with%20observations%2C%20which%20can%20include%20remote%20sensing%0Aimagery%20and%20ground%20station%20measurements%2C%20with%20uncertainty%20quantification.%0AConventional%20methods%2C%20including%20Kalman%20filters%20and%20variational%20approaches%2C%0Ainherently%20rely%20on%20simplifying%20linear%20and%20Gaussian%20assumptions%2C%20and%20can%20be%0Acomputationally%20expensive.%20Nevertheless%2C%20with%20the%20rapid%20adoption%20of%20data-driven%0Amethods%20in%20many%20areas%20of%20computational%20sciences%2C%20we%20see%20the%20potential%20of%0Aemulating%20traditional%20data%20assimilation%20with%20deep%20learning%2C%20especially%0Agenerative%20models.%20In%20particular%2C%20the%20diffusion-based%20probabilistic%20framework%0Ahas%20large%20overlaps%20with%20data%20assimilation%20principles%3A%20both%20allows%20for%0Aconditional%20generation%20of%20samples%20with%20a%20Bayesian%20inverse%20framework.%20These%0Amodels%20have%20shown%20remarkable%20success%20in%20text-conditioned%20image%20generation%20or%0Aimage-controlled%20video%20synthesis.%20Likewise%2C%20one%20can%20frame%20data%20assimilation%20as%0Aobservation-conditioned%20state%20calibration.%20In%20this%20work%2C%20we%20propose%20SLAMS%3A%0AScore-based%20Latent%20Assimilation%20in%20Multimodal%20Setting.%20Specifically%2C%20we%0Aassimilate%20in-situ%20weather%20station%20data%20and%20ex-situ%20satellite%20imagery%20to%0Acalibrate%20the%20vertical%20temperature%20profiles%2C%20globally.%20Through%20extensive%0Aablation%2C%20we%20demonstrate%20that%20SLAMS%20is%20robust%20even%20in%20low-resolution%2C%20noisy%2C%0Aand%20sparse%20data%20settings.%20To%20our%20knowledge%2C%20our%20work%20is%20the%20first%20to%20apply%20deep%0Agenerative%20framework%20for%20multimodal%20data%20assimilation%20using%20real-world%0Adatasets%3B%20an%20important%20step%20for%20building%20robust%20computational%20simulators%2C%0Aincluding%20the%20next-generation%20Earth%20system%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/yongquan-qu/SLAMS%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06665v2&entry.124074799=Read"},
{"title": "Robust Noisy Label Learning via Two-Stream Sample Distillation", "author": "Sihan Bai and Sanping Zhou and Zheng Qin and Le Wang and Nanning Zheng", "abstract": "  Noisy label learning aims to learn robust networks under the supervision of\nnoisy labels, which plays a critical role in deep learning. Existing work\neither conducts sample selection or label correction to deal with noisy labels\nduring the model training process. In this paper, we design a simple yet\neffective sample selection framework, termed Two-Stream Sample Distillation\n(TSSD), for noisy label learning, which can extract more high-quality samples\nwith clean labels to improve the robustness of network training. Firstly, a\nnovel Parallel Sample Division (PSD) module is designed to generate a certain\ntraining set with sufficient reliable positive and negative samples by jointly\nconsidering the sample structure in feature space and the human prior in loss\nspace. Secondly, a novel Meta Sample Purification (MSP) module is further\ndesigned to mine adequate semi-hard samples from the remaining uncertain\ntraining set by learning a strong meta classifier with extra golden data. As a\nresult, more and more high-quality samples will be distilled from the noisy\ntraining set to train networks robustly in every iteration. Extensive\nexperiments on four benchmark datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, and Clothing-1M, show that our method has achieved\nstate-of-the-art results over its competitors.\n", "link": "http://arxiv.org/abs/2404.10499v1", "date": "2024-04-16", "relevancy": 1.5523, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5194}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5162}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5137}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Robust%20Noisy%20Label%20Learning%20via%20Two-Stream%20Sample%20Distillation&body=Title%3A%20Robust%20Noisy%20Label%20Learning%20via%20Two-Stream%20Sample%20Distillation%0AAuthor%3A%20Sihan%20Bai%20and%20Sanping%20Zhou%20and%20Zheng%20Qin%20and%20Le%20Wang%20and%20Nanning%20Zheng%0AAbstract%3A%20%20%20Noisy%20label%20learning%20aims%20to%20learn%20robust%20networks%20under%20the%20supervision%20of%0Anoisy%20labels%2C%20which%20plays%20a%20critical%20role%20in%20deep%20learning.%20Existing%20work%0Aeither%20conducts%20sample%20selection%20or%20label%20correction%20to%20deal%20with%20noisy%20labels%0Aduring%20the%20model%20training%20process.%20In%20this%20paper%2C%20we%20design%20a%20simple%20yet%0Aeffective%20sample%20selection%20framework%2C%20termed%20Two-Stream%20Sample%20Distillation%0A%28TSSD%29%2C%20for%20noisy%20label%20learning%2C%20which%20can%20extract%20more%20high-quality%20samples%0Awith%20clean%20labels%20to%20improve%20the%20robustness%20of%20network%20training.%20Firstly%2C%20a%0Anovel%20Parallel%20Sample%20Division%20%28PSD%29%20module%20is%20designed%20to%20generate%20a%20certain%0Atraining%20set%20with%20sufficient%20reliable%20positive%20and%20negative%20samples%20by%20jointly%0Aconsidering%20the%20sample%20structure%20in%20feature%20space%20and%20the%20human%20prior%20in%20loss%0Aspace.%20Secondly%2C%20a%20novel%20Meta%20Sample%20Purification%20%28MSP%29%20module%20is%20further%0Adesigned%20to%20mine%20adequate%20semi-hard%20samples%20from%20the%20remaining%20uncertain%0Atraining%20set%20by%20learning%20a%20strong%20meta%20classifier%20with%20extra%20golden%20data.%20As%20a%0Aresult%2C%20more%20and%20more%20high-quality%20samples%20will%20be%20distilled%20from%20the%20noisy%0Atraining%20set%20to%20train%20networks%20robustly%20in%20every%20iteration.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%2C%20including%20CIFAR-10%2C%20CIFAR-100%2C%0ATiny-ImageNet%2C%20and%20Clothing-1M%2C%20show%20that%20our%20method%20has%20achieved%0Astate-of-the-art%20results%20over%20its%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Noisy%20Label%20Learning%20via%20Two-Stream%20Sample%20Distillation&entry.906535625=Sihan%20Bai%20and%20Sanping%20Zhou%20and%20Zheng%20Qin%20and%20Le%20Wang%20and%20Nanning%20Zheng&entry.1292438233=%20%20Noisy%20label%20learning%20aims%20to%20learn%20robust%20networks%20under%20the%20supervision%20of%0Anoisy%20labels%2C%20which%20plays%20a%20critical%20role%20in%20deep%20learning.%20Existing%20work%0Aeither%20conducts%20sample%20selection%20or%20label%20correction%20to%20deal%20with%20noisy%20labels%0Aduring%20the%20model%20training%20process.%20In%20this%20paper%2C%20we%20design%20a%20simple%20yet%0Aeffective%20sample%20selection%20framework%2C%20termed%20Two-Stream%20Sample%20Distillation%0A%28TSSD%29%2C%20for%20noisy%20label%20learning%2C%20which%20can%20extract%20more%20high-quality%20samples%0Awith%20clean%20labels%20to%20improve%20the%20robustness%20of%20network%20training.%20Firstly%2C%20a%0Anovel%20Parallel%20Sample%20Division%20%28PSD%29%20module%20is%20designed%20to%20generate%20a%20certain%0Atraining%20set%20with%20sufficient%20reliable%20positive%20and%20negative%20samples%20by%20jointly%0Aconsidering%20the%20sample%20structure%20in%20feature%20space%20and%20the%20human%20prior%20in%20loss%0Aspace.%20Secondly%2C%20a%20novel%20Meta%20Sample%20Purification%20%28MSP%29%20module%20is%20further%0Adesigned%20to%20mine%20adequate%20semi-hard%20samples%20from%20the%20remaining%20uncertain%0Atraining%20set%20by%20learning%20a%20strong%20meta%20classifier%20with%20extra%20golden%20data.%20As%20a%0Aresult%2C%20more%20and%20more%20high-quality%20samples%20will%20be%20distilled%20from%20the%20noisy%0Atraining%20set%20to%20train%20networks%20robustly%20in%20every%20iteration.%20Extensive%0Aexperiments%20on%20four%20benchmark%20datasets%2C%20including%20CIFAR-10%2C%20CIFAR-100%2C%0ATiny-ImageNet%2C%20and%20Clothing-1M%2C%20show%20that%20our%20method%20has%20achieved%0Astate-of-the-art%20results%20over%20its%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10499v1&entry.124074799=Read"},
{"title": "Face-voice Association in Multilingual Environments (FAME) Challenge\n  2024 Evaluation Plan", "author": "Muhammad Saad Saeed and Shah Nawaz and Muhammad Salman Tahir and Rohan Kumar Das and Muhammad Zaigham Zaheer and Marta Moscati and Markus Schedl and Muhammad Haris Khan and Karthik Nandakumar and Muhammad Haroon Yousaf", "abstract": "  The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, the audio-visual systems are one\nof the widely used multimodal systems. In the recent years, associating face\nand voice of a person has gained attention due to presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) Challenge 2024 focuses on exploring face-voice association\nunder a unique condition of multilingual scenario. This condition is inspired\nfrom the fact that half of the world's population is bilingual and most often\npeople communicate under multilingual scenario. The challenge uses a dataset\nnamely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baselines and task details for the FAME Challenge.\n", "link": "http://arxiv.org/abs/2404.09342v2", "date": "2024-04-16", "relevancy": 1.3935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4666}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.432}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%20Challenge%0A%20%202024%20Evaluation%20Plan&body=Title%3A%20Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%20Challenge%0A%20%202024%20Evaluation%20Plan%0AAuthor%3A%20Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Muhammad%20Salman%20Tahir%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Marta%20Moscati%20and%20Markus%20Schedl%20and%20Muhammad%20Haris%20Khan%20and%20Karthik%20Nandakumar%20and%20Muhammad%20Haroon%20Yousaf%0AAbstract%3A%20%20%20The%20advancements%20of%20technology%20have%20led%20to%20the%20use%20of%20multimodal%20systems%20in%0Avarious%20real-world%20applications.%20Among%20them%2C%20the%20audio-visual%20systems%20are%20one%0Aof%20the%20widely%20used%20multimodal%20systems.%20In%20the%20recent%20years%2C%20associating%20face%0Aand%20voice%20of%20a%20person%20has%20gained%20attention%20due%20to%20presence%20of%20unique%0Acorrelation%20between%20them.%20The%20Face-voice%20Association%20in%20Multilingual%0AEnvironments%20%28FAME%29%20Challenge%202024%20focuses%20on%20exploring%20face-voice%20association%0Aunder%20a%20unique%20condition%20of%20multilingual%20scenario.%20This%20condition%20is%20inspired%0Afrom%20the%20fact%20that%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%20most%20often%0Apeople%20communicate%20under%20multilingual%20scenario.%20The%20challenge%20uses%20a%20dataset%0Anamely%2C%20Multilingual%20Audio-Visual%20%28MAV-Celeb%29%20for%20exploring%20face-voice%0Aassociation%20in%20multilingual%20environments.%20This%20report%20provides%20the%20details%20of%0Athe%20challenge%2C%20dataset%2C%20baselines%20and%20task%20details%20for%20the%20FAME%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09342v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%20Challenge%0A%20%202024%20Evaluation%20Plan&entry.906535625=Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Muhammad%20Salman%20Tahir%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Marta%20Moscati%20and%20Markus%20Schedl%20and%20Muhammad%20Haris%20Khan%20and%20Karthik%20Nandakumar%20and%20Muhammad%20Haroon%20Yousaf&entry.1292438233=%20%20The%20advancements%20of%20technology%20have%20led%20to%20the%20use%20of%20multimodal%20systems%20in%0Avarious%20real-world%20applications.%20Among%20them%2C%20the%20audio-visual%20systems%20are%20one%0Aof%20the%20widely%20used%20multimodal%20systems.%20In%20the%20recent%20years%2C%20associating%20face%0Aand%20voice%20of%20a%20person%20has%20gained%20attention%20due%20to%20presence%20of%20unique%0Acorrelation%20between%20them.%20The%20Face-voice%20Association%20in%20Multilingual%0AEnvironments%20%28FAME%29%20Challenge%202024%20focuses%20on%20exploring%20face-voice%20association%0Aunder%20a%20unique%20condition%20of%20multilingual%20scenario.%20This%20condition%20is%20inspired%0Afrom%20the%20fact%20that%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%20most%20often%0Apeople%20communicate%20under%20multilingual%20scenario.%20The%20challenge%20uses%20a%20dataset%0Anamely%2C%20Multilingual%20Audio-Visual%20%28MAV-Celeb%29%20for%20exploring%20face-voice%0Aassociation%20in%20multilingual%20environments.%20This%20report%20provides%20the%20details%20of%0Athe%20challenge%2C%20dataset%2C%20baselines%20and%20task%20details%20for%20the%20FAME%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09342v2&entry.124074799=Read"},
{"title": "Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem", "author": "Roumen Nikolaev Popov", "abstract": "  We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.\n", "link": "http://arxiv.org/abs/2404.10550v1", "date": "2024-04-16", "relevancy": 1.4442, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4958}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Analytical%20Approximation%20of%20the%20ELBO%20Gradient%20in%20the%20Context%20of%20the%0A%20%20Clutter%20Problem&body=Title%3A%20Analytical%20Approximation%20of%20the%20ELBO%20Gradient%20in%20the%20Context%20of%20the%0A%20%20Clutter%20Problem%0AAuthor%3A%20Roumen%20Nikolaev%20Popov%0AAbstract%3A%20%20%20We%20propose%20an%20analytical%20solution%20for%20approximating%20the%20gradient%20of%20the%0AEvidence%20Lower%20Bound%20%28ELBO%29%20in%20variational%20inference%20problems%20where%20the%0Astatistical%20model%20is%20a%20Bayesian%20network%20consisting%20of%20observations%20drawn%20from%20a%0Amixture%20of%20a%20Gaussian%20distribution%20embedded%20in%20unrelated%20clutter%2C%20known%20as%20the%0Aclutter%20problem.%20The%20method%20employs%20the%20reparameterization%20trick%20to%20move%20the%0Agradient%20operator%20inside%20the%20expectation%20and%20relies%20on%20the%20assumption%20that%2C%0Abecause%20the%20likelihood%20factorizes%20over%20the%20observed%20data%2C%20the%20variational%0Adistribution%20is%20generally%20more%20compactly%20supported%20than%20the%20Gaussian%0Adistribution%20in%20the%20likelihood%20factors.%20This%20allows%20efficient%20local%0Aapproximation%20of%20the%20individual%20likelihood%20factors%2C%20which%20leads%20to%20an%0Aanalytical%20solution%20for%20the%20integral%20defining%20the%20gradient%20expectation.%20We%0Aintegrate%20the%20proposed%20gradient%20approximation%20as%20the%20expectation%20step%20in%20an%20EM%0A%28Expectation%20Maximization%29%20algorithm%20for%20maximizing%20ELBO%20and%20test%20against%0Aclassical%20deterministic%20approaches%20in%20Bayesian%20inference%2C%20such%20as%20the%20Laplace%0Aapproximation%2C%20Expectation%20Propagation%20and%20Mean-Field%20Variational%20Inference.%0AThe%20proposed%20method%20demonstrates%20good%20accuracy%20and%20rate%20of%20convergence%20together%0Awith%20linear%20computational%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10550v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analytical%20Approximation%20of%20the%20ELBO%20Gradient%20in%20the%20Context%20of%20the%0A%20%20Clutter%20Problem&entry.906535625=Roumen%20Nikolaev%20Popov&entry.1292438233=%20%20We%20propose%20an%20analytical%20solution%20for%20approximating%20the%20gradient%20of%20the%0AEvidence%20Lower%20Bound%20%28ELBO%29%20in%20variational%20inference%20problems%20where%20the%0Astatistical%20model%20is%20a%20Bayesian%20network%20consisting%20of%20observations%20drawn%20from%20a%0Amixture%20of%20a%20Gaussian%20distribution%20embedded%20in%20unrelated%20clutter%2C%20known%20as%20the%0Aclutter%20problem.%20The%20method%20employs%20the%20reparameterization%20trick%20to%20move%20the%0Agradient%20operator%20inside%20the%20expectation%20and%20relies%20on%20the%20assumption%20that%2C%0Abecause%20the%20likelihood%20factorizes%20over%20the%20observed%20data%2C%20the%20variational%0Adistribution%20is%20generally%20more%20compactly%20supported%20than%20the%20Gaussian%0Adistribution%20in%20the%20likelihood%20factors.%20This%20allows%20efficient%20local%0Aapproximation%20of%20the%20individual%20likelihood%20factors%2C%20which%20leads%20to%20an%0Aanalytical%20solution%20for%20the%20integral%20defining%20the%20gradient%20expectation.%20We%0Aintegrate%20the%20proposed%20gradient%20approximation%20as%20the%20expectation%20step%20in%20an%20EM%0A%28Expectation%20Maximization%29%20algorithm%20for%20maximizing%20ELBO%20and%20test%20against%0Aclassical%20deterministic%20approaches%20in%20Bayesian%20inference%2C%20such%20as%20the%20Laplace%0Aapproximation%2C%20Expectation%20Propagation%20and%20Mean-Field%20Variational%20Inference.%0AThe%20proposed%20method%20demonstrates%20good%20accuracy%20and%20rate%20of%20convergence%20together%0Awith%20linear%20computational%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10550v1&entry.124074799=Read"},
{"title": "CMU-Flownet: Exploring Point Cloud Scene Flow Estimation in Occluded\n  Scenario", "author": "Jingze Chen and Junfeng Yao and Qiqin Lin and Lei Li", "abstract": "  Occlusions hinder point cloud frame alignment in LiDAR data, a challenge\ninadequately addressed by scene flow models tested mainly on occlusion-free\ndatasets. Attempts to integrate occlusion handling within networks often suffer\naccuracy issues due to two main limitations: a) the inadequate use of occlusion\ninformation, often merging it with flow estimation without an effective\nintegration strategy, and b) reliance on distance-weighted upsampling that\nfalls short in correcting occlusion-related errors. To address these\nchallenges, we introduce the Correlation Matrix Upsampling Flownet\n(CMU-Flownet), incorporating an occlusion estimation module within its cost\nvolume layer, alongside an Occlusion-aware Cost Volume (OCV) mechanism.\nSpecifically, we propose an enhanced upsampling approach that expands the\nsensory field of the sampling process which integrates a Correlation Matrix\ndesigned to evaluate point-level similarity. Meanwhile, our model robustly\nintegrates occlusion data within the context of scene flow, deploying this\ninformation strategically during the refinement phase of the flow estimation.\nThe efficacy of this approach is demonstrated through subsequent experimental\nvalidation. Empirical assessments reveal that CMU-Flownet establishes\nstate-of-the-art performance within the realms of occluded Flyingthings3D and\nKITTY datasets, surpassing previous methodologies across a majority of\nevaluated metrics.\n", "link": "http://arxiv.org/abs/2404.10571v1", "date": "2024-04-16", "relevancy": 1.6396, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CMU-Flownet%3A%20Exploring%20Point%20Cloud%20Scene%20Flow%20Estimation%20in%20Occluded%0A%20%20Scenario&body=Title%3A%20CMU-Flownet%3A%20Exploring%20Point%20Cloud%20Scene%20Flow%20Estimation%20in%20Occluded%0A%20%20Scenario%0AAuthor%3A%20Jingze%20Chen%20and%20Junfeng%20Yao%20and%20Qiqin%20Lin%20and%20Lei%20Li%0AAbstract%3A%20%20%20Occlusions%20hinder%20point%20cloud%20frame%20alignment%20in%20LiDAR%20data%2C%20a%20challenge%0Ainadequately%20addressed%20by%20scene%20flow%20models%20tested%20mainly%20on%20occlusion-free%0Adatasets.%20Attempts%20to%20integrate%20occlusion%20handling%20within%20networks%20often%20suffer%0Aaccuracy%20issues%20due%20to%20two%20main%20limitations%3A%20a%29%20the%20inadequate%20use%20of%20occlusion%0Ainformation%2C%20often%20merging%20it%20with%20flow%20estimation%20without%20an%20effective%0Aintegration%20strategy%2C%20and%20b%29%20reliance%20on%20distance-weighted%20upsampling%20that%0Afalls%20short%20in%20correcting%20occlusion-related%20errors.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20the%20Correlation%20Matrix%20Upsampling%20Flownet%0A%28CMU-Flownet%29%2C%20incorporating%20an%20occlusion%20estimation%20module%20within%20its%20cost%0Avolume%20layer%2C%20alongside%20an%20Occlusion-aware%20Cost%20Volume%20%28OCV%29%20mechanism.%0ASpecifically%2C%20we%20propose%20an%20enhanced%20upsampling%20approach%20that%20expands%20the%0Asensory%20field%20of%20the%20sampling%20process%20which%20integrates%20a%20Correlation%20Matrix%0Adesigned%20to%20evaluate%20point-level%20similarity.%20Meanwhile%2C%20our%20model%20robustly%0Aintegrates%20occlusion%20data%20within%20the%20context%20of%20scene%20flow%2C%20deploying%20this%0Ainformation%20strategically%20during%20the%20refinement%20phase%20of%20the%20flow%20estimation.%0AThe%20efficacy%20of%20this%20approach%20is%20demonstrated%20through%20subsequent%20experimental%0Avalidation.%20Empirical%20assessments%20reveal%20that%20CMU-Flownet%20establishes%0Astate-of-the-art%20performance%20within%20the%20realms%20of%20occluded%20Flyingthings3D%20and%0AKITTY%20datasets%2C%20surpassing%20previous%20methodologies%20across%20a%20majority%20of%0Aevaluated%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10571v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMU-Flownet%3A%20Exploring%20Point%20Cloud%20Scene%20Flow%20Estimation%20in%20Occluded%0A%20%20Scenario&entry.906535625=Jingze%20Chen%20and%20Junfeng%20Yao%20and%20Qiqin%20Lin%20and%20Lei%20Li&entry.1292438233=%20%20Occlusions%20hinder%20point%20cloud%20frame%20alignment%20in%20LiDAR%20data%2C%20a%20challenge%0Ainadequately%20addressed%20by%20scene%20flow%20models%20tested%20mainly%20on%20occlusion-free%0Adatasets.%20Attempts%20to%20integrate%20occlusion%20handling%20within%20networks%20often%20suffer%0Aaccuracy%20issues%20due%20to%20two%20main%20limitations%3A%20a%29%20the%20inadequate%20use%20of%20occlusion%0Ainformation%2C%20often%20merging%20it%20with%20flow%20estimation%20without%20an%20effective%0Aintegration%20strategy%2C%20and%20b%29%20reliance%20on%20distance-weighted%20upsampling%20that%0Afalls%20short%20in%20correcting%20occlusion-related%20errors.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20the%20Correlation%20Matrix%20Upsampling%20Flownet%0A%28CMU-Flownet%29%2C%20incorporating%20an%20occlusion%20estimation%20module%20within%20its%20cost%0Avolume%20layer%2C%20alongside%20an%20Occlusion-aware%20Cost%20Volume%20%28OCV%29%20mechanism.%0ASpecifically%2C%20we%20propose%20an%20enhanced%20upsampling%20approach%20that%20expands%20the%0Asensory%20field%20of%20the%20sampling%20process%20which%20integrates%20a%20Correlation%20Matrix%0Adesigned%20to%20evaluate%20point-level%20similarity.%20Meanwhile%2C%20our%20model%20robustly%0Aintegrates%20occlusion%20data%20within%20the%20context%20of%20scene%20flow%2C%20deploying%20this%0Ainformation%20strategically%20during%20the%20refinement%20phase%20of%20the%20flow%20estimation.%0AThe%20efficacy%20of%20this%20approach%20is%20demonstrated%20through%20subsequent%20experimental%0Avalidation.%20Empirical%20assessments%20reveal%20that%20CMU-Flownet%20establishes%0Astate-of-the-art%20performance%20within%20the%20realms%20of%20occluded%20Flyingthings3D%20and%0AKITTY%20datasets%2C%20surpassing%20previous%20methodologies%20across%20a%20majority%20of%0Aevaluated%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10571v1&entry.124074799=Read"},
{"title": "A/B testing under Interference with Partial Network Information", "author": "Shiv Shankar and Ritwik Sinha and Yash Chandak and Saayan Mitra and Madalina Fiterau", "abstract": "  A/B tests are often required to be conducted on subjects that might have\nsocial connections. For e.g., experiments on social media, or medical and\nsocial interventions to control the spread of an epidemic. In such settings,\nthe SUTVA assumption for randomized-controlled trials is violated due to\nnetwork interference, or spill-over effects, as treatments to group A can\npotentially also affect the control group B. When the underlying social network\nis known exactly, prior works have demonstrated how to conduct A/B tests\nadequately to estimate the global average treatment effect (GATE). However, in\npractice, it is often impossible to obtain knowledge about the exact underlying\nnetwork. In this paper, we present UNITE: a novel estimator that relax this\nassumption and can identify GATE while only relying on knowledge of the\nsuperset of neighbors for any subject in the graph. Through theoretical\nanalysis and extensive experiments, we show that the proposed approach performs\nbetter in comparison to standard estimators.\n", "link": "http://arxiv.org/abs/2404.10547v1", "date": "2024-04-16", "relevancy": 1.6398, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4188}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4035}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A/B%20testing%20under%20Interference%20with%20Partial%20Network%20Information&body=Title%3A%20A/B%20testing%20under%20Interference%20with%20Partial%20Network%20Information%0AAuthor%3A%20Shiv%20Shankar%20and%20Ritwik%20Sinha%20and%20Yash%20Chandak%20and%20Saayan%20Mitra%20and%20Madalina%20Fiterau%0AAbstract%3A%20%20%20A/B%20tests%20are%20often%20required%20to%20be%20conducted%20on%20subjects%20that%20might%20have%0Asocial%20connections.%20For%20e.g.%2C%20experiments%20on%20social%20media%2C%20or%20medical%20and%0Asocial%20interventions%20to%20control%20the%20spread%20of%20an%20epidemic.%20In%20such%20settings%2C%0Athe%20SUTVA%20assumption%20for%20randomized-controlled%20trials%20is%20violated%20due%20to%0Anetwork%20interference%2C%20or%20spill-over%20effects%2C%20as%20treatments%20to%20group%20A%20can%0Apotentially%20also%20affect%20the%20control%20group%20B.%20When%20the%20underlying%20social%20network%0Ais%20known%20exactly%2C%20prior%20works%20have%20demonstrated%20how%20to%20conduct%20A/B%20tests%0Aadequately%20to%20estimate%20the%20global%20average%20treatment%20effect%20%28GATE%29.%20However%2C%20in%0Apractice%2C%20it%20is%20often%20impossible%20to%20obtain%20knowledge%20about%20the%20exact%20underlying%0Anetwork.%20In%20this%20paper%2C%20we%20present%20UNITE%3A%20a%20novel%20estimator%20that%20relax%20this%0Aassumption%20and%20can%20identify%20GATE%20while%20only%20relying%20on%20knowledge%20of%20the%0Asuperset%20of%20neighbors%20for%20any%20subject%20in%20the%20graph.%20Through%20theoretical%0Aanalysis%20and%20extensive%20experiments%2C%20we%20show%20that%20the%20proposed%20approach%20performs%0Abetter%20in%20comparison%20to%20standard%20estimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10547v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A/B%20testing%20under%20Interference%20with%20Partial%20Network%20Information&entry.906535625=Shiv%20Shankar%20and%20Ritwik%20Sinha%20and%20Yash%20Chandak%20and%20Saayan%20Mitra%20and%20Madalina%20Fiterau&entry.1292438233=%20%20A/B%20tests%20are%20often%20required%20to%20be%20conducted%20on%20subjects%20that%20might%20have%0Asocial%20connections.%20For%20e.g.%2C%20experiments%20on%20social%20media%2C%20or%20medical%20and%0Asocial%20interventions%20to%20control%20the%20spread%20of%20an%20epidemic.%20In%20such%20settings%2C%0Athe%20SUTVA%20assumption%20for%20randomized-controlled%20trials%20is%20violated%20due%20to%0Anetwork%20interference%2C%20or%20spill-over%20effects%2C%20as%20treatments%20to%20group%20A%20can%0Apotentially%20also%20affect%20the%20control%20group%20B.%20When%20the%20underlying%20social%20network%0Ais%20known%20exactly%2C%20prior%20works%20have%20demonstrated%20how%20to%20conduct%20A/B%20tests%0Aadequately%20to%20estimate%20the%20global%20average%20treatment%20effect%20%28GATE%29.%20However%2C%20in%0Apractice%2C%20it%20is%20often%20impossible%20to%20obtain%20knowledge%20about%20the%20exact%20underlying%0Anetwork.%20In%20this%20paper%2C%20we%20present%20UNITE%3A%20a%20novel%20estimator%20that%20relax%20this%0Aassumption%20and%20can%20identify%20GATE%20while%20only%20relying%20on%20knowledge%20of%20the%0Asuperset%20of%20neighbors%20for%20any%20subject%20in%20the%20graph.%20Through%20theoretical%0Aanalysis%20and%20extensive%20experiments%2C%20we%20show%20that%20the%20proposed%20approach%20performs%0Abetter%20in%20comparison%20to%20standard%20estimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10547v1&entry.124074799=Read"},
{"title": "A Computer Vision-Based Quality Assessment Technique for the automatic\n  control of consumables for analytical laboratories", "author": "Meriam Zribi and Paolo Pagliuca and Francesca Pitolli", "abstract": "  The rapid growth of the Industry 4.0 paradigm is increasing the pressure to\ndevelop effective automated monitoring systems. Artificial Intelligence (AI) is\na convenient tool to improve the efficiency of industrial processes while\nreducing errors and waste. In fact, it allows the use of real-time data to\nincrease the effectiveness of monitoring systems, minimize errors, make the\nproduction process more sustainable, and save costs. In this paper, a novel\nautomatic monitoring system is proposed in the context of production process of\nplastic consumables used in analysis laboratories, with the aim to increase the\neffectiveness of the control process currently performed by a human operator.\nIn particular, we considered the problem of classifying the presence or absence\nof a transparent anticoagulant substance inside test tubes. Specifically, a\nhand-designed deep network model is used and compared with some\nstate-of-the-art models for its ability to categorize different images of vials\nthat can be either filled with the anticoagulant or empty. Collected results\nindicate that the proposed approach is competitive with state-of-the-art models\nin terms of accuracy. Furthermore, we increased the complexity of the task by\ntraining the models on the ability to discriminate not only the presence or\nabsence of the anticoagulant inside the vial, but also the size of the test\ntube. The analysis performed in the latter scenario confirms the\ncompetitiveness of our approach. Moreover, our model is remarkably superior in\nterms of its generalization ability and requires significantly fewer resources.\nThese results suggest the possibility of successfully implementing such a model\nin the production process of a plastic consumables company.\n", "link": "http://arxiv.org/abs/2404.10454v1", "date": "2024-04-16", "relevancy": 0.9944, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Computer%20Vision-Based%20Quality%20Assessment%20Technique%20for%20the%20automatic%0A%20%20control%20of%20consumables%20for%20analytical%20laboratories&body=Title%3A%20A%20Computer%20Vision-Based%20Quality%20Assessment%20Technique%20for%20the%20automatic%0A%20%20control%20of%20consumables%20for%20analytical%20laboratories%0AAuthor%3A%20Meriam%20Zribi%20and%20Paolo%20Pagliuca%20and%20Francesca%20Pitolli%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20the%20Industry%204.0%20paradigm%20is%20increasing%20the%20pressure%20to%0Adevelop%20effective%20automated%20monitoring%20systems.%20Artificial%20Intelligence%20%28AI%29%20is%0Aa%20convenient%20tool%20to%20improve%20the%20efficiency%20of%20industrial%20processes%20while%0Areducing%20errors%20and%20waste.%20In%20fact%2C%20it%20allows%20the%20use%20of%20real-time%20data%20to%0Aincrease%20the%20effectiveness%20of%20monitoring%20systems%2C%20minimize%20errors%2C%20make%20the%0Aproduction%20process%20more%20sustainable%2C%20and%20save%20costs.%20In%20this%20paper%2C%20a%20novel%0Aautomatic%20monitoring%20system%20is%20proposed%20in%20the%20context%20of%20production%20process%20of%0Aplastic%20consumables%20used%20in%20analysis%20laboratories%2C%20with%20the%20aim%20to%20increase%20the%0Aeffectiveness%20of%20the%20control%20process%20currently%20performed%20by%20a%20human%20operator.%0AIn%20particular%2C%20we%20considered%20the%20problem%20of%20classifying%20the%20presence%20or%20absence%0Aof%20a%20transparent%20anticoagulant%20substance%20inside%20test%20tubes.%20Specifically%2C%20a%0Ahand-designed%20deep%20network%20model%20is%20used%20and%20compared%20with%20some%0Astate-of-the-art%20models%20for%20its%20ability%20to%20categorize%20different%20images%20of%20vials%0Athat%20can%20be%20either%20filled%20with%20the%20anticoagulant%20or%20empty.%20Collected%20results%0Aindicate%20that%20the%20proposed%20approach%20is%20competitive%20with%20state-of-the-art%20models%0Ain%20terms%20of%20accuracy.%20Furthermore%2C%20we%20increased%20the%20complexity%20of%20the%20task%20by%0Atraining%20the%20models%20on%20the%20ability%20to%20discriminate%20not%20only%20the%20presence%20or%0Aabsence%20of%20the%20anticoagulant%20inside%20the%20vial%2C%20but%20also%20the%20size%20of%20the%20test%0Atube.%20The%20analysis%20performed%20in%20the%20latter%20scenario%20confirms%20the%0Acompetitiveness%20of%20our%20approach.%20Moreover%2C%20our%20model%20is%20remarkably%20superior%20in%0Aterms%20of%20its%20generalization%20ability%20and%20requires%20significantly%20fewer%20resources.%0AThese%20results%20suggest%20the%20possibility%20of%20successfully%20implementing%20such%20a%20model%0Ain%20the%20production%20process%20of%20a%20plastic%20consumables%20company.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10454v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Computer%20Vision-Based%20Quality%20Assessment%20Technique%20for%20the%20automatic%0A%20%20control%20of%20consumables%20for%20analytical%20laboratories&entry.906535625=Meriam%20Zribi%20and%20Paolo%20Pagliuca%20and%20Francesca%20Pitolli&entry.1292438233=%20%20The%20rapid%20growth%20of%20the%20Industry%204.0%20paradigm%20is%20increasing%20the%20pressure%20to%0Adevelop%20effective%20automated%20monitoring%20systems.%20Artificial%20Intelligence%20%28AI%29%20is%0Aa%20convenient%20tool%20to%20improve%20the%20efficiency%20of%20industrial%20processes%20while%0Areducing%20errors%20and%20waste.%20In%20fact%2C%20it%20allows%20the%20use%20of%20real-time%20data%20to%0Aincrease%20the%20effectiveness%20of%20monitoring%20systems%2C%20minimize%20errors%2C%20make%20the%0Aproduction%20process%20more%20sustainable%2C%20and%20save%20costs.%20In%20this%20paper%2C%20a%20novel%0Aautomatic%20monitoring%20system%20is%20proposed%20in%20the%20context%20of%20production%20process%20of%0Aplastic%20consumables%20used%20in%20analysis%20laboratories%2C%20with%20the%20aim%20to%20increase%20the%0Aeffectiveness%20of%20the%20control%20process%20currently%20performed%20by%20a%20human%20operator.%0AIn%20particular%2C%20we%20considered%20the%20problem%20of%20classifying%20the%20presence%20or%20absence%0Aof%20a%20transparent%20anticoagulant%20substance%20inside%20test%20tubes.%20Specifically%2C%20a%0Ahand-designed%20deep%20network%20model%20is%20used%20and%20compared%20with%20some%0Astate-of-the-art%20models%20for%20its%20ability%20to%20categorize%20different%20images%20of%20vials%0Athat%20can%20be%20either%20filled%20with%20the%20anticoagulant%20or%20empty.%20Collected%20results%0Aindicate%20that%20the%20proposed%20approach%20is%20competitive%20with%20state-of-the-art%20models%0Ain%20terms%20of%20accuracy.%20Furthermore%2C%20we%20increased%20the%20complexity%20of%20the%20task%20by%0Atraining%20the%20models%20on%20the%20ability%20to%20discriminate%20not%20only%20the%20presence%20or%0Aabsence%20of%20the%20anticoagulant%20inside%20the%20vial%2C%20but%20also%20the%20size%20of%20the%20test%0Atube.%20The%20analysis%20performed%20in%20the%20latter%20scenario%20confirms%20the%0Acompetitiveness%20of%20our%20approach.%20Moreover%2C%20our%20model%20is%20remarkably%20superior%20in%0Aterms%20of%20its%20generalization%20ability%20and%20requires%20significantly%20fewer%20resources.%0AThese%20results%20suggest%20the%20possibility%20of%20successfully%20implementing%20such%20a%20model%0Ain%20the%20production%20process%20of%20a%20plastic%20consumables%20company.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10454v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


