<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241020.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera", "author": "Jian Huang and Chengrui Dong and Peidong Liu", "abstract": "  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n", "link": "http://arxiv.org/abs/2410.08107v2", "date": "2024-10-18", "relevancy": 3.4506, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7431}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.678}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera&body=Title%3A%20IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera%0AAuthor%3A%20Jian%20Huang%20and%20Chengrui%20Dong%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20and%20explicit%203D%20Gaussian%20Splatting%20%283D-GS%29%20for%0Anovel%20view%20synthesis%20have%20achieved%20remarkable%20progress%20with%20frame-based%20camera%0A%28e.g.%20RGB%20and%20RGB-D%20cameras%29%20recently.%20Compared%20to%20frame-based%20camera%2C%20a%20novel%0Atype%20of%20bio-inspired%20visual%20sensor%2C%20i.e.%20event%20camera%2C%20has%20demonstrated%0Aadvantages%20in%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20low%20power%0Aconsumption%20and%20low%20latency.%20Due%20to%20its%20unique%20asynchronous%20and%20irregular%20data%0Acapturing%20process%2C%20limited%20work%20has%20been%20proposed%20to%20apply%20neural%0Arepresentation%20or%203D%20Gaussian%20splatting%20for%20an%20event%20camera.%20In%20this%20work%2C%20we%0Apresent%20IncEventGS%2C%20an%20incremental%203D%20Gaussian%20Splatting%20reconstruction%0Aalgorithm%20with%20a%20single%20event%20camera.%20To%20recover%20the%203D%20scene%20representation%0Aincrementally%2C%20we%20exploit%20the%20tracking%20and%20mapping%20paradigm%20of%20conventional%0ASLAM%20pipelines%20for%20IncEventGS.%20Given%20the%20incoming%20event%20stream%2C%20the%20tracker%0Afirstly%20estimates%20an%20initial%20camera%20motion%20based%20on%20prior%20reconstructed%203D-GS%0Ascene%20representation.%20The%20mapper%20then%20jointly%20refines%20both%20the%203D%20scene%0Arepresentation%20and%20camera%20motion%20based%20on%20the%20previously%20estimated%20motion%0Atrajectory%20from%20the%20tracker.%20The%20experimental%20results%20demonstrate%20that%0AIncEventGS%20delivers%20superior%20performance%20compared%20to%20prior%20NeRF-based%20methods%0Aand%20other%20related%20baselines%2C%20even%20we%20do%20not%20have%20the%20ground-truth%20camera%20poses.%0AFurthermore%2C%20our%20method%20can%20also%20deliver%20better%20performance%20compared%20to%0Astate-of-the-art%20event%20visual%20odometry%20methods%20in%20terms%20of%20camera%20motion%0Aestimation.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/wu-cvgl/IncEventGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncEventGS%253A%2520Pose-Free%2520Gaussian%2520Splatting%2520from%2520a%2520Single%2520Event%2520Camera%26entry.906535625%3DJian%2520Huang%2520and%2520Chengrui%2520Dong%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representation%2520and%2520explicit%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520for%250Anovel%2520view%2520synthesis%2520have%2520achieved%2520remarkable%2520progress%2520with%2520frame-based%2520camera%250A%2528e.g.%2520RGB%2520and%2520RGB-D%2520cameras%2529%2520recently.%2520Compared%2520to%2520frame-based%2520camera%252C%2520a%2520novel%250Atype%2520of%2520bio-inspired%2520visual%2520sensor%252C%2520i.e.%2520event%2520camera%252C%2520has%2520demonstrated%250Aadvantages%2520in%2520high%2520temporal%2520resolution%252C%2520high%2520dynamic%2520range%252C%2520low%2520power%250Aconsumption%2520and%2520low%2520latency.%2520Due%2520to%2520its%2520unique%2520asynchronous%2520and%2520irregular%2520data%250Acapturing%2520process%252C%2520limited%2520work%2520has%2520been%2520proposed%2520to%2520apply%2520neural%250Arepresentation%2520or%25203D%2520Gaussian%2520splatting%2520for%2520an%2520event%2520camera.%2520In%2520this%2520work%252C%2520we%250Apresent%2520IncEventGS%252C%2520an%2520incremental%25203D%2520Gaussian%2520Splatting%2520reconstruction%250Aalgorithm%2520with%2520a%2520single%2520event%2520camera.%2520To%2520recover%2520the%25203D%2520scene%2520representation%250Aincrementally%252C%2520we%2520exploit%2520the%2520tracking%2520and%2520mapping%2520paradigm%2520of%2520conventional%250ASLAM%2520pipelines%2520for%2520IncEventGS.%2520Given%2520the%2520incoming%2520event%2520stream%252C%2520the%2520tracker%250Afirstly%2520estimates%2520an%2520initial%2520camera%2520motion%2520based%2520on%2520prior%2520reconstructed%25203D-GS%250Ascene%2520representation.%2520The%2520mapper%2520then%2520jointly%2520refines%2520both%2520the%25203D%2520scene%250Arepresentation%2520and%2520camera%2520motion%2520based%2520on%2520the%2520previously%2520estimated%2520motion%250Atrajectory%2520from%2520the%2520tracker.%2520The%2520experimental%2520results%2520demonstrate%2520that%250AIncEventGS%2520delivers%2520superior%2520performance%2520compared%2520to%2520prior%2520NeRF-based%2520methods%250Aand%2520other%2520related%2520baselines%252C%2520even%2520we%2520do%2520not%2520have%2520the%2520ground-truth%2520camera%2520poses.%250AFurthermore%252C%2520our%2520method%2520can%2520also%2520deliver%2520better%2520performance%2520compared%2520to%250Astate-of-the-art%2520event%2520visual%2520odometry%2520methods%2520in%2520terms%2520of%2520camera%2520motion%250Aestimation.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/wu-cvgl/IncEventGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IncEventGS%3A%20Pose-Free%20Gaussian%20Splatting%20from%20a%20Single%20Event%20Camera&entry.906535625=Jian%20Huang%20and%20Chengrui%20Dong%20and%20Peidong%20Liu&entry.1292438233=%20%20Implicit%20neural%20representation%20and%20explicit%203D%20Gaussian%20Splatting%20%283D-GS%29%20for%0Anovel%20view%20synthesis%20have%20achieved%20remarkable%20progress%20with%20frame-based%20camera%0A%28e.g.%20RGB%20and%20RGB-D%20cameras%29%20recently.%20Compared%20to%20frame-based%20camera%2C%20a%20novel%0Atype%20of%20bio-inspired%20visual%20sensor%2C%20i.e.%20event%20camera%2C%20has%20demonstrated%0Aadvantages%20in%20high%20temporal%20resolution%2C%20high%20dynamic%20range%2C%20low%20power%0Aconsumption%20and%20low%20latency.%20Due%20to%20its%20unique%20asynchronous%20and%20irregular%20data%0Acapturing%20process%2C%20limited%20work%20has%20been%20proposed%20to%20apply%20neural%0Arepresentation%20or%203D%20Gaussian%20splatting%20for%20an%20event%20camera.%20In%20this%20work%2C%20we%0Apresent%20IncEventGS%2C%20an%20incremental%203D%20Gaussian%20Splatting%20reconstruction%0Aalgorithm%20with%20a%20single%20event%20camera.%20To%20recover%20the%203D%20scene%20representation%0Aincrementally%2C%20we%20exploit%20the%20tracking%20and%20mapping%20paradigm%20of%20conventional%0ASLAM%20pipelines%20for%20IncEventGS.%20Given%20the%20incoming%20event%20stream%2C%20the%20tracker%0Afirstly%20estimates%20an%20initial%20camera%20motion%20based%20on%20prior%20reconstructed%203D-GS%0Ascene%20representation.%20The%20mapper%20then%20jointly%20refines%20both%20the%203D%20scene%0Arepresentation%20and%20camera%20motion%20based%20on%20the%20previously%20estimated%20motion%0Atrajectory%20from%20the%20tracker.%20The%20experimental%20results%20demonstrate%20that%0AIncEventGS%20delivers%20superior%20performance%20compared%20to%20prior%20NeRF-based%20methods%0Aand%20other%20related%20baselines%2C%20even%20we%20do%20not%20have%20the%20ground-truth%20camera%20poses.%0AFurthermore%2C%20our%20method%20can%20also%20deliver%20better%20performance%20compared%20to%0Astate-of-the-art%20event%20visual%20odometry%20methods%20in%20terms%20of%20camera%20motion%0Aestimation.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/wu-cvgl/IncEventGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08107v2&entry.124074799=Read"},
{"title": "CLIP-VAD: Exploiting Vision-Language Models for Voice Activity Detection", "author": "Andrea Appiani and Cigdem Beyan", "abstract": "  Voice Activity Detection (VAD) is the process of automatically determining\nwhether a person is speaking and identifying the timing of their speech in an\naudiovisual data. Traditionally, this task has been tackled by processing\neither audio signals or visual data, or by combining both modalities through\nfusion or joint learning. In our study, drawing inspiration from recent\nadvancements in visual-language models, we introduce a novel approach\nleveraging Contrastive Language-Image Pretraining (CLIP) models. The CLIP\nvisual encoder analyzes video segments composed of the upper body of an\nindividual, while the text encoder handles textual descriptions automatically\ngenerated through prompt engineering. Subsequently, embeddings from these\nencoders are fused through a deep neural network to perform VAD. Our\nexperimental analysis across three VAD benchmarks showcases the superior\nperformance of our method compared to existing visual VAD approaches. Notably,\nour approach outperforms several audio-visual methods despite its simplicity,\nand without requiring pre-training on extensive audio-visual datasets.\n", "link": "http://arxiv.org/abs/2410.14509v1", "date": "2024-10-18", "relevancy": 3.0025, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-VAD%3A%20Exploiting%20Vision-Language%20Models%20for%20Voice%20Activity%20Detection&body=Title%3A%20CLIP-VAD%3A%20Exploiting%20Vision-Language%20Models%20for%20Voice%20Activity%20Detection%0AAuthor%3A%20Andrea%20Appiani%20and%20Cigdem%20Beyan%0AAbstract%3A%20%20%20Voice%20Activity%20Detection%20%28VAD%29%20is%20the%20process%20of%20automatically%20determining%0Awhether%20a%20person%20is%20speaking%20and%20identifying%20the%20timing%20of%20their%20speech%20in%20an%0Aaudiovisual%20data.%20Traditionally%2C%20this%20task%20has%20been%20tackled%20by%20processing%0Aeither%20audio%20signals%20or%20visual%20data%2C%20or%20by%20combining%20both%20modalities%20through%0Afusion%20or%20joint%20learning.%20In%20our%20study%2C%20drawing%20inspiration%20from%20recent%0Aadvancements%20in%20visual-language%20models%2C%20we%20introduce%20a%20novel%20approach%0Aleveraging%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20models.%20The%20CLIP%0Avisual%20encoder%20analyzes%20video%20segments%20composed%20of%20the%20upper%20body%20of%20an%0Aindividual%2C%20while%20the%20text%20encoder%20handles%20textual%20descriptions%20automatically%0Agenerated%20through%20prompt%20engineering.%20Subsequently%2C%20embeddings%20from%20these%0Aencoders%20are%20fused%20through%20a%20deep%20neural%20network%20to%20perform%20VAD.%20Our%0Aexperimental%20analysis%20across%20three%20VAD%20benchmarks%20showcases%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20existing%20visual%20VAD%20approaches.%20Notably%2C%0Aour%20approach%20outperforms%20several%20audio-visual%20methods%20despite%20its%20simplicity%2C%0Aand%20without%20requiring%20pre-training%20on%20extensive%20audio-visual%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-VAD%253A%2520Exploiting%2520Vision-Language%2520Models%2520for%2520Voice%2520Activity%2520Detection%26entry.906535625%3DAndrea%2520Appiani%2520and%2520Cigdem%2520Beyan%26entry.1292438233%3D%2520%2520Voice%2520Activity%2520Detection%2520%2528VAD%2529%2520is%2520the%2520process%2520of%2520automatically%2520determining%250Awhether%2520a%2520person%2520is%2520speaking%2520and%2520identifying%2520the%2520timing%2520of%2520their%2520speech%2520in%2520an%250Aaudiovisual%2520data.%2520Traditionally%252C%2520this%2520task%2520has%2520been%2520tackled%2520by%2520processing%250Aeither%2520audio%2520signals%2520or%2520visual%2520data%252C%2520or%2520by%2520combining%2520both%2520modalities%2520through%250Afusion%2520or%2520joint%2520learning.%2520In%2520our%2520study%252C%2520drawing%2520inspiration%2520from%2520recent%250Aadvancements%2520in%2520visual-language%2520models%252C%2520we%2520introduce%2520a%2520novel%2520approach%250Aleveraging%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520models.%2520The%2520CLIP%250Avisual%2520encoder%2520analyzes%2520video%2520segments%2520composed%2520of%2520the%2520upper%2520body%2520of%2520an%250Aindividual%252C%2520while%2520the%2520text%2520encoder%2520handles%2520textual%2520descriptions%2520automatically%250Agenerated%2520through%2520prompt%2520engineering.%2520Subsequently%252C%2520embeddings%2520from%2520these%250Aencoders%2520are%2520fused%2520through%2520a%2520deep%2520neural%2520network%2520to%2520perform%2520VAD.%2520Our%250Aexperimental%2520analysis%2520across%2520three%2520VAD%2520benchmarks%2520showcases%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520compared%2520to%2520existing%2520visual%2520VAD%2520approaches.%2520Notably%252C%250Aour%2520approach%2520outperforms%2520several%2520audio-visual%2520methods%2520despite%2520its%2520simplicity%252C%250Aand%2520without%2520requiring%2520pre-training%2520on%2520extensive%2520audio-visual%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-VAD%3A%20Exploiting%20Vision-Language%20Models%20for%20Voice%20Activity%20Detection&entry.906535625=Andrea%20Appiani%20and%20Cigdem%20Beyan&entry.1292438233=%20%20Voice%20Activity%20Detection%20%28VAD%29%20is%20the%20process%20of%20automatically%20determining%0Awhether%20a%20person%20is%20speaking%20and%20identifying%20the%20timing%20of%20their%20speech%20in%20an%0Aaudiovisual%20data.%20Traditionally%2C%20this%20task%20has%20been%20tackled%20by%20processing%0Aeither%20audio%20signals%20or%20visual%20data%2C%20or%20by%20combining%20both%20modalities%20through%0Afusion%20or%20joint%20learning.%20In%20our%20study%2C%20drawing%20inspiration%20from%20recent%0Aadvancements%20in%20visual-language%20models%2C%20we%20introduce%20a%20novel%20approach%0Aleveraging%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20models.%20The%20CLIP%0Avisual%20encoder%20analyzes%20video%20segments%20composed%20of%20the%20upper%20body%20of%20an%0Aindividual%2C%20while%20the%20text%20encoder%20handles%20textual%20descriptions%20automatically%0Agenerated%20through%20prompt%20engineering.%20Subsequently%2C%20embeddings%20from%20these%0Aencoders%20are%20fused%20through%20a%20deep%20neural%20network%20to%20perform%20VAD.%20Our%0Aexperimental%20analysis%20across%20three%20VAD%20benchmarks%20showcases%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20existing%20visual%20VAD%20approaches.%20Notably%2C%0Aour%20approach%20outperforms%20several%20audio-visual%20methods%20despite%20its%20simplicity%2C%0Aand%20without%20requiring%20pre-training%20on%20extensive%20audio-visual%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14509v1&entry.124074799=Read"},
{"title": "Fundus to Fluorescein Angiography Video Generation as a Retinal\n  Generative Foundation Model", "author": "Weiyi Zhang and Jiancheng Yang and Ruoyu Chen and Siyu Huang and Pusheng Xu and Xiaolan Chen and Shanfu Lu and Hongyu Cao and Mingguang He and Danli Shi", "abstract": "  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring\nretinal vascular issues but is limited by its invasive nature and restricted\naccessibility compared to color fundus (CF) imaging. Existing methods that\nconvert CF images to FFA are confined to static image generation, missing the\ndynamic lesional changes. We introduce Fundus2Video, an autoregressive\ngenerative adversarial network (GAN) model that generates dynamic FFA videos\nfrom single CF images. Fundus2Video excels in video generation, achieving an\nFVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the\nfidelity of the generated videos. Additionally, the model's generator\ndemonstrates remarkable downstream transferability across ten external public\ndatasets, including blood vessel segmentation, retinal disease diagnosis,\nsystemic disease prediction, and multimodal retrieval, showcasing impressive\nzero-shot and few-shot capabilities. These findings position Fundus2Video as a\npowerful, non-invasive alternative to FFA exams and a versatile retinal\ngenerative foundation model that captures both static and temporal retinal\nfeatures, enabling the representation of complex inter-modality relationships.\n", "link": "http://arxiv.org/abs/2410.13242v2", "date": "2024-10-18", "relevancy": 2.9796, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6242}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5956}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fundus%20to%20Fluorescein%20Angiography%20Video%20Generation%20as%20a%20Retinal%0A%20%20Generative%20Foundation%20Model&body=Title%3A%20Fundus%20to%20Fluorescein%20Angiography%20Video%20Generation%20as%20a%20Retinal%0A%20%20Generative%20Foundation%20Model%0AAuthor%3A%20Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Ruoyu%20Chen%20and%20Siyu%20Huang%20and%20Pusheng%20Xu%20and%20Xiaolan%20Chen%20and%20Shanfu%20Lu%20and%20Hongyu%20Cao%20and%20Mingguang%20He%20and%20Danli%20Shi%0AAbstract%3A%20%20%20Fundus%20fluorescein%20angiography%20%28FFA%29%20is%20crucial%20for%20diagnosing%20and%20monitoring%0Aretinal%20vascular%20issues%20but%20is%20limited%20by%20its%20invasive%20nature%20and%20restricted%0Aaccessibility%20compared%20to%20color%20fundus%20%28CF%29%20imaging.%20Existing%20methods%20that%0Aconvert%20CF%20images%20to%20FFA%20are%20confined%20to%20static%20image%20generation%2C%20missing%20the%0Adynamic%20lesional%20changes.%20We%20introduce%20Fundus2Video%2C%20an%20autoregressive%0Agenerative%20adversarial%20network%20%28GAN%29%20model%20that%20generates%20dynamic%20FFA%20videos%0Afrom%20single%20CF%20images.%20Fundus2Video%20excels%20in%20video%20generation%2C%20achieving%20an%0AFVD%20of%201497.12%20and%20a%20PSNR%20of%2011.77.%20Clinical%20experts%20have%20validated%20the%0Afidelity%20of%20the%20generated%20videos.%20Additionally%2C%20the%20model%27s%20generator%0Ademonstrates%20remarkable%20downstream%20transferability%20across%20ten%20external%20public%0Adatasets%2C%20including%20blood%20vessel%20segmentation%2C%20retinal%20disease%20diagnosis%2C%0Asystemic%20disease%20prediction%2C%20and%20multimodal%20retrieval%2C%20showcasing%20impressive%0Azero-shot%20and%20few-shot%20capabilities.%20These%20findings%20position%20Fundus2Video%20as%20a%0Apowerful%2C%20non-invasive%20alternative%20to%20FFA%20exams%20and%20a%20versatile%20retinal%0Agenerative%20foundation%20model%20that%20captures%20both%20static%20and%20temporal%20retinal%0Afeatures%2C%20enabling%20the%20representation%20of%20complex%20inter-modality%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFundus%2520to%2520Fluorescein%2520Angiography%2520Video%2520Generation%2520as%2520a%2520Retinal%250A%2520%2520Generative%2520Foundation%2520Model%26entry.906535625%3DWeiyi%2520Zhang%2520and%2520Jiancheng%2520Yang%2520and%2520Ruoyu%2520Chen%2520and%2520Siyu%2520Huang%2520and%2520Pusheng%2520Xu%2520and%2520Xiaolan%2520Chen%2520and%2520Shanfu%2520Lu%2520and%2520Hongyu%2520Cao%2520and%2520Mingguang%2520He%2520and%2520Danli%2520Shi%26entry.1292438233%3D%2520%2520Fundus%2520fluorescein%2520angiography%2520%2528FFA%2529%2520is%2520crucial%2520for%2520diagnosing%2520and%2520monitoring%250Aretinal%2520vascular%2520issues%2520but%2520is%2520limited%2520by%2520its%2520invasive%2520nature%2520and%2520restricted%250Aaccessibility%2520compared%2520to%2520color%2520fundus%2520%2528CF%2529%2520imaging.%2520Existing%2520methods%2520that%250Aconvert%2520CF%2520images%2520to%2520FFA%2520are%2520confined%2520to%2520static%2520image%2520generation%252C%2520missing%2520the%250Adynamic%2520lesional%2520changes.%2520We%2520introduce%2520Fundus2Video%252C%2520an%2520autoregressive%250Agenerative%2520adversarial%2520network%2520%2528GAN%2529%2520model%2520that%2520generates%2520dynamic%2520FFA%2520videos%250Afrom%2520single%2520CF%2520images.%2520Fundus2Video%2520excels%2520in%2520video%2520generation%252C%2520achieving%2520an%250AFVD%2520of%25201497.12%2520and%2520a%2520PSNR%2520of%252011.77.%2520Clinical%2520experts%2520have%2520validated%2520the%250Afidelity%2520of%2520the%2520generated%2520videos.%2520Additionally%252C%2520the%2520model%2527s%2520generator%250Ademonstrates%2520remarkable%2520downstream%2520transferability%2520across%2520ten%2520external%2520public%250Adatasets%252C%2520including%2520blood%2520vessel%2520segmentation%252C%2520retinal%2520disease%2520diagnosis%252C%250Asystemic%2520disease%2520prediction%252C%2520and%2520multimodal%2520retrieval%252C%2520showcasing%2520impressive%250Azero-shot%2520and%2520few-shot%2520capabilities.%2520These%2520findings%2520position%2520Fundus2Video%2520as%2520a%250Apowerful%252C%2520non-invasive%2520alternative%2520to%2520FFA%2520exams%2520and%2520a%2520versatile%2520retinal%250Agenerative%2520foundation%2520model%2520that%2520captures%2520both%2520static%2520and%2520temporal%2520retinal%250Afeatures%252C%2520enabling%2520the%2520representation%2520of%2520complex%2520inter-modality%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fundus%20to%20Fluorescein%20Angiography%20Video%20Generation%20as%20a%20Retinal%0A%20%20Generative%20Foundation%20Model&entry.906535625=Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Ruoyu%20Chen%20and%20Siyu%20Huang%20and%20Pusheng%20Xu%20and%20Xiaolan%20Chen%20and%20Shanfu%20Lu%20and%20Hongyu%20Cao%20and%20Mingguang%20He%20and%20Danli%20Shi&entry.1292438233=%20%20Fundus%20fluorescein%20angiography%20%28FFA%29%20is%20crucial%20for%20diagnosing%20and%20monitoring%0Aretinal%20vascular%20issues%20but%20is%20limited%20by%20its%20invasive%20nature%20and%20restricted%0Aaccessibility%20compared%20to%20color%20fundus%20%28CF%29%20imaging.%20Existing%20methods%20that%0Aconvert%20CF%20images%20to%20FFA%20are%20confined%20to%20static%20image%20generation%2C%20missing%20the%0Adynamic%20lesional%20changes.%20We%20introduce%20Fundus2Video%2C%20an%20autoregressive%0Agenerative%20adversarial%20network%20%28GAN%29%20model%20that%20generates%20dynamic%20FFA%20videos%0Afrom%20single%20CF%20images.%20Fundus2Video%20excels%20in%20video%20generation%2C%20achieving%20an%0AFVD%20of%201497.12%20and%20a%20PSNR%20of%2011.77.%20Clinical%20experts%20have%20validated%20the%0Afidelity%20of%20the%20generated%20videos.%20Additionally%2C%20the%20model%27s%20generator%0Ademonstrates%20remarkable%20downstream%20transferability%20across%20ten%20external%20public%0Adatasets%2C%20including%20blood%20vessel%20segmentation%2C%20retinal%20disease%20diagnosis%2C%0Asystemic%20disease%20prediction%2C%20and%20multimodal%20retrieval%2C%20showcasing%20impressive%0Azero-shot%20and%20few-shot%20capabilities.%20These%20findings%20position%20Fundus2Video%20as%20a%0Apowerful%2C%20non-invasive%20alternative%20to%20FFA%20exams%20and%20a%20versatile%20retinal%0Agenerative%20foundation%20model%20that%20captures%20both%20static%20and%20temporal%20retinal%0Afeatures%2C%20enabling%20the%20representation%20of%20complex%20inter-modality%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13242v2&entry.124074799=Read"},
{"title": "Graph Optimality-Aware Stochastic LiDAR Bundle Adjustment with\n  Progressive Spatial Smoothing", "author": "Jianping Li and Thien-Minh Nguyen and Muqing Cao and Shenghai Yuan and Tzu-Yi Hung and Lihua Xie", "abstract": "  Large-scale LiDAR Bundle Adjustment (LBA) for refining sensor orientation and\npoint cloud accuracy simultaneously is a fundamental task in photogrammetry and\nrobotics, particularly as low-cost 3D sensors are increasingly used for 3D\nmapping in complex scenes. Unlike pose-graph-based methods that rely solely on\npairwise relationships between LiDAR frames, LBA leverages raw LiDAR\ncorrespondences to achieve more precise results, especially when initial pose\nestimates are unreliable for low-cost sensors. However, existing LBA methods\nface challenges such as simplistic planar correspondences, extensive\nobservations, and dense normal matrices in the least-squares problem, which\nlimit robustness, efficiency, and scalability. To address these issues, we\npropose a Graph Optimality-aware Stochastic Optimization scheme with\nProgressive Spatial Smoothing, namely PSS-GOSO, to achieve \\textit{robust},\n\\textit{efficient}, and \\textit{scalable} LBA. The Progressive Spatial\nSmoothing (PSS) module extracts \\textit{robust} LiDAR feature association\nexploiting the prior structure information obtained by the polynomial smooth\nkernel. The Graph Optimality-aware Stochastic Optimization (GOSO) module first\nsparsifies the graph according to optimality for an \\textit{efficient}\noptimization. GOSO then utilizes stochastic clustering and graph\nmarginalization to solve the large-scale state estimation problem for a\n\\textit{scalable} LBA. We validate PSS-GOSO across diverse scenes captured by\nvarious platforms, demonstrating its superior performance compared to existing\nmethods.\n", "link": "http://arxiv.org/abs/2410.14565v1", "date": "2024-10-18", "relevancy": 2.9301, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5981}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Optimality-Aware%20Stochastic%20LiDAR%20Bundle%20Adjustment%20with%0A%20%20Progressive%20Spatial%20Smoothing&body=Title%3A%20Graph%20Optimality-Aware%20Stochastic%20LiDAR%20Bundle%20Adjustment%20with%0A%20%20Progressive%20Spatial%20Smoothing%0AAuthor%3A%20Jianping%20Li%20and%20Thien-Minh%20Nguyen%20and%20Muqing%20Cao%20and%20Shenghai%20Yuan%20and%20Tzu-Yi%20Hung%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Large-scale%20LiDAR%20Bundle%20Adjustment%20%28LBA%29%20for%20refining%20sensor%20orientation%20and%0Apoint%20cloud%20accuracy%20simultaneously%20is%20a%20fundamental%20task%20in%20photogrammetry%20and%0Arobotics%2C%20particularly%20as%20low-cost%203D%20sensors%20are%20increasingly%20used%20for%203D%0Amapping%20in%20complex%20scenes.%20Unlike%20pose-graph-based%20methods%20that%20rely%20solely%20on%0Apairwise%20relationships%20between%20LiDAR%20frames%2C%20LBA%20leverages%20raw%20LiDAR%0Acorrespondences%20to%20achieve%20more%20precise%20results%2C%20especially%20when%20initial%20pose%0Aestimates%20are%20unreliable%20for%20low-cost%20sensors.%20However%2C%20existing%20LBA%20methods%0Aface%20challenges%20such%20as%20simplistic%20planar%20correspondences%2C%20extensive%0Aobservations%2C%20and%20dense%20normal%20matrices%20in%20the%20least-squares%20problem%2C%20which%0Alimit%20robustness%2C%20efficiency%2C%20and%20scalability.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Graph%20Optimality-aware%20Stochastic%20Optimization%20scheme%20with%0AProgressive%20Spatial%20Smoothing%2C%20namely%20PSS-GOSO%2C%20to%20achieve%20%5Ctextit%7Brobust%7D%2C%0A%5Ctextit%7Befficient%7D%2C%20and%20%5Ctextit%7Bscalable%7D%20LBA.%20The%20Progressive%20Spatial%0ASmoothing%20%28PSS%29%20module%20extracts%20%5Ctextit%7Brobust%7D%20LiDAR%20feature%20association%0Aexploiting%20the%20prior%20structure%20information%20obtained%20by%20the%20polynomial%20smooth%0Akernel.%20The%20Graph%20Optimality-aware%20Stochastic%20Optimization%20%28GOSO%29%20module%20first%0Asparsifies%20the%20graph%20according%20to%20optimality%20for%20an%20%5Ctextit%7Befficient%7D%0Aoptimization.%20GOSO%20then%20utilizes%20stochastic%20clustering%20and%20graph%0Amarginalization%20to%20solve%20the%20large-scale%20state%20estimation%20problem%20for%20a%0A%5Ctextit%7Bscalable%7D%20LBA.%20We%20validate%20PSS-GOSO%20across%20diverse%20scenes%20captured%20by%0Avarious%20platforms%2C%20demonstrating%20its%20superior%20performance%20compared%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Optimality-Aware%2520Stochastic%2520LiDAR%2520Bundle%2520Adjustment%2520with%250A%2520%2520Progressive%2520Spatial%2520Smoothing%26entry.906535625%3DJianping%2520Li%2520and%2520Thien-Minh%2520Nguyen%2520and%2520Muqing%2520Cao%2520and%2520Shenghai%2520Yuan%2520and%2520Tzu-Yi%2520Hung%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Large-scale%2520LiDAR%2520Bundle%2520Adjustment%2520%2528LBA%2529%2520for%2520refining%2520sensor%2520orientation%2520and%250Apoint%2520cloud%2520accuracy%2520simultaneously%2520is%2520a%2520fundamental%2520task%2520in%2520photogrammetry%2520and%250Arobotics%252C%2520particularly%2520as%2520low-cost%25203D%2520sensors%2520are%2520increasingly%2520used%2520for%25203D%250Amapping%2520in%2520complex%2520scenes.%2520Unlike%2520pose-graph-based%2520methods%2520that%2520rely%2520solely%2520on%250Apairwise%2520relationships%2520between%2520LiDAR%2520frames%252C%2520LBA%2520leverages%2520raw%2520LiDAR%250Acorrespondences%2520to%2520achieve%2520more%2520precise%2520results%252C%2520especially%2520when%2520initial%2520pose%250Aestimates%2520are%2520unreliable%2520for%2520low-cost%2520sensors.%2520However%252C%2520existing%2520LBA%2520methods%250Aface%2520challenges%2520such%2520as%2520simplistic%2520planar%2520correspondences%252C%2520extensive%250Aobservations%252C%2520and%2520dense%2520normal%2520matrices%2520in%2520the%2520least-squares%2520problem%252C%2520which%250Alimit%2520robustness%252C%2520efficiency%252C%2520and%2520scalability.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520Graph%2520Optimality-aware%2520Stochastic%2520Optimization%2520scheme%2520with%250AProgressive%2520Spatial%2520Smoothing%252C%2520namely%2520PSS-GOSO%252C%2520to%2520achieve%2520%255Ctextit%257Brobust%257D%252C%250A%255Ctextit%257Befficient%257D%252C%2520and%2520%255Ctextit%257Bscalable%257D%2520LBA.%2520The%2520Progressive%2520Spatial%250ASmoothing%2520%2528PSS%2529%2520module%2520extracts%2520%255Ctextit%257Brobust%257D%2520LiDAR%2520feature%2520association%250Aexploiting%2520the%2520prior%2520structure%2520information%2520obtained%2520by%2520the%2520polynomial%2520smooth%250Akernel.%2520The%2520Graph%2520Optimality-aware%2520Stochastic%2520Optimization%2520%2528GOSO%2529%2520module%2520first%250Asparsifies%2520the%2520graph%2520according%2520to%2520optimality%2520for%2520an%2520%255Ctextit%257Befficient%257D%250Aoptimization.%2520GOSO%2520then%2520utilizes%2520stochastic%2520clustering%2520and%2520graph%250Amarginalization%2520to%2520solve%2520the%2520large-scale%2520state%2520estimation%2520problem%2520for%2520a%250A%255Ctextit%257Bscalable%257D%2520LBA.%2520We%2520validate%2520PSS-GOSO%2520across%2520diverse%2520scenes%2520captured%2520by%250Avarious%2520platforms%252C%2520demonstrating%2520its%2520superior%2520performance%2520compared%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Optimality-Aware%20Stochastic%20LiDAR%20Bundle%20Adjustment%20with%0A%20%20Progressive%20Spatial%20Smoothing&entry.906535625=Jianping%20Li%20and%20Thien-Minh%20Nguyen%20and%20Muqing%20Cao%20and%20Shenghai%20Yuan%20and%20Tzu-Yi%20Hung%20and%20Lihua%20Xie&entry.1292438233=%20%20Large-scale%20LiDAR%20Bundle%20Adjustment%20%28LBA%29%20for%20refining%20sensor%20orientation%20and%0Apoint%20cloud%20accuracy%20simultaneously%20is%20a%20fundamental%20task%20in%20photogrammetry%20and%0Arobotics%2C%20particularly%20as%20low-cost%203D%20sensors%20are%20increasingly%20used%20for%203D%0Amapping%20in%20complex%20scenes.%20Unlike%20pose-graph-based%20methods%20that%20rely%20solely%20on%0Apairwise%20relationships%20between%20LiDAR%20frames%2C%20LBA%20leverages%20raw%20LiDAR%0Acorrespondences%20to%20achieve%20more%20precise%20results%2C%20especially%20when%20initial%20pose%0Aestimates%20are%20unreliable%20for%20low-cost%20sensors.%20However%2C%20existing%20LBA%20methods%0Aface%20challenges%20such%20as%20simplistic%20planar%20correspondences%2C%20extensive%0Aobservations%2C%20and%20dense%20normal%20matrices%20in%20the%20least-squares%20problem%2C%20which%0Alimit%20robustness%2C%20efficiency%2C%20and%20scalability.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Graph%20Optimality-aware%20Stochastic%20Optimization%20scheme%20with%0AProgressive%20Spatial%20Smoothing%2C%20namely%20PSS-GOSO%2C%20to%20achieve%20%5Ctextit%7Brobust%7D%2C%0A%5Ctextit%7Befficient%7D%2C%20and%20%5Ctextit%7Bscalable%7D%20LBA.%20The%20Progressive%20Spatial%0ASmoothing%20%28PSS%29%20module%20extracts%20%5Ctextit%7Brobust%7D%20LiDAR%20feature%20association%0Aexploiting%20the%20prior%20structure%20information%20obtained%20by%20the%20polynomial%20smooth%0Akernel.%20The%20Graph%20Optimality-aware%20Stochastic%20Optimization%20%28GOSO%29%20module%20first%0Asparsifies%20the%20graph%20according%20to%20optimality%20for%20an%20%5Ctextit%7Befficient%7D%0Aoptimization.%20GOSO%20then%20utilizes%20stochastic%20clustering%20and%20graph%0Amarginalization%20to%20solve%20the%20large-scale%20state%20estimation%20problem%20for%20a%0A%5Ctextit%7Bscalable%7D%20LBA.%20We%20validate%20PSS-GOSO%20across%20diverse%20scenes%20captured%20by%0Avarious%20platforms%2C%20demonstrating%20its%20superior%20performance%20compared%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14565v1&entry.124074799=Read"},
{"title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples", "author": "Baiqi Li and Zhiqiu Lin and Wenxuan Peng and Jean de Dieu Nyandwi and Daniel Jiang and Zixian Ma and Simran Khanuja and Ranjay Krishna and Graham Neubig and Deva Ramanan", "abstract": "  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n", "link": "http://arxiv.org/abs/2410.14669v1", "date": "2024-10-18", "relevancy": 2.8212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples&body=Title%3A%20NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples%0AAuthor%3A%20Baiqi%20Li%20and%20Zhiqiu%20Lin%20and%20Wenxuan%20Peng%20and%20Jean%20de%20Dieu%20Nyandwi%20and%20Daniel%20Jiang%20and%20Zixian%20Ma%20and%20Simran%20Khanuja%20and%20Ranjay%20Krishna%20and%20Graham%20Neubig%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%0Avisual-question-answering%20%28VQA%29%20benchmarks%20that%20evaluate%20complex%0Avisio-linguistic%20reasoning.%20However%2C%20are%20these%20models%20truly%20effective%3F%20In%20this%0Awork%2C%20we%20show%20that%20VLMs%20still%20struggle%20with%20natural%20images%20and%20questions%20that%0Ahumans%20can%20easily%20answer%2C%20which%20we%20term%20natural%20adversarial%20samples.%20We%20also%0Afind%20it%20surprisingly%20easy%20to%20generate%20these%20VQA%20samples%20from%20natural%20image-text%0Acorpora%20using%20off-the-shelf%20models%20like%20CLIP%20and%20ChatGPT.%20We%20propose%20a%0Asemi-automated%20approach%20to%20collect%20a%20new%20benchmark%2C%20NaturalBench%2C%20for%20reliably%0Aevaluating%20VLMs%20with%2010%2C000%20human-verified%20VQA%20samples.%20Crucially%2C%20we%20adopt%20a%0A%24%5Ctextbf%7Bvision-centric%7D%24%20design%20by%20pairing%20each%20question%20with%20two%20images%20that%0Ayield%20different%20answers%2C%20preventing%20blind%20solutions%20from%20answering%20without%0Ausing%20the%20images.%20This%20makes%20NaturalBench%20more%20challenging%20than%20previous%0Abenchmarks%20that%20can%20be%20solved%20with%20commonsense%20priors.%20We%20evaluate%2053%0Astate-of-the-art%20VLMs%20on%20NaturalBench%2C%20showing%20that%20models%20like%0ALLaVA-OneVision%2C%20Cambrian-1%2C%20Llama3.2-Vision%2C%20Molmo%2C%20Qwen2-VL%2C%20and%20even%20GPT-4o%0Alag%2050%25-70%25%20behind%20human%20performance%20%28over%2090%25%29.%20We%20analyze%20why%20NaturalBench%20is%0Ahard%20from%20two%20angles%3A%20%281%29%20Compositionality%3A%20Solving%20NaturalBench%20requires%0Adiverse%20visio-linguistic%20skills%2C%20including%20understanding%20attribute%20bindings%2C%0Aobject%20relationships%2C%20and%20advanced%20reasoning%20like%20logic%20and%20counting.%20To%20this%0Aend%2C%20unlike%20prior%20work%20that%20uses%20a%20single%20tag%20per%20sample%2C%20we%20tag%20each%0ANaturalBench%20sample%20with%201%20to%208%20skill%20tags%20for%20fine-grained%20evaluation.%20%282%29%0ABiases%3A%20NaturalBench%20exposes%20severe%20biases%20in%20VLMs%2C%20as%20models%20often%20choose%20the%0Asame%20answer%20regardless%20of%20the%20image.%20Lastly%2C%20we%20apply%20our%20benchmark%20curation%0Amethod%20to%20diverse%20data%20sources%2C%20including%20long%20captions%20%28over%20100%20words%29%20and%0Anon-English%20languages%20like%20Chinese%20and%20Hindi%2C%20highlighting%20its%20potential%20for%0Adynamic%20evaluations%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaturalBench%253A%2520Evaluating%2520Vision-Language%2520Models%2520on%2520Natural%2520Adversarial%250A%2520%2520Samples%26entry.906535625%3DBaiqi%2520Li%2520and%2520Zhiqiu%2520Lin%2520and%2520Wenxuan%2520Peng%2520and%2520Jean%2520de%2520Dieu%2520Nyandwi%2520and%2520Daniel%2520Jiang%2520and%2520Zixian%2520Ma%2520and%2520Simran%2520Khanuja%2520and%2520Ranjay%2520Krishna%2520and%2520Graham%2520Neubig%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520recent%250Avisual-question-answering%2520%2528VQA%2529%2520benchmarks%2520that%2520evaluate%2520complex%250Avisio-linguistic%2520reasoning.%2520However%252C%2520are%2520these%2520models%2520truly%2520effective%253F%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520VLMs%2520still%2520struggle%2520with%2520natural%2520images%2520and%2520questions%2520that%250Ahumans%2520can%2520easily%2520answer%252C%2520which%2520we%2520term%2520natural%2520adversarial%2520samples.%2520We%2520also%250Afind%2520it%2520surprisingly%2520easy%2520to%2520generate%2520these%2520VQA%2520samples%2520from%2520natural%2520image-text%250Acorpora%2520using%2520off-the-shelf%2520models%2520like%2520CLIP%2520and%2520ChatGPT.%2520We%2520propose%2520a%250Asemi-automated%2520approach%2520to%2520collect%2520a%2520new%2520benchmark%252C%2520NaturalBench%252C%2520for%2520reliably%250Aevaluating%2520VLMs%2520with%252010%252C000%2520human-verified%2520VQA%2520samples.%2520Crucially%252C%2520we%2520adopt%2520a%250A%2524%255Ctextbf%257Bvision-centric%257D%2524%2520design%2520by%2520pairing%2520each%2520question%2520with%2520two%2520images%2520that%250Ayield%2520different%2520answers%252C%2520preventing%2520blind%2520solutions%2520from%2520answering%2520without%250Ausing%2520the%2520images.%2520This%2520makes%2520NaturalBench%2520more%2520challenging%2520than%2520previous%250Abenchmarks%2520that%2520can%2520be%2520solved%2520with%2520commonsense%2520priors.%2520We%2520evaluate%252053%250Astate-of-the-art%2520VLMs%2520on%2520NaturalBench%252C%2520showing%2520that%2520models%2520like%250ALLaVA-OneVision%252C%2520Cambrian-1%252C%2520Llama3.2-Vision%252C%2520Molmo%252C%2520Qwen2-VL%252C%2520and%2520even%2520GPT-4o%250Alag%252050%2525-70%2525%2520behind%2520human%2520performance%2520%2528over%252090%2525%2529.%2520We%2520analyze%2520why%2520NaturalBench%2520is%250Ahard%2520from%2520two%2520angles%253A%2520%25281%2529%2520Compositionality%253A%2520Solving%2520NaturalBench%2520requires%250Adiverse%2520visio-linguistic%2520skills%252C%2520including%2520understanding%2520attribute%2520bindings%252C%250Aobject%2520relationships%252C%2520and%2520advanced%2520reasoning%2520like%2520logic%2520and%2520counting.%2520To%2520this%250Aend%252C%2520unlike%2520prior%2520work%2520that%2520uses%2520a%2520single%2520tag%2520per%2520sample%252C%2520we%2520tag%2520each%250ANaturalBench%2520sample%2520with%25201%2520to%25208%2520skill%2520tags%2520for%2520fine-grained%2520evaluation.%2520%25282%2529%250ABiases%253A%2520NaturalBench%2520exposes%2520severe%2520biases%2520in%2520VLMs%252C%2520as%2520models%2520often%2520choose%2520the%250Asame%2520answer%2520regardless%2520of%2520the%2520image.%2520Lastly%252C%2520we%2520apply%2520our%2520benchmark%2520curation%250Amethod%2520to%2520diverse%2520data%2520sources%252C%2520including%2520long%2520captions%2520%2528over%2520100%2520words%2529%2520and%250Anon-English%2520languages%2520like%2520Chinese%2520and%2520Hindi%252C%2520highlighting%2520its%2520potential%2520for%250Adynamic%2520evaluations%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaturalBench%3A%20Evaluating%20Vision-Language%20Models%20on%20Natural%20Adversarial%0A%20%20Samples&entry.906535625=Baiqi%20Li%20and%20Zhiqiu%20Lin%20and%20Wenxuan%20Peng%20and%20Jean%20de%20Dieu%20Nyandwi%20and%20Daniel%20Jiang%20and%20Zixian%20Ma%20and%20Simran%20Khanuja%20and%20Ranjay%20Krishna%20and%20Graham%20Neubig%20and%20Deva%20Ramanan&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20recent%0Avisual-question-answering%20%28VQA%29%20benchmarks%20that%20evaluate%20complex%0Avisio-linguistic%20reasoning.%20However%2C%20are%20these%20models%20truly%20effective%3F%20In%20this%0Awork%2C%20we%20show%20that%20VLMs%20still%20struggle%20with%20natural%20images%20and%20questions%20that%0Ahumans%20can%20easily%20answer%2C%20which%20we%20term%20natural%20adversarial%20samples.%20We%20also%0Afind%20it%20surprisingly%20easy%20to%20generate%20these%20VQA%20samples%20from%20natural%20image-text%0Acorpora%20using%20off-the-shelf%20models%20like%20CLIP%20and%20ChatGPT.%20We%20propose%20a%0Asemi-automated%20approach%20to%20collect%20a%20new%20benchmark%2C%20NaturalBench%2C%20for%20reliably%0Aevaluating%20VLMs%20with%2010%2C000%20human-verified%20VQA%20samples.%20Crucially%2C%20we%20adopt%20a%0A%24%5Ctextbf%7Bvision-centric%7D%24%20design%20by%20pairing%20each%20question%20with%20two%20images%20that%0Ayield%20different%20answers%2C%20preventing%20blind%20solutions%20from%20answering%20without%0Ausing%20the%20images.%20This%20makes%20NaturalBench%20more%20challenging%20than%20previous%0Abenchmarks%20that%20can%20be%20solved%20with%20commonsense%20priors.%20We%20evaluate%2053%0Astate-of-the-art%20VLMs%20on%20NaturalBench%2C%20showing%20that%20models%20like%0ALLaVA-OneVision%2C%20Cambrian-1%2C%20Llama3.2-Vision%2C%20Molmo%2C%20Qwen2-VL%2C%20and%20even%20GPT-4o%0Alag%2050%25-70%25%20behind%20human%20performance%20%28over%2090%25%29.%20We%20analyze%20why%20NaturalBench%20is%0Ahard%20from%20two%20angles%3A%20%281%29%20Compositionality%3A%20Solving%20NaturalBench%20requires%0Adiverse%20visio-linguistic%20skills%2C%20including%20understanding%20attribute%20bindings%2C%0Aobject%20relationships%2C%20and%20advanced%20reasoning%20like%20logic%20and%20counting.%20To%20this%0Aend%2C%20unlike%20prior%20work%20that%20uses%20a%20single%20tag%20per%20sample%2C%20we%20tag%20each%0ANaturalBench%20sample%20with%201%20to%208%20skill%20tags%20for%20fine-grained%20evaluation.%20%282%29%0ABiases%3A%20NaturalBench%20exposes%20severe%20biases%20in%20VLMs%2C%20as%20models%20often%20choose%20the%0Asame%20answer%20regardless%20of%20the%20image.%20Lastly%2C%20we%20apply%20our%20benchmark%20curation%0Amethod%20to%20diverse%20data%20sources%2C%20including%20long%20captions%20%28over%20100%20words%29%20and%0Anon-English%20languages%20like%20Chinese%20and%20Hindi%2C%20highlighting%20its%20potential%20for%0Adynamic%20evaluations%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14669v1&entry.124074799=Read"},
{"title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis", "author": "Alexander Mai and Peter Hedman and George Kopanas and Dor Verbin and David Futschik and Qiangeng Xu and Falko Kuester and Jonathan T. Barron and Yinda Zhang", "abstract": "  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n", "link": "http://arxiv.org/abs/2410.01804v4", "date": "2024-10-18", "relevancy": 2.7838, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&body=Title%3A%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis%0AAuthor%3A%20Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron%20and%20Yinda%20Zhang%0AAbstract%3A%20%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01804v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVER%253A%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520for%2520Real-time%2520View%2520Synthesis%26entry.906535625%3DAlexander%2520Mai%2520and%2520Peter%2520Hedman%2520and%2520George%2520Kopanas%2520and%2520Dor%2520Verbin%2520and%2520David%2520Futschik%2520and%2520Qiangeng%2520Xu%2520and%2520Falko%2520Kuester%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Yinda%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520%2528EVER%2529%252C%2520a%2520method%2520for%250Areal-time%2520differentiable%2520emission-only%2520volume%2520rendering.%2520Unlike%2520recent%250Arasterization%2520based%2520approach%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520our%2520primitive%250Abased%2520representation%2520allows%2520for%2520exact%2520volume%2520rendering%252C%2520rather%2520than%2520alpha%250Acompositing%25203D%2520Gaussian%2520billboards.%2520As%2520such%252C%2520unlike%25203DGS%2520our%2520formulation%2520does%250Anot%2520suffer%2520from%2520popping%2520artifacts%2520and%2520view%2520dependent%2520density%252C%2520but%2520still%250Aachieves%2520frame%2520rates%2520of%2520%2524%255Csim%255C%252130%2524%2520FPS%2520at%2520720p%2520on%2520an%2520NVIDIA%2520RTX4090.%2520Since%2520our%250Aapproach%2520is%2520built%2520upon%2520ray%2520tracing%2520it%2520enables%2520effects%2520such%2520as%2520defocus%2520blur%2520and%250Acamera%2520distortion%2520%2528e.g.%2520such%2520as%2520from%2520fisheye%2520cameras%2529%252C%2520which%2520are%2520difficult%2520to%250Aachieve%2520by%2520rasterization.%2520We%2520show%2520that%2520our%2520method%2520is%2520more%2520accurate%2520with%2520fewer%250Ablending%2520issues%2520than%25203DGS%2520and%2520follow-up%2520work%2520on%2520view-consistent%2520rendering%252C%250Aespecially%2520on%2520the%2520challenging%2520large-scale%2520scenes%2520from%2520the%2520Zip-NeRF%2520dataset%250Awhere%2520it%2520achieves%2520sharpest%2520results%2520among%2520real-time%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01804v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&entry.906535625=Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jonathan%20T.%20Barron%20and%20Yinda%20Zhang&entry.1292438233=%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01804v4&entry.124074799=Read"},
{"title": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization", "author": "Frederic Kirstein and Terry Ruas and Robert Kratel and Bela Gipp", "abstract": "  Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.\n", "link": "http://arxiv.org/abs/2410.14545v1", "date": "2024-10-18", "relevancy": 2.761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell%20me%20what%20I%20need%20to%20know%3A%20Exploring%20LLM-based%20%28Personalized%29%0A%20%20Abstractive%20Multi-Source%20Meeting%20Summarization&body=Title%3A%20Tell%20me%20what%20I%20need%20to%20know%3A%20Exploring%20LLM-based%20%28Personalized%29%0A%20%20Abstractive%20Multi-Source%20Meeting%20Summarization%0AAuthor%3A%20Frederic%20Kirstein%20and%20Terry%20Ruas%20and%20Robert%20Kratel%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Meeting%20summarization%20is%20crucial%20in%20digital%20communication%2C%20but%20existing%0Asolutions%20struggle%20with%20salience%20identification%20to%20generate%20personalized%2C%0Aworkable%20summaries%2C%20and%20context%20understanding%20to%20fully%20comprehend%20the%20meetings%27%0Acontent.%20Previous%20attempts%20to%20address%20these%20issues%20by%20considering%20related%0Asupplementary%20resources%20%28e.g.%2C%20presentation%20slides%29%20alongside%20transcripts%20are%0Ahindered%20by%20models%27%20limited%20context%20sizes%20and%20handling%20the%20additional%0Acomplexities%20of%20the%20multi-source%20tasks%2C%20such%20as%20identifying%20relevant%0Ainformation%20in%20additional%20files%20and%20seamlessly%20aligning%20it%20with%20the%20meeting%0Acontent.%20This%20work%20explores%20multi-source%20meeting%20summarization%20considering%0Asupplementary%20materials%20through%20a%20three-stage%20large%20language%20model%20approach%3A%0Aidentifying%20transcript%20passages%20needing%20additional%20context%2C%20inferring%20relevant%0Adetails%20from%20supplementary%20materials%20and%20inserting%20them%20into%20the%20transcript%2C%0Aand%20generating%20a%20summary%20from%20this%20enriched%20transcript.%20Our%20multi-source%0Aapproach%20enhances%20model%20understanding%2C%20increasing%20summary%20relevance%20by%20~9%25%20and%0Aproducing%20more%20content-rich%20outputs.%20We%20introduce%20a%20personalization%20protocol%0Athat%20extracts%20participant%20characteristics%20and%20tailors%20summaries%20accordingly%2C%0Aimproving%20informativeness%20by%20~10%25.%20This%20work%20further%20provides%20insights%20on%0Aperformance-cost%20trade-offs%20across%20four%20leading%20model%20families%2C%20including%0Aedge-device%20capable%20options.%20Our%20approach%20can%20be%20extended%20to%20similar%20complex%0Agenerative%20tasks%20benefitting%20from%20additional%20resources%20and%20personalization%2C%0Asuch%20as%20dialogue%20systems%20and%20action%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell%2520me%2520what%2520I%2520need%2520to%2520know%253A%2520Exploring%2520LLM-based%2520%2528Personalized%2529%250A%2520%2520Abstractive%2520Multi-Source%2520Meeting%2520Summarization%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Terry%2520Ruas%2520and%2520Robert%2520Kratel%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Meeting%2520summarization%2520is%2520crucial%2520in%2520digital%2520communication%252C%2520but%2520existing%250Asolutions%2520struggle%2520with%2520salience%2520identification%2520to%2520generate%2520personalized%252C%250Aworkable%2520summaries%252C%2520and%2520context%2520understanding%2520to%2520fully%2520comprehend%2520the%2520meetings%2527%250Acontent.%2520Previous%2520attempts%2520to%2520address%2520these%2520issues%2520by%2520considering%2520related%250Asupplementary%2520resources%2520%2528e.g.%252C%2520presentation%2520slides%2529%2520alongside%2520transcripts%2520are%250Ahindered%2520by%2520models%2527%2520limited%2520context%2520sizes%2520and%2520handling%2520the%2520additional%250Acomplexities%2520of%2520the%2520multi-source%2520tasks%252C%2520such%2520as%2520identifying%2520relevant%250Ainformation%2520in%2520additional%2520files%2520and%2520seamlessly%2520aligning%2520it%2520with%2520the%2520meeting%250Acontent.%2520This%2520work%2520explores%2520multi-source%2520meeting%2520summarization%2520considering%250Asupplementary%2520materials%2520through%2520a%2520three-stage%2520large%2520language%2520model%2520approach%253A%250Aidentifying%2520transcript%2520passages%2520needing%2520additional%2520context%252C%2520inferring%2520relevant%250Adetails%2520from%2520supplementary%2520materials%2520and%2520inserting%2520them%2520into%2520the%2520transcript%252C%250Aand%2520generating%2520a%2520summary%2520from%2520this%2520enriched%2520transcript.%2520Our%2520multi-source%250Aapproach%2520enhances%2520model%2520understanding%252C%2520increasing%2520summary%2520relevance%2520by%2520~9%2525%2520and%250Aproducing%2520more%2520content-rich%2520outputs.%2520We%2520introduce%2520a%2520personalization%2520protocol%250Athat%2520extracts%2520participant%2520characteristics%2520and%2520tailors%2520summaries%2520accordingly%252C%250Aimproving%2520informativeness%2520by%2520~10%2525.%2520This%2520work%2520further%2520provides%2520insights%2520on%250Aperformance-cost%2520trade-offs%2520across%2520four%2520leading%2520model%2520families%252C%2520including%250Aedge-device%2520capable%2520options.%2520Our%2520approach%2520can%2520be%2520extended%2520to%2520similar%2520complex%250Agenerative%2520tasks%2520benefitting%2520from%2520additional%2520resources%2520and%2520personalization%252C%250Asuch%2520as%2520dialogue%2520systems%2520and%2520action%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20me%20what%20I%20need%20to%20know%3A%20Exploring%20LLM-based%20%28Personalized%29%0A%20%20Abstractive%20Multi-Source%20Meeting%20Summarization&entry.906535625=Frederic%20Kirstein%20and%20Terry%20Ruas%20and%20Robert%20Kratel%20and%20Bela%20Gipp&entry.1292438233=%20%20Meeting%20summarization%20is%20crucial%20in%20digital%20communication%2C%20but%20existing%0Asolutions%20struggle%20with%20salience%20identification%20to%20generate%20personalized%2C%0Aworkable%20summaries%2C%20and%20context%20understanding%20to%20fully%20comprehend%20the%20meetings%27%0Acontent.%20Previous%20attempts%20to%20address%20these%20issues%20by%20considering%20related%0Asupplementary%20resources%20%28e.g.%2C%20presentation%20slides%29%20alongside%20transcripts%20are%0Ahindered%20by%20models%27%20limited%20context%20sizes%20and%20handling%20the%20additional%0Acomplexities%20of%20the%20multi-source%20tasks%2C%20such%20as%20identifying%20relevant%0Ainformation%20in%20additional%20files%20and%20seamlessly%20aligning%20it%20with%20the%20meeting%0Acontent.%20This%20work%20explores%20multi-source%20meeting%20summarization%20considering%0Asupplementary%20materials%20through%20a%20three-stage%20large%20language%20model%20approach%3A%0Aidentifying%20transcript%20passages%20needing%20additional%20context%2C%20inferring%20relevant%0Adetails%20from%20supplementary%20materials%20and%20inserting%20them%20into%20the%20transcript%2C%0Aand%20generating%20a%20summary%20from%20this%20enriched%20transcript.%20Our%20multi-source%0Aapproach%20enhances%20model%20understanding%2C%20increasing%20summary%20relevance%20by%20~9%25%20and%0Aproducing%20more%20content-rich%20outputs.%20We%20introduce%20a%20personalization%20protocol%0Athat%20extracts%20participant%20characteristics%20and%20tailors%20summaries%20accordingly%2C%0Aimproving%20informativeness%20by%20~10%25.%20This%20work%20further%20provides%20insights%20on%0Aperformance-cost%20trade-offs%20across%20four%20leading%20model%20families%2C%20including%0Aedge-device%20capable%20options.%20Our%20approach%20can%20be%20extended%20to%20similar%20complex%0Agenerative%20tasks%20benefitting%20from%20additional%20resources%20and%20personalization%2C%0Asuch%20as%20dialogue%20systems%20and%20action%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14545v1&entry.124074799=Read"},
{"title": "Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation\n  Models for Multi-Task Learning", "author": "Yuxiang Lu and Shengcao Cao and Yu-Xiong Wang", "abstract": "  Vision Foundation Models (VFMs) have demonstrated outstanding performance on\nnumerous downstream tasks. However, due to their inherent representation biases\noriginating from different training paradigms, VFMs exhibit advantages and\ndisadvantages across distinct vision tasks. Although amalgamating the strengths\nof multiple VFMs for downstream tasks is an intuitive strategy, effectively\nexploiting these biases remains a significant challenge. In this paper, we\npropose a novel and versatile \"Swiss Army Knife\" (SAK) solution, which\nadaptively distills knowledge from a committee of VFMs to enhance multi-task\nlearning. Unlike existing methods that use a single backbone for knowledge\ntransfer, our approach preserves the unique representation bias of each teacher\nby collaborating the lightweight Teacher-Specific Adapter Path modules with the\nTeacher-Agnostic Stem. Through dynamic selection and combination of\nrepresentations with Mixture-of-Representations Routers, our SAK is capable of\nsynergizing the complementary strengths of multiple VFMs. Extensive experiments\nshow that our SAK remarkably outperforms prior state of the arts in multi-task\nlearning by 10% on the NYUD-v2 benchmark, while also providing a flexible and\nrobust framework that can readily accommodate more advanced model designs.\n", "link": "http://arxiv.org/abs/2410.14633v1", "date": "2024-10-18", "relevancy": 2.7545, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swiss%20Army%20Knife%3A%20Synergizing%20Biases%20in%20Knowledge%20from%20Vision%20Foundation%0A%20%20Models%20for%20Multi-Task%20Learning&body=Title%3A%20Swiss%20Army%20Knife%3A%20Synergizing%20Biases%20in%20Knowledge%20from%20Vision%20Foundation%0A%20%20Models%20for%20Multi-Task%20Learning%0AAuthor%3A%20Yuxiang%20Lu%20and%20Shengcao%20Cao%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20demonstrated%20outstanding%20performance%20on%0Anumerous%20downstream%20tasks.%20However%2C%20due%20to%20their%20inherent%20representation%20biases%0Aoriginating%20from%20different%20training%20paradigms%2C%20VFMs%20exhibit%20advantages%20and%0Adisadvantages%20across%20distinct%20vision%20tasks.%20Although%20amalgamating%20the%20strengths%0Aof%20multiple%20VFMs%20for%20downstream%20tasks%20is%20an%20intuitive%20strategy%2C%20effectively%0Aexploiting%20these%20biases%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20versatile%20%22Swiss%20Army%20Knife%22%20%28SAK%29%20solution%2C%20which%0Aadaptively%20distills%20knowledge%20from%20a%20committee%20of%20VFMs%20to%20enhance%20multi-task%0Alearning.%20Unlike%20existing%20methods%20that%20use%20a%20single%20backbone%20for%20knowledge%0Atransfer%2C%20our%20approach%20preserves%20the%20unique%20representation%20bias%20of%20each%20teacher%0Aby%20collaborating%20the%20lightweight%20Teacher-Specific%20Adapter%20Path%20modules%20with%20the%0ATeacher-Agnostic%20Stem.%20Through%20dynamic%20selection%20and%20combination%20of%0Arepresentations%20with%20Mixture-of-Representations%20Routers%2C%20our%20SAK%20is%20capable%20of%0Asynergizing%20the%20complementary%20strengths%20of%20multiple%20VFMs.%20Extensive%20experiments%0Ashow%20that%20our%20SAK%20remarkably%20outperforms%20prior%20state%20of%20the%20arts%20in%20multi-task%0Alearning%20by%2010%25%20on%20the%20NYUD-v2%20benchmark%2C%20while%20also%20providing%20a%20flexible%20and%0Arobust%20framework%20that%20can%20readily%20accommodate%20more%20advanced%20model%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwiss%2520Army%2520Knife%253A%2520Synergizing%2520Biases%2520in%2520Knowledge%2520from%2520Vision%2520Foundation%250A%2520%2520Models%2520for%2520Multi-Task%2520Learning%26entry.906535625%3DYuxiang%2520Lu%2520and%2520Shengcao%2520Cao%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520have%2520demonstrated%2520outstanding%2520performance%2520on%250Anumerous%2520downstream%2520tasks.%2520However%252C%2520due%2520to%2520their%2520inherent%2520representation%2520biases%250Aoriginating%2520from%2520different%2520training%2520paradigms%252C%2520VFMs%2520exhibit%2520advantages%2520and%250Adisadvantages%2520across%2520distinct%2520vision%2520tasks.%2520Although%2520amalgamating%2520the%2520strengths%250Aof%2520multiple%2520VFMs%2520for%2520downstream%2520tasks%2520is%2520an%2520intuitive%2520strategy%252C%2520effectively%250Aexploiting%2520these%2520biases%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520and%2520versatile%2520%2522Swiss%2520Army%2520Knife%2522%2520%2528SAK%2529%2520solution%252C%2520which%250Aadaptively%2520distills%2520knowledge%2520from%2520a%2520committee%2520of%2520VFMs%2520to%2520enhance%2520multi-task%250Alearning.%2520Unlike%2520existing%2520methods%2520that%2520use%2520a%2520single%2520backbone%2520for%2520knowledge%250Atransfer%252C%2520our%2520approach%2520preserves%2520the%2520unique%2520representation%2520bias%2520of%2520each%2520teacher%250Aby%2520collaborating%2520the%2520lightweight%2520Teacher-Specific%2520Adapter%2520Path%2520modules%2520with%2520the%250ATeacher-Agnostic%2520Stem.%2520Through%2520dynamic%2520selection%2520and%2520combination%2520of%250Arepresentations%2520with%2520Mixture-of-Representations%2520Routers%252C%2520our%2520SAK%2520is%2520capable%2520of%250Asynergizing%2520the%2520complementary%2520strengths%2520of%2520multiple%2520VFMs.%2520Extensive%2520experiments%250Ashow%2520that%2520our%2520SAK%2520remarkably%2520outperforms%2520prior%2520state%2520of%2520the%2520arts%2520in%2520multi-task%250Alearning%2520by%252010%2525%2520on%2520the%2520NYUD-v2%2520benchmark%252C%2520while%2520also%2520providing%2520a%2520flexible%2520and%250Arobust%2520framework%2520that%2520can%2520readily%2520accommodate%2520more%2520advanced%2520model%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swiss%20Army%20Knife%3A%20Synergizing%20Biases%20in%20Knowledge%20from%20Vision%20Foundation%0A%20%20Models%20for%20Multi-Task%20Learning&entry.906535625=Yuxiang%20Lu%20and%20Shengcao%20Cao%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20demonstrated%20outstanding%20performance%20on%0Anumerous%20downstream%20tasks.%20However%2C%20due%20to%20their%20inherent%20representation%20biases%0Aoriginating%20from%20different%20training%20paradigms%2C%20VFMs%20exhibit%20advantages%20and%0Adisadvantages%20across%20distinct%20vision%20tasks.%20Although%20amalgamating%20the%20strengths%0Aof%20multiple%20VFMs%20for%20downstream%20tasks%20is%20an%20intuitive%20strategy%2C%20effectively%0Aexploiting%20these%20biases%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20and%20versatile%20%22Swiss%20Army%20Knife%22%20%28SAK%29%20solution%2C%20which%0Aadaptively%20distills%20knowledge%20from%20a%20committee%20of%20VFMs%20to%20enhance%20multi-task%0Alearning.%20Unlike%20existing%20methods%20that%20use%20a%20single%20backbone%20for%20knowledge%0Atransfer%2C%20our%20approach%20preserves%20the%20unique%20representation%20bias%20of%20each%20teacher%0Aby%20collaborating%20the%20lightweight%20Teacher-Specific%20Adapter%20Path%20modules%20with%20the%0ATeacher-Agnostic%20Stem.%20Through%20dynamic%20selection%20and%20combination%20of%0Arepresentations%20with%20Mixture-of-Representations%20Routers%2C%20our%20SAK%20is%20capable%20of%0Asynergizing%20the%20complementary%20strengths%20of%20multiple%20VFMs.%20Extensive%20experiments%0Ashow%20that%20our%20SAK%20remarkably%20outperforms%20prior%20state%20of%20the%20arts%20in%20multi-task%0Alearning%20by%2010%25%20on%20the%20NYUD-v2%20benchmark%2C%20while%20also%20providing%20a%20flexible%20and%0Arobust%20framework%20that%20can%20readily%20accommodate%20more%20advanced%20model%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14633v1&entry.124074799=Read"},
{"title": "Learning Generative Interactive Environments By Trained Agent\n  Exploration", "author": "Naser Kazemi and Nedko Savov and Danda Paudel and Luc Van Gool", "abstract": "  World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .\n", "link": "http://arxiv.org/abs/2409.06445v2", "date": "2024-10-18", "relevancy": 2.7362, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.8151}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6089}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration&body=Title%3A%20Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration%0AAuthor%3A%20Naser%20Kazemi%20and%20Nedko%20Savov%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20World%20models%20are%20increasingly%20pivotal%20in%20interpreting%20and%20simulating%20the%0Arules%20and%20actions%20of%20complex%20environments.%20Genie%2C%20a%20recent%20model%2C%20excels%20at%0Alearning%20from%20visually%20diverse%20environments%20but%20relies%20on%20costly%0Ahuman-collected%20data.%20We%20observe%20that%20their%20alternative%20method%20of%20using%20random%0Aagents%20is%20too%20limited%20to%20explore%20the%20environment.%20We%20propose%20to%20improve%20the%0Amodel%20by%20employing%20reinforcement%20learning%20based%20agents%20for%20data%20generation.%0AThis%20approach%20produces%20diverse%20datasets%20that%20enhance%20the%20model%27s%20ability%20to%0Aadapt%20and%20perform%20well%20across%20various%20scenarios%20and%20realistic%20actions%20within%0Athe%20environment.%20In%20this%20paper%2C%20we%20first%20release%20the%20model%20GenieRedux%20-%20an%0Aimplementation%20based%20on%20Genie.%20Additionally%2C%20we%20introduce%20GenieRedux-G%2C%20a%0Avariant%20that%20uses%20the%20agent%27s%20readily%20available%20actions%20to%20factor%20out%20action%0Aprediction%20uncertainty%20during%20validation.%20Our%20evaluation%2C%20including%20a%0Areplication%20of%20the%20Coinrun%20case%20study%2C%20shows%20that%20GenieRedux-G%20achieves%0Asuperior%20visual%20fidelity%20and%20controllability%20using%20the%20trained%20agent%0Aexploration.%20The%20proposed%20approach%20is%20reproducable%2C%20scalable%20and%20adaptable%20to%0Anew%20types%20of%20environments.%20Our%20codebase%20is%20available%20at%0Ahttps%3A//github.com/insait-institute/GenieRedux%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generative%2520Interactive%2520Environments%2520By%2520Trained%2520Agent%250A%2520%2520Exploration%26entry.906535625%3DNaser%2520Kazemi%2520and%2520Nedko%2520Savov%2520and%2520Danda%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520World%2520models%2520are%2520increasingly%2520pivotal%2520in%2520interpreting%2520and%2520simulating%2520the%250Arules%2520and%2520actions%2520of%2520complex%2520environments.%2520Genie%252C%2520a%2520recent%2520model%252C%2520excels%2520at%250Alearning%2520from%2520visually%2520diverse%2520environments%2520but%2520relies%2520on%2520costly%250Ahuman-collected%2520data.%2520We%2520observe%2520that%2520their%2520alternative%2520method%2520of%2520using%2520random%250Aagents%2520is%2520too%2520limited%2520to%2520explore%2520the%2520environment.%2520We%2520propose%2520to%2520improve%2520the%250Amodel%2520by%2520employing%2520reinforcement%2520learning%2520based%2520agents%2520for%2520data%2520generation.%250AThis%2520approach%2520produces%2520diverse%2520datasets%2520that%2520enhance%2520the%2520model%2527s%2520ability%2520to%250Aadapt%2520and%2520perform%2520well%2520across%2520various%2520scenarios%2520and%2520realistic%2520actions%2520within%250Athe%2520environment.%2520In%2520this%2520paper%252C%2520we%2520first%2520release%2520the%2520model%2520GenieRedux%2520-%2520an%250Aimplementation%2520based%2520on%2520Genie.%2520Additionally%252C%2520we%2520introduce%2520GenieRedux-G%252C%2520a%250Avariant%2520that%2520uses%2520the%2520agent%2527s%2520readily%2520available%2520actions%2520to%2520factor%2520out%2520action%250Aprediction%2520uncertainty%2520during%2520validation.%2520Our%2520evaluation%252C%2520including%2520a%250Areplication%2520of%2520the%2520Coinrun%2520case%2520study%252C%2520shows%2520that%2520GenieRedux-G%2520achieves%250Asuperior%2520visual%2520fidelity%2520and%2520controllability%2520using%2520the%2520trained%2520agent%250Aexploration.%2520The%2520proposed%2520approach%2520is%2520reproducable%252C%2520scalable%2520and%2520adaptable%2520to%250Anew%2520types%2520of%2520environments.%2520Our%2520codebase%2520is%2520available%2520at%250Ahttps%253A//github.com/insait-institute/GenieRedux%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration&entry.906535625=Naser%20Kazemi%20and%20Nedko%20Savov%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20World%20models%20are%20increasingly%20pivotal%20in%20interpreting%20and%20simulating%20the%0Arules%20and%20actions%20of%20complex%20environments.%20Genie%2C%20a%20recent%20model%2C%20excels%20at%0Alearning%20from%20visually%20diverse%20environments%20but%20relies%20on%20costly%0Ahuman-collected%20data.%20We%20observe%20that%20their%20alternative%20method%20of%20using%20random%0Aagents%20is%20too%20limited%20to%20explore%20the%20environment.%20We%20propose%20to%20improve%20the%0Amodel%20by%20employing%20reinforcement%20learning%20based%20agents%20for%20data%20generation.%0AThis%20approach%20produces%20diverse%20datasets%20that%20enhance%20the%20model%27s%20ability%20to%0Aadapt%20and%20perform%20well%20across%20various%20scenarios%20and%20realistic%20actions%20within%0Athe%20environment.%20In%20this%20paper%2C%20we%20first%20release%20the%20model%20GenieRedux%20-%20an%0Aimplementation%20based%20on%20Genie.%20Additionally%2C%20we%20introduce%20GenieRedux-G%2C%20a%0Avariant%20that%20uses%20the%20agent%27s%20readily%20available%20actions%20to%20factor%20out%20action%0Aprediction%20uncertainty%20during%20validation.%20Our%20evaluation%2C%20including%20a%0Areplication%20of%20the%20Coinrun%20case%20study%2C%20shows%20that%20GenieRedux-G%20achieves%0Asuperior%20visual%20fidelity%20and%20controllability%20using%20the%20trained%20agent%0Aexploration.%20The%20proposed%20approach%20is%20reproducable%2C%20scalable%20and%20adaptable%20to%0Anew%20types%20of%20environments.%20Our%20codebase%20is%20available%20at%0Ahttps%3A//github.com/insait-institute/GenieRedux%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06445v2&entry.124074799=Read"},
{"title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings", "author": "Raghuveer Thirukovalluru and Bhuwan Dhingra", "abstract": "  Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.\n", "link": "http://arxiv.org/abs/2410.14635v1", "date": "2024-10-18", "relevancy": 2.642, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenEOL%3A%20Harnessing%20the%20Generative%20Power%20of%20LLMs%20for%20Training-Free%0A%20%20Sentence%20Embeddings&body=Title%3A%20GenEOL%3A%20Harnessing%20the%20Generative%20Power%20of%20LLMs%20for%20Training-Free%0A%20%20Sentence%20Embeddings%0AAuthor%3A%20Raghuveer%20Thirukovalluru%20and%20Bhuwan%20Dhingra%0AAbstract%3A%20%20%20Training-free%20embedding%20methods%20directly%20leverage%20pretrained%20large%20language%0Amodels%20%28LLMs%29%20to%20embed%20text%2C%20bypassing%20the%20costly%20and%20complex%20procedure%20of%0Acontrastive%20learning.%20Previous%20training-free%20embedding%20methods%20have%20mainly%0Afocused%20on%20optimizing%20embedding%20prompts%20and%20have%20overlooked%20the%20benefits%20of%0Autilizing%20the%20generative%20abilities%20of%20LLMs.%20We%20propose%20a%20novel%20method%2C%20GenEOL%2C%0Awhich%20uses%20LLMs%20to%20generate%20diverse%20transformations%20of%20a%20sentence%20that%20preserve%0Aits%20meaning%2C%20and%20aggregates%20the%20resulting%20embeddings%20of%20these%20transformations%0Ato%20enhance%20the%20overall%20sentence%20embedding.%20GenEOL%20significantly%20outperforms%20the%0Aexisting%20training-free%20embedding%20methods%20by%20an%20average%20of%202.85%20points%20across%0Aseveral%20LLMs%20on%20the%20sentence%20semantic%20text%20similarity%20%28STS%29%20benchmark.%20Our%0Aanalysis%20shows%20that%20GenEOL%20stabilizes%20representation%20quality%20across%20LLM%20layers%0Aand%20is%20robust%20to%20perturbations%20of%20embedding%20prompts.%20GenEOL%20also%20achieves%0Anotable%20gains%20on%20multiple%20clustering%2C%20reranking%20and%20pair-classification%20tasks%0Afrom%20the%20MTEB%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenEOL%253A%2520Harnessing%2520the%2520Generative%2520Power%2520of%2520LLMs%2520for%2520Training-Free%250A%2520%2520Sentence%2520Embeddings%26entry.906535625%3DRaghuveer%2520Thirukovalluru%2520and%2520Bhuwan%2520Dhingra%26entry.1292438233%3D%2520%2520Training-free%2520embedding%2520methods%2520directly%2520leverage%2520pretrained%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520embed%2520text%252C%2520bypassing%2520the%2520costly%2520and%2520complex%2520procedure%2520of%250Acontrastive%2520learning.%2520Previous%2520training-free%2520embedding%2520methods%2520have%2520mainly%250Afocused%2520on%2520optimizing%2520embedding%2520prompts%2520and%2520have%2520overlooked%2520the%2520benefits%2520of%250Autilizing%2520the%2520generative%2520abilities%2520of%2520LLMs.%2520We%2520propose%2520a%2520novel%2520method%252C%2520GenEOL%252C%250Awhich%2520uses%2520LLMs%2520to%2520generate%2520diverse%2520transformations%2520of%2520a%2520sentence%2520that%2520preserve%250Aits%2520meaning%252C%2520and%2520aggregates%2520the%2520resulting%2520embeddings%2520of%2520these%2520transformations%250Ato%2520enhance%2520the%2520overall%2520sentence%2520embedding.%2520GenEOL%2520significantly%2520outperforms%2520the%250Aexisting%2520training-free%2520embedding%2520methods%2520by%2520an%2520average%2520of%25202.85%2520points%2520across%250Aseveral%2520LLMs%2520on%2520the%2520sentence%2520semantic%2520text%2520similarity%2520%2528STS%2529%2520benchmark.%2520Our%250Aanalysis%2520shows%2520that%2520GenEOL%2520stabilizes%2520representation%2520quality%2520across%2520LLM%2520layers%250Aand%2520is%2520robust%2520to%2520perturbations%2520of%2520embedding%2520prompts.%2520GenEOL%2520also%2520achieves%250Anotable%2520gains%2520on%2520multiple%2520clustering%252C%2520reranking%2520and%2520pair-classification%2520tasks%250Afrom%2520the%2520MTEB%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenEOL%3A%20Harnessing%20the%20Generative%20Power%20of%20LLMs%20for%20Training-Free%0A%20%20Sentence%20Embeddings&entry.906535625=Raghuveer%20Thirukovalluru%20and%20Bhuwan%20Dhingra&entry.1292438233=%20%20Training-free%20embedding%20methods%20directly%20leverage%20pretrained%20large%20language%0Amodels%20%28LLMs%29%20to%20embed%20text%2C%20bypassing%20the%20costly%20and%20complex%20procedure%20of%0Acontrastive%20learning.%20Previous%20training-free%20embedding%20methods%20have%20mainly%0Afocused%20on%20optimizing%20embedding%20prompts%20and%20have%20overlooked%20the%20benefits%20of%0Autilizing%20the%20generative%20abilities%20of%20LLMs.%20We%20propose%20a%20novel%20method%2C%20GenEOL%2C%0Awhich%20uses%20LLMs%20to%20generate%20diverse%20transformations%20of%20a%20sentence%20that%20preserve%0Aits%20meaning%2C%20and%20aggregates%20the%20resulting%20embeddings%20of%20these%20transformations%0Ato%20enhance%20the%20overall%20sentence%20embedding.%20GenEOL%20significantly%20outperforms%20the%0Aexisting%20training-free%20embedding%20methods%20by%20an%20average%20of%202.85%20points%20across%0Aseveral%20LLMs%20on%20the%20sentence%20semantic%20text%20similarity%20%28STS%29%20benchmark.%20Our%0Aanalysis%20shows%20that%20GenEOL%20stabilizes%20representation%20quality%20across%20LLM%20layers%0Aand%20is%20robust%20to%20perturbations%20of%20embedding%20prompts.%20GenEOL%20also%20achieves%0Anotable%20gains%20on%20multiple%20clustering%2C%20reranking%20and%20pair-classification%20tasks%0Afrom%20the%20MTEB%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14635v1&entry.124074799=Read"},
{"title": "Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection", "author": "Aaron Alvarado Kristanto Julistiono and Davoud Ataee Tarzanagh and Navid Azizan", "abstract": "  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n", "link": "http://arxiv.org/abs/2410.14581v1", "date": "2024-10-18", "relevancy": 2.574, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5862}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Attention%20with%20Mirror%20Descent%3A%20Generalized%20Max-Margin%20Token%0A%20%20Selection&body=Title%3A%20Optimizing%20Attention%20with%20Mirror%20Descent%3A%20Generalized%20Max-Margin%20Token%0A%20%20Selection%0AAuthor%3A%20Aaron%20Alvarado%20Kristanto%20Julistiono%20and%20Davoud%20Ataee%20Tarzanagh%20and%20Navid%20Azizan%0AAbstract%3A%20%20%20Attention%20mechanisms%20have%20revolutionized%20several%20domains%20of%20artificial%0Aintelligence%2C%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20by%0Aenabling%20models%20to%20selectively%20focus%20on%20relevant%20parts%20of%20the%20input%20data.%20While%0Arecent%20work%20has%20characterized%20the%20optimization%20dynamics%20of%20gradient%20descent%0A%28GD%29%20in%20attention-based%20models%20and%20the%20structural%20properties%20of%20its%20preferred%0Asolutions%2C%20less%20is%20known%20about%20more%20general%20optimization%20algorithms%20such%20as%0Amirror%20descent%20%28MD%29.%20In%20this%20paper%2C%20we%20investigate%20the%20convergence%20properties%0Aand%20implicit%20biases%20of%20a%20family%20of%20MD%20algorithms%20tailored%20for%20softmax%20attention%0Amechanisms%2C%20with%20the%20potential%20function%20chosen%20as%20the%20%24p%24-th%20power%20of%20the%0A%24%5Cell_p%24-norm.%20Specifically%2C%20we%20show%20that%20these%20algorithms%20converge%20in%0Adirection%20to%20a%20generalized%20hard-margin%20SVM%20with%20an%20%24%5Cell_p%24-norm%20objective%20when%0Aapplied%20to%20a%20classification%20problem%20using%20a%20softmax%20attention%20model.%20Notably%2C%0Aour%20theoretical%20results%20reveal%20that%20the%20convergence%20rate%20is%20comparable%20to%20that%0Aof%20traditional%20GD%20in%20simpler%20models%2C%20despite%20the%20highly%20nonlinear%20and%20nonconvex%0Anature%20of%20the%20present%20problem.%20Additionally%2C%20we%20delve%20into%20the%20joint%0Aoptimization%20dynamics%20of%20the%20key-query%20matrix%20and%20the%20decoder%2C%20establishing%0Aconditions%20under%20which%20this%20complex%20joint%20optimization%20converges%20to%20their%0Arespective%20hard-margin%20SVM%20solutions.%20Lastly%2C%20our%20numerical%20experiments%20on%20real%0Adata%20demonstrate%20that%20MD%20algorithms%20improve%20generalization%20over%20standard%20GD%20and%0Aexcel%20in%20optimal%20token%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Attention%2520with%2520Mirror%2520Descent%253A%2520Generalized%2520Max-Margin%2520Token%250A%2520%2520Selection%26entry.906535625%3DAaron%2520Alvarado%2520Kristanto%2520Julistiono%2520and%2520Davoud%2520Ataee%2520Tarzanagh%2520and%2520Navid%2520Azizan%26entry.1292438233%3D%2520%2520Attention%2520mechanisms%2520have%2520revolutionized%2520several%2520domains%2520of%2520artificial%250Aintelligence%252C%2520such%2520as%2520natural%2520language%2520processing%2520and%2520computer%2520vision%252C%2520by%250Aenabling%2520models%2520to%2520selectively%2520focus%2520on%2520relevant%2520parts%2520of%2520the%2520input%2520data.%2520While%250Arecent%2520work%2520has%2520characterized%2520the%2520optimization%2520dynamics%2520of%2520gradient%2520descent%250A%2528GD%2529%2520in%2520attention-based%2520models%2520and%2520the%2520structural%2520properties%2520of%2520its%2520preferred%250Asolutions%252C%2520less%2520is%2520known%2520about%2520more%2520general%2520optimization%2520algorithms%2520such%2520as%250Amirror%2520descent%2520%2528MD%2529.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520convergence%2520properties%250Aand%2520implicit%2520biases%2520of%2520a%2520family%2520of%2520MD%2520algorithms%2520tailored%2520for%2520softmax%2520attention%250Amechanisms%252C%2520with%2520the%2520potential%2520function%2520chosen%2520as%2520the%2520%2524p%2524-th%2520power%2520of%2520the%250A%2524%255Cell_p%2524-norm.%2520Specifically%252C%2520we%2520show%2520that%2520these%2520algorithms%2520converge%2520in%250Adirection%2520to%2520a%2520generalized%2520hard-margin%2520SVM%2520with%2520an%2520%2524%255Cell_p%2524-norm%2520objective%2520when%250Aapplied%2520to%2520a%2520classification%2520problem%2520using%2520a%2520softmax%2520attention%2520model.%2520Notably%252C%250Aour%2520theoretical%2520results%2520reveal%2520that%2520the%2520convergence%2520rate%2520is%2520comparable%2520to%2520that%250Aof%2520traditional%2520GD%2520in%2520simpler%2520models%252C%2520despite%2520the%2520highly%2520nonlinear%2520and%2520nonconvex%250Anature%2520of%2520the%2520present%2520problem.%2520Additionally%252C%2520we%2520delve%2520into%2520the%2520joint%250Aoptimization%2520dynamics%2520of%2520the%2520key-query%2520matrix%2520and%2520the%2520decoder%252C%2520establishing%250Aconditions%2520under%2520which%2520this%2520complex%2520joint%2520optimization%2520converges%2520to%2520their%250Arespective%2520hard-margin%2520SVM%2520solutions.%2520Lastly%252C%2520our%2520numerical%2520experiments%2520on%2520real%250Adata%2520demonstrate%2520that%2520MD%2520algorithms%2520improve%2520generalization%2520over%2520standard%2520GD%2520and%250Aexcel%2520in%2520optimal%2520token%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Attention%20with%20Mirror%20Descent%3A%20Generalized%20Max-Margin%20Token%0A%20%20Selection&entry.906535625=Aaron%20Alvarado%20Kristanto%20Julistiono%20and%20Davoud%20Ataee%20Tarzanagh%20and%20Navid%20Azizan&entry.1292438233=%20%20Attention%20mechanisms%20have%20revolutionized%20several%20domains%20of%20artificial%0Aintelligence%2C%20such%20as%20natural%20language%20processing%20and%20computer%20vision%2C%20by%0Aenabling%20models%20to%20selectively%20focus%20on%20relevant%20parts%20of%20the%20input%20data.%20While%0Arecent%20work%20has%20characterized%20the%20optimization%20dynamics%20of%20gradient%20descent%0A%28GD%29%20in%20attention-based%20models%20and%20the%20structural%20properties%20of%20its%20preferred%0Asolutions%2C%20less%20is%20known%20about%20more%20general%20optimization%20algorithms%20such%20as%0Amirror%20descent%20%28MD%29.%20In%20this%20paper%2C%20we%20investigate%20the%20convergence%20properties%0Aand%20implicit%20biases%20of%20a%20family%20of%20MD%20algorithms%20tailored%20for%20softmax%20attention%0Amechanisms%2C%20with%20the%20potential%20function%20chosen%20as%20the%20%24p%24-th%20power%20of%20the%0A%24%5Cell_p%24-norm.%20Specifically%2C%20we%20show%20that%20these%20algorithms%20converge%20in%0Adirection%20to%20a%20generalized%20hard-margin%20SVM%20with%20an%20%24%5Cell_p%24-norm%20objective%20when%0Aapplied%20to%20a%20classification%20problem%20using%20a%20softmax%20attention%20model.%20Notably%2C%0Aour%20theoretical%20results%20reveal%20that%20the%20convergence%20rate%20is%20comparable%20to%20that%0Aof%20traditional%20GD%20in%20simpler%20models%2C%20despite%20the%20highly%20nonlinear%20and%20nonconvex%0Anature%20of%20the%20present%20problem.%20Additionally%2C%20we%20delve%20into%20the%20joint%0Aoptimization%20dynamics%20of%20the%20key-query%20matrix%20and%20the%20decoder%2C%20establishing%0Aconditions%20under%20which%20this%20complex%20joint%20optimization%20converges%20to%20their%0Arespective%20hard-margin%20SVM%20solutions.%20Lastly%2C%20our%20numerical%20experiments%20on%20real%0Adata%20demonstrate%20that%20MD%20algorithms%20improve%20generalization%20over%20standard%20GD%20and%0Aexcel%20in%20optimal%20token%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14581v1&entry.124074799=Read"},
{"title": "Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating\n  the Accuracy-Robustness Tradeoff", "author": "Peimeng Guan and Mark A. Davenport", "abstract": "  Inverse problems aim to reconstruct unseen data from corrupted or perturbed\nmeasurements. While most work focuses on improving reconstruction quality,\ngeneralization accuracy and robustness are equally important, especially for\nsafety-critical applications. Model-based architectures (MBAs), such as loop\nunrolling methods, are considered more interpretable and achieve better\nreconstructions. Empirical evidence suggests that MBAs are more robust to\nperturbations than black-box solvers, but the accuracy-robustness tradeoff in\nMBAs remains underexplored. In this work, we propose a simple yet effective\ntraining scheme for MBAs, called SGD jittering, which injects noise\niteration-wise during reconstruction. We theoretically demonstrate that SGD\njittering not only generalizes better than the standard mean squared error\ntraining but is also more robust to average-case attacks. We validate SGD\njittering using denoising toy examples, seismic deconvolution, and single-coil\nMRI reconstruction. The proposed method achieves cleaner reconstructions for\nout-of-distribution data and demonstrates enhanced robustness to adversarial\nattacks.\n", "link": "http://arxiv.org/abs/2410.14667v1", "date": "2024-10-18", "relevancy": 2.5549, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.538}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Gradient%20Descent%20Jittering%20for%20Inverse%20Problems%3A%20Alleviating%0A%20%20the%20Accuracy-Robustness%20Tradeoff&body=Title%3A%20Stochastic%20Gradient%20Descent%20Jittering%20for%20Inverse%20Problems%3A%20Alleviating%0A%20%20the%20Accuracy-Robustness%20Tradeoff%0AAuthor%3A%20Peimeng%20Guan%20and%20Mark%20A.%20Davenport%0AAbstract%3A%20%20%20Inverse%20problems%20aim%20to%20reconstruct%20unseen%20data%20from%20corrupted%20or%20perturbed%0Ameasurements.%20While%20most%20work%20focuses%20on%20improving%20reconstruction%20quality%2C%0Ageneralization%20accuracy%20and%20robustness%20are%20equally%20important%2C%20especially%20for%0Asafety-critical%20applications.%20Model-based%20architectures%20%28MBAs%29%2C%20such%20as%20loop%0Aunrolling%20methods%2C%20are%20considered%20more%20interpretable%20and%20achieve%20better%0Areconstructions.%20Empirical%20evidence%20suggests%20that%20MBAs%20are%20more%20robust%20to%0Aperturbations%20than%20black-box%20solvers%2C%20but%20the%20accuracy-robustness%20tradeoff%20in%0AMBAs%20remains%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Atraining%20scheme%20for%20MBAs%2C%20called%20SGD%20jittering%2C%20which%20injects%20noise%0Aiteration-wise%20during%20reconstruction.%20We%20theoretically%20demonstrate%20that%20SGD%0Ajittering%20not%20only%20generalizes%20better%20than%20the%20standard%20mean%20squared%20error%0Atraining%20but%20is%20also%20more%20robust%20to%20average-case%20attacks.%20We%20validate%20SGD%0Ajittering%20using%20denoising%20toy%20examples%2C%20seismic%20deconvolution%2C%20and%20single-coil%0AMRI%20reconstruction.%20The%20proposed%20method%20achieves%20cleaner%20reconstructions%20for%0Aout-of-distribution%20data%20and%20demonstrates%20enhanced%20robustness%20to%20adversarial%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Gradient%2520Descent%2520Jittering%2520for%2520Inverse%2520Problems%253A%2520Alleviating%250A%2520%2520the%2520Accuracy-Robustness%2520Tradeoff%26entry.906535625%3DPeimeng%2520Guan%2520and%2520Mark%2520A.%2520Davenport%26entry.1292438233%3D%2520%2520Inverse%2520problems%2520aim%2520to%2520reconstruct%2520unseen%2520data%2520from%2520corrupted%2520or%2520perturbed%250Ameasurements.%2520While%2520most%2520work%2520focuses%2520on%2520improving%2520reconstruction%2520quality%252C%250Ageneralization%2520accuracy%2520and%2520robustness%2520are%2520equally%2520important%252C%2520especially%2520for%250Asafety-critical%2520applications.%2520Model-based%2520architectures%2520%2528MBAs%2529%252C%2520such%2520as%2520loop%250Aunrolling%2520methods%252C%2520are%2520considered%2520more%2520interpretable%2520and%2520achieve%2520better%250Areconstructions.%2520Empirical%2520evidence%2520suggests%2520that%2520MBAs%2520are%2520more%2520robust%2520to%250Aperturbations%2520than%2520black-box%2520solvers%252C%2520but%2520the%2520accuracy-robustness%2520tradeoff%2520in%250AMBAs%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Atraining%2520scheme%2520for%2520MBAs%252C%2520called%2520SGD%2520jittering%252C%2520which%2520injects%2520noise%250Aiteration-wise%2520during%2520reconstruction.%2520We%2520theoretically%2520demonstrate%2520that%2520SGD%250Ajittering%2520not%2520only%2520generalizes%2520better%2520than%2520the%2520standard%2520mean%2520squared%2520error%250Atraining%2520but%2520is%2520also%2520more%2520robust%2520to%2520average-case%2520attacks.%2520We%2520validate%2520SGD%250Ajittering%2520using%2520denoising%2520toy%2520examples%252C%2520seismic%2520deconvolution%252C%2520and%2520single-coil%250AMRI%2520reconstruction.%2520The%2520proposed%2520method%2520achieves%2520cleaner%2520reconstructions%2520for%250Aout-of-distribution%2520data%2520and%2520demonstrates%2520enhanced%2520robustness%2520to%2520adversarial%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Gradient%20Descent%20Jittering%20for%20Inverse%20Problems%3A%20Alleviating%0A%20%20the%20Accuracy-Robustness%20Tradeoff&entry.906535625=Peimeng%20Guan%20and%20Mark%20A.%20Davenport&entry.1292438233=%20%20Inverse%20problems%20aim%20to%20reconstruct%20unseen%20data%20from%20corrupted%20or%20perturbed%0Ameasurements.%20While%20most%20work%20focuses%20on%20improving%20reconstruction%20quality%2C%0Ageneralization%20accuracy%20and%20robustness%20are%20equally%20important%2C%20especially%20for%0Asafety-critical%20applications.%20Model-based%20architectures%20%28MBAs%29%2C%20such%20as%20loop%0Aunrolling%20methods%2C%20are%20considered%20more%20interpretable%20and%20achieve%20better%0Areconstructions.%20Empirical%20evidence%20suggests%20that%20MBAs%20are%20more%20robust%20to%0Aperturbations%20than%20black-box%20solvers%2C%20but%20the%20accuracy-robustness%20tradeoff%20in%0AMBAs%20remains%20underexplored.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Atraining%20scheme%20for%20MBAs%2C%20called%20SGD%20jittering%2C%20which%20injects%20noise%0Aiteration-wise%20during%20reconstruction.%20We%20theoretically%20demonstrate%20that%20SGD%0Ajittering%20not%20only%20generalizes%20better%20than%20the%20standard%20mean%20squared%20error%0Atraining%20but%20is%20also%20more%20robust%20to%20average-case%20attacks.%20We%20validate%20SGD%0Ajittering%20using%20denoising%20toy%20examples%2C%20seismic%20deconvolution%2C%20and%20single-coil%0AMRI%20reconstruction.%20The%20proposed%20method%20achieves%20cleaner%20reconstructions%20for%0Aout-of-distribution%20data%20and%20demonstrates%20enhanced%20robustness%20to%20adversarial%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14667v1&entry.124074799=Read"},
{"title": "Contextual Document Embeddings", "author": "John X. Morris and Alexander M. Rush", "abstract": "  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n", "link": "http://arxiv.org/abs/2410.02525v3", "date": "2024-10-18", "relevancy": 2.5282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Document%20Embeddings&body=Title%3A%20Contextual%20Document%20Embeddings%0AAuthor%3A%20John%20X.%20Morris%20and%20Alexander%20M.%20Rush%0AAbstract%3A%20%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02525v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Document%2520Embeddings%26entry.906535625%3DJohn%2520X.%2520Morris%2520and%2520Alexander%2520M.%2520Rush%26entry.1292438233%3D%2520%2520Dense%2520document%2520embeddings%2520are%2520central%2520to%2520neural%2520retrieval.%2520The%2520dominant%250Aparadigm%2520is%2520to%2520train%2520and%2520construct%2520embeddings%2520by%2520running%2520encoders%2520directly%2520on%250Aindividual%2520documents.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520these%2520embeddings%252C%2520while%250Aeffective%252C%2520are%2520implicitly%2520out-of-context%2520for%2520targeted%2520use%2520cases%2520of%2520retrieval%252C%250Aand%2520that%2520a%2520contextualized%2520document%2520embedding%2520should%2520take%2520into%2520account%2520both%2520the%250Adocument%2520and%2520neighboring%2520documents%2520in%2520context%2520-%2520analogous%2520to%2520contextualized%250Aword%2520embeddings.%2520We%2520propose%2520two%2520complementary%2520methods%2520for%2520contextualized%250Adocument%2520embeddings%253A%2520first%252C%2520an%2520alternative%2520contrastive%2520learning%2520objective%2520that%250Aexplicitly%2520incorporates%2520the%2520document%2520neighbors%2520into%2520the%2520intra-batch%2520contextual%250Aloss%253B%2520second%252C%2520a%2520new%2520contextual%2520architecture%2520that%2520explicitly%2520encodes%2520neighbor%250Adocument%2520information%2520into%2520the%2520encoded%2520representation.%2520Results%2520show%2520that%2520both%250Amethods%2520achieve%2520better%2520performance%2520than%2520biencoders%2520in%2520several%2520settings%252C%2520with%250Adifferences%2520especially%2520pronounced%2520out-of-domain.%2520We%2520achieve%2520state-of-the-art%250Aresults%2520on%2520the%2520MTEB%2520benchmark%2520with%2520no%2520hard%2520negative%2520mining%252C%2520score%2520distillation%252C%250Adataset-specific%2520instructions%252C%2520intra-GPU%2520example-sharing%252C%2520or%2520extremely%2520large%250Abatch%2520sizes.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520improve%2520performance%2520on%2520any%250Acontrastive%2520learning%2520dataset%2520and%2520any%2520biencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02525v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Document%20Embeddings&entry.906535625=John%20X.%20Morris%20and%20Alexander%20M.%20Rush&entry.1292438233=%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02525v3&entry.124074799=Read"},
{"title": "Decomposing The Dark Matter of Sparse Autoencoders", "author": "Joshua Engels and Logan Riggs and Max Tegmark", "abstract": "  Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter--about half of the error vector itself and >90% of its\nnorm--can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations, including postulating a new type of \"introduced error\";\nthese insights imply that the part of the SAE error vector that cannot be\nlinearly predicted (\"nonlinear\" error) might be fundamentally different from\nthe linearly predictable component. To validate this hypothesis, we empirically\nanalyze nonlinear SAE error and show that 1) it contains fewer not yet learned\nfeatures, 2) SAEs trained on it are quantitatively worse, 3) it helps predict\nSAE per-token scaling behavior, and 4) it is responsible for a proportional\namount of the downstream increase in cross entropy loss when SAE activations\nare inserted into the model. Finally, we examine two methods to reduce\nnonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which\nleads to a very slight decrease in nonlinear error, and linear transformations\nfrom earlier layer SAE outputs, which leads to a larger reduction.\n", "link": "http://arxiv.org/abs/2410.14670v1", "date": "2024-10-18", "relevancy": 2.5235, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20The%20Dark%20Matter%20of%20Sparse%20Autoencoders&body=Title%3A%20Decomposing%20The%20Dark%20Matter%20of%20Sparse%20Autoencoders%0AAuthor%3A%20Joshua%20Engels%20and%20Logan%20Riggs%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20technique%20for%20decomposing%20language%0Amodel%20activations%20into%20interpretable%20linear%20features.%20However%2C%20current%20SAEs%0Afall%20short%20of%20completely%20explaining%20model%20performance%2C%20resulting%20in%20%22dark%0Amatter%22%3A%20unexplained%20variance%20in%20activations.%20This%20work%20investigates%20dark%0Amatter%20as%20an%20object%20of%20study%20in%20its%20own%20right.%20Surprisingly%2C%20we%20find%20that%20much%0Aof%20SAE%20dark%20matter--about%20half%20of%20the%20error%20vector%20itself%20and%20%3E90%25%20of%20its%0Anorm--can%20be%20linearly%20predicted%20from%20the%20initial%20activation%20vector.%0AAdditionally%2C%20we%20find%20that%20the%20scaling%20behavior%20of%20SAE%20error%20norms%20at%20a%20per%0Atoken%20level%20is%20remarkably%20predictable%3A%20larger%20SAEs%20mostly%20struggle%20to%0Areconstruct%20the%20same%20contexts%20as%20smaller%20SAEs.%20We%20build%20on%20the%20linear%0Arepresentation%20hypothesis%20to%20propose%20models%20of%20activations%20that%20might%20lead%20to%0Athese%20observations%2C%20including%20postulating%20a%20new%20type%20of%20%22introduced%20error%22%3B%0Athese%20insights%20imply%20that%20the%20part%20of%20the%20SAE%20error%20vector%20that%20cannot%20be%0Alinearly%20predicted%20%28%22nonlinear%22%20error%29%20might%20be%20fundamentally%20different%20from%0Athe%20linearly%20predictable%20component.%20To%20validate%20this%20hypothesis%2C%20we%20empirically%0Aanalyze%20nonlinear%20SAE%20error%20and%20show%20that%201%29%20it%20contains%20fewer%20not%20yet%20learned%0Afeatures%2C%202%29%20SAEs%20trained%20on%20it%20are%20quantitatively%20worse%2C%203%29%20it%20helps%20predict%0ASAE%20per-token%20scaling%20behavior%2C%20and%204%29%20it%20is%20responsible%20for%20a%20proportional%0Aamount%20of%20the%20downstream%20increase%20in%20cross%20entropy%20loss%20when%20SAE%20activations%0Aare%20inserted%20into%20the%20model.%20Finally%2C%20we%20examine%20two%20methods%20to%20reduce%0Anonlinear%20SAE%20error%20at%20a%20fixed%20sparsity%3A%20inference%20time%20gradient%20pursuit%2C%20which%0Aleads%20to%20a%20very%20slight%20decrease%20in%20nonlinear%20error%2C%20and%20linear%20transformations%0Afrom%20earlier%20layer%20SAE%20outputs%2C%20which%20leads%20to%20a%20larger%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520The%2520Dark%2520Matter%2520of%2520Sparse%2520Autoencoders%26entry.906535625%3DJoshua%2520Engels%2520and%2520Logan%2520Riggs%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520technique%2520for%2520decomposing%2520language%250Amodel%2520activations%2520into%2520interpretable%2520linear%2520features.%2520However%252C%2520current%2520SAEs%250Afall%2520short%2520of%2520completely%2520explaining%2520model%2520performance%252C%2520resulting%2520in%2520%2522dark%250Amatter%2522%253A%2520unexplained%2520variance%2520in%2520activations.%2520This%2520work%2520investigates%2520dark%250Amatter%2520as%2520an%2520object%2520of%2520study%2520in%2520its%2520own%2520right.%2520Surprisingly%252C%2520we%2520find%2520that%2520much%250Aof%2520SAE%2520dark%2520matter--about%2520half%2520of%2520the%2520error%2520vector%2520itself%2520and%2520%253E90%2525%2520of%2520its%250Anorm--can%2520be%2520linearly%2520predicted%2520from%2520the%2520initial%2520activation%2520vector.%250AAdditionally%252C%2520we%2520find%2520that%2520the%2520scaling%2520behavior%2520of%2520SAE%2520error%2520norms%2520at%2520a%2520per%250Atoken%2520level%2520is%2520remarkably%2520predictable%253A%2520larger%2520SAEs%2520mostly%2520struggle%2520to%250Areconstruct%2520the%2520same%2520contexts%2520as%2520smaller%2520SAEs.%2520We%2520build%2520on%2520the%2520linear%250Arepresentation%2520hypothesis%2520to%2520propose%2520models%2520of%2520activations%2520that%2520might%2520lead%2520to%250Athese%2520observations%252C%2520including%2520postulating%2520a%2520new%2520type%2520of%2520%2522introduced%2520error%2522%253B%250Athese%2520insights%2520imply%2520that%2520the%2520part%2520of%2520the%2520SAE%2520error%2520vector%2520that%2520cannot%2520be%250Alinearly%2520predicted%2520%2528%2522nonlinear%2522%2520error%2529%2520might%2520be%2520fundamentally%2520different%2520from%250Athe%2520linearly%2520predictable%2520component.%2520To%2520validate%2520this%2520hypothesis%252C%2520we%2520empirically%250Aanalyze%2520nonlinear%2520SAE%2520error%2520and%2520show%2520that%25201%2529%2520it%2520contains%2520fewer%2520not%2520yet%2520learned%250Afeatures%252C%25202%2529%2520SAEs%2520trained%2520on%2520it%2520are%2520quantitatively%2520worse%252C%25203%2529%2520it%2520helps%2520predict%250ASAE%2520per-token%2520scaling%2520behavior%252C%2520and%25204%2529%2520it%2520is%2520responsible%2520for%2520a%2520proportional%250Aamount%2520of%2520the%2520downstream%2520increase%2520in%2520cross%2520entropy%2520loss%2520when%2520SAE%2520activations%250Aare%2520inserted%2520into%2520the%2520model.%2520Finally%252C%2520we%2520examine%2520two%2520methods%2520to%2520reduce%250Anonlinear%2520SAE%2520error%2520at%2520a%2520fixed%2520sparsity%253A%2520inference%2520time%2520gradient%2520pursuit%252C%2520which%250Aleads%2520to%2520a%2520very%2520slight%2520decrease%2520in%2520nonlinear%2520error%252C%2520and%2520linear%2520transformations%250Afrom%2520earlier%2520layer%2520SAE%2520outputs%252C%2520which%2520leads%2520to%2520a%2520larger%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20The%20Dark%20Matter%20of%20Sparse%20Autoencoders&entry.906535625=Joshua%20Engels%20and%20Logan%20Riggs%20and%20Max%20Tegmark&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20technique%20for%20decomposing%20language%0Amodel%20activations%20into%20interpretable%20linear%20features.%20However%2C%20current%20SAEs%0Afall%20short%20of%20completely%20explaining%20model%20performance%2C%20resulting%20in%20%22dark%0Amatter%22%3A%20unexplained%20variance%20in%20activations.%20This%20work%20investigates%20dark%0Amatter%20as%20an%20object%20of%20study%20in%20its%20own%20right.%20Surprisingly%2C%20we%20find%20that%20much%0Aof%20SAE%20dark%20matter--about%20half%20of%20the%20error%20vector%20itself%20and%20%3E90%25%20of%20its%0Anorm--can%20be%20linearly%20predicted%20from%20the%20initial%20activation%20vector.%0AAdditionally%2C%20we%20find%20that%20the%20scaling%20behavior%20of%20SAE%20error%20norms%20at%20a%20per%0Atoken%20level%20is%20remarkably%20predictable%3A%20larger%20SAEs%20mostly%20struggle%20to%0Areconstruct%20the%20same%20contexts%20as%20smaller%20SAEs.%20We%20build%20on%20the%20linear%0Arepresentation%20hypothesis%20to%20propose%20models%20of%20activations%20that%20might%20lead%20to%0Athese%20observations%2C%20including%20postulating%20a%20new%20type%20of%20%22introduced%20error%22%3B%0Athese%20insights%20imply%20that%20the%20part%20of%20the%20SAE%20error%20vector%20that%20cannot%20be%0Alinearly%20predicted%20%28%22nonlinear%22%20error%29%20might%20be%20fundamentally%20different%20from%0Athe%20linearly%20predictable%20component.%20To%20validate%20this%20hypothesis%2C%20we%20empirically%0Aanalyze%20nonlinear%20SAE%20error%20and%20show%20that%201%29%20it%20contains%20fewer%20not%20yet%20learned%0Afeatures%2C%202%29%20SAEs%20trained%20on%20it%20are%20quantitatively%20worse%2C%203%29%20it%20helps%20predict%0ASAE%20per-token%20scaling%20behavior%2C%20and%204%29%20it%20is%20responsible%20for%20a%20proportional%0Aamount%20of%20the%20downstream%20increase%20in%20cross%20entropy%20loss%20when%20SAE%20activations%0Aare%20inserted%20into%20the%20model.%20Finally%2C%20we%20examine%20two%20methods%20to%20reduce%0Anonlinear%20SAE%20error%20at%20a%20fixed%20sparsity%3A%20inference%20time%20gradient%20pursuit%2C%20which%0Aleads%20to%20a%20very%20slight%20decrease%20in%20nonlinear%20error%2C%20and%20linear%20transformations%0Afrom%20earlier%20layer%20SAE%20outputs%2C%20which%20leads%20to%20a%20larger%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14670v1&entry.124074799=Read"},
{"title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding", "author": "Yan Shu and Peitian Zhang and Zheng Liu and Minghao Qin and Junjie Zhou and Tiejun Huang and Bo Zhao", "abstract": "  Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation.\n", "link": "http://arxiv.org/abs/2409.14485v3", "date": "2024-10-18", "relevancy": 2.5031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6391}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-XL%3A%20Extra-Long%20Vision%20Language%20Model%20for%20Hour-Scale%20Video%0A%20%20Understanding&body=Title%3A%20Video-XL%3A%20Extra-Long%20Vision%20Language%20Model%20for%20Hour-Scale%20Video%0A%20%20Understanding%0AAuthor%3A%20Yan%20Shu%20and%20Peitian%20Zhang%20and%20Zheng%20Liu%20and%20Minghao%20Qin%20and%20Junjie%20Zhou%20and%20Tiejun%20Huang%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20Although%20current%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%0Apromising%20results%20in%20video%20understanding%2C%20processing%20extremely%20long%20videos%0Aremains%20an%20ongoing%20challenge.%20Typically%2C%20MLLMs%20struggle%20with%20handling%20thousands%0Aof%20visual%20tokens%20that%20exceed%20the%20maximum%20context%20length%2C%20and%20they%20suffer%20from%0Athe%20information%20decay%20due%20to%20token%20aggregation.%20Another%20challenge%20is%20the%20high%0Acomputational%20cost%20stemming%20from%20the%20large%20number%20of%20video%20tokens.%20To%20tackle%0Athese%20issues%2C%20we%20propose%20Video-XL%2C%20an%20extra-long%20vision%20language%20model%20designed%0Afor%20efficient%20hour-scale%20video%20understanding.%20Specifically%2C%20we%20argue%20that%20LLMs%0Acan%20be%20adapted%20as%20effective%20visual%20condensers%20and%20propose%20Visual%20Context%20Latent%0ASummarization%20which%20condenses%20visual%20contexts%20into%20highly%20compact%20forms.%0AExtensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20promising%20results%20on%0Apopular%20long%20video%20understanding%20benchmarks.%20For%20example%2C%20Video-XL%20outperforms%0Athe%20current%20state-of-the-art%20method%20on%20VNBench%20by%20nearly%2010%5C%25%20in%20accuracy.%0AMoreover%2C%20Video-XL%20presents%20an%20impressive%20balance%20between%20efficiency%20and%0Aeffectiveness%2C%20processing%202048%20frames%20on%20a%20single%2080GB%20GPU%20while%20achieving%0Anearly%2095%25%20accuracy%20in%20the%20Needle-in-a-Haystack%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-XL%253A%2520Extra-Long%2520Vision%2520Language%2520Model%2520for%2520Hour-Scale%2520Video%250A%2520%2520Understanding%26entry.906535625%3DYan%2520Shu%2520and%2520Peitian%2520Zhang%2520and%2520Zheng%2520Liu%2520and%2520Minghao%2520Qin%2520and%2520Junjie%2520Zhou%2520and%2520Tiejun%2520Huang%2520and%2520Bo%2520Zhao%26entry.1292438233%3D%2520%2520Although%2520current%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%250Apromising%2520results%2520in%2520video%2520understanding%252C%2520processing%2520extremely%2520long%2520videos%250Aremains%2520an%2520ongoing%2520challenge.%2520Typically%252C%2520MLLMs%2520struggle%2520with%2520handling%2520thousands%250Aof%2520visual%2520tokens%2520that%2520exceed%2520the%2520maximum%2520context%2520length%252C%2520and%2520they%2520suffer%2520from%250Athe%2520information%2520decay%2520due%2520to%2520token%2520aggregation.%2520Another%2520challenge%2520is%2520the%2520high%250Acomputational%2520cost%2520stemming%2520from%2520the%2520large%2520number%2520of%2520video%2520tokens.%2520To%2520tackle%250Athese%2520issues%252C%2520we%2520propose%2520Video-XL%252C%2520an%2520extra-long%2520vision%2520language%2520model%2520designed%250Afor%2520efficient%2520hour-scale%2520video%2520understanding.%2520Specifically%252C%2520we%2520argue%2520that%2520LLMs%250Acan%2520be%2520adapted%2520as%2520effective%2520visual%2520condensers%2520and%2520propose%2520Visual%2520Context%2520Latent%250ASummarization%2520which%2520condenses%2520visual%2520contexts%2520into%2520highly%2520compact%2520forms.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520achieves%2520promising%2520results%2520on%250Apopular%2520long%2520video%2520understanding%2520benchmarks.%2520For%2520example%252C%2520Video-XL%2520outperforms%250Athe%2520current%2520state-of-the-art%2520method%2520on%2520VNBench%2520by%2520nearly%252010%255C%2525%2520in%2520accuracy.%250AMoreover%252C%2520Video-XL%2520presents%2520an%2520impressive%2520balance%2520between%2520efficiency%2520and%250Aeffectiveness%252C%2520processing%25202048%2520frames%2520on%2520a%2520single%252080GB%2520GPU%2520while%2520achieving%250Anearly%252095%2525%2520accuracy%2520in%2520the%2520Needle-in-a-Haystack%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-XL%3A%20Extra-Long%20Vision%20Language%20Model%20for%20Hour-Scale%20Video%0A%20%20Understanding&entry.906535625=Yan%20Shu%20and%20Peitian%20Zhang%20and%20Zheng%20Liu%20and%20Minghao%20Qin%20and%20Junjie%20Zhou%20and%20Tiejun%20Huang%20and%20Bo%20Zhao&entry.1292438233=%20%20Although%20current%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%0Apromising%20results%20in%20video%20understanding%2C%20processing%20extremely%20long%20videos%0Aremains%20an%20ongoing%20challenge.%20Typically%2C%20MLLMs%20struggle%20with%20handling%20thousands%0Aof%20visual%20tokens%20that%20exceed%20the%20maximum%20context%20length%2C%20and%20they%20suffer%20from%0Athe%20information%20decay%20due%20to%20token%20aggregation.%20Another%20challenge%20is%20the%20high%0Acomputational%20cost%20stemming%20from%20the%20large%20number%20of%20video%20tokens.%20To%20tackle%0Athese%20issues%2C%20we%20propose%20Video-XL%2C%20an%20extra-long%20vision%20language%20model%20designed%0Afor%20efficient%20hour-scale%20video%20understanding.%20Specifically%2C%20we%20argue%20that%20LLMs%0Acan%20be%20adapted%20as%20effective%20visual%20condensers%20and%20propose%20Visual%20Context%20Latent%0ASummarization%20which%20condenses%20visual%20contexts%20into%20highly%20compact%20forms.%0AExtensive%20experiments%20demonstrate%20that%20our%20model%20achieves%20promising%20results%20on%0Apopular%20long%20video%20understanding%20benchmarks.%20For%20example%2C%20Video-XL%20outperforms%0Athe%20current%20state-of-the-art%20method%20on%20VNBench%20by%20nearly%2010%5C%25%20in%20accuracy.%0AMoreover%2C%20Video-XL%20presents%20an%20impressive%20balance%20between%20efficiency%20and%0Aeffectiveness%2C%20processing%202048%20frames%20on%20a%20single%2080GB%20GPU%20while%20achieving%0Anearly%2095%25%20accuracy%20in%20the%20Needle-in-a-Haystack%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14485v3&entry.124074799=Read"},
{"title": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs", "author": "Runchu Tian and Yanghao Li and Yuepeng Fu and Siyang Deng and Qinyu Luo and Cheng Qian and Shuo Wang and Xin Cong and Zhong Zhang and Yesai Wu and Yankai Lin and Huadong Wang and Xiaojiang Liu", "abstract": "  Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.\n", "link": "http://arxiv.org/abs/2410.14641v1", "date": "2024-10-18", "relevancy": 2.4663, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance%20between%20Relevant%20Information%20Pieces%20Causes%20Bias%20in%20Long-Context%0A%20%20LLMs&body=Title%3A%20Distance%20between%20Relevant%20Information%20Pieces%20Causes%20Bias%20in%20Long-Context%0A%20%20LLMs%0AAuthor%3A%20Runchu%20Tian%20and%20Yanghao%20Li%20and%20Yuepeng%20Fu%20and%20Siyang%20Deng%20and%20Qinyu%20Luo%20and%20Cheng%20Qian%20and%20Shuo%20Wang%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yesai%20Wu%20and%20Yankai%20Lin%20and%20Huadong%20Wang%20and%20Xiaojiang%20Liu%0AAbstract%3A%20%20%20Positional%20bias%20in%20large%20language%20models%20%28LLMs%29%20hinders%20their%20ability%20to%0Aeffectively%20process%20long%20inputs.%20A%20prominent%20example%20is%20the%20%22lost%20in%20the%0Amiddle%22%20phenomenon%2C%20where%20LLMs%20struggle%20to%20utilize%20relevant%20information%0Asituated%20in%20the%20middle%20of%20the%20input.%20While%20prior%20research%20primarily%20focuses%20on%0Asingle%20pieces%20of%20relevant%20information%2C%20real-world%20applications%20often%20involve%0Amultiple%20relevant%20information%20pieces.%20To%20bridge%20this%20gap%2C%20we%20present%0ALongPiBench%2C%20a%20benchmark%20designed%20to%20assess%20positional%20bias%20involving%20multiple%0Apieces%20of%20relevant%20information.%20Thorough%20experiments%20are%20conducted%20with%20five%0Acommercial%20and%20six%20open-source%20models.%20These%20experiments%20reveal%20that%20while%20most%0Acurrent%20models%20are%20robust%20against%20the%20%22lost%20in%20the%20middle%22%20issue%2C%20there%20exist%0Asignificant%20biases%20related%20to%20the%20spacing%20of%20relevant%20information%20pieces.%20These%0Afindings%20highlight%20the%20importance%20of%20evaluating%20and%20reducing%20positional%20biases%0Ato%20advance%20LLM%27s%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance%2520between%2520Relevant%2520Information%2520Pieces%2520Causes%2520Bias%2520in%2520Long-Context%250A%2520%2520LLMs%26entry.906535625%3DRunchu%2520Tian%2520and%2520Yanghao%2520Li%2520and%2520Yuepeng%2520Fu%2520and%2520Siyang%2520Deng%2520and%2520Qinyu%2520Luo%2520and%2520Cheng%2520Qian%2520and%2520Shuo%2520Wang%2520and%2520Xin%2520Cong%2520and%2520Zhong%2520Zhang%2520and%2520Yesai%2520Wu%2520and%2520Yankai%2520Lin%2520and%2520Huadong%2520Wang%2520and%2520Xiaojiang%2520Liu%26entry.1292438233%3D%2520%2520Positional%2520bias%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520hinders%2520their%2520ability%2520to%250Aeffectively%2520process%2520long%2520inputs.%2520A%2520prominent%2520example%2520is%2520the%2520%2522lost%2520in%2520the%250Amiddle%2522%2520phenomenon%252C%2520where%2520LLMs%2520struggle%2520to%2520utilize%2520relevant%2520information%250Asituated%2520in%2520the%2520middle%2520of%2520the%2520input.%2520While%2520prior%2520research%2520primarily%2520focuses%2520on%250Asingle%2520pieces%2520of%2520relevant%2520information%252C%2520real-world%2520applications%2520often%2520involve%250Amultiple%2520relevant%2520information%2520pieces.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%250ALongPiBench%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520positional%2520bias%2520involving%2520multiple%250Apieces%2520of%2520relevant%2520information.%2520Thorough%2520experiments%2520are%2520conducted%2520with%2520five%250Acommercial%2520and%2520six%2520open-source%2520models.%2520These%2520experiments%2520reveal%2520that%2520while%2520most%250Acurrent%2520models%2520are%2520robust%2520against%2520the%2520%2522lost%2520in%2520the%2520middle%2522%2520issue%252C%2520there%2520exist%250Asignificant%2520biases%2520related%2520to%2520the%2520spacing%2520of%2520relevant%2520information%2520pieces.%2520These%250Afindings%2520highlight%2520the%2520importance%2520of%2520evaluating%2520and%2520reducing%2520positional%2520biases%250Ato%2520advance%2520LLM%2527s%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance%20between%20Relevant%20Information%20Pieces%20Causes%20Bias%20in%20Long-Context%0A%20%20LLMs&entry.906535625=Runchu%20Tian%20and%20Yanghao%20Li%20and%20Yuepeng%20Fu%20and%20Siyang%20Deng%20and%20Qinyu%20Luo%20and%20Cheng%20Qian%20and%20Shuo%20Wang%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yesai%20Wu%20and%20Yankai%20Lin%20and%20Huadong%20Wang%20and%20Xiaojiang%20Liu&entry.1292438233=%20%20Positional%20bias%20in%20large%20language%20models%20%28LLMs%29%20hinders%20their%20ability%20to%0Aeffectively%20process%20long%20inputs.%20A%20prominent%20example%20is%20the%20%22lost%20in%20the%0Amiddle%22%20phenomenon%2C%20where%20LLMs%20struggle%20to%20utilize%20relevant%20information%0Asituated%20in%20the%20middle%20of%20the%20input.%20While%20prior%20research%20primarily%20focuses%20on%0Asingle%20pieces%20of%20relevant%20information%2C%20real-world%20applications%20often%20involve%0Amultiple%20relevant%20information%20pieces.%20To%20bridge%20this%20gap%2C%20we%20present%0ALongPiBench%2C%20a%20benchmark%20designed%20to%20assess%20positional%20bias%20involving%20multiple%0Apieces%20of%20relevant%20information.%20Thorough%20experiments%20are%20conducted%20with%20five%0Acommercial%20and%20six%20open-source%20models.%20These%20experiments%20reveal%20that%20while%20most%0Acurrent%20models%20are%20robust%20against%20the%20%22lost%20in%20the%20middle%22%20issue%2C%20there%20exist%0Asignificant%20biases%20related%20to%20the%20spacing%20of%20relevant%20information%20pieces.%20These%0Afindings%20highlight%20the%20importance%20of%20evaluating%20and%20reducing%20positional%20biases%0Ato%20advance%20LLM%27s%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14641v1&entry.124074799=Read"},
{"title": "A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning", "author": "Shengjie Sun and Runze Liu and Jiafei Lyu and Jing-Wen Yang and Liangpeng Zhang and Xiu Li", "abstract": "  Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks.\n", "link": "http://arxiv.org/abs/2410.14660v1", "date": "2024-10-18", "relevancy": 2.425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large%20Language%20Model-Driven%20Reward%20Design%20Framework%20via%20Dynamic%0A%20%20Feedback%20for%20Reinforcement%20Learning&body=Title%3A%20A%20Large%20Language%20Model-Driven%20Reward%20Design%20Framework%20via%20Dynamic%0A%20%20Feedback%20for%20Reinforcement%20Learning%0AAuthor%3A%20Shengjie%20Sun%20and%20Runze%20Liu%20and%20Jiafei%20Lyu%20and%20Jing-Wen%20Yang%20and%20Liangpeng%20Zhang%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20significant%20potential%20in%20designing%0Areward%20functions%20for%20Reinforcement%20Learning%20%28RL%29%20tasks.%20However%2C%20obtaining%0Ahigh-quality%20reward%20code%20often%20involves%20human%20intervention%2C%20numerous%20LLM%0Aqueries%2C%20or%20repetitive%20RL%20training.%20To%20address%20these%20issues%2C%20we%20propose%20CARD%2C%20a%0ALLM-driven%20Reward%20Design%20framework%20that%20iteratively%20generates%20and%20improves%0Areward%20function%20code.%20Specifically%2C%20CARD%20includes%20a%20Coder%20that%20generates%20and%0Averifies%20the%20code%2C%20while%20a%20Evaluator%20provides%20dynamic%20feedback%20to%20guide%20the%0ACoder%20in%20improving%20the%20code%2C%20eliminating%20the%20need%20for%20human%20feedback.%20In%0Aaddition%20to%20process%20feedback%20and%20trajectory%20feedback%2C%20we%20introduce%20Trajectory%0APreference%20Evaluation%20%28TPE%29%2C%20which%20evaluates%20the%20current%20reward%20function%20based%0Aon%20trajectory%20preferences.%20If%20the%20code%20fails%20the%20TPE%2C%20the%20Evaluator%20provides%0Apreference%20feedback%2C%20avoiding%20RL%20training%20at%20every%20iteration%20and%20making%20the%0Areward%20function%20better%20aligned%20with%20the%20task%20objective.%20Empirical%20results%20on%0AMeta-World%20and%20ManiSkill2%20demonstrate%20that%20our%20method%20achieves%20an%20effective%0Abalance%20between%20task%20performance%20and%20token%20efficiency%2C%20outperforming%20or%0Amatching%20the%20baselines%20across%20all%20tasks.%20On%2010%20out%20of%2012%20tasks%2C%20CARD%20shows%0Abetter%20or%20comparable%20performance%20to%20policies%20trained%20with%20expert-designed%0Arewards%2C%20and%20our%20method%20even%20surpasses%20the%20oracle%20on%203%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large%2520Language%2520Model-Driven%2520Reward%2520Design%2520Framework%2520via%2520Dynamic%250A%2520%2520Feedback%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DShengjie%2520Sun%2520and%2520Runze%2520Liu%2520and%2520Jiafei%2520Lyu%2520and%2520Jing-Wen%2520Yang%2520and%2520Liangpeng%2520Zhang%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520potential%2520in%2520designing%250Areward%2520functions%2520for%2520Reinforcement%2520Learning%2520%2528RL%2529%2520tasks.%2520However%252C%2520obtaining%250Ahigh-quality%2520reward%2520code%2520often%2520involves%2520human%2520intervention%252C%2520numerous%2520LLM%250Aqueries%252C%2520or%2520repetitive%2520RL%2520training.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520CARD%252C%2520a%250ALLM-driven%2520Reward%2520Design%2520framework%2520that%2520iteratively%2520generates%2520and%2520improves%250Areward%2520function%2520code.%2520Specifically%252C%2520CARD%2520includes%2520a%2520Coder%2520that%2520generates%2520and%250Averifies%2520the%2520code%252C%2520while%2520a%2520Evaluator%2520provides%2520dynamic%2520feedback%2520to%2520guide%2520the%250ACoder%2520in%2520improving%2520the%2520code%252C%2520eliminating%2520the%2520need%2520for%2520human%2520feedback.%2520In%250Aaddition%2520to%2520process%2520feedback%2520and%2520trajectory%2520feedback%252C%2520we%2520introduce%2520Trajectory%250APreference%2520Evaluation%2520%2528TPE%2529%252C%2520which%2520evaluates%2520the%2520current%2520reward%2520function%2520based%250Aon%2520trajectory%2520preferences.%2520If%2520the%2520code%2520fails%2520the%2520TPE%252C%2520the%2520Evaluator%2520provides%250Apreference%2520feedback%252C%2520avoiding%2520RL%2520training%2520at%2520every%2520iteration%2520and%2520making%2520the%250Areward%2520function%2520better%2520aligned%2520with%2520the%2520task%2520objective.%2520Empirical%2520results%2520on%250AMeta-World%2520and%2520ManiSkill2%2520demonstrate%2520that%2520our%2520method%2520achieves%2520an%2520effective%250Abalance%2520between%2520task%2520performance%2520and%2520token%2520efficiency%252C%2520outperforming%2520or%250Amatching%2520the%2520baselines%2520across%2520all%2520tasks.%2520On%252010%2520out%2520of%252012%2520tasks%252C%2520CARD%2520shows%250Abetter%2520or%2520comparable%2520performance%2520to%2520policies%2520trained%2520with%2520expert-designed%250Arewards%252C%2520and%2520our%2520method%2520even%2520surpasses%2520the%2520oracle%2520on%25203%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large%20Language%20Model-Driven%20Reward%20Design%20Framework%20via%20Dynamic%0A%20%20Feedback%20for%20Reinforcement%20Learning&entry.906535625=Shengjie%20Sun%20and%20Runze%20Liu%20and%20Jiafei%20Lyu%20and%20Jing-Wen%20Yang%20and%20Liangpeng%20Zhang%20and%20Xiu%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20significant%20potential%20in%20designing%0Areward%20functions%20for%20Reinforcement%20Learning%20%28RL%29%20tasks.%20However%2C%20obtaining%0Ahigh-quality%20reward%20code%20often%20involves%20human%20intervention%2C%20numerous%20LLM%0Aqueries%2C%20or%20repetitive%20RL%20training.%20To%20address%20these%20issues%2C%20we%20propose%20CARD%2C%20a%0ALLM-driven%20Reward%20Design%20framework%20that%20iteratively%20generates%20and%20improves%0Areward%20function%20code.%20Specifically%2C%20CARD%20includes%20a%20Coder%20that%20generates%20and%0Averifies%20the%20code%2C%20while%20a%20Evaluator%20provides%20dynamic%20feedback%20to%20guide%20the%0ACoder%20in%20improving%20the%20code%2C%20eliminating%20the%20need%20for%20human%20feedback.%20In%0Aaddition%20to%20process%20feedback%20and%20trajectory%20feedback%2C%20we%20introduce%20Trajectory%0APreference%20Evaluation%20%28TPE%29%2C%20which%20evaluates%20the%20current%20reward%20function%20based%0Aon%20trajectory%20preferences.%20If%20the%20code%20fails%20the%20TPE%2C%20the%20Evaluator%20provides%0Apreference%20feedback%2C%20avoiding%20RL%20training%20at%20every%20iteration%20and%20making%20the%0Areward%20function%20better%20aligned%20with%20the%20task%20objective.%20Empirical%20results%20on%0AMeta-World%20and%20ManiSkill2%20demonstrate%20that%20our%20method%20achieves%20an%20effective%0Abalance%20between%20task%20performance%20and%20token%20efficiency%2C%20outperforming%20or%0Amatching%20the%20baselines%20across%20all%20tasks.%20On%2010%20out%20of%2012%20tasks%2C%20CARD%20shows%0Abetter%20or%20comparable%20performance%20to%20policies%20trained%20with%20expert-designed%0Arewards%2C%20and%20our%20method%20even%20surpasses%20the%20oracle%20on%203%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14660v1&entry.124074799=Read"},
{"title": "Large Language Models Are Overparameterized Text Encoders", "author": "Thennal D K and Tim Fischer and Chris Biemann", "abstract": "  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n", "link": "http://arxiv.org/abs/2410.14578v1", "date": "2024-10-18", "relevancy": 2.4135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Are%20Overparameterized%20Text%20Encoders&body=Title%3A%20Large%20Language%20Models%20Are%20Overparameterized%20Text%20Encoders%0AAuthor%3A%20Thennal%20D%20K%20and%20Tim%20Fischer%20and%20Chris%20Biemann%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20performance%20as%20text%20embedding%0Amodels%20when%20finetuned%20with%20supervised%20contrastive%20training.%20However%2C%20their%0Alarge%20size%20balloons%20inference%20time%20and%20memory%20requirements.%20In%20this%20paper%2C%20we%0Ashow%20that%20by%20pruning%20the%20last%20%24p%5C%25%24%20layers%20of%20an%20LLM%20before%20supervised%20training%0Afor%20only%201000%20steps%2C%20we%20can%20achieve%20a%20proportional%20reduction%20in%20memory%20and%0Ainference%20time.%20We%20evaluate%20four%20different%20state-of-the-art%20LLMs%20on%20text%0Aembedding%20tasks%20and%20find%20that%20our%20method%20can%20prune%20up%20to%2030%5C%25%20of%20layers%20with%0Anegligible%20impact%20on%20performance%20and%20up%20to%2080%5C%25%20with%20only%20a%20modest%20drop.%20With%0Aonly%20three%20lines%20of%20code%2C%20our%20method%20is%20easily%20implemented%20in%20any%20pipeline%20for%0Atransforming%20LLMs%20to%20text%20encoders.%20We%20also%20propose%20%24%5Ctext%7BL%7D%5E3%20%5Ctext%7BPrune%7D%24%2C%0Aa%20novel%20layer-pruning%20strategy%20based%20on%20the%20model%27s%20initial%20loss%20that%20provides%0Atwo%20optimal%20pruning%20configurations%3A%20a%20large%20variant%20with%20negligible%20performance%0Aloss%20and%20a%20small%20variant%20for%20resource-constrained%20settings.%20On%20average%2C%20the%0Alarge%20variant%20prunes%2021%5C%25%20of%20the%20parameters%20with%20a%20%24-0.3%24%20performance%20drop%2C%20and%0Athe%20small%20variant%20only%20suffers%20from%20a%20%24-5.1%24%20decrease%20while%20pruning%2074%5C%25%20of%20the%0Amodel.%20We%20consider%20these%20results%20strong%20evidence%20that%20LLMs%20are%0Aoverparameterized%20for%20text%20embedding%20tasks%2C%20and%20can%20be%20easily%20pruned.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Are%2520Overparameterized%2520Text%2520Encoders%26entry.906535625%3DThennal%2520D%2520K%2520and%2520Tim%2520Fischer%2520and%2520Chris%2520Biemann%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520performance%2520as%2520text%2520embedding%250Amodels%2520when%2520finetuned%2520with%2520supervised%2520contrastive%2520training.%2520However%252C%2520their%250Alarge%2520size%2520balloons%2520inference%2520time%2520and%2520memory%2520requirements.%2520In%2520this%2520paper%252C%2520we%250Ashow%2520that%2520by%2520pruning%2520the%2520last%2520%2524p%255C%2525%2524%2520layers%2520of%2520an%2520LLM%2520before%2520supervised%2520training%250Afor%2520only%25201000%2520steps%252C%2520we%2520can%2520achieve%2520a%2520proportional%2520reduction%2520in%2520memory%2520and%250Ainference%2520time.%2520We%2520evaluate%2520four%2520different%2520state-of-the-art%2520LLMs%2520on%2520text%250Aembedding%2520tasks%2520and%2520find%2520that%2520our%2520method%2520can%2520prune%2520up%2520to%252030%255C%2525%2520of%2520layers%2520with%250Anegligible%2520impact%2520on%2520performance%2520and%2520up%2520to%252080%255C%2525%2520with%2520only%2520a%2520modest%2520drop.%2520With%250Aonly%2520three%2520lines%2520of%2520code%252C%2520our%2520method%2520is%2520easily%2520implemented%2520in%2520any%2520pipeline%2520for%250Atransforming%2520LLMs%2520to%2520text%2520encoders.%2520We%2520also%2520propose%2520%2524%255Ctext%257BL%257D%255E3%2520%255Ctext%257BPrune%257D%2524%252C%250Aa%2520novel%2520layer-pruning%2520strategy%2520based%2520on%2520the%2520model%2527s%2520initial%2520loss%2520that%2520provides%250Atwo%2520optimal%2520pruning%2520configurations%253A%2520a%2520large%2520variant%2520with%2520negligible%2520performance%250Aloss%2520and%2520a%2520small%2520variant%2520for%2520resource-constrained%2520settings.%2520On%2520average%252C%2520the%250Alarge%2520variant%2520prunes%252021%255C%2525%2520of%2520the%2520parameters%2520with%2520a%2520%2524-0.3%2524%2520performance%2520drop%252C%2520and%250Athe%2520small%2520variant%2520only%2520suffers%2520from%2520a%2520%2524-5.1%2524%2520decrease%2520while%2520pruning%252074%255C%2525%2520of%2520the%250Amodel.%2520We%2520consider%2520these%2520results%2520strong%2520evidence%2520that%2520LLMs%2520are%250Aoverparameterized%2520for%2520text%2520embedding%2520tasks%252C%2520and%2520can%2520be%2520easily%2520pruned.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Are%20Overparameterized%20Text%20Encoders&entry.906535625=Thennal%20D%20K%20and%20Tim%20Fischer%20and%20Chris%20Biemann&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20performance%20as%20text%20embedding%0Amodels%20when%20finetuned%20with%20supervised%20contrastive%20training.%20However%2C%20their%0Alarge%20size%20balloons%20inference%20time%20and%20memory%20requirements.%20In%20this%20paper%2C%20we%0Ashow%20that%20by%20pruning%20the%20last%20%24p%5C%25%24%20layers%20of%20an%20LLM%20before%20supervised%20training%0Afor%20only%201000%20steps%2C%20we%20can%20achieve%20a%20proportional%20reduction%20in%20memory%20and%0Ainference%20time.%20We%20evaluate%20four%20different%20state-of-the-art%20LLMs%20on%20text%0Aembedding%20tasks%20and%20find%20that%20our%20method%20can%20prune%20up%20to%2030%5C%25%20of%20layers%20with%0Anegligible%20impact%20on%20performance%20and%20up%20to%2080%5C%25%20with%20only%20a%20modest%20drop.%20With%0Aonly%20three%20lines%20of%20code%2C%20our%20method%20is%20easily%20implemented%20in%20any%20pipeline%20for%0Atransforming%20LLMs%20to%20text%20encoders.%20We%20also%20propose%20%24%5Ctext%7BL%7D%5E3%20%5Ctext%7BPrune%7D%24%2C%0Aa%20novel%20layer-pruning%20strategy%20based%20on%20the%20model%27s%20initial%20loss%20that%20provides%0Atwo%20optimal%20pruning%20configurations%3A%20a%20large%20variant%20with%20negligible%20performance%0Aloss%20and%20a%20small%20variant%20for%20resource-constrained%20settings.%20On%20average%2C%20the%0Alarge%20variant%20prunes%2021%5C%25%20of%20the%20parameters%20with%20a%20%24-0.3%24%20performance%20drop%2C%20and%0Athe%20small%20variant%20only%20suffers%20from%20a%20%24-5.1%24%20decrease%20while%20pruning%2074%5C%25%20of%20the%0Amodel.%20We%20consider%20these%20results%20strong%20evidence%20that%20LLMs%20are%0Aoverparameterized%20for%20text%20embedding%20tasks%2C%20and%20can%20be%20easily%20pruned.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14578v1&entry.124074799=Read"},
{"title": "TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and\n  Heterogeneous Graphs", "author": "Julia Gastinger and Shenyang Huang and Mikhail Galkin and Erfan Loghmani and Ali Parviz and Farimah Poursafaei and Jacob Danovitch and Emanuele Rossi and Ioannis Koutis and Heiner Stuckenschmidt and Reihaneh Rabbany and Guillaume Rabusseau", "abstract": "  Multi-relational temporal graphs are powerful tools for modeling real-world\ndata, capturing the evolving and interconnected nature of entities over time.\nRecently, many novel models are proposed for ML on such graphs intensifying the\nneed for robust evaluation and standardized benchmark datasets. However, the\navailability of such resources remains scarce and evaluation faces added\ncomplexity due to reproducibility issues in experimental protocols. To address\nthese challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel\nbenchmarking framework tailored for evaluating methods for predicting future\nlinks on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a\nfocus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0\nfacilitates comprehensive evaluations by presenting eight novel datasets\nspanning five domains with up to 53 million edges. TGB 2.0 datasets are\nsignificantly larger than existing datasets in terms of number of nodes, edges,\nor timestamps. In addition, TGB 2.0 provides a reproducible and realistic\nevaluation pipeline for multi-relational temporal graphs. Through extensive\nexperimentation, we observe that 1) leveraging edge-type information is crucial\nto obtain high performance, 2) simple heuristic baselines are often competitive\nwith more complex methods, 3) most methods fail to run on our largest datasets,\nhighlighting the need for research on more scalable methods.\n", "link": "http://arxiv.org/abs/2406.09639v2", "date": "2024-10-18", "relevancy": 2.3915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4791}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TGB%202.0%3A%20A%20Benchmark%20for%20Learning%20on%20Temporal%20Knowledge%20Graphs%20and%0A%20%20Heterogeneous%20Graphs&body=Title%3A%20TGB%202.0%3A%20A%20Benchmark%20for%20Learning%20on%20Temporal%20Knowledge%20Graphs%20and%0A%20%20Heterogeneous%20Graphs%0AAuthor%3A%20Julia%20Gastinger%20and%20Shenyang%20Huang%20and%20Mikhail%20Galkin%20and%20Erfan%20Loghmani%20and%20Ali%20Parviz%20and%20Farimah%20Poursafaei%20and%20Jacob%20Danovitch%20and%20Emanuele%20Rossi%20and%20Ioannis%20Koutis%20and%20Heiner%20Stuckenschmidt%20and%20Reihaneh%20Rabbany%20and%20Guillaume%20Rabusseau%0AAbstract%3A%20%20%20Multi-relational%20temporal%20graphs%20are%20powerful%20tools%20for%20modeling%20real-world%0Adata%2C%20capturing%20the%20evolving%20and%20interconnected%20nature%20of%20entities%20over%20time.%0ARecently%2C%20many%20novel%20models%20are%20proposed%20for%20ML%20on%20such%20graphs%20intensifying%20the%0Aneed%20for%20robust%20evaluation%20and%20standardized%20benchmark%20datasets.%20However%2C%20the%0Aavailability%20of%20such%20resources%20remains%20scarce%20and%20evaluation%20faces%20added%0Acomplexity%20due%20to%20reproducibility%20issues%20in%20experimental%20protocols.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20Temporal%20Graph%20Benchmark%202.0%20%28TGB%202.0%29%2C%20a%20novel%0Abenchmarking%20framework%20tailored%20for%20evaluating%20methods%20for%20predicting%20future%0Alinks%20on%20Temporal%20Knowledge%20Graphs%20and%20Temporal%20Heterogeneous%20Graphs%20with%20a%0Afocus%20on%20large-scale%20datasets%2C%20extending%20the%20Temporal%20Graph%20Benchmark.%20TGB%202.0%0Afacilitates%20comprehensive%20evaluations%20by%20presenting%20eight%20novel%20datasets%0Aspanning%20five%20domains%20with%20up%20to%2053%20million%20edges.%20TGB%202.0%20datasets%20are%0Asignificantly%20larger%20than%20existing%20datasets%20in%20terms%20of%20number%20of%20nodes%2C%20edges%2C%0Aor%20timestamps.%20In%20addition%2C%20TGB%202.0%20provides%20a%20reproducible%20and%20realistic%0Aevaluation%20pipeline%20for%20multi-relational%20temporal%20graphs.%20Through%20extensive%0Aexperimentation%2C%20we%20observe%20that%201%29%20leveraging%20edge-type%20information%20is%20crucial%0Ato%20obtain%20high%20performance%2C%202%29%20simple%20heuristic%20baselines%20are%20often%20competitive%0Awith%20more%20complex%20methods%2C%203%29%20most%20methods%20fail%20to%20run%20on%20our%20largest%20datasets%2C%0Ahighlighting%20the%20need%20for%20research%20on%20more%20scalable%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTGB%25202.0%253A%2520A%2520Benchmark%2520for%2520Learning%2520on%2520Temporal%2520Knowledge%2520Graphs%2520and%250A%2520%2520Heterogeneous%2520Graphs%26entry.906535625%3DJulia%2520Gastinger%2520and%2520Shenyang%2520Huang%2520and%2520Mikhail%2520Galkin%2520and%2520Erfan%2520Loghmani%2520and%2520Ali%2520Parviz%2520and%2520Farimah%2520Poursafaei%2520and%2520Jacob%2520Danovitch%2520and%2520Emanuele%2520Rossi%2520and%2520Ioannis%2520Koutis%2520and%2520Heiner%2520Stuckenschmidt%2520and%2520Reihaneh%2520Rabbany%2520and%2520Guillaume%2520Rabusseau%26entry.1292438233%3D%2520%2520Multi-relational%2520temporal%2520graphs%2520are%2520powerful%2520tools%2520for%2520modeling%2520real-world%250Adata%252C%2520capturing%2520the%2520evolving%2520and%2520interconnected%2520nature%2520of%2520entities%2520over%2520time.%250ARecently%252C%2520many%2520novel%2520models%2520are%2520proposed%2520for%2520ML%2520on%2520such%2520graphs%2520intensifying%2520the%250Aneed%2520for%2520robust%2520evaluation%2520and%2520standardized%2520benchmark%2520datasets.%2520However%252C%2520the%250Aavailability%2520of%2520such%2520resources%2520remains%2520scarce%2520and%2520evaluation%2520faces%2520added%250Acomplexity%2520due%2520to%2520reproducibility%2520issues%2520in%2520experimental%2520protocols.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520Temporal%2520Graph%2520Benchmark%25202.0%2520%2528TGB%25202.0%2529%252C%2520a%2520novel%250Abenchmarking%2520framework%2520tailored%2520for%2520evaluating%2520methods%2520for%2520predicting%2520future%250Alinks%2520on%2520Temporal%2520Knowledge%2520Graphs%2520and%2520Temporal%2520Heterogeneous%2520Graphs%2520with%2520a%250Afocus%2520on%2520large-scale%2520datasets%252C%2520extending%2520the%2520Temporal%2520Graph%2520Benchmark.%2520TGB%25202.0%250Afacilitates%2520comprehensive%2520evaluations%2520by%2520presenting%2520eight%2520novel%2520datasets%250Aspanning%2520five%2520domains%2520with%2520up%2520to%252053%2520million%2520edges.%2520TGB%25202.0%2520datasets%2520are%250Asignificantly%2520larger%2520than%2520existing%2520datasets%2520in%2520terms%2520of%2520number%2520of%2520nodes%252C%2520edges%252C%250Aor%2520timestamps.%2520In%2520addition%252C%2520TGB%25202.0%2520provides%2520a%2520reproducible%2520and%2520realistic%250Aevaluation%2520pipeline%2520for%2520multi-relational%2520temporal%2520graphs.%2520Through%2520extensive%250Aexperimentation%252C%2520we%2520observe%2520that%25201%2529%2520leveraging%2520edge-type%2520information%2520is%2520crucial%250Ato%2520obtain%2520high%2520performance%252C%25202%2529%2520simple%2520heuristic%2520baselines%2520are%2520often%2520competitive%250Awith%2520more%2520complex%2520methods%252C%25203%2529%2520most%2520methods%2520fail%2520to%2520run%2520on%2520our%2520largest%2520datasets%252C%250Ahighlighting%2520the%2520need%2520for%2520research%2520on%2520more%2520scalable%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TGB%202.0%3A%20A%20Benchmark%20for%20Learning%20on%20Temporal%20Knowledge%20Graphs%20and%0A%20%20Heterogeneous%20Graphs&entry.906535625=Julia%20Gastinger%20and%20Shenyang%20Huang%20and%20Mikhail%20Galkin%20and%20Erfan%20Loghmani%20and%20Ali%20Parviz%20and%20Farimah%20Poursafaei%20and%20Jacob%20Danovitch%20and%20Emanuele%20Rossi%20and%20Ioannis%20Koutis%20and%20Heiner%20Stuckenschmidt%20and%20Reihaneh%20Rabbany%20and%20Guillaume%20Rabusseau&entry.1292438233=%20%20Multi-relational%20temporal%20graphs%20are%20powerful%20tools%20for%20modeling%20real-world%0Adata%2C%20capturing%20the%20evolving%20and%20interconnected%20nature%20of%20entities%20over%20time.%0ARecently%2C%20many%20novel%20models%20are%20proposed%20for%20ML%20on%20such%20graphs%20intensifying%20the%0Aneed%20for%20robust%20evaluation%20and%20standardized%20benchmark%20datasets.%20However%2C%20the%0Aavailability%20of%20such%20resources%20remains%20scarce%20and%20evaluation%20faces%20added%0Acomplexity%20due%20to%20reproducibility%20issues%20in%20experimental%20protocols.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20Temporal%20Graph%20Benchmark%202.0%20%28TGB%202.0%29%2C%20a%20novel%0Abenchmarking%20framework%20tailored%20for%20evaluating%20methods%20for%20predicting%20future%0Alinks%20on%20Temporal%20Knowledge%20Graphs%20and%20Temporal%20Heterogeneous%20Graphs%20with%20a%0Afocus%20on%20large-scale%20datasets%2C%20extending%20the%20Temporal%20Graph%20Benchmark.%20TGB%202.0%0Afacilitates%20comprehensive%20evaluations%20by%20presenting%20eight%20novel%20datasets%0Aspanning%20five%20domains%20with%20up%20to%2053%20million%20edges.%20TGB%202.0%20datasets%20are%0Asignificantly%20larger%20than%20existing%20datasets%20in%20terms%20of%20number%20of%20nodes%2C%20edges%2C%0Aor%20timestamps.%20In%20addition%2C%20TGB%202.0%20provides%20a%20reproducible%20and%20realistic%0Aevaluation%20pipeline%20for%20multi-relational%20temporal%20graphs.%20Through%20extensive%0Aexperimentation%2C%20we%20observe%20that%201%29%20leveraging%20edge-type%20information%20is%20crucial%0Ato%20obtain%20high%20performance%2C%202%29%20simple%20heuristic%20baselines%20are%20often%20competitive%0Awith%20more%20complex%20methods%2C%203%29%20most%20methods%20fail%20to%20run%20on%20our%20largest%20datasets%2C%0Ahighlighting%20the%20need%20for%20research%20on%20more%20scalable%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09639v2&entry.124074799=Read"},
{"title": "Do LLMs \"know\" internally when they follow instructions?", "author": "Juyeon Heo and Christina Heinze-Deml and Oussama Elachqar and Shirley Ren and Udhay Nallasamy and Andy Miller and Kwan Ho Ryan Chan and Jaya Narain", "abstract": "  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n", "link": "http://arxiv.org/abs/2410.14516v1", "date": "2024-10-18", "relevancy": 2.3755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F&body=Title%3A%20Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F%0AAuthor%3A%20Juyeon%20Heo%20and%20Christina%20Heinze-Deml%20and%20Oussama%20Elachqar%20and%20Shirley%20Ren%20and%20Udhay%20Nallasamy%20and%20Andy%20Miller%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Jaya%20Narain%0AAbstract%3A%20%20%20Instruction-following%20is%20crucial%20for%20building%20AI%20agents%20with%20large%20language%0Amodels%20%28LLMs%29%2C%20as%20these%20models%20must%20adhere%20strictly%20to%20user-provided%0Aconstraints%20and%20guidelines.%20However%2C%20LLMs%20often%20fail%20to%20follow%20even%20simple%20and%0Aclear%20instructions.%20To%20improve%20instruction-following%20behavior%20and%20prevent%0Aundesirable%20outputs%2C%20a%20deeper%20understanding%20of%20how%20LLMs%27%20internal%20states%20relate%0Ato%20these%20outcomes%20is%20required.%20Our%20analysis%20of%20LLM%20internal%20states%20reveal%20a%0Adimension%20in%20the%20input%20embedding%20space%20linked%20to%20successful%0Ainstruction-following.%20We%20demonstrate%20that%20modifying%20representations%20along%20this%0Adimension%20improves%20instruction-following%20success%20rates%20compared%20to%20random%0Achanges%2C%20without%20compromising%20response%20quality.%20Further%20investigation%20reveals%0Athat%20this%20dimension%20is%20more%20closely%20related%20to%20the%20phrasing%20of%20prompts%20rather%0Athan%20the%20inherent%20difficulty%20of%20the%20task%20or%20instructions.%20This%20discovery%20also%0Asuggests%20explanations%20for%20why%20LLMs%20sometimes%20fail%20to%20follow%20clear%20instructions%0Aand%20why%20prompt%20engineering%20is%20often%20effective%2C%20even%20when%20the%20content%20remains%0Alargely%20unchanged.%20This%20work%20provides%20insight%20into%20the%20internal%20workings%20of%0ALLMs%27%20instruction-following%2C%20paving%20the%20way%20for%20reliable%20LLM%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520%2522know%2522%2520internally%2520when%2520they%2520follow%2520instructions%253F%26entry.906535625%3DJuyeon%2520Heo%2520and%2520Christina%2520Heinze-Deml%2520and%2520Oussama%2520Elachqar%2520and%2520Shirley%2520Ren%2520and%2520Udhay%2520Nallasamy%2520and%2520Andy%2520Miller%2520and%2520Kwan%2520Ho%2520Ryan%2520Chan%2520and%2520Jaya%2520Narain%26entry.1292438233%3D%2520%2520Instruction-following%2520is%2520crucial%2520for%2520building%2520AI%2520agents%2520with%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520as%2520these%2520models%2520must%2520adhere%2520strictly%2520to%2520user-provided%250Aconstraints%2520and%2520guidelines.%2520However%252C%2520LLMs%2520often%2520fail%2520to%2520follow%2520even%2520simple%2520and%250Aclear%2520instructions.%2520To%2520improve%2520instruction-following%2520behavior%2520and%2520prevent%250Aundesirable%2520outputs%252C%2520a%2520deeper%2520understanding%2520of%2520how%2520LLMs%2527%2520internal%2520states%2520relate%250Ato%2520these%2520outcomes%2520is%2520required.%2520Our%2520analysis%2520of%2520LLM%2520internal%2520states%2520reveal%2520a%250Adimension%2520in%2520the%2520input%2520embedding%2520space%2520linked%2520to%2520successful%250Ainstruction-following.%2520We%2520demonstrate%2520that%2520modifying%2520representations%2520along%2520this%250Adimension%2520improves%2520instruction-following%2520success%2520rates%2520compared%2520to%2520random%250Achanges%252C%2520without%2520compromising%2520response%2520quality.%2520Further%2520investigation%2520reveals%250Athat%2520this%2520dimension%2520is%2520more%2520closely%2520related%2520to%2520the%2520phrasing%2520of%2520prompts%2520rather%250Athan%2520the%2520inherent%2520difficulty%2520of%2520the%2520task%2520or%2520instructions.%2520This%2520discovery%2520also%250Asuggests%2520explanations%2520for%2520why%2520LLMs%2520sometimes%2520fail%2520to%2520follow%2520clear%2520instructions%250Aand%2520why%2520prompt%2520engineering%2520is%2520often%2520effective%252C%2520even%2520when%2520the%2520content%2520remains%250Alargely%2520unchanged.%2520This%2520work%2520provides%2520insight%2520into%2520the%2520internal%2520workings%2520of%250ALLMs%2527%2520instruction-following%252C%2520paving%2520the%2520way%2520for%2520reliable%2520LLM%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20%22know%22%20internally%20when%20they%20follow%20instructions%3F&entry.906535625=Juyeon%20Heo%20and%20Christina%20Heinze-Deml%20and%20Oussama%20Elachqar%20and%20Shirley%20Ren%20and%20Udhay%20Nallasamy%20and%20Andy%20Miller%20and%20Kwan%20Ho%20Ryan%20Chan%20and%20Jaya%20Narain&entry.1292438233=%20%20Instruction-following%20is%20crucial%20for%20building%20AI%20agents%20with%20large%20language%0Amodels%20%28LLMs%29%2C%20as%20these%20models%20must%20adhere%20strictly%20to%20user-provided%0Aconstraints%20and%20guidelines.%20However%2C%20LLMs%20often%20fail%20to%20follow%20even%20simple%20and%0Aclear%20instructions.%20To%20improve%20instruction-following%20behavior%20and%20prevent%0Aundesirable%20outputs%2C%20a%20deeper%20understanding%20of%20how%20LLMs%27%20internal%20states%20relate%0Ato%20these%20outcomes%20is%20required.%20Our%20analysis%20of%20LLM%20internal%20states%20reveal%20a%0Adimension%20in%20the%20input%20embedding%20space%20linked%20to%20successful%0Ainstruction-following.%20We%20demonstrate%20that%20modifying%20representations%20along%20this%0Adimension%20improves%20instruction-following%20success%20rates%20compared%20to%20random%0Achanges%2C%20without%20compromising%20response%20quality.%20Further%20investigation%20reveals%0Athat%20this%20dimension%20is%20more%20closely%20related%20to%20the%20phrasing%20of%20prompts%20rather%0Athan%20the%20inherent%20difficulty%20of%20the%20task%20or%20instructions.%20This%20discovery%20also%0Asuggests%20explanations%20for%20why%20LLMs%20sometimes%20fail%20to%20follow%20clear%20instructions%0Aand%20why%20prompt%20engineering%20is%20often%20effective%2C%20even%20when%20the%20content%20remains%0Alargely%20unchanged.%20This%20work%20provides%20insight%20into%20the%20internal%20workings%20of%0ALLMs%27%20instruction-following%2C%20paving%20the%20way%20for%20reliable%20LLM%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14516v1&entry.124074799=Read"},
{"title": "EasyRec: Simple yet Effective Language Models for Recommendation", "author": "Xubin Ren and Chao Huang", "abstract": "  Deep neural networks have become a powerful technique for learning\nrepresentations from user-item interaction data in collaborative filtering (CF)\nfor recommender systems. However, many existing methods heavily rely on unique\nuser and item IDs, which limits their ability to perform well in practical\nzero-shot learning scenarios where sufficient training data may be unavailable.\nInspired by the success of language models (LMs) and their strong\ngeneralization capabilities, a crucial question arises: How can we harness the\npotential of language models to empower recommender systems and elevate its\ngeneralization capabilities to new heights? In this study, we propose EasyRec -\nan effective and easy-to-use approach that seamlessly integrates text-based\nsemantic understanding with collaborative signals. EasyRec employs a\ntext-behavior alignment framework, which combines contrastive learning with\ncollaborative language model tuning, to ensure a strong alignment between the\ntext-enhanced semantic space and the collaborative behavior information.\nExtensive empirical evaluations across diverse real-world datasets demonstrate\nthe superior performance of EasyRec compared to state-of-the-art alternative\nmodels, particularly in the challenging text-based zero-shot recommendation\nscenarios. Furthermore, the study highlights the potential of seamlessly\nintegrating EasyRec as a plug-and-play component into text-enhanced\ncollaborative filtering frameworks, thereby empowering existing recommender\nsystems to elevate their recommendation performance and adapt to the evolving\nuser preferences in dynamic environments. For better result reproducibility of\nour EasyRec framework, the model implementation details, source code, and\ndatasets are available at the link: https://github.com/HKUDS/EasyRec.\n", "link": "http://arxiv.org/abs/2408.08821v3", "date": "2024-10-18", "relevancy": 2.3539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation&body=Title%3A%20EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation%0AAuthor%3A%20Xubin%20Ren%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20become%20a%20powerful%20technique%20for%20learning%0Arepresentations%20from%20user-item%20interaction%20data%20in%20collaborative%20filtering%20%28CF%29%0Afor%20recommender%20systems.%20However%2C%20many%20existing%20methods%20heavily%20rely%20on%20unique%0Auser%20and%20item%20IDs%2C%20which%20limits%20their%20ability%20to%20perform%20well%20in%20practical%0Azero-shot%20learning%20scenarios%20where%20sufficient%20training%20data%20may%20be%20unavailable.%0AInspired%20by%20the%20success%20of%20language%20models%20%28LMs%29%20and%20their%20strong%0Ageneralization%20capabilities%2C%20a%20crucial%20question%20arises%3A%20How%20can%20we%20harness%20the%0Apotential%20of%20language%20models%20to%20empower%20recommender%20systems%20and%20elevate%20its%0Ageneralization%20capabilities%20to%20new%20heights%3F%20In%20this%20study%2C%20we%20propose%20EasyRec%20-%0Aan%20effective%20and%20easy-to-use%20approach%20that%20seamlessly%20integrates%20text-based%0Asemantic%20understanding%20with%20collaborative%20signals.%20EasyRec%20employs%20a%0Atext-behavior%20alignment%20framework%2C%20which%20combines%20contrastive%20learning%20with%0Acollaborative%20language%20model%20tuning%2C%20to%20ensure%20a%20strong%20alignment%20between%20the%0Atext-enhanced%20semantic%20space%20and%20the%20collaborative%20behavior%20information.%0AExtensive%20empirical%20evaluations%20across%20diverse%20real-world%20datasets%20demonstrate%0Athe%20superior%20performance%20of%20EasyRec%20compared%20to%20state-of-the-art%20alternative%0Amodels%2C%20particularly%20in%20the%20challenging%20text-based%20zero-shot%20recommendation%0Ascenarios.%20Furthermore%2C%20the%20study%20highlights%20the%20potential%20of%20seamlessly%0Aintegrating%20EasyRec%20as%20a%20plug-and-play%20component%20into%20text-enhanced%0Acollaborative%20filtering%20frameworks%2C%20thereby%20empowering%20existing%20recommender%0Asystems%20to%20elevate%20their%20recommendation%20performance%20and%20adapt%20to%20the%20evolving%0Auser%20preferences%20in%20dynamic%20environments.%20For%20better%20result%20reproducibility%20of%0Aour%20EasyRec%20framework%2C%20the%20model%20implementation%20details%2C%20source%20code%2C%20and%0Adatasets%20are%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/EasyRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyRec%253A%2520Simple%2520yet%2520Effective%2520Language%2520Models%2520for%2520Recommendation%26entry.906535625%3DXubin%2520Ren%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520become%2520a%2520powerful%2520technique%2520for%2520learning%250Arepresentations%2520from%2520user-item%2520interaction%2520data%2520in%2520collaborative%2520filtering%2520%2528CF%2529%250Afor%2520recommender%2520systems.%2520However%252C%2520many%2520existing%2520methods%2520heavily%2520rely%2520on%2520unique%250Auser%2520and%2520item%2520IDs%252C%2520which%2520limits%2520their%2520ability%2520to%2520perform%2520well%2520in%2520practical%250Azero-shot%2520learning%2520scenarios%2520where%2520sufficient%2520training%2520data%2520may%2520be%2520unavailable.%250AInspired%2520by%2520the%2520success%2520of%2520language%2520models%2520%2528LMs%2529%2520and%2520their%2520strong%250Ageneralization%2520capabilities%252C%2520a%2520crucial%2520question%2520arises%253A%2520How%2520can%2520we%2520harness%2520the%250Apotential%2520of%2520language%2520models%2520to%2520empower%2520recommender%2520systems%2520and%2520elevate%2520its%250Ageneralization%2520capabilities%2520to%2520new%2520heights%253F%2520In%2520this%2520study%252C%2520we%2520propose%2520EasyRec%2520-%250Aan%2520effective%2520and%2520easy-to-use%2520approach%2520that%2520seamlessly%2520integrates%2520text-based%250Asemantic%2520understanding%2520with%2520collaborative%2520signals.%2520EasyRec%2520employs%2520a%250Atext-behavior%2520alignment%2520framework%252C%2520which%2520combines%2520contrastive%2520learning%2520with%250Acollaborative%2520language%2520model%2520tuning%252C%2520to%2520ensure%2520a%2520strong%2520alignment%2520between%2520the%250Atext-enhanced%2520semantic%2520space%2520and%2520the%2520collaborative%2520behavior%2520information.%250AExtensive%2520empirical%2520evaluations%2520across%2520diverse%2520real-world%2520datasets%2520demonstrate%250Athe%2520superior%2520performance%2520of%2520EasyRec%2520compared%2520to%2520state-of-the-art%2520alternative%250Amodels%252C%2520particularly%2520in%2520the%2520challenging%2520text-based%2520zero-shot%2520recommendation%250Ascenarios.%2520Furthermore%252C%2520the%2520study%2520highlights%2520the%2520potential%2520of%2520seamlessly%250Aintegrating%2520EasyRec%2520as%2520a%2520plug-and-play%2520component%2520into%2520text-enhanced%250Acollaborative%2520filtering%2520frameworks%252C%2520thereby%2520empowering%2520existing%2520recommender%250Asystems%2520to%2520elevate%2520their%2520recommendation%2520performance%2520and%2520adapt%2520to%2520the%2520evolving%250Auser%2520preferences%2520in%2520dynamic%2520environments.%2520For%2520better%2520result%2520reproducibility%2520of%250Aour%2520EasyRec%2520framework%252C%2520the%2520model%2520implementation%2520details%252C%2520source%2520code%252C%2520and%250Adatasets%2520are%2520available%2520at%2520the%2520link%253A%2520https%253A//github.com/HKUDS/EasyRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyRec%3A%20Simple%20yet%20Effective%20Language%20Models%20for%20Recommendation&entry.906535625=Xubin%20Ren%20and%20Chao%20Huang&entry.1292438233=%20%20Deep%20neural%20networks%20have%20become%20a%20powerful%20technique%20for%20learning%0Arepresentations%20from%20user-item%20interaction%20data%20in%20collaborative%20filtering%20%28CF%29%0Afor%20recommender%20systems.%20However%2C%20many%20existing%20methods%20heavily%20rely%20on%20unique%0Auser%20and%20item%20IDs%2C%20which%20limits%20their%20ability%20to%20perform%20well%20in%20practical%0Azero-shot%20learning%20scenarios%20where%20sufficient%20training%20data%20may%20be%20unavailable.%0AInspired%20by%20the%20success%20of%20language%20models%20%28LMs%29%20and%20their%20strong%0Ageneralization%20capabilities%2C%20a%20crucial%20question%20arises%3A%20How%20can%20we%20harness%20the%0Apotential%20of%20language%20models%20to%20empower%20recommender%20systems%20and%20elevate%20its%0Ageneralization%20capabilities%20to%20new%20heights%3F%20In%20this%20study%2C%20we%20propose%20EasyRec%20-%0Aan%20effective%20and%20easy-to-use%20approach%20that%20seamlessly%20integrates%20text-based%0Asemantic%20understanding%20with%20collaborative%20signals.%20EasyRec%20employs%20a%0Atext-behavior%20alignment%20framework%2C%20which%20combines%20contrastive%20learning%20with%0Acollaborative%20language%20model%20tuning%2C%20to%20ensure%20a%20strong%20alignment%20between%20the%0Atext-enhanced%20semantic%20space%20and%20the%20collaborative%20behavior%20information.%0AExtensive%20empirical%20evaluations%20across%20diverse%20real-world%20datasets%20demonstrate%0Athe%20superior%20performance%20of%20EasyRec%20compared%20to%20state-of-the-art%20alternative%0Amodels%2C%20particularly%20in%20the%20challenging%20text-based%20zero-shot%20recommendation%0Ascenarios.%20Furthermore%2C%20the%20study%20highlights%20the%20potential%20of%20seamlessly%0Aintegrating%20EasyRec%20as%20a%20plug-and-play%20component%20into%20text-enhanced%0Acollaborative%20filtering%20frameworks%2C%20thereby%20empowering%20existing%20recommender%0Asystems%20to%20elevate%20their%20recommendation%20performance%20and%20adapt%20to%20the%20evolving%0Auser%20preferences%20in%20dynamic%20environments.%20For%20better%20result%20reproducibility%20of%0Aour%20EasyRec%20framework%2C%20the%20model%20implementation%20details%2C%20source%20code%2C%20and%0Adatasets%20are%20available%20at%20the%20link%3A%20https%3A//github.com/HKUDS/EasyRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08821v3&entry.124074799=Read"},
{"title": "Self-supervised contrastive learning performs non-linear system\n  identification", "author": "Rodrigo Gonz\u00e1lez Laiz and Tobias Schmidt and Steffen Schneider", "abstract": "  Self-supervised learning (SSL) approaches have brought tremendous success\nacross many tasks and domains. It has been argued that these successes can be\nattributed to a link between SSL and identifiable representation learning:\nTemporal structure and auxiliary variables ensure that latent representations\nare related to the true underlying generative factors of the data. Here, we\ndeepen this connection and show that SSL can perform system identification in\nlatent space. We propose DynCL, a framework to uncover linear, switching linear\nand non-linear dynamics under a non-linear observation model, give theoretical\nguarantees and validate them empirically.\n", "link": "http://arxiv.org/abs/2410.14673v1", "date": "2024-10-18", "relevancy": 2.3487, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4972}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20contrastive%20learning%20performs%20non-linear%20system%0A%20%20identification&body=Title%3A%20Self-supervised%20contrastive%20learning%20performs%20non-linear%20system%0A%20%20identification%0AAuthor%3A%20Rodrigo%20Gonz%C3%A1lez%20Laiz%20and%20Tobias%20Schmidt%20and%20Steffen%20Schneider%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20approaches%20have%20brought%20tremendous%20success%0Aacross%20many%20tasks%20and%20domains.%20It%20has%20been%20argued%20that%20these%20successes%20can%20be%0Aattributed%20to%20a%20link%20between%20SSL%20and%20identifiable%20representation%20learning%3A%0ATemporal%20structure%20and%20auxiliary%20variables%20ensure%20that%20latent%20representations%0Aare%20related%20to%20the%20true%20underlying%20generative%20factors%20of%20the%20data.%20Here%2C%20we%0Adeepen%20this%20connection%20and%20show%20that%20SSL%20can%20perform%20system%20identification%20in%0Alatent%20space.%20We%20propose%20DynCL%2C%20a%20framework%20to%20uncover%20linear%2C%20switching%20linear%0Aand%20non-linear%20dynamics%20under%20a%20non-linear%20observation%20model%2C%20give%20theoretical%0Aguarantees%20and%20validate%20them%20empirically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520contrastive%2520learning%2520performs%2520non-linear%2520system%250A%2520%2520identification%26entry.906535625%3DRodrigo%2520Gonz%25C3%25A1lez%2520Laiz%2520and%2520Tobias%2520Schmidt%2520and%2520Steffen%2520Schneider%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520approaches%2520have%2520brought%2520tremendous%2520success%250Aacross%2520many%2520tasks%2520and%2520domains.%2520It%2520has%2520been%2520argued%2520that%2520these%2520successes%2520can%2520be%250Aattributed%2520to%2520a%2520link%2520between%2520SSL%2520and%2520identifiable%2520representation%2520learning%253A%250ATemporal%2520structure%2520and%2520auxiliary%2520variables%2520ensure%2520that%2520latent%2520representations%250Aare%2520related%2520to%2520the%2520true%2520underlying%2520generative%2520factors%2520of%2520the%2520data.%2520Here%252C%2520we%250Adeepen%2520this%2520connection%2520and%2520show%2520that%2520SSL%2520can%2520perform%2520system%2520identification%2520in%250Alatent%2520space.%2520We%2520propose%2520DynCL%252C%2520a%2520framework%2520to%2520uncover%2520linear%252C%2520switching%2520linear%250Aand%2520non-linear%2520dynamics%2520under%2520a%2520non-linear%2520observation%2520model%252C%2520give%2520theoretical%250Aguarantees%2520and%2520validate%2520them%2520empirically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20contrastive%20learning%20performs%20non-linear%20system%0A%20%20identification&entry.906535625=Rodrigo%20Gonz%C3%A1lez%20Laiz%20and%20Tobias%20Schmidt%20and%20Steffen%20Schneider&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20approaches%20have%20brought%20tremendous%20success%0Aacross%20many%20tasks%20and%20domains.%20It%20has%20been%20argued%20that%20these%20successes%20can%20be%0Aattributed%20to%20a%20link%20between%20SSL%20and%20identifiable%20representation%20learning%3A%0ATemporal%20structure%20and%20auxiliary%20variables%20ensure%20that%20latent%20representations%0Aare%20related%20to%20the%20true%20underlying%20generative%20factors%20of%20the%20data.%20Here%2C%20we%0Adeepen%20this%20connection%20and%20show%20that%20SSL%20can%20perform%20system%20identification%20in%0Alatent%20space.%20We%20propose%20DynCL%2C%20a%20framework%20to%20uncover%20linear%2C%20switching%20linear%0Aand%20non-linear%20dynamics%20under%20a%20non-linear%20observation%20model%2C%20give%20theoretical%0Aguarantees%20and%20validate%20them%20empirically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14673v1&entry.124074799=Read"},
{"title": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment", "author": "Qin Liu and Fei Wang and Chaowei Xiao and Muhao Chen", "abstract": "  Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.\n", "link": "http://arxiv.org/abs/2410.14676v1", "date": "2024-10-18", "relevancy": 2.3099, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SudoLM%3A%20Learning%20Access%20Control%20of%20Parametric%20Knowledge%20with%0A%20%20Authorization%20Alignment&body=Title%3A%20SudoLM%3A%20Learning%20Access%20Control%20of%20Parametric%20Knowledge%20with%0A%20%20Authorization%20Alignment%0AAuthor%3A%20Qin%20Liu%20and%20Fei%20Wang%20and%20Chaowei%20Xiao%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20Existing%20preference%20alignment%20is%20a%20one-size-fits-all%20alignment%20mechanism%2C%0Awhere%20the%20part%20of%20the%20large%20language%20model%20%28LLM%29%20parametric%20knowledge%20with%0Anon-preferred%20features%20is%20uniformly%20blocked%20to%20all%20the%20users.%20However%2C%20this%0Apart%20of%20knowledge%20can%20be%20useful%20to%20advanced%20users%20whose%20expertise%20qualifies%0Athem%20to%20handle%20these%20information.%20The%20one-size-fits-all%20alignment%20mechanism%0Aundermines%20LLM%27s%20utility%20for%20these%20qualified%20users.%20To%20address%20this%20problem%2C%20we%0Apropose%20SudoLM%2C%20a%20framework%20that%20lets%20LLMs%20learn%20access%20control%20over%20specific%0Aparametric%20knowledge%20for%20users%20with%20different%20credentials%20via%20authorization%0Aalignment.%20SudoLM%20allows%20authorized%20users%20to%20unlock%20their%20access%20to%20all%20the%0Aparametric%20knowledge%20with%20an%20assigned%20SUDO%20key%20while%20blocking%20access%20to%0Anon-qualified%20users.%20Experiments%20on%20two%20application%20scenarios%20demonstrate%20that%0ASudoLM%20effectively%20controls%20the%20user%27s%20access%20to%20the%20parametric%20knowledge%20and%0Amaintains%20its%20general%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSudoLM%253A%2520Learning%2520Access%2520Control%2520of%2520Parametric%2520Knowledge%2520with%250A%2520%2520Authorization%2520Alignment%26entry.906535625%3DQin%2520Liu%2520and%2520Fei%2520Wang%2520and%2520Chaowei%2520Xiao%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520Existing%2520preference%2520alignment%2520is%2520a%2520one-size-fits-all%2520alignment%2520mechanism%252C%250Awhere%2520the%2520part%2520of%2520the%2520large%2520language%2520model%2520%2528LLM%2529%2520parametric%2520knowledge%2520with%250Anon-preferred%2520features%2520is%2520uniformly%2520blocked%2520to%2520all%2520the%2520users.%2520However%252C%2520this%250Apart%2520of%2520knowledge%2520can%2520be%2520useful%2520to%2520advanced%2520users%2520whose%2520expertise%2520qualifies%250Athem%2520to%2520handle%2520these%2520information.%2520The%2520one-size-fits-all%2520alignment%2520mechanism%250Aundermines%2520LLM%2527s%2520utility%2520for%2520these%2520qualified%2520users.%2520To%2520address%2520this%2520problem%252C%2520we%250Apropose%2520SudoLM%252C%2520a%2520framework%2520that%2520lets%2520LLMs%2520learn%2520access%2520control%2520over%2520specific%250Aparametric%2520knowledge%2520for%2520users%2520with%2520different%2520credentials%2520via%2520authorization%250Aalignment.%2520SudoLM%2520allows%2520authorized%2520users%2520to%2520unlock%2520their%2520access%2520to%2520all%2520the%250Aparametric%2520knowledge%2520with%2520an%2520assigned%2520SUDO%2520key%2520while%2520blocking%2520access%2520to%250Anon-qualified%2520users.%2520Experiments%2520on%2520two%2520application%2520scenarios%2520demonstrate%2520that%250ASudoLM%2520effectively%2520controls%2520the%2520user%2527s%2520access%2520to%2520the%2520parametric%2520knowledge%2520and%250Amaintains%2520its%2520general%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SudoLM%3A%20Learning%20Access%20Control%20of%20Parametric%20Knowledge%20with%0A%20%20Authorization%20Alignment&entry.906535625=Qin%20Liu%20and%20Fei%20Wang%20and%20Chaowei%20Xiao%20and%20Muhao%20Chen&entry.1292438233=%20%20Existing%20preference%20alignment%20is%20a%20one-size-fits-all%20alignment%20mechanism%2C%0Awhere%20the%20part%20of%20the%20large%20language%20model%20%28LLM%29%20parametric%20knowledge%20with%0Anon-preferred%20features%20is%20uniformly%20blocked%20to%20all%20the%20users.%20However%2C%20this%0Apart%20of%20knowledge%20can%20be%20useful%20to%20advanced%20users%20whose%20expertise%20qualifies%0Athem%20to%20handle%20these%20information.%20The%20one-size-fits-all%20alignment%20mechanism%0Aundermines%20LLM%27s%20utility%20for%20these%20qualified%20users.%20To%20address%20this%20problem%2C%20we%0Apropose%20SudoLM%2C%20a%20framework%20that%20lets%20LLMs%20learn%20access%20control%20over%20specific%0Aparametric%20knowledge%20for%20users%20with%20different%20credentials%20via%20authorization%0Aalignment.%20SudoLM%20allows%20authorized%20users%20to%20unlock%20their%20access%20to%20all%20the%0Aparametric%20knowledge%20with%20an%20assigned%20SUDO%20key%20while%20blocking%20access%20to%0Anon-qualified%20users.%20Experiments%20on%20two%20application%20scenarios%20demonstrate%20that%0ASudoLM%20effectively%20controls%20the%20user%27s%20access%20to%20the%20parametric%20knowledge%20and%0Amaintains%20its%20general%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14676v1&entry.124074799=Read"},
{"title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities", "author": "Shaozhe Hao and Xuantong Liu and Xianbiao Qi and Shihao Zhao and Bojia Zi and Rong Xiao and Kai Han and Kwan-Yee K. Wong", "abstract": "  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field.\n", "link": "http://arxiv.org/abs/2410.14672v1", "date": "2024-10-18", "relevancy": 2.3059, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.581}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities&body=Title%3A%20BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities%0AAuthor%3A%20Shaozhe%20Hao%20and%20Xuantong%20Liu%20and%20Xianbiao%20Qi%20and%20Shihao%20Zhao%20and%20Bojia%20Zi%20and%20Rong%20Xiao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%20We%20introduce%20BiGR%2C%20a%20novel%20conditional%20image%20generation%20model%20using%20compact%0Abinary%20latent%20codes%20for%20generative%20training%2C%20focusing%20on%20enhancing%20both%0Ageneration%20and%20representation%20capabilities.%20BiGR%20is%20the%20first%20conditional%0Agenerative%20model%20that%20unifies%20generation%20and%20discrimination%20within%20the%20same%0Aframework.%20BiGR%20features%20a%20binary%20tokenizer%2C%20a%20masked%20modeling%20mechanism%2C%20and%20a%0Abinary%20transcoder%20for%20binary%20code%20prediction.%20Additionally%2C%20we%20introduce%20a%0Anovel%20entropy-ordered%20sampling%20method%20to%20enable%20efficient%20image%20generation.%0AExtensive%20experiments%20validate%20BiGR%27s%20superior%20performance%20in%20generation%0Aquality%2C%20as%20measured%20by%20FID-50k%2C%20and%20representation%20capabilities%2C%20as%20evidenced%0Aby%20linear-probe%20accuracy.%20Moreover%2C%20BiGR%20showcases%20zero-shot%20generalization%0Aacross%20various%20vision%20tasks%2C%20enabling%20applications%20such%20as%20image%20inpainting%2C%0Aoutpainting%2C%20editing%2C%20interpolation%2C%20and%20enrichment%2C%20without%20the%20need%20for%0Astructural%20modifications.%20Our%20findings%20suggest%20that%20BiGR%20unifies%20generative%20and%0Adiscriminative%20tasks%20effectively%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athe%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiGR%253A%2520Harnessing%2520Binary%2520Latent%2520Codes%2520for%2520Image%2520Generation%2520and%2520Improved%250A%2520%2520Visual%2520Representation%2520Capabilities%26entry.906535625%3DShaozhe%2520Hao%2520and%2520Xuantong%2520Liu%2520and%2520Xianbiao%2520Qi%2520and%2520Shihao%2520Zhao%2520and%2520Bojia%2520Zi%2520and%2520Rong%2520Xiao%2520and%2520Kai%2520Han%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3D%2520%2520We%2520introduce%2520BiGR%252C%2520a%2520novel%2520conditional%2520image%2520generation%2520model%2520using%2520compact%250Abinary%2520latent%2520codes%2520for%2520generative%2520training%252C%2520focusing%2520on%2520enhancing%2520both%250Ageneration%2520and%2520representation%2520capabilities.%2520BiGR%2520is%2520the%2520first%2520conditional%250Agenerative%2520model%2520that%2520unifies%2520generation%2520and%2520discrimination%2520within%2520the%2520same%250Aframework.%2520BiGR%2520features%2520a%2520binary%2520tokenizer%252C%2520a%2520masked%2520modeling%2520mechanism%252C%2520and%2520a%250Abinary%2520transcoder%2520for%2520binary%2520code%2520prediction.%2520Additionally%252C%2520we%2520introduce%2520a%250Anovel%2520entropy-ordered%2520sampling%2520method%2520to%2520enable%2520efficient%2520image%2520generation.%250AExtensive%2520experiments%2520validate%2520BiGR%2527s%2520superior%2520performance%2520in%2520generation%250Aquality%252C%2520as%2520measured%2520by%2520FID-50k%252C%2520and%2520representation%2520capabilities%252C%2520as%2520evidenced%250Aby%2520linear-probe%2520accuracy.%2520Moreover%252C%2520BiGR%2520showcases%2520zero-shot%2520generalization%250Aacross%2520various%2520vision%2520tasks%252C%2520enabling%2520applications%2520such%2520as%2520image%2520inpainting%252C%250Aoutpainting%252C%2520editing%252C%2520interpolation%252C%2520and%2520enrichment%252C%2520without%2520the%2520need%2520for%250Astructural%2520modifications.%2520Our%2520findings%2520suggest%2520that%2520BiGR%2520unifies%2520generative%2520and%250Adiscriminative%2520tasks%2520effectively%252C%2520paving%2520the%2520way%2520for%2520further%2520advancements%2520in%250Athe%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiGR%3A%20Harnessing%20Binary%20Latent%20Codes%20for%20Image%20Generation%20and%20Improved%0A%20%20Visual%20Representation%20Capabilities&entry.906535625=Shaozhe%20Hao%20and%20Xuantong%20Liu%20and%20Xianbiao%20Qi%20and%20Shihao%20Zhao%20and%20Bojia%20Zi%20and%20Rong%20Xiao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%20We%20introduce%20BiGR%2C%20a%20novel%20conditional%20image%20generation%20model%20using%20compact%0Abinary%20latent%20codes%20for%20generative%20training%2C%20focusing%20on%20enhancing%20both%0Ageneration%20and%20representation%20capabilities.%20BiGR%20is%20the%20first%20conditional%0Agenerative%20model%20that%20unifies%20generation%20and%20discrimination%20within%20the%20same%0Aframework.%20BiGR%20features%20a%20binary%20tokenizer%2C%20a%20masked%20modeling%20mechanism%2C%20and%20a%0Abinary%20transcoder%20for%20binary%20code%20prediction.%20Additionally%2C%20we%20introduce%20a%0Anovel%20entropy-ordered%20sampling%20method%20to%20enable%20efficient%20image%20generation.%0AExtensive%20experiments%20validate%20BiGR%27s%20superior%20performance%20in%20generation%0Aquality%2C%20as%20measured%20by%20FID-50k%2C%20and%20representation%20capabilities%2C%20as%20evidenced%0Aby%20linear-probe%20accuracy.%20Moreover%2C%20BiGR%20showcases%20zero-shot%20generalization%0Aacross%20various%20vision%20tasks%2C%20enabling%20applications%20such%20as%20image%20inpainting%2C%0Aoutpainting%2C%20editing%2C%20interpolation%2C%20and%20enrichment%2C%20without%20the%20need%20for%0Astructural%20modifications.%20Our%20findings%20suggest%20that%20BiGR%20unifies%20generative%20and%0Adiscriminative%20tasks%20effectively%2C%20paving%20the%20way%20for%20further%20advancements%20in%0Athe%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14672v1&entry.124074799=Read"},
{"title": "Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts", "author": "Yukun Huang and Sanxing Chen and Hongyi Cai and Bhuwan Dhingra", "abstract": "  Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.\n", "link": "http://arxiv.org/abs/2410.14675v1", "date": "2024-10-18", "relevancy": 2.2613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Large%20Language%20Models%27%20Situated%20Faithfulness%20to%20External%0A%20%20Contexts&body=Title%3A%20Enhancing%20Large%20Language%20Models%27%20Situated%20Faithfulness%20to%20External%0A%20%20Contexts%0AAuthor%3A%20Yukun%20Huang%20and%20Sanxing%20Chen%20and%20Hongyi%20Cai%20and%20Bhuwan%20Dhingra%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20augmented%20with%20external%20information%20as%0Acontexts%2C%20but%20this%20external%20information%20can%20sometimes%20be%20inaccurate%20or%20even%0Aintentionally%20misleading.%20We%20argue%20that%20robust%20LLMs%20should%20demonstrate%20situated%0Afaithfulness%2C%20dynamically%20calibrating%20their%20trust%20in%20external%20information%20based%0Aon%20their%20confidence%20in%20the%20internal%20knowledge%20and%20the%20external%20context.%20To%0Abenchmark%20this%20capability%2C%20we%20evaluate%20LLMs%20across%20several%20QA%20datasets%2C%0Aincluding%20a%20newly%20created%20dataset%20called%20RedditQA%20featuring%20in-the-wild%0Aincorrect%20contexts%20sourced%20from%20Reddit%20posts.%20We%20show%20that%20when%20provided%20with%0Aboth%20correct%20and%20incorrect%20contexts%2C%20both%20open-source%20and%20proprietary%20models%0Atend%20to%20overly%20rely%20on%20external%20information%2C%20regardless%20of%20its%20factual%0Aaccuracy.%20To%20enhance%20situated%20faithfulness%2C%20we%20propose%20two%20approaches%3A%0ASelf-Guided%20Confidence%20Reasoning%20%28SCR%29%20and%20Rule-Based%20Confidence%20Reasoning%0A%28RCR%29.%20SCR%20enables%20models%20to%20self-access%20the%20confidence%20of%20external%20information%0Arelative%20to%20their%20own%20internal%20knowledge%20to%20produce%20the%20most%20accurate%20answer.%0ARCR%2C%20in%20contrast%2C%20extracts%20explicit%20confidence%20signals%20from%20the%20LLM%20and%0Adetermines%20the%20final%20answer%20using%20predefined%20rules.%20Our%20results%20show%20that%20for%0ALLMs%20with%20strong%20reasoning%20capabilities%2C%20such%20as%20GPT-4o%20and%20GPT-4o%20mini%2C%20SCR%0Aoutperforms%20RCR%2C%20achieving%20improvements%20of%20up%20to%2024.2%25%20over%20a%20direct%20input%0Aaugmentation%20baseline.%20Conversely%2C%20for%20a%20smaller%20model%20like%20Llama-3-8B%2C%20RCR%0Aoutperforms%20SCR.%20Fine-tuning%20SCR%20with%20our%20proposed%20Confidence%20Reasoning%20Direct%0APreference%20Optimization%20%28CR-DPO%29%20method%20improves%20performance%20on%20both%20seen%20and%0Aunseen%20datasets%2C%20yielding%20an%20average%20improvement%20of%208.9%25%20on%20Llama-3-8B.%20In%0Aaddition%20to%20quantitative%20results%2C%20we%20offer%20insights%20into%20the%20relative%20strengths%0Aof%20SCR%20and%20RCR.%20Our%20findings%20highlight%20promising%20avenues%20for%20improving%20situated%0Afaithfulness%20in%20LLMs.%20The%20data%20and%20code%20are%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Large%2520Language%2520Models%2527%2520Situated%2520Faithfulness%2520to%2520External%250A%2520%2520Contexts%26entry.906535625%3DYukun%2520Huang%2520and%2520Sanxing%2520Chen%2520and%2520Hongyi%2520Cai%2520and%2520Bhuwan%2520Dhingra%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520often%2520augmented%2520with%2520external%2520information%2520as%250Acontexts%252C%2520but%2520this%2520external%2520information%2520can%2520sometimes%2520be%2520inaccurate%2520or%2520even%250Aintentionally%2520misleading.%2520We%2520argue%2520that%2520robust%2520LLMs%2520should%2520demonstrate%2520situated%250Afaithfulness%252C%2520dynamically%2520calibrating%2520their%2520trust%2520in%2520external%2520information%2520based%250Aon%2520their%2520confidence%2520in%2520the%2520internal%2520knowledge%2520and%2520the%2520external%2520context.%2520To%250Abenchmark%2520this%2520capability%252C%2520we%2520evaluate%2520LLMs%2520across%2520several%2520QA%2520datasets%252C%250Aincluding%2520a%2520newly%2520created%2520dataset%2520called%2520RedditQA%2520featuring%2520in-the-wild%250Aincorrect%2520contexts%2520sourced%2520from%2520Reddit%2520posts.%2520We%2520show%2520that%2520when%2520provided%2520with%250Aboth%2520correct%2520and%2520incorrect%2520contexts%252C%2520both%2520open-source%2520and%2520proprietary%2520models%250Atend%2520to%2520overly%2520rely%2520on%2520external%2520information%252C%2520regardless%2520of%2520its%2520factual%250Aaccuracy.%2520To%2520enhance%2520situated%2520faithfulness%252C%2520we%2520propose%2520two%2520approaches%253A%250ASelf-Guided%2520Confidence%2520Reasoning%2520%2528SCR%2529%2520and%2520Rule-Based%2520Confidence%2520Reasoning%250A%2528RCR%2529.%2520SCR%2520enables%2520models%2520to%2520self-access%2520the%2520confidence%2520of%2520external%2520information%250Arelative%2520to%2520their%2520own%2520internal%2520knowledge%2520to%2520produce%2520the%2520most%2520accurate%2520answer.%250ARCR%252C%2520in%2520contrast%252C%2520extracts%2520explicit%2520confidence%2520signals%2520from%2520the%2520LLM%2520and%250Adetermines%2520the%2520final%2520answer%2520using%2520predefined%2520rules.%2520Our%2520results%2520show%2520that%2520for%250ALLMs%2520with%2520strong%2520reasoning%2520capabilities%252C%2520such%2520as%2520GPT-4o%2520and%2520GPT-4o%2520mini%252C%2520SCR%250Aoutperforms%2520RCR%252C%2520achieving%2520improvements%2520of%2520up%2520to%252024.2%2525%2520over%2520a%2520direct%2520input%250Aaugmentation%2520baseline.%2520Conversely%252C%2520for%2520a%2520smaller%2520model%2520like%2520Llama-3-8B%252C%2520RCR%250Aoutperforms%2520SCR.%2520Fine-tuning%2520SCR%2520with%2520our%2520proposed%2520Confidence%2520Reasoning%2520Direct%250APreference%2520Optimization%2520%2528CR-DPO%2529%2520method%2520improves%2520performance%2520on%2520both%2520seen%2520and%250Aunseen%2520datasets%252C%2520yielding%2520an%2520average%2520improvement%2520of%25208.9%2525%2520on%2520Llama-3-8B.%2520In%250Aaddition%2520to%2520quantitative%2520results%252C%2520we%2520offer%2520insights%2520into%2520the%2520relative%2520strengths%250Aof%2520SCR%2520and%2520RCR.%2520Our%2520findings%2520highlight%2520promising%2520avenues%2520for%2520improving%2520situated%250Afaithfulness%2520in%2520LLMs.%2520The%2520data%2520and%2520code%2520are%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Large%20Language%20Models%27%20Situated%20Faithfulness%20to%20External%0A%20%20Contexts&entry.906535625=Yukun%20Huang%20and%20Sanxing%20Chen%20and%20Hongyi%20Cai%20and%20Bhuwan%20Dhingra&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20often%20augmented%20with%20external%20information%20as%0Acontexts%2C%20but%20this%20external%20information%20can%20sometimes%20be%20inaccurate%20or%20even%0Aintentionally%20misleading.%20We%20argue%20that%20robust%20LLMs%20should%20demonstrate%20situated%0Afaithfulness%2C%20dynamically%20calibrating%20their%20trust%20in%20external%20information%20based%0Aon%20their%20confidence%20in%20the%20internal%20knowledge%20and%20the%20external%20context.%20To%0Abenchmark%20this%20capability%2C%20we%20evaluate%20LLMs%20across%20several%20QA%20datasets%2C%0Aincluding%20a%20newly%20created%20dataset%20called%20RedditQA%20featuring%20in-the-wild%0Aincorrect%20contexts%20sourced%20from%20Reddit%20posts.%20We%20show%20that%20when%20provided%20with%0Aboth%20correct%20and%20incorrect%20contexts%2C%20both%20open-source%20and%20proprietary%20models%0Atend%20to%20overly%20rely%20on%20external%20information%2C%20regardless%20of%20its%20factual%0Aaccuracy.%20To%20enhance%20situated%20faithfulness%2C%20we%20propose%20two%20approaches%3A%0ASelf-Guided%20Confidence%20Reasoning%20%28SCR%29%20and%20Rule-Based%20Confidence%20Reasoning%0A%28RCR%29.%20SCR%20enables%20models%20to%20self-access%20the%20confidence%20of%20external%20information%0Arelative%20to%20their%20own%20internal%20knowledge%20to%20produce%20the%20most%20accurate%20answer.%0ARCR%2C%20in%20contrast%2C%20extracts%20explicit%20confidence%20signals%20from%20the%20LLM%20and%0Adetermines%20the%20final%20answer%20using%20predefined%20rules.%20Our%20results%20show%20that%20for%0ALLMs%20with%20strong%20reasoning%20capabilities%2C%20such%20as%20GPT-4o%20and%20GPT-4o%20mini%2C%20SCR%0Aoutperforms%20RCR%2C%20achieving%20improvements%20of%20up%20to%2024.2%25%20over%20a%20direct%20input%0Aaugmentation%20baseline.%20Conversely%2C%20for%20a%20smaller%20model%20like%20Llama-3-8B%2C%20RCR%0Aoutperforms%20SCR.%20Fine-tuning%20SCR%20with%20our%20proposed%20Confidence%20Reasoning%20Direct%0APreference%20Optimization%20%28CR-DPO%29%20method%20improves%20performance%20on%20both%20seen%20and%0Aunseen%20datasets%2C%20yielding%20an%20average%20improvement%20of%208.9%25%20on%20Llama-3-8B.%20In%0Aaddition%20to%20quantitative%20results%2C%20we%20offer%20insights%20into%20the%20relative%20strengths%0Aof%20SCR%20and%20RCR.%20Our%20findings%20highlight%20promising%20avenues%20for%20improving%20situated%0Afaithfulness%20in%20LLMs.%20The%20data%20and%20code%20are%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14675v1&entry.124074799=Read"},
{"title": "Teaching Models to Balance Resisting and Accepting Persuasion", "author": "Elias Stengel-Eskin and Peter Hase and Mohit Bansal", "abstract": "  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.\n", "link": "http://arxiv.org/abs/2410.14596v1", "date": "2024-10-18", "relevancy": 2.2566, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion&body=Title%3A%20Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion%0AAuthor%3A%20Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20persuasion%2C%20which%20can%20pose%0Arisks%20when%20models%20are%20faced%20with%20an%20adversarial%20interlocutor.%20We%20take%20a%20first%0Astep%20towards%20defending%20models%20against%20persuasion%20while%20also%20arguing%20that%0Adefense%20against%20adversarial%20%28i.e.%20negative%29%20persuasion%20is%20only%20half%20of%20the%0Aequation%3A%20models%20should%20also%20be%20able%20to%20accept%20beneficial%20%28i.e.%20positive%29%0Apersuasion%20to%20improve%20their%20answers.%20We%20show%20that%20optimizing%20models%20for%20only%0Aone%20side%20results%20in%20poor%20performance%20on%20the%20other.%20In%20order%20to%20balance%20positive%0Aand%20negative%20persuasion%2C%20we%20introduce%20Persuasion-Balanced%20Training%20%28or%20PBT%29%2C%0Awhich%20leverages%20multi-agent%20recursive%20dialogue%20trees%20to%20create%20data%20and%20trains%0Amodels%20via%20preference%20optimization%20to%20accept%20persuasion%20when%20appropriate.%20PBT%0Aconsistently%20improves%20resistance%20to%20misinformation%20and%20resilience%20to%20being%0Achallenged%20while%20also%20resulting%20in%20the%20best%20overall%20performance%20on%20holistic%0Adata%20containing%20both%20positive%20and%20negative%20persuasion.%20Crucially%2C%20we%20show%20that%0APBT%20models%20are%20better%20teammates%20in%20multi-agent%20debates.%20We%20find%20that%20without%0APBT%2C%20pairs%20of%20stronger%20and%20weaker%20models%20have%20unstable%20performance%2C%20with%20the%0Aorder%20in%20which%20the%20models%20present%20their%20answers%20determining%20whether%20the%20team%0Aobtains%20the%20stronger%20or%20weaker%20model%27s%20performance.%20PBT%20leads%20to%20better%20and%0Amore%20stable%20results%20and%20less%20order%20dependence%2C%20with%20the%20stronger%20model%0Aconsistently%20pulling%20the%20weaker%20one%20up.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520Models%2520to%2520Balance%2520Resisting%2520and%2520Accepting%2520Persuasion%26entry.906535625%3DElias%2520Stengel-Eskin%2520and%2520Peter%2520Hase%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520susceptible%2520to%2520persuasion%252C%2520which%2520can%2520pose%250Arisks%2520when%2520models%2520are%2520faced%2520with%2520an%2520adversarial%2520interlocutor.%2520We%2520take%2520a%2520first%250Astep%2520towards%2520defending%2520models%2520against%2520persuasion%2520while%2520also%2520arguing%2520that%250Adefense%2520against%2520adversarial%2520%2528i.e.%2520negative%2529%2520persuasion%2520is%2520only%2520half%2520of%2520the%250Aequation%253A%2520models%2520should%2520also%2520be%2520able%2520to%2520accept%2520beneficial%2520%2528i.e.%2520positive%2529%250Apersuasion%2520to%2520improve%2520their%2520answers.%2520We%2520show%2520that%2520optimizing%2520models%2520for%2520only%250Aone%2520side%2520results%2520in%2520poor%2520performance%2520on%2520the%2520other.%2520In%2520order%2520to%2520balance%2520positive%250Aand%2520negative%2520persuasion%252C%2520we%2520introduce%2520Persuasion-Balanced%2520Training%2520%2528or%2520PBT%2529%252C%250Awhich%2520leverages%2520multi-agent%2520recursive%2520dialogue%2520trees%2520to%2520create%2520data%2520and%2520trains%250Amodels%2520via%2520preference%2520optimization%2520to%2520accept%2520persuasion%2520when%2520appropriate.%2520PBT%250Aconsistently%2520improves%2520resistance%2520to%2520misinformation%2520and%2520resilience%2520to%2520being%250Achallenged%2520while%2520also%2520resulting%2520in%2520the%2520best%2520overall%2520performance%2520on%2520holistic%250Adata%2520containing%2520both%2520positive%2520and%2520negative%2520persuasion.%2520Crucially%252C%2520we%2520show%2520that%250APBT%2520models%2520are%2520better%2520teammates%2520in%2520multi-agent%2520debates.%2520We%2520find%2520that%2520without%250APBT%252C%2520pairs%2520of%2520stronger%2520and%2520weaker%2520models%2520have%2520unstable%2520performance%252C%2520with%2520the%250Aorder%2520in%2520which%2520the%2520models%2520present%2520their%2520answers%2520determining%2520whether%2520the%2520team%250Aobtains%2520the%2520stronger%2520or%2520weaker%2520model%2527s%2520performance.%2520PBT%2520leads%2520to%2520better%2520and%250Amore%2520stable%2520results%2520and%2520less%2520order%2520dependence%252C%2520with%2520the%2520stronger%2520model%250Aconsistently%2520pulling%2520the%2520weaker%2520one%2520up.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Models%20to%20Balance%20Resisting%20and%20Accepting%20Persuasion&entry.906535625=Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20susceptible%20to%20persuasion%2C%20which%20can%20pose%0Arisks%20when%20models%20are%20faced%20with%20an%20adversarial%20interlocutor.%20We%20take%20a%20first%0Astep%20towards%20defending%20models%20against%20persuasion%20while%20also%20arguing%20that%0Adefense%20against%20adversarial%20%28i.e.%20negative%29%20persuasion%20is%20only%20half%20of%20the%0Aequation%3A%20models%20should%20also%20be%20able%20to%20accept%20beneficial%20%28i.e.%20positive%29%0Apersuasion%20to%20improve%20their%20answers.%20We%20show%20that%20optimizing%20models%20for%20only%0Aone%20side%20results%20in%20poor%20performance%20on%20the%20other.%20In%20order%20to%20balance%20positive%0Aand%20negative%20persuasion%2C%20we%20introduce%20Persuasion-Balanced%20Training%20%28or%20PBT%29%2C%0Awhich%20leverages%20multi-agent%20recursive%20dialogue%20trees%20to%20create%20data%20and%20trains%0Amodels%20via%20preference%20optimization%20to%20accept%20persuasion%20when%20appropriate.%20PBT%0Aconsistently%20improves%20resistance%20to%20misinformation%20and%20resilience%20to%20being%0Achallenged%20while%20also%20resulting%20in%20the%20best%20overall%20performance%20on%20holistic%0Adata%20containing%20both%20positive%20and%20negative%20persuasion.%20Crucially%2C%20we%20show%20that%0APBT%20models%20are%20better%20teammates%20in%20multi-agent%20debates.%20We%20find%20that%20without%0APBT%2C%20pairs%20of%20stronger%20and%20weaker%20models%20have%20unstable%20performance%2C%20with%20the%0Aorder%20in%20which%20the%20models%20present%20their%20answers%20determining%20whether%20the%20team%0Aobtains%20the%20stronger%20or%20weaker%20model%27s%20performance.%20PBT%20leads%20to%20better%20and%0Amore%20stable%20results%20and%20less%20order%20dependence%2C%20with%20the%20stronger%20model%0Aconsistently%20pulling%20the%20weaker%20one%20up.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14596v1&entry.124074799=Read"},
{"title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning\n  for Multimodal Classification", "author": "Raja Kumar and Raghav Singhal and Pranamya Kulkarni and Deval Mehta and Kshitij Jadhav", "abstract": "  Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research.\n", "link": "http://arxiv.org/abs/2409.17777v2", "date": "2024-10-18", "relevancy": 2.2452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6039}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5516}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification&body=Title%3A%20Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification%0AAuthor%3A%20Raja%20Kumar%20and%20Raghav%20Singhal%20and%20Pranamya%20Kulkarni%20and%20Deval%20Mehta%20and%20Kshitij%20Jadhav%0AAbstract%3A%20%20%20Deep%20multimodal%20learning%20has%20shown%20remarkable%20success%20by%20leveraging%0Acontrastive%20learning%20to%20capture%20explicit%20one-to-one%20relations%20across%0Amodalities.%20However%2C%20real-world%20data%20often%20exhibits%20shared%20relations%20beyond%0Asimple%20pairwise%20associations.%20We%20propose%20M3CoL%2C%20a%20Multimodal%20Mixup%20Contrastive%0ALearning%20approach%20to%20capture%20nuanced%20shared%20relations%20inherent%20in%20multimodal%0Adata.%20Our%20key%20contribution%20is%20a%20Mixup-based%20contrastive%20loss%20that%20learns%20robust%0Arepresentations%20by%20aligning%20mixed%20samples%20from%20one%20modality%20with%20their%0Acorresponding%20samples%20from%20other%20modalities%20thereby%20capturing%20shared%20relations%0Abetween%20them.%20For%20multimodal%20classification%20tasks%2C%20we%20introduce%20a%20framework%0Athat%20integrates%20a%20fusion%20module%20with%20unimodal%20prediction%20modules%20for%20auxiliary%0Asupervision%20during%20training%2C%20complemented%20by%20our%20proposed%20Mixup-based%0Acontrastive%20loss.%20Through%20extensive%20experiments%20on%20diverse%20datasets%20%28N24News%2C%0AROSMAP%2C%20BRCA%2C%20and%20Food-101%29%2C%20we%20demonstrate%20that%20M3CoL%20effectively%20captures%0Ashared%20multimodal%20relations%20and%20generalizes%20across%20domains.%20It%20outperforms%0Astate-of-the-art%20methods%20on%20N24News%2C%20ROSMAP%2C%20and%20BRCA%2C%20while%20achieving%0Acomparable%20performance%20on%20Food-101.%20Our%20work%20highlights%20the%20significance%20of%0Alearning%20shared%20relations%20for%20robust%20multimodal%20learning%2C%20opening%20up%20promising%0Aavenues%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Shared%2520Relations%2520via%2520Multimodal%2520Mixup%2520Contrastive%2520Learning%250A%2520%2520for%2520Multimodal%2520Classification%26entry.906535625%3DRaja%2520Kumar%2520and%2520Raghav%2520Singhal%2520and%2520Pranamya%2520Kulkarni%2520and%2520Deval%2520Mehta%2520and%2520Kshitij%2520Jadhav%26entry.1292438233%3D%2520%2520Deep%2520multimodal%2520learning%2520has%2520shown%2520remarkable%2520success%2520by%2520leveraging%250Acontrastive%2520learning%2520to%2520capture%2520explicit%2520one-to-one%2520relations%2520across%250Amodalities.%2520However%252C%2520real-world%2520data%2520often%2520exhibits%2520shared%2520relations%2520beyond%250Asimple%2520pairwise%2520associations.%2520We%2520propose%2520M3CoL%252C%2520a%2520Multimodal%2520Mixup%2520Contrastive%250ALearning%2520approach%2520to%2520capture%2520nuanced%2520shared%2520relations%2520inherent%2520in%2520multimodal%250Adata.%2520Our%2520key%2520contribution%2520is%2520a%2520Mixup-based%2520contrastive%2520loss%2520that%2520learns%2520robust%250Arepresentations%2520by%2520aligning%2520mixed%2520samples%2520from%2520one%2520modality%2520with%2520their%250Acorresponding%2520samples%2520from%2520other%2520modalities%2520thereby%2520capturing%2520shared%2520relations%250Abetween%2520them.%2520For%2520multimodal%2520classification%2520tasks%252C%2520we%2520introduce%2520a%2520framework%250Athat%2520integrates%2520a%2520fusion%2520module%2520with%2520unimodal%2520prediction%2520modules%2520for%2520auxiliary%250Asupervision%2520during%2520training%252C%2520complemented%2520by%2520our%2520proposed%2520Mixup-based%250Acontrastive%2520loss.%2520Through%2520extensive%2520experiments%2520on%2520diverse%2520datasets%2520%2528N24News%252C%250AROSMAP%252C%2520BRCA%252C%2520and%2520Food-101%2529%252C%2520we%2520demonstrate%2520that%2520M3CoL%2520effectively%2520captures%250Ashared%2520multimodal%2520relations%2520and%2520generalizes%2520across%2520domains.%2520It%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520N24News%252C%2520ROSMAP%252C%2520and%2520BRCA%252C%2520while%2520achieving%250Acomparable%2520performance%2520on%2520Food-101.%2520Our%2520work%2520highlights%2520the%2520significance%2520of%250Alearning%2520shared%2520relations%2520for%2520robust%2520multimodal%2520learning%252C%2520opening%2520up%2520promising%250Aavenues%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Shared%20Relations%20via%20Multimodal%20Mixup%20Contrastive%20Learning%0A%20%20for%20Multimodal%20Classification&entry.906535625=Raja%20Kumar%20and%20Raghav%20Singhal%20and%20Pranamya%20Kulkarni%20and%20Deval%20Mehta%20and%20Kshitij%20Jadhav&entry.1292438233=%20%20Deep%20multimodal%20learning%20has%20shown%20remarkable%20success%20by%20leveraging%0Acontrastive%20learning%20to%20capture%20explicit%20one-to-one%20relations%20across%0Amodalities.%20However%2C%20real-world%20data%20often%20exhibits%20shared%20relations%20beyond%0Asimple%20pairwise%20associations.%20We%20propose%20M3CoL%2C%20a%20Multimodal%20Mixup%20Contrastive%0ALearning%20approach%20to%20capture%20nuanced%20shared%20relations%20inherent%20in%20multimodal%0Adata.%20Our%20key%20contribution%20is%20a%20Mixup-based%20contrastive%20loss%20that%20learns%20robust%0Arepresentations%20by%20aligning%20mixed%20samples%20from%20one%20modality%20with%20their%0Acorresponding%20samples%20from%20other%20modalities%20thereby%20capturing%20shared%20relations%0Abetween%20them.%20For%20multimodal%20classification%20tasks%2C%20we%20introduce%20a%20framework%0Athat%20integrates%20a%20fusion%20module%20with%20unimodal%20prediction%20modules%20for%20auxiliary%0Asupervision%20during%20training%2C%20complemented%20by%20our%20proposed%20Mixup-based%0Acontrastive%20loss.%20Through%20extensive%20experiments%20on%20diverse%20datasets%20%28N24News%2C%0AROSMAP%2C%20BRCA%2C%20and%20Food-101%29%2C%20we%20demonstrate%20that%20M3CoL%20effectively%20captures%0Ashared%20multimodal%20relations%20and%20generalizes%20across%20domains.%20It%20outperforms%0Astate-of-the-art%20methods%20on%20N24News%2C%20ROSMAP%2C%20and%20BRCA%2C%20while%20achieving%0Acomparable%20performance%20on%20Food-101.%20Our%20work%20highlights%20the%20significance%20of%0Alearning%20shared%20relations%20for%20robust%20multimodal%20learning%2C%20opening%20up%20promising%0Aavenues%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17777v2&entry.124074799=Read"},
{"title": "Convergence of Manifold Filter-Combine Networks", "author": "David R. Johnson and Joyce Chew and Siddharth Viswanath and Edward De Brouwer and Deanna Needell and Smita Krishnaswamy and Michael Perlmutter", "abstract": "  In order to better understand manifold neural networks (MNNs), we introduce\nManifold Filter-Combine Networks (MFCNs). The filter-combine framework\nparallels the popular aggregate-combine paradigm for graph neural networks\n(GNNs) and naturally suggests many interesting families of MNNs which can be\ninterpreted as the manifold analog of various popular GNNs. We then propose a\nmethod for implementing MFCNs on high-dimensional point clouds that relies on\napproximating the manifold by a sparse graph. We prove that our method is\nconsistent in the sense that it converges to a continuum limit as the number of\ndata points tends to infinity.\n", "link": "http://arxiv.org/abs/2410.14639v1", "date": "2024-10-18", "relevancy": 2.2368, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20Manifold%20Filter-Combine%20Networks&body=Title%3A%20Convergence%20of%20Manifold%20Filter-Combine%20Networks%0AAuthor%3A%20David%20R.%20Johnson%20and%20Joyce%20Chew%20and%20Siddharth%20Viswanath%20and%20Edward%20De%20Brouwer%20and%20Deanna%20Needell%20and%20Smita%20Krishnaswamy%20and%20Michael%20Perlmutter%0AAbstract%3A%20%20%20In%20order%20to%20better%20understand%20manifold%20neural%20networks%20%28MNNs%29%2C%20we%20introduce%0AManifold%20Filter-Combine%20Networks%20%28MFCNs%29.%20The%20filter-combine%20framework%0Aparallels%20the%20popular%20aggregate-combine%20paradigm%20for%20graph%20neural%20networks%0A%28GNNs%29%20and%20naturally%20suggests%20many%20interesting%20families%20of%20MNNs%20which%20can%20be%0Ainterpreted%20as%20the%20manifold%20analog%20of%20various%20popular%20GNNs.%20We%20then%20propose%20a%0Amethod%20for%20implementing%20MFCNs%20on%20high-dimensional%20point%20clouds%20that%20relies%20on%0Aapproximating%20the%20manifold%20by%20a%20sparse%20graph.%20We%20prove%20that%20our%20method%20is%0Aconsistent%20in%20the%20sense%20that%20it%20converges%20to%20a%20continuum%20limit%20as%20the%20number%20of%0Adata%20points%20tends%20to%20infinity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520Manifold%2520Filter-Combine%2520Networks%26entry.906535625%3DDavid%2520R.%2520Johnson%2520and%2520Joyce%2520Chew%2520and%2520Siddharth%2520Viswanath%2520and%2520Edward%2520De%2520Brouwer%2520and%2520Deanna%2520Needell%2520and%2520Smita%2520Krishnaswamy%2520and%2520Michael%2520Perlmutter%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520better%2520understand%2520manifold%2520neural%2520networks%2520%2528MNNs%2529%252C%2520we%2520introduce%250AManifold%2520Filter-Combine%2520Networks%2520%2528MFCNs%2529.%2520The%2520filter-combine%2520framework%250Aparallels%2520the%2520popular%2520aggregate-combine%2520paradigm%2520for%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520and%2520naturally%2520suggests%2520many%2520interesting%2520families%2520of%2520MNNs%2520which%2520can%2520be%250Ainterpreted%2520as%2520the%2520manifold%2520analog%2520of%2520various%2520popular%2520GNNs.%2520We%2520then%2520propose%2520a%250Amethod%2520for%2520implementing%2520MFCNs%2520on%2520high-dimensional%2520point%2520clouds%2520that%2520relies%2520on%250Aapproximating%2520the%2520manifold%2520by%2520a%2520sparse%2520graph.%2520We%2520prove%2520that%2520our%2520method%2520is%250Aconsistent%2520in%2520the%2520sense%2520that%2520it%2520converges%2520to%2520a%2520continuum%2520limit%2520as%2520the%2520number%2520of%250Adata%2520points%2520tends%2520to%2520infinity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20Manifold%20Filter-Combine%20Networks&entry.906535625=David%20R.%20Johnson%20and%20Joyce%20Chew%20and%20Siddharth%20Viswanath%20and%20Edward%20De%20Brouwer%20and%20Deanna%20Needell%20and%20Smita%20Krishnaswamy%20and%20Michael%20Perlmutter&entry.1292438233=%20%20In%20order%20to%20better%20understand%20manifold%20neural%20networks%20%28MNNs%29%2C%20we%20introduce%0AManifold%20Filter-Combine%20Networks%20%28MFCNs%29.%20The%20filter-combine%20framework%0Aparallels%20the%20popular%20aggregate-combine%20paradigm%20for%20graph%20neural%20networks%0A%28GNNs%29%20and%20naturally%20suggests%20many%20interesting%20families%20of%20MNNs%20which%20can%20be%0Ainterpreted%20as%20the%20manifold%20analog%20of%20various%20popular%20GNNs.%20We%20then%20propose%20a%0Amethod%20for%20implementing%20MFCNs%20on%20high-dimensional%20point%20clouds%20that%20relies%20on%0Aapproximating%20the%20manifold%20by%20a%20sparse%20graph.%20We%20prove%20that%20our%20method%20is%0Aconsistent%20in%20the%20sense%20that%20it%20converges%20to%20a%20continuum%20limit%20as%20the%20number%20of%0Adata%20points%20tends%20to%20infinity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14639v1&entry.124074799=Read"},
{"title": "Boosting K-means for Big Data by Fusing Data Streaming with Global\n  Optimization", "author": "Ravil Mussabayev and Rustam Mussabayev", "abstract": "  K-means clustering is a cornerstone of data mining, but its efficiency\ndeteriorates when confronted with massive datasets. To address this limitation,\nwe propose a novel heuristic algorithm that leverages the Variable Neighborhood\nSearch (VNS) metaheuristic to optimize K-means clustering for big data. Our\napproach is based on the sequential optimization of the partial objective\nfunction landscapes obtained by restricting the Minimum Sum-of-Squares\nClustering (MSSC) formulation to random samples from the original big dataset.\nWithin each landscape, systematically expanding neighborhoods of the currently\nbest (incumbent) solution are explored by reinitializing all degenerate and a\nvarying number of additional centroids. Extensive and rigorous experimentation\non a large number of real-world datasets reveals that by transforming the\ntraditional local search into a global one, our algorithm significantly\nenhances the accuracy and efficiency of K-means clustering in big data\nenvironments, becoming the new state of the art in the field.\n", "link": "http://arxiv.org/abs/2410.14548v1", "date": "2024-10-18", "relevancy": 2.1949, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4536}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4386}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20K-means%20for%20Big%20Data%20by%20Fusing%20Data%20Streaming%20with%20Global%0A%20%20Optimization&body=Title%3A%20Boosting%20K-means%20for%20Big%20Data%20by%20Fusing%20Data%20Streaming%20with%20Global%0A%20%20Optimization%0AAuthor%3A%20Ravil%20Mussabayev%20and%20Rustam%20Mussabayev%0AAbstract%3A%20%20%20K-means%20clustering%20is%20a%20cornerstone%20of%20data%20mining%2C%20but%20its%20efficiency%0Adeteriorates%20when%20confronted%20with%20massive%20datasets.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20heuristic%20algorithm%20that%20leverages%20the%20Variable%20Neighborhood%0ASearch%20%28VNS%29%20metaheuristic%20to%20optimize%20K-means%20clustering%20for%20big%20data.%20Our%0Aapproach%20is%20based%20on%20the%20sequential%20optimization%20of%20the%20partial%20objective%0Afunction%20landscapes%20obtained%20by%20restricting%20the%20Minimum%20Sum-of-Squares%0AClustering%20%28MSSC%29%20formulation%20to%20random%20samples%20from%20the%20original%20big%20dataset.%0AWithin%20each%20landscape%2C%20systematically%20expanding%20neighborhoods%20of%20the%20currently%0Abest%20%28incumbent%29%20solution%20are%20explored%20by%20reinitializing%20all%20degenerate%20and%20a%0Avarying%20number%20of%20additional%20centroids.%20Extensive%20and%20rigorous%20experimentation%0Aon%20a%20large%20number%20of%20real-world%20datasets%20reveals%20that%20by%20transforming%20the%0Atraditional%20local%20search%20into%20a%20global%20one%2C%20our%20algorithm%20significantly%0Aenhances%20the%20accuracy%20and%20efficiency%20of%20K-means%20clustering%20in%20big%20data%0Aenvironments%2C%20becoming%20the%20new%20state%20of%20the%20art%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520K-means%2520for%2520Big%2520Data%2520by%2520Fusing%2520Data%2520Streaming%2520with%2520Global%250A%2520%2520Optimization%26entry.906535625%3DRavil%2520Mussabayev%2520and%2520Rustam%2520Mussabayev%26entry.1292438233%3D%2520%2520K-means%2520clustering%2520is%2520a%2520cornerstone%2520of%2520data%2520mining%252C%2520but%2520its%2520efficiency%250Adeteriorates%2520when%2520confronted%2520with%2520massive%2520datasets.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520novel%2520heuristic%2520algorithm%2520that%2520leverages%2520the%2520Variable%2520Neighborhood%250ASearch%2520%2528VNS%2529%2520metaheuristic%2520to%2520optimize%2520K-means%2520clustering%2520for%2520big%2520data.%2520Our%250Aapproach%2520is%2520based%2520on%2520the%2520sequential%2520optimization%2520of%2520the%2520partial%2520objective%250Afunction%2520landscapes%2520obtained%2520by%2520restricting%2520the%2520Minimum%2520Sum-of-Squares%250AClustering%2520%2528MSSC%2529%2520formulation%2520to%2520random%2520samples%2520from%2520the%2520original%2520big%2520dataset.%250AWithin%2520each%2520landscape%252C%2520systematically%2520expanding%2520neighborhoods%2520of%2520the%2520currently%250Abest%2520%2528incumbent%2529%2520solution%2520are%2520explored%2520by%2520reinitializing%2520all%2520degenerate%2520and%2520a%250Avarying%2520number%2520of%2520additional%2520centroids.%2520Extensive%2520and%2520rigorous%2520experimentation%250Aon%2520a%2520large%2520number%2520of%2520real-world%2520datasets%2520reveals%2520that%2520by%2520transforming%2520the%250Atraditional%2520local%2520search%2520into%2520a%2520global%2520one%252C%2520our%2520algorithm%2520significantly%250Aenhances%2520the%2520accuracy%2520and%2520efficiency%2520of%2520K-means%2520clustering%2520in%2520big%2520data%250Aenvironments%252C%2520becoming%2520the%2520new%2520state%2520of%2520the%2520art%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20K-means%20for%20Big%20Data%20by%20Fusing%20Data%20Streaming%20with%20Global%0A%20%20Optimization&entry.906535625=Ravil%20Mussabayev%20and%20Rustam%20Mussabayev&entry.1292438233=%20%20K-means%20clustering%20is%20a%20cornerstone%20of%20data%20mining%2C%20but%20its%20efficiency%0Adeteriorates%20when%20confronted%20with%20massive%20datasets.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20heuristic%20algorithm%20that%20leverages%20the%20Variable%20Neighborhood%0ASearch%20%28VNS%29%20metaheuristic%20to%20optimize%20K-means%20clustering%20for%20big%20data.%20Our%0Aapproach%20is%20based%20on%20the%20sequential%20optimization%20of%20the%20partial%20objective%0Afunction%20landscapes%20obtained%20by%20restricting%20the%20Minimum%20Sum-of-Squares%0AClustering%20%28MSSC%29%20formulation%20to%20random%20samples%20from%20the%20original%20big%20dataset.%0AWithin%20each%20landscape%2C%20systematically%20expanding%20neighborhoods%20of%20the%20currently%0Abest%20%28incumbent%29%20solution%20are%20explored%20by%20reinitializing%20all%20degenerate%20and%20a%0Avarying%20number%20of%20additional%20centroids.%20Extensive%20and%20rigorous%20experimentation%0Aon%20a%20large%20number%20of%20real-world%20datasets%20reveals%20that%20by%20transforming%20the%0Atraditional%20local%20search%20into%20a%20global%20one%2C%20our%20algorithm%20significantly%0Aenhances%20the%20accuracy%20and%20efficiency%20of%20K-means%20clustering%20in%20big%20data%0Aenvironments%2C%20becoming%20the%20new%20state%20of%20the%20art%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14548v1&entry.124074799=Read"},
{"title": "Movie101v2: Improved Movie Narration Benchmark", "author": "Zihao Yue and Yepeng Zhang and Ziheng Wang and Qin Jin", "abstract": "  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n", "link": "http://arxiv.org/abs/2404.13370v2", "date": "2024-10-18", "relevancy": 2.1519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Movie101v2%3A%20Improved%20Movie%20Narration%20Benchmark&body=Title%3A%20Movie101v2%3A%20Improved%20Movie%20Narration%20Benchmark%0AAuthor%3A%20Zihao%20Yue%20and%20Yepeng%20Zhang%20and%20Ziheng%20Wang%20and%20Qin%20Jin%0AAbstract%3A%20%20%20Automatic%20movie%20narration%20aims%20to%20generate%20video-aligned%20plot%20descriptions%20to%0Aassist%20visually%20impaired%20audiences.%20Unlike%20standard%20video%20captioning%2C%20it%0Ainvolves%20not%20only%20describing%20key%20visual%20details%20but%20also%20inferring%20plots%20that%0Aunfold%20across%20multiple%20movie%20shots%2C%20presenting%20distinct%20and%20complex%20challenges.%0ATo%20advance%20this%20field%2C%20we%20introduce%20Movie101v2%2C%20a%20large-scale%2C%20bilingual%0Adataset%20with%20enhanced%20data%20quality%20specifically%20designed%20for%20movie%20narration.%0ARevisiting%20the%20task%2C%20we%20propose%20breaking%20down%20the%20ultimate%20goal%20of%20automatic%0Amovie%20narration%20into%20three%20progressive%20stages%2C%20offering%20a%20clear%20roadmap%20with%0Acorresponding%20evaluation%20metrics.%20Based%20on%20our%20new%20benchmark%2C%20we%20baseline%20a%0Arange%20of%20large%20vision-language%20models%2C%20including%20GPT-4V%2C%20and%20conduct%20an%0Ain-depth%20analysis%20of%20the%20challenges%20in%20narration%20generation.%20Our%20findings%0Ahighlight%20that%20achieving%20applicable%20movie%20narration%20generation%20is%20a%20fascinating%0Agoal%20that%20requires%20significant%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovie101v2%253A%2520Improved%2520Movie%2520Narration%2520Benchmark%26entry.906535625%3DZihao%2520Yue%2520and%2520Yepeng%2520Zhang%2520and%2520Ziheng%2520Wang%2520and%2520Qin%2520Jin%26entry.1292438233%3D%2520%2520Automatic%2520movie%2520narration%2520aims%2520to%2520generate%2520video-aligned%2520plot%2520descriptions%2520to%250Aassist%2520visually%2520impaired%2520audiences.%2520Unlike%2520standard%2520video%2520captioning%252C%2520it%250Ainvolves%2520not%2520only%2520describing%2520key%2520visual%2520details%2520but%2520also%2520inferring%2520plots%2520that%250Aunfold%2520across%2520multiple%2520movie%2520shots%252C%2520presenting%2520distinct%2520and%2520complex%2520challenges.%250ATo%2520advance%2520this%2520field%252C%2520we%2520introduce%2520Movie101v2%252C%2520a%2520large-scale%252C%2520bilingual%250Adataset%2520with%2520enhanced%2520data%2520quality%2520specifically%2520designed%2520for%2520movie%2520narration.%250ARevisiting%2520the%2520task%252C%2520we%2520propose%2520breaking%2520down%2520the%2520ultimate%2520goal%2520of%2520automatic%250Amovie%2520narration%2520into%2520three%2520progressive%2520stages%252C%2520offering%2520a%2520clear%2520roadmap%2520with%250Acorresponding%2520evaluation%2520metrics.%2520Based%2520on%2520our%2520new%2520benchmark%252C%2520we%2520baseline%2520a%250Arange%2520of%2520large%2520vision-language%2520models%252C%2520including%2520GPT-4V%252C%2520and%2520conduct%2520an%250Ain-depth%2520analysis%2520of%2520the%2520challenges%2520in%2520narration%2520generation.%2520Our%2520findings%250Ahighlight%2520that%2520achieving%2520applicable%2520movie%2520narration%2520generation%2520is%2520a%2520fascinating%250Agoal%2520that%2520requires%2520significant%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Movie101v2%3A%20Improved%20Movie%20Narration%20Benchmark&entry.906535625=Zihao%20Yue%20and%20Yepeng%20Zhang%20and%20Ziheng%20Wang%20and%20Qin%20Jin&entry.1292438233=%20%20Automatic%20movie%20narration%20aims%20to%20generate%20video-aligned%20plot%20descriptions%20to%0Aassist%20visually%20impaired%20audiences.%20Unlike%20standard%20video%20captioning%2C%20it%0Ainvolves%20not%20only%20describing%20key%20visual%20details%20but%20also%20inferring%20plots%20that%0Aunfold%20across%20multiple%20movie%20shots%2C%20presenting%20distinct%20and%20complex%20challenges.%0ATo%20advance%20this%20field%2C%20we%20introduce%20Movie101v2%2C%20a%20large-scale%2C%20bilingual%0Adataset%20with%20enhanced%20data%20quality%20specifically%20designed%20for%20movie%20narration.%0ARevisiting%20the%20task%2C%20we%20propose%20breaking%20down%20the%20ultimate%20goal%20of%20automatic%0Amovie%20narration%20into%20three%20progressive%20stages%2C%20offering%20a%20clear%20roadmap%20with%0Acorresponding%20evaluation%20metrics.%20Based%20on%20our%20new%20benchmark%2C%20we%20baseline%20a%0Arange%20of%20large%20vision-language%20models%2C%20including%20GPT-4V%2C%20and%20conduct%20an%0Ain-depth%20analysis%20of%20the%20challenges%20in%20narration%20generation.%20Our%20findings%0Ahighlight%20that%20achieving%20applicable%20movie%20narration%20generation%20is%20a%20fascinating%0Agoal%20that%20requires%20significant%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13370v2&entry.124074799=Read"},
{"title": "DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail\n  Recovery and a Novel Contrastive Learning Paradigm", "author": "Gao Yu Lee and Tanmoy Dam and Md Meftahul Ferdaus and Daniel Puiu Poenar and Vu Duong", "abstract": "  Image dehazing is crucial for clarifying images obscured by haze or fog, but\ncurrent learning-based approaches is dependent on large volumes of training\ndata and hence consumed significant computational power. Additionally, their\nperformance is often inadequate under non-uniform or heavy haze. To address\nthese challenges, we developed the Detail Recovery And Contrastive DehazeNet,\nwhich facilitates efficient and effective dehazing via a dense dilated inverted\nresidual block and an attention-based detail recovery network that tailors\nenhancements to specific dehazed scene contexts. A major innovation is its\nability to train effectively with limited data, achieved through a novel\nquadruplet loss-based contrastive dehazing paradigm. This approach distinctly\nseparates hazy and clear image features while also distinguish lower-quality\nand higher-quality dehazed images obtained from each sub-modules of our\nnetwork, thereby refining the dehazing process to a larger extent. Extensive\ntests on a variety of benchmarked haze datasets demonstrated the superiority of\nour approach. The code repository for this work will be available soon.\n", "link": "http://arxiv.org/abs/2410.14595v1", "date": "2024-10-18", "relevancy": 2.1515, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5721}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRACO-DehazeNet%3A%20An%20Efficient%20Image%20Dehazing%20Network%20Combining%20Detail%0A%20%20Recovery%20and%20a%20Novel%20Contrastive%20Learning%20Paradigm&body=Title%3A%20DRACO-DehazeNet%3A%20An%20Efficient%20Image%20Dehazing%20Network%20Combining%20Detail%0A%20%20Recovery%20and%20a%20Novel%20Contrastive%20Learning%20Paradigm%0AAuthor%3A%20Gao%20Yu%20Lee%20and%20Tanmoy%20Dam%20and%20Md%20Meftahul%20Ferdaus%20and%20Daniel%20Puiu%20Poenar%20and%20Vu%20Duong%0AAbstract%3A%20%20%20Image%20dehazing%20is%20crucial%20for%20clarifying%20images%20obscured%20by%20haze%20or%20fog%2C%20but%0Acurrent%20learning-based%20approaches%20is%20dependent%20on%20large%20volumes%20of%20training%0Adata%20and%20hence%20consumed%20significant%20computational%20power.%20Additionally%2C%20their%0Aperformance%20is%20often%20inadequate%20under%20non-uniform%20or%20heavy%20haze.%20To%20address%0Athese%20challenges%2C%20we%20developed%20the%20Detail%20Recovery%20And%20Contrastive%20DehazeNet%2C%0Awhich%20facilitates%20efficient%20and%20effective%20dehazing%20via%20a%20dense%20dilated%20inverted%0Aresidual%20block%20and%20an%20attention-based%20detail%20recovery%20network%20that%20tailors%0Aenhancements%20to%20specific%20dehazed%20scene%20contexts.%20A%20major%20innovation%20is%20its%0Aability%20to%20train%20effectively%20with%20limited%20data%2C%20achieved%20through%20a%20novel%0Aquadruplet%20loss-based%20contrastive%20dehazing%20paradigm.%20This%20approach%20distinctly%0Aseparates%20hazy%20and%20clear%20image%20features%20while%20also%20distinguish%20lower-quality%0Aand%20higher-quality%20dehazed%20images%20obtained%20from%20each%20sub-modules%20of%20our%0Anetwork%2C%20thereby%20refining%20the%20dehazing%20process%20to%20a%20larger%20extent.%20Extensive%0Atests%20on%20a%20variety%20of%20benchmarked%20haze%20datasets%20demonstrated%20the%20superiority%20of%0Aour%20approach.%20The%20code%20repository%20for%20this%20work%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRACO-DehazeNet%253A%2520An%2520Efficient%2520Image%2520Dehazing%2520Network%2520Combining%2520Detail%250A%2520%2520Recovery%2520and%2520a%2520Novel%2520Contrastive%2520Learning%2520Paradigm%26entry.906535625%3DGao%2520Yu%2520Lee%2520and%2520Tanmoy%2520Dam%2520and%2520Md%2520Meftahul%2520Ferdaus%2520and%2520Daniel%2520Puiu%2520Poenar%2520and%2520Vu%2520Duong%26entry.1292438233%3D%2520%2520Image%2520dehazing%2520is%2520crucial%2520for%2520clarifying%2520images%2520obscured%2520by%2520haze%2520or%2520fog%252C%2520but%250Acurrent%2520learning-based%2520approaches%2520is%2520dependent%2520on%2520large%2520volumes%2520of%2520training%250Adata%2520and%2520hence%2520consumed%2520significant%2520computational%2520power.%2520Additionally%252C%2520their%250Aperformance%2520is%2520often%2520inadequate%2520under%2520non-uniform%2520or%2520heavy%2520haze.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520developed%2520the%2520Detail%2520Recovery%2520And%2520Contrastive%2520DehazeNet%252C%250Awhich%2520facilitates%2520efficient%2520and%2520effective%2520dehazing%2520via%2520a%2520dense%2520dilated%2520inverted%250Aresidual%2520block%2520and%2520an%2520attention-based%2520detail%2520recovery%2520network%2520that%2520tailors%250Aenhancements%2520to%2520specific%2520dehazed%2520scene%2520contexts.%2520A%2520major%2520innovation%2520is%2520its%250Aability%2520to%2520train%2520effectively%2520with%2520limited%2520data%252C%2520achieved%2520through%2520a%2520novel%250Aquadruplet%2520loss-based%2520contrastive%2520dehazing%2520paradigm.%2520This%2520approach%2520distinctly%250Aseparates%2520hazy%2520and%2520clear%2520image%2520features%2520while%2520also%2520distinguish%2520lower-quality%250Aand%2520higher-quality%2520dehazed%2520images%2520obtained%2520from%2520each%2520sub-modules%2520of%2520our%250Anetwork%252C%2520thereby%2520refining%2520the%2520dehazing%2520process%2520to%2520a%2520larger%2520extent.%2520Extensive%250Atests%2520on%2520a%2520variety%2520of%2520benchmarked%2520haze%2520datasets%2520demonstrated%2520the%2520superiority%2520of%250Aour%2520approach.%2520The%2520code%2520repository%2520for%2520this%2520work%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRACO-DehazeNet%3A%20An%20Efficient%20Image%20Dehazing%20Network%20Combining%20Detail%0A%20%20Recovery%20and%20a%20Novel%20Contrastive%20Learning%20Paradigm&entry.906535625=Gao%20Yu%20Lee%20and%20Tanmoy%20Dam%20and%20Md%20Meftahul%20Ferdaus%20and%20Daniel%20Puiu%20Poenar%20and%20Vu%20Duong&entry.1292438233=%20%20Image%20dehazing%20is%20crucial%20for%20clarifying%20images%20obscured%20by%20haze%20or%20fog%2C%20but%0Acurrent%20learning-based%20approaches%20is%20dependent%20on%20large%20volumes%20of%20training%0Adata%20and%20hence%20consumed%20significant%20computational%20power.%20Additionally%2C%20their%0Aperformance%20is%20often%20inadequate%20under%20non-uniform%20or%20heavy%20haze.%20To%20address%0Athese%20challenges%2C%20we%20developed%20the%20Detail%20Recovery%20And%20Contrastive%20DehazeNet%2C%0Awhich%20facilitates%20efficient%20and%20effective%20dehazing%20via%20a%20dense%20dilated%20inverted%0Aresidual%20block%20and%20an%20attention-based%20detail%20recovery%20network%20that%20tailors%0Aenhancements%20to%20specific%20dehazed%20scene%20contexts.%20A%20major%20innovation%20is%20its%0Aability%20to%20train%20effectively%20with%20limited%20data%2C%20achieved%20through%20a%20novel%0Aquadruplet%20loss-based%20contrastive%20dehazing%20paradigm.%20This%20approach%20distinctly%0Aseparates%20hazy%20and%20clear%20image%20features%20while%20also%20distinguish%20lower-quality%0Aand%20higher-quality%20dehazed%20images%20obtained%20from%20each%20sub-modules%20of%20our%0Anetwork%2C%20thereby%20refining%20the%20dehazing%20process%20to%20a%20larger%20extent.%20Extensive%0Atests%20on%20a%20variety%20of%20benchmarked%20haze%20datasets%20demonstrated%20the%20superiority%20of%0Aour%20approach.%20The%20code%20repository%20for%20this%20work%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14595v1&entry.124074799=Read"},
{"title": "Less is More: Selective Reduction of CT Data for Self-Supervised\n  Pre-Training of Deep Learning Models with Contrastive Learning Improves\n  Downstream Classification Performance", "author": "Daniel Wolf and Tristan Payer and Catharina Silvia Lisson and Christoph Gerhard Lisson and Meinrad Beer and Michael G\u00f6tz and Timo Ropinski", "abstract": "  Self-supervised pre-training of deep learning models with contrastive\nlearning is a widely used technique in image analysis. Current findings\nindicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular\ncharacteristics of these images. We hypothesize that the similarity of medical\nimages hinders the success of contrastive learning in the medical imaging\ndomain. To this end, we investigate different strategies based on deep\nembedding, information theory, and hashing in order to identify and reduce\nredundancy in medical pre-training datasets. The effect of these different\nreduction strategies on contrastive learning is evaluated on two pre-training\ndatasets and several downstream classification tasks. In all of our\nexperiments, dataset reduction leads to a considerable performance gain in\ndownstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the\nCOVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST\nClassification Challenge and 0.73 to 0.83 for a brain hemorrhage classification\ntask. Furthermore, pre-training is up to nine times faster due to the dataset\nreduction. In conclusion, the proposed approach highlights the importance of\ndataset quality and provides a transferable approach to improve contrastive\npre-training for classification downstream tasks on medical images.\n", "link": "http://arxiv.org/abs/2410.14524v1", "date": "2024-10-18", "relevancy": 2.1503, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.559}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5287}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Selective%20Reduction%20of%20CT%20Data%20for%20Self-Supervised%0A%20%20Pre-Training%20of%20Deep%20Learning%20Models%20with%20Contrastive%20Learning%20Improves%0A%20%20Downstream%20Classification%20Performance&body=Title%3A%20Less%20is%20More%3A%20Selective%20Reduction%20of%20CT%20Data%20for%20Self-Supervised%0A%20%20Pre-Training%20of%20Deep%20Learning%20Models%20with%20Contrastive%20Learning%20Improves%0A%20%20Downstream%20Classification%20Performance%0AAuthor%3A%20Daniel%20Wolf%20and%20Tristan%20Payer%20and%20Catharina%20Silvia%20Lisson%20and%20Christoph%20Gerhard%20Lisson%20and%20Meinrad%20Beer%20and%20Michael%20G%C3%B6tz%20and%20Timo%20Ropinski%0AAbstract%3A%20%20%20Self-supervised%20pre-training%20of%20deep%20learning%20models%20with%20contrastive%0Alearning%20is%20a%20widely%20used%20technique%20in%20image%20analysis.%20Current%20findings%0Aindicate%20a%20strong%20potential%20for%20contrastive%20pre-training%20on%20medical%20images.%0AHowever%2C%20further%20research%20is%20necessary%20to%20incorporate%20the%20particular%0Acharacteristics%20of%20these%20images.%20We%20hypothesize%20that%20the%20similarity%20of%20medical%0Aimages%20hinders%20the%20success%20of%20contrastive%20learning%20in%20the%20medical%20imaging%0Adomain.%20To%20this%20end%2C%20we%20investigate%20different%20strategies%20based%20on%20deep%0Aembedding%2C%20information%20theory%2C%20and%20hashing%20in%20order%20to%20identify%20and%20reduce%0Aredundancy%20in%20medical%20pre-training%20datasets.%20The%20effect%20of%20these%20different%0Areduction%20strategies%20on%20contrastive%20learning%20is%20evaluated%20on%20two%20pre-training%0Adatasets%20and%20several%20downstream%20classification%20tasks.%20In%20all%20of%20our%0Aexperiments%2C%20dataset%20reduction%20leads%20to%20a%20considerable%20performance%20gain%20in%0Adownstream%20tasks%2C%20e.g.%2C%20an%20AUC%20score%20improvement%20from%200.78%20to%200.83%20for%20the%0ACOVID%20CT%20Classification%20Grand%20Challenge%2C%200.97%20to%200.98%20for%20the%20OrganSMNIST%0AClassification%20Challenge%20and%200.73%20to%200.83%20for%20a%20brain%20hemorrhage%20classification%0Atask.%20Furthermore%2C%20pre-training%20is%20up%20to%20nine%20times%20faster%20due%20to%20the%20dataset%0Areduction.%20In%20conclusion%2C%20the%20proposed%20approach%20highlights%20the%20importance%20of%0Adataset%20quality%20and%20provides%20a%20transferable%20approach%20to%20improve%20contrastive%0Apre-training%20for%20classification%20downstream%20tasks%20on%20medical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Selective%2520Reduction%2520of%2520CT%2520Data%2520for%2520Self-Supervised%250A%2520%2520Pre-Training%2520of%2520Deep%2520Learning%2520Models%2520with%2520Contrastive%2520Learning%2520Improves%250A%2520%2520Downstream%2520Classification%2520Performance%26entry.906535625%3DDaniel%2520Wolf%2520and%2520Tristan%2520Payer%2520and%2520Catharina%2520Silvia%2520Lisson%2520and%2520Christoph%2520Gerhard%2520Lisson%2520and%2520Meinrad%2520Beer%2520and%2520Michael%2520G%25C3%25B6tz%2520and%2520Timo%2520Ropinski%26entry.1292438233%3D%2520%2520Self-supervised%2520pre-training%2520of%2520deep%2520learning%2520models%2520with%2520contrastive%250Alearning%2520is%2520a%2520widely%2520used%2520technique%2520in%2520image%2520analysis.%2520Current%2520findings%250Aindicate%2520a%2520strong%2520potential%2520for%2520contrastive%2520pre-training%2520on%2520medical%2520images.%250AHowever%252C%2520further%2520research%2520is%2520necessary%2520to%2520incorporate%2520the%2520particular%250Acharacteristics%2520of%2520these%2520images.%2520We%2520hypothesize%2520that%2520the%2520similarity%2520of%2520medical%250Aimages%2520hinders%2520the%2520success%2520of%2520contrastive%2520learning%2520in%2520the%2520medical%2520imaging%250Adomain.%2520To%2520this%2520end%252C%2520we%2520investigate%2520different%2520strategies%2520based%2520on%2520deep%250Aembedding%252C%2520information%2520theory%252C%2520and%2520hashing%2520in%2520order%2520to%2520identify%2520and%2520reduce%250Aredundancy%2520in%2520medical%2520pre-training%2520datasets.%2520The%2520effect%2520of%2520these%2520different%250Areduction%2520strategies%2520on%2520contrastive%2520learning%2520is%2520evaluated%2520on%2520two%2520pre-training%250Adatasets%2520and%2520several%2520downstream%2520classification%2520tasks.%2520In%2520all%2520of%2520our%250Aexperiments%252C%2520dataset%2520reduction%2520leads%2520to%2520a%2520considerable%2520performance%2520gain%2520in%250Adownstream%2520tasks%252C%2520e.g.%252C%2520an%2520AUC%2520score%2520improvement%2520from%25200.78%2520to%25200.83%2520for%2520the%250ACOVID%2520CT%2520Classification%2520Grand%2520Challenge%252C%25200.97%2520to%25200.98%2520for%2520the%2520OrganSMNIST%250AClassification%2520Challenge%2520and%25200.73%2520to%25200.83%2520for%2520a%2520brain%2520hemorrhage%2520classification%250Atask.%2520Furthermore%252C%2520pre-training%2520is%2520up%2520to%2520nine%2520times%2520faster%2520due%2520to%2520the%2520dataset%250Areduction.%2520In%2520conclusion%252C%2520the%2520proposed%2520approach%2520highlights%2520the%2520importance%2520of%250Adataset%2520quality%2520and%2520provides%2520a%2520transferable%2520approach%2520to%2520improve%2520contrastive%250Apre-training%2520for%2520classification%2520downstream%2520tasks%2520on%2520medical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Selective%20Reduction%20of%20CT%20Data%20for%20Self-Supervised%0A%20%20Pre-Training%20of%20Deep%20Learning%20Models%20with%20Contrastive%20Learning%20Improves%0A%20%20Downstream%20Classification%20Performance&entry.906535625=Daniel%20Wolf%20and%20Tristan%20Payer%20and%20Catharina%20Silvia%20Lisson%20and%20Christoph%20Gerhard%20Lisson%20and%20Meinrad%20Beer%20and%20Michael%20G%C3%B6tz%20and%20Timo%20Ropinski&entry.1292438233=%20%20Self-supervised%20pre-training%20of%20deep%20learning%20models%20with%20contrastive%0Alearning%20is%20a%20widely%20used%20technique%20in%20image%20analysis.%20Current%20findings%0Aindicate%20a%20strong%20potential%20for%20contrastive%20pre-training%20on%20medical%20images.%0AHowever%2C%20further%20research%20is%20necessary%20to%20incorporate%20the%20particular%0Acharacteristics%20of%20these%20images.%20We%20hypothesize%20that%20the%20similarity%20of%20medical%0Aimages%20hinders%20the%20success%20of%20contrastive%20learning%20in%20the%20medical%20imaging%0Adomain.%20To%20this%20end%2C%20we%20investigate%20different%20strategies%20based%20on%20deep%0Aembedding%2C%20information%20theory%2C%20and%20hashing%20in%20order%20to%20identify%20and%20reduce%0Aredundancy%20in%20medical%20pre-training%20datasets.%20The%20effect%20of%20these%20different%0Areduction%20strategies%20on%20contrastive%20learning%20is%20evaluated%20on%20two%20pre-training%0Adatasets%20and%20several%20downstream%20classification%20tasks.%20In%20all%20of%20our%0Aexperiments%2C%20dataset%20reduction%20leads%20to%20a%20considerable%20performance%20gain%20in%0Adownstream%20tasks%2C%20e.g.%2C%20an%20AUC%20score%20improvement%20from%200.78%20to%200.83%20for%20the%0ACOVID%20CT%20Classification%20Grand%20Challenge%2C%200.97%20to%200.98%20for%20the%20OrganSMNIST%0AClassification%20Challenge%20and%200.73%20to%200.83%20for%20a%20brain%20hemorrhage%20classification%0Atask.%20Furthermore%2C%20pre-training%20is%20up%20to%20nine%20times%20faster%20due%20to%20the%20dataset%0Areduction.%20In%20conclusion%2C%20the%20proposed%20approach%20highlights%20the%20importance%20of%0Adataset%20quality%20and%20provides%20a%20transferable%20approach%20to%20improve%20contrastive%0Apre-training%20for%20classification%20downstream%20tasks%20on%20medical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14524v1&entry.124074799=Read"},
{"title": "Domain Adaptive Safety Filters via Deep Operator Learning", "author": "Lakshmideepakreddy Manda and Shaoru Chen and Mahyar Fazlyab", "abstract": "  Learning-based approaches for constructing Control Barrier Functions (CBFs)\nare increasingly being explored for safety-critical control systems. However,\nthese methods typically require complete retraining when applied to unseen\nenvironments, limiting their adaptability. To address this, we propose a\nself-supervised deep operator learning framework that learns the mapping from\nenvironmental parameters to the corresponding CBF, rather than learning the CBF\ndirectly. Our approach leverages the residual of a parametric Partial\nDifferential Equation (PDE), where the solution defines a parametric CBF\napproximating the maximal control invariant set. This framework accommodates\ncomplex safety constraints, higher relative degrees, and actuation limits. We\ndemonstrate the effectiveness of the method through numerical experiments on\nnavigation tasks involving dynamic obstacles.\n", "link": "http://arxiv.org/abs/2410.14528v1", "date": "2024-10-18", "relevancy": 2.1328, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Adaptive%20Safety%20Filters%20via%20Deep%20Operator%20Learning&body=Title%3A%20Domain%20Adaptive%20Safety%20Filters%20via%20Deep%20Operator%20Learning%0AAuthor%3A%20Lakshmideepakreddy%20Manda%20and%20Shaoru%20Chen%20and%20Mahyar%20Fazlyab%0AAbstract%3A%20%20%20Learning-based%20approaches%20for%20constructing%20Control%20Barrier%20Functions%20%28CBFs%29%0Aare%20increasingly%20being%20explored%20for%20safety-critical%20control%20systems.%20However%2C%0Athese%20methods%20typically%20require%20complete%20retraining%20when%20applied%20to%20unseen%0Aenvironments%2C%20limiting%20their%20adaptability.%20To%20address%20this%2C%20we%20propose%20a%0Aself-supervised%20deep%20operator%20learning%20framework%20that%20learns%20the%20mapping%20from%0Aenvironmental%20parameters%20to%20the%20corresponding%20CBF%2C%20rather%20than%20learning%20the%20CBF%0Adirectly.%20Our%20approach%20leverages%20the%20residual%20of%20a%20parametric%20Partial%0ADifferential%20Equation%20%28PDE%29%2C%20where%20the%20solution%20defines%20a%20parametric%20CBF%0Aapproximating%20the%20maximal%20control%20invariant%20set.%20This%20framework%20accommodates%0Acomplex%20safety%20constraints%2C%20higher%20relative%20degrees%2C%20and%20actuation%20limits.%20We%0Ademonstrate%20the%20effectiveness%20of%20the%20method%20through%20numerical%20experiments%20on%0Anavigation%20tasks%20involving%20dynamic%20obstacles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Adaptive%2520Safety%2520Filters%2520via%2520Deep%2520Operator%2520Learning%26entry.906535625%3DLakshmideepakreddy%2520Manda%2520and%2520Shaoru%2520Chen%2520and%2520Mahyar%2520Fazlyab%26entry.1292438233%3D%2520%2520Learning-based%2520approaches%2520for%2520constructing%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%250Aare%2520increasingly%2520being%2520explored%2520for%2520safety-critical%2520control%2520systems.%2520However%252C%250Athese%2520methods%2520typically%2520require%2520complete%2520retraining%2520when%2520applied%2520to%2520unseen%250Aenvironments%252C%2520limiting%2520their%2520adaptability.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aself-supervised%2520deep%2520operator%2520learning%2520framework%2520that%2520learns%2520the%2520mapping%2520from%250Aenvironmental%2520parameters%2520to%2520the%2520corresponding%2520CBF%252C%2520rather%2520than%2520learning%2520the%2520CBF%250Adirectly.%2520Our%2520approach%2520leverages%2520the%2520residual%2520of%2520a%2520parametric%2520Partial%250ADifferential%2520Equation%2520%2528PDE%2529%252C%2520where%2520the%2520solution%2520defines%2520a%2520parametric%2520CBF%250Aapproximating%2520the%2520maximal%2520control%2520invariant%2520set.%2520This%2520framework%2520accommodates%250Acomplex%2520safety%2520constraints%252C%2520higher%2520relative%2520degrees%252C%2520and%2520actuation%2520limits.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520method%2520through%2520numerical%2520experiments%2520on%250Anavigation%2520tasks%2520involving%2520dynamic%2520obstacles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Adaptive%20Safety%20Filters%20via%20Deep%20Operator%20Learning&entry.906535625=Lakshmideepakreddy%20Manda%20and%20Shaoru%20Chen%20and%20Mahyar%20Fazlyab&entry.1292438233=%20%20Learning-based%20approaches%20for%20constructing%20Control%20Barrier%20Functions%20%28CBFs%29%0Aare%20increasingly%20being%20explored%20for%20safety-critical%20control%20systems.%20However%2C%0Athese%20methods%20typically%20require%20complete%20retraining%20when%20applied%20to%20unseen%0Aenvironments%2C%20limiting%20their%20adaptability.%20To%20address%20this%2C%20we%20propose%20a%0Aself-supervised%20deep%20operator%20learning%20framework%20that%20learns%20the%20mapping%20from%0Aenvironmental%20parameters%20to%20the%20corresponding%20CBF%2C%20rather%20than%20learning%20the%20CBF%0Adirectly.%20Our%20approach%20leverages%20the%20residual%20of%20a%20parametric%20Partial%0ADifferential%20Equation%20%28PDE%29%2C%20where%20the%20solution%20defines%20a%20parametric%20CBF%0Aapproximating%20the%20maximal%20control%20invariant%20set.%20This%20framework%20accommodates%0Acomplex%20safety%20constraints%2C%20higher%20relative%20degrees%2C%20and%20actuation%20limits.%20We%0Ademonstrate%20the%20effectiveness%20of%20the%20method%20through%20numerical%20experiments%20on%0Anavigation%20tasks%20involving%20dynamic%20obstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14528v1&entry.124074799=Read"},
{"title": "MultiOrg: A Multi-rater Organoid-detection Dataset", "author": "Christina Bukas and Harshavardhan Subramanian and Fenja See and Carina Steinchen and Ivan Ezhov and Gowtham Boosarpu and Sara Asgharpour and Gerald Burgstaller and Mareike Lehmann and Florian Kofler and Marie Piraud", "abstract": "  High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.\n", "link": "http://arxiv.org/abs/2410.14612v1", "date": "2024-10-18", "relevancy": 2.1202, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5233}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiOrg%3A%20A%20Multi-rater%20Organoid-detection%20Dataset&body=Title%3A%20MultiOrg%3A%20A%20Multi-rater%20Organoid-detection%20Dataset%0AAuthor%3A%20Christina%20Bukas%20and%20Harshavardhan%20Subramanian%20and%20Fenja%20See%20and%20Carina%20Steinchen%20and%20Ivan%20Ezhov%20and%20Gowtham%20Boosarpu%20and%20Sara%20Asgharpour%20and%20Gerald%20Burgstaller%20and%20Mareike%20Lehmann%20and%20Florian%20Kofler%20and%20Marie%20Piraud%0AAbstract%3A%20%20%20High-throughput%20image%20analysis%20in%20the%20biomedical%20domain%20has%20gained%0Asignificant%20attention%20in%20recent%20years%2C%20driving%20advancements%20in%20drug%20discovery%2C%0Adisease%20prediction%2C%20and%20personalized%20medicine.%20Organoids%2C%20specifically%2C%20are%20an%0Aactive%20area%20of%20research%2C%20providing%20excellent%20models%20for%20human%20organs%20and%20their%0Afunctions.%20Automating%20the%20quantification%20of%20organoids%20in%20microscopy%20images%0Awould%20provide%20an%20effective%20solution%20to%20overcome%20substantial%20manual%0Aquantification%20bottlenecks%2C%20particularly%20in%20high-throughput%20image%20analysis.%0AHowever%2C%20there%20is%20a%20notable%20lack%20of%20open%20biomedical%20datasets%2C%20in%20contrast%20to%0Aother%20domains%2C%20such%20as%20autonomous%20driving%2C%20and%2C%20notably%2C%20only%20few%20of%20them%20have%0Aattempted%20to%20quantify%20annotation%20uncertainty.%20In%20this%20work%2C%20we%20present%20MultiOrg%0Aa%20comprehensive%20organoid%20dataset%20tailored%20for%20object%20detection%20tasks%20with%0Auncertainty%20quantification.%20This%20dataset%20comprises%20over%20400%20high-resolution%202d%0Amicroscopy%20images%20and%20curated%20annotations%20of%20more%20than%2060%2C000%20organoids.%20Most%0Aimportantly%2C%20it%20includes%20three%20label%20sets%20for%20the%20test%20data%2C%20independently%0Aannotated%20by%20two%20experts%20at%20distinct%20time%20points.%20We%20additionally%20provide%20a%0Abenchmark%20for%20organoid%20detection%2C%20and%20make%20the%20best%20model%20available%20through%20an%0Aeasily%20installable%2C%20interactive%20plugin%20for%20the%20popular%20image%20visualization%20tool%0ANapari%2C%20to%20perform%20organoid%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiOrg%253A%2520A%2520Multi-rater%2520Organoid-detection%2520Dataset%26entry.906535625%3DChristina%2520Bukas%2520and%2520Harshavardhan%2520Subramanian%2520and%2520Fenja%2520See%2520and%2520Carina%2520Steinchen%2520and%2520Ivan%2520Ezhov%2520and%2520Gowtham%2520Boosarpu%2520and%2520Sara%2520Asgharpour%2520and%2520Gerald%2520Burgstaller%2520and%2520Mareike%2520Lehmann%2520and%2520Florian%2520Kofler%2520and%2520Marie%2520Piraud%26entry.1292438233%3D%2520%2520High-throughput%2520image%2520analysis%2520in%2520the%2520biomedical%2520domain%2520has%2520gained%250Asignificant%2520attention%2520in%2520recent%2520years%252C%2520driving%2520advancements%2520in%2520drug%2520discovery%252C%250Adisease%2520prediction%252C%2520and%2520personalized%2520medicine.%2520Organoids%252C%2520specifically%252C%2520are%2520an%250Aactive%2520area%2520of%2520research%252C%2520providing%2520excellent%2520models%2520for%2520human%2520organs%2520and%2520their%250Afunctions.%2520Automating%2520the%2520quantification%2520of%2520organoids%2520in%2520microscopy%2520images%250Awould%2520provide%2520an%2520effective%2520solution%2520to%2520overcome%2520substantial%2520manual%250Aquantification%2520bottlenecks%252C%2520particularly%2520in%2520high-throughput%2520image%2520analysis.%250AHowever%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%2520open%2520biomedical%2520datasets%252C%2520in%2520contrast%2520to%250Aother%2520domains%252C%2520such%2520as%2520autonomous%2520driving%252C%2520and%252C%2520notably%252C%2520only%2520few%2520of%2520them%2520have%250Aattempted%2520to%2520quantify%2520annotation%2520uncertainty.%2520In%2520this%2520work%252C%2520we%2520present%2520MultiOrg%250Aa%2520comprehensive%2520organoid%2520dataset%2520tailored%2520for%2520object%2520detection%2520tasks%2520with%250Auncertainty%2520quantification.%2520This%2520dataset%2520comprises%2520over%2520400%2520high-resolution%25202d%250Amicroscopy%2520images%2520and%2520curated%2520annotations%2520of%2520more%2520than%252060%252C000%2520organoids.%2520Most%250Aimportantly%252C%2520it%2520includes%2520three%2520label%2520sets%2520for%2520the%2520test%2520data%252C%2520independently%250Aannotated%2520by%2520two%2520experts%2520at%2520distinct%2520time%2520points.%2520We%2520additionally%2520provide%2520a%250Abenchmark%2520for%2520organoid%2520detection%252C%2520and%2520make%2520the%2520best%2520model%2520available%2520through%2520an%250Aeasily%2520installable%252C%2520interactive%2520plugin%2520for%2520the%2520popular%2520image%2520visualization%2520tool%250ANapari%252C%2520to%2520perform%2520organoid%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiOrg%3A%20A%20Multi-rater%20Organoid-detection%20Dataset&entry.906535625=Christina%20Bukas%20and%20Harshavardhan%20Subramanian%20and%20Fenja%20See%20and%20Carina%20Steinchen%20and%20Ivan%20Ezhov%20and%20Gowtham%20Boosarpu%20and%20Sara%20Asgharpour%20and%20Gerald%20Burgstaller%20and%20Mareike%20Lehmann%20and%20Florian%20Kofler%20and%20Marie%20Piraud&entry.1292438233=%20%20High-throughput%20image%20analysis%20in%20the%20biomedical%20domain%20has%20gained%0Asignificant%20attention%20in%20recent%20years%2C%20driving%20advancements%20in%20drug%20discovery%2C%0Adisease%20prediction%2C%20and%20personalized%20medicine.%20Organoids%2C%20specifically%2C%20are%20an%0Aactive%20area%20of%20research%2C%20providing%20excellent%20models%20for%20human%20organs%20and%20their%0Afunctions.%20Automating%20the%20quantification%20of%20organoids%20in%20microscopy%20images%0Awould%20provide%20an%20effective%20solution%20to%20overcome%20substantial%20manual%0Aquantification%20bottlenecks%2C%20particularly%20in%20high-throughput%20image%20analysis.%0AHowever%2C%20there%20is%20a%20notable%20lack%20of%20open%20biomedical%20datasets%2C%20in%20contrast%20to%0Aother%20domains%2C%20such%20as%20autonomous%20driving%2C%20and%2C%20notably%2C%20only%20few%20of%20them%20have%0Aattempted%20to%20quantify%20annotation%20uncertainty.%20In%20this%20work%2C%20we%20present%20MultiOrg%0Aa%20comprehensive%20organoid%20dataset%20tailored%20for%20object%20detection%20tasks%20with%0Auncertainty%20quantification.%20This%20dataset%20comprises%20over%20400%20high-resolution%202d%0Amicroscopy%20images%20and%20curated%20annotations%20of%20more%20than%2060%2C000%20organoids.%20Most%0Aimportantly%2C%20it%20includes%20three%20label%20sets%20for%20the%20test%20data%2C%20independently%0Aannotated%20by%20two%20experts%20at%20distinct%20time%20points.%20We%20additionally%20provide%20a%0Abenchmark%20for%20organoid%20detection%2C%20and%20make%20the%20best%20model%20available%20through%20an%0Aeasily%20installable%2C%20interactive%20plugin%20for%20the%20popular%20image%20visualization%20tool%0ANapari%2C%20to%20perform%20organoid%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14612v1&entry.124074799=Read"},
{"title": "Privacy-Preserving Decentralized AI with Confidential Computing", "author": "Dayeol Lee and Jorge Ant\u00f3nio and Hisham Khan", "abstract": "  This paper addresses privacy protection in decentralized Artificial\nIntelligence (AI) using Confidential Computing (CC) within the Atoma Network, a\ndecentralized AI platform designed for the Web3 domain. Decentralized AI\ndistributes AI services among multiple entities without centralized oversight,\nfostering transparency and robustness. However, this structure introduces\nsignificant privacy challenges, as sensitive assets such as proprietary models\nand personal data may be exposed to untrusted participants. Cryptography-based\nprivacy protection techniques such as zero-knowledge machine learning (zkML)\nsuffers prohibitive computational overhead. To address the limitation, we\npropose leveraging Confidential Computing (CC). Confidential Computing\nleverages hardware-based Trusted Execution Environments (TEEs) to provide\nisolation for processing sensitive data, ensuring that both model parameters\nand user data remain secure, even in decentralized, potentially untrusted\nenvironments. While TEEs face a few limitations, we believe they can bridge the\nprivacy gap in decentralized AI. We explore how we can integrate TEEs into\nAtoma's decentralized framework.\n", "link": "http://arxiv.org/abs/2410.13752v2", "date": "2024-10-18", "relevancy": 2.1126, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4279}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4208}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing&body=Title%3A%20Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing%0AAuthor%3A%20Dayeol%20Lee%20and%20Jorge%20Ant%C3%B3nio%20and%20Hisham%20Khan%0AAbstract%3A%20%20%20This%20paper%20addresses%20privacy%20protection%20in%20decentralized%20Artificial%0AIntelligence%20%28AI%29%20using%20Confidential%20Computing%20%28CC%29%20within%20the%20Atoma%20Network%2C%20a%0Adecentralized%20AI%20platform%20designed%20for%20the%20Web3%20domain.%20Decentralized%20AI%0Adistributes%20AI%20services%20among%20multiple%20entities%20without%20centralized%20oversight%2C%0Afostering%20transparency%20and%20robustness.%20However%2C%20this%20structure%20introduces%0Asignificant%20privacy%20challenges%2C%20as%20sensitive%20assets%20such%20as%20proprietary%20models%0Aand%20personal%20data%20may%20be%20exposed%20to%20untrusted%20participants.%20Cryptography-based%0Aprivacy%20protection%20techniques%20such%20as%20zero-knowledge%20machine%20learning%20%28zkML%29%0Asuffers%20prohibitive%20computational%20overhead.%20To%20address%20the%20limitation%2C%20we%0Apropose%20leveraging%20Confidential%20Computing%20%28CC%29.%20Confidential%20Computing%0Aleverages%20hardware-based%20Trusted%20Execution%20Environments%20%28TEEs%29%20to%20provide%0Aisolation%20for%20processing%20sensitive%20data%2C%20ensuring%20that%20both%20model%20parameters%0Aand%20user%20data%20remain%20secure%2C%20even%20in%20decentralized%2C%20potentially%20untrusted%0Aenvironments.%20While%20TEEs%20face%20a%20few%20limitations%2C%20we%20believe%20they%20can%20bridge%20the%0Aprivacy%20gap%20in%20decentralized%20AI.%20We%20explore%20how%20we%20can%20integrate%20TEEs%20into%0AAtoma%27s%20decentralized%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-Preserving%2520Decentralized%2520AI%2520with%2520Confidential%2520Computing%26entry.906535625%3DDayeol%2520Lee%2520and%2520Jorge%2520Ant%25C3%25B3nio%2520and%2520Hisham%2520Khan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520privacy%2520protection%2520in%2520decentralized%2520Artificial%250AIntelligence%2520%2528AI%2529%2520using%2520Confidential%2520Computing%2520%2528CC%2529%2520within%2520the%2520Atoma%2520Network%252C%2520a%250Adecentralized%2520AI%2520platform%2520designed%2520for%2520the%2520Web3%2520domain.%2520Decentralized%2520AI%250Adistributes%2520AI%2520services%2520among%2520multiple%2520entities%2520without%2520centralized%2520oversight%252C%250Afostering%2520transparency%2520and%2520robustness.%2520However%252C%2520this%2520structure%2520introduces%250Asignificant%2520privacy%2520challenges%252C%2520as%2520sensitive%2520assets%2520such%2520as%2520proprietary%2520models%250Aand%2520personal%2520data%2520may%2520be%2520exposed%2520to%2520untrusted%2520participants.%2520Cryptography-based%250Aprivacy%2520protection%2520techniques%2520such%2520as%2520zero-knowledge%2520machine%2520learning%2520%2528zkML%2529%250Asuffers%2520prohibitive%2520computational%2520overhead.%2520To%2520address%2520the%2520limitation%252C%2520we%250Apropose%2520leveraging%2520Confidential%2520Computing%2520%2528CC%2529.%2520Confidential%2520Computing%250Aleverages%2520hardware-based%2520Trusted%2520Execution%2520Environments%2520%2528TEEs%2529%2520to%2520provide%250Aisolation%2520for%2520processing%2520sensitive%2520data%252C%2520ensuring%2520that%2520both%2520model%2520parameters%250Aand%2520user%2520data%2520remain%2520secure%252C%2520even%2520in%2520decentralized%252C%2520potentially%2520untrusted%250Aenvironments.%2520While%2520TEEs%2520face%2520a%2520few%2520limitations%252C%2520we%2520believe%2520they%2520can%2520bridge%2520the%250Aprivacy%2520gap%2520in%2520decentralized%2520AI.%2520We%2520explore%2520how%2520we%2520can%2520integrate%2520TEEs%2520into%250AAtoma%2527s%2520decentralized%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Preserving%20Decentralized%20AI%20with%20Confidential%20Computing&entry.906535625=Dayeol%20Lee%20and%20Jorge%20Ant%C3%B3nio%20and%20Hisham%20Khan&entry.1292438233=%20%20This%20paper%20addresses%20privacy%20protection%20in%20decentralized%20Artificial%0AIntelligence%20%28AI%29%20using%20Confidential%20Computing%20%28CC%29%20within%20the%20Atoma%20Network%2C%20a%0Adecentralized%20AI%20platform%20designed%20for%20the%20Web3%20domain.%20Decentralized%20AI%0Adistributes%20AI%20services%20among%20multiple%20entities%20without%20centralized%20oversight%2C%0Afostering%20transparency%20and%20robustness.%20However%2C%20this%20structure%20introduces%0Asignificant%20privacy%20challenges%2C%20as%20sensitive%20assets%20such%20as%20proprietary%20models%0Aand%20personal%20data%20may%20be%20exposed%20to%20untrusted%20participants.%20Cryptography-based%0Aprivacy%20protection%20techniques%20such%20as%20zero-knowledge%20machine%20learning%20%28zkML%29%0Asuffers%20prohibitive%20computational%20overhead.%20To%20address%20the%20limitation%2C%20we%0Apropose%20leveraging%20Confidential%20Computing%20%28CC%29.%20Confidential%20Computing%0Aleverages%20hardware-based%20Trusted%20Execution%20Environments%20%28TEEs%29%20to%20provide%0Aisolation%20for%20processing%20sensitive%20data%2C%20ensuring%20that%20both%20model%20parameters%0Aand%20user%20data%20remain%20secure%2C%20even%20in%20decentralized%2C%20potentially%20untrusted%0Aenvironments.%20While%20TEEs%20face%20a%20few%20limitations%2C%20we%20believe%20they%20can%20bridge%20the%0Aprivacy%20gap%20in%20decentralized%20AI.%20We%20explore%20how%20we%20can%20integrate%20TEEs%20into%0AAtoma%27s%20decentralized%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13752v2&entry.124074799=Read"},
{"title": "On Efficient Variants of Segment Anything Model: A Survey", "author": "Xiaorui Sun and Jun Liu and Heng Tao Shen and Xiaofeng Zhu and Ping Hu", "abstract": "  The Segment Anything Model (SAM) is a foundational model for image\nsegmentation tasks, known for its strong generalization across diverse\napplications. However, its impressive performance comes with significant\ncomputational and resource demands, making it challenging to deploy in\nresource-limited environments such as edge devices. To address this, a variety\nof SAM variants have been proposed to enhance efficiency while keeping\naccuracy. This survey provides the first comprehensive review of these\nefficient SAM variants. We begin by exploring the motivations driving this\nresearch. We then present core techniques used in SAM and model acceleration.\nThis is followed by a detailed exploration of SAM acceleration strategies,\ncategorized by approach, and a discussion of several future research\ndirections. Finally, we offer a unified and extensive evaluation of these\nmethods across various hardware, assessing their efficiency and accuracy on\nrepresentative benchmarks, and providing a clear comparison of their overall\nperformance.\n", "link": "http://arxiv.org/abs/2410.04960v2", "date": "2024-10-18", "relevancy": 2.0992, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Efficient%20Variants%20of%20Segment%20Anything%20Model%3A%20A%20Survey&body=Title%3A%20On%20Efficient%20Variants%20of%20Segment%20Anything%20Model%3A%20A%20Survey%0AAuthor%3A%20Xiaorui%20Sun%20and%20Jun%20Liu%20and%20Heng%20Tao%20Shen%20and%20Xiaofeng%20Zhu%20and%20Ping%20Hu%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20foundational%20model%20for%20image%0Asegmentation%20tasks%2C%20known%20for%20its%20strong%20generalization%20across%20diverse%0Aapplications.%20However%2C%20its%20impressive%20performance%20comes%20with%20significant%0Acomputational%20and%20resource%20demands%2C%20making%20it%20challenging%20to%20deploy%20in%0Aresource-limited%20environments%20such%20as%20edge%20devices.%20To%20address%20this%2C%20a%20variety%0Aof%20SAM%20variants%20have%20been%20proposed%20to%20enhance%20efficiency%20while%20keeping%0Aaccuracy.%20This%20survey%20provides%20the%20first%20comprehensive%20review%20of%20these%0Aefficient%20SAM%20variants.%20We%20begin%20by%20exploring%20the%20motivations%20driving%20this%0Aresearch.%20We%20then%20present%20core%20techniques%20used%20in%20SAM%20and%20model%20acceleration.%0AThis%20is%20followed%20by%20a%20detailed%20exploration%20of%20SAM%20acceleration%20strategies%2C%0Acategorized%20by%20approach%2C%20and%20a%20discussion%20of%20several%20future%20research%0Adirections.%20Finally%2C%20we%20offer%20a%20unified%20and%20extensive%20evaluation%20of%20these%0Amethods%20across%20various%20hardware%2C%20assessing%20their%20efficiency%20and%20accuracy%20on%0Arepresentative%20benchmarks%2C%20and%20providing%20a%20clear%20comparison%20of%20their%20overall%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Efficient%2520Variants%2520of%2520Segment%2520Anything%2520Model%253A%2520A%2520Survey%26entry.906535625%3DXiaorui%2520Sun%2520and%2520Jun%2520Liu%2520and%2520Heng%2520Tao%2520Shen%2520and%2520Xiaofeng%2520Zhu%2520and%2520Ping%2520Hu%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520a%2520foundational%2520model%2520for%2520image%250Asegmentation%2520tasks%252C%2520known%2520for%2520its%2520strong%2520generalization%2520across%2520diverse%250Aapplications.%2520However%252C%2520its%2520impressive%2520performance%2520comes%2520with%2520significant%250Acomputational%2520and%2520resource%2520demands%252C%2520making%2520it%2520challenging%2520to%2520deploy%2520in%250Aresource-limited%2520environments%2520such%2520as%2520edge%2520devices.%2520To%2520address%2520this%252C%2520a%2520variety%250Aof%2520SAM%2520variants%2520have%2520been%2520proposed%2520to%2520enhance%2520efficiency%2520while%2520keeping%250Aaccuracy.%2520This%2520survey%2520provides%2520the%2520first%2520comprehensive%2520review%2520of%2520these%250Aefficient%2520SAM%2520variants.%2520We%2520begin%2520by%2520exploring%2520the%2520motivations%2520driving%2520this%250Aresearch.%2520We%2520then%2520present%2520core%2520techniques%2520used%2520in%2520SAM%2520and%2520model%2520acceleration.%250AThis%2520is%2520followed%2520by%2520a%2520detailed%2520exploration%2520of%2520SAM%2520acceleration%2520strategies%252C%250Acategorized%2520by%2520approach%252C%2520and%2520a%2520discussion%2520of%2520several%2520future%2520research%250Adirections.%2520Finally%252C%2520we%2520offer%2520a%2520unified%2520and%2520extensive%2520evaluation%2520of%2520these%250Amethods%2520across%2520various%2520hardware%252C%2520assessing%2520their%2520efficiency%2520and%2520accuracy%2520on%250Arepresentative%2520benchmarks%252C%2520and%2520providing%2520a%2520clear%2520comparison%2520of%2520their%2520overall%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Efficient%20Variants%20of%20Segment%20Anything%20Model%3A%20A%20Survey&entry.906535625=Xiaorui%20Sun%20and%20Jun%20Liu%20and%20Heng%20Tao%20Shen%20and%20Xiaofeng%20Zhu%20and%20Ping%20Hu&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20foundational%20model%20for%20image%0Asegmentation%20tasks%2C%20known%20for%20its%20strong%20generalization%20across%20diverse%0Aapplications.%20However%2C%20its%20impressive%20performance%20comes%20with%20significant%0Acomputational%20and%20resource%20demands%2C%20making%20it%20challenging%20to%20deploy%20in%0Aresource-limited%20environments%20such%20as%20edge%20devices.%20To%20address%20this%2C%20a%20variety%0Aof%20SAM%20variants%20have%20been%20proposed%20to%20enhance%20efficiency%20while%20keeping%0Aaccuracy.%20This%20survey%20provides%20the%20first%20comprehensive%20review%20of%20these%0Aefficient%20SAM%20variants.%20We%20begin%20by%20exploring%20the%20motivations%20driving%20this%0Aresearch.%20We%20then%20present%20core%20techniques%20used%20in%20SAM%20and%20model%20acceleration.%0AThis%20is%20followed%20by%20a%20detailed%20exploration%20of%20SAM%20acceleration%20strategies%2C%0Acategorized%20by%20approach%2C%20and%20a%20discussion%20of%20several%20future%20research%0Adirections.%20Finally%2C%20we%20offer%20a%20unified%20and%20extensive%20evaluation%20of%20these%0Amethods%20across%20various%20hardware%2C%20assessing%20their%20efficiency%20and%20accuracy%20on%0Arepresentative%20benchmarks%2C%20and%20providing%20a%20clear%20comparison%20of%20their%20overall%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04960v2&entry.124074799=Read"},
{"title": "SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space\n  Trajectory Similarity", "author": "Chuang Yang and Renhe Jiang and Xiaohang Xu and Chuan Xiao and Kaoru Sezaki", "abstract": "  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and\nFrechet, often incur quadratic time complexity, thus learning-based methods\nhave been proposed to accelerate the computation. The core idea is to train an\nencoder to transform trajectories into representation vectors and then compute\nvector similarity to approximate the ground truth. However, existing methods\nface dual challenges of effectiveness and efficiency: 1) they all utilize\nEuclidean distance to compute representation similarity, which leads to the\nsevere curse of dimensionality issue -- reducing the distinguishability among\nrepresentations and significantly affecting the accuracy of subsequent\nsimilarity search tasks; 2) most of them are trained in triplets manner and\noften necessitate additional information which downgrades the efficiency; 3)\nprevious studies, while emphasizing the scalability in terms of efficiency,\noverlooked the deterioration of effectiveness when the dataset size grows. To\ncope with these issues, we propose a simple, yet accurate, fast, scalable model\nthat only uses a single-layer vanilla transformer encoder as the feature\nextractor and employs tailored representation similarity functions to\napproximate various ground truth similarity measures. Extensive experiments\ndemonstrate our model significantly mitigates the curse of dimensionality issue\nand outperforms the state-of-the-arts in effectiveness, efficiency, and\nscalability.\n", "link": "http://arxiv.org/abs/2410.14629v1", "date": "2024-10-18", "relevancy": 2.097, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIMformer%3A%20Single-Layer%20Vanilla%20Transformer%20Can%20Learn%20Free-Space%0A%20%20Trajectory%20Similarity&body=Title%3A%20SIMformer%3A%20Single-Layer%20Vanilla%20Transformer%20Can%20Learn%20Free-Space%0A%20%20Trajectory%20Similarity%0AAuthor%3A%20Chuang%20Yang%20and%20Renhe%20Jiang%20and%20Xiaohang%20Xu%20and%20Chuan%20Xiao%20and%20Kaoru%20Sezaki%0AAbstract%3A%20%20%20Free-space%20trajectory%20similarity%20calculation%2C%20e.g.%2C%20DTW%2C%20Hausdorff%2C%20and%0AFrechet%2C%20often%20incur%20quadratic%20time%20complexity%2C%20thus%20learning-based%20methods%0Ahave%20been%20proposed%20to%20accelerate%20the%20computation.%20The%20core%20idea%20is%20to%20train%20an%0Aencoder%20to%20transform%20trajectories%20into%20representation%20vectors%20and%20then%20compute%0Avector%20similarity%20to%20approximate%20the%20ground%20truth.%20However%2C%20existing%20methods%0Aface%20dual%20challenges%20of%20effectiveness%20and%20efficiency%3A%201%29%20they%20all%20utilize%0AEuclidean%20distance%20to%20compute%20representation%20similarity%2C%20which%20leads%20to%20the%0Asevere%20curse%20of%20dimensionality%20issue%20--%20reducing%20the%20distinguishability%20among%0Arepresentations%20and%20significantly%20affecting%20the%20accuracy%20of%20subsequent%0Asimilarity%20search%20tasks%3B%202%29%20most%20of%20them%20are%20trained%20in%20triplets%20manner%20and%0Aoften%20necessitate%20additional%20information%20which%20downgrades%20the%20efficiency%3B%203%29%0Aprevious%20studies%2C%20while%20emphasizing%20the%20scalability%20in%20terms%20of%20efficiency%2C%0Aoverlooked%20the%20deterioration%20of%20effectiveness%20when%20the%20dataset%20size%20grows.%20To%0Acope%20with%20these%20issues%2C%20we%20propose%20a%20simple%2C%20yet%20accurate%2C%20fast%2C%20scalable%20model%0Athat%20only%20uses%20a%20single-layer%20vanilla%20transformer%20encoder%20as%20the%20feature%0Aextractor%20and%20employs%20tailored%20representation%20similarity%20functions%20to%0Aapproximate%20various%20ground%20truth%20similarity%20measures.%20Extensive%20experiments%0Ademonstrate%20our%20model%20significantly%20mitigates%20the%20curse%20of%20dimensionality%20issue%0Aand%20outperforms%20the%20state-of-the-arts%20in%20effectiveness%2C%20efficiency%2C%20and%0Ascalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIMformer%253A%2520Single-Layer%2520Vanilla%2520Transformer%2520Can%2520Learn%2520Free-Space%250A%2520%2520Trajectory%2520Similarity%26entry.906535625%3DChuang%2520Yang%2520and%2520Renhe%2520Jiang%2520and%2520Xiaohang%2520Xu%2520and%2520Chuan%2520Xiao%2520and%2520Kaoru%2520Sezaki%26entry.1292438233%3D%2520%2520Free-space%2520trajectory%2520similarity%2520calculation%252C%2520e.g.%252C%2520DTW%252C%2520Hausdorff%252C%2520and%250AFrechet%252C%2520often%2520incur%2520quadratic%2520time%2520complexity%252C%2520thus%2520learning-based%2520methods%250Ahave%2520been%2520proposed%2520to%2520accelerate%2520the%2520computation.%2520The%2520core%2520idea%2520is%2520to%2520train%2520an%250Aencoder%2520to%2520transform%2520trajectories%2520into%2520representation%2520vectors%2520and%2520then%2520compute%250Avector%2520similarity%2520to%2520approximate%2520the%2520ground%2520truth.%2520However%252C%2520existing%2520methods%250Aface%2520dual%2520challenges%2520of%2520effectiveness%2520and%2520efficiency%253A%25201%2529%2520they%2520all%2520utilize%250AEuclidean%2520distance%2520to%2520compute%2520representation%2520similarity%252C%2520which%2520leads%2520to%2520the%250Asevere%2520curse%2520of%2520dimensionality%2520issue%2520--%2520reducing%2520the%2520distinguishability%2520among%250Arepresentations%2520and%2520significantly%2520affecting%2520the%2520accuracy%2520of%2520subsequent%250Asimilarity%2520search%2520tasks%253B%25202%2529%2520most%2520of%2520them%2520are%2520trained%2520in%2520triplets%2520manner%2520and%250Aoften%2520necessitate%2520additional%2520information%2520which%2520downgrades%2520the%2520efficiency%253B%25203%2529%250Aprevious%2520studies%252C%2520while%2520emphasizing%2520the%2520scalability%2520in%2520terms%2520of%2520efficiency%252C%250Aoverlooked%2520the%2520deterioration%2520of%2520effectiveness%2520when%2520the%2520dataset%2520size%2520grows.%2520To%250Acope%2520with%2520these%2520issues%252C%2520we%2520propose%2520a%2520simple%252C%2520yet%2520accurate%252C%2520fast%252C%2520scalable%2520model%250Athat%2520only%2520uses%2520a%2520single-layer%2520vanilla%2520transformer%2520encoder%2520as%2520the%2520feature%250Aextractor%2520and%2520employs%2520tailored%2520representation%2520similarity%2520functions%2520to%250Aapproximate%2520various%2520ground%2520truth%2520similarity%2520measures.%2520Extensive%2520experiments%250Ademonstrate%2520our%2520model%2520significantly%2520mitigates%2520the%2520curse%2520of%2520dimensionality%2520issue%250Aand%2520outperforms%2520the%2520state-of-the-arts%2520in%2520effectiveness%252C%2520efficiency%252C%2520and%250Ascalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIMformer%3A%20Single-Layer%20Vanilla%20Transformer%20Can%20Learn%20Free-Space%0A%20%20Trajectory%20Similarity&entry.906535625=Chuang%20Yang%20and%20Renhe%20Jiang%20and%20Xiaohang%20Xu%20and%20Chuan%20Xiao%20and%20Kaoru%20Sezaki&entry.1292438233=%20%20Free-space%20trajectory%20similarity%20calculation%2C%20e.g.%2C%20DTW%2C%20Hausdorff%2C%20and%0AFrechet%2C%20often%20incur%20quadratic%20time%20complexity%2C%20thus%20learning-based%20methods%0Ahave%20been%20proposed%20to%20accelerate%20the%20computation.%20The%20core%20idea%20is%20to%20train%20an%0Aencoder%20to%20transform%20trajectories%20into%20representation%20vectors%20and%20then%20compute%0Avector%20similarity%20to%20approximate%20the%20ground%20truth.%20However%2C%20existing%20methods%0Aface%20dual%20challenges%20of%20effectiveness%20and%20efficiency%3A%201%29%20they%20all%20utilize%0AEuclidean%20distance%20to%20compute%20representation%20similarity%2C%20which%20leads%20to%20the%0Asevere%20curse%20of%20dimensionality%20issue%20--%20reducing%20the%20distinguishability%20among%0Arepresentations%20and%20significantly%20affecting%20the%20accuracy%20of%20subsequent%0Asimilarity%20search%20tasks%3B%202%29%20most%20of%20them%20are%20trained%20in%20triplets%20manner%20and%0Aoften%20necessitate%20additional%20information%20which%20downgrades%20the%20efficiency%3B%203%29%0Aprevious%20studies%2C%20while%20emphasizing%20the%20scalability%20in%20terms%20of%20efficiency%2C%0Aoverlooked%20the%20deterioration%20of%20effectiveness%20when%20the%20dataset%20size%20grows.%20To%0Acope%20with%20these%20issues%2C%20we%20propose%20a%20simple%2C%20yet%20accurate%2C%20fast%2C%20scalable%20model%0Athat%20only%20uses%20a%20single-layer%20vanilla%20transformer%20encoder%20as%20the%20feature%0Aextractor%20and%20employs%20tailored%20representation%20similarity%20functions%20to%0Aapproximate%20various%20ground%20truth%20similarity%20measures.%20Extensive%20experiments%0Ademonstrate%20our%20model%20significantly%20mitigates%20the%20curse%20of%20dimensionality%20issue%0Aand%20outperforms%20the%20state-of-the-arts%20in%20effectiveness%2C%20efficiency%2C%20and%0Ascalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14629v1&entry.124074799=Read"},
{"title": "Do LLMs estimate uncertainty well in instruction-following?", "author": "Juyeon Heo and Miao Xiong and Christina Heinze-Deml and Jaya Narain", "abstract": "  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n", "link": "http://arxiv.org/abs/2410.14582v1", "date": "2024-10-18", "relevancy": 2.0834, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6023}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F&body=Title%3A%20Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F%0AAuthor%3A%20Juyeon%20Heo%20and%20Miao%20Xiong%20and%20Christina%20Heinze-Deml%20and%20Jaya%20Narain%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20could%20be%20valuable%20personal%20AI%20agents%20across%0Avarious%20domains%2C%20provided%20they%20can%20precisely%20follow%20user%20instructions.%20However%2C%0Arecent%20studies%20have%20shown%20significant%20limitations%20in%20LLMs%27%0Ainstruction-following%20capabilities%2C%20raising%20concerns%20about%20their%20reliability%20in%0Ahigh-stakes%20applications.%20Accurately%20estimating%20LLMs%27%20uncertainty%20in%20adhering%0Ato%20instructions%20is%20critical%20to%20mitigating%20deployment%20risks.%20We%20present%2C%20to%20our%0Aknowledge%2C%20the%20first%20systematic%20evaluation%20of%20the%20uncertainty%20estimation%0Aabilities%20of%20LLMs%20in%20the%20context%20of%20instruction-following.%20Our%20study%20identifies%0Akey%20challenges%20with%20existing%20instruction-following%20benchmarks%2C%20where%20multiple%0Afactors%20are%20entangled%20with%20uncertainty%20stems%20from%20instruction-following%2C%0Acomplicating%20the%20isolation%20and%20comparison%20across%20methods%20and%20models.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20controlled%20evaluation%20setup%20with%20two%20benchmark%0Aversions%20of%20data%2C%20enabling%20a%20comprehensive%20comparison%20of%20uncertainty%20estimation%0Amethods%20under%20various%20conditions.%20Our%20findings%20show%20that%20existing%20uncertainty%0Amethods%20struggle%2C%20particularly%20when%20models%20make%20subtle%20errors%20in%20instruction%0Afollowing.%20While%20internal%20model%20states%20provide%20some%20improvement%2C%20they%20remain%0Ainadequate%20in%20more%20complex%20scenarios.%20The%20insights%20from%20our%20controlled%0Aevaluation%20setups%20provide%20a%20crucial%20understanding%20of%20LLMs%27%20limitations%20and%0Apotential%20for%20uncertainty%20estimation%20in%20instruction-following%20tasks%2C%20paving%20the%0Away%20for%20more%20trustworthy%20AI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520estimate%2520uncertainty%2520well%2520in%2520instruction-following%253F%26entry.906535625%3DJuyeon%2520Heo%2520and%2520Miao%2520Xiong%2520and%2520Christina%2520Heinze-Deml%2520and%2520Jaya%2520Narain%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520could%2520be%2520valuable%2520personal%2520AI%2520agents%2520across%250Avarious%2520domains%252C%2520provided%2520they%2520can%2520precisely%2520follow%2520user%2520instructions.%2520However%252C%250Arecent%2520studies%2520have%2520shown%2520significant%2520limitations%2520in%2520LLMs%2527%250Ainstruction-following%2520capabilities%252C%2520raising%2520concerns%2520about%2520their%2520reliability%2520in%250Ahigh-stakes%2520applications.%2520Accurately%2520estimating%2520LLMs%2527%2520uncertainty%2520in%2520adhering%250Ato%2520instructions%2520is%2520critical%2520to%2520mitigating%2520deployment%2520risks.%2520We%2520present%252C%2520to%2520our%250Aknowledge%252C%2520the%2520first%2520systematic%2520evaluation%2520of%2520the%2520uncertainty%2520estimation%250Aabilities%2520of%2520LLMs%2520in%2520the%2520context%2520of%2520instruction-following.%2520Our%2520study%2520identifies%250Akey%2520challenges%2520with%2520existing%2520instruction-following%2520benchmarks%252C%2520where%2520multiple%250Afactors%2520are%2520entangled%2520with%2520uncertainty%2520stems%2520from%2520instruction-following%252C%250Acomplicating%2520the%2520isolation%2520and%2520comparison%2520across%2520methods%2520and%2520models.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520a%2520controlled%2520evaluation%2520setup%2520with%2520two%2520benchmark%250Aversions%2520of%2520data%252C%2520enabling%2520a%2520comprehensive%2520comparison%2520of%2520uncertainty%2520estimation%250Amethods%2520under%2520various%2520conditions.%2520Our%2520findings%2520show%2520that%2520existing%2520uncertainty%250Amethods%2520struggle%252C%2520particularly%2520when%2520models%2520make%2520subtle%2520errors%2520in%2520instruction%250Afollowing.%2520While%2520internal%2520model%2520states%2520provide%2520some%2520improvement%252C%2520they%2520remain%250Ainadequate%2520in%2520more%2520complex%2520scenarios.%2520The%2520insights%2520from%2520our%2520controlled%250Aevaluation%2520setups%2520provide%2520a%2520crucial%2520understanding%2520of%2520LLMs%2527%2520limitations%2520and%250Apotential%2520for%2520uncertainty%2520estimation%2520in%2520instruction-following%2520tasks%252C%2520paving%2520the%250Away%2520for%2520more%2520trustworthy%2520AI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20estimate%20uncertainty%20well%20in%20instruction-following%3F&entry.906535625=Juyeon%20Heo%20and%20Miao%20Xiong%20and%20Christina%20Heinze-Deml%20and%20Jaya%20Narain&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20could%20be%20valuable%20personal%20AI%20agents%20across%0Avarious%20domains%2C%20provided%20they%20can%20precisely%20follow%20user%20instructions.%20However%2C%0Arecent%20studies%20have%20shown%20significant%20limitations%20in%20LLMs%27%0Ainstruction-following%20capabilities%2C%20raising%20concerns%20about%20their%20reliability%20in%0Ahigh-stakes%20applications.%20Accurately%20estimating%20LLMs%27%20uncertainty%20in%20adhering%0Ato%20instructions%20is%20critical%20to%20mitigating%20deployment%20risks.%20We%20present%2C%20to%20our%0Aknowledge%2C%20the%20first%20systematic%20evaluation%20of%20the%20uncertainty%20estimation%0Aabilities%20of%20LLMs%20in%20the%20context%20of%20instruction-following.%20Our%20study%20identifies%0Akey%20challenges%20with%20existing%20instruction-following%20benchmarks%2C%20where%20multiple%0Afactors%20are%20entangled%20with%20uncertainty%20stems%20from%20instruction-following%2C%0Acomplicating%20the%20isolation%20and%20comparison%20across%20methods%20and%20models.%20To%20address%0Athese%20issues%2C%20we%20introduce%20a%20controlled%20evaluation%20setup%20with%20two%20benchmark%0Aversions%20of%20data%2C%20enabling%20a%20comprehensive%20comparison%20of%20uncertainty%20estimation%0Amethods%20under%20various%20conditions.%20Our%20findings%20show%20that%20existing%20uncertainty%0Amethods%20struggle%2C%20particularly%20when%20models%20make%20subtle%20errors%20in%20instruction%0Afollowing.%20While%20internal%20model%20states%20provide%20some%20improvement%2C%20they%20remain%0Ainadequate%20in%20more%20complex%20scenarios.%20The%20insights%20from%20our%20controlled%0Aevaluation%20setups%20provide%20a%20crucial%20understanding%20of%20LLMs%27%20limitations%20and%0Apotential%20for%20uncertainty%20estimation%20in%20instruction-following%20tasks%2C%20paving%20the%0Away%20for%20more%20trustworthy%20AI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14582v1&entry.124074799=Read"},
{"title": "On Debiasing Text Embeddings Through Context Injection", "author": "Thomas Uriot", "abstract": "  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n", "link": "http://arxiv.org/abs/2410.12874v2", "date": "2024-10-18", "relevancy": 2.0737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Debiasing%20Text%20Embeddings%20Through%20Context%20Injection&body=Title%3A%20On%20Debiasing%20Text%20Embeddings%20Through%20Context%20Injection%0AAuthor%3A%20Thomas%20Uriot%0AAbstract%3A%20%20%20Current%20advances%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20made%20it%0Aincreasingly%20feasible%20to%20build%20applications%20leveraging%20textual%20data.%20Generally%2C%0Athe%20core%20of%20these%20applications%20rely%20on%20having%20a%20good%20semantic%20representation%20of%0Atext%20into%20vectors%2C%20via%20embedding%20models.%20However%2C%20it%20has%20been%20shown%20that%20these%0Aembeddings%20capture%20and%20perpetuate%20biases%20already%20present%20in%20text.%20While%20a%20few%0Atechniques%20have%20been%20proposed%20to%20debias%20embeddings%2C%20they%20do%20not%20take%20advantage%0Aof%20the%20recent%20advances%20in%20context%20understanding%20of%20modern%20embedding%20models.%20In%0Athis%20paper%2C%20we%20fill%20this%20gap%20by%20conducting%20a%20review%20of%2019%20embedding%20models%20by%0Aquantifying%20their%20biases%20and%20how%20well%20they%20respond%20to%20context%20injection%20as%20a%0Amean%20of%20debiasing.%20We%20show%20that%20higher%20performing%20models%20are%20more%20prone%20to%0Acapturing%20biases%2C%20but%20are%20also%20better%20at%20incorporating%20context.%20Surprisingly%2C%0Awe%20find%20that%20while%20models%20can%20easily%20embed%20affirmative%20semantics%2C%20they%20fail%20at%0Aembedding%20neutral%20semantics.%20Finally%2C%20in%20a%20retrieval%20task%2C%20we%20show%20that%20biases%0Ain%20embeddings%20can%20lead%20to%20non-desirable%20outcomes.%20We%20use%20our%20new-found%20insights%0Ato%20design%20a%20simple%20algorithm%20for%20top%20%24k%24%20retrieval%2C%20where%20%24k%24%20is%20dynamically%0Aselected.%20We%20show%20that%20our%20algorithm%20is%20able%20to%20retrieve%20all%20relevant%20gendered%0Aand%20neutral%20chunks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Debiasing%2520Text%2520Embeddings%2520Through%2520Context%2520Injection%26entry.906535625%3DThomas%2520Uriot%26entry.1292438233%3D%2520%2520Current%2520advances%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520have%2520made%2520it%250Aincreasingly%2520feasible%2520to%2520build%2520applications%2520leveraging%2520textual%2520data.%2520Generally%252C%250Athe%2520core%2520of%2520these%2520applications%2520rely%2520on%2520having%2520a%2520good%2520semantic%2520representation%2520of%250Atext%2520into%2520vectors%252C%2520via%2520embedding%2520models.%2520However%252C%2520it%2520has%2520been%2520shown%2520that%2520these%250Aembeddings%2520capture%2520and%2520perpetuate%2520biases%2520already%2520present%2520in%2520text.%2520While%2520a%2520few%250Atechniques%2520have%2520been%2520proposed%2520to%2520debias%2520embeddings%252C%2520they%2520do%2520not%2520take%2520advantage%250Aof%2520the%2520recent%2520advances%2520in%2520context%2520understanding%2520of%2520modern%2520embedding%2520models.%2520In%250Athis%2520paper%252C%2520we%2520fill%2520this%2520gap%2520by%2520conducting%2520a%2520review%2520of%252019%2520embedding%2520models%2520by%250Aquantifying%2520their%2520biases%2520and%2520how%2520well%2520they%2520respond%2520to%2520context%2520injection%2520as%2520a%250Amean%2520of%2520debiasing.%2520We%2520show%2520that%2520higher%2520performing%2520models%2520are%2520more%2520prone%2520to%250Acapturing%2520biases%252C%2520but%2520are%2520also%2520better%2520at%2520incorporating%2520context.%2520Surprisingly%252C%250Awe%2520find%2520that%2520while%2520models%2520can%2520easily%2520embed%2520affirmative%2520semantics%252C%2520they%2520fail%2520at%250Aembedding%2520neutral%2520semantics.%2520Finally%252C%2520in%2520a%2520retrieval%2520task%252C%2520we%2520show%2520that%2520biases%250Ain%2520embeddings%2520can%2520lead%2520to%2520non-desirable%2520outcomes.%2520We%2520use%2520our%2520new-found%2520insights%250Ato%2520design%2520a%2520simple%2520algorithm%2520for%2520top%2520%2524k%2524%2520retrieval%252C%2520where%2520%2524k%2524%2520is%2520dynamically%250Aselected.%2520We%2520show%2520that%2520our%2520algorithm%2520is%2520able%2520to%2520retrieve%2520all%2520relevant%2520gendered%250Aand%2520neutral%2520chunks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Debiasing%20Text%20Embeddings%20Through%20Context%20Injection&entry.906535625=Thomas%20Uriot&entry.1292438233=%20%20Current%20advances%20in%20Natural%20Language%20Processing%20%28NLP%29%20have%20made%20it%0Aincreasingly%20feasible%20to%20build%20applications%20leveraging%20textual%20data.%20Generally%2C%0Athe%20core%20of%20these%20applications%20rely%20on%20having%20a%20good%20semantic%20representation%20of%0Atext%20into%20vectors%2C%20via%20embedding%20models.%20However%2C%20it%20has%20been%20shown%20that%20these%0Aembeddings%20capture%20and%20perpetuate%20biases%20already%20present%20in%20text.%20While%20a%20few%0Atechniques%20have%20been%20proposed%20to%20debias%20embeddings%2C%20they%20do%20not%20take%20advantage%0Aof%20the%20recent%20advances%20in%20context%20understanding%20of%20modern%20embedding%20models.%20In%0Athis%20paper%2C%20we%20fill%20this%20gap%20by%20conducting%20a%20review%20of%2019%20embedding%20models%20by%0Aquantifying%20their%20biases%20and%20how%20well%20they%20respond%20to%20context%20injection%20as%20a%0Amean%20of%20debiasing.%20We%20show%20that%20higher%20performing%20models%20are%20more%20prone%20to%0Acapturing%20biases%2C%20but%20are%20also%20better%20at%20incorporating%20context.%20Surprisingly%2C%0Awe%20find%20that%20while%20models%20can%20easily%20embed%20affirmative%20semantics%2C%20they%20fail%20at%0Aembedding%20neutral%20semantics.%20Finally%2C%20in%20a%20retrieval%20task%2C%20we%20show%20that%20biases%0Ain%20embeddings%20can%20lead%20to%20non-desirable%20outcomes.%20We%20use%20our%20new-found%20insights%0Ato%20design%20a%20simple%20algorithm%20for%20top%20%24k%24%20retrieval%2C%20where%20%24k%24%20is%20dynamically%0Aselected.%20We%20show%20that%20our%20algorithm%20is%20able%20to%20retrieve%20all%20relevant%20gendered%0Aand%20neutral%20chunks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12874v2&entry.124074799=Read"},
{"title": "How Does Data Diversity Shape the Weight Landscape of Neural Networks?", "author": "Yang Ba and Michelle V. Mancenido and Rong Pan", "abstract": "  To enhance the generalization of machine learning models to unseen data,\ntechniques such as dropout, weight decay ($L_2$ regularization), and noise\naugmentation are commonly employed. While regularization methods (i.e., dropout\nand weight decay) are geared toward adjusting model parameters to prevent\noverfitting, data augmentation increases the diversity of the input training\nset, a method purported to improve accuracy and calibration error. In this\npaper, we investigate the impact of each of these techniques on the parameter\nspace of neural networks, with the goal of understanding how they alter the\nweight landscape in transfer learning scenarios. To accomplish this, we employ\nRandom Matrix Theory to analyze the eigenvalue distributions of pre-trained\nmodels, fine-tuned using these techniques but using different levels of data\ndiversity, for the same downstream tasks. We observe that diverse data\ninfluences the weight landscape in a similar fashion as dropout. Additionally,\nwe compare commonly used data augmentation methods with synthetic data created\nby generative models. We conclude that synthetic data can bring more diversity\ninto real input data, resulting in a better performance on out-of-distribution\ntest instances.\n", "link": "http://arxiv.org/abs/2410.14602v1", "date": "2024-10-18", "relevancy": 2.0712, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5387}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5174}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Does%20Data%20Diversity%20Shape%20the%20Weight%20Landscape%20of%20Neural%20Networks%3F&body=Title%3A%20How%20Does%20Data%20Diversity%20Shape%20the%20Weight%20Landscape%20of%20Neural%20Networks%3F%0AAuthor%3A%20Yang%20Ba%20and%20Michelle%20V.%20Mancenido%20and%20Rong%20Pan%0AAbstract%3A%20%20%20To%20enhance%20the%20generalization%20of%20machine%20learning%20models%20to%20unseen%20data%2C%0Atechniques%20such%20as%20dropout%2C%20weight%20decay%20%28%24L_2%24%20regularization%29%2C%20and%20noise%0Aaugmentation%20are%20commonly%20employed.%20While%20regularization%20methods%20%28i.e.%2C%20dropout%0Aand%20weight%20decay%29%20are%20geared%20toward%20adjusting%20model%20parameters%20to%20prevent%0Aoverfitting%2C%20data%20augmentation%20increases%20the%20diversity%20of%20the%20input%20training%0Aset%2C%20a%20method%20purported%20to%20improve%20accuracy%20and%20calibration%20error.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20each%20of%20these%20techniques%20on%20the%20parameter%0Aspace%20of%20neural%20networks%2C%20with%20the%20goal%20of%20understanding%20how%20they%20alter%20the%0Aweight%20landscape%20in%20transfer%20learning%20scenarios.%20To%20accomplish%20this%2C%20we%20employ%0ARandom%20Matrix%20Theory%20to%20analyze%20the%20eigenvalue%20distributions%20of%20pre-trained%0Amodels%2C%20fine-tuned%20using%20these%20techniques%20but%20using%20different%20levels%20of%20data%0Adiversity%2C%20for%20the%20same%20downstream%20tasks.%20We%20observe%20that%20diverse%20data%0Ainfluences%20the%20weight%20landscape%20in%20a%20similar%20fashion%20as%20dropout.%20Additionally%2C%0Awe%20compare%20commonly%20used%20data%20augmentation%20methods%20with%20synthetic%20data%20created%0Aby%20generative%20models.%20We%20conclude%20that%20synthetic%20data%20can%20bring%20more%20diversity%0Ainto%20real%20input%20data%2C%20resulting%20in%20a%20better%20performance%20on%20out-of-distribution%0Atest%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Does%2520Data%2520Diversity%2520Shape%2520the%2520Weight%2520Landscape%2520of%2520Neural%2520Networks%253F%26entry.906535625%3DYang%2520Ba%2520and%2520Michelle%2520V.%2520Mancenido%2520and%2520Rong%2520Pan%26entry.1292438233%3D%2520%2520To%2520enhance%2520the%2520generalization%2520of%2520machine%2520learning%2520models%2520to%2520unseen%2520data%252C%250Atechniques%2520such%2520as%2520dropout%252C%2520weight%2520decay%2520%2528%2524L_2%2524%2520regularization%2529%252C%2520and%2520noise%250Aaugmentation%2520are%2520commonly%2520employed.%2520While%2520regularization%2520methods%2520%2528i.e.%252C%2520dropout%250Aand%2520weight%2520decay%2529%2520are%2520geared%2520toward%2520adjusting%2520model%2520parameters%2520to%2520prevent%250Aoverfitting%252C%2520data%2520augmentation%2520increases%2520the%2520diversity%2520of%2520the%2520input%2520training%250Aset%252C%2520a%2520method%2520purported%2520to%2520improve%2520accuracy%2520and%2520calibration%2520error.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520each%2520of%2520these%2520techniques%2520on%2520the%2520parameter%250Aspace%2520of%2520neural%2520networks%252C%2520with%2520the%2520goal%2520of%2520understanding%2520how%2520they%2520alter%2520the%250Aweight%2520landscape%2520in%2520transfer%2520learning%2520scenarios.%2520To%2520accomplish%2520this%252C%2520we%2520employ%250ARandom%2520Matrix%2520Theory%2520to%2520analyze%2520the%2520eigenvalue%2520distributions%2520of%2520pre-trained%250Amodels%252C%2520fine-tuned%2520using%2520these%2520techniques%2520but%2520using%2520different%2520levels%2520of%2520data%250Adiversity%252C%2520for%2520the%2520same%2520downstream%2520tasks.%2520We%2520observe%2520that%2520diverse%2520data%250Ainfluences%2520the%2520weight%2520landscape%2520in%2520a%2520similar%2520fashion%2520as%2520dropout.%2520Additionally%252C%250Awe%2520compare%2520commonly%2520used%2520data%2520augmentation%2520methods%2520with%2520synthetic%2520data%2520created%250Aby%2520generative%2520models.%2520We%2520conclude%2520that%2520synthetic%2520data%2520can%2520bring%2520more%2520diversity%250Ainto%2520real%2520input%2520data%252C%2520resulting%2520in%2520a%2520better%2520performance%2520on%2520out-of-distribution%250Atest%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Does%20Data%20Diversity%20Shape%20the%20Weight%20Landscape%20of%20Neural%20Networks%3F&entry.906535625=Yang%20Ba%20and%20Michelle%20V.%20Mancenido%20and%20Rong%20Pan&entry.1292438233=%20%20To%20enhance%20the%20generalization%20of%20machine%20learning%20models%20to%20unseen%20data%2C%0Atechniques%20such%20as%20dropout%2C%20weight%20decay%20%28%24L_2%24%20regularization%29%2C%20and%20noise%0Aaugmentation%20are%20commonly%20employed.%20While%20regularization%20methods%20%28i.e.%2C%20dropout%0Aand%20weight%20decay%29%20are%20geared%20toward%20adjusting%20model%20parameters%20to%20prevent%0Aoverfitting%2C%20data%20augmentation%20increases%20the%20diversity%20of%20the%20input%20training%0Aset%2C%20a%20method%20purported%20to%20improve%20accuracy%20and%20calibration%20error.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20each%20of%20these%20techniques%20on%20the%20parameter%0Aspace%20of%20neural%20networks%2C%20with%20the%20goal%20of%20understanding%20how%20they%20alter%20the%0Aweight%20landscape%20in%20transfer%20learning%20scenarios.%20To%20accomplish%20this%2C%20we%20employ%0ARandom%20Matrix%20Theory%20to%20analyze%20the%20eigenvalue%20distributions%20of%20pre-trained%0Amodels%2C%20fine-tuned%20using%20these%20techniques%20but%20using%20different%20levels%20of%20data%0Adiversity%2C%20for%20the%20same%20downstream%20tasks.%20We%20observe%20that%20diverse%20data%0Ainfluences%20the%20weight%20landscape%20in%20a%20similar%20fashion%20as%20dropout.%20Additionally%2C%0Awe%20compare%20commonly%20used%20data%20augmentation%20methods%20with%20synthetic%20data%20created%0Aby%20generative%20models.%20We%20conclude%20that%20synthetic%20data%20can%20bring%20more%20diversity%0Ainto%20real%20input%20data%2C%20resulting%20in%20a%20better%20performance%20on%20out-of-distribution%0Atest%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14602v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor\n  Environments", "author": "Mariusz Wisniewski and Paraskevas Chatzithanos and Weisi Guo and Antonios Tsourdos", "abstract": "  Deep Reinforcement learning (DRL) is used to enable autonomous navigation in\nunknown environments. Most research assume perfect sensor data, but real-world\nenvironments may contain natural and artificial sensor noise and denial. Here,\nwe present a benchmark of both well-used and emerging DRL algorithms in a\nnavigation task with configurable sensor denial effects. In particular, we are\ninterested in comparing how different DRL methods (e.g. model-free PPO vs.\nmodel-based DreamerV3) are affected by sensor denial. We show that DreamerV3\noutperforms other methods in the visual end-to-end navigation task with a\ndynamic goal - and other methods are not able to learn this. Furthermore,\nDreamerV3 generally outperforms other methods in sensor-denied environments. In\norder to improve robustness, we use adversarial training and demonstrate an\nimproved performance in denied environments, although this generally comes with\na performance cost on the vanilla environments. We anticipate this benchmark of\ndifferent DRL methods and the usage of adversarial training to be a starting\npoint for the development of more elaborate navigation strategies that are\ncapable of dealing with uncertain and denied sensor readings.\n", "link": "http://arxiv.org/abs/2410.14616v1", "date": "2024-10-18", "relevancy": 2.0605, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Reinforcement%20Learning%20for%20Navigation%20in%20Denied%20Sensor%0A%20%20Environments&body=Title%3A%20Benchmarking%20Deep%20Reinforcement%20Learning%20for%20Navigation%20in%20Denied%20Sensor%0A%20%20Environments%0AAuthor%3A%20Mariusz%20Wisniewski%20and%20Paraskevas%20Chatzithanos%20and%20Weisi%20Guo%20and%20Antonios%20Tsourdos%0AAbstract%3A%20%20%20Deep%20Reinforcement%20learning%20%28DRL%29%20is%20used%20to%20enable%20autonomous%20navigation%20in%0Aunknown%20environments.%20Most%20research%20assume%20perfect%20sensor%20data%2C%20but%20real-world%0Aenvironments%20may%20contain%20natural%20and%20artificial%20sensor%20noise%20and%20denial.%20Here%2C%0Awe%20present%20a%20benchmark%20of%20both%20well-used%20and%20emerging%20DRL%20algorithms%20in%20a%0Anavigation%20task%20with%20configurable%20sensor%20denial%20effects.%20In%20particular%2C%20we%20are%0Ainterested%20in%20comparing%20how%20different%20DRL%20methods%20%28e.g.%20model-free%20PPO%20vs.%0Amodel-based%20DreamerV3%29%20are%20affected%20by%20sensor%20denial.%20We%20show%20that%20DreamerV3%0Aoutperforms%20other%20methods%20in%20the%20visual%20end-to-end%20navigation%20task%20with%20a%0Adynamic%20goal%20-%20and%20other%20methods%20are%20not%20able%20to%20learn%20this.%20Furthermore%2C%0ADreamerV3%20generally%20outperforms%20other%20methods%20in%20sensor-denied%20environments.%20In%0Aorder%20to%20improve%20robustness%2C%20we%20use%20adversarial%20training%20and%20demonstrate%20an%0Aimproved%20performance%20in%20denied%20environments%2C%20although%20this%20generally%20comes%20with%0Aa%20performance%20cost%20on%20the%20vanilla%20environments.%20We%20anticipate%20this%20benchmark%20of%0Adifferent%20DRL%20methods%20and%20the%20usage%20of%20adversarial%20training%20to%20be%20a%20starting%0Apoint%20for%20the%20development%20of%20more%20elaborate%20navigation%20strategies%20that%20are%0Acapable%20of%20dealing%20with%20uncertain%20and%20denied%20sensor%20readings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Deep%2520Reinforcement%2520Learning%2520for%2520Navigation%2520in%2520Denied%2520Sensor%250A%2520%2520Environments%26entry.906535625%3DMariusz%2520Wisniewski%2520and%2520Paraskevas%2520Chatzithanos%2520and%2520Weisi%2520Guo%2520and%2520Antonios%2520Tsourdos%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520learning%2520%2528DRL%2529%2520is%2520used%2520to%2520enable%2520autonomous%2520navigation%2520in%250Aunknown%2520environments.%2520Most%2520research%2520assume%2520perfect%2520sensor%2520data%252C%2520but%2520real-world%250Aenvironments%2520may%2520contain%2520natural%2520and%2520artificial%2520sensor%2520noise%2520and%2520denial.%2520Here%252C%250Awe%2520present%2520a%2520benchmark%2520of%2520both%2520well-used%2520and%2520emerging%2520DRL%2520algorithms%2520in%2520a%250Anavigation%2520task%2520with%2520configurable%2520sensor%2520denial%2520effects.%2520In%2520particular%252C%2520we%2520are%250Ainterested%2520in%2520comparing%2520how%2520different%2520DRL%2520methods%2520%2528e.g.%2520model-free%2520PPO%2520vs.%250Amodel-based%2520DreamerV3%2529%2520are%2520affected%2520by%2520sensor%2520denial.%2520We%2520show%2520that%2520DreamerV3%250Aoutperforms%2520other%2520methods%2520in%2520the%2520visual%2520end-to-end%2520navigation%2520task%2520with%2520a%250Adynamic%2520goal%2520-%2520and%2520other%2520methods%2520are%2520not%2520able%2520to%2520learn%2520this.%2520Furthermore%252C%250ADreamerV3%2520generally%2520outperforms%2520other%2520methods%2520in%2520sensor-denied%2520environments.%2520In%250Aorder%2520to%2520improve%2520robustness%252C%2520we%2520use%2520adversarial%2520training%2520and%2520demonstrate%2520an%250Aimproved%2520performance%2520in%2520denied%2520environments%252C%2520although%2520this%2520generally%2520comes%2520with%250Aa%2520performance%2520cost%2520on%2520the%2520vanilla%2520environments.%2520We%2520anticipate%2520this%2520benchmark%2520of%250Adifferent%2520DRL%2520methods%2520and%2520the%2520usage%2520of%2520adversarial%2520training%2520to%2520be%2520a%2520starting%250Apoint%2520for%2520the%2520development%2520of%2520more%2520elaborate%2520navigation%2520strategies%2520that%2520are%250Acapable%2520of%2520dealing%2520with%2520uncertain%2520and%2520denied%2520sensor%2520readings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Reinforcement%20Learning%20for%20Navigation%20in%20Denied%20Sensor%0A%20%20Environments&entry.906535625=Mariusz%20Wisniewski%20and%20Paraskevas%20Chatzithanos%20and%20Weisi%20Guo%20and%20Antonios%20Tsourdos&entry.1292438233=%20%20Deep%20Reinforcement%20learning%20%28DRL%29%20is%20used%20to%20enable%20autonomous%20navigation%20in%0Aunknown%20environments.%20Most%20research%20assume%20perfect%20sensor%20data%2C%20but%20real-world%0Aenvironments%20may%20contain%20natural%20and%20artificial%20sensor%20noise%20and%20denial.%20Here%2C%0Awe%20present%20a%20benchmark%20of%20both%20well-used%20and%20emerging%20DRL%20algorithms%20in%20a%0Anavigation%20task%20with%20configurable%20sensor%20denial%20effects.%20In%20particular%2C%20we%20are%0Ainterested%20in%20comparing%20how%20different%20DRL%20methods%20%28e.g.%20model-free%20PPO%20vs.%0Amodel-based%20DreamerV3%29%20are%20affected%20by%20sensor%20denial.%20We%20show%20that%20DreamerV3%0Aoutperforms%20other%20methods%20in%20the%20visual%20end-to-end%20navigation%20task%20with%20a%0Adynamic%20goal%20-%20and%20other%20methods%20are%20not%20able%20to%20learn%20this.%20Furthermore%2C%0ADreamerV3%20generally%20outperforms%20other%20methods%20in%20sensor-denied%20environments.%20In%0Aorder%20to%20improve%20robustness%2C%20we%20use%20adversarial%20training%20and%20demonstrate%20an%0Aimproved%20performance%20in%20denied%20environments%2C%20although%20this%20generally%20comes%20with%0Aa%20performance%20cost%20on%20the%20vanilla%20environments.%20We%20anticipate%20this%20benchmark%20of%0Adifferent%20DRL%20methods%20and%20the%20usage%20of%20adversarial%20training%20to%20be%20a%20starting%0Apoint%20for%20the%20development%20of%20more%20elaborate%20navigation%20strategies%20that%20are%0Acapable%20of%20dealing%20with%20uncertain%20and%20denied%20sensor%20readings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14616v1&entry.124074799=Read"},
{"title": "Reimagining partial thickness keratoplasty: An eye mountable robot for\n  autonomous big bubble needle insertion", "author": "Y. Wang and J. D. Opfermann and J. Yu and H. Yi and J. Kaluna and R. Biswas and R. Zuo and W. Gensheimer and A. Krieger and J. U. Kang", "abstract": "  Autonomous surgical robots have demonstrated significant potential to\nstandardize surgical outcomes, driving innovations that enhance safety and\nconsistency regardless of individual surgeon experience. Deep anterior lamellar\nkeratoplasty (DALK), a partial thickness corneal transplant surgery aimed at\nreplacing the anterior part of cornea above Descemet membrane (DM), would\ngreatly benefit from an autonomous surgical approach as it highly relies on\nsurgeon skill with high perforation rates. In this study, we proposed a novel\nautonomous surgical robotic system (AUTO-DALK) based on a customized neural\nnetwork capable of precise needle control and consistent big bubble demarcation\non cadaver and live rabbit models. We demonstrate the feasibility of an\nAI-based image-guided vertical drilling approach for big bubble generation, in\ncontrast to the conventional horizontal needle approach. Our system integrates\nan optical coherence tomography (OCT) fiber optic distal sensor into the\neye-mountable micro robotic system, which automatically segments OCT M-mode\ndepth signals to identify corneal layers using a custom deep learning\nalgorithm. It enables the robot to autonomously guide the needle to targeted\ntissue layers via a depth-controlled feedback loop. We compared autonomous\nneedle insertion performance and resulting pneumo-dissection using AUTO-DALK\nagainst 1) freehand insertion, 2) OCT sensor guided manual insertion, and 3)\nteleoperated robotic insertion, reporting significant improvements in insertion\ndepth, pneumo-dissection depth, task completion time, and big bubble formation.\nEx vivo and in vivo results indicate that the AI-driven, AUTO-DALK system, is a\npromising solution to standardize pneumo-dissection outcomes for partial\nthickness keratoplasty.\n", "link": "http://arxiv.org/abs/2410.14577v1", "date": "2024-10-18", "relevancy": 2.0492, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reimagining%20partial%20thickness%20keratoplasty%3A%20An%20eye%20mountable%20robot%20for%0A%20%20autonomous%20big%20bubble%20needle%20insertion&body=Title%3A%20Reimagining%20partial%20thickness%20keratoplasty%3A%20An%20eye%20mountable%20robot%20for%0A%20%20autonomous%20big%20bubble%20needle%20insertion%0AAuthor%3A%20Y.%20Wang%20and%20J.%20D.%20Opfermann%20and%20J.%20Yu%20and%20H.%20Yi%20and%20J.%20Kaluna%20and%20R.%20Biswas%20and%20R.%20Zuo%20and%20W.%20Gensheimer%20and%20A.%20Krieger%20and%20J.%20U.%20Kang%0AAbstract%3A%20%20%20Autonomous%20surgical%20robots%20have%20demonstrated%20significant%20potential%20to%0Astandardize%20surgical%20outcomes%2C%20driving%20innovations%20that%20enhance%20safety%20and%0Aconsistency%20regardless%20of%20individual%20surgeon%20experience.%20Deep%20anterior%20lamellar%0Akeratoplasty%20%28DALK%29%2C%20a%20partial%20thickness%20corneal%20transplant%20surgery%20aimed%20at%0Areplacing%20the%20anterior%20part%20of%20cornea%20above%20Descemet%20membrane%20%28DM%29%2C%20would%0Agreatly%20benefit%20from%20an%20autonomous%20surgical%20approach%20as%20it%20highly%20relies%20on%0Asurgeon%20skill%20with%20high%20perforation%20rates.%20In%20this%20study%2C%20we%20proposed%20a%20novel%0Aautonomous%20surgical%20robotic%20system%20%28AUTO-DALK%29%20based%20on%20a%20customized%20neural%0Anetwork%20capable%20of%20precise%20needle%20control%20and%20consistent%20big%20bubble%20demarcation%0Aon%20cadaver%20and%20live%20rabbit%20models.%20We%20demonstrate%20the%20feasibility%20of%20an%0AAI-based%20image-guided%20vertical%20drilling%20approach%20for%20big%20bubble%20generation%2C%20in%0Acontrast%20to%20the%20conventional%20horizontal%20needle%20approach.%20Our%20system%20integrates%0Aan%20optical%20coherence%20tomography%20%28OCT%29%20fiber%20optic%20distal%20sensor%20into%20the%0Aeye-mountable%20micro%20robotic%20system%2C%20which%20automatically%20segments%20OCT%20M-mode%0Adepth%20signals%20to%20identify%20corneal%20layers%20using%20a%20custom%20deep%20learning%0Aalgorithm.%20It%20enables%20the%20robot%20to%20autonomously%20guide%20the%20needle%20to%20targeted%0Atissue%20layers%20via%20a%20depth-controlled%20feedback%20loop.%20We%20compared%20autonomous%0Aneedle%20insertion%20performance%20and%20resulting%20pneumo-dissection%20using%20AUTO-DALK%0Aagainst%201%29%20freehand%20insertion%2C%202%29%20OCT%20sensor%20guided%20manual%20insertion%2C%20and%203%29%0Ateleoperated%20robotic%20insertion%2C%20reporting%20significant%20improvements%20in%20insertion%0Adepth%2C%20pneumo-dissection%20depth%2C%20task%20completion%20time%2C%20and%20big%20bubble%20formation.%0AEx%20vivo%20and%20in%20vivo%20results%20indicate%20that%20the%20AI-driven%2C%20AUTO-DALK%20system%2C%20is%20a%0Apromising%20solution%20to%20standardize%20pneumo-dissection%20outcomes%20for%20partial%0Athickness%20keratoplasty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReimagining%2520partial%2520thickness%2520keratoplasty%253A%2520An%2520eye%2520mountable%2520robot%2520for%250A%2520%2520autonomous%2520big%2520bubble%2520needle%2520insertion%26entry.906535625%3DY.%2520Wang%2520and%2520J.%2520D.%2520Opfermann%2520and%2520J.%2520Yu%2520and%2520H.%2520Yi%2520and%2520J.%2520Kaluna%2520and%2520R.%2520Biswas%2520and%2520R.%2520Zuo%2520and%2520W.%2520Gensheimer%2520and%2520A.%2520Krieger%2520and%2520J.%2520U.%2520Kang%26entry.1292438233%3D%2520%2520Autonomous%2520surgical%2520robots%2520have%2520demonstrated%2520significant%2520potential%2520to%250Astandardize%2520surgical%2520outcomes%252C%2520driving%2520innovations%2520that%2520enhance%2520safety%2520and%250Aconsistency%2520regardless%2520of%2520individual%2520surgeon%2520experience.%2520Deep%2520anterior%2520lamellar%250Akeratoplasty%2520%2528DALK%2529%252C%2520a%2520partial%2520thickness%2520corneal%2520transplant%2520surgery%2520aimed%2520at%250Areplacing%2520the%2520anterior%2520part%2520of%2520cornea%2520above%2520Descemet%2520membrane%2520%2528DM%2529%252C%2520would%250Agreatly%2520benefit%2520from%2520an%2520autonomous%2520surgical%2520approach%2520as%2520it%2520highly%2520relies%2520on%250Asurgeon%2520skill%2520with%2520high%2520perforation%2520rates.%2520In%2520this%2520study%252C%2520we%2520proposed%2520a%2520novel%250Aautonomous%2520surgical%2520robotic%2520system%2520%2528AUTO-DALK%2529%2520based%2520on%2520a%2520customized%2520neural%250Anetwork%2520capable%2520of%2520precise%2520needle%2520control%2520and%2520consistent%2520big%2520bubble%2520demarcation%250Aon%2520cadaver%2520and%2520live%2520rabbit%2520models.%2520We%2520demonstrate%2520the%2520feasibility%2520of%2520an%250AAI-based%2520image-guided%2520vertical%2520drilling%2520approach%2520for%2520big%2520bubble%2520generation%252C%2520in%250Acontrast%2520to%2520the%2520conventional%2520horizontal%2520needle%2520approach.%2520Our%2520system%2520integrates%250Aan%2520optical%2520coherence%2520tomography%2520%2528OCT%2529%2520fiber%2520optic%2520distal%2520sensor%2520into%2520the%250Aeye-mountable%2520micro%2520robotic%2520system%252C%2520which%2520automatically%2520segments%2520OCT%2520M-mode%250Adepth%2520signals%2520to%2520identify%2520corneal%2520layers%2520using%2520a%2520custom%2520deep%2520learning%250Aalgorithm.%2520It%2520enables%2520the%2520robot%2520to%2520autonomously%2520guide%2520the%2520needle%2520to%2520targeted%250Atissue%2520layers%2520via%2520a%2520depth-controlled%2520feedback%2520loop.%2520We%2520compared%2520autonomous%250Aneedle%2520insertion%2520performance%2520and%2520resulting%2520pneumo-dissection%2520using%2520AUTO-DALK%250Aagainst%25201%2529%2520freehand%2520insertion%252C%25202%2529%2520OCT%2520sensor%2520guided%2520manual%2520insertion%252C%2520and%25203%2529%250Ateleoperated%2520robotic%2520insertion%252C%2520reporting%2520significant%2520improvements%2520in%2520insertion%250Adepth%252C%2520pneumo-dissection%2520depth%252C%2520task%2520completion%2520time%252C%2520and%2520big%2520bubble%2520formation.%250AEx%2520vivo%2520and%2520in%2520vivo%2520results%2520indicate%2520that%2520the%2520AI-driven%252C%2520AUTO-DALK%2520system%252C%2520is%2520a%250Apromising%2520solution%2520to%2520standardize%2520pneumo-dissection%2520outcomes%2520for%2520partial%250Athickness%2520keratoplasty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reimagining%20partial%20thickness%20keratoplasty%3A%20An%20eye%20mountable%20robot%20for%0A%20%20autonomous%20big%20bubble%20needle%20insertion&entry.906535625=Y.%20Wang%20and%20J.%20D.%20Opfermann%20and%20J.%20Yu%20and%20H.%20Yi%20and%20J.%20Kaluna%20and%20R.%20Biswas%20and%20R.%20Zuo%20and%20W.%20Gensheimer%20and%20A.%20Krieger%20and%20J.%20U.%20Kang&entry.1292438233=%20%20Autonomous%20surgical%20robots%20have%20demonstrated%20significant%20potential%20to%0Astandardize%20surgical%20outcomes%2C%20driving%20innovations%20that%20enhance%20safety%20and%0Aconsistency%20regardless%20of%20individual%20surgeon%20experience.%20Deep%20anterior%20lamellar%0Akeratoplasty%20%28DALK%29%2C%20a%20partial%20thickness%20corneal%20transplant%20surgery%20aimed%20at%0Areplacing%20the%20anterior%20part%20of%20cornea%20above%20Descemet%20membrane%20%28DM%29%2C%20would%0Agreatly%20benefit%20from%20an%20autonomous%20surgical%20approach%20as%20it%20highly%20relies%20on%0Asurgeon%20skill%20with%20high%20perforation%20rates.%20In%20this%20study%2C%20we%20proposed%20a%20novel%0Aautonomous%20surgical%20robotic%20system%20%28AUTO-DALK%29%20based%20on%20a%20customized%20neural%0Anetwork%20capable%20of%20precise%20needle%20control%20and%20consistent%20big%20bubble%20demarcation%0Aon%20cadaver%20and%20live%20rabbit%20models.%20We%20demonstrate%20the%20feasibility%20of%20an%0AAI-based%20image-guided%20vertical%20drilling%20approach%20for%20big%20bubble%20generation%2C%20in%0Acontrast%20to%20the%20conventional%20horizontal%20needle%20approach.%20Our%20system%20integrates%0Aan%20optical%20coherence%20tomography%20%28OCT%29%20fiber%20optic%20distal%20sensor%20into%20the%0Aeye-mountable%20micro%20robotic%20system%2C%20which%20automatically%20segments%20OCT%20M-mode%0Adepth%20signals%20to%20identify%20corneal%20layers%20using%20a%20custom%20deep%20learning%0Aalgorithm.%20It%20enables%20the%20robot%20to%20autonomously%20guide%20the%20needle%20to%20targeted%0Atissue%20layers%20via%20a%20depth-controlled%20feedback%20loop.%20We%20compared%20autonomous%0Aneedle%20insertion%20performance%20and%20resulting%20pneumo-dissection%20using%20AUTO-DALK%0Aagainst%201%29%20freehand%20insertion%2C%202%29%20OCT%20sensor%20guided%20manual%20insertion%2C%20and%203%29%0Ateleoperated%20robotic%20insertion%2C%20reporting%20significant%20improvements%20in%20insertion%0Adepth%2C%20pneumo-dissection%20depth%2C%20task%20completion%20time%2C%20and%20big%20bubble%20formation.%0AEx%20vivo%20and%20in%20vivo%20results%20indicate%20that%20the%20AI-driven%2C%20AUTO-DALK%20system%2C%20is%20a%0Apromising%20solution%20to%20standardize%20pneumo-dissection%20outcomes%20for%20partial%0Athickness%20keratoplasty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14577v1&entry.124074799=Read"},
{"title": "DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph", "author": "Maitreya Prafulla Chitale and Uday Bindal and Rajakrishnan Rajkumar and Rahul Mishra", "abstract": "  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n", "link": "http://arxiv.org/abs/2410.14666v1", "date": "2024-10-18", "relevancy": 2.0369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5109}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiscoGraMS%3A%20Enhancing%20Movie%20Screen-Play%20Summarization%20using%20Movie%0A%20%20Character-Aware%20Discourse%20Graph&body=Title%3A%20DiscoGraMS%3A%20Enhancing%20Movie%20Screen-Play%20Summarization%20using%20Movie%0A%20%20Character-Aware%20Discourse%20Graph%0AAuthor%3A%20Maitreya%20Prafulla%20Chitale%20and%20Uday%20Bindal%20and%20Rajakrishnan%20Rajkumar%20and%20Rahul%20Mishra%0AAbstract%3A%20%20%20Summarizing%20movie%20screenplays%20presents%20a%20unique%20set%20of%20challenges%20compared%20to%0Astandard%20document%20summarization.%20Screenplays%20are%20not%20only%20lengthy%2C%20but%20also%0Afeature%20a%20complex%20interplay%20of%20characters%2C%20dialogues%2C%20and%20scenes%2C%20with%20numerous%0Adirect%20and%20subtle%20relationships%20and%20contextual%20nuances%20that%20are%20difficult%20for%0Amachine%20learning%20models%20to%20accurately%20capture%20and%20comprehend.%20Recent%20attempts%0Aat%20screenplay%20summarization%20focus%20on%20fine-tuning%20transformer-based%20pre-trained%0Amodels%2C%20but%20these%20models%20often%20fall%20short%20in%20capturing%20long-term%20dependencies%0Aand%20latent%20relationships%2C%20and%20frequently%20encounter%20the%20%22lost%20in%20the%20middle%22%0Aissue.%20To%20address%20these%20challenges%2C%20we%20introduce%20DiscoGraMS%2C%20a%20novel%20resource%0Athat%20represents%20movie%20scripts%20as%20a%20movie%20character-aware%20discourse%20graph%20%28CaD%0AGraph%29.%20This%20approach%20is%20well-suited%20for%20various%20downstream%20tasks%2C%20such%20as%0Asummarization%2C%20question-answering%2C%20and%20salience%20detection.%20The%20model%20aims%20to%0Apreserve%20all%20salient%20information%2C%20offering%20a%20more%20comprehensive%20and%20faithful%0Arepresentation%20of%20the%20screenplay%27s%20content.%20We%20further%20explore%20a%20baseline%0Amethod%20that%20combines%20the%20CaD%20Graph%20with%20the%20corresponding%20movie%20script%20through%0Aa%20late%20fusion%20of%20graph%20and%20text%20modalities%2C%20and%20we%20present%20very%20initial%0Apromising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscoGraMS%253A%2520Enhancing%2520Movie%2520Screen-Play%2520Summarization%2520using%2520Movie%250A%2520%2520Character-Aware%2520Discourse%2520Graph%26entry.906535625%3DMaitreya%2520Prafulla%2520Chitale%2520and%2520Uday%2520Bindal%2520and%2520Rajakrishnan%2520Rajkumar%2520and%2520Rahul%2520Mishra%26entry.1292438233%3D%2520%2520Summarizing%2520movie%2520screenplays%2520presents%2520a%2520unique%2520set%2520of%2520challenges%2520compared%2520to%250Astandard%2520document%2520summarization.%2520Screenplays%2520are%2520not%2520only%2520lengthy%252C%2520but%2520also%250Afeature%2520a%2520complex%2520interplay%2520of%2520characters%252C%2520dialogues%252C%2520and%2520scenes%252C%2520with%2520numerous%250Adirect%2520and%2520subtle%2520relationships%2520and%2520contextual%2520nuances%2520that%2520are%2520difficult%2520for%250Amachine%2520learning%2520models%2520to%2520accurately%2520capture%2520and%2520comprehend.%2520Recent%2520attempts%250Aat%2520screenplay%2520summarization%2520focus%2520on%2520fine-tuning%2520transformer-based%2520pre-trained%250Amodels%252C%2520but%2520these%2520models%2520often%2520fall%2520short%2520in%2520capturing%2520long-term%2520dependencies%250Aand%2520latent%2520relationships%252C%2520and%2520frequently%2520encounter%2520the%2520%2522lost%2520in%2520the%2520middle%2522%250Aissue.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520DiscoGraMS%252C%2520a%2520novel%2520resource%250Athat%2520represents%2520movie%2520scripts%2520as%2520a%2520movie%2520character-aware%2520discourse%2520graph%2520%2528CaD%250AGraph%2529.%2520This%2520approach%2520is%2520well-suited%2520for%2520various%2520downstream%2520tasks%252C%2520such%2520as%250Asummarization%252C%2520question-answering%252C%2520and%2520salience%2520detection.%2520The%2520model%2520aims%2520to%250Apreserve%2520all%2520salient%2520information%252C%2520offering%2520a%2520more%2520comprehensive%2520and%2520faithful%250Arepresentation%2520of%2520the%2520screenplay%2527s%2520content.%2520We%2520further%2520explore%2520a%2520baseline%250Amethod%2520that%2520combines%2520the%2520CaD%2520Graph%2520with%2520the%2520corresponding%2520movie%2520script%2520through%250Aa%2520late%2520fusion%2520of%2520graph%2520and%2520text%2520modalities%252C%2520and%2520we%2520present%2520very%2520initial%250Apromising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiscoGraMS%3A%20Enhancing%20Movie%20Screen-Play%20Summarization%20using%20Movie%0A%20%20Character-Aware%20Discourse%20Graph&entry.906535625=Maitreya%20Prafulla%20Chitale%20and%20Uday%20Bindal%20and%20Rajakrishnan%20Rajkumar%20and%20Rahul%20Mishra&entry.1292438233=%20%20Summarizing%20movie%20screenplays%20presents%20a%20unique%20set%20of%20challenges%20compared%20to%0Astandard%20document%20summarization.%20Screenplays%20are%20not%20only%20lengthy%2C%20but%20also%0Afeature%20a%20complex%20interplay%20of%20characters%2C%20dialogues%2C%20and%20scenes%2C%20with%20numerous%0Adirect%20and%20subtle%20relationships%20and%20contextual%20nuances%20that%20are%20difficult%20for%0Amachine%20learning%20models%20to%20accurately%20capture%20and%20comprehend.%20Recent%20attempts%0Aat%20screenplay%20summarization%20focus%20on%20fine-tuning%20transformer-based%20pre-trained%0Amodels%2C%20but%20these%20models%20often%20fall%20short%20in%20capturing%20long-term%20dependencies%0Aand%20latent%20relationships%2C%20and%20frequently%20encounter%20the%20%22lost%20in%20the%20middle%22%0Aissue.%20To%20address%20these%20challenges%2C%20we%20introduce%20DiscoGraMS%2C%20a%20novel%20resource%0Athat%20represents%20movie%20scripts%20as%20a%20movie%20character-aware%20discourse%20graph%20%28CaD%0AGraph%29.%20This%20approach%20is%20well-suited%20for%20various%20downstream%20tasks%2C%20such%20as%0Asummarization%2C%20question-answering%2C%20and%20salience%20detection.%20The%20model%20aims%20to%0Apreserve%20all%20salient%20information%2C%20offering%20a%20more%20comprehensive%20and%20faithful%0Arepresentation%20of%20the%20screenplay%27s%20content.%20We%20further%20explore%20a%20baseline%0Amethod%20that%20combines%20the%20CaD%20Graph%20with%20the%20corresponding%20movie%20script%20through%0Aa%20late%20fusion%20of%20graph%20and%20text%20modalities%2C%20and%20we%20present%20very%20initial%0Apromising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14666v1&entry.124074799=Read"},
{"title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search", "author": "Oliver Sieberling and Denis Kuznedelev and Eldar Kurtic and Dan Alistarh", "abstract": "  The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.\n", "link": "http://arxiv.org/abs/2410.14649v1", "date": "2024-10-18", "relevancy": 2.0224, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoPress%3A%20Towards%20Optimal%20Dynamic%20Model%20Compression%20via%20Evolutionary%0A%20%20Search&body=Title%3A%20EvoPress%3A%20Towards%20Optimal%20Dynamic%20Model%20Compression%20via%20Evolutionary%0A%20%20Search%0AAuthor%3A%20Oliver%20Sieberling%20and%20Denis%20Kuznedelev%20and%20Eldar%20Kurtic%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20The%20high%20computational%20costs%20of%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20a%0Aflurry%20of%20research%20on%20LLM%20compression%2C%20via%20methods%20such%20as%20quantization%2C%0Asparsification%2C%20or%20structured%20pruning.%20A%20new%20frontier%20in%20this%20area%20is%20given%20by%0A%5Cemph%7Bdynamic%2C%20non-uniform%7D%20compression%20methods%2C%20which%20adjust%20the%20compression%0Alevels%20%28e.g.%2C%20sparsity%29%20per-block%20or%20even%20per-layer%20in%20order%20to%20minimize%0Aaccuracy%20loss%2C%20while%20guaranteeing%20a%20global%20compression%20threshold.%20Yet%2C%20current%0Amethods%20rely%20on%20heuristics%20for%20identifying%20the%20%22importance%22%20of%20a%20given%20layer%0Atowards%20the%20loss%2C%20based%20on%20assumptions%20such%20as%20%5Cemph%7Berror%20monotonicity%7D%2C%20i.e.%0Athat%20the%20end-to-end%20model%20compression%20error%20is%20proportional%20to%20the%20sum%20of%0Alayer-wise%20errors.%20In%20this%20paper%2C%20we%20revisit%20this%20area%2C%20and%20propose%20a%20new%20and%0Ageneral%20approach%20for%20dynamic%20compression%20that%20is%20provably%20optimal%20in%20a%20given%0Ainput%20range.%20We%20begin%20from%20the%20motivating%20observation%20that%2C%20in%20general%2C%0A%5Cemph%7Berror%20monotonicity%20does%20not%20hold%20for%20LLMs%7D%3A%20compressed%20models%20with%20lower%0Asum%20of%20per-layer%20errors%20can%20perform%20%5Cemph%7Bworse%7D%20than%20models%20with%20higher%20error%0Asums.%20To%20address%20this%2C%20we%20propose%20a%20new%20general%20evolutionary%20framework%20for%0Adynamic%20LLM%20compression%20called%20EvoPress%2C%20which%20has%20provable%20convergence%2C%20and%0Alow%20sample%20and%20evaluation%20complexity.%20We%20show%20that%20these%20theoretical%20guarantees%0Alead%20to%20highly%20competitive%20practical%20performance%20for%20dynamic%20compression%20of%0ALlama%2C%20Mistral%20and%20Phi%20models.%20Via%20EvoPress%2C%20we%20set%20new%20state-of-the-art%0Aresults%20across%20all%20compression%20approaches%3A%20structural%20pruning%20%28block/layer%0Adropping%29%2C%20unstructured%20sparsity%2C%20as%20well%20as%20quantization%20with%20dynamic%0Abitwidths.%20Our%20code%20is%20available%20at%20https%3A//github.com/IST-DASLab/EvoPress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoPress%253A%2520Towards%2520Optimal%2520Dynamic%2520Model%2520Compression%2520via%2520Evolutionary%250A%2520%2520Search%26entry.906535625%3DOliver%2520Sieberling%2520and%2520Denis%2520Kuznedelev%2520and%2520Eldar%2520Kurtic%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520The%2520high%2520computational%2520costs%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520to%2520a%250Aflurry%2520of%2520research%2520on%2520LLM%2520compression%252C%2520via%2520methods%2520such%2520as%2520quantization%252C%250Asparsification%252C%2520or%2520structured%2520pruning.%2520A%2520new%2520frontier%2520in%2520this%2520area%2520is%2520given%2520by%250A%255Cemph%257Bdynamic%252C%2520non-uniform%257D%2520compression%2520methods%252C%2520which%2520adjust%2520the%2520compression%250Alevels%2520%2528e.g.%252C%2520sparsity%2529%2520per-block%2520or%2520even%2520per-layer%2520in%2520order%2520to%2520minimize%250Aaccuracy%2520loss%252C%2520while%2520guaranteeing%2520a%2520global%2520compression%2520threshold.%2520Yet%252C%2520current%250Amethods%2520rely%2520on%2520heuristics%2520for%2520identifying%2520the%2520%2522importance%2522%2520of%2520a%2520given%2520layer%250Atowards%2520the%2520loss%252C%2520based%2520on%2520assumptions%2520such%2520as%2520%255Cemph%257Berror%2520monotonicity%257D%252C%2520i.e.%250Athat%2520the%2520end-to-end%2520model%2520compression%2520error%2520is%2520proportional%2520to%2520the%2520sum%2520of%250Alayer-wise%2520errors.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520this%2520area%252C%2520and%2520propose%2520a%2520new%2520and%250Ageneral%2520approach%2520for%2520dynamic%2520compression%2520that%2520is%2520provably%2520optimal%2520in%2520a%2520given%250Ainput%2520range.%2520We%2520begin%2520from%2520the%2520motivating%2520observation%2520that%252C%2520in%2520general%252C%250A%255Cemph%257Berror%2520monotonicity%2520does%2520not%2520hold%2520for%2520LLMs%257D%253A%2520compressed%2520models%2520with%2520lower%250Asum%2520of%2520per-layer%2520errors%2520can%2520perform%2520%255Cemph%257Bworse%257D%2520than%2520models%2520with%2520higher%2520error%250Asums.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520general%2520evolutionary%2520framework%2520for%250Adynamic%2520LLM%2520compression%2520called%2520EvoPress%252C%2520which%2520has%2520provable%2520convergence%252C%2520and%250Alow%2520sample%2520and%2520evaluation%2520complexity.%2520We%2520show%2520that%2520these%2520theoretical%2520guarantees%250Alead%2520to%2520highly%2520competitive%2520practical%2520performance%2520for%2520dynamic%2520compression%2520of%250ALlama%252C%2520Mistral%2520and%2520Phi%2520models.%2520Via%2520EvoPress%252C%2520we%2520set%2520new%2520state-of-the-art%250Aresults%2520across%2520all%2520compression%2520approaches%253A%2520structural%2520pruning%2520%2528block/layer%250Adropping%2529%252C%2520unstructured%2520sparsity%252C%2520as%2520well%2520as%2520quantization%2520with%2520dynamic%250Abitwidths.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/IST-DASLab/EvoPress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoPress%3A%20Towards%20Optimal%20Dynamic%20Model%20Compression%20via%20Evolutionary%0A%20%20Search&entry.906535625=Oliver%20Sieberling%20and%20Denis%20Kuznedelev%20and%20Eldar%20Kurtic%20and%20Dan%20Alistarh&entry.1292438233=%20%20The%20high%20computational%20costs%20of%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20a%0Aflurry%20of%20research%20on%20LLM%20compression%2C%20via%20methods%20such%20as%20quantization%2C%0Asparsification%2C%20or%20structured%20pruning.%20A%20new%20frontier%20in%20this%20area%20is%20given%20by%0A%5Cemph%7Bdynamic%2C%20non-uniform%7D%20compression%20methods%2C%20which%20adjust%20the%20compression%0Alevels%20%28e.g.%2C%20sparsity%29%20per-block%20or%20even%20per-layer%20in%20order%20to%20minimize%0Aaccuracy%20loss%2C%20while%20guaranteeing%20a%20global%20compression%20threshold.%20Yet%2C%20current%0Amethods%20rely%20on%20heuristics%20for%20identifying%20the%20%22importance%22%20of%20a%20given%20layer%0Atowards%20the%20loss%2C%20based%20on%20assumptions%20such%20as%20%5Cemph%7Berror%20monotonicity%7D%2C%20i.e.%0Athat%20the%20end-to-end%20model%20compression%20error%20is%20proportional%20to%20the%20sum%20of%0Alayer-wise%20errors.%20In%20this%20paper%2C%20we%20revisit%20this%20area%2C%20and%20propose%20a%20new%20and%0Ageneral%20approach%20for%20dynamic%20compression%20that%20is%20provably%20optimal%20in%20a%20given%0Ainput%20range.%20We%20begin%20from%20the%20motivating%20observation%20that%2C%20in%20general%2C%0A%5Cemph%7Berror%20monotonicity%20does%20not%20hold%20for%20LLMs%7D%3A%20compressed%20models%20with%20lower%0Asum%20of%20per-layer%20errors%20can%20perform%20%5Cemph%7Bworse%7D%20than%20models%20with%20higher%20error%0Asums.%20To%20address%20this%2C%20we%20propose%20a%20new%20general%20evolutionary%20framework%20for%0Adynamic%20LLM%20compression%20called%20EvoPress%2C%20which%20has%20provable%20convergence%2C%20and%0Alow%20sample%20and%20evaluation%20complexity.%20We%20show%20that%20these%20theoretical%20guarantees%0Alead%20to%20highly%20competitive%20practical%20performance%20for%20dynamic%20compression%20of%0ALlama%2C%20Mistral%20and%20Phi%20models.%20Via%20EvoPress%2C%20we%20set%20new%20state-of-the-art%0Aresults%20across%20all%20compression%20approaches%3A%20structural%20pruning%20%28block/layer%0Adropping%29%2C%20unstructured%20sparsity%2C%20as%20well%20as%20quantization%20with%20dynamic%0Abitwidths.%20Our%20code%20is%20available%20at%20https%3A//github.com/IST-DASLab/EvoPress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14649v1&entry.124074799=Read"},
{"title": "TransBox: EL++-closed Ontology Embedding", "author": "Hui Yang and Jiaoyan Chen and Uli Sattler", "abstract": "  OWL (Web Ontology Language) ontologies, which are able to represent both\nrelational and type facts as standard knowledge graphs and complex domain\nknowledge in Description Logic (DL) axioms, are widely adopted in domains such\nas healthcare and bioinformatics. Inspired by the success of knowledge graph\nembeddings, embedding OWL ontologies has gained significant attention in recent\nyears. Current methods primarily focus on learning embeddings for atomic\nconcepts and roles, enabling the evaluation based on normalized axioms through\nspecially designed score functions. However, they often neglect the embedding\nof complex concepts, making it difficult to infer with more intricate axioms.\nThis limitation reduces their effectiveness in advanced reasoning tasks, such\nas Ontology Learning and ontology-mediated Query Answering. In this paper, we\npropose EL++-closed ontology embeddings which are able to represent any logical\nexpressions in DL via composition. Furthermore, we develop TransBox, an\neffective EL++-closed ontology embedding method that can handle many-to-one,\none-to-many and many-to-many relations. Our extensive experiments demonstrate\nthat TransBox often achieves state-of-the-art performance across various\nreal-world datasets for predicting complex axioms.\n", "link": "http://arxiv.org/abs/2410.14571v1", "date": "2024-10-18", "relevancy": 2.0131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5114}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransBox%3A%20EL%2B%2B-closed%20Ontology%20Embedding&body=Title%3A%20TransBox%3A%20EL%2B%2B-closed%20Ontology%20Embedding%0AAuthor%3A%20Hui%20Yang%20and%20Jiaoyan%20Chen%20and%20Uli%20Sattler%0AAbstract%3A%20%20%20OWL%20%28Web%20Ontology%20Language%29%20ontologies%2C%20which%20are%20able%20to%20represent%20both%0Arelational%20and%20type%20facts%20as%20standard%20knowledge%20graphs%20and%20complex%20domain%0Aknowledge%20in%20Description%20Logic%20%28DL%29%20axioms%2C%20are%20widely%20adopted%20in%20domains%20such%0Aas%20healthcare%20and%20bioinformatics.%20Inspired%20by%20the%20success%20of%20knowledge%20graph%0Aembeddings%2C%20embedding%20OWL%20ontologies%20has%20gained%20significant%20attention%20in%20recent%0Ayears.%20Current%20methods%20primarily%20focus%20on%20learning%20embeddings%20for%20atomic%0Aconcepts%20and%20roles%2C%20enabling%20the%20evaluation%20based%20on%20normalized%20axioms%20through%0Aspecially%20designed%20score%20functions.%20However%2C%20they%20often%20neglect%20the%20embedding%0Aof%20complex%20concepts%2C%20making%20it%20difficult%20to%20infer%20with%20more%20intricate%20axioms.%0AThis%20limitation%20reduces%20their%20effectiveness%20in%20advanced%20reasoning%20tasks%2C%20such%0Aas%20Ontology%20Learning%20and%20ontology-mediated%20Query%20Answering.%20In%20this%20paper%2C%20we%0Apropose%20EL%2B%2B-closed%20ontology%20embeddings%20which%20are%20able%20to%20represent%20any%20logical%0Aexpressions%20in%20DL%20via%20composition.%20Furthermore%2C%20we%20develop%20TransBox%2C%20an%0Aeffective%20EL%2B%2B-closed%20ontology%20embedding%20method%20that%20can%20handle%20many-to-one%2C%0Aone-to-many%20and%20many-to-many%20relations.%20Our%20extensive%20experiments%20demonstrate%0Athat%20TransBox%20often%20achieves%20state-of-the-art%20performance%20across%20various%0Areal-world%20datasets%20for%20predicting%20complex%20axioms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransBox%253A%2520EL%252B%252B-closed%2520Ontology%2520Embedding%26entry.906535625%3DHui%2520Yang%2520and%2520Jiaoyan%2520Chen%2520and%2520Uli%2520Sattler%26entry.1292438233%3D%2520%2520OWL%2520%2528Web%2520Ontology%2520Language%2529%2520ontologies%252C%2520which%2520are%2520able%2520to%2520represent%2520both%250Arelational%2520and%2520type%2520facts%2520as%2520standard%2520knowledge%2520graphs%2520and%2520complex%2520domain%250Aknowledge%2520in%2520Description%2520Logic%2520%2528DL%2529%2520axioms%252C%2520are%2520widely%2520adopted%2520in%2520domains%2520such%250Aas%2520healthcare%2520and%2520bioinformatics.%2520Inspired%2520by%2520the%2520success%2520of%2520knowledge%2520graph%250Aembeddings%252C%2520embedding%2520OWL%2520ontologies%2520has%2520gained%2520significant%2520attention%2520in%2520recent%250Ayears.%2520Current%2520methods%2520primarily%2520focus%2520on%2520learning%2520embeddings%2520for%2520atomic%250Aconcepts%2520and%2520roles%252C%2520enabling%2520the%2520evaluation%2520based%2520on%2520normalized%2520axioms%2520through%250Aspecially%2520designed%2520score%2520functions.%2520However%252C%2520they%2520often%2520neglect%2520the%2520embedding%250Aof%2520complex%2520concepts%252C%2520making%2520it%2520difficult%2520to%2520infer%2520with%2520more%2520intricate%2520axioms.%250AThis%2520limitation%2520reduces%2520their%2520effectiveness%2520in%2520advanced%2520reasoning%2520tasks%252C%2520such%250Aas%2520Ontology%2520Learning%2520and%2520ontology-mediated%2520Query%2520Answering.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520EL%252B%252B-closed%2520ontology%2520embeddings%2520which%2520are%2520able%2520to%2520represent%2520any%2520logical%250Aexpressions%2520in%2520DL%2520via%2520composition.%2520Furthermore%252C%2520we%2520develop%2520TransBox%252C%2520an%250Aeffective%2520EL%252B%252B-closed%2520ontology%2520embedding%2520method%2520that%2520can%2520handle%2520many-to-one%252C%250Aone-to-many%2520and%2520many-to-many%2520relations.%2520Our%2520extensive%2520experiments%2520demonstrate%250Athat%2520TransBox%2520often%2520achieves%2520state-of-the-art%2520performance%2520across%2520various%250Areal-world%2520datasets%2520for%2520predicting%2520complex%2520axioms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransBox%3A%20EL%2B%2B-closed%20Ontology%20Embedding&entry.906535625=Hui%20Yang%20and%20Jiaoyan%20Chen%20and%20Uli%20Sattler&entry.1292438233=%20%20OWL%20%28Web%20Ontology%20Language%29%20ontologies%2C%20which%20are%20able%20to%20represent%20both%0Arelational%20and%20type%20facts%20as%20standard%20knowledge%20graphs%20and%20complex%20domain%0Aknowledge%20in%20Description%20Logic%20%28DL%29%20axioms%2C%20are%20widely%20adopted%20in%20domains%20such%0Aas%20healthcare%20and%20bioinformatics.%20Inspired%20by%20the%20success%20of%20knowledge%20graph%0Aembeddings%2C%20embedding%20OWL%20ontologies%20has%20gained%20significant%20attention%20in%20recent%0Ayears.%20Current%20methods%20primarily%20focus%20on%20learning%20embeddings%20for%20atomic%0Aconcepts%20and%20roles%2C%20enabling%20the%20evaluation%20based%20on%20normalized%20axioms%20through%0Aspecially%20designed%20score%20functions.%20However%2C%20they%20often%20neglect%20the%20embedding%0Aof%20complex%20concepts%2C%20making%20it%20difficult%20to%20infer%20with%20more%20intricate%20axioms.%0AThis%20limitation%20reduces%20their%20effectiveness%20in%20advanced%20reasoning%20tasks%2C%20such%0Aas%20Ontology%20Learning%20and%20ontology-mediated%20Query%20Answering.%20In%20this%20paper%2C%20we%0Apropose%20EL%2B%2B-closed%20ontology%20embeddings%20which%20are%20able%20to%20represent%20any%20logical%0Aexpressions%20in%20DL%20via%20composition.%20Furthermore%2C%20we%20develop%20TransBox%2C%20an%0Aeffective%20EL%2B%2B-closed%20ontology%20embedding%20method%20that%20can%20handle%20many-to-one%2C%0Aone-to-many%20and%20many-to-many%20relations.%20Our%20extensive%20experiments%20demonstrate%0Athat%20TransBox%20often%20achieves%20state-of-the-art%20performance%20across%20various%0Areal-world%20datasets%20for%20predicting%20complex%20axioms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14571v1&entry.124074799=Read"},
{"title": "Kernel Density Estimators in Large Dimensions", "author": "Giulio Biroli and Marc M\u00e9zard", "abstract": "  This paper studies Kernel Density Estimation for a high-dimensional\ndistribution $\\rho(x)$. Traditional approaches have focused on the limit of\nlarge number of data points $n$ and fixed dimension $d$. We analyze instead the\nregime where both the number $n$ of data points $y_i$ and their dimensionality\n$d$ grow with a fixed ratio $\\alpha=(\\log n)/d$. Our study reveals three\ndistinct statistical regimes for the kernel-based estimate of the density $\\hat\n\\rho_h^{\\mathcal {D}}(x)=\\frac{1}{n h^d}\\sum_{i=1}^n\nK\\left(\\frac{x-y_i}{h}\\right)$, depending on the bandwidth $h$: a classical\nregime for large bandwidth where the Central Limit Theorem (CLT) holds, which\nis akin to the one found in traditional approaches. Below a certain value of\nthe bandwidth, $h_{CLT}(\\alpha)$, we find that the CLT breaks down. The\nstatistics of $\\hat\\rho_h^{\\mathcal {D}}(x)$ for a fixed $x$ drawn from\n$\\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable\ndistribution). In particular below a value $h_G(\\alpha)$, we find that\n$\\hat\\rho_h^{\\mathcal {D}}(x)$ is governed by extreme value statistics: only a\nfew points in the database matter and give the dominant contribution to the\ndensity estimator. We provide a detailed analysis for high-dimensional\nmultivariate Gaussian data. We show that the optimal bandwidth threshold based\non Kullback-Leibler divergence lies in the new statistical regime identified in\nthis paper. As known by practitioners, when decreasing the bandwidth a\nKernel-estimated estimated changes from a smooth curve to a collections of\npeaks centred on the data points. Our findings reveal that this general\nphenomenon is related to sharp transitions between phases characterized by\ndifferent statistical properties, and offer new insights for Kernel density\nestimation in high-dimensional settings.\n", "link": "http://arxiv.org/abs/2408.05807v3", "date": "2024-10-18", "relevancy": 2.0076, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4094}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3978}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Density%20Estimators%20in%20Large%20Dimensions&body=Title%3A%20Kernel%20Density%20Estimators%20in%20Large%20Dimensions%0AAuthor%3A%20Giulio%20Biroli%20and%20Marc%20M%C3%A9zard%0AAbstract%3A%20%20%20This%20paper%20studies%20Kernel%20Density%20Estimation%20for%20a%20high-dimensional%0Adistribution%20%24%5Crho%28x%29%24.%20Traditional%20approaches%20have%20focused%20on%20the%20limit%20of%0Alarge%20number%20of%20data%20points%20%24n%24%20and%20fixed%20dimension%20%24d%24.%20We%20analyze%20instead%20the%0Aregime%20where%20both%20the%20number%20%24n%24%20of%20data%20points%20%24y_i%24%20and%20their%20dimensionality%0A%24d%24%20grow%20with%20a%20fixed%20ratio%20%24%5Calpha%3D%28%5Clog%20n%29/d%24.%20Our%20study%20reveals%20three%0Adistinct%20statistical%20regimes%20for%20the%20kernel-based%20estimate%20of%20the%20density%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%3D%5Cfrac%7B1%7D%7Bn%20h%5Ed%7D%5Csum_%7Bi%3D1%7D%5En%0AK%5Cleft%28%5Cfrac%7Bx-y_i%7D%7Bh%7D%5Cright%29%24%2C%20depending%20on%20the%20bandwidth%20%24h%24%3A%20a%20classical%0Aregime%20for%20large%20bandwidth%20where%20the%20Central%20Limit%20Theorem%20%28CLT%29%20holds%2C%20which%0Ais%20akin%20to%20the%20one%20found%20in%20traditional%20approaches.%20Below%20a%20certain%20value%20of%0Athe%20bandwidth%2C%20%24h_%7BCLT%7D%28%5Calpha%29%24%2C%20we%20find%20that%20the%20CLT%20breaks%20down.%20The%0Astatistics%20of%20%24%5Chat%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20for%20a%20fixed%20%24x%24%20drawn%20from%0A%24%5Crho%28x%29%24%20is%20given%20by%20a%20heavy-tailed%20distribution%20%28an%20alpha-stable%0Adistribution%29.%20In%20particular%20below%20a%20value%20%24h_G%28%5Calpha%29%24%2C%20we%20find%20that%0A%24%5Chat%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20is%20governed%20by%20extreme%20value%20statistics%3A%20only%20a%0Afew%20points%20in%20the%20database%20matter%20and%20give%20the%20dominant%20contribution%20to%20the%0Adensity%20estimator.%20We%20provide%20a%20detailed%20analysis%20for%20high-dimensional%0Amultivariate%20Gaussian%20data.%20We%20show%20that%20the%20optimal%20bandwidth%20threshold%20based%0Aon%20Kullback-Leibler%20divergence%20lies%20in%20the%20new%20statistical%20regime%20identified%20in%0Athis%20paper.%20As%20known%20by%20practitioners%2C%20when%20decreasing%20the%20bandwidth%20a%0AKernel-estimated%20estimated%20changes%20from%20a%20smooth%20curve%20to%20a%20collections%20of%0Apeaks%20centred%20on%20the%20data%20points.%20Our%20findings%20reveal%20that%20this%20general%0Aphenomenon%20is%20related%20to%20sharp%20transitions%20between%20phases%20characterized%20by%0Adifferent%20statistical%20properties%2C%20and%20offer%20new%20insights%20for%20Kernel%20density%0Aestimation%20in%20high-dimensional%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05807v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Density%2520Estimators%2520in%2520Large%2520Dimensions%26entry.906535625%3DGiulio%2520Biroli%2520and%2520Marc%2520M%25C3%25A9zard%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520Kernel%2520Density%2520Estimation%2520for%2520a%2520high-dimensional%250Adistribution%2520%2524%255Crho%2528x%2529%2524.%2520Traditional%2520approaches%2520have%2520focused%2520on%2520the%2520limit%2520of%250Alarge%2520number%2520of%2520data%2520points%2520%2524n%2524%2520and%2520fixed%2520dimension%2520%2524d%2524.%2520We%2520analyze%2520instead%2520the%250Aregime%2520where%2520both%2520the%2520number%2520%2524n%2524%2520of%2520data%2520points%2520%2524y_i%2524%2520and%2520their%2520dimensionality%250A%2524d%2524%2520grow%2520with%2520a%2520fixed%2520ratio%2520%2524%255Calpha%253D%2528%255Clog%2520n%2529/d%2524.%2520Our%2520study%2520reveals%2520three%250Adistinct%2520statistical%2520regimes%2520for%2520the%2520kernel-based%2520estimate%2520of%2520the%2520density%2520%2524%255Chat%250A%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%253D%255Cfrac%257B1%257D%257Bn%2520h%255Ed%257D%255Csum_%257Bi%253D1%257D%255En%250AK%255Cleft%2528%255Cfrac%257Bx-y_i%257D%257Bh%257D%255Cright%2529%2524%252C%2520depending%2520on%2520the%2520bandwidth%2520%2524h%2524%253A%2520a%2520classical%250Aregime%2520for%2520large%2520bandwidth%2520where%2520the%2520Central%2520Limit%2520Theorem%2520%2528CLT%2529%2520holds%252C%2520which%250Ais%2520akin%2520to%2520the%2520one%2520found%2520in%2520traditional%2520approaches.%2520Below%2520a%2520certain%2520value%2520of%250Athe%2520bandwidth%252C%2520%2524h_%257BCLT%257D%2528%255Calpha%2529%2524%252C%2520we%2520find%2520that%2520the%2520CLT%2520breaks%2520down.%2520The%250Astatistics%2520of%2520%2524%255Chat%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%2524%2520for%2520a%2520fixed%2520%2524x%2524%2520drawn%2520from%250A%2524%255Crho%2528x%2529%2524%2520is%2520given%2520by%2520a%2520heavy-tailed%2520distribution%2520%2528an%2520alpha-stable%250Adistribution%2529.%2520In%2520particular%2520below%2520a%2520value%2520%2524h_G%2528%255Calpha%2529%2524%252C%2520we%2520find%2520that%250A%2524%255Chat%255Crho_h%255E%257B%255Cmathcal%2520%257BD%257D%257D%2528x%2529%2524%2520is%2520governed%2520by%2520extreme%2520value%2520statistics%253A%2520only%2520a%250Afew%2520points%2520in%2520the%2520database%2520matter%2520and%2520give%2520the%2520dominant%2520contribution%2520to%2520the%250Adensity%2520estimator.%2520We%2520provide%2520a%2520detailed%2520analysis%2520for%2520high-dimensional%250Amultivariate%2520Gaussian%2520data.%2520We%2520show%2520that%2520the%2520optimal%2520bandwidth%2520threshold%2520based%250Aon%2520Kullback-Leibler%2520divergence%2520lies%2520in%2520the%2520new%2520statistical%2520regime%2520identified%2520in%250Athis%2520paper.%2520As%2520known%2520by%2520practitioners%252C%2520when%2520decreasing%2520the%2520bandwidth%2520a%250AKernel-estimated%2520estimated%2520changes%2520from%2520a%2520smooth%2520curve%2520to%2520a%2520collections%2520of%250Apeaks%2520centred%2520on%2520the%2520data%2520points.%2520Our%2520findings%2520reveal%2520that%2520this%2520general%250Aphenomenon%2520is%2520related%2520to%2520sharp%2520transitions%2520between%2520phases%2520characterized%2520by%250Adifferent%2520statistical%2520properties%252C%2520and%2520offer%2520new%2520insights%2520for%2520Kernel%2520density%250Aestimation%2520in%2520high-dimensional%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05807v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Density%20Estimators%20in%20Large%20Dimensions&entry.906535625=Giulio%20Biroli%20and%20Marc%20M%C3%A9zard&entry.1292438233=%20%20This%20paper%20studies%20Kernel%20Density%20Estimation%20for%20a%20high-dimensional%0Adistribution%20%24%5Crho%28x%29%24.%20Traditional%20approaches%20have%20focused%20on%20the%20limit%20of%0Alarge%20number%20of%20data%20points%20%24n%24%20and%20fixed%20dimension%20%24d%24.%20We%20analyze%20instead%20the%0Aregime%20where%20both%20the%20number%20%24n%24%20of%20data%20points%20%24y_i%24%20and%20their%20dimensionality%0A%24d%24%20grow%20with%20a%20fixed%20ratio%20%24%5Calpha%3D%28%5Clog%20n%29/d%24.%20Our%20study%20reveals%20three%0Adistinct%20statistical%20regimes%20for%20the%20kernel-based%20estimate%20of%20the%20density%20%24%5Chat%0A%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%3D%5Cfrac%7B1%7D%7Bn%20h%5Ed%7D%5Csum_%7Bi%3D1%7D%5En%0AK%5Cleft%28%5Cfrac%7Bx-y_i%7D%7Bh%7D%5Cright%29%24%2C%20depending%20on%20the%20bandwidth%20%24h%24%3A%20a%20classical%0Aregime%20for%20large%20bandwidth%20where%20the%20Central%20Limit%20Theorem%20%28CLT%29%20holds%2C%20which%0Ais%20akin%20to%20the%20one%20found%20in%20traditional%20approaches.%20Below%20a%20certain%20value%20of%0Athe%20bandwidth%2C%20%24h_%7BCLT%7D%28%5Calpha%29%24%2C%20we%20find%20that%20the%20CLT%20breaks%20down.%20The%0Astatistics%20of%20%24%5Chat%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20for%20a%20fixed%20%24x%24%20drawn%20from%0A%24%5Crho%28x%29%24%20is%20given%20by%20a%20heavy-tailed%20distribution%20%28an%20alpha-stable%0Adistribution%29.%20In%20particular%20below%20a%20value%20%24h_G%28%5Calpha%29%24%2C%20we%20find%20that%0A%24%5Chat%5Crho_h%5E%7B%5Cmathcal%20%7BD%7D%7D%28x%29%24%20is%20governed%20by%20extreme%20value%20statistics%3A%20only%20a%0Afew%20points%20in%20the%20database%20matter%20and%20give%20the%20dominant%20contribution%20to%20the%0Adensity%20estimator.%20We%20provide%20a%20detailed%20analysis%20for%20high-dimensional%0Amultivariate%20Gaussian%20data.%20We%20show%20that%20the%20optimal%20bandwidth%20threshold%20based%0Aon%20Kullback-Leibler%20divergence%20lies%20in%20the%20new%20statistical%20regime%20identified%20in%0Athis%20paper.%20As%20known%20by%20practitioners%2C%20when%20decreasing%20the%20bandwidth%20a%0AKernel-estimated%20estimated%20changes%20from%20a%20smooth%20curve%20to%20a%20collections%20of%0Apeaks%20centred%20on%20the%20data%20points.%20Our%20findings%20reveal%20that%20this%20general%0Aphenomenon%20is%20related%20to%20sharp%20transitions%20between%20phases%20characterized%20by%0Adifferent%20statistical%20properties%2C%20and%20offer%20new%20insights%20for%20Kernel%20density%0Aestimation%20in%20high-dimensional%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05807v3&entry.124074799=Read"},
{"title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens", "author": "Zhepeng Cen and Yao Liu and Siliang Zeng and Pratik Chaudhar and Huzefa Rangwala and George Karypis and Rasool Fakoor", "abstract": "  Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.\n", "link": "http://arxiv.org/abs/2410.14655v1", "date": "2024-10-18", "relevancy": 2.0074, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5203}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens&body=Title%3A%20Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens%0AAuthor%3A%20Zhepeng%20Cen%20and%20Yao%20Liu%20and%20Siliang%20Zeng%20and%20Pratik%20Chaudhar%20and%20Huzefa%20Rangwala%20and%20George%20Karypis%20and%20Rasool%20Fakoor%0AAbstract%3A%20%20%20Language%20models%20are%20often%20trained%20to%20maximize%20the%20likelihood%20of%20the%20next%0Atoken%20given%20past%20tokens%20in%20the%20training%20dataset.%20However%2C%20during%20inference%0Atime%2C%20they%20are%20utilized%20differently%2C%20generating%20text%20sequentially%20and%0Aauto-regressively%20by%20using%20previously%20generated%20tokens%20as%20input%20to%20predict%20the%0Anext%20one.%20Marginal%20differences%20in%20predictions%20at%20each%20step%20can%20cascade%20over%0Asuccessive%20steps%2C%20resulting%20in%20different%20distributions%20from%20what%20the%20models%0Awere%20trained%20for%20and%20potentially%20leading%20to%20unpredictable%20behavior.%20This%20paper%0Aproposes%20two%20simple%20approaches%20based%20on%20model%20own%20generation%20to%20address%20this%0Adiscrepancy%20between%20the%20training%20and%20inference%20time.%20Our%20first%20approach%20is%0ABatch-Scheduled%20Sampling%2C%20where%2C%20during%20training%2C%20we%20stochastically%20choose%0Abetween%20the%20ground-truth%20token%20from%20the%20dataset%20and%20the%20model%27s%20own%20generated%0Atoken%20as%20input%20to%20predict%20the%20next%20token.%20This%20is%20done%20in%20an%20offline%20manner%2C%0Amodifying%20the%20context%20window%20by%20interleaving%20ground-truth%20tokens%20with%20those%0Agenerated%20by%20the%20model.%20Our%20second%20approach%20is%20Reference-Answer-based%0ACorrection%2C%20where%20we%20explicitly%20incorporate%20a%20self-correction%20capability%20into%0Athe%20model%20during%20training.%20This%20enables%20the%20model%20to%20effectively%20self-correct%0Athe%20gaps%20between%20the%20generated%20sequences%20and%20the%20ground%20truth%20data%20without%0Arelying%20on%20an%20external%20oracle%20model.%20By%20incorporating%20our%20proposed%20strategies%0Aduring%20training%2C%20we%20have%20observed%20an%20overall%20improvement%20in%20performance%0Acompared%20to%20baseline%20methods%2C%20as%20demonstrated%20by%20our%20extensive%20experiments%0Ausing%20summarization%2C%20general%20question-answering%2C%20and%20math%20question-answering%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Training-Inference%2520Gap%2520in%2520LLMs%2520by%2520Leveraging%2520Self-Generated%250A%2520%2520Tokens%26entry.906535625%3DZhepeng%2520Cen%2520and%2520Yao%2520Liu%2520and%2520Siliang%2520Zeng%2520and%2520Pratik%2520Chaudhar%2520and%2520Huzefa%2520Rangwala%2520and%2520George%2520Karypis%2520and%2520Rasool%2520Fakoor%26entry.1292438233%3D%2520%2520Language%2520models%2520are%2520often%2520trained%2520to%2520maximize%2520the%2520likelihood%2520of%2520the%2520next%250Atoken%2520given%2520past%2520tokens%2520in%2520the%2520training%2520dataset.%2520However%252C%2520during%2520inference%250Atime%252C%2520they%2520are%2520utilized%2520differently%252C%2520generating%2520text%2520sequentially%2520and%250Aauto-regressively%2520by%2520using%2520previously%2520generated%2520tokens%2520as%2520input%2520to%2520predict%2520the%250Anext%2520one.%2520Marginal%2520differences%2520in%2520predictions%2520at%2520each%2520step%2520can%2520cascade%2520over%250Asuccessive%2520steps%252C%2520resulting%2520in%2520different%2520distributions%2520from%2520what%2520the%2520models%250Awere%2520trained%2520for%2520and%2520potentially%2520leading%2520to%2520unpredictable%2520behavior.%2520This%2520paper%250Aproposes%2520two%2520simple%2520approaches%2520based%2520on%2520model%2520own%2520generation%2520to%2520address%2520this%250Adiscrepancy%2520between%2520the%2520training%2520and%2520inference%2520time.%2520Our%2520first%2520approach%2520is%250ABatch-Scheduled%2520Sampling%252C%2520where%252C%2520during%2520training%252C%2520we%2520stochastically%2520choose%250Abetween%2520the%2520ground-truth%2520token%2520from%2520the%2520dataset%2520and%2520the%2520model%2527s%2520own%2520generated%250Atoken%2520as%2520input%2520to%2520predict%2520the%2520next%2520token.%2520This%2520is%2520done%2520in%2520an%2520offline%2520manner%252C%250Amodifying%2520the%2520context%2520window%2520by%2520interleaving%2520ground-truth%2520tokens%2520with%2520those%250Agenerated%2520by%2520the%2520model.%2520Our%2520second%2520approach%2520is%2520Reference-Answer-based%250ACorrection%252C%2520where%2520we%2520explicitly%2520incorporate%2520a%2520self-correction%2520capability%2520into%250Athe%2520model%2520during%2520training.%2520This%2520enables%2520the%2520model%2520to%2520effectively%2520self-correct%250Athe%2520gaps%2520between%2520the%2520generated%2520sequences%2520and%2520the%2520ground%2520truth%2520data%2520without%250Arelying%2520on%2520an%2520external%2520oracle%2520model.%2520By%2520incorporating%2520our%2520proposed%2520strategies%250Aduring%2520training%252C%2520we%2520have%2520observed%2520an%2520overall%2520improvement%2520in%2520performance%250Acompared%2520to%2520baseline%2520methods%252C%2520as%2520demonstrated%2520by%2520our%2520extensive%2520experiments%250Ausing%2520summarization%252C%2520general%2520question-answering%252C%2520and%2520math%2520question-answering%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Training-Inference%20Gap%20in%20LLMs%20by%20Leveraging%20Self-Generated%0A%20%20Tokens&entry.906535625=Zhepeng%20Cen%20and%20Yao%20Liu%20and%20Siliang%20Zeng%20and%20Pratik%20Chaudhar%20and%20Huzefa%20Rangwala%20and%20George%20Karypis%20and%20Rasool%20Fakoor&entry.1292438233=%20%20Language%20models%20are%20often%20trained%20to%20maximize%20the%20likelihood%20of%20the%20next%0Atoken%20given%20past%20tokens%20in%20the%20training%20dataset.%20However%2C%20during%20inference%0Atime%2C%20they%20are%20utilized%20differently%2C%20generating%20text%20sequentially%20and%0Aauto-regressively%20by%20using%20previously%20generated%20tokens%20as%20input%20to%20predict%20the%0Anext%20one.%20Marginal%20differences%20in%20predictions%20at%20each%20step%20can%20cascade%20over%0Asuccessive%20steps%2C%20resulting%20in%20different%20distributions%20from%20what%20the%20models%0Awere%20trained%20for%20and%20potentially%20leading%20to%20unpredictable%20behavior.%20This%20paper%0Aproposes%20two%20simple%20approaches%20based%20on%20model%20own%20generation%20to%20address%20this%0Adiscrepancy%20between%20the%20training%20and%20inference%20time.%20Our%20first%20approach%20is%0ABatch-Scheduled%20Sampling%2C%20where%2C%20during%20training%2C%20we%20stochastically%20choose%0Abetween%20the%20ground-truth%20token%20from%20the%20dataset%20and%20the%20model%27s%20own%20generated%0Atoken%20as%20input%20to%20predict%20the%20next%20token.%20This%20is%20done%20in%20an%20offline%20manner%2C%0Amodifying%20the%20context%20window%20by%20interleaving%20ground-truth%20tokens%20with%20those%0Agenerated%20by%20the%20model.%20Our%20second%20approach%20is%20Reference-Answer-based%0ACorrection%2C%20where%20we%20explicitly%20incorporate%20a%20self-correction%20capability%20into%0Athe%20model%20during%20training.%20This%20enables%20the%20model%20to%20effectively%20self-correct%0Athe%20gaps%20between%20the%20generated%20sequences%20and%20the%20ground%20truth%20data%20without%0Arelying%20on%20an%20external%20oracle%20model.%20By%20incorporating%20our%20proposed%20strategies%0Aduring%20training%2C%20we%20have%20observed%20an%20overall%20improvement%20in%20performance%0Acompared%20to%20baseline%20methods%2C%20as%20demonstrated%20by%20our%20extensive%20experiments%0Ausing%20summarization%2C%20general%20question-answering%2C%20and%20math%20question-answering%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14655v1&entry.124074799=Read"},
{"title": "Towards Unsupervised Validation of Anomaly-Detection Models", "author": "Lihi Idan", "abstract": "  Unsupervised validation of anomaly-detection models is a highly challenging\ntask. While the common practices for model validation involve a labeled\nvalidation set, such validation sets cannot be constructed when the underlying\ndatasets are unlabeled. The lack of robust and efficient unsupervised\nmodel-validation techniques presents an acute challenge in the implementation\nof automated anomaly-detection pipelines, especially when there exists no prior\nknowledge of the model's performance on similar datasets. This work presents a\nnew paradigm to automated validation of anomaly-detection models, inspired by\nreal-world, collaborative decision-making mechanisms. We focus on two\ncommonly-used, unsupervised model-validation tasks -- model selection and model\nevaluation -- and provide extensive experimental results that demonstrate the\naccuracy and robustness of our approach on both tasks.\n", "link": "http://arxiv.org/abs/2410.14579v1", "date": "2024-10-18", "relevancy": 1.986, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5327}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4964}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unsupervised%20Validation%20of%20Anomaly-Detection%20Models&body=Title%3A%20Towards%20Unsupervised%20Validation%20of%20Anomaly-Detection%20Models%0AAuthor%3A%20Lihi%20Idan%0AAbstract%3A%20%20%20Unsupervised%20validation%20of%20anomaly-detection%20models%20is%20a%20highly%20challenging%0Atask.%20While%20the%20common%20practices%20for%20model%20validation%20involve%20a%20labeled%0Avalidation%20set%2C%20such%20validation%20sets%20cannot%20be%20constructed%20when%20the%20underlying%0Adatasets%20are%20unlabeled.%20The%20lack%20of%20robust%20and%20efficient%20unsupervised%0Amodel-validation%20techniques%20presents%20an%20acute%20challenge%20in%20the%20implementation%0Aof%20automated%20anomaly-detection%20pipelines%2C%20especially%20when%20there%20exists%20no%20prior%0Aknowledge%20of%20the%20model%27s%20performance%20on%20similar%20datasets.%20This%20work%20presents%20a%0Anew%20paradigm%20to%20automated%20validation%20of%20anomaly-detection%20models%2C%20inspired%20by%0Areal-world%2C%20collaborative%20decision-making%20mechanisms.%20We%20focus%20on%20two%0Acommonly-used%2C%20unsupervised%20model-validation%20tasks%20--%20model%20selection%20and%20model%0Aevaluation%20--%20and%20provide%20extensive%20experimental%20results%20that%20demonstrate%20the%0Aaccuracy%20and%20robustness%20of%20our%20approach%20on%20both%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unsupervised%2520Validation%2520of%2520Anomaly-Detection%2520Models%26entry.906535625%3DLihi%2520Idan%26entry.1292438233%3D%2520%2520Unsupervised%2520validation%2520of%2520anomaly-detection%2520models%2520is%2520a%2520highly%2520challenging%250Atask.%2520While%2520the%2520common%2520practices%2520for%2520model%2520validation%2520involve%2520a%2520labeled%250Avalidation%2520set%252C%2520such%2520validation%2520sets%2520cannot%2520be%2520constructed%2520when%2520the%2520underlying%250Adatasets%2520are%2520unlabeled.%2520The%2520lack%2520of%2520robust%2520and%2520efficient%2520unsupervised%250Amodel-validation%2520techniques%2520presents%2520an%2520acute%2520challenge%2520in%2520the%2520implementation%250Aof%2520automated%2520anomaly-detection%2520pipelines%252C%2520especially%2520when%2520there%2520exists%2520no%2520prior%250Aknowledge%2520of%2520the%2520model%2527s%2520performance%2520on%2520similar%2520datasets.%2520This%2520work%2520presents%2520a%250Anew%2520paradigm%2520to%2520automated%2520validation%2520of%2520anomaly-detection%2520models%252C%2520inspired%2520by%250Areal-world%252C%2520collaborative%2520decision-making%2520mechanisms.%2520We%2520focus%2520on%2520two%250Acommonly-used%252C%2520unsupervised%2520model-validation%2520tasks%2520--%2520model%2520selection%2520and%2520model%250Aevaluation%2520--%2520and%2520provide%2520extensive%2520experimental%2520results%2520that%2520demonstrate%2520the%250Aaccuracy%2520and%2520robustness%2520of%2520our%2520approach%2520on%2520both%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unsupervised%20Validation%20of%20Anomaly-Detection%20Models&entry.906535625=Lihi%20Idan&entry.1292438233=%20%20Unsupervised%20validation%20of%20anomaly-detection%20models%20is%20a%20highly%20challenging%0Atask.%20While%20the%20common%20practices%20for%20model%20validation%20involve%20a%20labeled%0Avalidation%20set%2C%20such%20validation%20sets%20cannot%20be%20constructed%20when%20the%20underlying%0Adatasets%20are%20unlabeled.%20The%20lack%20of%20robust%20and%20efficient%20unsupervised%0Amodel-validation%20techniques%20presents%20an%20acute%20challenge%20in%20the%20implementation%0Aof%20automated%20anomaly-detection%20pipelines%2C%20especially%20when%20there%20exists%20no%20prior%0Aknowledge%20of%20the%20model%27s%20performance%20on%20similar%20datasets.%20This%20work%20presents%20a%0Anew%20paradigm%20to%20automated%20validation%20of%20anomaly-detection%20models%2C%20inspired%20by%0Areal-world%2C%20collaborative%20decision-making%20mechanisms.%20We%20focus%20on%20two%0Acommonly-used%2C%20unsupervised%20model-validation%20tasks%20--%20model%20selection%20and%20model%0Aevaluation%20--%20and%20provide%20extensive%20experimental%20results%20that%20demonstrate%20the%0Aaccuracy%20and%20robustness%20of%20our%20approach%20on%20both%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14579v1&entry.124074799=Read"},
{"title": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions", "author": "Zhiyuan Peng and Jinming Nian and Alexandre Evfimievski and Yi Fang", "abstract": "  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n", "link": "http://arxiv.org/abs/2410.14567v1", "date": "2024-10-18", "relevancy": 1.9856, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5047}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAG-ConfusionQA%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Confusing%20Questions&body=Title%3A%20RAG-ConfusionQA%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Confusing%20Questions%0AAuthor%3A%20Zhiyuan%20Peng%20and%20Jinming%20Nian%20and%20Alexandre%20Evfimievski%20and%20Yi%20Fang%0AAbstract%3A%20%20%20Conversational%20AI%20agents%20use%20Retrieval%20Augmented%20Generation%20%28RAG%29%20to%20provide%0Averifiable%20document-grounded%20responses%20to%20user%20inquiries.%20However%2C%20many%20natural%0Aquestions%20do%20not%20have%20good%20answers%3A%20about%2025%5C%25%20contain%20false%0Aassumptions~%5Ccite%7BYu2023%3ACREPE%7D%2C%20and%20over%2050%5C%25%20are%0Aambiguous~%5Ccite%7BMin2020%3AAmbigQA%7D.%20RAG%20agents%20need%20high-quality%20data%20to%20improve%0Atheir%20responses%20to%20confusing%20questions.%20This%20paper%20presents%20a%20novel%20synthetic%0Adata%20generation%20method%20to%20efficiently%20create%20a%20diverse%20set%20of%20context-grounded%0Aconfusing%20questions%20from%20a%20given%20document%20corpus.%20We%20conduct%20an%20empirical%0Acomparative%20evaluation%20of%20several%20large%20language%20models%20as%20RAG%20agents%20to%0Ameasure%20the%20accuracy%20of%20confusion%20detection%20and%20appropriate%20response%0Ageneration.%20We%20contribute%20a%20benchmark%20dataset%20to%20the%20public%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAG-ConfusionQA%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520LLMs%2520on%2520Confusing%2520Questions%26entry.906535625%3DZhiyuan%2520Peng%2520and%2520Jinming%2520Nian%2520and%2520Alexandre%2520Evfimievski%2520and%2520Yi%2520Fang%26entry.1292438233%3D%2520%2520Conversational%2520AI%2520agents%2520use%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520to%2520provide%250Averifiable%2520document-grounded%2520responses%2520to%2520user%2520inquiries.%2520However%252C%2520many%2520natural%250Aquestions%2520do%2520not%2520have%2520good%2520answers%253A%2520about%252025%255C%2525%2520contain%2520false%250Aassumptions~%255Ccite%257BYu2023%253ACREPE%257D%252C%2520and%2520over%252050%255C%2525%2520are%250Aambiguous~%255Ccite%257BMin2020%253AAmbigQA%257D.%2520RAG%2520agents%2520need%2520high-quality%2520data%2520to%2520improve%250Atheir%2520responses%2520to%2520confusing%2520questions.%2520This%2520paper%2520presents%2520a%2520novel%2520synthetic%250Adata%2520generation%2520method%2520to%2520efficiently%2520create%2520a%2520diverse%2520set%2520of%2520context-grounded%250Aconfusing%2520questions%2520from%2520a%2520given%2520document%2520corpus.%2520We%2520conduct%2520an%2520empirical%250Acomparative%2520evaluation%2520of%2520several%2520large%2520language%2520models%2520as%2520RAG%2520agents%2520to%250Ameasure%2520the%2520accuracy%2520of%2520confusion%2520detection%2520and%2520appropriate%2520response%250Ageneration.%2520We%2520contribute%2520a%2520benchmark%2520dataset%2520to%2520the%2520public%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAG-ConfusionQA%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Confusing%20Questions&entry.906535625=Zhiyuan%20Peng%20and%20Jinming%20Nian%20and%20Alexandre%20Evfimievski%20and%20Yi%20Fang&entry.1292438233=%20%20Conversational%20AI%20agents%20use%20Retrieval%20Augmented%20Generation%20%28RAG%29%20to%20provide%0Averifiable%20document-grounded%20responses%20to%20user%20inquiries.%20However%2C%20many%20natural%0Aquestions%20do%20not%20have%20good%20answers%3A%20about%2025%5C%25%20contain%20false%0Aassumptions~%5Ccite%7BYu2023%3ACREPE%7D%2C%20and%20over%2050%5C%25%20are%0Aambiguous~%5Ccite%7BMin2020%3AAmbigQA%7D.%20RAG%20agents%20need%20high-quality%20data%20to%20improve%0Atheir%20responses%20to%20confusing%20questions.%20This%20paper%20presents%20a%20novel%20synthetic%0Adata%20generation%20method%20to%20efficiently%20create%20a%20diverse%20set%20of%20context-grounded%0Aconfusing%20questions%20from%20a%20given%20document%20corpus.%20We%20conduct%20an%20empirical%0Acomparative%20evaluation%20of%20several%20large%20language%20models%20as%20RAG%20agents%20to%0Ameasure%20the%20accuracy%20of%20confusion%20detection%20and%20appropriate%20response%0Ageneration.%20We%20contribute%20a%20benchmark%20dataset%20to%20the%20public%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14567v1&entry.124074799=Read"},
{"title": "A Hybrid Feature Fusion Deep Learning Framework for Leukemia Cancer\n  Detection in Microscopic Blood Sample Using Gated Recurrent Unit and\n  Uncertainty Quantification", "author": "Maksuda Akter and Rabea Khatun and Md Manowarul Islam", "abstract": "  Acute lymphoblastic leukemia (ALL) is the most malignant form of leukemia and\nthe most common cancer in adults and children. Traditionally, leukemia is\ndiagnosed by analyzing blood and bone marrow smears under a microscope, with\nadditional cytochemical tests for confirmation. However, these methods are\nexpensive, time consuming, and highly dependent on expert knowledge. In recent\nyears, deep learning, particularly Convolutional Neural Networks (CNNs), has\nprovided advanced methods for classifying microscopic smear images, aiding in\nthe detection of leukemic cells. These approaches are quick, cost effective,\nand not subject to human bias. However, most methods lack the ability to\nquantify uncertainty, which could lead to critical misdiagnoses. In this\nresearch, hybrid deep learning models (InceptionV3-GRU, EfficientNetB3-GRU,\nMobileNetV2-GRU) were implemented to classify ALL. Bayesian optimization was\nused to fine tune the model's hyperparameters and improve its performance.\nAdditionally, Deep Ensemble uncertainty quantification was applied to address\nuncertainty during leukemia image classification. The proposed models were\ntrained on the publicly available datasets ALL-IDB1 and ALL-IDB2. Their results\nwere then aggregated at the score level using the sum rule. The parallel\narchitecture used in these models offers a high level of confidence in\ndifferentiating between ALL and non-ALL cases. The proposed method achieved a\nremarkable detection accuracy rate of 100% on the ALL-IDB1 dataset, 98.07% on\nthe ALL-IDB2 dataset, and 98.64% on the combined dataset, demonstrating its\npotential for accurate and reliable leukemia diagnosis.\n", "link": "http://arxiv.org/abs/2410.14536v1", "date": "2024-10-18", "relevancy": 1.9501, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5072}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4858}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Feature%20Fusion%20Deep%20Learning%20Framework%20for%20Leukemia%20Cancer%0A%20%20Detection%20in%20Microscopic%20Blood%20Sample%20Using%20Gated%20Recurrent%20Unit%20and%0A%20%20Uncertainty%20Quantification&body=Title%3A%20A%20Hybrid%20Feature%20Fusion%20Deep%20Learning%20Framework%20for%20Leukemia%20Cancer%0A%20%20Detection%20in%20Microscopic%20Blood%20Sample%20Using%20Gated%20Recurrent%20Unit%20and%0A%20%20Uncertainty%20Quantification%0AAuthor%3A%20Maksuda%20Akter%20and%20Rabea%20Khatun%20and%20Md%20Manowarul%20Islam%0AAbstract%3A%20%20%20Acute%20lymphoblastic%20leukemia%20%28ALL%29%20is%20the%20most%20malignant%20form%20of%20leukemia%20and%0Athe%20most%20common%20cancer%20in%20adults%20and%20children.%20Traditionally%2C%20leukemia%20is%0Adiagnosed%20by%20analyzing%20blood%20and%20bone%20marrow%20smears%20under%20a%20microscope%2C%20with%0Aadditional%20cytochemical%20tests%20for%20confirmation.%20However%2C%20these%20methods%20are%0Aexpensive%2C%20time%20consuming%2C%20and%20highly%20dependent%20on%20expert%20knowledge.%20In%20recent%0Ayears%2C%20deep%20learning%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20has%0Aprovided%20advanced%20methods%20for%20classifying%20microscopic%20smear%20images%2C%20aiding%20in%0Athe%20detection%20of%20leukemic%20cells.%20These%20approaches%20are%20quick%2C%20cost%20effective%2C%0Aand%20not%20subject%20to%20human%20bias.%20However%2C%20most%20methods%20lack%20the%20ability%20to%0Aquantify%20uncertainty%2C%20which%20could%20lead%20to%20critical%20misdiagnoses.%20In%20this%0Aresearch%2C%20hybrid%20deep%20learning%20models%20%28InceptionV3-GRU%2C%20EfficientNetB3-GRU%2C%0AMobileNetV2-GRU%29%20were%20implemented%20to%20classify%20ALL.%20Bayesian%20optimization%20was%0Aused%20to%20fine%20tune%20the%20model%27s%20hyperparameters%20and%20improve%20its%20performance.%0AAdditionally%2C%20Deep%20Ensemble%20uncertainty%20quantification%20was%20applied%20to%20address%0Auncertainty%20during%20leukemia%20image%20classification.%20The%20proposed%20models%20were%0Atrained%20on%20the%20publicly%20available%20datasets%20ALL-IDB1%20and%20ALL-IDB2.%20Their%20results%0Awere%20then%20aggregated%20at%20the%20score%20level%20using%20the%20sum%20rule.%20The%20parallel%0Aarchitecture%20used%20in%20these%20models%20offers%20a%20high%20level%20of%20confidence%20in%0Adifferentiating%20between%20ALL%20and%20non-ALL%20cases.%20The%20proposed%20method%20achieved%20a%0Aremarkable%20detection%20accuracy%20rate%20of%20100%25%20on%20the%20ALL-IDB1%20dataset%2C%2098.07%25%20on%0Athe%20ALL-IDB2%20dataset%2C%20and%2098.64%25%20on%20the%20combined%20dataset%2C%20demonstrating%20its%0Apotential%20for%20accurate%20and%20reliable%20leukemia%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Feature%2520Fusion%2520Deep%2520Learning%2520Framework%2520for%2520Leukemia%2520Cancer%250A%2520%2520Detection%2520in%2520Microscopic%2520Blood%2520Sample%2520Using%2520Gated%2520Recurrent%2520Unit%2520and%250A%2520%2520Uncertainty%2520Quantification%26entry.906535625%3DMaksuda%2520Akter%2520and%2520Rabea%2520Khatun%2520and%2520Md%2520Manowarul%2520Islam%26entry.1292438233%3D%2520%2520Acute%2520lymphoblastic%2520leukemia%2520%2528ALL%2529%2520is%2520the%2520most%2520malignant%2520form%2520of%2520leukemia%2520and%250Athe%2520most%2520common%2520cancer%2520in%2520adults%2520and%2520children.%2520Traditionally%252C%2520leukemia%2520is%250Adiagnosed%2520by%2520analyzing%2520blood%2520and%2520bone%2520marrow%2520smears%2520under%2520a%2520microscope%252C%2520with%250Aadditional%2520cytochemical%2520tests%2520for%2520confirmation.%2520However%252C%2520these%2520methods%2520are%250Aexpensive%252C%2520time%2520consuming%252C%2520and%2520highly%2520dependent%2520on%2520expert%2520knowledge.%2520In%2520recent%250Ayears%252C%2520deep%2520learning%252C%2520particularly%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520has%250Aprovided%2520advanced%2520methods%2520for%2520classifying%2520microscopic%2520smear%2520images%252C%2520aiding%2520in%250Athe%2520detection%2520of%2520leukemic%2520cells.%2520These%2520approaches%2520are%2520quick%252C%2520cost%2520effective%252C%250Aand%2520not%2520subject%2520to%2520human%2520bias.%2520However%252C%2520most%2520methods%2520lack%2520the%2520ability%2520to%250Aquantify%2520uncertainty%252C%2520which%2520could%2520lead%2520to%2520critical%2520misdiagnoses.%2520In%2520this%250Aresearch%252C%2520hybrid%2520deep%2520learning%2520models%2520%2528InceptionV3-GRU%252C%2520EfficientNetB3-GRU%252C%250AMobileNetV2-GRU%2529%2520were%2520implemented%2520to%2520classify%2520ALL.%2520Bayesian%2520optimization%2520was%250Aused%2520to%2520fine%2520tune%2520the%2520model%2527s%2520hyperparameters%2520and%2520improve%2520its%2520performance.%250AAdditionally%252C%2520Deep%2520Ensemble%2520uncertainty%2520quantification%2520was%2520applied%2520to%2520address%250Auncertainty%2520during%2520leukemia%2520image%2520classification.%2520The%2520proposed%2520models%2520were%250Atrained%2520on%2520the%2520publicly%2520available%2520datasets%2520ALL-IDB1%2520and%2520ALL-IDB2.%2520Their%2520results%250Awere%2520then%2520aggregated%2520at%2520the%2520score%2520level%2520using%2520the%2520sum%2520rule.%2520The%2520parallel%250Aarchitecture%2520used%2520in%2520these%2520models%2520offers%2520a%2520high%2520level%2520of%2520confidence%2520in%250Adifferentiating%2520between%2520ALL%2520and%2520non-ALL%2520cases.%2520The%2520proposed%2520method%2520achieved%2520a%250Aremarkable%2520detection%2520accuracy%2520rate%2520of%2520100%2525%2520on%2520the%2520ALL-IDB1%2520dataset%252C%252098.07%2525%2520on%250Athe%2520ALL-IDB2%2520dataset%252C%2520and%252098.64%2525%2520on%2520the%2520combined%2520dataset%252C%2520demonstrating%2520its%250Apotential%2520for%2520accurate%2520and%2520reliable%2520leukemia%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Feature%20Fusion%20Deep%20Learning%20Framework%20for%20Leukemia%20Cancer%0A%20%20Detection%20in%20Microscopic%20Blood%20Sample%20Using%20Gated%20Recurrent%20Unit%20and%0A%20%20Uncertainty%20Quantification&entry.906535625=Maksuda%20Akter%20and%20Rabea%20Khatun%20and%20Md%20Manowarul%20Islam&entry.1292438233=%20%20Acute%20lymphoblastic%20leukemia%20%28ALL%29%20is%20the%20most%20malignant%20form%20of%20leukemia%20and%0Athe%20most%20common%20cancer%20in%20adults%20and%20children.%20Traditionally%2C%20leukemia%20is%0Adiagnosed%20by%20analyzing%20blood%20and%20bone%20marrow%20smears%20under%20a%20microscope%2C%20with%0Aadditional%20cytochemical%20tests%20for%20confirmation.%20However%2C%20these%20methods%20are%0Aexpensive%2C%20time%20consuming%2C%20and%20highly%20dependent%20on%20expert%20knowledge.%20In%20recent%0Ayears%2C%20deep%20learning%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20has%0Aprovided%20advanced%20methods%20for%20classifying%20microscopic%20smear%20images%2C%20aiding%20in%0Athe%20detection%20of%20leukemic%20cells.%20These%20approaches%20are%20quick%2C%20cost%20effective%2C%0Aand%20not%20subject%20to%20human%20bias.%20However%2C%20most%20methods%20lack%20the%20ability%20to%0Aquantify%20uncertainty%2C%20which%20could%20lead%20to%20critical%20misdiagnoses.%20In%20this%0Aresearch%2C%20hybrid%20deep%20learning%20models%20%28InceptionV3-GRU%2C%20EfficientNetB3-GRU%2C%0AMobileNetV2-GRU%29%20were%20implemented%20to%20classify%20ALL.%20Bayesian%20optimization%20was%0Aused%20to%20fine%20tune%20the%20model%27s%20hyperparameters%20and%20improve%20its%20performance.%0AAdditionally%2C%20Deep%20Ensemble%20uncertainty%20quantification%20was%20applied%20to%20address%0Auncertainty%20during%20leukemia%20image%20classification.%20The%20proposed%20models%20were%0Atrained%20on%20the%20publicly%20available%20datasets%20ALL-IDB1%20and%20ALL-IDB2.%20Their%20results%0Awere%20then%20aggregated%20at%20the%20score%20level%20using%20the%20sum%20rule.%20The%20parallel%0Aarchitecture%20used%20in%20these%20models%20offers%20a%20high%20level%20of%20confidence%20in%0Adifferentiating%20between%20ALL%20and%20non-ALL%20cases.%20The%20proposed%20method%20achieved%20a%0Aremarkable%20detection%20accuracy%20rate%20of%20100%25%20on%20the%20ALL-IDB1%20dataset%2C%2098.07%25%20on%0Athe%20ALL-IDB2%20dataset%2C%20and%2098.64%25%20on%20the%20combined%20dataset%2C%20demonstrating%20its%0Apotential%20for%20accurate%20and%20reliable%20leukemia%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14536v1&entry.124074799=Read"},
{"title": "Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map", "author": "J\u00e9rome Eertmans and Enrico Maria Vittuci and Vittorio Degli Esposti and Laurent Jacques and Claude Oestges", "abstract": "  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n", "link": "http://arxiv.org/abs/2410.14535v1", "date": "2024-10-18", "relevancy": 1.942, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map&body=Title%3A%20Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map%0AAuthor%3A%20J%C3%A9rome%20Eertmans%20and%20Enrico%20Maria%20Vittuci%20and%20Vittorio%20Degli%20Esposti%20and%20Laurent%20Jacques%20and%20Claude%20Oestges%0AAbstract%3A%20%20%20With%20the%20increasing%20presence%20of%20dynamic%20scenarios%2C%20such%20as%20Vehicle-to-Vehicle%0Acommunications%2C%20radio%20propagation%20modeling%20tools%20must%20adapt%20to%20the%20rapidly%0Achanging%20nature%20of%20the%20radio%20channel.%20Recently%2C%20both%20Differentiable%20and%20Dynamic%0ARay%20Tracing%20frameworks%20have%20emerged%20to%20address%20these%20challenges.%20However%2C%20there%0Ais%20often%20confusion%20about%20how%20these%20approaches%20differ%20and%20which%20one%20should%20be%0Aused%20in%20specific%20contexts.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20these%20two%0Atechniques%20and%20a%20comparative%20analysis%20against%20two%20state-of-the-art%20tools%3A%0A3DSCAT%20from%20UniBo%20and%20Sionna%20from%20NVIDIA.%20To%20provide%20a%20more%20precise%0Acharacterization%20of%20the%20scope%20of%20these%20methods%2C%20we%20introduce%20a%20novel%0Asimulation-based%20metric%2C%20the%20Multipath%20Lifetime%20Map%2C%20which%20enables%20the%0Aevaluation%20of%20spatial%20and%20temporal%20coherence%20in%20radio%20channels%20only%20based%20on%0Athe%20geometrical%20description%20of%20the%20environment.%20Finally%2C%20our%20metrics%20are%0Aevaluated%20on%20a%20classic%20urban%20street%20canyon%20scenario%2C%20yielding%20similar%20results%0Ato%20those%20obtained%20from%20measurement%20campaigns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Differentiable%2520and%2520Dynamic%2520Ray%2520Tracing%253A%2520Introducing%2520the%250A%2520%2520Multipath%2520Lifetime%2520Map%26entry.906535625%3DJ%25C3%25A9rome%2520Eertmans%2520and%2520Enrico%2520Maria%2520Vittuci%2520and%2520Vittorio%2520Degli%2520Esposti%2520and%2520Laurent%2520Jacques%2520and%2520Claude%2520Oestges%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520presence%2520of%2520dynamic%2520scenarios%252C%2520such%2520as%2520Vehicle-to-Vehicle%250Acommunications%252C%2520radio%2520propagation%2520modeling%2520tools%2520must%2520adapt%2520to%2520the%2520rapidly%250Achanging%2520nature%2520of%2520the%2520radio%2520channel.%2520Recently%252C%2520both%2520Differentiable%2520and%2520Dynamic%250ARay%2520Tracing%2520frameworks%2520have%2520emerged%2520to%2520address%2520these%2520challenges.%2520However%252C%2520there%250Ais%2520often%2520confusion%2520about%2520how%2520these%2520approaches%2520differ%2520and%2520which%2520one%2520should%2520be%250Aused%2520in%2520specific%2520contexts.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520overview%2520of%2520these%2520two%250Atechniques%2520and%2520a%2520comparative%2520analysis%2520against%2520two%2520state-of-the-art%2520tools%253A%250A3DSCAT%2520from%2520UniBo%2520and%2520Sionna%2520from%2520NVIDIA.%2520To%2520provide%2520a%2520more%2520precise%250Acharacterization%2520of%2520the%2520scope%2520of%2520these%2520methods%252C%2520we%2520introduce%2520a%2520novel%250Asimulation-based%2520metric%252C%2520the%2520Multipath%2520Lifetime%2520Map%252C%2520which%2520enables%2520the%250Aevaluation%2520of%2520spatial%2520and%2520temporal%2520coherence%2520in%2520radio%2520channels%2520only%2520based%2520on%250Athe%2520geometrical%2520description%2520of%2520the%2520environment.%2520Finally%252C%2520our%2520metrics%2520are%250Aevaluated%2520on%2520a%2520classic%2520urban%2520street%2520canyon%2520scenario%252C%2520yielding%2520similar%2520results%250Ato%2520those%2520obtained%2520from%2520measurement%2520campaigns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Differentiable%20and%20Dynamic%20Ray%20Tracing%3A%20Introducing%20the%0A%20%20Multipath%20Lifetime%20Map&entry.906535625=J%C3%A9rome%20Eertmans%20and%20Enrico%20Maria%20Vittuci%20and%20Vittorio%20Degli%20Esposti%20and%20Laurent%20Jacques%20and%20Claude%20Oestges&entry.1292438233=%20%20With%20the%20increasing%20presence%20of%20dynamic%20scenarios%2C%20such%20as%20Vehicle-to-Vehicle%0Acommunications%2C%20radio%20propagation%20modeling%20tools%20must%20adapt%20to%20the%20rapidly%0Achanging%20nature%20of%20the%20radio%20channel.%20Recently%2C%20both%20Differentiable%20and%20Dynamic%0ARay%20Tracing%20frameworks%20have%20emerged%20to%20address%20these%20challenges.%20However%2C%20there%0Ais%20often%20confusion%20about%20how%20these%20approaches%20differ%20and%20which%20one%20should%20be%0Aused%20in%20specific%20contexts.%20In%20this%20paper%2C%20we%20provide%20an%20overview%20of%20these%20two%0Atechniques%20and%20a%20comparative%20analysis%20against%20two%20state-of-the-art%20tools%3A%0A3DSCAT%20from%20UniBo%20and%20Sionna%20from%20NVIDIA.%20To%20provide%20a%20more%20precise%0Acharacterization%20of%20the%20scope%20of%20these%20methods%2C%20we%20introduce%20a%20novel%0Asimulation-based%20metric%2C%20the%20Multipath%20Lifetime%20Map%2C%20which%20enables%20the%0Aevaluation%20of%20spatial%20and%20temporal%20coherence%20in%20radio%20channels%20only%20based%20on%0Athe%20geometrical%20description%20of%20the%20environment.%20Finally%2C%20our%20metrics%20are%0Aevaluated%20on%20a%20classic%20urban%20street%20canyon%20scenario%2C%20yielding%20similar%20results%0Ato%20those%20obtained%20from%20measurement%20campaigns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14535v1&entry.124074799=Read"},
{"title": "Learning to Control the Smoothness of Graph Convolutional Network\n  Features", "author": "Shih-Hsin Wang and Justin Baker and Cory Hauck and Bao Wang", "abstract": "  The pioneering work of Oono and Suzuki [ICLR, 2020] and Cai and Wang\n[arXiv:2006.13318] initializes the analysis of the smoothness of graph\nconvolutional network (GCN) features. Their results reveal an intricate\nempirical correlation between node classification accuracy and the ratio of\nsmooth to non-smooth feature components. However, the optimal ratio that favors\nnode classification is unknown, and the non-smooth features of deep GCN with\nReLU or leaky ReLU activation function diminish. In this paper, we propose a\nnew strategy to let GCN learn node features with a desired smoothness --\nadapting to data and tasks -- to enhance node classification. Our approach has\nthree key steps: (1) We establish a geometric relationship between the input\nand output of ReLU or leaky ReLU. (2) Building on our geometric insights, we\naugment the message-passing process of graph convolutional layers (GCLs) with a\nlearnable term to modulate the smoothness of node features with computational\nefficiency. (3) We investigate the achievable ratio between smooth and\nnon-smooth feature components for GCNs with the augmented message-passing\nscheme. Our extensive numerical results show that the augmented message-passing\nschemes significantly improve node classification for GCN and some related\nmodels.\n", "link": "http://arxiv.org/abs/2410.14604v1", "date": "2024-10-18", "relevancy": 1.9405, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4948}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4883}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Control%20the%20Smoothness%20of%20Graph%20Convolutional%20Network%0A%20%20Features&body=Title%3A%20Learning%20to%20Control%20the%20Smoothness%20of%20Graph%20Convolutional%20Network%0A%20%20Features%0AAuthor%3A%20Shih-Hsin%20Wang%20and%20Justin%20Baker%20and%20Cory%20Hauck%20and%20Bao%20Wang%0AAbstract%3A%20%20%20The%20pioneering%20work%20of%20Oono%20and%20Suzuki%20%5BICLR%2C%202020%5D%20and%20Cai%20and%20Wang%0A%5BarXiv%3A2006.13318%5D%20initializes%20the%20analysis%20of%20the%20smoothness%20of%20graph%0Aconvolutional%20network%20%28GCN%29%20features.%20Their%20results%20reveal%20an%20intricate%0Aempirical%20correlation%20between%20node%20classification%20accuracy%20and%20the%20ratio%20of%0Asmooth%20to%20non-smooth%20feature%20components.%20However%2C%20the%20optimal%20ratio%20that%20favors%0Anode%20classification%20is%20unknown%2C%20and%20the%20non-smooth%20features%20of%20deep%20GCN%20with%0AReLU%20or%20leaky%20ReLU%20activation%20function%20diminish.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20strategy%20to%20let%20GCN%20learn%20node%20features%20with%20a%20desired%20smoothness%20--%0Aadapting%20to%20data%20and%20tasks%20--%20to%20enhance%20node%20classification.%20Our%20approach%20has%0Athree%20key%20steps%3A%20%281%29%20We%20establish%20a%20geometric%20relationship%20between%20the%20input%0Aand%20output%20of%20ReLU%20or%20leaky%20ReLU.%20%282%29%20Building%20on%20our%20geometric%20insights%2C%20we%0Aaugment%20the%20message-passing%20process%20of%20graph%20convolutional%20layers%20%28GCLs%29%20with%20a%0Alearnable%20term%20to%20modulate%20the%20smoothness%20of%20node%20features%20with%20computational%0Aefficiency.%20%283%29%20We%20investigate%20the%20achievable%20ratio%20between%20smooth%20and%0Anon-smooth%20feature%20components%20for%20GCNs%20with%20the%20augmented%20message-passing%0Ascheme.%20Our%20extensive%20numerical%20results%20show%20that%20the%20augmented%20message-passing%0Aschemes%20significantly%20improve%20node%20classification%20for%20GCN%20and%20some%20related%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Control%2520the%2520Smoothness%2520of%2520Graph%2520Convolutional%2520Network%250A%2520%2520Features%26entry.906535625%3DShih-Hsin%2520Wang%2520and%2520Justin%2520Baker%2520and%2520Cory%2520Hauck%2520and%2520Bao%2520Wang%26entry.1292438233%3D%2520%2520The%2520pioneering%2520work%2520of%2520Oono%2520and%2520Suzuki%2520%255BICLR%252C%25202020%255D%2520and%2520Cai%2520and%2520Wang%250A%255BarXiv%253A2006.13318%255D%2520initializes%2520the%2520analysis%2520of%2520the%2520smoothness%2520of%2520graph%250Aconvolutional%2520network%2520%2528GCN%2529%2520features.%2520Their%2520results%2520reveal%2520an%2520intricate%250Aempirical%2520correlation%2520between%2520node%2520classification%2520accuracy%2520and%2520the%2520ratio%2520of%250Asmooth%2520to%2520non-smooth%2520feature%2520components.%2520However%252C%2520the%2520optimal%2520ratio%2520that%2520favors%250Anode%2520classification%2520is%2520unknown%252C%2520and%2520the%2520non-smooth%2520features%2520of%2520deep%2520GCN%2520with%250AReLU%2520or%2520leaky%2520ReLU%2520activation%2520function%2520diminish.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anew%2520strategy%2520to%2520let%2520GCN%2520learn%2520node%2520features%2520with%2520a%2520desired%2520smoothness%2520--%250Aadapting%2520to%2520data%2520and%2520tasks%2520--%2520to%2520enhance%2520node%2520classification.%2520Our%2520approach%2520has%250Athree%2520key%2520steps%253A%2520%25281%2529%2520We%2520establish%2520a%2520geometric%2520relationship%2520between%2520the%2520input%250Aand%2520output%2520of%2520ReLU%2520or%2520leaky%2520ReLU.%2520%25282%2529%2520Building%2520on%2520our%2520geometric%2520insights%252C%2520we%250Aaugment%2520the%2520message-passing%2520process%2520of%2520graph%2520convolutional%2520layers%2520%2528GCLs%2529%2520with%2520a%250Alearnable%2520term%2520to%2520modulate%2520the%2520smoothness%2520of%2520node%2520features%2520with%2520computational%250Aefficiency.%2520%25283%2529%2520We%2520investigate%2520the%2520achievable%2520ratio%2520between%2520smooth%2520and%250Anon-smooth%2520feature%2520components%2520for%2520GCNs%2520with%2520the%2520augmented%2520message-passing%250Ascheme.%2520Our%2520extensive%2520numerical%2520results%2520show%2520that%2520the%2520augmented%2520message-passing%250Aschemes%2520significantly%2520improve%2520node%2520classification%2520for%2520GCN%2520and%2520some%2520related%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Control%20the%20Smoothness%20of%20Graph%20Convolutional%20Network%0A%20%20Features&entry.906535625=Shih-Hsin%20Wang%20and%20Justin%20Baker%20and%20Cory%20Hauck%20and%20Bao%20Wang&entry.1292438233=%20%20The%20pioneering%20work%20of%20Oono%20and%20Suzuki%20%5BICLR%2C%202020%5D%20and%20Cai%20and%20Wang%0A%5BarXiv%3A2006.13318%5D%20initializes%20the%20analysis%20of%20the%20smoothness%20of%20graph%0Aconvolutional%20network%20%28GCN%29%20features.%20Their%20results%20reveal%20an%20intricate%0Aempirical%20correlation%20between%20node%20classification%20accuracy%20and%20the%20ratio%20of%0Asmooth%20to%20non-smooth%20feature%20components.%20However%2C%20the%20optimal%20ratio%20that%20favors%0Anode%20classification%20is%20unknown%2C%20and%20the%20non-smooth%20features%20of%20deep%20GCN%20with%0AReLU%20or%20leaky%20ReLU%20activation%20function%20diminish.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20strategy%20to%20let%20GCN%20learn%20node%20features%20with%20a%20desired%20smoothness%20--%0Aadapting%20to%20data%20and%20tasks%20--%20to%20enhance%20node%20classification.%20Our%20approach%20has%0Athree%20key%20steps%3A%20%281%29%20We%20establish%20a%20geometric%20relationship%20between%20the%20input%0Aand%20output%20of%20ReLU%20or%20leaky%20ReLU.%20%282%29%20Building%20on%20our%20geometric%20insights%2C%20we%0Aaugment%20the%20message-passing%20process%20of%20graph%20convolutional%20layers%20%28GCLs%29%20with%20a%0Alearnable%20term%20to%20modulate%20the%20smoothness%20of%20node%20features%20with%20computational%0Aefficiency.%20%283%29%20We%20investigate%20the%20achievable%20ratio%20between%20smooth%20and%0Anon-smooth%20feature%20components%20for%20GCNs%20with%20the%20augmented%20message-passing%0Ascheme.%20Our%20extensive%20numerical%20results%20show%20that%20the%20augmented%20message-passing%0Aschemes%20significantly%20improve%20node%20classification%20for%20GCN%20and%20some%20related%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14604v1&entry.124074799=Read"},
{"title": "Understanding the difficulty of low-precision post-training quantization\n  of large language models", "author": "Zifei Xu and Sayeh Sharify and Wanzin Yazar and Tristan Webb and Xin Wang", "abstract": "  Large language models of high parameter counts are computationally expensive,\nyet can be made much more efficient by compressing their weights to very low\nnumerical precision. This can be achieved either through post-training\nquantization by minimizing local, layer-wise quantization errors, or through\nquantization-aware fine-tuning by minimizing the global loss function. In this\nstudy, we discovered that, under the same data constraint, the former approach\nnearly always fared worse than the latter, a phenomenon particularly prominent\nwhen the numerical precision is very low. We further showed that this\ndifficulty of post-training quantization arose from stark misalignment between\noptimization of the local and global objective functions. Our findings explains\nlimited utility in minimization of local quantization error and the importance\nof direct quantization-aware fine-tuning, in the regime of large models at very\nlow precision.\n", "link": "http://arxiv.org/abs/2410.14570v1", "date": "2024-10-18", "relevancy": 1.9267, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4901}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20difficulty%20of%20low-precision%20post-training%20quantization%0A%20%20of%20large%20language%20models&body=Title%3A%20Understanding%20the%20difficulty%20of%20low-precision%20post-training%20quantization%0A%20%20of%20large%20language%20models%0AAuthor%3A%20Zifei%20Xu%20and%20Sayeh%20Sharify%20and%20Wanzin%20Yazar%20and%20Tristan%20Webb%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20of%20high%20parameter%20counts%20are%20computationally%20expensive%2C%0Ayet%20can%20be%20made%20much%20more%20efficient%20by%20compressing%20their%20weights%20to%20very%20low%0Anumerical%20precision.%20This%20can%20be%20achieved%20either%20through%20post-training%0Aquantization%20by%20minimizing%20local%2C%20layer-wise%20quantization%20errors%2C%20or%20through%0Aquantization-aware%20fine-tuning%20by%20minimizing%20the%20global%20loss%20function.%20In%20this%0Astudy%2C%20we%20discovered%20that%2C%20under%20the%20same%20data%20constraint%2C%20the%20former%20approach%0Anearly%20always%20fared%20worse%20than%20the%20latter%2C%20a%20phenomenon%20particularly%20prominent%0Awhen%20the%20numerical%20precision%20is%20very%20low.%20We%20further%20showed%20that%20this%0Adifficulty%20of%20post-training%20quantization%20arose%20from%20stark%20misalignment%20between%0Aoptimization%20of%20the%20local%20and%20global%20objective%20functions.%20Our%20findings%20explains%0Alimited%20utility%20in%20minimization%20of%20local%20quantization%20error%20and%20the%20importance%0Aof%20direct%20quantization-aware%20fine-tuning%2C%20in%20the%20regime%20of%20large%20models%20at%20very%0Alow%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520difficulty%2520of%2520low-precision%2520post-training%2520quantization%250A%2520%2520of%2520large%2520language%2520models%26entry.906535625%3DZifei%2520Xu%2520and%2520Sayeh%2520Sharify%2520and%2520Wanzin%2520Yazar%2520and%2520Tristan%2520Webb%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520of%2520high%2520parameter%2520counts%2520are%2520computationally%2520expensive%252C%250Ayet%2520can%2520be%2520made%2520much%2520more%2520efficient%2520by%2520compressing%2520their%2520weights%2520to%2520very%2520low%250Anumerical%2520precision.%2520This%2520can%2520be%2520achieved%2520either%2520through%2520post-training%250Aquantization%2520by%2520minimizing%2520local%252C%2520layer-wise%2520quantization%2520errors%252C%2520or%2520through%250Aquantization-aware%2520fine-tuning%2520by%2520minimizing%2520the%2520global%2520loss%2520function.%2520In%2520this%250Astudy%252C%2520we%2520discovered%2520that%252C%2520under%2520the%2520same%2520data%2520constraint%252C%2520the%2520former%2520approach%250Anearly%2520always%2520fared%2520worse%2520than%2520the%2520latter%252C%2520a%2520phenomenon%2520particularly%2520prominent%250Awhen%2520the%2520numerical%2520precision%2520is%2520very%2520low.%2520We%2520further%2520showed%2520that%2520this%250Adifficulty%2520of%2520post-training%2520quantization%2520arose%2520from%2520stark%2520misalignment%2520between%250Aoptimization%2520of%2520the%2520local%2520and%2520global%2520objective%2520functions.%2520Our%2520findings%2520explains%250Alimited%2520utility%2520in%2520minimization%2520of%2520local%2520quantization%2520error%2520and%2520the%2520importance%250Aof%2520direct%2520quantization-aware%2520fine-tuning%252C%2520in%2520the%2520regime%2520of%2520large%2520models%2520at%2520very%250Alow%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20difficulty%20of%20low-precision%20post-training%20quantization%0A%20%20of%20large%20language%20models&entry.906535625=Zifei%20Xu%20and%20Sayeh%20Sharify%20and%20Wanzin%20Yazar%20and%20Tristan%20Webb%20and%20Xin%20Wang&entry.1292438233=%20%20Large%20language%20models%20of%20high%20parameter%20counts%20are%20computationally%20expensive%2C%0Ayet%20can%20be%20made%20much%20more%20efficient%20by%20compressing%20their%20weights%20to%20very%20low%0Anumerical%20precision.%20This%20can%20be%20achieved%20either%20through%20post-training%0Aquantization%20by%20minimizing%20local%2C%20layer-wise%20quantization%20errors%2C%20or%20through%0Aquantization-aware%20fine-tuning%20by%20minimizing%20the%20global%20loss%20function.%20In%20this%0Astudy%2C%20we%20discovered%20that%2C%20under%20the%20same%20data%20constraint%2C%20the%20former%20approach%0Anearly%20always%20fared%20worse%20than%20the%20latter%2C%20a%20phenomenon%20particularly%20prominent%0Awhen%20the%20numerical%20precision%20is%20very%20low.%20We%20further%20showed%20that%20this%0Adifficulty%20of%20post-training%20quantization%20arose%20from%20stark%20misalignment%20between%0Aoptimization%20of%20the%20local%20and%20global%20objective%20functions.%20Our%20findings%20explains%0Alimited%20utility%20in%20minimization%20of%20local%20quantization%20error%20and%20the%20importance%0Aof%20direct%20quantization-aware%20fine-tuning%2C%20in%20the%20regime%20of%20large%20models%20at%20very%0Alow%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14570v1&entry.124074799=Read"},
{"title": "On the Regularization of Learnable Embeddings for Time Series Processing", "author": "Luca Butera and Giovanni De Felice and Andrea Cini and Cesare Alippi", "abstract": "  In processing multiple time series, accounting for the individual features of\neach sequence can be challenging. To address this, modern deep learning methods\nfor time series analysis combine a shared (global) model with local layers,\nspecific to each time series, often implemented as learnable embeddings.\nIdeally, these local embeddings should encode meaningful representations of the\nunique dynamics of each sequence. However, when these are learned end-to-end as\nparameters of a forecasting model, they may end up acting as mere sequence\nidentifiers. Shared processing blocks may then become reliant on such\nidentifiers, limiting their transferability to new contexts. In this paper, we\naddress this issue by investigating methods to regularize the learning of local\nlearnable embeddings for time series processing. Specifically, we perform the\nfirst extensive empirical study on the subject and show how such\nregularizations consistently improve performance in widely adopted\narchitectures. Furthermore, we show that methods preventing the co-adaptation\nof local and global parameters are particularly effective in this context. This\nhypothesis is validated by comparing several methods preventing the downstream\nmodels from relying on sequence identifiers, going as far as completely\nresetting the embeddings during training. The obtained results provide an\nimportant contribution to understanding the interplay between learnable local\nparameters and shared processing layers: a key challenge in modern time series\nprocessing models and a step toward developing effective foundation models for\ntime series.\n", "link": "http://arxiv.org/abs/2410.14630v1", "date": "2024-10-18", "relevancy": 1.921, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4827}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Regularization%20of%20Learnable%20Embeddings%20for%20Time%20Series%20Processing&body=Title%3A%20On%20the%20Regularization%20of%20Learnable%20Embeddings%20for%20Time%20Series%20Processing%0AAuthor%3A%20Luca%20Butera%20and%20Giovanni%20De%20Felice%20and%20Andrea%20Cini%20and%20Cesare%20Alippi%0AAbstract%3A%20%20%20In%20processing%20multiple%20time%20series%2C%20accounting%20for%20the%20individual%20features%20of%0Aeach%20sequence%20can%20be%20challenging.%20To%20address%20this%2C%20modern%20deep%20learning%20methods%0Afor%20time%20series%20analysis%20combine%20a%20shared%20%28global%29%20model%20with%20local%20layers%2C%0Aspecific%20to%20each%20time%20series%2C%20often%20implemented%20as%20learnable%20embeddings.%0AIdeally%2C%20these%20local%20embeddings%20should%20encode%20meaningful%20representations%20of%20the%0Aunique%20dynamics%20of%20each%20sequence.%20However%2C%20when%20these%20are%20learned%20end-to-end%20as%0Aparameters%20of%20a%20forecasting%20model%2C%20they%20may%20end%20up%20acting%20as%20mere%20sequence%0Aidentifiers.%20Shared%20processing%20blocks%20may%20then%20become%20reliant%20on%20such%0Aidentifiers%2C%20limiting%20their%20transferability%20to%20new%20contexts.%20In%20this%20paper%2C%20we%0Aaddress%20this%20issue%20by%20investigating%20methods%20to%20regularize%20the%20learning%20of%20local%0Alearnable%20embeddings%20for%20time%20series%20processing.%20Specifically%2C%20we%20perform%20the%0Afirst%20extensive%20empirical%20study%20on%20the%20subject%20and%20show%20how%20such%0Aregularizations%20consistently%20improve%20performance%20in%20widely%20adopted%0Aarchitectures.%20Furthermore%2C%20we%20show%20that%20methods%20preventing%20the%20co-adaptation%0Aof%20local%20and%20global%20parameters%20are%20particularly%20effective%20in%20this%20context.%20This%0Ahypothesis%20is%20validated%20by%20comparing%20several%20methods%20preventing%20the%20downstream%0Amodels%20from%20relying%20on%20sequence%20identifiers%2C%20going%20as%20far%20as%20completely%0Aresetting%20the%20embeddings%20during%20training.%20The%20obtained%20results%20provide%20an%0Aimportant%20contribution%20to%20understanding%20the%20interplay%20between%20learnable%20local%0Aparameters%20and%20shared%20processing%20layers%3A%20a%20key%20challenge%20in%20modern%20time%20series%0Aprocessing%20models%20and%20a%20step%20toward%20developing%20effective%20foundation%20models%20for%0Atime%20series.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Regularization%2520of%2520Learnable%2520Embeddings%2520for%2520Time%2520Series%2520Processing%26entry.906535625%3DLuca%2520Butera%2520and%2520Giovanni%2520De%2520Felice%2520and%2520Andrea%2520Cini%2520and%2520Cesare%2520Alippi%26entry.1292438233%3D%2520%2520In%2520processing%2520multiple%2520time%2520series%252C%2520accounting%2520for%2520the%2520individual%2520features%2520of%250Aeach%2520sequence%2520can%2520be%2520challenging.%2520To%2520address%2520this%252C%2520modern%2520deep%2520learning%2520methods%250Afor%2520time%2520series%2520analysis%2520combine%2520a%2520shared%2520%2528global%2529%2520model%2520with%2520local%2520layers%252C%250Aspecific%2520to%2520each%2520time%2520series%252C%2520often%2520implemented%2520as%2520learnable%2520embeddings.%250AIdeally%252C%2520these%2520local%2520embeddings%2520should%2520encode%2520meaningful%2520representations%2520of%2520the%250Aunique%2520dynamics%2520of%2520each%2520sequence.%2520However%252C%2520when%2520these%2520are%2520learned%2520end-to-end%2520as%250Aparameters%2520of%2520a%2520forecasting%2520model%252C%2520they%2520may%2520end%2520up%2520acting%2520as%2520mere%2520sequence%250Aidentifiers.%2520Shared%2520processing%2520blocks%2520may%2520then%2520become%2520reliant%2520on%2520such%250Aidentifiers%252C%2520limiting%2520their%2520transferability%2520to%2520new%2520contexts.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520this%2520issue%2520by%2520investigating%2520methods%2520to%2520regularize%2520the%2520learning%2520of%2520local%250Alearnable%2520embeddings%2520for%2520time%2520series%2520processing.%2520Specifically%252C%2520we%2520perform%2520the%250Afirst%2520extensive%2520empirical%2520study%2520on%2520the%2520subject%2520and%2520show%2520how%2520such%250Aregularizations%2520consistently%2520improve%2520performance%2520in%2520widely%2520adopted%250Aarchitectures.%2520Furthermore%252C%2520we%2520show%2520that%2520methods%2520preventing%2520the%2520co-adaptation%250Aof%2520local%2520and%2520global%2520parameters%2520are%2520particularly%2520effective%2520in%2520this%2520context.%2520This%250Ahypothesis%2520is%2520validated%2520by%2520comparing%2520several%2520methods%2520preventing%2520the%2520downstream%250Amodels%2520from%2520relying%2520on%2520sequence%2520identifiers%252C%2520going%2520as%2520far%2520as%2520completely%250Aresetting%2520the%2520embeddings%2520during%2520training.%2520The%2520obtained%2520results%2520provide%2520an%250Aimportant%2520contribution%2520to%2520understanding%2520the%2520interplay%2520between%2520learnable%2520local%250Aparameters%2520and%2520shared%2520processing%2520layers%253A%2520a%2520key%2520challenge%2520in%2520modern%2520time%2520series%250Aprocessing%2520models%2520and%2520a%2520step%2520toward%2520developing%2520effective%2520foundation%2520models%2520for%250Atime%2520series.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Regularization%20of%20Learnable%20Embeddings%20for%20Time%20Series%20Processing&entry.906535625=Luca%20Butera%20and%20Giovanni%20De%20Felice%20and%20Andrea%20Cini%20and%20Cesare%20Alippi&entry.1292438233=%20%20In%20processing%20multiple%20time%20series%2C%20accounting%20for%20the%20individual%20features%20of%0Aeach%20sequence%20can%20be%20challenging.%20To%20address%20this%2C%20modern%20deep%20learning%20methods%0Afor%20time%20series%20analysis%20combine%20a%20shared%20%28global%29%20model%20with%20local%20layers%2C%0Aspecific%20to%20each%20time%20series%2C%20often%20implemented%20as%20learnable%20embeddings.%0AIdeally%2C%20these%20local%20embeddings%20should%20encode%20meaningful%20representations%20of%20the%0Aunique%20dynamics%20of%20each%20sequence.%20However%2C%20when%20these%20are%20learned%20end-to-end%20as%0Aparameters%20of%20a%20forecasting%20model%2C%20they%20may%20end%20up%20acting%20as%20mere%20sequence%0Aidentifiers.%20Shared%20processing%20blocks%20may%20then%20become%20reliant%20on%20such%0Aidentifiers%2C%20limiting%20their%20transferability%20to%20new%20contexts.%20In%20this%20paper%2C%20we%0Aaddress%20this%20issue%20by%20investigating%20methods%20to%20regularize%20the%20learning%20of%20local%0Alearnable%20embeddings%20for%20time%20series%20processing.%20Specifically%2C%20we%20perform%20the%0Afirst%20extensive%20empirical%20study%20on%20the%20subject%20and%20show%20how%20such%0Aregularizations%20consistently%20improve%20performance%20in%20widely%20adopted%0Aarchitectures.%20Furthermore%2C%20we%20show%20that%20methods%20preventing%20the%20co-adaptation%0Aof%20local%20and%20global%20parameters%20are%20particularly%20effective%20in%20this%20context.%20This%0Ahypothesis%20is%20validated%20by%20comparing%20several%20methods%20preventing%20the%20downstream%0Amodels%20from%20relying%20on%20sequence%20identifiers%2C%20going%20as%20far%20as%20completely%0Aresetting%20the%20embeddings%20during%20training.%20The%20obtained%20results%20provide%20an%0Aimportant%20contribution%20to%20understanding%20the%20interplay%20between%20learnable%20local%0Aparameters%20and%20shared%20processing%20layers%3A%20a%20key%20challenge%20in%20modern%20time%20series%0Aprocessing%20models%20and%20a%20step%20toward%20developing%20effective%20foundation%20models%20for%0Atime%20series.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14630v1&entry.124074799=Read"},
{"title": "Diffusion-based Semi-supervised Spectral Algorithm for Regression on\n  Manifolds", "author": "Weichun Xia and Jiaxin Jiang and Lei Shi", "abstract": "  We introduce a novel diffusion-based spectral algorithm to tackle regression\nanalysis on high-dimensional data, particularly data embedded within\nlower-dimensional manifolds. Traditional spectral algorithms often fall short\nin such contexts, primarily due to the reliance on predetermined kernel\nfunctions, which inadequately address the complex structures inherent in\nmanifold-based data. By employing graph Laplacian approximation, our method\nuses the local estimation property of heat kernel, offering an adaptive,\ndata-driven approach to overcome this obstacle. Another distinct advantage of\nour algorithm lies in its semi-supervised learning framework, enabling it to\nfully use the additional unlabeled data. This ability enhances the performance\nby allowing the algorithm to dig the spectrum and curvature of the data\nmanifold, providing a more comprehensive understanding of the dataset.\nMoreover, our algorithm performs in an entirely data-driven manner, operating\ndirectly within the intrinsic manifold structure of the data, without requiring\nany predefined manifold information. We provide a convergence analysis of our\nalgorithm. Our findings reveal that the algorithm achieves a convergence rate\nthat depends solely on the intrinsic dimension of the underlying manifold,\nthereby avoiding the curse of dimensionality associated with the higher ambient\ndimension.\n", "link": "http://arxiv.org/abs/2410.14539v1", "date": "2024-10-18", "relevancy": 1.9096, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4878}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4746}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-based%20Semi-supervised%20Spectral%20Algorithm%20for%20Regression%20on%0A%20%20Manifolds&body=Title%3A%20Diffusion-based%20Semi-supervised%20Spectral%20Algorithm%20for%20Regression%20on%0A%20%20Manifolds%0AAuthor%3A%20Weichun%20Xia%20and%20Jiaxin%20Jiang%20and%20Lei%20Shi%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20diffusion-based%20spectral%20algorithm%20to%20tackle%20regression%0Aanalysis%20on%20high-dimensional%20data%2C%20particularly%20data%20embedded%20within%0Alower-dimensional%20manifolds.%20Traditional%20spectral%20algorithms%20often%20fall%20short%0Ain%20such%20contexts%2C%20primarily%20due%20to%20the%20reliance%20on%20predetermined%20kernel%0Afunctions%2C%20which%20inadequately%20address%20the%20complex%20structures%20inherent%20in%0Amanifold-based%20data.%20By%20employing%20graph%20Laplacian%20approximation%2C%20our%20method%0Auses%20the%20local%20estimation%20property%20of%20heat%20kernel%2C%20offering%20an%20adaptive%2C%0Adata-driven%20approach%20to%20overcome%20this%20obstacle.%20Another%20distinct%20advantage%20of%0Aour%20algorithm%20lies%20in%20its%20semi-supervised%20learning%20framework%2C%20enabling%20it%20to%0Afully%20use%20the%20additional%20unlabeled%20data.%20This%20ability%20enhances%20the%20performance%0Aby%20allowing%20the%20algorithm%20to%20dig%20the%20spectrum%20and%20curvature%20of%20the%20data%0Amanifold%2C%20providing%20a%20more%20comprehensive%20understanding%20of%20the%20dataset.%0AMoreover%2C%20our%20algorithm%20performs%20in%20an%20entirely%20data-driven%20manner%2C%20operating%0Adirectly%20within%20the%20intrinsic%20manifold%20structure%20of%20the%20data%2C%20without%20requiring%0Aany%20predefined%20manifold%20information.%20We%20provide%20a%20convergence%20analysis%20of%20our%0Aalgorithm.%20Our%20findings%20reveal%20that%20the%20algorithm%20achieves%20a%20convergence%20rate%0Athat%20depends%20solely%20on%20the%20intrinsic%20dimension%20of%20the%20underlying%20manifold%2C%0Athereby%20avoiding%20the%20curse%20of%20dimensionality%20associated%20with%20the%20higher%20ambient%0Adimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-based%2520Semi-supervised%2520Spectral%2520Algorithm%2520for%2520Regression%2520on%250A%2520%2520Manifolds%26entry.906535625%3DWeichun%2520Xia%2520and%2520Jiaxin%2520Jiang%2520and%2520Lei%2520Shi%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520diffusion-based%2520spectral%2520algorithm%2520to%2520tackle%2520regression%250Aanalysis%2520on%2520high-dimensional%2520data%252C%2520particularly%2520data%2520embedded%2520within%250Alower-dimensional%2520manifolds.%2520Traditional%2520spectral%2520algorithms%2520often%2520fall%2520short%250Ain%2520such%2520contexts%252C%2520primarily%2520due%2520to%2520the%2520reliance%2520on%2520predetermined%2520kernel%250Afunctions%252C%2520which%2520inadequately%2520address%2520the%2520complex%2520structures%2520inherent%2520in%250Amanifold-based%2520data.%2520By%2520employing%2520graph%2520Laplacian%2520approximation%252C%2520our%2520method%250Auses%2520the%2520local%2520estimation%2520property%2520of%2520heat%2520kernel%252C%2520offering%2520an%2520adaptive%252C%250Adata-driven%2520approach%2520to%2520overcome%2520this%2520obstacle.%2520Another%2520distinct%2520advantage%2520of%250Aour%2520algorithm%2520lies%2520in%2520its%2520semi-supervised%2520learning%2520framework%252C%2520enabling%2520it%2520to%250Afully%2520use%2520the%2520additional%2520unlabeled%2520data.%2520This%2520ability%2520enhances%2520the%2520performance%250Aby%2520allowing%2520the%2520algorithm%2520to%2520dig%2520the%2520spectrum%2520and%2520curvature%2520of%2520the%2520data%250Amanifold%252C%2520providing%2520a%2520more%2520comprehensive%2520understanding%2520of%2520the%2520dataset.%250AMoreover%252C%2520our%2520algorithm%2520performs%2520in%2520an%2520entirely%2520data-driven%2520manner%252C%2520operating%250Adirectly%2520within%2520the%2520intrinsic%2520manifold%2520structure%2520of%2520the%2520data%252C%2520without%2520requiring%250Aany%2520predefined%2520manifold%2520information.%2520We%2520provide%2520a%2520convergence%2520analysis%2520of%2520our%250Aalgorithm.%2520Our%2520findings%2520reveal%2520that%2520the%2520algorithm%2520achieves%2520a%2520convergence%2520rate%250Athat%2520depends%2520solely%2520on%2520the%2520intrinsic%2520dimension%2520of%2520the%2520underlying%2520manifold%252C%250Athereby%2520avoiding%2520the%2520curse%2520of%2520dimensionality%2520associated%2520with%2520the%2520higher%2520ambient%250Adimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-based%20Semi-supervised%20Spectral%20Algorithm%20for%20Regression%20on%0A%20%20Manifolds&entry.906535625=Weichun%20Xia%20and%20Jiaxin%20Jiang%20and%20Lei%20Shi&entry.1292438233=%20%20We%20introduce%20a%20novel%20diffusion-based%20spectral%20algorithm%20to%20tackle%20regression%0Aanalysis%20on%20high-dimensional%20data%2C%20particularly%20data%20embedded%20within%0Alower-dimensional%20manifolds.%20Traditional%20spectral%20algorithms%20often%20fall%20short%0Ain%20such%20contexts%2C%20primarily%20due%20to%20the%20reliance%20on%20predetermined%20kernel%0Afunctions%2C%20which%20inadequately%20address%20the%20complex%20structures%20inherent%20in%0Amanifold-based%20data.%20By%20employing%20graph%20Laplacian%20approximation%2C%20our%20method%0Auses%20the%20local%20estimation%20property%20of%20heat%20kernel%2C%20offering%20an%20adaptive%2C%0Adata-driven%20approach%20to%20overcome%20this%20obstacle.%20Another%20distinct%20advantage%20of%0Aour%20algorithm%20lies%20in%20its%20semi-supervised%20learning%20framework%2C%20enabling%20it%20to%0Afully%20use%20the%20additional%20unlabeled%20data.%20This%20ability%20enhances%20the%20performance%0Aby%20allowing%20the%20algorithm%20to%20dig%20the%20spectrum%20and%20curvature%20of%20the%20data%0Amanifold%2C%20providing%20a%20more%20comprehensive%20understanding%20of%20the%20dataset.%0AMoreover%2C%20our%20algorithm%20performs%20in%20an%20entirely%20data-driven%20manner%2C%20operating%0Adirectly%20within%20the%20intrinsic%20manifold%20structure%20of%20the%20data%2C%20without%20requiring%0Aany%20predefined%20manifold%20information.%20We%20provide%20a%20convergence%20analysis%20of%20our%0Aalgorithm.%20Our%20findings%20reveal%20that%20the%20algorithm%20achieves%20a%20convergence%20rate%0Athat%20depends%20solely%20on%20the%20intrinsic%20dimension%20of%20the%20underlying%20manifold%2C%0Athereby%20avoiding%20the%20curse%20of%20dimensionality%20associated%20with%20the%20higher%20ambient%0Adimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14539v1&entry.124074799=Read"},
{"title": "Liger Kernel: Efficient Triton Kernels for LLM Training", "author": "Pin-Lun Hsu and Yun Dai and Vignesh Kothapalli and Qingquan Song and Shao Tang and Siyu Zhu and Steven Shimizu and Shivam Sahni and Haowen Ning and Yanning Chen", "abstract": "  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n", "link": "http://arxiv.org/abs/2410.10989v2", "date": "2024-10-18", "relevancy": 1.9027, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4559}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Liger%20Kernel%3A%20Efficient%20Triton%20Kernels%20for%20LLM%20Training&body=Title%3A%20Liger%20Kernel%3A%20Efficient%20Triton%20Kernels%20for%20LLM%20Training%0AAuthor%3A%20Pin-Lun%20Hsu%20and%20Yun%20Dai%20and%20Vignesh%20Kothapalli%20and%20Qingquan%20Song%20and%20Shao%20Tang%20and%20Siyu%20Zhu%20and%20Steven%20Shimizu%20and%20Shivam%20Sahni%20and%20Haowen%20Ning%20and%20Yanning%20Chen%0AAbstract%3A%20%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20efficiently%20at%20scale%20presents%20a%0Aformidable%20challenge%2C%20driven%20by%20their%20ever-increasing%20computational%20demands%20and%0Athe%20need%20for%20enhanced%20performance.%20In%20this%20work%2C%20we%20introduce%20Liger-Kernel%2C%20an%0Aopen-sourced%20set%20of%20Triton%20kernels%20developed%20specifically%20for%20LLM%20training.%0AWith%20kernel%20optimization%20techniques%20like%20kernel%20operation%20fusing%20and%20input%0Achunking%2C%20our%20kernels%20achieve%20on%20average%20a%2020%25%20increase%20in%20training%20throughput%0Aand%20a%2060%25%20reduction%20in%20GPU%20memory%20usage%20for%20popular%20LLMs%20compared%20to%0AHuggingFace%20implementations.%20In%20addition%2C%20Liger-Kernel%20is%20designed%20with%0Amodularity%2C%20accessibility%2C%20and%20adaptability%20in%20mind%2C%20catering%20to%20both%20casual%0Aand%20expert%20users.%20Comprehensive%20benchmarks%20and%20integration%20tests%20are%20built%20in%0Ato%20ensure%20compatibility%2C%20performance%2C%20correctness%2C%20and%20convergence%20across%0Adiverse%20computing%20environments%20and%20model%20architectures.%0A%20%20The%20source%20code%20is%20available%20under%20a%20permissive%20license%20at%3A%0Agithub.com/linkedin/Liger-Kernel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiger%2520Kernel%253A%2520Efficient%2520Triton%2520Kernels%2520for%2520LLM%2520Training%26entry.906535625%3DPin-Lun%2520Hsu%2520and%2520Yun%2520Dai%2520and%2520Vignesh%2520Kothapalli%2520and%2520Qingquan%2520Song%2520and%2520Shao%2520Tang%2520and%2520Siyu%2520Zhu%2520and%2520Steven%2520Shimizu%2520and%2520Shivam%2520Sahni%2520and%2520Haowen%2520Ning%2520and%2520Yanning%2520Chen%26entry.1292438233%3D%2520%2520Training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520efficiently%2520at%2520scale%2520presents%2520a%250Aformidable%2520challenge%252C%2520driven%2520by%2520their%2520ever-increasing%2520computational%2520demands%2520and%250Athe%2520need%2520for%2520enhanced%2520performance.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Liger-Kernel%252C%2520an%250Aopen-sourced%2520set%2520of%2520Triton%2520kernels%2520developed%2520specifically%2520for%2520LLM%2520training.%250AWith%2520kernel%2520optimization%2520techniques%2520like%2520kernel%2520operation%2520fusing%2520and%2520input%250Achunking%252C%2520our%2520kernels%2520achieve%2520on%2520average%2520a%252020%2525%2520increase%2520in%2520training%2520throughput%250Aand%2520a%252060%2525%2520reduction%2520in%2520GPU%2520memory%2520usage%2520for%2520popular%2520LLMs%2520compared%2520to%250AHuggingFace%2520implementations.%2520In%2520addition%252C%2520Liger-Kernel%2520is%2520designed%2520with%250Amodularity%252C%2520accessibility%252C%2520and%2520adaptability%2520in%2520mind%252C%2520catering%2520to%2520both%2520casual%250Aand%2520expert%2520users.%2520Comprehensive%2520benchmarks%2520and%2520integration%2520tests%2520are%2520built%2520in%250Ato%2520ensure%2520compatibility%252C%2520performance%252C%2520correctness%252C%2520and%2520convergence%2520across%250Adiverse%2520computing%2520environments%2520and%2520model%2520architectures.%250A%2520%2520The%2520source%2520code%2520is%2520available%2520under%2520a%2520permissive%2520license%2520at%253A%250Agithub.com/linkedin/Liger-Kernel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Liger%20Kernel%3A%20Efficient%20Triton%20Kernels%20for%20LLM%20Training&entry.906535625=Pin-Lun%20Hsu%20and%20Yun%20Dai%20and%20Vignesh%20Kothapalli%20and%20Qingquan%20Song%20and%20Shao%20Tang%20and%20Siyu%20Zhu%20and%20Steven%20Shimizu%20and%20Shivam%20Sahni%20and%20Haowen%20Ning%20and%20Yanning%20Chen&entry.1292438233=%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20efficiently%20at%20scale%20presents%20a%0Aformidable%20challenge%2C%20driven%20by%20their%20ever-increasing%20computational%20demands%20and%0Athe%20need%20for%20enhanced%20performance.%20In%20this%20work%2C%20we%20introduce%20Liger-Kernel%2C%20an%0Aopen-sourced%20set%20of%20Triton%20kernels%20developed%20specifically%20for%20LLM%20training.%0AWith%20kernel%20optimization%20techniques%20like%20kernel%20operation%20fusing%20and%20input%0Achunking%2C%20our%20kernels%20achieve%20on%20average%20a%2020%25%20increase%20in%20training%20throughput%0Aand%20a%2060%25%20reduction%20in%20GPU%20memory%20usage%20for%20popular%20LLMs%20compared%20to%0AHuggingFace%20implementations.%20In%20addition%2C%20Liger-Kernel%20is%20designed%20with%0Amodularity%2C%20accessibility%2C%20and%20adaptability%20in%20mind%2C%20catering%20to%20both%20casual%0Aand%20expert%20users.%20Comprehensive%20benchmarks%20and%20integration%20tests%20are%20built%20in%0Ato%20ensure%20compatibility%2C%20performance%2C%20correctness%2C%20and%20convergence%20across%0Adiverse%20computing%20environments%20and%20model%20architectures.%0A%20%20The%20source%20code%20is%20available%20under%20a%20permissive%20license%20at%3A%0Agithub.com/linkedin/Liger-Kernel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10989v2&entry.124074799=Read"},
{"title": "CELI: Controller-Embedded Language Model Interactions", "author": "Jan-Samuel Wagner and Dave DeCaprio and Abishek Chiffon Muthu Raja and Jonathan M. Holman and Lauren K. Brady and Sky C. Cheung and Hosein Barzekar and Eric Yang and Mark Anthony Martinez II and David Soong and Sriram Sridhar and Han Si and Brandon W. Higgs and Hisham Hamadeh and Scott Ogden", "abstract": "  We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.\n", "link": "http://arxiv.org/abs/2410.14627v1", "date": "2024-10-18", "relevancy": 1.8979, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CELI%3A%20Controller-Embedded%20Language%20Model%20Interactions&body=Title%3A%20CELI%3A%20Controller-Embedded%20Language%20Model%20Interactions%0AAuthor%3A%20Jan-Samuel%20Wagner%20and%20Dave%20DeCaprio%20and%20Abishek%20Chiffon%20Muthu%20Raja%20and%20Jonathan%20M.%20Holman%20and%20Lauren%20K.%20Brady%20and%20Sky%20C.%20Cheung%20and%20Hosein%20Barzekar%20and%20Eric%20Yang%20and%20Mark%20Anthony%20Martinez%20II%20and%20David%20Soong%20and%20Sriram%20Sridhar%20and%20Han%20Si%20and%20Brandon%20W.%20Higgs%20and%20Hisham%20Hamadeh%20and%20Scott%20Ogden%0AAbstract%3A%20%20%20We%20introduce%20Controller-Embedded%20Language%20Model%20Interactions%20%28CELI%29%2C%20a%0Aframework%20that%20integrates%20control%20logic%20directly%20within%20language%20model%20%28LM%29%0Aprompts%2C%20facilitating%20complex%2C%20multi-stage%20task%20execution.%20CELI%20addresses%0Alimitations%20of%20existing%20prompt%20engineering%20and%20workflow%20optimization%20techniques%0Aby%20embedding%20control%20logic%20directly%20within%20the%20operational%20context%20of%20language%0Amodels%2C%20enabling%20dynamic%20adaptation%20to%20evolving%20task%20requirements.%20Our%0Aframework%20transfers%20control%20from%20the%20traditional%20programming%20execution%0Aenvironment%20to%20the%20LMs%2C%20allowing%20them%20to%20autonomously%20manage%20computational%0Aworkflows%20while%20maintaining%20seamless%20interaction%20with%20external%20systems%20and%0Afunctions.%20CELI%20supports%20arbitrary%20function%20calls%20with%20variable%20arguments%2C%0Abridging%20the%20gap%20between%20LMs%27%20adaptive%20reasoning%20capabilities%20and%20conventional%0Asoftware%20paradigms%27%20structured%20control%20mechanisms.%20To%20evaluate%20CELI%27s%0Aversatility%20and%20effectiveness%2C%20we%20conducted%20case%20studies%20in%20two%20distinct%0Adomains%3A%20code%20generation%20%28HumanEval%20benchmark%29%20and%20multi-stage%20content%0Ageneration%20%28Wikipedia-style%20articles%29.%20The%20results%20demonstrate%20notable%0Aperformance%20improvements%20across%20a%20range%20of%20domains.%20CELI%20achieved%20a%204.9%0Apercentage%20point%20improvement%20over%20the%20best%20reported%20score%20of%20the%20baseline%20GPT-4%0Amodel%20on%20the%20HumanEval%20code%20generation%20benchmark.%20In%20multi-stage%20content%0Ageneration%2C%2094.4%25%20of%20CELI-produced%20Wikipedia-style%20articles%20met%20or%20exceeded%0Afirst%20draft%20quality%20when%20optimally%20configured%2C%20with%2044.4%25%20achieving%20high%0Aquality.%20These%20outcomes%20underscore%20CELI%27s%20potential%20for%20optimizing%20AI-driven%0Aworkflows%20across%20diverse%20computational%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCELI%253A%2520Controller-Embedded%2520Language%2520Model%2520Interactions%26entry.906535625%3DJan-Samuel%2520Wagner%2520and%2520Dave%2520DeCaprio%2520and%2520Abishek%2520Chiffon%2520Muthu%2520Raja%2520and%2520Jonathan%2520M.%2520Holman%2520and%2520Lauren%2520K.%2520Brady%2520and%2520Sky%2520C.%2520Cheung%2520and%2520Hosein%2520Barzekar%2520and%2520Eric%2520Yang%2520and%2520Mark%2520Anthony%2520Martinez%2520II%2520and%2520David%2520Soong%2520and%2520Sriram%2520Sridhar%2520and%2520Han%2520Si%2520and%2520Brandon%2520W.%2520Higgs%2520and%2520Hisham%2520Hamadeh%2520and%2520Scott%2520Ogden%26entry.1292438233%3D%2520%2520We%2520introduce%2520Controller-Embedded%2520Language%2520Model%2520Interactions%2520%2528CELI%2529%252C%2520a%250Aframework%2520that%2520integrates%2520control%2520logic%2520directly%2520within%2520language%2520model%2520%2528LM%2529%250Aprompts%252C%2520facilitating%2520complex%252C%2520multi-stage%2520task%2520execution.%2520CELI%2520addresses%250Alimitations%2520of%2520existing%2520prompt%2520engineering%2520and%2520workflow%2520optimization%2520techniques%250Aby%2520embedding%2520control%2520logic%2520directly%2520within%2520the%2520operational%2520context%2520of%2520language%250Amodels%252C%2520enabling%2520dynamic%2520adaptation%2520to%2520evolving%2520task%2520requirements.%2520Our%250Aframework%2520transfers%2520control%2520from%2520the%2520traditional%2520programming%2520execution%250Aenvironment%2520to%2520the%2520LMs%252C%2520allowing%2520them%2520to%2520autonomously%2520manage%2520computational%250Aworkflows%2520while%2520maintaining%2520seamless%2520interaction%2520with%2520external%2520systems%2520and%250Afunctions.%2520CELI%2520supports%2520arbitrary%2520function%2520calls%2520with%2520variable%2520arguments%252C%250Abridging%2520the%2520gap%2520between%2520LMs%2527%2520adaptive%2520reasoning%2520capabilities%2520and%2520conventional%250Asoftware%2520paradigms%2527%2520structured%2520control%2520mechanisms.%2520To%2520evaluate%2520CELI%2527s%250Aversatility%2520and%2520effectiveness%252C%2520we%2520conducted%2520case%2520studies%2520in%2520two%2520distinct%250Adomains%253A%2520code%2520generation%2520%2528HumanEval%2520benchmark%2529%2520and%2520multi-stage%2520content%250Ageneration%2520%2528Wikipedia-style%2520articles%2529.%2520The%2520results%2520demonstrate%2520notable%250Aperformance%2520improvements%2520across%2520a%2520range%2520of%2520domains.%2520CELI%2520achieved%2520a%25204.9%250Apercentage%2520point%2520improvement%2520over%2520the%2520best%2520reported%2520score%2520of%2520the%2520baseline%2520GPT-4%250Amodel%2520on%2520the%2520HumanEval%2520code%2520generation%2520benchmark.%2520In%2520multi-stage%2520content%250Ageneration%252C%252094.4%2525%2520of%2520CELI-produced%2520Wikipedia-style%2520articles%2520met%2520or%2520exceeded%250Afirst%2520draft%2520quality%2520when%2520optimally%2520configured%252C%2520with%252044.4%2525%2520achieving%2520high%250Aquality.%2520These%2520outcomes%2520underscore%2520CELI%2527s%2520potential%2520for%2520optimizing%2520AI-driven%250Aworkflows%2520across%2520diverse%2520computational%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CELI%3A%20Controller-Embedded%20Language%20Model%20Interactions&entry.906535625=Jan-Samuel%20Wagner%20and%20Dave%20DeCaprio%20and%20Abishek%20Chiffon%20Muthu%20Raja%20and%20Jonathan%20M.%20Holman%20and%20Lauren%20K.%20Brady%20and%20Sky%20C.%20Cheung%20and%20Hosein%20Barzekar%20and%20Eric%20Yang%20and%20Mark%20Anthony%20Martinez%20II%20and%20David%20Soong%20and%20Sriram%20Sridhar%20and%20Han%20Si%20and%20Brandon%20W.%20Higgs%20and%20Hisham%20Hamadeh%20and%20Scott%20Ogden&entry.1292438233=%20%20We%20introduce%20Controller-Embedded%20Language%20Model%20Interactions%20%28CELI%29%2C%20a%0Aframework%20that%20integrates%20control%20logic%20directly%20within%20language%20model%20%28LM%29%0Aprompts%2C%20facilitating%20complex%2C%20multi-stage%20task%20execution.%20CELI%20addresses%0Alimitations%20of%20existing%20prompt%20engineering%20and%20workflow%20optimization%20techniques%0Aby%20embedding%20control%20logic%20directly%20within%20the%20operational%20context%20of%20language%0Amodels%2C%20enabling%20dynamic%20adaptation%20to%20evolving%20task%20requirements.%20Our%0Aframework%20transfers%20control%20from%20the%20traditional%20programming%20execution%0Aenvironment%20to%20the%20LMs%2C%20allowing%20them%20to%20autonomously%20manage%20computational%0Aworkflows%20while%20maintaining%20seamless%20interaction%20with%20external%20systems%20and%0Afunctions.%20CELI%20supports%20arbitrary%20function%20calls%20with%20variable%20arguments%2C%0Abridging%20the%20gap%20between%20LMs%27%20adaptive%20reasoning%20capabilities%20and%20conventional%0Asoftware%20paradigms%27%20structured%20control%20mechanisms.%20To%20evaluate%20CELI%27s%0Aversatility%20and%20effectiveness%2C%20we%20conducted%20case%20studies%20in%20two%20distinct%0Adomains%3A%20code%20generation%20%28HumanEval%20benchmark%29%20and%20multi-stage%20content%0Ageneration%20%28Wikipedia-style%20articles%29.%20The%20results%20demonstrate%20notable%0Aperformance%20improvements%20across%20a%20range%20of%20domains.%20CELI%20achieved%20a%204.9%0Apercentage%20point%20improvement%20over%20the%20best%20reported%20score%20of%20the%20baseline%20GPT-4%0Amodel%20on%20the%20HumanEval%20code%20generation%20benchmark.%20In%20multi-stage%20content%0Ageneration%2C%2094.4%25%20of%20CELI-produced%20Wikipedia-style%20articles%20met%20or%20exceeded%0Afirst%20draft%20quality%20when%20optimally%20configured%2C%20with%2044.4%25%20achieving%20high%0Aquality.%20These%20outcomes%20underscore%20CELI%27s%20potential%20for%20optimizing%20AI-driven%0Aworkflows%20across%20diverse%20computational%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14627v1&entry.124074799=Read"},
{"title": "Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing", "author": "Gabriel Arpino and Xiaoqi Liu and Julia Gontarek and Ramji Venkataramanan", "abstract": "  We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression.\n", "link": "http://arxiv.org/abs/2404.07864v2", "date": "2024-10-18", "relevancy": 1.892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.48}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20Change%20Points%20in%20High-Dimensional%20Regression%20via%20Approximate%0A%20%20Message%20Passing&body=Title%3A%20Inferring%20Change%20Points%20in%20High-Dimensional%20Regression%20via%20Approximate%0A%20%20Message%20Passing%0AAuthor%3A%20Gabriel%20Arpino%20and%20Xiaoqi%20Liu%20and%20Julia%20Gontarek%20and%20Ramji%20Venkataramanan%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20localizing%20change%20points%20in%20a%20generalized%20linear%0Amodel%20%28GLM%29%2C%20a%20model%20that%20covers%20many%20widely%20studied%20problems%20in%20statistical%0Alearning%20including%20linear%2C%20logistic%2C%20and%20rectified%20linear%20regression.%20We%0Apropose%20a%20novel%20and%20computationally%20efficient%20Approximate%20Message%20Passing%20%28AMP%29%0Aalgorithm%20for%20estimating%20both%20the%20signals%20and%20the%20change%20point%20locations%2C%20and%0Arigorously%20characterize%20its%20performance%20in%20the%20high-dimensional%20limit%20where%20the%0Anumber%20of%20parameters%20%24p%24%20is%20proportional%20to%20the%20number%20of%20samples%20%24n%24.%20This%0Acharacterization%20is%20in%20terms%20of%20a%20state%20evolution%20recursion%2C%20which%20allows%20us%20to%0Aprecisely%20compute%20performance%20measures%20such%20as%20the%20asymptotic%20Hausdorff%20error%0Aof%20our%20change%20point%20estimates%2C%20and%20allows%20us%20to%20tailor%20the%20algorithm%20to%20take%0Aadvantage%20of%20any%20prior%20structural%20information%20on%20the%20signals%20and%20change%20points.%0AMoreover%2C%20we%20show%20how%20our%20AMP%20iterates%20can%20be%20used%20to%20efficiently%20compute%20a%0ABayesian%20posterior%20distribution%20over%20the%20change%20point%20locations%20in%20the%0Ahigh-dimensional%20limit.%20We%20validate%20our%20theory%20via%20numerical%20experiments%2C%20and%0Ademonstrate%20the%20favorable%20performance%20of%20our%20estimators%20on%20both%20synthetic%20and%0Areal%20data%20in%20the%20settings%20of%20linear%2C%20logistic%2C%20and%20rectified%20linear%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520Change%2520Points%2520in%2520High-Dimensional%2520Regression%2520via%2520Approximate%250A%2520%2520Message%2520Passing%26entry.906535625%3DGabriel%2520Arpino%2520and%2520Xiaoqi%2520Liu%2520and%2520Julia%2520Gontarek%2520and%2520Ramji%2520Venkataramanan%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520localizing%2520change%2520points%2520in%2520a%2520generalized%2520linear%250Amodel%2520%2528GLM%2529%252C%2520a%2520model%2520that%2520covers%2520many%2520widely%2520studied%2520problems%2520in%2520statistical%250Alearning%2520including%2520linear%252C%2520logistic%252C%2520and%2520rectified%2520linear%2520regression.%2520We%250Apropose%2520a%2520novel%2520and%2520computationally%2520efficient%2520Approximate%2520Message%2520Passing%2520%2528AMP%2529%250Aalgorithm%2520for%2520estimating%2520both%2520the%2520signals%2520and%2520the%2520change%2520point%2520locations%252C%2520and%250Arigorously%2520characterize%2520its%2520performance%2520in%2520the%2520high-dimensional%2520limit%2520where%2520the%250Anumber%2520of%2520parameters%2520%2524p%2524%2520is%2520proportional%2520to%2520the%2520number%2520of%2520samples%2520%2524n%2524.%2520This%250Acharacterization%2520is%2520in%2520terms%2520of%2520a%2520state%2520evolution%2520recursion%252C%2520which%2520allows%2520us%2520to%250Aprecisely%2520compute%2520performance%2520measures%2520such%2520as%2520the%2520asymptotic%2520Hausdorff%2520error%250Aof%2520our%2520change%2520point%2520estimates%252C%2520and%2520allows%2520us%2520to%2520tailor%2520the%2520algorithm%2520to%2520take%250Aadvantage%2520of%2520any%2520prior%2520structural%2520information%2520on%2520the%2520signals%2520and%2520change%2520points.%250AMoreover%252C%2520we%2520show%2520how%2520our%2520AMP%2520iterates%2520can%2520be%2520used%2520to%2520efficiently%2520compute%2520a%250ABayesian%2520posterior%2520distribution%2520over%2520the%2520change%2520point%2520locations%2520in%2520the%250Ahigh-dimensional%2520limit.%2520We%2520validate%2520our%2520theory%2520via%2520numerical%2520experiments%252C%2520and%250Ademonstrate%2520the%2520favorable%2520performance%2520of%2520our%2520estimators%2520on%2520both%2520synthetic%2520and%250Areal%2520data%2520in%2520the%2520settings%2520of%2520linear%252C%2520logistic%252C%2520and%2520rectified%2520linear%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20Change%20Points%20in%20High-Dimensional%20Regression%20via%20Approximate%0A%20%20Message%20Passing&entry.906535625=Gabriel%20Arpino%20and%20Xiaoqi%20Liu%20and%20Julia%20Gontarek%20and%20Ramji%20Venkataramanan&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20localizing%20change%20points%20in%20a%20generalized%20linear%0Amodel%20%28GLM%29%2C%20a%20model%20that%20covers%20many%20widely%20studied%20problems%20in%20statistical%0Alearning%20including%20linear%2C%20logistic%2C%20and%20rectified%20linear%20regression.%20We%0Apropose%20a%20novel%20and%20computationally%20efficient%20Approximate%20Message%20Passing%20%28AMP%29%0Aalgorithm%20for%20estimating%20both%20the%20signals%20and%20the%20change%20point%20locations%2C%20and%0Arigorously%20characterize%20its%20performance%20in%20the%20high-dimensional%20limit%20where%20the%0Anumber%20of%20parameters%20%24p%24%20is%20proportional%20to%20the%20number%20of%20samples%20%24n%24.%20This%0Acharacterization%20is%20in%20terms%20of%20a%20state%20evolution%20recursion%2C%20which%20allows%20us%20to%0Aprecisely%20compute%20performance%20measures%20such%20as%20the%20asymptotic%20Hausdorff%20error%0Aof%20our%20change%20point%20estimates%2C%20and%20allows%20us%20to%20tailor%20the%20algorithm%20to%20take%0Aadvantage%20of%20any%20prior%20structural%20information%20on%20the%20signals%20and%20change%20points.%0AMoreover%2C%20we%20show%20how%20our%20AMP%20iterates%20can%20be%20used%20to%20efficiently%20compute%20a%0ABayesian%20posterior%20distribution%20over%20the%20change%20point%20locations%20in%20the%0Ahigh-dimensional%20limit.%20We%20validate%20our%20theory%20via%20numerical%20experiments%2C%20and%0Ademonstrate%20the%20favorable%20performance%20of%20our%20estimators%20on%20both%20synthetic%20and%0Areal%20data%20in%20the%20settings%20of%20linear%2C%20logistic%2C%20and%20rectified%20linear%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07864v2&entry.124074799=Read"},
{"title": "System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam", "author": "Joost de Winter and Dimitra Dodou and Yke Bauke Eisma", "abstract": "  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n", "link": "http://arxiv.org/abs/2410.07114v2", "date": "2024-10-18", "relevancy": 1.8621, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%202%20thinking%20in%20OpenAI%27s%20o1-preview%20model%3A%20Near-perfect%20performance%0A%20%20on%20a%20mathematics%20exam&body=Title%3A%20System%202%20thinking%20in%20OpenAI%27s%20o1-preview%20model%3A%20Near-perfect%20performance%0A%20%20on%20a%20mathematics%20exam%0AAuthor%3A%20Joost%20de%20Winter%20and%20Dimitra%20Dodou%20and%20Yke%20Bauke%20Eisma%0AAbstract%3A%20%20%20The%20processes%20underlying%20human%20cognition%20are%20often%20divided%20into%20System%201%2C%0Awhich%20involves%20fast%2C%20intuitive%20thinking%2C%20and%20System%202%2C%20which%20involves%20slow%2C%0Adeliberate%20reasoning.%20Previously%2C%20large%20language%20models%20were%20criticized%20for%0Alacking%20the%20deeper%2C%20more%20analytical%20capabilities%20of%20System%202.%20In%20September%0A2024%2C%20OpenAI%20introduced%20the%20o1%20model%20series%2C%20designed%20to%20handle%20System%202-like%0Areasoning.%20While%20OpenAI%27s%20benchmarks%20are%20promising%2C%20independent%20validation%20is%0Astill%20needed.%20In%20this%20study%2C%20we%20tested%20the%20o1-preview%20model%20twice%20on%20the%20Dutch%0A%27Mathematics%20B%27%20final%20exam.%20It%20scored%20a%20near-perfect%2076%20and%2074%20out%20of%2076%0Apoints.%20For%20context%2C%20only%2024%20out%20of%2016%2C414%20students%20in%20the%20Netherlands%20achieved%0Aa%20perfect%20score.%20By%20comparison%2C%20the%20GPT-4o%20model%20scored%2066%20and%2062%20out%20of%2076%2C%0Awell%20above%20the%20Dutch%20average%20of%2040.63%20points.%20Neither%20model%20had%20access%20to%20the%0Aexam%20figures.%20Since%20there%20was%20a%20risk%20of%20model%20contamination%20%28i.e.%2C%20the%0Aknowledge%20cutoff%20of%20o1-preview%20and%20GPT-4o%20was%20after%20the%20exam%20was%20published%0Aonline%29%2C%20we%20repeated%20the%20procedure%20with%20a%20new%20Mathematics%20B%20exam%20that%20was%0Apublished%20after%20the%20cutoff%20date.%20The%20results%20again%20indicated%20that%20o1-preview%0Aperformed%20strongly%20%2897.8th%20percentile%29%2C%20which%20suggests%20that%20contamination%20was%0Anot%20a%20factor.%20We%20also%20show%20that%20there%20is%20some%20variability%20in%20the%20output%20of%0Ao1-preview%2C%20which%20means%20that%20sometimes%20there%20is%20%27luck%27%20%28the%20answer%20is%20correct%29%0Aor%20%27bad%20luck%27%20%28the%20output%20has%20diverged%20into%20something%20that%20is%20incorrect%29.%20We%0Ademonstrate%20that%20a%20self-consistency%20approach%2C%20where%20repeated%20prompts%20are%20given%0Aand%20the%20most%20common%20answer%20is%20selected%2C%20is%20a%20useful%20strategy%20for%20identifying%0Athe%20correct%20answer.%20It%20is%20concluded%20that%20while%20OpenAI%27s%20new%20model%20series%20holds%0Agreat%20potential%2C%20certain%20risks%20must%20be%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%25202%2520thinking%2520in%2520OpenAI%2527s%2520o1-preview%2520model%253A%2520Near-perfect%2520performance%250A%2520%2520on%2520a%2520mathematics%2520exam%26entry.906535625%3DJoost%2520de%2520Winter%2520and%2520Dimitra%2520Dodou%2520and%2520Yke%2520Bauke%2520Eisma%26entry.1292438233%3D%2520%2520The%2520processes%2520underlying%2520human%2520cognition%2520are%2520often%2520divided%2520into%2520System%25201%252C%250Awhich%2520involves%2520fast%252C%2520intuitive%2520thinking%252C%2520and%2520System%25202%252C%2520which%2520involves%2520slow%252C%250Adeliberate%2520reasoning.%2520Previously%252C%2520large%2520language%2520models%2520were%2520criticized%2520for%250Alacking%2520the%2520deeper%252C%2520more%2520analytical%2520capabilities%2520of%2520System%25202.%2520In%2520September%250A2024%252C%2520OpenAI%2520introduced%2520the%2520o1%2520model%2520series%252C%2520designed%2520to%2520handle%2520System%25202-like%250Areasoning.%2520While%2520OpenAI%2527s%2520benchmarks%2520are%2520promising%252C%2520independent%2520validation%2520is%250Astill%2520needed.%2520In%2520this%2520study%252C%2520we%2520tested%2520the%2520o1-preview%2520model%2520twice%2520on%2520the%2520Dutch%250A%2527Mathematics%2520B%2527%2520final%2520exam.%2520It%2520scored%2520a%2520near-perfect%252076%2520and%252074%2520out%2520of%252076%250Apoints.%2520For%2520context%252C%2520only%252024%2520out%2520of%252016%252C414%2520students%2520in%2520the%2520Netherlands%2520achieved%250Aa%2520perfect%2520score.%2520By%2520comparison%252C%2520the%2520GPT-4o%2520model%2520scored%252066%2520and%252062%2520out%2520of%252076%252C%250Awell%2520above%2520the%2520Dutch%2520average%2520of%252040.63%2520points.%2520Neither%2520model%2520had%2520access%2520to%2520the%250Aexam%2520figures.%2520Since%2520there%2520was%2520a%2520risk%2520of%2520model%2520contamination%2520%2528i.e.%252C%2520the%250Aknowledge%2520cutoff%2520of%2520o1-preview%2520and%2520GPT-4o%2520was%2520after%2520the%2520exam%2520was%2520published%250Aonline%2529%252C%2520we%2520repeated%2520the%2520procedure%2520with%2520a%2520new%2520Mathematics%2520B%2520exam%2520that%2520was%250Apublished%2520after%2520the%2520cutoff%2520date.%2520The%2520results%2520again%2520indicated%2520that%2520o1-preview%250Aperformed%2520strongly%2520%252897.8th%2520percentile%2529%252C%2520which%2520suggests%2520that%2520contamination%2520was%250Anot%2520a%2520factor.%2520We%2520also%2520show%2520that%2520there%2520is%2520some%2520variability%2520in%2520the%2520output%2520of%250Ao1-preview%252C%2520which%2520means%2520that%2520sometimes%2520there%2520is%2520%2527luck%2527%2520%2528the%2520answer%2520is%2520correct%2529%250Aor%2520%2527bad%2520luck%2527%2520%2528the%2520output%2520has%2520diverged%2520into%2520something%2520that%2520is%2520incorrect%2529.%2520We%250Ademonstrate%2520that%2520a%2520self-consistency%2520approach%252C%2520where%2520repeated%2520prompts%2520are%2520given%250Aand%2520the%2520most%2520common%2520answer%2520is%2520selected%252C%2520is%2520a%2520useful%2520strategy%2520for%2520identifying%250Athe%2520correct%2520answer.%2520It%2520is%2520concluded%2520that%2520while%2520OpenAI%2527s%2520new%2520model%2520series%2520holds%250Agreat%2520potential%252C%2520certain%2520risks%2520must%2520be%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%202%20thinking%20in%20OpenAI%27s%20o1-preview%20model%3A%20Near-perfect%20performance%0A%20%20on%20a%20mathematics%20exam&entry.906535625=Joost%20de%20Winter%20and%20Dimitra%20Dodou%20and%20Yke%20Bauke%20Eisma&entry.1292438233=%20%20The%20processes%20underlying%20human%20cognition%20are%20often%20divided%20into%20System%201%2C%0Awhich%20involves%20fast%2C%20intuitive%20thinking%2C%20and%20System%202%2C%20which%20involves%20slow%2C%0Adeliberate%20reasoning.%20Previously%2C%20large%20language%20models%20were%20criticized%20for%0Alacking%20the%20deeper%2C%20more%20analytical%20capabilities%20of%20System%202.%20In%20September%0A2024%2C%20OpenAI%20introduced%20the%20o1%20model%20series%2C%20designed%20to%20handle%20System%202-like%0Areasoning.%20While%20OpenAI%27s%20benchmarks%20are%20promising%2C%20independent%20validation%20is%0Astill%20needed.%20In%20this%20study%2C%20we%20tested%20the%20o1-preview%20model%20twice%20on%20the%20Dutch%0A%27Mathematics%20B%27%20final%20exam.%20It%20scored%20a%20near-perfect%2076%20and%2074%20out%20of%2076%0Apoints.%20For%20context%2C%20only%2024%20out%20of%2016%2C414%20students%20in%20the%20Netherlands%20achieved%0Aa%20perfect%20score.%20By%20comparison%2C%20the%20GPT-4o%20model%20scored%2066%20and%2062%20out%20of%2076%2C%0Awell%20above%20the%20Dutch%20average%20of%2040.63%20points.%20Neither%20model%20had%20access%20to%20the%0Aexam%20figures.%20Since%20there%20was%20a%20risk%20of%20model%20contamination%20%28i.e.%2C%20the%0Aknowledge%20cutoff%20of%20o1-preview%20and%20GPT-4o%20was%20after%20the%20exam%20was%20published%0Aonline%29%2C%20we%20repeated%20the%20procedure%20with%20a%20new%20Mathematics%20B%20exam%20that%20was%0Apublished%20after%20the%20cutoff%20date.%20The%20results%20again%20indicated%20that%20o1-preview%0Aperformed%20strongly%20%2897.8th%20percentile%29%2C%20which%20suggests%20that%20contamination%20was%0Anot%20a%20factor.%20We%20also%20show%20that%20there%20is%20some%20variability%20in%20the%20output%20of%0Ao1-preview%2C%20which%20means%20that%20sometimes%20there%20is%20%27luck%27%20%28the%20answer%20is%20correct%29%0Aor%20%27bad%20luck%27%20%28the%20output%20has%20diverged%20into%20something%20that%20is%20incorrect%29.%20We%0Ademonstrate%20that%20a%20self-consistency%20approach%2C%20where%20repeated%20prompts%20are%20given%0Aand%20the%20most%20common%20answer%20is%20selected%2C%20is%20a%20useful%20strategy%20for%20identifying%0Athe%20correct%20answer.%20It%20is%20concluded%20that%20while%20OpenAI%27s%20new%20model%20series%20holds%0Agreat%20potential%2C%20certain%20risks%20must%20be%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07114v2&entry.124074799=Read"},
{"title": "Real-time Fake News from Adversarial Feedback", "author": "Sanxing Chen and Yukun Huang and Bhuwan Dhingra", "abstract": "  We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.\n", "link": "http://arxiv.org/abs/2410.14651v1", "date": "2024-10-18", "relevancy": 1.8548, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4704}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Fake%20News%20from%20Adversarial%20Feedback&body=Title%3A%20Real-time%20Fake%20News%20from%20Adversarial%20Feedback%0AAuthor%3A%20Sanxing%20Chen%20and%20Yukun%20Huang%20and%20Bhuwan%20Dhingra%0AAbstract%3A%20%20%20We%20show%20that%20existing%20evaluations%20for%20fake%20news%20detection%20based%20on%0Aconventional%20sources%2C%20such%20as%20claims%20on%20fact-checking%20websites%2C%20result%20in%20an%0Aincreasing%20accuracy%20over%20time%20for%20LLM-based%20detectors%20--%20even%20after%20their%0Aknowledge%20cutoffs.%20This%20suggests%20that%20recent%20popular%20political%20claims%2C%20which%0Aform%20the%20majority%20of%20fake%20news%20on%20such%20sources%2C%20are%20easily%20classified%20using%0Asurface-level%20shallow%20patterns.%20Instead%2C%20we%20argue%20that%20a%20proper%20fake%20news%0Adetection%20dataset%20should%20test%20a%20model%27s%20ability%20to%20reason%20factually%20about%20the%0Acurrent%20world%20by%20retrieving%20and%20reading%20related%20evidence.%20To%20this%20end%2C%20we%0Adevelop%20a%20novel%20pipeline%20that%20leverages%20natural%20language%20feedback%20from%20a%0ARAG-based%20detector%20to%20iteratively%20modify%20real-time%20news%20into%20deceptive%20fake%0Anews%20that%20challenges%20LLMs.%20Our%20iterative%20rewrite%20decreases%20the%20binary%0Aclassification%20AUC%20by%20an%20absolute%2017.5%20percent%20for%20a%20strong%20RAG%20GPT-4o%0Adetector.%20Our%20experiments%20reveal%20the%20important%20role%20of%20RAG%20in%20both%20detecting%0Aand%20generating%20fake%20news%2C%20as%20retrieval-free%20LLM%20detectors%20are%20vulnerable%20to%0Aunseen%20events%20and%20adversarial%20attacks%2C%20while%20feedback%20from%20RAG%20detection%20helps%0Adiscover%20more%20deceitful%20patterns%20in%20fake%20news.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Fake%2520News%2520from%2520Adversarial%2520Feedback%26entry.906535625%3DSanxing%2520Chen%2520and%2520Yukun%2520Huang%2520and%2520Bhuwan%2520Dhingra%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520existing%2520evaluations%2520for%2520fake%2520news%2520detection%2520based%2520on%250Aconventional%2520sources%252C%2520such%2520as%2520claims%2520on%2520fact-checking%2520websites%252C%2520result%2520in%2520an%250Aincreasing%2520accuracy%2520over%2520time%2520for%2520LLM-based%2520detectors%2520--%2520even%2520after%2520their%250Aknowledge%2520cutoffs.%2520This%2520suggests%2520that%2520recent%2520popular%2520political%2520claims%252C%2520which%250Aform%2520the%2520majority%2520of%2520fake%2520news%2520on%2520such%2520sources%252C%2520are%2520easily%2520classified%2520using%250Asurface-level%2520shallow%2520patterns.%2520Instead%252C%2520we%2520argue%2520that%2520a%2520proper%2520fake%2520news%250Adetection%2520dataset%2520should%2520test%2520a%2520model%2527s%2520ability%2520to%2520reason%2520factually%2520about%2520the%250Acurrent%2520world%2520by%2520retrieving%2520and%2520reading%2520related%2520evidence.%2520To%2520this%2520end%252C%2520we%250Adevelop%2520a%2520novel%2520pipeline%2520that%2520leverages%2520natural%2520language%2520feedback%2520from%2520a%250ARAG-based%2520detector%2520to%2520iteratively%2520modify%2520real-time%2520news%2520into%2520deceptive%2520fake%250Anews%2520that%2520challenges%2520LLMs.%2520Our%2520iterative%2520rewrite%2520decreases%2520the%2520binary%250Aclassification%2520AUC%2520by%2520an%2520absolute%252017.5%2520percent%2520for%2520a%2520strong%2520RAG%2520GPT-4o%250Adetector.%2520Our%2520experiments%2520reveal%2520the%2520important%2520role%2520of%2520RAG%2520in%2520both%2520detecting%250Aand%2520generating%2520fake%2520news%252C%2520as%2520retrieval-free%2520LLM%2520detectors%2520are%2520vulnerable%2520to%250Aunseen%2520events%2520and%2520adversarial%2520attacks%252C%2520while%2520feedback%2520from%2520RAG%2520detection%2520helps%250Adiscover%2520more%2520deceitful%2520patterns%2520in%2520fake%2520news.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Fake%20News%20from%20Adversarial%20Feedback&entry.906535625=Sanxing%20Chen%20and%20Yukun%20Huang%20and%20Bhuwan%20Dhingra&entry.1292438233=%20%20We%20show%20that%20existing%20evaluations%20for%20fake%20news%20detection%20based%20on%0Aconventional%20sources%2C%20such%20as%20claims%20on%20fact-checking%20websites%2C%20result%20in%20an%0Aincreasing%20accuracy%20over%20time%20for%20LLM-based%20detectors%20--%20even%20after%20their%0Aknowledge%20cutoffs.%20This%20suggests%20that%20recent%20popular%20political%20claims%2C%20which%0Aform%20the%20majority%20of%20fake%20news%20on%20such%20sources%2C%20are%20easily%20classified%20using%0Asurface-level%20shallow%20patterns.%20Instead%2C%20we%20argue%20that%20a%20proper%20fake%20news%0Adetection%20dataset%20should%20test%20a%20model%27s%20ability%20to%20reason%20factually%20about%20the%0Acurrent%20world%20by%20retrieving%20and%20reading%20related%20evidence.%20To%20this%20end%2C%20we%0Adevelop%20a%20novel%20pipeline%20that%20leverages%20natural%20language%20feedback%20from%20a%0ARAG-based%20detector%20to%20iteratively%20modify%20real-time%20news%20into%20deceptive%20fake%0Anews%20that%20challenges%20LLMs.%20Our%20iterative%20rewrite%20decreases%20the%20binary%0Aclassification%20AUC%20by%20an%20absolute%2017.5%20percent%20for%20a%20strong%20RAG%20GPT-4o%0Adetector.%20Our%20experiments%20reveal%20the%20important%20role%20of%20RAG%20in%20both%20detecting%0Aand%20generating%20fake%20news%2C%20as%20retrieval-free%20LLM%20detectors%20are%20vulnerable%20to%0Aunseen%20events%20and%20adversarial%20attacks%2C%20while%20feedback%20from%20RAG%20detection%20helps%0Adiscover%20more%20deceitful%20patterns%20in%20fake%20news.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14651v1&entry.124074799=Read"},
{"title": "Temporal Fair Division of Indivisible Items", "author": "Edith Elkind and Alexander Lam and Mohamad Latifian and Tzeh Yuan Neoh and Nicholas Teh", "abstract": "  We study a fair division model where indivisible items arrive sequentially,\nand must be allocated immediately and irrevocably. Previous work on online fair\ndivision has shown impossibility results in achieving approximate envy-freeness\nunder these constraints. In contrast, we consider an informed setting where the\nalgorithm has complete knowledge of future items, and aim to ensure that the\ncumulative allocation at each round satisfies approximate envy-freeness --\nwhich we define as temporal envy-freeness up to one item (TEF1). We focus on\nsettings where items can be exclusively goods or exclusively chores. For goods,\nwhile TEF1 allocations may not always exist, we identify several special cases\nwhere they do -- two agents, two item types, generalized binary valuations,\nunimodal preferences -- and provide polynomial-time algorithms for these cases.\nWe also prove that determining the existence of a TEF1 allocation is NP-hard.\nFor chores, we establish analogous results for the special cases, but present a\nslightly weaker intractability result. We also establish the incompatibility\nbetween TEF1 and Pareto-optimality, with the implication that it is intractable\nto find a TEF1 allocation that maximizes any $p$-mean welfare, even for two\nagents.\n", "link": "http://arxiv.org/abs/2410.14593v1", "date": "2024-10-18", "relevancy": 1.8543, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3796}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3672}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Fair%20Division%20of%20Indivisible%20Items&body=Title%3A%20Temporal%20Fair%20Division%20of%20Indivisible%20Items%0AAuthor%3A%20Edith%20Elkind%20and%20Alexander%20Lam%20and%20Mohamad%20Latifian%20and%20Tzeh%20Yuan%20Neoh%20and%20Nicholas%20Teh%0AAbstract%3A%20%20%20We%20study%20a%20fair%20division%20model%20where%20indivisible%20items%20arrive%20sequentially%2C%0Aand%20must%20be%20allocated%20immediately%20and%20irrevocably.%20Previous%20work%20on%20online%20fair%0Adivision%20has%20shown%20impossibility%20results%20in%20achieving%20approximate%20envy-freeness%0Aunder%20these%20constraints.%20In%20contrast%2C%20we%20consider%20an%20informed%20setting%20where%20the%0Aalgorithm%20has%20complete%20knowledge%20of%20future%20items%2C%20and%20aim%20to%20ensure%20that%20the%0Acumulative%20allocation%20at%20each%20round%20satisfies%20approximate%20envy-freeness%20--%0Awhich%20we%20define%20as%20temporal%20envy-freeness%20up%20to%20one%20item%20%28TEF1%29.%20We%20focus%20on%0Asettings%20where%20items%20can%20be%20exclusively%20goods%20or%20exclusively%20chores.%20For%20goods%2C%0Awhile%20TEF1%20allocations%20may%20not%20always%20exist%2C%20we%20identify%20several%20special%20cases%0Awhere%20they%20do%20--%20two%20agents%2C%20two%20item%20types%2C%20generalized%20binary%20valuations%2C%0Aunimodal%20preferences%20--%20and%20provide%20polynomial-time%20algorithms%20for%20these%20cases.%0AWe%20also%20prove%20that%20determining%20the%20existence%20of%20a%20TEF1%20allocation%20is%20NP-hard.%0AFor%20chores%2C%20we%20establish%20analogous%20results%20for%20the%20special%20cases%2C%20but%20present%20a%0Aslightly%20weaker%20intractability%20result.%20We%20also%20establish%20the%20incompatibility%0Abetween%20TEF1%20and%20Pareto-optimality%2C%20with%20the%20implication%20that%20it%20is%20intractable%0Ato%20find%20a%20TEF1%20allocation%20that%20maximizes%20any%20%24p%24-mean%20welfare%2C%20even%20for%20two%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Fair%2520Division%2520of%2520Indivisible%2520Items%26entry.906535625%3DEdith%2520Elkind%2520and%2520Alexander%2520Lam%2520and%2520Mohamad%2520Latifian%2520and%2520Tzeh%2520Yuan%2520Neoh%2520and%2520Nicholas%2520Teh%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520fair%2520division%2520model%2520where%2520indivisible%2520items%2520arrive%2520sequentially%252C%250Aand%2520must%2520be%2520allocated%2520immediately%2520and%2520irrevocably.%2520Previous%2520work%2520on%2520online%2520fair%250Adivision%2520has%2520shown%2520impossibility%2520results%2520in%2520achieving%2520approximate%2520envy-freeness%250Aunder%2520these%2520constraints.%2520In%2520contrast%252C%2520we%2520consider%2520an%2520informed%2520setting%2520where%2520the%250Aalgorithm%2520has%2520complete%2520knowledge%2520of%2520future%2520items%252C%2520and%2520aim%2520to%2520ensure%2520that%2520the%250Acumulative%2520allocation%2520at%2520each%2520round%2520satisfies%2520approximate%2520envy-freeness%2520--%250Awhich%2520we%2520define%2520as%2520temporal%2520envy-freeness%2520up%2520to%2520one%2520item%2520%2528TEF1%2529.%2520We%2520focus%2520on%250Asettings%2520where%2520items%2520can%2520be%2520exclusively%2520goods%2520or%2520exclusively%2520chores.%2520For%2520goods%252C%250Awhile%2520TEF1%2520allocations%2520may%2520not%2520always%2520exist%252C%2520we%2520identify%2520several%2520special%2520cases%250Awhere%2520they%2520do%2520--%2520two%2520agents%252C%2520two%2520item%2520types%252C%2520generalized%2520binary%2520valuations%252C%250Aunimodal%2520preferences%2520--%2520and%2520provide%2520polynomial-time%2520algorithms%2520for%2520these%2520cases.%250AWe%2520also%2520prove%2520that%2520determining%2520the%2520existence%2520of%2520a%2520TEF1%2520allocation%2520is%2520NP-hard.%250AFor%2520chores%252C%2520we%2520establish%2520analogous%2520results%2520for%2520the%2520special%2520cases%252C%2520but%2520present%2520a%250Aslightly%2520weaker%2520intractability%2520result.%2520We%2520also%2520establish%2520the%2520incompatibility%250Abetween%2520TEF1%2520and%2520Pareto-optimality%252C%2520with%2520the%2520implication%2520that%2520it%2520is%2520intractable%250Ato%2520find%2520a%2520TEF1%2520allocation%2520that%2520maximizes%2520any%2520%2524p%2524-mean%2520welfare%252C%2520even%2520for%2520two%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Fair%20Division%20of%20Indivisible%20Items&entry.906535625=Edith%20Elkind%20and%20Alexander%20Lam%20and%20Mohamad%20Latifian%20and%20Tzeh%20Yuan%20Neoh%20and%20Nicholas%20Teh&entry.1292438233=%20%20We%20study%20a%20fair%20division%20model%20where%20indivisible%20items%20arrive%20sequentially%2C%0Aand%20must%20be%20allocated%20immediately%20and%20irrevocably.%20Previous%20work%20on%20online%20fair%0Adivision%20has%20shown%20impossibility%20results%20in%20achieving%20approximate%20envy-freeness%0Aunder%20these%20constraints.%20In%20contrast%2C%20we%20consider%20an%20informed%20setting%20where%20the%0Aalgorithm%20has%20complete%20knowledge%20of%20future%20items%2C%20and%20aim%20to%20ensure%20that%20the%0Acumulative%20allocation%20at%20each%20round%20satisfies%20approximate%20envy-freeness%20--%0Awhich%20we%20define%20as%20temporal%20envy-freeness%20up%20to%20one%20item%20%28TEF1%29.%20We%20focus%20on%0Asettings%20where%20items%20can%20be%20exclusively%20goods%20or%20exclusively%20chores.%20For%20goods%2C%0Awhile%20TEF1%20allocations%20may%20not%20always%20exist%2C%20we%20identify%20several%20special%20cases%0Awhere%20they%20do%20--%20two%20agents%2C%20two%20item%20types%2C%20generalized%20binary%20valuations%2C%0Aunimodal%20preferences%20--%20and%20provide%20polynomial-time%20algorithms%20for%20these%20cases.%0AWe%20also%20prove%20that%20determining%20the%20existence%20of%20a%20TEF1%20allocation%20is%20NP-hard.%0AFor%20chores%2C%20we%20establish%20analogous%20results%20for%20the%20special%20cases%2C%20but%20present%20a%0Aslightly%20weaker%20intractability%20result.%20We%20also%20establish%20the%20incompatibility%0Abetween%20TEF1%20and%20Pareto-optimality%2C%20with%20the%20implication%20that%20it%20is%20intractable%0Ato%20find%20a%20TEF1%20allocation%20that%20maximizes%20any%20%24p%24-mean%20welfare%2C%20even%20for%20two%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14593v1&entry.124074799=Read"},
{"title": "Modular Boundaries in Recurrent Neural Networks", "author": "Jacob Tanner and Sina Mansour L. and Ludovico Coletta and Alessandro Gozzi and Richard F. Betzel", "abstract": "  Recent theoretical and experimental work in neuroscience has focused on the\nrepresentational and dynamical character of neural manifolds --subspaces in\nneural activity space wherein many neurons coactivate. Importantly, neural\npopulations studied under this \"neural manifold hypothesis\" are continuous and\nnot cleanly divided into separate neural populations. This perspective clashes\nwith the \"modular hypothesis\" of brain organization, wherein neural elements\nmaintain an \"all-or-nothing\" affiliation with modules. In line with this\nmodular hypothesis, recent research on recurrent neural networks suggests that\nmulti-task networks become modular across training, such that different modules\nspecialize for task-general dynamical motifs. If the modular hypothesis is\ntrue, then it would be important to use a dimensionality reduction technique\nthat captures modular structure. Here, we investigate the features of such a\nmethod. We leverage RNNs as a model system to study the character of modular\nneural populations, using a community detection method from network science\nknown as modularity maximization to partition neurons into distinct modules.\nThese partitions allow us to ask the following question: do these modular\nboundaries matter to the system? ...\n", "link": "http://arxiv.org/abs/2310.20601v2", "date": "2024-10-18", "relevancy": 1.8506, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4727}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4635}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Boundaries%20in%20Recurrent%20Neural%20Networks&body=Title%3A%20Modular%20Boundaries%20in%20Recurrent%20Neural%20Networks%0AAuthor%3A%20Jacob%20Tanner%20and%20Sina%20Mansour%20L.%20and%20Ludovico%20Coletta%20and%20Alessandro%20Gozzi%20and%20Richard%20F.%20Betzel%0AAbstract%3A%20%20%20Recent%20theoretical%20and%20experimental%20work%20in%20neuroscience%20has%20focused%20on%20the%0Arepresentational%20and%20dynamical%20character%20of%20neural%20manifolds%20--subspaces%20in%0Aneural%20activity%20space%20wherein%20many%20neurons%20coactivate.%20Importantly%2C%20neural%0Apopulations%20studied%20under%20this%20%22neural%20manifold%20hypothesis%22%20are%20continuous%20and%0Anot%20cleanly%20divided%20into%20separate%20neural%20populations.%20This%20perspective%20clashes%0Awith%20the%20%22modular%20hypothesis%22%20of%20brain%20organization%2C%20wherein%20neural%20elements%0Amaintain%20an%20%22all-or-nothing%22%20affiliation%20with%20modules.%20In%20line%20with%20this%0Amodular%20hypothesis%2C%20recent%20research%20on%20recurrent%20neural%20networks%20suggests%20that%0Amulti-task%20networks%20become%20modular%20across%20training%2C%20such%20that%20different%20modules%0Aspecialize%20for%20task-general%20dynamical%20motifs.%20If%20the%20modular%20hypothesis%20is%0Atrue%2C%20then%20it%20would%20be%20important%20to%20use%20a%20dimensionality%20reduction%20technique%0Athat%20captures%20modular%20structure.%20Here%2C%20we%20investigate%20the%20features%20of%20such%20a%0Amethod.%20We%20leverage%20RNNs%20as%20a%20model%20system%20to%20study%20the%20character%20of%20modular%0Aneural%20populations%2C%20using%20a%20community%20detection%20method%20from%20network%20science%0Aknown%20as%20modularity%20maximization%20to%20partition%20neurons%20into%20distinct%20modules.%0AThese%20partitions%20allow%20us%20to%20ask%20the%20following%20question%3A%20do%20these%20modular%0Aboundaries%20matter%20to%20the%20system%3F%20...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Boundaries%2520in%2520Recurrent%2520Neural%2520Networks%26entry.906535625%3DJacob%2520Tanner%2520and%2520Sina%2520Mansour%2520L.%2520and%2520Ludovico%2520Coletta%2520and%2520Alessandro%2520Gozzi%2520and%2520Richard%2520F.%2520Betzel%26entry.1292438233%3D%2520%2520Recent%2520theoretical%2520and%2520experimental%2520work%2520in%2520neuroscience%2520has%2520focused%2520on%2520the%250Arepresentational%2520and%2520dynamical%2520character%2520of%2520neural%2520manifolds%2520--subspaces%2520in%250Aneural%2520activity%2520space%2520wherein%2520many%2520neurons%2520coactivate.%2520Importantly%252C%2520neural%250Apopulations%2520studied%2520under%2520this%2520%2522neural%2520manifold%2520hypothesis%2522%2520are%2520continuous%2520and%250Anot%2520cleanly%2520divided%2520into%2520separate%2520neural%2520populations.%2520This%2520perspective%2520clashes%250Awith%2520the%2520%2522modular%2520hypothesis%2522%2520of%2520brain%2520organization%252C%2520wherein%2520neural%2520elements%250Amaintain%2520an%2520%2522all-or-nothing%2522%2520affiliation%2520with%2520modules.%2520In%2520line%2520with%2520this%250Amodular%2520hypothesis%252C%2520recent%2520research%2520on%2520recurrent%2520neural%2520networks%2520suggests%2520that%250Amulti-task%2520networks%2520become%2520modular%2520across%2520training%252C%2520such%2520that%2520different%2520modules%250Aspecialize%2520for%2520task-general%2520dynamical%2520motifs.%2520If%2520the%2520modular%2520hypothesis%2520is%250Atrue%252C%2520then%2520it%2520would%2520be%2520important%2520to%2520use%2520a%2520dimensionality%2520reduction%2520technique%250Athat%2520captures%2520modular%2520structure.%2520Here%252C%2520we%2520investigate%2520the%2520features%2520of%2520such%2520a%250Amethod.%2520We%2520leverage%2520RNNs%2520as%2520a%2520model%2520system%2520to%2520study%2520the%2520character%2520of%2520modular%250Aneural%2520populations%252C%2520using%2520a%2520community%2520detection%2520method%2520from%2520network%2520science%250Aknown%2520as%2520modularity%2520maximization%2520to%2520partition%2520neurons%2520into%2520distinct%2520modules.%250AThese%2520partitions%2520allow%2520us%2520to%2520ask%2520the%2520following%2520question%253A%2520do%2520these%2520modular%250Aboundaries%2520matter%2520to%2520the%2520system%253F%2520...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Boundaries%20in%20Recurrent%20Neural%20Networks&entry.906535625=Jacob%20Tanner%20and%20Sina%20Mansour%20L.%20and%20Ludovico%20Coletta%20and%20Alessandro%20Gozzi%20and%20Richard%20F.%20Betzel&entry.1292438233=%20%20Recent%20theoretical%20and%20experimental%20work%20in%20neuroscience%20has%20focused%20on%20the%0Arepresentational%20and%20dynamical%20character%20of%20neural%20manifolds%20--subspaces%20in%0Aneural%20activity%20space%20wherein%20many%20neurons%20coactivate.%20Importantly%2C%20neural%0Apopulations%20studied%20under%20this%20%22neural%20manifold%20hypothesis%22%20are%20continuous%20and%0Anot%20cleanly%20divided%20into%20separate%20neural%20populations.%20This%20perspective%20clashes%0Awith%20the%20%22modular%20hypothesis%22%20of%20brain%20organization%2C%20wherein%20neural%20elements%0Amaintain%20an%20%22all-or-nothing%22%20affiliation%20with%20modules.%20In%20line%20with%20this%0Amodular%20hypothesis%2C%20recent%20research%20on%20recurrent%20neural%20networks%20suggests%20that%0Amulti-task%20networks%20become%20modular%20across%20training%2C%20such%20that%20different%20modules%0Aspecialize%20for%20task-general%20dynamical%20motifs.%20If%20the%20modular%20hypothesis%20is%0Atrue%2C%20then%20it%20would%20be%20important%20to%20use%20a%20dimensionality%20reduction%20technique%0Athat%20captures%20modular%20structure.%20Here%2C%20we%20investigate%20the%20features%20of%20such%20a%0Amethod.%20We%20leverage%20RNNs%20as%20a%20model%20system%20to%20study%20the%20character%20of%20modular%0Aneural%20populations%2C%20using%20a%20community%20detection%20method%20from%20network%20science%0Aknown%20as%20modularity%20maximization%20to%20partition%20neurons%20into%20distinct%20modules.%0AThese%20partitions%20allow%20us%20to%20ask%20the%20following%20question%3A%20do%20these%20modular%0Aboundaries%20matter%20to%20the%20system%3F%20...%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20601v2&entry.124074799=Read"},
{"title": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning", "author": "Manish Chandra and Debasis Ganguly and Iadh Ounis", "abstract": "  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n", "link": "http://arxiv.org/abs/2403.06402v2", "date": "2024-10-18", "relevancy": 1.8468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4788}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20size%20doesn%27t%20fit%20all%3A%20Predicting%20the%20Number%20of%20Examples%20for%0A%20%20In-Context%20Learning&body=Title%3A%20One%20size%20doesn%27t%20fit%20all%3A%20Predicting%20the%20Number%20of%20Examples%20for%0A%20%20In-Context%20Learning%0AAuthor%3A%20Manish%20Chandra%20and%20Debasis%20Ganguly%20and%20Iadh%20Ounis%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20refers%20to%20the%20process%20of%20adding%20a%20small%20number%20of%0Alocalized%20examples%20%28ones%20that%20are%20semantically%20similar%20to%20the%20input%29%20from%20a%0Atraining%20set%20of%20labelled%20data%20to%20an%20LLM%27s%20prompt%20with%20an%20objective%20to%0Aeffectively%20control%20the%20generative%20process%20seeking%20to%20improve%20the%20downstream%0Atask%20performance.%20Existing%20ICL%20approaches%20use%20an%20identical%20number%20of%20examples%0A%28a%20pre-configured%20hyper-parameter%29%20for%20each%20data%20instance.%20Our%20work%20alleviates%0Athe%20limitations%20of%20this%20%27one%20fits%20all%27%20approach%20by%20dynamically%20predicting%20the%0Anumber%20of%20examples%20for%20each%20data%20instance%20to%20be%20used%20in%20few-shot%20inference%20with%0ALLMs.%20In%20particular%2C%20we%20employ%20a%20multi-label%20classifier%2C%20the%20parameters%20of%0Awhich%20are%20fitted%20using%20a%20training%20set%2C%20where%20the%20label%20for%20each%20instance%20in%20the%0Atraining%20set%20indicates%20if%20using%20a%20specific%20value%20of%20k%20%28number%20of%20most%20similar%0Aexamples%20from%200%20up%20to%20a%20maximum%20value%29%20leads%20to%20correct%20k-shot%20downstream%0Apredictions.%20Our%20experiments%20on%20a%20number%20of%20text%20classification%20benchmarks%20show%0Athat%20AICL%20substantially%20outperforms%20standard%20ICL%20by%20up%20to%2017%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520size%2520doesn%2527t%2520fit%2520all%253A%2520Predicting%2520the%2520Number%2520of%2520Examples%2520for%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DManish%2520Chandra%2520and%2520Debasis%2520Ganguly%2520and%2520Iadh%2520Ounis%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520refers%2520to%2520the%2520process%2520of%2520adding%2520a%2520small%2520number%2520of%250Alocalized%2520examples%2520%2528ones%2520that%2520are%2520semantically%2520similar%2520to%2520the%2520input%2529%2520from%2520a%250Atraining%2520set%2520of%2520labelled%2520data%2520to%2520an%2520LLM%2527s%2520prompt%2520with%2520an%2520objective%2520to%250Aeffectively%2520control%2520the%2520generative%2520process%2520seeking%2520to%2520improve%2520the%2520downstream%250Atask%2520performance.%2520Existing%2520ICL%2520approaches%2520use%2520an%2520identical%2520number%2520of%2520examples%250A%2528a%2520pre-configured%2520hyper-parameter%2529%2520for%2520each%2520data%2520instance.%2520Our%2520work%2520alleviates%250Athe%2520limitations%2520of%2520this%2520%2527one%2520fits%2520all%2527%2520approach%2520by%2520dynamically%2520predicting%2520the%250Anumber%2520of%2520examples%2520for%2520each%2520data%2520instance%2520to%2520be%2520used%2520in%2520few-shot%2520inference%2520with%250ALLMs.%2520In%2520particular%252C%2520we%2520employ%2520a%2520multi-label%2520classifier%252C%2520the%2520parameters%2520of%250Awhich%2520are%2520fitted%2520using%2520a%2520training%2520set%252C%2520where%2520the%2520label%2520for%2520each%2520instance%2520in%2520the%250Atraining%2520set%2520indicates%2520if%2520using%2520a%2520specific%2520value%2520of%2520k%2520%2528number%2520of%2520most%2520similar%250Aexamples%2520from%25200%2520up%2520to%2520a%2520maximum%2520value%2529%2520leads%2520to%2520correct%2520k-shot%2520downstream%250Apredictions.%2520Our%2520experiments%2520on%2520a%2520number%2520of%2520text%2520classification%2520benchmarks%2520show%250Athat%2520AICL%2520substantially%2520outperforms%2520standard%2520ICL%2520by%2520up%2520to%252017%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20size%20doesn%27t%20fit%20all%3A%20Predicting%20the%20Number%20of%20Examples%20for%0A%20%20In-Context%20Learning&entry.906535625=Manish%20Chandra%20and%20Debasis%20Ganguly%20and%20Iadh%20Ounis&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20refers%20to%20the%20process%20of%20adding%20a%20small%20number%20of%0Alocalized%20examples%20%28ones%20that%20are%20semantically%20similar%20to%20the%20input%29%20from%20a%0Atraining%20set%20of%20labelled%20data%20to%20an%20LLM%27s%20prompt%20with%20an%20objective%20to%0Aeffectively%20control%20the%20generative%20process%20seeking%20to%20improve%20the%20downstream%0Atask%20performance.%20Existing%20ICL%20approaches%20use%20an%20identical%20number%20of%20examples%0A%28a%20pre-configured%20hyper-parameter%29%20for%20each%20data%20instance.%20Our%20work%20alleviates%0Athe%20limitations%20of%20this%20%27one%20fits%20all%27%20approach%20by%20dynamically%20predicting%20the%0Anumber%20of%20examples%20for%20each%20data%20instance%20to%20be%20used%20in%20few-shot%20inference%20with%0ALLMs.%20In%20particular%2C%20we%20employ%20a%20multi-label%20classifier%2C%20the%20parameters%20of%0Awhich%20are%20fitted%20using%20a%20training%20set%2C%20where%20the%20label%20for%20each%20instance%20in%20the%0Atraining%20set%20indicates%20if%20using%20a%20specific%20value%20of%20k%20%28number%20of%20most%20similar%0Aexamples%20from%200%20up%20to%20a%20maximum%20value%29%20leads%20to%20correct%20k-shot%20downstream%0Apredictions.%20Our%20experiments%20on%20a%20number%20of%20text%20classification%20benchmarks%20show%0Athat%20AICL%20substantially%20outperforms%20standard%20ICL%20by%20up%20to%2017%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06402v2&entry.124074799=Read"},
{"title": "Learning Linear Attention in Polynomial Time", "author": "Morris Yau and Ekin Aky\u00fcrek and Jiayuan Mao and Joshua B. Tenenbaum and Stefanie Jegelka and Jacob Andreas", "abstract": "  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n", "link": "http://arxiv.org/abs/2410.10101v2", "date": "2024-10-18", "relevancy": 1.8406, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Linear%20Attention%20in%20Polynomial%20Time&body=Title%3A%20Learning%20Linear%20Attention%20in%20Polynomial%20Time%0AAuthor%3A%20Morris%20Yau%20and%20Ekin%20Aky%C3%BCrek%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas%0AAbstract%3A%20%20%20Previous%20research%20has%20explored%20the%20computational%20expressivity%20of%20Transformer%0Amodels%20in%20simulating%20Boolean%20circuits%20or%20Turing%20machines.%20However%2C%20the%0Alearnability%20of%20these%20simulators%20from%20observational%20data%20has%20remained%20an%20open%0Aquestion.%20Our%20study%20addresses%20this%20gap%20by%20providing%20the%20first%20polynomial-time%0Alearnability%20results%20%28specifically%20strong%2C%20agnostic%20PAC%20learning%29%20for%0Asingle-layer%20Transformers%20with%20linear%20attention.%20We%20show%20that%20linear%20attention%0Amay%20be%20viewed%20as%20a%20linear%20predictor%20in%20a%20suitably%20defined%20RKHS.%20As%20a%0Aconsequence%2C%20the%20problem%20of%20learning%20any%20linear%20transformer%20may%20be%20converted%0Ainto%20the%20problem%20of%20learning%20an%20ordinary%20linear%20predictor%20in%20an%20expanded%0Afeature%20space%2C%20and%20any%20such%20predictor%20may%20be%20converted%20back%20into%20a%20multiheaded%0Alinear%20transformer.%20Moving%20to%20generalization%2C%20we%20show%20how%20to%20efficiently%0Aidentify%20training%20datasets%20for%20which%20every%20empirical%20risk%20minimizer%20is%0Aequivalent%20%28up%20to%20trivial%20symmetries%29%20to%20the%20linear%20Transformer%20that%20generated%0Athe%20data%2C%20thereby%20guaranteeing%20the%20learned%20model%20will%20correctly%20generalize%0Aacross%20all%20inputs.%20Finally%2C%20we%20provide%20examples%20of%20computations%20expressible%20via%0Alinear%20attention%20and%20therefore%20polynomial-time%20learnable%2C%20including%20associative%0Amemories%2C%20finite%20automata%2C%20and%20a%20class%20of%20Universal%20Turing%20Machine%20%28UTMs%29%20with%0Apolynomially%20bounded%20computation%20histories.%20We%20empirically%20validate%20our%0Atheoretical%20findings%20on%20three%20tasks%3A%20learning%20random%20linear%20attention%20networks%2C%0Akey--value%20associations%2C%20and%20learning%20to%20execute%20finite%20automata.%20Our%20findings%0Abridge%20a%20critical%20gap%20between%20theoretical%20expressivity%20and%20learnability%20of%0ATransformers%2C%20and%20show%20that%20flexible%20and%20general%20models%20of%20computation%20are%0Aefficiently%20learnable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Linear%2520Attention%2520in%2520Polynomial%2520Time%26entry.906535625%3DMorris%2520Yau%2520and%2520Ekin%2520Aky%25C3%25BCrek%2520and%2520Jiayuan%2520Mao%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Stefanie%2520Jegelka%2520and%2520Jacob%2520Andreas%26entry.1292438233%3D%2520%2520Previous%2520research%2520has%2520explored%2520the%2520computational%2520expressivity%2520of%2520Transformer%250Amodels%2520in%2520simulating%2520Boolean%2520circuits%2520or%2520Turing%2520machines.%2520However%252C%2520the%250Alearnability%2520of%2520these%2520simulators%2520from%2520observational%2520data%2520has%2520remained%2520an%2520open%250Aquestion.%2520Our%2520study%2520addresses%2520this%2520gap%2520by%2520providing%2520the%2520first%2520polynomial-time%250Alearnability%2520results%2520%2528specifically%2520strong%252C%2520agnostic%2520PAC%2520learning%2529%2520for%250Asingle-layer%2520Transformers%2520with%2520linear%2520attention.%2520We%2520show%2520that%2520linear%2520attention%250Amay%2520be%2520viewed%2520as%2520a%2520linear%2520predictor%2520in%2520a%2520suitably%2520defined%2520RKHS.%2520As%2520a%250Aconsequence%252C%2520the%2520problem%2520of%2520learning%2520any%2520linear%2520transformer%2520may%2520be%2520converted%250Ainto%2520the%2520problem%2520of%2520learning%2520an%2520ordinary%2520linear%2520predictor%2520in%2520an%2520expanded%250Afeature%2520space%252C%2520and%2520any%2520such%2520predictor%2520may%2520be%2520converted%2520back%2520into%2520a%2520multiheaded%250Alinear%2520transformer.%2520Moving%2520to%2520generalization%252C%2520we%2520show%2520how%2520to%2520efficiently%250Aidentify%2520training%2520datasets%2520for%2520which%2520every%2520empirical%2520risk%2520minimizer%2520is%250Aequivalent%2520%2528up%2520to%2520trivial%2520symmetries%2529%2520to%2520the%2520linear%2520Transformer%2520that%2520generated%250Athe%2520data%252C%2520thereby%2520guaranteeing%2520the%2520learned%2520model%2520will%2520correctly%2520generalize%250Aacross%2520all%2520inputs.%2520Finally%252C%2520we%2520provide%2520examples%2520of%2520computations%2520expressible%2520via%250Alinear%2520attention%2520and%2520therefore%2520polynomial-time%2520learnable%252C%2520including%2520associative%250Amemories%252C%2520finite%2520automata%252C%2520and%2520a%2520class%2520of%2520Universal%2520Turing%2520Machine%2520%2528UTMs%2529%2520with%250Apolynomially%2520bounded%2520computation%2520histories.%2520We%2520empirically%2520validate%2520our%250Atheoretical%2520findings%2520on%2520three%2520tasks%253A%2520learning%2520random%2520linear%2520attention%2520networks%252C%250Akey--value%2520associations%252C%2520and%2520learning%2520to%2520execute%2520finite%2520automata.%2520Our%2520findings%250Abridge%2520a%2520critical%2520gap%2520between%2520theoretical%2520expressivity%2520and%2520learnability%2520of%250ATransformers%252C%2520and%2520show%2520that%2520flexible%2520and%2520general%2520models%2520of%2520computation%2520are%250Aefficiently%2520learnable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Linear%20Attention%20in%20Polynomial%20Time&entry.906535625=Morris%20Yau%20and%20Ekin%20Aky%C3%BCrek%20and%20Jiayuan%20Mao%20and%20Joshua%20B.%20Tenenbaum%20and%20Stefanie%20Jegelka%20and%20Jacob%20Andreas&entry.1292438233=%20%20Previous%20research%20has%20explored%20the%20computational%20expressivity%20of%20Transformer%0Amodels%20in%20simulating%20Boolean%20circuits%20or%20Turing%20machines.%20However%2C%20the%0Alearnability%20of%20these%20simulators%20from%20observational%20data%20has%20remained%20an%20open%0Aquestion.%20Our%20study%20addresses%20this%20gap%20by%20providing%20the%20first%20polynomial-time%0Alearnability%20results%20%28specifically%20strong%2C%20agnostic%20PAC%20learning%29%20for%0Asingle-layer%20Transformers%20with%20linear%20attention.%20We%20show%20that%20linear%20attention%0Amay%20be%20viewed%20as%20a%20linear%20predictor%20in%20a%20suitably%20defined%20RKHS.%20As%20a%0Aconsequence%2C%20the%20problem%20of%20learning%20any%20linear%20transformer%20may%20be%20converted%0Ainto%20the%20problem%20of%20learning%20an%20ordinary%20linear%20predictor%20in%20an%20expanded%0Afeature%20space%2C%20and%20any%20such%20predictor%20may%20be%20converted%20back%20into%20a%20multiheaded%0Alinear%20transformer.%20Moving%20to%20generalization%2C%20we%20show%20how%20to%20efficiently%0Aidentify%20training%20datasets%20for%20which%20every%20empirical%20risk%20minimizer%20is%0Aequivalent%20%28up%20to%20trivial%20symmetries%29%20to%20the%20linear%20Transformer%20that%20generated%0Athe%20data%2C%20thereby%20guaranteeing%20the%20learned%20model%20will%20correctly%20generalize%0Aacross%20all%20inputs.%20Finally%2C%20we%20provide%20examples%20of%20computations%20expressible%20via%0Alinear%20attention%20and%20therefore%20polynomial-time%20learnable%2C%20including%20associative%0Amemories%2C%20finite%20automata%2C%20and%20a%20class%20of%20Universal%20Turing%20Machine%20%28UTMs%29%20with%0Apolynomially%20bounded%20computation%20histories.%20We%20empirically%20validate%20our%0Atheoretical%20findings%20on%20three%20tasks%3A%20learning%20random%20linear%20attention%20networks%2C%0Akey--value%20associations%2C%20and%20learning%20to%20execute%20finite%20automata.%20Our%20findings%0Abridge%20a%20critical%20gap%20between%20theoretical%20expressivity%20and%20learnability%20of%0ATransformers%2C%20and%20show%20that%20flexible%20and%20general%20models%20of%20computation%20are%0Aefficiently%20learnable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10101v2&entry.124074799=Read"},
{"title": "Online Reinforcement Learning with Passive Memory", "author": "Anay Pattanaik and Lav R. Varshney", "abstract": "  This paper considers an online reinforcement learning algorithm that\nleverages pre-collected data (passive memory) from the environment for online\ninteraction. We show that using passive memory improves performance and further\nprovide theoretical guarantees for regret that turns out to be near-minimax\noptimal. Results show that the quality of passive memory determines\nsub-optimality of the incurred regret. The proposed approach and results hold\nin both continuous and discrete state-action spaces.\n", "link": "http://arxiv.org/abs/2410.14665v1", "date": "2024-10-18", "relevancy": 1.8333, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4618}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Reinforcement%20Learning%20with%20Passive%20Memory&body=Title%3A%20Online%20Reinforcement%20Learning%20with%20Passive%20Memory%0AAuthor%3A%20Anay%20Pattanaik%20and%20Lav%20R.%20Varshney%0AAbstract%3A%20%20%20This%20paper%20considers%20an%20online%20reinforcement%20learning%20algorithm%20that%0Aleverages%20pre-collected%20data%20%28passive%20memory%29%20from%20the%20environment%20for%20online%0Ainteraction.%20We%20show%20that%20using%20passive%20memory%20improves%20performance%20and%20further%0Aprovide%20theoretical%20guarantees%20for%20regret%20that%20turns%20out%20to%20be%20near-minimax%0Aoptimal.%20Results%20show%20that%20the%20quality%20of%20passive%20memory%20determines%0Asub-optimality%20of%20the%20incurred%20regret.%20The%20proposed%20approach%20and%20results%20hold%0Ain%20both%20continuous%20and%20discrete%20state-action%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Reinforcement%2520Learning%2520with%2520Passive%2520Memory%26entry.906535625%3DAnay%2520Pattanaik%2520and%2520Lav%2520R.%2520Varshney%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520an%2520online%2520reinforcement%2520learning%2520algorithm%2520that%250Aleverages%2520pre-collected%2520data%2520%2528passive%2520memory%2529%2520from%2520the%2520environment%2520for%2520online%250Ainteraction.%2520We%2520show%2520that%2520using%2520passive%2520memory%2520improves%2520performance%2520and%2520further%250Aprovide%2520theoretical%2520guarantees%2520for%2520regret%2520that%2520turns%2520out%2520to%2520be%2520near-minimax%250Aoptimal.%2520Results%2520show%2520that%2520the%2520quality%2520of%2520passive%2520memory%2520determines%250Asub-optimality%2520of%2520the%2520incurred%2520regret.%2520The%2520proposed%2520approach%2520and%2520results%2520hold%250Ain%2520both%2520continuous%2520and%2520discrete%2520state-action%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Reinforcement%20Learning%20with%20Passive%20Memory&entry.906535625=Anay%20Pattanaik%20and%20Lav%20R.%20Varshney&entry.1292438233=%20%20This%20paper%20considers%20an%20online%20reinforcement%20learning%20algorithm%20that%0Aleverages%20pre-collected%20data%20%28passive%20memory%29%20from%20the%20environment%20for%20online%0Ainteraction.%20We%20show%20that%20using%20passive%20memory%20improves%20performance%20and%20further%0Aprovide%20theoretical%20guarantees%20for%20regret%20that%20turns%20out%20to%20be%20near-minimax%0Aoptimal.%20Results%20show%20that%20the%20quality%20of%20passive%20memory%20determines%0Asub-optimality%20of%20the%20incurred%20regret.%20The%20proposed%20approach%20and%20results%20hold%0Ain%20both%20continuous%20and%20discrete%20state-action%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14665v1&entry.124074799=Read"},
{"title": "Neural Combinatorial Clustered Bandits for Recommendation Systems", "author": "Baran Atalar and Carlee Joe-Wong", "abstract": "  We consider the contextual combinatorial bandit setting where in each round,\nthe learning agent, e.g., a recommender system, selects a subset of \"arms,\"\ne.g., products, and observes rewards for both the individual base arms, which\nare a function of known features (called \"context\"), and the super arm (the\nsubset of arms), which is a function of the base arm rewards. The agent's goal\nis to simultaneously learn the unknown reward functions and choose the\nhighest-reward arms. For example, the \"reward\" may represent a user's\nprobability of clicking on one of the recommended products. Conventional bandit\nmodels, however, employ restrictive reward function models in order to obtain\nperformance guarantees. We make use of deep neural networks to estimate and\nlearn the unknown reward functions and propose Neural UCB Clustering\n(NeUClust), which adopts a clustering approach to select the super arm in every\nround by exploiting underlying structure in the context space. Unlike prior\nneural bandit works, NeUClust uses a neural network to estimate the super arm\nreward and select the super arm, thus eliminating the need for a known\noptimization oracle. We non-trivially extend prior neural combinatorial bandit\nworks to prove that NeUClust achieves\n$\\widetilde{O}\\left(\\widetilde{d}\\sqrt{T}\\right)$ regret, where $\\widetilde{d}$\nis the effective dimension of a neural tangent kernel matrix, $T$ the number of\nrounds. Experiments on real world recommendation datasets show that NeUClust\nachieves better regret and reward than other contextual combinatorial and\nneural bandit algorithms.\n", "link": "http://arxiv.org/abs/2410.14586v1", "date": "2024-10-18", "relevancy": 1.8306, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4796}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Combinatorial%20Clustered%20Bandits%20for%20Recommendation%20Systems&body=Title%3A%20Neural%20Combinatorial%20Clustered%20Bandits%20for%20Recommendation%20Systems%0AAuthor%3A%20Baran%20Atalar%20and%20Carlee%20Joe-Wong%0AAbstract%3A%20%20%20We%20consider%20the%20contextual%20combinatorial%20bandit%20setting%20where%20in%20each%20round%2C%0Athe%20learning%20agent%2C%20e.g.%2C%20a%20recommender%20system%2C%20selects%20a%20subset%20of%20%22arms%2C%22%0Ae.g.%2C%20products%2C%20and%20observes%20rewards%20for%20both%20the%20individual%20base%20arms%2C%20which%0Aare%20a%20function%20of%20known%20features%20%28called%20%22context%22%29%2C%20and%20the%20super%20arm%20%28the%0Asubset%20of%20arms%29%2C%20which%20is%20a%20function%20of%20the%20base%20arm%20rewards.%20The%20agent%27s%20goal%0Ais%20to%20simultaneously%20learn%20the%20unknown%20reward%20functions%20and%20choose%20the%0Ahighest-reward%20arms.%20For%20example%2C%20the%20%22reward%22%20may%20represent%20a%20user%27s%0Aprobability%20of%20clicking%20on%20one%20of%20the%20recommended%20products.%20Conventional%20bandit%0Amodels%2C%20however%2C%20employ%20restrictive%20reward%20function%20models%20in%20order%20to%20obtain%0Aperformance%20guarantees.%20We%20make%20use%20of%20deep%20neural%20networks%20to%20estimate%20and%0Alearn%20the%20unknown%20reward%20functions%20and%20propose%20Neural%20UCB%20Clustering%0A%28NeUClust%29%2C%20which%20adopts%20a%20clustering%20approach%20to%20select%20the%20super%20arm%20in%20every%0Around%20by%20exploiting%20underlying%20structure%20in%20the%20context%20space.%20Unlike%20prior%0Aneural%20bandit%20works%2C%20NeUClust%20uses%20a%20neural%20network%20to%20estimate%20the%20super%20arm%0Areward%20and%20select%20the%20super%20arm%2C%20thus%20eliminating%20the%20need%20for%20a%20known%0Aoptimization%20oracle.%20We%20non-trivially%20extend%20prior%20neural%20combinatorial%20bandit%0Aworks%20to%20prove%20that%20NeUClust%20achieves%0A%24%5Cwidetilde%7BO%7D%5Cleft%28%5Cwidetilde%7Bd%7D%5Csqrt%7BT%7D%5Cright%29%24%20regret%2C%20where%20%24%5Cwidetilde%7Bd%7D%24%0Ais%20the%20effective%20dimension%20of%20a%20neural%20tangent%20kernel%20matrix%2C%20%24T%24%20the%20number%20of%0Arounds.%20Experiments%20on%20real%20world%20recommendation%20datasets%20show%20that%20NeUClust%0Aachieves%20better%20regret%20and%20reward%20than%20other%20contextual%20combinatorial%20and%0Aneural%20bandit%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Combinatorial%2520Clustered%2520Bandits%2520for%2520Recommendation%2520Systems%26entry.906535625%3DBaran%2520Atalar%2520and%2520Carlee%2520Joe-Wong%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520contextual%2520combinatorial%2520bandit%2520setting%2520where%2520in%2520each%2520round%252C%250Athe%2520learning%2520agent%252C%2520e.g.%252C%2520a%2520recommender%2520system%252C%2520selects%2520a%2520subset%2520of%2520%2522arms%252C%2522%250Ae.g.%252C%2520products%252C%2520and%2520observes%2520rewards%2520for%2520both%2520the%2520individual%2520base%2520arms%252C%2520which%250Aare%2520a%2520function%2520of%2520known%2520features%2520%2528called%2520%2522context%2522%2529%252C%2520and%2520the%2520super%2520arm%2520%2528the%250Asubset%2520of%2520arms%2529%252C%2520which%2520is%2520a%2520function%2520of%2520the%2520base%2520arm%2520rewards.%2520The%2520agent%2527s%2520goal%250Ais%2520to%2520simultaneously%2520learn%2520the%2520unknown%2520reward%2520functions%2520and%2520choose%2520the%250Ahighest-reward%2520arms.%2520For%2520example%252C%2520the%2520%2522reward%2522%2520may%2520represent%2520a%2520user%2527s%250Aprobability%2520of%2520clicking%2520on%2520one%2520of%2520the%2520recommended%2520products.%2520Conventional%2520bandit%250Amodels%252C%2520however%252C%2520employ%2520restrictive%2520reward%2520function%2520models%2520in%2520order%2520to%2520obtain%250Aperformance%2520guarantees.%2520We%2520make%2520use%2520of%2520deep%2520neural%2520networks%2520to%2520estimate%2520and%250Alearn%2520the%2520unknown%2520reward%2520functions%2520and%2520propose%2520Neural%2520UCB%2520Clustering%250A%2528NeUClust%2529%252C%2520which%2520adopts%2520a%2520clustering%2520approach%2520to%2520select%2520the%2520super%2520arm%2520in%2520every%250Around%2520by%2520exploiting%2520underlying%2520structure%2520in%2520the%2520context%2520space.%2520Unlike%2520prior%250Aneural%2520bandit%2520works%252C%2520NeUClust%2520uses%2520a%2520neural%2520network%2520to%2520estimate%2520the%2520super%2520arm%250Areward%2520and%2520select%2520the%2520super%2520arm%252C%2520thus%2520eliminating%2520the%2520need%2520for%2520a%2520known%250Aoptimization%2520oracle.%2520We%2520non-trivially%2520extend%2520prior%2520neural%2520combinatorial%2520bandit%250Aworks%2520to%2520prove%2520that%2520NeUClust%2520achieves%250A%2524%255Cwidetilde%257BO%257D%255Cleft%2528%255Cwidetilde%257Bd%257D%255Csqrt%257BT%257D%255Cright%2529%2524%2520regret%252C%2520where%2520%2524%255Cwidetilde%257Bd%257D%2524%250Ais%2520the%2520effective%2520dimension%2520of%2520a%2520neural%2520tangent%2520kernel%2520matrix%252C%2520%2524T%2524%2520the%2520number%2520of%250Arounds.%2520Experiments%2520on%2520real%2520world%2520recommendation%2520datasets%2520show%2520that%2520NeUClust%250Aachieves%2520better%2520regret%2520and%2520reward%2520than%2520other%2520contextual%2520combinatorial%2520and%250Aneural%2520bandit%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Combinatorial%20Clustered%20Bandits%20for%20Recommendation%20Systems&entry.906535625=Baran%20Atalar%20and%20Carlee%20Joe-Wong&entry.1292438233=%20%20We%20consider%20the%20contextual%20combinatorial%20bandit%20setting%20where%20in%20each%20round%2C%0Athe%20learning%20agent%2C%20e.g.%2C%20a%20recommender%20system%2C%20selects%20a%20subset%20of%20%22arms%2C%22%0Ae.g.%2C%20products%2C%20and%20observes%20rewards%20for%20both%20the%20individual%20base%20arms%2C%20which%0Aare%20a%20function%20of%20known%20features%20%28called%20%22context%22%29%2C%20and%20the%20super%20arm%20%28the%0Asubset%20of%20arms%29%2C%20which%20is%20a%20function%20of%20the%20base%20arm%20rewards.%20The%20agent%27s%20goal%0Ais%20to%20simultaneously%20learn%20the%20unknown%20reward%20functions%20and%20choose%20the%0Ahighest-reward%20arms.%20For%20example%2C%20the%20%22reward%22%20may%20represent%20a%20user%27s%0Aprobability%20of%20clicking%20on%20one%20of%20the%20recommended%20products.%20Conventional%20bandit%0Amodels%2C%20however%2C%20employ%20restrictive%20reward%20function%20models%20in%20order%20to%20obtain%0Aperformance%20guarantees.%20We%20make%20use%20of%20deep%20neural%20networks%20to%20estimate%20and%0Alearn%20the%20unknown%20reward%20functions%20and%20propose%20Neural%20UCB%20Clustering%0A%28NeUClust%29%2C%20which%20adopts%20a%20clustering%20approach%20to%20select%20the%20super%20arm%20in%20every%0Around%20by%20exploiting%20underlying%20structure%20in%20the%20context%20space.%20Unlike%20prior%0Aneural%20bandit%20works%2C%20NeUClust%20uses%20a%20neural%20network%20to%20estimate%20the%20super%20arm%0Areward%20and%20select%20the%20super%20arm%2C%20thus%20eliminating%20the%20need%20for%20a%20known%0Aoptimization%20oracle.%20We%20non-trivially%20extend%20prior%20neural%20combinatorial%20bandit%0Aworks%20to%20prove%20that%20NeUClust%20achieves%0A%24%5Cwidetilde%7BO%7D%5Cleft%28%5Cwidetilde%7Bd%7D%5Csqrt%7BT%7D%5Cright%29%24%20regret%2C%20where%20%24%5Cwidetilde%7Bd%7D%24%0Ais%20the%20effective%20dimension%20of%20a%20neural%20tangent%20kernel%20matrix%2C%20%24T%24%20the%20number%20of%0Arounds.%20Experiments%20on%20real%20world%20recommendation%20datasets%20show%20that%20NeUClust%0Aachieves%20better%20regret%20and%20reward%20than%20other%20contextual%20combinatorial%20and%0Aneural%20bandit%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14586v1&entry.124074799=Read"},
{"title": "English offensive text detection using CNN based Bi-GRU model", "author": "Tonmoy Roy and Md Robiul Islam and Asif Ahammad Miazee and Anika Antara and Al Amin and Sunjim Hossain", "abstract": "  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n", "link": "http://arxiv.org/abs/2409.15652v3", "date": "2024-10-18", "relevancy": 1.8242, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4548}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20English%20offensive%20text%20detection%20using%20CNN%20based%20Bi-GRU%20model&body=Title%3A%20English%20offensive%20text%20detection%20using%20CNN%20based%20Bi-GRU%20model%0AAuthor%3A%20Tonmoy%20Roy%20and%20Md%20Robiul%20Islam%20and%20Asif%20Ahammad%20Miazee%20and%20Anika%20Antara%20and%20Al%20Amin%20and%20Sunjim%20Hossain%0AAbstract%3A%20%20%20Over%20the%20years%2C%20the%20number%20of%20users%20of%20social%20media%20has%20increased%0Adrastically.%20People%20frequently%20share%20their%20thoughts%20through%20social%20platforms%2C%0Aand%20this%20leads%20to%20an%20increase%20in%20hate%20content.%20In%20this%20virtual%20community%2C%0Aindividuals%20share%20their%20views%2C%20express%20their%20feelings%2C%20and%20post%20photos%2C%20videos%2C%0Ablogs%2C%20and%20more.%20Social%20networking%20sites%20like%20Facebook%20and%20Twitter%20provide%0Aplatforms%20to%20share%20vast%20amounts%20of%20content%20with%20a%20single%20click.%20However%2C%20these%0Aplatforms%20do%20not%20impose%20restrictions%20on%20the%20uploaded%20content%2C%20which%20may%20include%0Aabusive%20language%20and%20explicit%20images%20unsuitable%20for%20social%20media.%20To%20resolve%0Athis%20issue%2C%20a%20new%20idea%20must%20be%20implemented%20to%20divide%20the%20inappropriate%20content.%0ANumerous%20studies%20have%20been%20done%20to%20automate%20the%20process.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20Bi-GRU-CNN%20model%20to%20classify%20whether%20the%20text%20is%20offensive%20or%0Anot.%20The%20combination%20of%20the%20Bi-GRU%20and%20CNN%20models%20outperforms%20the%20existing%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15652v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnglish%2520offensive%2520text%2520detection%2520using%2520CNN%2520based%2520Bi-GRU%2520model%26entry.906535625%3DTonmoy%2520Roy%2520and%2520Md%2520Robiul%2520Islam%2520and%2520Asif%2520Ahammad%2520Miazee%2520and%2520Anika%2520Antara%2520and%2520Al%2520Amin%2520and%2520Sunjim%2520Hossain%26entry.1292438233%3D%2520%2520Over%2520the%2520years%252C%2520the%2520number%2520of%2520users%2520of%2520social%2520media%2520has%2520increased%250Adrastically.%2520People%2520frequently%2520share%2520their%2520thoughts%2520through%2520social%2520platforms%252C%250Aand%2520this%2520leads%2520to%2520an%2520increase%2520in%2520hate%2520content.%2520In%2520this%2520virtual%2520community%252C%250Aindividuals%2520share%2520their%2520views%252C%2520express%2520their%2520feelings%252C%2520and%2520post%2520photos%252C%2520videos%252C%250Ablogs%252C%2520and%2520more.%2520Social%2520networking%2520sites%2520like%2520Facebook%2520and%2520Twitter%2520provide%250Aplatforms%2520to%2520share%2520vast%2520amounts%2520of%2520content%2520with%2520a%2520single%2520click.%2520However%252C%2520these%250Aplatforms%2520do%2520not%2520impose%2520restrictions%2520on%2520the%2520uploaded%2520content%252C%2520which%2520may%2520include%250Aabusive%2520language%2520and%2520explicit%2520images%2520unsuitable%2520for%2520social%2520media.%2520To%2520resolve%250Athis%2520issue%252C%2520a%2520new%2520idea%2520must%2520be%2520implemented%2520to%2520divide%2520the%2520inappropriate%2520content.%250ANumerous%2520studies%2520have%2520been%2520done%2520to%2520automate%2520the%2520process.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520new%2520Bi-GRU-CNN%2520model%2520to%2520classify%2520whether%2520the%2520text%2520is%2520offensive%2520or%250Anot.%2520The%2520combination%2520of%2520the%2520Bi-GRU%2520and%2520CNN%2520models%2520outperforms%2520the%2520existing%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15652v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=English%20offensive%20text%20detection%20using%20CNN%20based%20Bi-GRU%20model&entry.906535625=Tonmoy%20Roy%20and%20Md%20Robiul%20Islam%20and%20Asif%20Ahammad%20Miazee%20and%20Anika%20Antara%20and%20Al%20Amin%20and%20Sunjim%20Hossain&entry.1292438233=%20%20Over%20the%20years%2C%20the%20number%20of%20users%20of%20social%20media%20has%20increased%0Adrastically.%20People%20frequently%20share%20their%20thoughts%20through%20social%20platforms%2C%0Aand%20this%20leads%20to%20an%20increase%20in%20hate%20content.%20In%20this%20virtual%20community%2C%0Aindividuals%20share%20their%20views%2C%20express%20their%20feelings%2C%20and%20post%20photos%2C%20videos%2C%0Ablogs%2C%20and%20more.%20Social%20networking%20sites%20like%20Facebook%20and%20Twitter%20provide%0Aplatforms%20to%20share%20vast%20amounts%20of%20content%20with%20a%20single%20click.%20However%2C%20these%0Aplatforms%20do%20not%20impose%20restrictions%20on%20the%20uploaded%20content%2C%20which%20may%20include%0Aabusive%20language%20and%20explicit%20images%20unsuitable%20for%20social%20media.%20To%20resolve%0Athis%20issue%2C%20a%20new%20idea%20must%20be%20implemented%20to%20divide%20the%20inappropriate%20content.%0ANumerous%20studies%20have%20been%20done%20to%20automate%20the%20process.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20Bi-GRU-CNN%20model%20to%20classify%20whether%20the%20text%20is%20offensive%20or%0Anot.%20The%20combination%20of%20the%20Bi-GRU%20and%20CNN%20models%20outperforms%20the%20existing%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15652v3&entry.124074799=Read"},
{"title": "Building Trust in Black-box Optimization: A Comprehensive Framework for\n  Explainability", "author": "Nazanin Nezami and Hadis Anahideh", "abstract": "  Optimizing costly black-box functions within a constrained evaluation budget\npresents significant challenges in many real-world applications. Surrogate\nOptimization (SO) is a common resolution, yet its proprietary nature introduced\nby the complexity of surrogate models and the sampling core (e.g., acquisition\nfunctions) often leads to a lack of explainability and transparency. While\nexisting literature has primarily concentrated on enhancing convergence to\nglobal optima, the practical interpretation of newly proposed strategies\nremains underexplored, especially in batch evaluation settings. In this paper,\nwe propose \\emph{Inclusive} Explainability Metrics for Surrogate Optimization\n(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the\ntransparency, trustworthiness, and explainability of the SO approaches. Through\nthese metrics, we provide both intermediate and post-hoc explanations to\npractitioners before and after performing expensive evaluations to gain trust.\nWe consider four primary categories of metrics, each targeting a specific\naspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,\nOptimization Process Metrics, and Feature Importance. Our experimental\nevaluations demonstrate the significant potential of the proposed metrics\nacross different benchmarks.\n", "link": "http://arxiv.org/abs/2410.14573v1", "date": "2024-10-18", "relevancy": 1.8159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Trust%20in%20Black-box%20Optimization%3A%20A%20Comprehensive%20Framework%20for%0A%20%20Explainability&body=Title%3A%20Building%20Trust%20in%20Black-box%20Optimization%3A%20A%20Comprehensive%20Framework%20for%0A%20%20Explainability%0AAuthor%3A%20Nazanin%20Nezami%20and%20Hadis%20Anahideh%0AAbstract%3A%20%20%20Optimizing%20costly%20black-box%20functions%20within%20a%20constrained%20evaluation%20budget%0Apresents%20significant%20challenges%20in%20many%20real-world%20applications.%20Surrogate%0AOptimization%20%28SO%29%20is%20a%20common%20resolution%2C%20yet%20its%20proprietary%20nature%20introduced%0Aby%20the%20complexity%20of%20surrogate%20models%20and%20the%20sampling%20core%20%28e.g.%2C%20acquisition%0Afunctions%29%20often%20leads%20to%20a%20lack%20of%20explainability%20and%20transparency.%20While%0Aexisting%20literature%20has%20primarily%20concentrated%20on%20enhancing%20convergence%20to%0Aglobal%20optima%2C%20the%20practical%20interpretation%20of%20newly%20proposed%20strategies%0Aremains%20underexplored%2C%20especially%20in%20batch%20evaluation%20settings.%20In%20this%20paper%2C%0Awe%20propose%20%5Cemph%7BInclusive%7D%20Explainability%20Metrics%20for%20Surrogate%20Optimization%0A%28IEMSO%29%2C%20a%20comprehensive%20set%20of%20model-agnostic%20metrics%20designed%20to%20enhance%20the%0Atransparency%2C%20trustworthiness%2C%20and%20explainability%20of%20the%20SO%20approaches.%20Through%0Athese%20metrics%2C%20we%20provide%20both%20intermediate%20and%20post-hoc%20explanations%20to%0Apractitioners%20before%20and%20after%20performing%20expensive%20evaluations%20to%20gain%20trust.%0AWe%20consider%20four%20primary%20categories%20of%20metrics%2C%20each%20targeting%20a%20specific%0Aaspect%20of%20the%20SO%20process%3A%20Sampling%20Core%20Metrics%2C%20Batch%20Properties%20Metrics%2C%0AOptimization%20Process%20Metrics%2C%20and%20Feature%20Importance.%20Our%20experimental%0Aevaluations%20demonstrate%20the%20significant%20potential%20of%20the%20proposed%20metrics%0Aacross%20different%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Trust%2520in%2520Black-box%2520Optimization%253A%2520A%2520Comprehensive%2520Framework%2520for%250A%2520%2520Explainability%26entry.906535625%3DNazanin%2520Nezami%2520and%2520Hadis%2520Anahideh%26entry.1292438233%3D%2520%2520Optimizing%2520costly%2520black-box%2520functions%2520within%2520a%2520constrained%2520evaluation%2520budget%250Apresents%2520significant%2520challenges%2520in%2520many%2520real-world%2520applications.%2520Surrogate%250AOptimization%2520%2528SO%2529%2520is%2520a%2520common%2520resolution%252C%2520yet%2520its%2520proprietary%2520nature%2520introduced%250Aby%2520the%2520complexity%2520of%2520surrogate%2520models%2520and%2520the%2520sampling%2520core%2520%2528e.g.%252C%2520acquisition%250Afunctions%2529%2520often%2520leads%2520to%2520a%2520lack%2520of%2520explainability%2520and%2520transparency.%2520While%250Aexisting%2520literature%2520has%2520primarily%2520concentrated%2520on%2520enhancing%2520convergence%2520to%250Aglobal%2520optima%252C%2520the%2520practical%2520interpretation%2520of%2520newly%2520proposed%2520strategies%250Aremains%2520underexplored%252C%2520especially%2520in%2520batch%2520evaluation%2520settings.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520%255Cemph%257BInclusive%257D%2520Explainability%2520Metrics%2520for%2520Surrogate%2520Optimization%250A%2528IEMSO%2529%252C%2520a%2520comprehensive%2520set%2520of%2520model-agnostic%2520metrics%2520designed%2520to%2520enhance%2520the%250Atransparency%252C%2520trustworthiness%252C%2520and%2520explainability%2520of%2520the%2520SO%2520approaches.%2520Through%250Athese%2520metrics%252C%2520we%2520provide%2520both%2520intermediate%2520and%2520post-hoc%2520explanations%2520to%250Apractitioners%2520before%2520and%2520after%2520performing%2520expensive%2520evaluations%2520to%2520gain%2520trust.%250AWe%2520consider%2520four%2520primary%2520categories%2520of%2520metrics%252C%2520each%2520targeting%2520a%2520specific%250Aaspect%2520of%2520the%2520SO%2520process%253A%2520Sampling%2520Core%2520Metrics%252C%2520Batch%2520Properties%2520Metrics%252C%250AOptimization%2520Process%2520Metrics%252C%2520and%2520Feature%2520Importance.%2520Our%2520experimental%250Aevaluations%2520demonstrate%2520the%2520significant%2520potential%2520of%2520the%2520proposed%2520metrics%250Aacross%2520different%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Trust%20in%20Black-box%20Optimization%3A%20A%20Comprehensive%20Framework%20for%0A%20%20Explainability&entry.906535625=Nazanin%20Nezami%20and%20Hadis%20Anahideh&entry.1292438233=%20%20Optimizing%20costly%20black-box%20functions%20within%20a%20constrained%20evaluation%20budget%0Apresents%20significant%20challenges%20in%20many%20real-world%20applications.%20Surrogate%0AOptimization%20%28SO%29%20is%20a%20common%20resolution%2C%20yet%20its%20proprietary%20nature%20introduced%0Aby%20the%20complexity%20of%20surrogate%20models%20and%20the%20sampling%20core%20%28e.g.%2C%20acquisition%0Afunctions%29%20often%20leads%20to%20a%20lack%20of%20explainability%20and%20transparency.%20While%0Aexisting%20literature%20has%20primarily%20concentrated%20on%20enhancing%20convergence%20to%0Aglobal%20optima%2C%20the%20practical%20interpretation%20of%20newly%20proposed%20strategies%0Aremains%20underexplored%2C%20especially%20in%20batch%20evaluation%20settings.%20In%20this%20paper%2C%0Awe%20propose%20%5Cemph%7BInclusive%7D%20Explainability%20Metrics%20for%20Surrogate%20Optimization%0A%28IEMSO%29%2C%20a%20comprehensive%20set%20of%20model-agnostic%20metrics%20designed%20to%20enhance%20the%0Atransparency%2C%20trustworthiness%2C%20and%20explainability%20of%20the%20SO%20approaches.%20Through%0Athese%20metrics%2C%20we%20provide%20both%20intermediate%20and%20post-hoc%20explanations%20to%0Apractitioners%20before%20and%20after%20performing%20expensive%20evaluations%20to%20gain%20trust.%0AWe%20consider%20four%20primary%20categories%20of%20metrics%2C%20each%20targeting%20a%20specific%0Aaspect%20of%20the%20SO%20process%3A%20Sampling%20Core%20Metrics%2C%20Batch%20Properties%20Metrics%2C%0AOptimization%20Process%20Metrics%2C%20and%20Feature%20Importance.%20Our%20experimental%0Aevaluations%20demonstrate%20the%20significant%20potential%20of%20the%20proposed%20metrics%0Aacross%20different%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14573v1&entry.124074799=Read"},
{"title": "What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization", "author": "Frederic Kirstein and Jan Philip Wahle and Terry Ruas and Bela Gipp", "abstract": "  Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.\n", "link": "http://arxiv.org/abs/2404.11124v2", "date": "2024-10-18", "relevancy": 1.8099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4526}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%27s%20under%20the%20hood%3A%20Investigating%20Automatic%20Metrics%20on%20Meeting%0A%20%20Summarization&body=Title%3A%20What%27s%20under%20the%20hood%3A%20Investigating%20Automatic%20Metrics%20on%20Meeting%0A%20%20Summarization%0AAuthor%3A%20Frederic%20Kirstein%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20%20%20Meeting%20summarization%20has%20become%20a%20critical%20task%20considering%20the%20increase%20in%0Aonline%20interactions.%20While%20new%20techniques%20are%20introduced%20regularly%2C%20their%0Aevaluation%20uses%20metrics%20not%20designed%20to%20capture%20meeting-specific%20errors%2C%0Aundermining%20effective%20evaluation.%20This%20paper%20investigates%20what%20the%20frequently%0Aused%20automatic%20metrics%20capture%20and%20which%20errors%20they%20mask%20by%20correlating%0Aautomatic%20metric%20scores%20with%20human%20evaluations%20across%20a%20broad%20error%20taxonomy.%0AWe%20commence%20with%20a%20comprehensive%20literature%20review%20on%20English%20meeting%0Asummarization%20to%20define%20key%20challenges%20like%20speaker%20dynamics%20and%20contextual%0Aturn-taking%20and%20error%20types%20such%20as%20missing%20information%20and%20linguistic%0Ainaccuracy%2C%20concepts%20previously%20loosely%20defined%20in%20the%20field.%20We%20examine%20the%0Arelationship%20between%20characteristic%20challenges%20and%20errors%20by%20using%20annotated%0Atranscripts%20and%20summaries%20from%20Transformer-based%20sequence-to-sequence%20and%0Aautoregressive%20models%20from%20the%20general%20summary%20QMSum%20dataset.%20Through%0Aexperimental%20validation%2C%20we%20find%20that%20different%20model%20architectures%20respond%0Avariably%20to%20challenges%20in%20meeting%20transcripts%2C%20resulting%20in%20different%0Apronounced%20links%20between%20challenges%20and%20errors.%20Current%20default-used%20metrics%0Astruggle%20to%20capture%20observable%20errors%2C%20showing%20weak%20to%20mid-correlations%2C%20while%0Aa%20third%20of%20the%20correlations%20show%20trends%20of%20error%20masking.%20Only%20a%20subset%20reacts%0Aaccurately%20to%20specific%20errors%2C%20while%20most%20correlations%20show%20either%0Aunresponsiveness%20or%20failure%20to%20reflect%20the%20error%27s%20impact%20on%20summary%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2527s%2520under%2520the%2520hood%253A%2520Investigating%2520Automatic%2520Metrics%2520on%2520Meeting%250A%2520%2520Summarization%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Jan%2520Philip%2520Wahle%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3D%2520%2520Meeting%2520summarization%2520has%2520become%2520a%2520critical%2520task%2520considering%2520the%2520increase%2520in%250Aonline%2520interactions.%2520While%2520new%2520techniques%2520are%2520introduced%2520regularly%252C%2520their%250Aevaluation%2520uses%2520metrics%2520not%2520designed%2520to%2520capture%2520meeting-specific%2520errors%252C%250Aundermining%2520effective%2520evaluation.%2520This%2520paper%2520investigates%2520what%2520the%2520frequently%250Aused%2520automatic%2520metrics%2520capture%2520and%2520which%2520errors%2520they%2520mask%2520by%2520correlating%250Aautomatic%2520metric%2520scores%2520with%2520human%2520evaluations%2520across%2520a%2520broad%2520error%2520taxonomy.%250AWe%2520commence%2520with%2520a%2520comprehensive%2520literature%2520review%2520on%2520English%2520meeting%250Asummarization%2520to%2520define%2520key%2520challenges%2520like%2520speaker%2520dynamics%2520and%2520contextual%250Aturn-taking%2520and%2520error%2520types%2520such%2520as%2520missing%2520information%2520and%2520linguistic%250Ainaccuracy%252C%2520concepts%2520previously%2520loosely%2520defined%2520in%2520the%2520field.%2520We%2520examine%2520the%250Arelationship%2520between%2520characteristic%2520challenges%2520and%2520errors%2520by%2520using%2520annotated%250Atranscripts%2520and%2520summaries%2520from%2520Transformer-based%2520sequence-to-sequence%2520and%250Aautoregressive%2520models%2520from%2520the%2520general%2520summary%2520QMSum%2520dataset.%2520Through%250Aexperimental%2520validation%252C%2520we%2520find%2520that%2520different%2520model%2520architectures%2520respond%250Avariably%2520to%2520challenges%2520in%2520meeting%2520transcripts%252C%2520resulting%2520in%2520different%250Apronounced%2520links%2520between%2520challenges%2520and%2520errors.%2520Current%2520default-used%2520metrics%250Astruggle%2520to%2520capture%2520observable%2520errors%252C%2520showing%2520weak%2520to%2520mid-correlations%252C%2520while%250Aa%2520third%2520of%2520the%2520correlations%2520show%2520trends%2520of%2520error%2520masking.%2520Only%2520a%2520subset%2520reacts%250Aaccurately%2520to%2520specific%2520errors%252C%2520while%2520most%2520correlations%2520show%2520either%250Aunresponsiveness%2520or%2520failure%2520to%2520reflect%2520the%2520error%2527s%2520impact%2520on%2520summary%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%27s%20under%20the%20hood%3A%20Investigating%20Automatic%20Metrics%20on%20Meeting%0A%20%20Summarization&entry.906535625=Frederic%20Kirstein%20and%20Jan%20Philip%20Wahle%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=%20%20Meeting%20summarization%20has%20become%20a%20critical%20task%20considering%20the%20increase%20in%0Aonline%20interactions.%20While%20new%20techniques%20are%20introduced%20regularly%2C%20their%0Aevaluation%20uses%20metrics%20not%20designed%20to%20capture%20meeting-specific%20errors%2C%0Aundermining%20effective%20evaluation.%20This%20paper%20investigates%20what%20the%20frequently%0Aused%20automatic%20metrics%20capture%20and%20which%20errors%20they%20mask%20by%20correlating%0Aautomatic%20metric%20scores%20with%20human%20evaluations%20across%20a%20broad%20error%20taxonomy.%0AWe%20commence%20with%20a%20comprehensive%20literature%20review%20on%20English%20meeting%0Asummarization%20to%20define%20key%20challenges%20like%20speaker%20dynamics%20and%20contextual%0Aturn-taking%20and%20error%20types%20such%20as%20missing%20information%20and%20linguistic%0Ainaccuracy%2C%20concepts%20previously%20loosely%20defined%20in%20the%20field.%20We%20examine%20the%0Arelationship%20between%20characteristic%20challenges%20and%20errors%20by%20using%20annotated%0Atranscripts%20and%20summaries%20from%20Transformer-based%20sequence-to-sequence%20and%0Aautoregressive%20models%20from%20the%20general%20summary%20QMSum%20dataset.%20Through%0Aexperimental%20validation%2C%20we%20find%20that%20different%20model%20architectures%20respond%0Avariably%20to%20challenges%20in%20meeting%20transcripts%2C%20resulting%20in%20different%0Apronounced%20links%20between%20challenges%20and%20errors.%20Current%20default-used%20metrics%0Astruggle%20to%20capture%20observable%20errors%2C%20showing%20weak%20to%20mid-correlations%2C%20while%0Aa%20third%20of%20the%20correlations%20show%20trends%20of%20error%20masking.%20Only%20a%20subset%20reacts%0Aaccurately%20to%20specific%20errors%2C%20while%20most%20correlations%20show%20either%0Aunresponsiveness%20or%20failure%20to%20reflect%20the%20error%27s%20impact%20on%20summary%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11124v2&entry.124074799=Read"},
{"title": "Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose\n  Prior", "author": "Calvin-Khang Ta and Arindam Dutta and Rohit Kundu and Rohit Lal and Hannah Dela Cruz and Dripta S. Raychaudhuri and Amit Roy-Chowdhury", "abstract": "  The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human\npose estimation, providing a streamlined yet effective representation of the\nhuman body. However, ensuring the validity of SMPL configurations during tasks\nsuch as human mesh regression remains a significant challenge , highlighting\nthe necessity for a robust human pose prior capable of discerning realistic\nhuman poses. To address this, we introduce MOPED:\n\\underline{M}ulti-m\\underline{O}dal \\underline{P}os\\underline{E}\n\\underline{D}iffuser. MOPED is the first method to leverage a novel multi-modal\nconditional diffusion model as a prior for SMPL pose parameters. Our method\noffers powerful unconditional pose generation with the ability to condition on\nmulti-modal inputs such as images and text. This capability enhances the\napplicability of our approach by incorporating additional context often\noverlooked in traditional pose priors. Extensive experiments across three\ndistinct tasks-pose estimation, pose denoising, and pose completion-demonstrate\nthat our multi-modal diffusion model-based prior significantly outperforms\nexisting methods. These results indicate that our model captures a broader\nspectrum of plausible human poses.\n", "link": "http://arxiv.org/abs/2410.14540v1", "date": "2024-10-18", "relevancy": 1.8051, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6437}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6069}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Pose%20Diffuser%3A%20A%20Multimodal%20Generative%20Conditional%20Pose%0A%20%20Prior&body=Title%3A%20Multi-modal%20Pose%20Diffuser%3A%20A%20Multimodal%20Generative%20Conditional%20Pose%0A%20%20Prior%0AAuthor%3A%20Calvin-Khang%20Ta%20and%20Arindam%20Dutta%20and%20Rohit%20Kundu%20and%20Rohit%20Lal%20and%20Hannah%20Dela%20Cruz%20and%20Dripta%20S.%20Raychaudhuri%20and%20Amit%20Roy-Chowdhury%0AAbstract%3A%20%20%20The%20Skinned%20Multi-Person%20Linear%20%28SMPL%29%20model%20plays%20a%20crucial%20role%20in%203D%20human%0Apose%20estimation%2C%20providing%20a%20streamlined%20yet%20effective%20representation%20of%20the%0Ahuman%20body.%20However%2C%20ensuring%20the%20validity%20of%20SMPL%20configurations%20during%20tasks%0Asuch%20as%20human%20mesh%20regression%20remains%20a%20significant%20challenge%20%2C%20highlighting%0Athe%20necessity%20for%20a%20robust%20human%20pose%20prior%20capable%20of%20discerning%20realistic%0Ahuman%20poses.%20To%20address%20this%2C%20we%20introduce%20MOPED%3A%0A%5Cunderline%7BM%7Dulti-m%5Cunderline%7BO%7Ddal%20%5Cunderline%7BP%7Dos%5Cunderline%7BE%7D%0A%5Cunderline%7BD%7Diffuser.%20MOPED%20is%20the%20first%20method%20to%20leverage%20a%20novel%20multi-modal%0Aconditional%20diffusion%20model%20as%20a%20prior%20for%20SMPL%20pose%20parameters.%20Our%20method%0Aoffers%20powerful%20unconditional%20pose%20generation%20with%20the%20ability%20to%20condition%20on%0Amulti-modal%20inputs%20such%20as%20images%20and%20text.%20This%20capability%20enhances%20the%0Aapplicability%20of%20our%20approach%20by%20incorporating%20additional%20context%20often%0Aoverlooked%20in%20traditional%20pose%20priors.%20Extensive%20experiments%20across%20three%0Adistinct%20tasks-pose%20estimation%2C%20pose%20denoising%2C%20and%20pose%20completion-demonstrate%0Athat%20our%20multi-modal%20diffusion%20model-based%20prior%20significantly%20outperforms%0Aexisting%20methods.%20These%20results%20indicate%20that%20our%20model%20captures%20a%20broader%0Aspectrum%20of%20plausible%20human%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Pose%2520Diffuser%253A%2520A%2520Multimodal%2520Generative%2520Conditional%2520Pose%250A%2520%2520Prior%26entry.906535625%3DCalvin-Khang%2520Ta%2520and%2520Arindam%2520Dutta%2520and%2520Rohit%2520Kundu%2520and%2520Rohit%2520Lal%2520and%2520Hannah%2520Dela%2520Cruz%2520and%2520Dripta%2520S.%2520Raychaudhuri%2520and%2520Amit%2520Roy-Chowdhury%26entry.1292438233%3D%2520%2520The%2520Skinned%2520Multi-Person%2520Linear%2520%2528SMPL%2529%2520model%2520plays%2520a%2520crucial%2520role%2520in%25203D%2520human%250Apose%2520estimation%252C%2520providing%2520a%2520streamlined%2520yet%2520effective%2520representation%2520of%2520the%250Ahuman%2520body.%2520However%252C%2520ensuring%2520the%2520validity%2520of%2520SMPL%2520configurations%2520during%2520tasks%250Asuch%2520as%2520human%2520mesh%2520regression%2520remains%2520a%2520significant%2520challenge%2520%252C%2520highlighting%250Athe%2520necessity%2520for%2520a%2520robust%2520human%2520pose%2520prior%2520capable%2520of%2520discerning%2520realistic%250Ahuman%2520poses.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MOPED%253A%250A%255Cunderline%257BM%257Dulti-m%255Cunderline%257BO%257Ddal%2520%255Cunderline%257BP%257Dos%255Cunderline%257BE%257D%250A%255Cunderline%257BD%257Diffuser.%2520MOPED%2520is%2520the%2520first%2520method%2520to%2520leverage%2520a%2520novel%2520multi-modal%250Aconditional%2520diffusion%2520model%2520as%2520a%2520prior%2520for%2520SMPL%2520pose%2520parameters.%2520Our%2520method%250Aoffers%2520powerful%2520unconditional%2520pose%2520generation%2520with%2520the%2520ability%2520to%2520condition%2520on%250Amulti-modal%2520inputs%2520such%2520as%2520images%2520and%2520text.%2520This%2520capability%2520enhances%2520the%250Aapplicability%2520of%2520our%2520approach%2520by%2520incorporating%2520additional%2520context%2520often%250Aoverlooked%2520in%2520traditional%2520pose%2520priors.%2520Extensive%2520experiments%2520across%2520three%250Adistinct%2520tasks-pose%2520estimation%252C%2520pose%2520denoising%252C%2520and%2520pose%2520completion-demonstrate%250Athat%2520our%2520multi-modal%2520diffusion%2520model-based%2520prior%2520significantly%2520outperforms%250Aexisting%2520methods.%2520These%2520results%2520indicate%2520that%2520our%2520model%2520captures%2520a%2520broader%250Aspectrum%2520of%2520plausible%2520human%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Pose%20Diffuser%3A%20A%20Multimodal%20Generative%20Conditional%20Pose%0A%20%20Prior&entry.906535625=Calvin-Khang%20Ta%20and%20Arindam%20Dutta%20and%20Rohit%20Kundu%20and%20Rohit%20Lal%20and%20Hannah%20Dela%20Cruz%20and%20Dripta%20S.%20Raychaudhuri%20and%20Amit%20Roy-Chowdhury&entry.1292438233=%20%20The%20Skinned%20Multi-Person%20Linear%20%28SMPL%29%20model%20plays%20a%20crucial%20role%20in%203D%20human%0Apose%20estimation%2C%20providing%20a%20streamlined%20yet%20effective%20representation%20of%20the%0Ahuman%20body.%20However%2C%20ensuring%20the%20validity%20of%20SMPL%20configurations%20during%20tasks%0Asuch%20as%20human%20mesh%20regression%20remains%20a%20significant%20challenge%20%2C%20highlighting%0Athe%20necessity%20for%20a%20robust%20human%20pose%20prior%20capable%20of%20discerning%20realistic%0Ahuman%20poses.%20To%20address%20this%2C%20we%20introduce%20MOPED%3A%0A%5Cunderline%7BM%7Dulti-m%5Cunderline%7BO%7Ddal%20%5Cunderline%7BP%7Dos%5Cunderline%7BE%7D%0A%5Cunderline%7BD%7Diffuser.%20MOPED%20is%20the%20first%20method%20to%20leverage%20a%20novel%20multi-modal%0Aconditional%20diffusion%20model%20as%20a%20prior%20for%20SMPL%20pose%20parameters.%20Our%20method%0Aoffers%20powerful%20unconditional%20pose%20generation%20with%20the%20ability%20to%20condition%20on%0Amulti-modal%20inputs%20such%20as%20images%20and%20text.%20This%20capability%20enhances%20the%0Aapplicability%20of%20our%20approach%20by%20incorporating%20additional%20context%20often%0Aoverlooked%20in%20traditional%20pose%20priors.%20Extensive%20experiments%20across%20three%0Adistinct%20tasks-pose%20estimation%2C%20pose%20denoising%2C%20and%20pose%20completion-demonstrate%0Athat%20our%20multi-modal%20diffusion%20model-based%20prior%20significantly%20outperforms%0Aexisting%20methods.%20These%20results%20indicate%20that%20our%20model%20captures%20a%20broader%0Aspectrum%20of%20plausible%20human%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14540v1&entry.124074799=Read"},
{"title": "LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight\n  Loco-Manipulators", "author": "Changyi Lin and Xingyu Liu and Yuxiang Yang and Yaru Niu and Wenhao Yu and Tingnan Zhang and Jie Tan and Byron Boots and Ding Zhao", "abstract": "  Quadrupedal robots have emerged as versatile agents capable of locomoting and\nmanipulating in complex environments. Traditional designs typically rely on the\nrobot's inherent body parts or incorporate top-mounted arms for manipulation\ntasks. However, these configurations may limit the robot's operational\ndexterity, efficiency and adaptability, particularly in cluttered or\nconstrained spaces. In this work, we present LocoMan, a dexterous quadrupedal\nrobot with a novel morphology to perform versatile manipulation in diverse\nconstrained environments. By equipping a Unitree Go1 robot with two low-cost\nand lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan\nleverages the combined mobility and functionality of the legs and grippers for\ncomplex manipulation tasks that require precise 6D positioning of the end\neffector in a wide workspace. To harness the loco-manipulation capabilities of\nLocoMan, we introduce a unified control framework that extends the whole-body\ncontroller (WBC) to integrate the dynamics of loco-manipulators. Through\nexperiments, we validate that the proposed whole-body controller can accurately\nand stably follow desired 6D trajectories of the end effector and torso, which,\nwhen combined with the large workspace from our design, facilitates a diverse\nset of challenging dexterous loco-manipulation tasks in confined spaces, such\nas opening doors, plugging into sockets, picking objects in narrow and\nlow-lying spaces, and bimanual manipulation.\n", "link": "http://arxiv.org/abs/2403.18197v2", "date": "2024-10-18", "relevancy": 1.7069, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5909}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocoMan%3A%20Advancing%20Versatile%20Quadrupedal%20Dexterity%20with%20Lightweight%0A%20%20Loco-Manipulators&body=Title%3A%20LocoMan%3A%20Advancing%20Versatile%20Quadrupedal%20Dexterity%20with%20Lightweight%0A%20%20Loco-Manipulators%0AAuthor%3A%20Changyi%20Lin%20and%20Xingyu%20Liu%20and%20Yuxiang%20Yang%20and%20Yaru%20Niu%20and%20Wenhao%20Yu%20and%20Tingnan%20Zhang%20and%20Jie%20Tan%20and%20Byron%20Boots%20and%20Ding%20Zhao%0AAbstract%3A%20%20%20Quadrupedal%20robots%20have%20emerged%20as%20versatile%20agents%20capable%20of%20locomoting%20and%0Amanipulating%20in%20complex%20environments.%20Traditional%20designs%20typically%20rely%20on%20the%0Arobot%27s%20inherent%20body%20parts%20or%20incorporate%20top-mounted%20arms%20for%20manipulation%0Atasks.%20However%2C%20these%20configurations%20may%20limit%20the%20robot%27s%20operational%0Adexterity%2C%20efficiency%20and%20adaptability%2C%20particularly%20in%20cluttered%20or%0Aconstrained%20spaces.%20In%20this%20work%2C%20we%20present%20LocoMan%2C%20a%20dexterous%20quadrupedal%0Arobot%20with%20a%20novel%20morphology%20to%20perform%20versatile%20manipulation%20in%20diverse%0Aconstrained%20environments.%20By%20equipping%20a%20Unitree%20Go1%20robot%20with%20two%20low-cost%0Aand%20lightweight%20modular%203-DoF%20loco-manipulators%20on%20its%20front%20calves%2C%20LocoMan%0Aleverages%20the%20combined%20mobility%20and%20functionality%20of%20the%20legs%20and%20grippers%20for%0Acomplex%20manipulation%20tasks%20that%20require%20precise%206D%20positioning%20of%20the%20end%0Aeffector%20in%20a%20wide%20workspace.%20To%20harness%20the%20loco-manipulation%20capabilities%20of%0ALocoMan%2C%20we%20introduce%20a%20unified%20control%20framework%20that%20extends%20the%20whole-body%0Acontroller%20%28WBC%29%20to%20integrate%20the%20dynamics%20of%20loco-manipulators.%20Through%0Aexperiments%2C%20we%20validate%20that%20the%20proposed%20whole-body%20controller%20can%20accurately%0Aand%20stably%20follow%20desired%206D%20trajectories%20of%20the%20end%20effector%20and%20torso%2C%20which%2C%0Awhen%20combined%20with%20the%20large%20workspace%20from%20our%20design%2C%20facilitates%20a%20diverse%0Aset%20of%20challenging%20dexterous%20loco-manipulation%20tasks%20in%20confined%20spaces%2C%20such%0Aas%20opening%20doors%2C%20plugging%20into%20sockets%2C%20picking%20objects%20in%20narrow%20and%0Alow-lying%20spaces%2C%20and%20bimanual%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocoMan%253A%2520Advancing%2520Versatile%2520Quadrupedal%2520Dexterity%2520with%2520Lightweight%250A%2520%2520Loco-Manipulators%26entry.906535625%3DChangyi%2520Lin%2520and%2520Xingyu%2520Liu%2520and%2520Yuxiang%2520Yang%2520and%2520Yaru%2520Niu%2520and%2520Wenhao%2520Yu%2520and%2520Tingnan%2520Zhang%2520and%2520Jie%2520Tan%2520and%2520Byron%2520Boots%2520and%2520Ding%2520Zhao%26entry.1292438233%3D%2520%2520Quadrupedal%2520robots%2520have%2520emerged%2520as%2520versatile%2520agents%2520capable%2520of%2520locomoting%2520and%250Amanipulating%2520in%2520complex%2520environments.%2520Traditional%2520designs%2520typically%2520rely%2520on%2520the%250Arobot%2527s%2520inherent%2520body%2520parts%2520or%2520incorporate%2520top-mounted%2520arms%2520for%2520manipulation%250Atasks.%2520However%252C%2520these%2520configurations%2520may%2520limit%2520the%2520robot%2527s%2520operational%250Adexterity%252C%2520efficiency%2520and%2520adaptability%252C%2520particularly%2520in%2520cluttered%2520or%250Aconstrained%2520spaces.%2520In%2520this%2520work%252C%2520we%2520present%2520LocoMan%252C%2520a%2520dexterous%2520quadrupedal%250Arobot%2520with%2520a%2520novel%2520morphology%2520to%2520perform%2520versatile%2520manipulation%2520in%2520diverse%250Aconstrained%2520environments.%2520By%2520equipping%2520a%2520Unitree%2520Go1%2520robot%2520with%2520two%2520low-cost%250Aand%2520lightweight%2520modular%25203-DoF%2520loco-manipulators%2520on%2520its%2520front%2520calves%252C%2520LocoMan%250Aleverages%2520the%2520combined%2520mobility%2520and%2520functionality%2520of%2520the%2520legs%2520and%2520grippers%2520for%250Acomplex%2520manipulation%2520tasks%2520that%2520require%2520precise%25206D%2520positioning%2520of%2520the%2520end%250Aeffector%2520in%2520a%2520wide%2520workspace.%2520To%2520harness%2520the%2520loco-manipulation%2520capabilities%2520of%250ALocoMan%252C%2520we%2520introduce%2520a%2520unified%2520control%2520framework%2520that%2520extends%2520the%2520whole-body%250Acontroller%2520%2528WBC%2529%2520to%2520integrate%2520the%2520dynamics%2520of%2520loco-manipulators.%2520Through%250Aexperiments%252C%2520we%2520validate%2520that%2520the%2520proposed%2520whole-body%2520controller%2520can%2520accurately%250Aand%2520stably%2520follow%2520desired%25206D%2520trajectories%2520of%2520the%2520end%2520effector%2520and%2520torso%252C%2520which%252C%250Awhen%2520combined%2520with%2520the%2520large%2520workspace%2520from%2520our%2520design%252C%2520facilitates%2520a%2520diverse%250Aset%2520of%2520challenging%2520dexterous%2520loco-manipulation%2520tasks%2520in%2520confined%2520spaces%252C%2520such%250Aas%2520opening%2520doors%252C%2520plugging%2520into%2520sockets%252C%2520picking%2520objects%2520in%2520narrow%2520and%250Alow-lying%2520spaces%252C%2520and%2520bimanual%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocoMan%3A%20Advancing%20Versatile%20Quadrupedal%20Dexterity%20with%20Lightweight%0A%20%20Loco-Manipulators&entry.906535625=Changyi%20Lin%20and%20Xingyu%20Liu%20and%20Yuxiang%20Yang%20and%20Yaru%20Niu%20and%20Wenhao%20Yu%20and%20Tingnan%20Zhang%20and%20Jie%20Tan%20and%20Byron%20Boots%20and%20Ding%20Zhao&entry.1292438233=%20%20Quadrupedal%20robots%20have%20emerged%20as%20versatile%20agents%20capable%20of%20locomoting%20and%0Amanipulating%20in%20complex%20environments.%20Traditional%20designs%20typically%20rely%20on%20the%0Arobot%27s%20inherent%20body%20parts%20or%20incorporate%20top-mounted%20arms%20for%20manipulation%0Atasks.%20However%2C%20these%20configurations%20may%20limit%20the%20robot%27s%20operational%0Adexterity%2C%20efficiency%20and%20adaptability%2C%20particularly%20in%20cluttered%20or%0Aconstrained%20spaces.%20In%20this%20work%2C%20we%20present%20LocoMan%2C%20a%20dexterous%20quadrupedal%0Arobot%20with%20a%20novel%20morphology%20to%20perform%20versatile%20manipulation%20in%20diverse%0Aconstrained%20environments.%20By%20equipping%20a%20Unitree%20Go1%20robot%20with%20two%20low-cost%0Aand%20lightweight%20modular%203-DoF%20loco-manipulators%20on%20its%20front%20calves%2C%20LocoMan%0Aleverages%20the%20combined%20mobility%20and%20functionality%20of%20the%20legs%20and%20grippers%20for%0Acomplex%20manipulation%20tasks%20that%20require%20precise%206D%20positioning%20of%20the%20end%0Aeffector%20in%20a%20wide%20workspace.%20To%20harness%20the%20loco-manipulation%20capabilities%20of%0ALocoMan%2C%20we%20introduce%20a%20unified%20control%20framework%20that%20extends%20the%20whole-body%0Acontroller%20%28WBC%29%20to%20integrate%20the%20dynamics%20of%20loco-manipulators.%20Through%0Aexperiments%2C%20we%20validate%20that%20the%20proposed%20whole-body%20controller%20can%20accurately%0Aand%20stably%20follow%20desired%206D%20trajectories%20of%20the%20end%20effector%20and%20torso%2C%20which%2C%0Awhen%20combined%20with%20the%20large%20workspace%20from%20our%20design%2C%20facilitates%20a%20diverse%0Aset%20of%20challenging%20dexterous%20loco-manipulation%20tasks%20in%20confined%20spaces%2C%20such%0Aas%20opening%20doors%2C%20plugging%20into%20sockets%2C%20picking%20objects%20in%20narrow%20and%0Alow-lying%20spaces%2C%20and%20bimanual%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18197v2&entry.124074799=Read"},
{"title": "Measuring Diversity: Axioms and Challenges", "author": "Mikhail Mironov and Liudmila Prokhorenkova", "abstract": "  The concept of diversity is widely used in various applications: from image\nor molecule generation to recommender systems. Thus, being able to properly\nmeasure diversity is important. This paper addresses the problem of quantifying\ndiversity for a set of objects. First, we make a systematic review of existing\ndiversity measures and explore their undesirable behavior in some cases. Based\non this review, we formulate three desirable properties (axioms) of a reliable\ndiversity measure: monotonicity, uniqueness, and continuity. We show that none\nof the existing measures has all three properties and thus these measures are\nnot suitable for quantifying diversity. Then, we construct two examples of\nmeasures that have all the desirable properties, thus proving that the list of\naxioms is not self-contradicting. Unfortunately, the constructed examples are\ntoo computationally complex for practical use, thus we pose an open problem of\nconstructing a diversity measure that has all the listed properties and can be\ncomputed in practice.\n", "link": "http://arxiv.org/abs/2410.14556v1", "date": "2024-10-18", "relevancy": 1.6527, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Diversity%3A%20Axioms%20and%20Challenges&body=Title%3A%20Measuring%20Diversity%3A%20Axioms%20and%20Challenges%0AAuthor%3A%20Mikhail%20Mironov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20The%20concept%20of%20diversity%20is%20widely%20used%20in%20various%20applications%3A%20from%20image%0Aor%20molecule%20generation%20to%20recommender%20systems.%20Thus%2C%20being%20able%20to%20properly%0Ameasure%20diversity%20is%20important.%20This%20paper%20addresses%20the%20problem%20of%20quantifying%0Adiversity%20for%20a%20set%20of%20objects.%20First%2C%20we%20make%20a%20systematic%20review%20of%20existing%0Adiversity%20measures%20and%20explore%20their%20undesirable%20behavior%20in%20some%20cases.%20Based%0Aon%20this%20review%2C%20we%20formulate%20three%20desirable%20properties%20%28axioms%29%20of%20a%20reliable%0Adiversity%20measure%3A%20monotonicity%2C%20uniqueness%2C%20and%20continuity.%20We%20show%20that%20none%0Aof%20the%20existing%20measures%20has%20all%20three%20properties%20and%20thus%20these%20measures%20are%0Anot%20suitable%20for%20quantifying%20diversity.%20Then%2C%20we%20construct%20two%20examples%20of%0Ameasures%20that%20have%20all%20the%20desirable%20properties%2C%20thus%20proving%20that%20the%20list%20of%0Aaxioms%20is%20not%20self-contradicting.%20Unfortunately%2C%20the%20constructed%20examples%20are%0Atoo%20computationally%20complex%20for%20practical%20use%2C%20thus%20we%20pose%20an%20open%20problem%20of%0Aconstructing%20a%20diversity%20measure%20that%20has%20all%20the%20listed%20properties%20and%20can%20be%0Acomputed%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Diversity%253A%2520Axioms%2520and%2520Challenges%26entry.906535625%3DMikhail%2520Mironov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520diversity%2520is%2520widely%2520used%2520in%2520various%2520applications%253A%2520from%2520image%250Aor%2520molecule%2520generation%2520to%2520recommender%2520systems.%2520Thus%252C%2520being%2520able%2520to%2520properly%250Ameasure%2520diversity%2520is%2520important.%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520quantifying%250Adiversity%2520for%2520a%2520set%2520of%2520objects.%2520First%252C%2520we%2520make%2520a%2520systematic%2520review%2520of%2520existing%250Adiversity%2520measures%2520and%2520explore%2520their%2520undesirable%2520behavior%2520in%2520some%2520cases.%2520Based%250Aon%2520this%2520review%252C%2520we%2520formulate%2520three%2520desirable%2520properties%2520%2528axioms%2529%2520of%2520a%2520reliable%250Adiversity%2520measure%253A%2520monotonicity%252C%2520uniqueness%252C%2520and%2520continuity.%2520We%2520show%2520that%2520none%250Aof%2520the%2520existing%2520measures%2520has%2520all%2520three%2520properties%2520and%2520thus%2520these%2520measures%2520are%250Anot%2520suitable%2520for%2520quantifying%2520diversity.%2520Then%252C%2520we%2520construct%2520two%2520examples%2520of%250Ameasures%2520that%2520have%2520all%2520the%2520desirable%2520properties%252C%2520thus%2520proving%2520that%2520the%2520list%2520of%250Aaxioms%2520is%2520not%2520self-contradicting.%2520Unfortunately%252C%2520the%2520constructed%2520examples%2520are%250Atoo%2520computationally%2520complex%2520for%2520practical%2520use%252C%2520thus%2520we%2520pose%2520an%2520open%2520problem%2520of%250Aconstructing%2520a%2520diversity%2520measure%2520that%2520has%2520all%2520the%2520listed%2520properties%2520and%2520can%2520be%250Acomputed%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Diversity%3A%20Axioms%20and%20Challenges&entry.906535625=Mikhail%20Mironov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20The%20concept%20of%20diversity%20is%20widely%20used%20in%20various%20applications%3A%20from%20image%0Aor%20molecule%20generation%20to%20recommender%20systems.%20Thus%2C%20being%20able%20to%20properly%0Ameasure%20diversity%20is%20important.%20This%20paper%20addresses%20the%20problem%20of%20quantifying%0Adiversity%20for%20a%20set%20of%20objects.%20First%2C%20we%20make%20a%20systematic%20review%20of%20existing%0Adiversity%20measures%20and%20explore%20their%20undesirable%20behavior%20in%20some%20cases.%20Based%0Aon%20this%20review%2C%20we%20formulate%20three%20desirable%20properties%20%28axioms%29%20of%20a%20reliable%0Adiversity%20measure%3A%20monotonicity%2C%20uniqueness%2C%20and%20continuity.%20We%20show%20that%20none%0Aof%20the%20existing%20measures%20has%20all%20three%20properties%20and%20thus%20these%20measures%20are%0Anot%20suitable%20for%20quantifying%20diversity.%20Then%2C%20we%20construct%20two%20examples%20of%0Ameasures%20that%20have%20all%20the%20desirable%20properties%2C%20thus%20proving%20that%20the%20list%20of%0Aaxioms%20is%20not%20self-contradicting.%20Unfortunately%2C%20the%20constructed%20examples%20are%0Atoo%20computationally%20complex%20for%20practical%20use%2C%20thus%20we%20pose%20an%20open%20problem%20of%0Aconstructing%20a%20diversity%20measure%20that%20has%20all%20the%20listed%20properties%20and%20can%20be%0Acomputed%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14556v1&entry.124074799=Read"},
{"title": "Learning diffusion at lightspeed", "author": "Antonio Terpin and Nicolas Lanzetti and Martin Gadea and Florian D\u00f6rfler", "abstract": "  Diffusion regulates numerous natural processes and the dynamics of many\nsuccessful generative models. Existing models to learn the diffusion terms from\nobservational data rely on complex bilevel optimization problems and model only\nthe drift of the system. We propose a new simple model, JKOnet*, which bypasses\nthe complexity of existing architectures while presenting significantly\nenhanced representational capabilities: JKOnet* recovers the potential,\ninteraction, and internal energy components of the underlying diffusion\nprocess. JKOnet* minimizes a simple quadratic loss and outperforms other\nbaselines in terms of sample efficiency, computational complexity, and\naccuracy. Additionally, JKOnet* provides a closed-form optimal solution for\nlinearly parametrized functionals, and, when applied to predict the evolution\nof cellular processes from real-world data, it achieves state-of-the-art\naccuracy at a fraction of the computational cost of all existing methods. Our\nmethodology is based on the interpretation of diffusion processes as\nenergy-minimizing trajectories in the probability space via the so-called JKO\nscheme, which we study via its first-order optimality conditions.\n", "link": "http://arxiv.org/abs/2406.12616v2", "date": "2024-10-18", "relevancy": 1.5948, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.603}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5122}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20diffusion%20at%20lightspeed&body=Title%3A%20Learning%20diffusion%20at%20lightspeed%0AAuthor%3A%20Antonio%20Terpin%20and%20Nicolas%20Lanzetti%20and%20Martin%20Gadea%20and%20Florian%20D%C3%B6rfler%0AAbstract%3A%20%20%20Diffusion%20regulates%20numerous%20natural%20processes%20and%20the%20dynamics%20of%20many%0Asuccessful%20generative%20models.%20Existing%20models%20to%20learn%20the%20diffusion%20terms%20from%0Aobservational%20data%20rely%20on%20complex%20bilevel%20optimization%20problems%20and%20model%20only%0Athe%20drift%20of%20the%20system.%20We%20propose%20a%20new%20simple%20model%2C%20JKOnet%2A%2C%20which%20bypasses%0Athe%20complexity%20of%20existing%20architectures%20while%20presenting%20significantly%0Aenhanced%20representational%20capabilities%3A%20JKOnet%2A%20recovers%20the%20potential%2C%0Ainteraction%2C%20and%20internal%20energy%20components%20of%20the%20underlying%20diffusion%0Aprocess.%20JKOnet%2A%20minimizes%20a%20simple%20quadratic%20loss%20and%20outperforms%20other%0Abaselines%20in%20terms%20of%20sample%20efficiency%2C%20computational%20complexity%2C%20and%0Aaccuracy.%20Additionally%2C%20JKOnet%2A%20provides%20a%20closed-form%20optimal%20solution%20for%0Alinearly%20parametrized%20functionals%2C%20and%2C%20when%20applied%20to%20predict%20the%20evolution%0Aof%20cellular%20processes%20from%20real-world%20data%2C%20it%20achieves%20state-of-the-art%0Aaccuracy%20at%20a%20fraction%20of%20the%20computational%20cost%20of%20all%20existing%20methods.%20Our%0Amethodology%20is%20based%20on%20the%20interpretation%20of%20diffusion%20processes%20as%0Aenergy-minimizing%20trajectories%20in%20the%20probability%20space%20via%20the%20so-called%20JKO%0Ascheme%2C%20which%20we%20study%20via%20its%20first-order%20optimality%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520diffusion%2520at%2520lightspeed%26entry.906535625%3DAntonio%2520Terpin%2520and%2520Nicolas%2520Lanzetti%2520and%2520Martin%2520Gadea%2520and%2520Florian%2520D%25C3%25B6rfler%26entry.1292438233%3D%2520%2520Diffusion%2520regulates%2520numerous%2520natural%2520processes%2520and%2520the%2520dynamics%2520of%2520many%250Asuccessful%2520generative%2520models.%2520Existing%2520models%2520to%2520learn%2520the%2520diffusion%2520terms%2520from%250Aobservational%2520data%2520rely%2520on%2520complex%2520bilevel%2520optimization%2520problems%2520and%2520model%2520only%250Athe%2520drift%2520of%2520the%2520system.%2520We%2520propose%2520a%2520new%2520simple%2520model%252C%2520JKOnet%252A%252C%2520which%2520bypasses%250Athe%2520complexity%2520of%2520existing%2520architectures%2520while%2520presenting%2520significantly%250Aenhanced%2520representational%2520capabilities%253A%2520JKOnet%252A%2520recovers%2520the%2520potential%252C%250Ainteraction%252C%2520and%2520internal%2520energy%2520components%2520of%2520the%2520underlying%2520diffusion%250Aprocess.%2520JKOnet%252A%2520minimizes%2520a%2520simple%2520quadratic%2520loss%2520and%2520outperforms%2520other%250Abaselines%2520in%2520terms%2520of%2520sample%2520efficiency%252C%2520computational%2520complexity%252C%2520and%250Aaccuracy.%2520Additionally%252C%2520JKOnet%252A%2520provides%2520a%2520closed-form%2520optimal%2520solution%2520for%250Alinearly%2520parametrized%2520functionals%252C%2520and%252C%2520when%2520applied%2520to%2520predict%2520the%2520evolution%250Aof%2520cellular%2520processes%2520from%2520real-world%2520data%252C%2520it%2520achieves%2520state-of-the-art%250Aaccuracy%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost%2520of%2520all%2520existing%2520methods.%2520Our%250Amethodology%2520is%2520based%2520on%2520the%2520interpretation%2520of%2520diffusion%2520processes%2520as%250Aenergy-minimizing%2520trajectories%2520in%2520the%2520probability%2520space%2520via%2520the%2520so-called%2520JKO%250Ascheme%252C%2520which%2520we%2520study%2520via%2520its%2520first-order%2520optimality%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20diffusion%20at%20lightspeed&entry.906535625=Antonio%20Terpin%20and%20Nicolas%20Lanzetti%20and%20Martin%20Gadea%20and%20Florian%20D%C3%B6rfler&entry.1292438233=%20%20Diffusion%20regulates%20numerous%20natural%20processes%20and%20the%20dynamics%20of%20many%0Asuccessful%20generative%20models.%20Existing%20models%20to%20learn%20the%20diffusion%20terms%20from%0Aobservational%20data%20rely%20on%20complex%20bilevel%20optimization%20problems%20and%20model%20only%0Athe%20drift%20of%20the%20system.%20We%20propose%20a%20new%20simple%20model%2C%20JKOnet%2A%2C%20which%20bypasses%0Athe%20complexity%20of%20existing%20architectures%20while%20presenting%20significantly%0Aenhanced%20representational%20capabilities%3A%20JKOnet%2A%20recovers%20the%20potential%2C%0Ainteraction%2C%20and%20internal%20energy%20components%20of%20the%20underlying%20diffusion%0Aprocess.%20JKOnet%2A%20minimizes%20a%20simple%20quadratic%20loss%20and%20outperforms%20other%0Abaselines%20in%20terms%20of%20sample%20efficiency%2C%20computational%20complexity%2C%20and%0Aaccuracy.%20Additionally%2C%20JKOnet%2A%20provides%20a%20closed-form%20optimal%20solution%20for%0Alinearly%20parametrized%20functionals%2C%20and%2C%20when%20applied%20to%20predict%20the%20evolution%0Aof%20cellular%20processes%20from%20real-world%20data%2C%20it%20achieves%20state-of-the-art%0Aaccuracy%20at%20a%20fraction%20of%20the%20computational%20cost%20of%20all%20existing%20methods.%20Our%0Amethodology%20is%20based%20on%20the%20interpretation%20of%20diffusion%20processes%20as%0Aenergy-minimizing%20trajectories%20in%20the%20probability%20space%20via%20the%20so-called%20JKO%0Ascheme%2C%20which%20we%20study%20via%20its%20first-order%20optimality%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12616v2&entry.124074799=Read"},
{"title": "MCSFF: Multi-modal Consistency and Specificity Fusion Framework for\n  Entity Alignment", "author": "Wei Ai and Wen Deng and Hongyi Chen and Jiayi Du and Tao Meng and Yuntao Shou", "abstract": "  Multi-modal entity alignment (MMEA) is essential for enhancing knowledge\ngraphs and improving information retrieval and question-answering systems.\nExisting methods often focus on integrating modalities through their\ncomplementarity but overlook the specificity of each modality, which can\nobscure crucial features and reduce alignment accuracy. To solve this, we\npropose the Multi-modal Consistency and Specificity Fusion Framework (MCSFF),\nwhich innovatively integrates both complementary and specific aspects of\nmodalities. We utilize Scale Computing's hyper-converged infrastructure to\noptimize IT management and resource allocation in large-scale data processing.\nOur framework first computes similarity matrices for each modality using\nmodality embeddings to preserve their unique characteristics. Then, an\niterative update method denoises and enhances modality features to fully\nexpress critical information. Finally, we integrate the updated information\nfrom all modalities to create enriched and precise entity representations.\nExperiments show our method outperforms current state-of-the-art MMEA baselines\non the MMKG dataset, demonstrating its effectiveness and practical potential.\n", "link": "http://arxiv.org/abs/2410.14584v1", "date": "2024-10-18", "relevancy": 1.5631, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCSFF%3A%20Multi-modal%20Consistency%20and%20Specificity%20Fusion%20Framework%20for%0A%20%20Entity%20Alignment&body=Title%3A%20MCSFF%3A%20Multi-modal%20Consistency%20and%20Specificity%20Fusion%20Framework%20for%0A%20%20Entity%20Alignment%0AAuthor%3A%20Wei%20Ai%20and%20Wen%20Deng%20and%20Hongyi%20Chen%20and%20Jiayi%20Du%20and%20Tao%20Meng%20and%20Yuntao%20Shou%0AAbstract%3A%20%20%20Multi-modal%20entity%20alignment%20%28MMEA%29%20is%20essential%20for%20enhancing%20knowledge%0Agraphs%20and%20improving%20information%20retrieval%20and%20question-answering%20systems.%0AExisting%20methods%20often%20focus%20on%20integrating%20modalities%20through%20their%0Acomplementarity%20but%20overlook%20the%20specificity%20of%20each%20modality%2C%20which%20can%0Aobscure%20crucial%20features%20and%20reduce%20alignment%20accuracy.%20To%20solve%20this%2C%20we%0Apropose%20the%20Multi-modal%20Consistency%20and%20Specificity%20Fusion%20Framework%20%28MCSFF%29%2C%0Awhich%20innovatively%20integrates%20both%20complementary%20and%20specific%20aspects%20of%0Amodalities.%20We%20utilize%20Scale%20Computing%27s%20hyper-converged%20infrastructure%20to%0Aoptimize%20IT%20management%20and%20resource%20allocation%20in%20large-scale%20data%20processing.%0AOur%20framework%20first%20computes%20similarity%20matrices%20for%20each%20modality%20using%0Amodality%20embeddings%20to%20preserve%20their%20unique%20characteristics.%20Then%2C%20an%0Aiterative%20update%20method%20denoises%20and%20enhances%20modality%20features%20to%20fully%0Aexpress%20critical%20information.%20Finally%2C%20we%20integrate%20the%20updated%20information%0Afrom%20all%20modalities%20to%20create%20enriched%20and%20precise%20entity%20representations.%0AExperiments%20show%20our%20method%20outperforms%20current%20state-of-the-art%20MMEA%20baselines%0Aon%20the%20MMKG%20dataset%2C%20demonstrating%20its%20effectiveness%20and%20practical%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCSFF%253A%2520Multi-modal%2520Consistency%2520and%2520Specificity%2520Fusion%2520Framework%2520for%250A%2520%2520Entity%2520Alignment%26entry.906535625%3DWei%2520Ai%2520and%2520Wen%2520Deng%2520and%2520Hongyi%2520Chen%2520and%2520Jiayi%2520Du%2520and%2520Tao%2520Meng%2520and%2520Yuntao%2520Shou%26entry.1292438233%3D%2520%2520Multi-modal%2520entity%2520alignment%2520%2528MMEA%2529%2520is%2520essential%2520for%2520enhancing%2520knowledge%250Agraphs%2520and%2520improving%2520information%2520retrieval%2520and%2520question-answering%2520systems.%250AExisting%2520methods%2520often%2520focus%2520on%2520integrating%2520modalities%2520through%2520their%250Acomplementarity%2520but%2520overlook%2520the%2520specificity%2520of%2520each%2520modality%252C%2520which%2520can%250Aobscure%2520crucial%2520features%2520and%2520reduce%2520alignment%2520accuracy.%2520To%2520solve%2520this%252C%2520we%250Apropose%2520the%2520Multi-modal%2520Consistency%2520and%2520Specificity%2520Fusion%2520Framework%2520%2528MCSFF%2529%252C%250Awhich%2520innovatively%2520integrates%2520both%2520complementary%2520and%2520specific%2520aspects%2520of%250Amodalities.%2520We%2520utilize%2520Scale%2520Computing%2527s%2520hyper-converged%2520infrastructure%2520to%250Aoptimize%2520IT%2520management%2520and%2520resource%2520allocation%2520in%2520large-scale%2520data%2520processing.%250AOur%2520framework%2520first%2520computes%2520similarity%2520matrices%2520for%2520each%2520modality%2520using%250Amodality%2520embeddings%2520to%2520preserve%2520their%2520unique%2520characteristics.%2520Then%252C%2520an%250Aiterative%2520update%2520method%2520denoises%2520and%2520enhances%2520modality%2520features%2520to%2520fully%250Aexpress%2520critical%2520information.%2520Finally%252C%2520we%2520integrate%2520the%2520updated%2520information%250Afrom%2520all%2520modalities%2520to%2520create%2520enriched%2520and%2520precise%2520entity%2520representations.%250AExperiments%2520show%2520our%2520method%2520outperforms%2520current%2520state-of-the-art%2520MMEA%2520baselines%250Aon%2520the%2520MMKG%2520dataset%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520practical%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCSFF%3A%20Multi-modal%20Consistency%20and%20Specificity%20Fusion%20Framework%20for%0A%20%20Entity%20Alignment&entry.906535625=Wei%20Ai%20and%20Wen%20Deng%20and%20Hongyi%20Chen%20and%20Jiayi%20Du%20and%20Tao%20Meng%20and%20Yuntao%20Shou&entry.1292438233=%20%20Multi-modal%20entity%20alignment%20%28MMEA%29%20is%20essential%20for%20enhancing%20knowledge%0Agraphs%20and%20improving%20information%20retrieval%20and%20question-answering%20systems.%0AExisting%20methods%20often%20focus%20on%20integrating%20modalities%20through%20their%0Acomplementarity%20but%20overlook%20the%20specificity%20of%20each%20modality%2C%20which%20can%0Aobscure%20crucial%20features%20and%20reduce%20alignment%20accuracy.%20To%20solve%20this%2C%20we%0Apropose%20the%20Multi-modal%20Consistency%20and%20Specificity%20Fusion%20Framework%20%28MCSFF%29%2C%0Awhich%20innovatively%20integrates%20both%20complementary%20and%20specific%20aspects%20of%0Amodalities.%20We%20utilize%20Scale%20Computing%27s%20hyper-converged%20infrastructure%20to%0Aoptimize%20IT%20management%20and%20resource%20allocation%20in%20large-scale%20data%20processing.%0AOur%20framework%20first%20computes%20similarity%20matrices%20for%20each%20modality%20using%0Amodality%20embeddings%20to%20preserve%20their%20unique%20characteristics.%20Then%2C%20an%0Aiterative%20update%20method%20denoises%20and%20enhances%20modality%20features%20to%20fully%0Aexpress%20critical%20information.%20Finally%2C%20we%20integrate%20the%20updated%20information%0Afrom%20all%20modalities%20to%20create%20enriched%20and%20precise%20entity%20representations.%0AExperiments%20show%20our%20method%20outperforms%20current%20state-of-the-art%20MMEA%20baselines%0Aon%20the%20MMKG%20dataset%2C%20demonstrating%20its%20effectiveness%20and%20practical%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14584v1&entry.124074799=Read"},
{"title": "Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets", "author": "Namid R. Stillman and Rory Baggott", "abstract": "  Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.\n", "link": "http://arxiv.org/abs/2410.14587v1", "date": "2024-10-18", "relevancy": 1.55, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5358}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-Symbolic%20Traders%3A%20Assessing%20the%20Wisdom%20of%20AI%20Crowds%20in%20Markets&body=Title%3A%20Neuro-Symbolic%20Traders%3A%20Assessing%20the%20Wisdom%20of%20AI%20Crowds%20in%20Markets%0AAuthor%3A%20Namid%20R.%20Stillman%20and%20Rory%20Baggott%0AAbstract%3A%20%20%20Deep%20generative%20models%20are%20becoming%20increasingly%20used%20as%20tools%20for%20financial%0Aanalysis.%20However%2C%20it%20is%20unclear%20how%20these%20models%20will%20influence%20financial%0Amarkets%2C%20especially%20when%20they%20infer%20financial%20value%20in%20a%20semi-autonomous%20way.%0AIn%20this%20work%2C%20we%20explore%20the%20interplay%20between%20deep%20generative%20models%20and%0Amarket%20dynamics.%20We%20develop%20a%20form%20of%20virtual%20traders%20that%20use%20deep%20generative%0Amodels%20to%20make%20buy/sell%20decisions%2C%20which%20we%20term%20neuro-symbolic%20traders%2C%20and%0Aexpose%20them%20to%20a%20virtual%20market.%20Under%20our%20framework%2C%20neuro-symbolic%20traders%0Aare%20agents%20that%20use%20vision-language%20models%20to%20discover%20a%20model%20of%20the%0Afundamental%20value%20of%20an%20asset.%20Agents%20develop%20this%20model%20as%20a%20stochastic%0Adifferential%20equation%2C%20calibrated%20to%20market%20data%20using%20gradient%20descent.%20We%0Atest%20our%20neuro-symbolic%20traders%20on%20both%20synthetic%20data%20and%20real%20financial%20time%0Aseries%2C%20including%20an%20equity%20stock%2C%20commodity%2C%20and%20a%20foreign%20exchange%20pair.%20We%0Athen%20expose%20several%20groups%20of%20neuro-symbolic%20traders%20to%20a%20virtual%20market%0Aenvironment.%20This%20market%20environment%20allows%20for%20feedback%20between%20the%20traders%0Abelief%20of%20the%20underlying%20value%20to%20the%20observed%20price%20dynamics.%20We%20find%20that%0Athis%20leads%20to%20price%20suppression%20compared%20to%20the%20historical%20data%2C%20highlighting%20a%0Afuture%20risk%20to%20market%20stability.%20Our%20work%20is%20a%20first%20step%20towards%20quantifying%0Athe%20effect%20of%20deep%20generative%20agents%20on%20markets%20dynamics%20and%20sets%20out%20some%20of%0Athe%20potential%20risks%20and%20benefits%20of%20this%20approach%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-Symbolic%2520Traders%253A%2520Assessing%2520the%2520Wisdom%2520of%2520AI%2520Crowds%2520in%2520Markets%26entry.906535625%3DNamid%2520R.%2520Stillman%2520and%2520Rory%2520Baggott%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520are%2520becoming%2520increasingly%2520used%2520as%2520tools%2520for%2520financial%250Aanalysis.%2520However%252C%2520it%2520is%2520unclear%2520how%2520these%2520models%2520will%2520influence%2520financial%250Amarkets%252C%2520especially%2520when%2520they%2520infer%2520financial%2520value%2520in%2520a%2520semi-autonomous%2520way.%250AIn%2520this%2520work%252C%2520we%2520explore%2520the%2520interplay%2520between%2520deep%2520generative%2520models%2520and%250Amarket%2520dynamics.%2520We%2520develop%2520a%2520form%2520of%2520virtual%2520traders%2520that%2520use%2520deep%2520generative%250Amodels%2520to%2520make%2520buy/sell%2520decisions%252C%2520which%2520we%2520term%2520neuro-symbolic%2520traders%252C%2520and%250Aexpose%2520them%2520to%2520a%2520virtual%2520market.%2520Under%2520our%2520framework%252C%2520neuro-symbolic%2520traders%250Aare%2520agents%2520that%2520use%2520vision-language%2520models%2520to%2520discover%2520a%2520model%2520of%2520the%250Afundamental%2520value%2520of%2520an%2520asset.%2520Agents%2520develop%2520this%2520model%2520as%2520a%2520stochastic%250Adifferential%2520equation%252C%2520calibrated%2520to%2520market%2520data%2520using%2520gradient%2520descent.%2520We%250Atest%2520our%2520neuro-symbolic%2520traders%2520on%2520both%2520synthetic%2520data%2520and%2520real%2520financial%2520time%250Aseries%252C%2520including%2520an%2520equity%2520stock%252C%2520commodity%252C%2520and%2520a%2520foreign%2520exchange%2520pair.%2520We%250Athen%2520expose%2520several%2520groups%2520of%2520neuro-symbolic%2520traders%2520to%2520a%2520virtual%2520market%250Aenvironment.%2520This%2520market%2520environment%2520allows%2520for%2520feedback%2520between%2520the%2520traders%250Abelief%2520of%2520the%2520underlying%2520value%2520to%2520the%2520observed%2520price%2520dynamics.%2520We%2520find%2520that%250Athis%2520leads%2520to%2520price%2520suppression%2520compared%2520to%2520the%2520historical%2520data%252C%2520highlighting%2520a%250Afuture%2520risk%2520to%2520market%2520stability.%2520Our%2520work%2520is%2520a%2520first%2520step%2520towards%2520quantifying%250Athe%2520effect%2520of%2520deep%2520generative%2520agents%2520on%2520markets%2520dynamics%2520and%2520sets%2520out%2520some%2520of%250Athe%2520potential%2520risks%2520and%2520benefits%2520of%2520this%2520approach%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-Symbolic%20Traders%3A%20Assessing%20the%20Wisdom%20of%20AI%20Crowds%20in%20Markets&entry.906535625=Namid%20R.%20Stillman%20and%20Rory%20Baggott&entry.1292438233=%20%20Deep%20generative%20models%20are%20becoming%20increasingly%20used%20as%20tools%20for%20financial%0Aanalysis.%20However%2C%20it%20is%20unclear%20how%20these%20models%20will%20influence%20financial%0Amarkets%2C%20especially%20when%20they%20infer%20financial%20value%20in%20a%20semi-autonomous%20way.%0AIn%20this%20work%2C%20we%20explore%20the%20interplay%20between%20deep%20generative%20models%20and%0Amarket%20dynamics.%20We%20develop%20a%20form%20of%20virtual%20traders%20that%20use%20deep%20generative%0Amodels%20to%20make%20buy/sell%20decisions%2C%20which%20we%20term%20neuro-symbolic%20traders%2C%20and%0Aexpose%20them%20to%20a%20virtual%20market.%20Under%20our%20framework%2C%20neuro-symbolic%20traders%0Aare%20agents%20that%20use%20vision-language%20models%20to%20discover%20a%20model%20of%20the%0Afundamental%20value%20of%20an%20asset.%20Agents%20develop%20this%20model%20as%20a%20stochastic%0Adifferential%20equation%2C%20calibrated%20to%20market%20data%20using%20gradient%20descent.%20We%0Atest%20our%20neuro-symbolic%20traders%20on%20both%20synthetic%20data%20and%20real%20financial%20time%0Aseries%2C%20including%20an%20equity%20stock%2C%20commodity%2C%20and%20a%20foreign%20exchange%20pair.%20We%0Athen%20expose%20several%20groups%20of%20neuro-symbolic%20traders%20to%20a%20virtual%20market%0Aenvironment.%20This%20market%20environment%20allows%20for%20feedback%20between%20the%20traders%0Abelief%20of%20the%20underlying%20value%20to%20the%20observed%20price%20dynamics.%20We%20find%20that%0Athis%20leads%20to%20price%20suppression%20compared%20to%20the%20historical%20data%2C%20highlighting%20a%0Afuture%20risk%20to%20market%20stability.%20Our%20work%20is%20a%20first%20step%20towards%20quantifying%0Athe%20effect%20of%20deep%20generative%20agents%20on%20markets%20dynamics%20and%20sets%20out%20some%20of%0Athe%20potential%20risks%20and%20benefits%20of%20this%20approach%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14587v1&entry.124074799=Read"},
{"title": "Scalable Drift Monitoring in Medical Imaging AI", "author": "Jameson Merkow and Felix J. Dorfner and Xiyu Yang and Alexander Ersoy and Giridhar Dasegowda and Mannudeep Kalra and Matthew P. Lungren and Christopher P. Bridge and Ivan Tarapov", "abstract": "  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n", "link": "http://arxiv.org/abs/2410.13174v2", "date": "2024-10-18", "relevancy": 1.5442, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5241}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Drift%20Monitoring%20in%20Medical%20Imaging%20AI&body=Title%3A%20Scalable%20Drift%20Monitoring%20in%20Medical%20Imaging%20AI%0AAuthor%3A%20Jameson%20Merkow%20and%20Felix%20J.%20Dorfner%20and%20Xiyu%20Yang%20and%20Alexander%20Ersoy%20and%20Giridhar%20Dasegowda%20and%20Mannudeep%20Kalra%20and%20Matthew%20P.%20Lungren%20and%20Christopher%20P.%20Bridge%20and%20Ivan%20Tarapov%0AAbstract%3A%20%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20into%20medical%20imaging%20has%0Aadvanced%20clinical%20diagnostics%20but%20poses%20challenges%20in%20managing%20model%20drift%20and%0Aensuring%20long-term%20reliability.%20To%20address%20these%20challenges%2C%20we%20develop%20MMC%2B%2C%0Aan%20enhanced%20framework%20for%20scalable%20drift%20monitoring%2C%20building%20upon%20the%0ACheXstray%20framework%20that%20introduced%20real-time%20drift%20detection%20for%20medical%0Aimaging%20AI%20models%20using%20multi-modal%20data%20concordance.%20This%20work%20extends%20the%0Aoriginal%20framework%27s%20methodologies%2C%20providing%20a%20more%20scalable%20and%20adaptable%0Asolution%20for%20real-world%20healthcare%20settings%20and%20offers%20a%20reliable%20and%0Acost-effective%20alternative%20to%20continuous%20performance%20monitoring%20addressing%0Alimitations%20of%20both%20continuous%20and%20periodic%20monitoring%20methods.%20MMC%2B%20introduces%0Acritical%20improvements%20to%20the%20original%20framework%2C%20including%20more%20robust%20handling%0Aof%20diverse%20data%20streams%2C%20improved%20scalability%20with%20the%20integration%20of%0Afoundation%20models%20like%20MedImageInsight%20for%20high-dimensional%20image%20embeddings%0Awithout%20site-specific%20training%2C%20and%20the%20introduction%20of%20uncertainty%20bounds%20to%0Abetter%20capture%20drift%20in%20dynamic%20clinical%20environments.%20Validated%20with%0Areal-world%20data%20from%20Massachusetts%20General%20Hospital%20during%20the%20COVID-19%0Apandemic%2C%20MMC%2B%20effectively%20detects%20significant%20data%20shifts%20and%20correlates%20them%0Awith%20model%20performance%20changes.%20While%20not%20directly%20predicting%20performance%0Adegradation%2C%20MMC%2B%20serves%20as%20an%20early%20warning%20system%2C%20indicating%20when%20AI%20systems%0Amay%20deviate%20from%20acceptable%20performance%20bounds%20and%20enabling%20timely%0Ainterventions.%20By%20emphasizing%20the%20importance%20of%20monitoring%20diverse%20data%20streams%0Aand%20evaluating%20data%20shifts%20alongside%20model%20performance%2C%20this%20work%20contributes%0Ato%20the%20broader%20adoption%20and%20integration%20of%20AI%20solutions%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Drift%2520Monitoring%2520in%2520Medical%2520Imaging%2520AI%26entry.906535625%3DJameson%2520Merkow%2520and%2520Felix%2520J.%2520Dorfner%2520and%2520Xiyu%2520Yang%2520and%2520Alexander%2520Ersoy%2520and%2520Giridhar%2520Dasegowda%2520and%2520Mannudeep%2520Kalra%2520and%2520Matthew%2520P.%2520Lungren%2520and%2520Christopher%2520P.%2520Bridge%2520and%2520Ivan%2520Tarapov%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520into%2520medical%2520imaging%2520has%250Aadvanced%2520clinical%2520diagnostics%2520but%2520poses%2520challenges%2520in%2520managing%2520model%2520drift%2520and%250Aensuring%2520long-term%2520reliability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520develop%2520MMC%252B%252C%250Aan%2520enhanced%2520framework%2520for%2520scalable%2520drift%2520monitoring%252C%2520building%2520upon%2520the%250ACheXstray%2520framework%2520that%2520introduced%2520real-time%2520drift%2520detection%2520for%2520medical%250Aimaging%2520AI%2520models%2520using%2520multi-modal%2520data%2520concordance.%2520This%2520work%2520extends%2520the%250Aoriginal%2520framework%2527s%2520methodologies%252C%2520providing%2520a%2520more%2520scalable%2520and%2520adaptable%250Asolution%2520for%2520real-world%2520healthcare%2520settings%2520and%2520offers%2520a%2520reliable%2520and%250Acost-effective%2520alternative%2520to%2520continuous%2520performance%2520monitoring%2520addressing%250Alimitations%2520of%2520both%2520continuous%2520and%2520periodic%2520monitoring%2520methods.%2520MMC%252B%2520introduces%250Acritical%2520improvements%2520to%2520the%2520original%2520framework%252C%2520including%2520more%2520robust%2520handling%250Aof%2520diverse%2520data%2520streams%252C%2520improved%2520scalability%2520with%2520the%2520integration%2520of%250Afoundation%2520models%2520like%2520MedImageInsight%2520for%2520high-dimensional%2520image%2520embeddings%250Awithout%2520site-specific%2520training%252C%2520and%2520the%2520introduction%2520of%2520uncertainty%2520bounds%2520to%250Abetter%2520capture%2520drift%2520in%2520dynamic%2520clinical%2520environments.%2520Validated%2520with%250Areal-world%2520data%2520from%2520Massachusetts%2520General%2520Hospital%2520during%2520the%2520COVID-19%250Apandemic%252C%2520MMC%252B%2520effectively%2520detects%2520significant%2520data%2520shifts%2520and%2520correlates%2520them%250Awith%2520model%2520performance%2520changes.%2520While%2520not%2520directly%2520predicting%2520performance%250Adegradation%252C%2520MMC%252B%2520serves%2520as%2520an%2520early%2520warning%2520system%252C%2520indicating%2520when%2520AI%2520systems%250Amay%2520deviate%2520from%2520acceptable%2520performance%2520bounds%2520and%2520enabling%2520timely%250Ainterventions.%2520By%2520emphasizing%2520the%2520importance%2520of%2520monitoring%2520diverse%2520data%2520streams%250Aand%2520evaluating%2520data%2520shifts%2520alongside%2520model%2520performance%252C%2520this%2520work%2520contributes%250Ato%2520the%2520broader%2520adoption%2520and%2520integration%2520of%2520AI%2520solutions%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Drift%20Monitoring%20in%20Medical%20Imaging%20AI&entry.906535625=Jameson%20Merkow%20and%20Felix%20J.%20Dorfner%20and%20Xiyu%20Yang%20and%20Alexander%20Ersoy%20and%20Giridhar%20Dasegowda%20and%20Mannudeep%20Kalra%20and%20Matthew%20P.%20Lungren%20and%20Christopher%20P.%20Bridge%20and%20Ivan%20Tarapov&entry.1292438233=%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20into%20medical%20imaging%20has%0Aadvanced%20clinical%20diagnostics%20but%20poses%20challenges%20in%20managing%20model%20drift%20and%0Aensuring%20long-term%20reliability.%20To%20address%20these%20challenges%2C%20we%20develop%20MMC%2B%2C%0Aan%20enhanced%20framework%20for%20scalable%20drift%20monitoring%2C%20building%20upon%20the%0ACheXstray%20framework%20that%20introduced%20real-time%20drift%20detection%20for%20medical%0Aimaging%20AI%20models%20using%20multi-modal%20data%20concordance.%20This%20work%20extends%20the%0Aoriginal%20framework%27s%20methodologies%2C%20providing%20a%20more%20scalable%20and%20adaptable%0Asolution%20for%20real-world%20healthcare%20settings%20and%20offers%20a%20reliable%20and%0Acost-effective%20alternative%20to%20continuous%20performance%20monitoring%20addressing%0Alimitations%20of%20both%20continuous%20and%20periodic%20monitoring%20methods.%20MMC%2B%20introduces%0Acritical%20improvements%20to%20the%20original%20framework%2C%20including%20more%20robust%20handling%0Aof%20diverse%20data%20streams%2C%20improved%20scalability%20with%20the%20integration%20of%0Afoundation%20models%20like%20MedImageInsight%20for%20high-dimensional%20image%20embeddings%0Awithout%20site-specific%20training%2C%20and%20the%20introduction%20of%20uncertainty%20bounds%20to%0Abetter%20capture%20drift%20in%20dynamic%20clinical%20environments.%20Validated%20with%0Areal-world%20data%20from%20Massachusetts%20General%20Hospital%20during%20the%20COVID-19%0Apandemic%2C%20MMC%2B%20effectively%20detects%20significant%20data%20shifts%20and%20correlates%20them%0Awith%20model%20performance%20changes.%20While%20not%20directly%20predicting%20performance%0Adegradation%2C%20MMC%2B%20serves%20as%20an%20early%20warning%20system%2C%20indicating%20when%20AI%20systems%0Amay%20deviate%20from%20acceptable%20performance%20bounds%20and%20enabling%20timely%0Ainterventions.%20By%20emphasizing%20the%20importance%20of%20monitoring%20diverse%20data%20streams%0Aand%20evaluating%20data%20shifts%20alongside%20model%20performance%2C%20this%20work%20contributes%0Ato%20the%20broader%20adoption%20and%20integration%20of%20AI%20solutions%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13174v2&entry.124074799=Read"},
{"title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts", "author": "Rachel S. Y. Teo and Tan M. Nguyen", "abstract": "  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n", "link": "http://arxiv.org/abs/2410.14574v1", "date": "2024-10-18", "relevancy": 1.5232, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MomentumSMoE%3A%20Integrating%20Momentum%20into%20Sparse%20Mixture%20of%20Experts&body=Title%3A%20MomentumSMoE%3A%20Integrating%20Momentum%20into%20Sparse%20Mixture%20of%20Experts%0AAuthor%3A%20Rachel%20S.%20Y.%20Teo%20and%20Tan%20M.%20Nguyen%0AAbstract%3A%20%20%20Sparse%20Mixture%20of%20Experts%20%28SMoE%29%20has%20become%20the%20key%20to%20unlocking%20unparalleled%0Ascalability%20in%20deep%20learning.%20SMoE%20has%20the%20potential%20to%20exponentially%20increase%0Aparameter%20count%20while%20maintaining%20the%20efficiency%20of%20the%20model%20by%20only%0Aactivating%20a%20small%20subset%20of%20these%20parameters%20for%20a%20given%20sample.%20However%2C%20it%0Ahas%20been%20observed%20that%20SMoE%20suffers%20from%20unstable%20training%20and%20has%20difficulty%0Aadapting%20to%20new%20distributions%2C%20leading%20to%20the%20model%27s%20lack%20of%20robustness%20to%0Adata%20contamination.%20To%20overcome%20these%20limitations%2C%20we%20first%20establish%20a%0Aconnection%20between%20the%20dynamics%20of%20the%20expert%20representations%20in%20SMoEs%20and%0Agradient%20descent%20on%20a%20multi-objective%20optimization%20problem.%20Leveraging%20our%0Aframework%2C%20we%20then%20integrate%20momentum%20into%20SMoE%20and%20propose%20a%20new%20family%20of%0ASMoEs%20named%20MomentumSMoE.%20We%20theoretically%20prove%20and%20numerically%20demonstrate%0Athat%20MomentumSMoE%20is%20more%20stable%20and%20robust%20than%20SMoE.%20In%20particular%2C%20we%20verify%0Athe%20advantages%20of%20MomentumSMoE%20over%20SMoE%20on%20a%20variety%20of%20practical%20tasks%0Aincluding%20ImageNet-1K%20object%20recognition%20and%20WikiText-103%20language%20modeling.%20We%0Ademonstrate%20the%20applicability%20of%20MomentumSMoE%20to%20many%20types%20of%20SMoE%20models%2C%0Aincluding%20those%20in%20the%20Sparse%20MoE%20model%20for%20vision%20%28V-MoE%29%20and%20the%20Generalist%0ALanguage%20Model%20%28GLaM%29.%20We%20also%20show%20that%20other%20advanced%20momentum-based%0Aoptimization%20methods%2C%20such%20as%20Adam%2C%20can%20be%20easily%20incorporated%20into%20the%0AMomentumSMoE%20framework%20for%20designing%20new%20SMoE%20models%20with%20even%20better%0Aperformance%2C%20almost%20negligible%20additional%20computation%20cost%2C%20and%20simple%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMomentumSMoE%253A%2520Integrating%2520Momentum%2520into%2520Sparse%2520Mixture%2520of%2520Experts%26entry.906535625%3DRachel%2520S.%2520Y.%2520Teo%2520and%2520Tan%2520M.%2520Nguyen%26entry.1292438233%3D%2520%2520Sparse%2520Mixture%2520of%2520Experts%2520%2528SMoE%2529%2520has%2520become%2520the%2520key%2520to%2520unlocking%2520unparalleled%250Ascalability%2520in%2520deep%2520learning.%2520SMoE%2520has%2520the%2520potential%2520to%2520exponentially%2520increase%250Aparameter%2520count%2520while%2520maintaining%2520the%2520efficiency%2520of%2520the%2520model%2520by%2520only%250Aactivating%2520a%2520small%2520subset%2520of%2520these%2520parameters%2520for%2520a%2520given%2520sample.%2520However%252C%2520it%250Ahas%2520been%2520observed%2520that%2520SMoE%2520suffers%2520from%2520unstable%2520training%2520and%2520has%2520difficulty%250Aadapting%2520to%2520new%2520distributions%252C%2520leading%2520to%2520the%2520model%2527s%2520lack%2520of%2520robustness%2520to%250Adata%2520contamination.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520first%2520establish%2520a%250Aconnection%2520between%2520the%2520dynamics%2520of%2520the%2520expert%2520representations%2520in%2520SMoEs%2520and%250Agradient%2520descent%2520on%2520a%2520multi-objective%2520optimization%2520problem.%2520Leveraging%2520our%250Aframework%252C%2520we%2520then%2520integrate%2520momentum%2520into%2520SMoE%2520and%2520propose%2520a%2520new%2520family%2520of%250ASMoEs%2520named%2520MomentumSMoE.%2520We%2520theoretically%2520prove%2520and%2520numerically%2520demonstrate%250Athat%2520MomentumSMoE%2520is%2520more%2520stable%2520and%2520robust%2520than%2520SMoE.%2520In%2520particular%252C%2520we%2520verify%250Athe%2520advantages%2520of%2520MomentumSMoE%2520over%2520SMoE%2520on%2520a%2520variety%2520of%2520practical%2520tasks%250Aincluding%2520ImageNet-1K%2520object%2520recognition%2520and%2520WikiText-103%2520language%2520modeling.%2520We%250Ademonstrate%2520the%2520applicability%2520of%2520MomentumSMoE%2520to%2520many%2520types%2520of%2520SMoE%2520models%252C%250Aincluding%2520those%2520in%2520the%2520Sparse%2520MoE%2520model%2520for%2520vision%2520%2528V-MoE%2529%2520and%2520the%2520Generalist%250ALanguage%2520Model%2520%2528GLaM%2529.%2520We%2520also%2520show%2520that%2520other%2520advanced%2520momentum-based%250Aoptimization%2520methods%252C%2520such%2520as%2520Adam%252C%2520can%2520be%2520easily%2520incorporated%2520into%2520the%250AMomentumSMoE%2520framework%2520for%2520designing%2520new%2520SMoE%2520models%2520with%2520even%2520better%250Aperformance%252C%2520almost%2520negligible%2520additional%2520computation%2520cost%252C%2520and%2520simple%250Aimplementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MomentumSMoE%3A%20Integrating%20Momentum%20into%20Sparse%20Mixture%20of%20Experts&entry.906535625=Rachel%20S.%20Y.%20Teo%20and%20Tan%20M.%20Nguyen&entry.1292438233=%20%20Sparse%20Mixture%20of%20Experts%20%28SMoE%29%20has%20become%20the%20key%20to%20unlocking%20unparalleled%0Ascalability%20in%20deep%20learning.%20SMoE%20has%20the%20potential%20to%20exponentially%20increase%0Aparameter%20count%20while%20maintaining%20the%20efficiency%20of%20the%20model%20by%20only%0Aactivating%20a%20small%20subset%20of%20these%20parameters%20for%20a%20given%20sample.%20However%2C%20it%0Ahas%20been%20observed%20that%20SMoE%20suffers%20from%20unstable%20training%20and%20has%20difficulty%0Aadapting%20to%20new%20distributions%2C%20leading%20to%20the%20model%27s%20lack%20of%20robustness%20to%0Adata%20contamination.%20To%20overcome%20these%20limitations%2C%20we%20first%20establish%20a%0Aconnection%20between%20the%20dynamics%20of%20the%20expert%20representations%20in%20SMoEs%20and%0Agradient%20descent%20on%20a%20multi-objective%20optimization%20problem.%20Leveraging%20our%0Aframework%2C%20we%20then%20integrate%20momentum%20into%20SMoE%20and%20propose%20a%20new%20family%20of%0ASMoEs%20named%20MomentumSMoE.%20We%20theoretically%20prove%20and%20numerically%20demonstrate%0Athat%20MomentumSMoE%20is%20more%20stable%20and%20robust%20than%20SMoE.%20In%20particular%2C%20we%20verify%0Athe%20advantages%20of%20MomentumSMoE%20over%20SMoE%20on%20a%20variety%20of%20practical%20tasks%0Aincluding%20ImageNet-1K%20object%20recognition%20and%20WikiText-103%20language%20modeling.%20We%0Ademonstrate%20the%20applicability%20of%20MomentumSMoE%20to%20many%20types%20of%20SMoE%20models%2C%0Aincluding%20those%20in%20the%20Sparse%20MoE%20model%20for%20vision%20%28V-MoE%29%20and%20the%20Generalist%0ALanguage%20Model%20%28GLaM%29.%20We%20also%20show%20that%20other%20advanced%20momentum-based%0Aoptimization%20methods%2C%20such%20as%20Adam%2C%20can%20be%20easily%20incorporated%20into%20the%0AMomentumSMoE%20framework%20for%20designing%20new%20SMoE%20models%20with%20even%20better%0Aperformance%2C%20almost%20negligible%20additional%20computation%20cost%2C%20and%20simple%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14574v1&entry.124074799=Read"},
{"title": "Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows", "author": "Sandeep Nagar and Girish Varma", "abstract": "  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n", "link": "http://arxiv.org/abs/2410.14634v1", "date": "2024-10-18", "relevancy": 1.5224, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5782}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Backpropagation%20for%20Inverse%20of%20a%20Convolution%20with%20Application%0A%20%20to%20Normalizing%20Flows&body=Title%3A%20Parallel%20Backpropagation%20for%20Inverse%20of%20a%20Convolution%20with%20Application%0A%20%20to%20Normalizing%20Flows%0AAuthor%3A%20Sandeep%20Nagar%20and%20Girish%20Varma%0AAbstract%3A%20%20%20Inverse%20of%20an%20invertible%20convolution%20is%20an%20important%20operation%20that%20comes%20up%0Ain%20Normalizing%20Flows%2C%20Image%20Deblurring%2C%20etc.%20The%20naive%20algorithm%20for%0Abackpropagation%20of%20this%20operation%20using%20Gaussian%20elimination%20has%20running%20time%0A%24O%28n%5E3%29%24%20where%20%24n%24%20is%20the%20number%20of%20pixels%20in%20the%20image.%20We%20give%20a%20fast%0Aparallel%20backpropagation%20algorithm%20with%20running%20time%20%24O%28%5Csqrt%7Bn%7D%29%24%20for%20a%20square%0Aimage%20and%20provide%20a%20GPU%20implementation%20of%20the%20same.%20Inverse%20Convolutions%20are%0Ausually%20used%20in%20Normalizing%20Flows%20in%20the%20sampling%20pass%2C%20making%20them%20slow.%20We%0Apropose%20to%20use%20Inverse%20Convolutions%20in%20the%20forward%20%28image%20to%20latent%20vector%29%0Apass%20of%20the%20Normalizing%20flow.%20Since%20the%20sampling%20pass%20is%20the%20inverse%20of%20the%0Aforward%20pass%2C%20it%20will%20use%20convolutions%20only%2C%20resulting%20in%20efficient%20sampling%0Atimes.%20We%20use%20our%20parallel%20backpropagation%20algorithm%20for%20optimizing%20the%20inverse%0Aconvolution%20layer%20resulting%20in%20fast%20training%20times%20also.%20We%20implement%20this%0Aapproach%20in%20various%20Normalizing%20Flow%20backbones%2C%20resulting%20in%20our%20Inverse-Flow%0Amodels.%20We%20benchmark%20Inverse-Flow%20on%20standard%20datasets%20and%20show%20significantly%0Aimproved%20sampling%20times%20with%20similar%20bits%20per%20dimension%20compared%20to%20previous%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Backpropagation%2520for%2520Inverse%2520of%2520a%2520Convolution%2520with%2520Application%250A%2520%2520to%2520Normalizing%2520Flows%26entry.906535625%3DSandeep%2520Nagar%2520and%2520Girish%2520Varma%26entry.1292438233%3D%2520%2520Inverse%2520of%2520an%2520invertible%2520convolution%2520is%2520an%2520important%2520operation%2520that%2520comes%2520up%250Ain%2520Normalizing%2520Flows%252C%2520Image%2520Deblurring%252C%2520etc.%2520The%2520naive%2520algorithm%2520for%250Abackpropagation%2520of%2520this%2520operation%2520using%2520Gaussian%2520elimination%2520has%2520running%2520time%250A%2524O%2528n%255E3%2529%2524%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520pixels%2520in%2520the%2520image.%2520We%2520give%2520a%2520fast%250Aparallel%2520backpropagation%2520algorithm%2520with%2520running%2520time%2520%2524O%2528%255Csqrt%257Bn%257D%2529%2524%2520for%2520a%2520square%250Aimage%2520and%2520provide%2520a%2520GPU%2520implementation%2520of%2520the%2520same.%2520Inverse%2520Convolutions%2520are%250Ausually%2520used%2520in%2520Normalizing%2520Flows%2520in%2520the%2520sampling%2520pass%252C%2520making%2520them%2520slow.%2520We%250Apropose%2520to%2520use%2520Inverse%2520Convolutions%2520in%2520the%2520forward%2520%2528image%2520to%2520latent%2520vector%2529%250Apass%2520of%2520the%2520Normalizing%2520flow.%2520Since%2520the%2520sampling%2520pass%2520is%2520the%2520inverse%2520of%2520the%250Aforward%2520pass%252C%2520it%2520will%2520use%2520convolutions%2520only%252C%2520resulting%2520in%2520efficient%2520sampling%250Atimes.%2520We%2520use%2520our%2520parallel%2520backpropagation%2520algorithm%2520for%2520optimizing%2520the%2520inverse%250Aconvolution%2520layer%2520resulting%2520in%2520fast%2520training%2520times%2520also.%2520We%2520implement%2520this%250Aapproach%2520in%2520various%2520Normalizing%2520Flow%2520backbones%252C%2520resulting%2520in%2520our%2520Inverse-Flow%250Amodels.%2520We%2520benchmark%2520Inverse-Flow%2520on%2520standard%2520datasets%2520and%2520show%2520significantly%250Aimproved%2520sampling%2520times%2520with%2520similar%2520bits%2520per%2520dimension%2520compared%2520to%2520previous%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Backpropagation%20for%20Inverse%20of%20a%20Convolution%20with%20Application%0A%20%20to%20Normalizing%20Flows&entry.906535625=Sandeep%20Nagar%20and%20Girish%20Varma&entry.1292438233=%20%20Inverse%20of%20an%20invertible%20convolution%20is%20an%20important%20operation%20that%20comes%20up%0Ain%20Normalizing%20Flows%2C%20Image%20Deblurring%2C%20etc.%20The%20naive%20algorithm%20for%0Abackpropagation%20of%20this%20operation%20using%20Gaussian%20elimination%20has%20running%20time%0A%24O%28n%5E3%29%24%20where%20%24n%24%20is%20the%20number%20of%20pixels%20in%20the%20image.%20We%20give%20a%20fast%0Aparallel%20backpropagation%20algorithm%20with%20running%20time%20%24O%28%5Csqrt%7Bn%7D%29%24%20for%20a%20square%0Aimage%20and%20provide%20a%20GPU%20implementation%20of%20the%20same.%20Inverse%20Convolutions%20are%0Ausually%20used%20in%20Normalizing%20Flows%20in%20the%20sampling%20pass%2C%20making%20them%20slow.%20We%0Apropose%20to%20use%20Inverse%20Convolutions%20in%20the%20forward%20%28image%20to%20latent%20vector%29%0Apass%20of%20the%20Normalizing%20flow.%20Since%20the%20sampling%20pass%20is%20the%20inverse%20of%20the%0Aforward%20pass%2C%20it%20will%20use%20convolutions%20only%2C%20resulting%20in%20efficient%20sampling%0Atimes.%20We%20use%20our%20parallel%20backpropagation%20algorithm%20for%20optimizing%20the%20inverse%0Aconvolution%20layer%20resulting%20in%20fast%20training%20times%20also.%20We%20implement%20this%0Aapproach%20in%20various%20Normalizing%20Flow%20backbones%2C%20resulting%20in%20our%20Inverse-Flow%0Amodels.%20We%20benchmark%20Inverse-Flow%20on%20standard%20datasets%20and%20show%20significantly%0Aimproved%20sampling%20times%20with%20similar%20bits%20per%20dimension%20compared%20to%20previous%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14634v1&entry.124074799=Read"},
{"title": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media", "author": "Owen Cook and Charlie Grimshaw and Ben Wu and Sophie Dillon and Jack Hicks and Luke Jones and Thomas Smith and Matyas Szert and Xingyi Song", "abstract": "  Misinformation spreads rapidly on social media, confusing the truth and\ntargetting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.\n", "link": "http://arxiv.org/abs/2410.14515v1", "date": "2024-10-18", "relevancy": 1.4995, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5619}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media&body=Title%3A%20Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media%0AAuthor%3A%20Owen%20Cook%20and%20Charlie%20Grimshaw%20and%20Ben%20Wu%20and%20Sophie%20Dillon%20and%20Jack%20Hicks%20and%20Luke%20Jones%20and%20Thomas%20Smith%20and%20Matyas%20Szert%20and%20Xingyi%20Song%0AAbstract%3A%20%20%20Misinformation%20spreads%20rapidly%20on%20social%20media%2C%20confusing%20the%20truth%20and%0Atargetting%20potentially%20vulnerable%20people.%20To%20effectively%20mitigate%20the%20negative%0Aimpact%20of%20misinformation%2C%20it%20must%20first%20be%20accurately%20detected%20before%20applying%0Aa%20mitigation%20strategy%2C%20such%20as%20X%27s%20community%20notes%2C%20which%20is%20currently%20a%20manual%0Aprocess.%20This%20study%20takes%20a%20knowledge-based%20approach%20to%20misinformation%0Adetection%2C%20modelling%20the%20problem%20similarly%20to%20one%20of%20natural%20language%0Ainference.%20The%20EffiARA%20annotation%20framework%20is%20introduced%2C%20aiming%20to%20utilise%0Ainter-%20and%20intra-annotator%20agreement%20to%20understand%20the%20reliability%20of%20each%0Aannotator%20and%20influence%20the%20training%20of%20large%20language%20models%20for%0Aclassification%20based%20on%20annotator%20reliability.%20In%20assessing%20the%20EffiARA%0Aannotation%20framework%2C%20the%20Russo-Ukrainian%20Conflict%20Knowledge-Based%0AMisinformation%20Classification%20Dataset%20%28RUC-MCD%29%20was%20developed%20and%20made%20publicly%0Aavailable.%20This%20study%20finds%20that%20sample%20weighting%20using%20annotator%20reliability%0Aperforms%20the%20best%2C%20utilising%20both%20inter-%20and%20intra-annotator%20agreement%20and%0Asoft-label%20training.%20The%20highest%20classification%20performance%20achieved%20using%0ALlama-3.2-1B%20was%20a%20macro-F1%20of%200.757%20and%200.740%20using%20TwHIN-BERT-large.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Annotator%2520Reliability%2520Assessment%2520and%2520Sample%2520Weighting%2520for%250A%2520%2520Knowledge-Based%2520Misinformation%2520Detection%2520on%2520Social%2520Media%26entry.906535625%3DOwen%2520Cook%2520and%2520Charlie%2520Grimshaw%2520and%2520Ben%2520Wu%2520and%2520Sophie%2520Dillon%2520and%2520Jack%2520Hicks%2520and%2520Luke%2520Jones%2520and%2520Thomas%2520Smith%2520and%2520Matyas%2520Szert%2520and%2520Xingyi%2520Song%26entry.1292438233%3D%2520%2520Misinformation%2520spreads%2520rapidly%2520on%2520social%2520media%252C%2520confusing%2520the%2520truth%2520and%250Atargetting%2520potentially%2520vulnerable%2520people.%2520To%2520effectively%2520mitigate%2520the%2520negative%250Aimpact%2520of%2520misinformation%252C%2520it%2520must%2520first%2520be%2520accurately%2520detected%2520before%2520applying%250Aa%2520mitigation%2520strategy%252C%2520such%2520as%2520X%2527s%2520community%2520notes%252C%2520which%2520is%2520currently%2520a%2520manual%250Aprocess.%2520This%2520study%2520takes%2520a%2520knowledge-based%2520approach%2520to%2520misinformation%250Adetection%252C%2520modelling%2520the%2520problem%2520similarly%2520to%2520one%2520of%2520natural%2520language%250Ainference.%2520The%2520EffiARA%2520annotation%2520framework%2520is%2520introduced%252C%2520aiming%2520to%2520utilise%250Ainter-%2520and%2520intra-annotator%2520agreement%2520to%2520understand%2520the%2520reliability%2520of%2520each%250Aannotator%2520and%2520influence%2520the%2520training%2520of%2520large%2520language%2520models%2520for%250Aclassification%2520based%2520on%2520annotator%2520reliability.%2520In%2520assessing%2520the%2520EffiARA%250Aannotation%2520framework%252C%2520the%2520Russo-Ukrainian%2520Conflict%2520Knowledge-Based%250AMisinformation%2520Classification%2520Dataset%2520%2528RUC-MCD%2529%2520was%2520developed%2520and%2520made%2520publicly%250Aavailable.%2520This%2520study%2520finds%2520that%2520sample%2520weighting%2520using%2520annotator%2520reliability%250Aperforms%2520the%2520best%252C%2520utilising%2520both%2520inter-%2520and%2520intra-annotator%2520agreement%2520and%250Asoft-label%2520training.%2520The%2520highest%2520classification%2520performance%2520achieved%2520using%250ALlama-3.2-1B%2520was%2520a%2520macro-F1%2520of%25200.757%2520and%25200.740%2520using%2520TwHIN-BERT-large.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Annotator%20Reliability%20Assessment%20and%20Sample%20Weighting%20for%0A%20%20Knowledge-Based%20Misinformation%20Detection%20on%20Social%20Media&entry.906535625=Owen%20Cook%20and%20Charlie%20Grimshaw%20and%20Ben%20Wu%20and%20Sophie%20Dillon%20and%20Jack%20Hicks%20and%20Luke%20Jones%20and%20Thomas%20Smith%20and%20Matyas%20Szert%20and%20Xingyi%20Song&entry.1292438233=%20%20Misinformation%20spreads%20rapidly%20on%20social%20media%2C%20confusing%20the%20truth%20and%0Atargetting%20potentially%20vulnerable%20people.%20To%20effectively%20mitigate%20the%20negative%0Aimpact%20of%20misinformation%2C%20it%20must%20first%20be%20accurately%20detected%20before%20applying%0Aa%20mitigation%20strategy%2C%20such%20as%20X%27s%20community%20notes%2C%20which%20is%20currently%20a%20manual%0Aprocess.%20This%20study%20takes%20a%20knowledge-based%20approach%20to%20misinformation%0Adetection%2C%20modelling%20the%20problem%20similarly%20to%20one%20of%20natural%20language%0Ainference.%20The%20EffiARA%20annotation%20framework%20is%20introduced%2C%20aiming%20to%20utilise%0Ainter-%20and%20intra-annotator%20agreement%20to%20understand%20the%20reliability%20of%20each%0Aannotator%20and%20influence%20the%20training%20of%20large%20language%20models%20for%0Aclassification%20based%20on%20annotator%20reliability.%20In%20assessing%20the%20EffiARA%0Aannotation%20framework%2C%20the%20Russo-Ukrainian%20Conflict%20Knowledge-Based%0AMisinformation%20Classification%20Dataset%20%28RUC-MCD%29%20was%20developed%20and%20made%20publicly%0Aavailable.%20This%20study%20finds%20that%20sample%20weighting%20using%20annotator%20reliability%0Aperforms%20the%20best%2C%20utilising%20both%20inter-%20and%20intra-annotator%20agreement%20and%0Asoft-label%20training.%20The%20highest%20classification%20performance%20achieved%20using%0ALlama-3.2-1B%20was%20a%20macro-F1%20of%200.757%20and%200.740%20using%20TwHIN-BERT-large.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14515v1&entry.124074799=Read"},
{"title": "Retraining with Predicted Hard Labels Provably Increases Model Accuracy", "author": "Rudrajit Das and Inderjit S. Dhillon and Alessandro Epasto and Adel Javanmard and Jieming Mao and Vahab Mirrokni and Sujay Sanghavi and Peilin Zhong", "abstract": "  The performance of a model trained with \\textit{noisy labels} is often\nimproved by simply \\textit{retraining} the model with its own predicted\n\\textit{hard} labels (i.e., $1$/$0$ labels). Yet, a detailed theoretical\ncharacterization of this phenomenon is lacking. In this paper, we theoretically\nanalyze retraining in a linearly separable setting with randomly corrupted\nlabels given to us and prove that retraining can improve the population\naccuracy obtained by initially training with the given (noisy) labels. To the\nbest of our knowledge, this is the first such theoretical result. Retraining\nfinds application in improving training with local label differential privacy\n(DP) which involves training with noisy labels. We empirically show that\nretraining selectively on the samples for which the predicted label matches the\ngiven label significantly improves label DP training at \\textit{no extra\nprivacy cost}; we call this \\textit{consensus-based retraining}. As an example,\nwhen training ResNet-18 on CIFAR-100 with $\\epsilon=3$ label DP, we obtain\n$6.4\\%$ improvement in accuracy with consensus-based retraining.\n", "link": "http://arxiv.org/abs/2406.11206v2", "date": "2024-10-18", "relevancy": 1.4372, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4858}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4777}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retraining%20with%20Predicted%20Hard%20Labels%20Provably%20Increases%20Model%20Accuracy&body=Title%3A%20Retraining%20with%20Predicted%20Hard%20Labels%20Provably%20Increases%20Model%20Accuracy%0AAuthor%3A%20Rudrajit%20Das%20and%20Inderjit%20S.%20Dhillon%20and%20Alessandro%20Epasto%20and%20Adel%20Javanmard%20and%20Jieming%20Mao%20and%20Vahab%20Mirrokni%20and%20Sujay%20Sanghavi%20and%20Peilin%20Zhong%0AAbstract%3A%20%20%20The%20performance%20of%20a%20model%20trained%20with%20%5Ctextit%7Bnoisy%20labels%7D%20is%20often%0Aimproved%20by%20simply%20%5Ctextit%7Bretraining%7D%20the%20model%20with%20its%20own%20predicted%0A%5Ctextit%7Bhard%7D%20labels%20%28i.e.%2C%20%241%24/%240%24%20labels%29.%20Yet%2C%20a%20detailed%20theoretical%0Acharacterization%20of%20this%20phenomenon%20is%20lacking.%20In%20this%20paper%2C%20we%20theoretically%0Aanalyze%20retraining%20in%20a%20linearly%20separable%20setting%20with%20randomly%20corrupted%0Alabels%20given%20to%20us%20and%20prove%20that%20retraining%20can%20improve%20the%20population%0Aaccuracy%20obtained%20by%20initially%20training%20with%20the%20given%20%28noisy%29%20labels.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20such%20theoretical%20result.%20Retraining%0Afinds%20application%20in%20improving%20training%20with%20local%20label%20differential%20privacy%0A%28DP%29%20which%20involves%20training%20with%20noisy%20labels.%20We%20empirically%20show%20that%0Aretraining%20selectively%20on%20the%20samples%20for%20which%20the%20predicted%20label%20matches%20the%0Agiven%20label%20significantly%20improves%20label%20DP%20training%20at%20%5Ctextit%7Bno%20extra%0Aprivacy%20cost%7D%3B%20we%20call%20this%20%5Ctextit%7Bconsensus-based%20retraining%7D.%20As%20an%20example%2C%0Awhen%20training%20ResNet-18%20on%20CIFAR-100%20with%20%24%5Cepsilon%3D3%24%20label%20DP%2C%20we%20obtain%0A%246.4%5C%25%24%20improvement%20in%20accuracy%20with%20consensus-based%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetraining%2520with%2520Predicted%2520Hard%2520Labels%2520Provably%2520Increases%2520Model%2520Accuracy%26entry.906535625%3DRudrajit%2520Das%2520and%2520Inderjit%2520S.%2520Dhillon%2520and%2520Alessandro%2520Epasto%2520and%2520Adel%2520Javanmard%2520and%2520Jieming%2520Mao%2520and%2520Vahab%2520Mirrokni%2520and%2520Sujay%2520Sanghavi%2520and%2520Peilin%2520Zhong%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520a%2520model%2520trained%2520with%2520%255Ctextit%257Bnoisy%2520labels%257D%2520is%2520often%250Aimproved%2520by%2520simply%2520%255Ctextit%257Bretraining%257D%2520the%2520model%2520with%2520its%2520own%2520predicted%250A%255Ctextit%257Bhard%257D%2520labels%2520%2528i.e.%252C%2520%25241%2524/%25240%2524%2520labels%2529.%2520Yet%252C%2520a%2520detailed%2520theoretical%250Acharacterization%2520of%2520this%2520phenomenon%2520is%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520theoretically%250Aanalyze%2520retraining%2520in%2520a%2520linearly%2520separable%2520setting%2520with%2520randomly%2520corrupted%250Alabels%2520given%2520to%2520us%2520and%2520prove%2520that%2520retraining%2520can%2520improve%2520the%2520population%250Aaccuracy%2520obtained%2520by%2520initially%2520training%2520with%2520the%2520given%2520%2528noisy%2529%2520labels.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520such%2520theoretical%2520result.%2520Retraining%250Afinds%2520application%2520in%2520improving%2520training%2520with%2520local%2520label%2520differential%2520privacy%250A%2528DP%2529%2520which%2520involves%2520training%2520with%2520noisy%2520labels.%2520We%2520empirically%2520show%2520that%250Aretraining%2520selectively%2520on%2520the%2520samples%2520for%2520which%2520the%2520predicted%2520label%2520matches%2520the%250Agiven%2520label%2520significantly%2520improves%2520label%2520DP%2520training%2520at%2520%255Ctextit%257Bno%2520extra%250Aprivacy%2520cost%257D%253B%2520we%2520call%2520this%2520%255Ctextit%257Bconsensus-based%2520retraining%257D.%2520As%2520an%2520example%252C%250Awhen%2520training%2520ResNet-18%2520on%2520CIFAR-100%2520with%2520%2524%255Cepsilon%253D3%2524%2520label%2520DP%252C%2520we%2520obtain%250A%25246.4%255C%2525%2524%2520improvement%2520in%2520accuracy%2520with%2520consensus-based%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retraining%20with%20Predicted%20Hard%20Labels%20Provably%20Increases%20Model%20Accuracy&entry.906535625=Rudrajit%20Das%20and%20Inderjit%20S.%20Dhillon%20and%20Alessandro%20Epasto%20and%20Adel%20Javanmard%20and%20Jieming%20Mao%20and%20Vahab%20Mirrokni%20and%20Sujay%20Sanghavi%20and%20Peilin%20Zhong&entry.1292438233=%20%20The%20performance%20of%20a%20model%20trained%20with%20%5Ctextit%7Bnoisy%20labels%7D%20is%20often%0Aimproved%20by%20simply%20%5Ctextit%7Bretraining%7D%20the%20model%20with%20its%20own%20predicted%0A%5Ctextit%7Bhard%7D%20labels%20%28i.e.%2C%20%241%24/%240%24%20labels%29.%20Yet%2C%20a%20detailed%20theoretical%0Acharacterization%20of%20this%20phenomenon%20is%20lacking.%20In%20this%20paper%2C%20we%20theoretically%0Aanalyze%20retraining%20in%20a%20linearly%20separable%20setting%20with%20randomly%20corrupted%0Alabels%20given%20to%20us%20and%20prove%20that%20retraining%20can%20improve%20the%20population%0Aaccuracy%20obtained%20by%20initially%20training%20with%20the%20given%20%28noisy%29%20labels.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20such%20theoretical%20result.%20Retraining%0Afinds%20application%20in%20improving%20training%20with%20local%20label%20differential%20privacy%0A%28DP%29%20which%20involves%20training%20with%20noisy%20labels.%20We%20empirically%20show%20that%0Aretraining%20selectively%20on%20the%20samples%20for%20which%20the%20predicted%20label%20matches%20the%0Agiven%20label%20significantly%20improves%20label%20DP%20training%20at%20%5Ctextit%7Bno%20extra%0Aprivacy%20cost%7D%3B%20we%20call%20this%20%5Ctextit%7Bconsensus-based%20retraining%7D.%20As%20an%20example%2C%0Awhen%20training%20ResNet-18%20on%20CIFAR-100%20with%20%24%5Cepsilon%3D3%24%20label%20DP%2C%20we%20obtain%0A%246.4%5C%25%24%20improvement%20in%20accuracy%20with%20consensus-based%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11206v2&entry.124074799=Read"},
{"title": "The Traveling Bandit: A Framework for Bayesian Optimization with\n  Movement Costs", "author": "Qiyuan Chen and Raed Al Kontar", "abstract": "  This paper introduces a framework for Bayesian Optimization (BO) with metric\nmovement costs, addressing a critical challenge in practical applications where\ninput alterations incur varying costs. Our approach is a convenient plug-in\nthat seamlessly integrates with the existing literature on batched algorithms,\nwhere designs within batches are observed following the solution of a Traveling\nSalesman Problem. The proposed method provides a theoretical guarantee of\nconvergence in terms of movement costs for BO. Empirically, our method\neffectively reduces average movement costs over time while maintaining\ncomparable regret performance to conventional BO methods. This framework also\nshows promise for broader applications in various bandit settings with movement\ncosts.\n", "link": "http://arxiv.org/abs/2410.14533v1", "date": "2024-10-18", "relevancy": 1.4238, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5607}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Traveling%20Bandit%3A%20A%20Framework%20for%20Bayesian%20Optimization%20with%0A%20%20Movement%20Costs&body=Title%3A%20The%20Traveling%20Bandit%3A%20A%20Framework%20for%20Bayesian%20Optimization%20with%0A%20%20Movement%20Costs%0AAuthor%3A%20Qiyuan%20Chen%20and%20Raed%20Al%20Kontar%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20framework%20for%20Bayesian%20Optimization%20%28BO%29%20with%20metric%0Amovement%20costs%2C%20addressing%20a%20critical%20challenge%20in%20practical%20applications%20where%0Ainput%20alterations%20incur%20varying%20costs.%20Our%20approach%20is%20a%20convenient%20plug-in%0Athat%20seamlessly%20integrates%20with%20the%20existing%20literature%20on%20batched%20algorithms%2C%0Awhere%20designs%20within%20batches%20are%20observed%20following%20the%20solution%20of%20a%20Traveling%0ASalesman%20Problem.%20The%20proposed%20method%20provides%20a%20theoretical%20guarantee%20of%0Aconvergence%20in%20terms%20of%20movement%20costs%20for%20BO.%20Empirically%2C%20our%20method%0Aeffectively%20reduces%20average%20movement%20costs%20over%20time%20while%20maintaining%0Acomparable%20regret%20performance%20to%20conventional%20BO%20methods.%20This%20framework%20also%0Ashows%20promise%20for%20broader%20applications%20in%20various%20bandit%20settings%20with%20movement%0Acosts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Traveling%2520Bandit%253A%2520A%2520Framework%2520for%2520Bayesian%2520Optimization%2520with%250A%2520%2520Movement%2520Costs%26entry.906535625%3DQiyuan%2520Chen%2520and%2520Raed%2520Al%2520Kontar%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520framework%2520for%2520Bayesian%2520Optimization%2520%2528BO%2529%2520with%2520metric%250Amovement%2520costs%252C%2520addressing%2520a%2520critical%2520challenge%2520in%2520practical%2520applications%2520where%250Ainput%2520alterations%2520incur%2520varying%2520costs.%2520Our%2520approach%2520is%2520a%2520convenient%2520plug-in%250Athat%2520seamlessly%2520integrates%2520with%2520the%2520existing%2520literature%2520on%2520batched%2520algorithms%252C%250Awhere%2520designs%2520within%2520batches%2520are%2520observed%2520following%2520the%2520solution%2520of%2520a%2520Traveling%250ASalesman%2520Problem.%2520The%2520proposed%2520method%2520provides%2520a%2520theoretical%2520guarantee%2520of%250Aconvergence%2520in%2520terms%2520of%2520movement%2520costs%2520for%2520BO.%2520Empirically%252C%2520our%2520method%250Aeffectively%2520reduces%2520average%2520movement%2520costs%2520over%2520time%2520while%2520maintaining%250Acomparable%2520regret%2520performance%2520to%2520conventional%2520BO%2520methods.%2520This%2520framework%2520also%250Ashows%2520promise%2520for%2520broader%2520applications%2520in%2520various%2520bandit%2520settings%2520with%2520movement%250Acosts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Traveling%20Bandit%3A%20A%20Framework%20for%20Bayesian%20Optimization%20with%0A%20%20Movement%20Costs&entry.906535625=Qiyuan%20Chen%20and%20Raed%20Al%20Kontar&entry.1292438233=%20%20This%20paper%20introduces%20a%20framework%20for%20Bayesian%20Optimization%20%28BO%29%20with%20metric%0Amovement%20costs%2C%20addressing%20a%20critical%20challenge%20in%20practical%20applications%20where%0Ainput%20alterations%20incur%20varying%20costs.%20Our%20approach%20is%20a%20convenient%20plug-in%0Athat%20seamlessly%20integrates%20with%20the%20existing%20literature%20on%20batched%20algorithms%2C%0Awhere%20designs%20within%20batches%20are%20observed%20following%20the%20solution%20of%20a%20Traveling%0ASalesman%20Problem.%20The%20proposed%20method%20provides%20a%20theoretical%20guarantee%20of%0Aconvergence%20in%20terms%20of%20movement%20costs%20for%20BO.%20Empirically%2C%20our%20method%0Aeffectively%20reduces%20average%20movement%20costs%20over%20time%20while%20maintaining%0Acomparable%20regret%20performance%20to%20conventional%20BO%20methods.%20This%20framework%20also%0Ashows%20promise%20for%20broader%20applications%20in%20various%20bandit%20settings%20with%20movement%0Acosts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14533v1&entry.124074799=Read"},
{"title": "Streaming Deep Reinforcement Learning Finally Works", "author": "Mohamed Elsayed and Gautham Vasan and A. Rupam Mahmood", "abstract": "  Natural intelligence processes experience as a continuous stream, sensing,\nacting, and learning moment-by-moment in real time. Streaming learning, the\nmodus operandi of classic reinforcement learning (RL) algorithms like\nQ-learning and TD, mimics natural learning by using the most recent sample\nwithout storing it. This approach is also ideal for resource-constrained,\ncommunication-limited, and privacy-sensitive applications. However, in deep RL,\nlearners almost always use batch updates and replay buffers, making them\ncomputationally expensive and incompatible with streaming learning. Although\nthe prevalence of batch deep RL is often attributed to its sample efficiency, a\nmore critical reason for the absence of streaming deep RL is its frequent\ninstability and failure to learn, which we refer to as stream barrier. This\npaper introduces the stream-x algorithms, the first class of deep RL algorithms\nto overcome stream barrier for both prediction and control and match sample\nefficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,\nand Atari Games, we demonstrate stream barrier in existing algorithms and\nsuccessful stable learning with our stream-x algorithms: stream Q, stream AC,\nand stream TD, achieving the best model-free performance in DM Control Dog\nenvironments. A set of common techniques underlies the stream-x algorithms,\nenabling their success with a single set of hyperparameters and allowing for\neasy extension to other algorithms, thereby reviving streaming RL.\n", "link": "http://arxiv.org/abs/2410.14606v1", "date": "2024-10-18", "relevancy": 1.4205, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4823}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4771}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%20Deep%20Reinforcement%20Learning%20Finally%20Works&body=Title%3A%20Streaming%20Deep%20Reinforcement%20Learning%20Finally%20Works%0AAuthor%3A%20Mohamed%20Elsayed%20and%20Gautham%20Vasan%20and%20A.%20Rupam%20Mahmood%0AAbstract%3A%20%20%20Natural%20intelligence%20processes%20experience%20as%20a%20continuous%20stream%2C%20sensing%2C%0Aacting%2C%20and%20learning%20moment-by-moment%20in%20real%20time.%20Streaming%20learning%2C%20the%0Amodus%20operandi%20of%20classic%20reinforcement%20learning%20%28RL%29%20algorithms%20like%0AQ-learning%20and%20TD%2C%20mimics%20natural%20learning%20by%20using%20the%20most%20recent%20sample%0Awithout%20storing%20it.%20This%20approach%20is%20also%20ideal%20for%20resource-constrained%2C%0Acommunication-limited%2C%20and%20privacy-sensitive%20applications.%20However%2C%20in%20deep%20RL%2C%0Alearners%20almost%20always%20use%20batch%20updates%20and%20replay%20buffers%2C%20making%20them%0Acomputationally%20expensive%20and%20incompatible%20with%20streaming%20learning.%20Although%0Athe%20prevalence%20of%20batch%20deep%20RL%20is%20often%20attributed%20to%20its%20sample%20efficiency%2C%20a%0Amore%20critical%20reason%20for%20the%20absence%20of%20streaming%20deep%20RL%20is%20its%20frequent%0Ainstability%20and%20failure%20to%20learn%2C%20which%20we%20refer%20to%20as%20stream%20barrier.%20This%0Apaper%20introduces%20the%20stream-x%20algorithms%2C%20the%20first%20class%20of%20deep%20RL%20algorithms%0Ato%20overcome%20stream%20barrier%20for%20both%20prediction%20and%20control%20and%20match%20sample%0Aefficiency%20of%20batch%20RL.%20Through%20experiments%20in%20Mujoco%20Gym%2C%20DM%20Control%20Suite%2C%0Aand%20Atari%20Games%2C%20we%20demonstrate%20stream%20barrier%20in%20existing%20algorithms%20and%0Asuccessful%20stable%20learning%20with%20our%20stream-x%20algorithms%3A%20stream%20Q%2C%20stream%20AC%2C%0Aand%20stream%20TD%2C%20achieving%20the%20best%20model-free%20performance%20in%20DM%20Control%20Dog%0Aenvironments.%20A%20set%20of%20common%20techniques%20underlies%20the%20stream-x%20algorithms%2C%0Aenabling%20their%20success%20with%20a%20single%20set%20of%20hyperparameters%20and%20allowing%20for%0Aeasy%20extension%20to%20other%20algorithms%2C%20thereby%20reviving%20streaming%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%2520Deep%2520Reinforcement%2520Learning%2520Finally%2520Works%26entry.906535625%3DMohamed%2520Elsayed%2520and%2520Gautham%2520Vasan%2520and%2520A.%2520Rupam%2520Mahmood%26entry.1292438233%3D%2520%2520Natural%2520intelligence%2520processes%2520experience%2520as%2520a%2520continuous%2520stream%252C%2520sensing%252C%250Aacting%252C%2520and%2520learning%2520moment-by-moment%2520in%2520real%2520time.%2520Streaming%2520learning%252C%2520the%250Amodus%2520operandi%2520of%2520classic%2520reinforcement%2520learning%2520%2528RL%2529%2520algorithms%2520like%250AQ-learning%2520and%2520TD%252C%2520mimics%2520natural%2520learning%2520by%2520using%2520the%2520most%2520recent%2520sample%250Awithout%2520storing%2520it.%2520This%2520approach%2520is%2520also%2520ideal%2520for%2520resource-constrained%252C%250Acommunication-limited%252C%2520and%2520privacy-sensitive%2520applications.%2520However%252C%2520in%2520deep%2520RL%252C%250Alearners%2520almost%2520always%2520use%2520batch%2520updates%2520and%2520replay%2520buffers%252C%2520making%2520them%250Acomputationally%2520expensive%2520and%2520incompatible%2520with%2520streaming%2520learning.%2520Although%250Athe%2520prevalence%2520of%2520batch%2520deep%2520RL%2520is%2520often%2520attributed%2520to%2520its%2520sample%2520efficiency%252C%2520a%250Amore%2520critical%2520reason%2520for%2520the%2520absence%2520of%2520streaming%2520deep%2520RL%2520is%2520its%2520frequent%250Ainstability%2520and%2520failure%2520to%2520learn%252C%2520which%2520we%2520refer%2520to%2520as%2520stream%2520barrier.%2520This%250Apaper%2520introduces%2520the%2520stream-x%2520algorithms%252C%2520the%2520first%2520class%2520of%2520deep%2520RL%2520algorithms%250Ato%2520overcome%2520stream%2520barrier%2520for%2520both%2520prediction%2520and%2520control%2520and%2520match%2520sample%250Aefficiency%2520of%2520batch%2520RL.%2520Through%2520experiments%2520in%2520Mujoco%2520Gym%252C%2520DM%2520Control%2520Suite%252C%250Aand%2520Atari%2520Games%252C%2520we%2520demonstrate%2520stream%2520barrier%2520in%2520existing%2520algorithms%2520and%250Asuccessful%2520stable%2520learning%2520with%2520our%2520stream-x%2520algorithms%253A%2520stream%2520Q%252C%2520stream%2520AC%252C%250Aand%2520stream%2520TD%252C%2520achieving%2520the%2520best%2520model-free%2520performance%2520in%2520DM%2520Control%2520Dog%250Aenvironments.%2520A%2520set%2520of%2520common%2520techniques%2520underlies%2520the%2520stream-x%2520algorithms%252C%250Aenabling%2520their%2520success%2520with%2520a%2520single%2520set%2520of%2520hyperparameters%2520and%2520allowing%2520for%250Aeasy%2520extension%2520to%2520other%2520algorithms%252C%2520thereby%2520reviving%2520streaming%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%20Deep%20Reinforcement%20Learning%20Finally%20Works&entry.906535625=Mohamed%20Elsayed%20and%20Gautham%20Vasan%20and%20A.%20Rupam%20Mahmood&entry.1292438233=%20%20Natural%20intelligence%20processes%20experience%20as%20a%20continuous%20stream%2C%20sensing%2C%0Aacting%2C%20and%20learning%20moment-by-moment%20in%20real%20time.%20Streaming%20learning%2C%20the%0Amodus%20operandi%20of%20classic%20reinforcement%20learning%20%28RL%29%20algorithms%20like%0AQ-learning%20and%20TD%2C%20mimics%20natural%20learning%20by%20using%20the%20most%20recent%20sample%0Awithout%20storing%20it.%20This%20approach%20is%20also%20ideal%20for%20resource-constrained%2C%0Acommunication-limited%2C%20and%20privacy-sensitive%20applications.%20However%2C%20in%20deep%20RL%2C%0Alearners%20almost%20always%20use%20batch%20updates%20and%20replay%20buffers%2C%20making%20them%0Acomputationally%20expensive%20and%20incompatible%20with%20streaming%20learning.%20Although%0Athe%20prevalence%20of%20batch%20deep%20RL%20is%20often%20attributed%20to%20its%20sample%20efficiency%2C%20a%0Amore%20critical%20reason%20for%20the%20absence%20of%20streaming%20deep%20RL%20is%20its%20frequent%0Ainstability%20and%20failure%20to%20learn%2C%20which%20we%20refer%20to%20as%20stream%20barrier.%20This%0Apaper%20introduces%20the%20stream-x%20algorithms%2C%20the%20first%20class%20of%20deep%20RL%20algorithms%0Ato%20overcome%20stream%20barrier%20for%20both%20prediction%20and%20control%20and%20match%20sample%0Aefficiency%20of%20batch%20RL.%20Through%20experiments%20in%20Mujoco%20Gym%2C%20DM%20Control%20Suite%2C%0Aand%20Atari%20Games%2C%20we%20demonstrate%20stream%20barrier%20in%20existing%20algorithms%20and%0Asuccessful%20stable%20learning%20with%20our%20stream-x%20algorithms%3A%20stream%20Q%2C%20stream%20AC%2C%0Aand%20stream%20TD%2C%20achieving%20the%20best%20model-free%20performance%20in%20DM%20Control%20Dog%0Aenvironments.%20A%20set%20of%20common%20techniques%20underlies%20the%20stream-x%20algorithms%2C%0Aenabling%20their%20success%20with%20a%20single%20set%20of%20hyperparameters%20and%20allowing%20for%0Aeasy%20extension%20to%20other%20algorithms%2C%20thereby%20reviving%20streaming%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14606v1&entry.124074799=Read"},
{"title": "JAMUN: Transferable Molecular Conformational Ensemble Generation with\n  Walk-Jump Sampling", "author": "Ameya Daigavane and Bodhi P. Vani and Saeed Saremi and Joseph Kleinhenz and Joshua Rackers", "abstract": "  Conformational ensembles of protein structures are immensely important both\nto understanding protein function, and for drug discovery in novel modalities\nsuch as cryptic pockets. Current techniques for sampling ensembles are\ncomputationally inefficient, or do not transfer to systems outside their\ntraining data. We present walk-Jump Accelerated Molecular ensembles with\nUniversal Noise (JAMUN), a step towards the goal of efficiently sampling the\nBoltzmann distribution of arbitrary proteins. By extending Walk-Jump Sampling\nto point clouds, JAMUN enables ensemble generation at orders of magnitude\nfaster rates than traditional molecular dynamics or state-of-the-art ML\nmethods. Further, JAMUN is able to predict the stable basins of small peptides\nthat were not seen during training.\n", "link": "http://arxiv.org/abs/2410.14621v1", "date": "2024-10-18", "relevancy": 1.4067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JAMUN%3A%20Transferable%20Molecular%20Conformational%20Ensemble%20Generation%20with%0A%20%20Walk-Jump%20Sampling&body=Title%3A%20JAMUN%3A%20Transferable%20Molecular%20Conformational%20Ensemble%20Generation%20with%0A%20%20Walk-Jump%20Sampling%0AAuthor%3A%20Ameya%20Daigavane%20and%20Bodhi%20P.%20Vani%20and%20Saeed%20Saremi%20and%20Joseph%20Kleinhenz%20and%20Joshua%20Rackers%0AAbstract%3A%20%20%20Conformational%20ensembles%20of%20protein%20structures%20are%20immensely%20important%20both%0Ato%20understanding%20protein%20function%2C%20and%20for%20drug%20discovery%20in%20novel%20modalities%0Asuch%20as%20cryptic%20pockets.%20Current%20techniques%20for%20sampling%20ensembles%20are%0Acomputationally%20inefficient%2C%20or%20do%20not%20transfer%20to%20systems%20outside%20their%0Atraining%20data.%20We%20present%20walk-Jump%20Accelerated%20Molecular%20ensembles%20with%0AUniversal%20Noise%20%28JAMUN%29%2C%20a%20step%20towards%20the%20goal%20of%20efficiently%20sampling%20the%0ABoltzmann%20distribution%20of%20arbitrary%20proteins.%20By%20extending%20Walk-Jump%20Sampling%0Ato%20point%20clouds%2C%20JAMUN%20enables%20ensemble%20generation%20at%20orders%20of%20magnitude%0Afaster%20rates%20than%20traditional%20molecular%20dynamics%20or%20state-of-the-art%20ML%0Amethods.%20Further%2C%20JAMUN%20is%20able%20to%20predict%20the%20stable%20basins%20of%20small%20peptides%0Athat%20were%20not%20seen%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJAMUN%253A%2520Transferable%2520Molecular%2520Conformational%2520Ensemble%2520Generation%2520with%250A%2520%2520Walk-Jump%2520Sampling%26entry.906535625%3DAmeya%2520Daigavane%2520and%2520Bodhi%2520P.%2520Vani%2520and%2520Saeed%2520Saremi%2520and%2520Joseph%2520Kleinhenz%2520and%2520Joshua%2520Rackers%26entry.1292438233%3D%2520%2520Conformational%2520ensembles%2520of%2520protein%2520structures%2520are%2520immensely%2520important%2520both%250Ato%2520understanding%2520protein%2520function%252C%2520and%2520for%2520drug%2520discovery%2520in%2520novel%2520modalities%250Asuch%2520as%2520cryptic%2520pockets.%2520Current%2520techniques%2520for%2520sampling%2520ensembles%2520are%250Acomputationally%2520inefficient%252C%2520or%2520do%2520not%2520transfer%2520to%2520systems%2520outside%2520their%250Atraining%2520data.%2520We%2520present%2520walk-Jump%2520Accelerated%2520Molecular%2520ensembles%2520with%250AUniversal%2520Noise%2520%2528JAMUN%2529%252C%2520a%2520step%2520towards%2520the%2520goal%2520of%2520efficiently%2520sampling%2520the%250ABoltzmann%2520distribution%2520of%2520arbitrary%2520proteins.%2520By%2520extending%2520Walk-Jump%2520Sampling%250Ato%2520point%2520clouds%252C%2520JAMUN%2520enables%2520ensemble%2520generation%2520at%2520orders%2520of%2520magnitude%250Afaster%2520rates%2520than%2520traditional%2520molecular%2520dynamics%2520or%2520state-of-the-art%2520ML%250Amethods.%2520Further%252C%2520JAMUN%2520is%2520able%2520to%2520predict%2520the%2520stable%2520basins%2520of%2520small%2520peptides%250Athat%2520were%2520not%2520seen%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JAMUN%3A%20Transferable%20Molecular%20Conformational%20Ensemble%20Generation%20with%0A%20%20Walk-Jump%20Sampling&entry.906535625=Ameya%20Daigavane%20and%20Bodhi%20P.%20Vani%20and%20Saeed%20Saremi%20and%20Joseph%20Kleinhenz%20and%20Joshua%20Rackers&entry.1292438233=%20%20Conformational%20ensembles%20of%20protein%20structures%20are%20immensely%20important%20both%0Ato%20understanding%20protein%20function%2C%20and%20for%20drug%20discovery%20in%20novel%20modalities%0Asuch%20as%20cryptic%20pockets.%20Current%20techniques%20for%20sampling%20ensembles%20are%0Acomputationally%20inefficient%2C%20or%20do%20not%20transfer%20to%20systems%20outside%20their%0Atraining%20data.%20We%20present%20walk-Jump%20Accelerated%20Molecular%20ensembles%20with%0AUniversal%20Noise%20%28JAMUN%29%2C%20a%20step%20towards%20the%20goal%20of%20efficiently%20sampling%20the%0ABoltzmann%20distribution%20of%20arbitrary%20proteins.%20By%20extending%20Walk-Jump%20Sampling%0Ato%20point%20clouds%2C%20JAMUN%20enables%20ensemble%20generation%20at%20orders%20of%20magnitude%0Afaster%20rates%20than%20traditional%20molecular%20dynamics%20or%20state-of-the-art%20ML%0Amethods.%20Further%2C%20JAMUN%20is%20able%20to%20predict%20the%20stable%20basins%20of%20small%20peptides%0Athat%20were%20not%20seen%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14621v1&entry.124074799=Read"},
{"title": "HR-Bandit: Human-AI Collaborated Linear Recourse Bandit", "author": "Junyu Cao and Ruijiang Gao and Esmaeil Keyvanshokooh", "abstract": "  Human doctors frequently recommend actionable recourses that allow patients\nto modify their conditions to access more effective treatments. Inspired by\nsuch healthcare scenarios, we propose the Recourse Linear UCB\n($\\textsf{RLinUCB}$) algorithm, which optimizes both action selection and\nfeature modifications by balancing exploration and exploitation. We further\nextend this to the Human-AI Linear Recourse Bandit ($\\textsf{HR-Bandit}$),\nwhich integrates human expertise to enhance performance. $\\textsf{HR-Bandit}$\noffers three key guarantees: (i) a warm-start guarantee for improved initial\nperformance, (ii) a human-effort guarantee to minimize required human\ninteractions, and (iii) a robustness guarantee that ensures sublinear regret\neven when human decisions are suboptimal. Empirical results, including a\nhealthcare case study, validate its superior performance against existing\nbenchmarks.\n", "link": "http://arxiv.org/abs/2410.14640v1", "date": "2024-10-18", "relevancy": 1.4006, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HR-Bandit%3A%20Human-AI%20Collaborated%20Linear%20Recourse%20Bandit&body=Title%3A%20HR-Bandit%3A%20Human-AI%20Collaborated%20Linear%20Recourse%20Bandit%0AAuthor%3A%20Junyu%20Cao%20and%20Ruijiang%20Gao%20and%20Esmaeil%20Keyvanshokooh%0AAbstract%3A%20%20%20Human%20doctors%20frequently%20recommend%20actionable%20recourses%20that%20allow%20patients%0Ato%20modify%20their%20conditions%20to%20access%20more%20effective%20treatments.%20Inspired%20by%0Asuch%20healthcare%20scenarios%2C%20we%20propose%20the%20Recourse%20Linear%20UCB%0A%28%24%5Ctextsf%7BRLinUCB%7D%24%29%20algorithm%2C%20which%20optimizes%20both%20action%20selection%20and%0Afeature%20modifications%20by%20balancing%20exploration%20and%20exploitation.%20We%20further%0Aextend%20this%20to%20the%20Human-AI%20Linear%20Recourse%20Bandit%20%28%24%5Ctextsf%7BHR-Bandit%7D%24%29%2C%0Awhich%20integrates%20human%20expertise%20to%20enhance%20performance.%20%24%5Ctextsf%7BHR-Bandit%7D%24%0Aoffers%20three%20key%20guarantees%3A%20%28i%29%20a%20warm-start%20guarantee%20for%20improved%20initial%0Aperformance%2C%20%28ii%29%20a%20human-effort%20guarantee%20to%20minimize%20required%20human%0Ainteractions%2C%20and%20%28iii%29%20a%20robustness%20guarantee%20that%20ensures%20sublinear%20regret%0Aeven%20when%20human%20decisions%20are%20suboptimal.%20Empirical%20results%2C%20including%20a%0Ahealthcare%20case%20study%2C%20validate%20its%20superior%20performance%20against%20existing%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHR-Bandit%253A%2520Human-AI%2520Collaborated%2520Linear%2520Recourse%2520Bandit%26entry.906535625%3DJunyu%2520Cao%2520and%2520Ruijiang%2520Gao%2520and%2520Esmaeil%2520Keyvanshokooh%26entry.1292438233%3D%2520%2520Human%2520doctors%2520frequently%2520recommend%2520actionable%2520recourses%2520that%2520allow%2520patients%250Ato%2520modify%2520their%2520conditions%2520to%2520access%2520more%2520effective%2520treatments.%2520Inspired%2520by%250Asuch%2520healthcare%2520scenarios%252C%2520we%2520propose%2520the%2520Recourse%2520Linear%2520UCB%250A%2528%2524%255Ctextsf%257BRLinUCB%257D%2524%2529%2520algorithm%252C%2520which%2520optimizes%2520both%2520action%2520selection%2520and%250Afeature%2520modifications%2520by%2520balancing%2520exploration%2520and%2520exploitation.%2520We%2520further%250Aextend%2520this%2520to%2520the%2520Human-AI%2520Linear%2520Recourse%2520Bandit%2520%2528%2524%255Ctextsf%257BHR-Bandit%257D%2524%2529%252C%250Awhich%2520integrates%2520human%2520expertise%2520to%2520enhance%2520performance.%2520%2524%255Ctextsf%257BHR-Bandit%257D%2524%250Aoffers%2520three%2520key%2520guarantees%253A%2520%2528i%2529%2520a%2520warm-start%2520guarantee%2520for%2520improved%2520initial%250Aperformance%252C%2520%2528ii%2529%2520a%2520human-effort%2520guarantee%2520to%2520minimize%2520required%2520human%250Ainteractions%252C%2520and%2520%2528iii%2529%2520a%2520robustness%2520guarantee%2520that%2520ensures%2520sublinear%2520regret%250Aeven%2520when%2520human%2520decisions%2520are%2520suboptimal.%2520Empirical%2520results%252C%2520including%2520a%250Ahealthcare%2520case%2520study%252C%2520validate%2520its%2520superior%2520performance%2520against%2520existing%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HR-Bandit%3A%20Human-AI%20Collaborated%20Linear%20Recourse%20Bandit&entry.906535625=Junyu%20Cao%20and%20Ruijiang%20Gao%20and%20Esmaeil%20Keyvanshokooh&entry.1292438233=%20%20Human%20doctors%20frequently%20recommend%20actionable%20recourses%20that%20allow%20patients%0Ato%20modify%20their%20conditions%20to%20access%20more%20effective%20treatments.%20Inspired%20by%0Asuch%20healthcare%20scenarios%2C%20we%20propose%20the%20Recourse%20Linear%20UCB%0A%28%24%5Ctextsf%7BRLinUCB%7D%24%29%20algorithm%2C%20which%20optimizes%20both%20action%20selection%20and%0Afeature%20modifications%20by%20balancing%20exploration%20and%20exploitation.%20We%20further%0Aextend%20this%20to%20the%20Human-AI%20Linear%20Recourse%20Bandit%20%28%24%5Ctextsf%7BHR-Bandit%7D%24%29%2C%0Awhich%20integrates%20human%20expertise%20to%20enhance%20performance.%20%24%5Ctextsf%7BHR-Bandit%7D%24%0Aoffers%20three%20key%20guarantees%3A%20%28i%29%20a%20warm-start%20guarantee%20for%20improved%20initial%0Aperformance%2C%20%28ii%29%20a%20human-effort%20guarantee%20to%20minimize%20required%20human%0Ainteractions%2C%20and%20%28iii%29%20a%20robustness%20guarantee%20that%20ensures%20sublinear%20regret%0Aeven%20when%20human%20decisions%20are%20suboptimal.%20Empirical%20results%2C%20including%20a%0Ahealthcare%20case%20study%2C%20validate%20its%20superior%20performance%20against%20existing%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14640v1&entry.124074799=Read"},
{"title": "Harnessing Causality in Reinforcement Learning With Bagged Decision\n  Times", "author": "Daiqi Gao and Hsin-Yu Lai and Predrag Klasnja and Susan A. Murphy", "abstract": "  We consider reinforcement learning (RL) for a class of problems with bagged\ndecision times. A bag contains a finite sequence of consecutive decision times.\nThe transition dynamics are non-Markovian and non-stationary within a bag.\nFurther, all actions within a bag jointly impact a single reward, observed at\nthe end of the bag. Our goal is to construct an online RL algorithm to maximize\nthe discounted sum of the bag-specific rewards. To handle non-Markovian\ntransitions within a bag, we utilize an expert-provided causal directed acyclic\ngraph (DAG). Based on the DAG, we construct the states as a dynamical Bayesian\nsufficient statistic of the observed history, which results in Markovian state\ntransitions within and across bags. We then frame this problem as a periodic\nMarkov decision process (MDP) that allows non-stationarity within a period. An\nonline RL algorithm based on Bellman-equations for stationary MDPs is\ngeneralized to handle periodic MDPs. To justify the proposed RL algorithm, we\nshow that our constructed state achieves the maximal optimal value function\namong all state constructions for a periodic MDP. Further we prove the Bellman\noptimality equations for periodic MDPs. We evaluate the proposed method on\ntestbed variants, constructed with real data from a mobile health clinical\ntrial.\n", "link": "http://arxiv.org/abs/2410.14659v1", "date": "2024-10-18", "relevancy": 1.3816, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Causality%20in%20Reinforcement%20Learning%20With%20Bagged%20Decision%0A%20%20Times&body=Title%3A%20Harnessing%20Causality%20in%20Reinforcement%20Learning%20With%20Bagged%20Decision%0A%20%20Times%0AAuthor%3A%20Daiqi%20Gao%20and%20Hsin-Yu%20Lai%20and%20Predrag%20Klasnja%20and%20Susan%20A.%20Murphy%0AAbstract%3A%20%20%20We%20consider%20reinforcement%20learning%20%28RL%29%20for%20a%20class%20of%20problems%20with%20bagged%0Adecision%20times.%20A%20bag%20contains%20a%20finite%20sequence%20of%20consecutive%20decision%20times.%0AThe%20transition%20dynamics%20are%20non-Markovian%20and%20non-stationary%20within%20a%20bag.%0AFurther%2C%20all%20actions%20within%20a%20bag%20jointly%20impact%20a%20single%20reward%2C%20observed%20at%0Athe%20end%20of%20the%20bag.%20Our%20goal%20is%20to%20construct%20an%20online%20RL%20algorithm%20to%20maximize%0Athe%20discounted%20sum%20of%20the%20bag-specific%20rewards.%20To%20handle%20non-Markovian%0Atransitions%20within%20a%20bag%2C%20we%20utilize%20an%20expert-provided%20causal%20directed%20acyclic%0Agraph%20%28DAG%29.%20Based%20on%20the%20DAG%2C%20we%20construct%20the%20states%20as%20a%20dynamical%20Bayesian%0Asufficient%20statistic%20of%20the%20observed%20history%2C%20which%20results%20in%20Markovian%20state%0Atransitions%20within%20and%20across%20bags.%20We%20then%20frame%20this%20problem%20as%20a%20periodic%0AMarkov%20decision%20process%20%28MDP%29%20that%20allows%20non-stationarity%20within%20a%20period.%20An%0Aonline%20RL%20algorithm%20based%20on%20Bellman-equations%20for%20stationary%20MDPs%20is%0Ageneralized%20to%20handle%20periodic%20MDPs.%20To%20justify%20the%20proposed%20RL%20algorithm%2C%20we%0Ashow%20that%20our%20constructed%20state%20achieves%20the%20maximal%20optimal%20value%20function%0Aamong%20all%20state%20constructions%20for%20a%20periodic%20MDP.%20Further%20we%20prove%20the%20Bellman%0Aoptimality%20equations%20for%20periodic%20MDPs.%20We%20evaluate%20the%20proposed%20method%20on%0Atestbed%20variants%2C%20constructed%20with%20real%20data%20from%20a%20mobile%20health%20clinical%0Atrial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Causality%2520in%2520Reinforcement%2520Learning%2520With%2520Bagged%2520Decision%250A%2520%2520Times%26entry.906535625%3DDaiqi%2520Gao%2520and%2520Hsin-Yu%2520Lai%2520and%2520Predrag%2520Klasnja%2520and%2520Susan%2520A.%2520Murphy%26entry.1292438233%3D%2520%2520We%2520consider%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520a%2520class%2520of%2520problems%2520with%2520bagged%250Adecision%2520times.%2520A%2520bag%2520contains%2520a%2520finite%2520sequence%2520of%2520consecutive%2520decision%2520times.%250AThe%2520transition%2520dynamics%2520are%2520non-Markovian%2520and%2520non-stationary%2520within%2520a%2520bag.%250AFurther%252C%2520all%2520actions%2520within%2520a%2520bag%2520jointly%2520impact%2520a%2520single%2520reward%252C%2520observed%2520at%250Athe%2520end%2520of%2520the%2520bag.%2520Our%2520goal%2520is%2520to%2520construct%2520an%2520online%2520RL%2520algorithm%2520to%2520maximize%250Athe%2520discounted%2520sum%2520of%2520the%2520bag-specific%2520rewards.%2520To%2520handle%2520non-Markovian%250Atransitions%2520within%2520a%2520bag%252C%2520we%2520utilize%2520an%2520expert-provided%2520causal%2520directed%2520acyclic%250Agraph%2520%2528DAG%2529.%2520Based%2520on%2520the%2520DAG%252C%2520we%2520construct%2520the%2520states%2520as%2520a%2520dynamical%2520Bayesian%250Asufficient%2520statistic%2520of%2520the%2520observed%2520history%252C%2520which%2520results%2520in%2520Markovian%2520state%250Atransitions%2520within%2520and%2520across%2520bags.%2520We%2520then%2520frame%2520this%2520problem%2520as%2520a%2520periodic%250AMarkov%2520decision%2520process%2520%2528MDP%2529%2520that%2520allows%2520non-stationarity%2520within%2520a%2520period.%2520An%250Aonline%2520RL%2520algorithm%2520based%2520on%2520Bellman-equations%2520for%2520stationary%2520MDPs%2520is%250Ageneralized%2520to%2520handle%2520periodic%2520MDPs.%2520To%2520justify%2520the%2520proposed%2520RL%2520algorithm%252C%2520we%250Ashow%2520that%2520our%2520constructed%2520state%2520achieves%2520the%2520maximal%2520optimal%2520value%2520function%250Aamong%2520all%2520state%2520constructions%2520for%2520a%2520periodic%2520MDP.%2520Further%2520we%2520prove%2520the%2520Bellman%250Aoptimality%2520equations%2520for%2520periodic%2520MDPs.%2520We%2520evaluate%2520the%2520proposed%2520method%2520on%250Atestbed%2520variants%252C%2520constructed%2520with%2520real%2520data%2520from%2520a%2520mobile%2520health%2520clinical%250Atrial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Causality%20in%20Reinforcement%20Learning%20With%20Bagged%20Decision%0A%20%20Times&entry.906535625=Daiqi%20Gao%20and%20Hsin-Yu%20Lai%20and%20Predrag%20Klasnja%20and%20Susan%20A.%20Murphy&entry.1292438233=%20%20We%20consider%20reinforcement%20learning%20%28RL%29%20for%20a%20class%20of%20problems%20with%20bagged%0Adecision%20times.%20A%20bag%20contains%20a%20finite%20sequence%20of%20consecutive%20decision%20times.%0AThe%20transition%20dynamics%20are%20non-Markovian%20and%20non-stationary%20within%20a%20bag.%0AFurther%2C%20all%20actions%20within%20a%20bag%20jointly%20impact%20a%20single%20reward%2C%20observed%20at%0Athe%20end%20of%20the%20bag.%20Our%20goal%20is%20to%20construct%20an%20online%20RL%20algorithm%20to%20maximize%0Athe%20discounted%20sum%20of%20the%20bag-specific%20rewards.%20To%20handle%20non-Markovian%0Atransitions%20within%20a%20bag%2C%20we%20utilize%20an%20expert-provided%20causal%20directed%20acyclic%0Agraph%20%28DAG%29.%20Based%20on%20the%20DAG%2C%20we%20construct%20the%20states%20as%20a%20dynamical%20Bayesian%0Asufficient%20statistic%20of%20the%20observed%20history%2C%20which%20results%20in%20Markovian%20state%0Atransitions%20within%20and%20across%20bags.%20We%20then%20frame%20this%20problem%20as%20a%20periodic%0AMarkov%20decision%20process%20%28MDP%29%20that%20allows%20non-stationarity%20within%20a%20period.%20An%0Aonline%20RL%20algorithm%20based%20on%20Bellman-equations%20for%20stationary%20MDPs%20is%0Ageneralized%20to%20handle%20periodic%20MDPs.%20To%20justify%20the%20proposed%20RL%20algorithm%2C%20we%0Ashow%20that%20our%20constructed%20state%20achieves%20the%20maximal%20optimal%20value%20function%0Aamong%20all%20state%20constructions%20for%20a%20periodic%20MDP.%20Further%20we%20prove%20the%20Bellman%0Aoptimality%20equations%20for%20periodic%20MDPs.%20We%20evaluate%20the%20proposed%20method%20on%0Atestbed%20variants%2C%20constructed%20with%20real%20data%20from%20a%20mobile%20health%20clinical%0Atrial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14659v1&entry.124074799=Read"},
{"title": "Rethinking Distance Metrics for Counterfactual Explainability", "author": "Joshua Nathaniel Williams and Anurag Katakkar and Hoda Heidari and J. Zico Kolter", "abstract": "  Counterfactual explanations have been a popular method of post-hoc\nexplainability for a variety of settings in Machine Learning. Such methods\nfocus on explaining classifiers by generating new data points that are similar\nto a given reference, while receiving a more desirable prediction. In this\nwork, we investigate a framing for counterfactual generation methods that\nconsiders counterfactuals not as independent draws from a region around the\nreference, but as jointly sampled with the reference from the underlying data\ndistribution. Through this framing, we derive a distance metric, tailored for\ncounterfactual similarity that can be applied to a broad range of settings.\nThrough both quantitative and qualitative analyses of counterfactual generation\nmethods, we show that this framing allows us to express more nuanced\ndependencies among the covariates.\n", "link": "http://arxiv.org/abs/2410.14522v1", "date": "2024-10-18", "relevancy": 1.3752, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Distance%20Metrics%20for%20Counterfactual%20Explainability&body=Title%3A%20Rethinking%20Distance%20Metrics%20for%20Counterfactual%20Explainability%0AAuthor%3A%20Joshua%20Nathaniel%20Williams%20and%20Anurag%20Katakkar%20and%20Hoda%20Heidari%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Counterfactual%20explanations%20have%20been%20a%20popular%20method%20of%20post-hoc%0Aexplainability%20for%20a%20variety%20of%20settings%20in%20Machine%20Learning.%20Such%20methods%0Afocus%20on%20explaining%20classifiers%20by%20generating%20new%20data%20points%20that%20are%20similar%0Ato%20a%20given%20reference%2C%20while%20receiving%20a%20more%20desirable%20prediction.%20In%20this%0Awork%2C%20we%20investigate%20a%20framing%20for%20counterfactual%20generation%20methods%20that%0Aconsiders%20counterfactuals%20not%20as%20independent%20draws%20from%20a%20region%20around%20the%0Areference%2C%20but%20as%20jointly%20sampled%20with%20the%20reference%20from%20the%20underlying%20data%0Adistribution.%20Through%20this%20framing%2C%20we%20derive%20a%20distance%20metric%2C%20tailored%20for%0Acounterfactual%20similarity%20that%20can%20be%20applied%20to%20a%20broad%20range%20of%20settings.%0AThrough%20both%20quantitative%20and%20qualitative%20analyses%20of%20counterfactual%20generation%0Amethods%2C%20we%20show%20that%20this%20framing%20allows%20us%20to%20express%20more%20nuanced%0Adependencies%20among%20the%20covariates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Distance%2520Metrics%2520for%2520Counterfactual%2520Explainability%26entry.906535625%3DJoshua%2520Nathaniel%2520Williams%2520and%2520Anurag%2520Katakkar%2520and%2520Hoda%2520Heidari%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Counterfactual%2520explanations%2520have%2520been%2520a%2520popular%2520method%2520of%2520post-hoc%250Aexplainability%2520for%2520a%2520variety%2520of%2520settings%2520in%2520Machine%2520Learning.%2520Such%2520methods%250Afocus%2520on%2520explaining%2520classifiers%2520by%2520generating%2520new%2520data%2520points%2520that%2520are%2520similar%250Ato%2520a%2520given%2520reference%252C%2520while%2520receiving%2520a%2520more%2520desirable%2520prediction.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520a%2520framing%2520for%2520counterfactual%2520generation%2520methods%2520that%250Aconsiders%2520counterfactuals%2520not%2520as%2520independent%2520draws%2520from%2520a%2520region%2520around%2520the%250Areference%252C%2520but%2520as%2520jointly%2520sampled%2520with%2520the%2520reference%2520from%2520the%2520underlying%2520data%250Adistribution.%2520Through%2520this%2520framing%252C%2520we%2520derive%2520a%2520distance%2520metric%252C%2520tailored%2520for%250Acounterfactual%2520similarity%2520that%2520can%2520be%2520applied%2520to%2520a%2520broad%2520range%2520of%2520settings.%250AThrough%2520both%2520quantitative%2520and%2520qualitative%2520analyses%2520of%2520counterfactual%2520generation%250Amethods%252C%2520we%2520show%2520that%2520this%2520framing%2520allows%2520us%2520to%2520express%2520more%2520nuanced%250Adependencies%2520among%2520the%2520covariates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Distance%20Metrics%20for%20Counterfactual%20Explainability&entry.906535625=Joshua%20Nathaniel%20Williams%20and%20Anurag%20Katakkar%20and%20Hoda%20Heidari%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Counterfactual%20explanations%20have%20been%20a%20popular%20method%20of%20post-hoc%0Aexplainability%20for%20a%20variety%20of%20settings%20in%20Machine%20Learning.%20Such%20methods%0Afocus%20on%20explaining%20classifiers%20by%20generating%20new%20data%20points%20that%20are%20similar%0Ato%20a%20given%20reference%2C%20while%20receiving%20a%20more%20desirable%20prediction.%20In%20this%0Awork%2C%20we%20investigate%20a%20framing%20for%20counterfactual%20generation%20methods%20that%0Aconsiders%20counterfactuals%20not%20as%20independent%20draws%20from%20a%20region%20around%20the%0Areference%2C%20but%20as%20jointly%20sampled%20with%20the%20reference%20from%20the%20underlying%20data%0Adistribution.%20Through%20this%20framing%2C%20we%20derive%20a%20distance%20metric%2C%20tailored%20for%0Acounterfactual%20similarity%20that%20can%20be%20applied%20to%20a%20broad%20range%20of%20settings.%0AThrough%20both%20quantitative%20and%20qualitative%20analyses%20of%20counterfactual%20generation%0Amethods%2C%20we%20show%20that%20this%20framing%20allows%20us%20to%20express%20more%20nuanced%0Adependencies%20among%20the%20covariates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14522v1&entry.124074799=Read"},
{"title": "A Lipschitz spaces view of infinitely wide shallow neural networks", "author": "Francesca Bartolucci and Marcello Carioni and Jos\u00e9 A. Iglesias and Yury Korolev and Emanuele Naldi and Stefano Vigogna", "abstract": "  We revisit the mean field parametrization of shallow neural networks, using\nsigned measures on unbounded parameter spaces and duality pairings that take\ninto account the regularity and growth of activation functions. This setting\ndirectly leads to the use of unbalanced Kantorovich-Rubinstein norms defined by\nduality with Lipschitz functions, and of spaces of measures dual to those of\ncontinuous functions with controlled growth. These allow to make transparent\nthe need for total variation and moment bounds or penalization to obtain\nexistence of minimizers of variational formulations, under which we prove a\ncompactness result in strong Kantorovich-Rubinstein norm, and in the absence of\nwhich we show several examples demonstrating undesirable behavior. Further, the\nKantorovich-Rubinstein setting enables us to combine the advantages of a\ncompletely linear parametrization and ensuing reproducing kernel Banach space\nframework with optimal transport insights. We showcase this synergy with\nrepresenter theorems and uniform large data limits for empirical risk\nminimization, and in proposed formulations for distillation and fusion\napplications.\n", "link": "http://arxiv.org/abs/2410.14591v1", "date": "2024-10-18", "relevancy": 1.3662, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4616}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lipschitz%20spaces%20view%20of%20infinitely%20wide%20shallow%20neural%20networks&body=Title%3A%20A%20Lipschitz%20spaces%20view%20of%20infinitely%20wide%20shallow%20neural%20networks%0AAuthor%3A%20Francesca%20Bartolucci%20and%20Marcello%20Carioni%20and%20Jos%C3%A9%20A.%20Iglesias%20and%20Yury%20Korolev%20and%20Emanuele%20Naldi%20and%20Stefano%20Vigogna%0AAbstract%3A%20%20%20We%20revisit%20the%20mean%20field%20parametrization%20of%20shallow%20neural%20networks%2C%20using%0Asigned%20measures%20on%20unbounded%20parameter%20spaces%20and%20duality%20pairings%20that%20take%0Ainto%20account%20the%20regularity%20and%20growth%20of%20activation%20functions.%20This%20setting%0Adirectly%20leads%20to%20the%20use%20of%20unbalanced%20Kantorovich-Rubinstein%20norms%20defined%20by%0Aduality%20with%20Lipschitz%20functions%2C%20and%20of%20spaces%20of%20measures%20dual%20to%20those%20of%0Acontinuous%20functions%20with%20controlled%20growth.%20These%20allow%20to%20make%20transparent%0Athe%20need%20for%20total%20variation%20and%20moment%20bounds%20or%20penalization%20to%20obtain%0Aexistence%20of%20minimizers%20of%20variational%20formulations%2C%20under%20which%20we%20prove%20a%0Acompactness%20result%20in%20strong%20Kantorovich-Rubinstein%20norm%2C%20and%20in%20the%20absence%20of%0Awhich%20we%20show%20several%20examples%20demonstrating%20undesirable%20behavior.%20Further%2C%20the%0AKantorovich-Rubinstein%20setting%20enables%20us%20to%20combine%20the%20advantages%20of%20a%0Acompletely%20linear%20parametrization%20and%20ensuing%20reproducing%20kernel%20Banach%20space%0Aframework%20with%20optimal%20transport%20insights.%20We%20showcase%20this%20synergy%20with%0Arepresenter%20theorems%20and%20uniform%20large%20data%20limits%20for%20empirical%20risk%0Aminimization%2C%20and%20in%20proposed%20formulations%20for%20distillation%20and%20fusion%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lipschitz%2520spaces%2520view%2520of%2520infinitely%2520wide%2520shallow%2520neural%2520networks%26entry.906535625%3DFrancesca%2520Bartolucci%2520and%2520Marcello%2520Carioni%2520and%2520Jos%25C3%25A9%2520A.%2520Iglesias%2520and%2520Yury%2520Korolev%2520and%2520Emanuele%2520Naldi%2520and%2520Stefano%2520Vigogna%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520mean%2520field%2520parametrization%2520of%2520shallow%2520neural%2520networks%252C%2520using%250Asigned%2520measures%2520on%2520unbounded%2520parameter%2520spaces%2520and%2520duality%2520pairings%2520that%2520take%250Ainto%2520account%2520the%2520regularity%2520and%2520growth%2520of%2520activation%2520functions.%2520This%2520setting%250Adirectly%2520leads%2520to%2520the%2520use%2520of%2520unbalanced%2520Kantorovich-Rubinstein%2520norms%2520defined%2520by%250Aduality%2520with%2520Lipschitz%2520functions%252C%2520and%2520of%2520spaces%2520of%2520measures%2520dual%2520to%2520those%2520of%250Acontinuous%2520functions%2520with%2520controlled%2520growth.%2520These%2520allow%2520to%2520make%2520transparent%250Athe%2520need%2520for%2520total%2520variation%2520and%2520moment%2520bounds%2520or%2520penalization%2520to%2520obtain%250Aexistence%2520of%2520minimizers%2520of%2520variational%2520formulations%252C%2520under%2520which%2520we%2520prove%2520a%250Acompactness%2520result%2520in%2520strong%2520Kantorovich-Rubinstein%2520norm%252C%2520and%2520in%2520the%2520absence%2520of%250Awhich%2520we%2520show%2520several%2520examples%2520demonstrating%2520undesirable%2520behavior.%2520Further%252C%2520the%250AKantorovich-Rubinstein%2520setting%2520enables%2520us%2520to%2520combine%2520the%2520advantages%2520of%2520a%250Acompletely%2520linear%2520parametrization%2520and%2520ensuing%2520reproducing%2520kernel%2520Banach%2520space%250Aframework%2520with%2520optimal%2520transport%2520insights.%2520We%2520showcase%2520this%2520synergy%2520with%250Arepresenter%2520theorems%2520and%2520uniform%2520large%2520data%2520limits%2520for%2520empirical%2520risk%250Aminimization%252C%2520and%2520in%2520proposed%2520formulations%2520for%2520distillation%2520and%2520fusion%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lipschitz%20spaces%20view%20of%20infinitely%20wide%20shallow%20neural%20networks&entry.906535625=Francesca%20Bartolucci%20and%20Marcello%20Carioni%20and%20Jos%C3%A9%20A.%20Iglesias%20and%20Yury%20Korolev%20and%20Emanuele%20Naldi%20and%20Stefano%20Vigogna&entry.1292438233=%20%20We%20revisit%20the%20mean%20field%20parametrization%20of%20shallow%20neural%20networks%2C%20using%0Asigned%20measures%20on%20unbounded%20parameter%20spaces%20and%20duality%20pairings%20that%20take%0Ainto%20account%20the%20regularity%20and%20growth%20of%20activation%20functions.%20This%20setting%0Adirectly%20leads%20to%20the%20use%20of%20unbalanced%20Kantorovich-Rubinstein%20norms%20defined%20by%0Aduality%20with%20Lipschitz%20functions%2C%20and%20of%20spaces%20of%20measures%20dual%20to%20those%20of%0Acontinuous%20functions%20with%20controlled%20growth.%20These%20allow%20to%20make%20transparent%0Athe%20need%20for%20total%20variation%20and%20moment%20bounds%20or%20penalization%20to%20obtain%0Aexistence%20of%20minimizers%20of%20variational%20formulations%2C%20under%20which%20we%20prove%20a%0Acompactness%20result%20in%20strong%20Kantorovich-Rubinstein%20norm%2C%20and%20in%20the%20absence%20of%0Awhich%20we%20show%20several%20examples%20demonstrating%20undesirable%20behavior.%20Further%2C%20the%0AKantorovich-Rubinstein%20setting%20enables%20us%20to%20combine%20the%20advantages%20of%20a%0Acompletely%20linear%20parametrization%20and%20ensuing%20reproducing%20kernel%20Banach%20space%0Aframework%20with%20optimal%20transport%20insights.%20We%20showcase%20this%20synergy%20with%0Arepresenter%20theorems%20and%20uniform%20large%20data%20limits%20for%20empirical%20risk%0Aminimization%2C%20and%20in%20proposed%20formulations%20for%20distillation%20and%20fusion%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14591v1&entry.124074799=Read"},
{"title": "Learning With Multi-Group Guarantees For Clusterable Subpopulations", "author": "Jessica Dai and Nika Haghtalab and Eric Zhao", "abstract": "  A canonical desideratum for prediction problems is that performance\nguarantees should hold not just on average over the population, but also for\nmeaningful subpopulations within the overall population. But what constitutes a\nmeaningful subpopulation? In this work, we take the perspective that relevant\nsubpopulations should be defined with respect to the clusters that naturally\nemerge from the distribution of individuals for which predictions are being\nmade. In this view, a population refers to a mixture model whose components\nconstitute the relevant subpopulations. We suggest two formalisms for capturing\nper-subgroup guarantees: first, by attributing each individual to the component\nfrom which they were most likely drawn, given their features; and second, by\nattributing each individual to all components in proportion to their relative\nlikelihood of having been drawn from each component. Using online calibration\nas a case study, we study a \\variational algorithm that provides guarantees for\neach of these formalisms by handling all plausible underlying subpopulation\nstructures simultaneously, and achieve an $O(T^{1/2})$ rate even when the\nsubpopulations are not well-separated. In comparison, the more natural\ncluster-then-predict approach that first recovers the structure of the\nsubpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and\nrequires the subpopulations to be separable. Along the way, we prove that\nproviding per-subgroup calibration guarantees for underlying clusters can be\neasier than learning the clusters: separation between median subgroup features\nis required for the latter but not the former.\n", "link": "http://arxiv.org/abs/2410.14588v1", "date": "2024-10-18", "relevancy": 1.3562, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4838}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4457}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20With%20Multi-Group%20Guarantees%20For%20Clusterable%20Subpopulations&body=Title%3A%20Learning%20With%20Multi-Group%20Guarantees%20For%20Clusterable%20Subpopulations%0AAuthor%3A%20Jessica%20Dai%20and%20Nika%20Haghtalab%20and%20Eric%20Zhao%0AAbstract%3A%20%20%20A%20canonical%20desideratum%20for%20prediction%20problems%20is%20that%20performance%0Aguarantees%20should%20hold%20not%20just%20on%20average%20over%20the%20population%2C%20but%20also%20for%0Ameaningful%20subpopulations%20within%20the%20overall%20population.%20But%20what%20constitutes%20a%0Ameaningful%20subpopulation%3F%20In%20this%20work%2C%20we%20take%20the%20perspective%20that%20relevant%0Asubpopulations%20should%20be%20defined%20with%20respect%20to%20the%20clusters%20that%20naturally%0Aemerge%20from%20the%20distribution%20of%20individuals%20for%20which%20predictions%20are%20being%0Amade.%20In%20this%20view%2C%20a%20population%20refers%20to%20a%20mixture%20model%20whose%20components%0Aconstitute%20the%20relevant%20subpopulations.%20We%20suggest%20two%20formalisms%20for%20capturing%0Aper-subgroup%20guarantees%3A%20first%2C%20by%20attributing%20each%20individual%20to%20the%20component%0Afrom%20which%20they%20were%20most%20likely%20drawn%2C%20given%20their%20features%3B%20and%20second%2C%20by%0Aattributing%20each%20individual%20to%20all%20components%20in%20proportion%20to%20their%20relative%0Alikelihood%20of%20having%20been%20drawn%20from%20each%20component.%20Using%20online%20calibration%0Aas%20a%20case%20study%2C%20we%20study%20a%20%5Cvariational%20algorithm%20that%20provides%20guarantees%20for%0Aeach%20of%20these%20formalisms%20by%20handling%20all%20plausible%20underlying%20subpopulation%0Astructures%20simultaneously%2C%20and%20achieve%20an%20%24O%28T%5E%7B1/2%7D%29%24%20rate%20even%20when%20the%0Asubpopulations%20are%20not%20well-separated.%20In%20comparison%2C%20the%20more%20natural%0Acluster-then-predict%20approach%20that%20first%20recovers%20the%20structure%20of%20the%0Asubpopulations%20and%20then%20makes%20predictions%20suffers%20from%20a%20%24O%28T%5E%7B2/3%7D%29%24%20rate%20and%0Arequires%20the%20subpopulations%20to%20be%20separable.%20Along%20the%20way%2C%20we%20prove%20that%0Aproviding%20per-subgroup%20calibration%20guarantees%20for%20underlying%20clusters%20can%20be%0Aeasier%20than%20learning%20the%20clusters%3A%20separation%20between%20median%20subgroup%20features%0Ais%20required%20for%20the%20latter%20but%20not%20the%20former.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520With%2520Multi-Group%2520Guarantees%2520For%2520Clusterable%2520Subpopulations%26entry.906535625%3DJessica%2520Dai%2520and%2520Nika%2520Haghtalab%2520and%2520Eric%2520Zhao%26entry.1292438233%3D%2520%2520A%2520canonical%2520desideratum%2520for%2520prediction%2520problems%2520is%2520that%2520performance%250Aguarantees%2520should%2520hold%2520not%2520just%2520on%2520average%2520over%2520the%2520population%252C%2520but%2520also%2520for%250Ameaningful%2520subpopulations%2520within%2520the%2520overall%2520population.%2520But%2520what%2520constitutes%2520a%250Ameaningful%2520subpopulation%253F%2520In%2520this%2520work%252C%2520we%2520take%2520the%2520perspective%2520that%2520relevant%250Asubpopulations%2520should%2520be%2520defined%2520with%2520respect%2520to%2520the%2520clusters%2520that%2520naturally%250Aemerge%2520from%2520the%2520distribution%2520of%2520individuals%2520for%2520which%2520predictions%2520are%2520being%250Amade.%2520In%2520this%2520view%252C%2520a%2520population%2520refers%2520to%2520a%2520mixture%2520model%2520whose%2520components%250Aconstitute%2520the%2520relevant%2520subpopulations.%2520We%2520suggest%2520two%2520formalisms%2520for%2520capturing%250Aper-subgroup%2520guarantees%253A%2520first%252C%2520by%2520attributing%2520each%2520individual%2520to%2520the%2520component%250Afrom%2520which%2520they%2520were%2520most%2520likely%2520drawn%252C%2520given%2520their%2520features%253B%2520and%2520second%252C%2520by%250Aattributing%2520each%2520individual%2520to%2520all%2520components%2520in%2520proportion%2520to%2520their%2520relative%250Alikelihood%2520of%2520having%2520been%2520drawn%2520from%2520each%2520component.%2520Using%2520online%2520calibration%250Aas%2520a%2520case%2520study%252C%2520we%2520study%2520a%2520%255Cvariational%2520algorithm%2520that%2520provides%2520guarantees%2520for%250Aeach%2520of%2520these%2520formalisms%2520by%2520handling%2520all%2520plausible%2520underlying%2520subpopulation%250Astructures%2520simultaneously%252C%2520and%2520achieve%2520an%2520%2524O%2528T%255E%257B1/2%257D%2529%2524%2520rate%2520even%2520when%2520the%250Asubpopulations%2520are%2520not%2520well-separated.%2520In%2520comparison%252C%2520the%2520more%2520natural%250Acluster-then-predict%2520approach%2520that%2520first%2520recovers%2520the%2520structure%2520of%2520the%250Asubpopulations%2520and%2520then%2520makes%2520predictions%2520suffers%2520from%2520a%2520%2524O%2528T%255E%257B2/3%257D%2529%2524%2520rate%2520and%250Arequires%2520the%2520subpopulations%2520to%2520be%2520separable.%2520Along%2520the%2520way%252C%2520we%2520prove%2520that%250Aproviding%2520per-subgroup%2520calibration%2520guarantees%2520for%2520underlying%2520clusters%2520can%2520be%250Aeasier%2520than%2520learning%2520the%2520clusters%253A%2520separation%2520between%2520median%2520subgroup%2520features%250Ais%2520required%2520for%2520the%2520latter%2520but%2520not%2520the%2520former.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20With%20Multi-Group%20Guarantees%20For%20Clusterable%20Subpopulations&entry.906535625=Jessica%20Dai%20and%20Nika%20Haghtalab%20and%20Eric%20Zhao&entry.1292438233=%20%20A%20canonical%20desideratum%20for%20prediction%20problems%20is%20that%20performance%0Aguarantees%20should%20hold%20not%20just%20on%20average%20over%20the%20population%2C%20but%20also%20for%0Ameaningful%20subpopulations%20within%20the%20overall%20population.%20But%20what%20constitutes%20a%0Ameaningful%20subpopulation%3F%20In%20this%20work%2C%20we%20take%20the%20perspective%20that%20relevant%0Asubpopulations%20should%20be%20defined%20with%20respect%20to%20the%20clusters%20that%20naturally%0Aemerge%20from%20the%20distribution%20of%20individuals%20for%20which%20predictions%20are%20being%0Amade.%20In%20this%20view%2C%20a%20population%20refers%20to%20a%20mixture%20model%20whose%20components%0Aconstitute%20the%20relevant%20subpopulations.%20We%20suggest%20two%20formalisms%20for%20capturing%0Aper-subgroup%20guarantees%3A%20first%2C%20by%20attributing%20each%20individual%20to%20the%20component%0Afrom%20which%20they%20were%20most%20likely%20drawn%2C%20given%20their%20features%3B%20and%20second%2C%20by%0Aattributing%20each%20individual%20to%20all%20components%20in%20proportion%20to%20their%20relative%0Alikelihood%20of%20having%20been%20drawn%20from%20each%20component.%20Using%20online%20calibration%0Aas%20a%20case%20study%2C%20we%20study%20a%20%5Cvariational%20algorithm%20that%20provides%20guarantees%20for%0Aeach%20of%20these%20formalisms%20by%20handling%20all%20plausible%20underlying%20subpopulation%0Astructures%20simultaneously%2C%20and%20achieve%20an%20%24O%28T%5E%7B1/2%7D%29%24%20rate%20even%20when%20the%0Asubpopulations%20are%20not%20well-separated.%20In%20comparison%2C%20the%20more%20natural%0Acluster-then-predict%20approach%20that%20first%20recovers%20the%20structure%20of%20the%0Asubpopulations%20and%20then%20makes%20predictions%20suffers%20from%20a%20%24O%28T%5E%7B2/3%7D%29%24%20rate%20and%0Arequires%20the%20subpopulations%20to%20be%20separable.%20Along%20the%20way%2C%20we%20prove%20that%0Aproviding%20per-subgroup%20calibration%20guarantees%20for%20underlying%20clusters%20can%20be%0Aeasier%20than%20learning%20the%20clusters%3A%20separation%20between%20median%20subgroup%20features%0Ais%20required%20for%20the%20latter%20but%20not%20the%20former.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14588v1&entry.124074799=Read"},
{"title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback", "author": "Zonghai Yao and Aditya Parashar and Huixue Zhou and Won Seok Jang and Feiyun Ouyang and Zhichao Yang and Hong Yu", "abstract": "  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n", "link": "http://arxiv.org/abs/2410.13191v2", "date": "2024-10-18", "relevancy": 1.3325, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4609}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4574}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCQG-SRefine%3A%20Multiple%20Choice%20Question%20Generation%20and%20Evaluation%20with%0A%20%20Iterative%20Self-Critique%2C%20Correction%2C%20and%20Comparison%20Feedback&body=Title%3A%20MCQG-SRefine%3A%20Multiple%20Choice%20Question%20Generation%20and%20Evaluation%20with%0A%20%20Iterative%20Self-Critique%2C%20Correction%2C%20and%20Comparison%20Feedback%0AAuthor%3A%20Zonghai%20Yao%20and%20Aditya%20Parashar%20and%20Huixue%20Zhou%20and%20Won%20Seok%20Jang%20and%20Feiyun%20Ouyang%20and%20Zhichao%20Yang%20and%20Hong%20Yu%0AAbstract%3A%20%20%20Automatic%20question%20generation%20%28QG%29%20is%20essential%20for%20AI%20and%20NLP%2C%20particularly%0Ain%20intelligent%20tutoring%2C%20dialogue%20systems%2C%20and%20fact%20verification.%20Generating%0Amultiple-choice%20questions%20%28MCQG%29%20for%20professional%20exams%2C%20like%20the%20United%20States%0AMedical%20Licensing%20Examination%20%28USMLE%29%2C%20is%20particularly%20challenging%2C%20requiring%0Adomain%20expertise%20and%20complex%20multi-hop%20reasoning%20for%20high-quality%20questions.%0AHowever%2C%20current%20large%20language%20models%20%28LLMs%29%20like%20GPT-4%20struggle%20with%0Aprofessional%20MCQG%20due%20to%20outdated%20knowledge%2C%20hallucination%20issues%2C%20and%20prompt%0Asensitivity%2C%20resulting%20in%20unsatisfactory%20quality%20and%20difficulty.%20To%20address%0Athese%20challenges%2C%20we%20propose%20MCQG-SRefine%2C%20an%20LLM%20self-refine-based%20%28Critique%0Aand%20Correction%29%20framework%20for%20converting%20medical%20cases%20into%20high-quality%0AUSMLE-style%20questions.%20By%20integrating%20expert-driven%20prompt%20engineering%20with%0Aiterative%20self-critique%20and%20self-correction%20feedback%2C%20MCQG-SRefine%0Asignificantly%20enhances%20human%20expert%20satisfaction%20regarding%20both%20the%20quality%20and%0Adifficulty%20of%20the%20questions.%20Furthermore%2C%20we%20introduce%20an%20LLM-as-Judge-based%0Aautomatic%20metric%20to%20replace%20the%20complex%20and%20costly%20expert%20evaluation%20process%2C%0Aensuring%20reliable%20and%20expert-aligned%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCQG-SRefine%253A%2520Multiple%2520Choice%2520Question%2520Generation%2520and%2520Evaluation%2520with%250A%2520%2520Iterative%2520Self-Critique%252C%2520Correction%252C%2520and%2520Comparison%2520Feedback%26entry.906535625%3DZonghai%2520Yao%2520and%2520Aditya%2520Parashar%2520and%2520Huixue%2520Zhou%2520and%2520Won%2520Seok%2520Jang%2520and%2520Feiyun%2520Ouyang%2520and%2520Zhichao%2520Yang%2520and%2520Hong%2520Yu%26entry.1292438233%3D%2520%2520Automatic%2520question%2520generation%2520%2528QG%2529%2520is%2520essential%2520for%2520AI%2520and%2520NLP%252C%2520particularly%250Ain%2520intelligent%2520tutoring%252C%2520dialogue%2520systems%252C%2520and%2520fact%2520verification.%2520Generating%250Amultiple-choice%2520questions%2520%2528MCQG%2529%2520for%2520professional%2520exams%252C%2520like%2520the%2520United%2520States%250AMedical%2520Licensing%2520Examination%2520%2528USMLE%2529%252C%2520is%2520particularly%2520challenging%252C%2520requiring%250Adomain%2520expertise%2520and%2520complex%2520multi-hop%2520reasoning%2520for%2520high-quality%2520questions.%250AHowever%252C%2520current%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%2520GPT-4%2520struggle%2520with%250Aprofessional%2520MCQG%2520due%2520to%2520outdated%2520knowledge%252C%2520hallucination%2520issues%252C%2520and%2520prompt%250Asensitivity%252C%2520resulting%2520in%2520unsatisfactory%2520quality%2520and%2520difficulty.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520MCQG-SRefine%252C%2520an%2520LLM%2520self-refine-based%2520%2528Critique%250Aand%2520Correction%2529%2520framework%2520for%2520converting%2520medical%2520cases%2520into%2520high-quality%250AUSMLE-style%2520questions.%2520By%2520integrating%2520expert-driven%2520prompt%2520engineering%2520with%250Aiterative%2520self-critique%2520and%2520self-correction%2520feedback%252C%2520MCQG-SRefine%250Asignificantly%2520enhances%2520human%2520expert%2520satisfaction%2520regarding%2520both%2520the%2520quality%2520and%250Adifficulty%2520of%2520the%2520questions.%2520Furthermore%252C%2520we%2520introduce%2520an%2520LLM-as-Judge-based%250Aautomatic%2520metric%2520to%2520replace%2520the%2520complex%2520and%2520costly%2520expert%2520evaluation%2520process%252C%250Aensuring%2520reliable%2520and%2520expert-aligned%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCQG-SRefine%3A%20Multiple%20Choice%20Question%20Generation%20and%20Evaluation%20with%0A%20%20Iterative%20Self-Critique%2C%20Correction%2C%20and%20Comparison%20Feedback&entry.906535625=Zonghai%20Yao%20and%20Aditya%20Parashar%20and%20Huixue%20Zhou%20and%20Won%20Seok%20Jang%20and%20Feiyun%20Ouyang%20and%20Zhichao%20Yang%20and%20Hong%20Yu&entry.1292438233=%20%20Automatic%20question%20generation%20%28QG%29%20is%20essential%20for%20AI%20and%20NLP%2C%20particularly%0Ain%20intelligent%20tutoring%2C%20dialogue%20systems%2C%20and%20fact%20verification.%20Generating%0Amultiple-choice%20questions%20%28MCQG%29%20for%20professional%20exams%2C%20like%20the%20United%20States%0AMedical%20Licensing%20Examination%20%28USMLE%29%2C%20is%20particularly%20challenging%2C%20requiring%0Adomain%20expertise%20and%20complex%20multi-hop%20reasoning%20for%20high-quality%20questions.%0AHowever%2C%20current%20large%20language%20models%20%28LLMs%29%20like%20GPT-4%20struggle%20with%0Aprofessional%20MCQG%20due%20to%20outdated%20knowledge%2C%20hallucination%20issues%2C%20and%20prompt%0Asensitivity%2C%20resulting%20in%20unsatisfactory%20quality%20and%20difficulty.%20To%20address%0Athese%20challenges%2C%20we%20propose%20MCQG-SRefine%2C%20an%20LLM%20self-refine-based%20%28Critique%0Aand%20Correction%29%20framework%20for%20converting%20medical%20cases%20into%20high-quality%0AUSMLE-style%20questions.%20By%20integrating%20expert-driven%20prompt%20engineering%20with%0Aiterative%20self-critique%20and%20self-correction%20feedback%2C%20MCQG-SRefine%0Asignificantly%20enhances%20human%20expert%20satisfaction%20regarding%20both%20the%20quality%20and%0Adifficulty%20of%20the%20questions.%20Furthermore%2C%20we%20introduce%20an%20LLM-as-Judge-based%0Aautomatic%20metric%20to%20replace%20the%20complex%20and%20costly%20expert%20evaluation%20process%2C%0Aensuring%20reliable%20and%20expert-aligned%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13191v2&entry.124074799=Read"},
{"title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs", "author": "Hanna Kim and Minkyoo Song and Seung Ho Na and Seungwon Shin and Kimin Lee", "abstract": "  Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents.\n", "link": "http://arxiv.org/abs/2410.14569v1", "date": "2024-10-18", "relevancy": 1.3141, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LLMs%20Go%20Online%3A%20The%20Emerging%20Threat%20of%20Web-Enabled%20LLMs&body=Title%3A%20When%20LLMs%20Go%20Online%3A%20The%20Emerging%20Threat%20of%20Web-Enabled%20LLMs%0AAuthor%3A%20Hanna%20Kim%20and%20Minkyoo%20Song%20and%20Seung%20Ho%20Na%20and%20Seungwon%20Shin%20and%20Kimin%20Lee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20established%20them%20as%0Aagentic%20systems%20capable%20of%20planning%20and%20interacting%20with%20various%20tools.%20These%0ALLM%20agents%20are%20often%20paired%20with%20web-based%20tools%2C%20enabling%20access%20to%20diverse%0Asources%20and%20real-time%20information.%20Although%20these%20advancements%20offer%0Asignificant%20benefits%20across%20various%20applications%2C%20they%20also%20increase%20the%20risk%0Aof%20malicious%20use%2C%20particularly%20in%20cyberattacks%20involving%20personal%20information.%0AIn%20this%20work%2C%20we%20investigate%20the%20risks%20associated%20with%20misuse%20of%20LLM%20agents%20in%0Acyberattacks%20involving%20personal%20data.%20Specifically%2C%20we%20aim%20to%20understand%3A%201%29%0Ahow%20potent%20LLM%20agents%20can%20be%20when%20directed%20to%20conduct%20cyberattacks%2C%202%29%20how%0Acyberattacks%20are%20enhanced%20by%20web-based%20tools%2C%20and%203%29%20how%20affordable%20and%20easy%20it%0Abecomes%20to%20launch%20cyberattacks%20using%20LLM%20agents.%20We%20examine%20three%20attack%0Ascenarios%3A%20the%20collection%20of%20Personally%20Identifiable%20Information%20%28PII%29%2C%20the%0Ageneration%20of%20impersonation%20posts%2C%20and%20the%20creation%20of%20spear-phishing%20emails.%0AOur%20experiments%20reveal%20the%20effectiveness%20of%20LLM%20agents%20in%20these%20attacks%3A%20LLM%0Aagents%20achieved%20a%20precision%20of%20up%20to%2095.9%25%20in%20collecting%20PII%2C%20up%20to%2093.9%25%20of%0Aimpersonation%20posts%20created%20by%20LLM%20agents%20were%20evaluated%20as%20authentic%2C%20and%20the%0Aclick%20rate%20for%20links%20in%20spear%20phishing%20emails%20created%20by%20LLM%20agents%20reached%20up%0Ato%2046.67%25.%20Additionally%2C%20our%20findings%20underscore%20the%20limitations%20of%20existing%0Asafeguards%20in%20contemporary%20commercial%20LLMs%2C%20emphasizing%20the%20urgent%20need%20for%0Amore%20robust%20security%20measures%20to%20prevent%20the%20misuse%20of%20LLM%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LLMs%2520Go%2520Online%253A%2520The%2520Emerging%2520Threat%2520of%2520Web-Enabled%2520LLMs%26entry.906535625%3DHanna%2520Kim%2520and%2520Minkyoo%2520Song%2520and%2520Seung%2520Ho%2520Na%2520and%2520Seungwon%2520Shin%2520and%2520Kimin%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520established%2520them%2520as%250Aagentic%2520systems%2520capable%2520of%2520planning%2520and%2520interacting%2520with%2520various%2520tools.%2520These%250ALLM%2520agents%2520are%2520often%2520paired%2520with%2520web-based%2520tools%252C%2520enabling%2520access%2520to%2520diverse%250Asources%2520and%2520real-time%2520information.%2520Although%2520these%2520advancements%2520offer%250Asignificant%2520benefits%2520across%2520various%2520applications%252C%2520they%2520also%2520increase%2520the%2520risk%250Aof%2520malicious%2520use%252C%2520particularly%2520in%2520cyberattacks%2520involving%2520personal%2520information.%250AIn%2520this%2520work%252C%2520we%2520investigate%2520the%2520risks%2520associated%2520with%2520misuse%2520of%2520LLM%2520agents%2520in%250Acyberattacks%2520involving%2520personal%2520data.%2520Specifically%252C%2520we%2520aim%2520to%2520understand%253A%25201%2529%250Ahow%2520potent%2520LLM%2520agents%2520can%2520be%2520when%2520directed%2520to%2520conduct%2520cyberattacks%252C%25202%2529%2520how%250Acyberattacks%2520are%2520enhanced%2520by%2520web-based%2520tools%252C%2520and%25203%2529%2520how%2520affordable%2520and%2520easy%2520it%250Abecomes%2520to%2520launch%2520cyberattacks%2520using%2520LLM%2520agents.%2520We%2520examine%2520three%2520attack%250Ascenarios%253A%2520the%2520collection%2520of%2520Personally%2520Identifiable%2520Information%2520%2528PII%2529%252C%2520the%250Ageneration%2520of%2520impersonation%2520posts%252C%2520and%2520the%2520creation%2520of%2520spear-phishing%2520emails.%250AOur%2520experiments%2520reveal%2520the%2520effectiveness%2520of%2520LLM%2520agents%2520in%2520these%2520attacks%253A%2520LLM%250Aagents%2520achieved%2520a%2520precision%2520of%2520up%2520to%252095.9%2525%2520in%2520collecting%2520PII%252C%2520up%2520to%252093.9%2525%2520of%250Aimpersonation%2520posts%2520created%2520by%2520LLM%2520agents%2520were%2520evaluated%2520as%2520authentic%252C%2520and%2520the%250Aclick%2520rate%2520for%2520links%2520in%2520spear%2520phishing%2520emails%2520created%2520by%2520LLM%2520agents%2520reached%2520up%250Ato%252046.67%2525.%2520Additionally%252C%2520our%2520findings%2520underscore%2520the%2520limitations%2520of%2520existing%250Asafeguards%2520in%2520contemporary%2520commercial%2520LLMs%252C%2520emphasizing%2520the%2520urgent%2520need%2520for%250Amore%2520robust%2520security%2520measures%2520to%2520prevent%2520the%2520misuse%2520of%2520LLM%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LLMs%20Go%20Online%3A%20The%20Emerging%20Threat%20of%20Web-Enabled%20LLMs&entry.906535625=Hanna%20Kim%20and%20Minkyoo%20Song%20and%20Seung%20Ho%20Na%20and%20Seungwon%20Shin%20and%20Kimin%20Lee&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20established%20them%20as%0Aagentic%20systems%20capable%20of%20planning%20and%20interacting%20with%20various%20tools.%20These%0ALLM%20agents%20are%20often%20paired%20with%20web-based%20tools%2C%20enabling%20access%20to%20diverse%0Asources%20and%20real-time%20information.%20Although%20these%20advancements%20offer%0Asignificant%20benefits%20across%20various%20applications%2C%20they%20also%20increase%20the%20risk%0Aof%20malicious%20use%2C%20particularly%20in%20cyberattacks%20involving%20personal%20information.%0AIn%20this%20work%2C%20we%20investigate%20the%20risks%20associated%20with%20misuse%20of%20LLM%20agents%20in%0Acyberattacks%20involving%20personal%20data.%20Specifically%2C%20we%20aim%20to%20understand%3A%201%29%0Ahow%20potent%20LLM%20agents%20can%20be%20when%20directed%20to%20conduct%20cyberattacks%2C%202%29%20how%0Acyberattacks%20are%20enhanced%20by%20web-based%20tools%2C%20and%203%29%20how%20affordable%20and%20easy%20it%0Abecomes%20to%20launch%20cyberattacks%20using%20LLM%20agents.%20We%20examine%20three%20attack%0Ascenarios%3A%20the%20collection%20of%20Personally%20Identifiable%20Information%20%28PII%29%2C%20the%0Ageneration%20of%20impersonation%20posts%2C%20and%20the%20creation%20of%20spear-phishing%20emails.%0AOur%20experiments%20reveal%20the%20effectiveness%20of%20LLM%20agents%20in%20these%20attacks%3A%20LLM%0Aagents%20achieved%20a%20precision%20of%20up%20to%2095.9%25%20in%20collecting%20PII%2C%20up%20to%2093.9%25%20of%0Aimpersonation%20posts%20created%20by%20LLM%20agents%20were%20evaluated%20as%20authentic%2C%20and%20the%0Aclick%20rate%20for%20links%20in%20spear%20phishing%20emails%20created%20by%20LLM%20agents%20reached%20up%0Ato%2046.67%25.%20Additionally%2C%20our%20findings%20underscore%20the%20limitations%20of%20existing%0Asafeguards%20in%20contemporary%20commercial%20LLMs%2C%20emphasizing%20the%20urgent%20need%20for%0Amore%20robust%20security%20measures%20to%20prevent%20the%20misuse%20of%20LLM%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14569v1&entry.124074799=Read"},
{"title": "Computational Grounding of Responsibility Attribution and Anticipation\n  in LTLf", "author": "Giuseppe De Giacomo and Emiliano Lorini and Timothy Parker and Gianmarco Parretti", "abstract": "  Responsibility is one of the key notions in machine ethics and in the area of\nautonomous systems. It is a multi-faceted notion involving counterfactual\nreasoning about actions and strategies. In this paper, we study different\nvariants of responsibility in a strategic setting based on LTLf. We show a\nconnection with notions in reactive synthesis, including synthesis of winning,\ndominant, and best-effort strategies. This connection provides the building\nblocks for a computational grounding of responsibility including complexity\ncharacterizations and sound, complete, and optimal algorithms for attributing\nand anticipating responsibility.\n", "link": "http://arxiv.org/abs/2410.14544v1", "date": "2024-10-18", "relevancy": 1.2936, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4348}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Grounding%20of%20Responsibility%20Attribution%20and%20Anticipation%0A%20%20in%20LTLf&body=Title%3A%20Computational%20Grounding%20of%20Responsibility%20Attribution%20and%20Anticipation%0A%20%20in%20LTLf%0AAuthor%3A%20Giuseppe%20De%20Giacomo%20and%20Emiliano%20Lorini%20and%20Timothy%20Parker%20and%20Gianmarco%20Parretti%0AAbstract%3A%20%20%20Responsibility%20is%20one%20of%20the%20key%20notions%20in%20machine%20ethics%20and%20in%20the%20area%20of%0Aautonomous%20systems.%20It%20is%20a%20multi-faceted%20notion%20involving%20counterfactual%0Areasoning%20about%20actions%20and%20strategies.%20In%20this%20paper%2C%20we%20study%20different%0Avariants%20of%20responsibility%20in%20a%20strategic%20setting%20based%20on%20LTLf.%20We%20show%20a%0Aconnection%20with%20notions%20in%20reactive%20synthesis%2C%20including%20synthesis%20of%20winning%2C%0Adominant%2C%20and%20best-effort%20strategies.%20This%20connection%20provides%20the%20building%0Ablocks%20for%20a%20computational%20grounding%20of%20responsibility%20including%20complexity%0Acharacterizations%20and%20sound%2C%20complete%2C%20and%20optimal%20algorithms%20for%20attributing%0Aand%20anticipating%20responsibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Grounding%2520of%2520Responsibility%2520Attribution%2520and%2520Anticipation%250A%2520%2520in%2520LTLf%26entry.906535625%3DGiuseppe%2520De%2520Giacomo%2520and%2520Emiliano%2520Lorini%2520and%2520Timothy%2520Parker%2520and%2520Gianmarco%2520Parretti%26entry.1292438233%3D%2520%2520Responsibility%2520is%2520one%2520of%2520the%2520key%2520notions%2520in%2520machine%2520ethics%2520and%2520in%2520the%2520area%2520of%250Aautonomous%2520systems.%2520It%2520is%2520a%2520multi-faceted%2520notion%2520involving%2520counterfactual%250Areasoning%2520about%2520actions%2520and%2520strategies.%2520In%2520this%2520paper%252C%2520we%2520study%2520different%250Avariants%2520of%2520responsibility%2520in%2520a%2520strategic%2520setting%2520based%2520on%2520LTLf.%2520We%2520show%2520a%250Aconnection%2520with%2520notions%2520in%2520reactive%2520synthesis%252C%2520including%2520synthesis%2520of%2520winning%252C%250Adominant%252C%2520and%2520best-effort%2520strategies.%2520This%2520connection%2520provides%2520the%2520building%250Ablocks%2520for%2520a%2520computational%2520grounding%2520of%2520responsibility%2520including%2520complexity%250Acharacterizations%2520and%2520sound%252C%2520complete%252C%2520and%2520optimal%2520algorithms%2520for%2520attributing%250Aand%2520anticipating%2520responsibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Grounding%20of%20Responsibility%20Attribution%20and%20Anticipation%0A%20%20in%20LTLf&entry.906535625=Giuseppe%20De%20Giacomo%20and%20Emiliano%20Lorini%20and%20Timothy%20Parker%20and%20Gianmarco%20Parretti&entry.1292438233=%20%20Responsibility%20is%20one%20of%20the%20key%20notions%20in%20machine%20ethics%20and%20in%20the%20area%20of%0Aautonomous%20systems.%20It%20is%20a%20multi-faceted%20notion%20involving%20counterfactual%0Areasoning%20about%20actions%20and%20strategies.%20In%20this%20paper%2C%20we%20study%20different%0Avariants%20of%20responsibility%20in%20a%20strategic%20setting%20based%20on%20LTLf.%20We%20show%20a%0Aconnection%20with%20notions%20in%20reactive%20synthesis%2C%20including%20synthesis%20of%20winning%2C%0Adominant%2C%20and%20best-effort%20strategies.%20This%20connection%20provides%20the%20building%0Ablocks%20for%20a%20computational%20grounding%20of%20responsibility%20including%20complexity%0Acharacterizations%20and%20sound%2C%20complete%2C%20and%20optimal%20algorithms%20for%20attributing%0Aand%20anticipating%20responsibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14544v1&entry.124074799=Read"},
{"title": "A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning", "author": "Hongming Zhang and Ke Sun and Bo Xu and Linglong Kong and Martin M\u00fcller", "abstract": "  In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications.\n", "link": "http://arxiv.org/abs/2109.09889v3", "date": "2024-10-18", "relevancy": 1.038, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Distance-based%20Anomaly%20Detection%20Framework%20for%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20A%20Distance-based%20Anomaly%20Detection%20Framework%20for%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Hongming%20Zhang%20and%20Ke%20Sun%20and%20Bo%20Xu%20and%20Linglong%20Kong%20and%20Martin%20M%C3%BCller%0AAbstract%3A%20%20%20In%20deep%20reinforcement%20learning%20%28RL%29%20systems%2C%20abnormal%20states%20pose%20significant%0Arisks%20by%20potentially%20triggering%20unpredictable%20behaviors%20and%20unsafe%20actions%2C%0Athus%20impeding%20the%20deployment%20of%20RL%20systems%20in%20real-world%20scenarios.%20It%20is%0Acrucial%20for%20reliable%20decision-making%20systems%20to%20have%20the%20capability%20to%20cast%20an%0Aalert%20whenever%20they%20encounter%20unfamiliar%20observations%20that%20they%20are%20not%0Aequipped%20to%20handle.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Mahalanobis%0Adistance-based%20%28MD%29%20anomaly%20detection%20framework%2C%20called%20%5Ctextit%7BMDX%7D%2C%20for%20deep%0ARL%20algorithms.%20MDX%20simultaneously%20addresses%20random%2C%20adversarial%2C%20and%0Aout-of-distribution%20%28OOD%29%20state%20outliers%20in%20both%20offline%20and%20online%20settings.%0AIt%20utilizes%20Mahalanobis%20distance%20within%20class-conditional%20distributions%20for%0Aeach%20action%20and%20operates%20within%20a%20statistical%20hypothesis%20testing%20framework%0Aunder%20the%20Gaussian%20assumption.%20We%20further%20extend%20it%20to%20robust%20and%0Adistribution-free%20versions%20by%20incorporating%20Robust%20MD%20and%20conformal%20inference%0Atechniques.%20Through%20extensive%20experiments%20on%20classical%20control%20environments%2C%0AAtari%20games%2C%20and%20autonomous%20driving%20scenarios%2C%20we%20demonstrate%20the%20effectiveness%0Aof%20our%20MD-based%20detection%20framework.%20MDX%20offers%20a%20simple%2C%20unified%2C%20and%0Apractical%20anomaly%20detection%20tool%20for%20enhancing%20the%20safety%20and%20reliability%20of%20RL%0Asystems%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.09889v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Distance-based%2520Anomaly%2520Detection%2520Framework%2520for%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DHongming%2520Zhang%2520and%2520Ke%2520Sun%2520and%2520Bo%2520Xu%2520and%2520Linglong%2520Kong%2520and%2520Martin%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520In%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520systems%252C%2520abnormal%2520states%2520pose%2520significant%250Arisks%2520by%2520potentially%2520triggering%2520unpredictable%2520behaviors%2520and%2520unsafe%2520actions%252C%250Athus%2520impeding%2520the%2520deployment%2520of%2520RL%2520systems%2520in%2520real-world%2520scenarios.%2520It%2520is%250Acrucial%2520for%2520reliable%2520decision-making%2520systems%2520to%2520have%2520the%2520capability%2520to%2520cast%2520an%250Aalert%2520whenever%2520they%2520encounter%2520unfamiliar%2520observations%2520that%2520they%2520are%2520not%250Aequipped%2520to%2520handle.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Mahalanobis%250Adistance-based%2520%2528MD%2529%2520anomaly%2520detection%2520framework%252C%2520called%2520%255Ctextit%257BMDX%257D%252C%2520for%2520deep%250ARL%2520algorithms.%2520MDX%2520simultaneously%2520addresses%2520random%252C%2520adversarial%252C%2520and%250Aout-of-distribution%2520%2528OOD%2529%2520state%2520outliers%2520in%2520both%2520offline%2520and%2520online%2520settings.%250AIt%2520utilizes%2520Mahalanobis%2520distance%2520within%2520class-conditional%2520distributions%2520for%250Aeach%2520action%2520and%2520operates%2520within%2520a%2520statistical%2520hypothesis%2520testing%2520framework%250Aunder%2520the%2520Gaussian%2520assumption.%2520We%2520further%2520extend%2520it%2520to%2520robust%2520and%250Adistribution-free%2520versions%2520by%2520incorporating%2520Robust%2520MD%2520and%2520conformal%2520inference%250Atechniques.%2520Through%2520extensive%2520experiments%2520on%2520classical%2520control%2520environments%252C%250AAtari%2520games%252C%2520and%2520autonomous%2520driving%2520scenarios%252C%2520we%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520MD-based%2520detection%2520framework.%2520MDX%2520offers%2520a%2520simple%252C%2520unified%252C%2520and%250Apractical%2520anomaly%2520detection%2520tool%2520for%2520enhancing%2520the%2520safety%2520and%2520reliability%2520of%2520RL%250Asystems%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2109.09889v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Distance-based%20Anomaly%20Detection%20Framework%20for%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Hongming%20Zhang%20and%20Ke%20Sun%20and%20Bo%20Xu%20and%20Linglong%20Kong%20and%20Martin%20M%C3%BCller&entry.1292438233=%20%20In%20deep%20reinforcement%20learning%20%28RL%29%20systems%2C%20abnormal%20states%20pose%20significant%0Arisks%20by%20potentially%20triggering%20unpredictable%20behaviors%20and%20unsafe%20actions%2C%0Athus%20impeding%20the%20deployment%20of%20RL%20systems%20in%20real-world%20scenarios.%20It%20is%0Acrucial%20for%20reliable%20decision-making%20systems%20to%20have%20the%20capability%20to%20cast%20an%0Aalert%20whenever%20they%20encounter%20unfamiliar%20observations%20that%20they%20are%20not%0Aequipped%20to%20handle.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Mahalanobis%0Adistance-based%20%28MD%29%20anomaly%20detection%20framework%2C%20called%20%5Ctextit%7BMDX%7D%2C%20for%20deep%0ARL%20algorithms.%20MDX%20simultaneously%20addresses%20random%2C%20adversarial%2C%20and%0Aout-of-distribution%20%28OOD%29%20state%20outliers%20in%20both%20offline%20and%20online%20settings.%0AIt%20utilizes%20Mahalanobis%20distance%20within%20class-conditional%20distributions%20for%0Aeach%20action%20and%20operates%20within%20a%20statistical%20hypothesis%20testing%20framework%0Aunder%20the%20Gaussian%20assumption.%20We%20further%20extend%20it%20to%20robust%20and%0Adistribution-free%20versions%20by%20incorporating%20Robust%20MD%20and%20conformal%20inference%0Atechniques.%20Through%20extensive%20experiments%20on%20classical%20control%20environments%2C%0AAtari%20games%2C%20and%20autonomous%20driving%20scenarios%2C%20we%20demonstrate%20the%20effectiveness%0Aof%20our%20MD-based%20detection%20framework.%20MDX%20offers%20a%20simple%2C%20unified%2C%20and%0Apractical%20anomaly%20detection%20tool%20for%20enhancing%20the%20safety%20and%20reliability%20of%20RL%0Asystems%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.09889v3&entry.124074799=Read"},
{"title": "Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers\n  and Electronic Health Records", "author": "Chun Yin Kong and Picasso Vasquez and Makan Farhoodimoghadam and Chris Brandt and Titus C. Brown and Krystle L. Reagan and Allison Zwingenberger and Stefan M. Keller", "abstract": "  In the rapidly evolving landscape of veterinary healthcare, integrating\nmachine learning (ML) clinical decision-making tools with electronic health\nrecords (EHRs) promises to improve diagnostic accuracy and patient care.\nHowever, the seamless integration of ML classifiers into existing EHRs in\nveterinary medicine is frequently hindered by the rigidity of EHR systems or\nthe limited availability of IT resources. To address this shortcoming, we\npresent Anna, a freely-available software solution that provides ML classifier\nresults for EHR laboratory data in real-time.\n", "link": "http://arxiv.org/abs/2410.14625v1", "date": "2024-10-18", "relevancy": 1.3055, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.47}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4322}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20AI%20Accessibility%20in%20Veterinary%20Medicine%3A%20Linking%20Classifiers%0A%20%20and%20Electronic%20Health%20Records&body=Title%3A%20Enhancing%20AI%20Accessibility%20in%20Veterinary%20Medicine%3A%20Linking%20Classifiers%0A%20%20and%20Electronic%20Health%20Records%0AAuthor%3A%20Chun%20Yin%20Kong%20and%20Picasso%20Vasquez%20and%20Makan%20Farhoodimoghadam%20and%20Chris%20Brandt%20and%20Titus%20C.%20Brown%20and%20Krystle%20L.%20Reagan%20and%20Allison%20Zwingenberger%20and%20Stefan%20M.%20Keller%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20veterinary%20healthcare%2C%20integrating%0Amachine%20learning%20%28ML%29%20clinical%20decision-making%20tools%20with%20electronic%20health%0Arecords%20%28EHRs%29%20promises%20to%20improve%20diagnostic%20accuracy%20and%20patient%20care.%0AHowever%2C%20the%20seamless%20integration%20of%20ML%20classifiers%20into%20existing%20EHRs%20in%0Aveterinary%20medicine%20is%20frequently%20hindered%20by%20the%20rigidity%20of%20EHR%20systems%20or%0Athe%20limited%20availability%20of%20IT%20resources.%20To%20address%20this%20shortcoming%2C%20we%0Apresent%20Anna%2C%20a%20freely-available%20software%20solution%20that%20provides%20ML%20classifier%0Aresults%20for%20EHR%20laboratory%20data%20in%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520AI%2520Accessibility%2520in%2520Veterinary%2520Medicine%253A%2520Linking%2520Classifiers%250A%2520%2520and%2520Electronic%2520Health%2520Records%26entry.906535625%3DChun%2520Yin%2520Kong%2520and%2520Picasso%2520Vasquez%2520and%2520Makan%2520Farhoodimoghadam%2520and%2520Chris%2520Brandt%2520and%2520Titus%2520C.%2520Brown%2520and%2520Krystle%2520L.%2520Reagan%2520and%2520Allison%2520Zwingenberger%2520and%2520Stefan%2520M.%2520Keller%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520veterinary%2520healthcare%252C%2520integrating%250Amachine%2520learning%2520%2528ML%2529%2520clinical%2520decision-making%2520tools%2520with%2520electronic%2520health%250Arecords%2520%2528EHRs%2529%2520promises%2520to%2520improve%2520diagnostic%2520accuracy%2520and%2520patient%2520care.%250AHowever%252C%2520the%2520seamless%2520integration%2520of%2520ML%2520classifiers%2520into%2520existing%2520EHRs%2520in%250Aveterinary%2520medicine%2520is%2520frequently%2520hindered%2520by%2520the%2520rigidity%2520of%2520EHR%2520systems%2520or%250Athe%2520limited%2520availability%2520of%2520IT%2520resources.%2520To%2520address%2520this%2520shortcoming%252C%2520we%250Apresent%2520Anna%252C%2520a%2520freely-available%2520software%2520solution%2520that%2520provides%2520ML%2520classifier%250Aresults%2520for%2520EHR%2520laboratory%2520data%2520in%2520real-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20AI%20Accessibility%20in%20Veterinary%20Medicine%3A%20Linking%20Classifiers%0A%20%20and%20Electronic%20Health%20Records&entry.906535625=Chun%20Yin%20Kong%20and%20Picasso%20Vasquez%20and%20Makan%20Farhoodimoghadam%20and%20Chris%20Brandt%20and%20Titus%20C.%20Brown%20and%20Krystle%20L.%20Reagan%20and%20Allison%20Zwingenberger%20and%20Stefan%20M.%20Keller&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20veterinary%20healthcare%2C%20integrating%0Amachine%20learning%20%28ML%29%20clinical%20decision-making%20tools%20with%20electronic%20health%0Arecords%20%28EHRs%29%20promises%20to%20improve%20diagnostic%20accuracy%20and%20patient%20care.%0AHowever%2C%20the%20seamless%20integration%20of%20ML%20classifiers%20into%20existing%20EHRs%20in%0Aveterinary%20medicine%20is%20frequently%20hindered%20by%20the%20rigidity%20of%20EHR%20systems%20or%0Athe%20limited%20availability%20of%20IT%20resources.%20To%20address%20this%20shortcoming%2C%20we%0Apresent%20Anna%2C%20a%20freely-available%20software%20solution%20that%20provides%20ML%20classifier%0Aresults%20for%20EHR%20laboratory%20data%20in%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14625v1&entry.124074799=Read"},
{"title": "Using Sentiment and Technical Analysis to Predict Bitcoin with Machine\n  Learning", "author": "Arthur Emanuel de Oliveira Carosia", "abstract": "  Cryptocurrencies have gained significant attention in recent years due to\ntheir decentralized nature and potential for financial innovation. Thus, the\nability to accurately predict its price has become a subject of great interest\nfor investors, traders, and researchers. Some works in the literature show how\nBitcoin's market sentiment correlates with its price fluctuations in the\nmarket. However, papers that consider the sentiment of the market associated\nwith financial Technical Analysis indicators in order to predict Bitcoin's\nprice are still scarce. In this paper, we present a novel approach for\npredicting Bitcoin price movements by combining the Fear & Greedy Index, a\nmeasure of market sentiment, Technical Analysis indicators, and the potential\nof Machine Learning algorithms. This work represents a preliminary study on the\nimportance of sentiment metrics in cryptocurrency forecasting. Our initial\nexperiments demonstrate promising results considering investment returns,\nsurpassing the Buy & Hold baseline, and offering valuable insights about the\ncombination of indicators of sentiment and market in a cryptocurrency\nprediction model.\n", "link": "http://arxiv.org/abs/2410.14532v1", "date": "2024-10-18", "relevancy": 1.2194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3919}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Sentiment%20and%20Technical%20Analysis%20to%20Predict%20Bitcoin%20with%20Machine%0A%20%20Learning&body=Title%3A%20Using%20Sentiment%20and%20Technical%20Analysis%20to%20Predict%20Bitcoin%20with%20Machine%0A%20%20Learning%0AAuthor%3A%20Arthur%20Emanuel%20de%20Oliveira%20Carosia%0AAbstract%3A%20%20%20Cryptocurrencies%20have%20gained%20significant%20attention%20in%20recent%20years%20due%20to%0Atheir%20decentralized%20nature%20and%20potential%20for%20financial%20innovation.%20Thus%2C%20the%0Aability%20to%20accurately%20predict%20its%20price%20has%20become%20a%20subject%20of%20great%20interest%0Afor%20investors%2C%20traders%2C%20and%20researchers.%20Some%20works%20in%20the%20literature%20show%20how%0ABitcoin%27s%20market%20sentiment%20correlates%20with%20its%20price%20fluctuations%20in%20the%0Amarket.%20However%2C%20papers%20that%20consider%20the%20sentiment%20of%20the%20market%20associated%0Awith%20financial%20Technical%20Analysis%20indicators%20in%20order%20to%20predict%20Bitcoin%27s%0Aprice%20are%20still%20scarce.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%0Apredicting%20Bitcoin%20price%20movements%20by%20combining%20the%20Fear%20%26%20Greedy%20Index%2C%20a%0Ameasure%20of%20market%20sentiment%2C%20Technical%20Analysis%20indicators%2C%20and%20the%20potential%0Aof%20Machine%20Learning%20algorithms.%20This%20work%20represents%20a%20preliminary%20study%20on%20the%0Aimportance%20of%20sentiment%20metrics%20in%20cryptocurrency%20forecasting.%20Our%20initial%0Aexperiments%20demonstrate%20promising%20results%20considering%20investment%20returns%2C%0Asurpassing%20the%20Buy%20%26%20Hold%20baseline%2C%20and%20offering%20valuable%20insights%20about%20the%0Acombination%20of%20indicators%20of%20sentiment%20and%20market%20in%20a%20cryptocurrency%0Aprediction%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Sentiment%2520and%2520Technical%2520Analysis%2520to%2520Predict%2520Bitcoin%2520with%2520Machine%250A%2520%2520Learning%26entry.906535625%3DArthur%2520Emanuel%2520de%2520Oliveira%2520Carosia%26entry.1292438233%3D%2520%2520Cryptocurrencies%2520have%2520gained%2520significant%2520attention%2520in%2520recent%2520years%2520due%2520to%250Atheir%2520decentralized%2520nature%2520and%2520potential%2520for%2520financial%2520innovation.%2520Thus%252C%2520the%250Aability%2520to%2520accurately%2520predict%2520its%2520price%2520has%2520become%2520a%2520subject%2520of%2520great%2520interest%250Afor%2520investors%252C%2520traders%252C%2520and%2520researchers.%2520Some%2520works%2520in%2520the%2520literature%2520show%2520how%250ABitcoin%2527s%2520market%2520sentiment%2520correlates%2520with%2520its%2520price%2520fluctuations%2520in%2520the%250Amarket.%2520However%252C%2520papers%2520that%2520consider%2520the%2520sentiment%2520of%2520the%2520market%2520associated%250Awith%2520financial%2520Technical%2520Analysis%2520indicators%2520in%2520order%2520to%2520predict%2520Bitcoin%2527s%250Aprice%2520are%2520still%2520scarce.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%250Apredicting%2520Bitcoin%2520price%2520movements%2520by%2520combining%2520the%2520Fear%2520%2526%2520Greedy%2520Index%252C%2520a%250Ameasure%2520of%2520market%2520sentiment%252C%2520Technical%2520Analysis%2520indicators%252C%2520and%2520the%2520potential%250Aof%2520Machine%2520Learning%2520algorithms.%2520This%2520work%2520represents%2520a%2520preliminary%2520study%2520on%2520the%250Aimportance%2520of%2520sentiment%2520metrics%2520in%2520cryptocurrency%2520forecasting.%2520Our%2520initial%250Aexperiments%2520demonstrate%2520promising%2520results%2520considering%2520investment%2520returns%252C%250Asurpassing%2520the%2520Buy%2520%2526%2520Hold%2520baseline%252C%2520and%2520offering%2520valuable%2520insights%2520about%2520the%250Acombination%2520of%2520indicators%2520of%2520sentiment%2520and%2520market%2520in%2520a%2520cryptocurrency%250Aprediction%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Sentiment%20and%20Technical%20Analysis%20to%20Predict%20Bitcoin%20with%20Machine%0A%20%20Learning&entry.906535625=Arthur%20Emanuel%20de%20Oliveira%20Carosia&entry.1292438233=%20%20Cryptocurrencies%20have%20gained%20significant%20attention%20in%20recent%20years%20due%20to%0Atheir%20decentralized%20nature%20and%20potential%20for%20financial%20innovation.%20Thus%2C%20the%0Aability%20to%20accurately%20predict%20its%20price%20has%20become%20a%20subject%20of%20great%20interest%0Afor%20investors%2C%20traders%2C%20and%20researchers.%20Some%20works%20in%20the%20literature%20show%20how%0ABitcoin%27s%20market%20sentiment%20correlates%20with%20its%20price%20fluctuations%20in%20the%0Amarket.%20However%2C%20papers%20that%20consider%20the%20sentiment%20of%20the%20market%20associated%0Awith%20financial%20Technical%20Analysis%20indicators%20in%20order%20to%20predict%20Bitcoin%27s%0Aprice%20are%20still%20scarce.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20for%0Apredicting%20Bitcoin%20price%20movements%20by%20combining%20the%20Fear%20%26%20Greedy%20Index%2C%20a%0Ameasure%20of%20market%20sentiment%2C%20Technical%20Analysis%20indicators%2C%20and%20the%20potential%0Aof%20Machine%20Learning%20algorithms.%20This%20work%20represents%20a%20preliminary%20study%20on%20the%0Aimportance%20of%20sentiment%20metrics%20in%20cryptocurrency%20forecasting.%20Our%20initial%0Aexperiments%20demonstrate%20promising%20results%20considering%20investment%20returns%2C%0Asurpassing%20the%20Buy%20%26%20Hold%20baseline%2C%20and%20offering%20valuable%20insights%20about%20the%0Acombination%20of%20indicators%20of%20sentiment%20and%20market%20in%20a%20cryptocurrency%0Aprediction%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14532v1&entry.124074799=Read"},
{"title": "Contractivity and linear convergence in bilinear saddle-point problems:\n  An operator-theoretic approach", "author": "Colin Dirren and Mattia Bianchi and Panagiotis D. Grontas and John Lygeros and Florian D\u00f6rfler", "abstract": "  We study the convex-concave bilinear saddle-point problem $\\min_x \\max_y f(x)\n+ y^\\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$\nare strongly convex, and suitable rank conditions on the matrix $A$ hold. The\nsolution of this problem is at the core of many machine learning tasks. By\nemploying tools from operator theory, we systematically prove the contractivity\n(in turn, the linear convergence) of several first-order primal-dual\nalgorithms, including the Chambolle-Pock method. Our approach results in\nconcise and elegant proofs, and it yields new convergence guarantees and\ntighter bounds compared to known results.\n", "link": "http://arxiv.org/abs/2410.14592v1", "date": "2024-10-18", "relevancy": 1.2835, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4412}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4296}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contractivity%20and%20linear%20convergence%20in%20bilinear%20saddle-point%20problems%3A%0A%20%20An%20operator-theoretic%20approach&body=Title%3A%20Contractivity%20and%20linear%20convergence%20in%20bilinear%20saddle-point%20problems%3A%0A%20%20An%20operator-theoretic%20approach%0AAuthor%3A%20Colin%20Dirren%20and%20Mattia%20Bianchi%20and%20Panagiotis%20D.%20Grontas%20and%20John%20Lygeros%20and%20Florian%20D%C3%B6rfler%0AAbstract%3A%20%20%20We%20study%20the%20convex-concave%20bilinear%20saddle-point%20problem%20%24%5Cmin_x%20%5Cmax_y%20f%28x%29%0A%2B%20y%5E%5Ctop%20Ax%20-%20g%28y%29%24%2C%20where%20both%2C%20only%20one%2C%20or%20none%20of%20the%20functions%20%24f%24%20and%20%24g%24%0Aare%20strongly%20convex%2C%20and%20suitable%20rank%20conditions%20on%20the%20matrix%20%24A%24%20hold.%20The%0Asolution%20of%20this%20problem%20is%20at%20the%20core%20of%20many%20machine%20learning%20tasks.%20By%0Aemploying%20tools%20from%20operator%20theory%2C%20we%20systematically%20prove%20the%20contractivity%0A%28in%20turn%2C%20the%20linear%20convergence%29%20of%20several%20first-order%20primal-dual%0Aalgorithms%2C%20including%20the%20Chambolle-Pock%20method.%20Our%20approach%20results%20in%0Aconcise%20and%20elegant%20proofs%2C%20and%20it%20yields%20new%20convergence%20guarantees%20and%0Atighter%20bounds%20compared%20to%20known%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContractivity%2520and%2520linear%2520convergence%2520in%2520bilinear%2520saddle-point%2520problems%253A%250A%2520%2520An%2520operator-theoretic%2520approach%26entry.906535625%3DColin%2520Dirren%2520and%2520Mattia%2520Bianchi%2520and%2520Panagiotis%2520D.%2520Grontas%2520and%2520John%2520Lygeros%2520and%2520Florian%2520D%25C3%25B6rfler%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520convex-concave%2520bilinear%2520saddle-point%2520problem%2520%2524%255Cmin_x%2520%255Cmax_y%2520f%2528x%2529%250A%252B%2520y%255E%255Ctop%2520Ax%2520-%2520g%2528y%2529%2524%252C%2520where%2520both%252C%2520only%2520one%252C%2520or%2520none%2520of%2520the%2520functions%2520%2524f%2524%2520and%2520%2524g%2524%250Aare%2520strongly%2520convex%252C%2520and%2520suitable%2520rank%2520conditions%2520on%2520the%2520matrix%2520%2524A%2524%2520hold.%2520The%250Asolution%2520of%2520this%2520problem%2520is%2520at%2520the%2520core%2520of%2520many%2520machine%2520learning%2520tasks.%2520By%250Aemploying%2520tools%2520from%2520operator%2520theory%252C%2520we%2520systematically%2520prove%2520the%2520contractivity%250A%2528in%2520turn%252C%2520the%2520linear%2520convergence%2529%2520of%2520several%2520first-order%2520primal-dual%250Aalgorithms%252C%2520including%2520the%2520Chambolle-Pock%2520method.%2520Our%2520approach%2520results%2520in%250Aconcise%2520and%2520elegant%2520proofs%252C%2520and%2520it%2520yields%2520new%2520convergence%2520guarantees%2520and%250Atighter%2520bounds%2520compared%2520to%2520known%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contractivity%20and%20linear%20convergence%20in%20bilinear%20saddle-point%20problems%3A%0A%20%20An%20operator-theoretic%20approach&entry.906535625=Colin%20Dirren%20and%20Mattia%20Bianchi%20and%20Panagiotis%20D.%20Grontas%20and%20John%20Lygeros%20and%20Florian%20D%C3%B6rfler&entry.1292438233=%20%20We%20study%20the%20convex-concave%20bilinear%20saddle-point%20problem%20%24%5Cmin_x%20%5Cmax_y%20f%28x%29%0A%2B%20y%5E%5Ctop%20Ax%20-%20g%28y%29%24%2C%20where%20both%2C%20only%20one%2C%20or%20none%20of%20the%20functions%20%24f%24%20and%20%24g%24%0Aare%20strongly%20convex%2C%20and%20suitable%20rank%20conditions%20on%20the%20matrix%20%24A%24%20hold.%20The%0Asolution%20of%20this%20problem%20is%20at%20the%20core%20of%20many%20machine%20learning%20tasks.%20By%0Aemploying%20tools%20from%20operator%20theory%2C%20we%20systematically%20prove%20the%20contractivity%0A%28in%20turn%2C%20the%20linear%20convergence%29%20of%20several%20first-order%20primal-dual%0Aalgorithms%2C%20including%20the%20Chambolle-Pock%20method.%20Our%20approach%20results%20in%0Aconcise%20and%20elegant%20proofs%2C%20and%20it%20yields%20new%20convergence%20guarantees%20and%0Atighter%20bounds%20compared%20to%20known%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14592v1&entry.124074799=Read"},
{"title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing", "author": "Zhuoran Zhang and Yongxiang Li and Zijian Kan and Keyuan Cheng and Lijie Hu and Di Wang", "abstract": "  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n", "link": "http://arxiv.org/abs/2410.06331v2", "date": "2024-10-18", "relevancy": 0.9304, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4661}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locate-then-edit%20for%20Multi-hop%20Factual%20Recall%20under%20Knowledge%20Editing&body=Title%3A%20Locate-then-edit%20for%20Multi-hop%20Factual%20Recall%20under%20Knowledge%20Editing%0AAuthor%3A%20Zhuoran%20Zhang%20and%20Yongxiang%20Li%20and%20Zijian%20Kan%20and%20Keyuan%20Cheng%20and%20Lijie%20Hu%20and%20Di%20Wang%0AAbstract%3A%20%20%20The%20locate-then-edit%20paradigm%20has%20shown%20significant%20promise%20for%20knowledge%0Aediting%20%28KE%29%20in%20Large%20Language%20Models%20%28LLMs%29.%20While%20previous%20methods%20perform%0Awell%20on%20single-hop%20fact%20recall%20tasks%2C%20they%20consistently%20struggle%20with%20multi-hop%0Afactual%20recall%20tasks%20involving%20newly%20edited%20knowledge.%20In%20this%20paper%2C%0Aleveraging%20tools%20in%20mechanistic%20interpretability%2C%20we%20first%20identify%20that%20in%0Amulti-hop%20tasks%2C%20LLMs%20tend%20to%20retrieve%20implicit%20subject%20knowledge%20from%20deeper%0AMLP%20layers%2C%20unlike%20single-hop%20tasks%2C%20which%20rely%20on%20earlier%20layers.%20This%0Adistinction%20explains%20the%20poor%20performance%20of%20current%20methods%20in%20multi-hop%0Aqueries%2C%20as%20they%20primarily%20focus%20on%20editing%20shallow%20layers%2C%20leaving%20deeper%0Alayers%20unchanged.%20To%20address%20this%2C%20we%20propose%20IFMET%2C%20a%20novel%20locate-then-edit%0AKE%20approach%20designed%20to%20edit%20both%20shallow%20and%20deep%20MLP%20layers.%20IFMET%20employs%0Amulti-hop%20editing%20prompts%20and%20supplementary%20sets%20to%20locate%20and%20modify%20knowledge%0Aacross%20different%20reasoning%20stages.%20Experimental%20results%20demonstrate%20that%20IFMET%0Asignificantly%20improves%20performance%20on%20multi-hop%20factual%20recall%20tasks%2C%0Aeffectively%20overcoming%20the%20limitations%20of%20previous%20locate-then-edit%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocate-then-edit%2520for%2520Multi-hop%2520Factual%2520Recall%2520under%2520Knowledge%2520Editing%26entry.906535625%3DZhuoran%2520Zhang%2520and%2520Yongxiang%2520Li%2520and%2520Zijian%2520Kan%2520and%2520Keyuan%2520Cheng%2520and%2520Lijie%2520Hu%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520The%2520locate-then-edit%2520paradigm%2520has%2520shown%2520significant%2520promise%2520for%2520knowledge%250Aediting%2520%2528KE%2529%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520While%2520previous%2520methods%2520perform%250Awell%2520on%2520single-hop%2520fact%2520recall%2520tasks%252C%2520they%2520consistently%2520struggle%2520with%2520multi-hop%250Afactual%2520recall%2520tasks%2520involving%2520newly%2520edited%2520knowledge.%2520In%2520this%2520paper%252C%250Aleveraging%2520tools%2520in%2520mechanistic%2520interpretability%252C%2520we%2520first%2520identify%2520that%2520in%250Amulti-hop%2520tasks%252C%2520LLMs%2520tend%2520to%2520retrieve%2520implicit%2520subject%2520knowledge%2520from%2520deeper%250AMLP%2520layers%252C%2520unlike%2520single-hop%2520tasks%252C%2520which%2520rely%2520on%2520earlier%2520layers.%2520This%250Adistinction%2520explains%2520the%2520poor%2520performance%2520of%2520current%2520methods%2520in%2520multi-hop%250Aqueries%252C%2520as%2520they%2520primarily%2520focus%2520on%2520editing%2520shallow%2520layers%252C%2520leaving%2520deeper%250Alayers%2520unchanged.%2520To%2520address%2520this%252C%2520we%2520propose%2520IFMET%252C%2520a%2520novel%2520locate-then-edit%250AKE%2520approach%2520designed%2520to%2520edit%2520both%2520shallow%2520and%2520deep%2520MLP%2520layers.%2520IFMET%2520employs%250Amulti-hop%2520editing%2520prompts%2520and%2520supplementary%2520sets%2520to%2520locate%2520and%2520modify%2520knowledge%250Aacross%2520different%2520reasoning%2520stages.%2520Experimental%2520results%2520demonstrate%2520that%2520IFMET%250Asignificantly%2520improves%2520performance%2520on%2520multi-hop%2520factual%2520recall%2520tasks%252C%250Aeffectively%2520overcoming%2520the%2520limitations%2520of%2520previous%2520locate-then-edit%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate-then-edit%20for%20Multi-hop%20Factual%20Recall%20under%20Knowledge%20Editing&entry.906535625=Zhuoran%20Zhang%20and%20Yongxiang%20Li%20and%20Zijian%20Kan%20and%20Keyuan%20Cheng%20and%20Lijie%20Hu%20and%20Di%20Wang&entry.1292438233=%20%20The%20locate-then-edit%20paradigm%20has%20shown%20significant%20promise%20for%20knowledge%0Aediting%20%28KE%29%20in%20Large%20Language%20Models%20%28LLMs%29.%20While%20previous%20methods%20perform%0Awell%20on%20single-hop%20fact%20recall%20tasks%2C%20they%20consistently%20struggle%20with%20multi-hop%0Afactual%20recall%20tasks%20involving%20newly%20edited%20knowledge.%20In%20this%20paper%2C%0Aleveraging%20tools%20in%20mechanistic%20interpretability%2C%20we%20first%20identify%20that%20in%0Amulti-hop%20tasks%2C%20LLMs%20tend%20to%20retrieve%20implicit%20subject%20knowledge%20from%20deeper%0AMLP%20layers%2C%20unlike%20single-hop%20tasks%2C%20which%20rely%20on%20earlier%20layers.%20This%0Adistinction%20explains%20the%20poor%20performance%20of%20current%20methods%20in%20multi-hop%0Aqueries%2C%20as%20they%20primarily%20focus%20on%20editing%20shallow%20layers%2C%20leaving%20deeper%0Alayers%20unchanged.%20To%20address%20this%2C%20we%20propose%20IFMET%2C%20a%20novel%20locate-then-edit%0AKE%20approach%20designed%20to%20edit%20both%20shallow%20and%20deep%20MLP%20layers.%20IFMET%20employs%0Amulti-hop%20editing%20prompts%20and%20supplementary%20sets%20to%20locate%20and%20modify%20knowledge%0Aacross%20different%20reasoning%20stages.%20Experimental%20results%20demonstrate%20that%20IFMET%0Asignificantly%20improves%20performance%20on%20multi-hop%20factual%20recall%20tasks%2C%0Aeffectively%20overcoming%20the%20limitations%20of%20previous%20locate-then-edit%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06331v2&entry.124074799=Read"},
{"title": "syren-new: Precise formulae for the linear and nonlinear matter power\n  spectra with massive neutrinos and dynamical dark energy", "author": "Ce Sui and Deaglan J. Bartlett and Shivam Pandey and Harry Desmond and Pedro G. Ferreira and Benjamin D. Wandelt", "abstract": "  Current and future large scale structure surveys aim to constrain the\nneutrino mass and the equation of state of dark energy. We aim to construct\naccurate and interpretable symbolic approximations to the linear and nonlinear\nmatter power spectra as a function of cosmological parameters in extended\n$\\Lambda$CDM models which contain massive neutrinos and non-constant equations\nof state for dark energy. This constitutes an extension of the syren-halofit\nemulators to incorporate these two effects, which we call syren-new\n(SYmbolic-Regression-ENhanced power spectrum emulator with NEutrinos and\n$W_0-w_a$). We also obtain a simple approximation to the derived parameter\n$\\sigma_8$ as a function of the cosmological parameters for these models. Our\nresults for the linear power spectrum are designed to emulate CLASS, whereas\nfor the nonlinear case we aim to match the results of EuclidEmulator2. We\ncompare our results to existing emulators and $N$-body simulations. Our\nanalytic emulators for $\\sigma_8$, the linear and nonlinear power spectra\nachieve root mean squared errors of 0.1%, 0.3% and 1.3%, respectively, across a\nwide range of cosmological parameters, redshifts and wavenumbers. We verify\nthat emulator-related discrepancies are subdominant compared to observational\nerrors and other modelling uncertainties when computing shear power spectra for\nLSST-like surveys. Our expressions have similar accuracy to existing\n(numerical) emulators, but are at least an order of magnitude faster, both on a\nCPU and GPU. Our work greatly improves the accuracy, speed and range of\napplicability of current symbolic approximations to the linear and nonlinear\nmatter power spectra. We provide publicly available code for all symbolic\napproximations found.\n", "link": "http://arxiv.org/abs/2410.14623v1", "date": "2024-10-18", "relevancy": 1.1889, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4112}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20syren-new%3A%20Precise%20formulae%20for%20the%20linear%20and%20nonlinear%20matter%20power%0A%20%20spectra%20with%20massive%20neutrinos%20and%20dynamical%20dark%20energy&body=Title%3A%20syren-new%3A%20Precise%20formulae%20for%20the%20linear%20and%20nonlinear%20matter%20power%0A%20%20spectra%20with%20massive%20neutrinos%20and%20dynamical%20dark%20energy%0AAuthor%3A%20Ce%20Sui%20and%20Deaglan%20J.%20Bartlett%20and%20Shivam%20Pandey%20and%20Harry%20Desmond%20and%20Pedro%20G.%20Ferreira%20and%20Benjamin%20D.%20Wandelt%0AAbstract%3A%20%20%20Current%20and%20future%20large%20scale%20structure%20surveys%20aim%20to%20constrain%20the%0Aneutrino%20mass%20and%20the%20equation%20of%20state%20of%20dark%20energy.%20We%20aim%20to%20construct%0Aaccurate%20and%20interpretable%20symbolic%20approximations%20to%20the%20linear%20and%20nonlinear%0Amatter%20power%20spectra%20as%20a%20function%20of%20cosmological%20parameters%20in%20extended%0A%24%5CLambda%24CDM%20models%20which%20contain%20massive%20neutrinos%20and%20non-constant%20equations%0Aof%20state%20for%20dark%20energy.%20This%20constitutes%20an%20extension%20of%20the%20syren-halofit%0Aemulators%20to%20incorporate%20these%20two%20effects%2C%20which%20we%20call%20syren-new%0A%28SYmbolic-Regression-ENhanced%20power%20spectrum%20emulator%20with%20NEutrinos%20and%0A%24W_0-w_a%24%29.%20We%20also%20obtain%20a%20simple%20approximation%20to%20the%20derived%20parameter%0A%24%5Csigma_8%24%20as%20a%20function%20of%20the%20cosmological%20parameters%20for%20these%20models.%20Our%0Aresults%20for%20the%20linear%20power%20spectrum%20are%20designed%20to%20emulate%20CLASS%2C%20whereas%0Afor%20the%20nonlinear%20case%20we%20aim%20to%20match%20the%20results%20of%20EuclidEmulator2.%20We%0Acompare%20our%20results%20to%20existing%20emulators%20and%20%24N%24-body%20simulations.%20Our%0Aanalytic%20emulators%20for%20%24%5Csigma_8%24%2C%20the%20linear%20and%20nonlinear%20power%20spectra%0Aachieve%20root%20mean%20squared%20errors%20of%200.1%25%2C%200.3%25%20and%201.3%25%2C%20respectively%2C%20across%20a%0Awide%20range%20of%20cosmological%20parameters%2C%20redshifts%20and%20wavenumbers.%20We%20verify%0Athat%20emulator-related%20discrepancies%20are%20subdominant%20compared%20to%20observational%0Aerrors%20and%20other%20modelling%20uncertainties%20when%20computing%20shear%20power%20spectra%20for%0ALSST-like%20surveys.%20Our%20expressions%20have%20similar%20accuracy%20to%20existing%0A%28numerical%29%20emulators%2C%20but%20are%20at%20least%20an%20order%20of%20magnitude%20faster%2C%20both%20on%20a%0ACPU%20and%20GPU.%20Our%20work%20greatly%20improves%20the%20accuracy%2C%20speed%20and%20range%20of%0Aapplicability%20of%20current%20symbolic%20approximations%20to%20the%20linear%20and%20nonlinear%0Amatter%20power%20spectra.%20We%20provide%20publicly%20available%20code%20for%20all%20symbolic%0Aapproximations%20found.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsyren-new%253A%2520Precise%2520formulae%2520for%2520the%2520linear%2520and%2520nonlinear%2520matter%2520power%250A%2520%2520spectra%2520with%2520massive%2520neutrinos%2520and%2520dynamical%2520dark%2520energy%26entry.906535625%3DCe%2520Sui%2520and%2520Deaglan%2520J.%2520Bartlett%2520and%2520Shivam%2520Pandey%2520and%2520Harry%2520Desmond%2520and%2520Pedro%2520G.%2520Ferreira%2520and%2520Benjamin%2520D.%2520Wandelt%26entry.1292438233%3D%2520%2520Current%2520and%2520future%2520large%2520scale%2520structure%2520surveys%2520aim%2520to%2520constrain%2520the%250Aneutrino%2520mass%2520and%2520the%2520equation%2520of%2520state%2520of%2520dark%2520energy.%2520We%2520aim%2520to%2520construct%250Aaccurate%2520and%2520interpretable%2520symbolic%2520approximations%2520to%2520the%2520linear%2520and%2520nonlinear%250Amatter%2520power%2520spectra%2520as%2520a%2520function%2520of%2520cosmological%2520parameters%2520in%2520extended%250A%2524%255CLambda%2524CDM%2520models%2520which%2520contain%2520massive%2520neutrinos%2520and%2520non-constant%2520equations%250Aof%2520state%2520for%2520dark%2520energy.%2520This%2520constitutes%2520an%2520extension%2520of%2520the%2520syren-halofit%250Aemulators%2520to%2520incorporate%2520these%2520two%2520effects%252C%2520which%2520we%2520call%2520syren-new%250A%2528SYmbolic-Regression-ENhanced%2520power%2520spectrum%2520emulator%2520with%2520NEutrinos%2520and%250A%2524W_0-w_a%2524%2529.%2520We%2520also%2520obtain%2520a%2520simple%2520approximation%2520to%2520the%2520derived%2520parameter%250A%2524%255Csigma_8%2524%2520as%2520a%2520function%2520of%2520the%2520cosmological%2520parameters%2520for%2520these%2520models.%2520Our%250Aresults%2520for%2520the%2520linear%2520power%2520spectrum%2520are%2520designed%2520to%2520emulate%2520CLASS%252C%2520whereas%250Afor%2520the%2520nonlinear%2520case%2520we%2520aim%2520to%2520match%2520the%2520results%2520of%2520EuclidEmulator2.%2520We%250Acompare%2520our%2520results%2520to%2520existing%2520emulators%2520and%2520%2524N%2524-body%2520simulations.%2520Our%250Aanalytic%2520emulators%2520for%2520%2524%255Csigma_8%2524%252C%2520the%2520linear%2520and%2520nonlinear%2520power%2520spectra%250Aachieve%2520root%2520mean%2520squared%2520errors%2520of%25200.1%2525%252C%25200.3%2525%2520and%25201.3%2525%252C%2520respectively%252C%2520across%2520a%250Awide%2520range%2520of%2520cosmological%2520parameters%252C%2520redshifts%2520and%2520wavenumbers.%2520We%2520verify%250Athat%2520emulator-related%2520discrepancies%2520are%2520subdominant%2520compared%2520to%2520observational%250Aerrors%2520and%2520other%2520modelling%2520uncertainties%2520when%2520computing%2520shear%2520power%2520spectra%2520for%250ALSST-like%2520surveys.%2520Our%2520expressions%2520have%2520similar%2520accuracy%2520to%2520existing%250A%2528numerical%2529%2520emulators%252C%2520but%2520are%2520at%2520least%2520an%2520order%2520of%2520magnitude%2520faster%252C%2520both%2520on%2520a%250ACPU%2520and%2520GPU.%2520Our%2520work%2520greatly%2520improves%2520the%2520accuracy%252C%2520speed%2520and%2520range%2520of%250Aapplicability%2520of%2520current%2520symbolic%2520approximations%2520to%2520the%2520linear%2520and%2520nonlinear%250Amatter%2520power%2520spectra.%2520We%2520provide%2520publicly%2520available%2520code%2520for%2520all%2520symbolic%250Aapproximations%2520found.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=syren-new%3A%20Precise%20formulae%20for%20the%20linear%20and%20nonlinear%20matter%20power%0A%20%20spectra%20with%20massive%20neutrinos%20and%20dynamical%20dark%20energy&entry.906535625=Ce%20Sui%20and%20Deaglan%20J.%20Bartlett%20and%20Shivam%20Pandey%20and%20Harry%20Desmond%20and%20Pedro%20G.%20Ferreira%20and%20Benjamin%20D.%20Wandelt&entry.1292438233=%20%20Current%20and%20future%20large%20scale%20structure%20surveys%20aim%20to%20constrain%20the%0Aneutrino%20mass%20and%20the%20equation%20of%20state%20of%20dark%20energy.%20We%20aim%20to%20construct%0Aaccurate%20and%20interpretable%20symbolic%20approximations%20to%20the%20linear%20and%20nonlinear%0Amatter%20power%20spectra%20as%20a%20function%20of%20cosmological%20parameters%20in%20extended%0A%24%5CLambda%24CDM%20models%20which%20contain%20massive%20neutrinos%20and%20non-constant%20equations%0Aof%20state%20for%20dark%20energy.%20This%20constitutes%20an%20extension%20of%20the%20syren-halofit%0Aemulators%20to%20incorporate%20these%20two%20effects%2C%20which%20we%20call%20syren-new%0A%28SYmbolic-Regression-ENhanced%20power%20spectrum%20emulator%20with%20NEutrinos%20and%0A%24W_0-w_a%24%29.%20We%20also%20obtain%20a%20simple%20approximation%20to%20the%20derived%20parameter%0A%24%5Csigma_8%24%20as%20a%20function%20of%20the%20cosmological%20parameters%20for%20these%20models.%20Our%0Aresults%20for%20the%20linear%20power%20spectrum%20are%20designed%20to%20emulate%20CLASS%2C%20whereas%0Afor%20the%20nonlinear%20case%20we%20aim%20to%20match%20the%20results%20of%20EuclidEmulator2.%20We%0Acompare%20our%20results%20to%20existing%20emulators%20and%20%24N%24-body%20simulations.%20Our%0Aanalytic%20emulators%20for%20%24%5Csigma_8%24%2C%20the%20linear%20and%20nonlinear%20power%20spectra%0Aachieve%20root%20mean%20squared%20errors%20of%200.1%25%2C%200.3%25%20and%201.3%25%2C%20respectively%2C%20across%20a%0Awide%20range%20of%20cosmological%20parameters%2C%20redshifts%20and%20wavenumbers.%20We%20verify%0Athat%20emulator-related%20discrepancies%20are%20subdominant%20compared%20to%20observational%0Aerrors%20and%20other%20modelling%20uncertainties%20when%20computing%20shear%20power%20spectra%20for%0ALSST-like%20surveys.%20Our%20expressions%20have%20similar%20accuracy%20to%20existing%0A%28numerical%29%20emulators%2C%20but%20are%20at%20least%20an%20order%20of%20magnitude%20faster%2C%20both%20on%20a%0ACPU%20and%20GPU.%20Our%20work%20greatly%20improves%20the%20accuracy%2C%20speed%20and%20range%20of%0Aapplicability%20of%20current%20symbolic%20approximations%20to%20the%20linear%20and%20nonlinear%0Amatter%20power%20spectra.%20We%20provide%20publicly%20available%20code%20for%20all%20symbolic%0Aapproximations%20found.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14623v1&entry.124074799=Read"},
{"title": "LEAD: Latent Realignment for Human Motion Diffusion", "author": "Nefeli Andreou and Xi Wang and Victoria Fern\u00e1ndez Abrevaya and Marie-Paule Cani and Yiorgos Chrysanthou and Vicky Kalogeiton", "abstract": "  Our goal is to generate realistic human motion from natural language. Modern\nmethods often face a trade-off between model expressiveness and text-to-motion\nalignment. Some align text and motion latent spaces but sacrifice\nexpressiveness; others rely on diffusion models producing impressive motions,\nbut lacking semantic meaning in their latent space. This may compromise\nrealism, diversity, and applicability. Here, we address this by combining\nlatent diffusion with a realignment mechanism, producing a novel, semantically\nstructured space that encodes the semantics of language. Leveraging this\ncapability, we introduce the task of textual motion inversion to capture novel\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\nterms of realism, diversity, and text-motion consistency. Our qualitative\nanalysis and user study reveal that our synthesized motions are sharper, more\nhuman-like and comply better with the text compared to modern methods. For\nmotion textual inversion, our method demonstrates improved capacity in\ncapturing out-of-distribution characteristics in comparison to traditional\nVAEs.\n", "link": "http://arxiv.org/abs/2410.14508v1", "date": "2024-10-18", "relevancy": 1.156, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6393}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAD%3A%20Latent%20Realignment%20for%20Human%20Motion%20Diffusion&body=Title%3A%20LEAD%3A%20Latent%20Realignment%20for%20Human%20Motion%20Diffusion%0AAuthor%3A%20Nefeli%20Andreou%20and%20Xi%20Wang%20and%20Victoria%20Fern%C3%A1ndez%20Abrevaya%20and%20Marie-Paule%20Cani%20and%20Yiorgos%20Chrysanthou%20and%20Vicky%20Kalogeiton%0AAbstract%3A%20%20%20Our%20goal%20is%20to%20generate%20realistic%20human%20motion%20from%20natural%20language.%20Modern%0Amethods%20often%20face%20a%20trade-off%20between%20model%20expressiveness%20and%20text-to-motion%0Aalignment.%20Some%20align%20text%20and%20motion%20latent%20spaces%20but%20sacrifice%0Aexpressiveness%3B%20others%20rely%20on%20diffusion%20models%20producing%20impressive%20motions%2C%0Abut%20lacking%20semantic%20meaning%20in%20their%20latent%20space.%20This%20may%20compromise%0Arealism%2C%20diversity%2C%20and%20applicability.%20Here%2C%20we%20address%20this%20by%20combining%0Alatent%20diffusion%20with%20a%20realignment%20mechanism%2C%20producing%20a%20novel%2C%20semantically%0Astructured%20space%20that%20encodes%20the%20semantics%20of%20language.%20Leveraging%20this%0Acapability%2C%20we%20introduce%20the%20task%20of%20textual%20motion%20inversion%20to%20capture%20novel%0Amotion%20concepts%20from%20a%20few%20examples.%20For%20motion%20synthesis%2C%20we%20evaluate%20LEAD%20on%0AHumanML3D%20and%20KIT-ML%20and%20show%20comparable%20performance%20to%20the%20state-of-the-art%20in%0Aterms%20of%20realism%2C%20diversity%2C%20and%20text-motion%20consistency.%20Our%20qualitative%0Aanalysis%20and%20user%20study%20reveal%20that%20our%20synthesized%20motions%20are%20sharper%2C%20more%0Ahuman-like%20and%20comply%20better%20with%20the%20text%20compared%20to%20modern%20methods.%20For%0Amotion%20textual%20inversion%2C%20our%20method%20demonstrates%20improved%20capacity%20in%0Acapturing%20out-of-distribution%20characteristics%20in%20comparison%20to%20traditional%0AVAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAD%253A%2520Latent%2520Realignment%2520for%2520Human%2520Motion%2520Diffusion%26entry.906535625%3DNefeli%2520Andreou%2520and%2520Xi%2520Wang%2520and%2520Victoria%2520Fern%25C3%25A1ndez%2520Abrevaya%2520and%2520Marie-Paule%2520Cani%2520and%2520Yiorgos%2520Chrysanthou%2520and%2520Vicky%2520Kalogeiton%26entry.1292438233%3D%2520%2520Our%2520goal%2520is%2520to%2520generate%2520realistic%2520human%2520motion%2520from%2520natural%2520language.%2520Modern%250Amethods%2520often%2520face%2520a%2520trade-off%2520between%2520model%2520expressiveness%2520and%2520text-to-motion%250Aalignment.%2520Some%2520align%2520text%2520and%2520motion%2520latent%2520spaces%2520but%2520sacrifice%250Aexpressiveness%253B%2520others%2520rely%2520on%2520diffusion%2520models%2520producing%2520impressive%2520motions%252C%250Abut%2520lacking%2520semantic%2520meaning%2520in%2520their%2520latent%2520space.%2520This%2520may%2520compromise%250Arealism%252C%2520diversity%252C%2520and%2520applicability.%2520Here%252C%2520we%2520address%2520this%2520by%2520combining%250Alatent%2520diffusion%2520with%2520a%2520realignment%2520mechanism%252C%2520producing%2520a%2520novel%252C%2520semantically%250Astructured%2520space%2520that%2520encodes%2520the%2520semantics%2520of%2520language.%2520Leveraging%2520this%250Acapability%252C%2520we%2520introduce%2520the%2520task%2520of%2520textual%2520motion%2520inversion%2520to%2520capture%2520novel%250Amotion%2520concepts%2520from%2520a%2520few%2520examples.%2520For%2520motion%2520synthesis%252C%2520we%2520evaluate%2520LEAD%2520on%250AHumanML3D%2520and%2520KIT-ML%2520and%2520show%2520comparable%2520performance%2520to%2520the%2520state-of-the-art%2520in%250Aterms%2520of%2520realism%252C%2520diversity%252C%2520and%2520text-motion%2520consistency.%2520Our%2520qualitative%250Aanalysis%2520and%2520user%2520study%2520reveal%2520that%2520our%2520synthesized%2520motions%2520are%2520sharper%252C%2520more%250Ahuman-like%2520and%2520comply%2520better%2520with%2520the%2520text%2520compared%2520to%2520modern%2520methods.%2520For%250Amotion%2520textual%2520inversion%252C%2520our%2520method%2520demonstrates%2520improved%2520capacity%2520in%250Acapturing%2520out-of-distribution%2520characteristics%2520in%2520comparison%2520to%2520traditional%250AVAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAD%3A%20Latent%20Realignment%20for%20Human%20Motion%20Diffusion&entry.906535625=Nefeli%20Andreou%20and%20Xi%20Wang%20and%20Victoria%20Fern%C3%A1ndez%20Abrevaya%20and%20Marie-Paule%20Cani%20and%20Yiorgos%20Chrysanthou%20and%20Vicky%20Kalogeiton&entry.1292438233=%20%20Our%20goal%20is%20to%20generate%20realistic%20human%20motion%20from%20natural%20language.%20Modern%0Amethods%20often%20face%20a%20trade-off%20between%20model%20expressiveness%20and%20text-to-motion%0Aalignment.%20Some%20align%20text%20and%20motion%20latent%20spaces%20but%20sacrifice%0Aexpressiveness%3B%20others%20rely%20on%20diffusion%20models%20producing%20impressive%20motions%2C%0Abut%20lacking%20semantic%20meaning%20in%20their%20latent%20space.%20This%20may%20compromise%0Arealism%2C%20diversity%2C%20and%20applicability.%20Here%2C%20we%20address%20this%20by%20combining%0Alatent%20diffusion%20with%20a%20realignment%20mechanism%2C%20producing%20a%20novel%2C%20semantically%0Astructured%20space%20that%20encodes%20the%20semantics%20of%20language.%20Leveraging%20this%0Acapability%2C%20we%20introduce%20the%20task%20of%20textual%20motion%20inversion%20to%20capture%20novel%0Amotion%20concepts%20from%20a%20few%20examples.%20For%20motion%20synthesis%2C%20we%20evaluate%20LEAD%20on%0AHumanML3D%20and%20KIT-ML%20and%20show%20comparable%20performance%20to%20the%20state-of-the-art%20in%0Aterms%20of%20realism%2C%20diversity%2C%20and%20text-motion%20consistency.%20Our%20qualitative%0Aanalysis%20and%20user%20study%20reveal%20that%20our%20synthesized%20motions%20are%20sharper%2C%20more%0Ahuman-like%20and%20comply%20better%20with%20the%20text%20compared%20to%20modern%20methods.%20For%0Amotion%20textual%20inversion%2C%20our%20method%20demonstrates%20improved%20capacity%20in%0Acapturing%20out-of-distribution%20characteristics%20in%20comparison%20to%20traditional%0AVAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14508v1&entry.124074799=Read"},
{"title": "Asymptotically Optimal Change Detection for Unnormalized Pre- and\n  Post-Change Distributions", "author": "Arman Adibi and Sanjeev Kulkarni and H. Vincent Poor and Taposh Banerjee and Vahid Tarokh", "abstract": "  This paper addresses the problem of detecting changes when only unnormalized\npre- and post-change distributions are accessible. This situation happens in\nmany scenarios in physics such as in ferromagnetism, crystallography,\nmagneto-hydrodynamics, and thermodynamics, where the energy models are\ndifficult to normalize.\n  Our approach is based on the estimation of the Cumulative Sum (CUSUM)\nstatistics, which is known to produce optimal performance. We first present an\nintuitively appealing approximation method. Unfortunately, this produces a\nbiased estimator of the CUSUM statistics and may cause performance degradation.\nWe then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)\nalgorithm based on thermodynamic integration (TI) in order to estimate the\nlog-ratio of normalizing constants of pre- and post-change distributions. It is\nproved that this approach gives an unbiased estimate of the log-partition\nfunction and the CUSUM statistics, and leads to an asymptotically optimal\nperformance. Moreover, we derive a relationship between the required sample\nsize for thermodynamic integration and the desired detection delay performance,\noffering guidelines for practical parameter selection. Numerical studies are\nprovided demonstrating the efficacy of our approach.\n", "link": "http://arxiv.org/abs/2410.14615v1", "date": "2024-10-18", "relevancy": 1.2267, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4169}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotically%20Optimal%20Change%20Detection%20for%20Unnormalized%20Pre-%20and%0A%20%20Post-Change%20Distributions&body=Title%3A%20Asymptotically%20Optimal%20Change%20Detection%20for%20Unnormalized%20Pre-%20and%0A%20%20Post-Change%20Distributions%0AAuthor%3A%20Arman%20Adibi%20and%20Sanjeev%20Kulkarni%20and%20H.%20Vincent%20Poor%20and%20Taposh%20Banerjee%20and%20Vahid%20Tarokh%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20detecting%20changes%20when%20only%20unnormalized%0Apre-%20and%20post-change%20distributions%20are%20accessible.%20This%20situation%20happens%20in%0Amany%20scenarios%20in%20physics%20such%20as%20in%20ferromagnetism%2C%20crystallography%2C%0Amagneto-hydrodynamics%2C%20and%20thermodynamics%2C%20where%20the%20energy%20models%20are%0Adifficult%20to%20normalize.%0A%20%20Our%20approach%20is%20based%20on%20the%20estimation%20of%20the%20Cumulative%20Sum%20%28CUSUM%29%0Astatistics%2C%20which%20is%20known%20to%20produce%20optimal%20performance.%20We%20first%20present%20an%0Aintuitively%20appealing%20approximation%20method.%20Unfortunately%2C%20this%20produces%20a%0Abiased%20estimator%20of%20the%20CUSUM%20statistics%20and%20may%20cause%20performance%20degradation.%0AWe%20then%20propose%20the%20Log-Partition%20Approximation%20Cumulative%20Sum%20%28LPA-CUSUM%29%0Aalgorithm%20based%20on%20thermodynamic%20integration%20%28TI%29%20in%20order%20to%20estimate%20the%0Alog-ratio%20of%20normalizing%20constants%20of%20pre-%20and%20post-change%20distributions.%20It%20is%0Aproved%20that%20this%20approach%20gives%20an%20unbiased%20estimate%20of%20the%20log-partition%0Afunction%20and%20the%20CUSUM%20statistics%2C%20and%20leads%20to%20an%20asymptotically%20optimal%0Aperformance.%20Moreover%2C%20we%20derive%20a%20relationship%20between%20the%20required%20sample%0Asize%20for%20thermodynamic%20integration%20and%20the%20desired%20detection%20delay%20performance%2C%0Aoffering%20guidelines%20for%20practical%20parameter%20selection.%20Numerical%20studies%20are%0Aprovided%20demonstrating%20the%20efficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotically%2520Optimal%2520Change%2520Detection%2520for%2520Unnormalized%2520Pre-%2520and%250A%2520%2520Post-Change%2520Distributions%26entry.906535625%3DArman%2520Adibi%2520and%2520Sanjeev%2520Kulkarni%2520and%2520H.%2520Vincent%2520Poor%2520and%2520Taposh%2520Banerjee%2520and%2520Vahid%2520Tarokh%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520detecting%2520changes%2520when%2520only%2520unnormalized%250Apre-%2520and%2520post-change%2520distributions%2520are%2520accessible.%2520This%2520situation%2520happens%2520in%250Amany%2520scenarios%2520in%2520physics%2520such%2520as%2520in%2520ferromagnetism%252C%2520crystallography%252C%250Amagneto-hydrodynamics%252C%2520and%2520thermodynamics%252C%2520where%2520the%2520energy%2520models%2520are%250Adifficult%2520to%2520normalize.%250A%2520%2520Our%2520approach%2520is%2520based%2520on%2520the%2520estimation%2520of%2520the%2520Cumulative%2520Sum%2520%2528CUSUM%2529%250Astatistics%252C%2520which%2520is%2520known%2520to%2520produce%2520optimal%2520performance.%2520We%2520first%2520present%2520an%250Aintuitively%2520appealing%2520approximation%2520method.%2520Unfortunately%252C%2520this%2520produces%2520a%250Abiased%2520estimator%2520of%2520the%2520CUSUM%2520statistics%2520and%2520may%2520cause%2520performance%2520degradation.%250AWe%2520then%2520propose%2520the%2520Log-Partition%2520Approximation%2520Cumulative%2520Sum%2520%2528LPA-CUSUM%2529%250Aalgorithm%2520based%2520on%2520thermodynamic%2520integration%2520%2528TI%2529%2520in%2520order%2520to%2520estimate%2520the%250Alog-ratio%2520of%2520normalizing%2520constants%2520of%2520pre-%2520and%2520post-change%2520distributions.%2520It%2520is%250Aproved%2520that%2520this%2520approach%2520gives%2520an%2520unbiased%2520estimate%2520of%2520the%2520log-partition%250Afunction%2520and%2520the%2520CUSUM%2520statistics%252C%2520and%2520leads%2520to%2520an%2520asymptotically%2520optimal%250Aperformance.%2520Moreover%252C%2520we%2520derive%2520a%2520relationship%2520between%2520the%2520required%2520sample%250Asize%2520for%2520thermodynamic%2520integration%2520and%2520the%2520desired%2520detection%2520delay%2520performance%252C%250Aoffering%2520guidelines%2520for%2520practical%2520parameter%2520selection.%2520Numerical%2520studies%2520are%250Aprovided%2520demonstrating%2520the%2520efficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotically%20Optimal%20Change%20Detection%20for%20Unnormalized%20Pre-%20and%0A%20%20Post-Change%20Distributions&entry.906535625=Arman%20Adibi%20and%20Sanjeev%20Kulkarni%20and%20H.%20Vincent%20Poor%20and%20Taposh%20Banerjee%20and%20Vahid%20Tarokh&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20detecting%20changes%20when%20only%20unnormalized%0Apre-%20and%20post-change%20distributions%20are%20accessible.%20This%20situation%20happens%20in%0Amany%20scenarios%20in%20physics%20such%20as%20in%20ferromagnetism%2C%20crystallography%2C%0Amagneto-hydrodynamics%2C%20and%20thermodynamics%2C%20where%20the%20energy%20models%20are%0Adifficult%20to%20normalize.%0A%20%20Our%20approach%20is%20based%20on%20the%20estimation%20of%20the%20Cumulative%20Sum%20%28CUSUM%29%0Astatistics%2C%20which%20is%20known%20to%20produce%20optimal%20performance.%20We%20first%20present%20an%0Aintuitively%20appealing%20approximation%20method.%20Unfortunately%2C%20this%20produces%20a%0Abiased%20estimator%20of%20the%20CUSUM%20statistics%20and%20may%20cause%20performance%20degradation.%0AWe%20then%20propose%20the%20Log-Partition%20Approximation%20Cumulative%20Sum%20%28LPA-CUSUM%29%0Aalgorithm%20based%20on%20thermodynamic%20integration%20%28TI%29%20in%20order%20to%20estimate%20the%0Alog-ratio%20of%20normalizing%20constants%20of%20pre-%20and%20post-change%20distributions.%20It%20is%0Aproved%20that%20this%20approach%20gives%20an%20unbiased%20estimate%20of%20the%20log-partition%0Afunction%20and%20the%20CUSUM%20statistics%2C%20and%20leads%20to%20an%20asymptotically%20optimal%0Aperformance.%20Moreover%2C%20we%20derive%20a%20relationship%20between%20the%20required%20sample%0Asize%20for%20thermodynamic%20integration%20and%20the%20desired%20detection%20delay%20performance%2C%0Aoffering%20guidelines%20for%20practical%20parameter%20selection.%20Numerical%20studies%20are%0Aprovided%20demonstrating%20the%20efficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14615v1&entry.124074799=Read"},
{"title": "Clustering of timed sequences -- Application to the analysis of care\n  pathways", "author": "Thomas Guyet and Pierre Pinson and Enoal Gesny", "abstract": "  Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.\n", "link": "http://arxiv.org/abs/2404.15379v2", "date": "2024-10-18", "relevancy": 1.1414, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3912}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.381}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20of%20timed%20sequences%20--%20Application%20to%20the%20analysis%20of%20care%0A%20%20pathways&body=Title%3A%20Clustering%20of%20timed%20sequences%20--%20Application%20to%20the%20analysis%20of%20care%0A%20%20pathways%0AAuthor%3A%20Thomas%20Guyet%20and%20Pierre%20Pinson%20and%20Enoal%20Gesny%0AAbstract%3A%20%20%20Improving%20the%20future%20of%20healthcare%20starts%20by%20better%20understanding%20the%20current%0Aactual%20practices%20in%20hospital%20settings.%20This%20motivates%20the%20objective%20of%0Adiscovering%20typical%20care%20pathways%20from%20patient%20data.%20Revealing%20typical%20care%0Apathways%20can%20be%20achieved%20through%20clustering.%20The%20difficulty%20in%20clustering%20care%0Apathways%2C%20represented%20by%20sequences%20of%20timestamped%20events%2C%20lies%20in%20defining%20a%0Asemantically%20appropriate%20metric%20and%20clustering%20algorithms.%20In%20this%20article%2C%20we%0Aadapt%20two%20methods%20developed%20for%20time%20series%20to%20the%20clustering%20of%20timed%0Asequences%3A%20the%20drop-DTW%20metric%20and%20the%20DBA%20approach%20for%20the%20construction%20of%0Aaveraged%20time%20sequences.%20These%20methods%20are%20then%20applied%20in%20clustering%0Aalgorithms%20to%20propose%20original%20and%20sound%20clustering%20algorithms%20for%20timed%0Asequences.%20This%20approach%20is%20experimented%20with%20and%20evaluated%20on%20synthetic%20and%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520of%2520timed%2520sequences%2520--%2520Application%2520to%2520the%2520analysis%2520of%2520care%250A%2520%2520pathways%26entry.906535625%3DThomas%2520Guyet%2520and%2520Pierre%2520Pinson%2520and%2520Enoal%2520Gesny%26entry.1292438233%3D%2520%2520Improving%2520the%2520future%2520of%2520healthcare%2520starts%2520by%2520better%2520understanding%2520the%2520current%250Aactual%2520practices%2520in%2520hospital%2520settings.%2520This%2520motivates%2520the%2520objective%2520of%250Adiscovering%2520typical%2520care%2520pathways%2520from%2520patient%2520data.%2520Revealing%2520typical%2520care%250Apathways%2520can%2520be%2520achieved%2520through%2520clustering.%2520The%2520difficulty%2520in%2520clustering%2520care%250Apathways%252C%2520represented%2520by%2520sequences%2520of%2520timestamped%2520events%252C%2520lies%2520in%2520defining%2520a%250Asemantically%2520appropriate%2520metric%2520and%2520clustering%2520algorithms.%2520In%2520this%2520article%252C%2520we%250Aadapt%2520two%2520methods%2520developed%2520for%2520time%2520series%2520to%2520the%2520clustering%2520of%2520timed%250Asequences%253A%2520the%2520drop-DTW%2520metric%2520and%2520the%2520DBA%2520approach%2520for%2520the%2520construction%2520of%250Aaveraged%2520time%2520sequences.%2520These%2520methods%2520are%2520then%2520applied%2520in%2520clustering%250Aalgorithms%2520to%2520propose%2520original%2520and%2520sound%2520clustering%2520algorithms%2520for%2520timed%250Asequences.%2520This%2520approach%2520is%2520experimented%2520with%2520and%2520evaluated%2520on%2520synthetic%2520and%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20of%20timed%20sequences%20--%20Application%20to%20the%20analysis%20of%20care%0A%20%20pathways&entry.906535625=Thomas%20Guyet%20and%20Pierre%20Pinson%20and%20Enoal%20Gesny&entry.1292438233=%20%20Improving%20the%20future%20of%20healthcare%20starts%20by%20better%20understanding%20the%20current%0Aactual%20practices%20in%20hospital%20settings.%20This%20motivates%20the%20objective%20of%0Adiscovering%20typical%20care%20pathways%20from%20patient%20data.%20Revealing%20typical%20care%0Apathways%20can%20be%20achieved%20through%20clustering.%20The%20difficulty%20in%20clustering%20care%0Apathways%2C%20represented%20by%20sequences%20of%20timestamped%20events%2C%20lies%20in%20defining%20a%0Asemantically%20appropriate%20metric%20and%20clustering%20algorithms.%20In%20this%20article%2C%20we%0Aadapt%20two%20methods%20developed%20for%20time%20series%20to%20the%20clustering%20of%20timed%0Asequences%3A%20the%20drop-DTW%20metric%20and%20the%20DBA%20approach%20for%20the%20construction%20of%0Aaveraged%20time%20sequences.%20These%20methods%20are%20then%20applied%20in%20clustering%0Aalgorithms%20to%20propose%20original%20and%20sound%20clustering%20algorithms%20for%20timed%0Asequences.%20This%20approach%20is%20experimented%20with%20and%20evaluated%20on%20synthetic%20and%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15379v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


