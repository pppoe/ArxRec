<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241222.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Synthesizing Moving People with 3D Control", "author": "Boyi Li and Junming Chen and Jathushan Rajasegaran and Yossi Gandelsman and Alexei A. Efros and Jitendra Malik", "abstract": "  In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.\n", "link": "http://arxiv.org/abs/2401.10889v2", "date": "2024-12-20", "relevancy": 3.2031, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6449}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6449}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Moving%20People%20with%203D%20Control&body=Title%3A%20Synthesizing%20Moving%20People%20with%203D%20Control%0AAuthor%3A%20Boyi%20Li%20and%20Junming%20Chen%20and%20Jathushan%20Rajasegaran%20and%20Yossi%20Gandelsman%20and%20Alexei%20A.%20Efros%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20diffusion%20model-based%20framework%20for%20animating%0Apeople%20from%20a%20single%20image%20for%20a%20given%20target%203D%20motion%20sequence.%20Our%20approach%0Ahas%20two%20core%20components%3A%20a%29%20learning%20priors%20about%20invisible%20parts%20of%20the%20human%0Abody%20and%20clothing%2C%20and%20b%29%20rendering%20novel%20body%20poses%20with%20proper%20clothing%20and%0Atexture.%20For%20the%20first%20part%2C%20we%20learn%20an%20in-filling%20diffusion%20model%20to%0Ahallucinate%20unseen%20parts%20of%20a%20person%20given%20a%20single%20image.%20We%20train%20this%20model%0Aon%20texture%20map%20space%2C%20which%20makes%20it%20more%20sample-efficient%20since%20it%20is%0Ainvariant%20to%20pose%20and%20viewpoint.%20Second%2C%20we%20develop%20a%20diffusion-based%20rendering%0Apipeline%2C%20which%20is%20controlled%20by%203D%20human%20poses.%20This%20produces%20realistic%0Arenderings%20of%20novel%20poses%20of%20the%20person%2C%20including%20clothing%2C%20hair%2C%20and%0Aplausible%20in-filling%20of%20unseen%20regions.%20This%20disentangled%20approach%20allows%20our%0Amethod%20to%20generate%20a%20sequence%20of%20images%20that%20are%20faithful%20to%20the%20target%20motion%0Ain%20the%203D%20pose%20and%2C%20to%20the%20input%20image%20in%20terms%20of%20visual%20similarity.%20In%0Aaddition%20to%20that%2C%20the%203D%20control%20allows%20various%20synthetic%20camera%20trajectories%0Ato%20render%20a%20person.%20Our%20experiments%20show%20that%20our%20method%20is%20resilient%20in%0Agenerating%20prolonged%20motions%20and%20varied%20challenging%20and%20complex%20poses%20compared%0Ato%20prior%20methods.%20Please%20check%20our%20website%20for%20more%20details%3A%0Ahttps%3A//boyiliee.github.io/3DHM.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520Moving%2520People%2520with%25203D%2520Control%26entry.906535625%3DBoyi%2520Li%2520and%2520Junming%2520Chen%2520and%2520Jathushan%2520Rajasegaran%2520and%2520Yossi%2520Gandelsman%2520and%2520Alexei%2520A.%2520Efros%2520and%2520Jitendra%2520Malik%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520diffusion%2520model-based%2520framework%2520for%2520animating%250Apeople%2520from%2520a%2520single%2520image%2520for%2520a%2520given%2520target%25203D%2520motion%2520sequence.%2520Our%2520approach%250Ahas%2520two%2520core%2520components%253A%2520a%2529%2520learning%2520priors%2520about%2520invisible%2520parts%2520of%2520the%2520human%250Abody%2520and%2520clothing%252C%2520and%2520b%2529%2520rendering%2520novel%2520body%2520poses%2520with%2520proper%2520clothing%2520and%250Atexture.%2520For%2520the%2520first%2520part%252C%2520we%2520learn%2520an%2520in-filling%2520diffusion%2520model%2520to%250Ahallucinate%2520unseen%2520parts%2520of%2520a%2520person%2520given%2520a%2520single%2520image.%2520We%2520train%2520this%2520model%250Aon%2520texture%2520map%2520space%252C%2520which%2520makes%2520it%2520more%2520sample-efficient%2520since%2520it%2520is%250Ainvariant%2520to%2520pose%2520and%2520viewpoint.%2520Second%252C%2520we%2520develop%2520a%2520diffusion-based%2520rendering%250Apipeline%252C%2520which%2520is%2520controlled%2520by%25203D%2520human%2520poses.%2520This%2520produces%2520realistic%250Arenderings%2520of%2520novel%2520poses%2520of%2520the%2520person%252C%2520including%2520clothing%252C%2520hair%252C%2520and%250Aplausible%2520in-filling%2520of%2520unseen%2520regions.%2520This%2520disentangled%2520approach%2520allows%2520our%250Amethod%2520to%2520generate%2520a%2520sequence%2520of%2520images%2520that%2520are%2520faithful%2520to%2520the%2520target%2520motion%250Ain%2520the%25203D%2520pose%2520and%252C%2520to%2520the%2520input%2520image%2520in%2520terms%2520of%2520visual%2520similarity.%2520In%250Aaddition%2520to%2520that%252C%2520the%25203D%2520control%2520allows%2520various%2520synthetic%2520camera%2520trajectories%250Ato%2520render%2520a%2520person.%2520Our%2520experiments%2520show%2520that%2520our%2520method%2520is%2520resilient%2520in%250Agenerating%2520prolonged%2520motions%2520and%2520varied%2520challenging%2520and%2520complex%2520poses%2520compared%250Ato%2520prior%2520methods.%2520Please%2520check%2520our%2520website%2520for%2520more%2520details%253A%250Ahttps%253A//boyiliee.github.io/3DHM.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Moving%20People%20with%203D%20Control&entry.906535625=Boyi%20Li%20and%20Junming%20Chen%20and%20Jathushan%20Rajasegaran%20and%20Yossi%20Gandelsman%20and%20Alexei%20A.%20Efros%20and%20Jitendra%20Malik&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20diffusion%20model-based%20framework%20for%20animating%0Apeople%20from%20a%20single%20image%20for%20a%20given%20target%203D%20motion%20sequence.%20Our%20approach%0Ahas%20two%20core%20components%3A%20a%29%20learning%20priors%20about%20invisible%20parts%20of%20the%20human%0Abody%20and%20clothing%2C%20and%20b%29%20rendering%20novel%20body%20poses%20with%20proper%20clothing%20and%0Atexture.%20For%20the%20first%20part%2C%20we%20learn%20an%20in-filling%20diffusion%20model%20to%0Ahallucinate%20unseen%20parts%20of%20a%20person%20given%20a%20single%20image.%20We%20train%20this%20model%0Aon%20texture%20map%20space%2C%20which%20makes%20it%20more%20sample-efficient%20since%20it%20is%0Ainvariant%20to%20pose%20and%20viewpoint.%20Second%2C%20we%20develop%20a%20diffusion-based%20rendering%0Apipeline%2C%20which%20is%20controlled%20by%203D%20human%20poses.%20This%20produces%20realistic%0Arenderings%20of%20novel%20poses%20of%20the%20person%2C%20including%20clothing%2C%20hair%2C%20and%0Aplausible%20in-filling%20of%20unseen%20regions.%20This%20disentangled%20approach%20allows%20our%0Amethod%20to%20generate%20a%20sequence%20of%20images%20that%20are%20faithful%20to%20the%20target%20motion%0Ain%20the%203D%20pose%20and%2C%20to%20the%20input%20image%20in%20terms%20of%20visual%20similarity.%20In%0Aaddition%20to%20that%2C%20the%203D%20control%20allows%20various%20synthetic%20camera%20trajectories%0Ato%20render%20a%20person.%20Our%20experiments%20show%20that%20our%20method%20is%20resilient%20in%0Agenerating%20prolonged%20motions%20and%20varied%20challenging%20and%20complex%20poses%20compared%0Ato%20prior%20methods.%20Please%20check%20our%20website%20for%20more%20details%3A%0Ahttps%3A//boyiliee.github.io/3DHM.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10889v2&entry.124074799=Read"},
{"title": "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from\n  Defocused Images", "author": "Jungho Lee and Suhwan Cho and Taeoh Kim and Ho-Deok Jang and Minhyeok Lee and Geonho Cha and Dongyoon Wee and Dogyoon Lee and Sangyoun Lee", "abstract": "  3D Gaussian Splatting (3DGS) has attracted significant attention for its\nhigh-quality novel view rendering, inspiring research to address real-world\nchallenges. While conventional methods depend on sharp images for accurate\nscene reconstruction, real-world scenarios are often affected by defocus blur\ndue to finite depth of field, making it essential to account for realistic 3D\nscene representation. In this study, we propose CoCoGaussian, a Circle of\nConfusion-aware Gaussian Splatting that enables precise 3D scene representation\nusing only defocused images. CoCoGaussian addresses the challenge of defocus\nblur by modeling the Circle of Confusion (CoC) through a physically grounded\napproach based on the principles of photographic defocus. Exploiting 3D\nGaussians, we compute the CoC diameter from depth and learnable aperture\ninformation, generating multiple Gaussians to precisely capture the CoC shape.\nFurthermore, we introduce a learnable scaling factor to enhance robustness and\nprovide more flexibility in handling unreliable depth in scenes with reflective\nor refractive surfaces. Experiments on both synthetic and real-world datasets\ndemonstrate that CoCoGaussian achieves state-of-the-art performance across\nmultiple benchmarks.\n", "link": "http://arxiv.org/abs/2412.16028v1", "date": "2024-12-20", "relevancy": 3.1947, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6445}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6428}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoCoGaussian%3A%20Leveraging%20Circle%20of%20Confusion%20for%20Gaussian%20Splatting%20from%0A%20%20Defocused%20Images&body=Title%3A%20CoCoGaussian%3A%20Leveraging%20Circle%20of%20Confusion%20for%20Gaussian%20Splatting%20from%0A%20%20Defocused%20Images%0AAuthor%3A%20Jungho%20Lee%20and%20Suhwan%20Cho%20and%20Taeoh%20Kim%20and%20Ho-Deok%20Jang%20and%20Minhyeok%20Lee%20and%20Geonho%20Cha%20and%20Dongyoon%20Wee%20and%20Dogyoon%20Lee%20and%20Sangyoun%20Lee%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20significant%20attention%20for%20its%0Ahigh-quality%20novel%20view%20rendering%2C%20inspiring%20research%20to%20address%20real-world%0Achallenges.%20While%20conventional%20methods%20depend%20on%20sharp%20images%20for%20accurate%0Ascene%20reconstruction%2C%20real-world%20scenarios%20are%20often%20affected%20by%20defocus%20blur%0Adue%20to%20finite%20depth%20of%20field%2C%20making%20it%20essential%20to%20account%20for%20realistic%203D%0Ascene%20representation.%20In%20this%20study%2C%20we%20propose%20CoCoGaussian%2C%20a%20Circle%20of%0AConfusion-aware%20Gaussian%20Splatting%20that%20enables%20precise%203D%20scene%20representation%0Ausing%20only%20defocused%20images.%20CoCoGaussian%20addresses%20the%20challenge%20of%20defocus%0Ablur%20by%20modeling%20the%20Circle%20of%20Confusion%20%28CoC%29%20through%20a%20physically%20grounded%0Aapproach%20based%20on%20the%20principles%20of%20photographic%20defocus.%20Exploiting%203D%0AGaussians%2C%20we%20compute%20the%20CoC%20diameter%20from%20depth%20and%20learnable%20aperture%0Ainformation%2C%20generating%20multiple%20Gaussians%20to%20precisely%20capture%20the%20CoC%20shape.%0AFurthermore%2C%20we%20introduce%20a%20learnable%20scaling%20factor%20to%20enhance%20robustness%20and%0Aprovide%20more%20flexibility%20in%20handling%20unreliable%20depth%20in%20scenes%20with%20reflective%0Aor%20refractive%20surfaces.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20CoCoGaussian%20achieves%20state-of-the-art%20performance%20across%0Amultiple%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoCoGaussian%253A%2520Leveraging%2520Circle%2520of%2520Confusion%2520for%2520Gaussian%2520Splatting%2520from%250A%2520%2520Defocused%2520Images%26entry.906535625%3DJungho%2520Lee%2520and%2520Suhwan%2520Cho%2520and%2520Taeoh%2520Kim%2520and%2520Ho-Deok%2520Jang%2520and%2520Minhyeok%2520Lee%2520and%2520Geonho%2520Cha%2520and%2520Dongyoon%2520Wee%2520and%2520Dogyoon%2520Lee%2520and%2520Sangyoun%2520Lee%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520attracted%2520significant%2520attention%2520for%2520its%250Ahigh-quality%2520novel%2520view%2520rendering%252C%2520inspiring%2520research%2520to%2520address%2520real-world%250Achallenges.%2520While%2520conventional%2520methods%2520depend%2520on%2520sharp%2520images%2520for%2520accurate%250Ascene%2520reconstruction%252C%2520real-world%2520scenarios%2520are%2520often%2520affected%2520by%2520defocus%2520blur%250Adue%2520to%2520finite%2520depth%2520of%2520field%252C%2520making%2520it%2520essential%2520to%2520account%2520for%2520realistic%25203D%250Ascene%2520representation.%2520In%2520this%2520study%252C%2520we%2520propose%2520CoCoGaussian%252C%2520a%2520Circle%2520of%250AConfusion-aware%2520Gaussian%2520Splatting%2520that%2520enables%2520precise%25203D%2520scene%2520representation%250Ausing%2520only%2520defocused%2520images.%2520CoCoGaussian%2520addresses%2520the%2520challenge%2520of%2520defocus%250Ablur%2520by%2520modeling%2520the%2520Circle%2520of%2520Confusion%2520%2528CoC%2529%2520through%2520a%2520physically%2520grounded%250Aapproach%2520based%2520on%2520the%2520principles%2520of%2520photographic%2520defocus.%2520Exploiting%25203D%250AGaussians%252C%2520we%2520compute%2520the%2520CoC%2520diameter%2520from%2520depth%2520and%2520learnable%2520aperture%250Ainformation%252C%2520generating%2520multiple%2520Gaussians%2520to%2520precisely%2520capture%2520the%2520CoC%2520shape.%250AFurthermore%252C%2520we%2520introduce%2520a%2520learnable%2520scaling%2520factor%2520to%2520enhance%2520robustness%2520and%250Aprovide%2520more%2520flexibility%2520in%2520handling%2520unreliable%2520depth%2520in%2520scenes%2520with%2520reflective%250Aor%2520refractive%2520surfaces.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520that%2520CoCoGaussian%2520achieves%2520state-of-the-art%2520performance%2520across%250Amultiple%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoCoGaussian%3A%20Leveraging%20Circle%20of%20Confusion%20for%20Gaussian%20Splatting%20from%0A%20%20Defocused%20Images&entry.906535625=Jungho%20Lee%20and%20Suhwan%20Cho%20and%20Taeoh%20Kim%20and%20Ho-Deok%20Jang%20and%20Minhyeok%20Lee%20and%20Geonho%20Cha%20and%20Dongyoon%20Wee%20and%20Dogyoon%20Lee%20and%20Sangyoun%20Lee&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20significant%20attention%20for%20its%0Ahigh-quality%20novel%20view%20rendering%2C%20inspiring%20research%20to%20address%20real-world%0Achallenges.%20While%20conventional%20methods%20depend%20on%20sharp%20images%20for%20accurate%0Ascene%20reconstruction%2C%20real-world%20scenarios%20are%20often%20affected%20by%20defocus%20blur%0Adue%20to%20finite%20depth%20of%20field%2C%20making%20it%20essential%20to%20account%20for%20realistic%203D%0Ascene%20representation.%20In%20this%20study%2C%20we%20propose%20CoCoGaussian%2C%20a%20Circle%20of%0AConfusion-aware%20Gaussian%20Splatting%20that%20enables%20precise%203D%20scene%20representation%0Ausing%20only%20defocused%20images.%20CoCoGaussian%20addresses%20the%20challenge%20of%20defocus%0Ablur%20by%20modeling%20the%20Circle%20of%20Confusion%20%28CoC%29%20through%20a%20physically%20grounded%0Aapproach%20based%20on%20the%20principles%20of%20photographic%20defocus.%20Exploiting%203D%0AGaussians%2C%20we%20compute%20the%20CoC%20diameter%20from%20depth%20and%20learnable%20aperture%0Ainformation%2C%20generating%20multiple%20Gaussians%20to%20precisely%20capture%20the%20CoC%20shape.%0AFurthermore%2C%20we%20introduce%20a%20learnable%20scaling%20factor%20to%20enhance%20robustness%20and%0Aprovide%20more%20flexibility%20in%20handling%20unreliable%20depth%20in%20scenes%20with%20reflective%0Aor%20refractive%20surfaces.%20Experiments%20on%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20CoCoGaussian%20achieves%20state-of-the-art%20performance%20across%0Amultiple%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16028v1&entry.124074799=Read"},
{"title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing", "author": "Chun Gu and Xiaofei Wei and Zixuan Zeng and Yuxuan Yao and Li Zhang", "abstract": "  In inverse rendering, accurately modeling visibility and indirect radiance\nfor incident light is essential for capturing secondary effects. Due to the\nabsence of a powerful Gaussian ray tracer, previous 3DGS-based methods have\neither adopted a simplified rendering equation or used learnable parameters to\napproximate incident light, resulting in inaccurate material and lighting\nestimations. To this end, we introduce inter-reflective Gaussian splatting\n(IRGS) for inverse rendering. To capture inter-reflection, we apply the full\nrendering equation without simplification and compute incident radiance on the\nfly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we\npresent an efficient optimization scheme to handle the computational demands of\nMonte Carlo sampling for rendering equation evaluation. Furthermore, we\nintroduce a novel strategy for querying the indirect radiance of incident light\nwhen relighting the optimized scenes. Extensive experiments on multiple\nstandard benchmarks validate the effectiveness of IRGS, demonstrating its\ncapability to accurately model complex inter-reflection effects.\n", "link": "http://arxiv.org/abs/2412.15867v1", "date": "2024-12-20", "relevancy": 3.1171, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6855}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6424}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRGS%3A%20Inter-Reflective%20Gaussian%20Splatting%20with%202D%20Gaussian%20Ray%20Tracing&body=Title%3A%20IRGS%3A%20Inter-Reflective%20Gaussian%20Splatting%20with%202D%20Gaussian%20Ray%20Tracing%0AAuthor%3A%20Chun%20Gu%20and%20Xiaofei%20Wei%20and%20Zixuan%20Zeng%20and%20Yuxuan%20Yao%20and%20Li%20Zhang%0AAbstract%3A%20%20%20In%20inverse%20rendering%2C%20accurately%20modeling%20visibility%20and%20indirect%20radiance%0Afor%20incident%20light%20is%20essential%20for%20capturing%20secondary%20effects.%20Due%20to%20the%0Aabsence%20of%20a%20powerful%20Gaussian%20ray%20tracer%2C%20previous%203DGS-based%20methods%20have%0Aeither%20adopted%20a%20simplified%20rendering%20equation%20or%20used%20learnable%20parameters%20to%0Aapproximate%20incident%20light%2C%20resulting%20in%20inaccurate%20material%20and%20lighting%0Aestimations.%20To%20this%20end%2C%20we%20introduce%20inter-reflective%20Gaussian%20splatting%0A%28IRGS%29%20for%20inverse%20rendering.%20To%20capture%20inter-reflection%2C%20we%20apply%20the%20full%0Arendering%20equation%20without%20simplification%20and%20compute%20incident%20radiance%20on%20the%0Afly%20using%20the%20proposed%20differentiable%202D%20Gaussian%20ray%20tracing.%20Additionally%2C%20we%0Apresent%20an%20efficient%20optimization%20scheme%20to%20handle%20the%20computational%20demands%20of%0AMonte%20Carlo%20sampling%20for%20rendering%20equation%20evaluation.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20strategy%20for%20querying%20the%20indirect%20radiance%20of%20incident%20light%0Awhen%20relighting%20the%20optimized%20scenes.%20Extensive%20experiments%20on%20multiple%0Astandard%20benchmarks%20validate%20the%20effectiveness%20of%20IRGS%2C%20demonstrating%20its%0Acapability%20to%20accurately%20model%20complex%20inter-reflection%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRGS%253A%2520Inter-Reflective%2520Gaussian%2520Splatting%2520with%25202D%2520Gaussian%2520Ray%2520Tracing%26entry.906535625%3DChun%2520Gu%2520and%2520Xiaofei%2520Wei%2520and%2520Zixuan%2520Zeng%2520and%2520Yuxuan%2520Yao%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520In%2520inverse%2520rendering%252C%2520accurately%2520modeling%2520visibility%2520and%2520indirect%2520radiance%250Afor%2520incident%2520light%2520is%2520essential%2520for%2520capturing%2520secondary%2520effects.%2520Due%2520to%2520the%250Aabsence%2520of%2520a%2520powerful%2520Gaussian%2520ray%2520tracer%252C%2520previous%25203DGS-based%2520methods%2520have%250Aeither%2520adopted%2520a%2520simplified%2520rendering%2520equation%2520or%2520used%2520learnable%2520parameters%2520to%250Aapproximate%2520incident%2520light%252C%2520resulting%2520in%2520inaccurate%2520material%2520and%2520lighting%250Aestimations.%2520To%2520this%2520end%252C%2520we%2520introduce%2520inter-reflective%2520Gaussian%2520splatting%250A%2528IRGS%2529%2520for%2520inverse%2520rendering.%2520To%2520capture%2520inter-reflection%252C%2520we%2520apply%2520the%2520full%250Arendering%2520equation%2520without%2520simplification%2520and%2520compute%2520incident%2520radiance%2520on%2520the%250Afly%2520using%2520the%2520proposed%2520differentiable%25202D%2520Gaussian%2520ray%2520tracing.%2520Additionally%252C%2520we%250Apresent%2520an%2520efficient%2520optimization%2520scheme%2520to%2520handle%2520the%2520computational%2520demands%2520of%250AMonte%2520Carlo%2520sampling%2520for%2520rendering%2520equation%2520evaluation.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520novel%2520strategy%2520for%2520querying%2520the%2520indirect%2520radiance%2520of%2520incident%2520light%250Awhen%2520relighting%2520the%2520optimized%2520scenes.%2520Extensive%2520experiments%2520on%2520multiple%250Astandard%2520benchmarks%2520validate%2520the%2520effectiveness%2520of%2520IRGS%252C%2520demonstrating%2520its%250Acapability%2520to%2520accurately%2520model%2520complex%2520inter-reflection%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRGS%3A%20Inter-Reflective%20Gaussian%20Splatting%20with%202D%20Gaussian%20Ray%20Tracing&entry.906535625=Chun%20Gu%20and%20Xiaofei%20Wei%20and%20Zixuan%20Zeng%20and%20Yuxuan%20Yao%20and%20Li%20Zhang&entry.1292438233=%20%20In%20inverse%20rendering%2C%20accurately%20modeling%20visibility%20and%20indirect%20radiance%0Afor%20incident%20light%20is%20essential%20for%20capturing%20secondary%20effects.%20Due%20to%20the%0Aabsence%20of%20a%20powerful%20Gaussian%20ray%20tracer%2C%20previous%203DGS-based%20methods%20have%0Aeither%20adopted%20a%20simplified%20rendering%20equation%20or%20used%20learnable%20parameters%20to%0Aapproximate%20incident%20light%2C%20resulting%20in%20inaccurate%20material%20and%20lighting%0Aestimations.%20To%20this%20end%2C%20we%20introduce%20inter-reflective%20Gaussian%20splatting%0A%28IRGS%29%20for%20inverse%20rendering.%20To%20capture%20inter-reflection%2C%20we%20apply%20the%20full%0Arendering%20equation%20without%20simplification%20and%20compute%20incident%20radiance%20on%20the%0Afly%20using%20the%20proposed%20differentiable%202D%20Gaussian%20ray%20tracing.%20Additionally%2C%20we%0Apresent%20an%20efficient%20optimization%20scheme%20to%20handle%20the%20computational%20demands%20of%0AMonte%20Carlo%20sampling%20for%20rendering%20equation%20evaluation.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20strategy%20for%20querying%20the%20indirect%20radiance%20of%20incident%20light%0Awhen%20relighting%20the%20optimized%20scenes.%20Extensive%20experiments%20on%20multiple%0Astandard%20benchmarks%20validate%20the%20effectiveness%20of%20IRGS%2C%20demonstrating%20its%0Acapability%20to%20accurately%20model%20complex%20inter-reflection%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15867v1&entry.124074799=Read"},
{"title": "Vision Model Pre-training on Interleaved Image-Text Data via Latent\n  Compression Learning", "author": "Chenyu Yang and Xizhou Zhu and Jinguo Zhu and Weijie Su and Junjie Wang and Xuan Dong and Wenhai Wang and Lewei Lu and Bin Li and Jie Zhou and Yu Qiao and Jifeng Dai", "abstract": "  Recently, vision model pre-training has evolved from relying on manually\nannotated datasets to leveraging large-scale, web-crawled image-text data.\nDespite these advances, there is no pre-training method that effectively\nexploits the interleaved image-text data, which is very prevalent on the\nInternet. Inspired by the recent success of compression learning in natural\nlanguage processing, we propose a novel vision model pre-training method called\nLatent Compression Learning (LCL) for interleaved image-text data. This method\nperforms latent compression learning by maximizing the mutual information\nbetween the inputs and outputs of a causal attention model. The training\nobjective can be decomposed into two basic tasks: 1) contrastive learning\nbetween visual representation and preceding context, and 2) generating\nsubsequent text based on visual representation. Our experiments demonstrate\nthat our method not only matches the performance of CLIP on paired pre-training\ndatasets (e.g., LAION), but can also leverage interleaved pre-training data\n(e.g., MMC4) to learn robust visual representation from scratch, showcasing the\npotential of vision model pre-training with interleaved image-text data. Code\nis released at https://github.com/OpenGVLab/LCL.\n", "link": "http://arxiv.org/abs/2406.07543v2", "date": "2024-12-20", "relevancy": 2.9837, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning&body=Title%3A%20Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning%0AAuthor%3A%20Chenyu%20Yang%20and%20Xizhou%20Zhu%20and%20Jinguo%20Zhu%20and%20Weijie%20Su%20and%20Junjie%20Wang%20and%20Xuan%20Dong%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Bin%20Li%20and%20Jie%20Zhou%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Recently%2C%20vision%20model%20pre-training%20has%20evolved%20from%20relying%20on%20manually%0Aannotated%20datasets%20to%20leveraging%20large-scale%2C%20web-crawled%20image-text%20data.%0ADespite%20these%20advances%2C%20there%20is%20no%20pre-training%20method%20that%20effectively%0Aexploits%20the%20interleaved%20image-text%20data%2C%20which%20is%20very%20prevalent%20on%20the%0AInternet.%20Inspired%20by%20the%20recent%20success%20of%20compression%20learning%20in%20natural%0Alanguage%20processing%2C%20we%20propose%20a%20novel%20vision%20model%20pre-training%20method%20called%0ALatent%20Compression%20Learning%20%28LCL%29%20for%20interleaved%20image-text%20data.%20This%20method%0Aperforms%20latent%20compression%20learning%20by%20maximizing%20the%20mutual%20information%0Abetween%20the%20inputs%20and%20outputs%20of%20a%20causal%20attention%20model.%20The%20training%0Aobjective%20can%20be%20decomposed%20into%20two%20basic%20tasks%3A%201%29%20contrastive%20learning%0Abetween%20visual%20representation%20and%20preceding%20context%2C%20and%202%29%20generating%0Asubsequent%20text%20based%20on%20visual%20representation.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20matches%20the%20performance%20of%20CLIP%20on%20paired%20pre-training%0Adatasets%20%28e.g.%2C%20LAION%29%2C%20but%20can%20also%20leverage%20interleaved%20pre-training%20data%0A%28e.g.%2C%20MMC4%29%20to%20learn%20robust%20visual%20representation%20from%20scratch%2C%20showcasing%20the%0Apotential%20of%20vision%20model%20pre-training%20with%20interleaved%20image-text%20data.%20Code%0Ais%20released%20at%20https%3A//github.com/OpenGVLab/LCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Model%2520Pre-training%2520on%2520Interleaved%2520Image-Text%2520Data%2520via%2520Latent%250A%2520%2520Compression%2520Learning%26entry.906535625%3DChenyu%2520Yang%2520and%2520Xizhou%2520Zhu%2520and%2520Jinguo%2520Zhu%2520and%2520Weijie%2520Su%2520and%2520Junjie%2520Wang%2520and%2520Xuan%2520Dong%2520and%2520Wenhai%2520Wang%2520and%2520Lewei%2520Lu%2520and%2520Bin%2520Li%2520and%2520Jie%2520Zhou%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Recently%252C%2520vision%2520model%2520pre-training%2520has%2520evolved%2520from%2520relying%2520on%2520manually%250Aannotated%2520datasets%2520to%2520leveraging%2520large-scale%252C%2520web-crawled%2520image-text%2520data.%250ADespite%2520these%2520advances%252C%2520there%2520is%2520no%2520pre-training%2520method%2520that%2520effectively%250Aexploits%2520the%2520interleaved%2520image-text%2520data%252C%2520which%2520is%2520very%2520prevalent%2520on%2520the%250AInternet.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520compression%2520learning%2520in%2520natural%250Alanguage%2520processing%252C%2520we%2520propose%2520a%2520novel%2520vision%2520model%2520pre-training%2520method%2520called%250ALatent%2520Compression%2520Learning%2520%2528LCL%2529%2520for%2520interleaved%2520image-text%2520data.%2520This%2520method%250Aperforms%2520latent%2520compression%2520learning%2520by%2520maximizing%2520the%2520mutual%2520information%250Abetween%2520the%2520inputs%2520and%2520outputs%2520of%2520a%2520causal%2520attention%2520model.%2520The%2520training%250Aobjective%2520can%2520be%2520decomposed%2520into%2520two%2520basic%2520tasks%253A%25201%2529%2520contrastive%2520learning%250Abetween%2520visual%2520representation%2520and%2520preceding%2520context%252C%2520and%25202%2529%2520generating%250Asubsequent%2520text%2520based%2520on%2520visual%2520representation.%2520Our%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520not%2520only%2520matches%2520the%2520performance%2520of%2520CLIP%2520on%2520paired%2520pre-training%250Adatasets%2520%2528e.g.%252C%2520LAION%2529%252C%2520but%2520can%2520also%2520leverage%2520interleaved%2520pre-training%2520data%250A%2528e.g.%252C%2520MMC4%2529%2520to%2520learn%2520robust%2520visual%2520representation%2520from%2520scratch%252C%2520showcasing%2520the%250Apotential%2520of%2520vision%2520model%2520pre-training%2520with%2520interleaved%2520image-text%2520data.%2520Code%250Ais%2520released%2520at%2520https%253A//github.com/OpenGVLab/LCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning&entry.906535625=Chenyu%20Yang%20and%20Xizhou%20Zhu%20and%20Jinguo%20Zhu%20and%20Weijie%20Su%20and%20Junjie%20Wang%20and%20Xuan%20Dong%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Bin%20Li%20and%20Jie%20Zhou%20and%20Yu%20Qiao%20and%20Jifeng%20Dai&entry.1292438233=%20%20Recently%2C%20vision%20model%20pre-training%20has%20evolved%20from%20relying%20on%20manually%0Aannotated%20datasets%20to%20leveraging%20large-scale%2C%20web-crawled%20image-text%20data.%0ADespite%20these%20advances%2C%20there%20is%20no%20pre-training%20method%20that%20effectively%0Aexploits%20the%20interleaved%20image-text%20data%2C%20which%20is%20very%20prevalent%20on%20the%0AInternet.%20Inspired%20by%20the%20recent%20success%20of%20compression%20learning%20in%20natural%0Alanguage%20processing%2C%20we%20propose%20a%20novel%20vision%20model%20pre-training%20method%20called%0ALatent%20Compression%20Learning%20%28LCL%29%20for%20interleaved%20image-text%20data.%20This%20method%0Aperforms%20latent%20compression%20learning%20by%20maximizing%20the%20mutual%20information%0Abetween%20the%20inputs%20and%20outputs%20of%20a%20causal%20attention%20model.%20The%20training%0Aobjective%20can%20be%20decomposed%20into%20two%20basic%20tasks%3A%201%29%20contrastive%20learning%0Abetween%20visual%20representation%20and%20preceding%20context%2C%20and%202%29%20generating%0Asubsequent%20text%20based%20on%20visual%20representation.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20matches%20the%20performance%20of%20CLIP%20on%20paired%20pre-training%0Adatasets%20%28e.g.%2C%20LAION%29%2C%20but%20can%20also%20leverage%20interleaved%20pre-training%20data%0A%28e.g.%2C%20MMC4%29%20to%20learn%20robust%20visual%20representation%20from%20scratch%2C%20showcasing%20the%0Apotential%20of%20vision%20model%20pre-training%20with%20interleaved%20image-text%20data.%20Code%0Ais%20released%20at%20https%3A//github.com/OpenGVLab/LCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07543v2&entry.124074799=Read"},
{"title": "Sparse Point Clouds Assisted Learned Image Compression", "author": "Yiheng Jiang and Haotian Zhang and Li Li and Dong Liu and Zhu Li", "abstract": "  In the field of autonomous driving, a variety of sensor data types exist,\neach representing different modalities of the same scene. Therefore, it is\nfeasible to utilize data from other sensors to facilitate image compression.\nHowever, few techniques have explored the potential benefits of utilizing\ninter-modality correlations to enhance the image compression performance. In\nthis paper, motivated by the recent success of learned image compression, we\npropose a new framework that uses sparse point clouds to assist in learned\nimage compression in the autonomous driving scenario. We first project the 3D\nsparse point cloud onto a 2D plane, resulting in a sparse depth map. Utilizing\nthis depth map, we proceed to predict camera images. Subsequently, we use these\npredicted images to extract multi-scale structural features. These features are\nthen incorporated into learned image compression pipeline as additional\ninformation to improve the compression performance. Our proposed framework is\ncompatible with various mainstream learned image compression models, and we\nvalidate our approach using different existing image compression methods. The\nexperimental results show that incorporating point cloud assistance into the\ncompression pipeline consistently enhances the performance.\n", "link": "http://arxiv.org/abs/2412.15752v1", "date": "2024-12-20", "relevancy": 2.9592, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6409}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5942}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Point%20Clouds%20Assisted%20Learned%20Image%20Compression&body=Title%3A%20Sparse%20Point%20Clouds%20Assisted%20Learned%20Image%20Compression%0AAuthor%3A%20Yiheng%20Jiang%20and%20Haotian%20Zhang%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Zhu%20Li%0AAbstract%3A%20%20%20In%20the%20field%20of%20autonomous%20driving%2C%20a%20variety%20of%20sensor%20data%20types%20exist%2C%0Aeach%20representing%20different%20modalities%20of%20the%20same%20scene.%20Therefore%2C%20it%20is%0Afeasible%20to%20utilize%20data%20from%20other%20sensors%20to%20facilitate%20image%20compression.%0AHowever%2C%20few%20techniques%20have%20explored%20the%20potential%20benefits%20of%20utilizing%0Ainter-modality%20correlations%20to%20enhance%20the%20image%20compression%20performance.%20In%0Athis%20paper%2C%20motivated%20by%20the%20recent%20success%20of%20learned%20image%20compression%2C%20we%0Apropose%20a%20new%20framework%20that%20uses%20sparse%20point%20clouds%20to%20assist%20in%20learned%0Aimage%20compression%20in%20the%20autonomous%20driving%20scenario.%20We%20first%20project%20the%203D%0Asparse%20point%20cloud%20onto%20a%202D%20plane%2C%20resulting%20in%20a%20sparse%20depth%20map.%20Utilizing%0Athis%20depth%20map%2C%20we%20proceed%20to%20predict%20camera%20images.%20Subsequently%2C%20we%20use%20these%0Apredicted%20images%20to%20extract%20multi-scale%20structural%20features.%20These%20features%20are%0Athen%20incorporated%20into%20learned%20image%20compression%20pipeline%20as%20additional%0Ainformation%20to%20improve%20the%20compression%20performance.%20Our%20proposed%20framework%20is%0Acompatible%20with%20various%20mainstream%20learned%20image%20compression%20models%2C%20and%20we%0Avalidate%20our%20approach%20using%20different%20existing%20image%20compression%20methods.%20The%0Aexperimental%20results%20show%20that%20incorporating%20point%20cloud%20assistance%20into%20the%0Acompression%20pipeline%20consistently%20enhances%20the%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Point%2520Clouds%2520Assisted%2520Learned%2520Image%2520Compression%26entry.906535625%3DYiheng%2520Jiang%2520and%2520Haotian%2520Zhang%2520and%2520Li%2520Li%2520and%2520Dong%2520Liu%2520and%2520Zhu%2520Li%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520autonomous%2520driving%252C%2520a%2520variety%2520of%2520sensor%2520data%2520types%2520exist%252C%250Aeach%2520representing%2520different%2520modalities%2520of%2520the%2520same%2520scene.%2520Therefore%252C%2520it%2520is%250Afeasible%2520to%2520utilize%2520data%2520from%2520other%2520sensors%2520to%2520facilitate%2520image%2520compression.%250AHowever%252C%2520few%2520techniques%2520have%2520explored%2520the%2520potential%2520benefits%2520of%2520utilizing%250Ainter-modality%2520correlations%2520to%2520enhance%2520the%2520image%2520compression%2520performance.%2520In%250Athis%2520paper%252C%2520motivated%2520by%2520the%2520recent%2520success%2520of%2520learned%2520image%2520compression%252C%2520we%250Apropose%2520a%2520new%2520framework%2520that%2520uses%2520sparse%2520point%2520clouds%2520to%2520assist%2520in%2520learned%250Aimage%2520compression%2520in%2520the%2520autonomous%2520driving%2520scenario.%2520We%2520first%2520project%2520the%25203D%250Asparse%2520point%2520cloud%2520onto%2520a%25202D%2520plane%252C%2520resulting%2520in%2520a%2520sparse%2520depth%2520map.%2520Utilizing%250Athis%2520depth%2520map%252C%2520we%2520proceed%2520to%2520predict%2520camera%2520images.%2520Subsequently%252C%2520we%2520use%2520these%250Apredicted%2520images%2520to%2520extract%2520multi-scale%2520structural%2520features.%2520These%2520features%2520are%250Athen%2520incorporated%2520into%2520learned%2520image%2520compression%2520pipeline%2520as%2520additional%250Ainformation%2520to%2520improve%2520the%2520compression%2520performance.%2520Our%2520proposed%2520framework%2520is%250Acompatible%2520with%2520various%2520mainstream%2520learned%2520image%2520compression%2520models%252C%2520and%2520we%250Avalidate%2520our%2520approach%2520using%2520different%2520existing%2520image%2520compression%2520methods.%2520The%250Aexperimental%2520results%2520show%2520that%2520incorporating%2520point%2520cloud%2520assistance%2520into%2520the%250Acompression%2520pipeline%2520consistently%2520enhances%2520the%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Point%20Clouds%20Assisted%20Learned%20Image%20Compression&entry.906535625=Yiheng%20Jiang%20and%20Haotian%20Zhang%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Zhu%20Li&entry.1292438233=%20%20In%20the%20field%20of%20autonomous%20driving%2C%20a%20variety%20of%20sensor%20data%20types%20exist%2C%0Aeach%20representing%20different%20modalities%20of%20the%20same%20scene.%20Therefore%2C%20it%20is%0Afeasible%20to%20utilize%20data%20from%20other%20sensors%20to%20facilitate%20image%20compression.%0AHowever%2C%20few%20techniques%20have%20explored%20the%20potential%20benefits%20of%20utilizing%0Ainter-modality%20correlations%20to%20enhance%20the%20image%20compression%20performance.%20In%0Athis%20paper%2C%20motivated%20by%20the%20recent%20success%20of%20learned%20image%20compression%2C%20we%0Apropose%20a%20new%20framework%20that%20uses%20sparse%20point%20clouds%20to%20assist%20in%20learned%0Aimage%20compression%20in%20the%20autonomous%20driving%20scenario.%20We%20first%20project%20the%203D%0Asparse%20point%20cloud%20onto%20a%202D%20plane%2C%20resulting%20in%20a%20sparse%20depth%20map.%20Utilizing%0Athis%20depth%20map%2C%20we%20proceed%20to%20predict%20camera%20images.%20Subsequently%2C%20we%20use%20these%0Apredicted%20images%20to%20extract%20multi-scale%20structural%20features.%20These%20features%20are%0Athen%20incorporated%20into%20learned%20image%20compression%20pipeline%20as%20additional%0Ainformation%20to%20improve%20the%20compression%20performance.%20Our%20proposed%20framework%20is%0Acompatible%20with%20various%20mainstream%20learned%20image%20compression%20models%2C%20and%20we%0Avalidate%20our%20approach%20using%20different%20existing%20image%20compression%20methods.%20The%0Aexperimental%20results%20show%20that%20incorporating%20point%20cloud%20assistance%20into%20the%0Acompression%20pipeline%20consistently%20enhances%20the%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15752v1&entry.124074799=Read"},
{"title": "Rethinking Visual Dependency in Long-Context Reasoning for Large\n  Vision-Language Models", "author": "Yucheng Zhou and Zhi Rao and Jun Wan and Jianbing Shen", "abstract": "  Large Vision-Language Models (LVLMs) excel in cross-model tasks but\nexperience performance declines in long-context reasoning due to overreliance\non textual information and reduced visual dependency. In this study, we\nempirically analyze LVLMs in long-context reasoning, revealing that increased\ncontext length leads to a higher dependence on language at the expense of\nvisual dependency. To address this issue, we propose a novel training-free\ncontext pruning method that selectively removes less critical textual\ninformation. Our approach enhances visual dependency and reduces textual noise,\nthereby improving LVLM performance in long-context reasoning. We validate our\nmethod by constructing a long-context dataset, demonstrating its effectiveness\nacross various LVLMs. Moreover, further analysis confirms the robustness of\ndifferent token pruning strategies and preliminary explores scaling laws\nbetween pruning rates and context length.\n", "link": "http://arxiv.org/abs/2410.19732v2", "date": "2024-12-20", "relevancy": 2.9189, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Yucheng%20Zhou%20and%20Zhi%20Rao%20and%20Jun%20Wan%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20cross-model%20tasks%20but%0Aexperience%20performance%20declines%20in%20long-context%20reasoning%20due%20to%20overreliance%0Aon%20textual%20information%20and%20reduced%20visual%20dependency.%20In%20this%20study%2C%20we%0Aempirically%20analyze%20LVLMs%20in%20long-context%20reasoning%2C%20revealing%20that%20increased%0Acontext%20length%20leads%20to%20a%20higher%20dependence%20on%20language%20at%20the%20expense%20of%0Avisual%20dependency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20training-free%0Acontext%20pruning%20method%20that%20selectively%20removes%20less%20critical%20textual%0Ainformation.%20Our%20approach%20enhances%20visual%20dependency%20and%20reduces%20textual%20noise%2C%0Athereby%20improving%20LVLM%20performance%20in%20long-context%20reasoning.%20We%20validate%20our%0Amethod%20by%20constructing%20a%20long-context%20dataset%2C%20demonstrating%20its%20effectiveness%0Aacross%20various%20LVLMs.%20Moreover%2C%20further%20analysis%20confirms%20the%20robustness%20of%0Adifferent%20token%20pruning%20strategies%20and%20preliminary%20explores%20scaling%20laws%0Abetween%20pruning%20rates%20and%20context%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Visual%2520Dependency%2520in%2520Long-Context%2520Reasoning%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYucheng%2520Zhou%2520and%2520Zhi%2520Rao%2520and%2520Jun%2520Wan%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520excel%2520in%2520cross-model%2520tasks%2520but%250Aexperience%2520performance%2520declines%2520in%2520long-context%2520reasoning%2520due%2520to%2520overreliance%250Aon%2520textual%2520information%2520and%2520reduced%2520visual%2520dependency.%2520In%2520this%2520study%252C%2520we%250Aempirically%2520analyze%2520LVLMs%2520in%2520long-context%2520reasoning%252C%2520revealing%2520that%2520increased%250Acontext%2520length%2520leads%2520to%2520a%2520higher%2520dependence%2520on%2520language%2520at%2520the%2520expense%2520of%250Avisual%2520dependency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520training-free%250Acontext%2520pruning%2520method%2520that%2520selectively%2520removes%2520less%2520critical%2520textual%250Ainformation.%2520Our%2520approach%2520enhances%2520visual%2520dependency%2520and%2520reduces%2520textual%2520noise%252C%250Athereby%2520improving%2520LVLM%2520performance%2520in%2520long-context%2520reasoning.%2520We%2520validate%2520our%250Amethod%2520by%2520constructing%2520a%2520long-context%2520dataset%252C%2520demonstrating%2520its%2520effectiveness%250Aacross%2520various%2520LVLMs.%2520Moreover%252C%2520further%2520analysis%2520confirms%2520the%2520robustness%2520of%250Adifferent%2520token%2520pruning%2520strategies%2520and%2520preliminary%2520explores%2520scaling%2520laws%250Abetween%2520pruning%2520rates%2520and%2520context%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Visual%20Dependency%20in%20Long-Context%20Reasoning%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Yucheng%20Zhou%20and%20Zhi%20Rao%20and%20Jun%20Wan%20and%20Jianbing%20Shen&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20in%20cross-model%20tasks%20but%0Aexperience%20performance%20declines%20in%20long-context%20reasoning%20due%20to%20overreliance%0Aon%20textual%20information%20and%20reduced%20visual%20dependency.%20In%20this%20study%2C%20we%0Aempirically%20analyze%20LVLMs%20in%20long-context%20reasoning%2C%20revealing%20that%20increased%0Acontext%20length%20leads%20to%20a%20higher%20dependence%20on%20language%20at%20the%20expense%20of%0Avisual%20dependency.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20training-free%0Acontext%20pruning%20method%20that%20selectively%20removes%20less%20critical%20textual%0Ainformation.%20Our%20approach%20enhances%20visual%20dependency%20and%20reduces%20textual%20noise%2C%0Athereby%20improving%20LVLM%20performance%20in%20long-context%20reasoning.%20We%20validate%20our%0Amethod%20by%20constructing%20a%20long-context%20dataset%2C%20demonstrating%20its%20effectiveness%0Aacross%20various%20LVLMs.%20Moreover%2C%20further%20analysis%20confirms%20the%20robustness%20of%0Adifferent%20token%20pruning%20strategies%20and%20preliminary%20explores%20scaling%20laws%0Abetween%20pruning%20rates%20and%20context%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19732v2&entry.124074799=Read"},
{"title": "CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR\n  Sequences", "author": "Akshit Singh and Karan Bhakuni and Rajendra Nagar", "abstract": "  Neural distance fields (NDF) have emerged as a powerful tool for addressing\nchallenges in 3D computer vision and graphics downstream problems. While\nsignificant progress has been made to learn NDF from various kind of sensor\ndata, a crucial aspect that demands attention is the supervision of neural\nfields during training as the ground-truth NDFs are not available for\nlarge-scale outdoor scenes. Previous works have utilized various forms of\nexpected signed distance to guide model learning. Yet, these approaches often\nneed to pay more attention to critical considerations of surface geometry and\nare limited to small-scale implementations. To this end, we propose a novel\nmethodology leveraging second-order derivatives of the signed distance field\nfor improved neural field learning. Our approach addresses limitations by\naccurately estimating signed distance, offering a more comprehensive\nunderstanding of underlying geometry. To assess the efficacy of our\nmethodology, we conducted comparative evaluations against prevalent methods for\nmapping and localization tasks, which are primary application areas of NDF. Our\nresults demonstrate the superiority of the proposed approach, highlighting its\npotential for advancing the capabilities of neural distance fields in computer\nvision and graphics applications.\n", "link": "http://arxiv.org/abs/2412.15909v1", "date": "2024-12-20", "relevancy": 2.8712, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCNDF%3A%20Curvature%20Constrained%20Neural%20Distance%20Fields%20from%203D%20LiDAR%0A%20%20Sequences&body=Title%3A%20CCNDF%3A%20Curvature%20Constrained%20Neural%20Distance%20Fields%20from%203D%20LiDAR%0A%20%20Sequences%0AAuthor%3A%20Akshit%20Singh%20and%20Karan%20Bhakuni%20and%20Rajendra%20Nagar%0AAbstract%3A%20%20%20Neural%20distance%20fields%20%28NDF%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20addressing%0Achallenges%20in%203D%20computer%20vision%20and%20graphics%20downstream%20problems.%20While%0Asignificant%20progress%20has%20been%20made%20to%20learn%20NDF%20from%20various%20kind%20of%20sensor%0Adata%2C%20a%20crucial%20aspect%20that%20demands%20attention%20is%20the%20supervision%20of%20neural%0Afields%20during%20training%20as%20the%20ground-truth%20NDFs%20are%20not%20available%20for%0Alarge-scale%20outdoor%20scenes.%20Previous%20works%20have%20utilized%20various%20forms%20of%0Aexpected%20signed%20distance%20to%20guide%20model%20learning.%20Yet%2C%20these%20approaches%20often%0Aneed%20to%20pay%20more%20attention%20to%20critical%20considerations%20of%20surface%20geometry%20and%0Aare%20limited%20to%20small-scale%20implementations.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Amethodology%20leveraging%20second-order%20derivatives%20of%20the%20signed%20distance%20field%0Afor%20improved%20neural%20field%20learning.%20Our%20approach%20addresses%20limitations%20by%0Aaccurately%20estimating%20signed%20distance%2C%20offering%20a%20more%20comprehensive%0Aunderstanding%20of%20underlying%20geometry.%20To%20assess%20the%20efficacy%20of%20our%0Amethodology%2C%20we%20conducted%20comparative%20evaluations%20against%20prevalent%20methods%20for%0Amapping%20and%20localization%20tasks%2C%20which%20are%20primary%20application%20areas%20of%20NDF.%20Our%0Aresults%20demonstrate%20the%20superiority%20of%20the%20proposed%20approach%2C%20highlighting%20its%0Apotential%20for%20advancing%20the%20capabilities%20of%20neural%20distance%20fields%20in%20computer%0Avision%20and%20graphics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCNDF%253A%2520Curvature%2520Constrained%2520Neural%2520Distance%2520Fields%2520from%25203D%2520LiDAR%250A%2520%2520Sequences%26entry.906535625%3DAkshit%2520Singh%2520and%2520Karan%2520Bhakuni%2520and%2520Rajendra%2520Nagar%26entry.1292438233%3D%2520%2520Neural%2520distance%2520fields%2520%2528NDF%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520addressing%250Achallenges%2520in%25203D%2520computer%2520vision%2520and%2520graphics%2520downstream%2520problems.%2520While%250Asignificant%2520progress%2520has%2520been%2520made%2520to%2520learn%2520NDF%2520from%2520various%2520kind%2520of%2520sensor%250Adata%252C%2520a%2520crucial%2520aspect%2520that%2520demands%2520attention%2520is%2520the%2520supervision%2520of%2520neural%250Afields%2520during%2520training%2520as%2520the%2520ground-truth%2520NDFs%2520are%2520not%2520available%2520for%250Alarge-scale%2520outdoor%2520scenes.%2520Previous%2520works%2520have%2520utilized%2520various%2520forms%2520of%250Aexpected%2520signed%2520distance%2520to%2520guide%2520model%2520learning.%2520Yet%252C%2520these%2520approaches%2520often%250Aneed%2520to%2520pay%2520more%2520attention%2520to%2520critical%2520considerations%2520of%2520surface%2520geometry%2520and%250Aare%2520limited%2520to%2520small-scale%2520implementations.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Amethodology%2520leveraging%2520second-order%2520derivatives%2520of%2520the%2520signed%2520distance%2520field%250Afor%2520improved%2520neural%2520field%2520learning.%2520Our%2520approach%2520addresses%2520limitations%2520by%250Aaccurately%2520estimating%2520signed%2520distance%252C%2520offering%2520a%2520more%2520comprehensive%250Aunderstanding%2520of%2520underlying%2520geometry.%2520To%2520assess%2520the%2520efficacy%2520of%2520our%250Amethodology%252C%2520we%2520conducted%2520comparative%2520evaluations%2520against%2520prevalent%2520methods%2520for%250Amapping%2520and%2520localization%2520tasks%252C%2520which%2520are%2520primary%2520application%2520areas%2520of%2520NDF.%2520Our%250Aresults%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520approach%252C%2520highlighting%2520its%250Apotential%2520for%2520advancing%2520the%2520capabilities%2520of%2520neural%2520distance%2520fields%2520in%2520computer%250Avision%2520and%2520graphics%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCNDF%3A%20Curvature%20Constrained%20Neural%20Distance%20Fields%20from%203D%20LiDAR%0A%20%20Sequences&entry.906535625=Akshit%20Singh%20and%20Karan%20Bhakuni%20and%20Rajendra%20Nagar&entry.1292438233=%20%20Neural%20distance%20fields%20%28NDF%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20addressing%0Achallenges%20in%203D%20computer%20vision%20and%20graphics%20downstream%20problems.%20While%0Asignificant%20progress%20has%20been%20made%20to%20learn%20NDF%20from%20various%20kind%20of%20sensor%0Adata%2C%20a%20crucial%20aspect%20that%20demands%20attention%20is%20the%20supervision%20of%20neural%0Afields%20during%20training%20as%20the%20ground-truth%20NDFs%20are%20not%20available%20for%0Alarge-scale%20outdoor%20scenes.%20Previous%20works%20have%20utilized%20various%20forms%20of%0Aexpected%20signed%20distance%20to%20guide%20model%20learning.%20Yet%2C%20these%20approaches%20often%0Aneed%20to%20pay%20more%20attention%20to%20critical%20considerations%20of%20surface%20geometry%20and%0Aare%20limited%20to%20small-scale%20implementations.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Amethodology%20leveraging%20second-order%20derivatives%20of%20the%20signed%20distance%20field%0Afor%20improved%20neural%20field%20learning.%20Our%20approach%20addresses%20limitations%20by%0Aaccurately%20estimating%20signed%20distance%2C%20offering%20a%20more%20comprehensive%0Aunderstanding%20of%20underlying%20geometry.%20To%20assess%20the%20efficacy%20of%20our%0Amethodology%2C%20we%20conducted%20comparative%20evaluations%20against%20prevalent%20methods%20for%0Amapping%20and%20localization%20tasks%2C%20which%20are%20primary%20application%20areas%20of%20NDF.%20Our%0Aresults%20demonstrate%20the%20superiority%20of%20the%20proposed%20approach%2C%20highlighting%20its%0Apotential%20for%20advancing%20the%20capabilities%20of%20neural%20distance%20fields%20in%20computer%0Avision%20and%20graphics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15909v1&entry.124074799=Read"},
{"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with\n  Holistic Vision-Language Embedding", "author": "Chenxin Tao and Shiqian Su and Xizhou Zhu and Chenyu Zhang and Zhe Chen and Jiawen Liu and Wenhai Wang and Lewei Lu and Gao Huang and Yu Qiao and Jifeng Dai", "abstract": "  The rapid advance of Large Language Models (LLMs) has catalyzed the\ndevelopment of Vision-Language Models (VLMs). Monolithic VLMs, which avoid\nmodality-specific encoders, offer a promising alternative to the compositional\nones but face the challenge of inferior performance. Most existing monolithic\nVLMs require tuning pre-trained LLMs to acquire vision abilities, which may\ndegrade their language capabilities. To address this dilemma, this paper\npresents a novel high-performance monolithic VLM named HoVLE. We note that LLMs\nhave been shown capable of interpreting images, when image embeddings are\naligned with text embeddings. The challenge for current monolithic VLMs\nactually lies in the lack of a holistic embedding module for both vision and\nlanguage inputs. Therefore, HoVLE introduces a holistic embedding module that\nconverts visual and textual inputs into a shared space, allowing LLMs to\nprocess images in the same way as texts. Furthermore, a multi-stage training\nstrategy is carefully designed to empower the holistic embedding module. It is\nfirst trained to distill visual features from a pre-trained vision encoder and\ntext embeddings from the LLM, enabling large-scale training with unpaired\nrandom images and text tokens. The whole model further undergoes next-token\nprediction on multi-modal data to align the embeddings. Finally, an\ninstruction-tuning stage is incorporated. Our experiments show that HoVLE\nachieves performance close to leading compositional models on various\nbenchmarks, outperforming previous monolithic models by a large margin. Model\navailable at https://huggingface.co/OpenGVLab/HoVLE.\n", "link": "http://arxiv.org/abs/2412.16158v1", "date": "2024-12-20", "relevancy": 2.8299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoVLE%3A%20Unleashing%20the%20Power%20of%20Monolithic%20Vision-Language%20Models%20with%0A%20%20Holistic%20Vision-Language%20Embedding&body=Title%3A%20HoVLE%3A%20Unleashing%20the%20Power%20of%20Monolithic%20Vision-Language%20Models%20with%0A%20%20Holistic%20Vision-Language%20Embedding%0AAuthor%3A%20Chenxin%20Tao%20and%20Shiqian%20Su%20and%20Xizhou%20Zhu%20and%20Chenyu%20Zhang%20and%20Zhe%20Chen%20and%20Jiawen%20Liu%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Gao%20Huang%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20The%20rapid%20advance%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20the%0Adevelopment%20of%20Vision-Language%20Models%20%28VLMs%29.%20Monolithic%20VLMs%2C%20which%20avoid%0Amodality-specific%20encoders%2C%20offer%20a%20promising%20alternative%20to%20the%20compositional%0Aones%20but%20face%20the%20challenge%20of%20inferior%20performance.%20Most%20existing%20monolithic%0AVLMs%20require%20tuning%20pre-trained%20LLMs%20to%20acquire%20vision%20abilities%2C%20which%20may%0Adegrade%20their%20language%20capabilities.%20To%20address%20this%20dilemma%2C%20this%20paper%0Apresents%20a%20novel%20high-performance%20monolithic%20VLM%20named%20HoVLE.%20We%20note%20that%20LLMs%0Ahave%20been%20shown%20capable%20of%20interpreting%20images%2C%20when%20image%20embeddings%20are%0Aaligned%20with%20text%20embeddings.%20The%20challenge%20for%20current%20monolithic%20VLMs%0Aactually%20lies%20in%20the%20lack%20of%20a%20holistic%20embedding%20module%20for%20both%20vision%20and%0Alanguage%20inputs.%20Therefore%2C%20HoVLE%20introduces%20a%20holistic%20embedding%20module%20that%0Aconverts%20visual%20and%20textual%20inputs%20into%20a%20shared%20space%2C%20allowing%20LLMs%20to%0Aprocess%20images%20in%20the%20same%20way%20as%20texts.%20Furthermore%2C%20a%20multi-stage%20training%0Astrategy%20is%20carefully%20designed%20to%20empower%20the%20holistic%20embedding%20module.%20It%20is%0Afirst%20trained%20to%20distill%20visual%20features%20from%20a%20pre-trained%20vision%20encoder%20and%0Atext%20embeddings%20from%20the%20LLM%2C%20enabling%20large-scale%20training%20with%20unpaired%0Arandom%20images%20and%20text%20tokens.%20The%20whole%20model%20further%20undergoes%20next-token%0Aprediction%20on%20multi-modal%20data%20to%20align%20the%20embeddings.%20Finally%2C%20an%0Ainstruction-tuning%20stage%20is%20incorporated.%20Our%20experiments%20show%20that%20HoVLE%0Aachieves%20performance%20close%20to%20leading%20compositional%20models%20on%20various%0Abenchmarks%2C%20outperforming%20previous%20monolithic%20models%20by%20a%20large%20margin.%20Model%0Aavailable%20at%20https%3A//huggingface.co/OpenGVLab/HoVLE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoVLE%253A%2520Unleashing%2520the%2520Power%2520of%2520Monolithic%2520Vision-Language%2520Models%2520with%250A%2520%2520Holistic%2520Vision-Language%2520Embedding%26entry.906535625%3DChenxin%2520Tao%2520and%2520Shiqian%2520Su%2520and%2520Xizhou%2520Zhu%2520and%2520Chenyu%2520Zhang%2520and%2520Zhe%2520Chen%2520and%2520Jiawen%2520Liu%2520and%2520Wenhai%2520Wang%2520and%2520Lewei%2520Lu%2520and%2520Gao%2520Huang%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520The%2520rapid%2520advance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520catalyzed%2520the%250Adevelopment%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520Monolithic%2520VLMs%252C%2520which%2520avoid%250Amodality-specific%2520encoders%252C%2520offer%2520a%2520promising%2520alternative%2520to%2520the%2520compositional%250Aones%2520but%2520face%2520the%2520challenge%2520of%2520inferior%2520performance.%2520Most%2520existing%2520monolithic%250AVLMs%2520require%2520tuning%2520pre-trained%2520LLMs%2520to%2520acquire%2520vision%2520abilities%252C%2520which%2520may%250Adegrade%2520their%2520language%2520capabilities.%2520To%2520address%2520this%2520dilemma%252C%2520this%2520paper%250Apresents%2520a%2520novel%2520high-performance%2520monolithic%2520VLM%2520named%2520HoVLE.%2520We%2520note%2520that%2520LLMs%250Ahave%2520been%2520shown%2520capable%2520of%2520interpreting%2520images%252C%2520when%2520image%2520embeddings%2520are%250Aaligned%2520with%2520text%2520embeddings.%2520The%2520challenge%2520for%2520current%2520monolithic%2520VLMs%250Aactually%2520lies%2520in%2520the%2520lack%2520of%2520a%2520holistic%2520embedding%2520module%2520for%2520both%2520vision%2520and%250Alanguage%2520inputs.%2520Therefore%252C%2520HoVLE%2520introduces%2520a%2520holistic%2520embedding%2520module%2520that%250Aconverts%2520visual%2520and%2520textual%2520inputs%2520into%2520a%2520shared%2520space%252C%2520allowing%2520LLMs%2520to%250Aprocess%2520images%2520in%2520the%2520same%2520way%2520as%2520texts.%2520Furthermore%252C%2520a%2520multi-stage%2520training%250Astrategy%2520is%2520carefully%2520designed%2520to%2520empower%2520the%2520holistic%2520embedding%2520module.%2520It%2520is%250Afirst%2520trained%2520to%2520distill%2520visual%2520features%2520from%2520a%2520pre-trained%2520vision%2520encoder%2520and%250Atext%2520embeddings%2520from%2520the%2520LLM%252C%2520enabling%2520large-scale%2520training%2520with%2520unpaired%250Arandom%2520images%2520and%2520text%2520tokens.%2520The%2520whole%2520model%2520further%2520undergoes%2520next-token%250Aprediction%2520on%2520multi-modal%2520data%2520to%2520align%2520the%2520embeddings.%2520Finally%252C%2520an%250Ainstruction-tuning%2520stage%2520is%2520incorporated.%2520Our%2520experiments%2520show%2520that%2520HoVLE%250Aachieves%2520performance%2520close%2520to%2520leading%2520compositional%2520models%2520on%2520various%250Abenchmarks%252C%2520outperforming%2520previous%2520monolithic%2520models%2520by%2520a%2520large%2520margin.%2520Model%250Aavailable%2520at%2520https%253A//huggingface.co/OpenGVLab/HoVLE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoVLE%3A%20Unleashing%20the%20Power%20of%20Monolithic%20Vision-Language%20Models%20with%0A%20%20Holistic%20Vision-Language%20Embedding&entry.906535625=Chenxin%20Tao%20and%20Shiqian%20Su%20and%20Xizhou%20Zhu%20and%20Chenyu%20Zhang%20and%20Zhe%20Chen%20and%20Jiawen%20Liu%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Gao%20Huang%20and%20Yu%20Qiao%20and%20Jifeng%20Dai&entry.1292438233=%20%20The%20rapid%20advance%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20the%0Adevelopment%20of%20Vision-Language%20Models%20%28VLMs%29.%20Monolithic%20VLMs%2C%20which%20avoid%0Amodality-specific%20encoders%2C%20offer%20a%20promising%20alternative%20to%20the%20compositional%0Aones%20but%20face%20the%20challenge%20of%20inferior%20performance.%20Most%20existing%20monolithic%0AVLMs%20require%20tuning%20pre-trained%20LLMs%20to%20acquire%20vision%20abilities%2C%20which%20may%0Adegrade%20their%20language%20capabilities.%20To%20address%20this%20dilemma%2C%20this%20paper%0Apresents%20a%20novel%20high-performance%20monolithic%20VLM%20named%20HoVLE.%20We%20note%20that%20LLMs%0Ahave%20been%20shown%20capable%20of%20interpreting%20images%2C%20when%20image%20embeddings%20are%0Aaligned%20with%20text%20embeddings.%20The%20challenge%20for%20current%20monolithic%20VLMs%0Aactually%20lies%20in%20the%20lack%20of%20a%20holistic%20embedding%20module%20for%20both%20vision%20and%0Alanguage%20inputs.%20Therefore%2C%20HoVLE%20introduces%20a%20holistic%20embedding%20module%20that%0Aconverts%20visual%20and%20textual%20inputs%20into%20a%20shared%20space%2C%20allowing%20LLMs%20to%0Aprocess%20images%20in%20the%20same%20way%20as%20texts.%20Furthermore%2C%20a%20multi-stage%20training%0Astrategy%20is%20carefully%20designed%20to%20empower%20the%20holistic%20embedding%20module.%20It%20is%0Afirst%20trained%20to%20distill%20visual%20features%20from%20a%20pre-trained%20vision%20encoder%20and%0Atext%20embeddings%20from%20the%20LLM%2C%20enabling%20large-scale%20training%20with%20unpaired%0Arandom%20images%20and%20text%20tokens.%20The%20whole%20model%20further%20undergoes%20next-token%0Aprediction%20on%20multi-modal%20data%20to%20align%20the%20embeddings.%20Finally%2C%20an%0Ainstruction-tuning%20stage%20is%20incorporated.%20Our%20experiments%20show%20that%20HoVLE%0Aachieves%20performance%20close%20to%20leading%20compositional%20models%20on%20various%0Abenchmarks%2C%20outperforming%20previous%20monolithic%20models%20by%20a%20large%20margin.%20Model%0Aavailable%20at%20https%3A//huggingface.co/OpenGVLab/HoVLE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16158v1&entry.124074799=Read"},
{"title": "Align Anything: Training All-Modality Models to Follow Instructions with\n  Language Feedback", "author": "Jiaming Ji and Jiayi Zhou and Hantao Lou and Boyuan Chen and Donghai Hong and Xuyao Wang and Wenqi Chen and Kaile Wang and Rui Pan and Jiahao Li and Mohan Wang and Josef Dai and Tianyi Qiu and Hua Xu and Dong Li and Weipeng Chen and Jun Song and Bo Zheng and Yaodong Yang", "abstract": "  Reinforcement learning from human feedback (RLHF) has proven effective in\nenhancing the instruction-following capabilities of large language models;\nhowever, it remains underexplored in the cross-modality domain. As the number\nof modalities increases, aligning all-modality models with human intentions --\nsuch as instruction following -- becomes a pressing challenge. In this work, we\nmake the first attempt to fine-tune all-modality models (i.e. input and output\nwith any modality, also named any-to-any models) using human preference data\nacross all modalities (including text, image, audio, and video), ensuring its\nbehavior aligns with human intentions. This endeavor presents several\nchallenges. First, there is no large-scale all-modality human preference data\nin existing open-source resources, as most datasets are limited to specific\nmodalities, predominantly text and image. Secondly, the effectiveness of binary\npreferences in RLHF for post-training alignment in complex all-modality\nscenarios remains an unexplored area. Finally, there is a lack of a systematic\nframework to evaluate the capabilities of all-modality models, particularly\nregarding modality selection and synergy. To address these challenges, we\npropose the align-anything framework, which includes meticulously annotated\n200k all-modality human preference data. Then, we introduce an alignment method\nthat learns from unified language feedback, effectively capturing complex\nmodality-specific human preferences and enhancing the model's\ninstruction-following capabilities. Furthermore, to assess performance\nimprovements in all-modality models after post-training alignment, we construct\na challenging all-modality capability evaluation framework -- eval-anything.\nAll data, models, and code frameworks have been open-sourced for the community.\nFor more details, please refer to\nhttps://github.com/PKU-Alignment/align-anything.\n", "link": "http://arxiv.org/abs/2412.15838v1", "date": "2024-12-20", "relevancy": 2.795, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align%20Anything%3A%20Training%20All-Modality%20Models%20to%20Follow%20Instructions%20with%0A%20%20Language%20Feedback&body=Title%3A%20Align%20Anything%3A%20Training%20All-Modality%20Models%20to%20Follow%20Instructions%20with%0A%20%20Language%20Feedback%0AAuthor%3A%20Jiaming%20Ji%20and%20Jiayi%20Zhou%20and%20Hantao%20Lou%20and%20Boyuan%20Chen%20and%20Donghai%20Hong%20and%20Xuyao%20Wang%20and%20Wenqi%20Chen%20and%20Kaile%20Wang%20and%20Rui%20Pan%20and%20Jiahao%20Li%20and%20Mohan%20Wang%20and%20Josef%20Dai%20and%20Tianyi%20Qiu%20and%20Hua%20Xu%20and%20Dong%20Li%20and%20Weipeng%20Chen%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Yaodong%20Yang%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20proven%20effective%20in%0Aenhancing%20the%20instruction-following%20capabilities%20of%20large%20language%20models%3B%0Ahowever%2C%20it%20remains%20underexplored%20in%20the%20cross-modality%20domain.%20As%20the%20number%0Aof%20modalities%20increases%2C%20aligning%20all-modality%20models%20with%20human%20intentions%20--%0Asuch%20as%20instruction%20following%20--%20becomes%20a%20pressing%20challenge.%20In%20this%20work%2C%20we%0Amake%20the%20first%20attempt%20to%20fine-tune%20all-modality%20models%20%28i.e.%20input%20and%20output%0Awith%20any%20modality%2C%20also%20named%20any-to-any%20models%29%20using%20human%20preference%20data%0Aacross%20all%20modalities%20%28including%20text%2C%20image%2C%20audio%2C%20and%20video%29%2C%20ensuring%20its%0Abehavior%20aligns%20with%20human%20intentions.%20This%20endeavor%20presents%20several%0Achallenges.%20First%2C%20there%20is%20no%20large-scale%20all-modality%20human%20preference%20data%0Ain%20existing%20open-source%20resources%2C%20as%20most%20datasets%20are%20limited%20to%20specific%0Amodalities%2C%20predominantly%20text%20and%20image.%20Secondly%2C%20the%20effectiveness%20of%20binary%0Apreferences%20in%20RLHF%20for%20post-training%20alignment%20in%20complex%20all-modality%0Ascenarios%20remains%20an%20unexplored%20area.%20Finally%2C%20there%20is%20a%20lack%20of%20a%20systematic%0Aframework%20to%20evaluate%20the%20capabilities%20of%20all-modality%20models%2C%20particularly%0Aregarding%20modality%20selection%20and%20synergy.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20align-anything%20framework%2C%20which%20includes%20meticulously%20annotated%0A200k%20all-modality%20human%20preference%20data.%20Then%2C%20we%20introduce%20an%20alignment%20method%0Athat%20learns%20from%20unified%20language%20feedback%2C%20effectively%20capturing%20complex%0Amodality-specific%20human%20preferences%20and%20enhancing%20the%20model%27s%0Ainstruction-following%20capabilities.%20Furthermore%2C%20to%20assess%20performance%0Aimprovements%20in%20all-modality%20models%20after%20post-training%20alignment%2C%20we%20construct%0Aa%20challenging%20all-modality%20capability%20evaluation%20framework%20--%20eval-anything.%0AAll%20data%2C%20models%2C%20and%20code%20frameworks%20have%20been%20open-sourced%20for%20the%20community.%0AFor%20more%20details%2C%20please%20refer%20to%0Ahttps%3A//github.com/PKU-Alignment/align-anything.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign%2520Anything%253A%2520Training%2520All-Modality%2520Models%2520to%2520Follow%2520Instructions%2520with%250A%2520%2520Language%2520Feedback%26entry.906535625%3DJiaming%2520Ji%2520and%2520Jiayi%2520Zhou%2520and%2520Hantao%2520Lou%2520and%2520Boyuan%2520Chen%2520and%2520Donghai%2520Hong%2520and%2520Xuyao%2520Wang%2520and%2520Wenqi%2520Chen%2520and%2520Kaile%2520Wang%2520and%2520Rui%2520Pan%2520and%2520Jiahao%2520Li%2520and%2520Mohan%2520Wang%2520and%2520Josef%2520Dai%2520and%2520Tianyi%2520Qiu%2520and%2520Hua%2520Xu%2520and%2520Dong%2520Li%2520and%2520Weipeng%2520Chen%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Yaodong%2520Yang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520has%2520proven%2520effective%2520in%250Aenhancing%2520the%2520instruction-following%2520capabilities%2520of%2520large%2520language%2520models%253B%250Ahowever%252C%2520it%2520remains%2520underexplored%2520in%2520the%2520cross-modality%2520domain.%2520As%2520the%2520number%250Aof%2520modalities%2520increases%252C%2520aligning%2520all-modality%2520models%2520with%2520human%2520intentions%2520--%250Asuch%2520as%2520instruction%2520following%2520--%2520becomes%2520a%2520pressing%2520challenge.%2520In%2520this%2520work%252C%2520we%250Amake%2520the%2520first%2520attempt%2520to%2520fine-tune%2520all-modality%2520models%2520%2528i.e.%2520input%2520and%2520output%250Awith%2520any%2520modality%252C%2520also%2520named%2520any-to-any%2520models%2529%2520using%2520human%2520preference%2520data%250Aacross%2520all%2520modalities%2520%2528including%2520text%252C%2520image%252C%2520audio%252C%2520and%2520video%2529%252C%2520ensuring%2520its%250Abehavior%2520aligns%2520with%2520human%2520intentions.%2520This%2520endeavor%2520presents%2520several%250Achallenges.%2520First%252C%2520there%2520is%2520no%2520large-scale%2520all-modality%2520human%2520preference%2520data%250Ain%2520existing%2520open-source%2520resources%252C%2520as%2520most%2520datasets%2520are%2520limited%2520to%2520specific%250Amodalities%252C%2520predominantly%2520text%2520and%2520image.%2520Secondly%252C%2520the%2520effectiveness%2520of%2520binary%250Apreferences%2520in%2520RLHF%2520for%2520post-training%2520alignment%2520in%2520complex%2520all-modality%250Ascenarios%2520remains%2520an%2520unexplored%2520area.%2520Finally%252C%2520there%2520is%2520a%2520lack%2520of%2520a%2520systematic%250Aframework%2520to%2520evaluate%2520the%2520capabilities%2520of%2520all-modality%2520models%252C%2520particularly%250Aregarding%2520modality%2520selection%2520and%2520synergy.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520the%2520align-anything%2520framework%252C%2520which%2520includes%2520meticulously%2520annotated%250A200k%2520all-modality%2520human%2520preference%2520data.%2520Then%252C%2520we%2520introduce%2520an%2520alignment%2520method%250Athat%2520learns%2520from%2520unified%2520language%2520feedback%252C%2520effectively%2520capturing%2520complex%250Amodality-specific%2520human%2520preferences%2520and%2520enhancing%2520the%2520model%2527s%250Ainstruction-following%2520capabilities.%2520Furthermore%252C%2520to%2520assess%2520performance%250Aimprovements%2520in%2520all-modality%2520models%2520after%2520post-training%2520alignment%252C%2520we%2520construct%250Aa%2520challenging%2520all-modality%2520capability%2520evaluation%2520framework%2520--%2520eval-anything.%250AAll%2520data%252C%2520models%252C%2520and%2520code%2520frameworks%2520have%2520been%2520open-sourced%2520for%2520the%2520community.%250AFor%2520more%2520details%252C%2520please%2520refer%2520to%250Ahttps%253A//github.com/PKU-Alignment/align-anything.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20Anything%3A%20Training%20All-Modality%20Models%20to%20Follow%20Instructions%20with%0A%20%20Language%20Feedback&entry.906535625=Jiaming%20Ji%20and%20Jiayi%20Zhou%20and%20Hantao%20Lou%20and%20Boyuan%20Chen%20and%20Donghai%20Hong%20and%20Xuyao%20Wang%20and%20Wenqi%20Chen%20and%20Kaile%20Wang%20and%20Rui%20Pan%20and%20Jiahao%20Li%20and%20Mohan%20Wang%20and%20Josef%20Dai%20and%20Tianyi%20Qiu%20and%20Hua%20Xu%20and%20Dong%20Li%20and%20Weipeng%20Chen%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Yaodong%20Yang&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20proven%20effective%20in%0Aenhancing%20the%20instruction-following%20capabilities%20of%20large%20language%20models%3B%0Ahowever%2C%20it%20remains%20underexplored%20in%20the%20cross-modality%20domain.%20As%20the%20number%0Aof%20modalities%20increases%2C%20aligning%20all-modality%20models%20with%20human%20intentions%20--%0Asuch%20as%20instruction%20following%20--%20becomes%20a%20pressing%20challenge.%20In%20this%20work%2C%20we%0Amake%20the%20first%20attempt%20to%20fine-tune%20all-modality%20models%20%28i.e.%20input%20and%20output%0Awith%20any%20modality%2C%20also%20named%20any-to-any%20models%29%20using%20human%20preference%20data%0Aacross%20all%20modalities%20%28including%20text%2C%20image%2C%20audio%2C%20and%20video%29%2C%20ensuring%20its%0Abehavior%20aligns%20with%20human%20intentions.%20This%20endeavor%20presents%20several%0Achallenges.%20First%2C%20there%20is%20no%20large-scale%20all-modality%20human%20preference%20data%0Ain%20existing%20open-source%20resources%2C%20as%20most%20datasets%20are%20limited%20to%20specific%0Amodalities%2C%20predominantly%20text%20and%20image.%20Secondly%2C%20the%20effectiveness%20of%20binary%0Apreferences%20in%20RLHF%20for%20post-training%20alignment%20in%20complex%20all-modality%0Ascenarios%20remains%20an%20unexplored%20area.%20Finally%2C%20there%20is%20a%20lack%20of%20a%20systematic%0Aframework%20to%20evaluate%20the%20capabilities%20of%20all-modality%20models%2C%20particularly%0Aregarding%20modality%20selection%20and%20synergy.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20align-anything%20framework%2C%20which%20includes%20meticulously%20annotated%0A200k%20all-modality%20human%20preference%20data.%20Then%2C%20we%20introduce%20an%20alignment%20method%0Athat%20learns%20from%20unified%20language%20feedback%2C%20effectively%20capturing%20complex%0Amodality-specific%20human%20preferences%20and%20enhancing%20the%20model%27s%0Ainstruction-following%20capabilities.%20Furthermore%2C%20to%20assess%20performance%0Aimprovements%20in%20all-modality%20models%20after%20post-training%20alignment%2C%20we%20construct%0Aa%20challenging%20all-modality%20capability%20evaluation%20framework%20--%20eval-anything.%0AAll%20data%2C%20models%2C%20and%20code%20frameworks%20have%20been%20open-sourced%20for%20the%20community.%0AFor%20more%20details%2C%20please%20refer%20to%0Ahttps%3A//github.com/PKU-Alignment/align-anything.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15838v1&entry.124074799=Read"},
{"title": "NeuroPump: Simultaneous Geometric and Color Rectification for Underwater\n  Images", "author": "Yue Guo and Haoxiang Liao and Haibin Ling and Bingyao Huang", "abstract": "  Underwater image restoration aims to remove geometric and color distortions\ndue to water refraction, absorption and scattering. Previous studies focus on\nrestoring either color or the geometry, but to our best knowledge, not both.\nHowever, in practice it may be cumbersome to address the two rectifications\none-by-one. In this paper, we propose NeuroPump, a self-supervised method to\nsimultaneously optimize and rectify underwater geometry and color as if water\nwere pumped out. The key idea is to explicitly model refraction, absorption and\nscattering in Neural Radiance Field (NeRF) pipeline, such that it not only\nperforms simultaneous geometric and color rectification, but also enables to\nsynthesize novel views and optical effects by controlling the decoupled\nparameters. In addition, to address issue of lack of real paired ground truth\nimages, we propose an underwater 360 benchmark dataset that has real paired\n(i.e., with and without water) images. Our method clearly outperforms other\nbaselines both quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2412.15890v1", "date": "2024-12-20", "relevancy": 2.7909, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5733}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5569}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroPump%3A%20Simultaneous%20Geometric%20and%20Color%20Rectification%20for%20Underwater%0A%20%20Images&body=Title%3A%20NeuroPump%3A%20Simultaneous%20Geometric%20and%20Color%20Rectification%20for%20Underwater%0A%20%20Images%0AAuthor%3A%20Yue%20Guo%20and%20Haoxiang%20Liao%20and%20Haibin%20Ling%20and%20Bingyao%20Huang%0AAbstract%3A%20%20%20Underwater%20image%20restoration%20aims%20to%20remove%20geometric%20and%20color%20distortions%0Adue%20to%20water%20refraction%2C%20absorption%20and%20scattering.%20Previous%20studies%20focus%20on%0Arestoring%20either%20color%20or%20the%20geometry%2C%20but%20to%20our%20best%20knowledge%2C%20not%20both.%0AHowever%2C%20in%20practice%20it%20may%20be%20cumbersome%20to%20address%20the%20two%20rectifications%0Aone-by-one.%20In%20this%20paper%2C%20we%20propose%20NeuroPump%2C%20a%20self-supervised%20method%20to%0Asimultaneously%20optimize%20and%20rectify%20underwater%20geometry%20and%20color%20as%20if%20water%0Awere%20pumped%20out.%20The%20key%20idea%20is%20to%20explicitly%20model%20refraction%2C%20absorption%20and%0Ascattering%20in%20Neural%20Radiance%20Field%20%28NeRF%29%20pipeline%2C%20such%20that%20it%20not%20only%0Aperforms%20simultaneous%20geometric%20and%20color%20rectification%2C%20but%20also%20enables%20to%0Asynthesize%20novel%20views%20and%20optical%20effects%20by%20controlling%20the%20decoupled%0Aparameters.%20In%20addition%2C%20to%20address%20issue%20of%20lack%20of%20real%20paired%20ground%20truth%0Aimages%2C%20we%20propose%20an%20underwater%20360%20benchmark%20dataset%20that%20has%20real%20paired%0A%28i.e.%2C%20with%20and%20without%20water%29%20images.%20Our%20method%20clearly%20outperforms%20other%0Abaselines%20both%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroPump%253A%2520Simultaneous%2520Geometric%2520and%2520Color%2520Rectification%2520for%2520Underwater%250A%2520%2520Images%26entry.906535625%3DYue%2520Guo%2520and%2520Haoxiang%2520Liao%2520and%2520Haibin%2520Ling%2520and%2520Bingyao%2520Huang%26entry.1292438233%3D%2520%2520Underwater%2520image%2520restoration%2520aims%2520to%2520remove%2520geometric%2520and%2520color%2520distortions%250Adue%2520to%2520water%2520refraction%252C%2520absorption%2520and%2520scattering.%2520Previous%2520studies%2520focus%2520on%250Arestoring%2520either%2520color%2520or%2520the%2520geometry%252C%2520but%2520to%2520our%2520best%2520knowledge%252C%2520not%2520both.%250AHowever%252C%2520in%2520practice%2520it%2520may%2520be%2520cumbersome%2520to%2520address%2520the%2520two%2520rectifications%250Aone-by-one.%2520In%2520this%2520paper%252C%2520we%2520propose%2520NeuroPump%252C%2520a%2520self-supervised%2520method%2520to%250Asimultaneously%2520optimize%2520and%2520rectify%2520underwater%2520geometry%2520and%2520color%2520as%2520if%2520water%250Awere%2520pumped%2520out.%2520The%2520key%2520idea%2520is%2520to%2520explicitly%2520model%2520refraction%252C%2520absorption%2520and%250Ascattering%2520in%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520pipeline%252C%2520such%2520that%2520it%2520not%2520only%250Aperforms%2520simultaneous%2520geometric%2520and%2520color%2520rectification%252C%2520but%2520also%2520enables%2520to%250Asynthesize%2520novel%2520views%2520and%2520optical%2520effects%2520by%2520controlling%2520the%2520decoupled%250Aparameters.%2520In%2520addition%252C%2520to%2520address%2520issue%2520of%2520lack%2520of%2520real%2520paired%2520ground%2520truth%250Aimages%252C%2520we%2520propose%2520an%2520underwater%2520360%2520benchmark%2520dataset%2520that%2520has%2520real%2520paired%250A%2528i.e.%252C%2520with%2520and%2520without%2520water%2529%2520images.%2520Our%2520method%2520clearly%2520outperforms%2520other%250Abaselines%2520both%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroPump%3A%20Simultaneous%20Geometric%20and%20Color%20Rectification%20for%20Underwater%0A%20%20Images&entry.906535625=Yue%20Guo%20and%20Haoxiang%20Liao%20and%20Haibin%20Ling%20and%20Bingyao%20Huang&entry.1292438233=%20%20Underwater%20image%20restoration%20aims%20to%20remove%20geometric%20and%20color%20distortions%0Adue%20to%20water%20refraction%2C%20absorption%20and%20scattering.%20Previous%20studies%20focus%20on%0Arestoring%20either%20color%20or%20the%20geometry%2C%20but%20to%20our%20best%20knowledge%2C%20not%20both.%0AHowever%2C%20in%20practice%20it%20may%20be%20cumbersome%20to%20address%20the%20two%20rectifications%0Aone-by-one.%20In%20this%20paper%2C%20we%20propose%20NeuroPump%2C%20a%20self-supervised%20method%20to%0Asimultaneously%20optimize%20and%20rectify%20underwater%20geometry%20and%20color%20as%20if%20water%0Awere%20pumped%20out.%20The%20key%20idea%20is%20to%20explicitly%20model%20refraction%2C%20absorption%20and%0Ascattering%20in%20Neural%20Radiance%20Field%20%28NeRF%29%20pipeline%2C%20such%20that%20it%20not%20only%0Aperforms%20simultaneous%20geometric%20and%20color%20rectification%2C%20but%20also%20enables%20to%0Asynthesize%20novel%20views%20and%20optical%20effects%20by%20controlling%20the%20decoupled%0Aparameters.%20In%20addition%2C%20to%20address%20issue%20of%20lack%20of%20real%20paired%20ground%20truth%0Aimages%2C%20we%20propose%20an%20underwater%20360%20benchmark%20dataset%20that%20has%20real%20paired%0A%28i.e.%2C%20with%20and%20without%20water%29%20images.%20Our%20method%20clearly%20outperforms%20other%0Abaselines%20both%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15890v1&entry.124074799=Read"},
{"title": "PruneVid: Visual Token Pruning for Efficient Video Large Language Models", "author": "Xiaohu Huang and Hao Zhou and Kai Han", "abstract": "  In this paper, we introduce PruneVid, a visual token pruning method designed\nto enhance the efficiency of multi-modal video understanding. Large Language\nModels (LLMs) have shown promising performance in video tasks due to their\nextended capabilities in comprehending visual modalities. However, the\nsubstantial redundancy in video data presents significant computational\nchallenges for LLMs. To address this issue, we introduce a training-free method\nthat 1) minimizes video redundancy by merging spatial-temporal tokens, and 2)\nleverages LLMs' reasoning capabilities to selectively prune visual features\nrelevant to question tokens, enhancing model efficiency. We validate our method\nacross multiple video benchmarks, which demonstrate that PruneVid can prune\nover 80% of tokens while maintaining competitive performance combined with\ndifferent model networks. This highlights its superior effectiveness and\nefficiency compared to existing pruning methods. Code:\nhttps://github.com/Visual-AI/PruneVid.\n", "link": "http://arxiv.org/abs/2412.16117v1", "date": "2024-12-20", "relevancy": 2.7902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PruneVid%3A%20Visual%20Token%20Pruning%20for%20Efficient%20Video%20Large%20Language%20Models&body=Title%3A%20PruneVid%3A%20Visual%20Token%20Pruning%20for%20Efficient%20Video%20Large%20Language%20Models%0AAuthor%3A%20Xiaohu%20Huang%20and%20Hao%20Zhou%20and%20Kai%20Han%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20PruneVid%2C%20a%20visual%20token%20pruning%20method%20designed%0Ato%20enhance%20the%20efficiency%20of%20multi-modal%20video%20understanding.%20Large%20Language%0AModels%20%28LLMs%29%20have%20shown%20promising%20performance%20in%20video%20tasks%20due%20to%20their%0Aextended%20capabilities%20in%20comprehending%20visual%20modalities.%20However%2C%20the%0Asubstantial%20redundancy%20in%20video%20data%20presents%20significant%20computational%0Achallenges%20for%20LLMs.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20training-free%20method%0Athat%201%29%20minimizes%20video%20redundancy%20by%20merging%20spatial-temporal%20tokens%2C%20and%202%29%0Aleverages%20LLMs%27%20reasoning%20capabilities%20to%20selectively%20prune%20visual%20features%0Arelevant%20to%20question%20tokens%2C%20enhancing%20model%20efficiency.%20We%20validate%20our%20method%0Aacross%20multiple%20video%20benchmarks%2C%20which%20demonstrate%20that%20PruneVid%20can%20prune%0Aover%2080%25%20of%20tokens%20while%20maintaining%20competitive%20performance%20combined%20with%0Adifferent%20model%20networks.%20This%20highlights%20its%20superior%20effectiveness%20and%0Aefficiency%20compared%20to%20existing%20pruning%20methods.%20Code%3A%0Ahttps%3A//github.com/Visual-AI/PruneVid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruneVid%253A%2520Visual%2520Token%2520Pruning%2520for%2520Efficient%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DXiaohu%2520Huang%2520and%2520Hao%2520Zhou%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PruneVid%252C%2520a%2520visual%2520token%2520pruning%2520method%2520designed%250Ato%2520enhance%2520the%2520efficiency%2520of%2520multi-modal%2520video%2520understanding.%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520shown%2520promising%2520performance%2520in%2520video%2520tasks%2520due%2520to%2520their%250Aextended%2520capabilities%2520in%2520comprehending%2520visual%2520modalities.%2520However%252C%2520the%250Asubstantial%2520redundancy%2520in%2520video%2520data%2520presents%2520significant%2520computational%250Achallenges%2520for%2520LLMs.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520training-free%2520method%250Athat%25201%2529%2520minimizes%2520video%2520redundancy%2520by%2520merging%2520spatial-temporal%2520tokens%252C%2520and%25202%2529%250Aleverages%2520LLMs%2527%2520reasoning%2520capabilities%2520to%2520selectively%2520prune%2520visual%2520features%250Arelevant%2520to%2520question%2520tokens%252C%2520enhancing%2520model%2520efficiency.%2520We%2520validate%2520our%2520method%250Aacross%2520multiple%2520video%2520benchmarks%252C%2520which%2520demonstrate%2520that%2520PruneVid%2520can%2520prune%250Aover%252080%2525%2520of%2520tokens%2520while%2520maintaining%2520competitive%2520performance%2520combined%2520with%250Adifferent%2520model%2520networks.%2520This%2520highlights%2520its%2520superior%2520effectiveness%2520and%250Aefficiency%2520compared%2520to%2520existing%2520pruning%2520methods.%2520Code%253A%250Ahttps%253A//github.com/Visual-AI/PruneVid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PruneVid%3A%20Visual%20Token%20Pruning%20for%20Efficient%20Video%20Large%20Language%20Models&entry.906535625=Xiaohu%20Huang%20and%20Hao%20Zhou%20and%20Kai%20Han&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20PruneVid%2C%20a%20visual%20token%20pruning%20method%20designed%0Ato%20enhance%20the%20efficiency%20of%20multi-modal%20video%20understanding.%20Large%20Language%0AModels%20%28LLMs%29%20have%20shown%20promising%20performance%20in%20video%20tasks%20due%20to%20their%0Aextended%20capabilities%20in%20comprehending%20visual%20modalities.%20However%2C%20the%0Asubstantial%20redundancy%20in%20video%20data%20presents%20significant%20computational%0Achallenges%20for%20LLMs.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20training-free%20method%0Athat%201%29%20minimizes%20video%20redundancy%20by%20merging%20spatial-temporal%20tokens%2C%20and%202%29%0Aleverages%20LLMs%27%20reasoning%20capabilities%20to%20selectively%20prune%20visual%20features%0Arelevant%20to%20question%20tokens%2C%20enhancing%20model%20efficiency.%20We%20validate%20our%20method%0Aacross%20multiple%20video%20benchmarks%2C%20which%20demonstrate%20that%20PruneVid%20can%20prune%0Aover%2080%25%20of%20tokens%20while%20maintaining%20competitive%20performance%20combined%20with%0Adifferent%20model%20networks.%20This%20highlights%20its%20superior%20effectiveness%20and%0Aefficiency%20compared%20to%20existing%20pruning%20methods.%20Code%3A%0Ahttps%3A//github.com/Visual-AI/PruneVid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16117v1&entry.124074799=Read"},
{"title": "NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for\n  Vision of Autonomous Systems", "author": "Laura Weihl and Bilal Wehbe and Andrzej W\u0105sowski", "abstract": "  Autonomous inspection of infrastructure on land and in water is a quickly\ngrowing market, with applications including surveying constructions, monitoring\nplants, and tracking environmental changes in on- and off-shore wind energy\nfarms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles\noverfitting of controllers to simulation conditions fundamentally leads to poor\nperformance in the operation environment. There is a pressing need for more\ndiverse and realistic test data that accurately represents the challenges faced\nby these systems. We address the challenge of generating perception test data\nfor autonomous systems by leveraging Neural Radiance Fields to generate\nrealistic and diverse test images, and integrating them into a metamorphic\ntesting framework for vision components such as vSLAM and object detection. Our\ntool, N2R-Tester, allows training models of custom scenes and rendering test\nimages from perturbed positions. An experimental evaluation of N2R-Tester on\neight different vision components in AUVs and UAVs demonstrates the efficacy\nand versatility of the approach.\n", "link": "http://arxiv.org/abs/2412.16141v1", "date": "2024-12-20", "relevancy": 2.7468, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5544}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-To-Real%20Tester%3A%20Neural%20Radiance%20Fields%20as%20Test%20Image%20Generators%20for%0A%20%20Vision%20of%20Autonomous%20Systems&body=Title%3A%20NeRF-To-Real%20Tester%3A%20Neural%20Radiance%20Fields%20as%20Test%20Image%20Generators%20for%0A%20%20Vision%20of%20Autonomous%20Systems%0AAuthor%3A%20Laura%20Weihl%20and%20Bilal%20Wehbe%20and%20Andrzej%20W%C4%85sowski%0AAbstract%3A%20%20%20Autonomous%20inspection%20of%20infrastructure%20on%20land%20and%20in%20water%20is%20a%20quickly%0Agrowing%20market%2C%20with%20applications%20including%20surveying%20constructions%2C%20monitoring%0Aplants%2C%20and%20tracking%20environmental%20changes%20in%20on-%20and%20off-shore%20wind%20energy%0Afarms.%20For%20Autonomous%20Underwater%20Vehicles%20and%20Unmanned%20Aerial%20Vehicles%0Aoverfitting%20of%20controllers%20to%20simulation%20conditions%20fundamentally%20leads%20to%20poor%0Aperformance%20in%20the%20operation%20environment.%20There%20is%20a%20pressing%20need%20for%20more%0Adiverse%20and%20realistic%20test%20data%20that%20accurately%20represents%20the%20challenges%20faced%0Aby%20these%20systems.%20We%20address%20the%20challenge%20of%20generating%20perception%20test%20data%0Afor%20autonomous%20systems%20by%20leveraging%20Neural%20Radiance%20Fields%20to%20generate%0Arealistic%20and%20diverse%20test%20images%2C%20and%20integrating%20them%20into%20a%20metamorphic%0Atesting%20framework%20for%20vision%20components%20such%20as%20vSLAM%20and%20object%20detection.%20Our%0Atool%2C%20N2R-Tester%2C%20allows%20training%20models%20of%20custom%20scenes%20and%20rendering%20test%0Aimages%20from%20perturbed%20positions.%20An%20experimental%20evaluation%20of%20N2R-Tester%20on%0Aeight%20different%20vision%20components%20in%20AUVs%20and%20UAVs%20demonstrates%20the%20efficacy%0Aand%20versatility%20of%20the%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-To-Real%2520Tester%253A%2520Neural%2520Radiance%2520Fields%2520as%2520Test%2520Image%2520Generators%2520for%250A%2520%2520Vision%2520of%2520Autonomous%2520Systems%26entry.906535625%3DLaura%2520Weihl%2520and%2520Bilal%2520Wehbe%2520and%2520Andrzej%2520W%25C4%2585sowski%26entry.1292438233%3D%2520%2520Autonomous%2520inspection%2520of%2520infrastructure%2520on%2520land%2520and%2520in%2520water%2520is%2520a%2520quickly%250Agrowing%2520market%252C%2520with%2520applications%2520including%2520surveying%2520constructions%252C%2520monitoring%250Aplants%252C%2520and%2520tracking%2520environmental%2520changes%2520in%2520on-%2520and%2520off-shore%2520wind%2520energy%250Afarms.%2520For%2520Autonomous%2520Underwater%2520Vehicles%2520and%2520Unmanned%2520Aerial%2520Vehicles%250Aoverfitting%2520of%2520controllers%2520to%2520simulation%2520conditions%2520fundamentally%2520leads%2520to%2520poor%250Aperformance%2520in%2520the%2520operation%2520environment.%2520There%2520is%2520a%2520pressing%2520need%2520for%2520more%250Adiverse%2520and%2520realistic%2520test%2520data%2520that%2520accurately%2520represents%2520the%2520challenges%2520faced%250Aby%2520these%2520systems.%2520We%2520address%2520the%2520challenge%2520of%2520generating%2520perception%2520test%2520data%250Afor%2520autonomous%2520systems%2520by%2520leveraging%2520Neural%2520Radiance%2520Fields%2520to%2520generate%250Arealistic%2520and%2520diverse%2520test%2520images%252C%2520and%2520integrating%2520them%2520into%2520a%2520metamorphic%250Atesting%2520framework%2520for%2520vision%2520components%2520such%2520as%2520vSLAM%2520and%2520object%2520detection.%2520Our%250Atool%252C%2520N2R-Tester%252C%2520allows%2520training%2520models%2520of%2520custom%2520scenes%2520and%2520rendering%2520test%250Aimages%2520from%2520perturbed%2520positions.%2520An%2520experimental%2520evaluation%2520of%2520N2R-Tester%2520on%250Aeight%2520different%2520vision%2520components%2520in%2520AUVs%2520and%2520UAVs%2520demonstrates%2520the%2520efficacy%250Aand%2520versatility%2520of%2520the%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-To-Real%20Tester%3A%20Neural%20Radiance%20Fields%20as%20Test%20Image%20Generators%20for%0A%20%20Vision%20of%20Autonomous%20Systems&entry.906535625=Laura%20Weihl%20and%20Bilal%20Wehbe%20and%20Andrzej%20W%C4%85sowski&entry.1292438233=%20%20Autonomous%20inspection%20of%20infrastructure%20on%20land%20and%20in%20water%20is%20a%20quickly%0Agrowing%20market%2C%20with%20applications%20including%20surveying%20constructions%2C%20monitoring%0Aplants%2C%20and%20tracking%20environmental%20changes%20in%20on-%20and%20off-shore%20wind%20energy%0Afarms.%20For%20Autonomous%20Underwater%20Vehicles%20and%20Unmanned%20Aerial%20Vehicles%0Aoverfitting%20of%20controllers%20to%20simulation%20conditions%20fundamentally%20leads%20to%20poor%0Aperformance%20in%20the%20operation%20environment.%20There%20is%20a%20pressing%20need%20for%20more%0Adiverse%20and%20realistic%20test%20data%20that%20accurately%20represents%20the%20challenges%20faced%0Aby%20these%20systems.%20We%20address%20the%20challenge%20of%20generating%20perception%20test%20data%0Afor%20autonomous%20systems%20by%20leveraging%20Neural%20Radiance%20Fields%20to%20generate%0Arealistic%20and%20diverse%20test%20images%2C%20and%20integrating%20them%20into%20a%20metamorphic%0Atesting%20framework%20for%20vision%20components%20such%20as%20vSLAM%20and%20object%20detection.%20Our%0Atool%2C%20N2R-Tester%2C%20allows%20training%20models%20of%20custom%20scenes%20and%20rendering%20test%0Aimages%20from%20perturbed%20positions.%20An%20experimental%20evaluation%20of%20N2R-Tester%20on%0Aeight%20different%20vision%20components%20in%20AUVs%20and%20UAVs%20demonstrates%20the%20efficacy%0Aand%20versatility%20of%20the%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16141v1&entry.124074799=Read"},
{"title": "MGDA: Model-based Goal Data Augmentation for Offline Goal-conditioned\n  Weighted Supervised Learning", "author": "Xing Lei and Xuetao Zhang and Donglin Wang", "abstract": "  Recently, a state-of-the-art family of algorithms, known as Goal-Conditioned\nWeighted Supervised Learning (GCWSL) methods, has been introduced to tackle\nchallenges in offline goal-conditioned reinforcement learning (RL). GCWSL\noptimizes a lower bound of the goal-conditioned RL objective and has\ndemonstrated outstanding performance across diverse goal-reaching tasks,\nproviding a simple, effective, and stable solution. However, prior research has\nidentified a critical limitation of GCWSL: the lack of trajectory stitching\ncapabilities. To address this, goal data augmentation strategies have been\nproposed to enhance these methods. Nevertheless, existing techniques often\nstruggle to sample suitable augmented goals for GCWSL effectively. In this\npaper, we establish unified principles for goal data augmentation, focusing on\ngoal diversity, action optimality, and goal reachability. Based on these\nprinciples, we propose a Model-based Goal Data Augmentation (MGDA) approach,\nwhich leverages a learned dynamics model to sample more suitable augmented\ngoals. MGDA uniquely incorporates the local Lipschitz continuity assumption\nwithin the learned model to mitigate the impact of compounding errors.\nEmpirical results show that MGDA significantly enhances the performance of\nGCWSL methods on both state-based and vision-based maze datasets, surpassing\nprevious goal data augmentation techniques in improving stitching capabilities.\n", "link": "http://arxiv.org/abs/2412.11410v2", "date": "2024-12-20", "relevancy": 2.7291, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5727}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.535}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGDA%3A%20Model-based%20Goal%20Data%20Augmentation%20for%20Offline%20Goal-conditioned%0A%20%20Weighted%20Supervised%20Learning&body=Title%3A%20MGDA%3A%20Model-based%20Goal%20Data%20Augmentation%20for%20Offline%20Goal-conditioned%0A%20%20Weighted%20Supervised%20Learning%0AAuthor%3A%20Xing%20Lei%20and%20Xuetao%20Zhang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Recently%2C%20a%20state-of-the-art%20family%20of%20algorithms%2C%20known%20as%20Goal-Conditioned%0AWeighted%20Supervised%20Learning%20%28GCWSL%29%20methods%2C%20has%20been%20introduced%20to%20tackle%0Achallenges%20in%20offline%20goal-conditioned%20reinforcement%20learning%20%28RL%29.%20GCWSL%0Aoptimizes%20a%20lower%20bound%20of%20the%20goal-conditioned%20RL%20objective%20and%20has%0Ademonstrated%20outstanding%20performance%20across%20diverse%20goal-reaching%20tasks%2C%0Aproviding%20a%20simple%2C%20effective%2C%20and%20stable%20solution.%20However%2C%20prior%20research%20has%0Aidentified%20a%20critical%20limitation%20of%20GCWSL%3A%20the%20lack%20of%20trajectory%20stitching%0Acapabilities.%20To%20address%20this%2C%20goal%20data%20augmentation%20strategies%20have%20been%0Aproposed%20to%20enhance%20these%20methods.%20Nevertheless%2C%20existing%20techniques%20often%0Astruggle%20to%20sample%20suitable%20augmented%20goals%20for%20GCWSL%20effectively.%20In%20this%0Apaper%2C%20we%20establish%20unified%20principles%20for%20goal%20data%20augmentation%2C%20focusing%20on%0Agoal%20diversity%2C%20action%20optimality%2C%20and%20goal%20reachability.%20Based%20on%20these%0Aprinciples%2C%20we%20propose%20a%20Model-based%20Goal%20Data%20Augmentation%20%28MGDA%29%20approach%2C%0Awhich%20leverages%20a%20learned%20dynamics%20model%20to%20sample%20more%20suitable%20augmented%0Agoals.%20MGDA%20uniquely%20incorporates%20the%20local%20Lipschitz%20continuity%20assumption%0Awithin%20the%20learned%20model%20to%20mitigate%20the%20impact%20of%20compounding%20errors.%0AEmpirical%20results%20show%20that%20MGDA%20significantly%20enhances%20the%20performance%20of%0AGCWSL%20methods%20on%20both%20state-based%20and%20vision-based%20maze%20datasets%2C%20surpassing%0Aprevious%20goal%20data%20augmentation%20techniques%20in%20improving%20stitching%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGDA%253A%2520Model-based%2520Goal%2520Data%2520Augmentation%2520for%2520Offline%2520Goal-conditioned%250A%2520%2520Weighted%2520Supervised%2520Learning%26entry.906535625%3DXing%2520Lei%2520and%2520Xuetao%2520Zhang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520state-of-the-art%2520family%2520of%2520algorithms%252C%2520known%2520as%2520Goal-Conditioned%250AWeighted%2520Supervised%2520Learning%2520%2528GCWSL%2529%2520methods%252C%2520has%2520been%2520introduced%2520to%2520tackle%250Achallenges%2520in%2520offline%2520goal-conditioned%2520reinforcement%2520learning%2520%2528RL%2529.%2520GCWSL%250Aoptimizes%2520a%2520lower%2520bound%2520of%2520the%2520goal-conditioned%2520RL%2520objective%2520and%2520has%250Ademonstrated%2520outstanding%2520performance%2520across%2520diverse%2520goal-reaching%2520tasks%252C%250Aproviding%2520a%2520simple%252C%2520effective%252C%2520and%2520stable%2520solution.%2520However%252C%2520prior%2520research%2520has%250Aidentified%2520a%2520critical%2520limitation%2520of%2520GCWSL%253A%2520the%2520lack%2520of%2520trajectory%2520stitching%250Acapabilities.%2520To%2520address%2520this%252C%2520goal%2520data%2520augmentation%2520strategies%2520have%2520been%250Aproposed%2520to%2520enhance%2520these%2520methods.%2520Nevertheless%252C%2520existing%2520techniques%2520often%250Astruggle%2520to%2520sample%2520suitable%2520augmented%2520goals%2520for%2520GCWSL%2520effectively.%2520In%2520this%250Apaper%252C%2520we%2520establish%2520unified%2520principles%2520for%2520goal%2520data%2520augmentation%252C%2520focusing%2520on%250Agoal%2520diversity%252C%2520action%2520optimality%252C%2520and%2520goal%2520reachability.%2520Based%2520on%2520these%250Aprinciples%252C%2520we%2520propose%2520a%2520Model-based%2520Goal%2520Data%2520Augmentation%2520%2528MGDA%2529%2520approach%252C%250Awhich%2520leverages%2520a%2520learned%2520dynamics%2520model%2520to%2520sample%2520more%2520suitable%2520augmented%250Agoals.%2520MGDA%2520uniquely%2520incorporates%2520the%2520local%2520Lipschitz%2520continuity%2520assumption%250Awithin%2520the%2520learned%2520model%2520to%2520mitigate%2520the%2520impact%2520of%2520compounding%2520errors.%250AEmpirical%2520results%2520show%2520that%2520MGDA%2520significantly%2520enhances%2520the%2520performance%2520of%250AGCWSL%2520methods%2520on%2520both%2520state-based%2520and%2520vision-based%2520maze%2520datasets%252C%2520surpassing%250Aprevious%2520goal%2520data%2520augmentation%2520techniques%2520in%2520improving%2520stitching%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGDA%3A%20Model-based%20Goal%20Data%20Augmentation%20for%20Offline%20Goal-conditioned%0A%20%20Weighted%20Supervised%20Learning&entry.906535625=Xing%20Lei%20and%20Xuetao%20Zhang%20and%20Donglin%20Wang&entry.1292438233=%20%20Recently%2C%20a%20state-of-the-art%20family%20of%20algorithms%2C%20known%20as%20Goal-Conditioned%0AWeighted%20Supervised%20Learning%20%28GCWSL%29%20methods%2C%20has%20been%20introduced%20to%20tackle%0Achallenges%20in%20offline%20goal-conditioned%20reinforcement%20learning%20%28RL%29.%20GCWSL%0Aoptimizes%20a%20lower%20bound%20of%20the%20goal-conditioned%20RL%20objective%20and%20has%0Ademonstrated%20outstanding%20performance%20across%20diverse%20goal-reaching%20tasks%2C%0Aproviding%20a%20simple%2C%20effective%2C%20and%20stable%20solution.%20However%2C%20prior%20research%20has%0Aidentified%20a%20critical%20limitation%20of%20GCWSL%3A%20the%20lack%20of%20trajectory%20stitching%0Acapabilities.%20To%20address%20this%2C%20goal%20data%20augmentation%20strategies%20have%20been%0Aproposed%20to%20enhance%20these%20methods.%20Nevertheless%2C%20existing%20techniques%20often%0Astruggle%20to%20sample%20suitable%20augmented%20goals%20for%20GCWSL%20effectively.%20In%20this%0Apaper%2C%20we%20establish%20unified%20principles%20for%20goal%20data%20augmentation%2C%20focusing%20on%0Agoal%20diversity%2C%20action%20optimality%2C%20and%20goal%20reachability.%20Based%20on%20these%0Aprinciples%2C%20we%20propose%20a%20Model-based%20Goal%20Data%20Augmentation%20%28MGDA%29%20approach%2C%0Awhich%20leverages%20a%20learned%20dynamics%20model%20to%20sample%20more%20suitable%20augmented%0Agoals.%20MGDA%20uniquely%20incorporates%20the%20local%20Lipschitz%20continuity%20assumption%0Awithin%20the%20learned%20model%20to%20mitigate%20the%20impact%20of%20compounding%20errors.%0AEmpirical%20results%20show%20that%20MGDA%20significantly%20enhances%20the%20performance%20of%0AGCWSL%20methods%20on%20both%20state-based%20and%20vision-based%20maze%20datasets%2C%20surpassing%0Aprevious%20goal%20data%20augmentation%20techniques%20in%20improving%20stitching%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11410v2&entry.124074799=Read"},
{"title": "VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in\n  Large Vision-Language Models", "author": "Dexter Neo and Tsuhan Chen", "abstract": "  Large Vision-Language Models (LVLMs) have made remarkable developments along\nwith the recent surge of large language models. Despite their advancements,\nLVLMs have a tendency to generate plausible yet inaccurate or inconsistent\ninformation based on the provided source content. This phenomenon, also known\nas ``hallucinations\" can have serious downstream implications during the\ndeployment of LVLMs. To address this, we present VORD a simple and effective\nmethod that alleviates hallucinations by calibrating token predictions based on\nordinal relationships between modified image pairs. VORD is presented in two\nforms: 1.) a minimalist training-free variant which eliminates implausible\ntokens from modified image pairs, and 2.) a trainable objective function that\npenalizes unlikely tokens. Our experiments demonstrate that VORD delivers\nbetter calibration and effectively mitigates object hallucinations on a\nwide-range of LVLM benchmarks.\n", "link": "http://arxiv.org/abs/2412.15739v1", "date": "2024-12-20", "relevancy": 2.7207, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5588}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VORD%3A%20Visual%20Ordinal%20Calibration%20for%20Mitigating%20Object%20Hallucinations%20in%0A%20%20Large%20Vision-Language%20Models&body=Title%3A%20VORD%3A%20Visual%20Ordinal%20Calibration%20for%20Mitigating%20Object%20Hallucinations%20in%0A%20%20Large%20Vision-Language%20Models%0AAuthor%3A%20Dexter%20Neo%20and%20Tsuhan%20Chen%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20remarkable%20developments%20along%0Awith%20the%20recent%20surge%20of%20large%20language%20models.%20Despite%20their%20advancements%2C%0ALVLMs%20have%20a%20tendency%20to%20generate%20plausible%20yet%20inaccurate%20or%20inconsistent%0Ainformation%20based%20on%20the%20provided%20source%20content.%20This%20phenomenon%2C%20also%20known%0Aas%20%60%60hallucinations%22%20can%20have%20serious%20downstream%20implications%20during%20the%0Adeployment%20of%20LVLMs.%20To%20address%20this%2C%20we%20present%20VORD%20a%20simple%20and%20effective%0Amethod%20that%20alleviates%20hallucinations%20by%20calibrating%20token%20predictions%20based%20on%0Aordinal%20relationships%20between%20modified%20image%20pairs.%20VORD%20is%20presented%20in%20two%0Aforms%3A%201.%29%20a%20minimalist%20training-free%20variant%20which%20eliminates%20implausible%0Atokens%20from%20modified%20image%20pairs%2C%20and%202.%29%20a%20trainable%20objective%20function%20that%0Apenalizes%20unlikely%20tokens.%20Our%20experiments%20demonstrate%20that%20VORD%20delivers%0Abetter%20calibration%20and%20effectively%20mitigates%20object%20hallucinations%20on%20a%0Awide-range%20of%20LVLM%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVORD%253A%2520Visual%2520Ordinal%2520Calibration%2520for%2520Mitigating%2520Object%2520Hallucinations%2520in%250A%2520%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DDexter%2520Neo%2520and%2520Tsuhan%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520made%2520remarkable%2520developments%2520along%250Awith%2520the%2520recent%2520surge%2520of%2520large%2520language%2520models.%2520Despite%2520their%2520advancements%252C%250ALVLMs%2520have%2520a%2520tendency%2520to%2520generate%2520plausible%2520yet%2520inaccurate%2520or%2520inconsistent%250Ainformation%2520based%2520on%2520the%2520provided%2520source%2520content.%2520This%2520phenomenon%252C%2520also%2520known%250Aas%2520%2560%2560hallucinations%2522%2520can%2520have%2520serious%2520downstream%2520implications%2520during%2520the%250Adeployment%2520of%2520LVLMs.%2520To%2520address%2520this%252C%2520we%2520present%2520VORD%2520a%2520simple%2520and%2520effective%250Amethod%2520that%2520alleviates%2520hallucinations%2520by%2520calibrating%2520token%2520predictions%2520based%2520on%250Aordinal%2520relationships%2520between%2520modified%2520image%2520pairs.%2520VORD%2520is%2520presented%2520in%2520two%250Aforms%253A%25201.%2529%2520a%2520minimalist%2520training-free%2520variant%2520which%2520eliminates%2520implausible%250Atokens%2520from%2520modified%2520image%2520pairs%252C%2520and%25202.%2529%2520a%2520trainable%2520objective%2520function%2520that%250Apenalizes%2520unlikely%2520tokens.%2520Our%2520experiments%2520demonstrate%2520that%2520VORD%2520delivers%250Abetter%2520calibration%2520and%2520effectively%2520mitigates%2520object%2520hallucinations%2520on%2520a%250Awide-range%2520of%2520LVLM%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VORD%3A%20Visual%20Ordinal%20Calibration%20for%20Mitigating%20Object%20Hallucinations%20in%0A%20%20Large%20Vision-Language%20Models&entry.906535625=Dexter%20Neo%20and%20Tsuhan%20Chen&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20remarkable%20developments%20along%0Awith%20the%20recent%20surge%20of%20large%20language%20models.%20Despite%20their%20advancements%2C%0ALVLMs%20have%20a%20tendency%20to%20generate%20plausible%20yet%20inaccurate%20or%20inconsistent%0Ainformation%20based%20on%20the%20provided%20source%20content.%20This%20phenomenon%2C%20also%20known%0Aas%20%60%60hallucinations%22%20can%20have%20serious%20downstream%20implications%20during%20the%0Adeployment%20of%20LVLMs.%20To%20address%20this%2C%20we%20present%20VORD%20a%20simple%20and%20effective%0Amethod%20that%20alleviates%20hallucinations%20by%20calibrating%20token%20predictions%20based%20on%0Aordinal%20relationships%20between%20modified%20image%20pairs.%20VORD%20is%20presented%20in%20two%0Aforms%3A%201.%29%20a%20minimalist%20training-free%20variant%20which%20eliminates%20implausible%0Atokens%20from%20modified%20image%20pairs%2C%20and%202.%29%20a%20trainable%20objective%20function%20that%0Apenalizes%20unlikely%20tokens.%20Our%20experiments%20demonstrate%20that%20VORD%20delivers%0Abetter%20calibration%20and%20effectively%20mitigates%20object%20hallucinations%20on%20a%0Awide-range%20of%20LVLM%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15739v1&entry.124074799=Read"},
{"title": "Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization\n  Perspective", "author": "Semih Cayci", "abstract": "  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.\n", "link": "http://arxiv.org/abs/2412.14031v3", "date": "2024-12-20", "relevancy": 2.7127, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.566}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5544}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective&body=Title%3A%20Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective%0AAuthor%3A%20Semih%20Cayci%0AAbstract%3A%20%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Afactor%20yields%20robustness%20to%20ill-conditioned%20kernels%2C%20analogous%20to%20the%0Aunderparameterized%20regime.%20These%20findings%20demonstrate%20the%20potential%20of%0AGauss-Newton%20methods%20for%20efficiently%20optimizing%20neural%20networks%2C%20particularly%0Ain%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%20small%20singular%0Avalues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauss-Newton%2520Dynamics%2520for%2520Neural%2520Networks%253A%2520A%2520Riemannian%2520Optimization%250A%2520%2520Perspective%26entry.906535625%3DSemih%2520Cayci%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520convergence%2520of%2520Gauss-Newton%2520dynamics%2520for%2520training%2520neural%250Anetworks%2520with%2520smooth%2520activation%2520functions.%2520In%2520the%2520underparameterized%2520regime%252C%250Athe%2520Gauss-Newton%2520gradient%2520flow%2520induces%2520a%2520Riemannian%2520gradient%2520flow%2520on%2520a%250Alow-dimensional%252C%2520smooth%252C%2520embedded%2520submanifold%2520of%2520the%2520Euclidean%2520output%2520space.%250AUsing%2520tools%2520from%2520Riemannian%2520optimization%252C%2520we%2520prove%2520%255Cemph%257Blast-iterate%257D%250Aconvergence%2520of%2520the%2520Riemannian%2520gradient%2520flow%2520to%2520the%2520optimal%2520in-class%2520predictor%250Aat%2520an%2520%255Cemph%257Bexponential%2520rate%257D%2520that%2520is%2520independent%2520of%2520the%2520conditioning%2520of%2520the%250AGram%2520matrix%252C%2520%255Cemph%257Bwithout%257D%2520requiring%2520explicit%2520regularization.%2520We%2520further%250Acharacterize%2520the%2520critical%2520impacts%2520of%2520the%2520neural%2520network%2520scaling%2520factor%2520and%2520the%250Ainitialization%2520on%2520the%2520convergence%2520behavior.%2520In%2520the%2520overparameterized%2520regime%252C%2520we%250Ashow%2520that%2520the%2520Levenberg-Marquardt%2520dynamics%2520with%2520an%2520appropriately%2520chosen%2520damping%250Afactor%2520yields%2520robustness%2520to%2520ill-conditioned%2520kernels%252C%2520analogous%2520to%2520the%250Aunderparameterized%2520regime.%2520These%2520findings%2520demonstrate%2520the%2520potential%2520of%250AGauss-Newton%2520methods%2520for%2520efficiently%2520optimizing%2520neural%2520networks%252C%2520particularly%250Ain%2520ill-conditioned%2520problems%2520where%2520kernel%2520and%2520Gram%2520matrices%2520have%2520small%2520singular%250Avalues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective&entry.906535625=Semih%20Cayci&entry.1292438233=%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Afactor%20yields%20robustness%20to%20ill-conditioned%20kernels%2C%20analogous%20to%20the%0Aunderparameterized%20regime.%20These%20findings%20demonstrate%20the%20potential%20of%0AGauss-Newton%20methods%20for%20efficiently%20optimizing%20neural%20networks%2C%20particularly%0Ain%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%20small%20singular%0Avalues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14031v3&entry.124074799=Read"},
{"title": "POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search", "author": "Chong-Yang Xiang and Jun-Yan He and Zhi-Qi Cheng and Xiao Wu and Xian-Sheng Hua", "abstract": "  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2410.09583v5", "date": "2024-12-20", "relevancy": 2.7007, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5876}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.526}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search&body=Title%3A%20POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search%0AAuthor%3A%20Chong-Yang%20Xiang%20and%20Jun-Yan%20He%20and%20Zhi-Qi%20Cheng%20and%20Xiao%20Wu%20and%20Xian-Sheng%20Hua%0AAbstract%3A%20%20%20Achieving%20a%20balance%20between%20accuracy%20and%20efficiency%20is%20a%20critical%20challenge%0Ain%20facial%20landmark%20detection%20%28FLD%29.%20This%20paper%20introduces%20Parallel%20Optimal%0APosition%20Search%20%28POPoS%29%2C%20a%20high-precision%20encoding-decoding%20framework%20designed%0Ato%20address%20the%20limitations%20of%20traditional%20FLD%20methods.%20POPoS%20employs%20three%20key%0Acontributions%3A%20%281%29%20Pseudo-range%20multilateration%20is%20utilized%20to%20correct%20heatmap%0Aerrors%2C%20improving%20landmark%20localization%20accuracy.%20By%20integrating%20multiple%0Aanchor%20points%2C%20it%20reduces%20the%20impact%20of%20individual%20heatmap%20inaccuracies%2C%0Aleading%20to%20robust%20overall%20positioning.%20%282%29%20To%20enhance%20the%20pseudo-range%20accuracy%0Aof%20selected%20anchor%20points%2C%20a%20new%20loss%20function%2C%20named%20multilateration%20anchor%0Aloss%2C%20is%20proposed.%20This%20loss%20function%20enhances%20the%20accuracy%20of%20the%20distance%0Amap%2C%20mitigates%20the%20risk%20of%20local%20optima%2C%20and%20ensures%20optimal%20solutions.%20%283%29%20A%0Asingle-step%20parallel%20computation%20algorithm%20is%20introduced%2C%20boosting%0Acomputational%20efficiency%20and%20reducing%20processing%20time.%20Extensive%20evaluations%0Aacross%20five%20benchmark%20datasets%20demonstrate%20that%20POPoS%20consistently%20outperforms%0Aexisting%20methods%2C%20particularly%20excelling%20in%20low-resolution%20heatmaps%20scenarios%0Awith%20minimal%20computational%20overhead.%20These%20advantages%20make%20POPoS%20a%20highly%0Aefficient%20and%20accurate%20tool%20for%20FLD%2C%20with%20broad%20applicability%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09583v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOPoS%253A%2520Improving%2520Efficient%2520and%2520Robust%2520Facial%2520Landmark%2520Detection%2520with%250A%2520%2520Parallel%2520Optimal%2520Position%2520Search%26entry.906535625%3DChong-Yang%2520Xiang%2520and%2520Jun-Yan%2520He%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Xiao%2520Wu%2520and%2520Xian-Sheng%2520Hua%26entry.1292438233%3D%2520%2520Achieving%2520a%2520balance%2520between%2520accuracy%2520and%2520efficiency%2520is%2520a%2520critical%2520challenge%250Ain%2520facial%2520landmark%2520detection%2520%2528FLD%2529.%2520This%2520paper%2520introduces%2520Parallel%2520Optimal%250APosition%2520Search%2520%2528POPoS%2529%252C%2520a%2520high-precision%2520encoding-decoding%2520framework%2520designed%250Ato%2520address%2520the%2520limitations%2520of%2520traditional%2520FLD%2520methods.%2520POPoS%2520employs%2520three%2520key%250Acontributions%253A%2520%25281%2529%2520Pseudo-range%2520multilateration%2520is%2520utilized%2520to%2520correct%2520heatmap%250Aerrors%252C%2520improving%2520landmark%2520localization%2520accuracy.%2520By%2520integrating%2520multiple%250Aanchor%2520points%252C%2520it%2520reduces%2520the%2520impact%2520of%2520individual%2520heatmap%2520inaccuracies%252C%250Aleading%2520to%2520robust%2520overall%2520positioning.%2520%25282%2529%2520To%2520enhance%2520the%2520pseudo-range%2520accuracy%250Aof%2520selected%2520anchor%2520points%252C%2520a%2520new%2520loss%2520function%252C%2520named%2520multilateration%2520anchor%250Aloss%252C%2520is%2520proposed.%2520This%2520loss%2520function%2520enhances%2520the%2520accuracy%2520of%2520the%2520distance%250Amap%252C%2520mitigates%2520the%2520risk%2520of%2520local%2520optima%252C%2520and%2520ensures%2520optimal%2520solutions.%2520%25283%2529%2520A%250Asingle-step%2520parallel%2520computation%2520algorithm%2520is%2520introduced%252C%2520boosting%250Acomputational%2520efficiency%2520and%2520reducing%2520processing%2520time.%2520Extensive%2520evaluations%250Aacross%2520five%2520benchmark%2520datasets%2520demonstrate%2520that%2520POPoS%2520consistently%2520outperforms%250Aexisting%2520methods%252C%2520particularly%2520excelling%2520in%2520low-resolution%2520heatmaps%2520scenarios%250Awith%2520minimal%2520computational%2520overhead.%2520These%2520advantages%2520make%2520POPoS%2520a%2520highly%250Aefficient%2520and%2520accurate%2520tool%2520for%2520FLD%252C%2520with%2520broad%2520applicability%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09583v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POPoS%3A%20Improving%20Efficient%20and%20Robust%20Facial%20Landmark%20Detection%20with%0A%20%20Parallel%20Optimal%20Position%20Search&entry.906535625=Chong-Yang%20Xiang%20and%20Jun-Yan%20He%20and%20Zhi-Qi%20Cheng%20and%20Xiao%20Wu%20and%20Xian-Sheng%20Hua&entry.1292438233=%20%20Achieving%20a%20balance%20between%20accuracy%20and%20efficiency%20is%20a%20critical%20challenge%0Ain%20facial%20landmark%20detection%20%28FLD%29.%20This%20paper%20introduces%20Parallel%20Optimal%0APosition%20Search%20%28POPoS%29%2C%20a%20high-precision%20encoding-decoding%20framework%20designed%0Ato%20address%20the%20limitations%20of%20traditional%20FLD%20methods.%20POPoS%20employs%20three%20key%0Acontributions%3A%20%281%29%20Pseudo-range%20multilateration%20is%20utilized%20to%20correct%20heatmap%0Aerrors%2C%20improving%20landmark%20localization%20accuracy.%20By%20integrating%20multiple%0Aanchor%20points%2C%20it%20reduces%20the%20impact%20of%20individual%20heatmap%20inaccuracies%2C%0Aleading%20to%20robust%20overall%20positioning.%20%282%29%20To%20enhance%20the%20pseudo-range%20accuracy%0Aof%20selected%20anchor%20points%2C%20a%20new%20loss%20function%2C%20named%20multilateration%20anchor%0Aloss%2C%20is%20proposed.%20This%20loss%20function%20enhances%20the%20accuracy%20of%20the%20distance%0Amap%2C%20mitigates%20the%20risk%20of%20local%20optima%2C%20and%20ensures%20optimal%20solutions.%20%283%29%20A%0Asingle-step%20parallel%20computation%20algorithm%20is%20introduced%2C%20boosting%0Acomputational%20efficiency%20and%20reducing%20processing%20time.%20Extensive%20evaluations%0Aacross%20five%20benchmark%20datasets%20demonstrate%20that%20POPoS%20consistently%20outperforms%0Aexisting%20methods%2C%20particularly%20excelling%20in%20low-resolution%20heatmaps%20scenarios%0Awith%20minimal%20computational%20overhead.%20These%20advantages%20make%20POPoS%20a%20highly%0Aefficient%20and%20accurate%20tool%20for%20FLD%2C%20with%20broad%20applicability%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09583v5&entry.124074799=Read"},
{"title": "Data-Centric Improvements for Enhancing Multi-Modal Understanding in\n  Spoken Conversation Modeling", "author": "Maximillian Chen and Ruoxi Sun and Sercan \u00d6. Ar\u0131k", "abstract": "  Conversational assistants are increasingly popular across diverse real-world\napplications, highlighting the need for advanced multimodal speech modeling.\nSpeech, as a natural mode of communication, encodes rich user-specific\ncharacteristics such as speaking rate and pitch, making it critical for\neffective interaction. Our work introduces a data-centric customization\napproach for efficiently enhancing multimodal understanding in conversational\nspeech modeling. Central to our contributions is a novel multi-task learning\nparadigm that involves designing auxiliary tasks to utilize a small amount of\nspeech data. Our approach achieves state-of-the-art performance on the\nSpoken-SQuAD benchmark, using only 10% of the training data with open-weight\nmodels, establishing a robust and efficient framework for audio-centric\nconversational modeling. We also introduce ASK-QA, the first dataset for\nmulti-turn spoken dialogue with ambiguous user requests and dynamic evaluation\ninputs. Code and data forthcoming.\n", "link": "http://arxiv.org/abs/2412.15995v1", "date": "2024-12-20", "relevancy": 2.7003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Centric%20Improvements%20for%20Enhancing%20Multi-Modal%20Understanding%20in%0A%20%20Spoken%20Conversation%20Modeling&body=Title%3A%20Data-Centric%20Improvements%20for%20Enhancing%20Multi-Modal%20Understanding%20in%0A%20%20Spoken%20Conversation%20Modeling%0AAuthor%3A%20Maximillian%20Chen%20and%20Ruoxi%20Sun%20and%20Sercan%20%C3%96.%20Ar%C4%B1k%0AAbstract%3A%20%20%20Conversational%20assistants%20are%20increasingly%20popular%20across%20diverse%20real-world%0Aapplications%2C%20highlighting%20the%20need%20for%20advanced%20multimodal%20speech%20modeling.%0ASpeech%2C%20as%20a%20natural%20mode%20of%20communication%2C%20encodes%20rich%20user-specific%0Acharacteristics%20such%20as%20speaking%20rate%20and%20pitch%2C%20making%20it%20critical%20for%0Aeffective%20interaction.%20Our%20work%20introduces%20a%20data-centric%20customization%0Aapproach%20for%20efficiently%20enhancing%20multimodal%20understanding%20in%20conversational%0Aspeech%20modeling.%20Central%20to%20our%20contributions%20is%20a%20novel%20multi-task%20learning%0Aparadigm%20that%20involves%20designing%20auxiliary%20tasks%20to%20utilize%20a%20small%20amount%20of%0Aspeech%20data.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20the%0ASpoken-SQuAD%20benchmark%2C%20using%20only%2010%25%20of%20the%20training%20data%20with%20open-weight%0Amodels%2C%20establishing%20a%20robust%20and%20efficient%20framework%20for%20audio-centric%0Aconversational%20modeling.%20We%20also%20introduce%20ASK-QA%2C%20the%20first%20dataset%20for%0Amulti-turn%20spoken%20dialogue%20with%20ambiguous%20user%20requests%20and%20dynamic%20evaluation%0Ainputs.%20Code%20and%20data%20forthcoming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Centric%2520Improvements%2520for%2520Enhancing%2520Multi-Modal%2520Understanding%2520in%250A%2520%2520Spoken%2520Conversation%2520Modeling%26entry.906535625%3DMaximillian%2520Chen%2520and%2520Ruoxi%2520Sun%2520and%2520Sercan%2520%25C3%2596.%2520Ar%25C4%25B1k%26entry.1292438233%3D%2520%2520Conversational%2520assistants%2520are%2520increasingly%2520popular%2520across%2520diverse%2520real-world%250Aapplications%252C%2520highlighting%2520the%2520need%2520for%2520advanced%2520multimodal%2520speech%2520modeling.%250ASpeech%252C%2520as%2520a%2520natural%2520mode%2520of%2520communication%252C%2520encodes%2520rich%2520user-specific%250Acharacteristics%2520such%2520as%2520speaking%2520rate%2520and%2520pitch%252C%2520making%2520it%2520critical%2520for%250Aeffective%2520interaction.%2520Our%2520work%2520introduces%2520a%2520data-centric%2520customization%250Aapproach%2520for%2520efficiently%2520enhancing%2520multimodal%2520understanding%2520in%2520conversational%250Aspeech%2520modeling.%2520Central%2520to%2520our%2520contributions%2520is%2520a%2520novel%2520multi-task%2520learning%250Aparadigm%2520that%2520involves%2520designing%2520auxiliary%2520tasks%2520to%2520utilize%2520a%2520small%2520amount%2520of%250Aspeech%2520data.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250ASpoken-SQuAD%2520benchmark%252C%2520using%2520only%252010%2525%2520of%2520the%2520training%2520data%2520with%2520open-weight%250Amodels%252C%2520establishing%2520a%2520robust%2520and%2520efficient%2520framework%2520for%2520audio-centric%250Aconversational%2520modeling.%2520We%2520also%2520introduce%2520ASK-QA%252C%2520the%2520first%2520dataset%2520for%250Amulti-turn%2520spoken%2520dialogue%2520with%2520ambiguous%2520user%2520requests%2520and%2520dynamic%2520evaluation%250Ainputs.%2520Code%2520and%2520data%2520forthcoming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Centric%20Improvements%20for%20Enhancing%20Multi-Modal%20Understanding%20in%0A%20%20Spoken%20Conversation%20Modeling&entry.906535625=Maximillian%20Chen%20and%20Ruoxi%20Sun%20and%20Sercan%20%C3%96.%20Ar%C4%B1k&entry.1292438233=%20%20Conversational%20assistants%20are%20increasingly%20popular%20across%20diverse%20real-world%0Aapplications%2C%20highlighting%20the%20need%20for%20advanced%20multimodal%20speech%20modeling.%0ASpeech%2C%20as%20a%20natural%20mode%20of%20communication%2C%20encodes%20rich%20user-specific%0Acharacteristics%20such%20as%20speaking%20rate%20and%20pitch%2C%20making%20it%20critical%20for%0Aeffective%20interaction.%20Our%20work%20introduces%20a%20data-centric%20customization%0Aapproach%20for%20efficiently%20enhancing%20multimodal%20understanding%20in%20conversational%0Aspeech%20modeling.%20Central%20to%20our%20contributions%20is%20a%20novel%20multi-task%20learning%0Aparadigm%20that%20involves%20designing%20auxiliary%20tasks%20to%20utilize%20a%20small%20amount%20of%0Aspeech%20data.%20Our%20approach%20achieves%20state-of-the-art%20performance%20on%20the%0ASpoken-SQuAD%20benchmark%2C%20using%20only%2010%25%20of%20the%20training%20data%20with%20open-weight%0Amodels%2C%20establishing%20a%20robust%20and%20efficient%20framework%20for%20audio-centric%0Aconversational%20modeling.%20We%20also%20introduce%20ASK-QA%2C%20the%20first%20dataset%20for%0Amulti-turn%20spoken%20dialogue%20with%20ambiguous%20user%20requests%20and%20dynamic%20evaluation%0Ainputs.%20Code%20and%20data%20forthcoming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15995v1&entry.124074799=Read"},
{"title": "Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly", "author": "Peyman Hosseini and Ignacio Castro and Iacopo Ghinassi and Matthew Purver", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.\n", "link": "http://arxiv.org/abs/2408.01866v3", "date": "2024-12-20", "relevancy": 2.6855, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Solutions%20For%20An%20Intriguing%20Failure%20of%20LLMs%3A%20Long%20Context%0A%20%20Window%20Does%20Not%20Mean%20LLMs%20Can%20Analyze%20Long%20Sequences%20Flawlessly&body=Title%3A%20Efficient%20Solutions%20For%20An%20Intriguing%20Failure%20of%20LLMs%3A%20Long%20Context%0A%20%20Window%20Does%20Not%20Mean%20LLMs%20Can%20Analyze%20Long%20Sequences%20Flawlessly%0AAuthor%3A%20Peyman%20Hosseini%20and%20Ignacio%20Castro%20and%20Iacopo%20Ghinassi%20and%20Matthew%20Purver%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acomprehending%20and%20analyzing%20lengthy%20sequential%20inputs%2C%20owing%20to%20their%20extensive%0Acontext%20windows%20that%20allow%20processing%20millions%20of%20tokens%20in%20a%20single%20forward%0Apass.%20However%2C%20this%20paper%20uncovers%20a%20surprising%20limitation%3A%20LLMs%20fall%20short%0Awhen%20handling%20long%20input%20sequences.%20We%20investigate%20this%20issue%20using%20three%0Adatasets%20and%20two%20tasks%20%28sentiment%20analysis%20and%20news%20categorization%29%20across%0Avarious%20LLMs%2C%20including%20Claude%203%2C%20Gemini%20Pro%2C%20GPT%203.5%20Turbo%2C%20Llama%203%20Instruct%2C%0Aand%20Mistral%20Instruct%20models.%20To%20address%20this%20limitation%2C%20we%20propose%20and%0Aevaluate%20ad-hoc%20solutions%20that%20substantially%20enhance%20LLMs%27%20performance%20on%20long%0Ainput%20sequences%20by%20up%20to%2050%25%2C%20while%20reducing%20API%20cost%20and%20latency%20by%20up%20to%2093%25%0Aand%2050%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01866v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Solutions%2520For%2520An%2520Intriguing%2520Failure%2520of%2520LLMs%253A%2520Long%2520Context%250A%2520%2520Window%2520Does%2520Not%2520Mean%2520LLMs%2520Can%2520Analyze%2520Long%2520Sequences%2520Flawlessly%26entry.906535625%3DPeyman%2520Hosseini%2520and%2520Ignacio%2520Castro%2520and%2520Iacopo%2520Ghinassi%2520and%2520Matthew%2520Purver%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Acomprehending%2520and%2520analyzing%2520lengthy%2520sequential%2520inputs%252C%2520owing%2520to%2520their%2520extensive%250Acontext%2520windows%2520that%2520allow%2520processing%2520millions%2520of%2520tokens%2520in%2520a%2520single%2520forward%250Apass.%2520However%252C%2520this%2520paper%2520uncovers%2520a%2520surprising%2520limitation%253A%2520LLMs%2520fall%2520short%250Awhen%2520handling%2520long%2520input%2520sequences.%2520We%2520investigate%2520this%2520issue%2520using%2520three%250Adatasets%2520and%2520two%2520tasks%2520%2528sentiment%2520analysis%2520and%2520news%2520categorization%2529%2520across%250Avarious%2520LLMs%252C%2520including%2520Claude%25203%252C%2520Gemini%2520Pro%252C%2520GPT%25203.5%2520Turbo%252C%2520Llama%25203%2520Instruct%252C%250Aand%2520Mistral%2520Instruct%2520models.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520and%250Aevaluate%2520ad-hoc%2520solutions%2520that%2520substantially%2520enhance%2520LLMs%2527%2520performance%2520on%2520long%250Ainput%2520sequences%2520by%2520up%2520to%252050%2525%252C%2520while%2520reducing%2520API%2520cost%2520and%2520latency%2520by%2520up%2520to%252093%2525%250Aand%252050%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01866v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Solutions%20For%20An%20Intriguing%20Failure%20of%20LLMs%3A%20Long%20Context%0A%20%20Window%20Does%20Not%20Mean%20LLMs%20Can%20Analyze%20Long%20Sequences%20Flawlessly&entry.906535625=Peyman%20Hosseini%20and%20Ignacio%20Castro%20and%20Iacopo%20Ghinassi%20and%20Matthew%20Purver&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acomprehending%20and%20analyzing%20lengthy%20sequential%20inputs%2C%20owing%20to%20their%20extensive%0Acontext%20windows%20that%20allow%20processing%20millions%20of%20tokens%20in%20a%20single%20forward%0Apass.%20However%2C%20this%20paper%20uncovers%20a%20surprising%20limitation%3A%20LLMs%20fall%20short%0Awhen%20handling%20long%20input%20sequences.%20We%20investigate%20this%20issue%20using%20three%0Adatasets%20and%20two%20tasks%20%28sentiment%20analysis%20and%20news%20categorization%29%20across%0Avarious%20LLMs%2C%20including%20Claude%203%2C%20Gemini%20Pro%2C%20GPT%203.5%20Turbo%2C%20Llama%203%20Instruct%2C%0Aand%20Mistral%20Instruct%20models.%20To%20address%20this%20limitation%2C%20we%20propose%20and%0Aevaluate%20ad-hoc%20solutions%20that%20substantially%20enhance%20LLMs%27%20performance%20on%20long%0Ainput%20sequences%20by%20up%20to%2050%25%2C%20while%20reducing%20API%20cost%20and%20latency%20by%20up%20to%2093%25%0Aand%2050%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01866v3&entry.124074799=Read"},
{"title": "Language Repository for Long Video Understanding", "author": "Kumara Kahatapitiya and Kanchana Ranasinghe and Jongwoo Park and Michael S. Ryoo", "abstract": "  Language has become a prominent modality in computer vision with the rise of\nLLMs. Despite supporting long context-lengths, their effectiveness in handling\nlong-term information gradually declines with input length. This becomes\ncritical, especially in applications such as long-form video understanding. In\nthis paper, we introduce a Language Repository (LangRepo) for LLMs, that\nmaintains concise and structured information as an interpretable (i.e.,\nall-textual) representation. Our repository is updated iteratively based on\nmulti-scale video chunks. We introduce write and read operations that focus on\npruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.\n", "link": "http://arxiv.org/abs/2403.14622v2", "date": "2024-12-20", "relevancy": 2.6715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Repository%20for%20Long%20Video%20Understanding&body=Title%3A%20Language%20Repository%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Kumara%20Kahatapitiya%20and%20Kanchana%20Ranasinghe%20and%20Jongwoo%20Park%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Language%20has%20become%20a%20prominent%20modality%20in%20computer%20vision%20with%20the%20rise%20of%0ALLMs.%20Despite%20supporting%20long%20context-lengths%2C%20their%20effectiveness%20in%20handling%0Along-term%20information%20gradually%20declines%20with%20input%20length.%20This%20becomes%0Acritical%2C%20especially%20in%20applications%20such%20as%20long-form%20video%20understanding.%20In%0Athis%20paper%2C%20we%20introduce%20a%20Language%20Repository%20%28LangRepo%29%20for%20LLMs%2C%20that%0Amaintains%20concise%20and%20structured%20information%20as%20an%20interpretable%20%28i.e.%2C%0Aall-textual%29%20representation.%20Our%20repository%20is%20updated%20iteratively%20based%20on%0Amulti-scale%20video%20chunks.%20We%20introduce%20write%20and%20read%20operations%20that%20focus%20on%0Apruning%20redundancies%20in%20text%2C%20and%20extracting%20information%20at%20various%20temporal%0Ascales.%20The%20proposed%20framework%20is%20evaluated%20on%20zero-shot%20visual%0Aquestion-answering%20benchmarks%20including%20EgoSchema%2C%20NExT-QA%2C%20IntentQA%20and%0ANExT-GQA%2C%20showing%20state-of-the-art%20performance%20at%20its%20scale.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/kkahatapitiya/LangRepo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Repository%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DKumara%2520Kahatapitiya%2520and%2520Kanchana%2520Ranasinghe%2520and%2520Jongwoo%2520Park%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Language%2520has%2520become%2520a%2520prominent%2520modality%2520in%2520computer%2520vision%2520with%2520the%2520rise%2520of%250ALLMs.%2520Despite%2520supporting%2520long%2520context-lengths%252C%2520their%2520effectiveness%2520in%2520handling%250Along-term%2520information%2520gradually%2520declines%2520with%2520input%2520length.%2520This%2520becomes%250Acritical%252C%2520especially%2520in%2520applications%2520such%2520as%2520long-form%2520video%2520understanding.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520Language%2520Repository%2520%2528LangRepo%2529%2520for%2520LLMs%252C%2520that%250Amaintains%2520concise%2520and%2520structured%2520information%2520as%2520an%2520interpretable%2520%2528i.e.%252C%250Aall-textual%2529%2520representation.%2520Our%2520repository%2520is%2520updated%2520iteratively%2520based%2520on%250Amulti-scale%2520video%2520chunks.%2520We%2520introduce%2520write%2520and%2520read%2520operations%2520that%2520focus%2520on%250Apruning%2520redundancies%2520in%2520text%252C%2520and%2520extracting%2520information%2520at%2520various%2520temporal%250Ascales.%2520The%2520proposed%2520framework%2520is%2520evaluated%2520on%2520zero-shot%2520visual%250Aquestion-answering%2520benchmarks%2520including%2520EgoSchema%252C%2520NExT-QA%252C%2520IntentQA%2520and%250ANExT-GQA%252C%2520showing%2520state-of-the-art%2520performance%2520at%2520its%2520scale.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/kkahatapitiya/LangRepo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Repository%20for%20Long%20Video%20Understanding&entry.906535625=Kumara%20Kahatapitiya%20and%20Kanchana%20Ranasinghe%20and%20Jongwoo%20Park%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Language%20has%20become%20a%20prominent%20modality%20in%20computer%20vision%20with%20the%20rise%20of%0ALLMs.%20Despite%20supporting%20long%20context-lengths%2C%20their%20effectiveness%20in%20handling%0Along-term%20information%20gradually%20declines%20with%20input%20length.%20This%20becomes%0Acritical%2C%20especially%20in%20applications%20such%20as%20long-form%20video%20understanding.%20In%0Athis%20paper%2C%20we%20introduce%20a%20Language%20Repository%20%28LangRepo%29%20for%20LLMs%2C%20that%0Amaintains%20concise%20and%20structured%20information%20as%20an%20interpretable%20%28i.e.%2C%0Aall-textual%29%20representation.%20Our%20repository%20is%20updated%20iteratively%20based%20on%0Amulti-scale%20video%20chunks.%20We%20introduce%20write%20and%20read%20operations%20that%20focus%20on%0Apruning%20redundancies%20in%20text%2C%20and%20extracting%20information%20at%20various%20temporal%0Ascales.%20The%20proposed%20framework%20is%20evaluated%20on%20zero-shot%20visual%0Aquestion-answering%20benchmarks%20including%20EgoSchema%2C%20NExT-QA%2C%20IntentQA%20and%0ANExT-GQA%2C%20showing%20state-of-the-art%20performance%20at%20its%20scale.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/kkahatapitiya/LangRepo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14622v2&entry.124074799=Read"},
{"title": "Explicit View-labels Matter: A Multifacet Complementarity Study of\n  Multi-view Clustering", "author": "Chuanxing Geng and Aiyang Han and Songcan Chen", "abstract": "  Consistency and complementarity are two key ingredients for boosting\nmulti-view clustering (MVC). Recently with the introduction of popular\ncontrastive learning, the consistency learning of views has been further\nenhanced in MVC, leading to promising performance. However, by contrast, the\ncomplementarity has not received sufficient attention except just in the\nfeature facet, where the Hilbert Schmidt Independence Criterion term or the\nindependent encoder-decoder network is usually adopted to capture view-specific\ninformation. This motivates us to reconsider the complementarity learning of\nviews comprehensively from multiple facets including the feature-, view-label-\nand contrast- facets, while maintaining the view consistency. We empirically\nfind that all the facets contribute to the complementarity learning, especially\nthe view-label facet, which is usually neglected by existing methods. Based on\nthis, a simple yet effective \\underline{M}ultifacet \\underline{C}omplementarity\nlearning framework for \\underline{M}ulti-\\underline{V}iew\n\\underline{C}lustering (MCMVC) is naturally developed, which fuses multifacet\ncomplementarity information, especially explicitly embedding the view-label\ninformation. To our best knowledge, it is the first time to use view-labels\nexplicitly to guide the complementarity learning of views. Compared with the\nSOTA baselines, MCMVC achieves remarkable improvements, e.g., by average\nmargins over $5.00\\%$ and $7.00\\%$ respectively in complete and incomplete MVC\nsettings on Caltech101-20 in terms of three evaluation metrics.\n", "link": "http://arxiv.org/abs/2205.02507v3", "date": "2024-12-20", "relevancy": 2.6064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20View-labels%20Matter%3A%20A%20Multifacet%20Complementarity%20Study%20of%0A%20%20Multi-view%20Clustering&body=Title%3A%20Explicit%20View-labels%20Matter%3A%20A%20Multifacet%20Complementarity%20Study%20of%0A%20%20Multi-view%20Clustering%0AAuthor%3A%20Chuanxing%20Geng%20and%20Aiyang%20Han%20and%20Songcan%20Chen%0AAbstract%3A%20%20%20Consistency%20and%20complementarity%20are%20two%20key%20ingredients%20for%20boosting%0Amulti-view%20clustering%20%28MVC%29.%20Recently%20with%20the%20introduction%20of%20popular%0Acontrastive%20learning%2C%20the%20consistency%20learning%20of%20views%20has%20been%20further%0Aenhanced%20in%20MVC%2C%20leading%20to%20promising%20performance.%20However%2C%20by%20contrast%2C%20the%0Acomplementarity%20has%20not%20received%20sufficient%20attention%20except%20just%20in%20the%0Afeature%20facet%2C%20where%20the%20Hilbert%20Schmidt%20Independence%20Criterion%20term%20or%20the%0Aindependent%20encoder-decoder%20network%20is%20usually%20adopted%20to%20capture%20view-specific%0Ainformation.%20This%20motivates%20us%20to%20reconsider%20the%20complementarity%20learning%20of%0Aviews%20comprehensively%20from%20multiple%20facets%20including%20the%20feature-%2C%20view-label-%0Aand%20contrast-%20facets%2C%20while%20maintaining%20the%20view%20consistency.%20We%20empirically%0Afind%20that%20all%20the%20facets%20contribute%20to%20the%20complementarity%20learning%2C%20especially%0Athe%20view-label%20facet%2C%20which%20is%20usually%20neglected%20by%20existing%20methods.%20Based%20on%0Athis%2C%20a%20simple%20yet%20effective%20%5Cunderline%7BM%7Dultifacet%20%5Cunderline%7BC%7Domplementarity%0Alearning%20framework%20for%20%5Cunderline%7BM%7Dulti-%5Cunderline%7BV%7Diew%0A%5Cunderline%7BC%7Dlustering%20%28MCMVC%29%20is%20naturally%20developed%2C%20which%20fuses%20multifacet%0Acomplementarity%20information%2C%20especially%20explicitly%20embedding%20the%20view-label%0Ainformation.%20To%20our%20best%20knowledge%2C%20it%20is%20the%20first%20time%20to%20use%20view-labels%0Aexplicitly%20to%20guide%20the%20complementarity%20learning%20of%20views.%20Compared%20with%20the%0ASOTA%20baselines%2C%20MCMVC%20achieves%20remarkable%20improvements%2C%20e.g.%2C%20by%20average%0Amargins%20over%20%245.00%5C%25%24%20and%20%247.00%5C%25%24%20respectively%20in%20complete%20and%20incomplete%20MVC%0Asettings%20on%20Caltech101-20%20in%20terms%20of%20three%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.02507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520View-labels%2520Matter%253A%2520A%2520Multifacet%2520Complementarity%2520Study%2520of%250A%2520%2520Multi-view%2520Clustering%26entry.906535625%3DChuanxing%2520Geng%2520and%2520Aiyang%2520Han%2520and%2520Songcan%2520Chen%26entry.1292438233%3D%2520%2520Consistency%2520and%2520complementarity%2520are%2520two%2520key%2520ingredients%2520for%2520boosting%250Amulti-view%2520clustering%2520%2528MVC%2529.%2520Recently%2520with%2520the%2520introduction%2520of%2520popular%250Acontrastive%2520learning%252C%2520the%2520consistency%2520learning%2520of%2520views%2520has%2520been%2520further%250Aenhanced%2520in%2520MVC%252C%2520leading%2520to%2520promising%2520performance.%2520However%252C%2520by%2520contrast%252C%2520the%250Acomplementarity%2520has%2520not%2520received%2520sufficient%2520attention%2520except%2520just%2520in%2520the%250Afeature%2520facet%252C%2520where%2520the%2520Hilbert%2520Schmidt%2520Independence%2520Criterion%2520term%2520or%2520the%250Aindependent%2520encoder-decoder%2520network%2520is%2520usually%2520adopted%2520to%2520capture%2520view-specific%250Ainformation.%2520This%2520motivates%2520us%2520to%2520reconsider%2520the%2520complementarity%2520learning%2520of%250Aviews%2520comprehensively%2520from%2520multiple%2520facets%2520including%2520the%2520feature-%252C%2520view-label-%250Aand%2520contrast-%2520facets%252C%2520while%2520maintaining%2520the%2520view%2520consistency.%2520We%2520empirically%250Afind%2520that%2520all%2520the%2520facets%2520contribute%2520to%2520the%2520complementarity%2520learning%252C%2520especially%250Athe%2520view-label%2520facet%252C%2520which%2520is%2520usually%2520neglected%2520by%2520existing%2520methods.%2520Based%2520on%250Athis%252C%2520a%2520simple%2520yet%2520effective%2520%255Cunderline%257BM%257Dultifacet%2520%255Cunderline%257BC%257Domplementarity%250Alearning%2520framework%2520for%2520%255Cunderline%257BM%257Dulti-%255Cunderline%257BV%257Diew%250A%255Cunderline%257BC%257Dlustering%2520%2528MCMVC%2529%2520is%2520naturally%2520developed%252C%2520which%2520fuses%2520multifacet%250Acomplementarity%2520information%252C%2520especially%2520explicitly%2520embedding%2520the%2520view-label%250Ainformation.%2520To%2520our%2520best%2520knowledge%252C%2520it%2520is%2520the%2520first%2520time%2520to%2520use%2520view-labels%250Aexplicitly%2520to%2520guide%2520the%2520complementarity%2520learning%2520of%2520views.%2520Compared%2520with%2520the%250ASOTA%2520baselines%252C%2520MCMVC%2520achieves%2520remarkable%2520improvements%252C%2520e.g.%252C%2520by%2520average%250Amargins%2520over%2520%25245.00%255C%2525%2524%2520and%2520%25247.00%255C%2525%2524%2520respectively%2520in%2520complete%2520and%2520incomplete%2520MVC%250Asettings%2520on%2520Caltech101-20%2520in%2520terms%2520of%2520three%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.02507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20View-labels%20Matter%3A%20A%20Multifacet%20Complementarity%20Study%20of%0A%20%20Multi-view%20Clustering&entry.906535625=Chuanxing%20Geng%20and%20Aiyang%20Han%20and%20Songcan%20Chen&entry.1292438233=%20%20Consistency%20and%20complementarity%20are%20two%20key%20ingredients%20for%20boosting%0Amulti-view%20clustering%20%28MVC%29.%20Recently%20with%20the%20introduction%20of%20popular%0Acontrastive%20learning%2C%20the%20consistency%20learning%20of%20views%20has%20been%20further%0Aenhanced%20in%20MVC%2C%20leading%20to%20promising%20performance.%20However%2C%20by%20contrast%2C%20the%0Acomplementarity%20has%20not%20received%20sufficient%20attention%20except%20just%20in%20the%0Afeature%20facet%2C%20where%20the%20Hilbert%20Schmidt%20Independence%20Criterion%20term%20or%20the%0Aindependent%20encoder-decoder%20network%20is%20usually%20adopted%20to%20capture%20view-specific%0Ainformation.%20This%20motivates%20us%20to%20reconsider%20the%20complementarity%20learning%20of%0Aviews%20comprehensively%20from%20multiple%20facets%20including%20the%20feature-%2C%20view-label-%0Aand%20contrast-%20facets%2C%20while%20maintaining%20the%20view%20consistency.%20We%20empirically%0Afind%20that%20all%20the%20facets%20contribute%20to%20the%20complementarity%20learning%2C%20especially%0Athe%20view-label%20facet%2C%20which%20is%20usually%20neglected%20by%20existing%20methods.%20Based%20on%0Athis%2C%20a%20simple%20yet%20effective%20%5Cunderline%7BM%7Dultifacet%20%5Cunderline%7BC%7Domplementarity%0Alearning%20framework%20for%20%5Cunderline%7BM%7Dulti-%5Cunderline%7BV%7Diew%0A%5Cunderline%7BC%7Dlustering%20%28MCMVC%29%20is%20naturally%20developed%2C%20which%20fuses%20multifacet%0Acomplementarity%20information%2C%20especially%20explicitly%20embedding%20the%20view-label%0Ainformation.%20To%20our%20best%20knowledge%2C%20it%20is%20the%20first%20time%20to%20use%20view-labels%0Aexplicitly%20to%20guide%20the%20complementarity%20learning%20of%20views.%20Compared%20with%20the%0ASOTA%20baselines%2C%20MCMVC%20achieves%20remarkable%20improvements%2C%20e.g.%2C%20by%20average%0Amargins%20over%20%245.00%5C%25%24%20and%20%247.00%5C%25%24%20respectively%20in%20complete%20and%20incomplete%20MVC%0Asettings%20on%20Caltech101-20%20in%20terms%20of%20three%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.02507v3&entry.124074799=Read"},
{"title": "Segmentation of arbitrary features in very high resolution remote\n  sensing imagery", "author": "Henry Cording and Yves Plancherel and Pablo Brito-Parada", "abstract": "  Very high resolution (VHR) mapping through remote sensing (RS) imagery\npresents a new opportunity to inform decision-making and sustainable practices\nin countless domains. Efficient processing of big VHR data requires automated\ntools applicable to numerous geographic regions and features. Contemporary RS\nstudies address this challenge by employing deep learning (DL) models for\nspecific datasets or features, which limits their applicability across\ncontexts.\n  The present research aims to overcome this limitation by introducing\nEcoMapper, a scalable solution to segment arbitrary features in VHR RS imagery.\nEcoMapper fully automates processing of geospatial data, DL model training, and\ninference. Models trained with EcoMapper successfully segmented two distinct\nfeatures in a real-world UAV dataset, achieving scores competitive with prior\nstudies which employed context-specific models.\n  To evaluate EcoMapper, many additional models were trained on permutations of\nprincipal field survey characteristics (FSCs). A relationship was discovered\nallowing derivation of optimal ground sampling distance from feature size,\ntermed Cording Index (CI). A comprehensive methodology for field surveys was\ndeveloped to ensure DL methods can be applied effectively to collected data.\n  The EcoMapper code accompanying this work is available at\nhttps://github.com/hcording/ecomapper .\n", "link": "http://arxiv.org/abs/2412.16046v1", "date": "2024-12-20", "relevancy": 2.5826, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20of%20arbitrary%20features%20in%20very%20high%20resolution%20remote%0A%20%20sensing%20imagery&body=Title%3A%20Segmentation%20of%20arbitrary%20features%20in%20very%20high%20resolution%20remote%0A%20%20sensing%20imagery%0AAuthor%3A%20Henry%20Cording%20and%20Yves%20Plancherel%20and%20Pablo%20Brito-Parada%0AAbstract%3A%20%20%20Very%20high%20resolution%20%28VHR%29%20mapping%20through%20remote%20sensing%20%28RS%29%20imagery%0Apresents%20a%20new%20opportunity%20to%20inform%20decision-making%20and%20sustainable%20practices%0Ain%20countless%20domains.%20Efficient%20processing%20of%20big%20VHR%20data%20requires%20automated%0Atools%20applicable%20to%20numerous%20geographic%20regions%20and%20features.%20Contemporary%20RS%0Astudies%20address%20this%20challenge%20by%20employing%20deep%20learning%20%28DL%29%20models%20for%0Aspecific%20datasets%20or%20features%2C%20which%20limits%20their%20applicability%20across%0Acontexts.%0A%20%20The%20present%20research%20aims%20to%20overcome%20this%20limitation%20by%20introducing%0AEcoMapper%2C%20a%20scalable%20solution%20to%20segment%20arbitrary%20features%20in%20VHR%20RS%20imagery.%0AEcoMapper%20fully%20automates%20processing%20of%20geospatial%20data%2C%20DL%20model%20training%2C%20and%0Ainference.%20Models%20trained%20with%20EcoMapper%20successfully%20segmented%20two%20distinct%0Afeatures%20in%20a%20real-world%20UAV%20dataset%2C%20achieving%20scores%20competitive%20with%20prior%0Astudies%20which%20employed%20context-specific%20models.%0A%20%20To%20evaluate%20EcoMapper%2C%20many%20additional%20models%20were%20trained%20on%20permutations%20of%0Aprincipal%20field%20survey%20characteristics%20%28FSCs%29.%20A%20relationship%20was%20discovered%0Aallowing%20derivation%20of%20optimal%20ground%20sampling%20distance%20from%20feature%20size%2C%0Atermed%20Cording%20Index%20%28CI%29.%20A%20comprehensive%20methodology%20for%20field%20surveys%20was%0Adeveloped%20to%20ensure%20DL%20methods%20can%20be%20applied%20effectively%20to%20collected%20data.%0A%20%20The%20EcoMapper%20code%20accompanying%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/hcording/ecomapper%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520of%2520arbitrary%2520features%2520in%2520very%2520high%2520resolution%2520remote%250A%2520%2520sensing%2520imagery%26entry.906535625%3DHenry%2520Cording%2520and%2520Yves%2520Plancherel%2520and%2520Pablo%2520Brito-Parada%26entry.1292438233%3D%2520%2520Very%2520high%2520resolution%2520%2528VHR%2529%2520mapping%2520through%2520remote%2520sensing%2520%2528RS%2529%2520imagery%250Apresents%2520a%2520new%2520opportunity%2520to%2520inform%2520decision-making%2520and%2520sustainable%2520practices%250Ain%2520countless%2520domains.%2520Efficient%2520processing%2520of%2520big%2520VHR%2520data%2520requires%2520automated%250Atools%2520applicable%2520to%2520numerous%2520geographic%2520regions%2520and%2520features.%2520Contemporary%2520RS%250Astudies%2520address%2520this%2520challenge%2520by%2520employing%2520deep%2520learning%2520%2528DL%2529%2520models%2520for%250Aspecific%2520datasets%2520or%2520features%252C%2520which%2520limits%2520their%2520applicability%2520across%250Acontexts.%250A%2520%2520The%2520present%2520research%2520aims%2520to%2520overcome%2520this%2520limitation%2520by%2520introducing%250AEcoMapper%252C%2520a%2520scalable%2520solution%2520to%2520segment%2520arbitrary%2520features%2520in%2520VHR%2520RS%2520imagery.%250AEcoMapper%2520fully%2520automates%2520processing%2520of%2520geospatial%2520data%252C%2520DL%2520model%2520training%252C%2520and%250Ainference.%2520Models%2520trained%2520with%2520EcoMapper%2520successfully%2520segmented%2520two%2520distinct%250Afeatures%2520in%2520a%2520real-world%2520UAV%2520dataset%252C%2520achieving%2520scores%2520competitive%2520with%2520prior%250Astudies%2520which%2520employed%2520context-specific%2520models.%250A%2520%2520To%2520evaluate%2520EcoMapper%252C%2520many%2520additional%2520models%2520were%2520trained%2520on%2520permutations%2520of%250Aprincipal%2520field%2520survey%2520characteristics%2520%2528FSCs%2529.%2520A%2520relationship%2520was%2520discovered%250Aallowing%2520derivation%2520of%2520optimal%2520ground%2520sampling%2520distance%2520from%2520feature%2520size%252C%250Atermed%2520Cording%2520Index%2520%2528CI%2529.%2520A%2520comprehensive%2520methodology%2520for%2520field%2520surveys%2520was%250Adeveloped%2520to%2520ensure%2520DL%2520methods%2520can%2520be%2520applied%2520effectively%2520to%2520collected%2520data.%250A%2520%2520The%2520EcoMapper%2520code%2520accompanying%2520this%2520work%2520is%2520available%2520at%250Ahttps%253A//github.com/hcording/ecomapper%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20of%20arbitrary%20features%20in%20very%20high%20resolution%20remote%0A%20%20sensing%20imagery&entry.906535625=Henry%20Cording%20and%20Yves%20Plancherel%20and%20Pablo%20Brito-Parada&entry.1292438233=%20%20Very%20high%20resolution%20%28VHR%29%20mapping%20through%20remote%20sensing%20%28RS%29%20imagery%0Apresents%20a%20new%20opportunity%20to%20inform%20decision-making%20and%20sustainable%20practices%0Ain%20countless%20domains.%20Efficient%20processing%20of%20big%20VHR%20data%20requires%20automated%0Atools%20applicable%20to%20numerous%20geographic%20regions%20and%20features.%20Contemporary%20RS%0Astudies%20address%20this%20challenge%20by%20employing%20deep%20learning%20%28DL%29%20models%20for%0Aspecific%20datasets%20or%20features%2C%20which%20limits%20their%20applicability%20across%0Acontexts.%0A%20%20The%20present%20research%20aims%20to%20overcome%20this%20limitation%20by%20introducing%0AEcoMapper%2C%20a%20scalable%20solution%20to%20segment%20arbitrary%20features%20in%20VHR%20RS%20imagery.%0AEcoMapper%20fully%20automates%20processing%20of%20geospatial%20data%2C%20DL%20model%20training%2C%20and%0Ainference.%20Models%20trained%20with%20EcoMapper%20successfully%20segmented%20two%20distinct%0Afeatures%20in%20a%20real-world%20UAV%20dataset%2C%20achieving%20scores%20competitive%20with%20prior%0Astudies%20which%20employed%20context-specific%20models.%0A%20%20To%20evaluate%20EcoMapper%2C%20many%20additional%20models%20were%20trained%20on%20permutations%20of%0Aprincipal%20field%20survey%20characteristics%20%28FSCs%29.%20A%20relationship%20was%20discovered%0Aallowing%20derivation%20of%20optimal%20ground%20sampling%20distance%20from%20feature%20size%2C%0Atermed%20Cording%20Index%20%28CI%29.%20A%20comprehensive%20methodology%20for%20field%20surveys%20was%0Adeveloped%20to%20ensure%20DL%20methods%20can%20be%20applied%20effectively%20to%20collected%20data.%0A%20%20The%20EcoMapper%20code%20accompanying%20this%20work%20is%20available%20at%0Ahttps%3A//github.com/hcording/ecomapper%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16046v1&entry.124074799=Read"},
{"title": "Experience of Training a 1.7B-Parameter LLaMa Model From Scratch", "author": "Miles Q. Li and Benjamin C. M. Fung and Shih-Chia Huang", "abstract": "  Pretraining large language models is a complex endeavor influenced by\nmultiple factors, including model architecture, data quality, training\ncontinuity, and hardware constraints. In this paper, we share insights gained\nfrom the experience of training DMaS-LLaMa-Lite, a fully open source,\n1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of\ncarefully curated data. We chronicle the full training trajectory, documenting\nhow evolving validation loss levels and downstream benchmarks reflect\ntransitions from incoherent text to fluent, contextually grounded output.\nBeyond pretraining, we extend our analysis to include a post-training phase\nfocused on instruction tuning, where the model was refined to produce more\ncontextually appropriate, user-aligned responses. We highlight practical\nconsiderations such as the importance of restoring optimizer states when\nresuming from checkpoints, and the impact of hardware changes on training\nstability and throughput. While qualitative evaluation provides an intuitive\nunderstanding of model improvements, our analysis extends to various\nperformance benchmarks, demonstrating how high-quality data and thoughtful\nscaling enable competitive results with significantly fewer training tokens. By\ndetailing these experiences and offering training logs, checkpoints, and sample\noutputs, we aim to guide future researchers and practitioners in refining their\npretraining strategies. The training script is available on Github at\nhttps://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model\ncheckpoints are available on Huggingface at\nhttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.\n", "link": "http://arxiv.org/abs/2412.13335v2", "date": "2024-12-20", "relevancy": 2.5406, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experience%20of%20Training%20a%201.7B-Parameter%20LLaMa%20Model%20From%20Scratch&body=Title%3A%20Experience%20of%20Training%20a%201.7B-Parameter%20LLaMa%20Model%20From%20Scratch%0AAuthor%3A%20Miles%20Q.%20Li%20and%20Benjamin%20C.%20M.%20Fung%20and%20Shih-Chia%20Huang%0AAbstract%3A%20%20%20Pretraining%20large%20language%20models%20is%20a%20complex%20endeavor%20influenced%20by%0Amultiple%20factors%2C%20including%20model%20architecture%2C%20data%20quality%2C%20training%0Acontinuity%2C%20and%20hardware%20constraints.%20In%20this%20paper%2C%20we%20share%20insights%20gained%0Afrom%20the%20experience%20of%20training%20DMaS-LLaMa-Lite%2C%20a%20fully%20open%20source%2C%0A1.7-billion-parameter%2C%20LLaMa-based%20model%2C%20on%20approximately%2020%20billion%20tokens%20of%0Acarefully%20curated%20data.%20We%20chronicle%20the%20full%20training%20trajectory%2C%20documenting%0Ahow%20evolving%20validation%20loss%20levels%20and%20downstream%20benchmarks%20reflect%0Atransitions%20from%20incoherent%20text%20to%20fluent%2C%20contextually%20grounded%20output.%0ABeyond%20pretraining%2C%20we%20extend%20our%20analysis%20to%20include%20a%20post-training%20phase%0Afocused%20on%20instruction%20tuning%2C%20where%20the%20model%20was%20refined%20to%20produce%20more%0Acontextually%20appropriate%2C%20user-aligned%20responses.%20We%20highlight%20practical%0Aconsiderations%20such%20as%20the%20importance%20of%20restoring%20optimizer%20states%20when%0Aresuming%20from%20checkpoints%2C%20and%20the%20impact%20of%20hardware%20changes%20on%20training%0Astability%20and%20throughput.%20While%20qualitative%20evaluation%20provides%20an%20intuitive%0Aunderstanding%20of%20model%20improvements%2C%20our%20analysis%20extends%20to%20various%0Aperformance%20benchmarks%2C%20demonstrating%20how%20high-quality%20data%20and%20thoughtful%0Ascaling%20enable%20competitive%20results%20with%20significantly%20fewer%20training%20tokens.%20By%0Adetailing%20these%20experiences%20and%20offering%20training%20logs%2C%20checkpoints%2C%20and%20sample%0Aoutputs%2C%20we%20aim%20to%20guide%20future%20researchers%20and%20practitioners%20in%20refining%20their%0Apretraining%20strategies.%20The%20training%20script%20is%20available%20on%20Github%20at%0Ahttps%3A//github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code.%20The%20model%0Acheckpoints%20are%20available%20on%20Huggingface%20at%0Ahttps%3A//huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperience%2520of%2520Training%2520a%25201.7B-Parameter%2520LLaMa%2520Model%2520From%2520Scratch%26entry.906535625%3DMiles%2520Q.%2520Li%2520and%2520Benjamin%2520C.%2520M.%2520Fung%2520and%2520Shih-Chia%2520Huang%26entry.1292438233%3D%2520%2520Pretraining%2520large%2520language%2520models%2520is%2520a%2520complex%2520endeavor%2520influenced%2520by%250Amultiple%2520factors%252C%2520including%2520model%2520architecture%252C%2520data%2520quality%252C%2520training%250Acontinuity%252C%2520and%2520hardware%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520share%2520insights%2520gained%250Afrom%2520the%2520experience%2520of%2520training%2520DMaS-LLaMa-Lite%252C%2520a%2520fully%2520open%2520source%252C%250A1.7-billion-parameter%252C%2520LLaMa-based%2520model%252C%2520on%2520approximately%252020%2520billion%2520tokens%2520of%250Acarefully%2520curated%2520data.%2520We%2520chronicle%2520the%2520full%2520training%2520trajectory%252C%2520documenting%250Ahow%2520evolving%2520validation%2520loss%2520levels%2520and%2520downstream%2520benchmarks%2520reflect%250Atransitions%2520from%2520incoherent%2520text%2520to%2520fluent%252C%2520contextually%2520grounded%2520output.%250ABeyond%2520pretraining%252C%2520we%2520extend%2520our%2520analysis%2520to%2520include%2520a%2520post-training%2520phase%250Afocused%2520on%2520instruction%2520tuning%252C%2520where%2520the%2520model%2520was%2520refined%2520to%2520produce%2520more%250Acontextually%2520appropriate%252C%2520user-aligned%2520responses.%2520We%2520highlight%2520practical%250Aconsiderations%2520such%2520as%2520the%2520importance%2520of%2520restoring%2520optimizer%2520states%2520when%250Aresuming%2520from%2520checkpoints%252C%2520and%2520the%2520impact%2520of%2520hardware%2520changes%2520on%2520training%250Astability%2520and%2520throughput.%2520While%2520qualitative%2520evaluation%2520provides%2520an%2520intuitive%250Aunderstanding%2520of%2520model%2520improvements%252C%2520our%2520analysis%2520extends%2520to%2520various%250Aperformance%2520benchmarks%252C%2520demonstrating%2520how%2520high-quality%2520data%2520and%2520thoughtful%250Ascaling%2520enable%2520competitive%2520results%2520with%2520significantly%2520fewer%2520training%2520tokens.%2520By%250Adetailing%2520these%2520experiences%2520and%2520offering%2520training%2520logs%252C%2520checkpoints%252C%2520and%2520sample%250Aoutputs%252C%2520we%2520aim%2520to%2520guide%2520future%2520researchers%2520and%2520practitioners%2520in%2520refining%2520their%250Apretraining%2520strategies.%2520The%2520training%2520script%2520is%2520available%2520on%2520Github%2520at%250Ahttps%253A//github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code.%2520The%2520model%250Acheckpoints%2520are%2520available%2520on%2520Huggingface%2520at%250Ahttps%253A//huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experience%20of%20Training%20a%201.7B-Parameter%20LLaMa%20Model%20From%20Scratch&entry.906535625=Miles%20Q.%20Li%20and%20Benjamin%20C.%20M.%20Fung%20and%20Shih-Chia%20Huang&entry.1292438233=%20%20Pretraining%20large%20language%20models%20is%20a%20complex%20endeavor%20influenced%20by%0Amultiple%20factors%2C%20including%20model%20architecture%2C%20data%20quality%2C%20training%0Acontinuity%2C%20and%20hardware%20constraints.%20In%20this%20paper%2C%20we%20share%20insights%20gained%0Afrom%20the%20experience%20of%20training%20DMaS-LLaMa-Lite%2C%20a%20fully%20open%20source%2C%0A1.7-billion-parameter%2C%20LLaMa-based%20model%2C%20on%20approximately%2020%20billion%20tokens%20of%0Acarefully%20curated%20data.%20We%20chronicle%20the%20full%20training%20trajectory%2C%20documenting%0Ahow%20evolving%20validation%20loss%20levels%20and%20downstream%20benchmarks%20reflect%0Atransitions%20from%20incoherent%20text%20to%20fluent%2C%20contextually%20grounded%20output.%0ABeyond%20pretraining%2C%20we%20extend%20our%20analysis%20to%20include%20a%20post-training%20phase%0Afocused%20on%20instruction%20tuning%2C%20where%20the%20model%20was%20refined%20to%20produce%20more%0Acontextually%20appropriate%2C%20user-aligned%20responses.%20We%20highlight%20practical%0Aconsiderations%20such%20as%20the%20importance%20of%20restoring%20optimizer%20states%20when%0Aresuming%20from%20checkpoints%2C%20and%20the%20impact%20of%20hardware%20changes%20on%20training%0Astability%20and%20throughput.%20While%20qualitative%20evaluation%20provides%20an%20intuitive%0Aunderstanding%20of%20model%20improvements%2C%20our%20analysis%20extends%20to%20various%0Aperformance%20benchmarks%2C%20demonstrating%20how%20high-quality%20data%20and%20thoughtful%0Ascaling%20enable%20competitive%20results%20with%20significantly%20fewer%20training%20tokens.%20By%0Adetailing%20these%20experiences%20and%20offering%20training%20logs%2C%20checkpoints%2C%20and%20sample%0Aoutputs%2C%20we%20aim%20to%20guide%20future%20researchers%20and%20practitioners%20in%20refining%20their%0Apretraining%20strategies.%20The%20training%20script%20is%20available%20on%20Github%20at%0Ahttps%3A//github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code.%20The%20model%0Acheckpoints%20are%20available%20on%20Huggingface%20at%0Ahttps%3A//huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13335v2&entry.124074799=Read"},
{"title": "GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction", "author": "Zesong Yang and Ru Zhang and Jiale Shi and Zixiang Ai and Boming Zhao and Hujun Bao and Luwei Yang and Zhaopeng Cui", "abstract": "  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n", "link": "http://arxiv.org/abs/2412.14939v2", "date": "2024-12-20", "relevancy": 2.5002, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6233}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&body=Title%3A%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGURecon%253A%2520Learning%2520Detailed%25203D%2520Geometric%2520Uncertainties%2520for%2520Neural%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DZesong%2520Yang%2520and%2520Ru%2520Zhang%2520and%2520Jiale%2520Shi%2520and%2520Zixiang%2520Ai%2520and%2520Boming%2520Zhao%2520and%2520Hujun%2520Bao%2520and%2520Luwei%2520Yang%2520and%2520Zhaopeng%2520Cui%26entry.1292438233%3D%2520%2520Neural%2520surface%2520representation%2520has%2520demonstrated%2520remarkable%2520success%2520in%2520the%250Aareas%2520of%2520novel%2520view%2520synthesis%2520and%25203D%2520reconstruction.%2520However%252C%2520assessing%2520the%250Ageometric%2520quality%2520of%25203D%2520reconstructions%2520in%2520the%2520absence%2520of%2520ground%2520truth%2520mesh%250Aremains%2520a%2520significant%2520challenge%252C%2520due%2520to%2520its%2520rendering-based%2520optimization%250Aprocess%2520and%2520entangled%2520learning%2520of%2520appearance%2520and%2520geometry%2520with%2520photometric%250Alosses.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520i.e%252C%2520GURecon%252C%2520which%250Aestablishes%2520a%2520geometric%2520uncertainty%2520field%2520for%2520the%2520neural%2520surface%2520based%2520on%250Ageometric%2520consistency.%2520Different%2520from%2520existing%2520methods%2520that%2520rely%2520on%250Arendering-based%2520measurement%252C%2520GURecon%2520models%2520a%2520continuous%25203D%2520uncertainty%2520field%250Afor%2520the%2520reconstructed%2520surface%252C%2520and%2520is%2520learned%2520by%2520an%2520online%2520distillation%250Aapproach%2520without%2520introducing%2520real%2520geometric%2520information%2520for%2520supervision.%250AMoreover%252C%2520in%2520order%2520to%2520mitigate%2520the%2520interference%2520of%2520illumination%2520on%2520geometric%250Aconsistency%252C%2520a%2520decoupled%2520field%2520is%2520learned%2520and%2520exploited%2520to%2520finetune%2520the%250Auncertainty%2520field.%2520Experiments%2520on%2520various%2520datasets%2520demonstrate%2520the%2520superiority%250Aof%2520GURecon%2520in%2520modeling%25203D%2520geometric%2520uncertainty%252C%2520as%2520well%2520as%2520its%2520plug-and-play%250Aextension%2520to%2520various%2520neural%2520surface%2520representations%2520and%2520improvement%2520on%250Adownstream%2520tasks%2520such%2520as%2520incremental%2520reconstruction.%2520The%2520code%2520and%2520supplementary%250Amaterial%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//zju3dv.github.io/GURecon/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&entry.906535625=Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14939v2&entry.124074799=Read"},
{"title": "Self-supervised Spatial-Temporal Learner for Precipitation Nowcasting", "author": "Haotian Li and Arno Siebes and Siamak Mehrkanoon", "abstract": "  Nowcasting, the short-term prediction of weather, is essential for making\ntimely and weather-dependent decisions. Specifically, precipitation nowcasting\naims to predict precipitation at a local level within a 6-hour time frame. This\ntask can be framed as a spatial-temporal sequence forecasting problem, where\ndeep learning methods have been particularly effective. However, despite\nadvancements in self-supervised learning, most successful methods for\nnowcasting remain fully supervised. Self-supervised learning is advantageous\nfor pretraining models to learn representations without requiring extensive\nlabeled data. In this work, we leverage the benefits of self-supervised\nlearning and integrate it with spatial-temporal learning to develop a novel\nmodel, SpaT-SparK. SpaT-SparK comprises a CNN-based encoder-decoder structure\npretrained with a masked image modeling (MIM) task and a translation network\nthat captures temporal relationships among past and future precipitation maps\nin downstream tasks. We conducted experiments on the NL-50 dataset to evaluate\nthe performance of SpaT-SparK. The results demonstrate that SpaT-SparK\noutperforms existing baseline supervised models, such as SmaAt-UNet, providing\nmore accurate nowcasting predictions.\n", "link": "http://arxiv.org/abs/2412.15917v1", "date": "2024-12-20", "relevancy": 2.4899, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4943}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Spatial-Temporal%20Learner%20for%20Precipitation%20Nowcasting&body=Title%3A%20Self-supervised%20Spatial-Temporal%20Learner%20for%20Precipitation%20Nowcasting%0AAuthor%3A%20Haotian%20Li%20and%20Arno%20Siebes%20and%20Siamak%20Mehrkanoon%0AAbstract%3A%20%20%20Nowcasting%2C%20the%20short-term%20prediction%20of%20weather%2C%20is%20essential%20for%20making%0Atimely%20and%20weather-dependent%20decisions.%20Specifically%2C%20precipitation%20nowcasting%0Aaims%20to%20predict%20precipitation%20at%20a%20local%20level%20within%20a%206-hour%20time%20frame.%20This%0Atask%20can%20be%20framed%20as%20a%20spatial-temporal%20sequence%20forecasting%20problem%2C%20where%0Adeep%20learning%20methods%20have%20been%20particularly%20effective.%20However%2C%20despite%0Aadvancements%20in%20self-supervised%20learning%2C%20most%20successful%20methods%20for%0Anowcasting%20remain%20fully%20supervised.%20Self-supervised%20learning%20is%20advantageous%0Afor%20pretraining%20models%20to%20learn%20representations%20without%20requiring%20extensive%0Alabeled%20data.%20In%20this%20work%2C%20we%20leverage%20the%20benefits%20of%20self-supervised%0Alearning%20and%20integrate%20it%20with%20spatial-temporal%20learning%20to%20develop%20a%20novel%0Amodel%2C%20SpaT-SparK.%20SpaT-SparK%20comprises%20a%20CNN-based%20encoder-decoder%20structure%0Apretrained%20with%20a%20masked%20image%20modeling%20%28MIM%29%20task%20and%20a%20translation%20network%0Athat%20captures%20temporal%20relationships%20among%20past%20and%20future%20precipitation%20maps%0Ain%20downstream%20tasks.%20We%20conducted%20experiments%20on%20the%20NL-50%20dataset%20to%20evaluate%0Athe%20performance%20of%20SpaT-SparK.%20The%20results%20demonstrate%20that%20SpaT-SparK%0Aoutperforms%20existing%20baseline%20supervised%20models%2C%20such%20as%20SmaAt-UNet%2C%20providing%0Amore%20accurate%20nowcasting%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Spatial-Temporal%2520Learner%2520for%2520Precipitation%2520Nowcasting%26entry.906535625%3DHaotian%2520Li%2520and%2520Arno%2520Siebes%2520and%2520Siamak%2520Mehrkanoon%26entry.1292438233%3D%2520%2520Nowcasting%252C%2520the%2520short-term%2520prediction%2520of%2520weather%252C%2520is%2520essential%2520for%2520making%250Atimely%2520and%2520weather-dependent%2520decisions.%2520Specifically%252C%2520precipitation%2520nowcasting%250Aaims%2520to%2520predict%2520precipitation%2520at%2520a%2520local%2520level%2520within%2520a%25206-hour%2520time%2520frame.%2520This%250Atask%2520can%2520be%2520framed%2520as%2520a%2520spatial-temporal%2520sequence%2520forecasting%2520problem%252C%2520where%250Adeep%2520learning%2520methods%2520have%2520been%2520particularly%2520effective.%2520However%252C%2520despite%250Aadvancements%2520in%2520self-supervised%2520learning%252C%2520most%2520successful%2520methods%2520for%250Anowcasting%2520remain%2520fully%2520supervised.%2520Self-supervised%2520learning%2520is%2520advantageous%250Afor%2520pretraining%2520models%2520to%2520learn%2520representations%2520without%2520requiring%2520extensive%250Alabeled%2520data.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520benefits%2520of%2520self-supervised%250Alearning%2520and%2520integrate%2520it%2520with%2520spatial-temporal%2520learning%2520to%2520develop%2520a%2520novel%250Amodel%252C%2520SpaT-SparK.%2520SpaT-SparK%2520comprises%2520a%2520CNN-based%2520encoder-decoder%2520structure%250Apretrained%2520with%2520a%2520masked%2520image%2520modeling%2520%2528MIM%2529%2520task%2520and%2520a%2520translation%2520network%250Athat%2520captures%2520temporal%2520relationships%2520among%2520past%2520and%2520future%2520precipitation%2520maps%250Ain%2520downstream%2520tasks.%2520We%2520conducted%2520experiments%2520on%2520the%2520NL-50%2520dataset%2520to%2520evaluate%250Athe%2520performance%2520of%2520SpaT-SparK.%2520The%2520results%2520demonstrate%2520that%2520SpaT-SparK%250Aoutperforms%2520existing%2520baseline%2520supervised%2520models%252C%2520such%2520as%2520SmaAt-UNet%252C%2520providing%250Amore%2520accurate%2520nowcasting%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Spatial-Temporal%20Learner%20for%20Precipitation%20Nowcasting&entry.906535625=Haotian%20Li%20and%20Arno%20Siebes%20and%20Siamak%20Mehrkanoon&entry.1292438233=%20%20Nowcasting%2C%20the%20short-term%20prediction%20of%20weather%2C%20is%20essential%20for%20making%0Atimely%20and%20weather-dependent%20decisions.%20Specifically%2C%20precipitation%20nowcasting%0Aaims%20to%20predict%20precipitation%20at%20a%20local%20level%20within%20a%206-hour%20time%20frame.%20This%0Atask%20can%20be%20framed%20as%20a%20spatial-temporal%20sequence%20forecasting%20problem%2C%20where%0Adeep%20learning%20methods%20have%20been%20particularly%20effective.%20However%2C%20despite%0Aadvancements%20in%20self-supervised%20learning%2C%20most%20successful%20methods%20for%0Anowcasting%20remain%20fully%20supervised.%20Self-supervised%20learning%20is%20advantageous%0Afor%20pretraining%20models%20to%20learn%20representations%20without%20requiring%20extensive%0Alabeled%20data.%20In%20this%20work%2C%20we%20leverage%20the%20benefits%20of%20self-supervised%0Alearning%20and%20integrate%20it%20with%20spatial-temporal%20learning%20to%20develop%20a%20novel%0Amodel%2C%20SpaT-SparK.%20SpaT-SparK%20comprises%20a%20CNN-based%20encoder-decoder%20structure%0Apretrained%20with%20a%20masked%20image%20modeling%20%28MIM%29%20task%20and%20a%20translation%20network%0Athat%20captures%20temporal%20relationships%20among%20past%20and%20future%20precipitation%20maps%0Ain%20downstream%20tasks.%20We%20conducted%20experiments%20on%20the%20NL-50%20dataset%20to%20evaluate%0Athe%20performance%20of%20SpaT-SparK.%20The%20results%20demonstrate%20that%20SpaT-SparK%0Aoutperforms%20existing%20baseline%20supervised%20models%2C%20such%20as%20SmaAt-UNet%2C%20providing%0Amore%20accurate%20nowcasting%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15917v1&entry.124074799=Read"},
{"title": "Can Generative Video Models Help Pose Estimation?", "author": "Ruojin Cai and Jason Y. Zhang and Philipp Henzler and Zhengqi Li and Noah Snavely and Ricardo Martin-Brualla", "abstract": "  Pairwise pose estimation from images with little or no overlap is an open\nchallenge in computer vision. Existing methods, even those trained on\nlarge-scale datasets, struggle in these scenarios due to the lack of\nidentifiable correspondences or visual overlap. Inspired by the human ability\nto infer spatial relationships from diverse scenes, we propose a novel\napproach, InterPose, that leverages the rich priors encoded within pre-trained\ngenerative video models. We propose to use a video model to hallucinate\nintermediate frames between two input images, effectively creating a dense,\nvisual transition, which significantly simplifies the problem of pose\nestimation. Since current video models can still produce implausible motion or\ninconsistent geometry, we introduce a self-consistency score that evaluates the\nconsistency of pose predictions from sampled videos. We demonstrate that our\napproach generalizes among three state-of-the-art video models and show\nconsistent improvements over the state-of-the-art DUSt3R on four diverse\ndatasets encompassing indoor, outdoor, and object-centric scenes. Our findings\nsuggest a promising avenue for improving pose estimation models by leveraging\nlarge generative models trained on vast amounts of video data, which is more\nreadily available than 3D data. See our project page for results:\nhttps://inter-pose.github.io/.\n", "link": "http://arxiv.org/abs/2412.16155v1", "date": "2024-12-20", "relevancy": 2.4878, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6347}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6215}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Generative%20Video%20Models%20Help%20Pose%20Estimation%3F&body=Title%3A%20Can%20Generative%20Video%20Models%20Help%20Pose%20Estimation%3F%0AAuthor%3A%20Ruojin%20Cai%20and%20Jason%20Y.%20Zhang%20and%20Philipp%20Henzler%20and%20Zhengqi%20Li%20and%20Noah%20Snavely%20and%20Ricardo%20Martin-Brualla%0AAbstract%3A%20%20%20Pairwise%20pose%20estimation%20from%20images%20with%20little%20or%20no%20overlap%20is%20an%20open%0Achallenge%20in%20computer%20vision.%20Existing%20methods%2C%20even%20those%20trained%20on%0Alarge-scale%20datasets%2C%20struggle%20in%20these%20scenarios%20due%20to%20the%20lack%20of%0Aidentifiable%20correspondences%20or%20visual%20overlap.%20Inspired%20by%20the%20human%20ability%0Ato%20infer%20spatial%20relationships%20from%20diverse%20scenes%2C%20we%20propose%20a%20novel%0Aapproach%2C%20InterPose%2C%20that%20leverages%20the%20rich%20priors%20encoded%20within%20pre-trained%0Agenerative%20video%20models.%20We%20propose%20to%20use%20a%20video%20model%20to%20hallucinate%0Aintermediate%20frames%20between%20two%20input%20images%2C%20effectively%20creating%20a%20dense%2C%0Avisual%20transition%2C%20which%20significantly%20simplifies%20the%20problem%20of%20pose%0Aestimation.%20Since%20current%20video%20models%20can%20still%20produce%20implausible%20motion%20or%0Ainconsistent%20geometry%2C%20we%20introduce%20a%20self-consistency%20score%20that%20evaluates%20the%0Aconsistency%20of%20pose%20predictions%20from%20sampled%20videos.%20We%20demonstrate%20that%20our%0Aapproach%20generalizes%20among%20three%20state-of-the-art%20video%20models%20and%20show%0Aconsistent%20improvements%20over%20the%20state-of-the-art%20DUSt3R%20on%20four%20diverse%0Adatasets%20encompassing%20indoor%2C%20outdoor%2C%20and%20object-centric%20scenes.%20Our%20findings%0Asuggest%20a%20promising%20avenue%20for%20improving%20pose%20estimation%20models%20by%20leveraging%0Alarge%20generative%20models%20trained%20on%20vast%20amounts%20of%20video%20data%2C%20which%20is%20more%0Areadily%20available%20than%203D%20data.%20See%20our%20project%20page%20for%20results%3A%0Ahttps%3A//inter-pose.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Generative%2520Video%2520Models%2520Help%2520Pose%2520Estimation%253F%26entry.906535625%3DRuojin%2520Cai%2520and%2520Jason%2520Y.%2520Zhang%2520and%2520Philipp%2520Henzler%2520and%2520Zhengqi%2520Li%2520and%2520Noah%2520Snavely%2520and%2520Ricardo%2520Martin-Brualla%26entry.1292438233%3D%2520%2520Pairwise%2520pose%2520estimation%2520from%2520images%2520with%2520little%2520or%2520no%2520overlap%2520is%2520an%2520open%250Achallenge%2520in%2520computer%2520vision.%2520Existing%2520methods%252C%2520even%2520those%2520trained%2520on%250Alarge-scale%2520datasets%252C%2520struggle%2520in%2520these%2520scenarios%2520due%2520to%2520the%2520lack%2520of%250Aidentifiable%2520correspondences%2520or%2520visual%2520overlap.%2520Inspired%2520by%2520the%2520human%2520ability%250Ato%2520infer%2520spatial%2520relationships%2520from%2520diverse%2520scenes%252C%2520we%2520propose%2520a%2520novel%250Aapproach%252C%2520InterPose%252C%2520that%2520leverages%2520the%2520rich%2520priors%2520encoded%2520within%2520pre-trained%250Agenerative%2520video%2520models.%2520We%2520propose%2520to%2520use%2520a%2520video%2520model%2520to%2520hallucinate%250Aintermediate%2520frames%2520between%2520two%2520input%2520images%252C%2520effectively%2520creating%2520a%2520dense%252C%250Avisual%2520transition%252C%2520which%2520significantly%2520simplifies%2520the%2520problem%2520of%2520pose%250Aestimation.%2520Since%2520current%2520video%2520models%2520can%2520still%2520produce%2520implausible%2520motion%2520or%250Ainconsistent%2520geometry%252C%2520we%2520introduce%2520a%2520self-consistency%2520score%2520that%2520evaluates%2520the%250Aconsistency%2520of%2520pose%2520predictions%2520from%2520sampled%2520videos.%2520We%2520demonstrate%2520that%2520our%250Aapproach%2520generalizes%2520among%2520three%2520state-of-the-art%2520video%2520models%2520and%2520show%250Aconsistent%2520improvements%2520over%2520the%2520state-of-the-art%2520DUSt3R%2520on%2520four%2520diverse%250Adatasets%2520encompassing%2520indoor%252C%2520outdoor%252C%2520and%2520object-centric%2520scenes.%2520Our%2520findings%250Asuggest%2520a%2520promising%2520avenue%2520for%2520improving%2520pose%2520estimation%2520models%2520by%2520leveraging%250Alarge%2520generative%2520models%2520trained%2520on%2520vast%2520amounts%2520of%2520video%2520data%252C%2520which%2520is%2520more%250Areadily%2520available%2520than%25203D%2520data.%2520See%2520our%2520project%2520page%2520for%2520results%253A%250Ahttps%253A//inter-pose.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Generative%20Video%20Models%20Help%20Pose%20Estimation%3F&entry.906535625=Ruojin%20Cai%20and%20Jason%20Y.%20Zhang%20and%20Philipp%20Henzler%20and%20Zhengqi%20Li%20and%20Noah%20Snavely%20and%20Ricardo%20Martin-Brualla&entry.1292438233=%20%20Pairwise%20pose%20estimation%20from%20images%20with%20little%20or%20no%20overlap%20is%20an%20open%0Achallenge%20in%20computer%20vision.%20Existing%20methods%2C%20even%20those%20trained%20on%0Alarge-scale%20datasets%2C%20struggle%20in%20these%20scenarios%20due%20to%20the%20lack%20of%0Aidentifiable%20correspondences%20or%20visual%20overlap.%20Inspired%20by%20the%20human%20ability%0Ato%20infer%20spatial%20relationships%20from%20diverse%20scenes%2C%20we%20propose%20a%20novel%0Aapproach%2C%20InterPose%2C%20that%20leverages%20the%20rich%20priors%20encoded%20within%20pre-trained%0Agenerative%20video%20models.%20We%20propose%20to%20use%20a%20video%20model%20to%20hallucinate%0Aintermediate%20frames%20between%20two%20input%20images%2C%20effectively%20creating%20a%20dense%2C%0Avisual%20transition%2C%20which%20significantly%20simplifies%20the%20problem%20of%20pose%0Aestimation.%20Since%20current%20video%20models%20can%20still%20produce%20implausible%20motion%20or%0Ainconsistent%20geometry%2C%20we%20introduce%20a%20self-consistency%20score%20that%20evaluates%20the%0Aconsistency%20of%20pose%20predictions%20from%20sampled%20videos.%20We%20demonstrate%20that%20our%0Aapproach%20generalizes%20among%20three%20state-of-the-art%20video%20models%20and%20show%0Aconsistent%20improvements%20over%20the%20state-of-the-art%20DUSt3R%20on%20four%20diverse%0Adatasets%20encompassing%20indoor%2C%20outdoor%2C%20and%20object-centric%20scenes.%20Our%20findings%0Asuggest%20a%20promising%20avenue%20for%20improving%20pose%20estimation%20models%20by%20leveraging%0Alarge%20generative%20models%20trained%20on%20vast%20amounts%20of%20video%20data%2C%20which%20is%20more%0Areadily%20available%20than%203D%20data.%20See%20our%20project%20page%20for%20results%3A%0Ahttps%3A//inter-pose.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16155v1&entry.124074799=Read"},
{"title": "Less is More: Towards Green Code Large Language Models via Unified\n  Structural Pruning", "author": "Guang Yang and Yu Zhou and Xiangyu Zhang and Wei Cheng and Ke Liu and Xiang Chen and Terry Yue Zhuo and Taolue Chen", "abstract": "  The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.\n", "link": "http://arxiv.org/abs/2412.15921v1", "date": "2024-12-20", "relevancy": 2.4861, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20Towards%20Green%20Code%20Large%20Language%20Models%20via%20Unified%0A%20%20Structural%20Pruning&body=Title%3A%20Less%20is%20More%3A%20Towards%20Green%20Code%20Large%20Language%20Models%20via%20Unified%0A%20%20Structural%20Pruning%0AAuthor%3A%20Guang%20Yang%20and%20Yu%20Zhou%20and%20Xiangyu%20Zhang%20and%20Wei%20Cheng%20and%20Ke%20Liu%20and%20Xiang%20Chen%20and%20Terry%20Yue%20Zhuo%20and%20Taolue%20Chen%0AAbstract%3A%20%20%20The%20extensive%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generative%0Acoding%20tasks%20has%20raised%20concerns%20due%20to%20their%20high%20computational%20demands%20and%0Aenergy%20consumption.%20Unlike%20previous%20structural%20pruning%20methods%20designed%20for%0Aclassification%20models%20that%20deal%20with%20lowdimensional%20classification%20logits%2C%0Agenerative%20Code%20LLMs%20produce%20high-dimensional%20token%20logit%20sequences%2C%20making%0Atraditional%20pruning%20objectives%20inherently%20limited.%20Moreover%2C%20existing%20single%0Acomponent%20pruning%20approaches%20further%20constrain%20the%20effectiveness%20when%20applied%0Ato%20generative%20Code%20LLMs.%20In%20response%2C%20we%20propose%20Flab-Pruner%2C%20an%20innovative%0Aunified%20structural%20pruning%20method%20that%20combines%20vocabulary%2C%20layer%2C%20and%0AFeed-Forward%20Network%20%28FFN%29%20pruning.%20This%20approach%20effectively%20reduces%20model%0Aparameters%20while%20maintaining%20performance.%20Additionally%2C%20we%20introduce%20a%0Acustomized%20code%20instruction%20data%20strategy%20for%20coding%20tasks%20to%20enhance%20the%0Aperformance%20recovery%20efficiency%20of%20the%20pruned%20model.%20Through%20extensive%0Aevaluations%20on%20three%20state-of-the-art%20Code%20LLMs%20across%20multiple%20generative%0Acoding%20tasks%2C%20the%20results%20demonstrate%20that%20Flab-Pruner%20retains%2097%25%20of%20the%0Aoriginal%20performance%20after%20pruning%2022%25%20of%20the%20parameters%20and%20achieves%20the%20same%0Aor%20even%20better%20performance%20after%20post-training.%20The%20pruned%20models%20exhibit%0Asignificant%20improvements%20in%20storage%2C%20GPU%20usage%2C%20computational%20efficiency%2C%20and%0Aenvironmental%20impact%2C%20while%20maintaining%20well%20robustness.%20Our%20research%20provides%0Aa%20sustainable%20solution%20for%20green%20software%20engineering%20and%20promotes%20the%0Aefficient%20deployment%20of%20LLMs%20in%20real-world%20generative%20coding%20intelligence%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520Towards%2520Green%2520Code%2520Large%2520Language%2520Models%2520via%2520Unified%250A%2520%2520Structural%2520Pruning%26entry.906535625%3DGuang%2520Yang%2520and%2520Yu%2520Zhou%2520and%2520Xiangyu%2520Zhang%2520and%2520Wei%2520Cheng%2520and%2520Ke%2520Liu%2520and%2520Xiang%2520Chen%2520and%2520Terry%2520Yue%2520Zhuo%2520and%2520Taolue%2520Chen%26entry.1292438233%3D%2520%2520The%2520extensive%2520application%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520generative%250Acoding%2520tasks%2520has%2520raised%2520concerns%2520due%2520to%2520their%2520high%2520computational%2520demands%2520and%250Aenergy%2520consumption.%2520Unlike%2520previous%2520structural%2520pruning%2520methods%2520designed%2520for%250Aclassification%2520models%2520that%2520deal%2520with%2520lowdimensional%2520classification%2520logits%252C%250Agenerative%2520Code%2520LLMs%2520produce%2520high-dimensional%2520token%2520logit%2520sequences%252C%2520making%250Atraditional%2520pruning%2520objectives%2520inherently%2520limited.%2520Moreover%252C%2520existing%2520single%250Acomponent%2520pruning%2520approaches%2520further%2520constrain%2520the%2520effectiveness%2520when%2520applied%250Ato%2520generative%2520Code%2520LLMs.%2520In%2520response%252C%2520we%2520propose%2520Flab-Pruner%252C%2520an%2520innovative%250Aunified%2520structural%2520pruning%2520method%2520that%2520combines%2520vocabulary%252C%2520layer%252C%2520and%250AFeed-Forward%2520Network%2520%2528FFN%2529%2520pruning.%2520This%2520approach%2520effectively%2520reduces%2520model%250Aparameters%2520while%2520maintaining%2520performance.%2520Additionally%252C%2520we%2520introduce%2520a%250Acustomized%2520code%2520instruction%2520data%2520strategy%2520for%2520coding%2520tasks%2520to%2520enhance%2520the%250Aperformance%2520recovery%2520efficiency%2520of%2520the%2520pruned%2520model.%2520Through%2520extensive%250Aevaluations%2520on%2520three%2520state-of-the-art%2520Code%2520LLMs%2520across%2520multiple%2520generative%250Acoding%2520tasks%252C%2520the%2520results%2520demonstrate%2520that%2520Flab-Pruner%2520retains%252097%2525%2520of%2520the%250Aoriginal%2520performance%2520after%2520pruning%252022%2525%2520of%2520the%2520parameters%2520and%2520achieves%2520the%2520same%250Aor%2520even%2520better%2520performance%2520after%2520post-training.%2520The%2520pruned%2520models%2520exhibit%250Asignificant%2520improvements%2520in%2520storage%252C%2520GPU%2520usage%252C%2520computational%2520efficiency%252C%2520and%250Aenvironmental%2520impact%252C%2520while%2520maintaining%2520well%2520robustness.%2520Our%2520research%2520provides%250Aa%2520sustainable%2520solution%2520for%2520green%2520software%2520engineering%2520and%2520promotes%2520the%250Aefficient%2520deployment%2520of%2520LLMs%2520in%2520real-world%2520generative%2520coding%2520intelligence%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20Towards%20Green%20Code%20Large%20Language%20Models%20via%20Unified%0A%20%20Structural%20Pruning&entry.906535625=Guang%20Yang%20and%20Yu%20Zhou%20and%20Xiangyu%20Zhang%20and%20Wei%20Cheng%20and%20Ke%20Liu%20and%20Xiang%20Chen%20and%20Terry%20Yue%20Zhuo%20and%20Taolue%20Chen&entry.1292438233=%20%20The%20extensive%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generative%0Acoding%20tasks%20has%20raised%20concerns%20due%20to%20their%20high%20computational%20demands%20and%0Aenergy%20consumption.%20Unlike%20previous%20structural%20pruning%20methods%20designed%20for%0Aclassification%20models%20that%20deal%20with%20lowdimensional%20classification%20logits%2C%0Agenerative%20Code%20LLMs%20produce%20high-dimensional%20token%20logit%20sequences%2C%20making%0Atraditional%20pruning%20objectives%20inherently%20limited.%20Moreover%2C%20existing%20single%0Acomponent%20pruning%20approaches%20further%20constrain%20the%20effectiveness%20when%20applied%0Ato%20generative%20Code%20LLMs.%20In%20response%2C%20we%20propose%20Flab-Pruner%2C%20an%20innovative%0Aunified%20structural%20pruning%20method%20that%20combines%20vocabulary%2C%20layer%2C%20and%0AFeed-Forward%20Network%20%28FFN%29%20pruning.%20This%20approach%20effectively%20reduces%20model%0Aparameters%20while%20maintaining%20performance.%20Additionally%2C%20we%20introduce%20a%0Acustomized%20code%20instruction%20data%20strategy%20for%20coding%20tasks%20to%20enhance%20the%0Aperformance%20recovery%20efficiency%20of%20the%20pruned%20model.%20Through%20extensive%0Aevaluations%20on%20three%20state-of-the-art%20Code%20LLMs%20across%20multiple%20generative%0Acoding%20tasks%2C%20the%20results%20demonstrate%20that%20Flab-Pruner%20retains%2097%25%20of%20the%0Aoriginal%20performance%20after%20pruning%2022%25%20of%20the%20parameters%20and%20achieves%20the%20same%0Aor%20even%20better%20performance%20after%20post-training.%20The%20pruned%20models%20exhibit%0Asignificant%20improvements%20in%20storage%2C%20GPU%20usage%2C%20computational%20efficiency%2C%20and%0Aenvironmental%20impact%2C%20while%20maintaining%20well%20robustness.%20Our%20research%20provides%0Aa%20sustainable%20solution%20for%20green%20software%20engineering%20and%20promotes%20the%0Aefficient%20deployment%20of%20LLMs%20in%20real-world%20generative%20coding%20intelligence%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15921v1&entry.124074799=Read"},
{"title": "TelcoLM: collecting data, adapting, and benchmarking language models for\n  the telecommunication domain", "author": "Camille Barboule and Viet-Phi Huynh and Adrien Bufort and Yoan Chabot and G\u00e9raldine Damnati and Gw\u00e9nol\u00e9 Lecorv\u00e9", "abstract": "  Despite outstanding processes in many tasks, Large Language Models (LLMs)\nstill lack accuracy when dealing with highly technical domains. Especially,\ntelecommunications (telco) is a particularly challenging domain due the large\namount of lexical, semantic and conceptual peculiarities. Yet, this domain\nholds many valuable use cases, directly linked to industrial needs. Hence, this\npaper studies how LLMs can be adapted to the telco domain. It reports our\neffort to (i) collect a massive corpus of domain-specific data (800M tokens,\n80K instructions), (ii) perform adaptation using various methodologies, and\n(iii) benchmark them against larger generalist models in downstream tasks that\nrequire extensive knowledge of telecommunications. Our experiments on\nLlama-2-7b show that domain-adapted models can challenge the large generalist\nmodels. They also suggest that adaptation can be restricted to a unique\ninstruction-tuning step, dicarding the need for any fine-tuning on raw texts\nbeforehand.\n", "link": "http://arxiv.org/abs/2412.15891v1", "date": "2024-12-20", "relevancy": 2.4856, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TelcoLM%3A%20collecting%20data%2C%20adapting%2C%20and%20benchmarking%20language%20models%20for%0A%20%20the%20telecommunication%20domain&body=Title%3A%20TelcoLM%3A%20collecting%20data%2C%20adapting%2C%20and%20benchmarking%20language%20models%20for%0A%20%20the%20telecommunication%20domain%0AAuthor%3A%20Camille%20Barboule%20and%20Viet-Phi%20Huynh%20and%20Adrien%20Bufort%20and%20Yoan%20Chabot%20and%20G%C3%A9raldine%20Damnati%20and%20Gw%C3%A9nol%C3%A9%20Lecorv%C3%A9%0AAbstract%3A%20%20%20Despite%20outstanding%20processes%20in%20many%20tasks%2C%20Large%20Language%20Models%20%28LLMs%29%0Astill%20lack%20accuracy%20when%20dealing%20with%20highly%20technical%20domains.%20Especially%2C%0Atelecommunications%20%28telco%29%20is%20a%20particularly%20challenging%20domain%20due%20the%20large%0Aamount%20of%20lexical%2C%20semantic%20and%20conceptual%20peculiarities.%20Yet%2C%20this%20domain%0Aholds%20many%20valuable%20use%20cases%2C%20directly%20linked%20to%20industrial%20needs.%20Hence%2C%20this%0Apaper%20studies%20how%20LLMs%20can%20be%20adapted%20to%20the%20telco%20domain.%20It%20reports%20our%0Aeffort%20to%20%28i%29%20collect%20a%20massive%20corpus%20of%20domain-specific%20data%20%28800M%20tokens%2C%0A80K%20instructions%29%2C%20%28ii%29%20perform%20adaptation%20using%20various%20methodologies%2C%20and%0A%28iii%29%20benchmark%20them%20against%20larger%20generalist%20models%20in%20downstream%20tasks%20that%0Arequire%20extensive%20knowledge%20of%20telecommunications.%20Our%20experiments%20on%0ALlama-2-7b%20show%20that%20domain-adapted%20models%20can%20challenge%20the%20large%20generalist%0Amodels.%20They%20also%20suggest%20that%20adaptation%20can%20be%20restricted%20to%20a%20unique%0Ainstruction-tuning%20step%2C%20dicarding%20the%20need%20for%20any%20fine-tuning%20on%20raw%20texts%0Abeforehand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTelcoLM%253A%2520collecting%2520data%252C%2520adapting%252C%2520and%2520benchmarking%2520language%2520models%2520for%250A%2520%2520the%2520telecommunication%2520domain%26entry.906535625%3DCamille%2520Barboule%2520and%2520Viet-Phi%2520Huynh%2520and%2520Adrien%2520Bufort%2520and%2520Yoan%2520Chabot%2520and%2520G%25C3%25A9raldine%2520Damnati%2520and%2520Gw%25C3%25A9nol%25C3%25A9%2520Lecorv%25C3%25A9%26entry.1292438233%3D%2520%2520Despite%2520outstanding%2520processes%2520in%2520many%2520tasks%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Astill%2520lack%2520accuracy%2520when%2520dealing%2520with%2520highly%2520technical%2520domains.%2520Especially%252C%250Atelecommunications%2520%2528telco%2529%2520is%2520a%2520particularly%2520challenging%2520domain%2520due%2520the%2520large%250Aamount%2520of%2520lexical%252C%2520semantic%2520and%2520conceptual%2520peculiarities.%2520Yet%252C%2520this%2520domain%250Aholds%2520many%2520valuable%2520use%2520cases%252C%2520directly%2520linked%2520to%2520industrial%2520needs.%2520Hence%252C%2520this%250Apaper%2520studies%2520how%2520LLMs%2520can%2520be%2520adapted%2520to%2520the%2520telco%2520domain.%2520It%2520reports%2520our%250Aeffort%2520to%2520%2528i%2529%2520collect%2520a%2520massive%2520corpus%2520of%2520domain-specific%2520data%2520%2528800M%2520tokens%252C%250A80K%2520instructions%2529%252C%2520%2528ii%2529%2520perform%2520adaptation%2520using%2520various%2520methodologies%252C%2520and%250A%2528iii%2529%2520benchmark%2520them%2520against%2520larger%2520generalist%2520models%2520in%2520downstream%2520tasks%2520that%250Arequire%2520extensive%2520knowledge%2520of%2520telecommunications.%2520Our%2520experiments%2520on%250ALlama-2-7b%2520show%2520that%2520domain-adapted%2520models%2520can%2520challenge%2520the%2520large%2520generalist%250Amodels.%2520They%2520also%2520suggest%2520that%2520adaptation%2520can%2520be%2520restricted%2520to%2520a%2520unique%250Ainstruction-tuning%2520step%252C%2520dicarding%2520the%2520need%2520for%2520any%2520fine-tuning%2520on%2520raw%2520texts%250Abeforehand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TelcoLM%3A%20collecting%20data%2C%20adapting%2C%20and%20benchmarking%20language%20models%20for%0A%20%20the%20telecommunication%20domain&entry.906535625=Camille%20Barboule%20and%20Viet-Phi%20Huynh%20and%20Adrien%20Bufort%20and%20Yoan%20Chabot%20and%20G%C3%A9raldine%20Damnati%20and%20Gw%C3%A9nol%C3%A9%20Lecorv%C3%A9&entry.1292438233=%20%20Despite%20outstanding%20processes%20in%20many%20tasks%2C%20Large%20Language%20Models%20%28LLMs%29%0Astill%20lack%20accuracy%20when%20dealing%20with%20highly%20technical%20domains.%20Especially%2C%0Atelecommunications%20%28telco%29%20is%20a%20particularly%20challenging%20domain%20due%20the%20large%0Aamount%20of%20lexical%2C%20semantic%20and%20conceptual%20peculiarities.%20Yet%2C%20this%20domain%0Aholds%20many%20valuable%20use%20cases%2C%20directly%20linked%20to%20industrial%20needs.%20Hence%2C%20this%0Apaper%20studies%20how%20LLMs%20can%20be%20adapted%20to%20the%20telco%20domain.%20It%20reports%20our%0Aeffort%20to%20%28i%29%20collect%20a%20massive%20corpus%20of%20domain-specific%20data%20%28800M%20tokens%2C%0A80K%20instructions%29%2C%20%28ii%29%20perform%20adaptation%20using%20various%20methodologies%2C%20and%0A%28iii%29%20benchmark%20them%20against%20larger%20generalist%20models%20in%20downstream%20tasks%20that%0Arequire%20extensive%20knowledge%20of%20telecommunications.%20Our%20experiments%20on%0ALlama-2-7b%20show%20that%20domain-adapted%20models%20can%20challenge%20the%20large%20generalist%0Amodels.%20They%20also%20suggest%20that%20adaptation%20can%20be%20restricted%20to%20a%20unique%0Ainstruction-tuning%20step%2C%20dicarding%20the%20need%20for%20any%20fine-tuning%20on%20raw%20texts%0Abeforehand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15891v1&entry.124074799=Read"},
{"title": "MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer\n  Classification and Detection", "author": "Andrea Moglia and Elia Clement Nastasio and Luca Mainardi and Pietro Cerveri", "abstract": "  Problem: Pancreas radiological imaging is challenging due to the small size,\nblurred boundaries, and variability of shape and position of the organ among\npatients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large\nLanguage Model (MLLM), as an interactive chatbot to support clinicians in\npancreas cancer diagnosis by integrating visual and textual information.\nMethods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way\nfor pancreas detection, tumor classification, and tumor detection with\nmultimodal prompts combining questions and computed tomography scans from the\nNational Institute of Health (NIH), and Medical Segmentation Decathlon (MSD)\ndatasets. The AbdomenCT-1k dataset was used to detect the liver, spleen,\nkidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over\nUnion (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD\ndatasets, respectively. For the pancreas cancer classification task on the MSD\ndataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878,\nrespectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for\nmulti-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney,\n0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor\ndetection task, the IoU score was 0.168 on the MSD dataset. Conclusions:\nMiniGPT-Pancreas represents a promising solution to support clinicians in the\nclassification of pancreas images with pancreas tumors. Future research is\nneeded to improve the score on the detection task, especially for pancreas\ntumors.\n", "link": "http://arxiv.org/abs/2412.15925v1", "date": "2024-12-20", "relevancy": 2.4788, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniGPT-Pancreas%3A%20Multimodal%20Large%20Language%20Model%20for%20Pancreas%20Cancer%0A%20%20Classification%20and%20Detection&body=Title%3A%20MiniGPT-Pancreas%3A%20Multimodal%20Large%20Language%20Model%20for%20Pancreas%20Cancer%0A%20%20Classification%20and%20Detection%0AAuthor%3A%20Andrea%20Moglia%20and%20Elia%20Clement%20Nastasio%20and%20Luca%20Mainardi%20and%20Pietro%20Cerveri%0AAbstract%3A%20%20%20Problem%3A%20Pancreas%20radiological%20imaging%20is%20challenging%20due%20to%20the%20small%20size%2C%0Ablurred%20boundaries%2C%20and%20variability%20of%20shape%20and%20position%20of%20the%20organ%20among%0Apatients.%20Goal%3A%20In%20this%20work%20we%20present%20MiniGPT-Pancreas%2C%20a%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20as%20an%20interactive%20chatbot%20to%20support%20clinicians%20in%0Apancreas%20cancer%20diagnosis%20by%20integrating%20visual%20and%20textual%20information.%0AMethods%3A%20MiniGPT-v2%2C%20a%20general-purpose%20MLLM%2C%20was%20fine-tuned%20in%20a%20cascaded%20way%0Afor%20pancreas%20detection%2C%20tumor%20classification%2C%20and%20tumor%20detection%20with%0Amultimodal%20prompts%20combining%20questions%20and%20computed%20tomography%20scans%20from%20the%0ANational%20Institute%20of%20Health%20%28NIH%29%2C%20and%20Medical%20Segmentation%20Decathlon%20%28MSD%29%0Adatasets.%20The%20AbdomenCT-1k%20dataset%20was%20used%20to%20detect%20the%20liver%2C%20spleen%2C%0Akidney%2C%20and%20pancreas.%20Results%3A%20MiniGPT-Pancreas%20achieved%20an%20Intersection%20over%0AUnion%20%28IoU%29%20of%200.595%20and%200.550%20for%20the%20detection%20of%20pancreas%20on%20NIH%20and%20MSD%0Adatasets%2C%20respectively.%20For%20the%20pancreas%20cancer%20classification%20task%20on%20the%20MSD%0Adataset%2C%20accuracy%2C%20precision%2C%20and%20recall%20were%200.876%2C%200.874%2C%20and%200.878%2C%0Arespectively.%20When%20evaluating%20MiniGPT-Pancreas%20on%20the%20AbdomenCT-1k%20dataset%20for%0Amulti-organ%20detection%2C%20the%20IoU%20was%200.8399%20for%20the%20liver%2C%200.722%20for%20the%20kidney%2C%0A0.705%20for%20the%20spleen%2C%20and%200.497%20for%20the%20pancreas.%20For%20the%20pancreas%20tumor%0Adetection%20task%2C%20the%20IoU%20score%20was%200.168%20on%20the%20MSD%20dataset.%20Conclusions%3A%0AMiniGPT-Pancreas%20represents%20a%20promising%20solution%20to%20support%20clinicians%20in%20the%0Aclassification%20of%20pancreas%20images%20with%20pancreas%20tumors.%20Future%20research%20is%0Aneeded%20to%20improve%20the%20score%20on%20the%20detection%20task%2C%20especially%20for%20pancreas%0Atumors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniGPT-Pancreas%253A%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Pancreas%2520Cancer%250A%2520%2520Classification%2520and%2520Detection%26entry.906535625%3DAndrea%2520Moglia%2520and%2520Elia%2520Clement%2520Nastasio%2520and%2520Luca%2520Mainardi%2520and%2520Pietro%2520Cerveri%26entry.1292438233%3D%2520%2520Problem%253A%2520Pancreas%2520radiological%2520imaging%2520is%2520challenging%2520due%2520to%2520the%2520small%2520size%252C%250Ablurred%2520boundaries%252C%2520and%2520variability%2520of%2520shape%2520and%2520position%2520of%2520the%2520organ%2520among%250Apatients.%2520Goal%253A%2520In%2520this%2520work%2520we%2520present%2520MiniGPT-Pancreas%252C%2520a%2520Multimodal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%252C%2520as%2520an%2520interactive%2520chatbot%2520to%2520support%2520clinicians%2520in%250Apancreas%2520cancer%2520diagnosis%2520by%2520integrating%2520visual%2520and%2520textual%2520information.%250AMethods%253A%2520MiniGPT-v2%252C%2520a%2520general-purpose%2520MLLM%252C%2520was%2520fine-tuned%2520in%2520a%2520cascaded%2520way%250Afor%2520pancreas%2520detection%252C%2520tumor%2520classification%252C%2520and%2520tumor%2520detection%2520with%250Amultimodal%2520prompts%2520combining%2520questions%2520and%2520computed%2520tomography%2520scans%2520from%2520the%250ANational%2520Institute%2520of%2520Health%2520%2528NIH%2529%252C%2520and%2520Medical%2520Segmentation%2520Decathlon%2520%2528MSD%2529%250Adatasets.%2520The%2520AbdomenCT-1k%2520dataset%2520was%2520used%2520to%2520detect%2520the%2520liver%252C%2520spleen%252C%250Akidney%252C%2520and%2520pancreas.%2520Results%253A%2520MiniGPT-Pancreas%2520achieved%2520an%2520Intersection%2520over%250AUnion%2520%2528IoU%2529%2520of%25200.595%2520and%25200.550%2520for%2520the%2520detection%2520of%2520pancreas%2520on%2520NIH%2520and%2520MSD%250Adatasets%252C%2520respectively.%2520For%2520the%2520pancreas%2520cancer%2520classification%2520task%2520on%2520the%2520MSD%250Adataset%252C%2520accuracy%252C%2520precision%252C%2520and%2520recall%2520were%25200.876%252C%25200.874%252C%2520and%25200.878%252C%250Arespectively.%2520When%2520evaluating%2520MiniGPT-Pancreas%2520on%2520the%2520AbdomenCT-1k%2520dataset%2520for%250Amulti-organ%2520detection%252C%2520the%2520IoU%2520was%25200.8399%2520for%2520the%2520liver%252C%25200.722%2520for%2520the%2520kidney%252C%250A0.705%2520for%2520the%2520spleen%252C%2520and%25200.497%2520for%2520the%2520pancreas.%2520For%2520the%2520pancreas%2520tumor%250Adetection%2520task%252C%2520the%2520IoU%2520score%2520was%25200.168%2520on%2520the%2520MSD%2520dataset.%2520Conclusions%253A%250AMiniGPT-Pancreas%2520represents%2520a%2520promising%2520solution%2520to%2520support%2520clinicians%2520in%2520the%250Aclassification%2520of%2520pancreas%2520images%2520with%2520pancreas%2520tumors.%2520Future%2520research%2520is%250Aneeded%2520to%2520improve%2520the%2520score%2520on%2520the%2520detection%2520task%252C%2520especially%2520for%2520pancreas%250Atumors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniGPT-Pancreas%3A%20Multimodal%20Large%20Language%20Model%20for%20Pancreas%20Cancer%0A%20%20Classification%20and%20Detection&entry.906535625=Andrea%20Moglia%20and%20Elia%20Clement%20Nastasio%20and%20Luca%20Mainardi%20and%20Pietro%20Cerveri&entry.1292438233=%20%20Problem%3A%20Pancreas%20radiological%20imaging%20is%20challenging%20due%20to%20the%20small%20size%2C%0Ablurred%20boundaries%2C%20and%20variability%20of%20shape%20and%20position%20of%20the%20organ%20among%0Apatients.%20Goal%3A%20In%20this%20work%20we%20present%20MiniGPT-Pancreas%2C%20a%20Multimodal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20as%20an%20interactive%20chatbot%20to%20support%20clinicians%20in%0Apancreas%20cancer%20diagnosis%20by%20integrating%20visual%20and%20textual%20information.%0AMethods%3A%20MiniGPT-v2%2C%20a%20general-purpose%20MLLM%2C%20was%20fine-tuned%20in%20a%20cascaded%20way%0Afor%20pancreas%20detection%2C%20tumor%20classification%2C%20and%20tumor%20detection%20with%0Amultimodal%20prompts%20combining%20questions%20and%20computed%20tomography%20scans%20from%20the%0ANational%20Institute%20of%20Health%20%28NIH%29%2C%20and%20Medical%20Segmentation%20Decathlon%20%28MSD%29%0Adatasets.%20The%20AbdomenCT-1k%20dataset%20was%20used%20to%20detect%20the%20liver%2C%20spleen%2C%0Akidney%2C%20and%20pancreas.%20Results%3A%20MiniGPT-Pancreas%20achieved%20an%20Intersection%20over%0AUnion%20%28IoU%29%20of%200.595%20and%200.550%20for%20the%20detection%20of%20pancreas%20on%20NIH%20and%20MSD%0Adatasets%2C%20respectively.%20For%20the%20pancreas%20cancer%20classification%20task%20on%20the%20MSD%0Adataset%2C%20accuracy%2C%20precision%2C%20and%20recall%20were%200.876%2C%200.874%2C%20and%200.878%2C%0Arespectively.%20When%20evaluating%20MiniGPT-Pancreas%20on%20the%20AbdomenCT-1k%20dataset%20for%0Amulti-organ%20detection%2C%20the%20IoU%20was%200.8399%20for%20the%20liver%2C%200.722%20for%20the%20kidney%2C%0A0.705%20for%20the%20spleen%2C%20and%200.497%20for%20the%20pancreas.%20For%20the%20pancreas%20tumor%0Adetection%20task%2C%20the%20IoU%20score%20was%200.168%20on%20the%20MSD%20dataset.%20Conclusions%3A%0AMiniGPT-Pancreas%20represents%20a%20promising%20solution%20to%20support%20clinicians%20in%20the%0Aclassification%20of%20pancreas%20images%20with%20pancreas%20tumors.%20Future%20research%20is%0Aneeded%20to%20improve%20the%20score%20on%20the%20detection%20task%2C%20especially%20for%20pancreas%0Atumors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15925v1&entry.124074799=Read"},
{"title": "Frequency Is What You Need: Word-frequency Masking Benefits\n  Vision-Language Model Pre-training", "author": "Mingliang Liang and Martha Larson", "abstract": "  Vision Language Models (VLMs) can be trained more efficiently if training\nsets can be reduced in size. Recent work has shown the benefits of masking text\nduring VLM training using a variety of approaches: truncation, random masking,\nblock masking and syntax masking. In this paper, we show that the best masking\nstrategy changes over training epochs and that, given sufficient training\nepochs, word frequency information is what you need to achieve the best\nperformance. Experiments on a large range of data sets demonstrate the\nadvantages of our approach, called Contrastive Language-Image Pre-training with\nword Frequency Masking (CLIPF). The benefits are particularly evident as the\nnumber of input tokens decreases. We analyze the impact of CLIPF vs. other\nmasking approaches on word frequency balance and discuss the apparently\ncritical contribution of CLIPF in maintaining word frequency balance across POS\ncategories.\n", "link": "http://arxiv.org/abs/2412.16148v1", "date": "2024-12-20", "relevancy": 2.4764, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency%20Is%20What%20You%20Need%3A%20Word-frequency%20Masking%20Benefits%0A%20%20Vision-Language%20Model%20Pre-training&body=Title%3A%20Frequency%20Is%20What%20You%20Need%3A%20Word-frequency%20Masking%20Benefits%0A%20%20Vision-Language%20Model%20Pre-training%0AAuthor%3A%20Mingliang%20Liang%20and%20Martha%20Larson%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20can%20be%20trained%20more%20efficiently%20if%20training%0Asets%20can%20be%20reduced%20in%20size.%20Recent%20work%20has%20shown%20the%20benefits%20of%20masking%20text%0Aduring%20VLM%20training%20using%20a%20variety%20of%20approaches%3A%20truncation%2C%20random%20masking%2C%0Ablock%20masking%20and%20syntax%20masking.%20In%20this%20paper%2C%20we%20show%20that%20the%20best%20masking%0Astrategy%20changes%20over%20training%20epochs%20and%20that%2C%20given%20sufficient%20training%0Aepochs%2C%20word%20frequency%20information%20is%20what%20you%20need%20to%20achieve%20the%20best%0Aperformance.%20Experiments%20on%20a%20large%20range%20of%20data%20sets%20demonstrate%20the%0Aadvantages%20of%20our%20approach%2C%20called%20Contrastive%20Language-Image%20Pre-training%20with%0Aword%20Frequency%20Masking%20%28CLIPF%29.%20The%20benefits%20are%20particularly%20evident%20as%20the%0Anumber%20of%20input%20tokens%20decreases.%20We%20analyze%20the%20impact%20of%20CLIPF%20vs.%20other%0Amasking%20approaches%20on%20word%20frequency%20balance%20and%20discuss%20the%20apparently%0Acritical%20contribution%20of%20CLIPF%20in%20maintaining%20word%20frequency%20balance%20across%20POS%0Acategories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency%2520Is%2520What%2520You%2520Need%253A%2520Word-frequency%2520Masking%2520Benefits%250A%2520%2520Vision-Language%2520Model%2520Pre-training%26entry.906535625%3DMingliang%2520Liang%2520and%2520Martha%2520Larson%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520can%2520be%2520trained%2520more%2520efficiently%2520if%2520training%250Asets%2520can%2520be%2520reduced%2520in%2520size.%2520Recent%2520work%2520has%2520shown%2520the%2520benefits%2520of%2520masking%2520text%250Aduring%2520VLM%2520training%2520using%2520a%2520variety%2520of%2520approaches%253A%2520truncation%252C%2520random%2520masking%252C%250Ablock%2520masking%2520and%2520syntax%2520masking.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520the%2520best%2520masking%250Astrategy%2520changes%2520over%2520training%2520epochs%2520and%2520that%252C%2520given%2520sufficient%2520training%250Aepochs%252C%2520word%2520frequency%2520information%2520is%2520what%2520you%2520need%2520to%2520achieve%2520the%2520best%250Aperformance.%2520Experiments%2520on%2520a%2520large%2520range%2520of%2520data%2520sets%2520demonstrate%2520the%250Aadvantages%2520of%2520our%2520approach%252C%2520called%2520Contrastive%2520Language-Image%2520Pre-training%2520with%250Aword%2520Frequency%2520Masking%2520%2528CLIPF%2529.%2520The%2520benefits%2520are%2520particularly%2520evident%2520as%2520the%250Anumber%2520of%2520input%2520tokens%2520decreases.%2520We%2520analyze%2520the%2520impact%2520of%2520CLIPF%2520vs.%2520other%250Amasking%2520approaches%2520on%2520word%2520frequency%2520balance%2520and%2520discuss%2520the%2520apparently%250Acritical%2520contribution%2520of%2520CLIPF%2520in%2520maintaining%2520word%2520frequency%2520balance%2520across%2520POS%250Acategories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency%20Is%20What%20You%20Need%3A%20Word-frequency%20Masking%20Benefits%0A%20%20Vision-Language%20Model%20Pre-training&entry.906535625=Mingliang%20Liang%20and%20Martha%20Larson&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20can%20be%20trained%20more%20efficiently%20if%20training%0Asets%20can%20be%20reduced%20in%20size.%20Recent%20work%20has%20shown%20the%20benefits%20of%20masking%20text%0Aduring%20VLM%20training%20using%20a%20variety%20of%20approaches%3A%20truncation%2C%20random%20masking%2C%0Ablock%20masking%20and%20syntax%20masking.%20In%20this%20paper%2C%20we%20show%20that%20the%20best%20masking%0Astrategy%20changes%20over%20training%20epochs%20and%20that%2C%20given%20sufficient%20training%0Aepochs%2C%20word%20frequency%20information%20is%20what%20you%20need%20to%20achieve%20the%20best%0Aperformance.%20Experiments%20on%20a%20large%20range%20of%20data%20sets%20demonstrate%20the%0Aadvantages%20of%20our%20approach%2C%20called%20Contrastive%20Language-Image%20Pre-training%20with%0Aword%20Frequency%20Masking%20%28CLIPF%29.%20The%20benefits%20are%20particularly%20evident%20as%20the%0Anumber%20of%20input%20tokens%20decreases.%20We%20analyze%20the%20impact%20of%20CLIPF%20vs.%20other%0Amasking%20approaches%20on%20word%20frequency%20balance%20and%20discuss%20the%20apparently%0Acritical%20contribution%20of%20CLIPF%20in%20maintaining%20word%20frequency%20balance%20across%20POS%0Acategories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16148v1&entry.124074799=Read"},
{"title": "Camera-Based Localization and Enhanced Normalized Mutual Information", "author": "Vishnu Teja Kunde and Jean-Francois Chamberland and Siddharth Agarwal", "abstract": "  Robust and fine localization algorithms are crucial for autonomous driving.\nFor the production of such vehicles as a commodity, affordable sensing\nsolutions and reliable localization algorithms must be designed. This work\nconsiders scenarios where the sensor data comes from images captured by an\ninexpensive camera mounted on the vehicle and where the vehicle contains a fine\nglobal map. Such localization algorithms typically involve finding the section\nin the global map that best matches the captured image. In harsh environments,\nboth the global map and the captured image can be noisy. Because of physical\nconstraints on camera placement, the image captured by the camera can be viewed\nas a noisy perspective transformed version of the road in the global map. Thus,\nan optimal algorithm should take into account the unequal noise power in\nvarious regions of the captured image, and the intrinsic uncertainty in the\nglobal map due to environmental variations. This article briefly reviews two\nmatching methods: (i) standard inner product (SIP) and (ii) normalized mutual\ninformation (NMI). It then proposes novel and principled modifications to\nimprove the performance of these algorithms significantly in noisy\nenvironments. These enhancements are inspired by the physical constraints\nassociated with autonomous vehicles. They are grounded in statistical signal\nprocessing and, in some context, are provably better. Numerical simulations\ndemonstrate the effectiveness of such modifications.\n", "link": "http://arxiv.org/abs/2412.16137v1", "date": "2024-12-20", "relevancy": 2.4734, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6638}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera-Based%20Localization%20and%20Enhanced%20Normalized%20Mutual%20Information&body=Title%3A%20Camera-Based%20Localization%20and%20Enhanced%20Normalized%20Mutual%20Information%0AAuthor%3A%20Vishnu%20Teja%20Kunde%20and%20Jean-Francois%20Chamberland%20and%20Siddharth%20Agarwal%0AAbstract%3A%20%20%20Robust%20and%20fine%20localization%20algorithms%20are%20crucial%20for%20autonomous%20driving.%0AFor%20the%20production%20of%20such%20vehicles%20as%20a%20commodity%2C%20affordable%20sensing%0Asolutions%20and%20reliable%20localization%20algorithms%20must%20be%20designed.%20This%20work%0Aconsiders%20scenarios%20where%20the%20sensor%20data%20comes%20from%20images%20captured%20by%20an%0Ainexpensive%20camera%20mounted%20on%20the%20vehicle%20and%20where%20the%20vehicle%20contains%20a%20fine%0Aglobal%20map.%20Such%20localization%20algorithms%20typically%20involve%20finding%20the%20section%0Ain%20the%20global%20map%20that%20best%20matches%20the%20captured%20image.%20In%20harsh%20environments%2C%0Aboth%20the%20global%20map%20and%20the%20captured%20image%20can%20be%20noisy.%20Because%20of%20physical%0Aconstraints%20on%20camera%20placement%2C%20the%20image%20captured%20by%20the%20camera%20can%20be%20viewed%0Aas%20a%20noisy%20perspective%20transformed%20version%20of%20the%20road%20in%20the%20global%20map.%20Thus%2C%0Aan%20optimal%20algorithm%20should%20take%20into%20account%20the%20unequal%20noise%20power%20in%0Avarious%20regions%20of%20the%20captured%20image%2C%20and%20the%20intrinsic%20uncertainty%20in%20the%0Aglobal%20map%20due%20to%20environmental%20variations.%20This%20article%20briefly%20reviews%20two%0Amatching%20methods%3A%20%28i%29%20standard%20inner%20product%20%28SIP%29%20and%20%28ii%29%20normalized%20mutual%0Ainformation%20%28NMI%29.%20It%20then%20proposes%20novel%20and%20principled%20modifications%20to%0Aimprove%20the%20performance%20of%20these%20algorithms%20significantly%20in%20noisy%0Aenvironments.%20These%20enhancements%20are%20inspired%20by%20the%20physical%20constraints%0Aassociated%20with%20autonomous%20vehicles.%20They%20are%20grounded%20in%20statistical%20signal%0Aprocessing%20and%2C%20in%20some%20context%2C%20are%20provably%20better.%20Numerical%20simulations%0Ademonstrate%20the%20effectiveness%20of%20such%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera-Based%2520Localization%2520and%2520Enhanced%2520Normalized%2520Mutual%2520Information%26entry.906535625%3DVishnu%2520Teja%2520Kunde%2520and%2520Jean-Francois%2520Chamberland%2520and%2520Siddharth%2520Agarwal%26entry.1292438233%3D%2520%2520Robust%2520and%2520fine%2520localization%2520algorithms%2520are%2520crucial%2520for%2520autonomous%2520driving.%250AFor%2520the%2520production%2520of%2520such%2520vehicles%2520as%2520a%2520commodity%252C%2520affordable%2520sensing%250Asolutions%2520and%2520reliable%2520localization%2520algorithms%2520must%2520be%2520designed.%2520This%2520work%250Aconsiders%2520scenarios%2520where%2520the%2520sensor%2520data%2520comes%2520from%2520images%2520captured%2520by%2520an%250Ainexpensive%2520camera%2520mounted%2520on%2520the%2520vehicle%2520and%2520where%2520the%2520vehicle%2520contains%2520a%2520fine%250Aglobal%2520map.%2520Such%2520localization%2520algorithms%2520typically%2520involve%2520finding%2520the%2520section%250Ain%2520the%2520global%2520map%2520that%2520best%2520matches%2520the%2520captured%2520image.%2520In%2520harsh%2520environments%252C%250Aboth%2520the%2520global%2520map%2520and%2520the%2520captured%2520image%2520can%2520be%2520noisy.%2520Because%2520of%2520physical%250Aconstraints%2520on%2520camera%2520placement%252C%2520the%2520image%2520captured%2520by%2520the%2520camera%2520can%2520be%2520viewed%250Aas%2520a%2520noisy%2520perspective%2520transformed%2520version%2520of%2520the%2520road%2520in%2520the%2520global%2520map.%2520Thus%252C%250Aan%2520optimal%2520algorithm%2520should%2520take%2520into%2520account%2520the%2520unequal%2520noise%2520power%2520in%250Avarious%2520regions%2520of%2520the%2520captured%2520image%252C%2520and%2520the%2520intrinsic%2520uncertainty%2520in%2520the%250Aglobal%2520map%2520due%2520to%2520environmental%2520variations.%2520This%2520article%2520briefly%2520reviews%2520two%250Amatching%2520methods%253A%2520%2528i%2529%2520standard%2520inner%2520product%2520%2528SIP%2529%2520and%2520%2528ii%2529%2520normalized%2520mutual%250Ainformation%2520%2528NMI%2529.%2520It%2520then%2520proposes%2520novel%2520and%2520principled%2520modifications%2520to%250Aimprove%2520the%2520performance%2520of%2520these%2520algorithms%2520significantly%2520in%2520noisy%250Aenvironments.%2520These%2520enhancements%2520are%2520inspired%2520by%2520the%2520physical%2520constraints%250Aassociated%2520with%2520autonomous%2520vehicles.%2520They%2520are%2520grounded%2520in%2520statistical%2520signal%250Aprocessing%2520and%252C%2520in%2520some%2520context%252C%2520are%2520provably%2520better.%2520Numerical%2520simulations%250Ademonstrate%2520the%2520effectiveness%2520of%2520such%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera-Based%20Localization%20and%20Enhanced%20Normalized%20Mutual%20Information&entry.906535625=Vishnu%20Teja%20Kunde%20and%20Jean-Francois%20Chamberland%20and%20Siddharth%20Agarwal&entry.1292438233=%20%20Robust%20and%20fine%20localization%20algorithms%20are%20crucial%20for%20autonomous%20driving.%0AFor%20the%20production%20of%20such%20vehicles%20as%20a%20commodity%2C%20affordable%20sensing%0Asolutions%20and%20reliable%20localization%20algorithms%20must%20be%20designed.%20This%20work%0Aconsiders%20scenarios%20where%20the%20sensor%20data%20comes%20from%20images%20captured%20by%20an%0Ainexpensive%20camera%20mounted%20on%20the%20vehicle%20and%20where%20the%20vehicle%20contains%20a%20fine%0Aglobal%20map.%20Such%20localization%20algorithms%20typically%20involve%20finding%20the%20section%0Ain%20the%20global%20map%20that%20best%20matches%20the%20captured%20image.%20In%20harsh%20environments%2C%0Aboth%20the%20global%20map%20and%20the%20captured%20image%20can%20be%20noisy.%20Because%20of%20physical%0Aconstraints%20on%20camera%20placement%2C%20the%20image%20captured%20by%20the%20camera%20can%20be%20viewed%0Aas%20a%20noisy%20perspective%20transformed%20version%20of%20the%20road%20in%20the%20global%20map.%20Thus%2C%0Aan%20optimal%20algorithm%20should%20take%20into%20account%20the%20unequal%20noise%20power%20in%0Avarious%20regions%20of%20the%20captured%20image%2C%20and%20the%20intrinsic%20uncertainty%20in%20the%0Aglobal%20map%20due%20to%20environmental%20variations.%20This%20article%20briefly%20reviews%20two%0Amatching%20methods%3A%20%28i%29%20standard%20inner%20product%20%28SIP%29%20and%20%28ii%29%20normalized%20mutual%0Ainformation%20%28NMI%29.%20It%20then%20proposes%20novel%20and%20principled%20modifications%20to%0Aimprove%20the%20performance%20of%20these%20algorithms%20significantly%20in%20noisy%0Aenvironments.%20These%20enhancements%20are%20inspired%20by%20the%20physical%20constraints%0Aassociated%20with%20autonomous%20vehicles.%20They%20are%20grounded%20in%20statistical%20signal%0Aprocessing%20and%2C%20in%20some%20context%2C%20are%20provably%20better.%20Numerical%20simulations%0Ademonstrate%20the%20effectiveness%20of%20such%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16137v1&entry.124074799=Read"},
{"title": "A Modern Take on Visual Relationship Reasoning for Grasp Planning", "author": "Paolo Rabino and Tatiana Tommasi", "abstract": "  Interacting with real-world cluttered scenes pose several challenges to\nrobotic agents that need to understand complex spatial dependencies among the\nobserved objects to determine optimal pick sequences or efficient object\nretrieval strategies. Existing solutions typically manage simplified scenarios\nand focus on predicting pairwise object relationships following an initial\nobject detection phase, but often overlook the global context or struggle with\nhandling redundant and missing object relations. In this work, we present a\nmodern take on visual relational reasoning for grasp planning. We introduce\nD3GD, a novel testbed that includes bin picking scenes with up to 35 objects\nfrom 97 distinct categories. Additionally, we propose D3G, a new end-to-end\ntransformer-based dependency graph generation model that simultaneously detects\nobjects and produces an adjacency matrix representing their spatial\nrelationships. Recognizing the limitations of standard metrics, we employ the\nAverage Precision of Relationships for the first time to evaluate model\nperformance, conducting an extensive experimental benchmark. The obtained\nresults establish our approach as the new state-of-the-art for this task,\nlaying the foundation for future research in robotic manipulation. We publicly\nrelease the code and dataset at https://paolotron.github.io/d3g.github.io.\n", "link": "http://arxiv.org/abs/2409.02035v2", "date": "2024-12-20", "relevancy": 2.4502, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6464}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Modern%20Take%20on%20Visual%20Relationship%20Reasoning%20for%20Grasp%20Planning&body=Title%3A%20A%20Modern%20Take%20on%20Visual%20Relationship%20Reasoning%20for%20Grasp%20Planning%0AAuthor%3A%20Paolo%20Rabino%20and%20Tatiana%20Tommasi%0AAbstract%3A%20%20%20Interacting%20with%20real-world%20cluttered%20scenes%20pose%20several%20challenges%20to%0Arobotic%20agents%20that%20need%20to%20understand%20complex%20spatial%20dependencies%20among%20the%0Aobserved%20objects%20to%20determine%20optimal%20pick%20sequences%20or%20efficient%20object%0Aretrieval%20strategies.%20Existing%20solutions%20typically%20manage%20simplified%20scenarios%0Aand%20focus%20on%20predicting%20pairwise%20object%20relationships%20following%20an%20initial%0Aobject%20detection%20phase%2C%20but%20often%20overlook%20the%20global%20context%20or%20struggle%20with%0Ahandling%20redundant%20and%20missing%20object%20relations.%20In%20this%20work%2C%20we%20present%20a%0Amodern%20take%20on%20visual%20relational%20reasoning%20for%20grasp%20planning.%20We%20introduce%0AD3GD%2C%20a%20novel%20testbed%20that%20includes%20bin%20picking%20scenes%20with%20up%20to%2035%20objects%0Afrom%2097%20distinct%20categories.%20Additionally%2C%20we%20propose%20D3G%2C%20a%20new%20end-to-end%0Atransformer-based%20dependency%20graph%20generation%20model%20that%20simultaneously%20detects%0Aobjects%20and%20produces%20an%20adjacency%20matrix%20representing%20their%20spatial%0Arelationships.%20Recognizing%20the%20limitations%20of%20standard%20metrics%2C%20we%20employ%20the%0AAverage%20Precision%20of%20Relationships%20for%20the%20first%20time%20to%20evaluate%20model%0Aperformance%2C%20conducting%20an%20extensive%20experimental%20benchmark.%20The%20obtained%0Aresults%20establish%20our%20approach%20as%20the%20new%20state-of-the-art%20for%20this%20task%2C%0Alaying%20the%20foundation%20for%20future%20research%20in%20robotic%20manipulation.%20We%20publicly%0Arelease%20the%20code%20and%20dataset%20at%20https%3A//paolotron.github.io/d3g.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Modern%2520Take%2520on%2520Visual%2520Relationship%2520Reasoning%2520for%2520Grasp%2520Planning%26entry.906535625%3DPaolo%2520Rabino%2520and%2520Tatiana%2520Tommasi%26entry.1292438233%3D%2520%2520Interacting%2520with%2520real-world%2520cluttered%2520scenes%2520pose%2520several%2520challenges%2520to%250Arobotic%2520agents%2520that%2520need%2520to%2520understand%2520complex%2520spatial%2520dependencies%2520among%2520the%250Aobserved%2520objects%2520to%2520determine%2520optimal%2520pick%2520sequences%2520or%2520efficient%2520object%250Aretrieval%2520strategies.%2520Existing%2520solutions%2520typically%2520manage%2520simplified%2520scenarios%250Aand%2520focus%2520on%2520predicting%2520pairwise%2520object%2520relationships%2520following%2520an%2520initial%250Aobject%2520detection%2520phase%252C%2520but%2520often%2520overlook%2520the%2520global%2520context%2520or%2520struggle%2520with%250Ahandling%2520redundant%2520and%2520missing%2520object%2520relations.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Amodern%2520take%2520on%2520visual%2520relational%2520reasoning%2520for%2520grasp%2520planning.%2520We%2520introduce%250AD3GD%252C%2520a%2520novel%2520testbed%2520that%2520includes%2520bin%2520picking%2520scenes%2520with%2520up%2520to%252035%2520objects%250Afrom%252097%2520distinct%2520categories.%2520Additionally%252C%2520we%2520propose%2520D3G%252C%2520a%2520new%2520end-to-end%250Atransformer-based%2520dependency%2520graph%2520generation%2520model%2520that%2520simultaneously%2520detects%250Aobjects%2520and%2520produces%2520an%2520adjacency%2520matrix%2520representing%2520their%2520spatial%250Arelationships.%2520Recognizing%2520the%2520limitations%2520of%2520standard%2520metrics%252C%2520we%2520employ%2520the%250AAverage%2520Precision%2520of%2520Relationships%2520for%2520the%2520first%2520time%2520to%2520evaluate%2520model%250Aperformance%252C%2520conducting%2520an%2520extensive%2520experimental%2520benchmark.%2520The%2520obtained%250Aresults%2520establish%2520our%2520approach%2520as%2520the%2520new%2520state-of-the-art%2520for%2520this%2520task%252C%250Alaying%2520the%2520foundation%2520for%2520future%2520research%2520in%2520robotic%2520manipulation.%2520We%2520publicly%250Arelease%2520the%2520code%2520and%2520dataset%2520at%2520https%253A//paolotron.github.io/d3g.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Modern%20Take%20on%20Visual%20Relationship%20Reasoning%20for%20Grasp%20Planning&entry.906535625=Paolo%20Rabino%20and%20Tatiana%20Tommasi&entry.1292438233=%20%20Interacting%20with%20real-world%20cluttered%20scenes%20pose%20several%20challenges%20to%0Arobotic%20agents%20that%20need%20to%20understand%20complex%20spatial%20dependencies%20among%20the%0Aobserved%20objects%20to%20determine%20optimal%20pick%20sequences%20or%20efficient%20object%0Aretrieval%20strategies.%20Existing%20solutions%20typically%20manage%20simplified%20scenarios%0Aand%20focus%20on%20predicting%20pairwise%20object%20relationships%20following%20an%20initial%0Aobject%20detection%20phase%2C%20but%20often%20overlook%20the%20global%20context%20or%20struggle%20with%0Ahandling%20redundant%20and%20missing%20object%20relations.%20In%20this%20work%2C%20we%20present%20a%0Amodern%20take%20on%20visual%20relational%20reasoning%20for%20grasp%20planning.%20We%20introduce%0AD3GD%2C%20a%20novel%20testbed%20that%20includes%20bin%20picking%20scenes%20with%20up%20to%2035%20objects%0Afrom%2097%20distinct%20categories.%20Additionally%2C%20we%20propose%20D3G%2C%20a%20new%20end-to-end%0Atransformer-based%20dependency%20graph%20generation%20model%20that%20simultaneously%20detects%0Aobjects%20and%20produces%20an%20adjacency%20matrix%20representing%20their%20spatial%0Arelationships.%20Recognizing%20the%20limitations%20of%20standard%20metrics%2C%20we%20employ%20the%0AAverage%20Precision%20of%20Relationships%20for%20the%20first%20time%20to%20evaluate%20model%0Aperformance%2C%20conducting%20an%20extensive%20experimental%20benchmark.%20The%20obtained%0Aresults%20establish%20our%20approach%20as%20the%20new%20state-of-the-art%20for%20this%20task%2C%0Alaying%20the%20foundation%20for%20future%20research%20in%20robotic%20manipulation.%20We%20publicly%0Arelease%20the%20code%20and%20dataset%20at%20https%3A//paolotron.github.io/d3g.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02035v2&entry.124074799=Read"},
{"title": "Spectral Self-supervised Feature Selection", "author": "Daniel Segal and Ofir Lindenbaum and Ariel Jaffe", "abstract": "  Choosing a meaningful subset of features from high-dimensional observations\nin unsupervised settings can greatly enhance the accuracy of downstream\nanalysis, such as clustering or dimensionality reduction, and provide valuable\ninsights into the sources of heterogeneity in a given dataset. In this paper,\nwe propose a self-supervised graph-based approach for unsupervised feature\nselection. Our method's core involves computing robust pseudo-labels by\napplying simple processing steps to the graph Laplacian's eigenvectors. The\nsubset of eigenvectors used for computing pseudo-labels is chosen based on a\nmodel stability criterion. We then measure the importance of each feature by\ntraining a surrogate model to predict the pseudo-labels from the observations.\nOur approach is shown to be robust to challenging scenarios, such as the\npresence of outliers and complex substructures. We demonstrate the\neffectiveness of our method through experiments on real-world datasets, showing\nits robustness across multiple domains, particularly its effectiveness on\nbiological datasets.\n", "link": "http://arxiv.org/abs/2407.09061v2", "date": "2024-12-20", "relevancy": 2.4467, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5165}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4908}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Self-supervised%20Feature%20Selection&body=Title%3A%20Spectral%20Self-supervised%20Feature%20Selection%0AAuthor%3A%20Daniel%20Segal%20and%20Ofir%20Lindenbaum%20and%20Ariel%20Jaffe%0AAbstract%3A%20%20%20Choosing%20a%20meaningful%20subset%20of%20features%20from%20high-dimensional%20observations%0Ain%20unsupervised%20settings%20can%20greatly%20enhance%20the%20accuracy%20of%20downstream%0Aanalysis%2C%20such%20as%20clustering%20or%20dimensionality%20reduction%2C%20and%20provide%20valuable%0Ainsights%20into%20the%20sources%20of%20heterogeneity%20in%20a%20given%20dataset.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20graph-based%20approach%20for%20unsupervised%20feature%0Aselection.%20Our%20method%27s%20core%20involves%20computing%20robust%20pseudo-labels%20by%0Aapplying%20simple%20processing%20steps%20to%20the%20graph%20Laplacian%27s%20eigenvectors.%20The%0Asubset%20of%20eigenvectors%20used%20for%20computing%20pseudo-labels%20is%20chosen%20based%20on%20a%0Amodel%20stability%20criterion.%20We%20then%20measure%20the%20importance%20of%20each%20feature%20by%0Atraining%20a%20surrogate%20model%20to%20predict%20the%20pseudo-labels%20from%20the%20observations.%0AOur%20approach%20is%20shown%20to%20be%20robust%20to%20challenging%20scenarios%2C%20such%20as%20the%0Apresence%20of%20outliers%20and%20complex%20substructures.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20experiments%20on%20real-world%20datasets%2C%20showing%0Aits%20robustness%20across%20multiple%20domains%2C%20particularly%20its%20effectiveness%20on%0Abiological%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Self-supervised%2520Feature%2520Selection%26entry.906535625%3DDaniel%2520Segal%2520and%2520Ofir%2520Lindenbaum%2520and%2520Ariel%2520Jaffe%26entry.1292438233%3D%2520%2520Choosing%2520a%2520meaningful%2520subset%2520of%2520features%2520from%2520high-dimensional%2520observations%250Ain%2520unsupervised%2520settings%2520can%2520greatly%2520enhance%2520the%2520accuracy%2520of%2520downstream%250Aanalysis%252C%2520such%2520as%2520clustering%2520or%2520dimensionality%2520reduction%252C%2520and%2520provide%2520valuable%250Ainsights%2520into%2520the%2520sources%2520of%2520heterogeneity%2520in%2520a%2520given%2520dataset.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520self-supervised%2520graph-based%2520approach%2520for%2520unsupervised%2520feature%250Aselection.%2520Our%2520method%2527s%2520core%2520involves%2520computing%2520robust%2520pseudo-labels%2520by%250Aapplying%2520simple%2520processing%2520steps%2520to%2520the%2520graph%2520Laplacian%2527s%2520eigenvectors.%2520The%250Asubset%2520of%2520eigenvectors%2520used%2520for%2520computing%2520pseudo-labels%2520is%2520chosen%2520based%2520on%2520a%250Amodel%2520stability%2520criterion.%2520We%2520then%2520measure%2520the%2520importance%2520of%2520each%2520feature%2520by%250Atraining%2520a%2520surrogate%2520model%2520to%2520predict%2520the%2520pseudo-labels%2520from%2520the%2520observations.%250AOur%2520approach%2520is%2520shown%2520to%2520be%2520robust%2520to%2520challenging%2520scenarios%252C%2520such%2520as%2520the%250Apresence%2520of%2520outliers%2520and%2520complex%2520substructures.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520through%2520experiments%2520on%2520real-world%2520datasets%252C%2520showing%250Aits%2520robustness%2520across%2520multiple%2520domains%252C%2520particularly%2520its%2520effectiveness%2520on%250Abiological%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Self-supervised%20Feature%20Selection&entry.906535625=Daniel%20Segal%20and%20Ofir%20Lindenbaum%20and%20Ariel%20Jaffe&entry.1292438233=%20%20Choosing%20a%20meaningful%20subset%20of%20features%20from%20high-dimensional%20observations%0Ain%20unsupervised%20settings%20can%20greatly%20enhance%20the%20accuracy%20of%20downstream%0Aanalysis%2C%20such%20as%20clustering%20or%20dimensionality%20reduction%2C%20and%20provide%20valuable%0Ainsights%20into%20the%20sources%20of%20heterogeneity%20in%20a%20given%20dataset.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20graph-based%20approach%20for%20unsupervised%20feature%0Aselection.%20Our%20method%27s%20core%20involves%20computing%20robust%20pseudo-labels%20by%0Aapplying%20simple%20processing%20steps%20to%20the%20graph%20Laplacian%27s%20eigenvectors.%20The%0Asubset%20of%20eigenvectors%20used%20for%20computing%20pseudo-labels%20is%20chosen%20based%20on%20a%0Amodel%20stability%20criterion.%20We%20then%20measure%20the%20importance%20of%20each%20feature%20by%0Atraining%20a%20surrogate%20model%20to%20predict%20the%20pseudo-labels%20from%20the%20observations.%0AOur%20approach%20is%20shown%20to%20be%20robust%20to%20challenging%20scenarios%2C%20such%20as%20the%0Apresence%20of%20outliers%20and%20complex%20substructures.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20experiments%20on%20real-world%20datasets%2C%20showing%0Aits%20robustness%20across%20multiple%20domains%2C%20particularly%20its%20effectiveness%20on%0Abiological%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09061v2&entry.124074799=Read"},
{"title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss", "author": "Shijie Wang and Samaneh Azadi and Rohit Girdhar and Saketh Rambhatla and Chen Sun and Xi Yin", "abstract": "  Text-Image-to-Video (TI2V) generation aims to generate a video from an image\nfollowing a text description, which is also referred to as text-guided image\nanimation. Most existing methods struggle to generate videos that align well\nwith the text prompts, particularly when motion is specified. To overcome this\nlimitation, we introduce MotiF, a simple yet effective approach that directs\nthe model's learning to the regions with more motion, thereby improving the\ntext alignment and motion generation. We use optical flow to generate a motion\nheatmap and weight the loss according to the intensity of the motion. This\nmodified objective leads to noticeable improvements and complements existing\nmethods that utilize motion priors as model inputs. Additionally, due to the\nlack of a diverse benchmark for evaluating TI2V generation, we propose TI2V\nBench, a dataset consists of 320 image-text pairs for robust evaluation. We\npresent a human evaluation protocol that asks the annotators to select an\noverall preference between two videos followed by their justifications. Through\na comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced\nmodels, achieving an average preference of 72%. The TI2V Bench is released in\nhttps://wang-sj16.github.io/motif/.\n", "link": "http://arxiv.org/abs/2412.16153v1", "date": "2024-12-20", "relevancy": 2.425, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6508}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6058}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotiF%3A%20Making%20Text%20Count%20in%20Image%20Animation%20with%20Motion%20Focal%20Loss&body=Title%3A%20MotiF%3A%20Making%20Text%20Count%20in%20Image%20Animation%20with%20Motion%20Focal%20Loss%0AAuthor%3A%20Shijie%20Wang%20and%20Samaneh%20Azadi%20and%20Rohit%20Girdhar%20and%20Saketh%20Rambhatla%20and%20Chen%20Sun%20and%20Xi%20Yin%0AAbstract%3A%20%20%20Text-Image-to-Video%20%28TI2V%29%20generation%20aims%20to%20generate%20a%20video%20from%20an%20image%0Afollowing%20a%20text%20description%2C%20which%20is%20also%20referred%20to%20as%20text-guided%20image%0Aanimation.%20Most%20existing%20methods%20struggle%20to%20generate%20videos%20that%20align%20well%0Awith%20the%20text%20prompts%2C%20particularly%20when%20motion%20is%20specified.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20MotiF%2C%20a%20simple%20yet%20effective%20approach%20that%20directs%0Athe%20model%27s%20learning%20to%20the%20regions%20with%20more%20motion%2C%20thereby%20improving%20the%0Atext%20alignment%20and%20motion%20generation.%20We%20use%20optical%20flow%20to%20generate%20a%20motion%0Aheatmap%20and%20weight%20the%20loss%20according%20to%20the%20intensity%20of%20the%20motion.%20This%0Amodified%20objective%20leads%20to%20noticeable%20improvements%20and%20complements%20existing%0Amethods%20that%20utilize%20motion%20priors%20as%20model%20inputs.%20Additionally%2C%20due%20to%20the%0Alack%20of%20a%20diverse%20benchmark%20for%20evaluating%20TI2V%20generation%2C%20we%20propose%20TI2V%0ABench%2C%20a%20dataset%20consists%20of%20320%20image-text%20pairs%20for%20robust%20evaluation.%20We%0Apresent%20a%20human%20evaluation%20protocol%20that%20asks%20the%20annotators%20to%20select%20an%0Aoverall%20preference%20between%20two%20videos%20followed%20by%20their%20justifications.%20Through%0Aa%20comprehensive%20evaluation%20on%20TI2V%20Bench%2C%20MotiF%20outperforms%20nine%20open-sourced%0Amodels%2C%20achieving%20an%20average%20preference%20of%2072%25.%20The%20TI2V%20Bench%20is%20released%20in%0Ahttps%3A//wang-sj16.github.io/motif/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotiF%253A%2520Making%2520Text%2520Count%2520in%2520Image%2520Animation%2520with%2520Motion%2520Focal%2520Loss%26entry.906535625%3DShijie%2520Wang%2520and%2520Samaneh%2520Azadi%2520and%2520Rohit%2520Girdhar%2520and%2520Saketh%2520Rambhatla%2520and%2520Chen%2520Sun%2520and%2520Xi%2520Yin%26entry.1292438233%3D%2520%2520Text-Image-to-Video%2520%2528TI2V%2529%2520generation%2520aims%2520to%2520generate%2520a%2520video%2520from%2520an%2520image%250Afollowing%2520a%2520text%2520description%252C%2520which%2520is%2520also%2520referred%2520to%2520as%2520text-guided%2520image%250Aanimation.%2520Most%2520existing%2520methods%2520struggle%2520to%2520generate%2520videos%2520that%2520align%2520well%250Awith%2520the%2520text%2520prompts%252C%2520particularly%2520when%2520motion%2520is%2520specified.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520introduce%2520MotiF%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520directs%250Athe%2520model%2527s%2520learning%2520to%2520the%2520regions%2520with%2520more%2520motion%252C%2520thereby%2520improving%2520the%250Atext%2520alignment%2520and%2520motion%2520generation.%2520We%2520use%2520optical%2520flow%2520to%2520generate%2520a%2520motion%250Aheatmap%2520and%2520weight%2520the%2520loss%2520according%2520to%2520the%2520intensity%2520of%2520the%2520motion.%2520This%250Amodified%2520objective%2520leads%2520to%2520noticeable%2520improvements%2520and%2520complements%2520existing%250Amethods%2520that%2520utilize%2520motion%2520priors%2520as%2520model%2520inputs.%2520Additionally%252C%2520due%2520to%2520the%250Alack%2520of%2520a%2520diverse%2520benchmark%2520for%2520evaluating%2520TI2V%2520generation%252C%2520we%2520propose%2520TI2V%250ABench%252C%2520a%2520dataset%2520consists%2520of%2520320%2520image-text%2520pairs%2520for%2520robust%2520evaluation.%2520We%250Apresent%2520a%2520human%2520evaluation%2520protocol%2520that%2520asks%2520the%2520annotators%2520to%2520select%2520an%250Aoverall%2520preference%2520between%2520two%2520videos%2520followed%2520by%2520their%2520justifications.%2520Through%250Aa%2520comprehensive%2520evaluation%2520on%2520TI2V%2520Bench%252C%2520MotiF%2520outperforms%2520nine%2520open-sourced%250Amodels%252C%2520achieving%2520an%2520average%2520preference%2520of%252072%2525.%2520The%2520TI2V%2520Bench%2520is%2520released%2520in%250Ahttps%253A//wang-sj16.github.io/motif/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotiF%3A%20Making%20Text%20Count%20in%20Image%20Animation%20with%20Motion%20Focal%20Loss&entry.906535625=Shijie%20Wang%20and%20Samaneh%20Azadi%20and%20Rohit%20Girdhar%20and%20Saketh%20Rambhatla%20and%20Chen%20Sun%20and%20Xi%20Yin&entry.1292438233=%20%20Text-Image-to-Video%20%28TI2V%29%20generation%20aims%20to%20generate%20a%20video%20from%20an%20image%0Afollowing%20a%20text%20description%2C%20which%20is%20also%20referred%20to%20as%20text-guided%20image%0Aanimation.%20Most%20existing%20methods%20struggle%20to%20generate%20videos%20that%20align%20well%0Awith%20the%20text%20prompts%2C%20particularly%20when%20motion%20is%20specified.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20MotiF%2C%20a%20simple%20yet%20effective%20approach%20that%20directs%0Athe%20model%27s%20learning%20to%20the%20regions%20with%20more%20motion%2C%20thereby%20improving%20the%0Atext%20alignment%20and%20motion%20generation.%20We%20use%20optical%20flow%20to%20generate%20a%20motion%0Aheatmap%20and%20weight%20the%20loss%20according%20to%20the%20intensity%20of%20the%20motion.%20This%0Amodified%20objective%20leads%20to%20noticeable%20improvements%20and%20complements%20existing%0Amethods%20that%20utilize%20motion%20priors%20as%20model%20inputs.%20Additionally%2C%20due%20to%20the%0Alack%20of%20a%20diverse%20benchmark%20for%20evaluating%20TI2V%20generation%2C%20we%20propose%20TI2V%0ABench%2C%20a%20dataset%20consists%20of%20320%20image-text%20pairs%20for%20robust%20evaluation.%20We%0Apresent%20a%20human%20evaluation%20protocol%20that%20asks%20the%20annotators%20to%20select%20an%0Aoverall%20preference%20between%20two%20videos%20followed%20by%20their%20justifications.%20Through%0Aa%20comprehensive%20evaluation%20on%20TI2V%20Bench%2C%20MotiF%20outperforms%20nine%20open-sourced%0Amodels%2C%20achieving%20an%20average%20preference%20of%2072%25.%20The%20TI2V%20Bench%20is%20released%20in%0Ahttps%3A//wang-sj16.github.io/motif/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16153v1&entry.124074799=Read"},
{"title": "Personalized Representation from Personalized Generation", "author": "Shobhita Sundaram and Julia Chae and Yonglong Tian and Sara Beery and Phillip Isola", "abstract": "  Modern vision models excel at general purpose downstream tasks. It is\nunclear, however, how they may be used for personalized vision tasks, which are\nboth fine-grained and data-scarce. Recent works have successfully applied\nsynthetic data to general-purpose representation learning, while advances in\nT2I diffusion models have enabled the generation of personalized images from\njust a few real examples. Here, we explore a potential connection between these\nideas, and formalize the challenge of using personalized synthetic data to\nlearn personalized representations, which encode knowledge about an object of\ninterest and may be flexibly applied to any downstream task relating to the\ntarget object. We introduce an evaluation suite for this challenge, including\nreformulations of two existing datasets and a novel dataset explicitly\nconstructed for this purpose, and propose a contrastive learning approach that\nmakes creative use of image generators. We show that our method improves\npersonalized representation learning for diverse downstream tasks, from\nrecognition to segmentation, and analyze characteristics of image generation\napproaches that are key to this gain.\n", "link": "http://arxiv.org/abs/2412.16156v1", "date": "2024-12-20", "relevancy": 2.3826, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6061}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5997}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Representation%20from%20Personalized%20Generation&body=Title%3A%20Personalized%20Representation%20from%20Personalized%20Generation%0AAuthor%3A%20Shobhita%20Sundaram%20and%20Julia%20Chae%20and%20Yonglong%20Tian%20and%20Sara%20Beery%20and%20Phillip%20Isola%0AAbstract%3A%20%20%20Modern%20vision%20models%20excel%20at%20general%20purpose%20downstream%20tasks.%20It%20is%0Aunclear%2C%20however%2C%20how%20they%20may%20be%20used%20for%20personalized%20vision%20tasks%2C%20which%20are%0Aboth%20fine-grained%20and%20data-scarce.%20Recent%20works%20have%20successfully%20applied%0Asynthetic%20data%20to%20general-purpose%20representation%20learning%2C%20while%20advances%20in%0AT2I%20diffusion%20models%20have%20enabled%20the%20generation%20of%20personalized%20images%20from%0Ajust%20a%20few%20real%20examples.%20Here%2C%20we%20explore%20a%20potential%20connection%20between%20these%0Aideas%2C%20and%20formalize%20the%20challenge%20of%20using%20personalized%20synthetic%20data%20to%0Alearn%20personalized%20representations%2C%20which%20encode%20knowledge%20about%20an%20object%20of%0Ainterest%20and%20may%20be%20flexibly%20applied%20to%20any%20downstream%20task%20relating%20to%20the%0Atarget%20object.%20We%20introduce%20an%20evaluation%20suite%20for%20this%20challenge%2C%20including%0Areformulations%20of%20two%20existing%20datasets%20and%20a%20novel%20dataset%20explicitly%0Aconstructed%20for%20this%20purpose%2C%20and%20propose%20a%20contrastive%20learning%20approach%20that%0Amakes%20creative%20use%20of%20image%20generators.%20We%20show%20that%20our%20method%20improves%0Apersonalized%20representation%20learning%20for%20diverse%20downstream%20tasks%2C%20from%0Arecognition%20to%20segmentation%2C%20and%20analyze%20characteristics%20of%20image%20generation%0Aapproaches%20that%20are%20key%20to%20this%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Representation%2520from%2520Personalized%2520Generation%26entry.906535625%3DShobhita%2520Sundaram%2520and%2520Julia%2520Chae%2520and%2520Yonglong%2520Tian%2520and%2520Sara%2520Beery%2520and%2520Phillip%2520Isola%26entry.1292438233%3D%2520%2520Modern%2520vision%2520models%2520excel%2520at%2520general%2520purpose%2520downstream%2520tasks.%2520It%2520is%250Aunclear%252C%2520however%252C%2520how%2520they%2520may%2520be%2520used%2520for%2520personalized%2520vision%2520tasks%252C%2520which%2520are%250Aboth%2520fine-grained%2520and%2520data-scarce.%2520Recent%2520works%2520have%2520successfully%2520applied%250Asynthetic%2520data%2520to%2520general-purpose%2520representation%2520learning%252C%2520while%2520advances%2520in%250AT2I%2520diffusion%2520models%2520have%2520enabled%2520the%2520generation%2520of%2520personalized%2520images%2520from%250Ajust%2520a%2520few%2520real%2520examples.%2520Here%252C%2520we%2520explore%2520a%2520potential%2520connection%2520between%2520these%250Aideas%252C%2520and%2520formalize%2520the%2520challenge%2520of%2520using%2520personalized%2520synthetic%2520data%2520to%250Alearn%2520personalized%2520representations%252C%2520which%2520encode%2520knowledge%2520about%2520an%2520object%2520of%250Ainterest%2520and%2520may%2520be%2520flexibly%2520applied%2520to%2520any%2520downstream%2520task%2520relating%2520to%2520the%250Atarget%2520object.%2520We%2520introduce%2520an%2520evaluation%2520suite%2520for%2520this%2520challenge%252C%2520including%250Areformulations%2520of%2520two%2520existing%2520datasets%2520and%2520a%2520novel%2520dataset%2520explicitly%250Aconstructed%2520for%2520this%2520purpose%252C%2520and%2520propose%2520a%2520contrastive%2520learning%2520approach%2520that%250Amakes%2520creative%2520use%2520of%2520image%2520generators.%2520We%2520show%2520that%2520our%2520method%2520improves%250Apersonalized%2520representation%2520learning%2520for%2520diverse%2520downstream%2520tasks%252C%2520from%250Arecognition%2520to%2520segmentation%252C%2520and%2520analyze%2520characteristics%2520of%2520image%2520generation%250Aapproaches%2520that%2520are%2520key%2520to%2520this%2520gain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Representation%20from%20Personalized%20Generation&entry.906535625=Shobhita%20Sundaram%20and%20Julia%20Chae%20and%20Yonglong%20Tian%20and%20Sara%20Beery%20and%20Phillip%20Isola&entry.1292438233=%20%20Modern%20vision%20models%20excel%20at%20general%20purpose%20downstream%20tasks.%20It%20is%0Aunclear%2C%20however%2C%20how%20they%20may%20be%20used%20for%20personalized%20vision%20tasks%2C%20which%20are%0Aboth%20fine-grained%20and%20data-scarce.%20Recent%20works%20have%20successfully%20applied%0Asynthetic%20data%20to%20general-purpose%20representation%20learning%2C%20while%20advances%20in%0AT2I%20diffusion%20models%20have%20enabled%20the%20generation%20of%20personalized%20images%20from%0Ajust%20a%20few%20real%20examples.%20Here%2C%20we%20explore%20a%20potential%20connection%20between%20these%0Aideas%2C%20and%20formalize%20the%20challenge%20of%20using%20personalized%20synthetic%20data%20to%0Alearn%20personalized%20representations%2C%20which%20encode%20knowledge%20about%20an%20object%20of%0Ainterest%20and%20may%20be%20flexibly%20applied%20to%20any%20downstream%20task%20relating%20to%20the%0Atarget%20object.%20We%20introduce%20an%20evaluation%20suite%20for%20this%20challenge%2C%20including%0Areformulations%20of%20two%20existing%20datasets%20and%20a%20novel%20dataset%20explicitly%0Aconstructed%20for%20this%20purpose%2C%20and%20propose%20a%20contrastive%20learning%20approach%20that%0Amakes%20creative%20use%20of%20image%20generators.%20We%20show%20that%20our%20method%20improves%0Apersonalized%20representation%20learning%20for%20diverse%20downstream%20tasks%2C%20from%0Arecognition%20to%20segmentation%2C%20and%20analyze%20characteristics%20of%20image%20generation%0Aapproaches%20that%20are%20key%20to%20this%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16156v1&entry.124074799=Read"},
{"title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning", "author": "Jie-Jing Shao and Xiao-Wen Yang and Bo-Wen Zhang and Baizhi Chen and Wen-Da Wei and Guohao Cai and Zhenhua Dong and Lan-Zhe Guo and Yu-feng Li", "abstract": "  Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.\n", "link": "http://arxiv.org/abs/2412.13682v2", "date": "2024-12-20", "relevancy": 2.376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning&body=Title%3A%20ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning%0AAuthor%3A%20Jie-Jing%20Shao%20and%20Xiao-Wen%20Yang%20and%20Bo-Wen%20Zhang%20and%20Baizhi%20Chen%20and%20Wen-Da%20Wei%20and%20Guohao%20Cai%20and%20Zhenhua%20Dong%20and%20Lan-Zhe%20Guo%20and%20Yu-feng%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20LLMs%2C%20particularly%20in%20language%20reasoning%20and%20tool%0Aintegration%2C%20have%20rapidly%20sparked%20the%20real-world%20development%20of%20Language%0AAgents.%20Among%20these%2C%20travel%20planning%20represents%20a%20prominent%20domain%2C%20combining%0Aacademic%20challenges%20with%20practical%20value%20due%20to%20its%20complexity%20and%20market%0Ademand.%20However%2C%20existing%20benchmarks%20fail%20to%20reflect%20the%20diverse%2C%20real-world%0Arequirements%20crucial%20for%20deployment.%20To%20address%20this%20gap%2C%20we%20introduce%0AChinaTravel%2C%20a%20benchmark%20specifically%20designed%20for%20authentic%20Chinese%20travel%0Aplanning%20scenarios.%20We%20collect%20the%20travel%20requirements%20from%20questionnaires%20and%0Apropose%20a%20compositionally%20generalizable%20domain-specific%20language%20that%20enables%20a%0Ascalable%20evaluation%20process%2C%20covering%20feasibility%2C%20constraint%20satisfaction%2C%20and%0Apreference%20comparison.%20Empirical%20studies%20reveal%20the%20potential%20of%20neuro-symbolic%0Aagents%20in%20travel%20planning%2C%20achieving%20a%20constraint%20satisfaction%20rate%20of%2027.9%25%2C%0Asignificantly%20surpassing%20purely%20neural%20models%20at%202.6%25.%20Moreover%2C%20we%20identify%0Akey%20challenges%20in%20real-world%20travel%20planning%20deployments%2C%20including%20open%0Alanguage%20reasoning%20and%20unseen%20concept%20composition.%20These%20findings%20highlight%20the%0Asignificance%20of%20ChinaTravel%20as%20a%20pivotal%20milestone%20for%20advancing%20language%0Aagents%20in%20complex%2C%20real-world%20planning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChinaTravel%253A%2520A%2520Real-World%2520Benchmark%2520for%2520Language%2520Agents%2520in%2520Chinese%250A%2520%2520Travel%2520Planning%26entry.906535625%3DJie-Jing%2520Shao%2520and%2520Xiao-Wen%2520Yang%2520and%2520Bo-Wen%2520Zhang%2520and%2520Baizhi%2520Chen%2520and%2520Wen-Da%2520Wei%2520and%2520Guohao%2520Cai%2520and%2520Zhenhua%2520Dong%2520and%2520Lan-Zhe%2520Guo%2520and%2520Yu-feng%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520LLMs%252C%2520particularly%2520in%2520language%2520reasoning%2520and%2520tool%250Aintegration%252C%2520have%2520rapidly%2520sparked%2520the%2520real-world%2520development%2520of%2520Language%250AAgents.%2520Among%2520these%252C%2520travel%2520planning%2520represents%2520a%2520prominent%2520domain%252C%2520combining%250Aacademic%2520challenges%2520with%2520practical%2520value%2520due%2520to%2520its%2520complexity%2520and%2520market%250Ademand.%2520However%252C%2520existing%2520benchmarks%2520fail%2520to%2520reflect%2520the%2520diverse%252C%2520real-world%250Arequirements%2520crucial%2520for%2520deployment.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AChinaTravel%252C%2520a%2520benchmark%2520specifically%2520designed%2520for%2520authentic%2520Chinese%2520travel%250Aplanning%2520scenarios.%2520We%2520collect%2520the%2520travel%2520requirements%2520from%2520questionnaires%2520and%250Apropose%2520a%2520compositionally%2520generalizable%2520domain-specific%2520language%2520that%2520enables%2520a%250Ascalable%2520evaluation%2520process%252C%2520covering%2520feasibility%252C%2520constraint%2520satisfaction%252C%2520and%250Apreference%2520comparison.%2520Empirical%2520studies%2520reveal%2520the%2520potential%2520of%2520neuro-symbolic%250Aagents%2520in%2520travel%2520planning%252C%2520achieving%2520a%2520constraint%2520satisfaction%2520rate%2520of%252027.9%2525%252C%250Asignificantly%2520surpassing%2520purely%2520neural%2520models%2520at%25202.6%2525.%2520Moreover%252C%2520we%2520identify%250Akey%2520challenges%2520in%2520real-world%2520travel%2520planning%2520deployments%252C%2520including%2520open%250Alanguage%2520reasoning%2520and%2520unseen%2520concept%2520composition.%2520These%2520findings%2520highlight%2520the%250Asignificance%2520of%2520ChinaTravel%2520as%2520a%2520pivotal%2520milestone%2520for%2520advancing%2520language%250Aagents%2520in%2520complex%252C%2520real-world%2520planning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning&entry.906535625=Jie-Jing%20Shao%20and%20Xiao-Wen%20Yang%20and%20Bo-Wen%20Zhang%20and%20Baizhi%20Chen%20and%20Wen-Da%20Wei%20and%20Guohao%20Cai%20and%20Zhenhua%20Dong%20and%20Lan-Zhe%20Guo%20and%20Yu-feng%20Li&entry.1292438233=%20%20Recent%20advances%20in%20LLMs%2C%20particularly%20in%20language%20reasoning%20and%20tool%0Aintegration%2C%20have%20rapidly%20sparked%20the%20real-world%20development%20of%20Language%0AAgents.%20Among%20these%2C%20travel%20planning%20represents%20a%20prominent%20domain%2C%20combining%0Aacademic%20challenges%20with%20practical%20value%20due%20to%20its%20complexity%20and%20market%0Ademand.%20However%2C%20existing%20benchmarks%20fail%20to%20reflect%20the%20diverse%2C%20real-world%0Arequirements%20crucial%20for%20deployment.%20To%20address%20this%20gap%2C%20we%20introduce%0AChinaTravel%2C%20a%20benchmark%20specifically%20designed%20for%20authentic%20Chinese%20travel%0Aplanning%20scenarios.%20We%20collect%20the%20travel%20requirements%20from%20questionnaires%20and%0Apropose%20a%20compositionally%20generalizable%20domain-specific%20language%20that%20enables%20a%0Ascalable%20evaluation%20process%2C%20covering%20feasibility%2C%20constraint%20satisfaction%2C%20and%0Apreference%20comparison.%20Empirical%20studies%20reveal%20the%20potential%20of%20neuro-symbolic%0Aagents%20in%20travel%20planning%2C%20achieving%20a%20constraint%20satisfaction%20rate%20of%2027.9%25%2C%0Asignificantly%20surpassing%20purely%20neural%20models%20at%202.6%25.%20Moreover%2C%20we%20identify%0Akey%20challenges%20in%20real-world%20travel%20planning%20deployments%2C%20including%20open%0Alanguage%20reasoning%20and%20unseen%20concept%20composition.%20These%20findings%20highlight%20the%0Asignificance%20of%20ChinaTravel%20as%20a%20pivotal%20milestone%20for%20advancing%20language%0Aagents%20in%20complex%2C%20real-world%20planning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13682v2&entry.124074799=Read"},
{"title": "On Robust Cross Domain Alignment", "author": "Anish Chakrabarty and Arkaprabha Basu and Swagatam Das", "abstract": "  The Gromov-Wasserstein (GW) distance is an effective measure of alignment\nbetween distributions supported on distinct ambient spaces. Calculating\nessentially the mutual departure from isometry, it has found vast usage in\ndomain translation and network analysis. It has long been shown to be\nvulnerable to contamination in the underlying measures. All efforts to\nintroduce robustness in GW have been inspired by similar techniques in optimal\ntransport (OT), which predominantly advocate partial mass transport or\nunbalancing. In contrast, the cross-domain alignment problem being\nfundamentally different from OT, demands specific solutions to tackle diverse\napplications and contamination regimes. Deriving from robust statistics, we\ndiscuss three contextually novel techniques to robustify GW and its variants.\nFor each method, we explore metric properties and robustness guarantees along\nwith their co-dependencies and individual relations with the GW distance. For a\ncomprehensive view, we empirically validate their superior resilience to\ncontamination under real machine learning tasks against state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2412.15861v1", "date": "2024-12-20", "relevancy": 2.3599, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4887}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4678}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Robust%20Cross%20Domain%20Alignment&body=Title%3A%20On%20Robust%20Cross%20Domain%20Alignment%0AAuthor%3A%20Anish%20Chakrabarty%20and%20Arkaprabha%20Basu%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20The%20Gromov-Wasserstein%20%28GW%29%20distance%20is%20an%20effective%20measure%20of%20alignment%0Abetween%20distributions%20supported%20on%20distinct%20ambient%20spaces.%20Calculating%0Aessentially%20the%20mutual%20departure%20from%20isometry%2C%20it%20has%20found%20vast%20usage%20in%0Adomain%20translation%20and%20network%20analysis.%20It%20has%20long%20been%20shown%20to%20be%0Avulnerable%20to%20contamination%20in%20the%20underlying%20measures.%20All%20efforts%20to%0Aintroduce%20robustness%20in%20GW%20have%20been%20inspired%20by%20similar%20techniques%20in%20optimal%0Atransport%20%28OT%29%2C%20which%20predominantly%20advocate%20partial%20mass%20transport%20or%0Aunbalancing.%20In%20contrast%2C%20the%20cross-domain%20alignment%20problem%20being%0Afundamentally%20different%20from%20OT%2C%20demands%20specific%20solutions%20to%20tackle%20diverse%0Aapplications%20and%20contamination%20regimes.%20Deriving%20from%20robust%20statistics%2C%20we%0Adiscuss%20three%20contextually%20novel%20techniques%20to%20robustify%20GW%20and%20its%20variants.%0AFor%20each%20method%2C%20we%20explore%20metric%20properties%20and%20robustness%20guarantees%20along%0Awith%20their%20co-dependencies%20and%20individual%20relations%20with%20the%20GW%20distance.%20For%20a%0Acomprehensive%20view%2C%20we%20empirically%20validate%20their%20superior%20resilience%20to%0Acontamination%20under%20real%20machine%20learning%20tasks%20against%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Robust%2520Cross%2520Domain%2520Alignment%26entry.906535625%3DAnish%2520Chakrabarty%2520and%2520Arkaprabha%2520Basu%2520and%2520Swagatam%2520Das%26entry.1292438233%3D%2520%2520The%2520Gromov-Wasserstein%2520%2528GW%2529%2520distance%2520is%2520an%2520effective%2520measure%2520of%2520alignment%250Abetween%2520distributions%2520supported%2520on%2520distinct%2520ambient%2520spaces.%2520Calculating%250Aessentially%2520the%2520mutual%2520departure%2520from%2520isometry%252C%2520it%2520has%2520found%2520vast%2520usage%2520in%250Adomain%2520translation%2520and%2520network%2520analysis.%2520It%2520has%2520long%2520been%2520shown%2520to%2520be%250Avulnerable%2520to%2520contamination%2520in%2520the%2520underlying%2520measures.%2520All%2520efforts%2520to%250Aintroduce%2520robustness%2520in%2520GW%2520have%2520been%2520inspired%2520by%2520similar%2520techniques%2520in%2520optimal%250Atransport%2520%2528OT%2529%252C%2520which%2520predominantly%2520advocate%2520partial%2520mass%2520transport%2520or%250Aunbalancing.%2520In%2520contrast%252C%2520the%2520cross-domain%2520alignment%2520problem%2520being%250Afundamentally%2520different%2520from%2520OT%252C%2520demands%2520specific%2520solutions%2520to%2520tackle%2520diverse%250Aapplications%2520and%2520contamination%2520regimes.%2520Deriving%2520from%2520robust%2520statistics%252C%2520we%250Adiscuss%2520three%2520contextually%2520novel%2520techniques%2520to%2520robustify%2520GW%2520and%2520its%2520variants.%250AFor%2520each%2520method%252C%2520we%2520explore%2520metric%2520properties%2520and%2520robustness%2520guarantees%2520along%250Awith%2520their%2520co-dependencies%2520and%2520individual%2520relations%2520with%2520the%2520GW%2520distance.%2520For%2520a%250Acomprehensive%2520view%252C%2520we%2520empirically%2520validate%2520their%2520superior%2520resilience%2520to%250Acontamination%2520under%2520real%2520machine%2520learning%2520tasks%2520against%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Robust%20Cross%20Domain%20Alignment&entry.906535625=Anish%20Chakrabarty%20and%20Arkaprabha%20Basu%20and%20Swagatam%20Das&entry.1292438233=%20%20The%20Gromov-Wasserstein%20%28GW%29%20distance%20is%20an%20effective%20measure%20of%20alignment%0Abetween%20distributions%20supported%20on%20distinct%20ambient%20spaces.%20Calculating%0Aessentially%20the%20mutual%20departure%20from%20isometry%2C%20it%20has%20found%20vast%20usage%20in%0Adomain%20translation%20and%20network%20analysis.%20It%20has%20long%20been%20shown%20to%20be%0Avulnerable%20to%20contamination%20in%20the%20underlying%20measures.%20All%20efforts%20to%0Aintroduce%20robustness%20in%20GW%20have%20been%20inspired%20by%20similar%20techniques%20in%20optimal%0Atransport%20%28OT%29%2C%20which%20predominantly%20advocate%20partial%20mass%20transport%20or%0Aunbalancing.%20In%20contrast%2C%20the%20cross-domain%20alignment%20problem%20being%0Afundamentally%20different%20from%20OT%2C%20demands%20specific%20solutions%20to%20tackle%20diverse%0Aapplications%20and%20contamination%20regimes.%20Deriving%20from%20robust%20statistics%2C%20we%0Adiscuss%20three%20contextually%20novel%20techniques%20to%20robustify%20GW%20and%20its%20variants.%0AFor%20each%20method%2C%20we%20explore%20metric%20properties%20and%20robustness%20guarantees%20along%0Awith%20their%20co-dependencies%20and%20individual%20relations%20with%20the%20GW%20distance.%20For%20a%0Acomprehensive%20view%2C%20we%20empirically%20validate%20their%20superior%20resilience%20to%0Acontamination%20under%20real%20machine%20learning%20tasks%20against%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15861v1&entry.124074799=Read"},
{"title": "FORCE: Physics-aware Human-object Interaction", "author": "Xiaohan Zhang and Bharat Lal Bhatnagar and Sebastian Starke and Ilya Petrov and Vladimir Guzov and Helisa Dhamo and Eduardo P\u00e9rez-Pellitero and Gerard Pons-Moll", "abstract": "  Interactions between human and objects are influenced not only by the\nobject's pose and shape, but also by physical attributes such as object mass\nand surface friction. They introduce important motion nuances that are\nessential for diversity and realism. Despite advancements in recent\nhuman-object interaction methods, this aspect has been overlooked. Generating\nnuanced human motion presents two challenges. First, it is non-trivial to learn\nfrom multi-modal human and object information derived from both the physical\nand non-physical attributes. Second, there exists no dataset capturing nuanced\nhuman interactions with objects of varying physical properties, hampering model\ndevelopment. This work addresses the gap by introducing the FORCE model, an\napproach for synthesizing diverse, nuanced human-object interactions by\nmodeling physical attributes. Our key insight is that human motion is dictated\nby the interrelation between the force exerted by the human and the perceived\nresistance. Guided by a novel intuitive physics encoding, the model captures\nthe interplay between human force and resistance. Experiments also demonstrate\nincorporating human force facilitates learning multi-class motion. Accompanying\nour model, we contribute a dataset, which features diverse, different-styled\nmotion through interactions with varying resistances.\n", "link": "http://arxiv.org/abs/2403.11237v2", "date": "2024-12-20", "relevancy": 2.3539, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6135}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORCE%3A%20Physics-aware%20Human-object%20Interaction&body=Title%3A%20FORCE%3A%20Physics-aware%20Human-object%20Interaction%0AAuthor%3A%20Xiaohan%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Sebastian%20Starke%20and%20Ilya%20Petrov%20and%20Vladimir%20Guzov%20and%20Helisa%20Dhamo%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20Interactions%20between%20human%20and%20objects%20are%20influenced%20not%20only%20by%20the%0Aobject%27s%20pose%20and%20shape%2C%20but%20also%20by%20physical%20attributes%20such%20as%20object%20mass%0Aand%20surface%20friction.%20They%20introduce%20important%20motion%20nuances%20that%20are%0Aessential%20for%20diversity%20and%20realism.%20Despite%20advancements%20in%20recent%0Ahuman-object%20interaction%20methods%2C%20this%20aspect%20has%20been%20overlooked.%20Generating%0Anuanced%20human%20motion%20presents%20two%20challenges.%20First%2C%20it%20is%20non-trivial%20to%20learn%0Afrom%20multi-modal%20human%20and%20object%20information%20derived%20from%20both%20the%20physical%0Aand%20non-physical%20attributes.%20Second%2C%20there%20exists%20no%20dataset%20capturing%20nuanced%0Ahuman%20interactions%20with%20objects%20of%20varying%20physical%20properties%2C%20hampering%20model%0Adevelopment.%20This%20work%20addresses%20the%20gap%20by%20introducing%20the%20FORCE%20model%2C%20an%0Aapproach%20for%20synthesizing%20diverse%2C%20nuanced%20human-object%20interactions%20by%0Amodeling%20physical%20attributes.%20Our%20key%20insight%20is%20that%20human%20motion%20is%20dictated%0Aby%20the%20interrelation%20between%20the%20force%20exerted%20by%20the%20human%20and%20the%20perceived%0Aresistance.%20Guided%20by%20a%20novel%20intuitive%20physics%20encoding%2C%20the%20model%20captures%0Athe%20interplay%20between%20human%20force%20and%20resistance.%20Experiments%20also%20demonstrate%0Aincorporating%20human%20force%20facilitates%20learning%20multi-class%20motion.%20Accompanying%0Aour%20model%2C%20we%20contribute%20a%20dataset%2C%20which%20features%20diverse%2C%20different-styled%0Amotion%20through%20interactions%20with%20varying%20resistances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORCE%253A%2520Physics-aware%2520Human-object%2520Interaction%26entry.906535625%3DXiaohan%2520Zhang%2520and%2520Bharat%2520Lal%2520Bhatnagar%2520and%2520Sebastian%2520Starke%2520and%2520Ilya%2520Petrov%2520and%2520Vladimir%2520Guzov%2520and%2520Helisa%2520Dhamo%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520Interactions%2520between%2520human%2520and%2520objects%2520are%2520influenced%2520not%2520only%2520by%2520the%250Aobject%2527s%2520pose%2520and%2520shape%252C%2520but%2520also%2520by%2520physical%2520attributes%2520such%2520as%2520object%2520mass%250Aand%2520surface%2520friction.%2520They%2520introduce%2520important%2520motion%2520nuances%2520that%2520are%250Aessential%2520for%2520diversity%2520and%2520realism.%2520Despite%2520advancements%2520in%2520recent%250Ahuman-object%2520interaction%2520methods%252C%2520this%2520aspect%2520has%2520been%2520overlooked.%2520Generating%250Anuanced%2520human%2520motion%2520presents%2520two%2520challenges.%2520First%252C%2520it%2520is%2520non-trivial%2520to%2520learn%250Afrom%2520multi-modal%2520human%2520and%2520object%2520information%2520derived%2520from%2520both%2520the%2520physical%250Aand%2520non-physical%2520attributes.%2520Second%252C%2520there%2520exists%2520no%2520dataset%2520capturing%2520nuanced%250Ahuman%2520interactions%2520with%2520objects%2520of%2520varying%2520physical%2520properties%252C%2520hampering%2520model%250Adevelopment.%2520This%2520work%2520addresses%2520the%2520gap%2520by%2520introducing%2520the%2520FORCE%2520model%252C%2520an%250Aapproach%2520for%2520synthesizing%2520diverse%252C%2520nuanced%2520human-object%2520interactions%2520by%250Amodeling%2520physical%2520attributes.%2520Our%2520key%2520insight%2520is%2520that%2520human%2520motion%2520is%2520dictated%250Aby%2520the%2520interrelation%2520between%2520the%2520force%2520exerted%2520by%2520the%2520human%2520and%2520the%2520perceived%250Aresistance.%2520Guided%2520by%2520a%2520novel%2520intuitive%2520physics%2520encoding%252C%2520the%2520model%2520captures%250Athe%2520interplay%2520between%2520human%2520force%2520and%2520resistance.%2520Experiments%2520also%2520demonstrate%250Aincorporating%2520human%2520force%2520facilitates%2520learning%2520multi-class%2520motion.%2520Accompanying%250Aour%2520model%252C%2520we%2520contribute%2520a%2520dataset%252C%2520which%2520features%2520diverse%252C%2520different-styled%250Amotion%2520through%2520interactions%2520with%2520varying%2520resistances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORCE%3A%20Physics-aware%20Human-object%20Interaction&entry.906535625=Xiaohan%20Zhang%20and%20Bharat%20Lal%20Bhatnagar%20and%20Sebastian%20Starke%20and%20Ilya%20Petrov%20and%20Vladimir%20Guzov%20and%20Helisa%20Dhamo%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20Interactions%20between%20human%20and%20objects%20are%20influenced%20not%20only%20by%20the%0Aobject%27s%20pose%20and%20shape%2C%20but%20also%20by%20physical%20attributes%20such%20as%20object%20mass%0Aand%20surface%20friction.%20They%20introduce%20important%20motion%20nuances%20that%20are%0Aessential%20for%20diversity%20and%20realism.%20Despite%20advancements%20in%20recent%0Ahuman-object%20interaction%20methods%2C%20this%20aspect%20has%20been%20overlooked.%20Generating%0Anuanced%20human%20motion%20presents%20two%20challenges.%20First%2C%20it%20is%20non-trivial%20to%20learn%0Afrom%20multi-modal%20human%20and%20object%20information%20derived%20from%20both%20the%20physical%0Aand%20non-physical%20attributes.%20Second%2C%20there%20exists%20no%20dataset%20capturing%20nuanced%0Ahuman%20interactions%20with%20objects%20of%20varying%20physical%20properties%2C%20hampering%20model%0Adevelopment.%20This%20work%20addresses%20the%20gap%20by%20introducing%20the%20FORCE%20model%2C%20an%0Aapproach%20for%20synthesizing%20diverse%2C%20nuanced%20human-object%20interactions%20by%0Amodeling%20physical%20attributes.%20Our%20key%20insight%20is%20that%20human%20motion%20is%20dictated%0Aby%20the%20interrelation%20between%20the%20force%20exerted%20by%20the%20human%20and%20the%20perceived%0Aresistance.%20Guided%20by%20a%20novel%20intuitive%20physics%20encoding%2C%20the%20model%20captures%0Athe%20interplay%20between%20human%20force%20and%20resistance.%20Experiments%20also%20demonstrate%0Aincorporating%20human%20force%20facilitates%20learning%20multi-class%20motion.%20Accompanying%0Aour%20model%2C%20we%20contribute%20a%20dataset%2C%20which%20features%20diverse%2C%20different-styled%0Amotion%20through%20interactions%20with%20varying%20resistances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11237v2&entry.124074799=Read"},
{"title": "Cross-Modal Few-Shot Learning with Second-Order Neural Ordinary\n  Differential Equations", "author": "Yi Zhang and Chun-Wun Cheng and Junyi He and Zhihai He and Carola-Bibiane Sch\u00f6nlieb and Yuyan Chen and Angelica I Aviles-Rivero", "abstract": "  We introduce SONO, a novel method leveraging Second-Order Neural Ordinary\nDifferential Equations (Second-Order NODEs) to enhance cross-modal few-shot\nlearning. By employing a simple yet effective architecture consisting of a\nSecond-Order NODEs model paired with a cross-modal classifier, SONO addresses\nthe significant challenge of overfitting, which is common in few-shot scenarios\ndue to limited training examples. Our second-order approach can approximate a\nbroader class of functions, enhancing the model's expressive power and feature\ngeneralization capabilities. We initialize our cross-modal classifier with text\nembeddings derived from class-relevant prompts, streamlining training\nefficiency by avoiding the need for frequent text encoder processing.\nAdditionally, we utilize text-based image augmentation, exploiting CLIP's\nrobust image-text correlation to enrich training data significantly. Extensive\nexperiments across multiple datasets demonstrate that SONO outperforms existing\nstate-of-the-art methods in few-shot learning performance.\n", "link": "http://arxiv.org/abs/2412.15813v1", "date": "2024-12-20", "relevancy": 2.3499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Few-Shot%20Learning%20with%20Second-Order%20Neural%20Ordinary%0A%20%20Differential%20Equations&body=Title%3A%20Cross-Modal%20Few-Shot%20Learning%20with%20Second-Order%20Neural%20Ordinary%0A%20%20Differential%20Equations%0AAuthor%3A%20Yi%20Zhang%20and%20Chun-Wun%20Cheng%20and%20Junyi%20He%20and%20Zhihai%20He%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Yuyan%20Chen%20and%20Angelica%20I%20Aviles-Rivero%0AAbstract%3A%20%20%20We%20introduce%20SONO%2C%20a%20novel%20method%20leveraging%20Second-Order%20Neural%20Ordinary%0ADifferential%20Equations%20%28Second-Order%20NODEs%29%20to%20enhance%20cross-modal%20few-shot%0Alearning.%20By%20employing%20a%20simple%20yet%20effective%20architecture%20consisting%20of%20a%0ASecond-Order%20NODEs%20model%20paired%20with%20a%20cross-modal%20classifier%2C%20SONO%20addresses%0Athe%20significant%20challenge%20of%20overfitting%2C%20which%20is%20common%20in%20few-shot%20scenarios%0Adue%20to%20limited%20training%20examples.%20Our%20second-order%20approach%20can%20approximate%20a%0Abroader%20class%20of%20functions%2C%20enhancing%20the%20model%27s%20expressive%20power%20and%20feature%0Ageneralization%20capabilities.%20We%20initialize%20our%20cross-modal%20classifier%20with%20text%0Aembeddings%20derived%20from%20class-relevant%20prompts%2C%20streamlining%20training%0Aefficiency%20by%20avoiding%20the%20need%20for%20frequent%20text%20encoder%20processing.%0AAdditionally%2C%20we%20utilize%20text-based%20image%20augmentation%2C%20exploiting%20CLIP%27s%0Arobust%20image-text%20correlation%20to%20enrich%20training%20data%20significantly.%20Extensive%0Aexperiments%20across%20multiple%20datasets%20demonstrate%20that%20SONO%20outperforms%20existing%0Astate-of-the-art%20methods%20in%20few-shot%20learning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Few-Shot%2520Learning%2520with%2520Second-Order%2520Neural%2520Ordinary%250A%2520%2520Differential%2520Equations%26entry.906535625%3DYi%2520Zhang%2520and%2520Chun-Wun%2520Cheng%2520and%2520Junyi%2520He%2520and%2520Zhihai%2520He%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Yuyan%2520Chen%2520and%2520Angelica%2520I%2520Aviles-Rivero%26entry.1292438233%3D%2520%2520We%2520introduce%2520SONO%252C%2520a%2520novel%2520method%2520leveraging%2520Second-Order%2520Neural%2520Ordinary%250ADifferential%2520Equations%2520%2528Second-Order%2520NODEs%2529%2520to%2520enhance%2520cross-modal%2520few-shot%250Alearning.%2520By%2520employing%2520a%2520simple%2520yet%2520effective%2520architecture%2520consisting%2520of%2520a%250ASecond-Order%2520NODEs%2520model%2520paired%2520with%2520a%2520cross-modal%2520classifier%252C%2520SONO%2520addresses%250Athe%2520significant%2520challenge%2520of%2520overfitting%252C%2520which%2520is%2520common%2520in%2520few-shot%2520scenarios%250Adue%2520to%2520limited%2520training%2520examples.%2520Our%2520second-order%2520approach%2520can%2520approximate%2520a%250Abroader%2520class%2520of%2520functions%252C%2520enhancing%2520the%2520model%2527s%2520expressive%2520power%2520and%2520feature%250Ageneralization%2520capabilities.%2520We%2520initialize%2520our%2520cross-modal%2520classifier%2520with%2520text%250Aembeddings%2520derived%2520from%2520class-relevant%2520prompts%252C%2520streamlining%2520training%250Aefficiency%2520by%2520avoiding%2520the%2520need%2520for%2520frequent%2520text%2520encoder%2520processing.%250AAdditionally%252C%2520we%2520utilize%2520text-based%2520image%2520augmentation%252C%2520exploiting%2520CLIP%2527s%250Arobust%2520image-text%2520correlation%2520to%2520enrich%2520training%2520data%2520significantly.%2520Extensive%250Aexperiments%2520across%2520multiple%2520datasets%2520demonstrate%2520that%2520SONO%2520outperforms%2520existing%250Astate-of-the-art%2520methods%2520in%2520few-shot%2520learning%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Few-Shot%20Learning%20with%20Second-Order%20Neural%20Ordinary%0A%20%20Differential%20Equations&entry.906535625=Yi%20Zhang%20and%20Chun-Wun%20Cheng%20and%20Junyi%20He%20and%20Zhihai%20He%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Yuyan%20Chen%20and%20Angelica%20I%20Aviles-Rivero&entry.1292438233=%20%20We%20introduce%20SONO%2C%20a%20novel%20method%20leveraging%20Second-Order%20Neural%20Ordinary%0ADifferential%20Equations%20%28Second-Order%20NODEs%29%20to%20enhance%20cross-modal%20few-shot%0Alearning.%20By%20employing%20a%20simple%20yet%20effective%20architecture%20consisting%20of%20a%0ASecond-Order%20NODEs%20model%20paired%20with%20a%20cross-modal%20classifier%2C%20SONO%20addresses%0Athe%20significant%20challenge%20of%20overfitting%2C%20which%20is%20common%20in%20few-shot%20scenarios%0Adue%20to%20limited%20training%20examples.%20Our%20second-order%20approach%20can%20approximate%20a%0Abroader%20class%20of%20functions%2C%20enhancing%20the%20model%27s%20expressive%20power%20and%20feature%0Ageneralization%20capabilities.%20We%20initialize%20our%20cross-modal%20classifier%20with%20text%0Aembeddings%20derived%20from%20class-relevant%20prompts%2C%20streamlining%20training%0Aefficiency%20by%20avoiding%20the%20need%20for%20frequent%20text%20encoder%20processing.%0AAdditionally%2C%20we%20utilize%20text-based%20image%20augmentation%2C%20exploiting%20CLIP%27s%0Arobust%20image-text%20correlation%20to%20enrich%20training%20data%20significantly.%20Extensive%0Aexperiments%20across%20multiple%20datasets%20demonstrate%20that%20SONO%20outperforms%20existing%0Astate-of-the-art%20methods%20in%20few-shot%20learning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15813v1&entry.124074799=Read"},
{"title": "RiTTA: Modeling Event Relations in Text-to-Audio Generation", "author": "Yuhang He and Yash Jain and Xubo Liu and Andrew Markham and Vibhav Vineet", "abstract": "  Despite significant advancements in Text-to-Audio (TTA) generation models\nachieving high-fidelity audio with fine-grained context understanding, they\nstruggle to model the relations between audio events described in the input\ntext. However, previous TTA methods have not systematically explored audio\nevent relation modeling, nor have they proposed frameworks to enhance this\ncapability. In this work, we systematically study audio event relation modeling\nin TTA generation models. We first establish a benchmark for this task by: 1.\nproposing a comprehensive relation corpus covering all potential relations in\nreal-world scenarios; 2. introducing a new audio event corpus encompassing\ncommonly heard audios; and 3. proposing new evaluation metrics to assess audio\nevent relation modeling from various perspectives. Furthermore, we propose a\nfinetuning framework to enhance existing TTA models ability to model audio\nevents relation. Code is available at: https://github.com/yuhanghe01/RiTTA\n", "link": "http://arxiv.org/abs/2412.15922v1", "date": "2024-12-20", "relevancy": 2.3447, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RiTTA%3A%20Modeling%20Event%20Relations%20in%20Text-to-Audio%20Generation&body=Title%3A%20RiTTA%3A%20Modeling%20Event%20Relations%20in%20Text-to-Audio%20Generation%0AAuthor%3A%20Yuhang%20He%20and%20Yash%20Jain%20and%20Xubo%20Liu%20and%20Andrew%20Markham%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20Text-to-Audio%20%28TTA%29%20generation%20models%0Aachieving%20high-fidelity%20audio%20with%20fine-grained%20context%20understanding%2C%20they%0Astruggle%20to%20model%20the%20relations%20between%20audio%20events%20described%20in%20the%20input%0Atext.%20However%2C%20previous%20TTA%20methods%20have%20not%20systematically%20explored%20audio%0Aevent%20relation%20modeling%2C%20nor%20have%20they%20proposed%20frameworks%20to%20enhance%20this%0Acapability.%20In%20this%20work%2C%20we%20systematically%20study%20audio%20event%20relation%20modeling%0Ain%20TTA%20generation%20models.%20We%20first%20establish%20a%20benchmark%20for%20this%20task%20by%3A%201.%0Aproposing%20a%20comprehensive%20relation%20corpus%20covering%20all%20potential%20relations%20in%0Areal-world%20scenarios%3B%202.%20introducing%20a%20new%20audio%20event%20corpus%20encompassing%0Acommonly%20heard%20audios%3B%20and%203.%20proposing%20new%20evaluation%20metrics%20to%20assess%20audio%0Aevent%20relation%20modeling%20from%20various%20perspectives.%20Furthermore%2C%20we%20propose%20a%0Afinetuning%20framework%20to%20enhance%20existing%20TTA%20models%20ability%20to%20model%20audio%0Aevents%20relation.%20Code%20is%20available%20at%3A%20https%3A//github.com/yuhanghe01/RiTTA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiTTA%253A%2520Modeling%2520Event%2520Relations%2520in%2520Text-to-Audio%2520Generation%26entry.906535625%3DYuhang%2520He%2520and%2520Yash%2520Jain%2520and%2520Xubo%2520Liu%2520and%2520Andrew%2520Markham%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520Text-to-Audio%2520%2528TTA%2529%2520generation%2520models%250Aachieving%2520high-fidelity%2520audio%2520with%2520fine-grained%2520context%2520understanding%252C%2520they%250Astruggle%2520to%2520model%2520the%2520relations%2520between%2520audio%2520events%2520described%2520in%2520the%2520input%250Atext.%2520However%252C%2520previous%2520TTA%2520methods%2520have%2520not%2520systematically%2520explored%2520audio%250Aevent%2520relation%2520modeling%252C%2520nor%2520have%2520they%2520proposed%2520frameworks%2520to%2520enhance%2520this%250Acapability.%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%2520audio%2520event%2520relation%2520modeling%250Ain%2520TTA%2520generation%2520models.%2520We%2520first%2520establish%2520a%2520benchmark%2520for%2520this%2520task%2520by%253A%25201.%250Aproposing%2520a%2520comprehensive%2520relation%2520corpus%2520covering%2520all%2520potential%2520relations%2520in%250Areal-world%2520scenarios%253B%25202.%2520introducing%2520a%2520new%2520audio%2520event%2520corpus%2520encompassing%250Acommonly%2520heard%2520audios%253B%2520and%25203.%2520proposing%2520new%2520evaluation%2520metrics%2520to%2520assess%2520audio%250Aevent%2520relation%2520modeling%2520from%2520various%2520perspectives.%2520Furthermore%252C%2520we%2520propose%2520a%250Afinetuning%2520framework%2520to%2520enhance%2520existing%2520TTA%2520models%2520ability%2520to%2520model%2520audio%250Aevents%2520relation.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/yuhanghe01/RiTTA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RiTTA%3A%20Modeling%20Event%20Relations%20in%20Text-to-Audio%20Generation&entry.906535625=Yuhang%20He%20and%20Yash%20Jain%20and%20Xubo%20Liu%20and%20Andrew%20Markham%20and%20Vibhav%20Vineet&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20Text-to-Audio%20%28TTA%29%20generation%20models%0Aachieving%20high-fidelity%20audio%20with%20fine-grained%20context%20understanding%2C%20they%0Astruggle%20to%20model%20the%20relations%20between%20audio%20events%20described%20in%20the%20input%0Atext.%20However%2C%20previous%20TTA%20methods%20have%20not%20systematically%20explored%20audio%0Aevent%20relation%20modeling%2C%20nor%20have%20they%20proposed%20frameworks%20to%20enhance%20this%0Acapability.%20In%20this%20work%2C%20we%20systematically%20study%20audio%20event%20relation%20modeling%0Ain%20TTA%20generation%20models.%20We%20first%20establish%20a%20benchmark%20for%20this%20task%20by%3A%201.%0Aproposing%20a%20comprehensive%20relation%20corpus%20covering%20all%20potential%20relations%20in%0Areal-world%20scenarios%3B%202.%20introducing%20a%20new%20audio%20event%20corpus%20encompassing%0Acommonly%20heard%20audios%3B%20and%203.%20proposing%20new%20evaluation%20metrics%20to%20assess%20audio%0Aevent%20relation%20modeling%20from%20various%20perspectives.%20Furthermore%2C%20we%20propose%20a%0Afinetuning%20framework%20to%20enhance%20existing%20TTA%20models%20ability%20to%20model%20audio%0Aevents%20relation.%20Code%20is%20available%20at%3A%20https%3A//github.com/yuhanghe01/RiTTA%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15922v1&entry.124074799=Read"},
{"title": "Prompt-based Unifying Inference Attack on Graph Neural Networks", "author": "Yuecen Wei and Xingcheng Fu and Lingyun Liu and Qingyun Sun and Hao Peng and Chunming Hu", "abstract": "  Graph neural networks (GNNs) provide important prospective insights in\napplications such as social behavior analysis and financial risk analysis based\non their powerful learning capabilities on graph data. Nevertheless, GNNs'\npredictive performance relies on the quality of task-specific node labels, so\nit is common practice to improve the model's generalization ability in the\ndownstream execution of decision-making tasks through pre-training. Graph\nprompting is a prudent choice but risky without taking measures to prevent data\nleakage. In other words, in high-risk decision scenarios, prompt learning can\ninfer private information by accessing model parameters trained on private data\n(publishing model parameters in pre-training, i.e., without directly leaking\nthe raw data, is a tacitly accepted trend). However, myriad graph inference\nattacks necessitate tailored module design and processing to enhance inference\ncapabilities due to variations in supervision signals. In this paper, we\npropose a novel Prompt-based unifying Inference Attack framework on GNNs, named\nProIA. Specifically, ProIA retains the crucial topological information of the\ngraph during pre-training, enhancing the background knowledge of the inference\nattack model. It then utilizes a unified prompt and introduces additional\ndisentanglement factors in downstream attacks to adapt to task-relevant\nknowledge. Finally, extensive experiments show that ProIA enhances attack\ncapabilities and demonstrates remarkable adaptability to various inference\nattacks.\n", "link": "http://arxiv.org/abs/2412.15735v1", "date": "2024-12-20", "relevancy": 2.3398, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4902}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-based%20Unifying%20Inference%20Attack%20on%20Graph%20Neural%20Networks&body=Title%3A%20Prompt-based%20Unifying%20Inference%20Attack%20on%20Graph%20Neural%20Networks%0AAuthor%3A%20Yuecen%20Wei%20and%20Xingcheng%20Fu%20and%20Lingyun%20Liu%20and%20Qingyun%20Sun%20and%20Hao%20Peng%20and%20Chunming%20Hu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20provide%20important%20prospective%20insights%20in%0Aapplications%20such%20as%20social%20behavior%20analysis%20and%20financial%20risk%20analysis%20based%0Aon%20their%20powerful%20learning%20capabilities%20on%20graph%20data.%20Nevertheless%2C%20GNNs%27%0Apredictive%20performance%20relies%20on%20the%20quality%20of%20task-specific%20node%20labels%2C%20so%0Ait%20is%20common%20practice%20to%20improve%20the%20model%27s%20generalization%20ability%20in%20the%0Adownstream%20execution%20of%20decision-making%20tasks%20through%20pre-training.%20Graph%0Aprompting%20is%20a%20prudent%20choice%20but%20risky%20without%20taking%20measures%20to%20prevent%20data%0Aleakage.%20In%20other%20words%2C%20in%20high-risk%20decision%20scenarios%2C%20prompt%20learning%20can%0Ainfer%20private%20information%20by%20accessing%20model%20parameters%20trained%20on%20private%20data%0A%28publishing%20model%20parameters%20in%20pre-training%2C%20i.e.%2C%20without%20directly%20leaking%0Athe%20raw%20data%2C%20is%20a%20tacitly%20accepted%20trend%29.%20However%2C%20myriad%20graph%20inference%0Aattacks%20necessitate%20tailored%20module%20design%20and%20processing%20to%20enhance%20inference%0Acapabilities%20due%20to%20variations%20in%20supervision%20signals.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Prompt-based%20unifying%20Inference%20Attack%20framework%20on%20GNNs%2C%20named%0AProIA.%20Specifically%2C%20ProIA%20retains%20the%20crucial%20topological%20information%20of%20the%0Agraph%20during%20pre-training%2C%20enhancing%20the%20background%20knowledge%20of%20the%20inference%0Aattack%20model.%20It%20then%20utilizes%20a%20unified%20prompt%20and%20introduces%20additional%0Adisentanglement%20factors%20in%20downstream%20attacks%20to%20adapt%20to%20task-relevant%0Aknowledge.%20Finally%2C%20extensive%20experiments%20show%20that%20ProIA%20enhances%20attack%0Acapabilities%20and%20demonstrates%20remarkable%20adaptability%20to%20various%20inference%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-based%2520Unifying%2520Inference%2520Attack%2520on%2520Graph%2520Neural%2520Networks%26entry.906535625%3DYuecen%2520Wei%2520and%2520Xingcheng%2520Fu%2520and%2520Lingyun%2520Liu%2520and%2520Qingyun%2520Sun%2520and%2520Hao%2520Peng%2520and%2520Chunming%2520Hu%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520provide%2520important%2520prospective%2520insights%2520in%250Aapplications%2520such%2520as%2520social%2520behavior%2520analysis%2520and%2520financial%2520risk%2520analysis%2520based%250Aon%2520their%2520powerful%2520learning%2520capabilities%2520on%2520graph%2520data.%2520Nevertheless%252C%2520GNNs%2527%250Apredictive%2520performance%2520relies%2520on%2520the%2520quality%2520of%2520task-specific%2520node%2520labels%252C%2520so%250Ait%2520is%2520common%2520practice%2520to%2520improve%2520the%2520model%2527s%2520generalization%2520ability%2520in%2520the%250Adownstream%2520execution%2520of%2520decision-making%2520tasks%2520through%2520pre-training.%2520Graph%250Aprompting%2520is%2520a%2520prudent%2520choice%2520but%2520risky%2520without%2520taking%2520measures%2520to%2520prevent%2520data%250Aleakage.%2520In%2520other%2520words%252C%2520in%2520high-risk%2520decision%2520scenarios%252C%2520prompt%2520learning%2520can%250Ainfer%2520private%2520information%2520by%2520accessing%2520model%2520parameters%2520trained%2520on%2520private%2520data%250A%2528publishing%2520model%2520parameters%2520in%2520pre-training%252C%2520i.e.%252C%2520without%2520directly%2520leaking%250Athe%2520raw%2520data%252C%2520is%2520a%2520tacitly%2520accepted%2520trend%2529.%2520However%252C%2520myriad%2520graph%2520inference%250Aattacks%2520necessitate%2520tailored%2520module%2520design%2520and%2520processing%2520to%2520enhance%2520inference%250Acapabilities%2520due%2520to%2520variations%2520in%2520supervision%2520signals.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520Prompt-based%2520unifying%2520Inference%2520Attack%2520framework%2520on%2520GNNs%252C%2520named%250AProIA.%2520Specifically%252C%2520ProIA%2520retains%2520the%2520crucial%2520topological%2520information%2520of%2520the%250Agraph%2520during%2520pre-training%252C%2520enhancing%2520the%2520background%2520knowledge%2520of%2520the%2520inference%250Aattack%2520model.%2520It%2520then%2520utilizes%2520a%2520unified%2520prompt%2520and%2520introduces%2520additional%250Adisentanglement%2520factors%2520in%2520downstream%2520attacks%2520to%2520adapt%2520to%2520task-relevant%250Aknowledge.%2520Finally%252C%2520extensive%2520experiments%2520show%2520that%2520ProIA%2520enhances%2520attack%250Acapabilities%2520and%2520demonstrates%2520remarkable%2520adaptability%2520to%2520various%2520inference%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-based%20Unifying%20Inference%20Attack%20on%20Graph%20Neural%20Networks&entry.906535625=Yuecen%20Wei%20and%20Xingcheng%20Fu%20and%20Lingyun%20Liu%20and%20Qingyun%20Sun%20and%20Hao%20Peng%20and%20Chunming%20Hu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20provide%20important%20prospective%20insights%20in%0Aapplications%20such%20as%20social%20behavior%20analysis%20and%20financial%20risk%20analysis%20based%0Aon%20their%20powerful%20learning%20capabilities%20on%20graph%20data.%20Nevertheless%2C%20GNNs%27%0Apredictive%20performance%20relies%20on%20the%20quality%20of%20task-specific%20node%20labels%2C%20so%0Ait%20is%20common%20practice%20to%20improve%20the%20model%27s%20generalization%20ability%20in%20the%0Adownstream%20execution%20of%20decision-making%20tasks%20through%20pre-training.%20Graph%0Aprompting%20is%20a%20prudent%20choice%20but%20risky%20without%20taking%20measures%20to%20prevent%20data%0Aleakage.%20In%20other%20words%2C%20in%20high-risk%20decision%20scenarios%2C%20prompt%20learning%20can%0Ainfer%20private%20information%20by%20accessing%20model%20parameters%20trained%20on%20private%20data%0A%28publishing%20model%20parameters%20in%20pre-training%2C%20i.e.%2C%20without%20directly%20leaking%0Athe%20raw%20data%2C%20is%20a%20tacitly%20accepted%20trend%29.%20However%2C%20myriad%20graph%20inference%0Aattacks%20necessitate%20tailored%20module%20design%20and%20processing%20to%20enhance%20inference%0Acapabilities%20due%20to%20variations%20in%20supervision%20signals.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20Prompt-based%20unifying%20Inference%20Attack%20framework%20on%20GNNs%2C%20named%0AProIA.%20Specifically%2C%20ProIA%20retains%20the%20crucial%20topological%20information%20of%20the%0Agraph%20during%20pre-training%2C%20enhancing%20the%20background%20knowledge%20of%20the%20inference%0Aattack%20model.%20It%20then%20utilizes%20a%20unified%20prompt%20and%20introduces%20additional%0Adisentanglement%20factors%20in%20downstream%20attacks%20to%20adapt%20to%20task-relevant%0Aknowledge.%20Finally%2C%20extensive%20experiments%20show%20that%20ProIA%20enhances%20attack%0Acapabilities%20and%20demonstrates%20remarkable%20adaptability%20to%20various%20inference%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15735v1&entry.124074799=Read"},
{"title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs", "author": "Lei Lu and Zhepeng Wang and Runxue Bao and Mengbing Wang and Fangyi Li and Yawen Wu and Weiwen Jiang and Jie Xu and Yanzhi Wang and Shangqian Gao", "abstract": "  Existing pruning techniques for large language models (LLMs) targeting\ndomain-specific applications typically follow a two-stage process: pruning the\npretrained general-purpose LLMs and then fine-tuning the pruned LLMs on\nspecific domains. However, the pruning decisions, derived from the pretrained\nweights, remain unchanged during fine-tuning, even if the weights have been\nupdated. Therefore, such a combination of the pruning decisions and the\nfinetuned weights may be suboptimal, leading to non-negligible performance\ndegradation. To address these limitations, we propose ATP: All-in-One Tuning\nand Structural Pruning, a unified one-stage structural pruning and fine-tuning\napproach that dynamically identifies the current optimal substructure\nthroughout the fine-tuning phase via a trainable pruning decision generator.\nMoreover, given the limited available data for domain-specific applications,\nLow-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In\nATP, we introduce LoRA-aware forward and sparsity regularization to ensure that\nthe substructures corresponding to the learned pruning decisions can be\ndirectly removed after the ATP process. ATP outperforms the state-of-the-art\ntwo-stage pruning methods on tasks in the legal and healthcare domains. More\nspecifically, ATP recovers up to 88% and 91% performance of the dense model\nwhen pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.\n", "link": "http://arxiv.org/abs/2412.14426v2", "date": "2024-12-20", "relevancy": 2.3394, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4725}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-in-One%20Tuning%20and%20Structural%20Pruning%20for%20Domain-Specific%20LLMs&body=Title%3A%20All-in-One%20Tuning%20and%20Structural%20Pruning%20for%20Domain-Specific%20LLMs%0AAuthor%3A%20Lei%20Lu%20and%20Zhepeng%20Wang%20and%20Runxue%20Bao%20and%20Mengbing%20Wang%20and%20Fangyi%20Li%20and%20Yawen%20Wu%20and%20Weiwen%20Jiang%20and%20Jie%20Xu%20and%20Yanzhi%20Wang%20and%20Shangqian%20Gao%0AAbstract%3A%20%20%20Existing%20pruning%20techniques%20for%20large%20language%20models%20%28LLMs%29%20targeting%0Adomain-specific%20applications%20typically%20follow%20a%20two-stage%20process%3A%20pruning%20the%0Apretrained%20general-purpose%20LLMs%20and%20then%20fine-tuning%20the%20pruned%20LLMs%20on%0Aspecific%20domains.%20However%2C%20the%20pruning%20decisions%2C%20derived%20from%20the%20pretrained%0Aweights%2C%20remain%20unchanged%20during%20fine-tuning%2C%20even%20if%20the%20weights%20have%20been%0Aupdated.%20Therefore%2C%20such%20a%20combination%20of%20the%20pruning%20decisions%20and%20the%0Afinetuned%20weights%20may%20be%20suboptimal%2C%20leading%20to%20non-negligible%20performance%0Adegradation.%20To%20address%20these%20limitations%2C%20we%20propose%20ATP%3A%20All-in-One%20Tuning%0Aand%20Structural%20Pruning%2C%20a%20unified%20one-stage%20structural%20pruning%20and%20fine-tuning%0Aapproach%20that%20dynamically%20identifies%20the%20current%20optimal%20substructure%0Athroughout%20the%20fine-tuning%20phase%20via%20a%20trainable%20pruning%20decision%20generator.%0AMoreover%2C%20given%20the%20limited%20available%20data%20for%20domain-specific%20applications%2C%0ALow-Rank%20Adaptation%20%28LoRA%29%20becomes%20a%20common%20technique%20to%20fine-tune%20the%20LLMs.%20In%0AATP%2C%20we%20introduce%20LoRA-aware%20forward%20and%20sparsity%20regularization%20to%20ensure%20that%0Athe%20substructures%20corresponding%20to%20the%20learned%20pruning%20decisions%20can%20be%0Adirectly%20removed%20after%20the%20ATP%20process.%20ATP%20outperforms%20the%20state-of-the-art%0Atwo-stage%20pruning%20methods%20on%20tasks%20in%20the%20legal%20and%20healthcare%20domains.%20More%0Aspecifically%2C%20ATP%20recovers%20up%20to%2088%25%20and%2091%25%20performance%20of%20the%20dense%20model%0Awhen%20pruning%2040%25%20parameters%20of%20LLaMA2-7B%20and%20LLaMA3-8B%20models%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-in-One%2520Tuning%2520and%2520Structural%2520Pruning%2520for%2520Domain-Specific%2520LLMs%26entry.906535625%3DLei%2520Lu%2520and%2520Zhepeng%2520Wang%2520and%2520Runxue%2520Bao%2520and%2520Mengbing%2520Wang%2520and%2520Fangyi%2520Li%2520and%2520Yawen%2520Wu%2520and%2520Weiwen%2520Jiang%2520and%2520Jie%2520Xu%2520and%2520Yanzhi%2520Wang%2520and%2520Shangqian%2520Gao%26entry.1292438233%3D%2520%2520Existing%2520pruning%2520techniques%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520targeting%250Adomain-specific%2520applications%2520typically%2520follow%2520a%2520two-stage%2520process%253A%2520pruning%2520the%250Apretrained%2520general-purpose%2520LLMs%2520and%2520then%2520fine-tuning%2520the%2520pruned%2520LLMs%2520on%250Aspecific%2520domains.%2520However%252C%2520the%2520pruning%2520decisions%252C%2520derived%2520from%2520the%2520pretrained%250Aweights%252C%2520remain%2520unchanged%2520during%2520fine-tuning%252C%2520even%2520if%2520the%2520weights%2520have%2520been%250Aupdated.%2520Therefore%252C%2520such%2520a%2520combination%2520of%2520the%2520pruning%2520decisions%2520and%2520the%250Afinetuned%2520weights%2520may%2520be%2520suboptimal%252C%2520leading%2520to%2520non-negligible%2520performance%250Adegradation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ATP%253A%2520All-in-One%2520Tuning%250Aand%2520Structural%2520Pruning%252C%2520a%2520unified%2520one-stage%2520structural%2520pruning%2520and%2520fine-tuning%250Aapproach%2520that%2520dynamically%2520identifies%2520the%2520current%2520optimal%2520substructure%250Athroughout%2520the%2520fine-tuning%2520phase%2520via%2520a%2520trainable%2520pruning%2520decision%2520generator.%250AMoreover%252C%2520given%2520the%2520limited%2520available%2520data%2520for%2520domain-specific%2520applications%252C%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520becomes%2520a%2520common%2520technique%2520to%2520fine-tune%2520the%2520LLMs.%2520In%250AATP%252C%2520we%2520introduce%2520LoRA-aware%2520forward%2520and%2520sparsity%2520regularization%2520to%2520ensure%2520that%250Athe%2520substructures%2520corresponding%2520to%2520the%2520learned%2520pruning%2520decisions%2520can%2520be%250Adirectly%2520removed%2520after%2520the%2520ATP%2520process.%2520ATP%2520outperforms%2520the%2520state-of-the-art%250Atwo-stage%2520pruning%2520methods%2520on%2520tasks%2520in%2520the%2520legal%2520and%2520healthcare%2520domains.%2520More%250Aspecifically%252C%2520ATP%2520recovers%2520up%2520to%252088%2525%2520and%252091%2525%2520performance%2520of%2520the%2520dense%2520model%250Awhen%2520pruning%252040%2525%2520parameters%2520of%2520LLaMA2-7B%2520and%2520LLaMA3-8B%2520models%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-in-One%20Tuning%20and%20Structural%20Pruning%20for%20Domain-Specific%20LLMs&entry.906535625=Lei%20Lu%20and%20Zhepeng%20Wang%20and%20Runxue%20Bao%20and%20Mengbing%20Wang%20and%20Fangyi%20Li%20and%20Yawen%20Wu%20and%20Weiwen%20Jiang%20and%20Jie%20Xu%20and%20Yanzhi%20Wang%20and%20Shangqian%20Gao&entry.1292438233=%20%20Existing%20pruning%20techniques%20for%20large%20language%20models%20%28LLMs%29%20targeting%0Adomain-specific%20applications%20typically%20follow%20a%20two-stage%20process%3A%20pruning%20the%0Apretrained%20general-purpose%20LLMs%20and%20then%20fine-tuning%20the%20pruned%20LLMs%20on%0Aspecific%20domains.%20However%2C%20the%20pruning%20decisions%2C%20derived%20from%20the%20pretrained%0Aweights%2C%20remain%20unchanged%20during%20fine-tuning%2C%20even%20if%20the%20weights%20have%20been%0Aupdated.%20Therefore%2C%20such%20a%20combination%20of%20the%20pruning%20decisions%20and%20the%0Afinetuned%20weights%20may%20be%20suboptimal%2C%20leading%20to%20non-negligible%20performance%0Adegradation.%20To%20address%20these%20limitations%2C%20we%20propose%20ATP%3A%20All-in-One%20Tuning%0Aand%20Structural%20Pruning%2C%20a%20unified%20one-stage%20structural%20pruning%20and%20fine-tuning%0Aapproach%20that%20dynamically%20identifies%20the%20current%20optimal%20substructure%0Athroughout%20the%20fine-tuning%20phase%20via%20a%20trainable%20pruning%20decision%20generator.%0AMoreover%2C%20given%20the%20limited%20available%20data%20for%20domain-specific%20applications%2C%0ALow-Rank%20Adaptation%20%28LoRA%29%20becomes%20a%20common%20technique%20to%20fine-tune%20the%20LLMs.%20In%0AATP%2C%20we%20introduce%20LoRA-aware%20forward%20and%20sparsity%20regularization%20to%20ensure%20that%0Athe%20substructures%20corresponding%20to%20the%20learned%20pruning%20decisions%20can%20be%0Adirectly%20removed%20after%20the%20ATP%20process.%20ATP%20outperforms%20the%20state-of-the-art%0Atwo-stage%20pruning%20methods%20on%20tasks%20in%20the%20legal%20and%20healthcare%20domains.%20More%0Aspecifically%2C%20ATP%20recovers%20up%20to%2088%25%20and%2091%25%20performance%20of%20the%20dense%20model%0Awhen%20pruning%2040%25%20parameters%20of%20LLaMA2-7B%20and%20LLaMA3-8B%20models%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14426v2&entry.124074799=Read"},
{"title": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning", "author": "Heming Zhang and Di Huang and Yixin Chen and Fuhai Li", "abstract": "  The integration of multi-omic data is pivotal for understanding complex\ndiseases, but its high dimensionality and noise present significant challenges.\nGraph Neural Networks (GNNs) offer a robust framework for analyzing large-scale\nsignaling pathways and protein-protein interaction networks, yet they face\nlimitations in expressivity when capturing intricate biological relationships.\nTo address this, we propose Graph Sequence Language Model (GraphSeqLM), a\nframework that enhances GNNs with biological sequence embeddings generated by\nLarge Language Models (LLMs). These embeddings encode structural and biological\nproperties of DNA, RNA, and proteins, augmenting GNNs with enriched features\nfor analyzing sample-specific multi-omic data. By integrating topological,\nsequence-derived, and biological information, GraphSeqLM demonstrates superior\npredictive accuracy and outperforms existing methods, paving the way for more\neffective multi-omic data integration in precision medicine.\n", "link": "http://arxiv.org/abs/2412.15790v1", "date": "2024-12-20", "relevancy": 2.2906, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphSeqLM%3A%20A%20Unified%20Graph%20Language%20Framework%20for%20Omic%20Graph%20Learning&body=Title%3A%20GraphSeqLM%3A%20A%20Unified%20Graph%20Language%20Framework%20for%20Omic%20Graph%20Learning%0AAuthor%3A%20Heming%20Zhang%20and%20Di%20Huang%20and%20Yixin%20Chen%20and%20Fuhai%20Li%0AAbstract%3A%20%20%20The%20integration%20of%20multi-omic%20data%20is%20pivotal%20for%20understanding%20complex%0Adiseases%2C%20but%20its%20high%20dimensionality%20and%20noise%20present%20significant%20challenges.%0AGraph%20Neural%20Networks%20%28GNNs%29%20offer%20a%20robust%20framework%20for%20analyzing%20large-scale%0Asignaling%20pathways%20and%20protein-protein%20interaction%20networks%2C%20yet%20they%20face%0Alimitations%20in%20expressivity%20when%20capturing%20intricate%20biological%20relationships.%0ATo%20address%20this%2C%20we%20propose%20Graph%20Sequence%20Language%20Model%20%28GraphSeqLM%29%2C%20a%0Aframework%20that%20enhances%20GNNs%20with%20biological%20sequence%20embeddings%20generated%20by%0ALarge%20Language%20Models%20%28LLMs%29.%20These%20embeddings%20encode%20structural%20and%20biological%0Aproperties%20of%20DNA%2C%20RNA%2C%20and%20proteins%2C%20augmenting%20GNNs%20with%20enriched%20features%0Afor%20analyzing%20sample-specific%20multi-omic%20data.%20By%20integrating%20topological%2C%0Asequence-derived%2C%20and%20biological%20information%2C%20GraphSeqLM%20demonstrates%20superior%0Apredictive%20accuracy%20and%20outperforms%20existing%20methods%2C%20paving%20the%20way%20for%20more%0Aeffective%20multi-omic%20data%20integration%20in%20precision%20medicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphSeqLM%253A%2520A%2520Unified%2520Graph%2520Language%2520Framework%2520for%2520Omic%2520Graph%2520Learning%26entry.906535625%3DHeming%2520Zhang%2520and%2520Di%2520Huang%2520and%2520Yixin%2520Chen%2520and%2520Fuhai%2520Li%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520multi-omic%2520data%2520is%2520pivotal%2520for%2520understanding%2520complex%250Adiseases%252C%2520but%2520its%2520high%2520dimensionality%2520and%2520noise%2520present%2520significant%2520challenges.%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520offer%2520a%2520robust%2520framework%2520for%2520analyzing%2520large-scale%250Asignaling%2520pathways%2520and%2520protein-protein%2520interaction%2520networks%252C%2520yet%2520they%2520face%250Alimitations%2520in%2520expressivity%2520when%2520capturing%2520intricate%2520biological%2520relationships.%250ATo%2520address%2520this%252C%2520we%2520propose%2520Graph%2520Sequence%2520Language%2520Model%2520%2528GraphSeqLM%2529%252C%2520a%250Aframework%2520that%2520enhances%2520GNNs%2520with%2520biological%2520sequence%2520embeddings%2520generated%2520by%250ALarge%2520Language%2520Models%2520%2528LLMs%2529.%2520These%2520embeddings%2520encode%2520structural%2520and%2520biological%250Aproperties%2520of%2520DNA%252C%2520RNA%252C%2520and%2520proteins%252C%2520augmenting%2520GNNs%2520with%2520enriched%2520features%250Afor%2520analyzing%2520sample-specific%2520multi-omic%2520data.%2520By%2520integrating%2520topological%252C%250Asequence-derived%252C%2520and%2520biological%2520information%252C%2520GraphSeqLM%2520demonstrates%2520superior%250Apredictive%2520accuracy%2520and%2520outperforms%2520existing%2520methods%252C%2520paving%2520the%2520way%2520for%2520more%250Aeffective%2520multi-omic%2520data%2520integration%2520in%2520precision%2520medicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphSeqLM%3A%20A%20Unified%20Graph%20Language%20Framework%20for%20Omic%20Graph%20Learning&entry.906535625=Heming%20Zhang%20and%20Di%20Huang%20and%20Yixin%20Chen%20and%20Fuhai%20Li&entry.1292438233=%20%20The%20integration%20of%20multi-omic%20data%20is%20pivotal%20for%20understanding%20complex%0Adiseases%2C%20but%20its%20high%20dimensionality%20and%20noise%20present%20significant%20challenges.%0AGraph%20Neural%20Networks%20%28GNNs%29%20offer%20a%20robust%20framework%20for%20analyzing%20large-scale%0Asignaling%20pathways%20and%20protein-protein%20interaction%20networks%2C%20yet%20they%20face%0Alimitations%20in%20expressivity%20when%20capturing%20intricate%20biological%20relationships.%0ATo%20address%20this%2C%20we%20propose%20Graph%20Sequence%20Language%20Model%20%28GraphSeqLM%29%2C%20a%0Aframework%20that%20enhances%20GNNs%20with%20biological%20sequence%20embeddings%20generated%20by%0ALarge%20Language%20Models%20%28LLMs%29.%20These%20embeddings%20encode%20structural%20and%20biological%0Aproperties%20of%20DNA%2C%20RNA%2C%20and%20proteins%2C%20augmenting%20GNNs%20with%20enriched%20features%0Afor%20analyzing%20sample-specific%20multi-omic%20data.%20By%20integrating%20topological%2C%0Asequence-derived%2C%20and%20biological%20information%2C%20GraphSeqLM%20demonstrates%20superior%0Apredictive%20accuracy%20and%20outperforms%20existing%20methods%2C%20paving%20the%20way%20for%20more%0Aeffective%20multi-omic%20data%20integration%20in%20precision%20medicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15790v1&entry.124074799=Read"},
{"title": "Sims: An Interactive Tool for Geospatial Matching and Clustering", "author": "Akram Zaytar and Girmaw Abebe Tadesse and Caleb Robinson and Eduardo G. Bendito and Medha Devare and Meklit Chernet and Gilles Q. Hacheme and Rahul Dodhia and Juan M. Lavista Ferres", "abstract": "  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to perform\nclustering and similarity search over defined regions of interest using Google\nEarth Engine as a backend. Sims is designed to complement existing modeling\ntools by focusing on feature exploration rather than model creation. We\ndemonstrate the utility of Sims through a case study analyzing simulated maize\nyield data in Rwanda, where we evaluate how different combinations of soil,\nweather, and agronomic features affect the clustering of yield response zones.\nSims is open source and available at https://github.com/microsoft/Sims\n", "link": "http://arxiv.org/abs/2412.10184v2", "date": "2024-12-20", "relevancy": 2.29, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4637}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering&body=Title%3A%20Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering%0AAuthor%3A%20Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Eduardo%20G.%20Bendito%20and%20Medha%20Devare%20and%20Meklit%20Chernet%20and%20Gilles%20Q.%20Hacheme%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%0AAbstract%3A%20%20%20Acquiring%2C%20processing%2C%20and%20visualizing%20geospatial%20data%20requires%20significant%0Acomputing%20resources%2C%20especially%20for%20large%20spatio-temporal%20domains.%20This%0Achallenge%20hinders%20the%20rapid%20discovery%20of%20predictive%20features%2C%20which%20is%0Aessential%20for%20advancing%20geospatial%20modeling.%20To%20address%20this%2C%20we%20developed%0ASimilarity%20Search%20%28Sims%29%2C%20a%20no-code%20web%20tool%20that%20allows%20users%20to%20perform%0Aclustering%20and%20similarity%20search%20over%20defined%20regions%20of%20interest%20using%20Google%0AEarth%20Engine%20as%20a%20backend.%20Sims%20is%20designed%20to%20complement%20existing%20modeling%0Atools%20by%20focusing%20on%20feature%20exploration%20rather%20than%20model%20creation.%20We%0Ademonstrate%20the%20utility%20of%20Sims%20through%20a%20case%20study%20analyzing%20simulated%20maize%0Ayield%20data%20in%20Rwanda%2C%20where%20we%20evaluate%20how%20different%20combinations%20of%20soil%2C%0Aweather%2C%20and%20agronomic%20features%20affect%20the%20clustering%20of%20yield%20response%20zones.%0ASims%20is%20open%20source%20and%20available%20at%20https%3A//github.com/microsoft/Sims%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSims%253A%2520An%2520Interactive%2520Tool%2520for%2520Geospatial%2520Matching%2520and%2520Clustering%26entry.906535625%3DAkram%2520Zaytar%2520and%2520Girmaw%2520Abebe%2520Tadesse%2520and%2520Caleb%2520Robinson%2520and%2520Eduardo%2520G.%2520Bendito%2520and%2520Medha%2520Devare%2520and%2520Meklit%2520Chernet%2520and%2520Gilles%2520Q.%2520Hacheme%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%26entry.1292438233%3D%2520%2520Acquiring%252C%2520processing%252C%2520and%2520visualizing%2520geospatial%2520data%2520requires%2520significant%250Acomputing%2520resources%252C%2520especially%2520for%2520large%2520spatio-temporal%2520domains.%2520This%250Achallenge%2520hinders%2520the%2520rapid%2520discovery%2520of%2520predictive%2520features%252C%2520which%2520is%250Aessential%2520for%2520advancing%2520geospatial%2520modeling.%2520To%2520address%2520this%252C%2520we%2520developed%250ASimilarity%2520Search%2520%2528Sims%2529%252C%2520a%2520no-code%2520web%2520tool%2520that%2520allows%2520users%2520to%2520perform%250Aclustering%2520and%2520similarity%2520search%2520over%2520defined%2520regions%2520of%2520interest%2520using%2520Google%250AEarth%2520Engine%2520as%2520a%2520backend.%2520Sims%2520is%2520designed%2520to%2520complement%2520existing%2520modeling%250Atools%2520by%2520focusing%2520on%2520feature%2520exploration%2520rather%2520than%2520model%2520creation.%2520We%250Ademonstrate%2520the%2520utility%2520of%2520Sims%2520through%2520a%2520case%2520study%2520analyzing%2520simulated%2520maize%250Ayield%2520data%2520in%2520Rwanda%252C%2520where%2520we%2520evaluate%2520how%2520different%2520combinations%2520of%2520soil%252C%250Aweather%252C%2520and%2520agronomic%2520features%2520affect%2520the%2520clustering%2520of%2520yield%2520response%2520zones.%250ASims%2520is%2520open%2520source%2520and%2520available%2520at%2520https%253A//github.com/microsoft/Sims%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sims%3A%20An%20Interactive%20Tool%20for%20Geospatial%20Matching%20and%20Clustering&entry.906535625=Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Eduardo%20G.%20Bendito%20and%20Medha%20Devare%20and%20Meklit%20Chernet%20and%20Gilles%20Q.%20Hacheme%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres&entry.1292438233=%20%20Acquiring%2C%20processing%2C%20and%20visualizing%20geospatial%20data%20requires%20significant%0Acomputing%20resources%2C%20especially%20for%20large%20spatio-temporal%20domains.%20This%0Achallenge%20hinders%20the%20rapid%20discovery%20of%20predictive%20features%2C%20which%20is%0Aessential%20for%20advancing%20geospatial%20modeling.%20To%20address%20this%2C%20we%20developed%0ASimilarity%20Search%20%28Sims%29%2C%20a%20no-code%20web%20tool%20that%20allows%20users%20to%20perform%0Aclustering%20and%20similarity%20search%20over%20defined%20regions%20of%20interest%20using%20Google%0AEarth%20Engine%20as%20a%20backend.%20Sims%20is%20designed%20to%20complement%20existing%20modeling%0Atools%20by%20focusing%20on%20feature%20exploration%20rather%20than%20model%20creation.%20We%0Ademonstrate%20the%20utility%20of%20Sims%20through%20a%20case%20study%20analyzing%20simulated%20maize%0Ayield%20data%20in%20Rwanda%2C%20where%20we%20evaluate%20how%20different%20combinations%20of%20soil%2C%0Aweather%2C%20and%20agronomic%20features%20affect%20the%20clustering%20of%20yield%20response%20zones.%0ASims%20is%20open%20source%20and%20available%20at%20https%3A//github.com/microsoft/Sims%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10184v2&entry.124074799=Read"},
{"title": "Multi-dimensional Visual Prompt Enhanced Image Restoration via\n  Mamba-Transformer Aggregation", "author": "Aiwen Jiang and Hourong Chen and Zhiwen Chen and Jihua Ye and Mingwen Wang", "abstract": "  Recent efforts on image restoration have focused on developing \"all-in-one\"\nmodels that can handle different degradation types and levels within single\nmodel. However, most of mainstream Transformer-based ones confronted with\ndilemma between model capabilities and computation burdens, since\nself-attention mechanism quadratically increase in computational complexity\nwith respect to image size, and has inadequacies in capturing long-range\ndependencies. Most of Mamba-related ones solely scanned feature map in spatial\ndimension for global modeling, failing to fully utilize information in channel\ndimension. To address aforementioned problems, this paper has proposed to fully\nutilize complementary advantages from Mamba and Transformer without sacrificing\ncomputation efficiency. Specifically, the selective scanning mechanism of Mamba\nis employed to focus on spatial modeling, enabling capture long-range spatial\ndependencies under linear complexity. The self-attention mechanism of\nTransformer is applied to focus on channel modeling, avoiding high computation\nburdens that are in quadratic growth with image's spatial dimensions. Moreover,\nto enrich informative prompts for effective image restoration,\nmulti-dimensional prompt learning modules are proposed to learn prompt-flows\nfrom multi-scale encoder/decoder layers, benefiting for revealing underlying\ncharacteristic of various degradations from both spatial and channel\nperspectives, therefore, enhancing the capabilities of \"all-in-one\" model to\nsolve various restoration tasks. Extensive experiment results on several image\nrestoration benchmark tasks such as image denoising, dehazing, and deraining,\nhave demonstrated that the proposed method can achieve new state-of-the-art\nperformance, compared with many popular mainstream methods. Related source\ncodes and pre-trained parameters will be public on github\nhttps://github.com/12138-chr/MTAIR.\n", "link": "http://arxiv.org/abs/2412.15845v1", "date": "2024-12-20", "relevancy": 2.2809, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5804}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-dimensional%20Visual%20Prompt%20Enhanced%20Image%20Restoration%20via%0A%20%20Mamba-Transformer%20Aggregation&body=Title%3A%20Multi-dimensional%20Visual%20Prompt%20Enhanced%20Image%20Restoration%20via%0A%20%20Mamba-Transformer%20Aggregation%0AAuthor%3A%20Aiwen%20Jiang%20and%20Hourong%20Chen%20and%20Zhiwen%20Chen%20and%20Jihua%20Ye%20and%20Mingwen%20Wang%0AAbstract%3A%20%20%20Recent%20efforts%20on%20image%20restoration%20have%20focused%20on%20developing%20%22all-in-one%22%0Amodels%20that%20can%20handle%20different%20degradation%20types%20and%20levels%20within%20single%0Amodel.%20However%2C%20most%20of%20mainstream%20Transformer-based%20ones%20confronted%20with%0Adilemma%20between%20model%20capabilities%20and%20computation%20burdens%2C%20since%0Aself-attention%20mechanism%20quadratically%20increase%20in%20computational%20complexity%0Awith%20respect%20to%20image%20size%2C%20and%20has%20inadequacies%20in%20capturing%20long-range%0Adependencies.%20Most%20of%20Mamba-related%20ones%20solely%20scanned%20feature%20map%20in%20spatial%0Adimension%20for%20global%20modeling%2C%20failing%20to%20fully%20utilize%20information%20in%20channel%0Adimension.%20To%20address%20aforementioned%20problems%2C%20this%20paper%20has%20proposed%20to%20fully%0Autilize%20complementary%20advantages%20from%20Mamba%20and%20Transformer%20without%20sacrificing%0Acomputation%20efficiency.%20Specifically%2C%20the%20selective%20scanning%20mechanism%20of%20Mamba%0Ais%20employed%20to%20focus%20on%20spatial%20modeling%2C%20enabling%20capture%20long-range%20spatial%0Adependencies%20under%20linear%20complexity.%20The%20self-attention%20mechanism%20of%0ATransformer%20is%20applied%20to%20focus%20on%20channel%20modeling%2C%20avoiding%20high%20computation%0Aburdens%20that%20are%20in%20quadratic%20growth%20with%20image%27s%20spatial%20dimensions.%20Moreover%2C%0Ato%20enrich%20informative%20prompts%20for%20effective%20image%20restoration%2C%0Amulti-dimensional%20prompt%20learning%20modules%20are%20proposed%20to%20learn%20prompt-flows%0Afrom%20multi-scale%20encoder/decoder%20layers%2C%20benefiting%20for%20revealing%20underlying%0Acharacteristic%20of%20various%20degradations%20from%20both%20spatial%20and%20channel%0Aperspectives%2C%20therefore%2C%20enhancing%20the%20capabilities%20of%20%22all-in-one%22%20model%20to%0Asolve%20various%20restoration%20tasks.%20Extensive%20experiment%20results%20on%20several%20image%0Arestoration%20benchmark%20tasks%20such%20as%20image%20denoising%2C%20dehazing%2C%20and%20deraining%2C%0Ahave%20demonstrated%20that%20the%20proposed%20method%20can%20achieve%20new%20state-of-the-art%0Aperformance%2C%20compared%20with%20many%20popular%20mainstream%20methods.%20Related%20source%0Acodes%20and%20pre-trained%20parameters%20will%20be%20public%20on%20github%0Ahttps%3A//github.com/12138-chr/MTAIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-dimensional%2520Visual%2520Prompt%2520Enhanced%2520Image%2520Restoration%2520via%250A%2520%2520Mamba-Transformer%2520Aggregation%26entry.906535625%3DAiwen%2520Jiang%2520and%2520Hourong%2520Chen%2520and%2520Zhiwen%2520Chen%2520and%2520Jihua%2520Ye%2520and%2520Mingwen%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520efforts%2520on%2520image%2520restoration%2520have%2520focused%2520on%2520developing%2520%2522all-in-one%2522%250Amodels%2520that%2520can%2520handle%2520different%2520degradation%2520types%2520and%2520levels%2520within%2520single%250Amodel.%2520However%252C%2520most%2520of%2520mainstream%2520Transformer-based%2520ones%2520confronted%2520with%250Adilemma%2520between%2520model%2520capabilities%2520and%2520computation%2520burdens%252C%2520since%250Aself-attention%2520mechanism%2520quadratically%2520increase%2520in%2520computational%2520complexity%250Awith%2520respect%2520to%2520image%2520size%252C%2520and%2520has%2520inadequacies%2520in%2520capturing%2520long-range%250Adependencies.%2520Most%2520of%2520Mamba-related%2520ones%2520solely%2520scanned%2520feature%2520map%2520in%2520spatial%250Adimension%2520for%2520global%2520modeling%252C%2520failing%2520to%2520fully%2520utilize%2520information%2520in%2520channel%250Adimension.%2520To%2520address%2520aforementioned%2520problems%252C%2520this%2520paper%2520has%2520proposed%2520to%2520fully%250Autilize%2520complementary%2520advantages%2520from%2520Mamba%2520and%2520Transformer%2520without%2520sacrificing%250Acomputation%2520efficiency.%2520Specifically%252C%2520the%2520selective%2520scanning%2520mechanism%2520of%2520Mamba%250Ais%2520employed%2520to%2520focus%2520on%2520spatial%2520modeling%252C%2520enabling%2520capture%2520long-range%2520spatial%250Adependencies%2520under%2520linear%2520complexity.%2520The%2520self-attention%2520mechanism%2520of%250ATransformer%2520is%2520applied%2520to%2520focus%2520on%2520channel%2520modeling%252C%2520avoiding%2520high%2520computation%250Aburdens%2520that%2520are%2520in%2520quadratic%2520growth%2520with%2520image%2527s%2520spatial%2520dimensions.%2520Moreover%252C%250Ato%2520enrich%2520informative%2520prompts%2520for%2520effective%2520image%2520restoration%252C%250Amulti-dimensional%2520prompt%2520learning%2520modules%2520are%2520proposed%2520to%2520learn%2520prompt-flows%250Afrom%2520multi-scale%2520encoder/decoder%2520layers%252C%2520benefiting%2520for%2520revealing%2520underlying%250Acharacteristic%2520of%2520various%2520degradations%2520from%2520both%2520spatial%2520and%2520channel%250Aperspectives%252C%2520therefore%252C%2520enhancing%2520the%2520capabilities%2520of%2520%2522all-in-one%2522%2520model%2520to%250Asolve%2520various%2520restoration%2520tasks.%2520Extensive%2520experiment%2520results%2520on%2520several%2520image%250Arestoration%2520benchmark%2520tasks%2520such%2520as%2520image%2520denoising%252C%2520dehazing%252C%2520and%2520deraining%252C%250Ahave%2520demonstrated%2520that%2520the%2520proposed%2520method%2520can%2520achieve%2520new%2520state-of-the-art%250Aperformance%252C%2520compared%2520with%2520many%2520popular%2520mainstream%2520methods.%2520Related%2520source%250Acodes%2520and%2520pre-trained%2520parameters%2520will%2520be%2520public%2520on%2520github%250Ahttps%253A//github.com/12138-chr/MTAIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-dimensional%20Visual%20Prompt%20Enhanced%20Image%20Restoration%20via%0A%20%20Mamba-Transformer%20Aggregation&entry.906535625=Aiwen%20Jiang%20and%20Hourong%20Chen%20and%20Zhiwen%20Chen%20and%20Jihua%20Ye%20and%20Mingwen%20Wang&entry.1292438233=%20%20Recent%20efforts%20on%20image%20restoration%20have%20focused%20on%20developing%20%22all-in-one%22%0Amodels%20that%20can%20handle%20different%20degradation%20types%20and%20levels%20within%20single%0Amodel.%20However%2C%20most%20of%20mainstream%20Transformer-based%20ones%20confronted%20with%0Adilemma%20between%20model%20capabilities%20and%20computation%20burdens%2C%20since%0Aself-attention%20mechanism%20quadratically%20increase%20in%20computational%20complexity%0Awith%20respect%20to%20image%20size%2C%20and%20has%20inadequacies%20in%20capturing%20long-range%0Adependencies.%20Most%20of%20Mamba-related%20ones%20solely%20scanned%20feature%20map%20in%20spatial%0Adimension%20for%20global%20modeling%2C%20failing%20to%20fully%20utilize%20information%20in%20channel%0Adimension.%20To%20address%20aforementioned%20problems%2C%20this%20paper%20has%20proposed%20to%20fully%0Autilize%20complementary%20advantages%20from%20Mamba%20and%20Transformer%20without%20sacrificing%0Acomputation%20efficiency.%20Specifically%2C%20the%20selective%20scanning%20mechanism%20of%20Mamba%0Ais%20employed%20to%20focus%20on%20spatial%20modeling%2C%20enabling%20capture%20long-range%20spatial%0Adependencies%20under%20linear%20complexity.%20The%20self-attention%20mechanism%20of%0ATransformer%20is%20applied%20to%20focus%20on%20channel%20modeling%2C%20avoiding%20high%20computation%0Aburdens%20that%20are%20in%20quadratic%20growth%20with%20image%27s%20spatial%20dimensions.%20Moreover%2C%0Ato%20enrich%20informative%20prompts%20for%20effective%20image%20restoration%2C%0Amulti-dimensional%20prompt%20learning%20modules%20are%20proposed%20to%20learn%20prompt-flows%0Afrom%20multi-scale%20encoder/decoder%20layers%2C%20benefiting%20for%20revealing%20underlying%0Acharacteristic%20of%20various%20degradations%20from%20both%20spatial%20and%20channel%0Aperspectives%2C%20therefore%2C%20enhancing%20the%20capabilities%20of%20%22all-in-one%22%20model%20to%0Asolve%20various%20restoration%20tasks.%20Extensive%20experiment%20results%20on%20several%20image%0Arestoration%20benchmark%20tasks%20such%20as%20image%20denoising%2C%20dehazing%2C%20and%20deraining%2C%0Ahave%20demonstrated%20that%20the%20proposed%20method%20can%20achieve%20new%20state-of-the-art%0Aperformance%2C%20compared%20with%20many%20popular%20mainstream%20methods.%20Related%20source%0Acodes%20and%20pre-trained%20parameters%20will%20be%20public%20on%20github%0Ahttps%3A//github.com/12138-chr/MTAIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15845v1&entry.124074799=Read"},
{"title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension", "author": "Yongdong Luo and Xiawu Zheng and Xiao Yang and Guilin Li and Haojia Lin and Jinfa Huang and Jiayi Ji and Fei Chao and Jiebo Luo and Rongrong Ji", "abstract": "  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n", "link": "http://arxiv.org/abs/2411.13093v3", "date": "2024-12-20", "relevancy": 2.2775, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5729}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension&body=Title%3A%20Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension%0AAuthor%3A%20Yongdong%20Luo%20and%20Xiawu%20Zheng%20and%20Xiao%20Yang%20and%20Guilin%20Li%20and%20Haojia%20Lin%20and%20Jinfa%20Huang%20and%20Jiayi%20Ji%20and%20Fei%20Chao%20and%20Jiebo%20Luo%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Existing%20large%20video-language%20models%20%28LVLMs%29%20struggle%20to%20comprehend%20long%0Avideos%20correctly%20due%20to%20limited%20context.%20To%20address%20this%20problem%2C%20fine-tuning%0Along-context%20LVLMs%20and%20employing%20GPT-based%20agents%20have%20emerged%20as%20promising%0Asolutions.%20However%2C%20fine-tuning%20LVLMs%20would%20require%20extensive%20high-quality%20data%0Aand%20substantial%20GPU%20resources%2C%20while%20GPT-based%20agents%20would%20rely%20on%20proprietary%0Amodels%20%28e.g.%2C%20GPT-4o%29.%20In%20this%20paper%2C%20we%20propose%20Video%20Retrieval-Augmented%0AGeneration%20%28Video-RAG%29%2C%20a%20training-free%20and%20cost-effective%20pipeline%20that%0Aemploys%20visually-aligned%20auxiliary%20texts%20to%20help%20facilitate%20cross-modality%0Aalignment%20while%20providing%20additional%20information%20beyond%20the%20visual%20content.%0ASpecifically%2C%20we%20leverage%20open-source%20external%20tools%20to%20extract%0Avisually-aligned%20information%20from%20pure%20video%20data%20%28e.g.%2C%20audio%2C%20optical%0Acharacter%2C%20and%20object%20detection%29%2C%20and%20incorporate%20the%20extracted%20information%0Ainto%20an%20existing%20LVLM%20as%20auxiliary%20texts%2C%20alongside%20video%20frames%20and%20queries%2C%0Ain%20a%20plug-and-play%20manner.%20Our%20Video-RAG%20offers%20several%20key%20advantages%3A%20%28i%29%0Alightweight%20with%20low%20computing%20overhead%20due%20to%20single-turn%20retrieval%3B%20%28ii%29%20easy%0Aimplementation%20and%20compatibility%20with%20any%20LVLM%3B%20and%20%28iii%29%20significant%2C%0Aconsistent%20performance%20gains%20across%20long%20video%20understanding%20benchmarks%2C%0Aincluding%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench.%20Notably%2C%20our%20model%20demonstrates%0Asuperior%20performance%20over%20proprietary%20models%20like%20Gemini-1.5-Pro%20and%20GPT-4o%0Awhen%20utilized%20with%20a%2072B%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13093v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-RAG%253A%2520Visually-aligned%2520Retrieval-Augmented%2520Long%2520Video%2520Comprehension%26entry.906535625%3DYongdong%2520Luo%2520and%2520Xiawu%2520Zheng%2520and%2520Xiao%2520Yang%2520and%2520Guilin%2520Li%2520and%2520Haojia%2520Lin%2520and%2520Jinfa%2520Huang%2520and%2520Jiayi%2520Ji%2520and%2520Fei%2520Chao%2520and%2520Jiebo%2520Luo%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Existing%2520large%2520video-language%2520models%2520%2528LVLMs%2529%2520struggle%2520to%2520comprehend%2520long%250Avideos%2520correctly%2520due%2520to%2520limited%2520context.%2520To%2520address%2520this%2520problem%252C%2520fine-tuning%250Along-context%2520LVLMs%2520and%2520employing%2520GPT-based%2520agents%2520have%2520emerged%2520as%2520promising%250Asolutions.%2520However%252C%2520fine-tuning%2520LVLMs%2520would%2520require%2520extensive%2520high-quality%2520data%250Aand%2520substantial%2520GPU%2520resources%252C%2520while%2520GPT-based%2520agents%2520would%2520rely%2520on%2520proprietary%250Amodels%2520%2528e.g.%252C%2520GPT-4o%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Video%2520Retrieval-Augmented%250AGeneration%2520%2528Video-RAG%2529%252C%2520a%2520training-free%2520and%2520cost-effective%2520pipeline%2520that%250Aemploys%2520visually-aligned%2520auxiliary%2520texts%2520to%2520help%2520facilitate%2520cross-modality%250Aalignment%2520while%2520providing%2520additional%2520information%2520beyond%2520the%2520visual%2520content.%250ASpecifically%252C%2520we%2520leverage%2520open-source%2520external%2520tools%2520to%2520extract%250Avisually-aligned%2520information%2520from%2520pure%2520video%2520data%2520%2528e.g.%252C%2520audio%252C%2520optical%250Acharacter%252C%2520and%2520object%2520detection%2529%252C%2520and%2520incorporate%2520the%2520extracted%2520information%250Ainto%2520an%2520existing%2520LVLM%2520as%2520auxiliary%2520texts%252C%2520alongside%2520video%2520frames%2520and%2520queries%252C%250Ain%2520a%2520plug-and-play%2520manner.%2520Our%2520Video-RAG%2520offers%2520several%2520key%2520advantages%253A%2520%2528i%2529%250Alightweight%2520with%2520low%2520computing%2520overhead%2520due%2520to%2520single-turn%2520retrieval%253B%2520%2528ii%2529%2520easy%250Aimplementation%2520and%2520compatibility%2520with%2520any%2520LVLM%253B%2520and%2520%2528iii%2529%2520significant%252C%250Aconsistent%2520performance%2520gains%2520across%2520long%2520video%2520understanding%2520benchmarks%252C%250Aincluding%2520Video-MME%252C%2520MLVU%252C%2520and%2520LongVideoBench.%2520Notably%252C%2520our%2520model%2520demonstrates%250Asuperior%2520performance%2520over%2520proprietary%2520models%2520like%2520Gemini-1.5-Pro%2520and%2520GPT-4o%250Awhen%2520utilized%2520with%2520a%252072B%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13093v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension&entry.906535625=Yongdong%20Luo%20and%20Xiawu%20Zheng%20and%20Xiao%20Yang%20and%20Guilin%20Li%20and%20Haojia%20Lin%20and%20Jinfa%20Huang%20and%20Jiayi%20Ji%20and%20Fei%20Chao%20and%20Jiebo%20Luo%20and%20Rongrong%20Ji&entry.1292438233=%20%20Existing%20large%20video-language%20models%20%28LVLMs%29%20struggle%20to%20comprehend%20long%0Avideos%20correctly%20due%20to%20limited%20context.%20To%20address%20this%20problem%2C%20fine-tuning%0Along-context%20LVLMs%20and%20employing%20GPT-based%20agents%20have%20emerged%20as%20promising%0Asolutions.%20However%2C%20fine-tuning%20LVLMs%20would%20require%20extensive%20high-quality%20data%0Aand%20substantial%20GPU%20resources%2C%20while%20GPT-based%20agents%20would%20rely%20on%20proprietary%0Amodels%20%28e.g.%2C%20GPT-4o%29.%20In%20this%20paper%2C%20we%20propose%20Video%20Retrieval-Augmented%0AGeneration%20%28Video-RAG%29%2C%20a%20training-free%20and%20cost-effective%20pipeline%20that%0Aemploys%20visually-aligned%20auxiliary%20texts%20to%20help%20facilitate%20cross-modality%0Aalignment%20while%20providing%20additional%20information%20beyond%20the%20visual%20content.%0ASpecifically%2C%20we%20leverage%20open-source%20external%20tools%20to%20extract%0Avisually-aligned%20information%20from%20pure%20video%20data%20%28e.g.%2C%20audio%2C%20optical%0Acharacter%2C%20and%20object%20detection%29%2C%20and%20incorporate%20the%20extracted%20information%0Ainto%20an%20existing%20LVLM%20as%20auxiliary%20texts%2C%20alongside%20video%20frames%20and%20queries%2C%0Ain%20a%20plug-and-play%20manner.%20Our%20Video-RAG%20offers%20several%20key%20advantages%3A%20%28i%29%0Alightweight%20with%20low%20computing%20overhead%20due%20to%20single-turn%20retrieval%3B%20%28ii%29%20easy%0Aimplementation%20and%20compatibility%20with%20any%20LVLM%3B%20and%20%28iii%29%20significant%2C%0Aconsistent%20performance%20gains%20across%20long%20video%20understanding%20benchmarks%2C%0Aincluding%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench.%20Notably%2C%20our%20model%20demonstrates%0Asuperior%20performance%20over%20proprietary%20models%20like%20Gemini-1.5-Pro%20and%20GPT-4o%0Awhen%20utilized%20with%20a%2072B%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13093v3&entry.124074799=Read"},
{"title": "Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding", "author": "Heng Guo and Jianfeng Zhang and Ke Yan and Le Lu and Minfeng Xu", "abstract": "  Automatic parsing of human anatomies at the instance-level from 3D computed\ntomography (CT) is a prerequisite step for many clinical applications. The\npresence of pathologies, broken structures or limited field-of-view (FOV) can\nall make anatomy parsing algorithms vulnerable. In this work, we explore how to\nleverage and implement the successful detection-then-segmentation paradigm for\n3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering the complicated shapes, sizes, and orientations of\nanatomies, without loss of generality, we present a nine degrees of freedom\n(9-DoF) pose estimation solution in full 3D space using a novel single-stage,\nnon-hierarchical representation. Our whole framework is executed in a steerable\nmanner where any anatomy of interest can be directly retrieved to further boost\ninference efficiency. We have validated our method on three medical imaging\nparsing tasks: ribs, spine, and abdominal organs. For rib parsing, CT scans\nhave been annotated at the rib instance-level for quantitative evaluation,\nsimilarly for spine vertebrae and abdominal organs. Extensive experiments on\n9-DoF box detection and rib instance segmentation demonstrate the high\nefficiency and effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%), compared favorably against\nseveral strong baselines (e.g., CenterNet, FCOS, and nnU-Net). For spine\nparsing and abdominal multi-organ segmentation, our method achieves competitive\nresults on par with state-of-the-art methods on the public CTSpine1K dataset\nand FLARE22 competition, respectively. Our annotations, code, and models are\navailable at: https://github.com/alibaba-damo-academy/Med_Query.\n", "link": "http://arxiv.org/abs/2212.02014v3", "date": "2024-12-20", "relevancy": 2.2662, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med-Query%3A%20Steerable%20Parsing%20of%209-DoF%20Medical%20Anatomies%20with%20Query%0A%20%20Embedding&body=Title%3A%20Med-Query%3A%20Steerable%20Parsing%20of%209-DoF%20Medical%20Anatomies%20with%20Query%0A%20%20Embedding%0AAuthor%3A%20Heng%20Guo%20and%20Jianfeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Minfeng%20Xu%0AAbstract%3A%20%20%20Automatic%20parsing%20of%20human%20anatomies%20at%20the%20instance-level%20from%203D%20computed%0Atomography%20%28CT%29%20is%20a%20prerequisite%20step%20for%20many%20clinical%20applications.%20The%0Apresence%20of%20pathologies%2C%20broken%20structures%20or%20limited%20field-of-view%20%28FOV%29%20can%0Aall%20make%20anatomy%20parsing%20algorithms%20vulnerable.%20In%20this%20work%2C%20we%20explore%20how%20to%0Aleverage%20and%20implement%20the%20successful%20detection-then-segmentation%20paradigm%20for%0A3D%20medical%20data%2C%20and%20propose%20a%20steerable%2C%20robust%2C%20and%20efficient%20computing%0Aframework%20for%20detection%2C%20identification%2C%20and%20segmentation%20of%20anatomies%20in%20CT%0Ascans.%20Considering%20the%20complicated%20shapes%2C%20sizes%2C%20and%20orientations%20of%0Aanatomies%2C%20without%20loss%20of%20generality%2C%20we%20present%20a%20nine%20degrees%20of%20freedom%0A%289-DoF%29%20pose%20estimation%20solution%20in%20full%203D%20space%20using%20a%20novel%20single-stage%2C%0Anon-hierarchical%20representation.%20Our%20whole%20framework%20is%20executed%20in%20a%20steerable%0Amanner%20where%20any%20anatomy%20of%20interest%20can%20be%20directly%20retrieved%20to%20further%20boost%0Ainference%20efficiency.%20We%20have%20validated%20our%20method%20on%20three%20medical%20imaging%0Aparsing%20tasks%3A%20ribs%2C%20spine%2C%20and%20abdominal%20organs.%20For%20rib%20parsing%2C%20CT%20scans%0Ahave%20been%20annotated%20at%20the%20rib%20instance-level%20for%20quantitative%20evaluation%2C%0Asimilarly%20for%20spine%20vertebrae%20and%20abdominal%20organs.%20Extensive%20experiments%20on%0A9-DoF%20box%20detection%20and%20rib%20instance%20segmentation%20demonstrate%20the%20high%0Aefficiency%20and%20effectiveness%20of%20our%20framework%20%28with%20the%20identification%20rate%20of%0A97.0%25%20and%20the%20segmentation%20Dice%20score%20of%2090.9%25%29%2C%20compared%20favorably%20against%0Aseveral%20strong%20baselines%20%28e.g.%2C%20CenterNet%2C%20FCOS%2C%20and%20nnU-Net%29.%20For%20spine%0Aparsing%20and%20abdominal%20multi-organ%20segmentation%2C%20our%20method%20achieves%20competitive%0Aresults%20on%20par%20with%20state-of-the-art%20methods%20on%20the%20public%20CTSpine1K%20dataset%0Aand%20FLARE22%20competition%2C%20respectively.%20Our%20annotations%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/alibaba-damo-academy/Med_Query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.02014v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed-Query%253A%2520Steerable%2520Parsing%2520of%25209-DoF%2520Medical%2520Anatomies%2520with%2520Query%250A%2520%2520Embedding%26entry.906535625%3DHeng%2520Guo%2520and%2520Jianfeng%2520Zhang%2520and%2520Ke%2520Yan%2520and%2520Le%2520Lu%2520and%2520Minfeng%2520Xu%26entry.1292438233%3D%2520%2520Automatic%2520parsing%2520of%2520human%2520anatomies%2520at%2520the%2520instance-level%2520from%25203D%2520computed%250Atomography%2520%2528CT%2529%2520is%2520a%2520prerequisite%2520step%2520for%2520many%2520clinical%2520applications.%2520The%250Apresence%2520of%2520pathologies%252C%2520broken%2520structures%2520or%2520limited%2520field-of-view%2520%2528FOV%2529%2520can%250Aall%2520make%2520anatomy%2520parsing%2520algorithms%2520vulnerable.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%2520to%250Aleverage%2520and%2520implement%2520the%2520successful%2520detection-then-segmentation%2520paradigm%2520for%250A3D%2520medical%2520data%252C%2520and%2520propose%2520a%2520steerable%252C%2520robust%252C%2520and%2520efficient%2520computing%250Aframework%2520for%2520detection%252C%2520identification%252C%2520and%2520segmentation%2520of%2520anatomies%2520in%2520CT%250Ascans.%2520Considering%2520the%2520complicated%2520shapes%252C%2520sizes%252C%2520and%2520orientations%2520of%250Aanatomies%252C%2520without%2520loss%2520of%2520generality%252C%2520we%2520present%2520a%2520nine%2520degrees%2520of%2520freedom%250A%25289-DoF%2529%2520pose%2520estimation%2520solution%2520in%2520full%25203D%2520space%2520using%2520a%2520novel%2520single-stage%252C%250Anon-hierarchical%2520representation.%2520Our%2520whole%2520framework%2520is%2520executed%2520in%2520a%2520steerable%250Amanner%2520where%2520any%2520anatomy%2520of%2520interest%2520can%2520be%2520directly%2520retrieved%2520to%2520further%2520boost%250Ainference%2520efficiency.%2520We%2520have%2520validated%2520our%2520method%2520on%2520three%2520medical%2520imaging%250Aparsing%2520tasks%253A%2520ribs%252C%2520spine%252C%2520and%2520abdominal%2520organs.%2520For%2520rib%2520parsing%252C%2520CT%2520scans%250Ahave%2520been%2520annotated%2520at%2520the%2520rib%2520instance-level%2520for%2520quantitative%2520evaluation%252C%250Asimilarly%2520for%2520spine%2520vertebrae%2520and%2520abdominal%2520organs.%2520Extensive%2520experiments%2520on%250A9-DoF%2520box%2520detection%2520and%2520rib%2520instance%2520segmentation%2520demonstrate%2520the%2520high%250Aefficiency%2520and%2520effectiveness%2520of%2520our%2520framework%2520%2528with%2520the%2520identification%2520rate%2520of%250A97.0%2525%2520and%2520the%2520segmentation%2520Dice%2520score%2520of%252090.9%2525%2529%252C%2520compared%2520favorably%2520against%250Aseveral%2520strong%2520baselines%2520%2528e.g.%252C%2520CenterNet%252C%2520FCOS%252C%2520and%2520nnU-Net%2529.%2520For%2520spine%250Aparsing%2520and%2520abdominal%2520multi-organ%2520segmentation%252C%2520our%2520method%2520achieves%2520competitive%250Aresults%2520on%2520par%2520with%2520state-of-the-art%2520methods%2520on%2520the%2520public%2520CTSpine1K%2520dataset%250Aand%2520FLARE22%2520competition%252C%2520respectively.%2520Our%2520annotations%252C%2520code%252C%2520and%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/alibaba-damo-academy/Med_Query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.02014v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med-Query%3A%20Steerable%20Parsing%20of%209-DoF%20Medical%20Anatomies%20with%20Query%0A%20%20Embedding&entry.906535625=Heng%20Guo%20and%20Jianfeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Minfeng%20Xu&entry.1292438233=%20%20Automatic%20parsing%20of%20human%20anatomies%20at%20the%20instance-level%20from%203D%20computed%0Atomography%20%28CT%29%20is%20a%20prerequisite%20step%20for%20many%20clinical%20applications.%20The%0Apresence%20of%20pathologies%2C%20broken%20structures%20or%20limited%20field-of-view%20%28FOV%29%20can%0Aall%20make%20anatomy%20parsing%20algorithms%20vulnerable.%20In%20this%20work%2C%20we%20explore%20how%20to%0Aleverage%20and%20implement%20the%20successful%20detection-then-segmentation%20paradigm%20for%0A3D%20medical%20data%2C%20and%20propose%20a%20steerable%2C%20robust%2C%20and%20efficient%20computing%0Aframework%20for%20detection%2C%20identification%2C%20and%20segmentation%20of%20anatomies%20in%20CT%0Ascans.%20Considering%20the%20complicated%20shapes%2C%20sizes%2C%20and%20orientations%20of%0Aanatomies%2C%20without%20loss%20of%20generality%2C%20we%20present%20a%20nine%20degrees%20of%20freedom%0A%289-DoF%29%20pose%20estimation%20solution%20in%20full%203D%20space%20using%20a%20novel%20single-stage%2C%0Anon-hierarchical%20representation.%20Our%20whole%20framework%20is%20executed%20in%20a%20steerable%0Amanner%20where%20any%20anatomy%20of%20interest%20can%20be%20directly%20retrieved%20to%20further%20boost%0Ainference%20efficiency.%20We%20have%20validated%20our%20method%20on%20three%20medical%20imaging%0Aparsing%20tasks%3A%20ribs%2C%20spine%2C%20and%20abdominal%20organs.%20For%20rib%20parsing%2C%20CT%20scans%0Ahave%20been%20annotated%20at%20the%20rib%20instance-level%20for%20quantitative%20evaluation%2C%0Asimilarly%20for%20spine%20vertebrae%20and%20abdominal%20organs.%20Extensive%20experiments%20on%0A9-DoF%20box%20detection%20and%20rib%20instance%20segmentation%20demonstrate%20the%20high%0Aefficiency%20and%20effectiveness%20of%20our%20framework%20%28with%20the%20identification%20rate%20of%0A97.0%25%20and%20the%20segmentation%20Dice%20score%20of%2090.9%25%29%2C%20compared%20favorably%20against%0Aseveral%20strong%20baselines%20%28e.g.%2C%20CenterNet%2C%20FCOS%2C%20and%20nnU-Net%29.%20For%20spine%0Aparsing%20and%20abdominal%20multi-organ%20segmentation%2C%20our%20method%20achieves%20competitive%0Aresults%20on%20par%20with%20state-of-the-art%20methods%20on%20the%20public%20CTSpine1K%20dataset%0Aand%20FLARE22%20competition%2C%20respectively.%20Our%20annotations%2C%20code%2C%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/alibaba-damo-academy/Med_Query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.02014v3&entry.124074799=Read"},
{"title": "Learning ECG Signal Features Without Backpropagation Using Linear Laws", "author": "P\u00e9ter P\u00f3sfay and Marcell T. Kurbucz and P\u00e9ter Kov\u00e1cs and Antal Jakov\u00e1c", "abstract": "  This paper introduces LLT-ECG, a novel method for electrocardiogram (ECG)\nsignal classification that leverages concepts from theoretical physics to\nautomatically generate features from time series data. Unlike traditional deep\nlearning approaches, LLT-ECG operates in a forward manner, eliminating the need\nfor backpropagation and hyperparameter tuning. By identifying linear laws that\ncapture shared patterns within specific classes, the proposed method constructs\na compact and verifiable representation, enhancing the effectiveness of\ndownstream classifiers. We demonstrate LLT-ECG's state-of-the-art performance\non real-world ECG datasets from PhysioNet, underscoring its potential for\nmedical applications where speed and verifiability are crucial.\n", "link": "http://arxiv.org/abs/2307.01930v2", "date": "2024-12-20", "relevancy": 2.2503, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4713}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.448}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20ECG%20Signal%20Features%20Without%20Backpropagation%20Using%20Linear%20Laws&body=Title%3A%20Learning%20ECG%20Signal%20Features%20Without%20Backpropagation%20Using%20Linear%20Laws%0AAuthor%3A%20P%C3%A9ter%20P%C3%B3sfay%20and%20Marcell%20T.%20Kurbucz%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20Antal%20Jakov%C3%A1c%0AAbstract%3A%20%20%20This%20paper%20introduces%20LLT-ECG%2C%20a%20novel%20method%20for%20electrocardiogram%20%28ECG%29%0Asignal%20classification%20that%20leverages%20concepts%20from%20theoretical%20physics%20to%0Aautomatically%20generate%20features%20from%20time%20series%20data.%20Unlike%20traditional%20deep%0Alearning%20approaches%2C%20LLT-ECG%20operates%20in%20a%20forward%20manner%2C%20eliminating%20the%20need%0Afor%20backpropagation%20and%20hyperparameter%20tuning.%20By%20identifying%20linear%20laws%20that%0Acapture%20shared%20patterns%20within%20specific%20classes%2C%20the%20proposed%20method%20constructs%0Aa%20compact%20and%20verifiable%20representation%2C%20enhancing%20the%20effectiveness%20of%0Adownstream%20classifiers.%20We%20demonstrate%20LLT-ECG%27s%20state-of-the-art%20performance%0Aon%20real-world%20ECG%20datasets%20from%20PhysioNet%2C%20underscoring%20its%20potential%20for%0Amedical%20applications%20where%20speed%20and%20verifiability%20are%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01930v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520ECG%2520Signal%2520Features%2520Without%2520Backpropagation%2520Using%2520Linear%2520Laws%26entry.906535625%3DP%25C3%25A9ter%2520P%25C3%25B3sfay%2520and%2520Marcell%2520T.%2520Kurbucz%2520and%2520P%25C3%25A9ter%2520Kov%25C3%25A1cs%2520and%2520Antal%2520Jakov%25C3%25A1c%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LLT-ECG%252C%2520a%2520novel%2520method%2520for%2520electrocardiogram%2520%2528ECG%2529%250Asignal%2520classification%2520that%2520leverages%2520concepts%2520from%2520theoretical%2520physics%2520to%250Aautomatically%2520generate%2520features%2520from%2520time%2520series%2520data.%2520Unlike%2520traditional%2520deep%250Alearning%2520approaches%252C%2520LLT-ECG%2520operates%2520in%2520a%2520forward%2520manner%252C%2520eliminating%2520the%2520need%250Afor%2520backpropagation%2520and%2520hyperparameter%2520tuning.%2520By%2520identifying%2520linear%2520laws%2520that%250Acapture%2520shared%2520patterns%2520within%2520specific%2520classes%252C%2520the%2520proposed%2520method%2520constructs%250Aa%2520compact%2520and%2520verifiable%2520representation%252C%2520enhancing%2520the%2520effectiveness%2520of%250Adownstream%2520classifiers.%2520We%2520demonstrate%2520LLT-ECG%2527s%2520state-of-the-art%2520performance%250Aon%2520real-world%2520ECG%2520datasets%2520from%2520PhysioNet%252C%2520underscoring%2520its%2520potential%2520for%250Amedical%2520applications%2520where%2520speed%2520and%2520verifiability%2520are%2520crucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01930v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20ECG%20Signal%20Features%20Without%20Backpropagation%20Using%20Linear%20Laws&entry.906535625=P%C3%A9ter%20P%C3%B3sfay%20and%20Marcell%20T.%20Kurbucz%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20Antal%20Jakov%C3%A1c&entry.1292438233=%20%20This%20paper%20introduces%20LLT-ECG%2C%20a%20novel%20method%20for%20electrocardiogram%20%28ECG%29%0Asignal%20classification%20that%20leverages%20concepts%20from%20theoretical%20physics%20to%0Aautomatically%20generate%20features%20from%20time%20series%20data.%20Unlike%20traditional%20deep%0Alearning%20approaches%2C%20LLT-ECG%20operates%20in%20a%20forward%20manner%2C%20eliminating%20the%20need%0Afor%20backpropagation%20and%20hyperparameter%20tuning.%20By%20identifying%20linear%20laws%20that%0Acapture%20shared%20patterns%20within%20specific%20classes%2C%20the%20proposed%20method%20constructs%0Aa%20compact%20and%20verifiable%20representation%2C%20enhancing%20the%20effectiveness%20of%0Adownstream%20classifiers.%20We%20demonstrate%20LLT-ECG%27s%20state-of-the-art%20performance%0Aon%20real-world%20ECG%20datasets%20from%20PhysioNet%2C%20underscoring%20its%20potential%20for%0Amedical%20applications%20where%20speed%20and%20verifiability%20are%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01930v2&entry.124074799=Read"},
{"title": "Label-Efficient Data Augmentation with Video Diffusion Models for\n  Guidewire Segmentation in Cardiac Fluoroscopy", "author": "Shaoyan Pan and Yikang Liu and Lin Zhao and Eric Z. Chen and Xiao Chen and Terrence Chen and Shanhui Sun", "abstract": "  The accurate segmentation of guidewires in interventional cardiac fluoroscopy\nvideos is crucial for computer-aided navigation tasks. Although deep learning\nmethods have demonstrated high accuracy and robustness in wire segmentation,\nthey require substantial annotated datasets for generalizability, underscoring\nthe need for extensive labeled data to enhance model performance. To address\nthis challenge, we propose the Segmentation-guided Frame-consistency Video\nDiffusion Model (SF-VD) to generate large collections of labeled fluoroscopy\nvideos, augmenting the training data for wire segmentation networks. SF-VD\nleverages videos with limited annotations by independently modeling scene\ndistribution and motion distribution. It first samples the scene distribution\nby generating 2D fluoroscopy images with wires positioned according to a\nspecified input mask, and then samples the motion distribution by progressively\ngenerating subsequent frames, ensuring frame-to-frame coherence through a\nframe-consistency strategy. A segmentation-guided mechanism further refines the\nprocess by adjusting wire contrast, ensuring a diverse range of visibility in\nthe synthesized image. Evaluation on a fluoroscopy dataset confirms the\nsuperior quality of the generated videos and shows significant improvements in\nguidewire segmentation.\n", "link": "http://arxiv.org/abs/2412.16050v1", "date": "2024-12-20", "relevancy": 2.2365, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5617}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5604}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-Efficient%20Data%20Augmentation%20with%20Video%20Diffusion%20Models%20for%0A%20%20Guidewire%20Segmentation%20in%20Cardiac%20Fluoroscopy&body=Title%3A%20Label-Efficient%20Data%20Augmentation%20with%20Video%20Diffusion%20Models%20for%0A%20%20Guidewire%20Segmentation%20in%20Cardiac%20Fluoroscopy%0AAuthor%3A%20Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20The%20accurate%20segmentation%20of%20guidewires%20in%20interventional%20cardiac%20fluoroscopy%0Avideos%20is%20crucial%20for%20computer-aided%20navigation%20tasks.%20Although%20deep%20learning%0Amethods%20have%20demonstrated%20high%20accuracy%20and%20robustness%20in%20wire%20segmentation%2C%0Athey%20require%20substantial%20annotated%20datasets%20for%20generalizability%2C%20underscoring%0Athe%20need%20for%20extensive%20labeled%20data%20to%20enhance%20model%20performance.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Segmentation-guided%20Frame-consistency%20Video%0ADiffusion%20Model%20%28SF-VD%29%20to%20generate%20large%20collections%20of%20labeled%20fluoroscopy%0Avideos%2C%20augmenting%20the%20training%20data%20for%20wire%20segmentation%20networks.%20SF-VD%0Aleverages%20videos%20with%20limited%20annotations%20by%20independently%20modeling%20scene%0Adistribution%20and%20motion%20distribution.%20It%20first%20samples%20the%20scene%20distribution%0Aby%20generating%202D%20fluoroscopy%20images%20with%20wires%20positioned%20according%20to%20a%0Aspecified%20input%20mask%2C%20and%20then%20samples%20the%20motion%20distribution%20by%20progressively%0Agenerating%20subsequent%20frames%2C%20ensuring%20frame-to-frame%20coherence%20through%20a%0Aframe-consistency%20strategy.%20A%20segmentation-guided%20mechanism%20further%20refines%20the%0Aprocess%20by%20adjusting%20wire%20contrast%2C%20ensuring%20a%20diverse%20range%20of%20visibility%20in%0Athe%20synthesized%20image.%20Evaluation%20on%20a%20fluoroscopy%20dataset%20confirms%20the%0Asuperior%20quality%20of%20the%20generated%20videos%20and%20shows%20significant%20improvements%20in%0Aguidewire%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-Efficient%2520Data%2520Augmentation%2520with%2520Video%2520Diffusion%2520Models%2520for%250A%2520%2520Guidewire%2520Segmentation%2520in%2520Cardiac%2520Fluoroscopy%26entry.906535625%3DShaoyan%2520Pan%2520and%2520Yikang%2520Liu%2520and%2520Lin%2520Zhao%2520and%2520Eric%2520Z.%2520Chen%2520and%2520Xiao%2520Chen%2520and%2520Terrence%2520Chen%2520and%2520Shanhui%2520Sun%26entry.1292438233%3D%2520%2520The%2520accurate%2520segmentation%2520of%2520guidewires%2520in%2520interventional%2520cardiac%2520fluoroscopy%250Avideos%2520is%2520crucial%2520for%2520computer-aided%2520navigation%2520tasks.%2520Although%2520deep%2520learning%250Amethods%2520have%2520demonstrated%2520high%2520accuracy%2520and%2520robustness%2520in%2520wire%2520segmentation%252C%250Athey%2520require%2520substantial%2520annotated%2520datasets%2520for%2520generalizability%252C%2520underscoring%250Athe%2520need%2520for%2520extensive%2520labeled%2520data%2520to%2520enhance%2520model%2520performance.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520the%2520Segmentation-guided%2520Frame-consistency%2520Video%250ADiffusion%2520Model%2520%2528SF-VD%2529%2520to%2520generate%2520large%2520collections%2520of%2520labeled%2520fluoroscopy%250Avideos%252C%2520augmenting%2520the%2520training%2520data%2520for%2520wire%2520segmentation%2520networks.%2520SF-VD%250Aleverages%2520videos%2520with%2520limited%2520annotations%2520by%2520independently%2520modeling%2520scene%250Adistribution%2520and%2520motion%2520distribution.%2520It%2520first%2520samples%2520the%2520scene%2520distribution%250Aby%2520generating%25202D%2520fluoroscopy%2520images%2520with%2520wires%2520positioned%2520according%2520to%2520a%250Aspecified%2520input%2520mask%252C%2520and%2520then%2520samples%2520the%2520motion%2520distribution%2520by%2520progressively%250Agenerating%2520subsequent%2520frames%252C%2520ensuring%2520frame-to-frame%2520coherence%2520through%2520a%250Aframe-consistency%2520strategy.%2520A%2520segmentation-guided%2520mechanism%2520further%2520refines%2520the%250Aprocess%2520by%2520adjusting%2520wire%2520contrast%252C%2520ensuring%2520a%2520diverse%2520range%2520of%2520visibility%2520in%250Athe%2520synthesized%2520image.%2520Evaluation%2520on%2520a%2520fluoroscopy%2520dataset%2520confirms%2520the%250Asuperior%2520quality%2520of%2520the%2520generated%2520videos%2520and%2520shows%2520significant%2520improvements%2520in%250Aguidewire%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-Efficient%20Data%20Augmentation%20with%20Video%20Diffusion%20Models%20for%0A%20%20Guidewire%20Segmentation%20in%20Cardiac%20Fluoroscopy&entry.906535625=Shaoyan%20Pan%20and%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20The%20accurate%20segmentation%20of%20guidewires%20in%20interventional%20cardiac%20fluoroscopy%0Avideos%20is%20crucial%20for%20computer-aided%20navigation%20tasks.%20Although%20deep%20learning%0Amethods%20have%20demonstrated%20high%20accuracy%20and%20robustness%20in%20wire%20segmentation%2C%0Athey%20require%20substantial%20annotated%20datasets%20for%20generalizability%2C%20underscoring%0Athe%20need%20for%20extensive%20labeled%20data%20to%20enhance%20model%20performance.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20Segmentation-guided%20Frame-consistency%20Video%0ADiffusion%20Model%20%28SF-VD%29%20to%20generate%20large%20collections%20of%20labeled%20fluoroscopy%0Avideos%2C%20augmenting%20the%20training%20data%20for%20wire%20segmentation%20networks.%20SF-VD%0Aleverages%20videos%20with%20limited%20annotations%20by%20independently%20modeling%20scene%0Adistribution%20and%20motion%20distribution.%20It%20first%20samples%20the%20scene%20distribution%0Aby%20generating%202D%20fluoroscopy%20images%20with%20wires%20positioned%20according%20to%20a%0Aspecified%20input%20mask%2C%20and%20then%20samples%20the%20motion%20distribution%20by%20progressively%0Agenerating%20subsequent%20frames%2C%20ensuring%20frame-to-frame%20coherence%20through%20a%0Aframe-consistency%20strategy.%20A%20segmentation-guided%20mechanism%20further%20refines%20the%0Aprocess%20by%20adjusting%20wire%20contrast%2C%20ensuring%20a%20diverse%20range%20of%20visibility%20in%0Athe%20synthesized%20image.%20Evaluation%20on%20a%20fluoroscopy%20dataset%20confirms%20the%0Asuperior%20quality%20of%20the%20generated%20videos%20and%20shows%20significant%20improvements%20in%0Aguidewire%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16050v1&entry.124074799=Read"},
{"title": "Single Exposure Quantitative Phase Imaging with a Conventional\n  Microscope using Diffusion Models", "author": "Gabriel della Maggiora and Luis Alberto Croquevielle and Harry Horsley and Thomas Heinis and Artur Yakimovich", "abstract": "  Phase imaging is gaining importance due to its applications in fields like\nbiomedical imaging and material characterization. In biomedical applications,\nit can provide quantitative information missing in label-free microscopy\nmodalities. One of the most prominent methods in phase quantification is the\nTransport-of-Intensity Equation (TIE). TIE often requires multiple acquisitions\nat different defocus distances, which is not always feasible in a clinical\nsetting. To address this issue, we propose to use chromatic aberrations to\ninduce the required through-focus images with a single exposure, effectively\ngenerating a through-focus stack. Since the defocus distance induced by the\naberrations is small, conventional TIE solvers are insufficient to address the\nresulting artifacts. We propose Zero-Mean Diffusion, a modified version of\ndiffusion models designed for quantitative image prediction, and train it with\nsynthetic data to ensure robust phase retrieval. Our contributions offer an\nalternative TIE approach that leverages chromatic aberrations, achieving\naccurate single-exposure phase measurement with white light and thus improving\nthe efficiency of phase imaging. Moreover, we present a new class of diffusion\nmodels that are well-suited for quantitative data and have a sound theoretical\nbasis. To validate our approach, we employ a widespread brightfield microscope\nequipped with a commercially available color camera. We apply our model to\nclinical microscopy of patients' urine, obtaining accurate phase measurements.\n", "link": "http://arxiv.org/abs/2406.04388v2", "date": "2024-12-20", "relevancy": 2.23, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.572}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5546}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Exposure%20Quantitative%20Phase%20Imaging%20with%20a%20Conventional%0A%20%20Microscope%20using%20Diffusion%20Models&body=Title%3A%20Single%20Exposure%20Quantitative%20Phase%20Imaging%20with%20a%20Conventional%0A%20%20Microscope%20using%20Diffusion%20Models%0AAuthor%3A%20Gabriel%20della%20Maggiora%20and%20Luis%20Alberto%20Croquevielle%20and%20Harry%20Horsley%20and%20Thomas%20Heinis%20and%20Artur%20Yakimovich%0AAbstract%3A%20%20%20Phase%20imaging%20is%20gaining%20importance%20due%20to%20its%20applications%20in%20fields%20like%0Abiomedical%20imaging%20and%20material%20characterization.%20In%20biomedical%20applications%2C%0Ait%20can%20provide%20quantitative%20information%20missing%20in%20label-free%20microscopy%0Amodalities.%20One%20of%20the%20most%20prominent%20methods%20in%20phase%20quantification%20is%20the%0ATransport-of-Intensity%20Equation%20%28TIE%29.%20TIE%20often%20requires%20multiple%20acquisitions%0Aat%20different%20defocus%20distances%2C%20which%20is%20not%20always%20feasible%20in%20a%20clinical%0Asetting.%20To%20address%20this%20issue%2C%20we%20propose%20to%20use%20chromatic%20aberrations%20to%0Ainduce%20the%20required%20through-focus%20images%20with%20a%20single%20exposure%2C%20effectively%0Agenerating%20a%20through-focus%20stack.%20Since%20the%20defocus%20distance%20induced%20by%20the%0Aaberrations%20is%20small%2C%20conventional%20TIE%20solvers%20are%20insufficient%20to%20address%20the%0Aresulting%20artifacts.%20We%20propose%20Zero-Mean%20Diffusion%2C%20a%20modified%20version%20of%0Adiffusion%20models%20designed%20for%20quantitative%20image%20prediction%2C%20and%20train%20it%20with%0Asynthetic%20data%20to%20ensure%20robust%20phase%20retrieval.%20Our%20contributions%20offer%20an%0Aalternative%20TIE%20approach%20that%20leverages%20chromatic%20aberrations%2C%20achieving%0Aaccurate%20single-exposure%20phase%20measurement%20with%20white%20light%20and%20thus%20improving%0Athe%20efficiency%20of%20phase%20imaging.%20Moreover%2C%20we%20present%20a%20new%20class%20of%20diffusion%0Amodels%20that%20are%20well-suited%20for%20quantitative%20data%20and%20have%20a%20sound%20theoretical%0Abasis.%20To%20validate%20our%20approach%2C%20we%20employ%20a%20widespread%20brightfield%20microscope%0Aequipped%20with%20a%20commercially%20available%20color%20camera.%20We%20apply%20our%20model%20to%0Aclinical%20microscopy%20of%20patients%27%20urine%2C%20obtaining%20accurate%20phase%20measurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04388v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Exposure%2520Quantitative%2520Phase%2520Imaging%2520with%2520a%2520Conventional%250A%2520%2520Microscope%2520using%2520Diffusion%2520Models%26entry.906535625%3DGabriel%2520della%2520Maggiora%2520and%2520Luis%2520Alberto%2520Croquevielle%2520and%2520Harry%2520Horsley%2520and%2520Thomas%2520Heinis%2520and%2520Artur%2520Yakimovich%26entry.1292438233%3D%2520%2520Phase%2520imaging%2520is%2520gaining%2520importance%2520due%2520to%2520its%2520applications%2520in%2520fields%2520like%250Abiomedical%2520imaging%2520and%2520material%2520characterization.%2520In%2520biomedical%2520applications%252C%250Ait%2520can%2520provide%2520quantitative%2520information%2520missing%2520in%2520label-free%2520microscopy%250Amodalities.%2520One%2520of%2520the%2520most%2520prominent%2520methods%2520in%2520phase%2520quantification%2520is%2520the%250ATransport-of-Intensity%2520Equation%2520%2528TIE%2529.%2520TIE%2520often%2520requires%2520multiple%2520acquisitions%250Aat%2520different%2520defocus%2520distances%252C%2520which%2520is%2520not%2520always%2520feasible%2520in%2520a%2520clinical%250Asetting.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520to%2520use%2520chromatic%2520aberrations%2520to%250Ainduce%2520the%2520required%2520through-focus%2520images%2520with%2520a%2520single%2520exposure%252C%2520effectively%250Agenerating%2520a%2520through-focus%2520stack.%2520Since%2520the%2520defocus%2520distance%2520induced%2520by%2520the%250Aaberrations%2520is%2520small%252C%2520conventional%2520TIE%2520solvers%2520are%2520insufficient%2520to%2520address%2520the%250Aresulting%2520artifacts.%2520We%2520propose%2520Zero-Mean%2520Diffusion%252C%2520a%2520modified%2520version%2520of%250Adiffusion%2520models%2520designed%2520for%2520quantitative%2520image%2520prediction%252C%2520and%2520train%2520it%2520with%250Asynthetic%2520data%2520to%2520ensure%2520robust%2520phase%2520retrieval.%2520Our%2520contributions%2520offer%2520an%250Aalternative%2520TIE%2520approach%2520that%2520leverages%2520chromatic%2520aberrations%252C%2520achieving%250Aaccurate%2520single-exposure%2520phase%2520measurement%2520with%2520white%2520light%2520and%2520thus%2520improving%250Athe%2520efficiency%2520of%2520phase%2520imaging.%2520Moreover%252C%2520we%2520present%2520a%2520new%2520class%2520of%2520diffusion%250Amodels%2520that%2520are%2520well-suited%2520for%2520quantitative%2520data%2520and%2520have%2520a%2520sound%2520theoretical%250Abasis.%2520To%2520validate%2520our%2520approach%252C%2520we%2520employ%2520a%2520widespread%2520brightfield%2520microscope%250Aequipped%2520with%2520a%2520commercially%2520available%2520color%2520camera.%2520We%2520apply%2520our%2520model%2520to%250Aclinical%2520microscopy%2520of%2520patients%2527%2520urine%252C%2520obtaining%2520accurate%2520phase%2520measurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04388v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Exposure%20Quantitative%20Phase%20Imaging%20with%20a%20Conventional%0A%20%20Microscope%20using%20Diffusion%20Models&entry.906535625=Gabriel%20della%20Maggiora%20and%20Luis%20Alberto%20Croquevielle%20and%20Harry%20Horsley%20and%20Thomas%20Heinis%20and%20Artur%20Yakimovich&entry.1292438233=%20%20Phase%20imaging%20is%20gaining%20importance%20due%20to%20its%20applications%20in%20fields%20like%0Abiomedical%20imaging%20and%20material%20characterization.%20In%20biomedical%20applications%2C%0Ait%20can%20provide%20quantitative%20information%20missing%20in%20label-free%20microscopy%0Amodalities.%20One%20of%20the%20most%20prominent%20methods%20in%20phase%20quantification%20is%20the%0ATransport-of-Intensity%20Equation%20%28TIE%29.%20TIE%20often%20requires%20multiple%20acquisitions%0Aat%20different%20defocus%20distances%2C%20which%20is%20not%20always%20feasible%20in%20a%20clinical%0Asetting.%20To%20address%20this%20issue%2C%20we%20propose%20to%20use%20chromatic%20aberrations%20to%0Ainduce%20the%20required%20through-focus%20images%20with%20a%20single%20exposure%2C%20effectively%0Agenerating%20a%20through-focus%20stack.%20Since%20the%20defocus%20distance%20induced%20by%20the%0Aaberrations%20is%20small%2C%20conventional%20TIE%20solvers%20are%20insufficient%20to%20address%20the%0Aresulting%20artifacts.%20We%20propose%20Zero-Mean%20Diffusion%2C%20a%20modified%20version%20of%0Adiffusion%20models%20designed%20for%20quantitative%20image%20prediction%2C%20and%20train%20it%20with%0Asynthetic%20data%20to%20ensure%20robust%20phase%20retrieval.%20Our%20contributions%20offer%20an%0Aalternative%20TIE%20approach%20that%20leverages%20chromatic%20aberrations%2C%20achieving%0Aaccurate%20single-exposure%20phase%20measurement%20with%20white%20light%20and%20thus%20improving%0Athe%20efficiency%20of%20phase%20imaging.%20Moreover%2C%20we%20present%20a%20new%20class%20of%20diffusion%0Amodels%20that%20are%20well-suited%20for%20quantitative%20data%20and%20have%20a%20sound%20theoretical%0Abasis.%20To%20validate%20our%20approach%2C%20we%20employ%20a%20widespread%20brightfield%20microscope%0Aequipped%20with%20a%20commercially%20available%20color%20camera.%20We%20apply%20our%20model%20to%0Aclinical%20microscopy%20of%20patients%27%20urine%2C%20obtaining%20accurate%20phase%20measurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04388v2&entry.124074799=Read"},
{"title": "COLUMBUS: Evaluating COgnitive Lateral Understanding through\n  Multiple-choice reBUSes", "author": "Koen Kraaijveld and Yifan Jiang and Kaixin Ma and Filip Ilievski", "abstract": "  While visual question-answering (VQA) benchmarks have catalyzed the\ndevelopment of reasoning techniques, they have focused on vertical thinking.\nEffective problem-solving also necessitates lateral thinking, which remains\nunderstudied in AI and has not been used to test visual perception systems. To\nbridge this gap, we formulate visual lateral thinking as a multiple-choice\nquestion-answering task and describe a three-step taxonomy-driven methodology\nfor instantiating task examples. Then, we develop COLUMBUS, a synthetic\nbenchmark that applies the task pipeline to create QA sets with text and icon\nrebus puzzles based on publicly available collections of compounds and common\nphrases. COLUMBUS comprises over 1,000 puzzles, each with four answer\ncandidates. While the SotA vision-language models (VLMs) achieve decent\nperformance, our evaluation demonstrates a substantial gap between humans and\nmodels. VLMs benefit from human-curated descriptions but struggle to\nself-generate such representations at the right level of abstraction.\n", "link": "http://arxiv.org/abs/2409.04053v2", "date": "2024-12-20", "relevancy": 2.2116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5601}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COLUMBUS%3A%20Evaluating%20COgnitive%20Lateral%20Understanding%20through%0A%20%20Multiple-choice%20reBUSes&body=Title%3A%20COLUMBUS%3A%20Evaluating%20COgnitive%20Lateral%20Understanding%20through%0A%20%20Multiple-choice%20reBUSes%0AAuthor%3A%20Koen%20Kraaijveld%20and%20Yifan%20Jiang%20and%20Kaixin%20Ma%20and%20Filip%20Ilievski%0AAbstract%3A%20%20%20While%20visual%20question-answering%20%28VQA%29%20benchmarks%20have%20catalyzed%20the%0Adevelopment%20of%20reasoning%20techniques%2C%20they%20have%20focused%20on%20vertical%20thinking.%0AEffective%20problem-solving%20also%20necessitates%20lateral%20thinking%2C%20which%20remains%0Aunderstudied%20in%20AI%20and%20has%20not%20been%20used%20to%20test%20visual%20perception%20systems.%20To%0Abridge%20this%20gap%2C%20we%20formulate%20visual%20lateral%20thinking%20as%20a%20multiple-choice%0Aquestion-answering%20task%20and%20describe%20a%20three-step%20taxonomy-driven%20methodology%0Afor%20instantiating%20task%20examples.%20Then%2C%20we%20develop%20COLUMBUS%2C%20a%20synthetic%0Abenchmark%20that%20applies%20the%20task%20pipeline%20to%20create%20QA%20sets%20with%20text%20and%20icon%0Arebus%20puzzles%20based%20on%20publicly%20available%20collections%20of%20compounds%20and%20common%0Aphrases.%20COLUMBUS%20comprises%20over%201%2C000%20puzzles%2C%20each%20with%20four%20answer%0Acandidates.%20While%20the%20SotA%20vision-language%20models%20%28VLMs%29%20achieve%20decent%0Aperformance%2C%20our%20evaluation%20demonstrates%20a%20substantial%20gap%20between%20humans%20and%0Amodels.%20VLMs%20benefit%20from%20human-curated%20descriptions%20but%20struggle%20to%0Aself-generate%20such%20representations%20at%20the%20right%20level%20of%20abstraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOLUMBUS%253A%2520Evaluating%2520COgnitive%2520Lateral%2520Understanding%2520through%250A%2520%2520Multiple-choice%2520reBUSes%26entry.906535625%3DKoen%2520Kraaijveld%2520and%2520Yifan%2520Jiang%2520and%2520Kaixin%2520Ma%2520and%2520Filip%2520Ilievski%26entry.1292438233%3D%2520%2520While%2520visual%2520question-answering%2520%2528VQA%2529%2520benchmarks%2520have%2520catalyzed%2520the%250Adevelopment%2520of%2520reasoning%2520techniques%252C%2520they%2520have%2520focused%2520on%2520vertical%2520thinking.%250AEffective%2520problem-solving%2520also%2520necessitates%2520lateral%2520thinking%252C%2520which%2520remains%250Aunderstudied%2520in%2520AI%2520and%2520has%2520not%2520been%2520used%2520to%2520test%2520visual%2520perception%2520systems.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520formulate%2520visual%2520lateral%2520thinking%2520as%2520a%2520multiple-choice%250Aquestion-answering%2520task%2520and%2520describe%2520a%2520three-step%2520taxonomy-driven%2520methodology%250Afor%2520instantiating%2520task%2520examples.%2520Then%252C%2520we%2520develop%2520COLUMBUS%252C%2520a%2520synthetic%250Abenchmark%2520that%2520applies%2520the%2520task%2520pipeline%2520to%2520create%2520QA%2520sets%2520with%2520text%2520and%2520icon%250Arebus%2520puzzles%2520based%2520on%2520publicly%2520available%2520collections%2520of%2520compounds%2520and%2520common%250Aphrases.%2520COLUMBUS%2520comprises%2520over%25201%252C000%2520puzzles%252C%2520each%2520with%2520four%2520answer%250Acandidates.%2520While%2520the%2520SotA%2520vision-language%2520models%2520%2528VLMs%2529%2520achieve%2520decent%250Aperformance%252C%2520our%2520evaluation%2520demonstrates%2520a%2520substantial%2520gap%2520between%2520humans%2520and%250Amodels.%2520VLMs%2520benefit%2520from%2520human-curated%2520descriptions%2520but%2520struggle%2520to%250Aself-generate%2520such%2520representations%2520at%2520the%2520right%2520level%2520of%2520abstraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COLUMBUS%3A%20Evaluating%20COgnitive%20Lateral%20Understanding%20through%0A%20%20Multiple-choice%20reBUSes&entry.906535625=Koen%20Kraaijveld%20and%20Yifan%20Jiang%20and%20Kaixin%20Ma%20and%20Filip%20Ilievski&entry.1292438233=%20%20While%20visual%20question-answering%20%28VQA%29%20benchmarks%20have%20catalyzed%20the%0Adevelopment%20of%20reasoning%20techniques%2C%20they%20have%20focused%20on%20vertical%20thinking.%0AEffective%20problem-solving%20also%20necessitates%20lateral%20thinking%2C%20which%20remains%0Aunderstudied%20in%20AI%20and%20has%20not%20been%20used%20to%20test%20visual%20perception%20systems.%20To%0Abridge%20this%20gap%2C%20we%20formulate%20visual%20lateral%20thinking%20as%20a%20multiple-choice%0Aquestion-answering%20task%20and%20describe%20a%20three-step%20taxonomy-driven%20methodology%0Afor%20instantiating%20task%20examples.%20Then%2C%20we%20develop%20COLUMBUS%2C%20a%20synthetic%0Abenchmark%20that%20applies%20the%20task%20pipeline%20to%20create%20QA%20sets%20with%20text%20and%20icon%0Arebus%20puzzles%20based%20on%20publicly%20available%20collections%20of%20compounds%20and%20common%0Aphrases.%20COLUMBUS%20comprises%20over%201%2C000%20puzzles%2C%20each%20with%20four%20answer%0Acandidates.%20While%20the%20SotA%20vision-language%20models%20%28VLMs%29%20achieve%20decent%0Aperformance%2C%20our%20evaluation%20demonstrates%20a%20substantial%20gap%20between%20humans%20and%0Amodels.%20VLMs%20benefit%20from%20human-curated%20descriptions%20but%20struggle%20to%0Aself-generate%20such%20representations%20at%20the%20right%20level%20of%20abstraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04053v2&entry.124074799=Read"},
{"title": "Clustering Time-Evolving Networks Using the Spatio-Temporal Graph\n  Laplacian", "author": "Maia Trower and Nata\u0161a Djurdjevac Conrad and Stefan Klus", "abstract": "  Time-evolving graphs arise frequently when modeling complex dynamical systems\nsuch as social networks, traffic flow, and biological processes. Developing\ntechniques to identify and analyze communities in these time-varying graph\nstructures is an important challenge. In this work, we generalize existing\nspectral clustering algorithms from static to dynamic graphs using canonical\ncorrelation analysis (CCA) to capture the temporal evolution of clusters. Based\non this extended canonical correlation framework, we define the spatio-temporal\ngraph Laplacian and investigate its spectral properties. We connect these\nconcepts to dynamical systems theory via transfer operators, and illustrate the\nadvantages of our method on benchmark graphs by comparison with existing\nmethods. We show that the spatio-temporal graph Laplacian allows for a clear\ninterpretation of cluster structure evolution over time for directed and\nundirected graphs.\n", "link": "http://arxiv.org/abs/2407.12864v3", "date": "2024-12-20", "relevancy": 2.2022, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4515}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4371}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian&body=Title%3A%20Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian%0AAuthor%3A%20Maia%20Trower%20and%20Nata%C5%A1a%20Djurdjevac%20Conrad%20and%20Stefan%20Klus%0AAbstract%3A%20%20%20Time-evolving%20graphs%20arise%20frequently%20when%20modeling%20complex%20dynamical%20systems%0Asuch%20as%20social%20networks%2C%20traffic%20flow%2C%20and%20biological%20processes.%20Developing%0Atechniques%20to%20identify%20and%20analyze%20communities%20in%20these%20time-varying%20graph%0Astructures%20is%20an%20important%20challenge.%20In%20this%20work%2C%20we%20generalize%20existing%0Aspectral%20clustering%20algorithms%20from%20static%20to%20dynamic%20graphs%20using%20canonical%0Acorrelation%20analysis%20%28CCA%29%20to%20capture%20the%20temporal%20evolution%20of%20clusters.%20Based%0Aon%20this%20extended%20canonical%20correlation%20framework%2C%20we%20define%20the%20spatio-temporal%0Agraph%20Laplacian%20and%20investigate%20its%20spectral%20properties.%20We%20connect%20these%0Aconcepts%20to%20dynamical%20systems%20theory%20via%20transfer%20operators%2C%20and%20illustrate%20the%0Aadvantages%20of%20our%20method%20on%20benchmark%20graphs%20by%20comparison%20with%20existing%0Amethods.%20We%20show%20that%20the%20spatio-temporal%20graph%20Laplacian%20allows%20for%20a%20clear%0Ainterpretation%20of%20cluster%20structure%20evolution%20over%20time%20for%20directed%20and%0Aundirected%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12864v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520Time-Evolving%2520Networks%2520Using%2520the%2520Spatio-Temporal%2520Graph%250A%2520%2520Laplacian%26entry.906535625%3DMaia%2520Trower%2520and%2520Nata%25C5%25A1a%2520Djurdjevac%2520Conrad%2520and%2520Stefan%2520Klus%26entry.1292438233%3D%2520%2520Time-evolving%2520graphs%2520arise%2520frequently%2520when%2520modeling%2520complex%2520dynamical%2520systems%250Asuch%2520as%2520social%2520networks%252C%2520traffic%2520flow%252C%2520and%2520biological%2520processes.%2520Developing%250Atechniques%2520to%2520identify%2520and%2520analyze%2520communities%2520in%2520these%2520time-varying%2520graph%250Astructures%2520is%2520an%2520important%2520challenge.%2520In%2520this%2520work%252C%2520we%2520generalize%2520existing%250Aspectral%2520clustering%2520algorithms%2520from%2520static%2520to%2520dynamic%2520graphs%2520using%2520canonical%250Acorrelation%2520analysis%2520%2528CCA%2529%2520to%2520capture%2520the%2520temporal%2520evolution%2520of%2520clusters.%2520Based%250Aon%2520this%2520extended%2520canonical%2520correlation%2520framework%252C%2520we%2520define%2520the%2520spatio-temporal%250Agraph%2520Laplacian%2520and%2520investigate%2520its%2520spectral%2520properties.%2520We%2520connect%2520these%250Aconcepts%2520to%2520dynamical%2520systems%2520theory%2520via%2520transfer%2520operators%252C%2520and%2520illustrate%2520the%250Aadvantages%2520of%2520our%2520method%2520on%2520benchmark%2520graphs%2520by%2520comparison%2520with%2520existing%250Amethods.%2520We%2520show%2520that%2520the%2520spatio-temporal%2520graph%2520Laplacian%2520allows%2520for%2520a%2520clear%250Ainterpretation%2520of%2520cluster%2520structure%2520evolution%2520over%2520time%2520for%2520directed%2520and%250Aundirected%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12864v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20Time-Evolving%20Networks%20Using%20the%20Spatio-Temporal%20Graph%0A%20%20Laplacian&entry.906535625=Maia%20Trower%20and%20Nata%C5%A1a%20Djurdjevac%20Conrad%20and%20Stefan%20Klus&entry.1292438233=%20%20Time-evolving%20graphs%20arise%20frequently%20when%20modeling%20complex%20dynamical%20systems%0Asuch%20as%20social%20networks%2C%20traffic%20flow%2C%20and%20biological%20processes.%20Developing%0Atechniques%20to%20identify%20and%20analyze%20communities%20in%20these%20time-varying%20graph%0Astructures%20is%20an%20important%20challenge.%20In%20this%20work%2C%20we%20generalize%20existing%0Aspectral%20clustering%20algorithms%20from%20static%20to%20dynamic%20graphs%20using%20canonical%0Acorrelation%20analysis%20%28CCA%29%20to%20capture%20the%20temporal%20evolution%20of%20clusters.%20Based%0Aon%20this%20extended%20canonical%20correlation%20framework%2C%20we%20define%20the%20spatio-temporal%0Agraph%20Laplacian%20and%20investigate%20its%20spectral%20properties.%20We%20connect%20these%0Aconcepts%20to%20dynamical%20systems%20theory%20via%20transfer%20operators%2C%20and%20illustrate%20the%0Aadvantages%20of%20our%20method%20on%20benchmark%20graphs%20by%20comparison%20with%20existing%0Amethods.%20We%20show%20that%20the%20spatio-temporal%20graph%20Laplacian%20allows%20for%20a%20clear%0Ainterpretation%20of%20cluster%20structure%20evolution%20over%20time%20for%20directed%20and%0Aundirected%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12864v3&entry.124074799=Read"},
{"title": "Improving Quantization-aware Training of Low-Precision Network via Block\n  Replacement on Full-Precision Counterpart", "author": "Chengting Yu and Shu Yang and Fengzhao Zhang and Hanzhi Ma and Aili Wang and Er-Ping Li", "abstract": "  Quantization-aware training (QAT) is a common paradigm for network\nquantization, in which the training phase incorporates the simulation of the\nlow-precision computation to optimize the quantization parameters in alignment\nwith the task goals. However, direct training of low-precision networks\ngenerally faces two obstacles: 1. The low-precision model exhibits limited\nrepresentation capabilities and cannot directly replicate full-precision\ncalculations, which constitutes a deficiency compared to full-precision\nalternatives; 2. Non-ideal deviations during gradient propagation are a common\nconsequence of employing pseudo-gradients as approximations in derived\nquantized functions. In this paper, we propose a general QAT framework for\nalleviating the aforementioned concerns by permitting the forward and backward\nprocesses of the low-precision network to be guided by the full-precision\npartner during training. In conjunction with the direct training of the\nquantization model, intermediate mixed-precision models are generated through\nthe block-by-block replacement on the full-precision model and working\nsimultaneously with the low-precision backbone, which enables the integration\nof quantized low-precision blocks into full-precision networks throughout the\ntraining phase. Consequently, each quantized block is capable of: 1. simulating\nfull-precision representation during forward passes; 2. obtaining gradients\nwith improved estimation during backward passes. We demonstrate that the\nproposed method achieves state-of-the-art results for 4-, 3-, and 2-bit\nquantization on ImageNet and CIFAR-10. The proposed framework provides a\ncompatible extension for most QAT methods and only requires a concise wrapper\nfor existing codes.\n", "link": "http://arxiv.org/abs/2412.15846v1", "date": "2024-12-20", "relevancy": 2.1795, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5761}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5271}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Quantization-aware%20Training%20of%20Low-Precision%20Network%20via%20Block%0A%20%20Replacement%20on%20Full-Precision%20Counterpart&body=Title%3A%20Improving%20Quantization-aware%20Training%20of%20Low-Precision%20Network%20via%20Block%0A%20%20Replacement%20on%20Full-Precision%20Counterpart%0AAuthor%3A%20Chengting%20Yu%20and%20Shu%20Yang%20and%20Fengzhao%20Zhang%20and%20Hanzhi%20Ma%20and%20Aili%20Wang%20and%20Er-Ping%20Li%0AAbstract%3A%20%20%20Quantization-aware%20training%20%28QAT%29%20is%20a%20common%20paradigm%20for%20network%0Aquantization%2C%20in%20which%20the%20training%20phase%20incorporates%20the%20simulation%20of%20the%0Alow-precision%20computation%20to%20optimize%20the%20quantization%20parameters%20in%20alignment%0Awith%20the%20task%20goals.%20However%2C%20direct%20training%20of%20low-precision%20networks%0Agenerally%20faces%20two%20obstacles%3A%201.%20The%20low-precision%20model%20exhibits%20limited%0Arepresentation%20capabilities%20and%20cannot%20directly%20replicate%20full-precision%0Acalculations%2C%20which%20constitutes%20a%20deficiency%20compared%20to%20full-precision%0Aalternatives%3B%202.%20Non-ideal%20deviations%20during%20gradient%20propagation%20are%20a%20common%0Aconsequence%20of%20employing%20pseudo-gradients%20as%20approximations%20in%20derived%0Aquantized%20functions.%20In%20this%20paper%2C%20we%20propose%20a%20general%20QAT%20framework%20for%0Aalleviating%20the%20aforementioned%20concerns%20by%20permitting%20the%20forward%20and%20backward%0Aprocesses%20of%20the%20low-precision%20network%20to%20be%20guided%20by%20the%20full-precision%0Apartner%20during%20training.%20In%20conjunction%20with%20the%20direct%20training%20of%20the%0Aquantization%20model%2C%20intermediate%20mixed-precision%20models%20are%20generated%20through%0Athe%20block-by-block%20replacement%20on%20the%20full-precision%20model%20and%20working%0Asimultaneously%20with%20the%20low-precision%20backbone%2C%20which%20enables%20the%20integration%0Aof%20quantized%20low-precision%20blocks%20into%20full-precision%20networks%20throughout%20the%0Atraining%20phase.%20Consequently%2C%20each%20quantized%20block%20is%20capable%20of%3A%201.%20simulating%0Afull-precision%20representation%20during%20forward%20passes%3B%202.%20obtaining%20gradients%0Awith%20improved%20estimation%20during%20backward%20passes.%20We%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20results%20for%204-%2C%203-%2C%20and%202-bit%0Aquantization%20on%20ImageNet%20and%20CIFAR-10.%20The%20proposed%20framework%20provides%20a%0Acompatible%20extension%20for%20most%20QAT%20methods%20and%20only%20requires%20a%20concise%20wrapper%0Afor%20existing%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Quantization-aware%2520Training%2520of%2520Low-Precision%2520Network%2520via%2520Block%250A%2520%2520Replacement%2520on%2520Full-Precision%2520Counterpart%26entry.906535625%3DChengting%2520Yu%2520and%2520Shu%2520Yang%2520and%2520Fengzhao%2520Zhang%2520and%2520Hanzhi%2520Ma%2520and%2520Aili%2520Wang%2520and%2520Er-Ping%2520Li%26entry.1292438233%3D%2520%2520Quantization-aware%2520training%2520%2528QAT%2529%2520is%2520a%2520common%2520paradigm%2520for%2520network%250Aquantization%252C%2520in%2520which%2520the%2520training%2520phase%2520incorporates%2520the%2520simulation%2520of%2520the%250Alow-precision%2520computation%2520to%2520optimize%2520the%2520quantization%2520parameters%2520in%2520alignment%250Awith%2520the%2520task%2520goals.%2520However%252C%2520direct%2520training%2520of%2520low-precision%2520networks%250Agenerally%2520faces%2520two%2520obstacles%253A%25201.%2520The%2520low-precision%2520model%2520exhibits%2520limited%250Arepresentation%2520capabilities%2520and%2520cannot%2520directly%2520replicate%2520full-precision%250Acalculations%252C%2520which%2520constitutes%2520a%2520deficiency%2520compared%2520to%2520full-precision%250Aalternatives%253B%25202.%2520Non-ideal%2520deviations%2520during%2520gradient%2520propagation%2520are%2520a%2520common%250Aconsequence%2520of%2520employing%2520pseudo-gradients%2520as%2520approximations%2520in%2520derived%250Aquantized%2520functions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520general%2520QAT%2520framework%2520for%250Aalleviating%2520the%2520aforementioned%2520concerns%2520by%2520permitting%2520the%2520forward%2520and%2520backward%250Aprocesses%2520of%2520the%2520low-precision%2520network%2520to%2520be%2520guided%2520by%2520the%2520full-precision%250Apartner%2520during%2520training.%2520In%2520conjunction%2520with%2520the%2520direct%2520training%2520of%2520the%250Aquantization%2520model%252C%2520intermediate%2520mixed-precision%2520models%2520are%2520generated%2520through%250Athe%2520block-by-block%2520replacement%2520on%2520the%2520full-precision%2520model%2520and%2520working%250Asimultaneously%2520with%2520the%2520low-precision%2520backbone%252C%2520which%2520enables%2520the%2520integration%250Aof%2520quantized%2520low-precision%2520blocks%2520into%2520full-precision%2520networks%2520throughout%2520the%250Atraining%2520phase.%2520Consequently%252C%2520each%2520quantized%2520block%2520is%2520capable%2520of%253A%25201.%2520simulating%250Afull-precision%2520representation%2520during%2520forward%2520passes%253B%25202.%2520obtaining%2520gradients%250Awith%2520improved%2520estimation%2520during%2520backward%2520passes.%2520We%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520achieves%2520state-of-the-art%2520results%2520for%25204-%252C%25203-%252C%2520and%25202-bit%250Aquantization%2520on%2520ImageNet%2520and%2520CIFAR-10.%2520The%2520proposed%2520framework%2520provides%2520a%250Acompatible%2520extension%2520for%2520most%2520QAT%2520methods%2520and%2520only%2520requires%2520a%2520concise%2520wrapper%250Afor%2520existing%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Quantization-aware%20Training%20of%20Low-Precision%20Network%20via%20Block%0A%20%20Replacement%20on%20Full-Precision%20Counterpart&entry.906535625=Chengting%20Yu%20and%20Shu%20Yang%20and%20Fengzhao%20Zhang%20and%20Hanzhi%20Ma%20and%20Aili%20Wang%20and%20Er-Ping%20Li&entry.1292438233=%20%20Quantization-aware%20training%20%28QAT%29%20is%20a%20common%20paradigm%20for%20network%0Aquantization%2C%20in%20which%20the%20training%20phase%20incorporates%20the%20simulation%20of%20the%0Alow-precision%20computation%20to%20optimize%20the%20quantization%20parameters%20in%20alignment%0Awith%20the%20task%20goals.%20However%2C%20direct%20training%20of%20low-precision%20networks%0Agenerally%20faces%20two%20obstacles%3A%201.%20The%20low-precision%20model%20exhibits%20limited%0Arepresentation%20capabilities%20and%20cannot%20directly%20replicate%20full-precision%0Acalculations%2C%20which%20constitutes%20a%20deficiency%20compared%20to%20full-precision%0Aalternatives%3B%202.%20Non-ideal%20deviations%20during%20gradient%20propagation%20are%20a%20common%0Aconsequence%20of%20employing%20pseudo-gradients%20as%20approximations%20in%20derived%0Aquantized%20functions.%20In%20this%20paper%2C%20we%20propose%20a%20general%20QAT%20framework%20for%0Aalleviating%20the%20aforementioned%20concerns%20by%20permitting%20the%20forward%20and%20backward%0Aprocesses%20of%20the%20low-precision%20network%20to%20be%20guided%20by%20the%20full-precision%0Apartner%20during%20training.%20In%20conjunction%20with%20the%20direct%20training%20of%20the%0Aquantization%20model%2C%20intermediate%20mixed-precision%20models%20are%20generated%20through%0Athe%20block-by-block%20replacement%20on%20the%20full-precision%20model%20and%20working%0Asimultaneously%20with%20the%20low-precision%20backbone%2C%20which%20enables%20the%20integration%0Aof%20quantized%20low-precision%20blocks%20into%20full-precision%20networks%20throughout%20the%0Atraining%20phase.%20Consequently%2C%20each%20quantized%20block%20is%20capable%20of%3A%201.%20simulating%0Afull-precision%20representation%20during%20forward%20passes%3B%202.%20obtaining%20gradients%0Awith%20improved%20estimation%20during%20backward%20passes.%20We%20demonstrate%20that%20the%0Aproposed%20method%20achieves%20state-of-the-art%20results%20for%204-%2C%203-%2C%20and%202-bit%0Aquantization%20on%20ImageNet%20and%20CIFAR-10.%20The%20proposed%20framework%20provides%20a%0Acompatible%20extension%20for%20most%20QAT%20methods%20and%20only%20requires%20a%20concise%20wrapper%0Afor%20existing%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15846v1&entry.124074799=Read"},
{"title": "Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease\n  Detection based on Spontaneous Speech", "author": "Jonathan Heitz and Gerold Schneider and Nicolas Langer", "abstract": "  Alzheimer's Disease (AD) is a significant and growing public health concern.\nInvestigating alterations in speech and language patterns offers a promising\npath towards cost-effective and non-invasive early detection of AD on a large\nscale. Large language models (LLMs), such as GPT, have enabled powerful new\npossibilities for semantic text analysis. In this study, we leverage GPT-4 to\nextract five semantic features from transcripts of spontaneous patient speech.\nThe features capture known symptoms of AD, but they are difficult to quantify\neffectively using traditional methods of computational linguistics. We\ndemonstrate the clinical significance of these features and further validate\none of them (\"Word-Finding Difficulties\") against a proxy measure and human\nraters. When combined with established linguistic features and a Random Forest\nclassifier, the GPT-derived features significantly improve the detection of AD.\nOur approach proves effective for both manually transcribed and automatically\ngenerated transcripts, representing a novel and impactful use of recent\nadvancements in LLMs for AD speech analysis.\n", "link": "http://arxiv.org/abs/2412.15772v1", "date": "2024-12-20", "relevancy": 2.1675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4347}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linguistic%20Features%20Extracted%20by%20GPT-4%20Improve%20Alzheimer%27s%20Disease%0A%20%20Detection%20based%20on%20Spontaneous%20Speech&body=Title%3A%20Linguistic%20Features%20Extracted%20by%20GPT-4%20Improve%20Alzheimer%27s%20Disease%0A%20%20Detection%20based%20on%20Spontaneous%20Speech%0AAuthor%3A%20Jonathan%20Heitz%20and%20Gerold%20Schneider%20and%20Nicolas%20Langer%0AAbstract%3A%20%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20significant%20and%20growing%20public%20health%20concern.%0AInvestigating%20alterations%20in%20speech%20and%20language%20patterns%20offers%20a%20promising%0Apath%20towards%20cost-effective%20and%20non-invasive%20early%20detection%20of%20AD%20on%20a%20large%0Ascale.%20Large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT%2C%20have%20enabled%20powerful%20new%0Apossibilities%20for%20semantic%20text%20analysis.%20In%20this%20study%2C%20we%20leverage%20GPT-4%20to%0Aextract%20five%20semantic%20features%20from%20transcripts%20of%20spontaneous%20patient%20speech.%0AThe%20features%20capture%20known%20symptoms%20of%20AD%2C%20but%20they%20are%20difficult%20to%20quantify%0Aeffectively%20using%20traditional%20methods%20of%20computational%20linguistics.%20We%0Ademonstrate%20the%20clinical%20significance%20of%20these%20features%20and%20further%20validate%0Aone%20of%20them%20%28%22Word-Finding%20Difficulties%22%29%20against%20a%20proxy%20measure%20and%20human%0Araters.%20When%20combined%20with%20established%20linguistic%20features%20and%20a%20Random%20Forest%0Aclassifier%2C%20the%20GPT-derived%20features%20significantly%20improve%20the%20detection%20of%20AD.%0AOur%20approach%20proves%20effective%20for%20both%20manually%20transcribed%20and%20automatically%0Agenerated%20transcripts%2C%20representing%20a%20novel%20and%20impactful%20use%20of%20recent%0Aadvancements%20in%20LLMs%20for%20AD%20speech%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinguistic%2520Features%2520Extracted%2520by%2520GPT-4%2520Improve%2520Alzheimer%2527s%2520Disease%250A%2520%2520Detection%2520based%2520on%2520Spontaneous%2520Speech%26entry.906535625%3DJonathan%2520Heitz%2520and%2520Gerold%2520Schneider%2520and%2520Nicolas%2520Langer%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520a%2520significant%2520and%2520growing%2520public%2520health%2520concern.%250AInvestigating%2520alterations%2520in%2520speech%2520and%2520language%2520patterns%2520offers%2520a%2520promising%250Apath%2520towards%2520cost-effective%2520and%2520non-invasive%2520early%2520detection%2520of%2520AD%2520on%2520a%2520large%250Ascale.%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%2520GPT%252C%2520have%2520enabled%2520powerful%2520new%250Apossibilities%2520for%2520semantic%2520text%2520analysis.%2520In%2520this%2520study%252C%2520we%2520leverage%2520GPT-4%2520to%250Aextract%2520five%2520semantic%2520features%2520from%2520transcripts%2520of%2520spontaneous%2520patient%2520speech.%250AThe%2520features%2520capture%2520known%2520symptoms%2520of%2520AD%252C%2520but%2520they%2520are%2520difficult%2520to%2520quantify%250Aeffectively%2520using%2520traditional%2520methods%2520of%2520computational%2520linguistics.%2520We%250Ademonstrate%2520the%2520clinical%2520significance%2520of%2520these%2520features%2520and%2520further%2520validate%250Aone%2520of%2520them%2520%2528%2522Word-Finding%2520Difficulties%2522%2529%2520against%2520a%2520proxy%2520measure%2520and%2520human%250Araters.%2520When%2520combined%2520with%2520established%2520linguistic%2520features%2520and%2520a%2520Random%2520Forest%250Aclassifier%252C%2520the%2520GPT-derived%2520features%2520significantly%2520improve%2520the%2520detection%2520of%2520AD.%250AOur%2520approach%2520proves%2520effective%2520for%2520both%2520manually%2520transcribed%2520and%2520automatically%250Agenerated%2520transcripts%252C%2520representing%2520a%2520novel%2520and%2520impactful%2520use%2520of%2520recent%250Aadvancements%2520in%2520LLMs%2520for%2520AD%2520speech%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linguistic%20Features%20Extracted%20by%20GPT-4%20Improve%20Alzheimer%27s%20Disease%0A%20%20Detection%20based%20on%20Spontaneous%20Speech&entry.906535625=Jonathan%20Heitz%20and%20Gerold%20Schneider%20and%20Nicolas%20Langer&entry.1292438233=%20%20Alzheimer%27s%20Disease%20%28AD%29%20is%20a%20significant%20and%20growing%20public%20health%20concern.%0AInvestigating%20alterations%20in%20speech%20and%20language%20patterns%20offers%20a%20promising%0Apath%20towards%20cost-effective%20and%20non-invasive%20early%20detection%20of%20AD%20on%20a%20large%0Ascale.%20Large%20language%20models%20%28LLMs%29%2C%20such%20as%20GPT%2C%20have%20enabled%20powerful%20new%0Apossibilities%20for%20semantic%20text%20analysis.%20In%20this%20study%2C%20we%20leverage%20GPT-4%20to%0Aextract%20five%20semantic%20features%20from%20transcripts%20of%20spontaneous%20patient%20speech.%0AThe%20features%20capture%20known%20symptoms%20of%20AD%2C%20but%20they%20are%20difficult%20to%20quantify%0Aeffectively%20using%20traditional%20methods%20of%20computational%20linguistics.%20We%0Ademonstrate%20the%20clinical%20significance%20of%20these%20features%20and%20further%20validate%0Aone%20of%20them%20%28%22Word-Finding%20Difficulties%22%29%20against%20a%20proxy%20measure%20and%20human%0Araters.%20When%20combined%20with%20established%20linguistic%20features%20and%20a%20Random%20Forest%0Aclassifier%2C%20the%20GPT-derived%20features%20significantly%20improve%20the%20detection%20of%20AD.%0AOur%20approach%20proves%20effective%20for%20both%20manually%20transcribed%20and%20automatically%0Agenerated%20transcripts%2C%20representing%20a%20novel%20and%20impactful%20use%20of%20recent%0Aadvancements%20in%20LLMs%20for%20AD%20speech%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15772v1&entry.124074799=Read"},
{"title": "LEDA: Log-Euclidean Diffeomorphic Autoencoder for Efficient Statistical\n  Analysis of Diffeomorphism", "author": "Krithika Iyer and Shireen Elhabian and Sarang Joshi", "abstract": "  Image registration is a core task in computational anatomy that establishes\ncorrespondences between images. Invertible deformable registration, which\ncomputes a deformation field and handles complex, non-linear transformation, is\nessential for tracking anatomical variations, especially in neuroimaging\napplications where inter-subject differences and longitudinal changes are key.\nAnalyzing the deformation fields is challenging due to their non-linearity,\nlimiting statistical analysis. However, traditional approaches for analyzing\ndeformation fields are computationally expensive, sensitive to initialization,\nand prone to numerical errors, especially when the deformation is far from the\nidentity. To address these limitations, we propose the Log-Euclidean\nDiffeomorphic Autoencoder (LEDA), an innovative framework designed to compute\nthe principal logarithm of deformation fields by efficiently predicting\nconsecutive square roots. LEDA operates within a linearized latent space that\nadheres to the diffeomorphisms group action laws, enhancing our model's\nrobustness and applicability. We also introduce a loss function to enforce\ninverse consistency, ensuring accurate latent representations of deformation\nfields. Extensive experiments with the OASIS-1 dataset demonstrate the\neffectiveness of LEDA in accurately modeling and analyzing complex non-linear\ndeformations while maintaining inverse consistency. Additionally, we evaluate\nits ability to capture and incorporate clinical variables, enhancing its\nrelevance for clinical applications.\n", "link": "http://arxiv.org/abs/2412.16129v1", "date": "2024-12-20", "relevancy": 2.1653, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5696}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEDA%3A%20Log-Euclidean%20Diffeomorphic%20Autoencoder%20for%20Efficient%20Statistical%0A%20%20Analysis%20of%20Diffeomorphism&body=Title%3A%20LEDA%3A%20Log-Euclidean%20Diffeomorphic%20Autoencoder%20for%20Efficient%20Statistical%0A%20%20Analysis%20of%20Diffeomorphism%0AAuthor%3A%20Krithika%20Iyer%20and%20Shireen%20Elhabian%20and%20Sarang%20Joshi%0AAbstract%3A%20%20%20Image%20registration%20is%20a%20core%20task%20in%20computational%20anatomy%20that%20establishes%0Acorrespondences%20between%20images.%20Invertible%20deformable%20registration%2C%20which%0Acomputes%20a%20deformation%20field%20and%20handles%20complex%2C%20non-linear%20transformation%2C%20is%0Aessential%20for%20tracking%20anatomical%20variations%2C%20especially%20in%20neuroimaging%0Aapplications%20where%20inter-subject%20differences%20and%20longitudinal%20changes%20are%20key.%0AAnalyzing%20the%20deformation%20fields%20is%20challenging%20due%20to%20their%20non-linearity%2C%0Alimiting%20statistical%20analysis.%20However%2C%20traditional%20approaches%20for%20analyzing%0Adeformation%20fields%20are%20computationally%20expensive%2C%20sensitive%20to%20initialization%2C%0Aand%20prone%20to%20numerical%20errors%2C%20especially%20when%20the%20deformation%20is%20far%20from%20the%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Log-Euclidean%0ADiffeomorphic%20Autoencoder%20%28LEDA%29%2C%20an%20innovative%20framework%20designed%20to%20compute%0Athe%20principal%20logarithm%20of%20deformation%20fields%20by%20efficiently%20predicting%0Aconsecutive%20square%20roots.%20LEDA%20operates%20within%20a%20linearized%20latent%20space%20that%0Aadheres%20to%20the%20diffeomorphisms%20group%20action%20laws%2C%20enhancing%20our%20model%27s%0Arobustness%20and%20applicability.%20We%20also%20introduce%20a%20loss%20function%20to%20enforce%0Ainverse%20consistency%2C%20ensuring%20accurate%20latent%20representations%20of%20deformation%0Afields.%20Extensive%20experiments%20with%20the%20OASIS-1%20dataset%20demonstrate%20the%0Aeffectiveness%20of%20LEDA%20in%20accurately%20modeling%20and%20analyzing%20complex%20non-linear%0Adeformations%20while%20maintaining%20inverse%20consistency.%20Additionally%2C%20we%20evaluate%0Aits%20ability%20to%20capture%20and%20incorporate%20clinical%20variables%2C%20enhancing%20its%0Arelevance%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEDA%253A%2520Log-Euclidean%2520Diffeomorphic%2520Autoencoder%2520for%2520Efficient%2520Statistical%250A%2520%2520Analysis%2520of%2520Diffeomorphism%26entry.906535625%3DKrithika%2520Iyer%2520and%2520Shireen%2520Elhabian%2520and%2520Sarang%2520Joshi%26entry.1292438233%3D%2520%2520Image%2520registration%2520is%2520a%2520core%2520task%2520in%2520computational%2520anatomy%2520that%2520establishes%250Acorrespondences%2520between%2520images.%2520Invertible%2520deformable%2520registration%252C%2520which%250Acomputes%2520a%2520deformation%2520field%2520and%2520handles%2520complex%252C%2520non-linear%2520transformation%252C%2520is%250Aessential%2520for%2520tracking%2520anatomical%2520variations%252C%2520especially%2520in%2520neuroimaging%250Aapplications%2520where%2520inter-subject%2520differences%2520and%2520longitudinal%2520changes%2520are%2520key.%250AAnalyzing%2520the%2520deformation%2520fields%2520is%2520challenging%2520due%2520to%2520their%2520non-linearity%252C%250Alimiting%2520statistical%2520analysis.%2520However%252C%2520traditional%2520approaches%2520for%2520analyzing%250Adeformation%2520fields%2520are%2520computationally%2520expensive%252C%2520sensitive%2520to%2520initialization%252C%250Aand%2520prone%2520to%2520numerical%2520errors%252C%2520especially%2520when%2520the%2520deformation%2520is%2520far%2520from%2520the%250Aidentity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Log-Euclidean%250ADiffeomorphic%2520Autoencoder%2520%2528LEDA%2529%252C%2520an%2520innovative%2520framework%2520designed%2520to%2520compute%250Athe%2520principal%2520logarithm%2520of%2520deformation%2520fields%2520by%2520efficiently%2520predicting%250Aconsecutive%2520square%2520roots.%2520LEDA%2520operates%2520within%2520a%2520linearized%2520latent%2520space%2520that%250Aadheres%2520to%2520the%2520diffeomorphisms%2520group%2520action%2520laws%252C%2520enhancing%2520our%2520model%2527s%250Arobustness%2520and%2520applicability.%2520We%2520also%2520introduce%2520a%2520loss%2520function%2520to%2520enforce%250Ainverse%2520consistency%252C%2520ensuring%2520accurate%2520latent%2520representations%2520of%2520deformation%250Afields.%2520Extensive%2520experiments%2520with%2520the%2520OASIS-1%2520dataset%2520demonstrate%2520the%250Aeffectiveness%2520of%2520LEDA%2520in%2520accurately%2520modeling%2520and%2520analyzing%2520complex%2520non-linear%250Adeformations%2520while%2520maintaining%2520inverse%2520consistency.%2520Additionally%252C%2520we%2520evaluate%250Aits%2520ability%2520to%2520capture%2520and%2520incorporate%2520clinical%2520variables%252C%2520enhancing%2520its%250Arelevance%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEDA%3A%20Log-Euclidean%20Diffeomorphic%20Autoencoder%20for%20Efficient%20Statistical%0A%20%20Analysis%20of%20Diffeomorphism&entry.906535625=Krithika%20Iyer%20and%20Shireen%20Elhabian%20and%20Sarang%20Joshi&entry.1292438233=%20%20Image%20registration%20is%20a%20core%20task%20in%20computational%20anatomy%20that%20establishes%0Acorrespondences%20between%20images.%20Invertible%20deformable%20registration%2C%20which%0Acomputes%20a%20deformation%20field%20and%20handles%20complex%2C%20non-linear%20transformation%2C%20is%0Aessential%20for%20tracking%20anatomical%20variations%2C%20especially%20in%20neuroimaging%0Aapplications%20where%20inter-subject%20differences%20and%20longitudinal%20changes%20are%20key.%0AAnalyzing%20the%20deformation%20fields%20is%20challenging%20due%20to%20their%20non-linearity%2C%0Alimiting%20statistical%20analysis.%20However%2C%20traditional%20approaches%20for%20analyzing%0Adeformation%20fields%20are%20computationally%20expensive%2C%20sensitive%20to%20initialization%2C%0Aand%20prone%20to%20numerical%20errors%2C%20especially%20when%20the%20deformation%20is%20far%20from%20the%0Aidentity.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Log-Euclidean%0ADiffeomorphic%20Autoencoder%20%28LEDA%29%2C%20an%20innovative%20framework%20designed%20to%20compute%0Athe%20principal%20logarithm%20of%20deformation%20fields%20by%20efficiently%20predicting%0Aconsecutive%20square%20roots.%20LEDA%20operates%20within%20a%20linearized%20latent%20space%20that%0Aadheres%20to%20the%20diffeomorphisms%20group%20action%20laws%2C%20enhancing%20our%20model%27s%0Arobustness%20and%20applicability.%20We%20also%20introduce%20a%20loss%20function%20to%20enforce%0Ainverse%20consistency%2C%20ensuring%20accurate%20latent%20representations%20of%20deformation%0Afields.%20Extensive%20experiments%20with%20the%20OASIS-1%20dataset%20demonstrate%20the%0Aeffectiveness%20of%20LEDA%20in%20accurately%20modeling%20and%20analyzing%20complex%20non-linear%0Adeformations%20while%20maintaining%20inverse%20consistency.%20Additionally%2C%20we%20evaluate%0Aits%20ability%20to%20capture%20and%20incorporate%20clinical%20variables%2C%20enhancing%20its%0Arelevance%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16129v1&entry.124074799=Read"},
{"title": "Joint Supervised and Self-supervised Learning for MRI Reconstruction", "author": "George Yiasemis and Nikita Moriakov and Clara I. S\u00e1nchez and Jan-Jakob Sonke and Jonas Teuwen", "abstract": "  Magnetic Resonance Imaging (MRI) represents an important diagnostic modality;\nhowever, its inherently slow acquisition process poses challenges in obtaining\nfully-sampled $k$-space data under motion. In the absence of fully-sampled\nacquisitions, serving as ground truths, training deep learning algorithms in a\nsupervised manner to predict the underlying ground truth image becomes\nchallenging. To address this limitation, self-supervised methods have emerged\nas a viable alternative, leveraging available subsampled $k$-space data to\ntrain deep neural networks for MRI reconstruction. Nevertheless, these\napproaches often fall short when compared to supervised methods. We propose\nJoint Supervised and Self-supervised Learning (JSSL), a novel training approach\nfor deep learning-based MRI reconstruction algorithms aimed at enhancing\nreconstruction quality in cases where target datasets containing fully-sampled\n$k$-space measurements are unavailable. JSSL operates by simultaneously\ntraining a model in a self-supervised learning setting, using subsampled data\nfrom the target dataset(s), and in a supervised learning manner, utilizing\ndatasets with fully-sampled $k$-space data, referred to as proxy datasets. We\ndemonstrate JSSL's efficacy using subsampled prostate or cardiac MRI data as\nthe target datasets, with fully-sampled brain and knee, or brain, knee and\nprostate $k$-space acquisitions, respectively, as proxy datasets. Our results\nshowcase substantial improvements over conventional self-supervised methods,\nvalidated using common image quality metrics. Furthermore, we provide\ntheoretical motivations for JSSL and establish \"rule-of-thumb\" guidelines for\ntraining MRI reconstruction models. JSSL effectively enhances MRI\nreconstruction quality in scenarios where fully-sampled $k$-space data is not\navailable, leveraging the strengths of supervised learning by incorporating\nproxy datasets.\n", "link": "http://arxiv.org/abs/2311.15856v3", "date": "2024-12-20", "relevancy": 2.1465, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.532}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%20Reconstruction&body=Title%3A%20Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%20Reconstruction%0AAuthor%3A%20George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Clara%20I.%20S%C3%A1nchez%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20represents%20an%20important%20diagnostic%20modality%3B%0Ahowever%2C%20its%20inherently%20slow%20acquisition%20process%20poses%20challenges%20in%20obtaining%0Afully-sampled%20%24k%24-space%20data%20under%20motion.%20In%20the%20absence%20of%20fully-sampled%0Aacquisitions%2C%20serving%20as%20ground%20truths%2C%20training%20deep%20learning%20algorithms%20in%20a%0Asupervised%20manner%20to%20predict%20the%20underlying%20ground%20truth%20image%20becomes%0Achallenging.%20To%20address%20this%20limitation%2C%20self-supervised%20methods%20have%20emerged%0Aas%20a%20viable%20alternative%2C%20leveraging%20available%20subsampled%20%24k%24-space%20data%20to%0Atrain%20deep%20neural%20networks%20for%20MRI%20reconstruction.%20Nevertheless%2C%20these%0Aapproaches%20often%20fall%20short%20when%20compared%20to%20supervised%20methods.%20We%20propose%0AJoint%20Supervised%20and%20Self-supervised%20Learning%20%28JSSL%29%2C%20a%20novel%20training%20approach%0Afor%20deep%20learning-based%20MRI%20reconstruction%20algorithms%20aimed%20at%20enhancing%0Areconstruction%20quality%20in%20cases%20where%20target%20datasets%20containing%20fully-sampled%0A%24k%24-space%20measurements%20are%20unavailable.%20JSSL%20operates%20by%20simultaneously%0Atraining%20a%20model%20in%20a%20self-supervised%20learning%20setting%2C%20using%20subsampled%20data%0Afrom%20the%20target%20dataset%28s%29%2C%20and%20in%20a%20supervised%20learning%20manner%2C%20utilizing%0Adatasets%20with%20fully-sampled%20%24k%24-space%20data%2C%20referred%20to%20as%20proxy%20datasets.%20We%0Ademonstrate%20JSSL%27s%20efficacy%20using%20subsampled%20prostate%20or%20cardiac%20MRI%20data%20as%0Athe%20target%20datasets%2C%20with%20fully-sampled%20brain%20and%20knee%2C%20or%20brain%2C%20knee%20and%0Aprostate%20%24k%24-space%20acquisitions%2C%20respectively%2C%20as%20proxy%20datasets.%20Our%20results%0Ashowcase%20substantial%20improvements%20over%20conventional%20self-supervised%20methods%2C%0Avalidated%20using%20common%20image%20quality%20metrics.%20Furthermore%2C%20we%20provide%0Atheoretical%20motivations%20for%20JSSL%20and%20establish%20%22rule-of-thumb%22%20guidelines%20for%0Atraining%20MRI%20reconstruction%20models.%20JSSL%20effectively%20enhances%20MRI%0Areconstruction%20quality%20in%20scenarios%20where%20fully-sampled%20%24k%24-space%20data%20is%20not%0Aavailable%2C%20leveraging%20the%20strengths%20of%20supervised%20learning%20by%20incorporating%0Aproxy%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15856v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Supervised%2520and%2520Self-supervised%2520Learning%2520for%2520MRI%2520Reconstruction%26entry.906535625%3DGeorge%2520Yiasemis%2520and%2520Nikita%2520Moriakov%2520and%2520Clara%2520I.%2520S%25C3%25A1nchez%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Jonas%2520Teuwen%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520represents%2520an%2520important%2520diagnostic%2520modality%253B%250Ahowever%252C%2520its%2520inherently%2520slow%2520acquisition%2520process%2520poses%2520challenges%2520in%2520obtaining%250Afully-sampled%2520%2524k%2524-space%2520data%2520under%2520motion.%2520In%2520the%2520absence%2520of%2520fully-sampled%250Aacquisitions%252C%2520serving%2520as%2520ground%2520truths%252C%2520training%2520deep%2520learning%2520algorithms%2520in%2520a%250Asupervised%2520manner%2520to%2520predict%2520the%2520underlying%2520ground%2520truth%2520image%2520becomes%250Achallenging.%2520To%2520address%2520this%2520limitation%252C%2520self-supervised%2520methods%2520have%2520emerged%250Aas%2520a%2520viable%2520alternative%252C%2520leveraging%2520available%2520subsampled%2520%2524k%2524-space%2520data%2520to%250Atrain%2520deep%2520neural%2520networks%2520for%2520MRI%2520reconstruction.%2520Nevertheless%252C%2520these%250Aapproaches%2520often%2520fall%2520short%2520when%2520compared%2520to%2520supervised%2520methods.%2520We%2520propose%250AJoint%2520Supervised%2520and%2520Self-supervised%2520Learning%2520%2528JSSL%2529%252C%2520a%2520novel%2520training%2520approach%250Afor%2520deep%2520learning-based%2520MRI%2520reconstruction%2520algorithms%2520aimed%2520at%2520enhancing%250Areconstruction%2520quality%2520in%2520cases%2520where%2520target%2520datasets%2520containing%2520fully-sampled%250A%2524k%2524-space%2520measurements%2520are%2520unavailable.%2520JSSL%2520operates%2520by%2520simultaneously%250Atraining%2520a%2520model%2520in%2520a%2520self-supervised%2520learning%2520setting%252C%2520using%2520subsampled%2520data%250Afrom%2520the%2520target%2520dataset%2528s%2529%252C%2520and%2520in%2520a%2520supervised%2520learning%2520manner%252C%2520utilizing%250Adatasets%2520with%2520fully-sampled%2520%2524k%2524-space%2520data%252C%2520referred%2520to%2520as%2520proxy%2520datasets.%2520We%250Ademonstrate%2520JSSL%2527s%2520efficacy%2520using%2520subsampled%2520prostate%2520or%2520cardiac%2520MRI%2520data%2520as%250Athe%2520target%2520datasets%252C%2520with%2520fully-sampled%2520brain%2520and%2520knee%252C%2520or%2520brain%252C%2520knee%2520and%250Aprostate%2520%2524k%2524-space%2520acquisitions%252C%2520respectively%252C%2520as%2520proxy%2520datasets.%2520Our%2520results%250Ashowcase%2520substantial%2520improvements%2520over%2520conventional%2520self-supervised%2520methods%252C%250Avalidated%2520using%2520common%2520image%2520quality%2520metrics.%2520Furthermore%252C%2520we%2520provide%250Atheoretical%2520motivations%2520for%2520JSSL%2520and%2520establish%2520%2522rule-of-thumb%2522%2520guidelines%2520for%250Atraining%2520MRI%2520reconstruction%2520models.%2520JSSL%2520effectively%2520enhances%2520MRI%250Areconstruction%2520quality%2520in%2520scenarios%2520where%2520fully-sampled%2520%2524k%2524-space%2520data%2520is%2520not%250Aavailable%252C%2520leveraging%2520the%2520strengths%2520of%2520supervised%2520learning%2520by%2520incorporating%250Aproxy%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15856v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Supervised%20and%20Self-supervised%20Learning%20for%20MRI%20Reconstruction&entry.906535625=George%20Yiasemis%20and%20Nikita%20Moriakov%20and%20Clara%20I.%20S%C3%A1nchez%20and%20Jan-Jakob%20Sonke%20and%20Jonas%20Teuwen&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20represents%20an%20important%20diagnostic%20modality%3B%0Ahowever%2C%20its%20inherently%20slow%20acquisition%20process%20poses%20challenges%20in%20obtaining%0Afully-sampled%20%24k%24-space%20data%20under%20motion.%20In%20the%20absence%20of%20fully-sampled%0Aacquisitions%2C%20serving%20as%20ground%20truths%2C%20training%20deep%20learning%20algorithms%20in%20a%0Asupervised%20manner%20to%20predict%20the%20underlying%20ground%20truth%20image%20becomes%0Achallenging.%20To%20address%20this%20limitation%2C%20self-supervised%20methods%20have%20emerged%0Aas%20a%20viable%20alternative%2C%20leveraging%20available%20subsampled%20%24k%24-space%20data%20to%0Atrain%20deep%20neural%20networks%20for%20MRI%20reconstruction.%20Nevertheless%2C%20these%0Aapproaches%20often%20fall%20short%20when%20compared%20to%20supervised%20methods.%20We%20propose%0AJoint%20Supervised%20and%20Self-supervised%20Learning%20%28JSSL%29%2C%20a%20novel%20training%20approach%0Afor%20deep%20learning-based%20MRI%20reconstruction%20algorithms%20aimed%20at%20enhancing%0Areconstruction%20quality%20in%20cases%20where%20target%20datasets%20containing%20fully-sampled%0A%24k%24-space%20measurements%20are%20unavailable.%20JSSL%20operates%20by%20simultaneously%0Atraining%20a%20model%20in%20a%20self-supervised%20learning%20setting%2C%20using%20subsampled%20data%0Afrom%20the%20target%20dataset%28s%29%2C%20and%20in%20a%20supervised%20learning%20manner%2C%20utilizing%0Adatasets%20with%20fully-sampled%20%24k%24-space%20data%2C%20referred%20to%20as%20proxy%20datasets.%20We%0Ademonstrate%20JSSL%27s%20efficacy%20using%20subsampled%20prostate%20or%20cardiac%20MRI%20data%20as%0Athe%20target%20datasets%2C%20with%20fully-sampled%20brain%20and%20knee%2C%20or%20brain%2C%20knee%20and%0Aprostate%20%24k%24-space%20acquisitions%2C%20respectively%2C%20as%20proxy%20datasets.%20Our%20results%0Ashowcase%20substantial%20improvements%20over%20conventional%20self-supervised%20methods%2C%0Avalidated%20using%20common%20image%20quality%20metrics.%20Furthermore%2C%20we%20provide%0Atheoretical%20motivations%20for%20JSSL%20and%20establish%20%22rule-of-thumb%22%20guidelines%20for%0Atraining%20MRI%20reconstruction%20models.%20JSSL%20effectively%20enhances%20MRI%0Areconstruction%20quality%20in%20scenarios%20where%20fully-sampled%20%24k%24-space%20data%20is%20not%0Aavailable%2C%20leveraging%20the%20strengths%20of%20supervised%20learning%20by%20incorporating%0Aproxy%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15856v3&entry.124074799=Read"},
{"title": "Demystifying the Potential of ChatGPT-4 Vision for Construction Progress\n  Monitoring", "author": "Ahmet Bahaddin Ersoz", "abstract": "  The integration of Large Vision-Language Models (LVLMs) such as OpenAI's\nGPT-4 Vision into various sectors has marked a significant evolution in the\nfield of artificial intelligence, particularly in the analysis and\ninterpretation of visual data. This paper explores the practical application of\nGPT-4 Vision in the construction industry, focusing on its capabilities in\nmonitoring and tracking the progress of construction projects. Utilizing\nhigh-resolution aerial imagery of construction sites, the study examines how\nGPT-4 Vision performs detailed scene analysis and tracks developmental changes\nover time. The findings demonstrate that while GPT-4 Vision is proficient in\nidentifying construction stages, materials, and machinery, it faces challenges\nwith precise object localization and segmentation. Despite these limitations,\nthe potential for future advancements in this technology is considerable. This\nresearch not only highlights the current state and opportunities of using LVLMs\nin construction but also discusses future directions for enhancing the model's\nutility through domain-specific training and integration with other computer\nvision techniques and digital twins.\n", "link": "http://arxiv.org/abs/2412.16108v1", "date": "2024-12-20", "relevancy": 2.1294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Potential%20of%20ChatGPT-4%20Vision%20for%20Construction%20Progress%0A%20%20Monitoring&body=Title%3A%20Demystifying%20the%20Potential%20of%20ChatGPT-4%20Vision%20for%20Construction%20Progress%0A%20%20Monitoring%0AAuthor%3A%20Ahmet%20Bahaddin%20Ersoz%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20such%20as%20OpenAI%27s%0AGPT-4%20Vision%20into%20various%20sectors%20has%20marked%20a%20significant%20evolution%20in%20the%0Afield%20of%20artificial%20intelligence%2C%20particularly%20in%20the%20analysis%20and%0Ainterpretation%20of%20visual%20data.%20This%20paper%20explores%20the%20practical%20application%20of%0AGPT-4%20Vision%20in%20the%20construction%20industry%2C%20focusing%20on%20its%20capabilities%20in%0Amonitoring%20and%20tracking%20the%20progress%20of%20construction%20projects.%20Utilizing%0Ahigh-resolution%20aerial%20imagery%20of%20construction%20sites%2C%20the%20study%20examines%20how%0AGPT-4%20Vision%20performs%20detailed%20scene%20analysis%20and%20tracks%20developmental%20changes%0Aover%20time.%20The%20findings%20demonstrate%20that%20while%20GPT-4%20Vision%20is%20proficient%20in%0Aidentifying%20construction%20stages%2C%20materials%2C%20and%20machinery%2C%20it%20faces%20challenges%0Awith%20precise%20object%20localization%20and%20segmentation.%20Despite%20these%20limitations%2C%0Athe%20potential%20for%20future%20advancements%20in%20this%20technology%20is%20considerable.%20This%0Aresearch%20not%20only%20highlights%20the%20current%20state%20and%20opportunities%20of%20using%20LVLMs%0Ain%20construction%20but%20also%20discusses%20future%20directions%20for%20enhancing%20the%20model%27s%0Autility%20through%20domain-specific%20training%20and%20integration%20with%20other%20computer%0Avision%20techniques%20and%20digital%20twins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Potential%2520of%2520ChatGPT-4%2520Vision%2520for%2520Construction%2520Progress%250A%2520%2520Monitoring%26entry.906535625%3DAhmet%2520Bahaddin%2520Ersoz%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520such%2520as%2520OpenAI%2527s%250AGPT-4%2520Vision%2520into%2520various%2520sectors%2520has%2520marked%2520a%2520significant%2520evolution%2520in%2520the%250Afield%2520of%2520artificial%2520intelligence%252C%2520particularly%2520in%2520the%2520analysis%2520and%250Ainterpretation%2520of%2520visual%2520data.%2520This%2520paper%2520explores%2520the%2520practical%2520application%2520of%250AGPT-4%2520Vision%2520in%2520the%2520construction%2520industry%252C%2520focusing%2520on%2520its%2520capabilities%2520in%250Amonitoring%2520and%2520tracking%2520the%2520progress%2520of%2520construction%2520projects.%2520Utilizing%250Ahigh-resolution%2520aerial%2520imagery%2520of%2520construction%2520sites%252C%2520the%2520study%2520examines%2520how%250AGPT-4%2520Vision%2520performs%2520detailed%2520scene%2520analysis%2520and%2520tracks%2520developmental%2520changes%250Aover%2520time.%2520The%2520findings%2520demonstrate%2520that%2520while%2520GPT-4%2520Vision%2520is%2520proficient%2520in%250Aidentifying%2520construction%2520stages%252C%2520materials%252C%2520and%2520machinery%252C%2520it%2520faces%2520challenges%250Awith%2520precise%2520object%2520localization%2520and%2520segmentation.%2520Despite%2520these%2520limitations%252C%250Athe%2520potential%2520for%2520future%2520advancements%2520in%2520this%2520technology%2520is%2520considerable.%2520This%250Aresearch%2520not%2520only%2520highlights%2520the%2520current%2520state%2520and%2520opportunities%2520of%2520using%2520LVLMs%250Ain%2520construction%2520but%2520also%2520discusses%2520future%2520directions%2520for%2520enhancing%2520the%2520model%2527s%250Autility%2520through%2520domain-specific%2520training%2520and%2520integration%2520with%2520other%2520computer%250Avision%2520techniques%2520and%2520digital%2520twins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Potential%20of%20ChatGPT-4%20Vision%20for%20Construction%20Progress%0A%20%20Monitoring&entry.906535625=Ahmet%20Bahaddin%20Ersoz&entry.1292438233=%20%20The%20integration%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20such%20as%20OpenAI%27s%0AGPT-4%20Vision%20into%20various%20sectors%20has%20marked%20a%20significant%20evolution%20in%20the%0Afield%20of%20artificial%20intelligence%2C%20particularly%20in%20the%20analysis%20and%0Ainterpretation%20of%20visual%20data.%20This%20paper%20explores%20the%20practical%20application%20of%0AGPT-4%20Vision%20in%20the%20construction%20industry%2C%20focusing%20on%20its%20capabilities%20in%0Amonitoring%20and%20tracking%20the%20progress%20of%20construction%20projects.%20Utilizing%0Ahigh-resolution%20aerial%20imagery%20of%20construction%20sites%2C%20the%20study%20examines%20how%0AGPT-4%20Vision%20performs%20detailed%20scene%20analysis%20and%20tracks%20developmental%20changes%0Aover%20time.%20The%20findings%20demonstrate%20that%20while%20GPT-4%20Vision%20is%20proficient%20in%0Aidentifying%20construction%20stages%2C%20materials%2C%20and%20machinery%2C%20it%20faces%20challenges%0Awith%20precise%20object%20localization%20and%20segmentation.%20Despite%20these%20limitations%2C%0Athe%20potential%20for%20future%20advancements%20in%20this%20technology%20is%20considerable.%20This%0Aresearch%20not%20only%20highlights%20the%20current%20state%20and%20opportunities%20of%20using%20LVLMs%0Ain%20construction%20but%20also%20discusses%20future%20directions%20for%20enhancing%20the%20model%27s%0Autility%20through%20domain-specific%20training%20and%20integration%20with%20other%20computer%0Avision%20techniques%20and%20digital%20twins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16108v1&entry.124074799=Read"},
{"title": "The common ground of DAE approaches. An overview of diverse DAE\n  frameworks emphasizing their commonalities", "author": "Diana Est\u00e9vez Schwarz and Ren\u00e9 Lamour and Roswitha M\u00e4rz", "abstract": "  We analyze different approaches to differential-algebraic equations with\nattention to the implemented rank conditions of various matrix functions. These\nconditions are apparently very different and certain rank drops in some matrix\nfunctions actually indicate a critical solution behavior. We look for common\nground by considering various index and regularity notions from literature\ngeneralizing the Kronecker index of regular matrix pencils. In detail, starting\nfrom the most transparent reduction framework, we work out a comprehensive\nregularity concept with canonical characteristic values applicable across all\nframeworks and prove the equivalence of thirteen distinct definitions of\nregularity. This makes it possible to use the findings of all these concepts\ntogether. Additionally, we show why not only the index but also these canonical\ncharacteristic values are crucial to describe the properties of the DAE.\n", "link": "http://arxiv.org/abs/2412.15866v1", "date": "2024-12-20", "relevancy": 2.1153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20common%20ground%20of%20DAE%20approaches.%20An%20overview%20of%20diverse%20DAE%0A%20%20frameworks%20emphasizing%20their%20commonalities&body=Title%3A%20The%20common%20ground%20of%20DAE%20approaches.%20An%20overview%20of%20diverse%20DAE%0A%20%20frameworks%20emphasizing%20their%20commonalities%0AAuthor%3A%20Diana%20Est%C3%A9vez%20Schwarz%20and%20Ren%C3%A9%20Lamour%20and%20Roswitha%20M%C3%A4rz%0AAbstract%3A%20%20%20We%20analyze%20different%20approaches%20to%20differential-algebraic%20equations%20with%0Aattention%20to%20the%20implemented%20rank%20conditions%20of%20various%20matrix%20functions.%20These%0Aconditions%20are%20apparently%20very%20different%20and%20certain%20rank%20drops%20in%20some%20matrix%0Afunctions%20actually%20indicate%20a%20critical%20solution%20behavior.%20We%20look%20for%20common%0Aground%20by%20considering%20various%20index%20and%20regularity%20notions%20from%20literature%0Ageneralizing%20the%20Kronecker%20index%20of%20regular%20matrix%20pencils.%20In%20detail%2C%20starting%0Afrom%20the%20most%20transparent%20reduction%20framework%2C%20we%20work%20out%20a%20comprehensive%0Aregularity%20concept%20with%20canonical%20characteristic%20values%20applicable%20across%20all%0Aframeworks%20and%20prove%20the%20equivalence%20of%20thirteen%20distinct%20definitions%20of%0Aregularity.%20This%20makes%20it%20possible%20to%20use%20the%20findings%20of%20all%20these%20concepts%0Atogether.%20Additionally%2C%20we%20show%20why%20not%20only%20the%20index%20but%20also%20these%20canonical%0Acharacteristic%20values%20are%20crucial%20to%20describe%20the%20properties%20of%20the%20DAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520common%2520ground%2520of%2520DAE%2520approaches.%2520An%2520overview%2520of%2520diverse%2520DAE%250A%2520%2520frameworks%2520emphasizing%2520their%2520commonalities%26entry.906535625%3DDiana%2520Est%25C3%25A9vez%2520Schwarz%2520and%2520Ren%25C3%25A9%2520Lamour%2520and%2520Roswitha%2520M%25C3%25A4rz%26entry.1292438233%3D%2520%2520We%2520analyze%2520different%2520approaches%2520to%2520differential-algebraic%2520equations%2520with%250Aattention%2520to%2520the%2520implemented%2520rank%2520conditions%2520of%2520various%2520matrix%2520functions.%2520These%250Aconditions%2520are%2520apparently%2520very%2520different%2520and%2520certain%2520rank%2520drops%2520in%2520some%2520matrix%250Afunctions%2520actually%2520indicate%2520a%2520critical%2520solution%2520behavior.%2520We%2520look%2520for%2520common%250Aground%2520by%2520considering%2520various%2520index%2520and%2520regularity%2520notions%2520from%2520literature%250Ageneralizing%2520the%2520Kronecker%2520index%2520of%2520regular%2520matrix%2520pencils.%2520In%2520detail%252C%2520starting%250Afrom%2520the%2520most%2520transparent%2520reduction%2520framework%252C%2520we%2520work%2520out%2520a%2520comprehensive%250Aregularity%2520concept%2520with%2520canonical%2520characteristic%2520values%2520applicable%2520across%2520all%250Aframeworks%2520and%2520prove%2520the%2520equivalence%2520of%2520thirteen%2520distinct%2520definitions%2520of%250Aregularity.%2520This%2520makes%2520it%2520possible%2520to%2520use%2520the%2520findings%2520of%2520all%2520these%2520concepts%250Atogether.%2520Additionally%252C%2520we%2520show%2520why%2520not%2520only%2520the%2520index%2520but%2520also%2520these%2520canonical%250Acharacteristic%2520values%2520are%2520crucial%2520to%2520describe%2520the%2520properties%2520of%2520the%2520DAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20common%20ground%20of%20DAE%20approaches.%20An%20overview%20of%20diverse%20DAE%0A%20%20frameworks%20emphasizing%20their%20commonalities&entry.906535625=Diana%20Est%C3%A9vez%20Schwarz%20and%20Ren%C3%A9%20Lamour%20and%20Roswitha%20M%C3%A4rz&entry.1292438233=%20%20We%20analyze%20different%20approaches%20to%20differential-algebraic%20equations%20with%0Aattention%20to%20the%20implemented%20rank%20conditions%20of%20various%20matrix%20functions.%20These%0Aconditions%20are%20apparently%20very%20different%20and%20certain%20rank%20drops%20in%20some%20matrix%0Afunctions%20actually%20indicate%20a%20critical%20solution%20behavior.%20We%20look%20for%20common%0Aground%20by%20considering%20various%20index%20and%20regularity%20notions%20from%20literature%0Ageneralizing%20the%20Kronecker%20index%20of%20regular%20matrix%20pencils.%20In%20detail%2C%20starting%0Afrom%20the%20most%20transparent%20reduction%20framework%2C%20we%20work%20out%20a%20comprehensive%0Aregularity%20concept%20with%20canonical%20characteristic%20values%20applicable%20across%20all%0Aframeworks%20and%20prove%20the%20equivalence%20of%20thirteen%20distinct%20definitions%20of%0Aregularity.%20This%20makes%20it%20possible%20to%20use%20the%20findings%20of%20all%20these%20concepts%0Atogether.%20Additionally%2C%20we%20show%20why%20not%20only%20the%20index%20but%20also%20these%20canonical%0Acharacteristic%20values%20are%20crucial%20to%20describe%20the%20properties%20of%20the%20DAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15866v1&entry.124074799=Read"},
{"title": "Efficient MedSAMs: Segment Anything in Medical Images on Laptop", "author": "Jun Ma and Feifei Li and Sumin Kim and Reza Asakereh and Bao-Hiep Le and Dang-Khoa Nguyen-Vu and Alexander Pfefferle and Muxin Wei and Ruochen Gao and Donghang Lyu and Songxiao Yang and Lennart Purucker and Zdravko Marinov and Marius Staring and Haisheng Lu and Thuy Thanh Dao and Xincheng Ye and Zhi Li and Gianluca Brugnara and Philipp Vollmuth and Martha Foltyn-Dumitru and Jaeyoung Cho and Mustafa Ahmed Mahmutoglu and Martin Bendszus and Irada Pfl\u00fcger and Aditya Rastogi and Dong Ni and Xin Yang and Guang-Quan Zhou and Kaini Wang and Nicholas Heller and Nikolaos Papanikolopoulos and Christopher Weight and Yubing Tong and Jayaram K Udupa and Cahill J. Patrick and Yaqi Wang and Yifan Zhang and Francisco Contijoch and Elliot McVeigh and Xin Ye and Shucheng He and Robert Haase and Thomas Pinetz and Alexander Radbruch and Inga Krause and Erich Kobler and Jian He and Yucheng Tang and Haichun Yang and Yuankai Huo and Gongning Luo and Kaisar Kushibar and Jandos Amankulov and Dias Toleshbayev and Amangeldi Mukhamejan and Jan Egger and Antonio Pepe and Christina Gsaxner and Gijs Luijten and Shohei Fujita and Tomohiro Kikuchi and Benedikt Wiestler and Jan S. Kirschke and Ezequiel de la Rosa and Federico Bolelli and Luca Lumetti and Costantino Grana and Kunpeng Xie and Guomin Wu and Behrus Puladi and Carlos Mart\u00edn-Isla and Karim Lekadir and Victor M. Campello and Wei Shao and Wayne Brisbane and Hongxu Jiang and Hao Wei and Wu Yuan and Shuangle Li and Yuyin Zhou and Bo Wang", "abstract": "  Promptable segmentation foundation models have emerged as a transformative\napproach to addressing the diverse needs in medical images, but most existing\nmodels require expensive computing, posing a big barrier to their adoption in\nclinical practice. In this work, we organized the first international\ncompetition dedicated to promptable medical image segmentation, featuring a\nlarge-scale dataset spanning nine common imaging modalities from over 20\ndifferent institutions. The top teams developed lightweight segmentation\nfoundation models and implemented an efficient inference pipeline that\nsubstantially reduced computational requirements while maintaining\nstate-of-the-art segmentation accuracy. Moreover, the post-challenge phase\nadvanced the algorithms through the design of performance booster and\nreproducibility tasks, resulting in improved algorithms and validated\nreproducibility of the winning solution. Furthermore, the best-performing\nalgorithms have been incorporated into the open-source software with a\nuser-friendly interface to facilitate clinical adoption. The data and code are\npublicly available to foster the further development of medical image\nsegmentation foundation models and pave the way for impactful real-world\napplications.\n", "link": "http://arxiv.org/abs/2412.16085v1", "date": "2024-12-20", "relevancy": 2.1135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5652}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20MedSAMs%3A%20Segment%20Anything%20in%20Medical%20Images%20on%20Laptop&body=Title%3A%20Efficient%20MedSAMs%3A%20Segment%20Anything%20in%20Medical%20Images%20on%20Laptop%0AAuthor%3A%20Jun%20Ma%20and%20Feifei%20Li%20and%20Sumin%20Kim%20and%20Reza%20Asakereh%20and%20Bao-Hiep%20Le%20and%20Dang-Khoa%20Nguyen-Vu%20and%20Alexander%20Pfefferle%20and%20Muxin%20Wei%20and%20Ruochen%20Gao%20and%20Donghang%20Lyu%20and%20Songxiao%20Yang%20and%20Lennart%20Purucker%20and%20Zdravko%20Marinov%20and%20Marius%20Staring%20and%20Haisheng%20Lu%20and%20Thuy%20Thanh%20Dao%20and%20Xincheng%20Ye%20and%20Zhi%20Li%20and%20Gianluca%20Brugnara%20and%20Philipp%20Vollmuth%20and%20Martha%20Foltyn-Dumitru%20and%20Jaeyoung%20Cho%20and%20Mustafa%20Ahmed%20Mahmutoglu%20and%20Martin%20Bendszus%20and%20Irada%20Pfl%C3%BCger%20and%20Aditya%20Rastogi%20and%20Dong%20Ni%20and%20Xin%20Yang%20and%20Guang-Quan%20Zhou%20and%20Kaini%20Wang%20and%20Nicholas%20Heller%20and%20Nikolaos%20Papanikolopoulos%20and%20Christopher%20Weight%20and%20Yubing%20Tong%20and%20Jayaram%20K%20Udupa%20and%20Cahill%20J.%20Patrick%20and%20Yaqi%20Wang%20and%20Yifan%20Zhang%20and%20Francisco%20Contijoch%20and%20Elliot%20McVeigh%20and%20Xin%20Ye%20and%20Shucheng%20He%20and%20Robert%20Haase%20and%20Thomas%20Pinetz%20and%20Alexander%20Radbruch%20and%20Inga%20Krause%20and%20Erich%20Kobler%20and%20Jian%20He%20and%20Yucheng%20Tang%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%20and%20Gongning%20Luo%20and%20Kaisar%20Kushibar%20and%20Jandos%20Amankulov%20and%20Dias%20Toleshbayev%20and%20Amangeldi%20Mukhamejan%20and%20Jan%20Egger%20and%20Antonio%20Pepe%20and%20Christina%20Gsaxner%20and%20Gijs%20Luijten%20and%20Shohei%20Fujita%20and%20Tomohiro%20Kikuchi%20and%20Benedikt%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Ezequiel%20de%20la%20Rosa%20and%20Federico%20Bolelli%20and%20Luca%20Lumetti%20and%20Costantino%20Grana%20and%20Kunpeng%20Xie%20and%20Guomin%20Wu%20and%20Behrus%20Puladi%20and%20Carlos%20Mart%C3%ADn-Isla%20and%20Karim%20Lekadir%20and%20Victor%20M.%20Campello%20and%20Wei%20Shao%20and%20Wayne%20Brisbane%20and%20Hongxu%20Jiang%20and%20Hao%20Wei%20and%20Wu%20Yuan%20and%20Shuangle%20Li%20and%20Yuyin%20Zhou%20and%20Bo%20Wang%0AAbstract%3A%20%20%20Promptable%20segmentation%20foundation%20models%20have%20emerged%20as%20a%20transformative%0Aapproach%20to%20addressing%20the%20diverse%20needs%20in%20medical%20images%2C%20but%20most%20existing%0Amodels%20require%20expensive%20computing%2C%20posing%20a%20big%20barrier%20to%20their%20adoption%20in%0Aclinical%20practice.%20In%20this%20work%2C%20we%20organized%20the%20first%20international%0Acompetition%20dedicated%20to%20promptable%20medical%20image%20segmentation%2C%20featuring%20a%0Alarge-scale%20dataset%20spanning%20nine%20common%20imaging%20modalities%20from%20over%2020%0Adifferent%20institutions.%20The%20top%20teams%20developed%20lightweight%20segmentation%0Afoundation%20models%20and%20implemented%20an%20efficient%20inference%20pipeline%20that%0Asubstantially%20reduced%20computational%20requirements%20while%20maintaining%0Astate-of-the-art%20segmentation%20accuracy.%20Moreover%2C%20the%20post-challenge%20phase%0Aadvanced%20the%20algorithms%20through%20the%20design%20of%20performance%20booster%20and%0Areproducibility%20tasks%2C%20resulting%20in%20improved%20algorithms%20and%20validated%0Areproducibility%20of%20the%20winning%20solution.%20Furthermore%2C%20the%20best-performing%0Aalgorithms%20have%20been%20incorporated%20into%20the%20open-source%20software%20with%20a%0Auser-friendly%20interface%20to%20facilitate%20clinical%20adoption.%20The%20data%20and%20code%20are%0Apublicly%20available%20to%20foster%20the%20further%20development%20of%20medical%20image%0Asegmentation%20foundation%20models%20and%20pave%20the%20way%20for%20impactful%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520MedSAMs%253A%2520Segment%2520Anything%2520in%2520Medical%2520Images%2520on%2520Laptop%26entry.906535625%3DJun%2520Ma%2520and%2520Feifei%2520Li%2520and%2520Sumin%2520Kim%2520and%2520Reza%2520Asakereh%2520and%2520Bao-Hiep%2520Le%2520and%2520Dang-Khoa%2520Nguyen-Vu%2520and%2520Alexander%2520Pfefferle%2520and%2520Muxin%2520Wei%2520and%2520Ruochen%2520Gao%2520and%2520Donghang%2520Lyu%2520and%2520Songxiao%2520Yang%2520and%2520Lennart%2520Purucker%2520and%2520Zdravko%2520Marinov%2520and%2520Marius%2520Staring%2520and%2520Haisheng%2520Lu%2520and%2520Thuy%2520Thanh%2520Dao%2520and%2520Xincheng%2520Ye%2520and%2520Zhi%2520Li%2520and%2520Gianluca%2520Brugnara%2520and%2520Philipp%2520Vollmuth%2520and%2520Martha%2520Foltyn-Dumitru%2520and%2520Jaeyoung%2520Cho%2520and%2520Mustafa%2520Ahmed%2520Mahmutoglu%2520and%2520Martin%2520Bendszus%2520and%2520Irada%2520Pfl%25C3%25BCger%2520and%2520Aditya%2520Rastogi%2520and%2520Dong%2520Ni%2520and%2520Xin%2520Yang%2520and%2520Guang-Quan%2520Zhou%2520and%2520Kaini%2520Wang%2520and%2520Nicholas%2520Heller%2520and%2520Nikolaos%2520Papanikolopoulos%2520and%2520Christopher%2520Weight%2520and%2520Yubing%2520Tong%2520and%2520Jayaram%2520K%2520Udupa%2520and%2520Cahill%2520J.%2520Patrick%2520and%2520Yaqi%2520Wang%2520and%2520Yifan%2520Zhang%2520and%2520Francisco%2520Contijoch%2520and%2520Elliot%2520McVeigh%2520and%2520Xin%2520Ye%2520and%2520Shucheng%2520He%2520and%2520Robert%2520Haase%2520and%2520Thomas%2520Pinetz%2520and%2520Alexander%2520Radbruch%2520and%2520Inga%2520Krause%2520and%2520Erich%2520Kobler%2520and%2520Jian%2520He%2520and%2520Yucheng%2520Tang%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%2520and%2520Gongning%2520Luo%2520and%2520Kaisar%2520Kushibar%2520and%2520Jandos%2520Amankulov%2520and%2520Dias%2520Toleshbayev%2520and%2520Amangeldi%2520Mukhamejan%2520and%2520Jan%2520Egger%2520and%2520Antonio%2520Pepe%2520and%2520Christina%2520Gsaxner%2520and%2520Gijs%2520Luijten%2520and%2520Shohei%2520Fujita%2520and%2520Tomohiro%2520Kikuchi%2520and%2520Benedikt%2520Wiestler%2520and%2520Jan%2520S.%2520Kirschke%2520and%2520Ezequiel%2520de%2520la%2520Rosa%2520and%2520Federico%2520Bolelli%2520and%2520Luca%2520Lumetti%2520and%2520Costantino%2520Grana%2520and%2520Kunpeng%2520Xie%2520and%2520Guomin%2520Wu%2520and%2520Behrus%2520Puladi%2520and%2520Carlos%2520Mart%25C3%25ADn-Isla%2520and%2520Karim%2520Lekadir%2520and%2520Victor%2520M.%2520Campello%2520and%2520Wei%2520Shao%2520and%2520Wayne%2520Brisbane%2520and%2520Hongxu%2520Jiang%2520and%2520Hao%2520Wei%2520and%2520Wu%2520Yuan%2520and%2520Shuangle%2520Li%2520and%2520Yuyin%2520Zhou%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520Promptable%2520segmentation%2520foundation%2520models%2520have%2520emerged%2520as%2520a%2520transformative%250Aapproach%2520to%2520addressing%2520the%2520diverse%2520needs%2520in%2520medical%2520images%252C%2520but%2520most%2520existing%250Amodels%2520require%2520expensive%2520computing%252C%2520posing%2520a%2520big%2520barrier%2520to%2520their%2520adoption%2520in%250Aclinical%2520practice.%2520In%2520this%2520work%252C%2520we%2520organized%2520the%2520first%2520international%250Acompetition%2520dedicated%2520to%2520promptable%2520medical%2520image%2520segmentation%252C%2520featuring%2520a%250Alarge-scale%2520dataset%2520spanning%2520nine%2520common%2520imaging%2520modalities%2520from%2520over%252020%250Adifferent%2520institutions.%2520The%2520top%2520teams%2520developed%2520lightweight%2520segmentation%250Afoundation%2520models%2520and%2520implemented%2520an%2520efficient%2520inference%2520pipeline%2520that%250Asubstantially%2520reduced%2520computational%2520requirements%2520while%2520maintaining%250Astate-of-the-art%2520segmentation%2520accuracy.%2520Moreover%252C%2520the%2520post-challenge%2520phase%250Aadvanced%2520the%2520algorithms%2520through%2520the%2520design%2520of%2520performance%2520booster%2520and%250Areproducibility%2520tasks%252C%2520resulting%2520in%2520improved%2520algorithms%2520and%2520validated%250Areproducibility%2520of%2520the%2520winning%2520solution.%2520Furthermore%252C%2520the%2520best-performing%250Aalgorithms%2520have%2520been%2520incorporated%2520into%2520the%2520open-source%2520software%2520with%2520a%250Auser-friendly%2520interface%2520to%2520facilitate%2520clinical%2520adoption.%2520The%2520data%2520and%2520code%2520are%250Apublicly%2520available%2520to%2520foster%2520the%2520further%2520development%2520of%2520medical%2520image%250Asegmentation%2520foundation%2520models%2520and%2520pave%2520the%2520way%2520for%2520impactful%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20MedSAMs%3A%20Segment%20Anything%20in%20Medical%20Images%20on%20Laptop&entry.906535625=Jun%20Ma%20and%20Feifei%20Li%20and%20Sumin%20Kim%20and%20Reza%20Asakereh%20and%20Bao-Hiep%20Le%20and%20Dang-Khoa%20Nguyen-Vu%20and%20Alexander%20Pfefferle%20and%20Muxin%20Wei%20and%20Ruochen%20Gao%20and%20Donghang%20Lyu%20and%20Songxiao%20Yang%20and%20Lennart%20Purucker%20and%20Zdravko%20Marinov%20and%20Marius%20Staring%20and%20Haisheng%20Lu%20and%20Thuy%20Thanh%20Dao%20and%20Xincheng%20Ye%20and%20Zhi%20Li%20and%20Gianluca%20Brugnara%20and%20Philipp%20Vollmuth%20and%20Martha%20Foltyn-Dumitru%20and%20Jaeyoung%20Cho%20and%20Mustafa%20Ahmed%20Mahmutoglu%20and%20Martin%20Bendszus%20and%20Irada%20Pfl%C3%BCger%20and%20Aditya%20Rastogi%20and%20Dong%20Ni%20and%20Xin%20Yang%20and%20Guang-Quan%20Zhou%20and%20Kaini%20Wang%20and%20Nicholas%20Heller%20and%20Nikolaos%20Papanikolopoulos%20and%20Christopher%20Weight%20and%20Yubing%20Tong%20and%20Jayaram%20K%20Udupa%20and%20Cahill%20J.%20Patrick%20and%20Yaqi%20Wang%20and%20Yifan%20Zhang%20and%20Francisco%20Contijoch%20and%20Elliot%20McVeigh%20and%20Xin%20Ye%20and%20Shucheng%20He%20and%20Robert%20Haase%20and%20Thomas%20Pinetz%20and%20Alexander%20Radbruch%20and%20Inga%20Krause%20and%20Erich%20Kobler%20and%20Jian%20He%20and%20Yucheng%20Tang%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%20and%20Gongning%20Luo%20and%20Kaisar%20Kushibar%20and%20Jandos%20Amankulov%20and%20Dias%20Toleshbayev%20and%20Amangeldi%20Mukhamejan%20and%20Jan%20Egger%20and%20Antonio%20Pepe%20and%20Christina%20Gsaxner%20and%20Gijs%20Luijten%20and%20Shohei%20Fujita%20and%20Tomohiro%20Kikuchi%20and%20Benedikt%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Ezequiel%20de%20la%20Rosa%20and%20Federico%20Bolelli%20and%20Luca%20Lumetti%20and%20Costantino%20Grana%20and%20Kunpeng%20Xie%20and%20Guomin%20Wu%20and%20Behrus%20Puladi%20and%20Carlos%20Mart%C3%ADn-Isla%20and%20Karim%20Lekadir%20and%20Victor%20M.%20Campello%20and%20Wei%20Shao%20and%20Wayne%20Brisbane%20and%20Hongxu%20Jiang%20and%20Hao%20Wei%20and%20Wu%20Yuan%20and%20Shuangle%20Li%20and%20Yuyin%20Zhou%20and%20Bo%20Wang&entry.1292438233=%20%20Promptable%20segmentation%20foundation%20models%20have%20emerged%20as%20a%20transformative%0Aapproach%20to%20addressing%20the%20diverse%20needs%20in%20medical%20images%2C%20but%20most%20existing%0Amodels%20require%20expensive%20computing%2C%20posing%20a%20big%20barrier%20to%20their%20adoption%20in%0Aclinical%20practice.%20In%20this%20work%2C%20we%20organized%20the%20first%20international%0Acompetition%20dedicated%20to%20promptable%20medical%20image%20segmentation%2C%20featuring%20a%0Alarge-scale%20dataset%20spanning%20nine%20common%20imaging%20modalities%20from%20over%2020%0Adifferent%20institutions.%20The%20top%20teams%20developed%20lightweight%20segmentation%0Afoundation%20models%20and%20implemented%20an%20efficient%20inference%20pipeline%20that%0Asubstantially%20reduced%20computational%20requirements%20while%20maintaining%0Astate-of-the-art%20segmentation%20accuracy.%20Moreover%2C%20the%20post-challenge%20phase%0Aadvanced%20the%20algorithms%20through%20the%20design%20of%20performance%20booster%20and%0Areproducibility%20tasks%2C%20resulting%20in%20improved%20algorithms%20and%20validated%0Areproducibility%20of%20the%20winning%20solution.%20Furthermore%2C%20the%20best-performing%0Aalgorithms%20have%20been%20incorporated%20into%20the%20open-source%20software%20with%20a%0Auser-friendly%20interface%20to%20facilitate%20clinical%20adoption.%20The%20data%20and%20code%20are%0Apublicly%20available%20to%20foster%20the%20further%20development%20of%20medical%20image%0Asegmentation%20foundation%20models%20and%20pave%20the%20way%20for%20impactful%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16085v1&entry.124074799=Read"},
{"title": "Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture", "author": "Jingze Shi and Bingheng Wu", "abstract": "  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n", "link": "http://arxiv.org/abs/2412.11834v3", "date": "2024-12-20", "relevancy": 2.1052, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture&body=Title%3A%20Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture%0AAuthor%3A%20Jingze%20Shi%20and%20Bingheng%20Wu%0AAbstract%3A%20%20%20In%20order%20to%20make%20the%20foundation%20model%20more%20efficient%20and%20effective%2C%20our%20idea%0Ais%20combining%20sequence%20transformation%20and%20state%20transformation.%20First%2C%20we%20prove%0Athe%20availability%20of%20rotary%20position%20embedding%20in%20the%20state%20space%20duality%0Aalgorithm%2C%20which%20reduces%20the%20perplexity%20of%20the%20hybrid%20quadratic%20causal%0Aself-attention%20and%20state%20space%20duality%20by%20more%20than%204%25%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20unifies%20position%20encoding.%20Second%2C%20we%20propose%0Adynamic%20mask%20attention%2C%20which%20maintains%20100%25%20accuracy%20in%20the%20more%20challenging%0Amulti-query%20associative%20recall%20task%2C%20improving%20by%20more%20than%20150%25%20compared%20to%0Aquadratic%20causal%20self-attention%20and%20state%20space%20duality%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20selectively%20filters%20relevant%20information.%0AThird%2C%20we%20design%20cross%20domain%20mixture%20of%20experts%2C%20which%20makes%20the%20computational%0Aspeed%20of%20expert%20retrieval%20with%20more%20than%201024%20experts%208%20to%2010%20times%20faster%20than%0Athe%20mixture%20of%20experts%2C%20to%20ensure%20that%20the%20combining%20state%20transformation%0Aquickly%20retrieval%20mixture.%20Finally%2C%20we%20summarize%20these%20matrix%20algorithms%20that%0Acan%20form%20the%20foundation%20model%3A%20Wonderful%20Matrices%2C%20which%20can%20be%20a%20competitor%20to%0Apopular%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11834v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderful%2520Matrices%253A%2520Combining%2520for%2520a%2520More%2520Efficient%2520and%2520Effective%250A%2520%2520Foundation%2520Model%2520Architecture%26entry.906535625%3DJingze%2520Shi%2520and%2520Bingheng%2520Wu%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520make%2520the%2520foundation%2520model%2520more%2520efficient%2520and%2520effective%252C%2520our%2520idea%250Ais%2520combining%2520sequence%2520transformation%2520and%2520state%2520transformation.%2520First%252C%2520we%2520prove%250Athe%2520availability%2520of%2520rotary%2520position%2520embedding%2520in%2520the%2520state%2520space%2520duality%250Aalgorithm%252C%2520which%2520reduces%2520the%2520perplexity%2520of%2520the%2520hybrid%2520quadratic%2520causal%250Aself-attention%2520and%2520state%2520space%2520duality%2520by%2520more%2520than%25204%2525%252C%2520to%2520ensure%2520that%2520the%250Acombining%2520sequence%2520transformation%2520unifies%2520position%2520encoding.%2520Second%252C%2520we%2520propose%250Adynamic%2520mask%2520attention%252C%2520which%2520maintains%2520100%2525%2520accuracy%2520in%2520the%2520more%2520challenging%250Amulti-query%2520associative%2520recall%2520task%252C%2520improving%2520by%2520more%2520than%2520150%2525%2520compared%2520to%250Aquadratic%2520causal%2520self-attention%2520and%2520state%2520space%2520duality%252C%2520to%2520ensure%2520that%2520the%250Acombining%2520sequence%2520transformation%2520selectively%2520filters%2520relevant%2520information.%250AThird%252C%2520we%2520design%2520cross%2520domain%2520mixture%2520of%2520experts%252C%2520which%2520makes%2520the%2520computational%250Aspeed%2520of%2520expert%2520retrieval%2520with%2520more%2520than%25201024%2520experts%25208%2520to%252010%2520times%2520faster%2520than%250Athe%2520mixture%2520of%2520experts%252C%2520to%2520ensure%2520that%2520the%2520combining%2520state%2520transformation%250Aquickly%2520retrieval%2520mixture.%2520Finally%252C%2520we%2520summarize%2520these%2520matrix%2520algorithms%2520that%250Acan%2520form%2520the%2520foundation%2520model%253A%2520Wonderful%2520Matrices%252C%2520which%2520can%2520be%2520a%2520competitor%2520to%250Apopular%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11834v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture&entry.906535625=Jingze%20Shi%20and%20Bingheng%20Wu&entry.1292438233=%20%20In%20order%20to%20make%20the%20foundation%20model%20more%20efficient%20and%20effective%2C%20our%20idea%0Ais%20combining%20sequence%20transformation%20and%20state%20transformation.%20First%2C%20we%20prove%0Athe%20availability%20of%20rotary%20position%20embedding%20in%20the%20state%20space%20duality%0Aalgorithm%2C%20which%20reduces%20the%20perplexity%20of%20the%20hybrid%20quadratic%20causal%0Aself-attention%20and%20state%20space%20duality%20by%20more%20than%204%25%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20unifies%20position%20encoding.%20Second%2C%20we%20propose%0Adynamic%20mask%20attention%2C%20which%20maintains%20100%25%20accuracy%20in%20the%20more%20challenging%0Amulti-query%20associative%20recall%20task%2C%20improving%20by%20more%20than%20150%25%20compared%20to%0Aquadratic%20causal%20self-attention%20and%20state%20space%20duality%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20selectively%20filters%20relevant%20information.%0AThird%2C%20we%20design%20cross%20domain%20mixture%20of%20experts%2C%20which%20makes%20the%20computational%0Aspeed%20of%20expert%20retrieval%20with%20more%20than%201024%20experts%208%20to%2010%20times%20faster%20than%0Athe%20mixture%20of%20experts%2C%20to%20ensure%20that%20the%20combining%20state%20transformation%0Aquickly%20retrieval%20mixture.%20Finally%2C%20we%20summarize%20these%20matrix%20algorithms%20that%0Acan%20form%20the%20foundation%20model%3A%20Wonderful%20Matrices%2C%20which%20can%20be%20a%20competitor%20to%0Apopular%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11834v3&entry.124074799=Read"},
{"title": "Learning Low Degree Hypergraphs", "author": "Eric Balkanski and Oussama Hanguir and Shatian Wang", "abstract": "  We study the problem of learning a hypergraph via edge detecting queries. In\nthis problem, a learner queries subsets of vertices of a hidden hypergraph and\nobserves whether these subsets contain an edge or not. In general, learning a\nhypergraph with $m$ edges of maximum size $d$ requires $\\Omega((2m/d)^{d/2})$\nqueries. In this paper, we aim to identify families of hypergraphs that can be\nlearned without suffering from a query complexity that grows exponentially in\nthe size of the edges.\n  We show that hypermatchings and low-degree near-uniform hypergraphs with $n$\nvertices are learnable with poly$(n)$ queries. For learning hypermatchings\n(hypergraphs of maximum degree $ 1$), we give an $O(\\log^3 n)$-round algorithm\nwith $O(n \\log^5 n)$ queries. We complement this upper bound by showing that\nthere are no algorithms with poly$(n)$ queries that learn hypermatchings in\n$o(\\log \\log n)$ adaptive rounds. For hypergraphs with maximum degree $\\Delta$\nand edge size ratio $\\rho$, we give a non-adaptive algorithm with $O((2n)^{\\rho\n\\Delta+1}\\log^2 n)$ queries. To the best of our knowledge, these are the first\nalgorithms with poly$(n, m)$ query complexity for learning non-trivial families\nof hypergraphs that have a super-constant number of edges of super-constant\nsize.\n", "link": "http://arxiv.org/abs/2202.09989v3", "date": "2024-12-20", "relevancy": 2.096, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4338}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4335}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Low%20Degree%20Hypergraphs&body=Title%3A%20Learning%20Low%20Degree%20Hypergraphs%0AAuthor%3A%20Eric%20Balkanski%20and%20Oussama%20Hanguir%20and%20Shatian%20Wang%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20a%20hypergraph%20via%20edge%20detecting%20queries.%20In%0Athis%20problem%2C%20a%20learner%20queries%20subsets%20of%20vertices%20of%20a%20hidden%20hypergraph%20and%0Aobserves%20whether%20these%20subsets%20contain%20an%20edge%20or%20not.%20In%20general%2C%20learning%20a%0Ahypergraph%20with%20%24m%24%20edges%20of%20maximum%20size%20%24d%24%20requires%20%24%5COmega%28%282m/d%29%5E%7Bd/2%7D%29%24%0Aqueries.%20In%20this%20paper%2C%20we%20aim%20to%20identify%20families%20of%20hypergraphs%20that%20can%20be%0Alearned%20without%20suffering%20from%20a%20query%20complexity%20that%20grows%20exponentially%20in%0Athe%20size%20of%20the%20edges.%0A%20%20We%20show%20that%20hypermatchings%20and%20low-degree%20near-uniform%20hypergraphs%20with%20%24n%24%0Avertices%20are%20learnable%20with%20poly%24%28n%29%24%20queries.%20For%20learning%20hypermatchings%0A%28hypergraphs%20of%20maximum%20degree%20%24%201%24%29%2C%20we%20give%20an%20%24O%28%5Clog%5E3%20n%29%24-round%20algorithm%0Awith%20%24O%28n%20%5Clog%5E5%20n%29%24%20queries.%20We%20complement%20this%20upper%20bound%20by%20showing%20that%0Athere%20are%20no%20algorithms%20with%20poly%24%28n%29%24%20queries%20that%20learn%20hypermatchings%20in%0A%24o%28%5Clog%20%5Clog%20n%29%24%20adaptive%20rounds.%20For%20hypergraphs%20with%20maximum%20degree%20%24%5CDelta%24%0Aand%20edge%20size%20ratio%20%24%5Crho%24%2C%20we%20give%20a%20non-adaptive%20algorithm%20with%20%24O%28%282n%29%5E%7B%5Crho%0A%5CDelta%2B1%7D%5Clog%5E2%20n%29%24%20queries.%20To%20the%20best%20of%20our%20knowledge%2C%20these%20are%20the%20first%0Aalgorithms%20with%20poly%24%28n%2C%20m%29%24%20query%20complexity%20for%20learning%20non-trivial%20families%0Aof%20hypergraphs%20that%20have%20a%20super-constant%20number%20of%20edges%20of%20super-constant%0Asize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.09989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Low%2520Degree%2520Hypergraphs%26entry.906535625%3DEric%2520Balkanski%2520and%2520Oussama%2520Hanguir%2520and%2520Shatian%2520Wang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520a%2520hypergraph%2520via%2520edge%2520detecting%2520queries.%2520In%250Athis%2520problem%252C%2520a%2520learner%2520queries%2520subsets%2520of%2520vertices%2520of%2520a%2520hidden%2520hypergraph%2520and%250Aobserves%2520whether%2520these%2520subsets%2520contain%2520an%2520edge%2520or%2520not.%2520In%2520general%252C%2520learning%2520a%250Ahypergraph%2520with%2520%2524m%2524%2520edges%2520of%2520maximum%2520size%2520%2524d%2524%2520requires%2520%2524%255COmega%2528%25282m/d%2529%255E%257Bd/2%257D%2529%2524%250Aqueries.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520identify%2520families%2520of%2520hypergraphs%2520that%2520can%2520be%250Alearned%2520without%2520suffering%2520from%2520a%2520query%2520complexity%2520that%2520grows%2520exponentially%2520in%250Athe%2520size%2520of%2520the%2520edges.%250A%2520%2520We%2520show%2520that%2520hypermatchings%2520and%2520low-degree%2520near-uniform%2520hypergraphs%2520with%2520%2524n%2524%250Avertices%2520are%2520learnable%2520with%2520poly%2524%2528n%2529%2524%2520queries.%2520For%2520learning%2520hypermatchings%250A%2528hypergraphs%2520of%2520maximum%2520degree%2520%2524%25201%2524%2529%252C%2520we%2520give%2520an%2520%2524O%2528%255Clog%255E3%2520n%2529%2524-round%2520algorithm%250Awith%2520%2524O%2528n%2520%255Clog%255E5%2520n%2529%2524%2520queries.%2520We%2520complement%2520this%2520upper%2520bound%2520by%2520showing%2520that%250Athere%2520are%2520no%2520algorithms%2520with%2520poly%2524%2528n%2529%2524%2520queries%2520that%2520learn%2520hypermatchings%2520in%250A%2524o%2528%255Clog%2520%255Clog%2520n%2529%2524%2520adaptive%2520rounds.%2520For%2520hypergraphs%2520with%2520maximum%2520degree%2520%2524%255CDelta%2524%250Aand%2520edge%2520size%2520ratio%2520%2524%255Crho%2524%252C%2520we%2520give%2520a%2520non-adaptive%2520algorithm%2520with%2520%2524O%2528%25282n%2529%255E%257B%255Crho%250A%255CDelta%252B1%257D%255Clog%255E2%2520n%2529%2524%2520queries.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520these%2520are%2520the%2520first%250Aalgorithms%2520with%2520poly%2524%2528n%252C%2520m%2529%2524%2520query%2520complexity%2520for%2520learning%2520non-trivial%2520families%250Aof%2520hypergraphs%2520that%2520have%2520a%2520super-constant%2520number%2520of%2520edges%2520of%2520super-constant%250Asize.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.09989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Low%20Degree%20Hypergraphs&entry.906535625=Eric%20Balkanski%20and%20Oussama%20Hanguir%20and%20Shatian%20Wang&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20a%20hypergraph%20via%20edge%20detecting%20queries.%20In%0Athis%20problem%2C%20a%20learner%20queries%20subsets%20of%20vertices%20of%20a%20hidden%20hypergraph%20and%0Aobserves%20whether%20these%20subsets%20contain%20an%20edge%20or%20not.%20In%20general%2C%20learning%20a%0Ahypergraph%20with%20%24m%24%20edges%20of%20maximum%20size%20%24d%24%20requires%20%24%5COmega%28%282m/d%29%5E%7Bd/2%7D%29%24%0Aqueries.%20In%20this%20paper%2C%20we%20aim%20to%20identify%20families%20of%20hypergraphs%20that%20can%20be%0Alearned%20without%20suffering%20from%20a%20query%20complexity%20that%20grows%20exponentially%20in%0Athe%20size%20of%20the%20edges.%0A%20%20We%20show%20that%20hypermatchings%20and%20low-degree%20near-uniform%20hypergraphs%20with%20%24n%24%0Avertices%20are%20learnable%20with%20poly%24%28n%29%24%20queries.%20For%20learning%20hypermatchings%0A%28hypergraphs%20of%20maximum%20degree%20%24%201%24%29%2C%20we%20give%20an%20%24O%28%5Clog%5E3%20n%29%24-round%20algorithm%0Awith%20%24O%28n%20%5Clog%5E5%20n%29%24%20queries.%20We%20complement%20this%20upper%20bound%20by%20showing%20that%0Athere%20are%20no%20algorithms%20with%20poly%24%28n%29%24%20queries%20that%20learn%20hypermatchings%20in%0A%24o%28%5Clog%20%5Clog%20n%29%24%20adaptive%20rounds.%20For%20hypergraphs%20with%20maximum%20degree%20%24%5CDelta%24%0Aand%20edge%20size%20ratio%20%24%5Crho%24%2C%20we%20give%20a%20non-adaptive%20algorithm%20with%20%24O%28%282n%29%5E%7B%5Crho%0A%5CDelta%2B1%7D%5Clog%5E2%20n%29%24%20queries.%20To%20the%20best%20of%20our%20knowledge%2C%20these%20are%20the%20first%0Aalgorithms%20with%20poly%24%28n%2C%20m%29%24%20query%20complexity%20for%20learning%20non-trivial%20families%0Aof%20hypergraphs%20that%20have%20a%20super-constant%20number%20of%20edges%20of%20super-constant%0Asize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.09989v3&entry.124074799=Read"},
{"title": "Enhancing Generalized Few-Shot Semantic Segmentation via Effective\n  Knowledge Transfer", "author": "Xinyue Chen and Miaojing Shi and Zijian Zhou and Lianghua He and Sophia Tsoka", "abstract": "  Generalized few-shot semantic segmentation (GFSS) aims to segment objects of\nboth base and novel classes, using sufficient samples of base classes and few\nsamples of novel classes. Representative GFSS approaches typically employ a\ntwo-phase training scheme, involving base class pre-training followed by novel\nclass fine-tuning, to learn the classifiers for base and novel classes\nrespectively. Nevertheless, distribution gap exists between base and novel\nclasses in this process. To narrow this gap, we exploit effective knowledge\ntransfer from base to novel classes. First, a novel prototype modulation module\nis designed to modulate novel class prototypes by exploiting the correlations\nbetween base and novel classes. Second, a novel classifier calibration module\nis proposed to calibrate the weight distribution of the novel classifier\naccording to that of the base classifier. Furthermore, existing GFSS approaches\nsuffer from a lack of contextual information for novel classes due to their\nlimited samples, we thereby introduce a context consistency learning scheme to\ntransfer the contextual knowledge from base to novel classes. Extensive\nexperiments on PASCAL-5$^i$ and COCO-20$^i$ demonstrate that our approach\nsignificantly enhances the state of the art in the GFSS setting. The code is\navailable at: https://github.com/HHHHedy/GFSS-EKT.\n", "link": "http://arxiv.org/abs/2412.15835v1", "date": "2024-12-20", "relevancy": 2.0839, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Generalized%20Few-Shot%20Semantic%20Segmentation%20via%20Effective%0A%20%20Knowledge%20Transfer&body=Title%3A%20Enhancing%20Generalized%20Few-Shot%20Semantic%20Segmentation%20via%20Effective%0A%20%20Knowledge%20Transfer%0AAuthor%3A%20Xinyue%20Chen%20and%20Miaojing%20Shi%20and%20Zijian%20Zhou%20and%20Lianghua%20He%20and%20Sophia%20Tsoka%0AAbstract%3A%20%20%20Generalized%20few-shot%20semantic%20segmentation%20%28GFSS%29%20aims%20to%20segment%20objects%20of%0Aboth%20base%20and%20novel%20classes%2C%20using%20sufficient%20samples%20of%20base%20classes%20and%20few%0Asamples%20of%20novel%20classes.%20Representative%20GFSS%20approaches%20typically%20employ%20a%0Atwo-phase%20training%20scheme%2C%20involving%20base%20class%20pre-training%20followed%20by%20novel%0Aclass%20fine-tuning%2C%20to%20learn%20the%20classifiers%20for%20base%20and%20novel%20classes%0Arespectively.%20Nevertheless%2C%20distribution%20gap%20exists%20between%20base%20and%20novel%0Aclasses%20in%20this%20process.%20To%20narrow%20this%20gap%2C%20we%20exploit%20effective%20knowledge%0Atransfer%20from%20base%20to%20novel%20classes.%20First%2C%20a%20novel%20prototype%20modulation%20module%0Ais%20designed%20to%20modulate%20novel%20class%20prototypes%20by%20exploiting%20the%20correlations%0Abetween%20base%20and%20novel%20classes.%20Second%2C%20a%20novel%20classifier%20calibration%20module%0Ais%20proposed%20to%20calibrate%20the%20weight%20distribution%20of%20the%20novel%20classifier%0Aaccording%20to%20that%20of%20the%20base%20classifier.%20Furthermore%2C%20existing%20GFSS%20approaches%0Asuffer%20from%20a%20lack%20of%20contextual%20information%20for%20novel%20classes%20due%20to%20their%0Alimited%20samples%2C%20we%20thereby%20introduce%20a%20context%20consistency%20learning%20scheme%20to%0Atransfer%20the%20contextual%20knowledge%20from%20base%20to%20novel%20classes.%20Extensive%0Aexperiments%20on%20PASCAL-5%24%5Ei%24%20and%20COCO-20%24%5Ei%24%20demonstrate%20that%20our%20approach%0Asignificantly%20enhances%20the%20state%20of%20the%20art%20in%20the%20GFSS%20setting.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/HHHHedy/GFSS-EKT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Generalized%2520Few-Shot%2520Semantic%2520Segmentation%2520via%2520Effective%250A%2520%2520Knowledge%2520Transfer%26entry.906535625%3DXinyue%2520Chen%2520and%2520Miaojing%2520Shi%2520and%2520Zijian%2520Zhou%2520and%2520Lianghua%2520He%2520and%2520Sophia%2520Tsoka%26entry.1292438233%3D%2520%2520Generalized%2520few-shot%2520semantic%2520segmentation%2520%2528GFSS%2529%2520aims%2520to%2520segment%2520objects%2520of%250Aboth%2520base%2520and%2520novel%2520classes%252C%2520using%2520sufficient%2520samples%2520of%2520base%2520classes%2520and%2520few%250Asamples%2520of%2520novel%2520classes.%2520Representative%2520GFSS%2520approaches%2520typically%2520employ%2520a%250Atwo-phase%2520training%2520scheme%252C%2520involving%2520base%2520class%2520pre-training%2520followed%2520by%2520novel%250Aclass%2520fine-tuning%252C%2520to%2520learn%2520the%2520classifiers%2520for%2520base%2520and%2520novel%2520classes%250Arespectively.%2520Nevertheless%252C%2520distribution%2520gap%2520exists%2520between%2520base%2520and%2520novel%250Aclasses%2520in%2520this%2520process.%2520To%2520narrow%2520this%2520gap%252C%2520we%2520exploit%2520effective%2520knowledge%250Atransfer%2520from%2520base%2520to%2520novel%2520classes.%2520First%252C%2520a%2520novel%2520prototype%2520modulation%2520module%250Ais%2520designed%2520to%2520modulate%2520novel%2520class%2520prototypes%2520by%2520exploiting%2520the%2520correlations%250Abetween%2520base%2520and%2520novel%2520classes.%2520Second%252C%2520a%2520novel%2520classifier%2520calibration%2520module%250Ais%2520proposed%2520to%2520calibrate%2520the%2520weight%2520distribution%2520of%2520the%2520novel%2520classifier%250Aaccording%2520to%2520that%2520of%2520the%2520base%2520classifier.%2520Furthermore%252C%2520existing%2520GFSS%2520approaches%250Asuffer%2520from%2520a%2520lack%2520of%2520contextual%2520information%2520for%2520novel%2520classes%2520due%2520to%2520their%250Alimited%2520samples%252C%2520we%2520thereby%2520introduce%2520a%2520context%2520consistency%2520learning%2520scheme%2520to%250Atransfer%2520the%2520contextual%2520knowledge%2520from%2520base%2520to%2520novel%2520classes.%2520Extensive%250Aexperiments%2520on%2520PASCAL-5%2524%255Ei%2524%2520and%2520COCO-20%2524%255Ei%2524%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520enhances%2520the%2520state%2520of%2520the%2520art%2520in%2520the%2520GFSS%2520setting.%2520The%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/HHHHedy/GFSS-EKT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Generalized%20Few-Shot%20Semantic%20Segmentation%20via%20Effective%0A%20%20Knowledge%20Transfer&entry.906535625=Xinyue%20Chen%20and%20Miaojing%20Shi%20and%20Zijian%20Zhou%20and%20Lianghua%20He%20and%20Sophia%20Tsoka&entry.1292438233=%20%20Generalized%20few-shot%20semantic%20segmentation%20%28GFSS%29%20aims%20to%20segment%20objects%20of%0Aboth%20base%20and%20novel%20classes%2C%20using%20sufficient%20samples%20of%20base%20classes%20and%20few%0Asamples%20of%20novel%20classes.%20Representative%20GFSS%20approaches%20typically%20employ%20a%0Atwo-phase%20training%20scheme%2C%20involving%20base%20class%20pre-training%20followed%20by%20novel%0Aclass%20fine-tuning%2C%20to%20learn%20the%20classifiers%20for%20base%20and%20novel%20classes%0Arespectively.%20Nevertheless%2C%20distribution%20gap%20exists%20between%20base%20and%20novel%0Aclasses%20in%20this%20process.%20To%20narrow%20this%20gap%2C%20we%20exploit%20effective%20knowledge%0Atransfer%20from%20base%20to%20novel%20classes.%20First%2C%20a%20novel%20prototype%20modulation%20module%0Ais%20designed%20to%20modulate%20novel%20class%20prototypes%20by%20exploiting%20the%20correlations%0Abetween%20base%20and%20novel%20classes.%20Second%2C%20a%20novel%20classifier%20calibration%20module%0Ais%20proposed%20to%20calibrate%20the%20weight%20distribution%20of%20the%20novel%20classifier%0Aaccording%20to%20that%20of%20the%20base%20classifier.%20Furthermore%2C%20existing%20GFSS%20approaches%0Asuffer%20from%20a%20lack%20of%20contextual%20information%20for%20novel%20classes%20due%20to%20their%0Alimited%20samples%2C%20we%20thereby%20introduce%20a%20context%20consistency%20learning%20scheme%20to%0Atransfer%20the%20contextual%20knowledge%20from%20base%20to%20novel%20classes.%20Extensive%0Aexperiments%20on%20PASCAL-5%24%5Ei%24%20and%20COCO-20%24%5Ei%24%20demonstrate%20that%20our%20approach%0Asignificantly%20enhances%20the%20state%20of%20the%20art%20in%20the%20GFSS%20setting.%20The%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/HHHHedy/GFSS-EKT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15835v1&entry.124074799=Read"},
{"title": "Self-Supervised Radiograph Anatomical Region Classification -- How Clean\n  Is Your Real-World Data?", "author": "Simon Langer and Jessica Ritter and Rickmer Braren and Daniel Rueckert and Paul Hager", "abstract": "  Modern deep learning-based clinical imaging workflows rely on accurate labels\nof the examined anatomical region. Knowing the anatomical region is required to\nselect applicable downstream models and to effectively generate cohorts of high\nquality data for future medical and machine learning research efforts. However,\nthis information may not be available in externally sourced data or generally\ncontain data entry errors. To address this problem, we show the effectiveness\nof self-supervised methods such as SimCLR and BYOL as well as supervised\ncontrastive deep learning methods in assigning one of 14 anatomical region\nclasses in our in-house dataset of 48,434 skeletal radiographs. We achieve a\nstrong linear evaluation accuracy of 96.6% with a single model and 97.7% using\nan ensemble approach. Furthermore, only a few labeled instances (1% of the\ntraining set) suffice to achieve an accuracy of 92.2%, enabling usage in\nlow-label and thus low-resource scenarios. Our model can be used to correct\ndata entry mistakes: a follow-up analysis of the test set errors of our\nbest-performing single model by an expert radiologist identified 35% incorrect\nlabels and 11% out-of-domain images. When accounted for, the radiograph\nanatomical region labelling performance increased -- without and with an\nensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%.\n", "link": "http://arxiv.org/abs/2412.15967v1", "date": "2024-12-20", "relevancy": 2.0775, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Radiograph%20Anatomical%20Region%20Classification%20--%20How%20Clean%0A%20%20Is%20Your%20Real-World%20Data%3F&body=Title%3A%20Self-Supervised%20Radiograph%20Anatomical%20Region%20Classification%20--%20How%20Clean%0A%20%20Is%20Your%20Real-World%20Data%3F%0AAuthor%3A%20Simon%20Langer%20and%20Jessica%20Ritter%20and%20Rickmer%20Braren%20and%20Daniel%20Rueckert%20and%20Paul%20Hager%0AAbstract%3A%20%20%20Modern%20deep%20learning-based%20clinical%20imaging%20workflows%20rely%20on%20accurate%20labels%0Aof%20the%20examined%20anatomical%20region.%20Knowing%20the%20anatomical%20region%20is%20required%20to%0Aselect%20applicable%20downstream%20models%20and%20to%20effectively%20generate%20cohorts%20of%20high%0Aquality%20data%20for%20future%20medical%20and%20machine%20learning%20research%20efforts.%20However%2C%0Athis%20information%20may%20not%20be%20available%20in%20externally%20sourced%20data%20or%20generally%0Acontain%20data%20entry%20errors.%20To%20address%20this%20problem%2C%20we%20show%20the%20effectiveness%0Aof%20self-supervised%20methods%20such%20as%20SimCLR%20and%20BYOL%20as%20well%20as%20supervised%0Acontrastive%20deep%20learning%20methods%20in%20assigning%20one%20of%2014%20anatomical%20region%0Aclasses%20in%20our%20in-house%20dataset%20of%2048%2C434%20skeletal%20radiographs.%20We%20achieve%20a%0Astrong%20linear%20evaluation%20accuracy%20of%2096.6%25%20with%20a%20single%20model%20and%2097.7%25%20using%0Aan%20ensemble%20approach.%20Furthermore%2C%20only%20a%20few%20labeled%20instances%20%281%25%20of%20the%0Atraining%20set%29%20suffice%20to%20achieve%20an%20accuracy%20of%2092.2%25%2C%20enabling%20usage%20in%0Alow-label%20and%20thus%20low-resource%20scenarios.%20Our%20model%20can%20be%20used%20to%20correct%0Adata%20entry%20mistakes%3A%20a%20follow-up%20analysis%20of%20the%20test%20set%20errors%20of%20our%0Abest-performing%20single%20model%20by%20an%20expert%20radiologist%20identified%2035%25%20incorrect%0Alabels%20and%2011%25%20out-of-domain%20images.%20When%20accounted%20for%2C%20the%20radiograph%0Aanatomical%20region%20labelling%20performance%20increased%20--%20without%20and%20with%20an%0Aensemble%2C%20respectively%20--%20to%20a%20theoretical%20accuracy%20of%2098.0%25%20and%2098.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Radiograph%2520Anatomical%2520Region%2520Classification%2520--%2520How%2520Clean%250A%2520%2520Is%2520Your%2520Real-World%2520Data%253F%26entry.906535625%3DSimon%2520Langer%2520and%2520Jessica%2520Ritter%2520and%2520Rickmer%2520Braren%2520and%2520Daniel%2520Rueckert%2520and%2520Paul%2520Hager%26entry.1292438233%3D%2520%2520Modern%2520deep%2520learning-based%2520clinical%2520imaging%2520workflows%2520rely%2520on%2520accurate%2520labels%250Aof%2520the%2520examined%2520anatomical%2520region.%2520Knowing%2520the%2520anatomical%2520region%2520is%2520required%2520to%250Aselect%2520applicable%2520downstream%2520models%2520and%2520to%2520effectively%2520generate%2520cohorts%2520of%2520high%250Aquality%2520data%2520for%2520future%2520medical%2520and%2520machine%2520learning%2520research%2520efforts.%2520However%252C%250Athis%2520information%2520may%2520not%2520be%2520available%2520in%2520externally%2520sourced%2520data%2520or%2520generally%250Acontain%2520data%2520entry%2520errors.%2520To%2520address%2520this%2520problem%252C%2520we%2520show%2520the%2520effectiveness%250Aof%2520self-supervised%2520methods%2520such%2520as%2520SimCLR%2520and%2520BYOL%2520as%2520well%2520as%2520supervised%250Acontrastive%2520deep%2520learning%2520methods%2520in%2520assigning%2520one%2520of%252014%2520anatomical%2520region%250Aclasses%2520in%2520our%2520in-house%2520dataset%2520of%252048%252C434%2520skeletal%2520radiographs.%2520We%2520achieve%2520a%250Astrong%2520linear%2520evaluation%2520accuracy%2520of%252096.6%2525%2520with%2520a%2520single%2520model%2520and%252097.7%2525%2520using%250Aan%2520ensemble%2520approach.%2520Furthermore%252C%2520only%2520a%2520few%2520labeled%2520instances%2520%25281%2525%2520of%2520the%250Atraining%2520set%2529%2520suffice%2520to%2520achieve%2520an%2520accuracy%2520of%252092.2%2525%252C%2520enabling%2520usage%2520in%250Alow-label%2520and%2520thus%2520low-resource%2520scenarios.%2520Our%2520model%2520can%2520be%2520used%2520to%2520correct%250Adata%2520entry%2520mistakes%253A%2520a%2520follow-up%2520analysis%2520of%2520the%2520test%2520set%2520errors%2520of%2520our%250Abest-performing%2520single%2520model%2520by%2520an%2520expert%2520radiologist%2520identified%252035%2525%2520incorrect%250Alabels%2520and%252011%2525%2520out-of-domain%2520images.%2520When%2520accounted%2520for%252C%2520the%2520radiograph%250Aanatomical%2520region%2520labelling%2520performance%2520increased%2520--%2520without%2520and%2520with%2520an%250Aensemble%252C%2520respectively%2520--%2520to%2520a%2520theoretical%2520accuracy%2520of%252098.0%2525%2520and%252098.8%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Radiograph%20Anatomical%20Region%20Classification%20--%20How%20Clean%0A%20%20Is%20Your%20Real-World%20Data%3F&entry.906535625=Simon%20Langer%20and%20Jessica%20Ritter%20and%20Rickmer%20Braren%20and%20Daniel%20Rueckert%20and%20Paul%20Hager&entry.1292438233=%20%20Modern%20deep%20learning-based%20clinical%20imaging%20workflows%20rely%20on%20accurate%20labels%0Aof%20the%20examined%20anatomical%20region.%20Knowing%20the%20anatomical%20region%20is%20required%20to%0Aselect%20applicable%20downstream%20models%20and%20to%20effectively%20generate%20cohorts%20of%20high%0Aquality%20data%20for%20future%20medical%20and%20machine%20learning%20research%20efforts.%20However%2C%0Athis%20information%20may%20not%20be%20available%20in%20externally%20sourced%20data%20or%20generally%0Acontain%20data%20entry%20errors.%20To%20address%20this%20problem%2C%20we%20show%20the%20effectiveness%0Aof%20self-supervised%20methods%20such%20as%20SimCLR%20and%20BYOL%20as%20well%20as%20supervised%0Acontrastive%20deep%20learning%20methods%20in%20assigning%20one%20of%2014%20anatomical%20region%0Aclasses%20in%20our%20in-house%20dataset%20of%2048%2C434%20skeletal%20radiographs.%20We%20achieve%20a%0Astrong%20linear%20evaluation%20accuracy%20of%2096.6%25%20with%20a%20single%20model%20and%2097.7%25%20using%0Aan%20ensemble%20approach.%20Furthermore%2C%20only%20a%20few%20labeled%20instances%20%281%25%20of%20the%0Atraining%20set%29%20suffice%20to%20achieve%20an%20accuracy%20of%2092.2%25%2C%20enabling%20usage%20in%0Alow-label%20and%20thus%20low-resource%20scenarios.%20Our%20model%20can%20be%20used%20to%20correct%0Adata%20entry%20mistakes%3A%20a%20follow-up%20analysis%20of%20the%20test%20set%20errors%20of%20our%0Abest-performing%20single%20model%20by%20an%20expert%20radiologist%20identified%2035%25%20incorrect%0Alabels%20and%2011%25%20out-of-domain%20images.%20When%20accounted%20for%2C%20the%20radiograph%0Aanatomical%20region%20labelling%20performance%20increased%20--%20without%20and%20with%20an%0Aensemble%2C%20respectively%20--%20to%20a%20theoretical%20accuracy%20of%2098.0%25%20and%2098.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15967v1&entry.124074799=Read"},
{"title": "SafeCFG: Redirecting Harmful Classifier-Free Guidance for Safe\n  Generation", "author": "Jiadong Pan and Hongcheng Gao and Liang Li and Zheng-Jun Zha and Qingming Huang and Jiebo Luo", "abstract": "  Diffusion models (DMs) have demonstrated exceptional performance in\ntext-to-image (T2I) tasks, leading to their widespread use. With the\nintroduction of classifier-free guidance (CFG), the quality of images generated\nby DMs is improved. However, DMs can generate more harmful images by\nmaliciously guiding the image generation process through CFG. Some safe\nguidance methods aim to mitigate the risk of generating harmful images but\noften reduce the quality of clean image generation. To address this issue, we\nintroduce the Harmful Guidance Redirector (HGR), which redirects harmful CFG\ndirection while preserving clean CFG direction during image generation,\ntransforming CFG into SafeCFG and achieving high safety and quality generation.\nWe train HGR to redirect multiple harmful CFG directions simultaneously,\ndemonstrating its ability to eliminate various harmful elements while\npreserving high-quality generation. Additionally, we find that HGR can detect\nimage harmfulness, allowing for unsupervised fine-tuning of safe diffusion\nmodels without pre-defined clean or harmful labels. Experimental results show\nthat by incorporating HGR, images generated by diffusion models achieve both\nhigh quality and strong safety, and safe DMs trained through unsupervised\nmethods according to the harmfulness detected by HGR also exhibit good safety\nperformance. The codes will be publicly available.\n", "link": "http://arxiv.org/abs/2412.16039v1", "date": "2024-12-20", "relevancy": 2.0751, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5346}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeCFG%3A%20Redirecting%20Harmful%20Classifier-Free%20Guidance%20for%20Safe%0A%20%20Generation&body=Title%3A%20SafeCFG%3A%20Redirecting%20Harmful%20Classifier-Free%20Guidance%20for%20Safe%0A%20%20Generation%0AAuthor%3A%20Jiadong%20Pan%20and%20Hongcheng%20Gao%20and%20Liang%20Li%20and%20Zheng-Jun%20Zha%20and%20Qingming%20Huang%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20performance%20in%0Atext-to-image%20%28T2I%29%20tasks%2C%20leading%20to%20their%20widespread%20use.%20With%20the%0Aintroduction%20of%20classifier-free%20guidance%20%28CFG%29%2C%20the%20quality%20of%20images%20generated%0Aby%20DMs%20is%20improved.%20However%2C%20DMs%20can%20generate%20more%20harmful%20images%20by%0Amaliciously%20guiding%20the%20image%20generation%20process%20through%20CFG.%20Some%20safe%0Aguidance%20methods%20aim%20to%20mitigate%20the%20risk%20of%20generating%20harmful%20images%20but%0Aoften%20reduce%20the%20quality%20of%20clean%20image%20generation.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Harmful%20Guidance%20Redirector%20%28HGR%29%2C%20which%20redirects%20harmful%20CFG%0Adirection%20while%20preserving%20clean%20CFG%20direction%20during%20image%20generation%2C%0Atransforming%20CFG%20into%20SafeCFG%20and%20achieving%20high%20safety%20and%20quality%20generation.%0AWe%20train%20HGR%20to%20redirect%20multiple%20harmful%20CFG%20directions%20simultaneously%2C%0Ademonstrating%20its%20ability%20to%20eliminate%20various%20harmful%20elements%20while%0Apreserving%20high-quality%20generation.%20Additionally%2C%20we%20find%20that%20HGR%20can%20detect%0Aimage%20harmfulness%2C%20allowing%20for%20unsupervised%20fine-tuning%20of%20safe%20diffusion%0Amodels%20without%20pre-defined%20clean%20or%20harmful%20labels.%20Experimental%20results%20show%0Athat%20by%20incorporating%20HGR%2C%20images%20generated%20by%20diffusion%20models%20achieve%20both%0Ahigh%20quality%20and%20strong%20safety%2C%20and%20safe%20DMs%20trained%20through%20unsupervised%0Amethods%20according%20to%20the%20harmfulness%20detected%20by%20HGR%20also%20exhibit%20good%20safety%0Aperformance.%20The%20codes%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeCFG%253A%2520Redirecting%2520Harmful%2520Classifier-Free%2520Guidance%2520for%2520Safe%250A%2520%2520Generation%26entry.906535625%3DJiadong%2520Pan%2520and%2520Hongcheng%2520Gao%2520and%2520Liang%2520Li%2520and%2520Zheng-Jun%2520Zha%2520and%2520Qingming%2520Huang%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Atext-to-image%2520%2528T2I%2529%2520tasks%252C%2520leading%2520to%2520their%2520widespread%2520use.%2520With%2520the%250Aintroduction%2520of%2520classifier-free%2520guidance%2520%2528CFG%2529%252C%2520the%2520quality%2520of%2520images%2520generated%250Aby%2520DMs%2520is%2520improved.%2520However%252C%2520DMs%2520can%2520generate%2520more%2520harmful%2520images%2520by%250Amaliciously%2520guiding%2520the%2520image%2520generation%2520process%2520through%2520CFG.%2520Some%2520safe%250Aguidance%2520methods%2520aim%2520to%2520mitigate%2520the%2520risk%2520of%2520generating%2520harmful%2520images%2520but%250Aoften%2520reduce%2520the%2520quality%2520of%2520clean%2520image%2520generation.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520the%2520Harmful%2520Guidance%2520Redirector%2520%2528HGR%2529%252C%2520which%2520redirects%2520harmful%2520CFG%250Adirection%2520while%2520preserving%2520clean%2520CFG%2520direction%2520during%2520image%2520generation%252C%250Atransforming%2520CFG%2520into%2520SafeCFG%2520and%2520achieving%2520high%2520safety%2520and%2520quality%2520generation.%250AWe%2520train%2520HGR%2520to%2520redirect%2520multiple%2520harmful%2520CFG%2520directions%2520simultaneously%252C%250Ademonstrating%2520its%2520ability%2520to%2520eliminate%2520various%2520harmful%2520elements%2520while%250Apreserving%2520high-quality%2520generation.%2520Additionally%252C%2520we%2520find%2520that%2520HGR%2520can%2520detect%250Aimage%2520harmfulness%252C%2520allowing%2520for%2520unsupervised%2520fine-tuning%2520of%2520safe%2520diffusion%250Amodels%2520without%2520pre-defined%2520clean%2520or%2520harmful%2520labels.%2520Experimental%2520results%2520show%250Athat%2520by%2520incorporating%2520HGR%252C%2520images%2520generated%2520by%2520diffusion%2520models%2520achieve%2520both%250Ahigh%2520quality%2520and%2520strong%2520safety%252C%2520and%2520safe%2520DMs%2520trained%2520through%2520unsupervised%250Amethods%2520according%2520to%2520the%2520harmfulness%2520detected%2520by%2520HGR%2520also%2520exhibit%2520good%2520safety%250Aperformance.%2520The%2520codes%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeCFG%3A%20Redirecting%20Harmful%20Classifier-Free%20Guidance%20for%20Safe%0A%20%20Generation&entry.906535625=Jiadong%20Pan%20and%20Hongcheng%20Gao%20and%20Liang%20Li%20and%20Zheng-Jun%20Zha%20and%20Qingming%20Huang%20and%20Jiebo%20Luo&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20demonstrated%20exceptional%20performance%20in%0Atext-to-image%20%28T2I%29%20tasks%2C%20leading%20to%20their%20widespread%20use.%20With%20the%0Aintroduction%20of%20classifier-free%20guidance%20%28CFG%29%2C%20the%20quality%20of%20images%20generated%0Aby%20DMs%20is%20improved.%20However%2C%20DMs%20can%20generate%20more%20harmful%20images%20by%0Amaliciously%20guiding%20the%20image%20generation%20process%20through%20CFG.%20Some%20safe%0Aguidance%20methods%20aim%20to%20mitigate%20the%20risk%20of%20generating%20harmful%20images%20but%0Aoften%20reduce%20the%20quality%20of%20clean%20image%20generation.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Harmful%20Guidance%20Redirector%20%28HGR%29%2C%20which%20redirects%20harmful%20CFG%0Adirection%20while%20preserving%20clean%20CFG%20direction%20during%20image%20generation%2C%0Atransforming%20CFG%20into%20SafeCFG%20and%20achieving%20high%20safety%20and%20quality%20generation.%0AWe%20train%20HGR%20to%20redirect%20multiple%20harmful%20CFG%20directions%20simultaneously%2C%0Ademonstrating%20its%20ability%20to%20eliminate%20various%20harmful%20elements%20while%0Apreserving%20high-quality%20generation.%20Additionally%2C%20we%20find%20that%20HGR%20can%20detect%0Aimage%20harmfulness%2C%20allowing%20for%20unsupervised%20fine-tuning%20of%20safe%20diffusion%0Amodels%20without%20pre-defined%20clean%20or%20harmful%20labels.%20Experimental%20results%20show%0Athat%20by%20incorporating%20HGR%2C%20images%20generated%20by%20diffusion%20models%20achieve%20both%0Ahigh%20quality%20and%20strong%20safety%2C%20and%20safe%20DMs%20trained%20through%20unsupervised%0Amethods%20according%20to%20the%20harmfulness%20detected%20by%20HGR%20also%20exhibit%20good%20safety%0Aperformance.%20The%20codes%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16039v1&entry.124074799=Read"},
{"title": "Probabilistic Latent Variable Modeling for Dynamic Friction\n  Identification and Estimation", "author": "Victor Vantilborgh and Sander De Witte and Frederik Ostyn and Tom Lefebvre and Guillaume Crevecoeur", "abstract": "  Precise identification of dynamic models in robotics is essential to support\ncontrol design, friction compensation, output torque estimation, etc. A\nlongstanding challenge remains in the identification of friction models for\nrobotic joints, given the numerous physical phenomena affecting the underlying\nfriction dynamics which result into nonlinear characteristics and hysteresis\nbehaviour in particular. These phenomena proof difficult to be modelled and\ncaptured accurately using physical analogies alone. This has motivated\nresearchers to shift from physics-based to data-driven models. Currently, these\nmethods are still limited in their ability to generalize effectively to typical\nindustrial robot deployement, characterized by high- and low-velocity\noperations and frequent direction reversals. Empirical observations motivate\nthe use of dynamic friction models but these remain particulary challenging to\nestablish. To address the current limitations, we propose to account for\nunidentified dynamics in the robot joints using latent dynamic states. The\nfriction model may then utilize both the dynamic robot state and additional\ninformation encoded in the latent state to evaluate the friction torque. We\ncast this stochastic and partially unsupervised identification problem as a\nstandard probabilistic representation learning problem. In this work both the\nfriction model and latent state dynamics are parametrized as neural networks\nand integrated in the conventional lumped parameter dynamic robot model. The\ncomplete dynamics model is directly learned from the noisy encoder measurements\nin the robot joints. We use the Expectation-Maximisation (EM) algorithm to find\na Maximum Likelihood Estimate (MLE) of the model parameters. The effectiveness\nof the proposed method is validated in terms of open-loop prediction accuracy\nin comparison with baseline methods, using the Kuka KR6 R700 as a test\nplatform.\n", "link": "http://arxiv.org/abs/2412.15756v1", "date": "2024-12-20", "relevancy": 2.0708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6485}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4921}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Latent%20Variable%20Modeling%20for%20Dynamic%20Friction%0A%20%20Identification%20and%20Estimation&body=Title%3A%20Probabilistic%20Latent%20Variable%20Modeling%20for%20Dynamic%20Friction%0A%20%20Identification%20and%20Estimation%0AAuthor%3A%20Victor%20Vantilborgh%20and%20Sander%20De%20Witte%20and%20Frederik%20Ostyn%20and%20Tom%20Lefebvre%20and%20Guillaume%20Crevecoeur%0AAbstract%3A%20%20%20Precise%20identification%20of%20dynamic%20models%20in%20robotics%20is%20essential%20to%20support%0Acontrol%20design%2C%20friction%20compensation%2C%20output%20torque%20estimation%2C%20etc.%20A%0Alongstanding%20challenge%20remains%20in%20the%20identification%20of%20friction%20models%20for%0Arobotic%20joints%2C%20given%20the%20numerous%20physical%20phenomena%20affecting%20the%20underlying%0Afriction%20dynamics%20which%20result%20into%20nonlinear%20characteristics%20and%20hysteresis%0Abehaviour%20in%20particular.%20These%20phenomena%20proof%20difficult%20to%20be%20modelled%20and%0Acaptured%20accurately%20using%20physical%20analogies%20alone.%20This%20has%20motivated%0Aresearchers%20to%20shift%20from%20physics-based%20to%20data-driven%20models.%20Currently%2C%20these%0Amethods%20are%20still%20limited%20in%20their%20ability%20to%20generalize%20effectively%20to%20typical%0Aindustrial%20robot%20deployement%2C%20characterized%20by%20high-%20and%20low-velocity%0Aoperations%20and%20frequent%20direction%20reversals.%20Empirical%20observations%20motivate%0Athe%20use%20of%20dynamic%20friction%20models%20but%20these%20remain%20particulary%20challenging%20to%0Aestablish.%20To%20address%20the%20current%20limitations%2C%20we%20propose%20to%20account%20for%0Aunidentified%20dynamics%20in%20the%20robot%20joints%20using%20latent%20dynamic%20states.%20The%0Afriction%20model%20may%20then%20utilize%20both%20the%20dynamic%20robot%20state%20and%20additional%0Ainformation%20encoded%20in%20the%20latent%20state%20to%20evaluate%20the%20friction%20torque.%20We%0Acast%20this%20stochastic%20and%20partially%20unsupervised%20identification%20problem%20as%20a%0Astandard%20probabilistic%20representation%20learning%20problem.%20In%20this%20work%20both%20the%0Afriction%20model%20and%20latent%20state%20dynamics%20are%20parametrized%20as%20neural%20networks%0Aand%20integrated%20in%20the%20conventional%20lumped%20parameter%20dynamic%20robot%20model.%20The%0Acomplete%20dynamics%20model%20is%20directly%20learned%20from%20the%20noisy%20encoder%20measurements%0Ain%20the%20robot%20joints.%20We%20use%20the%20Expectation-Maximisation%20%28EM%29%20algorithm%20to%20find%0Aa%20Maximum%20Likelihood%20Estimate%20%28MLE%29%20of%20the%20model%20parameters.%20The%20effectiveness%0Aof%20the%20proposed%20method%20is%20validated%20in%20terms%20of%20open-loop%20prediction%20accuracy%0Ain%20comparison%20with%20baseline%20methods%2C%20using%20the%20Kuka%20KR6%20R700%20as%20a%20test%0Aplatform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Latent%2520Variable%2520Modeling%2520for%2520Dynamic%2520Friction%250A%2520%2520Identification%2520and%2520Estimation%26entry.906535625%3DVictor%2520Vantilborgh%2520and%2520Sander%2520De%2520Witte%2520and%2520Frederik%2520Ostyn%2520and%2520Tom%2520Lefebvre%2520and%2520Guillaume%2520Crevecoeur%26entry.1292438233%3D%2520%2520Precise%2520identification%2520of%2520dynamic%2520models%2520in%2520robotics%2520is%2520essential%2520to%2520support%250Acontrol%2520design%252C%2520friction%2520compensation%252C%2520output%2520torque%2520estimation%252C%2520etc.%2520A%250Alongstanding%2520challenge%2520remains%2520in%2520the%2520identification%2520of%2520friction%2520models%2520for%250Arobotic%2520joints%252C%2520given%2520the%2520numerous%2520physical%2520phenomena%2520affecting%2520the%2520underlying%250Afriction%2520dynamics%2520which%2520result%2520into%2520nonlinear%2520characteristics%2520and%2520hysteresis%250Abehaviour%2520in%2520particular.%2520These%2520phenomena%2520proof%2520difficult%2520to%2520be%2520modelled%2520and%250Acaptured%2520accurately%2520using%2520physical%2520analogies%2520alone.%2520This%2520has%2520motivated%250Aresearchers%2520to%2520shift%2520from%2520physics-based%2520to%2520data-driven%2520models.%2520Currently%252C%2520these%250Amethods%2520are%2520still%2520limited%2520in%2520their%2520ability%2520to%2520generalize%2520effectively%2520to%2520typical%250Aindustrial%2520robot%2520deployement%252C%2520characterized%2520by%2520high-%2520and%2520low-velocity%250Aoperations%2520and%2520frequent%2520direction%2520reversals.%2520Empirical%2520observations%2520motivate%250Athe%2520use%2520of%2520dynamic%2520friction%2520models%2520but%2520these%2520remain%2520particulary%2520challenging%2520to%250Aestablish.%2520To%2520address%2520the%2520current%2520limitations%252C%2520we%2520propose%2520to%2520account%2520for%250Aunidentified%2520dynamics%2520in%2520the%2520robot%2520joints%2520using%2520latent%2520dynamic%2520states.%2520The%250Afriction%2520model%2520may%2520then%2520utilize%2520both%2520the%2520dynamic%2520robot%2520state%2520and%2520additional%250Ainformation%2520encoded%2520in%2520the%2520latent%2520state%2520to%2520evaluate%2520the%2520friction%2520torque.%2520We%250Acast%2520this%2520stochastic%2520and%2520partially%2520unsupervised%2520identification%2520problem%2520as%2520a%250Astandard%2520probabilistic%2520representation%2520learning%2520problem.%2520In%2520this%2520work%2520both%2520the%250Afriction%2520model%2520and%2520latent%2520state%2520dynamics%2520are%2520parametrized%2520as%2520neural%2520networks%250Aand%2520integrated%2520in%2520the%2520conventional%2520lumped%2520parameter%2520dynamic%2520robot%2520model.%2520The%250Acomplete%2520dynamics%2520model%2520is%2520directly%2520learned%2520from%2520the%2520noisy%2520encoder%2520measurements%250Ain%2520the%2520robot%2520joints.%2520We%2520use%2520the%2520Expectation-Maximisation%2520%2528EM%2529%2520algorithm%2520to%2520find%250Aa%2520Maximum%2520Likelihood%2520Estimate%2520%2528MLE%2529%2520of%2520the%2520model%2520parameters.%2520The%2520effectiveness%250Aof%2520the%2520proposed%2520method%2520is%2520validated%2520in%2520terms%2520of%2520open-loop%2520prediction%2520accuracy%250Ain%2520comparison%2520with%2520baseline%2520methods%252C%2520using%2520the%2520Kuka%2520KR6%2520R700%2520as%2520a%2520test%250Aplatform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Latent%20Variable%20Modeling%20for%20Dynamic%20Friction%0A%20%20Identification%20and%20Estimation&entry.906535625=Victor%20Vantilborgh%20and%20Sander%20De%20Witte%20and%20Frederik%20Ostyn%20and%20Tom%20Lefebvre%20and%20Guillaume%20Crevecoeur&entry.1292438233=%20%20Precise%20identification%20of%20dynamic%20models%20in%20robotics%20is%20essential%20to%20support%0Acontrol%20design%2C%20friction%20compensation%2C%20output%20torque%20estimation%2C%20etc.%20A%0Alongstanding%20challenge%20remains%20in%20the%20identification%20of%20friction%20models%20for%0Arobotic%20joints%2C%20given%20the%20numerous%20physical%20phenomena%20affecting%20the%20underlying%0Afriction%20dynamics%20which%20result%20into%20nonlinear%20characteristics%20and%20hysteresis%0Abehaviour%20in%20particular.%20These%20phenomena%20proof%20difficult%20to%20be%20modelled%20and%0Acaptured%20accurately%20using%20physical%20analogies%20alone.%20This%20has%20motivated%0Aresearchers%20to%20shift%20from%20physics-based%20to%20data-driven%20models.%20Currently%2C%20these%0Amethods%20are%20still%20limited%20in%20their%20ability%20to%20generalize%20effectively%20to%20typical%0Aindustrial%20robot%20deployement%2C%20characterized%20by%20high-%20and%20low-velocity%0Aoperations%20and%20frequent%20direction%20reversals.%20Empirical%20observations%20motivate%0Athe%20use%20of%20dynamic%20friction%20models%20but%20these%20remain%20particulary%20challenging%20to%0Aestablish.%20To%20address%20the%20current%20limitations%2C%20we%20propose%20to%20account%20for%0Aunidentified%20dynamics%20in%20the%20robot%20joints%20using%20latent%20dynamic%20states.%20The%0Afriction%20model%20may%20then%20utilize%20both%20the%20dynamic%20robot%20state%20and%20additional%0Ainformation%20encoded%20in%20the%20latent%20state%20to%20evaluate%20the%20friction%20torque.%20We%0Acast%20this%20stochastic%20and%20partially%20unsupervised%20identification%20problem%20as%20a%0Astandard%20probabilistic%20representation%20learning%20problem.%20In%20this%20work%20both%20the%0Afriction%20model%20and%20latent%20state%20dynamics%20are%20parametrized%20as%20neural%20networks%0Aand%20integrated%20in%20the%20conventional%20lumped%20parameter%20dynamic%20robot%20model.%20The%0Acomplete%20dynamics%20model%20is%20directly%20learned%20from%20the%20noisy%20encoder%20measurements%0Ain%20the%20robot%20joints.%20We%20use%20the%20Expectation-Maximisation%20%28EM%29%20algorithm%20to%20find%0Aa%20Maximum%20Likelihood%20Estimate%20%28MLE%29%20of%20the%20model%20parameters.%20The%20effectiveness%0Aof%20the%20proposed%20method%20is%20validated%20in%20terms%20of%20open-loop%20prediction%20accuracy%0Ain%20comparison%20with%20baseline%20methods%2C%20using%20the%20Kuka%20KR6%20R700%20as%20a%20test%0Aplatform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15756v1&entry.124074799=Read"},
{"title": "Residual Multi-Fidelity Neural Network Computing", "author": "Owen Davis and Mohammad Motamed and Raul Tempone", "abstract": "  In this work, we consider the general problem of constructing a neural\nnetwork surrogate model using multi-fidelity information. Motivated by\nerror-complexity estimates for ReLU neural networks, we formulate the\ncorrelation between an inexpensive low-fidelity model and an expensive\nhigh-fidelity model as a possibly non-linear residual function. This function\ndefines a mapping between 1) the shared input space of the models along with\nthe low-fidelity model output, and 2) the discrepancy between the outputs of\nthe two models. The computational framework proceeds by training two neural\nnetworks to work in concert. The first network learns the residual function on\na small set of high- and low-fidelity data. Once trained, this network is used\nto generate additional synthetic high-fidelity data, which is used in the\ntraining of the second network. The trained second network then acts as our\nsurrogate for the high-fidelity quantity of interest. We present four numerical\nexamples to demonstrate the power of the proposed framework, showing that\nsignificant savings in computational cost may be achieved when the output\npredictions are desired to be accurate within small tolerances.\n", "link": "http://arxiv.org/abs/2310.03572v3", "date": "2024-12-20", "relevancy": 2.0698, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5199}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5194}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Multi-Fidelity%20Neural%20Network%20Computing&body=Title%3A%20Residual%20Multi-Fidelity%20Neural%20Network%20Computing%0AAuthor%3A%20Owen%20Davis%20and%20Mohammad%20Motamed%20and%20Raul%20Tempone%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20consider%20the%20general%20problem%20of%20constructing%20a%20neural%0Anetwork%20surrogate%20model%20using%20multi-fidelity%20information.%20Motivated%20by%0Aerror-complexity%20estimates%20for%20ReLU%20neural%20networks%2C%20we%20formulate%20the%0Acorrelation%20between%20an%20inexpensive%20low-fidelity%20model%20and%20an%20expensive%0Ahigh-fidelity%20model%20as%20a%20possibly%20non-linear%20residual%20function.%20This%20function%0Adefines%20a%20mapping%20between%201%29%20the%20shared%20input%20space%20of%20the%20models%20along%20with%0Athe%20low-fidelity%20model%20output%2C%20and%202%29%20the%20discrepancy%20between%20the%20outputs%20of%0Athe%20two%20models.%20The%20computational%20framework%20proceeds%20by%20training%20two%20neural%0Anetworks%20to%20work%20in%20concert.%20The%20first%20network%20learns%20the%20residual%20function%20on%0Aa%20small%20set%20of%20high-%20and%20low-fidelity%20data.%20Once%20trained%2C%20this%20network%20is%20used%0Ato%20generate%20additional%20synthetic%20high-fidelity%20data%2C%20which%20is%20used%20in%20the%0Atraining%20of%20the%20second%20network.%20The%20trained%20second%20network%20then%20acts%20as%20our%0Asurrogate%20for%20the%20high-fidelity%20quantity%20of%20interest.%20We%20present%20four%20numerical%0Aexamples%20to%20demonstrate%20the%20power%20of%20the%20proposed%20framework%2C%20showing%20that%0Asignificant%20savings%20in%20computational%20cost%20may%20be%20achieved%20when%20the%20output%0Apredictions%20are%20desired%20to%20be%20accurate%20within%20small%20tolerances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03572v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Multi-Fidelity%2520Neural%2520Network%2520Computing%26entry.906535625%3DOwen%2520Davis%2520and%2520Mohammad%2520Motamed%2520and%2520Raul%2520Tempone%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520general%2520problem%2520of%2520constructing%2520a%2520neural%250Anetwork%2520surrogate%2520model%2520using%2520multi-fidelity%2520information.%2520Motivated%2520by%250Aerror-complexity%2520estimates%2520for%2520ReLU%2520neural%2520networks%252C%2520we%2520formulate%2520the%250Acorrelation%2520between%2520an%2520inexpensive%2520low-fidelity%2520model%2520and%2520an%2520expensive%250Ahigh-fidelity%2520model%2520as%2520a%2520possibly%2520non-linear%2520residual%2520function.%2520This%2520function%250Adefines%2520a%2520mapping%2520between%25201%2529%2520the%2520shared%2520input%2520space%2520of%2520the%2520models%2520along%2520with%250Athe%2520low-fidelity%2520model%2520output%252C%2520and%25202%2529%2520the%2520discrepancy%2520between%2520the%2520outputs%2520of%250Athe%2520two%2520models.%2520The%2520computational%2520framework%2520proceeds%2520by%2520training%2520two%2520neural%250Anetworks%2520to%2520work%2520in%2520concert.%2520The%2520first%2520network%2520learns%2520the%2520residual%2520function%2520on%250Aa%2520small%2520set%2520of%2520high-%2520and%2520low-fidelity%2520data.%2520Once%2520trained%252C%2520this%2520network%2520is%2520used%250Ato%2520generate%2520additional%2520synthetic%2520high-fidelity%2520data%252C%2520which%2520is%2520used%2520in%2520the%250Atraining%2520of%2520the%2520second%2520network.%2520The%2520trained%2520second%2520network%2520then%2520acts%2520as%2520our%250Asurrogate%2520for%2520the%2520high-fidelity%2520quantity%2520of%2520interest.%2520We%2520present%2520four%2520numerical%250Aexamples%2520to%2520demonstrate%2520the%2520power%2520of%2520the%2520proposed%2520framework%252C%2520showing%2520that%250Asignificant%2520savings%2520in%2520computational%2520cost%2520may%2520be%2520achieved%2520when%2520the%2520output%250Apredictions%2520are%2520desired%2520to%2520be%2520accurate%2520within%2520small%2520tolerances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03572v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Multi-Fidelity%20Neural%20Network%20Computing&entry.906535625=Owen%20Davis%20and%20Mohammad%20Motamed%20and%20Raul%20Tempone&entry.1292438233=%20%20In%20this%20work%2C%20we%20consider%20the%20general%20problem%20of%20constructing%20a%20neural%0Anetwork%20surrogate%20model%20using%20multi-fidelity%20information.%20Motivated%20by%0Aerror-complexity%20estimates%20for%20ReLU%20neural%20networks%2C%20we%20formulate%20the%0Acorrelation%20between%20an%20inexpensive%20low-fidelity%20model%20and%20an%20expensive%0Ahigh-fidelity%20model%20as%20a%20possibly%20non-linear%20residual%20function.%20This%20function%0Adefines%20a%20mapping%20between%201%29%20the%20shared%20input%20space%20of%20the%20models%20along%20with%0Athe%20low-fidelity%20model%20output%2C%20and%202%29%20the%20discrepancy%20between%20the%20outputs%20of%0Athe%20two%20models.%20The%20computational%20framework%20proceeds%20by%20training%20two%20neural%0Anetworks%20to%20work%20in%20concert.%20The%20first%20network%20learns%20the%20residual%20function%20on%0Aa%20small%20set%20of%20high-%20and%20low-fidelity%20data.%20Once%20trained%2C%20this%20network%20is%20used%0Ato%20generate%20additional%20synthetic%20high-fidelity%20data%2C%20which%20is%20used%20in%20the%0Atraining%20of%20the%20second%20network.%20The%20trained%20second%20network%20then%20acts%20as%20our%0Asurrogate%20for%20the%20high-fidelity%20quantity%20of%20interest.%20We%20present%20four%20numerical%0Aexamples%20to%20demonstrate%20the%20power%20of%20the%20proposed%20framework%2C%20showing%20that%0Asignificant%20savings%20in%20computational%20cost%20may%20be%20achieved%20when%20the%20output%0Apredictions%20are%20desired%20to%20be%20accurate%20within%20small%20tolerances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03572v3&entry.124074799=Read"},
{"title": "Learning Temporally Equivariance for Degenerative Disease Progression in\n  OCT by Predicting Future Representations", "author": "Taha Emre and Arunava Chakravarty and Dmitrii Lachinov and Antoine Rivail and Ursula Schmidt-Erfurth and Hrvoje Bogunovi\u0107", "abstract": "  Contrastive pretraining provides robust representations by ensuring their\ninvariance to different image transformations while simultaneously preventing\nrepresentational collapse. Equivariant contrastive learning, on the other hand,\nprovides representations sensitive to specific image transformations while\nremaining invariant to others. By introducing equivariance to time-induced\ntransformations, such as disease-related anatomical changes in longitudinal\nimaging, the model can effectively capture such changes in the representation\nspace. In this work, we propose a Time-equivariant Contrastive Learning (TC)\nmethod. First, an encoder embeds two unlabeled scans from different time points\nof the same patient into the representation space. Next, a temporal\nequivariance module is trained to predict the representation of a later visit\nbased on the representation from one of the previous visits and the\ncorresponding time interval with a novel regularization loss term while\npreserving the invariance property to irrelevant image transformations. On a\nlarge longitudinal dataset, our model clearly outperforms existing equivariant\ncontrastive methods in predicting progression from intermediate age-related\nmacular degeneration (AMD) to advanced wet-AMD within a specified time-window.\n", "link": "http://arxiv.org/abs/2405.09404v3", "date": "2024-12-20", "relevancy": 2.0695, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5037}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Temporally%20Equivariance%20for%20Degenerative%20Disease%20Progression%20in%0A%20%20OCT%20by%20Predicting%20Future%20Representations&body=Title%3A%20Learning%20Temporally%20Equivariance%20for%20Degenerative%20Disease%20Progression%20in%0A%20%20OCT%20by%20Predicting%20Future%20Representations%0AAuthor%3A%20Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Dmitrii%20Lachinov%20and%20Antoine%20Rivail%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20Contrastive%20pretraining%20provides%20robust%20representations%20by%20ensuring%20their%0Ainvariance%20to%20different%20image%20transformations%20while%20simultaneously%20preventing%0Arepresentational%20collapse.%20Equivariant%20contrastive%20learning%2C%20on%20the%20other%20hand%2C%0Aprovides%20representations%20sensitive%20to%20specific%20image%20transformations%20while%0Aremaining%20invariant%20to%20others.%20By%20introducing%20equivariance%20to%20time-induced%0Atransformations%2C%20such%20as%20disease-related%20anatomical%20changes%20in%20longitudinal%0Aimaging%2C%20the%20model%20can%20effectively%20capture%20such%20changes%20in%20the%20representation%0Aspace.%20In%20this%20work%2C%20we%20propose%20a%20Time-equivariant%20Contrastive%20Learning%20%28TC%29%0Amethod.%20First%2C%20an%20encoder%20embeds%20two%20unlabeled%20scans%20from%20different%20time%20points%0Aof%20the%20same%20patient%20into%20the%20representation%20space.%20Next%2C%20a%20temporal%0Aequivariance%20module%20is%20trained%20to%20predict%20the%20representation%20of%20a%20later%20visit%0Abased%20on%20the%20representation%20from%20one%20of%20the%20previous%20visits%20and%20the%0Acorresponding%20time%20interval%20with%20a%20novel%20regularization%20loss%20term%20while%0Apreserving%20the%20invariance%20property%20to%20irrelevant%20image%20transformations.%20On%20a%0Alarge%20longitudinal%20dataset%2C%20our%20model%20clearly%20outperforms%20existing%20equivariant%0Acontrastive%20methods%20in%20predicting%20progression%20from%20intermediate%20age-related%0Amacular%20degeneration%20%28AMD%29%20to%20advanced%20wet-AMD%20within%20a%20specified%20time-window.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09404v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Temporally%2520Equivariance%2520for%2520Degenerative%2520Disease%2520Progression%2520in%250A%2520%2520OCT%2520by%2520Predicting%2520Future%2520Representations%26entry.906535625%3DTaha%2520Emre%2520and%2520Arunava%2520Chakravarty%2520and%2520Dmitrii%2520Lachinov%2520and%2520Antoine%2520Rivail%2520and%2520Ursula%2520Schmidt-Erfurth%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520Contrastive%2520pretraining%2520provides%2520robust%2520representations%2520by%2520ensuring%2520their%250Ainvariance%2520to%2520different%2520image%2520transformations%2520while%2520simultaneously%2520preventing%250Arepresentational%2520collapse.%2520Equivariant%2520contrastive%2520learning%252C%2520on%2520the%2520other%2520hand%252C%250Aprovides%2520representations%2520sensitive%2520to%2520specific%2520image%2520transformations%2520while%250Aremaining%2520invariant%2520to%2520others.%2520By%2520introducing%2520equivariance%2520to%2520time-induced%250Atransformations%252C%2520such%2520as%2520disease-related%2520anatomical%2520changes%2520in%2520longitudinal%250Aimaging%252C%2520the%2520model%2520can%2520effectively%2520capture%2520such%2520changes%2520in%2520the%2520representation%250Aspace.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Time-equivariant%2520Contrastive%2520Learning%2520%2528TC%2529%250Amethod.%2520First%252C%2520an%2520encoder%2520embeds%2520two%2520unlabeled%2520scans%2520from%2520different%2520time%2520points%250Aof%2520the%2520same%2520patient%2520into%2520the%2520representation%2520space.%2520Next%252C%2520a%2520temporal%250Aequivariance%2520module%2520is%2520trained%2520to%2520predict%2520the%2520representation%2520of%2520a%2520later%2520visit%250Abased%2520on%2520the%2520representation%2520from%2520one%2520of%2520the%2520previous%2520visits%2520and%2520the%250Acorresponding%2520time%2520interval%2520with%2520a%2520novel%2520regularization%2520loss%2520term%2520while%250Apreserving%2520the%2520invariance%2520property%2520to%2520irrelevant%2520image%2520transformations.%2520On%2520a%250Alarge%2520longitudinal%2520dataset%252C%2520our%2520model%2520clearly%2520outperforms%2520existing%2520equivariant%250Acontrastive%2520methods%2520in%2520predicting%2520progression%2520from%2520intermediate%2520age-related%250Amacular%2520degeneration%2520%2528AMD%2529%2520to%2520advanced%2520wet-AMD%2520within%2520a%2520specified%2520time-window.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09404v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Temporally%20Equivariance%20for%20Degenerative%20Disease%20Progression%20in%0A%20%20OCT%20by%20Predicting%20Future%20Representations&entry.906535625=Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Dmitrii%20Lachinov%20and%20Antoine%20Rivail%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20Contrastive%20pretraining%20provides%20robust%20representations%20by%20ensuring%20their%0Ainvariance%20to%20different%20image%20transformations%20while%20simultaneously%20preventing%0Arepresentational%20collapse.%20Equivariant%20contrastive%20learning%2C%20on%20the%20other%20hand%2C%0Aprovides%20representations%20sensitive%20to%20specific%20image%20transformations%20while%0Aremaining%20invariant%20to%20others.%20By%20introducing%20equivariance%20to%20time-induced%0Atransformations%2C%20such%20as%20disease-related%20anatomical%20changes%20in%20longitudinal%0Aimaging%2C%20the%20model%20can%20effectively%20capture%20such%20changes%20in%20the%20representation%0Aspace.%20In%20this%20work%2C%20we%20propose%20a%20Time-equivariant%20Contrastive%20Learning%20%28TC%29%0Amethod.%20First%2C%20an%20encoder%20embeds%20two%20unlabeled%20scans%20from%20different%20time%20points%0Aof%20the%20same%20patient%20into%20the%20representation%20space.%20Next%2C%20a%20temporal%0Aequivariance%20module%20is%20trained%20to%20predict%20the%20representation%20of%20a%20later%20visit%0Abased%20on%20the%20representation%20from%20one%20of%20the%20previous%20visits%20and%20the%0Acorresponding%20time%20interval%20with%20a%20novel%20regularization%20loss%20term%20while%0Apreserving%20the%20invariance%20property%20to%20irrelevant%20image%20transformations.%20On%20a%0Alarge%20longitudinal%20dataset%2C%20our%20model%20clearly%20outperforms%20existing%20equivariant%0Acontrastive%20methods%20in%20predicting%20progression%20from%20intermediate%20age-related%0Amacular%20degeneration%20%28AMD%29%20to%20advanced%20wet-AMD%20within%20a%20specified%20time-window.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09404v3&entry.124074799=Read"},
{"title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in\n  LLMs", "author": "Zhongshen Zeng and Yinhong Liu and Yingjia Wan and Jingyao Li and Pengguang Chen and Jianbo Dai and Yuxuan Yao and Rongwu Xu and Zehan Qi and Wanru Zhao and Linling Shen and Jianqiao Lu and Haochen Tan and Yukang Chen and Hao Zhang and Zhan Shi and Bailin Wang and Zhijiang Guo and Jiaya Jia", "abstract": "  Large language models (LLMs) have shown increasing capability in\nproblem-solving and decision-making, largely based on the step-by-step\nchain-of-thought reasoning processes. However, evaluating these reasoning\nabilities has become increasingly challenging. Existing outcome-based\nbenchmarks are beginning to saturate, becoming less effective in tracking\nmeaningful progress. To address this, we present a process-based benchmark\nMR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and\nanalyse potential errors in automatically generated reasoning steps. Our\nmeta-reasoning paradigm is especially suited for system-2 slow thinking,\nmirroring the human cognitive process of carefully examining assumptions,\nconditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975\nquestions curated by human experts across a wide range of subjects, including\nphysics, chemistry, logic, coding, and more. Through our designed metrics for\nassessing meta-reasoning on this benchmark, we identify interesting limitations\nand weaknesses of current LLMs (open-source and closed-source models). For\nexample, with models like the o1 series from OpenAI demonstrating strong\nperformance by effectively scrutinizing the solution space, many other\nstate-of-the-art models fall significantly behind on MR-Ben, exposing potential\nshortcomings in their training strategies and inference methodologies.\n", "link": "http://arxiv.org/abs/2406.13975v3", "date": "2024-12-20", "relevancy": 2.0488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR-Ben%3A%20A%20Meta-Reasoning%20Benchmark%20for%20Evaluating%20System-2%20Thinking%20in%0A%20%20LLMs&body=Title%3A%20MR-Ben%3A%20A%20Meta-Reasoning%20Benchmark%20for%20Evaluating%20System-2%20Thinking%20in%0A%20%20LLMs%0AAuthor%3A%20Zhongshen%20Zeng%20and%20Yinhong%20Liu%20and%20Yingjia%20Wan%20and%20Jingyao%20Li%20and%20Pengguang%20Chen%20and%20Jianbo%20Dai%20and%20Yuxuan%20Yao%20and%20Rongwu%20Xu%20and%20Zehan%20Qi%20and%20Wanru%20Zhao%20and%20Linling%20Shen%20and%20Jianqiao%20Lu%20and%20Haochen%20Tan%20and%20Yukang%20Chen%20and%20Hao%20Zhang%20and%20Zhan%20Shi%20and%20Bailin%20Wang%20and%20Zhijiang%20Guo%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20increasing%20capability%20in%0Aproblem-solving%20and%20decision-making%2C%20largely%20based%20on%20the%20step-by-step%0Achain-of-thought%20reasoning%20processes.%20However%2C%20evaluating%20these%20reasoning%0Aabilities%20has%20become%20increasingly%20challenging.%20Existing%20outcome-based%0Abenchmarks%20are%20beginning%20to%20saturate%2C%20becoming%20less%20effective%20in%20tracking%0Ameaningful%20progress.%20To%20address%20this%2C%20we%20present%20a%20process-based%20benchmark%0AMR-Ben%20that%20demands%20a%20meta-reasoning%20skill%2C%20where%20LMs%20are%20asked%20to%20locate%20and%0Aanalyse%20potential%20errors%20in%20automatically%20generated%20reasoning%20steps.%20Our%0Ameta-reasoning%20paradigm%20is%20especially%20suited%20for%20system-2%20slow%20thinking%2C%0Amirroring%20the%20human%20cognitive%20process%20of%20carefully%20examining%20assumptions%2C%0Aconditions%2C%20calculations%2C%20and%20logic%20to%20identify%20mistakes.MR-Ben%20comprises%205%2C975%0Aquestions%20curated%20by%20human%20experts%20across%20a%20wide%20range%20of%20subjects%2C%20including%0Aphysics%2C%20chemistry%2C%20logic%2C%20coding%2C%20and%20more.%20Through%20our%20designed%20metrics%20for%0Aassessing%20meta-reasoning%20on%20this%20benchmark%2C%20we%20identify%20interesting%20limitations%0Aand%20weaknesses%20of%20current%20LLMs%20%28open-source%20and%20closed-source%20models%29.%20For%0Aexample%2C%20with%20models%20like%20the%20o1%20series%20from%20OpenAI%20demonstrating%20strong%0Aperformance%20by%20effectively%20scrutinizing%20the%20solution%20space%2C%20many%20other%0Astate-of-the-art%20models%20fall%20significantly%20behind%20on%20MR-Ben%2C%20exposing%20potential%0Ashortcomings%20in%20their%20training%20strategies%20and%20inference%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13975v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR-Ben%253A%2520A%2520Meta-Reasoning%2520Benchmark%2520for%2520Evaluating%2520System-2%2520Thinking%2520in%250A%2520%2520LLMs%26entry.906535625%3DZhongshen%2520Zeng%2520and%2520Yinhong%2520Liu%2520and%2520Yingjia%2520Wan%2520and%2520Jingyao%2520Li%2520and%2520Pengguang%2520Chen%2520and%2520Jianbo%2520Dai%2520and%2520Yuxuan%2520Yao%2520and%2520Rongwu%2520Xu%2520and%2520Zehan%2520Qi%2520and%2520Wanru%2520Zhao%2520and%2520Linling%2520Shen%2520and%2520Jianqiao%2520Lu%2520and%2520Haochen%2520Tan%2520and%2520Yukang%2520Chen%2520and%2520Hao%2520Zhang%2520and%2520Zhan%2520Shi%2520and%2520Bailin%2520Wang%2520and%2520Zhijiang%2520Guo%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520increasing%2520capability%2520in%250Aproblem-solving%2520and%2520decision-making%252C%2520largely%2520based%2520on%2520the%2520step-by-step%250Achain-of-thought%2520reasoning%2520processes.%2520However%252C%2520evaluating%2520these%2520reasoning%250Aabilities%2520has%2520become%2520increasingly%2520challenging.%2520Existing%2520outcome-based%250Abenchmarks%2520are%2520beginning%2520to%2520saturate%252C%2520becoming%2520less%2520effective%2520in%2520tracking%250Ameaningful%2520progress.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520process-based%2520benchmark%250AMR-Ben%2520that%2520demands%2520a%2520meta-reasoning%2520skill%252C%2520where%2520LMs%2520are%2520asked%2520to%2520locate%2520and%250Aanalyse%2520potential%2520errors%2520in%2520automatically%2520generated%2520reasoning%2520steps.%2520Our%250Ameta-reasoning%2520paradigm%2520is%2520especially%2520suited%2520for%2520system-2%2520slow%2520thinking%252C%250Amirroring%2520the%2520human%2520cognitive%2520process%2520of%2520carefully%2520examining%2520assumptions%252C%250Aconditions%252C%2520calculations%252C%2520and%2520logic%2520to%2520identify%2520mistakes.MR-Ben%2520comprises%25205%252C975%250Aquestions%2520curated%2520by%2520human%2520experts%2520across%2520a%2520wide%2520range%2520of%2520subjects%252C%2520including%250Aphysics%252C%2520chemistry%252C%2520logic%252C%2520coding%252C%2520and%2520more.%2520Through%2520our%2520designed%2520metrics%2520for%250Aassessing%2520meta-reasoning%2520on%2520this%2520benchmark%252C%2520we%2520identify%2520interesting%2520limitations%250Aand%2520weaknesses%2520of%2520current%2520LLMs%2520%2528open-source%2520and%2520closed-source%2520models%2529.%2520For%250Aexample%252C%2520with%2520models%2520like%2520the%2520o1%2520series%2520from%2520OpenAI%2520demonstrating%2520strong%250Aperformance%2520by%2520effectively%2520scrutinizing%2520the%2520solution%2520space%252C%2520many%2520other%250Astate-of-the-art%2520models%2520fall%2520significantly%2520behind%2520on%2520MR-Ben%252C%2520exposing%2520potential%250Ashortcomings%2520in%2520their%2520training%2520strategies%2520and%2520inference%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13975v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR-Ben%3A%20A%20Meta-Reasoning%20Benchmark%20for%20Evaluating%20System-2%20Thinking%20in%0A%20%20LLMs&entry.906535625=Zhongshen%20Zeng%20and%20Yinhong%20Liu%20and%20Yingjia%20Wan%20and%20Jingyao%20Li%20and%20Pengguang%20Chen%20and%20Jianbo%20Dai%20and%20Yuxuan%20Yao%20and%20Rongwu%20Xu%20and%20Zehan%20Qi%20and%20Wanru%20Zhao%20and%20Linling%20Shen%20and%20Jianqiao%20Lu%20and%20Haochen%20Tan%20and%20Yukang%20Chen%20and%20Hao%20Zhang%20and%20Zhan%20Shi%20and%20Bailin%20Wang%20and%20Zhijiang%20Guo%20and%20Jiaya%20Jia&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20increasing%20capability%20in%0Aproblem-solving%20and%20decision-making%2C%20largely%20based%20on%20the%20step-by-step%0Achain-of-thought%20reasoning%20processes.%20However%2C%20evaluating%20these%20reasoning%0Aabilities%20has%20become%20increasingly%20challenging.%20Existing%20outcome-based%0Abenchmarks%20are%20beginning%20to%20saturate%2C%20becoming%20less%20effective%20in%20tracking%0Ameaningful%20progress.%20To%20address%20this%2C%20we%20present%20a%20process-based%20benchmark%0AMR-Ben%20that%20demands%20a%20meta-reasoning%20skill%2C%20where%20LMs%20are%20asked%20to%20locate%20and%0Aanalyse%20potential%20errors%20in%20automatically%20generated%20reasoning%20steps.%20Our%0Ameta-reasoning%20paradigm%20is%20especially%20suited%20for%20system-2%20slow%20thinking%2C%0Amirroring%20the%20human%20cognitive%20process%20of%20carefully%20examining%20assumptions%2C%0Aconditions%2C%20calculations%2C%20and%20logic%20to%20identify%20mistakes.MR-Ben%20comprises%205%2C975%0Aquestions%20curated%20by%20human%20experts%20across%20a%20wide%20range%20of%20subjects%2C%20including%0Aphysics%2C%20chemistry%2C%20logic%2C%20coding%2C%20and%20more.%20Through%20our%20designed%20metrics%20for%0Aassessing%20meta-reasoning%20on%20this%20benchmark%2C%20we%20identify%20interesting%20limitations%0Aand%20weaknesses%20of%20current%20LLMs%20%28open-source%20and%20closed-source%20models%29.%20For%0Aexample%2C%20with%20models%20like%20the%20o1%20series%20from%20OpenAI%20demonstrating%20strong%0Aperformance%20by%20effectively%20scrutinizing%20the%20solution%20space%2C%20many%20other%0Astate-of-the-art%20models%20fall%20significantly%20behind%20on%20MR-Ben%2C%20exposing%20potential%0Ashortcomings%20in%20their%20training%20strategies%20and%20inference%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13975v3&entry.124074799=Read"},
{"title": "SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision\n  Mamba and Transformer Networks", "author": "Meng Lou and Yunxiang Fu and Yizhou Yu", "abstract": "  Due to the capability of dynamic state space models (SSMs) in capturing\nlong-range dependencies with linear-time computational complexity, Mamba has\nshown notable performance in NLP tasks. This has inspired the rapid development\nof Mamba-based vision models, resulting in promising results in visual\nrecognition tasks. However, such models are not capable of distilling features\nacross layers through feature aggregation, interaction, and selection.\nMoreover, existing cross-layer feature aggregation methods designed for CNNs or\nViTs are not practical in Mamba-based models due to high computational costs.\nTherefore, this paper aims to introduce an efficient cross-layer feature\naggregation mechanism for vision backbone networks. Inspired by the Retinal\nGanglion Cells (RGCs) in the human visual system, we propose a new sparse\ncross-layer connection mechanism termed SparX to effectively improve\ncross-layer feature interaction and reuse. Specifically, we build two different\ntypes of network layers: ganglion layers and normal layers. The former has\nhigher connectivity and complexity, enabling multi-layer feature aggregation\nand interaction in an input-dependent manner. In contrast, the latter has lower\nconnectivity and complexity. By interleaving these two types of layers, we\ndesign a new family of vision backbone networks with sparsely cross-connected\nlayers, achieving an excellent trade-off among model size, computational cost,\nmemory cost, and accuracy in comparison to its counterparts. For instance, with\nfewer parameters, SparX-Mamba-T improves the top-1 accuracy of VMamba-T from\n82.5\\% to 83.5\\%, while SparX-Swin-T achieves a 1.3\\% increase in top-1\naccuracy compared to Swin-T. Extensive experimental results demonstrate that\nour new connection mechanism possesses both superior performance and\ngeneralization capabilities on various vision tasks.\n", "link": "http://arxiv.org/abs/2409.09649v2", "date": "2024-12-20", "relevancy": 2.033, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5065}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparX%3A%20A%20Sparse%20Cross-Layer%20Connection%20Mechanism%20for%20Hierarchical%20Vision%0A%20%20Mamba%20and%20Transformer%20Networks&body=Title%3A%20SparX%3A%20A%20Sparse%20Cross-Layer%20Connection%20Mechanism%20for%20Hierarchical%20Vision%0A%20%20Mamba%20and%20Transformer%20Networks%0AAuthor%3A%20Meng%20Lou%20and%20Yunxiang%20Fu%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20Due%20to%20the%20capability%20of%20dynamic%20state%20space%20models%20%28SSMs%29%20in%20capturing%0Along-range%20dependencies%20with%20linear-time%20computational%20complexity%2C%20Mamba%20has%0Ashown%20notable%20performance%20in%20NLP%20tasks.%20This%20has%20inspired%20the%20rapid%20development%0Aof%20Mamba-based%20vision%20models%2C%20resulting%20in%20promising%20results%20in%20visual%0Arecognition%20tasks.%20However%2C%20such%20models%20are%20not%20capable%20of%20distilling%20features%0Aacross%20layers%20through%20feature%20aggregation%2C%20interaction%2C%20and%20selection.%0AMoreover%2C%20existing%20cross-layer%20feature%20aggregation%20methods%20designed%20for%20CNNs%20or%0AViTs%20are%20not%20practical%20in%20Mamba-based%20models%20due%20to%20high%20computational%20costs.%0ATherefore%2C%20this%20paper%20aims%20to%20introduce%20an%20efficient%20cross-layer%20feature%0Aaggregation%20mechanism%20for%20vision%20backbone%20networks.%20Inspired%20by%20the%20Retinal%0AGanglion%20Cells%20%28RGCs%29%20in%20the%20human%20visual%20system%2C%20we%20propose%20a%20new%20sparse%0Across-layer%20connection%20mechanism%20termed%20SparX%20to%20effectively%20improve%0Across-layer%20feature%20interaction%20and%20reuse.%20Specifically%2C%20we%20build%20two%20different%0Atypes%20of%20network%20layers%3A%20ganglion%20layers%20and%20normal%20layers.%20The%20former%20has%0Ahigher%20connectivity%20and%20complexity%2C%20enabling%20multi-layer%20feature%20aggregation%0Aand%20interaction%20in%20an%20input-dependent%20manner.%20In%20contrast%2C%20the%20latter%20has%20lower%0Aconnectivity%20and%20complexity.%20By%20interleaving%20these%20two%20types%20of%20layers%2C%20we%0Adesign%20a%20new%20family%20of%20vision%20backbone%20networks%20with%20sparsely%20cross-connected%0Alayers%2C%20achieving%20an%20excellent%20trade-off%20among%20model%20size%2C%20computational%20cost%2C%0Amemory%20cost%2C%20and%20accuracy%20in%20comparison%20to%20its%20counterparts.%20For%20instance%2C%20with%0Afewer%20parameters%2C%20SparX-Mamba-T%20improves%20the%20top-1%20accuracy%20of%20VMamba-T%20from%0A82.5%5C%25%20to%2083.5%5C%25%2C%20while%20SparX-Swin-T%20achieves%20a%201.3%5C%25%20increase%20in%20top-1%0Aaccuracy%20compared%20to%20Swin-T.%20Extensive%20experimental%20results%20demonstrate%20that%0Aour%20new%20connection%20mechanism%20possesses%20both%20superior%20performance%20and%0Ageneralization%20capabilities%20on%20various%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09649v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparX%253A%2520A%2520Sparse%2520Cross-Layer%2520Connection%2520Mechanism%2520for%2520Hierarchical%2520Vision%250A%2520%2520Mamba%2520and%2520Transformer%2520Networks%26entry.906535625%3DMeng%2520Lou%2520and%2520Yunxiang%2520Fu%2520and%2520Yizhou%2520Yu%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520capability%2520of%2520dynamic%2520state%2520space%2520models%2520%2528SSMs%2529%2520in%2520capturing%250Along-range%2520dependencies%2520with%2520linear-time%2520computational%2520complexity%252C%2520Mamba%2520has%250Ashown%2520notable%2520performance%2520in%2520NLP%2520tasks.%2520This%2520has%2520inspired%2520the%2520rapid%2520development%250Aof%2520Mamba-based%2520vision%2520models%252C%2520resulting%2520in%2520promising%2520results%2520in%2520visual%250Arecognition%2520tasks.%2520However%252C%2520such%2520models%2520are%2520not%2520capable%2520of%2520distilling%2520features%250Aacross%2520layers%2520through%2520feature%2520aggregation%252C%2520interaction%252C%2520and%2520selection.%250AMoreover%252C%2520existing%2520cross-layer%2520feature%2520aggregation%2520methods%2520designed%2520for%2520CNNs%2520or%250AViTs%2520are%2520not%2520practical%2520in%2520Mamba-based%2520models%2520due%2520to%2520high%2520computational%2520costs.%250ATherefore%252C%2520this%2520paper%2520aims%2520to%2520introduce%2520an%2520efficient%2520cross-layer%2520feature%250Aaggregation%2520mechanism%2520for%2520vision%2520backbone%2520networks.%2520Inspired%2520by%2520the%2520Retinal%250AGanglion%2520Cells%2520%2528RGCs%2529%2520in%2520the%2520human%2520visual%2520system%252C%2520we%2520propose%2520a%2520new%2520sparse%250Across-layer%2520connection%2520mechanism%2520termed%2520SparX%2520to%2520effectively%2520improve%250Across-layer%2520feature%2520interaction%2520and%2520reuse.%2520Specifically%252C%2520we%2520build%2520two%2520different%250Atypes%2520of%2520network%2520layers%253A%2520ganglion%2520layers%2520and%2520normal%2520layers.%2520The%2520former%2520has%250Ahigher%2520connectivity%2520and%2520complexity%252C%2520enabling%2520multi-layer%2520feature%2520aggregation%250Aand%2520interaction%2520in%2520an%2520input-dependent%2520manner.%2520In%2520contrast%252C%2520the%2520latter%2520has%2520lower%250Aconnectivity%2520and%2520complexity.%2520By%2520interleaving%2520these%2520two%2520types%2520of%2520layers%252C%2520we%250Adesign%2520a%2520new%2520family%2520of%2520vision%2520backbone%2520networks%2520with%2520sparsely%2520cross-connected%250Alayers%252C%2520achieving%2520an%2520excellent%2520trade-off%2520among%2520model%2520size%252C%2520computational%2520cost%252C%250Amemory%2520cost%252C%2520and%2520accuracy%2520in%2520comparison%2520to%2520its%2520counterparts.%2520For%2520instance%252C%2520with%250Afewer%2520parameters%252C%2520SparX-Mamba-T%2520improves%2520the%2520top-1%2520accuracy%2520of%2520VMamba-T%2520from%250A82.5%255C%2525%2520to%252083.5%255C%2525%252C%2520while%2520SparX-Swin-T%2520achieves%2520a%25201.3%255C%2525%2520increase%2520in%2520top-1%250Aaccuracy%2520compared%2520to%2520Swin-T.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%250Aour%2520new%2520connection%2520mechanism%2520possesses%2520both%2520superior%2520performance%2520and%250Ageneralization%2520capabilities%2520on%2520various%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09649v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparX%3A%20A%20Sparse%20Cross-Layer%20Connection%20Mechanism%20for%20Hierarchical%20Vision%0A%20%20Mamba%20and%20Transformer%20Networks&entry.906535625=Meng%20Lou%20and%20Yunxiang%20Fu%20and%20Yizhou%20Yu&entry.1292438233=%20%20Due%20to%20the%20capability%20of%20dynamic%20state%20space%20models%20%28SSMs%29%20in%20capturing%0Along-range%20dependencies%20with%20linear-time%20computational%20complexity%2C%20Mamba%20has%0Ashown%20notable%20performance%20in%20NLP%20tasks.%20This%20has%20inspired%20the%20rapid%20development%0Aof%20Mamba-based%20vision%20models%2C%20resulting%20in%20promising%20results%20in%20visual%0Arecognition%20tasks.%20However%2C%20such%20models%20are%20not%20capable%20of%20distilling%20features%0Aacross%20layers%20through%20feature%20aggregation%2C%20interaction%2C%20and%20selection.%0AMoreover%2C%20existing%20cross-layer%20feature%20aggregation%20methods%20designed%20for%20CNNs%20or%0AViTs%20are%20not%20practical%20in%20Mamba-based%20models%20due%20to%20high%20computational%20costs.%0ATherefore%2C%20this%20paper%20aims%20to%20introduce%20an%20efficient%20cross-layer%20feature%0Aaggregation%20mechanism%20for%20vision%20backbone%20networks.%20Inspired%20by%20the%20Retinal%0AGanglion%20Cells%20%28RGCs%29%20in%20the%20human%20visual%20system%2C%20we%20propose%20a%20new%20sparse%0Across-layer%20connection%20mechanism%20termed%20SparX%20to%20effectively%20improve%0Across-layer%20feature%20interaction%20and%20reuse.%20Specifically%2C%20we%20build%20two%20different%0Atypes%20of%20network%20layers%3A%20ganglion%20layers%20and%20normal%20layers.%20The%20former%20has%0Ahigher%20connectivity%20and%20complexity%2C%20enabling%20multi-layer%20feature%20aggregation%0Aand%20interaction%20in%20an%20input-dependent%20manner.%20In%20contrast%2C%20the%20latter%20has%20lower%0Aconnectivity%20and%20complexity.%20By%20interleaving%20these%20two%20types%20of%20layers%2C%20we%0Adesign%20a%20new%20family%20of%20vision%20backbone%20networks%20with%20sparsely%20cross-connected%0Alayers%2C%20achieving%20an%20excellent%20trade-off%20among%20model%20size%2C%20computational%20cost%2C%0Amemory%20cost%2C%20and%20accuracy%20in%20comparison%20to%20its%20counterparts.%20For%20instance%2C%20with%0Afewer%20parameters%2C%20SparX-Mamba-T%20improves%20the%20top-1%20accuracy%20of%20VMamba-T%20from%0A82.5%5C%25%20to%2083.5%5C%25%2C%20while%20SparX-Swin-T%20achieves%20a%201.3%5C%25%20increase%20in%20top-1%0Aaccuracy%20compared%20to%20Swin-T.%20Extensive%20experimental%20results%20demonstrate%20that%0Aour%20new%20connection%20mechanism%20possesses%20both%20superior%20performance%20and%0Ageneralization%20capabilities%20on%20various%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09649v2&entry.124074799=Read"},
{"title": "Deciphering the Underserved: Benchmarking LLM OCR for Low-Resource\n  Scripts", "author": "Muhammad Abdullah Sohail and Salaar Masood and Hamza Iqbal", "abstract": "  This study investigates the potential of Large Language Models (LLMs),\nparticularly GPT-4o, for Optical Character Recognition (OCR) in low-resource\nscripts such as Urdu, Albanian, and Tajik, with English serving as a benchmark.\nUsing a meticulously curated dataset of 2,520 images incorporating controlled\nvariations in text length, font size, background color, and blur, the research\nsimulates diverse real-world challenges. Results emphasize the limitations of\nzero-shot LLM-based OCR, particularly for linguistically complex scripts,\nhighlighting the need for annotated datasets and fine-tuned models. This work\nunderscores the urgency of addressing accessibility gaps in text digitization,\npaving the way for inclusive and robust OCR solutions for underserved\nlanguages.\n", "link": "http://arxiv.org/abs/2412.16119v1", "date": "2024-12-20", "relevancy": 2.0248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20the%20Underserved%3A%20Benchmarking%20LLM%20OCR%20for%20Low-Resource%0A%20%20Scripts&body=Title%3A%20Deciphering%20the%20Underserved%3A%20Benchmarking%20LLM%20OCR%20for%20Low-Resource%0A%20%20Scripts%0AAuthor%3A%20Muhammad%20Abdullah%20Sohail%20and%20Salaar%20Masood%20and%20Hamza%20Iqbal%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20GPT-4o%2C%20for%20Optical%20Character%20Recognition%20%28OCR%29%20in%20low-resource%0Ascripts%20such%20as%20Urdu%2C%20Albanian%2C%20and%20Tajik%2C%20with%20English%20serving%20as%20a%20benchmark.%0AUsing%20a%20meticulously%20curated%20dataset%20of%202%2C520%20images%20incorporating%20controlled%0Avariations%20in%20text%20length%2C%20font%20size%2C%20background%20color%2C%20and%20blur%2C%20the%20research%0Asimulates%20diverse%20real-world%20challenges.%20Results%20emphasize%20the%20limitations%20of%0Azero-shot%20LLM-based%20OCR%2C%20particularly%20for%20linguistically%20complex%20scripts%2C%0Ahighlighting%20the%20need%20for%20annotated%20datasets%20and%20fine-tuned%20models.%20This%20work%0Aunderscores%20the%20urgency%20of%20addressing%20accessibility%20gaps%20in%20text%20digitization%2C%0Apaving%20the%20way%20for%20inclusive%20and%20robust%20OCR%20solutions%20for%20underserved%0Alanguages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520the%2520Underserved%253A%2520Benchmarking%2520LLM%2520OCR%2520for%2520Low-Resource%250A%2520%2520Scripts%26entry.906535625%3DMuhammad%2520Abdullah%2520Sohail%2520and%2520Salaar%2520Masood%2520and%2520Hamza%2520Iqbal%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Aparticularly%2520GPT-4o%252C%2520for%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520in%2520low-resource%250Ascripts%2520such%2520as%2520Urdu%252C%2520Albanian%252C%2520and%2520Tajik%252C%2520with%2520English%2520serving%2520as%2520a%2520benchmark.%250AUsing%2520a%2520meticulously%2520curated%2520dataset%2520of%25202%252C520%2520images%2520incorporating%2520controlled%250Avariations%2520in%2520text%2520length%252C%2520font%2520size%252C%2520background%2520color%252C%2520and%2520blur%252C%2520the%2520research%250Asimulates%2520diverse%2520real-world%2520challenges.%2520Results%2520emphasize%2520the%2520limitations%2520of%250Azero-shot%2520LLM-based%2520OCR%252C%2520particularly%2520for%2520linguistically%2520complex%2520scripts%252C%250Ahighlighting%2520the%2520need%2520for%2520annotated%2520datasets%2520and%2520fine-tuned%2520models.%2520This%2520work%250Aunderscores%2520the%2520urgency%2520of%2520addressing%2520accessibility%2520gaps%2520in%2520text%2520digitization%252C%250Apaving%2520the%2520way%2520for%2520inclusive%2520and%2520robust%2520OCR%2520solutions%2520for%2520underserved%250Alanguages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20the%20Underserved%3A%20Benchmarking%20LLM%20OCR%20for%20Low-Resource%0A%20%20Scripts&entry.906535625=Muhammad%20Abdullah%20Sohail%20and%20Salaar%20Masood%20and%20Hamza%20Iqbal&entry.1292438233=%20%20This%20study%20investigates%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Aparticularly%20GPT-4o%2C%20for%20Optical%20Character%20Recognition%20%28OCR%29%20in%20low-resource%0Ascripts%20such%20as%20Urdu%2C%20Albanian%2C%20and%20Tajik%2C%20with%20English%20serving%20as%20a%20benchmark.%0AUsing%20a%20meticulously%20curated%20dataset%20of%202%2C520%20images%20incorporating%20controlled%0Avariations%20in%20text%20length%2C%20font%20size%2C%20background%20color%2C%20and%20blur%2C%20the%20research%0Asimulates%20diverse%20real-world%20challenges.%20Results%20emphasize%20the%20limitations%20of%0Azero-shot%20LLM-based%20OCR%2C%20particularly%20for%20linguistically%20complex%20scripts%2C%0Ahighlighting%20the%20need%20for%20annotated%20datasets%20and%20fine-tuned%20models.%20This%20work%0Aunderscores%20the%20urgency%20of%20addressing%20accessibility%20gaps%20in%20text%20digitization%2C%0Apaving%20the%20way%20for%20inclusive%20and%20robust%20OCR%20solutions%20for%20underserved%0Alanguages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16119v1&entry.124074799=Read"},
{"title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study", "author": "Imed Keraghel and Stanislas Morbieu and Mohamed Nadif", "abstract": "  Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.\n", "link": "http://arxiv.org/abs/2401.10825v3", "date": "2024-12-20", "relevancy": 2.0148, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Named%20Entity%20Recognition%3A%20A%20Comprehensive%20Survey%20and%0A%20%20Comparative%20Study&body=Title%3A%20Recent%20Advances%20in%20Named%20Entity%20Recognition%3A%20A%20Comprehensive%20Survey%20and%0A%20%20Comparative%20Study%0AAuthor%3A%20Imed%20Keraghel%20and%20Stanislas%20Morbieu%20and%20Mohamed%20Nadif%0AAbstract%3A%20%20%20Named%20Entity%20Recognition%20seeks%20to%20extract%20substrings%20within%20a%20text%20that%20name%0Areal-world%20objects%20and%20to%20determine%20their%20type%20%28for%20example%2C%20whether%20they%20refer%0Ato%20persons%20or%20organizations%29.%20In%20this%20survey%2C%20we%20first%20present%20an%20overview%20of%0Arecent%20popular%20approaches%2C%20including%20advancements%20in%20Transformer-based%20methods%0Aand%20Large%20Language%20Models%20%28LLMs%29%20that%20have%20not%20had%20much%20coverage%20in%20other%0Asurveys.%20In%20addition%2C%20we%20discuss%20reinforcement%20learning%20and%20graph-based%0Aapproaches%2C%20highlighting%20their%20role%20in%20enhancing%20NER%20performance.%20Second%2C%20we%0Afocus%20on%20methods%20designed%20for%20datasets%20with%20scarce%20annotations.%20Third%2C%20we%0Aevaluate%20the%20performance%20of%20the%20main%20NER%20implementations%20on%20a%20variety%20of%0Adatasets%20with%20differing%20characteristics%20%28as%20regards%20their%20domain%2C%20their%20size%2C%0Aand%20their%20number%20of%20classes%29.%20We%20thus%20provide%20a%20deep%20comparison%20of%20algorithms%0Athat%20have%20never%20been%20considered%20together.%20Our%20experiments%20shed%20some%20light%20on%0Ahow%20the%20characteristics%20of%20datasets%20affect%20the%20behavior%20of%20the%20methods%20we%0Acompare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10825v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Named%2520Entity%2520Recognition%253A%2520A%2520Comprehensive%2520Survey%2520and%250A%2520%2520Comparative%2520Study%26entry.906535625%3DImed%2520Keraghel%2520and%2520Stanislas%2520Morbieu%2520and%2520Mohamed%2520Nadif%26entry.1292438233%3D%2520%2520Named%2520Entity%2520Recognition%2520seeks%2520to%2520extract%2520substrings%2520within%2520a%2520text%2520that%2520name%250Areal-world%2520objects%2520and%2520to%2520determine%2520their%2520type%2520%2528for%2520example%252C%2520whether%2520they%2520refer%250Ato%2520persons%2520or%2520organizations%2529.%2520In%2520this%2520survey%252C%2520we%2520first%2520present%2520an%2520overview%2520of%250Arecent%2520popular%2520approaches%252C%2520including%2520advancements%2520in%2520Transformer-based%2520methods%250Aand%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520have%2520not%2520had%2520much%2520coverage%2520in%2520other%250Asurveys.%2520In%2520addition%252C%2520we%2520discuss%2520reinforcement%2520learning%2520and%2520graph-based%250Aapproaches%252C%2520highlighting%2520their%2520role%2520in%2520enhancing%2520NER%2520performance.%2520Second%252C%2520we%250Afocus%2520on%2520methods%2520designed%2520for%2520datasets%2520with%2520scarce%2520annotations.%2520Third%252C%2520we%250Aevaluate%2520the%2520performance%2520of%2520the%2520main%2520NER%2520implementations%2520on%2520a%2520variety%2520of%250Adatasets%2520with%2520differing%2520characteristics%2520%2528as%2520regards%2520their%2520domain%252C%2520their%2520size%252C%250Aand%2520their%2520number%2520of%2520classes%2529.%2520We%2520thus%2520provide%2520a%2520deep%2520comparison%2520of%2520algorithms%250Athat%2520have%2520never%2520been%2520considered%2520together.%2520Our%2520experiments%2520shed%2520some%2520light%2520on%250Ahow%2520the%2520characteristics%2520of%2520datasets%2520affect%2520the%2520behavior%2520of%2520the%2520methods%2520we%250Acompare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10825v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Named%20Entity%20Recognition%3A%20A%20Comprehensive%20Survey%20and%0A%20%20Comparative%20Study&entry.906535625=Imed%20Keraghel%20and%20Stanislas%20Morbieu%20and%20Mohamed%20Nadif&entry.1292438233=%20%20Named%20Entity%20Recognition%20seeks%20to%20extract%20substrings%20within%20a%20text%20that%20name%0Areal-world%20objects%20and%20to%20determine%20their%20type%20%28for%20example%2C%20whether%20they%20refer%0Ato%20persons%20or%20organizations%29.%20In%20this%20survey%2C%20we%20first%20present%20an%20overview%20of%0Arecent%20popular%20approaches%2C%20including%20advancements%20in%20Transformer-based%20methods%0Aand%20Large%20Language%20Models%20%28LLMs%29%20that%20have%20not%20had%20much%20coverage%20in%20other%0Asurveys.%20In%20addition%2C%20we%20discuss%20reinforcement%20learning%20and%20graph-based%0Aapproaches%2C%20highlighting%20their%20role%20in%20enhancing%20NER%20performance.%20Second%2C%20we%0Afocus%20on%20methods%20designed%20for%20datasets%20with%20scarce%20annotations.%20Third%2C%20we%0Aevaluate%20the%20performance%20of%20the%20main%20NER%20implementations%20on%20a%20variety%20of%0Adatasets%20with%20differing%20characteristics%20%28as%20regards%20their%20domain%2C%20their%20size%2C%0Aand%20their%20number%20of%20classes%29.%20We%20thus%20provide%20a%20deep%20comparison%20of%20algorithms%0Athat%20have%20never%20been%20considered%20together.%20Our%20experiments%20shed%20some%20light%20on%0Ahow%20the%20characteristics%20of%20datasets%20affect%20the%20behavior%20of%20the%20methods%20we%0Acompare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10825v3&entry.124074799=Read"},
{"title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning", "author": "Jiapu Wang and Kai Sun and Linhao Luo and Wei Wei and Yongli Hu and Alan Wee-Chung Liew and Shirui Pan and Baocai Yin", "abstract": "  Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks.\n", "link": "http://arxiv.org/abs/2405.14170v2", "date": "2024-12-20", "relevancy": 2.0078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4936}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models-guided%20Dynamic%20Adaptation%20for%20Temporal%20Knowledge%0A%20%20Graph%20Reasoning&body=Title%3A%20Large%20Language%20Models-guided%20Dynamic%20Adaptation%20for%20Temporal%20Knowledge%0A%20%20Graph%20Reasoning%0AAuthor%3A%20Jiapu%20Wang%20and%20Kai%20Sun%20and%20Linhao%20Luo%20and%20Wei%20Wei%20and%20Yongli%20Hu%20and%20Alan%20Wee-Chung%20Liew%20and%20Shirui%20Pan%20and%20Baocai%20Yin%0AAbstract%3A%20%20%20Temporal%20Knowledge%20Graph%20Reasoning%20%28TKGR%29%20is%20the%20process%20of%20utilizing%0Atemporal%20information%20to%20capture%20complex%20relations%20within%20a%20Temporal%20Knowledge%0AGraph%20%28TKG%29%20to%20infer%20new%20knowledge.%20Conventional%20methods%20in%20TKGR%20typically%0Adepend%20on%20deep%20learning%20algorithms%20or%20temporal%20logical%20rules.%20However%2C%20deep%0Alearning-based%20TKGRs%20often%20lack%20interpretability%2C%20whereas%20rule-based%20TKGRs%0Astruggle%20to%20effectively%20learn%20temporal%20rules%20that%20capture%20temporal%20patterns.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20extensive%20knowledge%0Aand%20remarkable%20proficiency%20in%20temporal%20reasoning.%20Consequently%2C%20the%20employment%0Aof%20LLMs%20for%20Temporal%20Knowledge%20Graph%20Reasoning%20%28TKGR%29%20has%20sparked%20increasing%0Ainterest%20among%20researchers.%20Nonetheless%2C%20LLMs%20are%20known%20to%20function%20as%20black%0Aboxes%2C%20making%20it%20challenging%20to%20comprehend%20their%20reasoning%20process.%0AAdditionally%2C%20due%20to%20the%20resource-intensive%20nature%20of%20fine-tuning%2C%20promptly%0Aupdating%20LLMs%20to%20integrate%20evolving%20knowledge%20within%20TKGs%20for%20reasoning%20is%0Aimpractical.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20Large%0ALanguage%20Models-guided%20Dynamic%20Adaptation%20%28LLM-DA%29%20method%20for%20reasoning%20on%0ATKGs.%20Specifically%2C%20LLM-DA%20harnesses%20the%20capabilities%20of%20LLMs%20to%20analyze%0Ahistorical%20data%20and%20extract%20temporal%20logical%20rules.%20These%20rules%20unveil%20temporal%0Apatterns%20and%20facilitate%20interpretable%20reasoning.%20To%20account%20for%20the%20evolving%0Anature%20of%20TKGs%2C%20a%20dynamic%20adaptation%20strategy%20is%20proposed%20to%20update%20the%0ALLM-generated%20rules%20with%20the%20latest%20events.%20This%20ensures%20that%20the%20extracted%0Arules%20always%20incorporate%20the%20most%20recent%20knowledge%20and%20better%20generalize%20to%20the%0Apredictions%20on%20future%20events.%20Experimental%20results%20show%20that%20without%20the%20need%0Aof%20fine-tuning%2C%20LLM-DA%20significantly%20improves%20the%20accuracy%20of%20reasoning%20over%0Aseveral%20common%20datasets%2C%20providing%20a%20robust%20framework%20for%20TKGR%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models-guided%2520Dynamic%2520Adaptation%2520for%2520Temporal%2520Knowledge%250A%2520%2520Graph%2520Reasoning%26entry.906535625%3DJiapu%2520Wang%2520and%2520Kai%2520Sun%2520and%2520Linhao%2520Luo%2520and%2520Wei%2520Wei%2520and%2520Yongli%2520Hu%2520and%2520Alan%2520Wee-Chung%2520Liew%2520and%2520Shirui%2520Pan%2520and%2520Baocai%2520Yin%26entry.1292438233%3D%2520%2520Temporal%2520Knowledge%2520Graph%2520Reasoning%2520%2528TKGR%2529%2520is%2520the%2520process%2520of%2520utilizing%250Atemporal%2520information%2520to%2520capture%2520complex%2520relations%2520within%2520a%2520Temporal%2520Knowledge%250AGraph%2520%2528TKG%2529%2520to%2520infer%2520new%2520knowledge.%2520Conventional%2520methods%2520in%2520TKGR%2520typically%250Adepend%2520on%2520deep%2520learning%2520algorithms%2520or%2520temporal%2520logical%2520rules.%2520However%252C%2520deep%250Alearning-based%2520TKGRs%2520often%2520lack%2520interpretability%252C%2520whereas%2520rule-based%2520TKGRs%250Astruggle%2520to%2520effectively%2520learn%2520temporal%2520rules%2520that%2520capture%2520temporal%2520patterns.%250ARecently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520extensive%2520knowledge%250Aand%2520remarkable%2520proficiency%2520in%2520temporal%2520reasoning.%2520Consequently%252C%2520the%2520employment%250Aof%2520LLMs%2520for%2520Temporal%2520Knowledge%2520Graph%2520Reasoning%2520%2528TKGR%2529%2520has%2520sparked%2520increasing%250Ainterest%2520among%2520researchers.%2520Nonetheless%252C%2520LLMs%2520are%2520known%2520to%2520function%2520as%2520black%250Aboxes%252C%2520making%2520it%2520challenging%2520to%2520comprehend%2520their%2520reasoning%2520process.%250AAdditionally%252C%2520due%2520to%2520the%2520resource-intensive%2520nature%2520of%2520fine-tuning%252C%2520promptly%250Aupdating%2520LLMs%2520to%2520integrate%2520evolving%2520knowledge%2520within%2520TKGs%2520for%2520reasoning%2520is%250Aimpractical.%2520To%2520address%2520these%2520challenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520Large%250ALanguage%2520Models-guided%2520Dynamic%2520Adaptation%2520%2528LLM-DA%2529%2520method%2520for%2520reasoning%2520on%250ATKGs.%2520Specifically%252C%2520LLM-DA%2520harnesses%2520the%2520capabilities%2520of%2520LLMs%2520to%2520analyze%250Ahistorical%2520data%2520and%2520extract%2520temporal%2520logical%2520rules.%2520These%2520rules%2520unveil%2520temporal%250Apatterns%2520and%2520facilitate%2520interpretable%2520reasoning.%2520To%2520account%2520for%2520the%2520evolving%250Anature%2520of%2520TKGs%252C%2520a%2520dynamic%2520adaptation%2520strategy%2520is%2520proposed%2520to%2520update%2520the%250ALLM-generated%2520rules%2520with%2520the%2520latest%2520events.%2520This%2520ensures%2520that%2520the%2520extracted%250Arules%2520always%2520incorporate%2520the%2520most%2520recent%2520knowledge%2520and%2520better%2520generalize%2520to%2520the%250Apredictions%2520on%2520future%2520events.%2520Experimental%2520results%2520show%2520that%2520without%2520the%2520need%250Aof%2520fine-tuning%252C%2520LLM-DA%2520significantly%2520improves%2520the%2520accuracy%2520of%2520reasoning%2520over%250Aseveral%2520common%2520datasets%252C%2520providing%2520a%2520robust%2520framework%2520for%2520TKGR%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models-guided%20Dynamic%20Adaptation%20for%20Temporal%20Knowledge%0A%20%20Graph%20Reasoning&entry.906535625=Jiapu%20Wang%20and%20Kai%20Sun%20and%20Linhao%20Luo%20and%20Wei%20Wei%20and%20Yongli%20Hu%20and%20Alan%20Wee-Chung%20Liew%20and%20Shirui%20Pan%20and%20Baocai%20Yin&entry.1292438233=%20%20Temporal%20Knowledge%20Graph%20Reasoning%20%28TKGR%29%20is%20the%20process%20of%20utilizing%0Atemporal%20information%20to%20capture%20complex%20relations%20within%20a%20Temporal%20Knowledge%0AGraph%20%28TKG%29%20to%20infer%20new%20knowledge.%20Conventional%20methods%20in%20TKGR%20typically%0Adepend%20on%20deep%20learning%20algorithms%20or%20temporal%20logical%20rules.%20However%2C%20deep%0Alearning-based%20TKGRs%20often%20lack%20interpretability%2C%20whereas%20rule-based%20TKGRs%0Astruggle%20to%20effectively%20learn%20temporal%20rules%20that%20capture%20temporal%20patterns.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20extensive%20knowledge%0Aand%20remarkable%20proficiency%20in%20temporal%20reasoning.%20Consequently%2C%20the%20employment%0Aof%20LLMs%20for%20Temporal%20Knowledge%20Graph%20Reasoning%20%28TKGR%29%20has%20sparked%20increasing%0Ainterest%20among%20researchers.%20Nonetheless%2C%20LLMs%20are%20known%20to%20function%20as%20black%0Aboxes%2C%20making%20it%20challenging%20to%20comprehend%20their%20reasoning%20process.%0AAdditionally%2C%20due%20to%20the%20resource-intensive%20nature%20of%20fine-tuning%2C%20promptly%0Aupdating%20LLMs%20to%20integrate%20evolving%20knowledge%20within%20TKGs%20for%20reasoning%20is%0Aimpractical.%20To%20address%20these%20challenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20Large%0ALanguage%20Models-guided%20Dynamic%20Adaptation%20%28LLM-DA%29%20method%20for%20reasoning%20on%0ATKGs.%20Specifically%2C%20LLM-DA%20harnesses%20the%20capabilities%20of%20LLMs%20to%20analyze%0Ahistorical%20data%20and%20extract%20temporal%20logical%20rules.%20These%20rules%20unveil%20temporal%0Apatterns%20and%20facilitate%20interpretable%20reasoning.%20To%20account%20for%20the%20evolving%0Anature%20of%20TKGs%2C%20a%20dynamic%20adaptation%20strategy%20is%20proposed%20to%20update%20the%0ALLM-generated%20rules%20with%20the%20latest%20events.%20This%20ensures%20that%20the%20extracted%0Arules%20always%20incorporate%20the%20most%20recent%20knowledge%20and%20better%20generalize%20to%20the%0Apredictions%20on%20future%20events.%20Experimental%20results%20show%20that%20without%20the%20need%0Aof%20fine-tuning%2C%20LLM-DA%20significantly%20improves%20the%20accuracy%20of%20reasoning%20over%0Aseveral%20common%20datasets%2C%20providing%20a%20robust%20framework%20for%20TKGR%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14170v2&entry.124074799=Read"},
{"title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\n  Up", "author": "Songhua Liu and Zhenxiong Tan and Xinchao Wang", "abstract": "  Diffusion Transformers (DiT) have become a leading architecture in image\ngeneration. However, the quadratic complexity of attention mechanisms, which\nare responsible for modeling token-wise relationships, results in significant\nlatency when generating high-resolution images. To address this issue, we aim\nat a linear attention mechanism in this paper that reduces the complexity of\npre-trained DiTs to linear. We begin our exploration with a comprehensive\nsummary of existing efficient attention mechanisms and identify four key\nfactors crucial for successful linearization of pre-trained DiTs: locality,\nformulation consistency, high-rank attention maps, and feature integrity. Based\non these insights, we introduce a convolution-like local attention strategy\ntermed CLEAR, which limits feature interactions to a local window around each\nquery token, and thus achieves linear complexity. Our experiments indicate\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\nDiT to a student model with linear complexity, yielding results comparable to\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\nand accelerates generation by 6.3 times for generating 8K-resolution images.\nFurthermore, we investigate favorable properties in the distilled attention\nlayers, such as zero-shot generalization cross various models and plugins, and\nimproved support for multi-GPU parallel inference. Models and codes are\navailable here: https://github.com/Huage001/CLEAR.\n", "link": "http://arxiv.org/abs/2412.16112v1", "date": "2024-12-20", "relevancy": 2.0045, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6809}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6589}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEAR%3A%20Conv-Like%20Linearization%20Revs%20Pre-Trained%20Diffusion%20Transformers%0A%20%20Up&body=Title%3A%20CLEAR%3A%20Conv-Like%20Linearization%20Revs%20Pre-Trained%20Diffusion%20Transformers%0A%20%20Up%0AAuthor%3A%20Songhua%20Liu%20and%20Zhenxiong%20Tan%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiT%29%20have%20become%20a%20leading%20architecture%20in%20image%0Ageneration.%20However%2C%20the%20quadratic%20complexity%20of%20attention%20mechanisms%2C%20which%0Aare%20responsible%20for%20modeling%20token-wise%20relationships%2C%20results%20in%20significant%0Alatency%20when%20generating%20high-resolution%20images.%20To%20address%20this%20issue%2C%20we%20aim%0Aat%20a%20linear%20attention%20mechanism%20in%20this%20paper%20that%20reduces%20the%20complexity%20of%0Apre-trained%20DiTs%20to%20linear.%20We%20begin%20our%20exploration%20with%20a%20comprehensive%0Asummary%20of%20existing%20efficient%20attention%20mechanisms%20and%20identify%20four%20key%0Afactors%20crucial%20for%20successful%20linearization%20of%20pre-trained%20DiTs%3A%20locality%2C%0Aformulation%20consistency%2C%20high-rank%20attention%20maps%2C%20and%20feature%20integrity.%20Based%0Aon%20these%20insights%2C%20we%20introduce%20a%20convolution-like%20local%20attention%20strategy%0Atermed%20CLEAR%2C%20which%20limits%20feature%20interactions%20to%20a%20local%20window%20around%20each%0Aquery%20token%2C%20and%20thus%20achieves%20linear%20complexity.%20Our%20experiments%20indicate%0Athat%2C%20by%20fine-tuning%20the%20attention%20layer%20on%20merely%2010K%20self-generated%20samples%0Afor%2010K%20iterations%2C%20we%20can%20effectively%20transfer%20knowledge%20from%20a%20pre-trained%0ADiT%20to%20a%20student%20model%20with%20linear%20complexity%2C%20yielding%20results%20comparable%20to%0Athe%20teacher%20model.%20Simultaneously%2C%20it%20reduces%20attention%20computations%20by%2099.5%25%0Aand%20accelerates%20generation%20by%206.3%20times%20for%20generating%208K-resolution%20images.%0AFurthermore%2C%20we%20investigate%20favorable%20properties%20in%20the%20distilled%20attention%0Alayers%2C%20such%20as%20zero-shot%20generalization%20cross%20various%20models%20and%20plugins%2C%20and%0Aimproved%20support%20for%20multi-GPU%20parallel%20inference.%20Models%20and%20codes%20are%0Aavailable%20here%3A%20https%3A//github.com/Huage001/CLEAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEAR%253A%2520Conv-Like%2520Linearization%2520Revs%2520Pre-Trained%2520Diffusion%2520Transformers%250A%2520%2520Up%26entry.906535625%3DSonghua%2520Liu%2520and%2520Zhenxiong%2520Tan%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiT%2529%2520have%2520become%2520a%2520leading%2520architecture%2520in%2520image%250Ageneration.%2520However%252C%2520the%2520quadratic%2520complexity%2520of%2520attention%2520mechanisms%252C%2520which%250Aare%2520responsible%2520for%2520modeling%2520token-wise%2520relationships%252C%2520results%2520in%2520significant%250Alatency%2520when%2520generating%2520high-resolution%2520images.%2520To%2520address%2520this%2520issue%252C%2520we%2520aim%250Aat%2520a%2520linear%2520attention%2520mechanism%2520in%2520this%2520paper%2520that%2520reduces%2520the%2520complexity%2520of%250Apre-trained%2520DiTs%2520to%2520linear.%2520We%2520begin%2520our%2520exploration%2520with%2520a%2520comprehensive%250Asummary%2520of%2520existing%2520efficient%2520attention%2520mechanisms%2520and%2520identify%2520four%2520key%250Afactors%2520crucial%2520for%2520successful%2520linearization%2520of%2520pre-trained%2520DiTs%253A%2520locality%252C%250Aformulation%2520consistency%252C%2520high-rank%2520attention%2520maps%252C%2520and%2520feature%2520integrity.%2520Based%250Aon%2520these%2520insights%252C%2520we%2520introduce%2520a%2520convolution-like%2520local%2520attention%2520strategy%250Atermed%2520CLEAR%252C%2520which%2520limits%2520feature%2520interactions%2520to%2520a%2520local%2520window%2520around%2520each%250Aquery%2520token%252C%2520and%2520thus%2520achieves%2520linear%2520complexity.%2520Our%2520experiments%2520indicate%250Athat%252C%2520by%2520fine-tuning%2520the%2520attention%2520layer%2520on%2520merely%252010K%2520self-generated%2520samples%250Afor%252010K%2520iterations%252C%2520we%2520can%2520effectively%2520transfer%2520knowledge%2520from%2520a%2520pre-trained%250ADiT%2520to%2520a%2520student%2520model%2520with%2520linear%2520complexity%252C%2520yielding%2520results%2520comparable%2520to%250Athe%2520teacher%2520model.%2520Simultaneously%252C%2520it%2520reduces%2520attention%2520computations%2520by%252099.5%2525%250Aand%2520accelerates%2520generation%2520by%25206.3%2520times%2520for%2520generating%25208K-resolution%2520images.%250AFurthermore%252C%2520we%2520investigate%2520favorable%2520properties%2520in%2520the%2520distilled%2520attention%250Alayers%252C%2520such%2520as%2520zero-shot%2520generalization%2520cross%2520various%2520models%2520and%2520plugins%252C%2520and%250Aimproved%2520support%2520for%2520multi-GPU%2520parallel%2520inference.%2520Models%2520and%2520codes%2520are%250Aavailable%2520here%253A%2520https%253A//github.com/Huage001/CLEAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEAR%3A%20Conv-Like%20Linearization%20Revs%20Pre-Trained%20Diffusion%20Transformers%0A%20%20Up&entry.906535625=Songhua%20Liu%20and%20Zhenxiong%20Tan%20and%20Xinchao%20Wang&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiT%29%20have%20become%20a%20leading%20architecture%20in%20image%0Ageneration.%20However%2C%20the%20quadratic%20complexity%20of%20attention%20mechanisms%2C%20which%0Aare%20responsible%20for%20modeling%20token-wise%20relationships%2C%20results%20in%20significant%0Alatency%20when%20generating%20high-resolution%20images.%20To%20address%20this%20issue%2C%20we%20aim%0Aat%20a%20linear%20attention%20mechanism%20in%20this%20paper%20that%20reduces%20the%20complexity%20of%0Apre-trained%20DiTs%20to%20linear.%20We%20begin%20our%20exploration%20with%20a%20comprehensive%0Asummary%20of%20existing%20efficient%20attention%20mechanisms%20and%20identify%20four%20key%0Afactors%20crucial%20for%20successful%20linearization%20of%20pre-trained%20DiTs%3A%20locality%2C%0Aformulation%20consistency%2C%20high-rank%20attention%20maps%2C%20and%20feature%20integrity.%20Based%0Aon%20these%20insights%2C%20we%20introduce%20a%20convolution-like%20local%20attention%20strategy%0Atermed%20CLEAR%2C%20which%20limits%20feature%20interactions%20to%20a%20local%20window%20around%20each%0Aquery%20token%2C%20and%20thus%20achieves%20linear%20complexity.%20Our%20experiments%20indicate%0Athat%2C%20by%20fine-tuning%20the%20attention%20layer%20on%20merely%2010K%20self-generated%20samples%0Afor%2010K%20iterations%2C%20we%20can%20effectively%20transfer%20knowledge%20from%20a%20pre-trained%0ADiT%20to%20a%20student%20model%20with%20linear%20complexity%2C%20yielding%20results%20comparable%20to%0Athe%20teacher%20model.%20Simultaneously%2C%20it%20reduces%20attention%20computations%20by%2099.5%25%0Aand%20accelerates%20generation%20by%206.3%20times%20for%20generating%208K-resolution%20images.%0AFurthermore%2C%20we%20investigate%20favorable%20properties%20in%20the%20distilled%20attention%0Alayers%2C%20such%20as%20zero-shot%20generalization%20cross%20various%20models%20and%20plugins%2C%20and%0Aimproved%20support%20for%20multi-GPU%20parallel%20inference.%20Models%20and%20codes%20are%0Aavailable%20here%3A%20https%3A//github.com/Huage001/CLEAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16112v1&entry.124074799=Read"},
{"title": "EF-Net: A Deep Learning Approach Combining Word Embeddings and Feature\n  Fusion for Patient Disposition Analysis", "author": "Nafisa Binte Feroz and Chandrima Sarker and Tanzima Ahsan and K M Arefeen Sultan and Raqeebir Rab", "abstract": "  One of the most urgent problems is the overcrowding in emergency departments\n(EDs), caused by an aging population and rising healthcare costs. Patient\ndispositions have become more complex as a result of the strain on hospital\ninfrastructure and the scarcity of medical resources. Individuals with more\ndangerous health issues should be prioritized in the emergency room. Thus, our\nresearch aims to develop a prediction model for patient disposition using\nEF-Net. This model will incorporate categorical features into the neural\nnetwork layer and add numerical features with the embedded categorical\nfeatures. We combine the EF-Net and XGBoost models to attain higher accuracy in\nour results. The result is generated using the soft voting technique. In\nEF-Net, we attained an accuracy of 95.33%, whereas in the Ensemble Model, we\nachieved an accuracy of 96%. The experiment's analysis shows that EF-Net\nsurpasses existing works in accuracy, AUROC, and F1-Score on the MIMIC-IV-ED\ndataset, demonstrating its potential as a scalable solution for patient\ndisposition assessment. Our code is available at\nhttps://github.com/nafisa67/thesis\n", "link": "http://arxiv.org/abs/2412.16134v1", "date": "2024-12-20", "relevancy": 1.9818, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5386}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5106}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EF-Net%3A%20A%20Deep%20Learning%20Approach%20Combining%20Word%20Embeddings%20and%20Feature%0A%20%20Fusion%20for%20Patient%20Disposition%20Analysis&body=Title%3A%20EF-Net%3A%20A%20Deep%20Learning%20Approach%20Combining%20Word%20Embeddings%20and%20Feature%0A%20%20Fusion%20for%20Patient%20Disposition%20Analysis%0AAuthor%3A%20Nafisa%20Binte%20Feroz%20and%20Chandrima%20Sarker%20and%20Tanzima%20Ahsan%20and%20K%20M%20Arefeen%20Sultan%20and%20Raqeebir%20Rab%0AAbstract%3A%20%20%20One%20of%20the%20most%20urgent%20problems%20is%20the%20overcrowding%20in%20emergency%20departments%0A%28EDs%29%2C%20caused%20by%20an%20aging%20population%20and%20rising%20healthcare%20costs.%20Patient%0Adispositions%20have%20become%20more%20complex%20as%20a%20result%20of%20the%20strain%20on%20hospital%0Ainfrastructure%20and%20the%20scarcity%20of%20medical%20resources.%20Individuals%20with%20more%0Adangerous%20health%20issues%20should%20be%20prioritized%20in%20the%20emergency%20room.%20Thus%2C%20our%0Aresearch%20aims%20to%20develop%20a%20prediction%20model%20for%20patient%20disposition%20using%0AEF-Net.%20This%20model%20will%20incorporate%20categorical%20features%20into%20the%20neural%0Anetwork%20layer%20and%20add%20numerical%20features%20with%20the%20embedded%20categorical%0Afeatures.%20We%20combine%20the%20EF-Net%20and%20XGBoost%20models%20to%20attain%20higher%20accuracy%20in%0Aour%20results.%20The%20result%20is%20generated%20using%20the%20soft%20voting%20technique.%20In%0AEF-Net%2C%20we%20attained%20an%20accuracy%20of%2095.33%25%2C%20whereas%20in%20the%20Ensemble%20Model%2C%20we%0Aachieved%20an%20accuracy%20of%2096%25.%20The%20experiment%27s%20analysis%20shows%20that%20EF-Net%0Asurpasses%20existing%20works%20in%20accuracy%2C%20AUROC%2C%20and%20F1-Score%20on%20the%20MIMIC-IV-ED%0Adataset%2C%20demonstrating%20its%20potential%20as%20a%20scalable%20solution%20for%20patient%0Adisposition%20assessment.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/nafisa67/thesis%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEF-Net%253A%2520A%2520Deep%2520Learning%2520Approach%2520Combining%2520Word%2520Embeddings%2520and%2520Feature%250A%2520%2520Fusion%2520for%2520Patient%2520Disposition%2520Analysis%26entry.906535625%3DNafisa%2520Binte%2520Feroz%2520and%2520Chandrima%2520Sarker%2520and%2520Tanzima%2520Ahsan%2520and%2520K%2520M%2520Arefeen%2520Sultan%2520and%2520Raqeebir%2520Rab%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520urgent%2520problems%2520is%2520the%2520overcrowding%2520in%2520emergency%2520departments%250A%2528EDs%2529%252C%2520caused%2520by%2520an%2520aging%2520population%2520and%2520rising%2520healthcare%2520costs.%2520Patient%250Adispositions%2520have%2520become%2520more%2520complex%2520as%2520a%2520result%2520of%2520the%2520strain%2520on%2520hospital%250Ainfrastructure%2520and%2520the%2520scarcity%2520of%2520medical%2520resources.%2520Individuals%2520with%2520more%250Adangerous%2520health%2520issues%2520should%2520be%2520prioritized%2520in%2520the%2520emergency%2520room.%2520Thus%252C%2520our%250Aresearch%2520aims%2520to%2520develop%2520a%2520prediction%2520model%2520for%2520patient%2520disposition%2520using%250AEF-Net.%2520This%2520model%2520will%2520incorporate%2520categorical%2520features%2520into%2520the%2520neural%250Anetwork%2520layer%2520and%2520add%2520numerical%2520features%2520with%2520the%2520embedded%2520categorical%250Afeatures.%2520We%2520combine%2520the%2520EF-Net%2520and%2520XGBoost%2520models%2520to%2520attain%2520higher%2520accuracy%2520in%250Aour%2520results.%2520The%2520result%2520is%2520generated%2520using%2520the%2520soft%2520voting%2520technique.%2520In%250AEF-Net%252C%2520we%2520attained%2520an%2520accuracy%2520of%252095.33%2525%252C%2520whereas%2520in%2520the%2520Ensemble%2520Model%252C%2520we%250Aachieved%2520an%2520accuracy%2520of%252096%2525.%2520The%2520experiment%2527s%2520analysis%2520shows%2520that%2520EF-Net%250Asurpasses%2520existing%2520works%2520in%2520accuracy%252C%2520AUROC%252C%2520and%2520F1-Score%2520on%2520the%2520MIMIC-IV-ED%250Adataset%252C%2520demonstrating%2520its%2520potential%2520as%2520a%2520scalable%2520solution%2520for%2520patient%250Adisposition%2520assessment.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/nafisa67/thesis%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EF-Net%3A%20A%20Deep%20Learning%20Approach%20Combining%20Word%20Embeddings%20and%20Feature%0A%20%20Fusion%20for%20Patient%20Disposition%20Analysis&entry.906535625=Nafisa%20Binte%20Feroz%20and%20Chandrima%20Sarker%20and%20Tanzima%20Ahsan%20and%20K%20M%20Arefeen%20Sultan%20and%20Raqeebir%20Rab&entry.1292438233=%20%20One%20of%20the%20most%20urgent%20problems%20is%20the%20overcrowding%20in%20emergency%20departments%0A%28EDs%29%2C%20caused%20by%20an%20aging%20population%20and%20rising%20healthcare%20costs.%20Patient%0Adispositions%20have%20become%20more%20complex%20as%20a%20result%20of%20the%20strain%20on%20hospital%0Ainfrastructure%20and%20the%20scarcity%20of%20medical%20resources.%20Individuals%20with%20more%0Adangerous%20health%20issues%20should%20be%20prioritized%20in%20the%20emergency%20room.%20Thus%2C%20our%0Aresearch%20aims%20to%20develop%20a%20prediction%20model%20for%20patient%20disposition%20using%0AEF-Net.%20This%20model%20will%20incorporate%20categorical%20features%20into%20the%20neural%0Anetwork%20layer%20and%20add%20numerical%20features%20with%20the%20embedded%20categorical%0Afeatures.%20We%20combine%20the%20EF-Net%20and%20XGBoost%20models%20to%20attain%20higher%20accuracy%20in%0Aour%20results.%20The%20result%20is%20generated%20using%20the%20soft%20voting%20technique.%20In%0AEF-Net%2C%20we%20attained%20an%20accuracy%20of%2095.33%25%2C%20whereas%20in%20the%20Ensemble%20Model%2C%20we%0Aachieved%20an%20accuracy%20of%2096%25.%20The%20experiment%27s%20analysis%20shows%20that%20EF-Net%0Asurpasses%20existing%20works%20in%20accuracy%2C%20AUROC%2C%20and%20F1-Score%20on%20the%20MIMIC-IV-ED%0Adataset%2C%20demonstrating%20its%20potential%20as%20a%20scalable%20solution%20for%20patient%0Adisposition%20assessment.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/nafisa67/thesis%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16134v1&entry.124074799=Read"},
{"title": "Video Diffusion Transformers are In-Context Learners", "author": "Zhengcong Fei and Di Qiu and Changqian Yu and Debang Li and Mingyuan Fan and Xiang Wen", "abstract": "  This paper investigates a solution for enabling in-context capabilities of\nvideo diffusion transformers, with minimal tuning required for activation.\nSpecifically, we propose a simple pipeline to leverage in-context generation:\n($\\textbf{i}$) concatenate videos along spacial or time dimension,\n($\\textbf{ii}$) jointly caption multi-scene video clips from one source, and\n($\\textbf{iii}$) apply task-specific fine-tuning using carefully curated small\ndatasets. Through a series of diverse controllable tasks, we demonstrate\nqualitatively that existing advanced text-to-video models can effectively\nperform in-context generation. Notably, it allows for the creation of\nconsistent multi-scene videos exceeding 30 seconds in duration, without\nadditional computational overhead. Importantly, this method requires no\nmodifications to the original models, results in high-fidelity video outputs\nthat better align with prompt specifications and maintain role consistency. Our\nframework presents a valuable tool for the research community and offers\ncritical insights for advancing product-level controllable video generation\nsystems. The data, code, and model weights are publicly available at:\n\\url{https://github.com/feizc/Video-In-Context}.\n", "link": "http://arxiv.org/abs/2412.10783v2", "date": "2024-12-20", "relevancy": 1.9817, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.684}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6739}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Diffusion%20Transformers%20are%20In-Context%20Learners&body=Title%3A%20Video%20Diffusion%20Transformers%20are%20In-Context%20Learners%0AAuthor%3A%20Zhengcong%20Fei%20and%20Di%20Qiu%20and%20Changqian%20Yu%20and%20Debang%20Li%20and%20Mingyuan%20Fan%20and%20Xiang%20Wen%0AAbstract%3A%20%20%20This%20paper%20investigates%20a%20solution%20for%20enabling%20in-context%20capabilities%20of%0Avideo%20diffusion%20transformers%2C%20with%20minimal%20tuning%20required%20for%20activation.%0ASpecifically%2C%20we%20propose%20a%20simple%20pipeline%20to%20leverage%20in-context%20generation%3A%0A%28%24%5Ctextbf%7Bi%7D%24%29%20concatenate%20videos%20along%20spacial%20or%20time%20dimension%2C%0A%28%24%5Ctextbf%7Bii%7D%24%29%20jointly%20caption%20multi-scene%20video%20clips%20from%20one%20source%2C%20and%0A%28%24%5Ctextbf%7Biii%7D%24%29%20apply%20task-specific%20fine-tuning%20using%20carefully%20curated%20small%0Adatasets.%20Through%20a%20series%20of%20diverse%20controllable%20tasks%2C%20we%20demonstrate%0Aqualitatively%20that%20existing%20advanced%20text-to-video%20models%20can%20effectively%0Aperform%20in-context%20generation.%20Notably%2C%20it%20allows%20for%20the%20creation%20of%0Aconsistent%20multi-scene%20videos%20exceeding%2030%20seconds%20in%20duration%2C%20without%0Aadditional%20computational%20overhead.%20Importantly%2C%20this%20method%20requires%20no%0Amodifications%20to%20the%20original%20models%2C%20results%20in%20high-fidelity%20video%20outputs%0Athat%20better%20align%20with%20prompt%20specifications%20and%20maintain%20role%20consistency.%20Our%0Aframework%20presents%20a%20valuable%20tool%20for%20the%20research%20community%20and%20offers%0Acritical%20insights%20for%20advancing%20product-level%20controllable%20video%20generation%0Asystems.%20The%20data%2C%20code%2C%20and%20model%20weights%20are%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/feizc/Video-In-Context%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Diffusion%2520Transformers%2520are%2520In-Context%2520Learners%26entry.906535625%3DZhengcong%2520Fei%2520and%2520Di%2520Qiu%2520and%2520Changqian%2520Yu%2520and%2520Debang%2520Li%2520and%2520Mingyuan%2520Fan%2520and%2520Xiang%2520Wen%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520a%2520solution%2520for%2520enabling%2520in-context%2520capabilities%2520of%250Avideo%2520diffusion%2520transformers%252C%2520with%2520minimal%2520tuning%2520required%2520for%2520activation.%250ASpecifically%252C%2520we%2520propose%2520a%2520simple%2520pipeline%2520to%2520leverage%2520in-context%2520generation%253A%250A%2528%2524%255Ctextbf%257Bi%257D%2524%2529%2520concatenate%2520videos%2520along%2520spacial%2520or%2520time%2520dimension%252C%250A%2528%2524%255Ctextbf%257Bii%257D%2524%2529%2520jointly%2520caption%2520multi-scene%2520video%2520clips%2520from%2520one%2520source%252C%2520and%250A%2528%2524%255Ctextbf%257Biii%257D%2524%2529%2520apply%2520task-specific%2520fine-tuning%2520using%2520carefully%2520curated%2520small%250Adatasets.%2520Through%2520a%2520series%2520of%2520diverse%2520controllable%2520tasks%252C%2520we%2520demonstrate%250Aqualitatively%2520that%2520existing%2520advanced%2520text-to-video%2520models%2520can%2520effectively%250Aperform%2520in-context%2520generation.%2520Notably%252C%2520it%2520allows%2520for%2520the%2520creation%2520of%250Aconsistent%2520multi-scene%2520videos%2520exceeding%252030%2520seconds%2520in%2520duration%252C%2520without%250Aadditional%2520computational%2520overhead.%2520Importantly%252C%2520this%2520method%2520requires%2520no%250Amodifications%2520to%2520the%2520original%2520models%252C%2520results%2520in%2520high-fidelity%2520video%2520outputs%250Athat%2520better%2520align%2520with%2520prompt%2520specifications%2520and%2520maintain%2520role%2520consistency.%2520Our%250Aframework%2520presents%2520a%2520valuable%2520tool%2520for%2520the%2520research%2520community%2520and%2520offers%250Acritical%2520insights%2520for%2520advancing%2520product-level%2520controllable%2520video%2520generation%250Asystems.%2520The%2520data%252C%2520code%252C%2520and%2520model%2520weights%2520are%2520publicly%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/feizc/Video-In-Context%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Diffusion%20Transformers%20are%20In-Context%20Learners&entry.906535625=Zhengcong%20Fei%20and%20Di%20Qiu%20and%20Changqian%20Yu%20and%20Debang%20Li%20and%20Mingyuan%20Fan%20and%20Xiang%20Wen&entry.1292438233=%20%20This%20paper%20investigates%20a%20solution%20for%20enabling%20in-context%20capabilities%20of%0Avideo%20diffusion%20transformers%2C%20with%20minimal%20tuning%20required%20for%20activation.%0ASpecifically%2C%20we%20propose%20a%20simple%20pipeline%20to%20leverage%20in-context%20generation%3A%0A%28%24%5Ctextbf%7Bi%7D%24%29%20concatenate%20videos%20along%20spacial%20or%20time%20dimension%2C%0A%28%24%5Ctextbf%7Bii%7D%24%29%20jointly%20caption%20multi-scene%20video%20clips%20from%20one%20source%2C%20and%0A%28%24%5Ctextbf%7Biii%7D%24%29%20apply%20task-specific%20fine-tuning%20using%20carefully%20curated%20small%0Adatasets.%20Through%20a%20series%20of%20diverse%20controllable%20tasks%2C%20we%20demonstrate%0Aqualitatively%20that%20existing%20advanced%20text-to-video%20models%20can%20effectively%0Aperform%20in-context%20generation.%20Notably%2C%20it%20allows%20for%20the%20creation%20of%0Aconsistent%20multi-scene%20videos%20exceeding%2030%20seconds%20in%20duration%2C%20without%0Aadditional%20computational%20overhead.%20Importantly%2C%20this%20method%20requires%20no%0Amodifications%20to%20the%20original%20models%2C%20results%20in%20high-fidelity%20video%20outputs%0Athat%20better%20align%20with%20prompt%20specifications%20and%20maintain%20role%20consistency.%20Our%0Aframework%20presents%20a%20valuable%20tool%20for%20the%20research%20community%20and%20offers%0Acritical%20insights%20for%20advancing%20product-level%20controllable%20video%20generation%0Asystems.%20The%20data%2C%20code%2C%20and%20model%20weights%20are%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/feizc/Video-In-Context%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10783v2&entry.124074799=Read"},
{"title": "SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage\n  Estimation in the Wild", "author": "Jannik Els\u00e4\u00dfer and Laura Weihl and Veronika Cheplygina and Lisbeth Tangaa Nielsen", "abstract": "  Seagrass meadows play a crucial role in marine ecosystems, providing\nimportant services such as carbon sequestration, water quality improvement, and\nhabitat provision. Monitoring the distribution and abundance of seagrass is\nessential for environmental impact assessments and conservation efforts.\nHowever, the current manual methods of analyzing underwater video transects to\nassess seagrass coverage are time-consuming and subjective. This work explores\nthe use of deep learning models to automate the process of seagrass detection\nand coverage estimation from underwater video data. A dataset of over 8,300\nannotated underwater images was created, and several deep learning\narchitectures, including ResNet, InceptionNetV3, DenseNet, and Vision\nTransformer, were evaluated for the task of binary classification of ``Eelgrass\nPresent'' and ``Eelgrass Absent'' images. The results demonstrate that deep\nlearning models, particularly the Vision Transformer, can achieve high\nperformance in predicting eelgrass presence, with AUROC scores exceeding 0.95\non the final test dataset. The use of transfer learning and the application of\nthe Deep WaveNet underwater image enhancement model further improved the\nmodels' capabilities. The proposed methodology allows for the efficient\nprocessing of large volumes of video data, enabling the acquisition of much\nmore detailed information on seagrass distributions compared to current manual\nmethods. This information is crucial for environmental impact assessments and\nmonitoring programs, as seagrasses are important indicators of coastal\necosystem health. Overall, this project demonstrates the value that deep\nlearning can bring to the field of marine ecology and environmental monitoring.\n", "link": "http://arxiv.org/abs/2412.16147v1", "date": "2024-12-20", "relevancy": 1.9806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeagrassFinder%3A%20Deep%20Learning%20for%20Eelgrass%20Detection%20and%20Coverage%0A%20%20Estimation%20in%20the%20Wild&body=Title%3A%20SeagrassFinder%3A%20Deep%20Learning%20for%20Eelgrass%20Detection%20and%20Coverage%0A%20%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Jannik%20Els%C3%A4%C3%9Fer%20and%20Laura%20Weihl%20and%20Veronika%20Cheplygina%20and%20Lisbeth%20Tangaa%20Nielsen%0AAbstract%3A%20%20%20Seagrass%20meadows%20play%20a%20crucial%20role%20in%20marine%20ecosystems%2C%20providing%0Aimportant%20services%20such%20as%20carbon%20sequestration%2C%20water%20quality%20improvement%2C%20and%0Ahabitat%20provision.%20Monitoring%20the%20distribution%20and%20abundance%20of%20seagrass%20is%0Aessential%20for%20environmental%20impact%20assessments%20and%20conservation%20efforts.%0AHowever%2C%20the%20current%20manual%20methods%20of%20analyzing%20underwater%20video%20transects%20to%0Aassess%20seagrass%20coverage%20are%20time-consuming%20and%20subjective.%20This%20work%20explores%0Athe%20use%20of%20deep%20learning%20models%20to%20automate%20the%20process%20of%20seagrass%20detection%0Aand%20coverage%20estimation%20from%20underwater%20video%20data.%20A%20dataset%20of%20over%208%2C300%0Aannotated%20underwater%20images%20was%20created%2C%20and%20several%20deep%20learning%0Aarchitectures%2C%20including%20ResNet%2C%20InceptionNetV3%2C%20DenseNet%2C%20and%20Vision%0ATransformer%2C%20were%20evaluated%20for%20the%20task%20of%20binary%20classification%20of%20%60%60Eelgrass%0APresent%27%27%20and%20%60%60Eelgrass%20Absent%27%27%20images.%20The%20results%20demonstrate%20that%20deep%0Alearning%20models%2C%20particularly%20the%20Vision%20Transformer%2C%20can%20achieve%20high%0Aperformance%20in%20predicting%20eelgrass%20presence%2C%20with%20AUROC%20scores%20exceeding%200.95%0Aon%20the%20final%20test%20dataset.%20The%20use%20of%20transfer%20learning%20and%20the%20application%20of%0Athe%20Deep%20WaveNet%20underwater%20image%20enhancement%20model%20further%20improved%20the%0Amodels%27%20capabilities.%20The%20proposed%20methodology%20allows%20for%20the%20efficient%0Aprocessing%20of%20large%20volumes%20of%20video%20data%2C%20enabling%20the%20acquisition%20of%20much%0Amore%20detailed%20information%20on%20seagrass%20distributions%20compared%20to%20current%20manual%0Amethods.%20This%20information%20is%20crucial%20for%20environmental%20impact%20assessments%20and%0Amonitoring%20programs%2C%20as%20seagrasses%20are%20important%20indicators%20of%20coastal%0Aecosystem%20health.%20Overall%2C%20this%20project%20demonstrates%20the%20value%20that%20deep%0Alearning%20can%20bring%20to%20the%20field%20of%20marine%20ecology%20and%20environmental%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeagrassFinder%253A%2520Deep%2520Learning%2520for%2520Eelgrass%2520Detection%2520and%2520Coverage%250A%2520%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DJannik%2520Els%25C3%25A4%25C3%259Fer%2520and%2520Laura%2520Weihl%2520and%2520Veronika%2520Cheplygina%2520and%2520Lisbeth%2520Tangaa%2520Nielsen%26entry.1292438233%3D%2520%2520Seagrass%2520meadows%2520play%2520a%2520crucial%2520role%2520in%2520marine%2520ecosystems%252C%2520providing%250Aimportant%2520services%2520such%2520as%2520carbon%2520sequestration%252C%2520water%2520quality%2520improvement%252C%2520and%250Ahabitat%2520provision.%2520Monitoring%2520the%2520distribution%2520and%2520abundance%2520of%2520seagrass%2520is%250Aessential%2520for%2520environmental%2520impact%2520assessments%2520and%2520conservation%2520efforts.%250AHowever%252C%2520the%2520current%2520manual%2520methods%2520of%2520analyzing%2520underwater%2520video%2520transects%2520to%250Aassess%2520seagrass%2520coverage%2520are%2520time-consuming%2520and%2520subjective.%2520This%2520work%2520explores%250Athe%2520use%2520of%2520deep%2520learning%2520models%2520to%2520automate%2520the%2520process%2520of%2520seagrass%2520detection%250Aand%2520coverage%2520estimation%2520from%2520underwater%2520video%2520data.%2520A%2520dataset%2520of%2520over%25208%252C300%250Aannotated%2520underwater%2520images%2520was%2520created%252C%2520and%2520several%2520deep%2520learning%250Aarchitectures%252C%2520including%2520ResNet%252C%2520InceptionNetV3%252C%2520DenseNet%252C%2520and%2520Vision%250ATransformer%252C%2520were%2520evaluated%2520for%2520the%2520task%2520of%2520binary%2520classification%2520of%2520%2560%2560Eelgrass%250APresent%2527%2527%2520and%2520%2560%2560Eelgrass%2520Absent%2527%2527%2520images.%2520The%2520results%2520demonstrate%2520that%2520deep%250Alearning%2520models%252C%2520particularly%2520the%2520Vision%2520Transformer%252C%2520can%2520achieve%2520high%250Aperformance%2520in%2520predicting%2520eelgrass%2520presence%252C%2520with%2520AUROC%2520scores%2520exceeding%25200.95%250Aon%2520the%2520final%2520test%2520dataset.%2520The%2520use%2520of%2520transfer%2520learning%2520and%2520the%2520application%2520of%250Athe%2520Deep%2520WaveNet%2520underwater%2520image%2520enhancement%2520model%2520further%2520improved%2520the%250Amodels%2527%2520capabilities.%2520The%2520proposed%2520methodology%2520allows%2520for%2520the%2520efficient%250Aprocessing%2520of%2520large%2520volumes%2520of%2520video%2520data%252C%2520enabling%2520the%2520acquisition%2520of%2520much%250Amore%2520detailed%2520information%2520on%2520seagrass%2520distributions%2520compared%2520to%2520current%2520manual%250Amethods.%2520This%2520information%2520is%2520crucial%2520for%2520environmental%2520impact%2520assessments%2520and%250Amonitoring%2520programs%252C%2520as%2520seagrasses%2520are%2520important%2520indicators%2520of%2520coastal%250Aecosystem%2520health.%2520Overall%252C%2520this%2520project%2520demonstrates%2520the%2520value%2520that%2520deep%250Alearning%2520can%2520bring%2520to%2520the%2520field%2520of%2520marine%2520ecology%2520and%2520environmental%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeagrassFinder%3A%20Deep%20Learning%20for%20Eelgrass%20Detection%20and%20Coverage%0A%20%20Estimation%20in%20the%20Wild&entry.906535625=Jannik%20Els%C3%A4%C3%9Fer%20and%20Laura%20Weihl%20and%20Veronika%20Cheplygina%20and%20Lisbeth%20Tangaa%20Nielsen&entry.1292438233=%20%20Seagrass%20meadows%20play%20a%20crucial%20role%20in%20marine%20ecosystems%2C%20providing%0Aimportant%20services%20such%20as%20carbon%20sequestration%2C%20water%20quality%20improvement%2C%20and%0Ahabitat%20provision.%20Monitoring%20the%20distribution%20and%20abundance%20of%20seagrass%20is%0Aessential%20for%20environmental%20impact%20assessments%20and%20conservation%20efforts.%0AHowever%2C%20the%20current%20manual%20methods%20of%20analyzing%20underwater%20video%20transects%20to%0Aassess%20seagrass%20coverage%20are%20time-consuming%20and%20subjective.%20This%20work%20explores%0Athe%20use%20of%20deep%20learning%20models%20to%20automate%20the%20process%20of%20seagrass%20detection%0Aand%20coverage%20estimation%20from%20underwater%20video%20data.%20A%20dataset%20of%20over%208%2C300%0Aannotated%20underwater%20images%20was%20created%2C%20and%20several%20deep%20learning%0Aarchitectures%2C%20including%20ResNet%2C%20InceptionNetV3%2C%20DenseNet%2C%20and%20Vision%0ATransformer%2C%20were%20evaluated%20for%20the%20task%20of%20binary%20classification%20of%20%60%60Eelgrass%0APresent%27%27%20and%20%60%60Eelgrass%20Absent%27%27%20images.%20The%20results%20demonstrate%20that%20deep%0Alearning%20models%2C%20particularly%20the%20Vision%20Transformer%2C%20can%20achieve%20high%0Aperformance%20in%20predicting%20eelgrass%20presence%2C%20with%20AUROC%20scores%20exceeding%200.95%0Aon%20the%20final%20test%20dataset.%20The%20use%20of%20transfer%20learning%20and%20the%20application%20of%0Athe%20Deep%20WaveNet%20underwater%20image%20enhancement%20model%20further%20improved%20the%0Amodels%27%20capabilities.%20The%20proposed%20methodology%20allows%20for%20the%20efficient%0Aprocessing%20of%20large%20volumes%20of%20video%20data%2C%20enabling%20the%20acquisition%20of%20much%0Amore%20detailed%20information%20on%20seagrass%20distributions%20compared%20to%20current%20manual%0Amethods.%20This%20information%20is%20crucial%20for%20environmental%20impact%20assessments%20and%0Amonitoring%20programs%2C%20as%20seagrasses%20are%20important%20indicators%20of%20coastal%0Aecosystem%20health.%20Overall%2C%20this%20project%20demonstrates%20the%20value%20that%20deep%0Alearning%20can%20bring%20to%20the%20field%20of%20marine%20ecology%20and%20environmental%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16147v1&entry.124074799=Read"},
{"title": "Monkey Transfer Learning Can Improve Human Pose Estimation", "author": "Bradley Scott and Clarisse de Vries and Aiden Durrant and Nir Oren and Edward Chadwick and Dimitra Blana", "abstract": "  In this study, we investigated whether transfer learning from macaque monkeys\ncould improve human pose estimation. Current state-of-the-art pose estimation\ntechniques, often employing deep neural networks, can match human annotation in\nnon-clinical datasets. However, they underperform in novel situations, limiting\ntheir generalisability to clinical populations with pathological movement\npatterns. Clinical datasets are not widely available for AI training due to\nethical challenges and a lack of data collection. We observe that data from\nother species may be able to bridge this gap by exposing the network to a\nbroader range of motion cues. We found that utilising data from other species\nand undertaking transfer learning improved human pose estimation in terms of\nprecision and recall compared to the benchmark, which was trained on humans\nonly. Compared to the benchmark, fewer human training examples were needed for\nthe transfer learning approach (1,000 vs 19,185). These results suggest that\nmacaque pose estimation can improve human pose estimation in clinical\nsituations. Future work should further explore the utility of pose estimation\ntrained with monkey data in clinical populations.\n", "link": "http://arxiv.org/abs/2412.15966v1", "date": "2024-12-20", "relevancy": 1.9802, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monkey%20Transfer%20Learning%20Can%20Improve%20Human%20Pose%20Estimation&body=Title%3A%20Monkey%20Transfer%20Learning%20Can%20Improve%20Human%20Pose%20Estimation%0AAuthor%3A%20Bradley%20Scott%20and%20Clarisse%20de%20Vries%20and%20Aiden%20Durrant%20and%20Nir%20Oren%20and%20Edward%20Chadwick%20and%20Dimitra%20Blana%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigated%20whether%20transfer%20learning%20from%20macaque%20monkeys%0Acould%20improve%20human%20pose%20estimation.%20Current%20state-of-the-art%20pose%20estimation%0Atechniques%2C%20often%20employing%20deep%20neural%20networks%2C%20can%20match%20human%20annotation%20in%0Anon-clinical%20datasets.%20However%2C%20they%20underperform%20in%20novel%20situations%2C%20limiting%0Atheir%20generalisability%20to%20clinical%20populations%20with%20pathological%20movement%0Apatterns.%20Clinical%20datasets%20are%20not%20widely%20available%20for%20AI%20training%20due%20to%0Aethical%20challenges%20and%20a%20lack%20of%20data%20collection.%20We%20observe%20that%20data%20from%0Aother%20species%20may%20be%20able%20to%20bridge%20this%20gap%20by%20exposing%20the%20network%20to%20a%0Abroader%20range%20of%20motion%20cues.%20We%20found%20that%20utilising%20data%20from%20other%20species%0Aand%20undertaking%20transfer%20learning%20improved%20human%20pose%20estimation%20in%20terms%20of%0Aprecision%20and%20recall%20compared%20to%20the%20benchmark%2C%20which%20was%20trained%20on%20humans%0Aonly.%20Compared%20to%20the%20benchmark%2C%20fewer%20human%20training%20examples%20were%20needed%20for%0Athe%20transfer%20learning%20approach%20%281%2C000%20vs%2019%2C185%29.%20These%20results%20suggest%20that%0Amacaque%20pose%20estimation%20can%20improve%20human%20pose%20estimation%20in%20clinical%0Asituations.%20Future%20work%20should%20further%20explore%20the%20utility%20of%20pose%20estimation%0Atrained%20with%20monkey%20data%20in%20clinical%20populations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonkey%2520Transfer%2520Learning%2520Can%2520Improve%2520Human%2520Pose%2520Estimation%26entry.906535625%3DBradley%2520Scott%2520and%2520Clarisse%2520de%2520Vries%2520and%2520Aiden%2520Durrant%2520and%2520Nir%2520Oren%2520and%2520Edward%2520Chadwick%2520and%2520Dimitra%2520Blana%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigated%2520whether%2520transfer%2520learning%2520from%2520macaque%2520monkeys%250Acould%2520improve%2520human%2520pose%2520estimation.%2520Current%2520state-of-the-art%2520pose%2520estimation%250Atechniques%252C%2520often%2520employing%2520deep%2520neural%2520networks%252C%2520can%2520match%2520human%2520annotation%2520in%250Anon-clinical%2520datasets.%2520However%252C%2520they%2520underperform%2520in%2520novel%2520situations%252C%2520limiting%250Atheir%2520generalisability%2520to%2520clinical%2520populations%2520with%2520pathological%2520movement%250Apatterns.%2520Clinical%2520datasets%2520are%2520not%2520widely%2520available%2520for%2520AI%2520training%2520due%2520to%250Aethical%2520challenges%2520and%2520a%2520lack%2520of%2520data%2520collection.%2520We%2520observe%2520that%2520data%2520from%250Aother%2520species%2520may%2520be%2520able%2520to%2520bridge%2520this%2520gap%2520by%2520exposing%2520the%2520network%2520to%2520a%250Abroader%2520range%2520of%2520motion%2520cues.%2520We%2520found%2520that%2520utilising%2520data%2520from%2520other%2520species%250Aand%2520undertaking%2520transfer%2520learning%2520improved%2520human%2520pose%2520estimation%2520in%2520terms%2520of%250Aprecision%2520and%2520recall%2520compared%2520to%2520the%2520benchmark%252C%2520which%2520was%2520trained%2520on%2520humans%250Aonly.%2520Compared%2520to%2520the%2520benchmark%252C%2520fewer%2520human%2520training%2520examples%2520were%2520needed%2520for%250Athe%2520transfer%2520learning%2520approach%2520%25281%252C000%2520vs%252019%252C185%2529.%2520These%2520results%2520suggest%2520that%250Amacaque%2520pose%2520estimation%2520can%2520improve%2520human%2520pose%2520estimation%2520in%2520clinical%250Asituations.%2520Future%2520work%2520should%2520further%2520explore%2520the%2520utility%2520of%2520pose%2520estimation%250Atrained%2520with%2520monkey%2520data%2520in%2520clinical%2520populations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monkey%20Transfer%20Learning%20Can%20Improve%20Human%20Pose%20Estimation&entry.906535625=Bradley%20Scott%20and%20Clarisse%20de%20Vries%20and%20Aiden%20Durrant%20and%20Nir%20Oren%20and%20Edward%20Chadwick%20and%20Dimitra%20Blana&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigated%20whether%20transfer%20learning%20from%20macaque%20monkeys%0Acould%20improve%20human%20pose%20estimation.%20Current%20state-of-the-art%20pose%20estimation%0Atechniques%2C%20often%20employing%20deep%20neural%20networks%2C%20can%20match%20human%20annotation%20in%0Anon-clinical%20datasets.%20However%2C%20they%20underperform%20in%20novel%20situations%2C%20limiting%0Atheir%20generalisability%20to%20clinical%20populations%20with%20pathological%20movement%0Apatterns.%20Clinical%20datasets%20are%20not%20widely%20available%20for%20AI%20training%20due%20to%0Aethical%20challenges%20and%20a%20lack%20of%20data%20collection.%20We%20observe%20that%20data%20from%0Aother%20species%20may%20be%20able%20to%20bridge%20this%20gap%20by%20exposing%20the%20network%20to%20a%0Abroader%20range%20of%20motion%20cues.%20We%20found%20that%20utilising%20data%20from%20other%20species%0Aand%20undertaking%20transfer%20learning%20improved%20human%20pose%20estimation%20in%20terms%20of%0Aprecision%20and%20recall%20compared%20to%20the%20benchmark%2C%20which%20was%20trained%20on%20humans%0Aonly.%20Compared%20to%20the%20benchmark%2C%20fewer%20human%20training%20examples%20were%20needed%20for%0Athe%20transfer%20learning%20approach%20%281%2C000%20vs%2019%2C185%29.%20These%20results%20suggest%20that%0Amacaque%20pose%20estimation%20can%20improve%20human%20pose%20estimation%20in%20clinical%0Asituations.%20Future%20work%20should%20further%20explore%20the%20utility%20of%20pose%20estimation%0Atrained%20with%20monkey%20data%20in%20clinical%20populations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15966v1&entry.124074799=Read"},
{"title": "Extracting Interpretable Task-Specific Circuits from Large Language\n  Models for Faster Inference", "author": "Jorge Garc\u00eda-Carrasco and Alejandro Mat\u00e9 and Juan Trujillo", "abstract": "  Large Language Models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, the size of LLMs is steadily increasing, hindering\ntheir application on computationally constrained environments. On the other\nhand, despite their general capabilities, there are many situations where only\none specific task is performed, rendering all other capabilities unnecessary\nand wasteful. This leads us to the following question: Is it possible to\nextract the minimal subset from an LLM that is able to perform a specific task\nin a faster, standalone manner? Recent works on Mechanistic Interpretability\n(MI) have shown that specific tasks are performed by a localized subset of\ncomponents, or circuit. However, current techniques used to identify the\ncircuit cannot be used to extract it for its standalone usage. In this work, we\npropose a novel approach to automatically extract the subset of the LLM that\nproperly performs a targeted task requiring no additional training and a small\namount of data samples. We evaluate our approach on different tasks and show\nthat the resulting models are (i) considerably smaller, reducing the number of\nparameters up to 82.77% and (ii) more interpretable, as they focus on the\ncircuit that is used to carry out the specific task, and can therefore be\nunderstood using MI techniques.\n", "link": "http://arxiv.org/abs/2412.15750v1", "date": "2024-12-20", "relevancy": 1.9684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Interpretable%20Task-Specific%20Circuits%20from%20Large%20Language%0A%20%20Models%20for%20Faster%20Inference&body=Title%3A%20Extracting%20Interpretable%20Task-Specific%20Circuits%20from%20Large%20Language%0A%20%20Models%20for%20Faster%20Inference%0AAuthor%3A%20Jorge%20Garc%C3%ADa-Carrasco%20and%20Alejandro%20Mat%C3%A9%20and%20Juan%20Trujillo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20However%2C%20the%20size%20of%20LLMs%20is%20steadily%20increasing%2C%20hindering%0Atheir%20application%20on%20computationally%20constrained%20environments.%20On%20the%20other%0Ahand%2C%20despite%20their%20general%20capabilities%2C%20there%20are%20many%20situations%20where%20only%0Aone%20specific%20task%20is%20performed%2C%20rendering%20all%20other%20capabilities%20unnecessary%0Aand%20wasteful.%20This%20leads%20us%20to%20the%20following%20question%3A%20Is%20it%20possible%20to%0Aextract%20the%20minimal%20subset%20from%20an%20LLM%20that%20is%20able%20to%20perform%20a%20specific%20task%0Ain%20a%20faster%2C%20standalone%20manner%3F%20Recent%20works%20on%20Mechanistic%20Interpretability%0A%28MI%29%20have%20shown%20that%20specific%20tasks%20are%20performed%20by%20a%20localized%20subset%20of%0Acomponents%2C%20or%20circuit.%20However%2C%20current%20techniques%20used%20to%20identify%20the%0Acircuit%20cannot%20be%20used%20to%20extract%20it%20for%20its%20standalone%20usage.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20to%20automatically%20extract%20the%20subset%20of%20the%20LLM%20that%0Aproperly%20performs%20a%20targeted%20task%20requiring%20no%20additional%20training%20and%20a%20small%0Aamount%20of%20data%20samples.%20We%20evaluate%20our%20approach%20on%20different%20tasks%20and%20show%0Athat%20the%20resulting%20models%20are%20%28i%29%20considerably%20smaller%2C%20reducing%20the%20number%20of%0Aparameters%20up%20to%2082.77%25%20and%20%28ii%29%20more%20interpretable%2C%20as%20they%20focus%20on%20the%0Acircuit%20that%20is%20used%20to%20carry%20out%20the%20specific%20task%2C%20and%20can%20therefore%20be%0Aunderstood%20using%20MI%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Interpretable%2520Task-Specific%2520Circuits%2520from%2520Large%2520Language%250A%2520%2520Models%2520for%2520Faster%2520Inference%26entry.906535625%3DJorge%2520Garc%25C3%25ADa-Carrasco%2520and%2520Alejandro%2520Mat%25C3%25A9%2520and%2520Juan%2520Trujillo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520tasks.%2520However%252C%2520the%2520size%2520of%2520LLMs%2520is%2520steadily%2520increasing%252C%2520hindering%250Atheir%2520application%2520on%2520computationally%2520constrained%2520environments.%2520On%2520the%2520other%250Ahand%252C%2520despite%2520their%2520general%2520capabilities%252C%2520there%2520are%2520many%2520situations%2520where%2520only%250Aone%2520specific%2520task%2520is%2520performed%252C%2520rendering%2520all%2520other%2520capabilities%2520unnecessary%250Aand%2520wasteful.%2520This%2520leads%2520us%2520to%2520the%2520following%2520question%253A%2520Is%2520it%2520possible%2520to%250Aextract%2520the%2520minimal%2520subset%2520from%2520an%2520LLM%2520that%2520is%2520able%2520to%2520perform%2520a%2520specific%2520task%250Ain%2520a%2520faster%252C%2520standalone%2520manner%253F%2520Recent%2520works%2520on%2520Mechanistic%2520Interpretability%250A%2528MI%2529%2520have%2520shown%2520that%2520specific%2520tasks%2520are%2520performed%2520by%2520a%2520localized%2520subset%2520of%250Acomponents%252C%2520or%2520circuit.%2520However%252C%2520current%2520techniques%2520used%2520to%2520identify%2520the%250Acircuit%2520cannot%2520be%2520used%2520to%2520extract%2520it%2520for%2520its%2520standalone%2520usage.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520to%2520automatically%2520extract%2520the%2520subset%2520of%2520the%2520LLM%2520that%250Aproperly%2520performs%2520a%2520targeted%2520task%2520requiring%2520no%2520additional%2520training%2520and%2520a%2520small%250Aamount%2520of%2520data%2520samples.%2520We%2520evaluate%2520our%2520approach%2520on%2520different%2520tasks%2520and%2520show%250Athat%2520the%2520resulting%2520models%2520are%2520%2528i%2529%2520considerably%2520smaller%252C%2520reducing%2520the%2520number%2520of%250Aparameters%2520up%2520to%252082.77%2525%2520and%2520%2528ii%2529%2520more%2520interpretable%252C%2520as%2520they%2520focus%2520on%2520the%250Acircuit%2520that%2520is%2520used%2520to%2520carry%2520out%2520the%2520specific%2520task%252C%2520and%2520can%2520therefore%2520be%250Aunderstood%2520using%2520MI%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Interpretable%20Task-Specific%20Circuits%20from%20Large%20Language%0A%20%20Models%20for%20Faster%20Inference&entry.906535625=Jorge%20Garc%C3%ADa-Carrasco%20and%20Alejandro%20Mat%C3%A9%20and%20Juan%20Trujillo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20performance%20across%20a%20wide%0Arange%20of%20tasks.%20However%2C%20the%20size%20of%20LLMs%20is%20steadily%20increasing%2C%20hindering%0Atheir%20application%20on%20computationally%20constrained%20environments.%20On%20the%20other%0Ahand%2C%20despite%20their%20general%20capabilities%2C%20there%20are%20many%20situations%20where%20only%0Aone%20specific%20task%20is%20performed%2C%20rendering%20all%20other%20capabilities%20unnecessary%0Aand%20wasteful.%20This%20leads%20us%20to%20the%20following%20question%3A%20Is%20it%20possible%20to%0Aextract%20the%20minimal%20subset%20from%20an%20LLM%20that%20is%20able%20to%20perform%20a%20specific%20task%0Ain%20a%20faster%2C%20standalone%20manner%3F%20Recent%20works%20on%20Mechanistic%20Interpretability%0A%28MI%29%20have%20shown%20that%20specific%20tasks%20are%20performed%20by%20a%20localized%20subset%20of%0Acomponents%2C%20or%20circuit.%20However%2C%20current%20techniques%20used%20to%20identify%20the%0Acircuit%20cannot%20be%20used%20to%20extract%20it%20for%20its%20standalone%20usage.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20approach%20to%20automatically%20extract%20the%20subset%20of%20the%20LLM%20that%0Aproperly%20performs%20a%20targeted%20task%20requiring%20no%20additional%20training%20and%20a%20small%0Aamount%20of%20data%20samples.%20We%20evaluate%20our%20approach%20on%20different%20tasks%20and%20show%0Athat%20the%20resulting%20models%20are%20%28i%29%20considerably%20smaller%2C%20reducing%20the%20number%20of%0Aparameters%20up%20to%2082.77%25%20and%20%28ii%29%20more%20interpretable%2C%20as%20they%20focus%20on%20the%0Acircuit%20that%20is%20used%20to%20carry%20out%20the%20specific%20task%2C%20and%20can%20therefore%20be%0Aunderstood%20using%20MI%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15750v1&entry.124074799=Read"},
{"title": "Faithful and Accurate Self-Attention Attribution for Message Passing\n  Neural Networks via the Computation Tree Viewpoint", "author": "Yong-Min Shin and Siqing Li and Xin Cao and Won-Yong Shin", "abstract": "  The self-attention mechanism has been adopted in various popular message\npassing neural networks (MPNNs), enabling the model to adaptively control the\namount of information that flows along the edges of the underlying graph. Such\nattention-based MPNNs (Att-GNNs) have also been used as a baseline for multiple\nstudies on explainable AI (XAI) since attention has steadily been seen as\nnatural model interpretations, while being a viewpoint that has already been\npopularized in other domains (e.g., natural language processing and computer\nvision). However, existing studies often use naive calculations to derive\nattribution scores from attention, undermining the potential of attention as\ninterpretations for Att-GNNs. In our study, we aim to fill the gap between the\nwidespread usage of Att-GNNs and their potential explainability via attention.\nTo this end, we propose GATT, edge attribution calculation method for\nself-attention MPNNs based on the computation tree, a rooted tree that reflects\nthe computation process of the underlying model. Despite its simplicity, we\nempirically demonstrate the effectiveness of GATT in three aspects of model\nexplanation: faithfulness, explanation accuracy, and case studies by using both\nsynthetic and real-world benchmark datasets. In all cases, the results\ndemonstrate that GATT greatly improves edge attribution scores, especially\ncompared to the previous naive approach. Our code is available at\nhttps://github.com/jordan7186/GAtt.\n", "link": "http://arxiv.org/abs/2406.04612v2", "date": "2024-12-20", "relevancy": 1.9664, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faithful%20and%20Accurate%20Self-Attention%20Attribution%20for%20Message%20Passing%0A%20%20Neural%20Networks%20via%20the%20Computation%20Tree%20Viewpoint&body=Title%3A%20Faithful%20and%20Accurate%20Self-Attention%20Attribution%20for%20Message%20Passing%0A%20%20Neural%20Networks%20via%20the%20Computation%20Tree%20Viewpoint%0AAuthor%3A%20Yong-Min%20Shin%20and%20Siqing%20Li%20and%20Xin%20Cao%20and%20Won-Yong%20Shin%0AAbstract%3A%20%20%20The%20self-attention%20mechanism%20has%20been%20adopted%20in%20various%20popular%20message%0Apassing%20neural%20networks%20%28MPNNs%29%2C%20enabling%20the%20model%20to%20adaptively%20control%20the%0Aamount%20of%20information%20that%20flows%20along%20the%20edges%20of%20the%20underlying%20graph.%20Such%0Aattention-based%20MPNNs%20%28Att-GNNs%29%20have%20also%20been%20used%20as%20a%20baseline%20for%20multiple%0Astudies%20on%20explainable%20AI%20%28XAI%29%20since%20attention%20has%20steadily%20been%20seen%20as%0Anatural%20model%20interpretations%2C%20while%20being%20a%20viewpoint%20that%20has%20already%20been%0Apopularized%20in%20other%20domains%20%28e.g.%2C%20natural%20language%20processing%20and%20computer%0Avision%29.%20However%2C%20existing%20studies%20often%20use%20naive%20calculations%20to%20derive%0Aattribution%20scores%20from%20attention%2C%20undermining%20the%20potential%20of%20attention%20as%0Ainterpretations%20for%20Att-GNNs.%20In%20our%20study%2C%20we%20aim%20to%20fill%20the%20gap%20between%20the%0Awidespread%20usage%20of%20Att-GNNs%20and%20their%20potential%20explainability%20via%20attention.%0ATo%20this%20end%2C%20we%20propose%20GATT%2C%20edge%20attribution%20calculation%20method%20for%0Aself-attention%20MPNNs%20based%20on%20the%20computation%20tree%2C%20a%20rooted%20tree%20that%20reflects%0Athe%20computation%20process%20of%20the%20underlying%20model.%20Despite%20its%20simplicity%2C%20we%0Aempirically%20demonstrate%20the%20effectiveness%20of%20GATT%20in%20three%20aspects%20of%20model%0Aexplanation%3A%20faithfulness%2C%20explanation%20accuracy%2C%20and%20case%20studies%20by%20using%20both%0Asynthetic%20and%20real-world%20benchmark%20datasets.%20In%20all%20cases%2C%20the%20results%0Ademonstrate%20that%20GATT%20greatly%20improves%20edge%20attribution%20scores%2C%20especially%0Acompared%20to%20the%20previous%20naive%20approach.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jordan7186/GAtt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaithful%2520and%2520Accurate%2520Self-Attention%2520Attribution%2520for%2520Message%2520Passing%250A%2520%2520Neural%2520Networks%2520via%2520the%2520Computation%2520Tree%2520Viewpoint%26entry.906535625%3DYong-Min%2520Shin%2520and%2520Siqing%2520Li%2520and%2520Xin%2520Cao%2520and%2520Won-Yong%2520Shin%26entry.1292438233%3D%2520%2520The%2520self-attention%2520mechanism%2520has%2520been%2520adopted%2520in%2520various%2520popular%2520message%250Apassing%2520neural%2520networks%2520%2528MPNNs%2529%252C%2520enabling%2520the%2520model%2520to%2520adaptively%2520control%2520the%250Aamount%2520of%2520information%2520that%2520flows%2520along%2520the%2520edges%2520of%2520the%2520underlying%2520graph.%2520Such%250Aattention-based%2520MPNNs%2520%2528Att-GNNs%2529%2520have%2520also%2520been%2520used%2520as%2520a%2520baseline%2520for%2520multiple%250Astudies%2520on%2520explainable%2520AI%2520%2528XAI%2529%2520since%2520attention%2520has%2520steadily%2520been%2520seen%2520as%250Anatural%2520model%2520interpretations%252C%2520while%2520being%2520a%2520viewpoint%2520that%2520has%2520already%2520been%250Apopularized%2520in%2520other%2520domains%2520%2528e.g.%252C%2520natural%2520language%2520processing%2520and%2520computer%250Avision%2529.%2520However%252C%2520existing%2520studies%2520often%2520use%2520naive%2520calculations%2520to%2520derive%250Aattribution%2520scores%2520from%2520attention%252C%2520undermining%2520the%2520potential%2520of%2520attention%2520as%250Ainterpretations%2520for%2520Att-GNNs.%2520In%2520our%2520study%252C%2520we%2520aim%2520to%2520fill%2520the%2520gap%2520between%2520the%250Awidespread%2520usage%2520of%2520Att-GNNs%2520and%2520their%2520potential%2520explainability%2520via%2520attention.%250ATo%2520this%2520end%252C%2520we%2520propose%2520GATT%252C%2520edge%2520attribution%2520calculation%2520method%2520for%250Aself-attention%2520MPNNs%2520based%2520on%2520the%2520computation%2520tree%252C%2520a%2520rooted%2520tree%2520that%2520reflects%250Athe%2520computation%2520process%2520of%2520the%2520underlying%2520model.%2520Despite%2520its%2520simplicity%252C%2520we%250Aempirically%2520demonstrate%2520the%2520effectiveness%2520of%2520GATT%2520in%2520three%2520aspects%2520of%2520model%250Aexplanation%253A%2520faithfulness%252C%2520explanation%2520accuracy%252C%2520and%2520case%2520studies%2520by%2520using%2520both%250Asynthetic%2520and%2520real-world%2520benchmark%2520datasets.%2520In%2520all%2520cases%252C%2520the%2520results%250Ademonstrate%2520that%2520GATT%2520greatly%2520improves%2520edge%2520attribution%2520scores%252C%2520especially%250Acompared%2520to%2520the%2520previous%2520naive%2520approach.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jordan7186/GAtt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faithful%20and%20Accurate%20Self-Attention%20Attribution%20for%20Message%20Passing%0A%20%20Neural%20Networks%20via%20the%20Computation%20Tree%20Viewpoint&entry.906535625=Yong-Min%20Shin%20and%20Siqing%20Li%20and%20Xin%20Cao%20and%20Won-Yong%20Shin&entry.1292438233=%20%20The%20self-attention%20mechanism%20has%20been%20adopted%20in%20various%20popular%20message%0Apassing%20neural%20networks%20%28MPNNs%29%2C%20enabling%20the%20model%20to%20adaptively%20control%20the%0Aamount%20of%20information%20that%20flows%20along%20the%20edges%20of%20the%20underlying%20graph.%20Such%0Aattention-based%20MPNNs%20%28Att-GNNs%29%20have%20also%20been%20used%20as%20a%20baseline%20for%20multiple%0Astudies%20on%20explainable%20AI%20%28XAI%29%20since%20attention%20has%20steadily%20been%20seen%20as%0Anatural%20model%20interpretations%2C%20while%20being%20a%20viewpoint%20that%20has%20already%20been%0Apopularized%20in%20other%20domains%20%28e.g.%2C%20natural%20language%20processing%20and%20computer%0Avision%29.%20However%2C%20existing%20studies%20often%20use%20naive%20calculations%20to%20derive%0Aattribution%20scores%20from%20attention%2C%20undermining%20the%20potential%20of%20attention%20as%0Ainterpretations%20for%20Att-GNNs.%20In%20our%20study%2C%20we%20aim%20to%20fill%20the%20gap%20between%20the%0Awidespread%20usage%20of%20Att-GNNs%20and%20their%20potential%20explainability%20via%20attention.%0ATo%20this%20end%2C%20we%20propose%20GATT%2C%20edge%20attribution%20calculation%20method%20for%0Aself-attention%20MPNNs%20based%20on%20the%20computation%20tree%2C%20a%20rooted%20tree%20that%20reflects%0Athe%20computation%20process%20of%20the%20underlying%20model.%20Despite%20its%20simplicity%2C%20we%0Aempirically%20demonstrate%20the%20effectiveness%20of%20GATT%20in%20three%20aspects%20of%20model%0Aexplanation%3A%20faithfulness%2C%20explanation%20accuracy%2C%20and%20case%20studies%20by%20using%20both%0Asynthetic%20and%20real-world%20benchmark%20datasets.%20In%20all%20cases%2C%20the%20results%0Ademonstrate%20that%20GATT%20greatly%20improves%20edge%20attribution%20scores%2C%20especially%0Acompared%20to%20the%20previous%20naive%20approach.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jordan7186/GAtt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04612v2&entry.124074799=Read"},
{"title": "From Model Based to Learned Regularization in Medical Image\n  Registration: A Comprehensive Review", "author": "Anna Reithmeir and Veronika Spieker and Vasiliki Sideri-Lampretsa and Daniel Rueckert and Julia A. Schnabel and Veronika A. Zimmer", "abstract": "  Image registration is fundamental in medical imaging applications, such as\ndisease progression analysis or radiation therapy planning. The primary\nobjective of image registration is to precisely capture the deformation between\ntwo or more images, typically achieved by minimizing an optimization problem.\nDue to its inherent ill-posedness, regularization is a key component in driving\nthe solution toward anatomically meaningful deformations. A wide range of\nregularization methods has been proposed for both conventional and deep\nlearning-based registration. However, the appropriate application of\nregularization techniques often depends on the specific registration problem,\nand no one-fits-all method exists. Despite its importance, regularization is\noften overlooked or addressed with default approaches, assuming existing\nmethods are sufficient. A comprehensive and structured review remains missing.\nThis review addresses this gap by introducing a novel taxonomy that\nsystematically categorizes the diverse range of proposed regularization\nmethods. It highlights the emerging field of learned regularization, which\nleverages data-driven techniques to automatically derive deformation properties\nfrom the data. Moreover, this review examines the transfer of regularization\nmethods from conventional to learning-based registration, identifies open\nchallenges, and outlines future research directions. By emphasizing the\ncritical role of regularization in image registration, we hope to inspire the\nresearch community to reconsider regularization strategies in modern\nregistration algorithms and to explore this rapidly evolving field further.\n", "link": "http://arxiv.org/abs/2412.15740v1", "date": "2024-12-20", "relevancy": 1.9613, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.498}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Model%20Based%20to%20Learned%20Regularization%20in%20Medical%20Image%0A%20%20Registration%3A%20A%20Comprehensive%20Review&body=Title%3A%20From%20Model%20Based%20to%20Learned%20Regularization%20in%20Medical%20Image%0A%20%20Registration%3A%20A%20Comprehensive%20Review%0AAuthor%3A%20Anna%20Reithmeir%20and%20Veronika%20Spieker%20and%20Vasiliki%20Sideri-Lampretsa%20and%20Daniel%20Rueckert%20and%20Julia%20A.%20Schnabel%20and%20Veronika%20A.%20Zimmer%0AAbstract%3A%20%20%20Image%20registration%20is%20fundamental%20in%20medical%20imaging%20applications%2C%20such%20as%0Adisease%20progression%20analysis%20or%20radiation%20therapy%20planning.%20The%20primary%0Aobjective%20of%20image%20registration%20is%20to%20precisely%20capture%20the%20deformation%20between%0Atwo%20or%20more%20images%2C%20typically%20achieved%20by%20minimizing%20an%20optimization%20problem.%0ADue%20to%20its%20inherent%20ill-posedness%2C%20regularization%20is%20a%20key%20component%20in%20driving%0Athe%20solution%20toward%20anatomically%20meaningful%20deformations.%20A%20wide%20range%20of%0Aregularization%20methods%20has%20been%20proposed%20for%20both%20conventional%20and%20deep%0Alearning-based%20registration.%20However%2C%20the%20appropriate%20application%20of%0Aregularization%20techniques%20often%20depends%20on%20the%20specific%20registration%20problem%2C%0Aand%20no%20one-fits-all%20method%20exists.%20Despite%20its%20importance%2C%20regularization%20is%0Aoften%20overlooked%20or%20addressed%20with%20default%20approaches%2C%20assuming%20existing%0Amethods%20are%20sufficient.%20A%20comprehensive%20and%20structured%20review%20remains%20missing.%0AThis%20review%20addresses%20this%20gap%20by%20introducing%20a%20novel%20taxonomy%20that%0Asystematically%20categorizes%20the%20diverse%20range%20of%20proposed%20regularization%0Amethods.%20It%20highlights%20the%20emerging%20field%20of%20learned%20regularization%2C%20which%0Aleverages%20data-driven%20techniques%20to%20automatically%20derive%20deformation%20properties%0Afrom%20the%20data.%20Moreover%2C%20this%20review%20examines%20the%20transfer%20of%20regularization%0Amethods%20from%20conventional%20to%20learning-based%20registration%2C%20identifies%20open%0Achallenges%2C%20and%20outlines%20future%20research%20directions.%20By%20emphasizing%20the%0Acritical%20role%20of%20regularization%20in%20image%20registration%2C%20we%20hope%20to%20inspire%20the%0Aresearch%20community%20to%20reconsider%20regularization%20strategies%20in%20modern%0Aregistration%20algorithms%20and%20to%20explore%20this%20rapidly%20evolving%20field%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Model%2520Based%2520to%2520Learned%2520Regularization%2520in%2520Medical%2520Image%250A%2520%2520Registration%253A%2520A%2520Comprehensive%2520Review%26entry.906535625%3DAnna%2520Reithmeir%2520and%2520Veronika%2520Spieker%2520and%2520Vasiliki%2520Sideri-Lampretsa%2520and%2520Daniel%2520Rueckert%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Veronika%2520A.%2520Zimmer%26entry.1292438233%3D%2520%2520Image%2520registration%2520is%2520fundamental%2520in%2520medical%2520imaging%2520applications%252C%2520such%2520as%250Adisease%2520progression%2520analysis%2520or%2520radiation%2520therapy%2520planning.%2520The%2520primary%250Aobjective%2520of%2520image%2520registration%2520is%2520to%2520precisely%2520capture%2520the%2520deformation%2520between%250Atwo%2520or%2520more%2520images%252C%2520typically%2520achieved%2520by%2520minimizing%2520an%2520optimization%2520problem.%250ADue%2520to%2520its%2520inherent%2520ill-posedness%252C%2520regularization%2520is%2520a%2520key%2520component%2520in%2520driving%250Athe%2520solution%2520toward%2520anatomically%2520meaningful%2520deformations.%2520A%2520wide%2520range%2520of%250Aregularization%2520methods%2520has%2520been%2520proposed%2520for%2520both%2520conventional%2520and%2520deep%250Alearning-based%2520registration.%2520However%252C%2520the%2520appropriate%2520application%2520of%250Aregularization%2520techniques%2520often%2520depends%2520on%2520the%2520specific%2520registration%2520problem%252C%250Aand%2520no%2520one-fits-all%2520method%2520exists.%2520Despite%2520its%2520importance%252C%2520regularization%2520is%250Aoften%2520overlooked%2520or%2520addressed%2520with%2520default%2520approaches%252C%2520assuming%2520existing%250Amethods%2520are%2520sufficient.%2520A%2520comprehensive%2520and%2520structured%2520review%2520remains%2520missing.%250AThis%2520review%2520addresses%2520this%2520gap%2520by%2520introducing%2520a%2520novel%2520taxonomy%2520that%250Asystematically%2520categorizes%2520the%2520diverse%2520range%2520of%2520proposed%2520regularization%250Amethods.%2520It%2520highlights%2520the%2520emerging%2520field%2520of%2520learned%2520regularization%252C%2520which%250Aleverages%2520data-driven%2520techniques%2520to%2520automatically%2520derive%2520deformation%2520properties%250Afrom%2520the%2520data.%2520Moreover%252C%2520this%2520review%2520examines%2520the%2520transfer%2520of%2520regularization%250Amethods%2520from%2520conventional%2520to%2520learning-based%2520registration%252C%2520identifies%2520open%250Achallenges%252C%2520and%2520outlines%2520future%2520research%2520directions.%2520By%2520emphasizing%2520the%250Acritical%2520role%2520of%2520regularization%2520in%2520image%2520registration%252C%2520we%2520hope%2520to%2520inspire%2520the%250Aresearch%2520community%2520to%2520reconsider%2520regularization%2520strategies%2520in%2520modern%250Aregistration%2520algorithms%2520and%2520to%2520explore%2520this%2520rapidly%2520evolving%2520field%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Model%20Based%20to%20Learned%20Regularization%20in%20Medical%20Image%0A%20%20Registration%3A%20A%20Comprehensive%20Review&entry.906535625=Anna%20Reithmeir%20and%20Veronika%20Spieker%20and%20Vasiliki%20Sideri-Lampretsa%20and%20Daniel%20Rueckert%20and%20Julia%20A.%20Schnabel%20and%20Veronika%20A.%20Zimmer&entry.1292438233=%20%20Image%20registration%20is%20fundamental%20in%20medical%20imaging%20applications%2C%20such%20as%0Adisease%20progression%20analysis%20or%20radiation%20therapy%20planning.%20The%20primary%0Aobjective%20of%20image%20registration%20is%20to%20precisely%20capture%20the%20deformation%20between%0Atwo%20or%20more%20images%2C%20typically%20achieved%20by%20minimizing%20an%20optimization%20problem.%0ADue%20to%20its%20inherent%20ill-posedness%2C%20regularization%20is%20a%20key%20component%20in%20driving%0Athe%20solution%20toward%20anatomically%20meaningful%20deformations.%20A%20wide%20range%20of%0Aregularization%20methods%20has%20been%20proposed%20for%20both%20conventional%20and%20deep%0Alearning-based%20registration.%20However%2C%20the%20appropriate%20application%20of%0Aregularization%20techniques%20often%20depends%20on%20the%20specific%20registration%20problem%2C%0Aand%20no%20one-fits-all%20method%20exists.%20Despite%20its%20importance%2C%20regularization%20is%0Aoften%20overlooked%20or%20addressed%20with%20default%20approaches%2C%20assuming%20existing%0Amethods%20are%20sufficient.%20A%20comprehensive%20and%20structured%20review%20remains%20missing.%0AThis%20review%20addresses%20this%20gap%20by%20introducing%20a%20novel%20taxonomy%20that%0Asystematically%20categorizes%20the%20diverse%20range%20of%20proposed%20regularization%0Amethods.%20It%20highlights%20the%20emerging%20field%20of%20learned%20regularization%2C%20which%0Aleverages%20data-driven%20techniques%20to%20automatically%20derive%20deformation%20properties%0Afrom%20the%20data.%20Moreover%2C%20this%20review%20examines%20the%20transfer%20of%20regularization%0Amethods%20from%20conventional%20to%20learning-based%20registration%2C%20identifies%20open%0Achallenges%2C%20and%20outlines%20future%20research%20directions.%20By%20emphasizing%20the%0Acritical%20role%20of%20regularization%20in%20image%20registration%2C%20we%20hope%20to%20inspire%20the%0Aresearch%20community%20to%20reconsider%20regularization%20strategies%20in%20modern%0Aregistration%20algorithms%20and%20to%20explore%20this%20rapidly%20evolving%20field%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15740v1&entry.124074799=Read"},
{"title": "Bi-directional Mapping of Morphology Metrics and 3D City Blocks for\n  Enhanced Characterization and Generation of Urban Form", "author": "Chenyi Cai and Biao Li and Qiyan Zhang and Xiao Wang and Filip Biljecki and Pieter Herthogs", "abstract": "  Urban morphology, examining city spatial configurations, links urban design\nto sustainability. Morphology metrics play a fundamental role in\nperformance-driven computational urban design (CUD) which integrates urban form\ngeneration, performance evaluation and optimization. However, a critical gap\nremains between performance evaluation and complex urban form generation,\ncaused by the disconnection between morphology metrics and urban form,\nparticularly in metric-to-form workflows. It prevents the application of\noptimized metrics to generate improved urban form with enhanced urban\nperformance. Formulating morphology metrics that not only effectively\ncharacterize complex urban forms but also enable the reconstruction of diverse\nforms is of significant importance. This paper highlights the importance of\nestablishing a bi-directional mapping between morphology metrics and complex\nurban form to enable the integration of urban form generation with performance\nevaluation. We present an approach that can 1) formulate morphology metrics to\nboth characterize urban forms and in reverse, retrieve diverse similar 3D urban\nforms, and 2) evaluate the effectiveness of morphology metrics in representing\n3D urban form characteristics of blocks by comparison. We demonstrate the\nmethodology with 3D urban models of New York City, covering 14,248 blocks. We\nuse neural networks and information retrieval for morphology metric encoding,\nurban form clustering and morphology metric evaluation. We identified an\neffective set of morphology metrics for characterizing block-scale urban forms\nthrough comparison. The proposed methodology tightly couples complex urban\nforms with morphology metrics, hence it can enable a seamless and bidirectional\nrelationship between urban form generation and optimization in\nperformance-driven urban design towards sustainable urban design and planning.\n", "link": "http://arxiv.org/abs/2412.15801v1", "date": "2024-12-20", "relevancy": 1.9587, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4924}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-directional%20Mapping%20of%20Morphology%20Metrics%20and%203D%20City%20Blocks%20for%0A%20%20Enhanced%20Characterization%20and%20Generation%20of%20Urban%20Form&body=Title%3A%20Bi-directional%20Mapping%20of%20Morphology%20Metrics%20and%203D%20City%20Blocks%20for%0A%20%20Enhanced%20Characterization%20and%20Generation%20of%20Urban%20Form%0AAuthor%3A%20Chenyi%20Cai%20and%20Biao%20Li%20and%20Qiyan%20Zhang%20and%20Xiao%20Wang%20and%20Filip%20Biljecki%20and%20Pieter%20Herthogs%0AAbstract%3A%20%20%20Urban%20morphology%2C%20examining%20city%20spatial%20configurations%2C%20links%20urban%20design%0Ato%20sustainability.%20Morphology%20metrics%20play%20a%20fundamental%20role%20in%0Aperformance-driven%20computational%20urban%20design%20%28CUD%29%20which%20integrates%20urban%20form%0Ageneration%2C%20performance%20evaluation%20and%20optimization.%20However%2C%20a%20critical%20gap%0Aremains%20between%20performance%20evaluation%20and%20complex%20urban%20form%20generation%2C%0Acaused%20by%20the%20disconnection%20between%20morphology%20metrics%20and%20urban%20form%2C%0Aparticularly%20in%20metric-to-form%20workflows.%20It%20prevents%20the%20application%20of%0Aoptimized%20metrics%20to%20generate%20improved%20urban%20form%20with%20enhanced%20urban%0Aperformance.%20Formulating%20morphology%20metrics%20that%20not%20only%20effectively%0Acharacterize%20complex%20urban%20forms%20but%20also%20enable%20the%20reconstruction%20of%20diverse%0Aforms%20is%20of%20significant%20importance.%20This%20paper%20highlights%20the%20importance%20of%0Aestablishing%20a%20bi-directional%20mapping%20between%20morphology%20metrics%20and%20complex%0Aurban%20form%20to%20enable%20the%20integration%20of%20urban%20form%20generation%20with%20performance%0Aevaluation.%20We%20present%20an%20approach%20that%20can%201%29%20formulate%20morphology%20metrics%20to%0Aboth%20characterize%20urban%20forms%20and%20in%20reverse%2C%20retrieve%20diverse%20similar%203D%20urban%0Aforms%2C%20and%202%29%20evaluate%20the%20effectiveness%20of%20morphology%20metrics%20in%20representing%0A3D%20urban%20form%20characteristics%20of%20blocks%20by%20comparison.%20We%20demonstrate%20the%0Amethodology%20with%203D%20urban%20models%20of%20New%20York%20City%2C%20covering%2014%2C248%20blocks.%20We%0Ause%20neural%20networks%20and%20information%20retrieval%20for%20morphology%20metric%20encoding%2C%0Aurban%20form%20clustering%20and%20morphology%20metric%20evaluation.%20We%20identified%20an%0Aeffective%20set%20of%20morphology%20metrics%20for%20characterizing%20block-scale%20urban%20forms%0Athrough%20comparison.%20The%20proposed%20methodology%20tightly%20couples%20complex%20urban%0Aforms%20with%20morphology%20metrics%2C%20hence%20it%20can%20enable%20a%20seamless%20and%20bidirectional%0Arelationship%20between%20urban%20form%20generation%20and%20optimization%20in%0Aperformance-driven%20urban%20design%20towards%20sustainable%20urban%20design%20and%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-directional%2520Mapping%2520of%2520Morphology%2520Metrics%2520and%25203D%2520City%2520Blocks%2520for%250A%2520%2520Enhanced%2520Characterization%2520and%2520Generation%2520of%2520Urban%2520Form%26entry.906535625%3DChenyi%2520Cai%2520and%2520Biao%2520Li%2520and%2520Qiyan%2520Zhang%2520and%2520Xiao%2520Wang%2520and%2520Filip%2520Biljecki%2520and%2520Pieter%2520Herthogs%26entry.1292438233%3D%2520%2520Urban%2520morphology%252C%2520examining%2520city%2520spatial%2520configurations%252C%2520links%2520urban%2520design%250Ato%2520sustainability.%2520Morphology%2520metrics%2520play%2520a%2520fundamental%2520role%2520in%250Aperformance-driven%2520computational%2520urban%2520design%2520%2528CUD%2529%2520which%2520integrates%2520urban%2520form%250Ageneration%252C%2520performance%2520evaluation%2520and%2520optimization.%2520However%252C%2520a%2520critical%2520gap%250Aremains%2520between%2520performance%2520evaluation%2520and%2520complex%2520urban%2520form%2520generation%252C%250Acaused%2520by%2520the%2520disconnection%2520between%2520morphology%2520metrics%2520and%2520urban%2520form%252C%250Aparticularly%2520in%2520metric-to-form%2520workflows.%2520It%2520prevents%2520the%2520application%2520of%250Aoptimized%2520metrics%2520to%2520generate%2520improved%2520urban%2520form%2520with%2520enhanced%2520urban%250Aperformance.%2520Formulating%2520morphology%2520metrics%2520that%2520not%2520only%2520effectively%250Acharacterize%2520complex%2520urban%2520forms%2520but%2520also%2520enable%2520the%2520reconstruction%2520of%2520diverse%250Aforms%2520is%2520of%2520significant%2520importance.%2520This%2520paper%2520highlights%2520the%2520importance%2520of%250Aestablishing%2520a%2520bi-directional%2520mapping%2520between%2520morphology%2520metrics%2520and%2520complex%250Aurban%2520form%2520to%2520enable%2520the%2520integration%2520of%2520urban%2520form%2520generation%2520with%2520performance%250Aevaluation.%2520We%2520present%2520an%2520approach%2520that%2520can%25201%2529%2520formulate%2520morphology%2520metrics%2520to%250Aboth%2520characterize%2520urban%2520forms%2520and%2520in%2520reverse%252C%2520retrieve%2520diverse%2520similar%25203D%2520urban%250Aforms%252C%2520and%25202%2529%2520evaluate%2520the%2520effectiveness%2520of%2520morphology%2520metrics%2520in%2520representing%250A3D%2520urban%2520form%2520characteristics%2520of%2520blocks%2520by%2520comparison.%2520We%2520demonstrate%2520the%250Amethodology%2520with%25203D%2520urban%2520models%2520of%2520New%2520York%2520City%252C%2520covering%252014%252C248%2520blocks.%2520We%250Ause%2520neural%2520networks%2520and%2520information%2520retrieval%2520for%2520morphology%2520metric%2520encoding%252C%250Aurban%2520form%2520clustering%2520and%2520morphology%2520metric%2520evaluation.%2520We%2520identified%2520an%250Aeffective%2520set%2520of%2520morphology%2520metrics%2520for%2520characterizing%2520block-scale%2520urban%2520forms%250Athrough%2520comparison.%2520The%2520proposed%2520methodology%2520tightly%2520couples%2520complex%2520urban%250Aforms%2520with%2520morphology%2520metrics%252C%2520hence%2520it%2520can%2520enable%2520a%2520seamless%2520and%2520bidirectional%250Arelationship%2520between%2520urban%2520form%2520generation%2520and%2520optimization%2520in%250Aperformance-driven%2520urban%2520design%2520towards%2520sustainable%2520urban%2520design%2520and%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-directional%20Mapping%20of%20Morphology%20Metrics%20and%203D%20City%20Blocks%20for%0A%20%20Enhanced%20Characterization%20and%20Generation%20of%20Urban%20Form&entry.906535625=Chenyi%20Cai%20and%20Biao%20Li%20and%20Qiyan%20Zhang%20and%20Xiao%20Wang%20and%20Filip%20Biljecki%20and%20Pieter%20Herthogs&entry.1292438233=%20%20Urban%20morphology%2C%20examining%20city%20spatial%20configurations%2C%20links%20urban%20design%0Ato%20sustainability.%20Morphology%20metrics%20play%20a%20fundamental%20role%20in%0Aperformance-driven%20computational%20urban%20design%20%28CUD%29%20which%20integrates%20urban%20form%0Ageneration%2C%20performance%20evaluation%20and%20optimization.%20However%2C%20a%20critical%20gap%0Aremains%20between%20performance%20evaluation%20and%20complex%20urban%20form%20generation%2C%0Acaused%20by%20the%20disconnection%20between%20morphology%20metrics%20and%20urban%20form%2C%0Aparticularly%20in%20metric-to-form%20workflows.%20It%20prevents%20the%20application%20of%0Aoptimized%20metrics%20to%20generate%20improved%20urban%20form%20with%20enhanced%20urban%0Aperformance.%20Formulating%20morphology%20metrics%20that%20not%20only%20effectively%0Acharacterize%20complex%20urban%20forms%20but%20also%20enable%20the%20reconstruction%20of%20diverse%0Aforms%20is%20of%20significant%20importance.%20This%20paper%20highlights%20the%20importance%20of%0Aestablishing%20a%20bi-directional%20mapping%20between%20morphology%20metrics%20and%20complex%0Aurban%20form%20to%20enable%20the%20integration%20of%20urban%20form%20generation%20with%20performance%0Aevaluation.%20We%20present%20an%20approach%20that%20can%201%29%20formulate%20morphology%20metrics%20to%0Aboth%20characterize%20urban%20forms%20and%20in%20reverse%2C%20retrieve%20diverse%20similar%203D%20urban%0Aforms%2C%20and%202%29%20evaluate%20the%20effectiveness%20of%20morphology%20metrics%20in%20representing%0A3D%20urban%20form%20characteristics%20of%20blocks%20by%20comparison.%20We%20demonstrate%20the%0Amethodology%20with%203D%20urban%20models%20of%20New%20York%20City%2C%20covering%2014%2C248%20blocks.%20We%0Ause%20neural%20networks%20and%20information%20retrieval%20for%20morphology%20metric%20encoding%2C%0Aurban%20form%20clustering%20and%20morphology%20metric%20evaluation.%20We%20identified%20an%0Aeffective%20set%20of%20morphology%20metrics%20for%20characterizing%20block-scale%20urban%20forms%0Athrough%20comparison.%20The%20proposed%20methodology%20tightly%20couples%20complex%20urban%0Aforms%20with%20morphology%20metrics%2C%20hence%20it%20can%20enable%20a%20seamless%20and%20bidirectional%0Arelationship%20between%20urban%20form%20generation%20and%20optimization%20in%0Aperformance-driven%20urban%20design%20towards%20sustainable%20urban%20design%20and%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15801v1&entry.124074799=Read"},
{"title": "Fair Distributed Machine Learning with Imbalanced Data as a Stackelberg\n  Evolutionary Game", "author": "Sebastian Niehaus and Ingo Roeder and Nico Scherf", "abstract": "  Decentralised learning enables the training of deep learning algorithms\nwithout centralising data sets, resulting in benefits such as improved data\nprivacy, operational efficiency and the fostering of data ownership policies.\nHowever, significant data imbalances pose a challenge in this framework.\nParticipants with smaller datasets in distributed learning environments often\nachieve poorer results than participants with larger datasets. Data imbalances\nare particularly pronounced in medical fields and are caused by different\npatient populations, technological inequalities and divergent data collection\npractices.\n  In this paper, we consider distributed learning as an Stackelberg\nevolutionary game. We present two algorithms for setting the weights of each\nnode's contribution to the global model in each training round: the\nDeterministic Stackelberg Weighting Model (DSWM) and the Adaptive Stackelberg\nWeighting Model (ASWM). We use three medical datasets to highlight the impact\nof dynamic weighting on underrepresented nodes in distributed learning. Our\nresults show that the ASWM significantly favours underrepresented nodes by\nimproving their performance by 2.713% in AUC. Meanwhile, nodes with larger\ndatasets experience only a modest average performance decrease of 0.441%.\n", "link": "http://arxiv.org/abs/2412.16079v1", "date": "2024-12-20", "relevancy": 1.957, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.514}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.473}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Distributed%20Machine%20Learning%20with%20Imbalanced%20Data%20as%20a%20Stackelberg%0A%20%20Evolutionary%20Game&body=Title%3A%20Fair%20Distributed%20Machine%20Learning%20with%20Imbalanced%20Data%20as%20a%20Stackelberg%0A%20%20Evolutionary%20Game%0AAuthor%3A%20Sebastian%20Niehaus%20and%20Ingo%20Roeder%20and%20Nico%20Scherf%0AAbstract%3A%20%20%20Decentralised%20learning%20enables%20the%20training%20of%20deep%20learning%20algorithms%0Awithout%20centralising%20data%20sets%2C%20resulting%20in%20benefits%20such%20as%20improved%20data%0Aprivacy%2C%20operational%20efficiency%20and%20the%20fostering%20of%20data%20ownership%20policies.%0AHowever%2C%20significant%20data%20imbalances%20pose%20a%20challenge%20in%20this%20framework.%0AParticipants%20with%20smaller%20datasets%20in%20distributed%20learning%20environments%20often%0Aachieve%20poorer%20results%20than%20participants%20with%20larger%20datasets.%20Data%20imbalances%0Aare%20particularly%20pronounced%20in%20medical%20fields%20and%20are%20caused%20by%20different%0Apatient%20populations%2C%20technological%20inequalities%20and%20divergent%20data%20collection%0Apractices.%0A%20%20In%20this%20paper%2C%20we%20consider%20distributed%20learning%20as%20an%20Stackelberg%0Aevolutionary%20game.%20We%20present%20two%20algorithms%20for%20setting%20the%20weights%20of%20each%0Anode%27s%20contribution%20to%20the%20global%20model%20in%20each%20training%20round%3A%20the%0ADeterministic%20Stackelberg%20Weighting%20Model%20%28DSWM%29%20and%20the%20Adaptive%20Stackelberg%0AWeighting%20Model%20%28ASWM%29.%20We%20use%20three%20medical%20datasets%20to%20highlight%20the%20impact%0Aof%20dynamic%20weighting%20on%20underrepresented%20nodes%20in%20distributed%20learning.%20Our%0Aresults%20show%20that%20the%20ASWM%20significantly%20favours%20underrepresented%20nodes%20by%0Aimproving%20their%20performance%20by%202.713%25%20in%20AUC.%20Meanwhile%2C%20nodes%20with%20larger%0Adatasets%20experience%20only%20a%20modest%20average%20performance%20decrease%20of%200.441%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Distributed%2520Machine%2520Learning%2520with%2520Imbalanced%2520Data%2520as%2520a%2520Stackelberg%250A%2520%2520Evolutionary%2520Game%26entry.906535625%3DSebastian%2520Niehaus%2520and%2520Ingo%2520Roeder%2520and%2520Nico%2520Scherf%26entry.1292438233%3D%2520%2520Decentralised%2520learning%2520enables%2520the%2520training%2520of%2520deep%2520learning%2520algorithms%250Awithout%2520centralising%2520data%2520sets%252C%2520resulting%2520in%2520benefits%2520such%2520as%2520improved%2520data%250Aprivacy%252C%2520operational%2520efficiency%2520and%2520the%2520fostering%2520of%2520data%2520ownership%2520policies.%250AHowever%252C%2520significant%2520data%2520imbalances%2520pose%2520a%2520challenge%2520in%2520this%2520framework.%250AParticipants%2520with%2520smaller%2520datasets%2520in%2520distributed%2520learning%2520environments%2520often%250Aachieve%2520poorer%2520results%2520than%2520participants%2520with%2520larger%2520datasets.%2520Data%2520imbalances%250Aare%2520particularly%2520pronounced%2520in%2520medical%2520fields%2520and%2520are%2520caused%2520by%2520different%250Apatient%2520populations%252C%2520technological%2520inequalities%2520and%2520divergent%2520data%2520collection%250Apractices.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520distributed%2520learning%2520as%2520an%2520Stackelberg%250Aevolutionary%2520game.%2520We%2520present%2520two%2520algorithms%2520for%2520setting%2520the%2520weights%2520of%2520each%250Anode%2527s%2520contribution%2520to%2520the%2520global%2520model%2520in%2520each%2520training%2520round%253A%2520the%250ADeterministic%2520Stackelberg%2520Weighting%2520Model%2520%2528DSWM%2529%2520and%2520the%2520Adaptive%2520Stackelberg%250AWeighting%2520Model%2520%2528ASWM%2529.%2520We%2520use%2520three%2520medical%2520datasets%2520to%2520highlight%2520the%2520impact%250Aof%2520dynamic%2520weighting%2520on%2520underrepresented%2520nodes%2520in%2520distributed%2520learning.%2520Our%250Aresults%2520show%2520that%2520the%2520ASWM%2520significantly%2520favours%2520underrepresented%2520nodes%2520by%250Aimproving%2520their%2520performance%2520by%25202.713%2525%2520in%2520AUC.%2520Meanwhile%252C%2520nodes%2520with%2520larger%250Adatasets%2520experience%2520only%2520a%2520modest%2520average%2520performance%2520decrease%2520of%25200.441%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Distributed%20Machine%20Learning%20with%20Imbalanced%20Data%20as%20a%20Stackelberg%0A%20%20Evolutionary%20Game&entry.906535625=Sebastian%20Niehaus%20and%20Ingo%20Roeder%20and%20Nico%20Scherf&entry.1292438233=%20%20Decentralised%20learning%20enables%20the%20training%20of%20deep%20learning%20algorithms%0Awithout%20centralising%20data%20sets%2C%20resulting%20in%20benefits%20such%20as%20improved%20data%0Aprivacy%2C%20operational%20efficiency%20and%20the%20fostering%20of%20data%20ownership%20policies.%0AHowever%2C%20significant%20data%20imbalances%20pose%20a%20challenge%20in%20this%20framework.%0AParticipants%20with%20smaller%20datasets%20in%20distributed%20learning%20environments%20often%0Aachieve%20poorer%20results%20than%20participants%20with%20larger%20datasets.%20Data%20imbalances%0Aare%20particularly%20pronounced%20in%20medical%20fields%20and%20are%20caused%20by%20different%0Apatient%20populations%2C%20technological%20inequalities%20and%20divergent%20data%20collection%0Apractices.%0A%20%20In%20this%20paper%2C%20we%20consider%20distributed%20learning%20as%20an%20Stackelberg%0Aevolutionary%20game.%20We%20present%20two%20algorithms%20for%20setting%20the%20weights%20of%20each%0Anode%27s%20contribution%20to%20the%20global%20model%20in%20each%20training%20round%3A%20the%0ADeterministic%20Stackelberg%20Weighting%20Model%20%28DSWM%29%20and%20the%20Adaptive%20Stackelberg%0AWeighting%20Model%20%28ASWM%29.%20We%20use%20three%20medical%20datasets%20to%20highlight%20the%20impact%0Aof%20dynamic%20weighting%20on%20underrepresented%20nodes%20in%20distributed%20learning.%20Our%0Aresults%20show%20that%20the%20ASWM%20significantly%20favours%20underrepresented%20nodes%20by%0Aimproving%20their%20performance%20by%202.713%25%20in%20AUC.%20Meanwhile%2C%20nodes%20with%20larger%0Adatasets%20experience%20only%20a%20modest%20average%20performance%20decrease%20of%200.441%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16079v1&entry.124074799=Read"},
{"title": "The Evolution of LLM Adoption in Industry Data Curation Practices", "author": "Crystal Qian and Michael Xieyang Liu and Emily Reif and Grady Simon and Nada Hussein and Nathan Clement and James Wexler and Carrie J. Cai and Michael Terry and Minsuk Kahng", "abstract": "  As large language models (LLMs) grow increasingly adept at processing\nunstructured text data, they offer new opportunities to enhance data curation\nworkflows. This paper explores the evolution of LLM adoption among\npractitioners at a large technology company, evaluating the impact of LLMs in\ndata curation tasks through participants' perceptions, integration strategies,\nand reported usage scenarios. Through a series of surveys, interviews, and user\nstudies, we provide a timely snapshot of how organizations are navigating a\npivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess\nLLM adoption in industry for development tasks (N=84), and facilitated expert\ninterviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we\nexplored practitioners' current and anticipated LLM usage through a user study\ninvolving two LLM-based prototypes (N=12). While each study addressed distinct\nresearch goals, they revealed a broader narrative about evolving LLM usage in\naggregate. We discovered an emerging shift in data understanding from\nheuristic-first, bottom-up approaches to insights-first, top-down workflows\nsupported by LLMs. Furthermore, to respond to a more complex data landscape,\ndata practitioners now supplement traditional subject-expert-created 'golden\ndatasets' with LLM-generated 'silver' datasets and rigorously validated 'super\ngolden' datasets curated by diverse experts. This research sheds light on the\ntransformative role of LLMs in large-scale analysis of unstructured data and\nhighlights opportunities for further tool development.\n", "link": "http://arxiv.org/abs/2412.16089v1", "date": "2024-12-20", "relevancy": 1.9425, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Evolution%20of%20LLM%20Adoption%20in%20Industry%20Data%20Curation%20Practices&body=Title%3A%20The%20Evolution%20of%20LLM%20Adoption%20in%20Industry%20Data%20Curation%20Practices%0AAuthor%3A%20Crystal%20Qian%20and%20Michael%20Xieyang%20Liu%20and%20Emily%20Reif%20and%20Grady%20Simon%20and%20Nada%20Hussein%20and%20Nathan%20Clement%20and%20James%20Wexler%20and%20Carrie%20J.%20Cai%20and%20Michael%20Terry%20and%20Minsuk%20Kahng%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20grow%20increasingly%20adept%20at%20processing%0Aunstructured%20text%20data%2C%20they%20offer%20new%20opportunities%20to%20enhance%20data%20curation%0Aworkflows.%20This%20paper%20explores%20the%20evolution%20of%20LLM%20adoption%20among%0Apractitioners%20at%20a%20large%20technology%20company%2C%20evaluating%20the%20impact%20of%20LLMs%20in%0Adata%20curation%20tasks%20through%20participants%27%20perceptions%2C%20integration%20strategies%2C%0Aand%20reported%20usage%20scenarios.%20Through%20a%20series%20of%20surveys%2C%20interviews%2C%20and%20user%0Astudies%2C%20we%20provide%20a%20timely%20snapshot%20of%20how%20organizations%20are%20navigating%20a%0Apivotal%20moment%20in%20LLM%20evolution.%20In%20Q2%202023%2C%20we%20conducted%20a%20survey%20to%20assess%0ALLM%20adoption%20in%20industry%20for%20development%20tasks%20%28N%3D84%29%2C%20and%20facilitated%20expert%0Ainterviews%20to%20assess%20evolving%20data%20needs%20%28N%3D10%29%20in%20Q3%202023.%20In%20Q2%202024%2C%20we%0Aexplored%20practitioners%27%20current%20and%20anticipated%20LLM%20usage%20through%20a%20user%20study%0Ainvolving%20two%20LLM-based%20prototypes%20%28N%3D12%29.%20While%20each%20study%20addressed%20distinct%0Aresearch%20goals%2C%20they%20revealed%20a%20broader%20narrative%20about%20evolving%20LLM%20usage%20in%0Aaggregate.%20We%20discovered%20an%20emerging%20shift%20in%20data%20understanding%20from%0Aheuristic-first%2C%20bottom-up%20approaches%20to%20insights-first%2C%20top-down%20workflows%0Asupported%20by%20LLMs.%20Furthermore%2C%20to%20respond%20to%20a%20more%20complex%20data%20landscape%2C%0Adata%20practitioners%20now%20supplement%20traditional%20subject-expert-created%20%27golden%0Adatasets%27%20with%20LLM-generated%20%27silver%27%20datasets%20and%20rigorously%20validated%20%27super%0Agolden%27%20datasets%20curated%20by%20diverse%20experts.%20This%20research%20sheds%20light%20on%20the%0Atransformative%20role%20of%20LLMs%20in%20large-scale%20analysis%20of%20unstructured%20data%20and%0Ahighlights%20opportunities%20for%20further%20tool%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Evolution%2520of%2520LLM%2520Adoption%2520in%2520Industry%2520Data%2520Curation%2520Practices%26entry.906535625%3DCrystal%2520Qian%2520and%2520Michael%2520Xieyang%2520Liu%2520and%2520Emily%2520Reif%2520and%2520Grady%2520Simon%2520and%2520Nada%2520Hussein%2520and%2520Nathan%2520Clement%2520and%2520James%2520Wexler%2520and%2520Carrie%2520J.%2520Cai%2520and%2520Michael%2520Terry%2520and%2520Minsuk%2520Kahng%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520grow%2520increasingly%2520adept%2520at%2520processing%250Aunstructured%2520text%2520data%252C%2520they%2520offer%2520new%2520opportunities%2520to%2520enhance%2520data%2520curation%250Aworkflows.%2520This%2520paper%2520explores%2520the%2520evolution%2520of%2520LLM%2520adoption%2520among%250Apractitioners%2520at%2520a%2520large%2520technology%2520company%252C%2520evaluating%2520the%2520impact%2520of%2520LLMs%2520in%250Adata%2520curation%2520tasks%2520through%2520participants%2527%2520perceptions%252C%2520integration%2520strategies%252C%250Aand%2520reported%2520usage%2520scenarios.%2520Through%2520a%2520series%2520of%2520surveys%252C%2520interviews%252C%2520and%2520user%250Astudies%252C%2520we%2520provide%2520a%2520timely%2520snapshot%2520of%2520how%2520organizations%2520are%2520navigating%2520a%250Apivotal%2520moment%2520in%2520LLM%2520evolution.%2520In%2520Q2%25202023%252C%2520we%2520conducted%2520a%2520survey%2520to%2520assess%250ALLM%2520adoption%2520in%2520industry%2520for%2520development%2520tasks%2520%2528N%253D84%2529%252C%2520and%2520facilitated%2520expert%250Ainterviews%2520to%2520assess%2520evolving%2520data%2520needs%2520%2528N%253D10%2529%2520in%2520Q3%25202023.%2520In%2520Q2%25202024%252C%2520we%250Aexplored%2520practitioners%2527%2520current%2520and%2520anticipated%2520LLM%2520usage%2520through%2520a%2520user%2520study%250Ainvolving%2520two%2520LLM-based%2520prototypes%2520%2528N%253D12%2529.%2520While%2520each%2520study%2520addressed%2520distinct%250Aresearch%2520goals%252C%2520they%2520revealed%2520a%2520broader%2520narrative%2520about%2520evolving%2520LLM%2520usage%2520in%250Aaggregate.%2520We%2520discovered%2520an%2520emerging%2520shift%2520in%2520data%2520understanding%2520from%250Aheuristic-first%252C%2520bottom-up%2520approaches%2520to%2520insights-first%252C%2520top-down%2520workflows%250Asupported%2520by%2520LLMs.%2520Furthermore%252C%2520to%2520respond%2520to%2520a%2520more%2520complex%2520data%2520landscape%252C%250Adata%2520practitioners%2520now%2520supplement%2520traditional%2520subject-expert-created%2520%2527golden%250Adatasets%2527%2520with%2520LLM-generated%2520%2527silver%2527%2520datasets%2520and%2520rigorously%2520validated%2520%2527super%250Agolden%2527%2520datasets%2520curated%2520by%2520diverse%2520experts.%2520This%2520research%2520sheds%2520light%2520on%2520the%250Atransformative%2520role%2520of%2520LLMs%2520in%2520large-scale%2520analysis%2520of%2520unstructured%2520data%2520and%250Ahighlights%2520opportunities%2520for%2520further%2520tool%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Evolution%20of%20LLM%20Adoption%20in%20Industry%20Data%20Curation%20Practices&entry.906535625=Crystal%20Qian%20and%20Michael%20Xieyang%20Liu%20and%20Emily%20Reif%20and%20Grady%20Simon%20and%20Nada%20Hussein%20and%20Nathan%20Clement%20and%20James%20Wexler%20and%20Carrie%20J.%20Cai%20and%20Michael%20Terry%20and%20Minsuk%20Kahng&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20grow%20increasingly%20adept%20at%20processing%0Aunstructured%20text%20data%2C%20they%20offer%20new%20opportunities%20to%20enhance%20data%20curation%0Aworkflows.%20This%20paper%20explores%20the%20evolution%20of%20LLM%20adoption%20among%0Apractitioners%20at%20a%20large%20technology%20company%2C%20evaluating%20the%20impact%20of%20LLMs%20in%0Adata%20curation%20tasks%20through%20participants%27%20perceptions%2C%20integration%20strategies%2C%0Aand%20reported%20usage%20scenarios.%20Through%20a%20series%20of%20surveys%2C%20interviews%2C%20and%20user%0Astudies%2C%20we%20provide%20a%20timely%20snapshot%20of%20how%20organizations%20are%20navigating%20a%0Apivotal%20moment%20in%20LLM%20evolution.%20In%20Q2%202023%2C%20we%20conducted%20a%20survey%20to%20assess%0ALLM%20adoption%20in%20industry%20for%20development%20tasks%20%28N%3D84%29%2C%20and%20facilitated%20expert%0Ainterviews%20to%20assess%20evolving%20data%20needs%20%28N%3D10%29%20in%20Q3%202023.%20In%20Q2%202024%2C%20we%0Aexplored%20practitioners%27%20current%20and%20anticipated%20LLM%20usage%20through%20a%20user%20study%0Ainvolving%20two%20LLM-based%20prototypes%20%28N%3D12%29.%20While%20each%20study%20addressed%20distinct%0Aresearch%20goals%2C%20they%20revealed%20a%20broader%20narrative%20about%20evolving%20LLM%20usage%20in%0Aaggregate.%20We%20discovered%20an%20emerging%20shift%20in%20data%20understanding%20from%0Aheuristic-first%2C%20bottom-up%20approaches%20to%20insights-first%2C%20top-down%20workflows%0Asupported%20by%20LLMs.%20Furthermore%2C%20to%20respond%20to%20a%20more%20complex%20data%20landscape%2C%0Adata%20practitioners%20now%20supplement%20traditional%20subject-expert-created%20%27golden%0Adatasets%27%20with%20LLM-generated%20%27silver%27%20datasets%20and%20rigorously%20validated%20%27super%0Agolden%27%20datasets%20curated%20by%20diverse%20experts.%20This%20research%20sheds%20light%20on%20the%0Atransformative%20role%20of%20LLMs%20in%20large-scale%20analysis%20of%20unstructured%20data%20and%0Ahighlights%20opportunities%20for%20further%20tool%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16089v1&entry.124074799=Read"},
{"title": "On the Suitability of pre-trained foundational LLMs for Analysis in\n  German Legal Education", "author": "Lorenz Wendlinger and Christian Braun and Abdullah Al Zubaer and Simon Alexander Nonn and Sarah Gro\u00dfkopf and Christofer Fellicious and Michael Granitzer", "abstract": "  We show that current open-source foundational LLMs possess instruction\ncapability and German legal background knowledge that is sufficient for some\nlegal analysis in an educational context. However, model capability breaks down\nin very specific tasks, such as the classification of \"Gutachtenstil\" appraisal\nstyle components, or with complex contexts, such as complete legal opinions.\nEven with extended context and effective prompting strategies, they cannot\nmatch the Bag-of-Words baseline. To combat this, we introduce a Retrieval\nAugmented Generation based prompt example selection method that substantially\nimproves predictions in high data availability scenarios. We further evaluate\nthe performance of pre-trained LLMs on two standard tasks for argument mining\nand automated essay scoring and find it to be more adequate. Throughout,\npre-trained LLMs improve upon the baseline in scenarios with little or no\nlabeled data with Chain-of-Thought prompting further helping in the zero-shot\ncase.\n", "link": "http://arxiv.org/abs/2412.15902v1", "date": "2024-12-20", "relevancy": 1.9412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Suitability%20of%20pre-trained%20foundational%20LLMs%20for%20Analysis%20in%0A%20%20German%20Legal%20Education&body=Title%3A%20On%20the%20Suitability%20of%20pre-trained%20foundational%20LLMs%20for%20Analysis%20in%0A%20%20German%20Legal%20Education%0AAuthor%3A%20Lorenz%20Wendlinger%20and%20Christian%20Braun%20and%20Abdullah%20Al%20Zubaer%20and%20Simon%20Alexander%20Nonn%20and%20Sarah%20Gro%C3%9Fkopf%20and%20Christofer%20Fellicious%20and%20Michael%20Granitzer%0AAbstract%3A%20%20%20We%20show%20that%20current%20open-source%20foundational%20LLMs%20possess%20instruction%0Acapability%20and%20German%20legal%20background%20knowledge%20that%20is%20sufficient%20for%20some%0Alegal%20analysis%20in%20an%20educational%20context.%20However%2C%20model%20capability%20breaks%20down%0Ain%20very%20specific%20tasks%2C%20such%20as%20the%20classification%20of%20%22Gutachtenstil%22%20appraisal%0Astyle%20components%2C%20or%20with%20complex%20contexts%2C%20such%20as%20complete%20legal%20opinions.%0AEven%20with%20extended%20context%20and%20effective%20prompting%20strategies%2C%20they%20cannot%0Amatch%20the%20Bag-of-Words%20baseline.%20To%20combat%20this%2C%20we%20introduce%20a%20Retrieval%0AAugmented%20Generation%20based%20prompt%20example%20selection%20method%20that%20substantially%0Aimproves%20predictions%20in%20high%20data%20availability%20scenarios.%20We%20further%20evaluate%0Athe%20performance%20of%20pre-trained%20LLMs%20on%20two%20standard%20tasks%20for%20argument%20mining%0Aand%20automated%20essay%20scoring%20and%20find%20it%20to%20be%20more%20adequate.%20Throughout%2C%0Apre-trained%20LLMs%20improve%20upon%20the%20baseline%20in%20scenarios%20with%20little%20or%20no%0Alabeled%20data%20with%20Chain-of-Thought%20prompting%20further%20helping%20in%20the%20zero-shot%0Acase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Suitability%2520of%2520pre-trained%2520foundational%2520LLMs%2520for%2520Analysis%2520in%250A%2520%2520German%2520Legal%2520Education%26entry.906535625%3DLorenz%2520Wendlinger%2520and%2520Christian%2520Braun%2520and%2520Abdullah%2520Al%2520Zubaer%2520and%2520Simon%2520Alexander%2520Nonn%2520and%2520Sarah%2520Gro%25C3%259Fkopf%2520and%2520Christofer%2520Fellicious%2520and%2520Michael%2520Granitzer%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520current%2520open-source%2520foundational%2520LLMs%2520possess%2520instruction%250Acapability%2520and%2520German%2520legal%2520background%2520knowledge%2520that%2520is%2520sufficient%2520for%2520some%250Alegal%2520analysis%2520in%2520an%2520educational%2520context.%2520However%252C%2520model%2520capability%2520breaks%2520down%250Ain%2520very%2520specific%2520tasks%252C%2520such%2520as%2520the%2520classification%2520of%2520%2522Gutachtenstil%2522%2520appraisal%250Astyle%2520components%252C%2520or%2520with%2520complex%2520contexts%252C%2520such%2520as%2520complete%2520legal%2520opinions.%250AEven%2520with%2520extended%2520context%2520and%2520effective%2520prompting%2520strategies%252C%2520they%2520cannot%250Amatch%2520the%2520Bag-of-Words%2520baseline.%2520To%2520combat%2520this%252C%2520we%2520introduce%2520a%2520Retrieval%250AAugmented%2520Generation%2520based%2520prompt%2520example%2520selection%2520method%2520that%2520substantially%250Aimproves%2520predictions%2520in%2520high%2520data%2520availability%2520scenarios.%2520We%2520further%2520evaluate%250Athe%2520performance%2520of%2520pre-trained%2520LLMs%2520on%2520two%2520standard%2520tasks%2520for%2520argument%2520mining%250Aand%2520automated%2520essay%2520scoring%2520and%2520find%2520it%2520to%2520be%2520more%2520adequate.%2520Throughout%252C%250Apre-trained%2520LLMs%2520improve%2520upon%2520the%2520baseline%2520in%2520scenarios%2520with%2520little%2520or%2520no%250Alabeled%2520data%2520with%2520Chain-of-Thought%2520prompting%2520further%2520helping%2520in%2520the%2520zero-shot%250Acase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Suitability%20of%20pre-trained%20foundational%20LLMs%20for%20Analysis%20in%0A%20%20German%20Legal%20Education&entry.906535625=Lorenz%20Wendlinger%20and%20Christian%20Braun%20and%20Abdullah%20Al%20Zubaer%20and%20Simon%20Alexander%20Nonn%20and%20Sarah%20Gro%C3%9Fkopf%20and%20Christofer%20Fellicious%20and%20Michael%20Granitzer&entry.1292438233=%20%20We%20show%20that%20current%20open-source%20foundational%20LLMs%20possess%20instruction%0Acapability%20and%20German%20legal%20background%20knowledge%20that%20is%20sufficient%20for%20some%0Alegal%20analysis%20in%20an%20educational%20context.%20However%2C%20model%20capability%20breaks%20down%0Ain%20very%20specific%20tasks%2C%20such%20as%20the%20classification%20of%20%22Gutachtenstil%22%20appraisal%0Astyle%20components%2C%20or%20with%20complex%20contexts%2C%20such%20as%20complete%20legal%20opinions.%0AEven%20with%20extended%20context%20and%20effective%20prompting%20strategies%2C%20they%20cannot%0Amatch%20the%20Bag-of-Words%20baseline.%20To%20combat%20this%2C%20we%20introduce%20a%20Retrieval%0AAugmented%20Generation%20based%20prompt%20example%20selection%20method%20that%20substantially%0Aimproves%20predictions%20in%20high%20data%20availability%20scenarios.%20We%20further%20evaluate%0Athe%20performance%20of%20pre-trained%20LLMs%20on%20two%20standard%20tasks%20for%20argument%20mining%0Aand%20automated%20essay%20scoring%20and%20find%20it%20to%20be%20more%20adequate.%20Throughout%2C%0Apre-trained%20LLMs%20improve%20upon%20the%20baseline%20in%20scenarios%20with%20little%20or%20no%0Alabeled%20data%20with%20Chain-of-Thought%20prompting%20further%20helping%20in%20the%20zero-shot%0Acase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15902v1&entry.124074799=Read"},
{"title": "Development of a Large-scale Dataset of Chest Computed Tomography\n  Reports in Japanese and a High-performance Finding Classification Model", "author": "Yosuke Yamagishi and Yuta Nakamura and Tomohiro Kikuchi and Yuki Sonoda and Hiroshi Hirakawa and Shintaro Kano and Satoshi Nakamura and Shouhei Hanaoka and Takeharu Yoshikawa and Osamu Abe", "abstract": "  Background: Recent advances in large language models highlight the need for\nhigh-quality multilingual medical datasets. While Japan leads globally in CT\nscanner deployment and utilization, the lack of large-scale Japanese radiology\ndatasets has hindered the development of specialized language models for\nmedical imaging analysis. Objective: To develop a comprehensive Japanese CT\nreport dataset through machine translation and establish a specialized language\nmodel for structured finding classification. Additionally, to create a\nrigorously validated evaluation dataset through expert radiologist review.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304\npatients) into Japanese using GPT-4o mini. The training dataset consisted of\n22,778 machine-translated reports, while the validation dataset included 150\nradiologist-revised reports. We developed CT-BERT-JPN based on\n\"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured\nfindings from Japanese radiology reports. Results: Translation metrics showed\nstrong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores\nranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression\nsections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in\n11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular\nseptal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1\nscores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in\nfour conditions. Conclusions: Our study establishes a robust Japanese CT report\ndataset and demonstrates the effectiveness of a specialized language model for\nstructured finding classification. The hybrid approach of machine translation\nand expert validation enables the creation of large-scale medical datasets\nwhile maintaining high quality.\n", "link": "http://arxiv.org/abs/2412.15907v1", "date": "2024-12-20", "relevancy": 1.9387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20a%20Large-scale%20Dataset%20of%20Chest%20Computed%20Tomography%0A%20%20Reports%20in%20Japanese%20and%20a%20High-performance%20Finding%20Classification%20Model&body=Title%3A%20Development%20of%20a%20Large-scale%20Dataset%20of%20Chest%20Computed%20Tomography%0A%20%20Reports%20in%20Japanese%20and%20a%20High-performance%20Finding%20Classification%20Model%0AAuthor%3A%20Yosuke%20Yamagishi%20and%20Yuta%20Nakamura%20and%20Tomohiro%20Kikuchi%20and%20Yuki%20Sonoda%20and%20Hiroshi%20Hirakawa%20and%20Shintaro%20Kano%20and%20Satoshi%20Nakamura%20and%20Shouhei%20Hanaoka%20and%20Takeharu%20Yoshikawa%20and%20Osamu%20Abe%0AAbstract%3A%20%20%20Background%3A%20Recent%20advances%20in%20large%20language%20models%20highlight%20the%20need%20for%0Ahigh-quality%20multilingual%20medical%20datasets.%20While%20Japan%20leads%20globally%20in%20CT%0Ascanner%20deployment%20and%20utilization%2C%20the%20lack%20of%20large-scale%20Japanese%20radiology%0Adatasets%20has%20hindered%20the%20development%20of%20specialized%20language%20models%20for%0Amedical%20imaging%20analysis.%20Objective%3A%20To%20develop%20a%20comprehensive%20Japanese%20CT%0Areport%20dataset%20through%20machine%20translation%20and%20establish%20a%20specialized%20language%0Amodel%20for%20structured%20finding%20classification.%20Additionally%2C%20to%20create%20a%0Arigorously%20validated%20evaluation%20dataset%20through%20expert%20radiologist%20review.%0AMethods%3A%20We%20translated%20the%20CT-RATE%20dataset%20%2824%2C283%20CT%20reports%20from%2021%2C304%0Apatients%29%20into%20Japanese%20using%20GPT-4o%20mini.%20The%20training%20dataset%20consisted%20of%0A22%2C778%20machine-translated%20reports%2C%20while%20the%20validation%20dataset%20included%20150%0Aradiologist-revised%20reports.%20We%20developed%20CT-BERT-JPN%20based%20on%0A%22tohoku-nlp/bert-base-japanese-v3%22%20architecture%20for%20extracting%2018%20structured%0Afindings%20from%20Japanese%20radiology%20reports.%20Results%3A%20Translation%20metrics%20showed%0Astrong%20performance%20with%20BLEU%20scores%20of%200.731%20and%200.690%2C%20and%20ROUGE%20scores%0Aranging%20from%200.770%20to%200.876%20for%20Findings%20and%20from%200.748%20to%200.857%20for%20Impression%0Asections.%20CT-BERT-JPN%20demonstrated%20superior%20performance%20compared%20to%20GPT-4o%20in%0A11%20out%20of%2018%20conditions%2C%20including%20lymphadenopathy%20%28%2B14.2%25%29%2C%20interlobular%0Aseptal%20thickening%20%28%2B10.9%25%29%2C%20and%20atelectasis%20%28%2B7.4%25%29.%20The%20model%20maintained%20F1%0Ascores%20exceeding%200.95%20in%2014%20out%20of%2018%20conditions%20and%20achieved%20perfect%20scores%20in%0Afour%20conditions.%20Conclusions%3A%20Our%20study%20establishes%20a%20robust%20Japanese%20CT%20report%0Adataset%20and%20demonstrates%20the%20effectiveness%20of%20a%20specialized%20language%20model%20for%0Astructured%20finding%20classification.%20The%20hybrid%20approach%20of%20machine%20translation%0Aand%20expert%20validation%20enables%20the%20creation%20of%20large-scale%20medical%20datasets%0Awhile%20maintaining%20high%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520a%2520Large-scale%2520Dataset%2520of%2520Chest%2520Computed%2520Tomography%250A%2520%2520Reports%2520in%2520Japanese%2520and%2520a%2520High-performance%2520Finding%2520Classification%2520Model%26entry.906535625%3DYosuke%2520Yamagishi%2520and%2520Yuta%2520Nakamura%2520and%2520Tomohiro%2520Kikuchi%2520and%2520Yuki%2520Sonoda%2520and%2520Hiroshi%2520Hirakawa%2520and%2520Shintaro%2520Kano%2520and%2520Satoshi%2520Nakamura%2520and%2520Shouhei%2520Hanaoka%2520and%2520Takeharu%2520Yoshikawa%2520and%2520Osamu%2520Abe%26entry.1292438233%3D%2520%2520Background%253A%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520highlight%2520the%2520need%2520for%250Ahigh-quality%2520multilingual%2520medical%2520datasets.%2520While%2520Japan%2520leads%2520globally%2520in%2520CT%250Ascanner%2520deployment%2520and%2520utilization%252C%2520the%2520lack%2520of%2520large-scale%2520Japanese%2520radiology%250Adatasets%2520has%2520hindered%2520the%2520development%2520of%2520specialized%2520language%2520models%2520for%250Amedical%2520imaging%2520analysis.%2520Objective%253A%2520To%2520develop%2520a%2520comprehensive%2520Japanese%2520CT%250Areport%2520dataset%2520through%2520machine%2520translation%2520and%2520establish%2520a%2520specialized%2520language%250Amodel%2520for%2520structured%2520finding%2520classification.%2520Additionally%252C%2520to%2520create%2520a%250Arigorously%2520validated%2520evaluation%2520dataset%2520through%2520expert%2520radiologist%2520review.%250AMethods%253A%2520We%2520translated%2520the%2520CT-RATE%2520dataset%2520%252824%252C283%2520CT%2520reports%2520from%252021%252C304%250Apatients%2529%2520into%2520Japanese%2520using%2520GPT-4o%2520mini.%2520The%2520training%2520dataset%2520consisted%2520of%250A22%252C778%2520machine-translated%2520reports%252C%2520while%2520the%2520validation%2520dataset%2520included%2520150%250Aradiologist-revised%2520reports.%2520We%2520developed%2520CT-BERT-JPN%2520based%2520on%250A%2522tohoku-nlp/bert-base-japanese-v3%2522%2520architecture%2520for%2520extracting%252018%2520structured%250Afindings%2520from%2520Japanese%2520radiology%2520reports.%2520Results%253A%2520Translation%2520metrics%2520showed%250Astrong%2520performance%2520with%2520BLEU%2520scores%2520of%25200.731%2520and%25200.690%252C%2520and%2520ROUGE%2520scores%250Aranging%2520from%25200.770%2520to%25200.876%2520for%2520Findings%2520and%2520from%25200.748%2520to%25200.857%2520for%2520Impression%250Asections.%2520CT-BERT-JPN%2520demonstrated%2520superior%2520performance%2520compared%2520to%2520GPT-4o%2520in%250A11%2520out%2520of%252018%2520conditions%252C%2520including%2520lymphadenopathy%2520%2528%252B14.2%2525%2529%252C%2520interlobular%250Aseptal%2520thickening%2520%2528%252B10.9%2525%2529%252C%2520and%2520atelectasis%2520%2528%252B7.4%2525%2529.%2520The%2520model%2520maintained%2520F1%250Ascores%2520exceeding%25200.95%2520in%252014%2520out%2520of%252018%2520conditions%2520and%2520achieved%2520perfect%2520scores%2520in%250Afour%2520conditions.%2520Conclusions%253A%2520Our%2520study%2520establishes%2520a%2520robust%2520Japanese%2520CT%2520report%250Adataset%2520and%2520demonstrates%2520the%2520effectiveness%2520of%2520a%2520specialized%2520language%2520model%2520for%250Astructured%2520finding%2520classification.%2520The%2520hybrid%2520approach%2520of%2520machine%2520translation%250Aand%2520expert%2520validation%2520enables%2520the%2520creation%2520of%2520large-scale%2520medical%2520datasets%250Awhile%2520maintaining%2520high%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20a%20Large-scale%20Dataset%20of%20Chest%20Computed%20Tomography%0A%20%20Reports%20in%20Japanese%20and%20a%20High-performance%20Finding%20Classification%20Model&entry.906535625=Yosuke%20Yamagishi%20and%20Yuta%20Nakamura%20and%20Tomohiro%20Kikuchi%20and%20Yuki%20Sonoda%20and%20Hiroshi%20Hirakawa%20and%20Shintaro%20Kano%20and%20Satoshi%20Nakamura%20and%20Shouhei%20Hanaoka%20and%20Takeharu%20Yoshikawa%20and%20Osamu%20Abe&entry.1292438233=%20%20Background%3A%20Recent%20advances%20in%20large%20language%20models%20highlight%20the%20need%20for%0Ahigh-quality%20multilingual%20medical%20datasets.%20While%20Japan%20leads%20globally%20in%20CT%0Ascanner%20deployment%20and%20utilization%2C%20the%20lack%20of%20large-scale%20Japanese%20radiology%0Adatasets%20has%20hindered%20the%20development%20of%20specialized%20language%20models%20for%0Amedical%20imaging%20analysis.%20Objective%3A%20To%20develop%20a%20comprehensive%20Japanese%20CT%0Areport%20dataset%20through%20machine%20translation%20and%20establish%20a%20specialized%20language%0Amodel%20for%20structured%20finding%20classification.%20Additionally%2C%20to%20create%20a%0Arigorously%20validated%20evaluation%20dataset%20through%20expert%20radiologist%20review.%0AMethods%3A%20We%20translated%20the%20CT-RATE%20dataset%20%2824%2C283%20CT%20reports%20from%2021%2C304%0Apatients%29%20into%20Japanese%20using%20GPT-4o%20mini.%20The%20training%20dataset%20consisted%20of%0A22%2C778%20machine-translated%20reports%2C%20while%20the%20validation%20dataset%20included%20150%0Aradiologist-revised%20reports.%20We%20developed%20CT-BERT-JPN%20based%20on%0A%22tohoku-nlp/bert-base-japanese-v3%22%20architecture%20for%20extracting%2018%20structured%0Afindings%20from%20Japanese%20radiology%20reports.%20Results%3A%20Translation%20metrics%20showed%0Astrong%20performance%20with%20BLEU%20scores%20of%200.731%20and%200.690%2C%20and%20ROUGE%20scores%0Aranging%20from%200.770%20to%200.876%20for%20Findings%20and%20from%200.748%20to%200.857%20for%20Impression%0Asections.%20CT-BERT-JPN%20demonstrated%20superior%20performance%20compared%20to%20GPT-4o%20in%0A11%20out%20of%2018%20conditions%2C%20including%20lymphadenopathy%20%28%2B14.2%25%29%2C%20interlobular%0Aseptal%20thickening%20%28%2B10.9%25%29%2C%20and%20atelectasis%20%28%2B7.4%25%29.%20The%20model%20maintained%20F1%0Ascores%20exceeding%200.95%20in%2014%20out%20of%2018%20conditions%20and%20achieved%20perfect%20scores%20in%0Afour%20conditions.%20Conclusions%3A%20Our%20study%20establishes%20a%20robust%20Japanese%20CT%20report%0Adataset%20and%20demonstrates%20the%20effectiveness%20of%20a%20specialized%20language%20model%20for%0Astructured%20finding%20classification.%20The%20hybrid%20approach%20of%20machine%20translation%0Aand%20expert%20validation%20enables%20the%20creation%20of%20large-scale%20medical%20datasets%0Awhile%20maintaining%20high%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15907v1&entry.124074799=Read"},
{"title": "Little is Enough: Boosting Privacy by Sharing Only Hard Labels in\n  Federated Semi-Supervised Learning", "author": "Amr Abourayya and Jens Kleesiek and Kanishka Rao and Erman Ayday and Bharat Rao and Geoff Webb and Michael Kamp", "abstract": "  In many critical applications, sensitive data is inherently distributed and\ncannot be centralized due to privacy concerns. A wide range of federated\nlearning approaches have been proposed to train models locally at each client\nwithout sharing their sensitive data, typically by exchanging model parameters,\nor probabilistic predictions (soft labels) on a public dataset or a combination\nof both. However, these methods still disclose private information and restrict\nlocal models to those that can be trained using gradient-based methods. We\npropose a federated co-training (FedCT) approach that improves privacy by\nsharing only definitive (hard) labels on a public unlabeled dataset. Clients\nuse a consensus of these shared labels as pseudo-labels for local training.\nThis federated co-training approach empirically enhances privacy without\ncompromising model quality. In addition, it allows the use of local models that\nare not suitable for parameter aggregation in traditional federated learning,\nsuch as gradient-boosted decision trees, rule ensembles, and random forests.\nFurthermore, we observe that FedCT performs effectively in federated\nfine-tuning of large language models, where its pseudo-labeling mechanism is\nparticularly beneficial. Empirical evaluations and theoretical analyses suggest\nits applicability across a range of federated learning scenarios.\n", "link": "http://arxiv.org/abs/2310.05696v4", "date": "2024-12-20", "relevancy": 1.9373, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4885}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Little%20is%20Enough%3A%20Boosting%20Privacy%20by%20Sharing%20Only%20Hard%20Labels%20in%0A%20%20Federated%20Semi-Supervised%20Learning&body=Title%3A%20Little%20is%20Enough%3A%20Boosting%20Privacy%20by%20Sharing%20Only%20Hard%20Labels%20in%0A%20%20Federated%20Semi-Supervised%20Learning%0AAuthor%3A%20Amr%20Abourayya%20and%20Jens%20Kleesiek%20and%20Kanishka%20Rao%20and%20Erman%20Ayday%20and%20Bharat%20Rao%20and%20Geoff%20Webb%20and%20Michael%20Kamp%0AAbstract%3A%20%20%20In%20many%20critical%20applications%2C%20sensitive%20data%20is%20inherently%20distributed%20and%0Acannot%20be%20centralized%20due%20to%20privacy%20concerns.%20A%20wide%20range%20of%20federated%0Alearning%20approaches%20have%20been%20proposed%20to%20train%20models%20locally%20at%20each%20client%0Awithout%20sharing%20their%20sensitive%20data%2C%20typically%20by%20exchanging%20model%20parameters%2C%0Aor%20probabilistic%20predictions%20%28soft%20labels%29%20on%20a%20public%20dataset%20or%20a%20combination%0Aof%20both.%20However%2C%20these%20methods%20still%20disclose%20private%20information%20and%20restrict%0Alocal%20models%20to%20those%20that%20can%20be%20trained%20using%20gradient-based%20methods.%20We%0Apropose%20a%20federated%20co-training%20%28FedCT%29%20approach%20that%20improves%20privacy%20by%0Asharing%20only%20definitive%20%28hard%29%20labels%20on%20a%20public%20unlabeled%20dataset.%20Clients%0Ause%20a%20consensus%20of%20these%20shared%20labels%20as%20pseudo-labels%20for%20local%20training.%0AThis%20federated%20co-training%20approach%20empirically%20enhances%20privacy%20without%0Acompromising%20model%20quality.%20In%20addition%2C%20it%20allows%20the%20use%20of%20local%20models%20that%0Aare%20not%20suitable%20for%20parameter%20aggregation%20in%20traditional%20federated%20learning%2C%0Asuch%20as%20gradient-boosted%20decision%20trees%2C%20rule%20ensembles%2C%20and%20random%20forests.%0AFurthermore%2C%20we%20observe%20that%20FedCT%20performs%20effectively%20in%20federated%0Afine-tuning%20of%20large%20language%20models%2C%20where%20its%20pseudo-labeling%20mechanism%20is%0Aparticularly%20beneficial.%20Empirical%20evaluations%20and%20theoretical%20analyses%20suggest%0Aits%20applicability%20across%20a%20range%20of%20federated%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05696v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLittle%2520is%2520Enough%253A%2520Boosting%2520Privacy%2520by%2520Sharing%2520Only%2520Hard%2520Labels%2520in%250A%2520%2520Federated%2520Semi-Supervised%2520Learning%26entry.906535625%3DAmr%2520Abourayya%2520and%2520Jens%2520Kleesiek%2520and%2520Kanishka%2520Rao%2520and%2520Erman%2520Ayday%2520and%2520Bharat%2520Rao%2520and%2520Geoff%2520Webb%2520and%2520Michael%2520Kamp%26entry.1292438233%3D%2520%2520In%2520many%2520critical%2520applications%252C%2520sensitive%2520data%2520is%2520inherently%2520distributed%2520and%250Acannot%2520be%2520centralized%2520due%2520to%2520privacy%2520concerns.%2520A%2520wide%2520range%2520of%2520federated%250Alearning%2520approaches%2520have%2520been%2520proposed%2520to%2520train%2520models%2520locally%2520at%2520each%2520client%250Awithout%2520sharing%2520their%2520sensitive%2520data%252C%2520typically%2520by%2520exchanging%2520model%2520parameters%252C%250Aor%2520probabilistic%2520predictions%2520%2528soft%2520labels%2529%2520on%2520a%2520public%2520dataset%2520or%2520a%2520combination%250Aof%2520both.%2520However%252C%2520these%2520methods%2520still%2520disclose%2520private%2520information%2520and%2520restrict%250Alocal%2520models%2520to%2520those%2520that%2520can%2520be%2520trained%2520using%2520gradient-based%2520methods.%2520We%250Apropose%2520a%2520federated%2520co-training%2520%2528FedCT%2529%2520approach%2520that%2520improves%2520privacy%2520by%250Asharing%2520only%2520definitive%2520%2528hard%2529%2520labels%2520on%2520a%2520public%2520unlabeled%2520dataset.%2520Clients%250Ause%2520a%2520consensus%2520of%2520these%2520shared%2520labels%2520as%2520pseudo-labels%2520for%2520local%2520training.%250AThis%2520federated%2520co-training%2520approach%2520empirically%2520enhances%2520privacy%2520without%250Acompromising%2520model%2520quality.%2520In%2520addition%252C%2520it%2520allows%2520the%2520use%2520of%2520local%2520models%2520that%250Aare%2520not%2520suitable%2520for%2520parameter%2520aggregation%2520in%2520traditional%2520federated%2520learning%252C%250Asuch%2520as%2520gradient-boosted%2520decision%2520trees%252C%2520rule%2520ensembles%252C%2520and%2520random%2520forests.%250AFurthermore%252C%2520we%2520observe%2520that%2520FedCT%2520performs%2520effectively%2520in%2520federated%250Afine-tuning%2520of%2520large%2520language%2520models%252C%2520where%2520its%2520pseudo-labeling%2520mechanism%2520is%250Aparticularly%2520beneficial.%2520Empirical%2520evaluations%2520and%2520theoretical%2520analyses%2520suggest%250Aits%2520applicability%2520across%2520a%2520range%2520of%2520federated%2520learning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05696v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Little%20is%20Enough%3A%20Boosting%20Privacy%20by%20Sharing%20Only%20Hard%20Labels%20in%0A%20%20Federated%20Semi-Supervised%20Learning&entry.906535625=Amr%20Abourayya%20and%20Jens%20Kleesiek%20and%20Kanishka%20Rao%20and%20Erman%20Ayday%20and%20Bharat%20Rao%20and%20Geoff%20Webb%20and%20Michael%20Kamp&entry.1292438233=%20%20In%20many%20critical%20applications%2C%20sensitive%20data%20is%20inherently%20distributed%20and%0Acannot%20be%20centralized%20due%20to%20privacy%20concerns.%20A%20wide%20range%20of%20federated%0Alearning%20approaches%20have%20been%20proposed%20to%20train%20models%20locally%20at%20each%20client%0Awithout%20sharing%20their%20sensitive%20data%2C%20typically%20by%20exchanging%20model%20parameters%2C%0Aor%20probabilistic%20predictions%20%28soft%20labels%29%20on%20a%20public%20dataset%20or%20a%20combination%0Aof%20both.%20However%2C%20these%20methods%20still%20disclose%20private%20information%20and%20restrict%0Alocal%20models%20to%20those%20that%20can%20be%20trained%20using%20gradient-based%20methods.%20We%0Apropose%20a%20federated%20co-training%20%28FedCT%29%20approach%20that%20improves%20privacy%20by%0Asharing%20only%20definitive%20%28hard%29%20labels%20on%20a%20public%20unlabeled%20dataset.%20Clients%0Ause%20a%20consensus%20of%20these%20shared%20labels%20as%20pseudo-labels%20for%20local%20training.%0AThis%20federated%20co-training%20approach%20empirically%20enhances%20privacy%20without%0Acompromising%20model%20quality.%20In%20addition%2C%20it%20allows%20the%20use%20of%20local%20models%20that%0Aare%20not%20suitable%20for%20parameter%20aggregation%20in%20traditional%20federated%20learning%2C%0Asuch%20as%20gradient-boosted%20decision%20trees%2C%20rule%20ensembles%2C%20and%20random%20forests.%0AFurthermore%2C%20we%20observe%20that%20FedCT%20performs%20effectively%20in%20federated%0Afine-tuning%20of%20large%20language%20models%2C%20where%20its%20pseudo-labeling%20mechanism%20is%0Aparticularly%20beneficial.%20Empirical%20evaluations%20and%20theoretical%20analyses%20suggest%0Aits%20applicability%20across%20a%20range%20of%20federated%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05696v4&entry.124074799=Read"},
{"title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical\n  Large Language Models", "author": "Shamus Sim and Tyrone Chen", "abstract": "  Background: Despite the current ubiquity of Large Language Models (LLMs)\nacross the medical domain, there is a surprising lack of studies which address\ntheir reasoning behaviour. We emphasise the importance of understanding\nreasoning behaviour as opposed to high-level prediction accuracies, since it is\nequivalent to explainable AI (XAI) in this context. In particular, achieving\nXAI in medical LLMs used in the clinical domain will have a significant impact\nacross the healthcare sector. Results: Therefore, we define the concept of\nreasoning behaviour in the specific context of medical LLMs. We then categorise\nand discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can\nempower medical professionals or machine learning engineers to gain insight\ninto the low-level reasoning operations of these previously obscure models.\nConclusion: The subsequent increased transparency and trust in medical machine\nlearning models by clinicians as well as patients will accelerate the\nintegration, application as well as further development of medical AI for the\nhealthcare system as a whole\n", "link": "http://arxiv.org/abs/2412.15748v1", "date": "2024-12-20", "relevancy": 1.9317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critique%20of%20Impure%20Reason%3A%20Unveiling%20the%20reasoning%20behaviour%20of%20medical%0A%20%20Large%20Language%20Models&body=Title%3A%20Critique%20of%20Impure%20Reason%3A%20Unveiling%20the%20reasoning%20behaviour%20of%20medical%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Shamus%20Sim%20and%20Tyrone%20Chen%0AAbstract%3A%20%20%20Background%3A%20Despite%20the%20current%20ubiquity%20of%20Large%20Language%20Models%20%28LLMs%29%0Aacross%20the%20medical%20domain%2C%20there%20is%20a%20surprising%20lack%20of%20studies%20which%20address%0Atheir%20reasoning%20behaviour.%20We%20emphasise%20the%20importance%20of%20understanding%0Areasoning%20behaviour%20as%20opposed%20to%20high-level%20prediction%20accuracies%2C%20since%20it%20is%0Aequivalent%20to%20explainable%20AI%20%28XAI%29%20in%20this%20context.%20In%20particular%2C%20achieving%0AXAI%20in%20medical%20LLMs%20used%20in%20the%20clinical%20domain%20will%20have%20a%20significant%20impact%0Aacross%20the%20healthcare%20sector.%20Results%3A%20Therefore%2C%20we%20define%20the%20concept%20of%0Areasoning%20behaviour%20in%20the%20specific%20context%20of%20medical%20LLMs.%20We%20then%20categorise%0Aand%20discuss%20the%20current%20state%20of%20the%20art%20of%20methods%20which%20evaluate%20reasoning%0Abehaviour%20in%20medical%20LLMs.%20Finally%2C%20we%20propose%20theoretical%20frameworks%20which%20can%0Aempower%20medical%20professionals%20or%20machine%20learning%20engineers%20to%20gain%20insight%0Ainto%20the%20low-level%20reasoning%20operations%20of%20these%20previously%20obscure%20models.%0AConclusion%3A%20The%20subsequent%20increased%20transparency%20and%20trust%20in%20medical%20machine%0Alearning%20models%20by%20clinicians%20as%20well%20as%20patients%20will%20accelerate%20the%0Aintegration%2C%20application%20as%20well%20as%20further%20development%20of%20medical%20AI%20for%20the%0Ahealthcare%20system%20as%20a%20whole%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritique%2520of%2520Impure%2520Reason%253A%2520Unveiling%2520the%2520reasoning%2520behaviour%2520of%2520medical%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DShamus%2520Sim%2520and%2520Tyrone%2520Chen%26entry.1292438233%3D%2520%2520Background%253A%2520Despite%2520the%2520current%2520ubiquity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aacross%2520the%2520medical%2520domain%252C%2520there%2520is%2520a%2520surprising%2520lack%2520of%2520studies%2520which%2520address%250Atheir%2520reasoning%2520behaviour.%2520We%2520emphasise%2520the%2520importance%2520of%2520understanding%250Areasoning%2520behaviour%2520as%2520opposed%2520to%2520high-level%2520prediction%2520accuracies%252C%2520since%2520it%2520is%250Aequivalent%2520to%2520explainable%2520AI%2520%2528XAI%2529%2520in%2520this%2520context.%2520In%2520particular%252C%2520achieving%250AXAI%2520in%2520medical%2520LLMs%2520used%2520in%2520the%2520clinical%2520domain%2520will%2520have%2520a%2520significant%2520impact%250Aacross%2520the%2520healthcare%2520sector.%2520Results%253A%2520Therefore%252C%2520we%2520define%2520the%2520concept%2520of%250Areasoning%2520behaviour%2520in%2520the%2520specific%2520context%2520of%2520medical%2520LLMs.%2520We%2520then%2520categorise%250Aand%2520discuss%2520the%2520current%2520state%2520of%2520the%2520art%2520of%2520methods%2520which%2520evaluate%2520reasoning%250Abehaviour%2520in%2520medical%2520LLMs.%2520Finally%252C%2520we%2520propose%2520theoretical%2520frameworks%2520which%2520can%250Aempower%2520medical%2520professionals%2520or%2520machine%2520learning%2520engineers%2520to%2520gain%2520insight%250Ainto%2520the%2520low-level%2520reasoning%2520operations%2520of%2520these%2520previously%2520obscure%2520models.%250AConclusion%253A%2520The%2520subsequent%2520increased%2520transparency%2520and%2520trust%2520in%2520medical%2520machine%250Alearning%2520models%2520by%2520clinicians%2520as%2520well%2520as%2520patients%2520will%2520accelerate%2520the%250Aintegration%252C%2520application%2520as%2520well%2520as%2520further%2520development%2520of%2520medical%2520AI%2520for%2520the%250Ahealthcare%2520system%2520as%2520a%2520whole%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critique%20of%20Impure%20Reason%3A%20Unveiling%20the%20reasoning%20behaviour%20of%20medical%0A%20%20Large%20Language%20Models&entry.906535625=Shamus%20Sim%20and%20Tyrone%20Chen&entry.1292438233=%20%20Background%3A%20Despite%20the%20current%20ubiquity%20of%20Large%20Language%20Models%20%28LLMs%29%0Aacross%20the%20medical%20domain%2C%20there%20is%20a%20surprising%20lack%20of%20studies%20which%20address%0Atheir%20reasoning%20behaviour.%20We%20emphasise%20the%20importance%20of%20understanding%0Areasoning%20behaviour%20as%20opposed%20to%20high-level%20prediction%20accuracies%2C%20since%20it%20is%0Aequivalent%20to%20explainable%20AI%20%28XAI%29%20in%20this%20context.%20In%20particular%2C%20achieving%0AXAI%20in%20medical%20LLMs%20used%20in%20the%20clinical%20domain%20will%20have%20a%20significant%20impact%0Aacross%20the%20healthcare%20sector.%20Results%3A%20Therefore%2C%20we%20define%20the%20concept%20of%0Areasoning%20behaviour%20in%20the%20specific%20context%20of%20medical%20LLMs.%20We%20then%20categorise%0Aand%20discuss%20the%20current%20state%20of%20the%20art%20of%20methods%20which%20evaluate%20reasoning%0Abehaviour%20in%20medical%20LLMs.%20Finally%2C%20we%20propose%20theoretical%20frameworks%20which%20can%0Aempower%20medical%20professionals%20or%20machine%20learning%20engineers%20to%20gain%20insight%0Ainto%20the%20low-level%20reasoning%20operations%20of%20these%20previously%20obscure%20models.%0AConclusion%3A%20The%20subsequent%20increased%20transparency%20and%20trust%20in%20medical%20machine%0Alearning%20models%20by%20clinicians%20as%20well%20as%20patients%20will%20accelerate%20the%0Aintegration%2C%20application%20as%20well%20as%20further%20development%20of%20medical%20AI%20for%20the%0Ahealthcare%20system%20as%20a%20whole%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15748v1&entry.124074799=Read"},
{"title": "Representation Learning of Daily Movement Data Using Text Encoders", "author": "Alexander Capstick and Tianyu Cui and Yu Chen and Payam Barnaghi", "abstract": "  Time-series representation learning is a key area of research for remote\nhealthcare monitoring applications. In this work, we focus on a dataset of\nrecordings of in-home activity from people living with Dementia. We design a\nrepresentation learning method based on converting activity to text strings\nthat can be encoded using a language model fine-tuned to transform data from\nthe same participants within a $30$-day window to similar embeddings in the\nvector space. This allows for clustering and vector searching over participants\nand days, and the identification of activity deviations to aid with\npersonalised delivery of care.\n", "link": "http://arxiv.org/abs/2405.04494v2", "date": "2024-12-20", "relevancy": 1.9289, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4934}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders&body=Title%3A%20Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders%0AAuthor%3A%20Alexander%20Capstick%20and%20Tianyu%20Cui%20and%20Yu%20Chen%20and%20Payam%20Barnaghi%0AAbstract%3A%20%20%20Time-series%20representation%20learning%20is%20a%20key%20area%20of%20research%20for%20remote%0Ahealthcare%20monitoring%20applications.%20In%20this%20work%2C%20we%20focus%20on%20a%20dataset%20of%0Arecordings%20of%20in-home%20activity%20from%20people%20living%20with%20Dementia.%20We%20design%20a%0Arepresentation%20learning%20method%20based%20on%20converting%20activity%20to%20text%20strings%0Athat%20can%20be%20encoded%20using%20a%20language%20model%20fine-tuned%20to%20transform%20data%20from%0Athe%20same%20participants%20within%20a%20%2430%24-day%20window%20to%20similar%20embeddings%20in%20the%0Avector%20space.%20This%20allows%20for%20clustering%20and%20vector%20searching%20over%20participants%0Aand%20days%2C%20and%20the%20identification%20of%20activity%20deviations%20to%20aid%20with%0Apersonalised%20delivery%20of%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520of%2520Daily%2520Movement%2520Data%2520Using%2520Text%2520Encoders%26entry.906535625%3DAlexander%2520Capstick%2520and%2520Tianyu%2520Cui%2520and%2520Yu%2520Chen%2520and%2520Payam%2520Barnaghi%26entry.1292438233%3D%2520%2520Time-series%2520representation%2520learning%2520is%2520a%2520key%2520area%2520of%2520research%2520for%2520remote%250Ahealthcare%2520monitoring%2520applications.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%2520dataset%2520of%250Arecordings%2520of%2520in-home%2520activity%2520from%2520people%2520living%2520with%2520Dementia.%2520We%2520design%2520a%250Arepresentation%2520learning%2520method%2520based%2520on%2520converting%2520activity%2520to%2520text%2520strings%250Athat%2520can%2520be%2520encoded%2520using%2520a%2520language%2520model%2520fine-tuned%2520to%2520transform%2520data%2520from%250Athe%2520same%2520participants%2520within%2520a%2520%252430%2524-day%2520window%2520to%2520similar%2520embeddings%2520in%2520the%250Avector%2520space.%2520This%2520allows%2520for%2520clustering%2520and%2520vector%2520searching%2520over%2520participants%250Aand%2520days%252C%2520and%2520the%2520identification%2520of%2520activity%2520deviations%2520to%2520aid%2520with%250Apersonalised%2520delivery%2520of%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20of%20Daily%20Movement%20Data%20Using%20Text%20Encoders&entry.906535625=Alexander%20Capstick%20and%20Tianyu%20Cui%20and%20Yu%20Chen%20and%20Payam%20Barnaghi&entry.1292438233=%20%20Time-series%20representation%20learning%20is%20a%20key%20area%20of%20research%20for%20remote%0Ahealthcare%20monitoring%20applications.%20In%20this%20work%2C%20we%20focus%20on%20a%20dataset%20of%0Arecordings%20of%20in-home%20activity%20from%20people%20living%20with%20Dementia.%20We%20design%20a%0Arepresentation%20learning%20method%20based%20on%20converting%20activity%20to%20text%20strings%0Athat%20can%20be%20encoded%20using%20a%20language%20model%20fine-tuned%20to%20transform%20data%20from%0Athe%20same%20participants%20within%20a%20%2430%24-day%20window%20to%20similar%20embeddings%20in%20the%0Avector%20space.%20This%20allows%20for%20clustering%20and%20vector%20searching%20over%20participants%0Aand%20days%2C%20and%20the%20identification%20of%20activity%20deviations%20to%20aid%20with%0Apersonalised%20delivery%20of%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04494v2&entry.124074799=Read"},
{"title": "Personalized Clustering via Targeted Representation Learning", "author": "Xiwen Geng and Suyun Zhao and Yixin Yu and Borui Peng and Pan Du and Hong Chen and Cuiping Li and Mengdie Wang", "abstract": "  Clustering traditionally aims to reveal a natural grouping structure within\nunlabeled data. However, this structure may not always align with users'\npreferences. In this paper, we propose a personalized clustering method that\nexplicitly performs targeted representation learning by interacting with users\nvia modicum task information (e.g., $\\textit{must-link}$ or\n$\\textit{cannot-link}$ pairs) to guide the clustering direction. We query users\nwith the most informative pairs, i.e., those pairs most hard to cluster and\nthose most easy to miscluster, to facilitate the representation learning in\nterms of the clustering preference. Moreover, by exploiting attention\nmechanism, the targeted representation is learned and augmented. By leveraging\nthe targeted representation and constrained contrastive loss as well,\npersonalized clustering is obtained. Theoretically, we verify that the risk of\npersonalized clustering is tightly bounded, guaranteeing that active queries to\nusers do mitigate the clustering risk. Experimentally, extensive results show\nthat our method performs well across different clustering tasks and datasets,\neven when only a limited number of queries are available.\n", "link": "http://arxiv.org/abs/2412.13690v2", "date": "2024-12-20", "relevancy": 1.9274, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Clustering%20via%20Targeted%20Representation%20Learning&body=Title%3A%20Personalized%20Clustering%20via%20Targeted%20Representation%20Learning%0AAuthor%3A%20Xiwen%20Geng%20and%20Suyun%20Zhao%20and%20Yixin%20Yu%20and%20Borui%20Peng%20and%20Pan%20Du%20and%20Hong%20Chen%20and%20Cuiping%20Li%20and%20Mengdie%20Wang%0AAbstract%3A%20%20%20Clustering%20traditionally%20aims%20to%20reveal%20a%20natural%20grouping%20structure%20within%0Aunlabeled%20data.%20However%2C%20this%20structure%20may%20not%20always%20align%20with%20users%27%0Apreferences.%20In%20this%20paper%2C%20we%20propose%20a%20personalized%20clustering%20method%20that%0Aexplicitly%20performs%20targeted%20representation%20learning%20by%20interacting%20with%20users%0Avia%20modicum%20task%20information%20%28e.g.%2C%20%24%5Ctextit%7Bmust-link%7D%24%20or%0A%24%5Ctextit%7Bcannot-link%7D%24%20pairs%29%20to%20guide%20the%20clustering%20direction.%20We%20query%20users%0Awith%20the%20most%20informative%20pairs%2C%20i.e.%2C%20those%20pairs%20most%20hard%20to%20cluster%20and%0Athose%20most%20easy%20to%20miscluster%2C%20to%20facilitate%20the%20representation%20learning%20in%0Aterms%20of%20the%20clustering%20preference.%20Moreover%2C%20by%20exploiting%20attention%0Amechanism%2C%20the%20targeted%20representation%20is%20learned%20and%20augmented.%20By%20leveraging%0Athe%20targeted%20representation%20and%20constrained%20contrastive%20loss%20as%20well%2C%0Apersonalized%20clustering%20is%20obtained.%20Theoretically%2C%20we%20verify%20that%20the%20risk%20of%0Apersonalized%20clustering%20is%20tightly%20bounded%2C%20guaranteeing%20that%20active%20queries%20to%0Ausers%20do%20mitigate%20the%20clustering%20risk.%20Experimentally%2C%20extensive%20results%20show%0Athat%20our%20method%20performs%20well%20across%20different%20clustering%20tasks%20and%20datasets%2C%0Aeven%20when%20only%20a%20limited%20number%20of%20queries%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Clustering%2520via%2520Targeted%2520Representation%2520Learning%26entry.906535625%3DXiwen%2520Geng%2520and%2520Suyun%2520Zhao%2520and%2520Yixin%2520Yu%2520and%2520Borui%2520Peng%2520and%2520Pan%2520Du%2520and%2520Hong%2520Chen%2520and%2520Cuiping%2520Li%2520and%2520Mengdie%2520Wang%26entry.1292438233%3D%2520%2520Clustering%2520traditionally%2520aims%2520to%2520reveal%2520a%2520natural%2520grouping%2520structure%2520within%250Aunlabeled%2520data.%2520However%252C%2520this%2520structure%2520may%2520not%2520always%2520align%2520with%2520users%2527%250Apreferences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520personalized%2520clustering%2520method%2520that%250Aexplicitly%2520performs%2520targeted%2520representation%2520learning%2520by%2520interacting%2520with%2520users%250Avia%2520modicum%2520task%2520information%2520%2528e.g.%252C%2520%2524%255Ctextit%257Bmust-link%257D%2524%2520or%250A%2524%255Ctextit%257Bcannot-link%257D%2524%2520pairs%2529%2520to%2520guide%2520the%2520clustering%2520direction.%2520We%2520query%2520users%250Awith%2520the%2520most%2520informative%2520pairs%252C%2520i.e.%252C%2520those%2520pairs%2520most%2520hard%2520to%2520cluster%2520and%250Athose%2520most%2520easy%2520to%2520miscluster%252C%2520to%2520facilitate%2520the%2520representation%2520learning%2520in%250Aterms%2520of%2520the%2520clustering%2520preference.%2520Moreover%252C%2520by%2520exploiting%2520attention%250Amechanism%252C%2520the%2520targeted%2520representation%2520is%2520learned%2520and%2520augmented.%2520By%2520leveraging%250Athe%2520targeted%2520representation%2520and%2520constrained%2520contrastive%2520loss%2520as%2520well%252C%250Apersonalized%2520clustering%2520is%2520obtained.%2520Theoretically%252C%2520we%2520verify%2520that%2520the%2520risk%2520of%250Apersonalized%2520clustering%2520is%2520tightly%2520bounded%252C%2520guaranteeing%2520that%2520active%2520queries%2520to%250Ausers%2520do%2520mitigate%2520the%2520clustering%2520risk.%2520Experimentally%252C%2520extensive%2520results%2520show%250Athat%2520our%2520method%2520performs%2520well%2520across%2520different%2520clustering%2520tasks%2520and%2520datasets%252C%250Aeven%2520when%2520only%2520a%2520limited%2520number%2520of%2520queries%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Clustering%20via%20Targeted%20Representation%20Learning&entry.906535625=Xiwen%20Geng%20and%20Suyun%20Zhao%20and%20Yixin%20Yu%20and%20Borui%20Peng%20and%20Pan%20Du%20and%20Hong%20Chen%20and%20Cuiping%20Li%20and%20Mengdie%20Wang&entry.1292438233=%20%20Clustering%20traditionally%20aims%20to%20reveal%20a%20natural%20grouping%20structure%20within%0Aunlabeled%20data.%20However%2C%20this%20structure%20may%20not%20always%20align%20with%20users%27%0Apreferences.%20In%20this%20paper%2C%20we%20propose%20a%20personalized%20clustering%20method%20that%0Aexplicitly%20performs%20targeted%20representation%20learning%20by%20interacting%20with%20users%0Avia%20modicum%20task%20information%20%28e.g.%2C%20%24%5Ctextit%7Bmust-link%7D%24%20or%0A%24%5Ctextit%7Bcannot-link%7D%24%20pairs%29%20to%20guide%20the%20clustering%20direction.%20We%20query%20users%0Awith%20the%20most%20informative%20pairs%2C%20i.e.%2C%20those%20pairs%20most%20hard%20to%20cluster%20and%0Athose%20most%20easy%20to%20miscluster%2C%20to%20facilitate%20the%20representation%20learning%20in%0Aterms%20of%20the%20clustering%20preference.%20Moreover%2C%20by%20exploiting%20attention%0Amechanism%2C%20the%20targeted%20representation%20is%20learned%20and%20augmented.%20By%20leveraging%0Athe%20targeted%20representation%20and%20constrained%20contrastive%20loss%20as%20well%2C%0Apersonalized%20clustering%20is%20obtained.%20Theoretically%2C%20we%20verify%20that%20the%20risk%20of%0Apersonalized%20clustering%20is%20tightly%20bounded%2C%20guaranteeing%20that%20active%20queries%20to%0Ausers%20do%20mitigate%20the%20clustering%20risk.%20Experimentally%2C%20extensive%20results%20show%0Athat%20our%20method%20performs%20well%20across%20different%20clustering%20tasks%20and%20datasets%2C%0Aeven%20when%20only%20a%20limited%20number%20of%20queries%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13690v2&entry.124074799=Read"},
{"title": "Efficient Curation of Invertebrate Image Datasets Using Feature\n  Embeddings and Automatic Size Comparison", "author": "Mikko Impi\u00f6 and Philipp M. Rehsen and Jenni Raitoharju", "abstract": "  The amount of image datasets collected for environmental monitoring purposes\nhas increased in the past years as computer vision assisted methods have gained\ninterest. Computer vision applications rely on high-quality datasets, making\ndata curation important. However, data curation is often done ad-hoc and the\nmethods used are rarely published. We present a method for curating large-scale\nimage datasets of invertebrates that contain multiple images of the same taxa\nand/or specimens and have relatively uniform background in the images. Our\napproach is based on extracting feature embeddings with pretrained deep neural\nnetworks, and using these embeddings to find visually most distinct images by\ncomparing their embeddings to the group prototype embedding. Also, we show that\na simple area-based size comparison approach is able to find a lot of common\nerroneous images, such as images containing detached body parts and\nmisclassified samples. In addition to the method, we propose using novel\nmetrics for evaluating human-in-the-loop outlier detection methods. The\nimplementations of the proposed curation methods, as well as a benchmark\ndataset containing annotated erroneous images, are publicly available in\nhttps://github.com/mikkoim/taxonomist-studio.\n", "link": "http://arxiv.org/abs/2412.15844v1", "date": "2024-12-20", "relevancy": 1.923, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4798}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Curation%20of%20Invertebrate%20Image%20Datasets%20Using%20Feature%0A%20%20Embeddings%20and%20Automatic%20Size%20Comparison&body=Title%3A%20Efficient%20Curation%20of%20Invertebrate%20Image%20Datasets%20Using%20Feature%0A%20%20Embeddings%20and%20Automatic%20Size%20Comparison%0AAuthor%3A%20Mikko%20Impi%C3%B6%20and%20Philipp%20M.%20Rehsen%20and%20Jenni%20Raitoharju%0AAbstract%3A%20%20%20The%20amount%20of%20image%20datasets%20collected%20for%20environmental%20monitoring%20purposes%0Ahas%20increased%20in%20the%20past%20years%20as%20computer%20vision%20assisted%20methods%20have%20gained%0Ainterest.%20Computer%20vision%20applications%20rely%20on%20high-quality%20datasets%2C%20making%0Adata%20curation%20important.%20However%2C%20data%20curation%20is%20often%20done%20ad-hoc%20and%20the%0Amethods%20used%20are%20rarely%20published.%20We%20present%20a%20method%20for%20curating%20large-scale%0Aimage%20datasets%20of%20invertebrates%20that%20contain%20multiple%20images%20of%20the%20same%20taxa%0Aand/or%20specimens%20and%20have%20relatively%20uniform%20background%20in%20the%20images.%20Our%0Aapproach%20is%20based%20on%20extracting%20feature%20embeddings%20with%20pretrained%20deep%20neural%0Anetworks%2C%20and%20using%20these%20embeddings%20to%20find%20visually%20most%20distinct%20images%20by%0Acomparing%20their%20embeddings%20to%20the%20group%20prototype%20embedding.%20Also%2C%20we%20show%20that%0Aa%20simple%20area-based%20size%20comparison%20approach%20is%20able%20to%20find%20a%20lot%20of%20common%0Aerroneous%20images%2C%20such%20as%20images%20containing%20detached%20body%20parts%20and%0Amisclassified%20samples.%20In%20addition%20to%20the%20method%2C%20we%20propose%20using%20novel%0Ametrics%20for%20evaluating%20human-in-the-loop%20outlier%20detection%20methods.%20The%0Aimplementations%20of%20the%20proposed%20curation%20methods%2C%20as%20well%20as%20a%20benchmark%0Adataset%20containing%20annotated%20erroneous%20images%2C%20are%20publicly%20available%20in%0Ahttps%3A//github.com/mikkoim/taxonomist-studio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Curation%2520of%2520Invertebrate%2520Image%2520Datasets%2520Using%2520Feature%250A%2520%2520Embeddings%2520and%2520Automatic%2520Size%2520Comparison%26entry.906535625%3DMikko%2520Impi%25C3%25B6%2520and%2520Philipp%2520M.%2520Rehsen%2520and%2520Jenni%2520Raitoharju%26entry.1292438233%3D%2520%2520The%2520amount%2520of%2520image%2520datasets%2520collected%2520for%2520environmental%2520monitoring%2520purposes%250Ahas%2520increased%2520in%2520the%2520past%2520years%2520as%2520computer%2520vision%2520assisted%2520methods%2520have%2520gained%250Ainterest.%2520Computer%2520vision%2520applications%2520rely%2520on%2520high-quality%2520datasets%252C%2520making%250Adata%2520curation%2520important.%2520However%252C%2520data%2520curation%2520is%2520often%2520done%2520ad-hoc%2520and%2520the%250Amethods%2520used%2520are%2520rarely%2520published.%2520We%2520present%2520a%2520method%2520for%2520curating%2520large-scale%250Aimage%2520datasets%2520of%2520invertebrates%2520that%2520contain%2520multiple%2520images%2520of%2520the%2520same%2520taxa%250Aand/or%2520specimens%2520and%2520have%2520relatively%2520uniform%2520background%2520in%2520the%2520images.%2520Our%250Aapproach%2520is%2520based%2520on%2520extracting%2520feature%2520embeddings%2520with%2520pretrained%2520deep%2520neural%250Anetworks%252C%2520and%2520using%2520these%2520embeddings%2520to%2520find%2520visually%2520most%2520distinct%2520images%2520by%250Acomparing%2520their%2520embeddings%2520to%2520the%2520group%2520prototype%2520embedding.%2520Also%252C%2520we%2520show%2520that%250Aa%2520simple%2520area-based%2520size%2520comparison%2520approach%2520is%2520able%2520to%2520find%2520a%2520lot%2520of%2520common%250Aerroneous%2520images%252C%2520such%2520as%2520images%2520containing%2520detached%2520body%2520parts%2520and%250Amisclassified%2520samples.%2520In%2520addition%2520to%2520the%2520method%252C%2520we%2520propose%2520using%2520novel%250Ametrics%2520for%2520evaluating%2520human-in-the-loop%2520outlier%2520detection%2520methods.%2520The%250Aimplementations%2520of%2520the%2520proposed%2520curation%2520methods%252C%2520as%2520well%2520as%2520a%2520benchmark%250Adataset%2520containing%2520annotated%2520erroneous%2520images%252C%2520are%2520publicly%2520available%2520in%250Ahttps%253A//github.com/mikkoim/taxonomist-studio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Curation%20of%20Invertebrate%20Image%20Datasets%20Using%20Feature%0A%20%20Embeddings%20and%20Automatic%20Size%20Comparison&entry.906535625=Mikko%20Impi%C3%B6%20and%20Philipp%20M.%20Rehsen%20and%20Jenni%20Raitoharju&entry.1292438233=%20%20The%20amount%20of%20image%20datasets%20collected%20for%20environmental%20monitoring%20purposes%0Ahas%20increased%20in%20the%20past%20years%20as%20computer%20vision%20assisted%20methods%20have%20gained%0Ainterest.%20Computer%20vision%20applications%20rely%20on%20high-quality%20datasets%2C%20making%0Adata%20curation%20important.%20However%2C%20data%20curation%20is%20often%20done%20ad-hoc%20and%20the%0Amethods%20used%20are%20rarely%20published.%20We%20present%20a%20method%20for%20curating%20large-scale%0Aimage%20datasets%20of%20invertebrates%20that%20contain%20multiple%20images%20of%20the%20same%20taxa%0Aand/or%20specimens%20and%20have%20relatively%20uniform%20background%20in%20the%20images.%20Our%0Aapproach%20is%20based%20on%20extracting%20feature%20embeddings%20with%20pretrained%20deep%20neural%0Anetworks%2C%20and%20using%20these%20embeddings%20to%20find%20visually%20most%20distinct%20images%20by%0Acomparing%20their%20embeddings%20to%20the%20group%20prototype%20embedding.%20Also%2C%20we%20show%20that%0Aa%20simple%20area-based%20size%20comparison%20approach%20is%20able%20to%20find%20a%20lot%20of%20common%0Aerroneous%20images%2C%20such%20as%20images%20containing%20detached%20body%20parts%20and%0Amisclassified%20samples.%20In%20addition%20to%20the%20method%2C%20we%20propose%20using%20novel%0Ametrics%20for%20evaluating%20human-in-the-loop%20outlier%20detection%20methods.%20The%0Aimplementations%20of%20the%20proposed%20curation%20methods%2C%20as%20well%20as%20a%20benchmark%0Adataset%20containing%20annotated%20erroneous%20images%2C%20are%20publicly%20available%20in%0Ahttps%3A//github.com/mikkoim/taxonomist-studio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15844v1&entry.124074799=Read"},
{"title": "Are You Human? An Adversarial Benchmark to Expose LLMs", "author": "Gilad Gressel and Rahul Pankajakshan and Yisroel Mirsky", "abstract": "  Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations.\n", "link": "http://arxiv.org/abs/2410.09569v2", "date": "2024-12-20", "relevancy": 1.9221, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20You%20Human%3F%20An%20Adversarial%20Benchmark%20to%20Expose%20LLMs&body=Title%3A%20Are%20You%20Human%3F%20An%20Adversarial%20Benchmark%20to%20Expose%20LLMs%0AAuthor%3A%20Gilad%20Gressel%20and%20Rahul%20Pankajakshan%20and%20Yisroel%20Mirsky%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20an%20alarming%20ability%20to%0Aimpersonate%20humans%20in%20conversation%2C%20raising%20concerns%20about%20their%20potential%0Amisuse%20in%20scams%20and%20deception.%20Humans%20have%20a%20right%20to%20know%20if%20they%20are%0Aconversing%20to%20an%20LLM.%20We%20evaluate%20text-based%20prompts%20designed%20as%20challenges%20to%0Aexpose%20LLM%20imposters%20in%20real-time.%20To%20this%20end%20we%20compile%20and%20release%20an%0Aopen-source%20benchmark%20dataset%20that%20includes%20%27implicit%20challenges%27%20that%20exploit%0Aan%20LLM%27s%20instruction-following%20mechanism%20to%20cause%20role%20deviation%2C%20and%20%27exlicit%0Achallenges%27%20that%20test%20an%20LLM%27s%20ability%20to%20perform%20simple%20tasks%20typically%20easy%0Afor%20humans%20but%20difficult%20for%20LLMs.%20Our%20evaluation%20of%209%20leading%20models%20from%20the%0ALMSYS%20leaderboard%20revealed%20that%20explicit%20challenges%20successfully%20detected%20LLMs%0Ain%2078.4%25%20of%20cases%2C%20while%20implicit%20challenges%20were%20effective%20in%2022.9%25%20of%0Ainstances.%20User%20studies%20validate%20the%20real-world%20applicability%20of%20our%20methods%2C%0Awith%20humans%20outperforming%20LLMs%20on%20explicit%20challenges%20%2878%25%20vs%2022%25%20success%0Arate%29.%20Our%20framework%20unexpectedly%20revealed%20that%20many%20study%20participants%20were%0Ausing%20LLMs%20to%20complete%20tasks%2C%20demonstrating%20its%20effectiveness%20in%20detecting%20both%0AAI%20impostors%20and%20human%20misuse%20of%20AI%20tools.%20This%20work%20addresses%20the%20critical%0Aneed%20for%20reliable%2C%20real-time%20LLM%20detection%20methods%20in%20high-stakes%0Aconversations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520You%2520Human%253F%2520An%2520Adversarial%2520Benchmark%2520to%2520Expose%2520LLMs%26entry.906535625%3DGilad%2520Gressel%2520and%2520Rahul%2520Pankajakshan%2520and%2520Yisroel%2520Mirsky%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520an%2520alarming%2520ability%2520to%250Aimpersonate%2520humans%2520in%2520conversation%252C%2520raising%2520concerns%2520about%2520their%2520potential%250Amisuse%2520in%2520scams%2520and%2520deception.%2520Humans%2520have%2520a%2520right%2520to%2520know%2520if%2520they%2520are%250Aconversing%2520to%2520an%2520LLM.%2520We%2520evaluate%2520text-based%2520prompts%2520designed%2520as%2520challenges%2520to%250Aexpose%2520LLM%2520imposters%2520in%2520real-time.%2520To%2520this%2520end%2520we%2520compile%2520and%2520release%2520an%250Aopen-source%2520benchmark%2520dataset%2520that%2520includes%2520%2527implicit%2520challenges%2527%2520that%2520exploit%250Aan%2520LLM%2527s%2520instruction-following%2520mechanism%2520to%2520cause%2520role%2520deviation%252C%2520and%2520%2527exlicit%250Achallenges%2527%2520that%2520test%2520an%2520LLM%2527s%2520ability%2520to%2520perform%2520simple%2520tasks%2520typically%2520easy%250Afor%2520humans%2520but%2520difficult%2520for%2520LLMs.%2520Our%2520evaluation%2520of%25209%2520leading%2520models%2520from%2520the%250ALMSYS%2520leaderboard%2520revealed%2520that%2520explicit%2520challenges%2520successfully%2520detected%2520LLMs%250Ain%252078.4%2525%2520of%2520cases%252C%2520while%2520implicit%2520challenges%2520were%2520effective%2520in%252022.9%2525%2520of%250Ainstances.%2520User%2520studies%2520validate%2520the%2520real-world%2520applicability%2520of%2520our%2520methods%252C%250Awith%2520humans%2520outperforming%2520LLMs%2520on%2520explicit%2520challenges%2520%252878%2525%2520vs%252022%2525%2520success%250Arate%2529.%2520Our%2520framework%2520unexpectedly%2520revealed%2520that%2520many%2520study%2520participants%2520were%250Ausing%2520LLMs%2520to%2520complete%2520tasks%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520detecting%2520both%250AAI%2520impostors%2520and%2520human%2520misuse%2520of%2520AI%2520tools.%2520This%2520work%2520addresses%2520the%2520critical%250Aneed%2520for%2520reliable%252C%2520real-time%2520LLM%2520detection%2520methods%2520in%2520high-stakes%250Aconversations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20You%20Human%3F%20An%20Adversarial%20Benchmark%20to%20Expose%20LLMs&entry.906535625=Gilad%20Gressel%20and%20Rahul%20Pankajakshan%20and%20Yisroel%20Mirsky&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20an%20alarming%20ability%20to%0Aimpersonate%20humans%20in%20conversation%2C%20raising%20concerns%20about%20their%20potential%0Amisuse%20in%20scams%20and%20deception.%20Humans%20have%20a%20right%20to%20know%20if%20they%20are%0Aconversing%20to%20an%20LLM.%20We%20evaluate%20text-based%20prompts%20designed%20as%20challenges%20to%0Aexpose%20LLM%20imposters%20in%20real-time.%20To%20this%20end%20we%20compile%20and%20release%20an%0Aopen-source%20benchmark%20dataset%20that%20includes%20%27implicit%20challenges%27%20that%20exploit%0Aan%20LLM%27s%20instruction-following%20mechanism%20to%20cause%20role%20deviation%2C%20and%20%27exlicit%0Achallenges%27%20that%20test%20an%20LLM%27s%20ability%20to%20perform%20simple%20tasks%20typically%20easy%0Afor%20humans%20but%20difficult%20for%20LLMs.%20Our%20evaluation%20of%209%20leading%20models%20from%20the%0ALMSYS%20leaderboard%20revealed%20that%20explicit%20challenges%20successfully%20detected%20LLMs%0Ain%2078.4%25%20of%20cases%2C%20while%20implicit%20challenges%20were%20effective%20in%2022.9%25%20of%0Ainstances.%20User%20studies%20validate%20the%20real-world%20applicability%20of%20our%20methods%2C%0Awith%20humans%20outperforming%20LLMs%20on%20explicit%20challenges%20%2878%25%20vs%2022%25%20success%0Arate%29.%20Our%20framework%20unexpectedly%20revealed%20that%20many%20study%20participants%20were%0Ausing%20LLMs%20to%20complete%20tasks%2C%20demonstrating%20its%20effectiveness%20in%20detecting%20both%0AAI%20impostors%20and%20human%20misuse%20of%20AI%20tools.%20This%20work%20addresses%20the%20critical%0Aneed%20for%20reliable%2C%20real-time%20LLM%20detection%20methods%20in%20high-stakes%0Aconversations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09569v2&entry.124074799=Read"},
{"title": "CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation", "author": "Muthukumar G and Jyosna Philip", "abstract": "  Remaining Useful Life (RUL) of a component or a system is defined as the\nlength from the current time to the end of the useful life. Accurate RUL\nestimation plays a crucial role in Predictive Maintenance applications.\nTraditional regression methods, both linear and non-linear, have struggled to\nachieve high accuracy in this domain. While Convolutional Neural Networks\n(CNNs) have shown improved accuracy, they often overlook the sequential nature\nof the data, relying instead on features derived from sliding windows. Since\nRUL prediction inherently involves multivariate time series analysis, robust\nsequence learning is essential. In this work, we propose a hybrid approach\ncombining Convolutional Neural Networks with Long Short-Term Memory (LSTM)\nnetworks for RUL estimation. Although CNN-based LSTM models have been applied\nto sequence prediction tasks in financial forecasting, this is the first\nattempt to adopt this approach for RUL estimation in prognostics. In this\napproach, CNN is first employed to efficiently extract features from the data,\nfollowed by LSTM, which uses these extracted features to predict RUL. This\nmethod effectively leverages sensor sequence information, uncovering hidden\npatterns within the data, even under multiple operating conditions and fault\nscenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the\nhighest accuracy, offering a superior score compared to the other methods.\n", "link": "http://arxiv.org/abs/2412.15998v1", "date": "2024-12-20", "relevancy": 1.4457, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4873}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN-LSTM%20Hybrid%20Deep%20Learning%20Model%20for%20Remaining%20Useful%20Life%20Estimation&body=Title%3A%20CNN-LSTM%20Hybrid%20Deep%20Learning%20Model%20for%20Remaining%20Useful%20Life%20Estimation%0AAuthor%3A%20Muthukumar%20G%20and%20Jyosna%20Philip%0AAbstract%3A%20%20%20Remaining%20Useful%20Life%20%28RUL%29%20of%20a%20component%20or%20a%20system%20is%20defined%20as%20the%0Alength%20from%20the%20current%20time%20to%20the%20end%20of%20the%20useful%20life.%20Accurate%20RUL%0Aestimation%20plays%20a%20crucial%20role%20in%20Predictive%20Maintenance%20applications.%0ATraditional%20regression%20methods%2C%20both%20linear%20and%20non-linear%2C%20have%20struggled%20to%0Aachieve%20high%20accuracy%20in%20this%20domain.%20While%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20have%20shown%20improved%20accuracy%2C%20they%20often%20overlook%20the%20sequential%20nature%0Aof%20the%20data%2C%20relying%20instead%20on%20features%20derived%20from%20sliding%20windows.%20Since%0ARUL%20prediction%20inherently%20involves%20multivariate%20time%20series%20analysis%2C%20robust%0Asequence%20learning%20is%20essential.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20approach%0Acombining%20Convolutional%20Neural%20Networks%20with%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetworks%20for%20RUL%20estimation.%20Although%20CNN-based%20LSTM%20models%20have%20been%20applied%0Ato%20sequence%20prediction%20tasks%20in%20financial%20forecasting%2C%20this%20is%20the%20first%0Aattempt%20to%20adopt%20this%20approach%20for%20RUL%20estimation%20in%20prognostics.%20In%20this%0Aapproach%2C%20CNN%20is%20first%20employed%20to%20efficiently%20extract%20features%20from%20the%20data%2C%0Afollowed%20by%20LSTM%2C%20which%20uses%20these%20extracted%20features%20to%20predict%20RUL.%20This%0Amethod%20effectively%20leverages%20sensor%20sequence%20information%2C%20uncovering%20hidden%0Apatterns%20within%20the%20data%2C%20even%20under%20multiple%20operating%20conditions%20and%20fault%0Ascenarios.%20Our%20results%20demonstrate%20that%20the%20hybrid%20CNN-LSTM%20model%20achieves%20the%0Ahighest%20accuracy%2C%20offering%20a%20superior%20score%20compared%20to%20the%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN-LSTM%2520Hybrid%2520Deep%2520Learning%2520Model%2520for%2520Remaining%2520Useful%2520Life%2520Estimation%26entry.906535625%3DMuthukumar%2520G%2520and%2520Jyosna%2520Philip%26entry.1292438233%3D%2520%2520Remaining%2520Useful%2520Life%2520%2528RUL%2529%2520of%2520a%2520component%2520or%2520a%2520system%2520is%2520defined%2520as%2520the%250Alength%2520from%2520the%2520current%2520time%2520to%2520the%2520end%2520of%2520the%2520useful%2520life.%2520Accurate%2520RUL%250Aestimation%2520plays%2520a%2520crucial%2520role%2520in%2520Predictive%2520Maintenance%2520applications.%250ATraditional%2520regression%2520methods%252C%2520both%2520linear%2520and%2520non-linear%252C%2520have%2520struggled%2520to%250Aachieve%2520high%2520accuracy%2520in%2520this%2520domain.%2520While%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529%2520have%2520shown%2520improved%2520accuracy%252C%2520they%2520often%2520overlook%2520the%2520sequential%2520nature%250Aof%2520the%2520data%252C%2520relying%2520instead%2520on%2520features%2520derived%2520from%2520sliding%2520windows.%2520Since%250ARUL%2520prediction%2520inherently%2520involves%2520multivariate%2520time%2520series%2520analysis%252C%2520robust%250Asequence%2520learning%2520is%2520essential.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520hybrid%2520approach%250Acombining%2520Convolutional%2520Neural%2520Networks%2520with%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%250Anetworks%2520for%2520RUL%2520estimation.%2520Although%2520CNN-based%2520LSTM%2520models%2520have%2520been%2520applied%250Ato%2520sequence%2520prediction%2520tasks%2520in%2520financial%2520forecasting%252C%2520this%2520is%2520the%2520first%250Aattempt%2520to%2520adopt%2520this%2520approach%2520for%2520RUL%2520estimation%2520in%2520prognostics.%2520In%2520this%250Aapproach%252C%2520CNN%2520is%2520first%2520employed%2520to%2520efficiently%2520extract%2520features%2520from%2520the%2520data%252C%250Afollowed%2520by%2520LSTM%252C%2520which%2520uses%2520these%2520extracted%2520features%2520to%2520predict%2520RUL.%2520This%250Amethod%2520effectively%2520leverages%2520sensor%2520sequence%2520information%252C%2520uncovering%2520hidden%250Apatterns%2520within%2520the%2520data%252C%2520even%2520under%2520multiple%2520operating%2520conditions%2520and%2520fault%250Ascenarios.%2520Our%2520results%2520demonstrate%2520that%2520the%2520hybrid%2520CNN-LSTM%2520model%2520achieves%2520the%250Ahighest%2520accuracy%252C%2520offering%2520a%2520superior%2520score%2520compared%2520to%2520the%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN-LSTM%20Hybrid%20Deep%20Learning%20Model%20for%20Remaining%20Useful%20Life%20Estimation&entry.906535625=Muthukumar%20G%20and%20Jyosna%20Philip&entry.1292438233=%20%20Remaining%20Useful%20Life%20%28RUL%29%20of%20a%20component%20or%20a%20system%20is%20defined%20as%20the%0Alength%20from%20the%20current%20time%20to%20the%20end%20of%20the%20useful%20life.%20Accurate%20RUL%0Aestimation%20plays%20a%20crucial%20role%20in%20Predictive%20Maintenance%20applications.%0ATraditional%20regression%20methods%2C%20both%20linear%20and%20non-linear%2C%20have%20struggled%20to%0Aachieve%20high%20accuracy%20in%20this%20domain.%20While%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20have%20shown%20improved%20accuracy%2C%20they%20often%20overlook%20the%20sequential%20nature%0Aof%20the%20data%2C%20relying%20instead%20on%20features%20derived%20from%20sliding%20windows.%20Since%0ARUL%20prediction%20inherently%20involves%20multivariate%20time%20series%20analysis%2C%20robust%0Asequence%20learning%20is%20essential.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20approach%0Acombining%20Convolutional%20Neural%20Networks%20with%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetworks%20for%20RUL%20estimation.%20Although%20CNN-based%20LSTM%20models%20have%20been%20applied%0Ato%20sequence%20prediction%20tasks%20in%20financial%20forecasting%2C%20this%20is%20the%20first%0Aattempt%20to%20adopt%20this%20approach%20for%20RUL%20estimation%20in%20prognostics.%20In%20this%0Aapproach%2C%20CNN%20is%20first%20employed%20to%20efficiently%20extract%20features%20from%20the%20data%2C%0Afollowed%20by%20LSTM%2C%20which%20uses%20these%20extracted%20features%20to%20predict%20RUL.%20This%0Amethod%20effectively%20leverages%20sensor%20sequence%20information%2C%20uncovering%20hidden%0Apatterns%20within%20the%20data%2C%20even%20under%20multiple%20operating%20conditions%20and%20fault%0Ascenarios.%20Our%20results%20demonstrate%20that%20the%20hybrid%20CNN-LSTM%20model%20achieves%20the%0Ahighest%20accuracy%2C%20offering%20a%20superior%20score%20compared%20to%20the%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15998v1&entry.124074799=Read"},
{"title": "MR-GDINO: Efficient Open-World Continual Object Detection", "author": "Bowen Dong and Zitong Huang and Guanglei Yang and Lei Zhang and Wangmeng Zuo", "abstract": "  Open-world (OW) recognition and detection models show strong zero- and\nfew-shot adaptation abilities, inspiring their use as initializations in\ncontinual learning methods to improve performance. Despite promising results on\nseen classes, such OW abilities on unseen classes are largely degenerated due\nto catastrophic forgetting. To tackle this challenge, we propose an open-world\ncontinual object detection task, requiring detectors to generalize to old, new,\nand unseen categories in continual learning scenarios. Based on this task, we\npresent a challenging yet practical OW-COD benchmark to assess detection\nabilities. The goal is to motivate OW detectors to simultaneously preserve\nlearned classes, adapt to new classes, and maintain open-world capabilities\nunder few-shot adaptations. To mitigate forgetting in unseen categories, we\npropose MR-GDINO, a strong, efficient and scalable baseline via memory and\nretrieval mechanisms within a highly scalable memory pool. Experimental results\nshow that existing continual detectors suffer from severe forgetting for both\nseen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting\nwith only 0.1% activated extra parameters, achieving state-of-the-art\nperformance for old, new, and unseen categories.\n", "link": "http://arxiv.org/abs/2412.15979v1", "date": "2024-12-20", "relevancy": 1.625, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5426}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5416}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR-GDINO%3A%20Efficient%20Open-World%20Continual%20Object%20Detection&body=Title%3A%20MR-GDINO%3A%20Efficient%20Open-World%20Continual%20Object%20Detection%0AAuthor%3A%20Bowen%20Dong%20and%20Zitong%20Huang%20and%20Guanglei%20Yang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Open-world%20%28OW%29%20recognition%20and%20detection%20models%20show%20strong%20zero-%20and%0Afew-shot%20adaptation%20abilities%2C%20inspiring%20their%20use%20as%20initializations%20in%0Acontinual%20learning%20methods%20to%20improve%20performance.%20Despite%20promising%20results%20on%0Aseen%20classes%2C%20such%20OW%20abilities%20on%20unseen%20classes%20are%20largely%20degenerated%20due%0Ato%20catastrophic%20forgetting.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20open-world%0Acontinual%20object%20detection%20task%2C%20requiring%20detectors%20to%20generalize%20to%20old%2C%20new%2C%0Aand%20unseen%20categories%20in%20continual%20learning%20scenarios.%20Based%20on%20this%20task%2C%20we%0Apresent%20a%20challenging%20yet%20practical%20OW-COD%20benchmark%20to%20assess%20detection%0Aabilities.%20The%20goal%20is%20to%20motivate%20OW%20detectors%20to%20simultaneously%20preserve%0Alearned%20classes%2C%20adapt%20to%20new%20classes%2C%20and%20maintain%20open-world%20capabilities%0Aunder%20few-shot%20adaptations.%20To%20mitigate%20forgetting%20in%20unseen%20categories%2C%20we%0Apropose%20MR-GDINO%2C%20a%20strong%2C%20efficient%20and%20scalable%20baseline%20via%20memory%20and%0Aretrieval%20mechanisms%20within%20a%20highly%20scalable%20memory%20pool.%20Experimental%20results%0Ashow%20that%20existing%20continual%20detectors%20suffer%20from%20severe%20forgetting%20for%20both%0Aseen%20and%20unseen%20categories.%20In%20contrast%2C%20MR-GDINO%20largely%20mitigates%20forgetting%0Awith%20only%200.1%25%20activated%20extra%20parameters%2C%20achieving%20state-of-the-art%0Aperformance%20for%20old%2C%20new%2C%20and%20unseen%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR-GDINO%253A%2520Efficient%2520Open-World%2520Continual%2520Object%2520Detection%26entry.906535625%3DBowen%2520Dong%2520and%2520Zitong%2520Huang%2520and%2520Guanglei%2520Yang%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Open-world%2520%2528OW%2529%2520recognition%2520and%2520detection%2520models%2520show%2520strong%2520zero-%2520and%250Afew-shot%2520adaptation%2520abilities%252C%2520inspiring%2520their%2520use%2520as%2520initializations%2520in%250Acontinual%2520learning%2520methods%2520to%2520improve%2520performance.%2520Despite%2520promising%2520results%2520on%250Aseen%2520classes%252C%2520such%2520OW%2520abilities%2520on%2520unseen%2520classes%2520are%2520largely%2520degenerated%2520due%250Ato%2520catastrophic%2520forgetting.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520an%2520open-world%250Acontinual%2520object%2520detection%2520task%252C%2520requiring%2520detectors%2520to%2520generalize%2520to%2520old%252C%2520new%252C%250Aand%2520unseen%2520categories%2520in%2520continual%2520learning%2520scenarios.%2520Based%2520on%2520this%2520task%252C%2520we%250Apresent%2520a%2520challenging%2520yet%2520practical%2520OW-COD%2520benchmark%2520to%2520assess%2520detection%250Aabilities.%2520The%2520goal%2520is%2520to%2520motivate%2520OW%2520detectors%2520to%2520simultaneously%2520preserve%250Alearned%2520classes%252C%2520adapt%2520to%2520new%2520classes%252C%2520and%2520maintain%2520open-world%2520capabilities%250Aunder%2520few-shot%2520adaptations.%2520To%2520mitigate%2520forgetting%2520in%2520unseen%2520categories%252C%2520we%250Apropose%2520MR-GDINO%252C%2520a%2520strong%252C%2520efficient%2520and%2520scalable%2520baseline%2520via%2520memory%2520and%250Aretrieval%2520mechanisms%2520within%2520a%2520highly%2520scalable%2520memory%2520pool.%2520Experimental%2520results%250Ashow%2520that%2520existing%2520continual%2520detectors%2520suffer%2520from%2520severe%2520forgetting%2520for%2520both%250Aseen%2520and%2520unseen%2520categories.%2520In%2520contrast%252C%2520MR-GDINO%2520largely%2520mitigates%2520forgetting%250Awith%2520only%25200.1%2525%2520activated%2520extra%2520parameters%252C%2520achieving%2520state-of-the-art%250Aperformance%2520for%2520old%252C%2520new%252C%2520and%2520unseen%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR-GDINO%3A%20Efficient%20Open-World%20Continual%20Object%20Detection&entry.906535625=Bowen%20Dong%20and%20Zitong%20Huang%20and%20Guanglei%20Yang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Open-world%20%28OW%29%20recognition%20and%20detection%20models%20show%20strong%20zero-%20and%0Afew-shot%20adaptation%20abilities%2C%20inspiring%20their%20use%20as%20initializations%20in%0Acontinual%20learning%20methods%20to%20improve%20performance.%20Despite%20promising%20results%20on%0Aseen%20classes%2C%20such%20OW%20abilities%20on%20unseen%20classes%20are%20largely%20degenerated%20due%0Ato%20catastrophic%20forgetting.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20open-world%0Acontinual%20object%20detection%20task%2C%20requiring%20detectors%20to%20generalize%20to%20old%2C%20new%2C%0Aand%20unseen%20categories%20in%20continual%20learning%20scenarios.%20Based%20on%20this%20task%2C%20we%0Apresent%20a%20challenging%20yet%20practical%20OW-COD%20benchmark%20to%20assess%20detection%0Aabilities.%20The%20goal%20is%20to%20motivate%20OW%20detectors%20to%20simultaneously%20preserve%0Alearned%20classes%2C%20adapt%20to%20new%20classes%2C%20and%20maintain%20open-world%20capabilities%0Aunder%20few-shot%20adaptations.%20To%20mitigate%20forgetting%20in%20unseen%20categories%2C%20we%0Apropose%20MR-GDINO%2C%20a%20strong%2C%20efficient%20and%20scalable%20baseline%20via%20memory%20and%0Aretrieval%20mechanisms%20within%20a%20highly%20scalable%20memory%20pool.%20Experimental%20results%0Ashow%20that%20existing%20continual%20detectors%20suffer%20from%20severe%20forgetting%20for%20both%0Aseen%20and%20unseen%20categories.%20In%20contrast%2C%20MR-GDINO%20largely%20mitigates%20forgetting%0Awith%20only%200.1%25%20activated%20extra%20parameters%2C%20achieving%20state-of-the-art%0Aperformance%20for%20old%2C%20new%2C%20and%20unseen%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15979v1&entry.124074799=Read"},
{"title": "APIRL: Deep Reinforcement Learning for REST API Fuzzing", "author": "Myles Foley and Sergio Maffeis", "abstract": "  REST APIs have become key components of web services. However, they often\ncontain logic flaws resulting in server side errors or security\nvulnerabilities. HTTP requests are used as test cases to find and mitigate such\nissues. Existing methods to modify requests, including those using deep\nlearning, suffer from limited performance and precision, relying on undirected\nsearch or making limited usage of the contextual information. In this paper we\npropose APIRL, a fully automated deep reinforcement learning tool for testing\nREST APIs. A key novelty of our approach is the use of feedback from a\ntransformer module pre-trained on JSON-structured data, akin to that used in\nAPI responses. This allows APIRL to learn the subtleties relating to test\noutcomes, and generalise to unseen API endpoints. We show APIRL can find\nsignificantly more bugs than the state-of-the-art in real world REST APIs while\nminimising the number of required test cases. We also study how reward\nfunctions, and other key design choices, affect learnt policies in a thorough\nablation study.\n", "link": "http://arxiv.org/abs/2412.15991v1", "date": "2024-12-20", "relevancy": 0.8277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4379}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4034}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APIRL%3A%20Deep%20Reinforcement%20Learning%20for%20REST%20API%20Fuzzing&body=Title%3A%20APIRL%3A%20Deep%20Reinforcement%20Learning%20for%20REST%20API%20Fuzzing%0AAuthor%3A%20Myles%20Foley%20and%20Sergio%20Maffeis%0AAbstract%3A%20%20%20REST%20APIs%20have%20become%20key%20components%20of%20web%20services.%20However%2C%20they%20often%0Acontain%20logic%20flaws%20resulting%20in%20server%20side%20errors%20or%20security%0Avulnerabilities.%20HTTP%20requests%20are%20used%20as%20test%20cases%20to%20find%20and%20mitigate%20such%0Aissues.%20Existing%20methods%20to%20modify%20requests%2C%20including%20those%20using%20deep%0Alearning%2C%20suffer%20from%20limited%20performance%20and%20precision%2C%20relying%20on%20undirected%0Asearch%20or%20making%20limited%20usage%20of%20the%20contextual%20information.%20In%20this%20paper%20we%0Apropose%20APIRL%2C%20a%20fully%20automated%20deep%20reinforcement%20learning%20tool%20for%20testing%0AREST%20APIs.%20A%20key%20novelty%20of%20our%20approach%20is%20the%20use%20of%20feedback%20from%20a%0Atransformer%20module%20pre-trained%20on%20JSON-structured%20data%2C%20akin%20to%20that%20used%20in%0AAPI%20responses.%20This%20allows%20APIRL%20to%20learn%20the%20subtleties%20relating%20to%20test%0Aoutcomes%2C%20and%20generalise%20to%20unseen%20API%20endpoints.%20We%20show%20APIRL%20can%20find%0Asignificantly%20more%20bugs%20than%20the%20state-of-the-art%20in%20real%20world%20REST%20APIs%20while%0Aminimising%20the%20number%20of%20required%20test%20cases.%20We%20also%20study%20how%20reward%0Afunctions%2C%20and%20other%20key%20design%20choices%2C%20affect%20learnt%20policies%20in%20a%20thorough%0Aablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPIRL%253A%2520Deep%2520Reinforcement%2520Learning%2520for%2520REST%2520API%2520Fuzzing%26entry.906535625%3DMyles%2520Foley%2520and%2520Sergio%2520Maffeis%26entry.1292438233%3D%2520%2520REST%2520APIs%2520have%2520become%2520key%2520components%2520of%2520web%2520services.%2520However%252C%2520they%2520often%250Acontain%2520logic%2520flaws%2520resulting%2520in%2520server%2520side%2520errors%2520or%2520security%250Avulnerabilities.%2520HTTP%2520requests%2520are%2520used%2520as%2520test%2520cases%2520to%2520find%2520and%2520mitigate%2520such%250Aissues.%2520Existing%2520methods%2520to%2520modify%2520requests%252C%2520including%2520those%2520using%2520deep%250Alearning%252C%2520suffer%2520from%2520limited%2520performance%2520and%2520precision%252C%2520relying%2520on%2520undirected%250Asearch%2520or%2520making%2520limited%2520usage%2520of%2520the%2520contextual%2520information.%2520In%2520this%2520paper%2520we%250Apropose%2520APIRL%252C%2520a%2520fully%2520automated%2520deep%2520reinforcement%2520learning%2520tool%2520for%2520testing%250AREST%2520APIs.%2520A%2520key%2520novelty%2520of%2520our%2520approach%2520is%2520the%2520use%2520of%2520feedback%2520from%2520a%250Atransformer%2520module%2520pre-trained%2520on%2520JSON-structured%2520data%252C%2520akin%2520to%2520that%2520used%2520in%250AAPI%2520responses.%2520This%2520allows%2520APIRL%2520to%2520learn%2520the%2520subtleties%2520relating%2520to%2520test%250Aoutcomes%252C%2520and%2520generalise%2520to%2520unseen%2520API%2520endpoints.%2520We%2520show%2520APIRL%2520can%2520find%250Asignificantly%2520more%2520bugs%2520than%2520the%2520state-of-the-art%2520in%2520real%2520world%2520REST%2520APIs%2520while%250Aminimising%2520the%2520number%2520of%2520required%2520test%2520cases.%2520We%2520also%2520study%2520how%2520reward%250Afunctions%252C%2520and%2520other%2520key%2520design%2520choices%252C%2520affect%2520learnt%2520policies%2520in%2520a%2520thorough%250Aablation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APIRL%3A%20Deep%20Reinforcement%20Learning%20for%20REST%20API%20Fuzzing&entry.906535625=Myles%20Foley%20and%20Sergio%20Maffeis&entry.1292438233=%20%20REST%20APIs%20have%20become%20key%20components%20of%20web%20services.%20However%2C%20they%20often%0Acontain%20logic%20flaws%20resulting%20in%20server%20side%20errors%20or%20security%0Avulnerabilities.%20HTTP%20requests%20are%20used%20as%20test%20cases%20to%20find%20and%20mitigate%20such%0Aissues.%20Existing%20methods%20to%20modify%20requests%2C%20including%20those%20using%20deep%0Alearning%2C%20suffer%20from%20limited%20performance%20and%20precision%2C%20relying%20on%20undirected%0Asearch%20or%20making%20limited%20usage%20of%20the%20contextual%20information.%20In%20this%20paper%20we%0Apropose%20APIRL%2C%20a%20fully%20automated%20deep%20reinforcement%20learning%20tool%20for%20testing%0AREST%20APIs.%20A%20key%20novelty%20of%20our%20approach%20is%20the%20use%20of%20feedback%20from%20a%0Atransformer%20module%20pre-trained%20on%20JSON-structured%20data%2C%20akin%20to%20that%20used%20in%0AAPI%20responses.%20This%20allows%20APIRL%20to%20learn%20the%20subtleties%20relating%20to%20test%0Aoutcomes%2C%20and%20generalise%20to%20unseen%20API%20endpoints.%20We%20show%20APIRL%20can%20find%0Asignificantly%20more%20bugs%20than%20the%20state-of-the-art%20in%20real%20world%20REST%20APIs%20while%0Aminimising%20the%20number%20of%20required%20test%20cases.%20We%20also%20study%20how%20reward%0Afunctions%2C%20and%20other%20key%20design%20choices%2C%20affect%20learnt%20policies%20in%20a%20thorough%0Aablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15991v1&entry.124074799=Read"},
{"title": "Speedup Techniques for Switchable Temporal Plan Graph Optimization", "author": "He Jiang and Muhan Lin and Jiaoyang Li", "abstract": "  Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for\nmultiple agents. However, during the execution of a MAPF plan, agents may\nencounter unexpected delays, which can lead to inefficiencies, deadlocks, or\neven collisions. To address these issues, the Switchable Temporal Plan Graph\nprovides a framework for finding an acyclic Temporal Plan Graph with the\nminimum execution cost under delays, ensuring deadlock- and collision-free\nexecution. Unfortunately, existing optimal algorithms, such as Mixed Integer\nLinear Programming and Graph-Based Switchable Edge Search (GSES), are often too\nslow for practical use. This paper introduces Improved GSES, which\nsignificantly accelerates GSES through four speedup techniques: stronger\nadmissible heuristics, edge grouping, prioritized branching, and incremental\nimplementation. Experiments conducted on four different map types with varying\nnumbers of agents demonstrate that Improved GSES consistently achieves over\ntwice the success rate of GSES and delivers up to a 30-fold speedup on\ninstances where both methods successfully find solutions.\n", "link": "http://arxiv.org/abs/2412.15908v1", "date": "2024-12-20", "relevancy": 1.3033, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4516}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4309}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speedup%20Techniques%20for%20Switchable%20Temporal%20Plan%20Graph%20Optimization&body=Title%3A%20Speedup%20Techniques%20for%20Switchable%20Temporal%20Plan%20Graph%20Optimization%0AAuthor%3A%20He%20Jiang%20and%20Muhan%20Lin%20and%20Jiaoyang%20Li%0AAbstract%3A%20%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20focuses%20on%20planning%20collision-free%20paths%20for%0Amultiple%20agents.%20However%2C%20during%20the%20execution%20of%20a%20MAPF%20plan%2C%20agents%20may%0Aencounter%20unexpected%20delays%2C%20which%20can%20lead%20to%20inefficiencies%2C%20deadlocks%2C%20or%0Aeven%20collisions.%20To%20address%20these%20issues%2C%20the%20Switchable%20Temporal%20Plan%20Graph%0Aprovides%20a%20framework%20for%20finding%20an%20acyclic%20Temporal%20Plan%20Graph%20with%20the%0Aminimum%20execution%20cost%20under%20delays%2C%20ensuring%20deadlock-%20and%20collision-free%0Aexecution.%20Unfortunately%2C%20existing%20optimal%20algorithms%2C%20such%20as%20Mixed%20Integer%0ALinear%20Programming%20and%20Graph-Based%20Switchable%20Edge%20Search%20%28GSES%29%2C%20are%20often%20too%0Aslow%20for%20practical%20use.%20This%20paper%20introduces%20Improved%20GSES%2C%20which%0Asignificantly%20accelerates%20GSES%20through%20four%20speedup%20techniques%3A%20stronger%0Aadmissible%20heuristics%2C%20edge%20grouping%2C%20prioritized%20branching%2C%20and%20incremental%0Aimplementation.%20Experiments%20conducted%20on%20four%20different%20map%20types%20with%20varying%0Anumbers%20of%20agents%20demonstrate%20that%20Improved%20GSES%20consistently%20achieves%20over%0Atwice%20the%20success%20rate%20of%20GSES%20and%20delivers%20up%20to%20a%2030-fold%20speedup%20on%0Ainstances%20where%20both%20methods%20successfully%20find%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeedup%2520Techniques%2520for%2520Switchable%2520Temporal%2520Plan%2520Graph%2520Optimization%26entry.906535625%3DHe%2520Jiang%2520and%2520Muhan%2520Lin%2520and%2520Jiaoyang%2520Li%26entry.1292438233%3D%2520%2520Multi-Agent%2520Path%2520Finding%2520%2528MAPF%2529%2520focuses%2520on%2520planning%2520collision-free%2520paths%2520for%250Amultiple%2520agents.%2520However%252C%2520during%2520the%2520execution%2520of%2520a%2520MAPF%2520plan%252C%2520agents%2520may%250Aencounter%2520unexpected%2520delays%252C%2520which%2520can%2520lead%2520to%2520inefficiencies%252C%2520deadlocks%252C%2520or%250Aeven%2520collisions.%2520To%2520address%2520these%2520issues%252C%2520the%2520Switchable%2520Temporal%2520Plan%2520Graph%250Aprovides%2520a%2520framework%2520for%2520finding%2520an%2520acyclic%2520Temporal%2520Plan%2520Graph%2520with%2520the%250Aminimum%2520execution%2520cost%2520under%2520delays%252C%2520ensuring%2520deadlock-%2520and%2520collision-free%250Aexecution.%2520Unfortunately%252C%2520existing%2520optimal%2520algorithms%252C%2520such%2520as%2520Mixed%2520Integer%250ALinear%2520Programming%2520and%2520Graph-Based%2520Switchable%2520Edge%2520Search%2520%2528GSES%2529%252C%2520are%2520often%2520too%250Aslow%2520for%2520practical%2520use.%2520This%2520paper%2520introduces%2520Improved%2520GSES%252C%2520which%250Asignificantly%2520accelerates%2520GSES%2520through%2520four%2520speedup%2520techniques%253A%2520stronger%250Aadmissible%2520heuristics%252C%2520edge%2520grouping%252C%2520prioritized%2520branching%252C%2520and%2520incremental%250Aimplementation.%2520Experiments%2520conducted%2520on%2520four%2520different%2520map%2520types%2520with%2520varying%250Anumbers%2520of%2520agents%2520demonstrate%2520that%2520Improved%2520GSES%2520consistently%2520achieves%2520over%250Atwice%2520the%2520success%2520rate%2520of%2520GSES%2520and%2520delivers%2520up%2520to%2520a%252030-fold%2520speedup%2520on%250Ainstances%2520where%2520both%2520methods%2520successfully%2520find%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speedup%20Techniques%20for%20Switchable%20Temporal%20Plan%20Graph%20Optimization&entry.906535625=He%20Jiang%20and%20Muhan%20Lin%20and%20Jiaoyang%20Li&entry.1292438233=%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20focuses%20on%20planning%20collision-free%20paths%20for%0Amultiple%20agents.%20However%2C%20during%20the%20execution%20of%20a%20MAPF%20plan%2C%20agents%20may%0Aencounter%20unexpected%20delays%2C%20which%20can%20lead%20to%20inefficiencies%2C%20deadlocks%2C%20or%0Aeven%20collisions.%20To%20address%20these%20issues%2C%20the%20Switchable%20Temporal%20Plan%20Graph%0Aprovides%20a%20framework%20for%20finding%20an%20acyclic%20Temporal%20Plan%20Graph%20with%20the%0Aminimum%20execution%20cost%20under%20delays%2C%20ensuring%20deadlock-%20and%20collision-free%0Aexecution.%20Unfortunately%2C%20existing%20optimal%20algorithms%2C%20such%20as%20Mixed%20Integer%0ALinear%20Programming%20and%20Graph-Based%20Switchable%20Edge%20Search%20%28GSES%29%2C%20are%20often%20too%0Aslow%20for%20practical%20use.%20This%20paper%20introduces%20Improved%20GSES%2C%20which%0Asignificantly%20accelerates%20GSES%20through%20four%20speedup%20techniques%3A%20stronger%0Aadmissible%20heuristics%2C%20edge%20grouping%2C%20prioritized%20branching%2C%20and%20incremental%0Aimplementation.%20Experiments%20conducted%20on%20four%20different%20map%20types%20with%20varying%0Anumbers%20of%20agents%20demonstrate%20that%20Improved%20GSES%20consistently%20achieves%20over%0Atwice%20the%20success%20rate%20of%20GSES%20and%20delivers%20up%20to%20a%2030-fold%20speedup%20on%0Ainstances%20where%20both%20methods%20successfully%20find%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15908v1&entry.124074799=Read"},
{"title": "Towards Projected and Incremental Pseudo-Boolean Model Counting", "author": "Suwei Yang and Kuldeep S. Meel", "abstract": "  Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.\n", "link": "http://arxiv.org/abs/2412.14485v2", "date": "2024-12-20", "relevancy": 1.2833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Projected%20and%20Incremental%20Pseudo-Boolean%20Model%20Counting&body=Title%3A%20Towards%20Projected%20and%20Incremental%20Pseudo-Boolean%20Model%20Counting%0AAuthor%3A%20Suwei%20Yang%20and%20Kuldeep%20S.%20Meel%0AAbstract%3A%20%20%20Model%20counting%20is%20a%20fundamental%20task%20that%20involves%20determining%20the%20number%20of%0Asatisfying%20assignments%20to%20a%20logical%20formula%2C%20typically%20in%20conjunctive%20normal%0Aform%20%28CNF%29.%20While%20CNF%20model%20counting%20has%20received%20extensive%20attention%20over%0Arecent%20decades%2C%20interest%20in%20Pseudo-Boolean%20%28PB%29%20model%20counting%20is%20just%20emerging%0Apartly%20due%20to%20the%20greater%20flexibility%20of%20PB%20formulas.%20As%20such%2C%20we%20observed%0Afeature%20gaps%20in%20existing%20PB%20counters%20such%20as%20a%20lack%20of%20support%20for%20projected%0Aand%20incremental%20settings%2C%20which%20could%20hinder%20adoption.%20In%20this%20work%2C%20our%20main%0Acontribution%20is%20the%20introduction%20of%20the%20PB%20model%20counter%20PBCount2%2C%20the%20first%0Aexact%20PB%20model%20counter%20with%20support%20for%20projected%20and%20incremental%20model%0Acounting.%20Our%20counter%2C%20PBCount2%2C%20uses%20our%20Least%20Occurrence%20Weighted%20Min%20Degree%0A%28LOW-MD%29%20computation%20ordering%20heuristic%20to%20support%20projected%20model%20counting%20and%0Aa%20cache%20mechanism%20to%20enable%20incremental%20model%20counting.%20In%20our%20evaluations%2C%0APBCount2%20completed%20at%20least%201.40x%20the%20number%20of%20benchmarks%20of%20competing%20methods%0Afor%20projected%20model%20counting%20and%20at%20least%201.18x%20of%20competing%20methods%20in%0Aincremental%20model%20counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Projected%2520and%2520Incremental%2520Pseudo-Boolean%2520Model%2520Counting%26entry.906535625%3DSuwei%2520Yang%2520and%2520Kuldeep%2520S.%2520Meel%26entry.1292438233%3D%2520%2520Model%2520counting%2520is%2520a%2520fundamental%2520task%2520that%2520involves%2520determining%2520the%2520number%2520of%250Asatisfying%2520assignments%2520to%2520a%2520logical%2520formula%252C%2520typically%2520in%2520conjunctive%2520normal%250Aform%2520%2528CNF%2529.%2520While%2520CNF%2520model%2520counting%2520has%2520received%2520extensive%2520attention%2520over%250Arecent%2520decades%252C%2520interest%2520in%2520Pseudo-Boolean%2520%2528PB%2529%2520model%2520counting%2520is%2520just%2520emerging%250Apartly%2520due%2520to%2520the%2520greater%2520flexibility%2520of%2520PB%2520formulas.%2520As%2520such%252C%2520we%2520observed%250Afeature%2520gaps%2520in%2520existing%2520PB%2520counters%2520such%2520as%2520a%2520lack%2520of%2520support%2520for%2520projected%250Aand%2520incremental%2520settings%252C%2520which%2520could%2520hinder%2520adoption.%2520In%2520this%2520work%252C%2520our%2520main%250Acontribution%2520is%2520the%2520introduction%2520of%2520the%2520PB%2520model%2520counter%2520PBCount2%252C%2520the%2520first%250Aexact%2520PB%2520model%2520counter%2520with%2520support%2520for%2520projected%2520and%2520incremental%2520model%250Acounting.%2520Our%2520counter%252C%2520PBCount2%252C%2520uses%2520our%2520Least%2520Occurrence%2520Weighted%2520Min%2520Degree%250A%2528LOW-MD%2529%2520computation%2520ordering%2520heuristic%2520to%2520support%2520projected%2520model%2520counting%2520and%250Aa%2520cache%2520mechanism%2520to%2520enable%2520incremental%2520model%2520counting.%2520In%2520our%2520evaluations%252C%250APBCount2%2520completed%2520at%2520least%25201.40x%2520the%2520number%2520of%2520benchmarks%2520of%2520competing%2520methods%250Afor%2520projected%2520model%2520counting%2520and%2520at%2520least%25201.18x%2520of%2520competing%2520methods%2520in%250Aincremental%2520model%2520counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Projected%20and%20Incremental%20Pseudo-Boolean%20Model%20Counting&entry.906535625=Suwei%20Yang%20and%20Kuldeep%20S.%20Meel&entry.1292438233=%20%20Model%20counting%20is%20a%20fundamental%20task%20that%20involves%20determining%20the%20number%20of%0Asatisfying%20assignments%20to%20a%20logical%20formula%2C%20typically%20in%20conjunctive%20normal%0Aform%20%28CNF%29.%20While%20CNF%20model%20counting%20has%20received%20extensive%20attention%20over%0Arecent%20decades%2C%20interest%20in%20Pseudo-Boolean%20%28PB%29%20model%20counting%20is%20just%20emerging%0Apartly%20due%20to%20the%20greater%20flexibility%20of%20PB%20formulas.%20As%20such%2C%20we%20observed%0Afeature%20gaps%20in%20existing%20PB%20counters%20such%20as%20a%20lack%20of%20support%20for%20projected%0Aand%20incremental%20settings%2C%20which%20could%20hinder%20adoption.%20In%20this%20work%2C%20our%20main%0Acontribution%20is%20the%20introduction%20of%20the%20PB%20model%20counter%20PBCount2%2C%20the%20first%0Aexact%20PB%20model%20counter%20with%20support%20for%20projected%20and%20incremental%20model%0Acounting.%20Our%20counter%2C%20PBCount2%2C%20uses%20our%20Least%20Occurrence%20Weighted%20Min%20Degree%0A%28LOW-MD%29%20computation%20ordering%20heuristic%20to%20support%20projected%20model%20counting%20and%0Aa%20cache%20mechanism%20to%20enable%20incremental%20model%20counting.%20In%20our%20evaluations%2C%0APBCount2%20completed%20at%20least%201.40x%20the%20number%20of%20benchmarks%20of%20competing%20methods%0Afor%20projected%20model%20counting%20and%20at%20least%201.18x%20of%20competing%20methods%20in%0Aincremental%20model%20counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14485v2&entry.124074799=Read"},
{"title": "LLAssist: Simple Tools for Automating Literature Review Using Large\n  Language Models", "author": "Christoforus Yoga Haryanto", "abstract": "  This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.\n", "link": "http://arxiv.org/abs/2407.13993v3", "date": "2024-12-20", "relevancy": 1.6922, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLAssist%3A%20Simple%20Tools%20for%20Automating%20Literature%20Review%20Using%20Large%0A%20%20Language%20Models&body=Title%3A%20LLAssist%3A%20Simple%20Tools%20for%20Automating%20Literature%20Review%20Using%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Christoforus%20Yoga%20Haryanto%0AAbstract%3A%20%20%20This%20paper%20introduces%20LLAssist%2C%20an%20open-source%20tool%20designed%20to%20streamline%0Aliterature%20reviews%20in%20academic%20research.%20In%20an%20era%20of%20exponential%20growth%20in%0Ascientific%20publications%2C%20researchers%20face%20mounting%20challenges%20in%20efficiently%0Aprocessing%20vast%20volumes%20of%20literature.%20LLAssist%20addresses%20this%20issue%20by%0Aleveraging%20Large%20Language%20Models%20%28LLMs%29%20and%20Natural%20Language%20Processing%20%28NLP%29%0Atechniques%20to%20automate%20key%20aspects%20of%20the%20review%20process.%20Specifically%2C%20it%0Aextracts%20important%20information%20from%20research%20articles%20and%20evaluates%20their%0Arelevance%20to%20user-defined%20research%20questions.%20The%20goal%20of%20LLAssist%20is%20to%0Asignificantly%20reduce%20the%20time%20and%20effort%20required%20for%20comprehensive%20literature%0Areviews%2C%20allowing%20researchers%20to%20focus%20more%20on%20analyzing%20and%20synthesizing%0Ainformation%20rather%20than%20on%20initial%20screening%20tasks.%20By%20automating%20parts%20of%20the%0Aliterature%20review%20workflow%2C%20LLAssist%20aims%20to%20help%20researchers%20manage%20the%0Agrowing%20volume%20of%20academic%20publications%20more%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13993v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLAssist%253A%2520Simple%2520Tools%2520for%2520Automating%2520Literature%2520Review%2520Using%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DChristoforus%2520Yoga%2520Haryanto%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LLAssist%252C%2520an%2520open-source%2520tool%2520designed%2520to%2520streamline%250Aliterature%2520reviews%2520in%2520academic%2520research.%2520In%2520an%2520era%2520of%2520exponential%2520growth%2520in%250Ascientific%2520publications%252C%2520researchers%2520face%2520mounting%2520challenges%2520in%2520efficiently%250Aprocessing%2520vast%2520volumes%2520of%2520literature.%2520LLAssist%2520addresses%2520this%2520issue%2520by%250Aleveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%250Atechniques%2520to%2520automate%2520key%2520aspects%2520of%2520the%2520review%2520process.%2520Specifically%252C%2520it%250Aextracts%2520important%2520information%2520from%2520research%2520articles%2520and%2520evaluates%2520their%250Arelevance%2520to%2520user-defined%2520research%2520questions.%2520The%2520goal%2520of%2520LLAssist%2520is%2520to%250Asignificantly%2520reduce%2520the%2520time%2520and%2520effort%2520required%2520for%2520comprehensive%2520literature%250Areviews%252C%2520allowing%2520researchers%2520to%2520focus%2520more%2520on%2520analyzing%2520and%2520synthesizing%250Ainformation%2520rather%2520than%2520on%2520initial%2520screening%2520tasks.%2520By%2520automating%2520parts%2520of%2520the%250Aliterature%2520review%2520workflow%252C%2520LLAssist%2520aims%2520to%2520help%2520researchers%2520manage%2520the%250Agrowing%2520volume%2520of%2520academic%2520publications%2520more%2520efficiently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13993v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLAssist%3A%20Simple%20Tools%20for%20Automating%20Literature%20Review%20Using%20Large%0A%20%20Language%20Models&entry.906535625=Christoforus%20Yoga%20Haryanto&entry.1292438233=%20%20This%20paper%20introduces%20LLAssist%2C%20an%20open-source%20tool%20designed%20to%20streamline%0Aliterature%20reviews%20in%20academic%20research.%20In%20an%20era%20of%20exponential%20growth%20in%0Ascientific%20publications%2C%20researchers%20face%20mounting%20challenges%20in%20efficiently%0Aprocessing%20vast%20volumes%20of%20literature.%20LLAssist%20addresses%20this%20issue%20by%0Aleveraging%20Large%20Language%20Models%20%28LLMs%29%20and%20Natural%20Language%20Processing%20%28NLP%29%0Atechniques%20to%20automate%20key%20aspects%20of%20the%20review%20process.%20Specifically%2C%20it%0Aextracts%20important%20information%20from%20research%20articles%20and%20evaluates%20their%0Arelevance%20to%20user-defined%20research%20questions.%20The%20goal%20of%20LLAssist%20is%20to%0Asignificantly%20reduce%20the%20time%20and%20effort%20required%20for%20comprehensive%20literature%0Areviews%2C%20allowing%20researchers%20to%20focus%20more%20on%20analyzing%20and%20synthesizing%0Ainformation%20rather%20than%20on%20initial%20screening%20tasks.%20By%20automating%20parts%20of%20the%0Aliterature%20review%20workflow%2C%20LLAssist%20aims%20to%20help%20researchers%20manage%20the%0Agrowing%20volume%20of%20academic%20publications%20more%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13993v3&entry.124074799=Read"},
{"title": "Factored space models: Towards causality between levels of abstraction", "author": "Scott Garrabrant and Matthias Georg Mayer and Magdalena Wache and Leon Lang and Sam Eisenstat and Holger Dell", "abstract": "  Causality plays an important role in understanding intelligent behavior, and\nthere is a wealth of literature on mathematical models for causality, most of\nwhich is focused on causal graphs. Causal graphs are a powerful tool for a wide\nrange of applications, in particular when the relevant variables are known and\nat the same level of abstraction. However, the given variables can also be\nunstructured data, like pixels of an image. Meanwhile, the causal variables,\nsuch as the positions of objects in the image, can be arbitrary deterministic\nfunctions of the given variables. Moreover, the causal variables may form a\nhierarchy of abstractions, in which the macro-level variables are deterministic\nfunctions of the micro-level variables. Causal graphs are limited when it comes\nto modeling this kind of situation. In the presence of deterministic\nrelationships there is generally no causal graph that satisfies both the Markov\ncondition and the faithfulness condition. We introduce factored space models as\nan alternative to causal graphs which naturally represent both probabilistic\nand deterministic relationships at all levels of abstraction. Moreover, we\nintroduce structural independence and establish that it is equivalent to\nstatistical independence in every distribution that factorizes over the\nfactored space. This theorem generalizes the classical soundness and\ncompleteness theorem for d-separation.\n", "link": "http://arxiv.org/abs/2412.02579v2", "date": "2024-12-20", "relevancy": 1.7496, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factored%20space%20models%3A%20Towards%20causality%20between%20levels%20of%20abstraction&body=Title%3A%20Factored%20space%20models%3A%20Towards%20causality%20between%20levels%20of%20abstraction%0AAuthor%3A%20Scott%20Garrabrant%20and%20Matthias%20Georg%20Mayer%20and%20Magdalena%20Wache%20and%20Leon%20Lang%20and%20Sam%20Eisenstat%20and%20Holger%20Dell%0AAbstract%3A%20%20%20Causality%20plays%20an%20important%20role%20in%20understanding%20intelligent%20behavior%2C%20and%0Athere%20is%20a%20wealth%20of%20literature%20on%20mathematical%20models%20for%20causality%2C%20most%20of%0Awhich%20is%20focused%20on%20causal%20graphs.%20Causal%20graphs%20are%20a%20powerful%20tool%20for%20a%20wide%0Arange%20of%20applications%2C%20in%20particular%20when%20the%20relevant%20variables%20are%20known%20and%0Aat%20the%20same%20level%20of%20abstraction.%20However%2C%20the%20given%20variables%20can%20also%20be%0Aunstructured%20data%2C%20like%20pixels%20of%20an%20image.%20Meanwhile%2C%20the%20causal%20variables%2C%0Asuch%20as%20the%20positions%20of%20objects%20in%20the%20image%2C%20can%20be%20arbitrary%20deterministic%0Afunctions%20of%20the%20given%20variables.%20Moreover%2C%20the%20causal%20variables%20may%20form%20a%0Ahierarchy%20of%20abstractions%2C%20in%20which%20the%20macro-level%20variables%20are%20deterministic%0Afunctions%20of%20the%20micro-level%20variables.%20Causal%20graphs%20are%20limited%20when%20it%20comes%0Ato%20modeling%20this%20kind%20of%20situation.%20In%20the%20presence%20of%20deterministic%0Arelationships%20there%20is%20generally%20no%20causal%20graph%20that%20satisfies%20both%20the%20Markov%0Acondition%20and%20the%20faithfulness%20condition.%20We%20introduce%20factored%20space%20models%20as%0Aan%20alternative%20to%20causal%20graphs%20which%20naturally%20represent%20both%20probabilistic%0Aand%20deterministic%20relationships%20at%20all%20levels%20of%20abstraction.%20Moreover%2C%20we%0Aintroduce%20structural%20independence%20and%20establish%20that%20it%20is%20equivalent%20to%0Astatistical%20independence%20in%20every%20distribution%20that%20factorizes%20over%20the%0Afactored%20space.%20This%20theorem%20generalizes%20the%20classical%20soundness%20and%0Acompleteness%20theorem%20for%20d-separation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactored%2520space%2520models%253A%2520Towards%2520causality%2520between%2520levels%2520of%2520abstraction%26entry.906535625%3DScott%2520Garrabrant%2520and%2520Matthias%2520Georg%2520Mayer%2520and%2520Magdalena%2520Wache%2520and%2520Leon%2520Lang%2520and%2520Sam%2520Eisenstat%2520and%2520Holger%2520Dell%26entry.1292438233%3D%2520%2520Causality%2520plays%2520an%2520important%2520role%2520in%2520understanding%2520intelligent%2520behavior%252C%2520and%250Athere%2520is%2520a%2520wealth%2520of%2520literature%2520on%2520mathematical%2520models%2520for%2520causality%252C%2520most%2520of%250Awhich%2520is%2520focused%2520on%2520causal%2520graphs.%2520Causal%2520graphs%2520are%2520a%2520powerful%2520tool%2520for%2520a%2520wide%250Arange%2520of%2520applications%252C%2520in%2520particular%2520when%2520the%2520relevant%2520variables%2520are%2520known%2520and%250Aat%2520the%2520same%2520level%2520of%2520abstraction.%2520However%252C%2520the%2520given%2520variables%2520can%2520also%2520be%250Aunstructured%2520data%252C%2520like%2520pixels%2520of%2520an%2520image.%2520Meanwhile%252C%2520the%2520causal%2520variables%252C%250Asuch%2520as%2520the%2520positions%2520of%2520objects%2520in%2520the%2520image%252C%2520can%2520be%2520arbitrary%2520deterministic%250Afunctions%2520of%2520the%2520given%2520variables.%2520Moreover%252C%2520the%2520causal%2520variables%2520may%2520form%2520a%250Ahierarchy%2520of%2520abstractions%252C%2520in%2520which%2520the%2520macro-level%2520variables%2520are%2520deterministic%250Afunctions%2520of%2520the%2520micro-level%2520variables.%2520Causal%2520graphs%2520are%2520limited%2520when%2520it%2520comes%250Ato%2520modeling%2520this%2520kind%2520of%2520situation.%2520In%2520the%2520presence%2520of%2520deterministic%250Arelationships%2520there%2520is%2520generally%2520no%2520causal%2520graph%2520that%2520satisfies%2520both%2520the%2520Markov%250Acondition%2520and%2520the%2520faithfulness%2520condition.%2520We%2520introduce%2520factored%2520space%2520models%2520as%250Aan%2520alternative%2520to%2520causal%2520graphs%2520which%2520naturally%2520represent%2520both%2520probabilistic%250Aand%2520deterministic%2520relationships%2520at%2520all%2520levels%2520of%2520abstraction.%2520Moreover%252C%2520we%250Aintroduce%2520structural%2520independence%2520and%2520establish%2520that%2520it%2520is%2520equivalent%2520to%250Astatistical%2520independence%2520in%2520every%2520distribution%2520that%2520factorizes%2520over%2520the%250Afactored%2520space.%2520This%2520theorem%2520generalizes%2520the%2520classical%2520soundness%2520and%250Acompleteness%2520theorem%2520for%2520d-separation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factored%20space%20models%3A%20Towards%20causality%20between%20levels%20of%20abstraction&entry.906535625=Scott%20Garrabrant%20and%20Matthias%20Georg%20Mayer%20and%20Magdalena%20Wache%20and%20Leon%20Lang%20and%20Sam%20Eisenstat%20and%20Holger%20Dell&entry.1292438233=%20%20Causality%20plays%20an%20important%20role%20in%20understanding%20intelligent%20behavior%2C%20and%0Athere%20is%20a%20wealth%20of%20literature%20on%20mathematical%20models%20for%20causality%2C%20most%20of%0Awhich%20is%20focused%20on%20causal%20graphs.%20Causal%20graphs%20are%20a%20powerful%20tool%20for%20a%20wide%0Arange%20of%20applications%2C%20in%20particular%20when%20the%20relevant%20variables%20are%20known%20and%0Aat%20the%20same%20level%20of%20abstraction.%20However%2C%20the%20given%20variables%20can%20also%20be%0Aunstructured%20data%2C%20like%20pixels%20of%20an%20image.%20Meanwhile%2C%20the%20causal%20variables%2C%0Asuch%20as%20the%20positions%20of%20objects%20in%20the%20image%2C%20can%20be%20arbitrary%20deterministic%0Afunctions%20of%20the%20given%20variables.%20Moreover%2C%20the%20causal%20variables%20may%20form%20a%0Ahierarchy%20of%20abstractions%2C%20in%20which%20the%20macro-level%20variables%20are%20deterministic%0Afunctions%20of%20the%20micro-level%20variables.%20Causal%20graphs%20are%20limited%20when%20it%20comes%0Ato%20modeling%20this%20kind%20of%20situation.%20In%20the%20presence%20of%20deterministic%0Arelationships%20there%20is%20generally%20no%20causal%20graph%20that%20satisfies%20both%20the%20Markov%0Acondition%20and%20the%20faithfulness%20condition.%20We%20introduce%20factored%20space%20models%20as%0Aan%20alternative%20to%20causal%20graphs%20which%20naturally%20represent%20both%20probabilistic%0Aand%20deterministic%20relationships%20at%20all%20levels%20of%20abstraction.%20Moreover%2C%20we%0Aintroduce%20structural%20independence%20and%20establish%20that%20it%20is%20equivalent%20to%0Astatistical%20independence%20in%20every%20distribution%20that%20factorizes%20over%20the%0Afactored%20space.%20This%20theorem%20generalizes%20the%20classical%20soundness%20and%0Acompleteness%20theorem%20for%20d-separation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02579v2&entry.124074799=Read"},
{"title": "Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI\n  Refactoring", "author": "Markus Borg", "abstract": "  In the software industry, the drive to add new features often overshadows the\nneed to improve existing code. Large Language Models (LLMs) offer a new\napproach to improving codebases at an unprecedented scale through AI-assisted\nrefactoring. However, LLMs come with inherent risks such as braking changes and\nthe introduction of security vulnerabilities. We advocate for encapsulating the\ninteraction with the models in IDEs and validating refactoring attempts using\ntrustworthy safeguards. However, equally important for the uptake of AI\nrefactoring is research on trust development. In this position paper, we\nposition our future work based on established models from research on human\nfactors in automation. We outline action research within CodeScene on\ndevelopment of 1) novel LLM safeguards and 2) user interaction that conveys an\nappropriate level of trust. The industry collaboration enables large-scale\nrepository analysis and A/B testing to continuously guide the design of our\nresearch interventions.\n", "link": "http://arxiv.org/abs/2412.15948v1", "date": "2024-12-20", "relevancy": 0.9449, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4721}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%20Calibration%20in%20IDEs%3A%20Paving%20the%20Way%20for%20Widespread%20Adoption%20of%20AI%0A%20%20Refactoring&body=Title%3A%20Trust%20Calibration%20in%20IDEs%3A%20Paving%20the%20Way%20for%20Widespread%20Adoption%20of%20AI%0A%20%20Refactoring%0AAuthor%3A%20Markus%20Borg%0AAbstract%3A%20%20%20In%20the%20software%20industry%2C%20the%20drive%20to%20add%20new%20features%20often%20overshadows%20the%0Aneed%20to%20improve%20existing%20code.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20new%0Aapproach%20to%20improving%20codebases%20at%20an%20unprecedented%20scale%20through%20AI-assisted%0Arefactoring.%20However%2C%20LLMs%20come%20with%20inherent%20risks%20such%20as%20braking%20changes%20and%0Athe%20introduction%20of%20security%20vulnerabilities.%20We%20advocate%20for%20encapsulating%20the%0Ainteraction%20with%20the%20models%20in%20IDEs%20and%20validating%20refactoring%20attempts%20using%0Atrustworthy%20safeguards.%20However%2C%20equally%20important%20for%20the%20uptake%20of%20AI%0Arefactoring%20is%20research%20on%20trust%20development.%20In%20this%20position%20paper%2C%20we%0Aposition%20our%20future%20work%20based%20on%20established%20models%20from%20research%20on%20human%0Afactors%20in%20automation.%20We%20outline%20action%20research%20within%20CodeScene%20on%0Adevelopment%20of%201%29%20novel%20LLM%20safeguards%20and%202%29%20user%20interaction%20that%20conveys%20an%0Aappropriate%20level%20of%20trust.%20The%20industry%20collaboration%20enables%20large-scale%0Arepository%20analysis%20and%20A/B%20testing%20to%20continuously%20guide%20the%20design%20of%20our%0Aresearch%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%2520Calibration%2520in%2520IDEs%253A%2520Paving%2520the%2520Way%2520for%2520Widespread%2520Adoption%2520of%2520AI%250A%2520%2520Refactoring%26entry.906535625%3DMarkus%2520Borg%26entry.1292438233%3D%2520%2520In%2520the%2520software%2520industry%252C%2520the%2520drive%2520to%2520add%2520new%2520features%2520often%2520overshadows%2520the%250Aneed%2520to%2520improve%2520existing%2520code.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520new%250Aapproach%2520to%2520improving%2520codebases%2520at%2520an%2520unprecedented%2520scale%2520through%2520AI-assisted%250Arefactoring.%2520However%252C%2520LLMs%2520come%2520with%2520inherent%2520risks%2520such%2520as%2520braking%2520changes%2520and%250Athe%2520introduction%2520of%2520security%2520vulnerabilities.%2520We%2520advocate%2520for%2520encapsulating%2520the%250Ainteraction%2520with%2520the%2520models%2520in%2520IDEs%2520and%2520validating%2520refactoring%2520attempts%2520using%250Atrustworthy%2520safeguards.%2520However%252C%2520equally%2520important%2520for%2520the%2520uptake%2520of%2520AI%250Arefactoring%2520is%2520research%2520on%2520trust%2520development.%2520In%2520this%2520position%2520paper%252C%2520we%250Aposition%2520our%2520future%2520work%2520based%2520on%2520established%2520models%2520from%2520research%2520on%2520human%250Afactors%2520in%2520automation.%2520We%2520outline%2520action%2520research%2520within%2520CodeScene%2520on%250Adevelopment%2520of%25201%2529%2520novel%2520LLM%2520safeguards%2520and%25202%2529%2520user%2520interaction%2520that%2520conveys%2520an%250Aappropriate%2520level%2520of%2520trust.%2520The%2520industry%2520collaboration%2520enables%2520large-scale%250Arepository%2520analysis%2520and%2520A/B%2520testing%2520to%2520continuously%2520guide%2520the%2520design%2520of%2520our%250Aresearch%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%20Calibration%20in%20IDEs%3A%20Paving%20the%20Way%20for%20Widespread%20Adoption%20of%20AI%0A%20%20Refactoring&entry.906535625=Markus%20Borg&entry.1292438233=%20%20In%20the%20software%20industry%2C%20the%20drive%20to%20add%20new%20features%20often%20overshadows%20the%0Aneed%20to%20improve%20existing%20code.%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20new%0Aapproach%20to%20improving%20codebases%20at%20an%20unprecedented%20scale%20through%20AI-assisted%0Arefactoring.%20However%2C%20LLMs%20come%20with%20inherent%20risks%20such%20as%20braking%20changes%20and%0Athe%20introduction%20of%20security%20vulnerabilities.%20We%20advocate%20for%20encapsulating%20the%0Ainteraction%20with%20the%20models%20in%20IDEs%20and%20validating%20refactoring%20attempts%20using%0Atrustworthy%20safeguards.%20However%2C%20equally%20important%20for%20the%20uptake%20of%20AI%0Arefactoring%20is%20research%20on%20trust%20development.%20In%20this%20position%20paper%2C%20we%0Aposition%20our%20future%20work%20based%20on%20established%20models%20from%20research%20on%20human%0Afactors%20in%20automation.%20We%20outline%20action%20research%20within%20CodeScene%20on%0Adevelopment%20of%201%29%20novel%20LLM%20safeguards%20and%202%29%20user%20interaction%20that%20conveys%20an%0Aappropriate%20level%20of%20trust.%20The%20industry%20collaboration%20enables%20large-scale%0Arepository%20analysis%20and%20A/B%20testing%20to%20continuously%20guide%20the%20design%20of%20our%0Aresearch%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15948v1&entry.124074799=Read"},
{"title": "Temporal Elections: Welfare, Strategyproofness, and Proportionality", "author": "Edith Elkind and Tzeh Yuan Neoh and Nicholas Teh", "abstract": "  We investigate a model of sequential decision-making where a single\nalternative is chosen at each round. We focus on two objectives -- utilitarian\nwelfare (Util) and egalitarian welfare (Egal) -- and consider the computational\ncomplexity of maximizing these objectives, as well as their compatibility with\nstrategyproofness and proportionality. We observe that maximizing Util is easy,\nbut the corresponding decision problem for Egal is NP-complete even in\nrestricted cases. We complement this hardness result for Egal with\nparameterized complexity analysis and an approximation algorithm. Additionally,\nwe show that, while a mechanism that outputs an outcome that maximizes Util is\nstrategyproof, all deterministic mechanisms for computing outcomes that\nmaximize Egal fail a very weak variant of strategyproofness, called non-obvious\nmanipulability (NOM). However, we show that when agents have non-empty approval\nsets at each timestep, choosing an Egal-maximizing outcome while breaking ties\nlexicographically satisfies NOM. Regarding proportionality, we prove that a\nproportional (PROP) outcome can be computed efficiently, but finding an outcome\nthat maximizes Util while guaranteeing PROP is NP-hard. We also derive upper\nand lower bounds on the (strong) price of proportionality with respect to Util\nand Egal. Some of our results extend to $p$-mean welfare measures other than\nEgal and Util.\n", "link": "http://arxiv.org/abs/2408.13637v2", "date": "2024-12-20", "relevancy": 1.4679, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3814}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3681}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Elections%3A%20Welfare%2C%20Strategyproofness%2C%20and%20Proportionality&body=Title%3A%20Temporal%20Elections%3A%20Welfare%2C%20Strategyproofness%2C%20and%20Proportionality%0AAuthor%3A%20Edith%20Elkind%20and%20Tzeh%20Yuan%20Neoh%20and%20Nicholas%20Teh%0AAbstract%3A%20%20%20We%20investigate%20a%20model%20of%20sequential%20decision-making%20where%20a%20single%0Aalternative%20is%20chosen%20at%20each%20round.%20We%20focus%20on%20two%20objectives%20--%20utilitarian%0Awelfare%20%28Util%29%20and%20egalitarian%20welfare%20%28Egal%29%20--%20and%20consider%20the%20computational%0Acomplexity%20of%20maximizing%20these%20objectives%2C%20as%20well%20as%20their%20compatibility%20with%0Astrategyproofness%20and%20proportionality.%20We%20observe%20that%20maximizing%20Util%20is%20easy%2C%0Abut%20the%20corresponding%20decision%20problem%20for%20Egal%20is%20NP-complete%20even%20in%0Arestricted%20cases.%20We%20complement%20this%20hardness%20result%20for%20Egal%20with%0Aparameterized%20complexity%20analysis%20and%20an%20approximation%20algorithm.%20Additionally%2C%0Awe%20show%20that%2C%20while%20a%20mechanism%20that%20outputs%20an%20outcome%20that%20maximizes%20Util%20is%0Astrategyproof%2C%20all%20deterministic%20mechanisms%20for%20computing%20outcomes%20that%0Amaximize%20Egal%20fail%20a%20very%20weak%20variant%20of%20strategyproofness%2C%20called%20non-obvious%0Amanipulability%20%28NOM%29.%20However%2C%20we%20show%20that%20when%20agents%20have%20non-empty%20approval%0Asets%20at%20each%20timestep%2C%20choosing%20an%20Egal-maximizing%20outcome%20while%20breaking%20ties%0Alexicographically%20satisfies%20NOM.%20Regarding%20proportionality%2C%20we%20prove%20that%20a%0Aproportional%20%28PROP%29%20outcome%20can%20be%20computed%20efficiently%2C%20but%20finding%20an%20outcome%0Athat%20maximizes%20Util%20while%20guaranteeing%20PROP%20is%20NP-hard.%20We%20also%20derive%20upper%0Aand%20lower%20bounds%20on%20the%20%28strong%29%20price%20of%20proportionality%20with%20respect%20to%20Util%0Aand%20Egal.%20Some%20of%20our%20results%20extend%20to%20%24p%24-mean%20welfare%20measures%20other%20than%0AEgal%20and%20Util.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Elections%253A%2520Welfare%252C%2520Strategyproofness%252C%2520and%2520Proportionality%26entry.906535625%3DEdith%2520Elkind%2520and%2520Tzeh%2520Yuan%2520Neoh%2520and%2520Nicholas%2520Teh%26entry.1292438233%3D%2520%2520We%2520investigate%2520a%2520model%2520of%2520sequential%2520decision-making%2520where%2520a%2520single%250Aalternative%2520is%2520chosen%2520at%2520each%2520round.%2520We%2520focus%2520on%2520two%2520objectives%2520--%2520utilitarian%250Awelfare%2520%2528Util%2529%2520and%2520egalitarian%2520welfare%2520%2528Egal%2529%2520--%2520and%2520consider%2520the%2520computational%250Acomplexity%2520of%2520maximizing%2520these%2520objectives%252C%2520as%2520well%2520as%2520their%2520compatibility%2520with%250Astrategyproofness%2520and%2520proportionality.%2520We%2520observe%2520that%2520maximizing%2520Util%2520is%2520easy%252C%250Abut%2520the%2520corresponding%2520decision%2520problem%2520for%2520Egal%2520is%2520NP-complete%2520even%2520in%250Arestricted%2520cases.%2520We%2520complement%2520this%2520hardness%2520result%2520for%2520Egal%2520with%250Aparameterized%2520complexity%2520analysis%2520and%2520an%2520approximation%2520algorithm.%2520Additionally%252C%250Awe%2520show%2520that%252C%2520while%2520a%2520mechanism%2520that%2520outputs%2520an%2520outcome%2520that%2520maximizes%2520Util%2520is%250Astrategyproof%252C%2520all%2520deterministic%2520mechanisms%2520for%2520computing%2520outcomes%2520that%250Amaximize%2520Egal%2520fail%2520a%2520very%2520weak%2520variant%2520of%2520strategyproofness%252C%2520called%2520non-obvious%250Amanipulability%2520%2528NOM%2529.%2520However%252C%2520we%2520show%2520that%2520when%2520agents%2520have%2520non-empty%2520approval%250Asets%2520at%2520each%2520timestep%252C%2520choosing%2520an%2520Egal-maximizing%2520outcome%2520while%2520breaking%2520ties%250Alexicographically%2520satisfies%2520NOM.%2520Regarding%2520proportionality%252C%2520we%2520prove%2520that%2520a%250Aproportional%2520%2528PROP%2529%2520outcome%2520can%2520be%2520computed%2520efficiently%252C%2520but%2520finding%2520an%2520outcome%250Athat%2520maximizes%2520Util%2520while%2520guaranteeing%2520PROP%2520is%2520NP-hard.%2520We%2520also%2520derive%2520upper%250Aand%2520lower%2520bounds%2520on%2520the%2520%2528strong%2529%2520price%2520of%2520proportionality%2520with%2520respect%2520to%2520Util%250Aand%2520Egal.%2520Some%2520of%2520our%2520results%2520extend%2520to%2520%2524p%2524-mean%2520welfare%2520measures%2520other%2520than%250AEgal%2520and%2520Util.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Elections%3A%20Welfare%2C%20Strategyproofness%2C%20and%20Proportionality&entry.906535625=Edith%20Elkind%20and%20Tzeh%20Yuan%20Neoh%20and%20Nicholas%20Teh&entry.1292438233=%20%20We%20investigate%20a%20model%20of%20sequential%20decision-making%20where%20a%20single%0Aalternative%20is%20chosen%20at%20each%20round.%20We%20focus%20on%20two%20objectives%20--%20utilitarian%0Awelfare%20%28Util%29%20and%20egalitarian%20welfare%20%28Egal%29%20--%20and%20consider%20the%20computational%0Acomplexity%20of%20maximizing%20these%20objectives%2C%20as%20well%20as%20their%20compatibility%20with%0Astrategyproofness%20and%20proportionality.%20We%20observe%20that%20maximizing%20Util%20is%20easy%2C%0Abut%20the%20corresponding%20decision%20problem%20for%20Egal%20is%20NP-complete%20even%20in%0Arestricted%20cases.%20We%20complement%20this%20hardness%20result%20for%20Egal%20with%0Aparameterized%20complexity%20analysis%20and%20an%20approximation%20algorithm.%20Additionally%2C%0Awe%20show%20that%2C%20while%20a%20mechanism%20that%20outputs%20an%20outcome%20that%20maximizes%20Util%20is%0Astrategyproof%2C%20all%20deterministic%20mechanisms%20for%20computing%20outcomes%20that%0Amaximize%20Egal%20fail%20a%20very%20weak%20variant%20of%20strategyproofness%2C%20called%20non-obvious%0Amanipulability%20%28NOM%29.%20However%2C%20we%20show%20that%20when%20agents%20have%20non-empty%20approval%0Asets%20at%20each%20timestep%2C%20choosing%20an%20Egal-maximizing%20outcome%20while%20breaking%20ties%0Alexicographically%20satisfies%20NOM.%20Regarding%20proportionality%2C%20we%20prove%20that%20a%0Aproportional%20%28PROP%29%20outcome%20can%20be%20computed%20efficiently%2C%20but%20finding%20an%20outcome%0Athat%20maximizes%20Util%20while%20guaranteeing%20PROP%20is%20NP-hard.%20We%20also%20derive%20upper%0Aand%20lower%20bounds%20on%20the%20%28strong%29%20price%20of%20proportionality%20with%20respect%20to%20Util%0Aand%20Egal.%20Some%20of%20our%20results%20extend%20to%20%24p%24-mean%20welfare%20measures%20other%20than%0AEgal%20and%20Util.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13637v2&entry.124074799=Read"},
{"title": "Foresight Social-aware Reinforcement Learning for Robot Navigation", "author": "Yanying Zhou and Shijie Li and Jochen Garcke", "abstract": "  When robots handle navigation tasks while avoiding collisions, they perform\nin crowded and complex environments not as good as in stable and homogeneous\nenvironments. This often results in a low success rate and poor efficiency.\nTherefore, we propose a novel Foresight Social-aware Reinforcement Learning\n(FSRL) framework for mobile robots to achieve collision-free navigation.\nCompared to previous learning-based methods, our approach is foresighted. It\nnot only considers the current human-robot interaction to avoid an immediate\ncollision, but also estimates upcoming social interactions to still keep\ndistance in the future. Furthermore, an efficiency constraint is introduced in\nour approach that significantly reduces navigation time. Comparative\nexperiments are performed to verify the effectiveness and efficiency of our\nproposed method under more realistic and challenging simulated environments.\n", "link": "http://arxiv.org/abs/2105.13409v2", "date": "2024-12-20", "relevancy": 1.7234, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5793}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5788}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foresight%20Social-aware%20Reinforcement%20Learning%20for%20Robot%20Navigation&body=Title%3A%20Foresight%20Social-aware%20Reinforcement%20Learning%20for%20Robot%20Navigation%0AAuthor%3A%20Yanying%20Zhou%20and%20Shijie%20Li%20and%20Jochen%20Garcke%0AAbstract%3A%20%20%20When%20robots%20handle%20navigation%20tasks%20while%20avoiding%20collisions%2C%20they%20perform%0Ain%20crowded%20and%20complex%20environments%20not%20as%20good%20as%20in%20stable%20and%20homogeneous%0Aenvironments.%20This%20often%20results%20in%20a%20low%20success%20rate%20and%20poor%20efficiency.%0ATherefore%2C%20we%20propose%20a%20novel%20Foresight%20Social-aware%20Reinforcement%20Learning%0A%28FSRL%29%20framework%20for%20mobile%20robots%20to%20achieve%20collision-free%20navigation.%0ACompared%20to%20previous%20learning-based%20methods%2C%20our%20approach%20is%20foresighted.%20It%0Anot%20only%20considers%20the%20current%20human-robot%20interaction%20to%20avoid%20an%20immediate%0Acollision%2C%20but%20also%20estimates%20upcoming%20social%20interactions%20to%20still%20keep%0Adistance%20in%20the%20future.%20Furthermore%2C%20an%20efficiency%20constraint%20is%20introduced%20in%0Aour%20approach%20that%20significantly%20reduces%20navigation%20time.%20Comparative%0Aexperiments%20are%20performed%20to%20verify%20the%20effectiveness%20and%20efficiency%20of%20our%0Aproposed%20method%20under%20more%20realistic%20and%20challenging%20simulated%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2105.13409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForesight%2520Social-aware%2520Reinforcement%2520Learning%2520for%2520Robot%2520Navigation%26entry.906535625%3DYanying%2520Zhou%2520and%2520Shijie%2520Li%2520and%2520Jochen%2520Garcke%26entry.1292438233%3D%2520%2520When%2520robots%2520handle%2520navigation%2520tasks%2520while%2520avoiding%2520collisions%252C%2520they%2520perform%250Ain%2520crowded%2520and%2520complex%2520environments%2520not%2520as%2520good%2520as%2520in%2520stable%2520and%2520homogeneous%250Aenvironments.%2520This%2520often%2520results%2520in%2520a%2520low%2520success%2520rate%2520and%2520poor%2520efficiency.%250ATherefore%252C%2520we%2520propose%2520a%2520novel%2520Foresight%2520Social-aware%2520Reinforcement%2520Learning%250A%2528FSRL%2529%2520framework%2520for%2520mobile%2520robots%2520to%2520achieve%2520collision-free%2520navigation.%250ACompared%2520to%2520previous%2520learning-based%2520methods%252C%2520our%2520approach%2520is%2520foresighted.%2520It%250Anot%2520only%2520considers%2520the%2520current%2520human-robot%2520interaction%2520to%2520avoid%2520an%2520immediate%250Acollision%252C%2520but%2520also%2520estimates%2520upcoming%2520social%2520interactions%2520to%2520still%2520keep%250Adistance%2520in%2520the%2520future.%2520Furthermore%252C%2520an%2520efficiency%2520constraint%2520is%2520introduced%2520in%250Aour%2520approach%2520that%2520significantly%2520reduces%2520navigation%2520time.%2520Comparative%250Aexperiments%2520are%2520performed%2520to%2520verify%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%250Aproposed%2520method%2520under%2520more%2520realistic%2520and%2520challenging%2520simulated%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2105.13409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foresight%20Social-aware%20Reinforcement%20Learning%20for%20Robot%20Navigation&entry.906535625=Yanying%20Zhou%20and%20Shijie%20Li%20and%20Jochen%20Garcke&entry.1292438233=%20%20When%20robots%20handle%20navigation%20tasks%20while%20avoiding%20collisions%2C%20they%20perform%0Ain%20crowded%20and%20complex%20environments%20not%20as%20good%20as%20in%20stable%20and%20homogeneous%0Aenvironments.%20This%20often%20results%20in%20a%20low%20success%20rate%20and%20poor%20efficiency.%0ATherefore%2C%20we%20propose%20a%20novel%20Foresight%20Social-aware%20Reinforcement%20Learning%0A%28FSRL%29%20framework%20for%20mobile%20robots%20to%20achieve%20collision-free%20navigation.%0ACompared%20to%20previous%20learning-based%20methods%2C%20our%20approach%20is%20foresighted.%20It%0Anot%20only%20considers%20the%20current%20human-robot%20interaction%20to%20avoid%20an%20immediate%0Acollision%2C%20but%20also%20estimates%20upcoming%20social%20interactions%20to%20still%20keep%0Adistance%20in%20the%20future.%20Furthermore%2C%20an%20efficiency%20constraint%20is%20introduced%20in%0Aour%20approach%20that%20significantly%20reduces%20navigation%20time.%20Comparative%0Aexperiments%20are%20performed%20to%20verify%20the%20effectiveness%20and%20efficiency%20of%20our%0Aproposed%20method%20under%20more%20realistic%20and%20challenging%20simulated%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2105.13409v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


