<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241107.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing", "author": "Matias Turkulainen and Xuqian Ren and Iaroslav Melekhov and Otto Seiskari and Esa Rahtu and Juho Kannala", "abstract": "  High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and\nAR applications. 3D Gaussian splatting, a novel differentiable rendering\ntechnique, has achieved state-of-the-art novel view synthesis results with high\nrendering speeds and relatively low training times. However, its performance on\nscenes commonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. In this work, we explore the use of readily\naccessible geometric cues to enhance Gaussian splatting optimization in\nchallenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting\nwith depth and normal cues to tackle challenging indoor datasets and showcase\ntechniques for efficient mesh extraction. Specifically, we regularize the\noptimization procedure with depth information, enforce local smoothness of\nnearby Gaussians, and use off-the-shelf monocular networks to achieve better\nalignment with the true scene geometry. We propose an adaptive depth loss based\non the gradient of color images, improving depth estimation and novel view\nsynthesis results over various baselines. Our simple yet effective\nregularization technique enables direct mesh extraction from the Gaussian\nrepresentation, yielding more physically accurate reconstructions of indoor\nscenes.\n", "link": "http://arxiv.org/abs/2403.17822v3", "date": "2024-11-07", "relevancy": 3.5974, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7243}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing&body=Title%3A%20DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing%0AAuthor%3A%20Matias%20Turkulainen%20and%20Xuqian%20Ren%20and%20Iaroslav%20Melekhov%20and%20Otto%20Seiskari%20and%20Esa%20Rahtu%20and%20Juho%20Kannala%0AAbstract%3A%20%20%20High-fidelity%203D%20reconstruction%20of%20common%20indoor%20scenes%20is%20crucial%20for%20VR%20and%0AAR%20applications.%203D%20Gaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%0Atechnique%2C%20has%20achieved%20state-of-the-art%20novel%20view%20synthesis%20results%20with%20high%0Arendering%20speeds%20and%20relatively%20low%20training%20times.%20However%2C%20its%20performance%20on%0Ascenes%20commonly%20seen%20in%20indoor%20datasets%20is%20poor%20due%20to%20the%20lack%20of%20geometric%0Aconstraints%20during%20optimization.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20readily%0Aaccessible%20geometric%20cues%20to%20enhance%20Gaussian%20splatting%20optimization%20in%0Achallenging%2C%20ill-posed%2C%20and%20textureless%20scenes.%20We%20extend%203D%20Gaussian%20splatting%0Awith%20depth%20and%20normal%20cues%20to%20tackle%20challenging%20indoor%20datasets%20and%20showcase%0Atechniques%20for%20efficient%20mesh%20extraction.%20Specifically%2C%20we%20regularize%20the%0Aoptimization%20procedure%20with%20depth%20information%2C%20enforce%20local%20smoothness%20of%0Anearby%20Gaussians%2C%20and%20use%20off-the-shelf%20monocular%20networks%20to%20achieve%20better%0Aalignment%20with%20the%20true%20scene%20geometry.%20We%20propose%20an%20adaptive%20depth%20loss%20based%0Aon%20the%20gradient%20of%20color%20images%2C%20improving%20depth%20estimation%20and%20novel%20view%0Asynthesis%20results%20over%20various%20baselines.%20Our%20simple%20yet%20effective%0Aregularization%20technique%20enables%20direct%20mesh%20extraction%20from%20the%20Gaussian%0Arepresentation%2C%20yielding%20more%20physically%20accurate%20reconstructions%20of%20indoor%0Ascenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17822v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDN-Splatter%253A%2520Depth%2520and%2520Normal%2520Priors%2520for%2520Gaussian%2520Splatting%2520and%2520Meshing%26entry.906535625%3DMatias%2520Turkulainen%2520and%2520Xuqian%2520Ren%2520and%2520Iaroslav%2520Melekhov%2520and%2520Otto%2520Seiskari%2520and%2520Esa%2520Rahtu%2520and%2520Juho%2520Kannala%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520reconstruction%2520of%2520common%2520indoor%2520scenes%2520is%2520crucial%2520for%2520VR%2520and%250AAR%2520applications.%25203D%2520Gaussian%2520splatting%252C%2520a%2520novel%2520differentiable%2520rendering%250Atechnique%252C%2520has%2520achieved%2520state-of-the-art%2520novel%2520view%2520synthesis%2520results%2520with%2520high%250Arendering%2520speeds%2520and%2520relatively%2520low%2520training%2520times.%2520However%252C%2520its%2520performance%2520on%250Ascenes%2520commonly%2520seen%2520in%2520indoor%2520datasets%2520is%2520poor%2520due%2520to%2520the%2520lack%2520of%2520geometric%250Aconstraints%2520during%2520optimization.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520use%2520of%2520readily%250Aaccessible%2520geometric%2520cues%2520to%2520enhance%2520Gaussian%2520splatting%2520optimization%2520in%250Achallenging%252C%2520ill-posed%252C%2520and%2520textureless%2520scenes.%2520We%2520extend%25203D%2520Gaussian%2520splatting%250Awith%2520depth%2520and%2520normal%2520cues%2520to%2520tackle%2520challenging%2520indoor%2520datasets%2520and%2520showcase%250Atechniques%2520for%2520efficient%2520mesh%2520extraction.%2520Specifically%252C%2520we%2520regularize%2520the%250Aoptimization%2520procedure%2520with%2520depth%2520information%252C%2520enforce%2520local%2520smoothness%2520of%250Anearby%2520Gaussians%252C%2520and%2520use%2520off-the-shelf%2520monocular%2520networks%2520to%2520achieve%2520better%250Aalignment%2520with%2520the%2520true%2520scene%2520geometry.%2520We%2520propose%2520an%2520adaptive%2520depth%2520loss%2520based%250Aon%2520the%2520gradient%2520of%2520color%2520images%252C%2520improving%2520depth%2520estimation%2520and%2520novel%2520view%250Asynthesis%2520results%2520over%2520various%2520baselines.%2520Our%2520simple%2520yet%2520effective%250Aregularization%2520technique%2520enables%2520direct%2520mesh%2520extraction%2520from%2520the%2520Gaussian%250Arepresentation%252C%2520yielding%2520more%2520physically%2520accurate%2520reconstructions%2520of%2520indoor%250Ascenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17822v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DN-Splatter%3A%20Depth%20and%20Normal%20Priors%20for%20Gaussian%20Splatting%20and%20Meshing&entry.906535625=Matias%20Turkulainen%20and%20Xuqian%20Ren%20and%20Iaroslav%20Melekhov%20and%20Otto%20Seiskari%20and%20Esa%20Rahtu%20and%20Juho%20Kannala&entry.1292438233=%20%20High-fidelity%203D%20reconstruction%20of%20common%20indoor%20scenes%20is%20crucial%20for%20VR%20and%0AAR%20applications.%203D%20Gaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%0Atechnique%2C%20has%20achieved%20state-of-the-art%20novel%20view%20synthesis%20results%20with%20high%0Arendering%20speeds%20and%20relatively%20low%20training%20times.%20However%2C%20its%20performance%20on%0Ascenes%20commonly%20seen%20in%20indoor%20datasets%20is%20poor%20due%20to%20the%20lack%20of%20geometric%0Aconstraints%20during%20optimization.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20readily%0Aaccessible%20geometric%20cues%20to%20enhance%20Gaussian%20splatting%20optimization%20in%0Achallenging%2C%20ill-posed%2C%20and%20textureless%20scenes.%20We%20extend%203D%20Gaussian%20splatting%0Awith%20depth%20and%20normal%20cues%20to%20tackle%20challenging%20indoor%20datasets%20and%20showcase%0Atechniques%20for%20efficient%20mesh%20extraction.%20Specifically%2C%20we%20regularize%20the%0Aoptimization%20procedure%20with%20depth%20information%2C%20enforce%20local%20smoothness%20of%0Anearby%20Gaussians%2C%20and%20use%20off-the-shelf%20monocular%20networks%20to%20achieve%20better%0Aalignment%20with%20the%20true%20scene%20geometry.%20We%20propose%20an%20adaptive%20depth%20loss%20based%0Aon%20the%20gradient%20of%20color%20images%2C%20improving%20depth%20estimation%20and%20novel%20view%0Asynthesis%20results%20over%20various%20baselines.%20Our%20simple%20yet%20effective%0Aregularization%20technique%20enables%20direct%20mesh%20extraction%20from%20the%20Gaussian%0Arepresentation%2C%20yielding%20more%20physically%20accurate%20reconstructions%20of%20indoor%0Ascenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17822v3&entry.124074799=Read"},
{"title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with\n  Controllable Video Diffusion", "author": "Wenqiang Sun and Shuo Chen and Fangfu Liu and Zilong Chen and Yueqi Duan and Jun Zhang and Yikai Wang", "abstract": "  In this paper, we introduce \\textbf{DimensionX}, a framework designed to\ngenerate photorealistic 3D and 4D scenes from just a single image with video\ndiffusion. Our approach begins with the insight that both the spatial structure\nof a 3D scene and the temporal evolution of a 4D scene can be effectively\nrepresented through sequences of video frames. While recent video diffusion\nmodels have shown remarkable success in producing vivid visuals, they face\nlimitations in directly recovering 3D/4D scenes due to limited spatial and\ntemporal controllability during generation. To overcome this, we propose\nST-Director, which decouples spatial and temporal factors in video diffusion by\nlearning dimension-aware LoRAs from dimension-variant data. This controllable\nvideo diffusion approach enables precise manipulation of spatial structure and\ntemporal dynamics, allowing us to reconstruct both 3D and 4D representations\nfrom sequential frames with the combination of spatial and temporal dimensions.\nAdditionally, to bridge the gap between generated videos and real-world scenes,\nwe introduce a trajectory-aware mechanism for 3D generation and an\nidentity-preserving denoising strategy for 4D generation. Extensive experiments\non various real-world and synthetic datasets demonstrate that DimensionX\nachieves superior results in controllable video generation, as well as in 3D\nand 4D scene generation, compared with previous methods.\n", "link": "http://arxiv.org/abs/2411.04928v1", "date": "2024-11-07", "relevancy": 3.4124, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7035}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7035}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DimensionX%3A%20Create%20Any%203D%20and%204D%20Scenes%20from%20a%20Single%20Image%20with%0A%20%20Controllable%20Video%20Diffusion&body=Title%3A%20DimensionX%3A%20Create%20Any%203D%20and%204D%20Scenes%20from%20a%20Single%20Image%20with%0A%20%20Controllable%20Video%20Diffusion%0AAuthor%3A%20Wenqiang%20Sun%20and%20Shuo%20Chen%20and%20Fangfu%20Liu%20and%20Zilong%20Chen%20and%20Yueqi%20Duan%20and%20Jun%20Zhang%20and%20Yikai%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BDimensionX%7D%2C%20a%20framework%20designed%20to%0Agenerate%20photorealistic%203D%20and%204D%20scenes%20from%20just%20a%20single%20image%20with%20video%0Adiffusion.%20Our%20approach%20begins%20with%20the%20insight%20that%20both%20the%20spatial%20structure%0Aof%20a%203D%20scene%20and%20the%20temporal%20evolution%20of%20a%204D%20scene%20can%20be%20effectively%0Arepresented%20through%20sequences%20of%20video%20frames.%20While%20recent%20video%20diffusion%0Amodels%20have%20shown%20remarkable%20success%20in%20producing%20vivid%20visuals%2C%20they%20face%0Alimitations%20in%20directly%20recovering%203D/4D%20scenes%20due%20to%20limited%20spatial%20and%0Atemporal%20controllability%20during%20generation.%20To%20overcome%20this%2C%20we%20propose%0AST-Director%2C%20which%20decouples%20spatial%20and%20temporal%20factors%20in%20video%20diffusion%20by%0Alearning%20dimension-aware%20LoRAs%20from%20dimension-variant%20data.%20This%20controllable%0Avideo%20diffusion%20approach%20enables%20precise%20manipulation%20of%20spatial%20structure%20and%0Atemporal%20dynamics%2C%20allowing%20us%20to%20reconstruct%20both%203D%20and%204D%20representations%0Afrom%20sequential%20frames%20with%20the%20combination%20of%20spatial%20and%20temporal%20dimensions.%0AAdditionally%2C%20to%20bridge%20the%20gap%20between%20generated%20videos%20and%20real-world%20scenes%2C%0Awe%20introduce%20a%20trajectory-aware%20mechanism%20for%203D%20generation%20and%20an%0Aidentity-preserving%20denoising%20strategy%20for%204D%20generation.%20Extensive%20experiments%0Aon%20various%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20DimensionX%0Aachieves%20superior%20results%20in%20controllable%20video%20generation%2C%20as%20well%20as%20in%203D%0Aand%204D%20scene%20generation%2C%20compared%20with%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimensionX%253A%2520Create%2520Any%25203D%2520and%25204D%2520Scenes%2520from%2520a%2520Single%2520Image%2520with%250A%2520%2520Controllable%2520Video%2520Diffusion%26entry.906535625%3DWenqiang%2520Sun%2520and%2520Shuo%2520Chen%2520and%2520Fangfu%2520Liu%2520and%2520Zilong%2520Chen%2520and%2520Yueqi%2520Duan%2520and%2520Jun%2520Zhang%2520and%2520Yikai%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BDimensionX%257D%252C%2520a%2520framework%2520designed%2520to%250Agenerate%2520photorealistic%25203D%2520and%25204D%2520scenes%2520from%2520just%2520a%2520single%2520image%2520with%2520video%250Adiffusion.%2520Our%2520approach%2520begins%2520with%2520the%2520insight%2520that%2520both%2520the%2520spatial%2520structure%250Aof%2520a%25203D%2520scene%2520and%2520the%2520temporal%2520evolution%2520of%2520a%25204D%2520scene%2520can%2520be%2520effectively%250Arepresented%2520through%2520sequences%2520of%2520video%2520frames.%2520While%2520recent%2520video%2520diffusion%250Amodels%2520have%2520shown%2520remarkable%2520success%2520in%2520producing%2520vivid%2520visuals%252C%2520they%2520face%250Alimitations%2520in%2520directly%2520recovering%25203D/4D%2520scenes%2520due%2520to%2520limited%2520spatial%2520and%250Atemporal%2520controllability%2520during%2520generation.%2520To%2520overcome%2520this%252C%2520we%2520propose%250AST-Director%252C%2520which%2520decouples%2520spatial%2520and%2520temporal%2520factors%2520in%2520video%2520diffusion%2520by%250Alearning%2520dimension-aware%2520LoRAs%2520from%2520dimension-variant%2520data.%2520This%2520controllable%250Avideo%2520diffusion%2520approach%2520enables%2520precise%2520manipulation%2520of%2520spatial%2520structure%2520and%250Atemporal%2520dynamics%252C%2520allowing%2520us%2520to%2520reconstruct%2520both%25203D%2520and%25204D%2520representations%250Afrom%2520sequential%2520frames%2520with%2520the%2520combination%2520of%2520spatial%2520and%2520temporal%2520dimensions.%250AAdditionally%252C%2520to%2520bridge%2520the%2520gap%2520between%2520generated%2520videos%2520and%2520real-world%2520scenes%252C%250Awe%2520introduce%2520a%2520trajectory-aware%2520mechanism%2520for%25203D%2520generation%2520and%2520an%250Aidentity-preserving%2520denoising%2520strategy%2520for%25204D%2520generation.%2520Extensive%2520experiments%250Aon%2520various%2520real-world%2520and%2520synthetic%2520datasets%2520demonstrate%2520that%2520DimensionX%250Aachieves%2520superior%2520results%2520in%2520controllable%2520video%2520generation%252C%2520as%2520well%2520as%2520in%25203D%250Aand%25204D%2520scene%2520generation%252C%2520compared%2520with%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DimensionX%3A%20Create%20Any%203D%20and%204D%20Scenes%20from%20a%20Single%20Image%20with%0A%20%20Controllable%20Video%20Diffusion&entry.906535625=Wenqiang%20Sun%20and%20Shuo%20Chen%20and%20Fangfu%20Liu%20and%20Zilong%20Chen%20and%20Yueqi%20Duan%20and%20Jun%20Zhang%20and%20Yikai%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BDimensionX%7D%2C%20a%20framework%20designed%20to%0Agenerate%20photorealistic%203D%20and%204D%20scenes%20from%20just%20a%20single%20image%20with%20video%0Adiffusion.%20Our%20approach%20begins%20with%20the%20insight%20that%20both%20the%20spatial%20structure%0Aof%20a%203D%20scene%20and%20the%20temporal%20evolution%20of%20a%204D%20scene%20can%20be%20effectively%0Arepresented%20through%20sequences%20of%20video%20frames.%20While%20recent%20video%20diffusion%0Amodels%20have%20shown%20remarkable%20success%20in%20producing%20vivid%20visuals%2C%20they%20face%0Alimitations%20in%20directly%20recovering%203D/4D%20scenes%20due%20to%20limited%20spatial%20and%0Atemporal%20controllability%20during%20generation.%20To%20overcome%20this%2C%20we%20propose%0AST-Director%2C%20which%20decouples%20spatial%20and%20temporal%20factors%20in%20video%20diffusion%20by%0Alearning%20dimension-aware%20LoRAs%20from%20dimension-variant%20data.%20This%20controllable%0Avideo%20diffusion%20approach%20enables%20precise%20manipulation%20of%20spatial%20structure%20and%0Atemporal%20dynamics%2C%20allowing%20us%20to%20reconstruct%20both%203D%20and%204D%20representations%0Afrom%20sequential%20frames%20with%20the%20combination%20of%20spatial%20and%20temporal%20dimensions.%0AAdditionally%2C%20to%20bridge%20the%20gap%20between%20generated%20videos%20and%20real-world%20scenes%2C%0Awe%20introduce%20a%20trajectory-aware%20mechanism%20for%203D%20generation%20and%20an%0Aidentity-preserving%20denoising%20strategy%20for%204D%20generation.%20Extensive%20experiments%0Aon%20various%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20DimensionX%0Aachieves%20superior%20results%20in%20controllable%20video%20generation%2C%20as%20well%20as%20in%203D%0Aand%204D%20scene%20generation%2C%20compared%20with%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04928v1&entry.124074799=Read"},
{"title": "MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views", "author": "Yuedong Chen and Chuanxia Zheng and Haofei Xu and Bohan Zhuang and Andrea Vedaldi and Tat-Jen Cham and Jianfei Cai", "abstract": "  We introduce MVSplat360, a feed-forward approach for 360{\\deg} novel view\nsynthesis (NVS) of diverse real-world scenes, using only sparse observations.\nThis setting is inherently ill-posed due to minimal overlap among input views\nand insufficient visual information provided, making it challenging for\nconventional methods to achieve high-quality results. Our MVSplat360 addresses\nthis by effectively combining geometry-aware 3D reconstruction with temporally\nconsistent video generation. Specifically, it refactors a feed-forward 3D\nGaussian Splatting (3DGS) model to render features directly into the latent\nspace of a pre-trained Stable Video Diffusion (SVD) model, where these features\nthen act as pose and visual cues to guide the denoising process and produce\nphotorealistic 3D-consistent views. Our model is end-to-end trainable and\nsupports rendering arbitrary views with as few as 5 sparse input views. To\nevaluate MVSplat360's performance, we introduce a new benchmark using the\nchallenging DL3DV-10K dataset, where MVSplat360 achieves superior visual\nquality compared to state-of-the-art methods on wide-sweeping or even 360{\\deg}\nNVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the\neffectiveness of our model. The video results are available on our project\npage: https://donydchen.github.io/mvsplat360.\n", "link": "http://arxiv.org/abs/2411.04924v1", "date": "2024-11-07", "relevancy": 3.0508, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6113}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6096}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVSplat360%3A%20Feed-Forward%20360%20Scene%20Synthesis%20from%20Sparse%20Views&body=Title%3A%20MVSplat360%3A%20Feed-Forward%20360%20Scene%20Synthesis%20from%20Sparse%20Views%0AAuthor%3A%20Yuedong%20Chen%20and%20Chuanxia%20Zheng%20and%20Haofei%20Xu%20and%20Bohan%20Zhuang%20and%20Andrea%20Vedaldi%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20We%20introduce%20MVSplat360%2C%20a%20feed-forward%20approach%20for%20360%7B%5Cdeg%7D%20novel%20view%0Asynthesis%20%28NVS%29%20of%20diverse%20real-world%20scenes%2C%20using%20only%20sparse%20observations.%0AThis%20setting%20is%20inherently%20ill-posed%20due%20to%20minimal%20overlap%20among%20input%20views%0Aand%20insufficient%20visual%20information%20provided%2C%20making%20it%20challenging%20for%0Aconventional%20methods%20to%20achieve%20high-quality%20results.%20Our%20MVSplat360%20addresses%0Athis%20by%20effectively%20combining%20geometry-aware%203D%20reconstruction%20with%20temporally%0Aconsistent%20video%20generation.%20Specifically%2C%20it%20refactors%20a%20feed-forward%203D%0AGaussian%20Splatting%20%283DGS%29%20model%20to%20render%20features%20directly%20into%20the%20latent%0Aspace%20of%20a%20pre-trained%20Stable%20Video%20Diffusion%20%28SVD%29%20model%2C%20where%20these%20features%0Athen%20act%20as%20pose%20and%20visual%20cues%20to%20guide%20the%20denoising%20process%20and%20produce%0Aphotorealistic%203D-consistent%20views.%20Our%20model%20is%20end-to-end%20trainable%20and%0Asupports%20rendering%20arbitrary%20views%20with%20as%20few%20as%205%20sparse%20input%20views.%20To%0Aevaluate%20MVSplat360%27s%20performance%2C%20we%20introduce%20a%20new%20benchmark%20using%20the%0Achallenging%20DL3DV-10K%20dataset%2C%20where%20MVSplat360%20achieves%20superior%20visual%0Aquality%20compared%20to%20state-of-the-art%20methods%20on%20wide-sweeping%20or%20even%20360%7B%5Cdeg%7D%0ANVS%20tasks.%20Experiments%20on%20the%20existing%20benchmark%20RealEstate10K%20also%20confirm%20the%0Aeffectiveness%20of%20our%20model.%20The%20video%20results%20are%20available%20on%20our%20project%0Apage%3A%20https%3A//donydchen.github.io/mvsplat360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVSplat360%253A%2520Feed-Forward%2520360%2520Scene%2520Synthesis%2520from%2520Sparse%2520Views%26entry.906535625%3DYuedong%2520Chen%2520and%2520Chuanxia%2520Zheng%2520and%2520Haofei%2520Xu%2520and%2520Bohan%2520Zhuang%2520and%2520Andrea%2520Vedaldi%2520and%2520Tat-Jen%2520Cham%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520We%2520introduce%2520MVSplat360%252C%2520a%2520feed-forward%2520approach%2520for%2520360%257B%255Cdeg%257D%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%2520of%2520diverse%2520real-world%2520scenes%252C%2520using%2520only%2520sparse%2520observations.%250AThis%2520setting%2520is%2520inherently%2520ill-posed%2520due%2520to%2520minimal%2520overlap%2520among%2520input%2520views%250Aand%2520insufficient%2520visual%2520information%2520provided%252C%2520making%2520it%2520challenging%2520for%250Aconventional%2520methods%2520to%2520achieve%2520high-quality%2520results.%2520Our%2520MVSplat360%2520addresses%250Athis%2520by%2520effectively%2520combining%2520geometry-aware%25203D%2520reconstruction%2520with%2520temporally%250Aconsistent%2520video%2520generation.%2520Specifically%252C%2520it%2520refactors%2520a%2520feed-forward%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520model%2520to%2520render%2520features%2520directly%2520into%2520the%2520latent%250Aspace%2520of%2520a%2520pre-trained%2520Stable%2520Video%2520Diffusion%2520%2528SVD%2529%2520model%252C%2520where%2520these%2520features%250Athen%2520act%2520as%2520pose%2520and%2520visual%2520cues%2520to%2520guide%2520the%2520denoising%2520process%2520and%2520produce%250Aphotorealistic%25203D-consistent%2520views.%2520Our%2520model%2520is%2520end-to-end%2520trainable%2520and%250Asupports%2520rendering%2520arbitrary%2520views%2520with%2520as%2520few%2520as%25205%2520sparse%2520input%2520views.%2520To%250Aevaluate%2520MVSplat360%2527s%2520performance%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520using%2520the%250Achallenging%2520DL3DV-10K%2520dataset%252C%2520where%2520MVSplat360%2520achieves%2520superior%2520visual%250Aquality%2520compared%2520to%2520state-of-the-art%2520methods%2520on%2520wide-sweeping%2520or%2520even%2520360%257B%255Cdeg%257D%250ANVS%2520tasks.%2520Experiments%2520on%2520the%2520existing%2520benchmark%2520RealEstate10K%2520also%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520model.%2520The%2520video%2520results%2520are%2520available%2520on%2520our%2520project%250Apage%253A%2520https%253A//donydchen.github.io/mvsplat360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSplat360%3A%20Feed-Forward%20360%20Scene%20Synthesis%20from%20Sparse%20Views&entry.906535625=Yuedong%20Chen%20and%20Chuanxia%20Zheng%20and%20Haofei%20Xu%20and%20Bohan%20Zhuang%20and%20Andrea%20Vedaldi%20and%20Tat-Jen%20Cham%20and%20Jianfei%20Cai&entry.1292438233=%20%20We%20introduce%20MVSplat360%2C%20a%20feed-forward%20approach%20for%20360%7B%5Cdeg%7D%20novel%20view%0Asynthesis%20%28NVS%29%20of%20diverse%20real-world%20scenes%2C%20using%20only%20sparse%20observations.%0AThis%20setting%20is%20inherently%20ill-posed%20due%20to%20minimal%20overlap%20among%20input%20views%0Aand%20insufficient%20visual%20information%20provided%2C%20making%20it%20challenging%20for%0Aconventional%20methods%20to%20achieve%20high-quality%20results.%20Our%20MVSplat360%20addresses%0Athis%20by%20effectively%20combining%20geometry-aware%203D%20reconstruction%20with%20temporally%0Aconsistent%20video%20generation.%20Specifically%2C%20it%20refactors%20a%20feed-forward%203D%0AGaussian%20Splatting%20%283DGS%29%20model%20to%20render%20features%20directly%20into%20the%20latent%0Aspace%20of%20a%20pre-trained%20Stable%20Video%20Diffusion%20%28SVD%29%20model%2C%20where%20these%20features%0Athen%20act%20as%20pose%20and%20visual%20cues%20to%20guide%20the%20denoising%20process%20and%20produce%0Aphotorealistic%203D-consistent%20views.%20Our%20model%20is%20end-to-end%20trainable%20and%0Asupports%20rendering%20arbitrary%20views%20with%20as%20few%20as%205%20sparse%20input%20views.%20To%0Aevaluate%20MVSplat360%27s%20performance%2C%20we%20introduce%20a%20new%20benchmark%20using%20the%0Achallenging%20DL3DV-10K%20dataset%2C%20where%20MVSplat360%20achieves%20superior%20visual%0Aquality%20compared%20to%20state-of-the-art%20methods%20on%20wide-sweeping%20or%20even%20360%7B%5Cdeg%7D%0ANVS%20tasks.%20Experiments%20on%20the%20existing%20benchmark%20RealEstate10K%20also%20confirm%20the%0Aeffectiveness%20of%20our%20model.%20The%20video%20results%20are%20available%20on%20our%20project%0Apage%3A%20https%3A//donydchen.github.io/mvsplat360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04924v1&entry.124074799=Read"},
{"title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation", "author": "Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu", "abstract": "  CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.\n", "link": "http://arxiv.org/abs/2411.04997v1", "date": "2024-11-07", "relevancy": 3.0359, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlock%20Richer%20Visual%20Representation&body=Title%3A%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlock%20Richer%20Visual%20Representation%0AAuthor%3A%20Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20CLIP%20is%20one%20of%20the%20most%20important%20multimodal%20foundational%20models%20today.%20What%0Apowers%20CLIP%27s%20capabilities%3F%20The%20rich%20supervision%20signals%20provided%20by%20natural%0Alanguage%2C%20the%20carrier%20of%20human%20knowledge%2C%20shape%20a%20powerful%20cross-modal%0Arepresentation%20space.%20However%2C%20with%20the%20rapid%20advancements%20in%20large%20language%0Amodels%20LLMs%20like%20GPT-4%20and%20LLaMA%2C%20the%20boundaries%20of%20language%20comprehension%20and%0Ageneration%20are%20continually%20being%20pushed.%20This%20raises%20an%20intriguing%20question%3A%0Acan%20the%20capabilities%20of%20LLMs%20be%20harnessed%20to%20further%20improve%20multimodal%0Arepresentation%20learning%3F%20The%20potential%20benefits%20of%20incorporating%20LLMs%20into%20CLIP%0Aare%20clear.%20LLMs%27%20strong%20textual%20understanding%20can%20fundamentally%20improve%20CLIP%27s%0Aability%20to%20handle%20image%20captions%2C%20drastically%20enhancing%20its%20ability%20to%20process%0Along%20and%20complex%20texts%2C%20a%20well-known%20limitation%20of%20vanilla%20CLIP.%20Moreover%2C%20LLMs%0Aare%20trained%20on%20a%20vast%20corpus%20of%20text%2C%20possessing%20open-world%20knowledge.%20This%0Aallows%20them%20to%20expand%20on%20caption%20information%20during%20training%2C%20increasing%20the%0Aefficiency%20of%20the%20learning%20process.%20In%20this%20paper%2C%20we%20propose%20LLM2CLIP%2C%20a%20novel%0Aapproach%20that%20embraces%20the%20power%20of%20LLMs%20to%20unlock%20CLIP%27s%20potential.%20By%0Afine-tuning%20the%20LLM%20in%20the%20caption%20space%20with%20contrastive%20learning%2C%20we%20extract%0Aits%20textual%20capabilities%20into%20the%20output%20embeddings%2C%20significantly%20improving%0Athe%20output%20layer%27s%20textual%20discriminability.%20We%20then%20design%20an%20efficient%0Atraining%20process%20where%20the%20fine-tuned%20LLM%20acts%20as%20a%20powerful%20teacher%20for%20CLIP%27s%0Avisual%20encoder.%20Thanks%20to%20the%20LLM%27s%20presence%2C%20we%20can%20now%20incorporate%20longer%20and%0Amore%20complex%20captions%20without%20being%20restricted%20by%20vanilla%20CLIP%27s%20text%20encoder%27s%0Acontext%20window%20and%20ability%20limitations.%20Our%20experiments%20demonstrate%20that%20this%0Aapproach%20brings%20substantial%20improvements%20in%20cross-modal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM2CLIP%253A%2520Powerful%2520Language%2520Model%2520Unlock%2520Richer%2520Visual%2520Representation%26entry.906535625%3DWeiquan%2520Huang%2520and%2520Aoqi%2520Wu%2520and%2520Yifan%2520Yang%2520and%2520Xufang%2520Luo%2520and%2520Yuqing%2520Yang%2520and%2520Liang%2520Hu%2520and%2520Qi%2520Dai%2520and%2520Xiyang%2520Dai%2520and%2520Dongdong%2520Chen%2520and%2520Chong%2520Luo%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520CLIP%2520is%2520one%2520of%2520the%2520most%2520important%2520multimodal%2520foundational%2520models%2520today.%2520What%250Apowers%2520CLIP%2527s%2520capabilities%253F%2520The%2520rich%2520supervision%2520signals%2520provided%2520by%2520natural%250Alanguage%252C%2520the%2520carrier%2520of%2520human%2520knowledge%252C%2520shape%2520a%2520powerful%2520cross-modal%250Arepresentation%2520space.%2520However%252C%2520with%2520the%2520rapid%2520advancements%2520in%2520large%2520language%250Amodels%2520LLMs%2520like%2520GPT-4%2520and%2520LLaMA%252C%2520the%2520boundaries%2520of%2520language%2520comprehension%2520and%250Ageneration%2520are%2520continually%2520being%2520pushed.%2520This%2520raises%2520an%2520intriguing%2520question%253A%250Acan%2520the%2520capabilities%2520of%2520LLMs%2520be%2520harnessed%2520to%2520further%2520improve%2520multimodal%250Arepresentation%2520learning%253F%2520The%2520potential%2520benefits%2520of%2520incorporating%2520LLMs%2520into%2520CLIP%250Aare%2520clear.%2520LLMs%2527%2520strong%2520textual%2520understanding%2520can%2520fundamentally%2520improve%2520CLIP%2527s%250Aability%2520to%2520handle%2520image%2520captions%252C%2520drastically%2520enhancing%2520its%2520ability%2520to%2520process%250Along%2520and%2520complex%2520texts%252C%2520a%2520well-known%2520limitation%2520of%2520vanilla%2520CLIP.%2520Moreover%252C%2520LLMs%250Aare%2520trained%2520on%2520a%2520vast%2520corpus%2520of%2520text%252C%2520possessing%2520open-world%2520knowledge.%2520This%250Aallows%2520them%2520to%2520expand%2520on%2520caption%2520information%2520during%2520training%252C%2520increasing%2520the%250Aefficiency%2520of%2520the%2520learning%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LLM2CLIP%252C%2520a%2520novel%250Aapproach%2520that%2520embraces%2520the%2520power%2520of%2520LLMs%2520to%2520unlock%2520CLIP%2527s%2520potential.%2520By%250Afine-tuning%2520the%2520LLM%2520in%2520the%2520caption%2520space%2520with%2520contrastive%2520learning%252C%2520we%2520extract%250Aits%2520textual%2520capabilities%2520into%2520the%2520output%2520embeddings%252C%2520significantly%2520improving%250Athe%2520output%2520layer%2527s%2520textual%2520discriminability.%2520We%2520then%2520design%2520an%2520efficient%250Atraining%2520process%2520where%2520the%2520fine-tuned%2520LLM%2520acts%2520as%2520a%2520powerful%2520teacher%2520for%2520CLIP%2527s%250Avisual%2520encoder.%2520Thanks%2520to%2520the%2520LLM%2527s%2520presence%252C%2520we%2520can%2520now%2520incorporate%2520longer%2520and%250Amore%2520complex%2520captions%2520without%2520being%2520restricted%2520by%2520vanilla%2520CLIP%2527s%2520text%2520encoder%2527s%250Acontext%2520window%2520and%2520ability%2520limitations.%2520Our%2520experiments%2520demonstrate%2520that%2520this%250Aapproach%2520brings%2520substantial%2520improvements%2520in%2520cross-modal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlock%20Richer%20Visual%20Representation&entry.906535625=Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu&entry.1292438233=%20%20CLIP%20is%20one%20of%20the%20most%20important%20multimodal%20foundational%20models%20today.%20What%0Apowers%20CLIP%27s%20capabilities%3F%20The%20rich%20supervision%20signals%20provided%20by%20natural%0Alanguage%2C%20the%20carrier%20of%20human%20knowledge%2C%20shape%20a%20powerful%20cross-modal%0Arepresentation%20space.%20However%2C%20with%20the%20rapid%20advancements%20in%20large%20language%0Amodels%20LLMs%20like%20GPT-4%20and%20LLaMA%2C%20the%20boundaries%20of%20language%20comprehension%20and%0Ageneration%20are%20continually%20being%20pushed.%20This%20raises%20an%20intriguing%20question%3A%0Acan%20the%20capabilities%20of%20LLMs%20be%20harnessed%20to%20further%20improve%20multimodal%0Arepresentation%20learning%3F%20The%20potential%20benefits%20of%20incorporating%20LLMs%20into%20CLIP%0Aare%20clear.%20LLMs%27%20strong%20textual%20understanding%20can%20fundamentally%20improve%20CLIP%27s%0Aability%20to%20handle%20image%20captions%2C%20drastically%20enhancing%20its%20ability%20to%20process%0Along%20and%20complex%20texts%2C%20a%20well-known%20limitation%20of%20vanilla%20CLIP.%20Moreover%2C%20LLMs%0Aare%20trained%20on%20a%20vast%20corpus%20of%20text%2C%20possessing%20open-world%20knowledge.%20This%0Aallows%20them%20to%20expand%20on%20caption%20information%20during%20training%2C%20increasing%20the%0Aefficiency%20of%20the%20learning%20process.%20In%20this%20paper%2C%20we%20propose%20LLM2CLIP%2C%20a%20novel%0Aapproach%20that%20embraces%20the%20power%20of%20LLMs%20to%20unlock%20CLIP%27s%20potential.%20By%0Afine-tuning%20the%20LLM%20in%20the%20caption%20space%20with%20contrastive%20learning%2C%20we%20extract%0Aits%20textual%20capabilities%20into%20the%20output%20embeddings%2C%20significantly%20improving%0Athe%20output%20layer%27s%20textual%20discriminability.%20We%20then%20design%20an%20efficient%0Atraining%20process%20where%20the%20fine-tuned%20LLM%20acts%20as%20a%20powerful%20teacher%20for%20CLIP%27s%0Avisual%20encoder.%20Thanks%20to%20the%20LLM%27s%20presence%2C%20we%20can%20now%20incorporate%20longer%20and%0Amore%20complex%20captions%20without%20being%20restricted%20by%20vanilla%20CLIP%27s%20text%20encoder%27s%0Acontext%20window%20and%20ability%20limitations.%20Our%20experiments%20demonstrate%20that%20this%0Aapproach%20brings%20substantial%20improvements%20in%20cross-modal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04997v1&entry.124074799=Read"},
{"title": "Analyzing The Language of Visual Tokens", "author": "David M. Chan and Rodolfo Corona and Joonyong Park and Cheol Jun Cho and Yutong Bai and Trevor Darrell", "abstract": "  With the introduction of transformer-based models for vision and language\ntasks, such as LLaVA and Chameleon, there has been renewed interest in the\ndiscrete tokenized representation of images. These models often treat image\npatches as discrete tokens, analogous to words in natural language, learning\njoint alignments between visual and human languages. However, little is known\nabout the statistical behavior of these visual languages - whether they follow\nsimilar frequency distributions, grammatical structures, or topologies as\nnatural languages. In this paper, we take a natural-language-centric approach\nto analyzing discrete visual languages and uncover striking similarities and\nfundamental differences. We demonstrate that, although visual languages adhere\nto Zipfian distributions, higher token innovation drives greater entropy and\nlower compression, with tokens predominantly representing object parts,\nindicating intermediate granularity. We also show that visual languages lack\ncohesive grammatical structures, leading to higher perplexity and weaker\nhierarchical organization compared to natural languages. Finally, we\ndemonstrate that, while vision models align more closely with natural languages\nthan other models, this alignment remains significantly weaker than the\ncohesion found within natural languages. Through these experiments, we\ndemonstrate how understanding the statistical properties of discrete visual\nlanguages can inform the design of more effective computer vision models.\n", "link": "http://arxiv.org/abs/2411.05001v1", "date": "2024-11-07", "relevancy": 3.0165, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6237}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20The%20Language%20of%20Visual%20Tokens&body=Title%3A%20Analyzing%20The%20Language%20of%20Visual%20Tokens%0AAuthor%3A%20David%20M.%20Chan%20and%20Rodolfo%20Corona%20and%20Joonyong%20Park%20and%20Cheol%20Jun%20Cho%20and%20Yutong%20Bai%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20With%20the%20introduction%20of%20transformer-based%20models%20for%20vision%20and%20language%0Atasks%2C%20such%20as%20LLaVA%20and%20Chameleon%2C%20there%20has%20been%20renewed%20interest%20in%20the%0Adiscrete%20tokenized%20representation%20of%20images.%20These%20models%20often%20treat%20image%0Apatches%20as%20discrete%20tokens%2C%20analogous%20to%20words%20in%20natural%20language%2C%20learning%0Ajoint%20alignments%20between%20visual%20and%20human%20languages.%20However%2C%20little%20is%20known%0Aabout%20the%20statistical%20behavior%20of%20these%20visual%20languages%20-%20whether%20they%20follow%0Asimilar%20frequency%20distributions%2C%20grammatical%20structures%2C%20or%20topologies%20as%0Anatural%20languages.%20In%20this%20paper%2C%20we%20take%20a%20natural-language-centric%20approach%0Ato%20analyzing%20discrete%20visual%20languages%20and%20uncover%20striking%20similarities%20and%0Afundamental%20differences.%20We%20demonstrate%20that%2C%20although%20visual%20languages%20adhere%0Ato%20Zipfian%20distributions%2C%20higher%20token%20innovation%20drives%20greater%20entropy%20and%0Alower%20compression%2C%20with%20tokens%20predominantly%20representing%20object%20parts%2C%0Aindicating%20intermediate%20granularity.%20We%20also%20show%20that%20visual%20languages%20lack%0Acohesive%20grammatical%20structures%2C%20leading%20to%20higher%20perplexity%20and%20weaker%0Ahierarchical%20organization%20compared%20to%20natural%20languages.%20Finally%2C%20we%0Ademonstrate%20that%2C%20while%20vision%20models%20align%20more%20closely%20with%20natural%20languages%0Athan%20other%20models%2C%20this%20alignment%20remains%20significantly%20weaker%20than%20the%0Acohesion%20found%20within%20natural%20languages.%20Through%20these%20experiments%2C%20we%0Ademonstrate%20how%20understanding%20the%20statistical%20properties%20of%20discrete%20visual%0Alanguages%20can%20inform%20the%20design%20of%20more%20effective%20computer%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520The%2520Language%2520of%2520Visual%2520Tokens%26entry.906535625%3DDavid%2520M.%2520Chan%2520and%2520Rodolfo%2520Corona%2520and%2520Joonyong%2520Park%2520and%2520Cheol%2520Jun%2520Cho%2520and%2520Yutong%2520Bai%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520With%2520the%2520introduction%2520of%2520transformer-based%2520models%2520for%2520vision%2520and%2520language%250Atasks%252C%2520such%2520as%2520LLaVA%2520and%2520Chameleon%252C%2520there%2520has%2520been%2520renewed%2520interest%2520in%2520the%250Adiscrete%2520tokenized%2520representation%2520of%2520images.%2520These%2520models%2520often%2520treat%2520image%250Apatches%2520as%2520discrete%2520tokens%252C%2520analogous%2520to%2520words%2520in%2520natural%2520language%252C%2520learning%250Ajoint%2520alignments%2520between%2520visual%2520and%2520human%2520languages.%2520However%252C%2520little%2520is%2520known%250Aabout%2520the%2520statistical%2520behavior%2520of%2520these%2520visual%2520languages%2520-%2520whether%2520they%2520follow%250Asimilar%2520frequency%2520distributions%252C%2520grammatical%2520structures%252C%2520or%2520topologies%2520as%250Anatural%2520languages.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520natural-language-centric%2520approach%250Ato%2520analyzing%2520discrete%2520visual%2520languages%2520and%2520uncover%2520striking%2520similarities%2520and%250Afundamental%2520differences.%2520We%2520demonstrate%2520that%252C%2520although%2520visual%2520languages%2520adhere%250Ato%2520Zipfian%2520distributions%252C%2520higher%2520token%2520innovation%2520drives%2520greater%2520entropy%2520and%250Alower%2520compression%252C%2520with%2520tokens%2520predominantly%2520representing%2520object%2520parts%252C%250Aindicating%2520intermediate%2520granularity.%2520We%2520also%2520show%2520that%2520visual%2520languages%2520lack%250Acohesive%2520grammatical%2520structures%252C%2520leading%2520to%2520higher%2520perplexity%2520and%2520weaker%250Ahierarchical%2520organization%2520compared%2520to%2520natural%2520languages.%2520Finally%252C%2520we%250Ademonstrate%2520that%252C%2520while%2520vision%2520models%2520align%2520more%2520closely%2520with%2520natural%2520languages%250Athan%2520other%2520models%252C%2520this%2520alignment%2520remains%2520significantly%2520weaker%2520than%2520the%250Acohesion%2520found%2520within%2520natural%2520languages.%2520Through%2520these%2520experiments%252C%2520we%250Ademonstrate%2520how%2520understanding%2520the%2520statistical%2520properties%2520of%2520discrete%2520visual%250Alanguages%2520can%2520inform%2520the%2520design%2520of%2520more%2520effective%2520computer%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20The%20Language%20of%20Visual%20Tokens&entry.906535625=David%20M.%20Chan%20and%20Rodolfo%20Corona%20and%20Joonyong%20Park%20and%20Cheol%20Jun%20Cho%20and%20Yutong%20Bai%20and%20Trevor%20Darrell&entry.1292438233=%20%20With%20the%20introduction%20of%20transformer-based%20models%20for%20vision%20and%20language%0Atasks%2C%20such%20as%20LLaVA%20and%20Chameleon%2C%20there%20has%20been%20renewed%20interest%20in%20the%0Adiscrete%20tokenized%20representation%20of%20images.%20These%20models%20often%20treat%20image%0Apatches%20as%20discrete%20tokens%2C%20analogous%20to%20words%20in%20natural%20language%2C%20learning%0Ajoint%20alignments%20between%20visual%20and%20human%20languages.%20However%2C%20little%20is%20known%0Aabout%20the%20statistical%20behavior%20of%20these%20visual%20languages%20-%20whether%20they%20follow%0Asimilar%20frequency%20distributions%2C%20grammatical%20structures%2C%20or%20topologies%20as%0Anatural%20languages.%20In%20this%20paper%2C%20we%20take%20a%20natural-language-centric%20approach%0Ato%20analyzing%20discrete%20visual%20languages%20and%20uncover%20striking%20similarities%20and%0Afundamental%20differences.%20We%20demonstrate%20that%2C%20although%20visual%20languages%20adhere%0Ato%20Zipfian%20distributions%2C%20higher%20token%20innovation%20drives%20greater%20entropy%20and%0Alower%20compression%2C%20with%20tokens%20predominantly%20representing%20object%20parts%2C%0Aindicating%20intermediate%20granularity.%20We%20also%20show%20that%20visual%20languages%20lack%0Acohesive%20grammatical%20structures%2C%20leading%20to%20higher%20perplexity%20and%20weaker%0Ahierarchical%20organization%20compared%20to%20natural%20languages.%20Finally%2C%20we%0Ademonstrate%20that%2C%20while%20vision%20models%20align%20more%20closely%20with%20natural%20languages%0Athan%20other%20models%2C%20this%20alignment%20remains%20significantly%20weaker%20than%20the%0Acohesion%20found%20within%20natural%20languages.%20Through%20these%20experiments%2C%20we%0Ademonstrate%20how%20understanding%20the%20statistical%20properties%20of%20discrete%20visual%0Alanguages%20can%20inform%20the%20design%20of%20more%20effective%20computer%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05001v1&entry.124074799=Read"},
{"title": "Differentiable Gaussian Representation for Incomplete CT Reconstruction", "author": "Shaokai Wu and Yuxiang Lu and Wei Ji and Suizhi Huang and Fengyu Yang and Shalayiding Sirejiding and Qichen He and Jing Tong and Yanbiao Ji and Yue Ding and Hongtao Lu", "abstract": "  Incomplete Computed Tomography (CT) benefits patients by reducing radiation\nexposure. However, reconstructing high-fidelity images from limited views or\nangles remains challenging due to the ill-posed nature of the problem. Deep\nLearning Reconstruction (DLR) methods have shown promise in enhancing image\nquality, but the paradox between training data diversity and high\ngeneralization ability remains unsolved. In this paper, we propose a novel\nGaussian Representation for Incomplete CT Reconstruction (GRCT) without the\nusage of any neural networks or full-dose CT data. Specifically, we model the\n3D volume as a set of learnable Gaussians, which are optimized directly from\nthe incomplete sinogram. Our method can be applied to multiple views and angles\nwithout changing the architecture. Additionally, we propose a differentiable\nFast CT Reconstruction method for efficient clinical usage. Extensive\nexperiments on multiple datasets and settings demonstrate significant\nimprovements in reconstruction quality metrics and high efficiency. We plan to\nrelease our code as open-source.\n", "link": "http://arxiv.org/abs/2411.04844v1", "date": "2024-11-07", "relevancy": 2.983, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6323}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.588}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Gaussian%20Representation%20for%20Incomplete%20CT%20Reconstruction&body=Title%3A%20Differentiable%20Gaussian%20Representation%20for%20Incomplete%20CT%20Reconstruction%0AAuthor%3A%20Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu%0AAbstract%3A%20%20%20Incomplete%20Computed%20Tomography%20%28CT%29%20benefits%20patients%20by%20reducing%20radiation%0Aexposure.%20However%2C%20reconstructing%20high-fidelity%20images%20from%20limited%20views%20or%0Aangles%20remains%20challenging%20due%20to%20the%20ill-posed%20nature%20of%20the%20problem.%20Deep%0ALearning%20Reconstruction%20%28DLR%29%20methods%20have%20shown%20promise%20in%20enhancing%20image%0Aquality%2C%20but%20the%20paradox%20between%20training%20data%20diversity%20and%20high%0Ageneralization%20ability%20remains%20unsolved.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AGaussian%20Representation%20for%20Incomplete%20CT%20Reconstruction%20%28GRCT%29%20without%20the%0Ausage%20of%20any%20neural%20networks%20or%20full-dose%20CT%20data.%20Specifically%2C%20we%20model%20the%0A3D%20volume%20as%20a%20set%20of%20learnable%20Gaussians%2C%20which%20are%20optimized%20directly%20from%0Athe%20incomplete%20sinogram.%20Our%20method%20can%20be%20applied%20to%20multiple%20views%20and%20angles%0Awithout%20changing%20the%20architecture.%20Additionally%2C%20we%20propose%20a%20differentiable%0AFast%20CT%20Reconstruction%20method%20for%20efficient%20clinical%20usage.%20Extensive%0Aexperiments%20on%20multiple%20datasets%20and%20settings%20demonstrate%20significant%0Aimprovements%20in%20reconstruction%20quality%20metrics%20and%20high%20efficiency.%20We%20plan%20to%0Arelease%20our%20code%20as%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Gaussian%2520Representation%2520for%2520Incomplete%2520CT%2520Reconstruction%26entry.906535625%3DShaokai%2520Wu%2520and%2520Yuxiang%2520Lu%2520and%2520Wei%2520Ji%2520and%2520Suizhi%2520Huang%2520and%2520Fengyu%2520Yang%2520and%2520Shalayiding%2520Sirejiding%2520and%2520Qichen%2520He%2520and%2520Jing%2520Tong%2520and%2520Yanbiao%2520Ji%2520and%2520Yue%2520Ding%2520and%2520Hongtao%2520Lu%26entry.1292438233%3D%2520%2520Incomplete%2520Computed%2520Tomography%2520%2528CT%2529%2520benefits%2520patients%2520by%2520reducing%2520radiation%250Aexposure.%2520However%252C%2520reconstructing%2520high-fidelity%2520images%2520from%2520limited%2520views%2520or%250Aangles%2520remains%2520challenging%2520due%2520to%2520the%2520ill-posed%2520nature%2520of%2520the%2520problem.%2520Deep%250ALearning%2520Reconstruction%2520%2528DLR%2529%2520methods%2520have%2520shown%2520promise%2520in%2520enhancing%2520image%250Aquality%252C%2520but%2520the%2520paradox%2520between%2520training%2520data%2520diversity%2520and%2520high%250Ageneralization%2520ability%2520remains%2520unsolved.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250AGaussian%2520Representation%2520for%2520Incomplete%2520CT%2520Reconstruction%2520%2528GRCT%2529%2520without%2520the%250Ausage%2520of%2520any%2520neural%2520networks%2520or%2520full-dose%2520CT%2520data.%2520Specifically%252C%2520we%2520model%2520the%250A3D%2520volume%2520as%2520a%2520set%2520of%2520learnable%2520Gaussians%252C%2520which%2520are%2520optimized%2520directly%2520from%250Athe%2520incomplete%2520sinogram.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520multiple%2520views%2520and%2520angles%250Awithout%2520changing%2520the%2520architecture.%2520Additionally%252C%2520we%2520propose%2520a%2520differentiable%250AFast%2520CT%2520Reconstruction%2520method%2520for%2520efficient%2520clinical%2520usage.%2520Extensive%250Aexperiments%2520on%2520multiple%2520datasets%2520and%2520settings%2520demonstrate%2520significant%250Aimprovements%2520in%2520reconstruction%2520quality%2520metrics%2520and%2520high%2520efficiency.%2520We%2520plan%2520to%250Arelease%2520our%2520code%2520as%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Gaussian%20Representation%20for%20Incomplete%20CT%20Reconstruction&entry.906535625=Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu&entry.1292438233=%20%20Incomplete%20Computed%20Tomography%20%28CT%29%20benefits%20patients%20by%20reducing%20radiation%0Aexposure.%20However%2C%20reconstructing%20high-fidelity%20images%20from%20limited%20views%20or%0Aangles%20remains%20challenging%20due%20to%20the%20ill-posed%20nature%20of%20the%20problem.%20Deep%0ALearning%20Reconstruction%20%28DLR%29%20methods%20have%20shown%20promise%20in%20enhancing%20image%0Aquality%2C%20but%20the%20paradox%20between%20training%20data%20diversity%20and%20high%0Ageneralization%20ability%20remains%20unsolved.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0AGaussian%20Representation%20for%20Incomplete%20CT%20Reconstruction%20%28GRCT%29%20without%20the%0Ausage%20of%20any%20neural%20networks%20or%20full-dose%20CT%20data.%20Specifically%2C%20we%20model%20the%0A3D%20volume%20as%20a%20set%20of%20learnable%20Gaussians%2C%20which%20are%20optimized%20directly%20from%0Athe%20incomplete%20sinogram.%20Our%20method%20can%20be%20applied%20to%20multiple%20views%20and%20angles%0Awithout%20changing%20the%20architecture.%20Additionally%2C%20we%20propose%20a%20differentiable%0AFast%20CT%20Reconstruction%20method%20for%20efficient%20clinical%20usage.%20Extensive%0Aexperiments%20on%20multiple%20datasets%20and%20settings%20demonstrate%20significant%0Aimprovements%20in%20reconstruction%20quality%20metrics%20and%20high%20efficiency.%20We%20plan%20to%0Arelease%20our%20code%20as%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04844v1&entry.124074799=Read"},
{"title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot\n  Planning", "author": "Gaoyue Zhou and Hengkai Pan and Yann LeCun and Lerrel Pinto", "abstract": "  The ability to predict future outcomes given control actions is fundamental\nfor physical reasoning. However, such predictive models, often called world\nmodels, have proven challenging to learn and are typically developed for\ntask-specific solutions with online policy learning. We argue that the true\npotential of world models lies in their ability to reason and plan across\ndiverse problems using only passive data. Concretely, we require world models\nto have the following three properties: 1) be trainable on offline,\npre-collected trajectories, 2) support test-time behavior optimization, and 3)\nfacilitate task-agnostic reasoning. To realize this, we present DINO World\nModel (DINO-WM), a new method to model visual dynamics without reconstructing\nthe visual world. DINO-WM leverages spatial patch features pre-trained with\nDINOv2, enabling it to learn from offline behavioral trajectories by predicting\nfuture patch features. This design allows DINO-WM to achieve observational\ngoals through action sequence optimization, facilitating task-agnostic behavior\nplanning by treating desired goal patch features as prediction targets. We\nevaluate DINO-WM across various domains, including maze navigation, tabletop\npushing, and particle manipulation. Our experiments demonstrate that DINO-WM\ncan generate zero-shot behavioral solutions at test time without relying on\nexpert demonstrations, reward modeling, or pre-learned inverse models. Notably,\nDINO-WM exhibits strong generalization capabilities compared to prior\nstate-of-the-art work, adapting to diverse task families such as arbitrarily\nconfigured mazes, push manipulation with varied object shapes, and\nmulti-particle scenarios.\n", "link": "http://arxiv.org/abs/2411.04983v1", "date": "2024-11-07", "relevancy": 2.9452, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-WM%3A%20World%20Models%20on%20Pre-trained%20Visual%20Features%20enable%20Zero-shot%0A%20%20Planning&body=Title%3A%20DINO-WM%3A%20World%20Models%20on%20Pre-trained%20Visual%20Features%20enable%20Zero-shot%0A%20%20Planning%0AAuthor%3A%20Gaoyue%20Zhou%20and%20Hengkai%20Pan%20and%20Yann%20LeCun%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20The%20ability%20to%20predict%20future%20outcomes%20given%20control%20actions%20is%20fundamental%0Afor%20physical%20reasoning.%20However%2C%20such%20predictive%20models%2C%20often%20called%20world%0Amodels%2C%20have%20proven%20challenging%20to%20learn%20and%20are%20typically%20developed%20for%0Atask-specific%20solutions%20with%20online%20policy%20learning.%20We%20argue%20that%20the%20true%0Apotential%20of%20world%20models%20lies%20in%20their%20ability%20to%20reason%20and%20plan%20across%0Adiverse%20problems%20using%20only%20passive%20data.%20Concretely%2C%20we%20require%20world%20models%0Ato%20have%20the%20following%20three%20properties%3A%201%29%20be%20trainable%20on%20offline%2C%0Apre-collected%20trajectories%2C%202%29%20support%20test-time%20behavior%20optimization%2C%20and%203%29%0Afacilitate%20task-agnostic%20reasoning.%20To%20realize%20this%2C%20we%20present%20DINO%20World%0AModel%20%28DINO-WM%29%2C%20a%20new%20method%20to%20model%20visual%20dynamics%20without%20reconstructing%0Athe%20visual%20world.%20DINO-WM%20leverages%20spatial%20patch%20features%20pre-trained%20with%0ADINOv2%2C%20enabling%20it%20to%20learn%20from%20offline%20behavioral%20trajectories%20by%20predicting%0Afuture%20patch%20features.%20This%20design%20allows%20DINO-WM%20to%20achieve%20observational%0Agoals%20through%20action%20sequence%20optimization%2C%20facilitating%20task-agnostic%20behavior%0Aplanning%20by%20treating%20desired%20goal%20patch%20features%20as%20prediction%20targets.%20We%0Aevaluate%20DINO-WM%20across%20various%20domains%2C%20including%20maze%20navigation%2C%20tabletop%0Apushing%2C%20and%20particle%20manipulation.%20Our%20experiments%20demonstrate%20that%20DINO-WM%0Acan%20generate%20zero-shot%20behavioral%20solutions%20at%20test%20time%20without%20relying%20on%0Aexpert%20demonstrations%2C%20reward%20modeling%2C%20or%20pre-learned%20inverse%20models.%20Notably%2C%0ADINO-WM%20exhibits%20strong%20generalization%20capabilities%20compared%20to%20prior%0Astate-of-the-art%20work%2C%20adapting%20to%20diverse%20task%20families%20such%20as%20arbitrarily%0Aconfigured%20mazes%2C%20push%20manipulation%20with%20varied%20object%20shapes%2C%20and%0Amulti-particle%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-WM%253A%2520World%2520Models%2520on%2520Pre-trained%2520Visual%2520Features%2520enable%2520Zero-shot%250A%2520%2520Planning%26entry.906535625%3DGaoyue%2520Zhou%2520and%2520Hengkai%2520Pan%2520and%2520Yann%2520LeCun%2520and%2520Lerrel%2520Pinto%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520predict%2520future%2520outcomes%2520given%2520control%2520actions%2520is%2520fundamental%250Afor%2520physical%2520reasoning.%2520However%252C%2520such%2520predictive%2520models%252C%2520often%2520called%2520world%250Amodels%252C%2520have%2520proven%2520challenging%2520to%2520learn%2520and%2520are%2520typically%2520developed%2520for%250Atask-specific%2520solutions%2520with%2520online%2520policy%2520learning.%2520We%2520argue%2520that%2520the%2520true%250Apotential%2520of%2520world%2520models%2520lies%2520in%2520their%2520ability%2520to%2520reason%2520and%2520plan%2520across%250Adiverse%2520problems%2520using%2520only%2520passive%2520data.%2520Concretely%252C%2520we%2520require%2520world%2520models%250Ato%2520have%2520the%2520following%2520three%2520properties%253A%25201%2529%2520be%2520trainable%2520on%2520offline%252C%250Apre-collected%2520trajectories%252C%25202%2529%2520support%2520test-time%2520behavior%2520optimization%252C%2520and%25203%2529%250Afacilitate%2520task-agnostic%2520reasoning.%2520To%2520realize%2520this%252C%2520we%2520present%2520DINO%2520World%250AModel%2520%2528DINO-WM%2529%252C%2520a%2520new%2520method%2520to%2520model%2520visual%2520dynamics%2520without%2520reconstructing%250Athe%2520visual%2520world.%2520DINO-WM%2520leverages%2520spatial%2520patch%2520features%2520pre-trained%2520with%250ADINOv2%252C%2520enabling%2520it%2520to%2520learn%2520from%2520offline%2520behavioral%2520trajectories%2520by%2520predicting%250Afuture%2520patch%2520features.%2520This%2520design%2520allows%2520DINO-WM%2520to%2520achieve%2520observational%250Agoals%2520through%2520action%2520sequence%2520optimization%252C%2520facilitating%2520task-agnostic%2520behavior%250Aplanning%2520by%2520treating%2520desired%2520goal%2520patch%2520features%2520as%2520prediction%2520targets.%2520We%250Aevaluate%2520DINO-WM%2520across%2520various%2520domains%252C%2520including%2520maze%2520navigation%252C%2520tabletop%250Apushing%252C%2520and%2520particle%2520manipulation.%2520Our%2520experiments%2520demonstrate%2520that%2520DINO-WM%250Acan%2520generate%2520zero-shot%2520behavioral%2520solutions%2520at%2520test%2520time%2520without%2520relying%2520on%250Aexpert%2520demonstrations%252C%2520reward%2520modeling%252C%2520or%2520pre-learned%2520inverse%2520models.%2520Notably%252C%250ADINO-WM%2520exhibits%2520strong%2520generalization%2520capabilities%2520compared%2520to%2520prior%250Astate-of-the-art%2520work%252C%2520adapting%2520to%2520diverse%2520task%2520families%2520such%2520as%2520arbitrarily%250Aconfigured%2520mazes%252C%2520push%2520manipulation%2520with%2520varied%2520object%2520shapes%252C%2520and%250Amulti-particle%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-WM%3A%20World%20Models%20on%20Pre-trained%20Visual%20Features%20enable%20Zero-shot%0A%20%20Planning&entry.906535625=Gaoyue%20Zhou%20and%20Hengkai%20Pan%20and%20Yann%20LeCun%20and%20Lerrel%20Pinto&entry.1292438233=%20%20The%20ability%20to%20predict%20future%20outcomes%20given%20control%20actions%20is%20fundamental%0Afor%20physical%20reasoning.%20However%2C%20such%20predictive%20models%2C%20often%20called%20world%0Amodels%2C%20have%20proven%20challenging%20to%20learn%20and%20are%20typically%20developed%20for%0Atask-specific%20solutions%20with%20online%20policy%20learning.%20We%20argue%20that%20the%20true%0Apotential%20of%20world%20models%20lies%20in%20their%20ability%20to%20reason%20and%20plan%20across%0Adiverse%20problems%20using%20only%20passive%20data.%20Concretely%2C%20we%20require%20world%20models%0Ato%20have%20the%20following%20three%20properties%3A%201%29%20be%20trainable%20on%20offline%2C%0Apre-collected%20trajectories%2C%202%29%20support%20test-time%20behavior%20optimization%2C%20and%203%29%0Afacilitate%20task-agnostic%20reasoning.%20To%20realize%20this%2C%20we%20present%20DINO%20World%0AModel%20%28DINO-WM%29%2C%20a%20new%20method%20to%20model%20visual%20dynamics%20without%20reconstructing%0Athe%20visual%20world.%20DINO-WM%20leverages%20spatial%20patch%20features%20pre-trained%20with%0ADINOv2%2C%20enabling%20it%20to%20learn%20from%20offline%20behavioral%20trajectories%20by%20predicting%0Afuture%20patch%20features.%20This%20design%20allows%20DINO-WM%20to%20achieve%20observational%0Agoals%20through%20action%20sequence%20optimization%2C%20facilitating%20task-agnostic%20behavior%0Aplanning%20by%20treating%20desired%20goal%20patch%20features%20as%20prediction%20targets.%20We%0Aevaluate%20DINO-WM%20across%20various%20domains%2C%20including%20maze%20navigation%2C%20tabletop%0Apushing%2C%20and%20particle%20manipulation.%20Our%20experiments%20demonstrate%20that%20DINO-WM%0Acan%20generate%20zero-shot%20behavioral%20solutions%20at%20test%20time%20without%20relying%20on%0Aexpert%20demonstrations%2C%20reward%20modeling%2C%20or%20pre-learned%20inverse%20models.%20Notably%2C%0ADINO-WM%20exhibits%20strong%20generalization%20capabilities%20compared%20to%20prior%0Astate-of-the-art%20work%2C%20adapting%20to%20diverse%20task%20families%20such%20as%20arbitrarily%0Aconfigured%20mazes%2C%20push%20manipulation%20with%20varied%20object%20shapes%2C%20and%0Amulti-particle%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04983v1&entry.124074799=Read"},
{"title": "GANESH: Generalizable NeRF for Lensless Imaging", "author": "Rakesh Raj Madavan and Akshat Kaimal and Badhrinarayanan K V and Vinayak Gupta and Rohit Choudhary and Chandrakala Shanmuganathan and Kaushik Mitra", "abstract": "  Lensless imaging offers a significant opportunity to develop ultra-compact\ncameras by removing the conventional bulky lens system. However, without a\nfocusing element, the sensor's output is no longer a direct image but a complex\nmultiplexed scene representation. Traditional methods have attempted to address\nthis challenge by employing learnable inversions and refinement models, but\nthese methods are primarily designed for 2D reconstruction and do not\ngeneralize well to 3D reconstruction. We introduce GANESH, a novel framework\ndesigned to enable simultaneous refinement and novel view synthesis from\nmulti-view lensless images. Unlike existing methods that require scene-specific\ntraining, our approach supports on-the-fly inference without retraining on each\nscene. Moreover, our framework allows us to tune our model to specific scenes,\nenhancing the rendering and refinement quality. To facilitate research in this\narea, we also present the first multi-view lensless dataset, LenslessScenes.\nExtensive experiments demonstrate that our method outperforms current\napproaches in reconstruction accuracy and refinement quality. Code and video\nresults are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/\n", "link": "http://arxiv.org/abs/2411.04810v1", "date": "2024-11-07", "relevancy": 2.9325, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6144}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5743}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANESH%3A%20Generalizable%20NeRF%20for%20Lensless%20Imaging&body=Title%3A%20GANESH%3A%20Generalizable%20NeRF%20for%20Lensless%20Imaging%0AAuthor%3A%20Rakesh%20Raj%20Madavan%20and%20Akshat%20Kaimal%20and%20Badhrinarayanan%20K%20V%20and%20Vinayak%20Gupta%20and%20Rohit%20Choudhary%20and%20Chandrakala%20Shanmuganathan%20and%20Kaushik%20Mitra%0AAbstract%3A%20%20%20Lensless%20imaging%20offers%20a%20significant%20opportunity%20to%20develop%20ultra-compact%0Acameras%20by%20removing%20the%20conventional%20bulky%20lens%20system.%20However%2C%20without%20a%0Afocusing%20element%2C%20the%20sensor%27s%20output%20is%20no%20longer%20a%20direct%20image%20but%20a%20complex%0Amultiplexed%20scene%20representation.%20Traditional%20methods%20have%20attempted%20to%20address%0Athis%20challenge%20by%20employing%20learnable%20inversions%20and%20refinement%20models%2C%20but%0Athese%20methods%20are%20primarily%20designed%20for%202D%20reconstruction%20and%20do%20not%0Ageneralize%20well%20to%203D%20reconstruction.%20We%20introduce%20GANESH%2C%20a%20novel%20framework%0Adesigned%20to%20enable%20simultaneous%20refinement%20and%20novel%20view%20synthesis%20from%0Amulti-view%20lensless%20images.%20Unlike%20existing%20methods%20that%20require%20scene-specific%0Atraining%2C%20our%20approach%20supports%20on-the-fly%20inference%20without%20retraining%20on%20each%0Ascene.%20Moreover%2C%20our%20framework%20allows%20us%20to%20tune%20our%20model%20to%20specific%20scenes%2C%0Aenhancing%20the%20rendering%20and%20refinement%20quality.%20To%20facilitate%20research%20in%20this%0Aarea%2C%20we%20also%20present%20the%20first%20multi-view%20lensless%20dataset%2C%20LenslessScenes.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20current%0Aapproaches%20in%20reconstruction%20accuracy%20and%20refinement%20quality.%20Code%20and%20video%0Aresults%20are%20available%20at%20https%3A//rakesh-123-cryp.github.io/Rakesh.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANESH%253A%2520Generalizable%2520NeRF%2520for%2520Lensless%2520Imaging%26entry.906535625%3DRakesh%2520Raj%2520Madavan%2520and%2520Akshat%2520Kaimal%2520and%2520Badhrinarayanan%2520K%2520V%2520and%2520Vinayak%2520Gupta%2520and%2520Rohit%2520Choudhary%2520and%2520Chandrakala%2520Shanmuganathan%2520and%2520Kaushik%2520Mitra%26entry.1292438233%3D%2520%2520Lensless%2520imaging%2520offers%2520a%2520significant%2520opportunity%2520to%2520develop%2520ultra-compact%250Acameras%2520by%2520removing%2520the%2520conventional%2520bulky%2520lens%2520system.%2520However%252C%2520without%2520a%250Afocusing%2520element%252C%2520the%2520sensor%2527s%2520output%2520is%2520no%2520longer%2520a%2520direct%2520image%2520but%2520a%2520complex%250Amultiplexed%2520scene%2520representation.%2520Traditional%2520methods%2520have%2520attempted%2520to%2520address%250Athis%2520challenge%2520by%2520employing%2520learnable%2520inversions%2520and%2520refinement%2520models%252C%2520but%250Athese%2520methods%2520are%2520primarily%2520designed%2520for%25202D%2520reconstruction%2520and%2520do%2520not%250Ageneralize%2520well%2520to%25203D%2520reconstruction.%2520We%2520introduce%2520GANESH%252C%2520a%2520novel%2520framework%250Adesigned%2520to%2520enable%2520simultaneous%2520refinement%2520and%2520novel%2520view%2520synthesis%2520from%250Amulti-view%2520lensless%2520images.%2520Unlike%2520existing%2520methods%2520that%2520require%2520scene-specific%250Atraining%252C%2520our%2520approach%2520supports%2520on-the-fly%2520inference%2520without%2520retraining%2520on%2520each%250Ascene.%2520Moreover%252C%2520our%2520framework%2520allows%2520us%2520to%2520tune%2520our%2520model%2520to%2520specific%2520scenes%252C%250Aenhancing%2520the%2520rendering%2520and%2520refinement%2520quality.%2520To%2520facilitate%2520research%2520in%2520this%250Aarea%252C%2520we%2520also%2520present%2520the%2520first%2520multi-view%2520lensless%2520dataset%252C%2520LenslessScenes.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520current%250Aapproaches%2520in%2520reconstruction%2520accuracy%2520and%2520refinement%2520quality.%2520Code%2520and%2520video%250Aresults%2520are%2520available%2520at%2520https%253A//rakesh-123-cryp.github.io/Rakesh.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANESH%3A%20Generalizable%20NeRF%20for%20Lensless%20Imaging&entry.906535625=Rakesh%20Raj%20Madavan%20and%20Akshat%20Kaimal%20and%20Badhrinarayanan%20K%20V%20and%20Vinayak%20Gupta%20and%20Rohit%20Choudhary%20and%20Chandrakala%20Shanmuganathan%20and%20Kaushik%20Mitra&entry.1292438233=%20%20Lensless%20imaging%20offers%20a%20significant%20opportunity%20to%20develop%20ultra-compact%0Acameras%20by%20removing%20the%20conventional%20bulky%20lens%20system.%20However%2C%20without%20a%0Afocusing%20element%2C%20the%20sensor%27s%20output%20is%20no%20longer%20a%20direct%20image%20but%20a%20complex%0Amultiplexed%20scene%20representation.%20Traditional%20methods%20have%20attempted%20to%20address%0Athis%20challenge%20by%20employing%20learnable%20inversions%20and%20refinement%20models%2C%20but%0Athese%20methods%20are%20primarily%20designed%20for%202D%20reconstruction%20and%20do%20not%0Ageneralize%20well%20to%203D%20reconstruction.%20We%20introduce%20GANESH%2C%20a%20novel%20framework%0Adesigned%20to%20enable%20simultaneous%20refinement%20and%20novel%20view%20synthesis%20from%0Amulti-view%20lensless%20images.%20Unlike%20existing%20methods%20that%20require%20scene-specific%0Atraining%2C%20our%20approach%20supports%20on-the-fly%20inference%20without%20retraining%20on%20each%0Ascene.%20Moreover%2C%20our%20framework%20allows%20us%20to%20tune%20our%20model%20to%20specific%20scenes%2C%0Aenhancing%20the%20rendering%20and%20refinement%20quality.%20To%20facilitate%20research%20in%20this%0Aarea%2C%20we%20also%20present%20the%20first%20multi-view%20lensless%20dataset%2C%20LenslessScenes.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20current%0Aapproaches%20in%20reconstruction%20accuracy%20and%20refinement%20quality.%20Code%20and%20video%0Aresults%20are%20available%20at%20https%3A//rakesh-123-cryp.github.io/Rakesh.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04810v1&entry.124074799=Read"},
{"title": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language\n  Models", "author": "Jonathan Fhima and Elad Ben Avraham and Oren Nuriel and Yair Kittenplon and Roy Ganz and Aviad Aberdam and Ron Litman", "abstract": "  Vision-Language (VL) models have garnered considerable research interest;\nhowever, they still face challenges in effectively handling text within images.\nTo address this limitation, researchers have developed two approaches. The\nfirst method involves utilizing external Optical Character Recognition (OCR)\ntools to extract textual information from images, which is then prepended to\nother textual inputs. The second strategy focuses on employing extremely\nhigh-resolution images to improve text recognition capabilities. In this paper,\nwe focus on enhancing the first strategy by introducing a novel method, named\nTAP-VL, which treats OCR information as a distinct modality and seamlessly\nintegrates it into any VL model. TAP-VL employs a lightweight transformer-based\nOCR module to receive OCR with layout information, compressing it into a short\nfixed-length sequence for input into the LLM. Initially, we conduct\nmodel-agnostic pretraining of the OCR module on unlabeled documents, followed\nby its integration into any VL architecture through brief fine-tuning.\nExtensive experiments demonstrate consistent performance improvements when\napplying TAP-VL to top-performing VL models, across scene-text and\ndocument-based VL benchmarks.\n", "link": "http://arxiv.org/abs/2411.04642v1", "date": "2024-11-07", "relevancy": 2.9196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6269}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAP-VL%3A%20Text%20Layout-Aware%20Pre-training%20for%20Enriched%20Vision-Language%0A%20%20Models&body=Title%3A%20TAP-VL%3A%20Text%20Layout-Aware%20Pre-training%20for%20Enriched%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Jonathan%20Fhima%20and%20Elad%20Ben%20Avraham%20and%20Oren%20Nuriel%20and%20Yair%20Kittenplon%20and%20Roy%20Ganz%20and%20Aviad%20Aberdam%20and%20Ron%20Litman%0AAbstract%3A%20%20%20Vision-Language%20%28VL%29%20models%20have%20garnered%20considerable%20research%20interest%3B%0Ahowever%2C%20they%20still%20face%20challenges%20in%20effectively%20handling%20text%20within%20images.%0ATo%20address%20this%20limitation%2C%20researchers%20have%20developed%20two%20approaches.%20The%0Afirst%20method%20involves%20utilizing%20external%20Optical%20Character%20Recognition%20%28OCR%29%0Atools%20to%20extract%20textual%20information%20from%20images%2C%20which%20is%20then%20prepended%20to%0Aother%20textual%20inputs.%20The%20second%20strategy%20focuses%20on%20employing%20extremely%0Ahigh-resolution%20images%20to%20improve%20text%20recognition%20capabilities.%20In%20this%20paper%2C%0Awe%20focus%20on%20enhancing%20the%20first%20strategy%20by%20introducing%20a%20novel%20method%2C%20named%0ATAP-VL%2C%20which%20treats%20OCR%20information%20as%20a%20distinct%20modality%20and%20seamlessly%0Aintegrates%20it%20into%20any%20VL%20model.%20TAP-VL%20employs%20a%20lightweight%20transformer-based%0AOCR%20module%20to%20receive%20OCR%20with%20layout%20information%2C%20compressing%20it%20into%20a%20short%0Afixed-length%20sequence%20for%20input%20into%20the%20LLM.%20Initially%2C%20we%20conduct%0Amodel-agnostic%20pretraining%20of%20the%20OCR%20module%20on%20unlabeled%20documents%2C%20followed%0Aby%20its%20integration%20into%20any%20VL%20architecture%20through%20brief%20fine-tuning.%0AExtensive%20experiments%20demonstrate%20consistent%20performance%20improvements%20when%0Aapplying%20TAP-VL%20to%20top-performing%20VL%20models%2C%20across%20scene-text%20and%0Adocument-based%20VL%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAP-VL%253A%2520Text%2520Layout-Aware%2520Pre-training%2520for%2520Enriched%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DJonathan%2520Fhima%2520and%2520Elad%2520Ben%2520Avraham%2520and%2520Oren%2520Nuriel%2520and%2520Yair%2520Kittenplon%2520and%2520Roy%2520Ganz%2520and%2520Aviad%2520Aberdam%2520and%2520Ron%2520Litman%26entry.1292438233%3D%2520%2520Vision-Language%2520%2528VL%2529%2520models%2520have%2520garnered%2520considerable%2520research%2520interest%253B%250Ahowever%252C%2520they%2520still%2520face%2520challenges%2520in%2520effectively%2520handling%2520text%2520within%2520images.%250ATo%2520address%2520this%2520limitation%252C%2520researchers%2520have%2520developed%2520two%2520approaches.%2520The%250Afirst%2520method%2520involves%2520utilizing%2520external%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%250Atools%2520to%2520extract%2520textual%2520information%2520from%2520images%252C%2520which%2520is%2520then%2520prepended%2520to%250Aother%2520textual%2520inputs.%2520The%2520second%2520strategy%2520focuses%2520on%2520employing%2520extremely%250Ahigh-resolution%2520images%2520to%2520improve%2520text%2520recognition%2520capabilities.%2520In%2520this%2520paper%252C%250Awe%2520focus%2520on%2520enhancing%2520the%2520first%2520strategy%2520by%2520introducing%2520a%2520novel%2520method%252C%2520named%250ATAP-VL%252C%2520which%2520treats%2520OCR%2520information%2520as%2520a%2520distinct%2520modality%2520and%2520seamlessly%250Aintegrates%2520it%2520into%2520any%2520VL%2520model.%2520TAP-VL%2520employs%2520a%2520lightweight%2520transformer-based%250AOCR%2520module%2520to%2520receive%2520OCR%2520with%2520layout%2520information%252C%2520compressing%2520it%2520into%2520a%2520short%250Afixed-length%2520sequence%2520for%2520input%2520into%2520the%2520LLM.%2520Initially%252C%2520we%2520conduct%250Amodel-agnostic%2520pretraining%2520of%2520the%2520OCR%2520module%2520on%2520unlabeled%2520documents%252C%2520followed%250Aby%2520its%2520integration%2520into%2520any%2520VL%2520architecture%2520through%2520brief%2520fine-tuning.%250AExtensive%2520experiments%2520demonstrate%2520consistent%2520performance%2520improvements%2520when%250Aapplying%2520TAP-VL%2520to%2520top-performing%2520VL%2520models%252C%2520across%2520scene-text%2520and%250Adocument-based%2520VL%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAP-VL%3A%20Text%20Layout-Aware%20Pre-training%20for%20Enriched%20Vision-Language%0A%20%20Models&entry.906535625=Jonathan%20Fhima%20and%20Elad%20Ben%20Avraham%20and%20Oren%20Nuriel%20and%20Yair%20Kittenplon%20and%20Roy%20Ganz%20and%20Aviad%20Aberdam%20and%20Ron%20Litman&entry.1292438233=%20%20Vision-Language%20%28VL%29%20models%20have%20garnered%20considerable%20research%20interest%3B%0Ahowever%2C%20they%20still%20face%20challenges%20in%20effectively%20handling%20text%20within%20images.%0ATo%20address%20this%20limitation%2C%20researchers%20have%20developed%20two%20approaches.%20The%0Afirst%20method%20involves%20utilizing%20external%20Optical%20Character%20Recognition%20%28OCR%29%0Atools%20to%20extract%20textual%20information%20from%20images%2C%20which%20is%20then%20prepended%20to%0Aother%20textual%20inputs.%20The%20second%20strategy%20focuses%20on%20employing%20extremely%0Ahigh-resolution%20images%20to%20improve%20text%20recognition%20capabilities.%20In%20this%20paper%2C%0Awe%20focus%20on%20enhancing%20the%20first%20strategy%20by%20introducing%20a%20novel%20method%2C%20named%0ATAP-VL%2C%20which%20treats%20OCR%20information%20as%20a%20distinct%20modality%20and%20seamlessly%0Aintegrates%20it%20into%20any%20VL%20model.%20TAP-VL%20employs%20a%20lightweight%20transformer-based%0AOCR%20module%20to%20receive%20OCR%20with%20layout%20information%2C%20compressing%20it%20into%20a%20short%0Afixed-length%20sequence%20for%20input%20into%20the%20LLM.%20Initially%2C%20we%20conduct%0Amodel-agnostic%20pretraining%20of%20the%20OCR%20module%20on%20unlabeled%20documents%2C%20followed%0Aby%20its%20integration%20into%20any%20VL%20architecture%20through%20brief%20fine-tuning.%0AExtensive%20experiments%20demonstrate%20consistent%20performance%20improvements%20when%0Aapplying%20TAP-VL%20to%20top-performing%20VL%20models%2C%20across%20scene-text%20and%0Adocument-based%20VL%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04642v1&entry.124074799=Read"},
{"title": "LoFi: Scalable Local Image Reconstruction with Implicit Neural\n  Representation", "author": "AmirEhsan Khorashadizadeh and Tob\u00edas I. Liaudat and Tianlin Liu and Jason D. McEwen and Ivan Dokmani\u0107", "abstract": "  Neural fields or implicit neural representations (INRs) have attracted\nsignificant attention in machine learning and signal processing due to their\nefficient continuous representation of images and 3D volumes. In this work, we\nbuild on INRs and introduce a coordinate-based local processing framework for\nsolving imaging inverse problems, termed LoFi (Local Field). Unlike\nconventional methods for image reconstruction, LoFi processes local information\nat each coordinate \\textit{separately} by multi-layer perceptrons (MLPs),\nrecovering the object at that specific coordinate. Similar to INRs, LoFi can\nrecover images at any continuous coordinate, enabling image reconstruction at\nmultiple resolutions. With comparable or better performance than standard CNNs\nfor image reconstruction, LoFi achieves excellent generalization to\nout-of-distribution data and memory usage almost independent of image\nresolution. Remarkably, training on $1024 \\times 1024$ images requires just 3GB\nof memory -- over 20 times less than the memory typically needed by standard\nCNNs. Additionally, LoFi's local design allows it to train on extremely small\ndatasets with less than 10 samples, without overfitting or the need for\nregularization or early stopping. Finally, we use LoFi as a denoising prior in\na plug-and-play framework for solving general inverse problems to benefit from\nits continuous image representation and strong generalization. Although trained\non low-resolution images, LoFi can be used as a low-dimensional prior to solve\ninverse problems at any resolution. We validate our framework across a variety\nof imaging modalities, from low-dose computed tomography to radio\ninterferometric imaging.\n", "link": "http://arxiv.org/abs/2411.04995v1", "date": "2024-11-07", "relevancy": 2.8743, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5804}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5744}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoFi%3A%20Scalable%20Local%20Image%20Reconstruction%20with%20Implicit%20Neural%0A%20%20Representation&body=Title%3A%20LoFi%3A%20Scalable%20Local%20Image%20Reconstruction%20with%20Implicit%20Neural%0A%20%20Representation%0AAuthor%3A%20AmirEhsan%20Khorashadizadeh%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Tianlin%20Liu%20and%20Jason%20D.%20McEwen%20and%20Ivan%20Dokmani%C4%87%0AAbstract%3A%20%20%20Neural%20fields%20or%20implicit%20neural%20representations%20%28INRs%29%20have%20attracted%0Asignificant%20attention%20in%20machine%20learning%20and%20signal%20processing%20due%20to%20their%0Aefficient%20continuous%20representation%20of%20images%20and%203D%20volumes.%20In%20this%20work%2C%20we%0Abuild%20on%20INRs%20and%20introduce%20a%20coordinate-based%20local%20processing%20framework%20for%0Asolving%20imaging%20inverse%20problems%2C%20termed%20LoFi%20%28Local%20Field%29.%20Unlike%0Aconventional%20methods%20for%20image%20reconstruction%2C%20LoFi%20processes%20local%20information%0Aat%20each%20coordinate%20%5Ctextit%7Bseparately%7D%20by%20multi-layer%20perceptrons%20%28MLPs%29%2C%0Arecovering%20the%20object%20at%20that%20specific%20coordinate.%20Similar%20to%20INRs%2C%20LoFi%20can%0Arecover%20images%20at%20any%20continuous%20coordinate%2C%20enabling%20image%20reconstruction%20at%0Amultiple%20resolutions.%20With%20comparable%20or%20better%20performance%20than%20standard%20CNNs%0Afor%20image%20reconstruction%2C%20LoFi%20achieves%20excellent%20generalization%20to%0Aout-of-distribution%20data%20and%20memory%20usage%20almost%20independent%20of%20image%0Aresolution.%20Remarkably%2C%20training%20on%20%241024%20%5Ctimes%201024%24%20images%20requires%20just%203GB%0Aof%20memory%20--%20over%2020%20times%20less%20than%20the%20memory%20typically%20needed%20by%20standard%0ACNNs.%20Additionally%2C%20LoFi%27s%20local%20design%20allows%20it%20to%20train%20on%20extremely%20small%0Adatasets%20with%20less%20than%2010%20samples%2C%20without%20overfitting%20or%20the%20need%20for%0Aregularization%20or%20early%20stopping.%20Finally%2C%20we%20use%20LoFi%20as%20a%20denoising%20prior%20in%0Aa%20plug-and-play%20framework%20for%20solving%20general%20inverse%20problems%20to%20benefit%20from%0Aits%20continuous%20image%20representation%20and%20strong%20generalization.%20Although%20trained%0Aon%20low-resolution%20images%2C%20LoFi%20can%20be%20used%20as%20a%20low-dimensional%20prior%20to%20solve%0Ainverse%20problems%20at%20any%20resolution.%20We%20validate%20our%20framework%20across%20a%20variety%0Aof%20imaging%20modalities%2C%20from%20low-dose%20computed%20tomography%20to%20radio%0Ainterferometric%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoFi%253A%2520Scalable%2520Local%2520Image%2520Reconstruction%2520with%2520Implicit%2520Neural%250A%2520%2520Representation%26entry.906535625%3DAmirEhsan%2520Khorashadizadeh%2520and%2520Tob%25C3%25ADas%2520I.%2520Liaudat%2520and%2520Tianlin%2520Liu%2520and%2520Jason%2520D.%2520McEwen%2520and%2520Ivan%2520Dokmani%25C4%2587%26entry.1292438233%3D%2520%2520Neural%2520fields%2520or%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520have%2520attracted%250Asignificant%2520attention%2520in%2520machine%2520learning%2520and%2520signal%2520processing%2520due%2520to%2520their%250Aefficient%2520continuous%2520representation%2520of%2520images%2520and%25203D%2520volumes.%2520In%2520this%2520work%252C%2520we%250Abuild%2520on%2520INRs%2520and%2520introduce%2520a%2520coordinate-based%2520local%2520processing%2520framework%2520for%250Asolving%2520imaging%2520inverse%2520problems%252C%2520termed%2520LoFi%2520%2528Local%2520Field%2529.%2520Unlike%250Aconventional%2520methods%2520for%2520image%2520reconstruction%252C%2520LoFi%2520processes%2520local%2520information%250Aat%2520each%2520coordinate%2520%255Ctextit%257Bseparately%257D%2520by%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%252C%250Arecovering%2520the%2520object%2520at%2520that%2520specific%2520coordinate.%2520Similar%2520to%2520INRs%252C%2520LoFi%2520can%250Arecover%2520images%2520at%2520any%2520continuous%2520coordinate%252C%2520enabling%2520image%2520reconstruction%2520at%250Amultiple%2520resolutions.%2520With%2520comparable%2520or%2520better%2520performance%2520than%2520standard%2520CNNs%250Afor%2520image%2520reconstruction%252C%2520LoFi%2520achieves%2520excellent%2520generalization%2520to%250Aout-of-distribution%2520data%2520and%2520memory%2520usage%2520almost%2520independent%2520of%2520image%250Aresolution.%2520Remarkably%252C%2520training%2520on%2520%25241024%2520%255Ctimes%25201024%2524%2520images%2520requires%2520just%25203GB%250Aof%2520memory%2520--%2520over%252020%2520times%2520less%2520than%2520the%2520memory%2520typically%2520needed%2520by%2520standard%250ACNNs.%2520Additionally%252C%2520LoFi%2527s%2520local%2520design%2520allows%2520it%2520to%2520train%2520on%2520extremely%2520small%250Adatasets%2520with%2520less%2520than%252010%2520samples%252C%2520without%2520overfitting%2520or%2520the%2520need%2520for%250Aregularization%2520or%2520early%2520stopping.%2520Finally%252C%2520we%2520use%2520LoFi%2520as%2520a%2520denoising%2520prior%2520in%250Aa%2520plug-and-play%2520framework%2520for%2520solving%2520general%2520inverse%2520problems%2520to%2520benefit%2520from%250Aits%2520continuous%2520image%2520representation%2520and%2520strong%2520generalization.%2520Although%2520trained%250Aon%2520low-resolution%2520images%252C%2520LoFi%2520can%2520be%2520used%2520as%2520a%2520low-dimensional%2520prior%2520to%2520solve%250Ainverse%2520problems%2520at%2520any%2520resolution.%2520We%2520validate%2520our%2520framework%2520across%2520a%2520variety%250Aof%2520imaging%2520modalities%252C%2520from%2520low-dose%2520computed%2520tomography%2520to%2520radio%250Ainterferometric%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoFi%3A%20Scalable%20Local%20Image%20Reconstruction%20with%20Implicit%20Neural%0A%20%20Representation&entry.906535625=AmirEhsan%20Khorashadizadeh%20and%20Tob%C3%ADas%20I.%20Liaudat%20and%20Tianlin%20Liu%20and%20Jason%20D.%20McEwen%20and%20Ivan%20Dokmani%C4%87&entry.1292438233=%20%20Neural%20fields%20or%20implicit%20neural%20representations%20%28INRs%29%20have%20attracted%0Asignificant%20attention%20in%20machine%20learning%20and%20signal%20processing%20due%20to%20their%0Aefficient%20continuous%20representation%20of%20images%20and%203D%20volumes.%20In%20this%20work%2C%20we%0Abuild%20on%20INRs%20and%20introduce%20a%20coordinate-based%20local%20processing%20framework%20for%0Asolving%20imaging%20inverse%20problems%2C%20termed%20LoFi%20%28Local%20Field%29.%20Unlike%0Aconventional%20methods%20for%20image%20reconstruction%2C%20LoFi%20processes%20local%20information%0Aat%20each%20coordinate%20%5Ctextit%7Bseparately%7D%20by%20multi-layer%20perceptrons%20%28MLPs%29%2C%0Arecovering%20the%20object%20at%20that%20specific%20coordinate.%20Similar%20to%20INRs%2C%20LoFi%20can%0Arecover%20images%20at%20any%20continuous%20coordinate%2C%20enabling%20image%20reconstruction%20at%0Amultiple%20resolutions.%20With%20comparable%20or%20better%20performance%20than%20standard%20CNNs%0Afor%20image%20reconstruction%2C%20LoFi%20achieves%20excellent%20generalization%20to%0Aout-of-distribution%20data%20and%20memory%20usage%20almost%20independent%20of%20image%0Aresolution.%20Remarkably%2C%20training%20on%20%241024%20%5Ctimes%201024%24%20images%20requires%20just%203GB%0Aof%20memory%20--%20over%2020%20times%20less%20than%20the%20memory%20typically%20needed%20by%20standard%0ACNNs.%20Additionally%2C%20LoFi%27s%20local%20design%20allows%20it%20to%20train%20on%20extremely%20small%0Adatasets%20with%20less%20than%2010%20samples%2C%20without%20overfitting%20or%20the%20need%20for%0Aregularization%20or%20early%20stopping.%20Finally%2C%20we%20use%20LoFi%20as%20a%20denoising%20prior%20in%0Aa%20plug-and-play%20framework%20for%20solving%20general%20inverse%20problems%20to%20benefit%20from%0Aits%20continuous%20image%20representation%20and%20strong%20generalization.%20Although%20trained%0Aon%20low-resolution%20images%2C%20LoFi%20can%20be%20used%20as%20a%20low-dimensional%20prior%20to%20solve%0Ainverse%20problems%20at%20any%20resolution.%20We%20validate%20our%20framework%20across%20a%20variety%0Aof%20imaging%20modalities%2C%20from%20low-dose%20computed%20tomography%20to%20radio%0Ainterferometric%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04995v1&entry.124074799=Read"},
{"title": "In the Era of Prompt Learning with Vision-Language Models", "author": "Ankit Jha", "abstract": "  Large-scale foundation models like CLIP have shown strong zero-shot\ngeneralization but struggle with domain shifts, limiting their adaptability. In\nour work, we introduce \\textsc{StyLIP}, a novel domain-agnostic prompt learning\nstrategy for Domain Generalization (DG). StyLIP disentangles visual style and\ncontent in CLIP`s vision encoder by using style projectors to learn\ndomain-specific prompt tokens and combining them with content features. Trained\ncontrastively, this approach enables seamless adaptation across domains,\noutperforming state-of-the-art methods on multiple DG benchmarks. Additionally,\nwe propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s\nfrozen vision backbone to learn domain-invariant prompts through image style\nand content features. By aligning domains in embedding space with entropy\nminimization, AD-CLIP effectively handles domain shifts, even when only target\ndomain samples are available. Lastly, we outline future work on class discovery\nusing prompt learning for semantic segmentation in remote sensing, focusing on\nidentifying novel or rare classes in unstructured environments. This paves the\nway for more adaptive and generalizable models in complex, real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2411.04892v1", "date": "2024-11-07", "relevancy": 2.8738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20the%20Era%20of%20Prompt%20Learning%20with%20Vision-Language%20Models&body=Title%3A%20In%20the%20Era%20of%20Prompt%20Learning%20with%20Vision-Language%20Models%0AAuthor%3A%20Ankit%20Jha%0AAbstract%3A%20%20%20Large-scale%20foundation%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%0Ageneralization%20but%20struggle%20with%20domain%20shifts%2C%20limiting%20their%20adaptability.%20In%0Aour%20work%2C%20we%20introduce%20%5Ctextsc%7BStyLIP%7D%2C%20a%20novel%20domain-agnostic%20prompt%20learning%0Astrategy%20for%20Domain%20Generalization%20%28DG%29.%20StyLIP%20disentangles%20visual%20style%20and%0Acontent%20in%20CLIP%60s%20vision%20encoder%20by%20using%20style%20projectors%20to%20learn%0Adomain-specific%20prompt%20tokens%20and%20combining%20them%20with%20content%20features.%20Trained%0Acontrastively%2C%20this%20approach%20enables%20seamless%20adaptation%20across%20domains%2C%0Aoutperforming%20state-of-the-art%20methods%20on%20multiple%20DG%20benchmarks.%20Additionally%2C%0Awe%20propose%20AD-CLIP%20for%20unsupervised%20domain%20adaptation%20%28DA%29%2C%20leveraging%20CLIP%60s%0Afrozen%20vision%20backbone%20to%20learn%20domain-invariant%20prompts%20through%20image%20style%0Aand%20content%20features.%20By%20aligning%20domains%20in%20embedding%20space%20with%20entropy%0Aminimization%2C%20AD-CLIP%20effectively%20handles%20domain%20shifts%2C%20even%20when%20only%20target%0Adomain%20samples%20are%20available.%20Lastly%2C%20we%20outline%20future%20work%20on%20class%20discovery%0Ausing%20prompt%20learning%20for%20semantic%20segmentation%20in%20remote%20sensing%2C%20focusing%20on%0Aidentifying%20novel%20or%20rare%20classes%20in%20unstructured%20environments.%20This%20paves%20the%0Away%20for%20more%20adaptive%20and%20generalizable%20models%20in%20complex%2C%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520the%2520Era%2520of%2520Prompt%2520Learning%2520with%2520Vision-Language%2520Models%26entry.906535625%3DAnkit%2520Jha%26entry.1292438233%3D%2520%2520Large-scale%2520foundation%2520models%2520like%2520CLIP%2520have%2520shown%2520strong%2520zero-shot%250Ageneralization%2520but%2520struggle%2520with%2520domain%2520shifts%252C%2520limiting%2520their%2520adaptability.%2520In%250Aour%2520work%252C%2520we%2520introduce%2520%255Ctextsc%257BStyLIP%257D%252C%2520a%2520novel%2520domain-agnostic%2520prompt%2520learning%250Astrategy%2520for%2520Domain%2520Generalization%2520%2528DG%2529.%2520StyLIP%2520disentangles%2520visual%2520style%2520and%250Acontent%2520in%2520CLIP%2560s%2520vision%2520encoder%2520by%2520using%2520style%2520projectors%2520to%2520learn%250Adomain-specific%2520prompt%2520tokens%2520and%2520combining%2520them%2520with%2520content%2520features.%2520Trained%250Acontrastively%252C%2520this%2520approach%2520enables%2520seamless%2520adaptation%2520across%2520domains%252C%250Aoutperforming%2520state-of-the-art%2520methods%2520on%2520multiple%2520DG%2520benchmarks.%2520Additionally%252C%250Awe%2520propose%2520AD-CLIP%2520for%2520unsupervised%2520domain%2520adaptation%2520%2528DA%2529%252C%2520leveraging%2520CLIP%2560s%250Afrozen%2520vision%2520backbone%2520to%2520learn%2520domain-invariant%2520prompts%2520through%2520image%2520style%250Aand%2520content%2520features.%2520By%2520aligning%2520domains%2520in%2520embedding%2520space%2520with%2520entropy%250Aminimization%252C%2520AD-CLIP%2520effectively%2520handles%2520domain%2520shifts%252C%2520even%2520when%2520only%2520target%250Adomain%2520samples%2520are%2520available.%2520Lastly%252C%2520we%2520outline%2520future%2520work%2520on%2520class%2520discovery%250Ausing%2520prompt%2520learning%2520for%2520semantic%2520segmentation%2520in%2520remote%2520sensing%252C%2520focusing%2520on%250Aidentifying%2520novel%2520or%2520rare%2520classes%2520in%2520unstructured%2520environments.%2520This%2520paves%2520the%250Away%2520for%2520more%2520adaptive%2520and%2520generalizable%2520models%2520in%2520complex%252C%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20the%20Era%20of%20Prompt%20Learning%20with%20Vision-Language%20Models&entry.906535625=Ankit%20Jha&entry.1292438233=%20%20Large-scale%20foundation%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%0Ageneralization%20but%20struggle%20with%20domain%20shifts%2C%20limiting%20their%20adaptability.%20In%0Aour%20work%2C%20we%20introduce%20%5Ctextsc%7BStyLIP%7D%2C%20a%20novel%20domain-agnostic%20prompt%20learning%0Astrategy%20for%20Domain%20Generalization%20%28DG%29.%20StyLIP%20disentangles%20visual%20style%20and%0Acontent%20in%20CLIP%60s%20vision%20encoder%20by%20using%20style%20projectors%20to%20learn%0Adomain-specific%20prompt%20tokens%20and%20combining%20them%20with%20content%20features.%20Trained%0Acontrastively%2C%20this%20approach%20enables%20seamless%20adaptation%20across%20domains%2C%0Aoutperforming%20state-of-the-art%20methods%20on%20multiple%20DG%20benchmarks.%20Additionally%2C%0Awe%20propose%20AD-CLIP%20for%20unsupervised%20domain%20adaptation%20%28DA%29%2C%20leveraging%20CLIP%60s%0Afrozen%20vision%20backbone%20to%20learn%20domain-invariant%20prompts%20through%20image%20style%0Aand%20content%20features.%20By%20aligning%20domains%20in%20embedding%20space%20with%20entropy%0Aminimization%2C%20AD-CLIP%20effectively%20handles%20domain%20shifts%2C%20even%20when%20only%20target%0Adomain%20samples%20are%20available.%20Lastly%2C%20we%20outline%20future%20work%20on%20class%20discovery%0Ausing%20prompt%20learning%20for%20semantic%20segmentation%20in%20remote%20sensing%2C%20focusing%20on%0Aidentifying%20novel%20or%20rare%20classes%20in%20unstructured%20environments.%20This%20paves%20the%0Away%20for%20more%20adaptive%20and%20generalizable%20models%20in%20complex%2C%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04892v1&entry.124074799=Read"},
{"title": "Personalized Federated Learning for Cross-view Geo-localization", "author": "Christos Anagnostopoulos and Alexandros Gkillas and Nikos Piperigkos and Aris S. Lalos", "abstract": "  In this paper we propose a methodology combining Federated Learning (FL) with\nCross-view Image Geo-localization (CVGL) techniques. We address the challenges\nof data privacy and heterogeneity in autonomous vehicle environments by\nproposing a personalized Federated Learning scenario that allows selective\nsharing of model parameters. Our method implements a coarse-to-fine approach,\nwhere clients share only the coarse feature extractors while keeping\nfine-grained features specific to local environments. We evaluate our approach\nagainst traditional centralized and single-client training schemes using the\nKITTI dataset combined with satellite imagery. Results demonstrate that our\nfederated CVGL method achieves performance close to centralized training while\nmaintaining data privacy. The proposed partial model sharing strategy shows\ncomparable or slightly better performance than classical FL, offering\nsignificant reduced communication overhead without sacrificing accuracy. Our\nwork contributes to more robust and privacy-preserving localization systems for\nautonomous vehicles operating in diverse environments\n", "link": "http://arxiv.org/abs/2411.04692v1", "date": "2024-11-07", "relevancy": 2.812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6089}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Federated%20Learning%20for%20Cross-view%20Geo-localization&body=Title%3A%20Personalized%20Federated%20Learning%20for%20Cross-view%20Geo-localization%0AAuthor%3A%20Christos%20Anagnostopoulos%20and%20Alexandros%20Gkillas%20and%20Nikos%20Piperigkos%20and%20Aris%20S.%20Lalos%0AAbstract%3A%20%20%20In%20this%20paper%20we%20propose%20a%20methodology%20combining%20Federated%20Learning%20%28FL%29%20with%0ACross-view%20Image%20Geo-localization%20%28CVGL%29%20techniques.%20We%20address%20the%20challenges%0Aof%20data%20privacy%20and%20heterogeneity%20in%20autonomous%20vehicle%20environments%20by%0Aproposing%20a%20personalized%20Federated%20Learning%20scenario%20that%20allows%20selective%0Asharing%20of%20model%20parameters.%20Our%20method%20implements%20a%20coarse-to-fine%20approach%2C%0Awhere%20clients%20share%20only%20the%20coarse%20feature%20extractors%20while%20keeping%0Afine-grained%20features%20specific%20to%20local%20environments.%20We%20evaluate%20our%20approach%0Aagainst%20traditional%20centralized%20and%20single-client%20training%20schemes%20using%20the%0AKITTI%20dataset%20combined%20with%20satellite%20imagery.%20Results%20demonstrate%20that%20our%0Afederated%20CVGL%20method%20achieves%20performance%20close%20to%20centralized%20training%20while%0Amaintaining%20data%20privacy.%20The%20proposed%20partial%20model%20sharing%20strategy%20shows%0Acomparable%20or%20slightly%20better%20performance%20than%20classical%20FL%2C%20offering%0Asignificant%20reduced%20communication%20overhead%20without%20sacrificing%20accuracy.%20Our%0Awork%20contributes%20to%20more%20robust%20and%20privacy-preserving%20localization%20systems%20for%0Aautonomous%20vehicles%20operating%20in%20diverse%20environments%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Federated%2520Learning%2520for%2520Cross-view%2520Geo-localization%26entry.906535625%3DChristos%2520Anagnostopoulos%2520and%2520Alexandros%2520Gkillas%2520and%2520Nikos%2520Piperigkos%2520and%2520Aris%2520S.%2520Lalos%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520propose%2520a%2520methodology%2520combining%2520Federated%2520Learning%2520%2528FL%2529%2520with%250ACross-view%2520Image%2520Geo-localization%2520%2528CVGL%2529%2520techniques.%2520We%2520address%2520the%2520challenges%250Aof%2520data%2520privacy%2520and%2520heterogeneity%2520in%2520autonomous%2520vehicle%2520environments%2520by%250Aproposing%2520a%2520personalized%2520Federated%2520Learning%2520scenario%2520that%2520allows%2520selective%250Asharing%2520of%2520model%2520parameters.%2520Our%2520method%2520implements%2520a%2520coarse-to-fine%2520approach%252C%250Awhere%2520clients%2520share%2520only%2520the%2520coarse%2520feature%2520extractors%2520while%2520keeping%250Afine-grained%2520features%2520specific%2520to%2520local%2520environments.%2520We%2520evaluate%2520our%2520approach%250Aagainst%2520traditional%2520centralized%2520and%2520single-client%2520training%2520schemes%2520using%2520the%250AKITTI%2520dataset%2520combined%2520with%2520satellite%2520imagery.%2520Results%2520demonstrate%2520that%2520our%250Afederated%2520CVGL%2520method%2520achieves%2520performance%2520close%2520to%2520centralized%2520training%2520while%250Amaintaining%2520data%2520privacy.%2520The%2520proposed%2520partial%2520model%2520sharing%2520strategy%2520shows%250Acomparable%2520or%2520slightly%2520better%2520performance%2520than%2520classical%2520FL%252C%2520offering%250Asignificant%2520reduced%2520communication%2520overhead%2520without%2520sacrificing%2520accuracy.%2520Our%250Awork%2520contributes%2520to%2520more%2520robust%2520and%2520privacy-preserving%2520localization%2520systems%2520for%250Aautonomous%2520vehicles%2520operating%2520in%2520diverse%2520environments%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Federated%20Learning%20for%20Cross-view%20Geo-localization&entry.906535625=Christos%20Anagnostopoulos%20and%20Alexandros%20Gkillas%20and%20Nikos%20Piperigkos%20and%20Aris%20S.%20Lalos&entry.1292438233=%20%20In%20this%20paper%20we%20propose%20a%20methodology%20combining%20Federated%20Learning%20%28FL%29%20with%0ACross-view%20Image%20Geo-localization%20%28CVGL%29%20techniques.%20We%20address%20the%20challenges%0Aof%20data%20privacy%20and%20heterogeneity%20in%20autonomous%20vehicle%20environments%20by%0Aproposing%20a%20personalized%20Federated%20Learning%20scenario%20that%20allows%20selective%0Asharing%20of%20model%20parameters.%20Our%20method%20implements%20a%20coarse-to-fine%20approach%2C%0Awhere%20clients%20share%20only%20the%20coarse%20feature%20extractors%20while%20keeping%0Afine-grained%20features%20specific%20to%20local%20environments.%20We%20evaluate%20our%20approach%0Aagainst%20traditional%20centralized%20and%20single-client%20training%20schemes%20using%20the%0AKITTI%20dataset%20combined%20with%20satellite%20imagery.%20Results%20demonstrate%20that%20our%0Afederated%20CVGL%20method%20achieves%20performance%20close%20to%20centralized%20training%20while%0Amaintaining%20data%20privacy.%20The%20proposed%20partial%20model%20sharing%20strategy%20shows%0Acomparable%20or%20slightly%20better%20performance%20than%20classical%20FL%2C%20offering%0Asignificant%20reduced%20communication%20overhead%20without%20sacrificing%20accuracy.%20Our%0Awork%20contributes%20to%20more%20robust%20and%20privacy-preserving%20localization%20systems%20for%0Aautonomous%20vehicles%20operating%20in%20diverse%20environments%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04692v1&entry.124074799=Read"},
{"title": "OneProt: Towards Multi-Modal Protein Foundation Models", "author": "Klemens Fl\u00f6ge and Srisruthi Udayakumar and Johanna Sommer and Marie Piraud and Stefan Kesselheim and Vincent Fortuin and Stephan G\u00fcnneman and Karel J van der Weg and Holger Gohlke and Alina Bazarova and Erinc Merdivan", "abstract": "  Recent AI advances have enabled multi-modal systems to model and translate\ndiverse information spaces. Extending beyond text and vision, we introduce\nOneProt, a multi-modal AI for proteins that integrates structural, sequence,\nalignment, and binding site data. Using the ImageBind framework, OneProt aligns\nthe latent spaces of modality encoders along protein sequences. It demonstrates\nstrong performance in retrieval tasks and surpasses state-of-the-art methods in\nvarious downstream tasks, including metal ion binding classification,\ngene-ontology annotation, and enzyme function prediction. This work expands\nmulti-modal capabilities in protein models, paving the way for applications in\ndrug discovery, biocatalytic reaction planning, and protein engineering.\n", "link": "http://arxiv.org/abs/2411.04863v1", "date": "2024-11-07", "relevancy": 2.8003, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models&body=Title%3A%20OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models%0AAuthor%3A%20Klemens%20Fl%C3%B6ge%20and%20Srisruthi%20Udayakumar%20and%20Johanna%20Sommer%20and%20Marie%20Piraud%20and%20Stefan%20Kesselheim%20and%20Vincent%20Fortuin%20and%20Stephan%20G%C3%BCnneman%20and%20Karel%20J%20van%20der%20Weg%20and%20Holger%20Gohlke%20and%20Alina%20Bazarova%20and%20Erinc%20Merdivan%0AAbstract%3A%20%20%20Recent%20AI%20advances%20have%20enabled%20multi-modal%20systems%20to%20model%20and%20translate%0Adiverse%20information%20spaces.%20Extending%20beyond%20text%20and%20vision%2C%20we%20introduce%0AOneProt%2C%20a%20multi-modal%20AI%20for%20proteins%20that%20integrates%20structural%2C%20sequence%2C%0Aalignment%2C%20and%20binding%20site%20data.%20Using%20the%20ImageBind%20framework%2C%20OneProt%20aligns%0Athe%20latent%20spaces%20of%20modality%20encoders%20along%20protein%20sequences.%20It%20demonstrates%0Astrong%20performance%20in%20retrieval%20tasks%20and%20surpasses%20state-of-the-art%20methods%20in%0Avarious%20downstream%20tasks%2C%20including%20metal%20ion%20binding%20classification%2C%0Agene-ontology%20annotation%2C%20and%20enzyme%20function%20prediction.%20This%20work%20expands%0Amulti-modal%20capabilities%20in%20protein%20models%2C%20paving%20the%20way%20for%20applications%20in%0Adrug%20discovery%2C%20biocatalytic%20reaction%20planning%2C%20and%20protein%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneProt%253A%2520Towards%2520Multi-Modal%2520Protein%2520Foundation%2520Models%26entry.906535625%3DKlemens%2520Fl%25C3%25B6ge%2520and%2520Srisruthi%2520Udayakumar%2520and%2520Johanna%2520Sommer%2520and%2520Marie%2520Piraud%2520and%2520Stefan%2520Kesselheim%2520and%2520Vincent%2520Fortuin%2520and%2520Stephan%2520G%25C3%25BCnneman%2520and%2520Karel%2520J%2520van%2520der%2520Weg%2520and%2520Holger%2520Gohlke%2520and%2520Alina%2520Bazarova%2520and%2520Erinc%2520Merdivan%26entry.1292438233%3D%2520%2520Recent%2520AI%2520advances%2520have%2520enabled%2520multi-modal%2520systems%2520to%2520model%2520and%2520translate%250Adiverse%2520information%2520spaces.%2520Extending%2520beyond%2520text%2520and%2520vision%252C%2520we%2520introduce%250AOneProt%252C%2520a%2520multi-modal%2520AI%2520for%2520proteins%2520that%2520integrates%2520structural%252C%2520sequence%252C%250Aalignment%252C%2520and%2520binding%2520site%2520data.%2520Using%2520the%2520ImageBind%2520framework%252C%2520OneProt%2520aligns%250Athe%2520latent%2520spaces%2520of%2520modality%2520encoders%2520along%2520protein%2520sequences.%2520It%2520demonstrates%250Astrong%2520performance%2520in%2520retrieval%2520tasks%2520and%2520surpasses%2520state-of-the-art%2520methods%2520in%250Avarious%2520downstream%2520tasks%252C%2520including%2520metal%2520ion%2520binding%2520classification%252C%250Agene-ontology%2520annotation%252C%2520and%2520enzyme%2520function%2520prediction.%2520This%2520work%2520expands%250Amulti-modal%2520capabilities%2520in%2520protein%2520models%252C%2520paving%2520the%2520way%2520for%2520applications%2520in%250Adrug%2520discovery%252C%2520biocatalytic%2520reaction%2520planning%252C%2520and%2520protein%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models&entry.906535625=Klemens%20Fl%C3%B6ge%20and%20Srisruthi%20Udayakumar%20and%20Johanna%20Sommer%20and%20Marie%20Piraud%20and%20Stefan%20Kesselheim%20and%20Vincent%20Fortuin%20and%20Stephan%20G%C3%BCnneman%20and%20Karel%20J%20van%20der%20Weg%20and%20Holger%20Gohlke%20and%20Alina%20Bazarova%20and%20Erinc%20Merdivan&entry.1292438233=%20%20Recent%20AI%20advances%20have%20enabled%20multi-modal%20systems%20to%20model%20and%20translate%0Adiverse%20information%20spaces.%20Extending%20beyond%20text%20and%20vision%2C%20we%20introduce%0AOneProt%2C%20a%20multi-modal%20AI%20for%20proteins%20that%20integrates%20structural%2C%20sequence%2C%0Aalignment%2C%20and%20binding%20site%20data.%20Using%20the%20ImageBind%20framework%2C%20OneProt%20aligns%0Athe%20latent%20spaces%20of%20modality%20encoders%20along%20protein%20sequences.%20It%20demonstrates%0Astrong%20performance%20in%20retrieval%20tasks%20and%20surpasses%20state-of-the-art%20methods%20in%0Avarious%20downstream%20tasks%2C%20including%20metal%20ion%20binding%20classification%2C%0Agene-ontology%20annotation%2C%20and%20enzyme%20function%20prediction.%20This%20work%20expands%0Amulti-modal%20capabilities%20in%20protein%20models%2C%20paving%20the%20way%20for%20applications%20in%0Adrug%20discovery%2C%20biocatalytic%20reaction%20planning%2C%20and%20protein%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04863v1&entry.124074799=Read"},
{"title": "Interpreting CLIP: Insights on the Robustness to ImageNet Distribution\n  Shifts", "author": "Jonathan Crabb\u00e9 and Pau Rodr\u00edguez and Vaishaal Shankar and Luca Zappella and Arno Blaas", "abstract": "  What distinguishes robust models from non-robust ones? While for ImageNet\ndistribution shifts it has been shown that such differences in robustness can\nbe traced back predominantly to differences in training data, so far it is not\nknown what that translates to in terms of what the model has learned. In this\nwork, we bridge this gap by probing the representation spaces of 16 robust\nzero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and\npretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),\nand comparing them to the representation spaces of less robust models with\nidentical backbones, but different (pre)training sets or objectives (CLIP\npretraining on ImageNet-Captions, and supervised training or finetuning on\nImageNet).Through this analysis, we generate three novel insights. Firstly, we\ndetect the presence of outlier features in robust zero-shot CLIP vision\nencoders, which to the best of our knowledge is the first time these are\nobserved in non-language and non-transformer models. Secondly, we find the\nexistence of outlier features to be an indication of ImageNet shift robustness\nin models, since we only find them in robust models in our analysis. Lastly, we\nalso investigate the number of unique encoded concepts in the representation\nspace and find zero-shot CLIP models to encode a higher number of unique\nconcepts in their representation space. However, we do not find this to be an\nindicator of ImageNet shift robustness and hypothesize that it is rather\nrelated to the language supervision. Since the presence of outlier features can\nbe detected without access to any data from shifted datasets, we believe that\nthey could be a useful tool for practitioners to get a feeling for the\ndistribution shift robustness of a pretrained model during deployment.\n", "link": "http://arxiv.org/abs/2310.13040v2", "date": "2024-11-07", "relevancy": 2.7979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20CLIP%3A%20Insights%20on%20the%20Robustness%20to%20ImageNet%20Distribution%0A%20%20Shifts&body=Title%3A%20Interpreting%20CLIP%3A%20Insights%20on%20the%20Robustness%20to%20ImageNet%20Distribution%0A%20%20Shifts%0AAuthor%3A%20Jonathan%20Crabb%C3%A9%20and%20Pau%20Rodr%C3%ADguez%20and%20Vaishaal%20Shankar%20and%20Luca%20Zappella%20and%20Arno%20Blaas%0AAbstract%3A%20%20%20What%20distinguishes%20robust%20models%20from%20non-robust%20ones%3F%20While%20for%20ImageNet%0Adistribution%20shifts%20it%20has%20been%20shown%20that%20such%20differences%20in%20robustness%20can%0Abe%20traced%20back%20predominantly%20to%20differences%20in%20training%20data%2C%20so%20far%20it%20is%20not%0Aknown%20what%20that%20translates%20to%20in%20terms%20of%20what%20the%20model%20has%20learned.%20In%20this%0Awork%2C%20we%20bridge%20this%20gap%20by%20probing%20the%20representation%20spaces%20of%2016%20robust%0Azero-shot%20CLIP%20vision%20encoders%20with%20various%20backbones%20%28ResNets%20and%20ViTs%29%20and%0Apretraining%20sets%20%28OpenAI%2C%20LAION-400M%2C%20LAION-2B%2C%20YFCC15M%2C%20CC12M%20and%20%7BDataComp%7D%29%2C%0Aand%20comparing%20them%20to%20the%20representation%20spaces%20of%20less%20robust%20models%20with%0Aidentical%20backbones%2C%20but%20different%20%28pre%29training%20sets%20or%20objectives%20%28CLIP%0Apretraining%20on%20ImageNet-Captions%2C%20and%20supervised%20training%20or%20finetuning%20on%0AImageNet%29.Through%20this%20analysis%2C%20we%20generate%20three%20novel%20insights.%20Firstly%2C%20we%0Adetect%20the%20presence%20of%20outlier%20features%20in%20robust%20zero-shot%20CLIP%20vision%0Aencoders%2C%20which%20to%20the%20best%20of%20our%20knowledge%20is%20the%20first%20time%20these%20are%0Aobserved%20in%20non-language%20and%20non-transformer%20models.%20Secondly%2C%20we%20find%20the%0Aexistence%20of%20outlier%20features%20to%20be%20an%20indication%20of%20ImageNet%20shift%20robustness%0Ain%20models%2C%20since%20we%20only%20find%20them%20in%20robust%20models%20in%20our%20analysis.%20Lastly%2C%20we%0Aalso%20investigate%20the%20number%20of%20unique%20encoded%20concepts%20in%20the%20representation%0Aspace%20and%20find%20zero-shot%20CLIP%20models%20to%20encode%20a%20higher%20number%20of%20unique%0Aconcepts%20in%20their%20representation%20space.%20However%2C%20we%20do%20not%20find%20this%20to%20be%20an%0Aindicator%20of%20ImageNet%20shift%20robustness%20and%20hypothesize%20that%20it%20is%20rather%0Arelated%20to%20the%20language%20supervision.%20Since%20the%20presence%20of%20outlier%20features%20can%0Abe%20detected%20without%20access%20to%20any%20data%20from%20shifted%20datasets%2C%20we%20believe%20that%0Athey%20could%20be%20a%20useful%20tool%20for%20practitioners%20to%20get%20a%20feeling%20for%20the%0Adistribution%20shift%20robustness%20of%20a%20pretrained%20model%20during%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520CLIP%253A%2520Insights%2520on%2520the%2520Robustness%2520to%2520ImageNet%2520Distribution%250A%2520%2520Shifts%26entry.906535625%3DJonathan%2520Crabb%25C3%25A9%2520and%2520Pau%2520Rodr%25C3%25ADguez%2520and%2520Vaishaal%2520Shankar%2520and%2520Luca%2520Zappella%2520and%2520Arno%2520Blaas%26entry.1292438233%3D%2520%2520What%2520distinguishes%2520robust%2520models%2520from%2520non-robust%2520ones%253F%2520While%2520for%2520ImageNet%250Adistribution%2520shifts%2520it%2520has%2520been%2520shown%2520that%2520such%2520differences%2520in%2520robustness%2520can%250Abe%2520traced%2520back%2520predominantly%2520to%2520differences%2520in%2520training%2520data%252C%2520so%2520far%2520it%2520is%2520not%250Aknown%2520what%2520that%2520translates%2520to%2520in%2520terms%2520of%2520what%2520the%2520model%2520has%2520learned.%2520In%2520this%250Awork%252C%2520we%2520bridge%2520this%2520gap%2520by%2520probing%2520the%2520representation%2520spaces%2520of%252016%2520robust%250Azero-shot%2520CLIP%2520vision%2520encoders%2520with%2520various%2520backbones%2520%2528ResNets%2520and%2520ViTs%2529%2520and%250Apretraining%2520sets%2520%2528OpenAI%252C%2520LAION-400M%252C%2520LAION-2B%252C%2520YFCC15M%252C%2520CC12M%2520and%2520%257BDataComp%257D%2529%252C%250Aand%2520comparing%2520them%2520to%2520the%2520representation%2520spaces%2520of%2520less%2520robust%2520models%2520with%250Aidentical%2520backbones%252C%2520but%2520different%2520%2528pre%2529training%2520sets%2520or%2520objectives%2520%2528CLIP%250Apretraining%2520on%2520ImageNet-Captions%252C%2520and%2520supervised%2520training%2520or%2520finetuning%2520on%250AImageNet%2529.Through%2520this%2520analysis%252C%2520we%2520generate%2520three%2520novel%2520insights.%2520Firstly%252C%2520we%250Adetect%2520the%2520presence%2520of%2520outlier%2520features%2520in%2520robust%2520zero-shot%2520CLIP%2520vision%250Aencoders%252C%2520which%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520is%2520the%2520first%2520time%2520these%2520are%250Aobserved%2520in%2520non-language%2520and%2520non-transformer%2520models.%2520Secondly%252C%2520we%2520find%2520the%250Aexistence%2520of%2520outlier%2520features%2520to%2520be%2520an%2520indication%2520of%2520ImageNet%2520shift%2520robustness%250Ain%2520models%252C%2520since%2520we%2520only%2520find%2520them%2520in%2520robust%2520models%2520in%2520our%2520analysis.%2520Lastly%252C%2520we%250Aalso%2520investigate%2520the%2520number%2520of%2520unique%2520encoded%2520concepts%2520in%2520the%2520representation%250Aspace%2520and%2520find%2520zero-shot%2520CLIP%2520models%2520to%2520encode%2520a%2520higher%2520number%2520of%2520unique%250Aconcepts%2520in%2520their%2520representation%2520space.%2520However%252C%2520we%2520do%2520not%2520find%2520this%2520to%2520be%2520an%250Aindicator%2520of%2520ImageNet%2520shift%2520robustness%2520and%2520hypothesize%2520that%2520it%2520is%2520rather%250Arelated%2520to%2520the%2520language%2520supervision.%2520Since%2520the%2520presence%2520of%2520outlier%2520features%2520can%250Abe%2520detected%2520without%2520access%2520to%2520any%2520data%2520from%2520shifted%2520datasets%252C%2520we%2520believe%2520that%250Athey%2520could%2520be%2520a%2520useful%2520tool%2520for%2520practitioners%2520to%2520get%2520a%2520feeling%2520for%2520the%250Adistribution%2520shift%2520robustness%2520of%2520a%2520pretrained%2520model%2520during%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20CLIP%3A%20Insights%20on%20the%20Robustness%20to%20ImageNet%20Distribution%0A%20%20Shifts&entry.906535625=Jonathan%20Crabb%C3%A9%20and%20Pau%20Rodr%C3%ADguez%20and%20Vaishaal%20Shankar%20and%20Luca%20Zappella%20and%20Arno%20Blaas&entry.1292438233=%20%20What%20distinguishes%20robust%20models%20from%20non-robust%20ones%3F%20While%20for%20ImageNet%0Adistribution%20shifts%20it%20has%20been%20shown%20that%20such%20differences%20in%20robustness%20can%0Abe%20traced%20back%20predominantly%20to%20differences%20in%20training%20data%2C%20so%20far%20it%20is%20not%0Aknown%20what%20that%20translates%20to%20in%20terms%20of%20what%20the%20model%20has%20learned.%20In%20this%0Awork%2C%20we%20bridge%20this%20gap%20by%20probing%20the%20representation%20spaces%20of%2016%20robust%0Azero-shot%20CLIP%20vision%20encoders%20with%20various%20backbones%20%28ResNets%20and%20ViTs%29%20and%0Apretraining%20sets%20%28OpenAI%2C%20LAION-400M%2C%20LAION-2B%2C%20YFCC15M%2C%20CC12M%20and%20%7BDataComp%7D%29%2C%0Aand%20comparing%20them%20to%20the%20representation%20spaces%20of%20less%20robust%20models%20with%0Aidentical%20backbones%2C%20but%20different%20%28pre%29training%20sets%20or%20objectives%20%28CLIP%0Apretraining%20on%20ImageNet-Captions%2C%20and%20supervised%20training%20or%20finetuning%20on%0AImageNet%29.Through%20this%20analysis%2C%20we%20generate%20three%20novel%20insights.%20Firstly%2C%20we%0Adetect%20the%20presence%20of%20outlier%20features%20in%20robust%20zero-shot%20CLIP%20vision%0Aencoders%2C%20which%20to%20the%20best%20of%20our%20knowledge%20is%20the%20first%20time%20these%20are%0Aobserved%20in%20non-language%20and%20non-transformer%20models.%20Secondly%2C%20we%20find%20the%0Aexistence%20of%20outlier%20features%20to%20be%20an%20indication%20of%20ImageNet%20shift%20robustness%0Ain%20models%2C%20since%20we%20only%20find%20them%20in%20robust%20models%20in%20our%20analysis.%20Lastly%2C%20we%0Aalso%20investigate%20the%20number%20of%20unique%20encoded%20concepts%20in%20the%20representation%0Aspace%20and%20find%20zero-shot%20CLIP%20models%20to%20encode%20a%20higher%20number%20of%20unique%0Aconcepts%20in%20their%20representation%20space.%20However%2C%20we%20do%20not%20find%20this%20to%20be%20an%0Aindicator%20of%20ImageNet%20shift%20robustness%20and%20hypothesize%20that%20it%20is%20rather%0Arelated%20to%20the%20language%20supervision.%20Since%20the%20presence%20of%20outlier%20features%20can%0Abe%20detected%20without%20access%20to%20any%20data%20from%20shifted%20datasets%2C%20we%20believe%20that%0Athey%20could%20be%20a%20useful%20tool%20for%20practitioners%20to%20get%20a%20feeling%20for%20the%0Adistribution%20shift%20robustness%20of%20a%20pretrained%20model%20during%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13040v2&entry.124074799=Read"},
{"title": "Robust Iris Centre Localisation for Assistive Eye-Gaze Tracking", "author": "Nipun Sandamal Ranasekara Pathiranage and Stefania Cristina and Kenneth P. Camilleri", "abstract": "  In this research work, we address the problem of robust iris centre\nlocalisation in unconstrained conditions as a core component of our eye-gaze\ntracking platform. We investigate the application of U-Net variants for\nsegmentation-based and regression-based approaches to improve our iris centre\nlocalisation, which was previously based on Bayes' classification. The achieved\nresults are comparable to or better than the state-of-the-art, offering a\ndrastic improvement over those achieved by the Bayes' classifier, and without\nsacrificing the real-time performance of our eye-gaze tracking platform.\n", "link": "http://arxiv.org/abs/2411.04912v1", "date": "2024-11-07", "relevancy": 2.7791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5907}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Iris%20Centre%20Localisation%20for%20Assistive%20Eye-Gaze%20Tracking&body=Title%3A%20Robust%20Iris%20Centre%20Localisation%20for%20Assistive%20Eye-Gaze%20Tracking%0AAuthor%3A%20Nipun%20Sandamal%20Ranasekara%20Pathiranage%20and%20Stefania%20Cristina%20and%20Kenneth%20P.%20Camilleri%0AAbstract%3A%20%20%20In%20this%20research%20work%2C%20we%20address%20the%20problem%20of%20robust%20iris%20centre%0Alocalisation%20in%20unconstrained%20conditions%20as%20a%20core%20component%20of%20our%20eye-gaze%0Atracking%20platform.%20We%20investigate%20the%20application%20of%20U-Net%20variants%20for%0Asegmentation-based%20and%20regression-based%20approaches%20to%20improve%20our%20iris%20centre%0Alocalisation%2C%20which%20was%20previously%20based%20on%20Bayes%27%20classification.%20The%20achieved%0Aresults%20are%20comparable%20to%20or%20better%20than%20the%20state-of-the-art%2C%20offering%20a%0Adrastic%20improvement%20over%20those%20achieved%20by%20the%20Bayes%27%20classifier%2C%20and%20without%0Asacrificing%20the%20real-time%20performance%20of%20our%20eye-gaze%20tracking%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Iris%2520Centre%2520Localisation%2520for%2520Assistive%2520Eye-Gaze%2520Tracking%26entry.906535625%3DNipun%2520Sandamal%2520Ranasekara%2520Pathiranage%2520and%2520Stefania%2520Cristina%2520and%2520Kenneth%2520P.%2520Camilleri%26entry.1292438233%3D%2520%2520In%2520this%2520research%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520robust%2520iris%2520centre%250Alocalisation%2520in%2520unconstrained%2520conditions%2520as%2520a%2520core%2520component%2520of%2520our%2520eye-gaze%250Atracking%2520platform.%2520We%2520investigate%2520the%2520application%2520of%2520U-Net%2520variants%2520for%250Asegmentation-based%2520and%2520regression-based%2520approaches%2520to%2520improve%2520our%2520iris%2520centre%250Alocalisation%252C%2520which%2520was%2520previously%2520based%2520on%2520Bayes%2527%2520classification.%2520The%2520achieved%250Aresults%2520are%2520comparable%2520to%2520or%2520better%2520than%2520the%2520state-of-the-art%252C%2520offering%2520a%250Adrastic%2520improvement%2520over%2520those%2520achieved%2520by%2520the%2520Bayes%2527%2520classifier%252C%2520and%2520without%250Asacrificing%2520the%2520real-time%2520performance%2520of%2520our%2520eye-gaze%2520tracking%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Iris%20Centre%20Localisation%20for%20Assistive%20Eye-Gaze%20Tracking&entry.906535625=Nipun%20Sandamal%20Ranasekara%20Pathiranage%20and%20Stefania%20Cristina%20and%20Kenneth%20P.%20Camilleri&entry.1292438233=%20%20In%20this%20research%20work%2C%20we%20address%20the%20problem%20of%20robust%20iris%20centre%0Alocalisation%20in%20unconstrained%20conditions%20as%20a%20core%20component%20of%20our%20eye-gaze%0Atracking%20platform.%20We%20investigate%20the%20application%20of%20U-Net%20variants%20for%0Asegmentation-based%20and%20regression-based%20approaches%20to%20improve%20our%20iris%20centre%0Alocalisation%2C%20which%20was%20previously%20based%20on%20Bayes%27%20classification.%20The%20achieved%0Aresults%20are%20comparable%20to%20or%20better%20than%20the%20state-of-the-art%2C%20offering%20a%0Adrastic%20improvement%20over%20those%20achieved%20by%20the%20Bayes%27%20classifier%2C%20and%20without%0Asacrificing%20the%20real-time%20performance%20of%20our%20eye-gaze%20tracking%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04912v1&entry.124074799=Read"},
{"title": "BrainSegFounder: Towards 3D Foundation Models for Neuroimage\n  Segmentation", "author": "Joseph Cox and Peng Liu and Skylar E. Stolte and Yunchao Yang and Kang Liu and Kyle B. See and Huiwen Ju and Ruogu Fang", "abstract": "  The burgeoning field of brain health research increasingly leverages\nartificial intelligence (AI) to interpret and analyze neurological data. This\nstudy introduces a novel approach towards the creation of medical foundation\nmodels by integrating a large-scale multi-modal magnetic resonance imaging\n(MRI) dataset derived from 41,400 participants in its own. Our method involves\na novel two-stage pretraining approach using vision transformers. The first\nstage is dedicated to encoding anatomical structures in generally healthy\nbrains, identifying key features such as shapes and sizes of different brain\nregions. The second stage concentrates on spatial information, encompassing\naspects like location and the relative positioning of brain structures. We\nrigorously evaluate our model, BrainFounder, using the Brain Tumor Segmentation\n(BraTS) challenge and Anatomical Tracings of Lesions After Stroke v2.0 (ATLAS\nv2.0) datasets. BrainFounder demonstrates a significant performance gain,\nsurpassing the achievements of the previous winning solutions using fully\nsupervised learning. Our findings underscore the impact of scaling up both the\ncomplexity of the model and the volume of unlabeled training data derived from\ngenerally healthy brains, which enhances the accuracy and predictive\ncapabilities of the model in complex neuroimaging tasks with MRI. The\nimplications of this research provide transformative insights and practical\napplications in healthcare and make substantial steps towards the creation of\nfoundation models for Medical AI. Our pretrained models and training code can\nbe found at https://github.com/lab-smile/GatorBrain.\n", "link": "http://arxiv.org/abs/2406.10395v3", "date": "2024-11-07", "relevancy": 2.7775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5603}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainSegFounder%3A%20Towards%203D%20Foundation%20Models%20for%20Neuroimage%0A%20%20Segmentation&body=Title%3A%20BrainSegFounder%3A%20Towards%203D%20Foundation%20Models%20for%20Neuroimage%0A%20%20Segmentation%0AAuthor%3A%20Joseph%20Cox%20and%20Peng%20Liu%20and%20Skylar%20E.%20Stolte%20and%20Yunchao%20Yang%20and%20Kang%20Liu%20and%20Kyle%20B.%20See%20and%20Huiwen%20Ju%20and%20Ruogu%20Fang%0AAbstract%3A%20%20%20The%20burgeoning%20field%20of%20brain%20health%20research%20increasingly%20leverages%0Aartificial%20intelligence%20%28AI%29%20to%20interpret%20and%20analyze%20neurological%20data.%20This%0Astudy%20introduces%20a%20novel%20approach%20towards%20the%20creation%20of%20medical%20foundation%0Amodels%20by%20integrating%20a%20large-scale%20multi-modal%20magnetic%20resonance%20imaging%0A%28MRI%29%20dataset%20derived%20from%2041%2C400%20participants%20in%20its%20own.%20Our%20method%20involves%0Aa%20novel%20two-stage%20pretraining%20approach%20using%20vision%20transformers.%20The%20first%0Astage%20is%20dedicated%20to%20encoding%20anatomical%20structures%20in%20generally%20healthy%0Abrains%2C%20identifying%20key%20features%20such%20as%20shapes%20and%20sizes%20of%20different%20brain%0Aregions.%20The%20second%20stage%20concentrates%20on%20spatial%20information%2C%20encompassing%0Aaspects%20like%20location%20and%20the%20relative%20positioning%20of%20brain%20structures.%20We%0Arigorously%20evaluate%20our%20model%2C%20BrainFounder%2C%20using%20the%20Brain%20Tumor%20Segmentation%0A%28BraTS%29%20challenge%20and%20Anatomical%20Tracings%20of%20Lesions%20After%20Stroke%20v2.0%20%28ATLAS%0Av2.0%29%20datasets.%20BrainFounder%20demonstrates%20a%20significant%20performance%20gain%2C%0Asurpassing%20the%20achievements%20of%20the%20previous%20winning%20solutions%20using%20fully%0Asupervised%20learning.%20Our%20findings%20underscore%20the%20impact%20of%20scaling%20up%20both%20the%0Acomplexity%20of%20the%20model%20and%20the%20volume%20of%20unlabeled%20training%20data%20derived%20from%0Agenerally%20healthy%20brains%2C%20which%20enhances%20the%20accuracy%20and%20predictive%0Acapabilities%20of%20the%20model%20in%20complex%20neuroimaging%20tasks%20with%20MRI.%20The%0Aimplications%20of%20this%20research%20provide%20transformative%20insights%20and%20practical%0Aapplications%20in%20healthcare%20and%20make%20substantial%20steps%20towards%20the%20creation%20of%0Afoundation%20models%20for%20Medical%20AI.%20Our%20pretrained%20models%20and%20training%20code%20can%0Abe%20found%20at%20https%3A//github.com/lab-smile/GatorBrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10395v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainSegFounder%253A%2520Towards%25203D%2520Foundation%2520Models%2520for%2520Neuroimage%250A%2520%2520Segmentation%26entry.906535625%3DJoseph%2520Cox%2520and%2520Peng%2520Liu%2520and%2520Skylar%2520E.%2520Stolte%2520and%2520Yunchao%2520Yang%2520and%2520Kang%2520Liu%2520and%2520Kyle%2520B.%2520See%2520and%2520Huiwen%2520Ju%2520and%2520Ruogu%2520Fang%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520field%2520of%2520brain%2520health%2520research%2520increasingly%2520leverages%250Aartificial%2520intelligence%2520%2528AI%2529%2520to%2520interpret%2520and%2520analyze%2520neurological%2520data.%2520This%250Astudy%2520introduces%2520a%2520novel%2520approach%2520towards%2520the%2520creation%2520of%2520medical%2520foundation%250Amodels%2520by%2520integrating%2520a%2520large-scale%2520multi-modal%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%2520dataset%2520derived%2520from%252041%252C400%2520participants%2520in%2520its%2520own.%2520Our%2520method%2520involves%250Aa%2520novel%2520two-stage%2520pretraining%2520approach%2520using%2520vision%2520transformers.%2520The%2520first%250Astage%2520is%2520dedicated%2520to%2520encoding%2520anatomical%2520structures%2520in%2520generally%2520healthy%250Abrains%252C%2520identifying%2520key%2520features%2520such%2520as%2520shapes%2520and%2520sizes%2520of%2520different%2520brain%250Aregions.%2520The%2520second%2520stage%2520concentrates%2520on%2520spatial%2520information%252C%2520encompassing%250Aaspects%2520like%2520location%2520and%2520the%2520relative%2520positioning%2520of%2520brain%2520structures.%2520We%250Arigorously%2520evaluate%2520our%2520model%252C%2520BrainFounder%252C%2520using%2520the%2520Brain%2520Tumor%2520Segmentation%250A%2528BraTS%2529%2520challenge%2520and%2520Anatomical%2520Tracings%2520of%2520Lesions%2520After%2520Stroke%2520v2.0%2520%2528ATLAS%250Av2.0%2529%2520datasets.%2520BrainFounder%2520demonstrates%2520a%2520significant%2520performance%2520gain%252C%250Asurpassing%2520the%2520achievements%2520of%2520the%2520previous%2520winning%2520solutions%2520using%2520fully%250Asupervised%2520learning.%2520Our%2520findings%2520underscore%2520the%2520impact%2520of%2520scaling%2520up%2520both%2520the%250Acomplexity%2520of%2520the%2520model%2520and%2520the%2520volume%2520of%2520unlabeled%2520training%2520data%2520derived%2520from%250Agenerally%2520healthy%2520brains%252C%2520which%2520enhances%2520the%2520accuracy%2520and%2520predictive%250Acapabilities%2520of%2520the%2520model%2520in%2520complex%2520neuroimaging%2520tasks%2520with%2520MRI.%2520The%250Aimplications%2520of%2520this%2520research%2520provide%2520transformative%2520insights%2520and%2520practical%250Aapplications%2520in%2520healthcare%2520and%2520make%2520substantial%2520steps%2520towards%2520the%2520creation%2520of%250Afoundation%2520models%2520for%2520Medical%2520AI.%2520Our%2520pretrained%2520models%2520and%2520training%2520code%2520can%250Abe%2520found%2520at%2520https%253A//github.com/lab-smile/GatorBrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10395v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainSegFounder%3A%20Towards%203D%20Foundation%20Models%20for%20Neuroimage%0A%20%20Segmentation&entry.906535625=Joseph%20Cox%20and%20Peng%20Liu%20and%20Skylar%20E.%20Stolte%20and%20Yunchao%20Yang%20and%20Kang%20Liu%20and%20Kyle%20B.%20See%20and%20Huiwen%20Ju%20and%20Ruogu%20Fang&entry.1292438233=%20%20The%20burgeoning%20field%20of%20brain%20health%20research%20increasingly%20leverages%0Aartificial%20intelligence%20%28AI%29%20to%20interpret%20and%20analyze%20neurological%20data.%20This%0Astudy%20introduces%20a%20novel%20approach%20towards%20the%20creation%20of%20medical%20foundation%0Amodels%20by%20integrating%20a%20large-scale%20multi-modal%20magnetic%20resonance%20imaging%0A%28MRI%29%20dataset%20derived%20from%2041%2C400%20participants%20in%20its%20own.%20Our%20method%20involves%0Aa%20novel%20two-stage%20pretraining%20approach%20using%20vision%20transformers.%20The%20first%0Astage%20is%20dedicated%20to%20encoding%20anatomical%20structures%20in%20generally%20healthy%0Abrains%2C%20identifying%20key%20features%20such%20as%20shapes%20and%20sizes%20of%20different%20brain%0Aregions.%20The%20second%20stage%20concentrates%20on%20spatial%20information%2C%20encompassing%0Aaspects%20like%20location%20and%20the%20relative%20positioning%20of%20brain%20structures.%20We%0Arigorously%20evaluate%20our%20model%2C%20BrainFounder%2C%20using%20the%20Brain%20Tumor%20Segmentation%0A%28BraTS%29%20challenge%20and%20Anatomical%20Tracings%20of%20Lesions%20After%20Stroke%20v2.0%20%28ATLAS%0Av2.0%29%20datasets.%20BrainFounder%20demonstrates%20a%20significant%20performance%20gain%2C%0Asurpassing%20the%20achievements%20of%20the%20previous%20winning%20solutions%20using%20fully%0Asupervised%20learning.%20Our%20findings%20underscore%20the%20impact%20of%20scaling%20up%20both%20the%0Acomplexity%20of%20the%20model%20and%20the%20volume%20of%20unlabeled%20training%20data%20derived%20from%0Agenerally%20healthy%20brains%2C%20which%20enhances%20the%20accuracy%20and%20predictive%0Acapabilities%20of%20the%20model%20in%20complex%20neuroimaging%20tasks%20with%20MRI.%20The%0Aimplications%20of%20this%20research%20provide%20transformative%20insights%20and%20practical%0Aapplications%20in%20healthcare%20and%20make%20substantial%20steps%20towards%20the%20creation%20of%0Afoundation%20models%20for%20Medical%20AI.%20Our%20pretrained%20models%20and%20training%20code%20can%0Abe%20found%20at%20https%3A//github.com/lab-smile/GatorBrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10395v3&entry.124074799=Read"},
{"title": "Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation\n  SAR Target Recognition Using Simulated Data", "author": "Xinzheng Zhang and Hui Zhu and Hongqian Zhuang", "abstract": "  Recently, an intriguing research trend for automatic target recognition (ATR)\nfrom synthetic aperture radar (SAR) imagery has arisen: using simulated data to\ntrain ATR models is a feasible solution to the issue of inadequate measured\ndata. To close the domain gap that exists between the real and simulated data,\nthe unsupervised domain adaptation (UDA) techniques are frequently exploited to\nconstruct ATR models. However, for UDA, the target domain lacks labeled data to\ndirect the model training, posing a great challenge to ATR performance. To\naddress the above problem, a semi-supervised domain adaptation (SSDA) framework\nhas been proposed adopting progressive multi-level alignments for simulated\ndata-aided SAR ATR. First, a progressive wavelet transform data augmentation\n(PWTDA) is presented by analyzing the discrepancies of wavelet decomposition\nsub-bands of two domain images, obtaining the domain-level alignment.\nSpecifically, the domain gap is narrowed by mixing the wavelet transform\nhigh-frequency sub-band components. Second, we develop an asymptotic\ninstance-prototype alignment (AIPA) strategy to push the source domain\ninstances close to the corresponding target prototypes, aiming to achieve\ncategory-level alignment. Moreover, the consistency alignment is implemented by\nexcavating the strong-weak augmentation consistency of both individual samples\nand the multi-sample relationship, enhancing the generalization capability of\nthe model. Extensive experiments on the Synthetic and Measured Paired Labeled\nExperiment (SAMPLE) dataset, indicate that our approach obtains recognition\naccuracies of 99.63% and 98.91% in two common experimental settings with only\none labeled sample per class of the target domain, outperforming the most\nadvanced SSDA techniques.\n", "link": "http://arxiv.org/abs/2411.04711v1", "date": "2024-11-07", "relevancy": 2.7759, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5706}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5672}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Multi-Level%20Alignments%20for%20Semi-Supervised%20Domain%20Adaptation%0A%20%20SAR%20Target%20Recognition%20Using%20Simulated%20Data&body=Title%3A%20Progressive%20Multi-Level%20Alignments%20for%20Semi-Supervised%20Domain%20Adaptation%0A%20%20SAR%20Target%20Recognition%20Using%20Simulated%20Data%0AAuthor%3A%20Xinzheng%20Zhang%20and%20Hui%20Zhu%20and%20Hongqian%20Zhuang%0AAbstract%3A%20%20%20Recently%2C%20an%20intriguing%20research%20trend%20for%20automatic%20target%20recognition%20%28ATR%29%0Afrom%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%20has%20arisen%3A%20using%20simulated%20data%20to%0Atrain%20ATR%20models%20is%20a%20feasible%20solution%20to%20the%20issue%20of%20inadequate%20measured%0Adata.%20To%20close%20the%20domain%20gap%20that%20exists%20between%20the%20real%20and%20simulated%20data%2C%0Athe%20unsupervised%20domain%20adaptation%20%28UDA%29%20techniques%20are%20frequently%20exploited%20to%0Aconstruct%20ATR%20models.%20However%2C%20for%20UDA%2C%20the%20target%20domain%20lacks%20labeled%20data%20to%0Adirect%20the%20model%20training%2C%20posing%20a%20great%20challenge%20to%20ATR%20performance.%20To%0Aaddress%20the%20above%20problem%2C%20a%20semi-supervised%20domain%20adaptation%20%28SSDA%29%20framework%0Ahas%20been%20proposed%20adopting%20progressive%20multi-level%20alignments%20for%20simulated%0Adata-aided%20SAR%20ATR.%20First%2C%20a%20progressive%20wavelet%20transform%20data%20augmentation%0A%28PWTDA%29%20is%20presented%20by%20analyzing%20the%20discrepancies%20of%20wavelet%20decomposition%0Asub-bands%20of%20two%20domain%20images%2C%20obtaining%20the%20domain-level%20alignment.%0ASpecifically%2C%20the%20domain%20gap%20is%20narrowed%20by%20mixing%20the%20wavelet%20transform%0Ahigh-frequency%20sub-band%20components.%20Second%2C%20we%20develop%20an%20asymptotic%0Ainstance-prototype%20alignment%20%28AIPA%29%20strategy%20to%20push%20the%20source%20domain%0Ainstances%20close%20to%20the%20corresponding%20target%20prototypes%2C%20aiming%20to%20achieve%0Acategory-level%20alignment.%20Moreover%2C%20the%20consistency%20alignment%20is%20implemented%20by%0Aexcavating%20the%20strong-weak%20augmentation%20consistency%20of%20both%20individual%20samples%0Aand%20the%20multi-sample%20relationship%2C%20enhancing%20the%20generalization%20capability%20of%0Athe%20model.%20Extensive%20experiments%20on%20the%20Synthetic%20and%20Measured%20Paired%20Labeled%0AExperiment%20%28SAMPLE%29%20dataset%2C%20indicate%20that%20our%20approach%20obtains%20recognition%0Aaccuracies%20of%2099.63%25%20and%2098.91%25%20in%20two%20common%20experimental%20settings%20with%20only%0Aone%20labeled%20sample%20per%20class%20of%20the%20target%20domain%2C%20outperforming%20the%20most%0Aadvanced%20SSDA%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Multi-Level%2520Alignments%2520for%2520Semi-Supervised%2520Domain%2520Adaptation%250A%2520%2520SAR%2520Target%2520Recognition%2520Using%2520Simulated%2520Data%26entry.906535625%3DXinzheng%2520Zhang%2520and%2520Hui%2520Zhu%2520and%2520Hongqian%2520Zhuang%26entry.1292438233%3D%2520%2520Recently%252C%2520an%2520intriguing%2520research%2520trend%2520for%2520automatic%2520target%2520recognition%2520%2528ATR%2529%250Afrom%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imagery%2520has%2520arisen%253A%2520using%2520simulated%2520data%2520to%250Atrain%2520ATR%2520models%2520is%2520a%2520feasible%2520solution%2520to%2520the%2520issue%2520of%2520inadequate%2520measured%250Adata.%2520To%2520close%2520the%2520domain%2520gap%2520that%2520exists%2520between%2520the%2520real%2520and%2520simulated%2520data%252C%250Athe%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520techniques%2520are%2520frequently%2520exploited%2520to%250Aconstruct%2520ATR%2520models.%2520However%252C%2520for%2520UDA%252C%2520the%2520target%2520domain%2520lacks%2520labeled%2520data%2520to%250Adirect%2520the%2520model%2520training%252C%2520posing%2520a%2520great%2520challenge%2520to%2520ATR%2520performance.%2520To%250Aaddress%2520the%2520above%2520problem%252C%2520a%2520semi-supervised%2520domain%2520adaptation%2520%2528SSDA%2529%2520framework%250Ahas%2520been%2520proposed%2520adopting%2520progressive%2520multi-level%2520alignments%2520for%2520simulated%250Adata-aided%2520SAR%2520ATR.%2520First%252C%2520a%2520progressive%2520wavelet%2520transform%2520data%2520augmentation%250A%2528PWTDA%2529%2520is%2520presented%2520by%2520analyzing%2520the%2520discrepancies%2520of%2520wavelet%2520decomposition%250Asub-bands%2520of%2520two%2520domain%2520images%252C%2520obtaining%2520the%2520domain-level%2520alignment.%250ASpecifically%252C%2520the%2520domain%2520gap%2520is%2520narrowed%2520by%2520mixing%2520the%2520wavelet%2520transform%250Ahigh-frequency%2520sub-band%2520components.%2520Second%252C%2520we%2520develop%2520an%2520asymptotic%250Ainstance-prototype%2520alignment%2520%2528AIPA%2529%2520strategy%2520to%2520push%2520the%2520source%2520domain%250Ainstances%2520close%2520to%2520the%2520corresponding%2520target%2520prototypes%252C%2520aiming%2520to%2520achieve%250Acategory-level%2520alignment.%2520Moreover%252C%2520the%2520consistency%2520alignment%2520is%2520implemented%2520by%250Aexcavating%2520the%2520strong-weak%2520augmentation%2520consistency%2520of%2520both%2520individual%2520samples%250Aand%2520the%2520multi-sample%2520relationship%252C%2520enhancing%2520the%2520generalization%2520capability%2520of%250Athe%2520model.%2520Extensive%2520experiments%2520on%2520the%2520Synthetic%2520and%2520Measured%2520Paired%2520Labeled%250AExperiment%2520%2528SAMPLE%2529%2520dataset%252C%2520indicate%2520that%2520our%2520approach%2520obtains%2520recognition%250Aaccuracies%2520of%252099.63%2525%2520and%252098.91%2525%2520in%2520two%2520common%2520experimental%2520settings%2520with%2520only%250Aone%2520labeled%2520sample%2520per%2520class%2520of%2520the%2520target%2520domain%252C%2520outperforming%2520the%2520most%250Aadvanced%2520SSDA%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Multi-Level%20Alignments%20for%20Semi-Supervised%20Domain%20Adaptation%0A%20%20SAR%20Target%20Recognition%20Using%20Simulated%20Data&entry.906535625=Xinzheng%20Zhang%20and%20Hui%20Zhu%20and%20Hongqian%20Zhuang&entry.1292438233=%20%20Recently%2C%20an%20intriguing%20research%20trend%20for%20automatic%20target%20recognition%20%28ATR%29%0Afrom%20synthetic%20aperture%20radar%20%28SAR%29%20imagery%20has%20arisen%3A%20using%20simulated%20data%20to%0Atrain%20ATR%20models%20is%20a%20feasible%20solution%20to%20the%20issue%20of%20inadequate%20measured%0Adata.%20To%20close%20the%20domain%20gap%20that%20exists%20between%20the%20real%20and%20simulated%20data%2C%0Athe%20unsupervised%20domain%20adaptation%20%28UDA%29%20techniques%20are%20frequently%20exploited%20to%0Aconstruct%20ATR%20models.%20However%2C%20for%20UDA%2C%20the%20target%20domain%20lacks%20labeled%20data%20to%0Adirect%20the%20model%20training%2C%20posing%20a%20great%20challenge%20to%20ATR%20performance.%20To%0Aaddress%20the%20above%20problem%2C%20a%20semi-supervised%20domain%20adaptation%20%28SSDA%29%20framework%0Ahas%20been%20proposed%20adopting%20progressive%20multi-level%20alignments%20for%20simulated%0Adata-aided%20SAR%20ATR.%20First%2C%20a%20progressive%20wavelet%20transform%20data%20augmentation%0A%28PWTDA%29%20is%20presented%20by%20analyzing%20the%20discrepancies%20of%20wavelet%20decomposition%0Asub-bands%20of%20two%20domain%20images%2C%20obtaining%20the%20domain-level%20alignment.%0ASpecifically%2C%20the%20domain%20gap%20is%20narrowed%20by%20mixing%20the%20wavelet%20transform%0Ahigh-frequency%20sub-band%20components.%20Second%2C%20we%20develop%20an%20asymptotic%0Ainstance-prototype%20alignment%20%28AIPA%29%20strategy%20to%20push%20the%20source%20domain%0Ainstances%20close%20to%20the%20corresponding%20target%20prototypes%2C%20aiming%20to%20achieve%0Acategory-level%20alignment.%20Moreover%2C%20the%20consistency%20alignment%20is%20implemented%20by%0Aexcavating%20the%20strong-weak%20augmentation%20consistency%20of%20both%20individual%20samples%0Aand%20the%20multi-sample%20relationship%2C%20enhancing%20the%20generalization%20capability%20of%0Athe%20model.%20Extensive%20experiments%20on%20the%20Synthetic%20and%20Measured%20Paired%20Labeled%0AExperiment%20%28SAMPLE%29%20dataset%2C%20indicate%20that%20our%20approach%20obtains%20recognition%0Aaccuracies%20of%2099.63%25%20and%2098.91%25%20in%20two%20common%20experimental%20settings%20with%20only%0Aone%20labeled%20sample%20per%20class%20of%20the%20target%20domain%2C%20outperforming%20the%20most%0Aadvanced%20SSDA%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04711v1&entry.124074799=Read"},
{"title": "CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep\n  Learning and Geometric Insights", "author": "Shahar Zuler and Shai Tejman-Yarden and Dan Raviv", "abstract": "  The ability to map left ventricle (LV) myocardial motion using computed\ntomography angiography (CTA) is essential to diagnosing cardiovascular\nconditions and guiding interventional procedures. Due to their inherent\nlocality, conventional neural networks typically have difficulty predicting\nsubtle tangential movements, which considerably lessens the level of precision\nat which myocardium three-dimensional (3D) mapping can be performed. Using 3D\noptical flow techniques and Functional Maps (FMs), we present a comprehensive\napproach to address this problem. FMs are known for their capacity to capture\nglobal geometric features, thus providing a fuller understanding of 3D\ngeometry. As an alternative to traditional segmentation-based priors, we employ\nsurface-based two-dimensional (2D) constraints derived from spectral\ncorrespondence methods. Our 3D deep learning architecture, based on the ARFlow\nmodel, is optimized to handle complex 3D motion analysis tasks. By\nincorporating FMs, we can capture the subtle tangential movements of the\nmyocardium surface precisely, hence significantly improving the accuracy of 3D\nmapping of the myocardium. The experimental results confirm the effectiveness\nof this method in enhancing myocardium motion analysis. This approach can\ncontribute to improving cardiovascular diagnosis and treatment. Our code and\nadditional resources are available at:\nhttps://shaharzuler.github.io/CardioSpectrumPage\n", "link": "http://arxiv.org/abs/2407.03794v2", "date": "2024-11-07", "relevancy": 2.7751, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5617}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5517}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardioSpectrum%3A%20Comprehensive%20Myocardium%20Motion%20Analysis%20with%203D%20Deep%0A%20%20Learning%20and%20Geometric%20Insights&body=Title%3A%20CardioSpectrum%3A%20Comprehensive%20Myocardium%20Motion%20Analysis%20with%203D%20Deep%0A%20%20Learning%20and%20Geometric%20Insights%0AAuthor%3A%20Shahar%20Zuler%20and%20Shai%20Tejman-Yarden%20and%20Dan%20Raviv%0AAbstract%3A%20%20%20The%20ability%20to%20map%20left%20ventricle%20%28LV%29%20myocardial%20motion%20using%20computed%0Atomography%20angiography%20%28CTA%29%20is%20essential%20to%20diagnosing%20cardiovascular%0Aconditions%20and%20guiding%20interventional%20procedures.%20Due%20to%20their%20inherent%0Alocality%2C%20conventional%20neural%20networks%20typically%20have%20difficulty%20predicting%0Asubtle%20tangential%20movements%2C%20which%20considerably%20lessens%20the%20level%20of%20precision%0Aat%20which%20myocardium%20three-dimensional%20%283D%29%20mapping%20can%20be%20performed.%20Using%203D%0Aoptical%20flow%20techniques%20and%20Functional%20Maps%20%28FMs%29%2C%20we%20present%20a%20comprehensive%0Aapproach%20to%20address%20this%20problem.%20FMs%20are%20known%20for%20their%20capacity%20to%20capture%0Aglobal%20geometric%20features%2C%20thus%20providing%20a%20fuller%20understanding%20of%203D%0Ageometry.%20As%20an%20alternative%20to%20traditional%20segmentation-based%20priors%2C%20we%20employ%0Asurface-based%20two-dimensional%20%282D%29%20constraints%20derived%20from%20spectral%0Acorrespondence%20methods.%20Our%203D%20deep%20learning%20architecture%2C%20based%20on%20the%20ARFlow%0Amodel%2C%20is%20optimized%20to%20handle%20complex%203D%20motion%20analysis%20tasks.%20By%0Aincorporating%20FMs%2C%20we%20can%20capture%20the%20subtle%20tangential%20movements%20of%20the%0Amyocardium%20surface%20precisely%2C%20hence%20significantly%20improving%20the%20accuracy%20of%203D%0Amapping%20of%20the%20myocardium.%20The%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20this%20method%20in%20enhancing%20myocardium%20motion%20analysis.%20This%20approach%20can%0Acontribute%20to%20improving%20cardiovascular%20diagnosis%20and%20treatment.%20Our%20code%20and%0Aadditional%20resources%20are%20available%20at%3A%0Ahttps%3A//shaharzuler.github.io/CardioSpectrumPage%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardioSpectrum%253A%2520Comprehensive%2520Myocardium%2520Motion%2520Analysis%2520with%25203D%2520Deep%250A%2520%2520Learning%2520and%2520Geometric%2520Insights%26entry.906535625%3DShahar%2520Zuler%2520and%2520Shai%2520Tejman-Yarden%2520and%2520Dan%2520Raviv%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520map%2520left%2520ventricle%2520%2528LV%2529%2520myocardial%2520motion%2520using%2520computed%250Atomography%2520angiography%2520%2528CTA%2529%2520is%2520essential%2520to%2520diagnosing%2520cardiovascular%250Aconditions%2520and%2520guiding%2520interventional%2520procedures.%2520Due%2520to%2520their%2520inherent%250Alocality%252C%2520conventional%2520neural%2520networks%2520typically%2520have%2520difficulty%2520predicting%250Asubtle%2520tangential%2520movements%252C%2520which%2520considerably%2520lessens%2520the%2520level%2520of%2520precision%250Aat%2520which%2520myocardium%2520three-dimensional%2520%25283D%2529%2520mapping%2520can%2520be%2520performed.%2520Using%25203D%250Aoptical%2520flow%2520techniques%2520and%2520Functional%2520Maps%2520%2528FMs%2529%252C%2520we%2520present%2520a%2520comprehensive%250Aapproach%2520to%2520address%2520this%2520problem.%2520FMs%2520are%2520known%2520for%2520their%2520capacity%2520to%2520capture%250Aglobal%2520geometric%2520features%252C%2520thus%2520providing%2520a%2520fuller%2520understanding%2520of%25203D%250Ageometry.%2520As%2520an%2520alternative%2520to%2520traditional%2520segmentation-based%2520priors%252C%2520we%2520employ%250Asurface-based%2520two-dimensional%2520%25282D%2529%2520constraints%2520derived%2520from%2520spectral%250Acorrespondence%2520methods.%2520Our%25203D%2520deep%2520learning%2520architecture%252C%2520based%2520on%2520the%2520ARFlow%250Amodel%252C%2520is%2520optimized%2520to%2520handle%2520complex%25203D%2520motion%2520analysis%2520tasks.%2520By%250Aincorporating%2520FMs%252C%2520we%2520can%2520capture%2520the%2520subtle%2520tangential%2520movements%2520of%2520the%250Amyocardium%2520surface%2520precisely%252C%2520hence%2520significantly%2520improving%2520the%2520accuracy%2520of%25203D%250Amapping%2520of%2520the%2520myocardium.%2520The%2520experimental%2520results%2520confirm%2520the%2520effectiveness%250Aof%2520this%2520method%2520in%2520enhancing%2520myocardium%2520motion%2520analysis.%2520This%2520approach%2520can%250Acontribute%2520to%2520improving%2520cardiovascular%2520diagnosis%2520and%2520treatment.%2520Our%2520code%2520and%250Aadditional%2520resources%2520are%2520available%2520at%253A%250Ahttps%253A//shaharzuler.github.io/CardioSpectrumPage%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioSpectrum%3A%20Comprehensive%20Myocardium%20Motion%20Analysis%20with%203D%20Deep%0A%20%20Learning%20and%20Geometric%20Insights&entry.906535625=Shahar%20Zuler%20and%20Shai%20Tejman-Yarden%20and%20Dan%20Raviv&entry.1292438233=%20%20The%20ability%20to%20map%20left%20ventricle%20%28LV%29%20myocardial%20motion%20using%20computed%0Atomography%20angiography%20%28CTA%29%20is%20essential%20to%20diagnosing%20cardiovascular%0Aconditions%20and%20guiding%20interventional%20procedures.%20Due%20to%20their%20inherent%0Alocality%2C%20conventional%20neural%20networks%20typically%20have%20difficulty%20predicting%0Asubtle%20tangential%20movements%2C%20which%20considerably%20lessens%20the%20level%20of%20precision%0Aat%20which%20myocardium%20three-dimensional%20%283D%29%20mapping%20can%20be%20performed.%20Using%203D%0Aoptical%20flow%20techniques%20and%20Functional%20Maps%20%28FMs%29%2C%20we%20present%20a%20comprehensive%0Aapproach%20to%20address%20this%20problem.%20FMs%20are%20known%20for%20their%20capacity%20to%20capture%0Aglobal%20geometric%20features%2C%20thus%20providing%20a%20fuller%20understanding%20of%203D%0Ageometry.%20As%20an%20alternative%20to%20traditional%20segmentation-based%20priors%2C%20we%20employ%0Asurface-based%20two-dimensional%20%282D%29%20constraints%20derived%20from%20spectral%0Acorrespondence%20methods.%20Our%203D%20deep%20learning%20architecture%2C%20based%20on%20the%20ARFlow%0Amodel%2C%20is%20optimized%20to%20handle%20complex%203D%20motion%20analysis%20tasks.%20By%0Aincorporating%20FMs%2C%20we%20can%20capture%20the%20subtle%20tangential%20movements%20of%20the%0Amyocardium%20surface%20precisely%2C%20hence%20significantly%20improving%20the%20accuracy%20of%203D%0Amapping%20of%20the%20myocardium.%20The%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20this%20method%20in%20enhancing%20myocardium%20motion%20analysis.%20This%20approach%20can%0Acontribute%20to%20improving%20cardiovascular%20diagnosis%20and%20treatment.%20Our%20code%20and%0Aadditional%20resources%20are%20available%20at%3A%0Ahttps%3A//shaharzuler.github.io/CardioSpectrumPage%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03794v2&entry.124074799=Read"},
{"title": "Evaluating alignment between humans and neural network representations\n  in image-based learning tasks", "author": "Can Demircan and Tankred Saanum and Leonardo Pettini and Marcel Binz and Blazej M Baczkowski and Christian F Doeller and Mona M Garvert and Eric Schulz", "abstract": "  Humans represent scenes and objects in rich feature spaces, carrying\ninformation that allows us to generalise about category memberships and\nabstract functions with few examples. What determines whether a neural network\nmodel generalises like a human? We tested how well the representations of $86$\npretrained neural network models mapped to human learning trajectories across\ntwo tasks where humans had to learn continuous relationships and categories of\nnatural images. In these tasks, both human participants and neural networks\nsuccessfully identified the relevant stimulus features within a few trials,\ndemonstrating effective generalisation. We found that while training dataset\nsize was a core determinant of alignment with human choices, contrastive\ntraining with multi-modal data (text and imagery) was a common feature of\ncurrently publicly available models that predicted human generalisation.\nIntrinsic dimensionality of representations had different effects on alignment\nfor different model types. Lastly, we tested three sets of human-aligned\nrepresentations and found no consistent improvements in predictive accuracy\ncompared to the baselines. In conclusion, pretrained neural networks can serve\nto extract representations for cognitive models, as they appear to capture some\nfundamental aspects of cognition that are transferable across tasks. Both our\nparadigms and modelling approach offer a novel way to quantify alignment\nbetween neural networks and humans and extend cognitive science into more\nnaturalistic domains.\n", "link": "http://arxiv.org/abs/2306.09377v2", "date": "2024-11-07", "relevancy": 2.7538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks&body=Title%3A%20Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks%0AAuthor%3A%20Can%20Demircan%20and%20Tankred%20Saanum%20and%20Leonardo%20Pettini%20and%20Marcel%20Binz%20and%20Blazej%20M%20Baczkowski%20and%20Christian%20F%20Doeller%20and%20Mona%20M%20Garvert%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Humans%20represent%20scenes%20and%20objects%20in%20rich%20feature%20spaces%2C%20carrying%0Ainformation%20that%20allows%20us%20to%20generalise%20about%20category%20memberships%20and%0Aabstract%20functions%20with%20few%20examples.%20What%20determines%20whether%20a%20neural%20network%0Amodel%20generalises%20like%20a%20human%3F%20We%20tested%20how%20well%20the%20representations%20of%20%2486%24%0Apretrained%20neural%20network%20models%20mapped%20to%20human%20learning%20trajectories%20across%0Atwo%20tasks%20where%20humans%20had%20to%20learn%20continuous%20relationships%20and%20categories%20of%0Anatural%20images.%20In%20these%20tasks%2C%20both%20human%20participants%20and%20neural%20networks%0Asuccessfully%20identified%20the%20relevant%20stimulus%20features%20within%20a%20few%20trials%2C%0Ademonstrating%20effective%20generalisation.%20We%20found%20that%20while%20training%20dataset%0Asize%20was%20a%20core%20determinant%20of%20alignment%20with%20human%20choices%2C%20contrastive%0Atraining%20with%20multi-modal%20data%20%28text%20and%20imagery%29%20was%20a%20common%20feature%20of%0Acurrently%20publicly%20available%20models%20that%20predicted%20human%20generalisation.%0AIntrinsic%20dimensionality%20of%20representations%20had%20different%20effects%20on%20alignment%0Afor%20different%20model%20types.%20Lastly%2C%20we%20tested%20three%20sets%20of%20human-aligned%0Arepresentations%20and%20found%20no%20consistent%20improvements%20in%20predictive%20accuracy%0Acompared%20to%20the%20baselines.%20In%20conclusion%2C%20pretrained%20neural%20networks%20can%20serve%0Ato%20extract%20representations%20for%20cognitive%20models%2C%20as%20they%20appear%20to%20capture%20some%0Afundamental%20aspects%20of%20cognition%20that%20are%20transferable%20across%20tasks.%20Both%20our%0Aparadigms%20and%20modelling%20approach%20offer%20a%20novel%20way%20to%20quantify%20alignment%0Abetween%20neural%20networks%20and%20humans%20and%20extend%20cognitive%20science%20into%20more%0Anaturalistic%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520alignment%2520between%2520humans%2520and%2520neural%2520network%2520representations%250A%2520%2520in%2520image-based%2520learning%2520tasks%26entry.906535625%3DCan%2520Demircan%2520and%2520Tankred%2520Saanum%2520and%2520Leonardo%2520Pettini%2520and%2520Marcel%2520Binz%2520and%2520Blazej%2520M%2520Baczkowski%2520and%2520Christian%2520F%2520Doeller%2520and%2520Mona%2520M%2520Garvert%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Humans%2520represent%2520scenes%2520and%2520objects%2520in%2520rich%2520feature%2520spaces%252C%2520carrying%250Ainformation%2520that%2520allows%2520us%2520to%2520generalise%2520about%2520category%2520memberships%2520and%250Aabstract%2520functions%2520with%2520few%2520examples.%2520What%2520determines%2520whether%2520a%2520neural%2520network%250Amodel%2520generalises%2520like%2520a%2520human%253F%2520We%2520tested%2520how%2520well%2520the%2520representations%2520of%2520%252486%2524%250Apretrained%2520neural%2520network%2520models%2520mapped%2520to%2520human%2520learning%2520trajectories%2520across%250Atwo%2520tasks%2520where%2520humans%2520had%2520to%2520learn%2520continuous%2520relationships%2520and%2520categories%2520of%250Anatural%2520images.%2520In%2520these%2520tasks%252C%2520both%2520human%2520participants%2520and%2520neural%2520networks%250Asuccessfully%2520identified%2520the%2520relevant%2520stimulus%2520features%2520within%2520a%2520few%2520trials%252C%250Ademonstrating%2520effective%2520generalisation.%2520We%2520found%2520that%2520while%2520training%2520dataset%250Asize%2520was%2520a%2520core%2520determinant%2520of%2520alignment%2520with%2520human%2520choices%252C%2520contrastive%250Atraining%2520with%2520multi-modal%2520data%2520%2528text%2520and%2520imagery%2529%2520was%2520a%2520common%2520feature%2520of%250Acurrently%2520publicly%2520available%2520models%2520that%2520predicted%2520human%2520generalisation.%250AIntrinsic%2520dimensionality%2520of%2520representations%2520had%2520different%2520effects%2520on%2520alignment%250Afor%2520different%2520model%2520types.%2520Lastly%252C%2520we%2520tested%2520three%2520sets%2520of%2520human-aligned%250Arepresentations%2520and%2520found%2520no%2520consistent%2520improvements%2520in%2520predictive%2520accuracy%250Acompared%2520to%2520the%2520baselines.%2520In%2520conclusion%252C%2520pretrained%2520neural%2520networks%2520can%2520serve%250Ato%2520extract%2520representations%2520for%2520cognitive%2520models%252C%2520as%2520they%2520appear%2520to%2520capture%2520some%250Afundamental%2520aspects%2520of%2520cognition%2520that%2520are%2520transferable%2520across%2520tasks.%2520Both%2520our%250Aparadigms%2520and%2520modelling%2520approach%2520offer%2520a%2520novel%2520way%2520to%2520quantify%2520alignment%250Abetween%2520neural%2520networks%2520and%2520humans%2520and%2520extend%2520cognitive%2520science%2520into%2520more%250Anaturalistic%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20alignment%20between%20humans%20and%20neural%20network%20representations%0A%20%20in%20image-based%20learning%20tasks&entry.906535625=Can%20Demircan%20and%20Tankred%20Saanum%20and%20Leonardo%20Pettini%20and%20Marcel%20Binz%20and%20Blazej%20M%20Baczkowski%20and%20Christian%20F%20Doeller%20and%20Mona%20M%20Garvert%20and%20Eric%20Schulz&entry.1292438233=%20%20Humans%20represent%20scenes%20and%20objects%20in%20rich%20feature%20spaces%2C%20carrying%0Ainformation%20that%20allows%20us%20to%20generalise%20about%20category%20memberships%20and%0Aabstract%20functions%20with%20few%20examples.%20What%20determines%20whether%20a%20neural%20network%0Amodel%20generalises%20like%20a%20human%3F%20We%20tested%20how%20well%20the%20representations%20of%20%2486%24%0Apretrained%20neural%20network%20models%20mapped%20to%20human%20learning%20trajectories%20across%0Atwo%20tasks%20where%20humans%20had%20to%20learn%20continuous%20relationships%20and%20categories%20of%0Anatural%20images.%20In%20these%20tasks%2C%20both%20human%20participants%20and%20neural%20networks%0Asuccessfully%20identified%20the%20relevant%20stimulus%20features%20within%20a%20few%20trials%2C%0Ademonstrating%20effective%20generalisation.%20We%20found%20that%20while%20training%20dataset%0Asize%20was%20a%20core%20determinant%20of%20alignment%20with%20human%20choices%2C%20contrastive%0Atraining%20with%20multi-modal%20data%20%28text%20and%20imagery%29%20was%20a%20common%20feature%20of%0Acurrently%20publicly%20available%20models%20that%20predicted%20human%20generalisation.%0AIntrinsic%20dimensionality%20of%20representations%20had%20different%20effects%20on%20alignment%0Afor%20different%20model%20types.%20Lastly%2C%20we%20tested%20three%20sets%20of%20human-aligned%0Arepresentations%20and%20found%20no%20consistent%20improvements%20in%20predictive%20accuracy%0Acompared%20to%20the%20baselines.%20In%20conclusion%2C%20pretrained%20neural%20networks%20can%20serve%0Ato%20extract%20representations%20for%20cognitive%20models%2C%20as%20they%20appear%20to%20capture%20some%0Afundamental%20aspects%20of%20cognition%20that%20are%20transferable%20across%20tasks.%20Both%20our%0Aparadigms%20and%20modelling%20approach%20offer%20a%20novel%20way%20to%20quantify%20alignment%0Abetween%20neural%20networks%20and%20humans%20and%20extend%20cognitive%20science%20into%20more%0Anaturalistic%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09377v2&entry.124074799=Read"},
{"title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs", "author": "Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael I. Jordan and Song Mei", "abstract": "  Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.\n", "link": "http://arxiv.org/abs/2410.13835v2", "date": "2024-11-07", "relevancy": 2.709, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs&body=Title%3A%20Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs%0AAuthor%3A%20Tianyu%20Guo%20and%20Druv%20Pai%20and%20Yu%20Bai%20and%20Jiantao%20Jiao%20and%20Michael%20I.%20Jordan%20and%20Song%20Mei%0AAbstract%3A%20%20%20Practitioners%20have%20consistently%20observed%20three%20puzzling%20phenomena%20in%0Atransformer-based%20large%20language%20models%20%28LLMs%29%3A%20attention%20sinks%2C%20value-state%0Adrains%2C%20and%20residual-state%20peaks%2C%20collectively%20referred%20to%20as%20extreme-token%0Aphenomena.%20These%20phenomena%20are%20characterized%20by%20certain%20so-called%20%22sink%20tokens%22%0Areceiving%20disproportionately%20high%20attention%20weights%2C%20exhibiting%20significantly%0Asmaller%20value%20states%2C%20and%20having%20much%20larger%20residual-state%20norms%20than%20those%20of%0Aother%20tokens.%20These%20extreme%20tokens%20give%20rise%20to%20various%20challenges%20in%20LLM%0Ainference%2C%20quantization%2C%20and%20interpretability.%0A%20%20We%20elucidate%20the%20mechanisms%20behind%20extreme-token%20phenomena.%20First%2C%20we%20show%0Athat%20these%20phenomena%20arise%20in%20very%20simple%20architectures%20--%20transformers%20with%0Aone%20to%20three%20layers%20--%20trained%20on%20a%20toy%20model%2C%20the%20Bigram-Backcopy%20%28BB%29%20task.%0AIn%20this%20setting%2C%20we%20identify%20an%20active-dormant%20mechanism%2C%20where%20attention%20heads%0Abecome%20sinks%20for%20specific%20input%20domains%20while%20remaining%20non-sinks%20for%20others.%0AOur%20theoretical%20analysis%20of%20the%20training%20dynamics%20reveals%20that%20these%20phenomena%0Aare%20driven%20by%20a%20mutual%20reinforcement%20mechanism.%20Building%20on%20these%20insights%2C%20we%0Apropose%20strategies%20to%20mitigate%20extreme-token%20phenomena%20during%20pretraining%2C%0Aincluding%20replacing%20softmax%20with%20ReLU%20and%20Adam%20with%20SGD.%20Next%2C%20we%20extend%20our%0Aanalysis%20to%20pretrained%20LLMs%2C%20including%20Llama%20and%20OLMo%2C%20showing%20that%20many%0Aattention%20heads%20exhibit%20a%20similar%20active-dormant%20mechanism%20as%20in%20the%20BB%20task%2C%0Aand%20that%20the%20mutual%20reinforcement%20mechanism%20also%20governs%20the%20emergence%20of%0Aextreme-token%20phenomena%20during%20LLM%20pretraining.%20Our%20results%20reveal%20that%20many%20of%0Athe%20static%20and%20dynamic%20properties%20of%20extreme-token%20phenomena%20predicted%20by%20the%0ABB%20task%20align%20with%20observations%20in%20pretrained%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive-Dormant%2520Attention%2520Heads%253A%2520Mechanistically%2520Demystifying%250A%2520%2520Extreme-Token%2520Phenomena%2520in%2520LLMs%26entry.906535625%3DTianyu%2520Guo%2520and%2520Druv%2520Pai%2520and%2520Yu%2520Bai%2520and%2520Jiantao%2520Jiao%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Song%2520Mei%26entry.1292438233%3D%2520%2520Practitioners%2520have%2520consistently%2520observed%2520three%2520puzzling%2520phenomena%2520in%250Atransformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%253A%2520attention%2520sinks%252C%2520value-state%250Adrains%252C%2520and%2520residual-state%2520peaks%252C%2520collectively%2520referred%2520to%2520as%2520extreme-token%250Aphenomena.%2520These%2520phenomena%2520are%2520characterized%2520by%2520certain%2520so-called%2520%2522sink%2520tokens%2522%250Areceiving%2520disproportionately%2520high%2520attention%2520weights%252C%2520exhibiting%2520significantly%250Asmaller%2520value%2520states%252C%2520and%2520having%2520much%2520larger%2520residual-state%2520norms%2520than%2520those%2520of%250Aother%2520tokens.%2520These%2520extreme%2520tokens%2520give%2520rise%2520to%2520various%2520challenges%2520in%2520LLM%250Ainference%252C%2520quantization%252C%2520and%2520interpretability.%250A%2520%2520We%2520elucidate%2520the%2520mechanisms%2520behind%2520extreme-token%2520phenomena.%2520First%252C%2520we%2520show%250Athat%2520these%2520phenomena%2520arise%2520in%2520very%2520simple%2520architectures%2520--%2520transformers%2520with%250Aone%2520to%2520three%2520layers%2520--%2520trained%2520on%2520a%2520toy%2520model%252C%2520the%2520Bigram-Backcopy%2520%2528BB%2529%2520task.%250AIn%2520this%2520setting%252C%2520we%2520identify%2520an%2520active-dormant%2520mechanism%252C%2520where%2520attention%2520heads%250Abecome%2520sinks%2520for%2520specific%2520input%2520domains%2520while%2520remaining%2520non-sinks%2520for%2520others.%250AOur%2520theoretical%2520analysis%2520of%2520the%2520training%2520dynamics%2520reveals%2520that%2520these%2520phenomena%250Aare%2520driven%2520by%2520a%2520mutual%2520reinforcement%2520mechanism.%2520Building%2520on%2520these%2520insights%252C%2520we%250Apropose%2520strategies%2520to%2520mitigate%2520extreme-token%2520phenomena%2520during%2520pretraining%252C%250Aincluding%2520replacing%2520softmax%2520with%2520ReLU%2520and%2520Adam%2520with%2520SGD.%2520Next%252C%2520we%2520extend%2520our%250Aanalysis%2520to%2520pretrained%2520LLMs%252C%2520including%2520Llama%2520and%2520OLMo%252C%2520showing%2520that%2520many%250Aattention%2520heads%2520exhibit%2520a%2520similar%2520active-dormant%2520mechanism%2520as%2520in%2520the%2520BB%2520task%252C%250Aand%2520that%2520the%2520mutual%2520reinforcement%2520mechanism%2520also%2520governs%2520the%2520emergence%2520of%250Aextreme-token%2520phenomena%2520during%2520LLM%2520pretraining.%2520Our%2520results%2520reveal%2520that%2520many%2520of%250Athe%2520static%2520and%2520dynamic%2520properties%2520of%2520extreme-token%2520phenomena%2520predicted%2520by%2520the%250ABB%2520task%2520align%2520with%2520observations%2520in%2520pretrained%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active-Dormant%20Attention%20Heads%3A%20Mechanistically%20Demystifying%0A%20%20Extreme-Token%20Phenomena%20in%20LLMs&entry.906535625=Tianyu%20Guo%20and%20Druv%20Pai%20and%20Yu%20Bai%20and%20Jiantao%20Jiao%20and%20Michael%20I.%20Jordan%20and%20Song%20Mei&entry.1292438233=%20%20Practitioners%20have%20consistently%20observed%20three%20puzzling%20phenomena%20in%0Atransformer-based%20large%20language%20models%20%28LLMs%29%3A%20attention%20sinks%2C%20value-state%0Adrains%2C%20and%20residual-state%20peaks%2C%20collectively%20referred%20to%20as%20extreme-token%0Aphenomena.%20These%20phenomena%20are%20characterized%20by%20certain%20so-called%20%22sink%20tokens%22%0Areceiving%20disproportionately%20high%20attention%20weights%2C%20exhibiting%20significantly%0Asmaller%20value%20states%2C%20and%20having%20much%20larger%20residual-state%20norms%20than%20those%20of%0Aother%20tokens.%20These%20extreme%20tokens%20give%20rise%20to%20various%20challenges%20in%20LLM%0Ainference%2C%20quantization%2C%20and%20interpretability.%0A%20%20We%20elucidate%20the%20mechanisms%20behind%20extreme-token%20phenomena.%20First%2C%20we%20show%0Athat%20these%20phenomena%20arise%20in%20very%20simple%20architectures%20--%20transformers%20with%0Aone%20to%20three%20layers%20--%20trained%20on%20a%20toy%20model%2C%20the%20Bigram-Backcopy%20%28BB%29%20task.%0AIn%20this%20setting%2C%20we%20identify%20an%20active-dormant%20mechanism%2C%20where%20attention%20heads%0Abecome%20sinks%20for%20specific%20input%20domains%20while%20remaining%20non-sinks%20for%20others.%0AOur%20theoretical%20analysis%20of%20the%20training%20dynamics%20reveals%20that%20these%20phenomena%0Aare%20driven%20by%20a%20mutual%20reinforcement%20mechanism.%20Building%20on%20these%20insights%2C%20we%0Apropose%20strategies%20to%20mitigate%20extreme-token%20phenomena%20during%20pretraining%2C%0Aincluding%20replacing%20softmax%20with%20ReLU%20and%20Adam%20with%20SGD.%20Next%2C%20we%20extend%20our%0Aanalysis%20to%20pretrained%20LLMs%2C%20including%20Llama%20and%20OLMo%2C%20showing%20that%20many%0Aattention%20heads%20exhibit%20a%20similar%20active-dormant%20mechanism%20as%20in%20the%20BB%20task%2C%0Aand%20that%20the%20mutual%20reinforcement%20mechanism%20also%20governs%20the%20emergence%20of%0Aextreme-token%20phenomena%20during%20LLM%20pretraining.%20Our%20results%20reveal%20that%20many%20of%0Athe%20static%20and%20dynamic%20properties%20of%20extreme-token%20phenomena%20predicted%20by%20the%0ABB%20task%20align%20with%20observations%20in%20pretrained%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13835v2&entry.124074799=Read"},
{"title": "ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset", "author": "Olaf Wysocki and Yue Tan and Thomas Froech and Yan Xia and Magdalena Wysocki and Ludwig Hoegner and Daniel Cremers and Christoph Holst", "abstract": "  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n", "link": "http://arxiv.org/abs/2411.04865v1", "date": "2024-11-07", "relevancy": 2.6939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&body=Title%3A%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset%0AAuthor%3A%20Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst%0AAbstract%3A%20%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZAHA%253A%2520Introducing%2520the%2520Level%2520of%2520Facade%2520Generalization%2520and%2520the%2520Large-Scale%250A%2520%2520Point%2520Cloud%2520Facade%2520Semantic%2520Segmentation%2520Benchmark%2520Dataset%26entry.906535625%3DOlaf%2520Wysocki%2520and%2520Yue%2520Tan%2520and%2520Thomas%2520Froech%2520and%2520Yan%2520Xia%2520and%2520Magdalena%2520Wysocki%2520and%2520Ludwig%2520Hoegner%2520and%2520Daniel%2520Cremers%2520and%2520Christoph%2520Holst%26entry.1292438233%3D%2520%2520Facade%2520semantic%2520segmentation%2520is%2520a%2520long-standing%2520challenge%2520in%2520photogrammetry%250Aand%2520computer%2520vision.%2520Although%2520the%2520last%2520decades%2520have%2520witnessed%2520the%2520influx%2520of%250Afacade%2520segmentation%2520methods%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%2520facade%2520classes%250Aand%2520data%2520covering%2520the%2520architectural%2520variability.%2520In%2520ZAHA%252C%2520we%2520introduce%2520Level%2520of%250AFacade%2520Generalization%2520%2528LoFG%2529%252C%2520novel%2520hierarchical%2520facade%2520classes%2520designed%2520based%250Aon%2520international%2520urban%2520modeling%2520standards%252C%2520ensuring%2520compatibility%2520with%250Areal-world%2520challenging%2520classes%2520and%2520uniform%2520methods%2527%2520comparison.%2520Realizing%2520the%250ALoFG%252C%2520we%2520present%2520to%2520date%2520the%2520largest%2520semantic%25203D%2520facade%2520segmentation%2520dataset%252C%250Aproviding%2520601%2520million%2520annotated%2520points%2520at%2520five%2520and%252015%2520classes%2520of%2520LoFG2%2520and%250ALoFG3%252C%2520respectively.%2520Moreover%252C%2520we%2520analyze%2520the%2520performance%2520of%2520baseline%2520semantic%250Asegmentation%2520methods%2520on%2520our%2520introduced%2520LoFG%2520classes%2520and%2520data%252C%2520complementing%2520it%250Awith%2520a%2520discussion%2520on%2520the%2520unresolved%2520challenges%2520for%2520facade%2520segmentation.%2520We%250Afirmly%2520believe%2520that%2520ZAHA%2520shall%2520facilitate%2520further%2520development%2520of%25203D%2520facade%250Asemantic%2520segmentation%2520methods%252C%2520enabling%2520robust%2520segmentation%2520indispensable%2520in%250Acreating%2520urban%2520digital%2520twins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&entry.906535625=Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst&entry.1292438233=%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04865v1&entry.124074799=Read"},
{"title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\n  Multi-document Understanding", "author": "Jaemin Cho and Debanjan Mahata and Ozan Irsoy and Yujie He and Mohit Bansal", "abstract": "  Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.\n", "link": "http://arxiv.org/abs/2411.04952v1", "date": "2024-11-07", "relevancy": 2.6936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3DocRAG%3A%20Multi-modal%20Retrieval%20is%20What%20You%20Need%20for%20Multi-page%0A%20%20Multi-document%20Understanding&body=Title%3A%20M3DocRAG%3A%20Multi-modal%20Retrieval%20is%20What%20You%20Need%20for%20Multi-page%0A%20%20Multi-document%20Understanding%0AAuthor%3A%20Jaemin%20Cho%20and%20Debanjan%20Mahata%20and%20Ozan%20Irsoy%20and%20Yujie%20He%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Document%20visual%20question%20answering%20%28DocVQA%29%20pipelines%20that%20answer%20questions%0Afrom%20documents%20have%20broad%20applications.%20Existing%20methods%20focus%20on%20handling%0Asingle-page%20documents%20with%20multi-modal%20language%20models%20%28MLMs%29%2C%20or%20rely%20on%0Atext-based%20retrieval-augmented%20generation%20%28RAG%29%20that%20uses%20text%20extraction%20tools%0Asuch%20as%20optical%20character%20recognition%20%28OCR%29.%20However%2C%20there%20are%20difficulties%20in%0Aapplying%20these%20methods%20in%20real-world%20scenarios%3A%20%28a%29%20questions%20often%20require%0Ainformation%20across%20different%20pages%20or%20documents%2C%20where%20MLMs%20cannot%20handle%20many%0Along%20documents%3B%20%28b%29%20documents%20often%20have%20important%20information%20in%20visual%0Aelements%20such%20as%20figures%2C%20but%20text%20extraction%20tools%20ignore%20them.%20We%20introduce%0AM3DocRAG%2C%20a%20novel%20multi-modal%20RAG%20framework%20that%20flexibly%20accommodates%20various%0Adocument%20contexts%20%28closed-domain%20and%20open-domain%29%2C%20question%20hops%20%28single-hop%0Aand%20multi-hop%29%2C%20and%20evidence%20modalities%20%28text%2C%20chart%2C%20figure%2C%20etc.%29.%20M3DocRAG%0Afinds%20relevant%20documents%20and%20answers%20questions%20using%20a%20multi-modal%20retriever%0Aand%20an%20MLM%2C%20so%20that%20it%20can%20efficiently%20handle%20single%20or%20many%20documents%20while%0Apreserving%20visual%20information.%20Since%20previous%20DocVQA%20datasets%20ask%20questions%20in%0Athe%20context%20of%20a%20specific%20document%2C%20we%20also%20present%20M3DocVQA%2C%20a%20new%20benchmark%0Afor%20evaluating%20open-domain%20DocVQA%20over%203%2C000%2B%20PDF%20documents%20with%2040%2C000%2B%20pages.%0AIn%20three%20benchmarks%20%28M3DocVQA/MMLongBench-Doc/MP-DocVQA%29%2C%20empirical%20results%0Ashow%20that%20M3DocRAG%20with%20ColPali%20and%20Qwen2-VL%207B%20achieves%20superior%20performance%0Athan%20many%20strong%20baselines%2C%20including%20state-of-the-art%20performance%20in%0AMP-DocVQA.%20We%20provide%20comprehensive%20analyses%20of%20different%20indexing%2C%20MLMs%2C%20and%0Aretrieval%20models.%20Lastly%2C%20we%20qualitatively%20show%20that%20M3DocRAG%20can%20successfully%0Ahandle%20various%20scenarios%2C%20such%20as%20when%20relevant%20information%20exists%20across%0Amultiple%20pages%20and%20when%20answer%20evidence%20only%20exists%20in%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3DocRAG%253A%2520Multi-modal%2520Retrieval%2520is%2520What%2520You%2520Need%2520for%2520Multi-page%250A%2520%2520Multi-document%2520Understanding%26entry.906535625%3DJaemin%2520Cho%2520and%2520Debanjan%2520Mahata%2520and%2520Ozan%2520Irsoy%2520and%2520Yujie%2520He%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Document%2520visual%2520question%2520answering%2520%2528DocVQA%2529%2520pipelines%2520that%2520answer%2520questions%250Afrom%2520documents%2520have%2520broad%2520applications.%2520Existing%2520methods%2520focus%2520on%2520handling%250Asingle-page%2520documents%2520with%2520multi-modal%2520language%2520models%2520%2528MLMs%2529%252C%2520or%2520rely%2520on%250Atext-based%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520that%2520uses%2520text%2520extraction%2520tools%250Asuch%2520as%2520optical%2520character%2520recognition%2520%2528OCR%2529.%2520However%252C%2520there%2520are%2520difficulties%2520in%250Aapplying%2520these%2520methods%2520in%2520real-world%2520scenarios%253A%2520%2528a%2529%2520questions%2520often%2520require%250Ainformation%2520across%2520different%2520pages%2520or%2520documents%252C%2520where%2520MLMs%2520cannot%2520handle%2520many%250Along%2520documents%253B%2520%2528b%2529%2520documents%2520often%2520have%2520important%2520information%2520in%2520visual%250Aelements%2520such%2520as%2520figures%252C%2520but%2520text%2520extraction%2520tools%2520ignore%2520them.%2520We%2520introduce%250AM3DocRAG%252C%2520a%2520novel%2520multi-modal%2520RAG%2520framework%2520that%2520flexibly%2520accommodates%2520various%250Adocument%2520contexts%2520%2528closed-domain%2520and%2520open-domain%2529%252C%2520question%2520hops%2520%2528single-hop%250Aand%2520multi-hop%2529%252C%2520and%2520evidence%2520modalities%2520%2528text%252C%2520chart%252C%2520figure%252C%2520etc.%2529.%2520M3DocRAG%250Afinds%2520relevant%2520documents%2520and%2520answers%2520questions%2520using%2520a%2520multi-modal%2520retriever%250Aand%2520an%2520MLM%252C%2520so%2520that%2520it%2520can%2520efficiently%2520handle%2520single%2520or%2520many%2520documents%2520while%250Apreserving%2520visual%2520information.%2520Since%2520previous%2520DocVQA%2520datasets%2520ask%2520questions%2520in%250Athe%2520context%2520of%2520a%2520specific%2520document%252C%2520we%2520also%2520present%2520M3DocVQA%252C%2520a%2520new%2520benchmark%250Afor%2520evaluating%2520open-domain%2520DocVQA%2520over%25203%252C000%252B%2520PDF%2520documents%2520with%252040%252C000%252B%2520pages.%250AIn%2520three%2520benchmarks%2520%2528M3DocVQA/MMLongBench-Doc/MP-DocVQA%2529%252C%2520empirical%2520results%250Ashow%2520that%2520M3DocRAG%2520with%2520ColPali%2520and%2520Qwen2-VL%25207B%2520achieves%2520superior%2520performance%250Athan%2520many%2520strong%2520baselines%252C%2520including%2520state-of-the-art%2520performance%2520in%250AMP-DocVQA.%2520We%2520provide%2520comprehensive%2520analyses%2520of%2520different%2520indexing%252C%2520MLMs%252C%2520and%250Aretrieval%2520models.%2520Lastly%252C%2520we%2520qualitatively%2520show%2520that%2520M3DocRAG%2520can%2520successfully%250Ahandle%2520various%2520scenarios%252C%2520such%2520as%2520when%2520relevant%2520information%2520exists%2520across%250Amultiple%2520pages%2520and%2520when%2520answer%2520evidence%2520only%2520exists%2520in%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3DocRAG%3A%20Multi-modal%20Retrieval%20is%20What%20You%20Need%20for%20Multi-page%0A%20%20Multi-document%20Understanding&entry.906535625=Jaemin%20Cho%20and%20Debanjan%20Mahata%20and%20Ozan%20Irsoy%20and%20Yujie%20He%20and%20Mohit%20Bansal&entry.1292438233=%20%20Document%20visual%20question%20answering%20%28DocVQA%29%20pipelines%20that%20answer%20questions%0Afrom%20documents%20have%20broad%20applications.%20Existing%20methods%20focus%20on%20handling%0Asingle-page%20documents%20with%20multi-modal%20language%20models%20%28MLMs%29%2C%20or%20rely%20on%0Atext-based%20retrieval-augmented%20generation%20%28RAG%29%20that%20uses%20text%20extraction%20tools%0Asuch%20as%20optical%20character%20recognition%20%28OCR%29.%20However%2C%20there%20are%20difficulties%20in%0Aapplying%20these%20methods%20in%20real-world%20scenarios%3A%20%28a%29%20questions%20often%20require%0Ainformation%20across%20different%20pages%20or%20documents%2C%20where%20MLMs%20cannot%20handle%20many%0Along%20documents%3B%20%28b%29%20documents%20often%20have%20important%20information%20in%20visual%0Aelements%20such%20as%20figures%2C%20but%20text%20extraction%20tools%20ignore%20them.%20We%20introduce%0AM3DocRAG%2C%20a%20novel%20multi-modal%20RAG%20framework%20that%20flexibly%20accommodates%20various%0Adocument%20contexts%20%28closed-domain%20and%20open-domain%29%2C%20question%20hops%20%28single-hop%0Aand%20multi-hop%29%2C%20and%20evidence%20modalities%20%28text%2C%20chart%2C%20figure%2C%20etc.%29.%20M3DocRAG%0Afinds%20relevant%20documents%20and%20answers%20questions%20using%20a%20multi-modal%20retriever%0Aand%20an%20MLM%2C%20so%20that%20it%20can%20efficiently%20handle%20single%20or%20many%20documents%20while%0Apreserving%20visual%20information.%20Since%20previous%20DocVQA%20datasets%20ask%20questions%20in%0Athe%20context%20of%20a%20specific%20document%2C%20we%20also%20present%20M3DocVQA%2C%20a%20new%20benchmark%0Afor%20evaluating%20open-domain%20DocVQA%20over%203%2C000%2B%20PDF%20documents%20with%2040%2C000%2B%20pages.%0AIn%20three%20benchmarks%20%28M3DocVQA/MMLongBench-Doc/MP-DocVQA%29%2C%20empirical%20results%0Ashow%20that%20M3DocRAG%20with%20ColPali%20and%20Qwen2-VL%207B%20achieves%20superior%20performance%0Athan%20many%20strong%20baselines%2C%20including%20state-of-the-art%20performance%20in%0AMP-DocVQA.%20We%20provide%20comprehensive%20analyses%20of%20different%20indexing%2C%20MLMs%2C%20and%0Aretrieval%20models.%20Lastly%2C%20we%20qualitatively%20show%20that%20M3DocRAG%20can%20successfully%0Ahandle%20various%20scenarios%2C%20such%20as%20when%20relevant%20information%20exists%20across%0Amultiple%20pages%20and%20when%20answer%20evidence%20only%20exists%20in%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04952v1&entry.124074799=Read"},
{"title": "Reciprocal Point Learning Network with Large Electromagnetic Kernel for\n  SAR Open-Set Recognition", "author": "Xiayang Xiao and Zhuoxuan Li and Ruyi Zhang and Jiacheng Chen and Haipeng Wang", "abstract": "  The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) methods lie in their confinement by the closed-environment\nassumption, hindering their effective and robust handling of unknown target\ncategories in open environments. Open Set Recognition (OSR), a pivotal facet\nfor algorithmic practicality, intends to categorize known classes while\ndenoting unknown ones as \"unknown.\" The chief challenge in OSR involves\nconcurrently mitigating risks associated with generalizing features from a\nrestricted set of known classes to numerous unknown samples and the open space\nexposure to potential unknown data. To enhance open-set SAR classification, a\nmethod called scattering kernel with reciprocal learning network is proposed.\nInitially, a feature learning framework is constructed based on reciprocal\npoint learning (RPL), establishing a bounded space for potential unknown\nclasses. This approach indirectly introduces unknown information into a learner\nconfined to known classes, thereby acquiring more concise and discriminative\nrepresentations. Subsequently, considering the variability in the imaging of\ntargets at different angles and the discreteness of components in SAR images, a\nproposal is made to design convolutional kernels based on large-sized attribute\nscattering center models. This enhances the ability to extract intrinsic\nnon-linear features and specific scattering characteristics in SAR images,\nthereby improving the discriminative features of the model and mitigating the\nimpact of imaging variations on classification performance. Experiments on the\nMSTAR datasets substantiate the superior performance of the proposed approach\ncalled ASC-RPL over mainstream methods.\n", "link": "http://arxiv.org/abs/2411.04693v1", "date": "2024-11-07", "relevancy": 2.6677, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5317}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reciprocal%20Point%20Learning%20Network%20with%20Large%20Electromagnetic%20Kernel%20for%0A%20%20SAR%20Open-Set%20Recognition&body=Title%3A%20Reciprocal%20Point%20Learning%20Network%20with%20Large%20Electromagnetic%20Kernel%20for%0A%20%20SAR%20Open-Set%20Recognition%0AAuthor%3A%20Xiayang%20Xiao%20and%20Zhuoxuan%20Li%20and%20Ruyi%20Zhang%20and%20Jiacheng%20Chen%20and%20Haipeng%20Wang%0AAbstract%3A%20%20%20The%20limitations%20of%20existing%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Automatic%20Target%0ARecognition%20%28ATR%29%20methods%20lie%20in%20their%20confinement%20by%20the%20closed-environment%0Aassumption%2C%20hindering%20their%20effective%20and%20robust%20handling%20of%20unknown%20target%0Acategories%20in%20open%20environments.%20Open%20Set%20Recognition%20%28OSR%29%2C%20a%20pivotal%20facet%0Afor%20algorithmic%20practicality%2C%20intends%20to%20categorize%20known%20classes%20while%0Adenoting%20unknown%20ones%20as%20%22unknown.%22%20The%20chief%20challenge%20in%20OSR%20involves%0Aconcurrently%20mitigating%20risks%20associated%20with%20generalizing%20features%20from%20a%0Arestricted%20set%20of%20known%20classes%20to%20numerous%20unknown%20samples%20and%20the%20open%20space%0Aexposure%20to%20potential%20unknown%20data.%20To%20enhance%20open-set%20SAR%20classification%2C%20a%0Amethod%20called%20scattering%20kernel%20with%20reciprocal%20learning%20network%20is%20proposed.%0AInitially%2C%20a%20feature%20learning%20framework%20is%20constructed%20based%20on%20reciprocal%0Apoint%20learning%20%28RPL%29%2C%20establishing%20a%20bounded%20space%20for%20potential%20unknown%0Aclasses.%20This%20approach%20indirectly%20introduces%20unknown%20information%20into%20a%20learner%0Aconfined%20to%20known%20classes%2C%20thereby%20acquiring%20more%20concise%20and%20discriminative%0Arepresentations.%20Subsequently%2C%20considering%20the%20variability%20in%20the%20imaging%20of%0Atargets%20at%20different%20angles%20and%20the%20discreteness%20of%20components%20in%20SAR%20images%2C%20a%0Aproposal%20is%20made%20to%20design%20convolutional%20kernels%20based%20on%20large-sized%20attribute%0Ascattering%20center%20models.%20This%20enhances%20the%20ability%20to%20extract%20intrinsic%0Anon-linear%20features%20and%20specific%20scattering%20characteristics%20in%20SAR%20images%2C%0Athereby%20improving%20the%20discriminative%20features%20of%20the%20model%20and%20mitigating%20the%0Aimpact%20of%20imaging%20variations%20on%20classification%20performance.%20Experiments%20on%20the%0AMSTAR%20datasets%20substantiate%20the%20superior%20performance%20of%20the%20proposed%20approach%0Acalled%20ASC-RPL%20over%20mainstream%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReciprocal%2520Point%2520Learning%2520Network%2520with%2520Large%2520Electromagnetic%2520Kernel%2520for%250A%2520%2520SAR%2520Open-Set%2520Recognition%26entry.906535625%3DXiayang%2520Xiao%2520and%2520Zhuoxuan%2520Li%2520and%2520Ruyi%2520Zhang%2520and%2520Jiacheng%2520Chen%2520and%2520Haipeng%2520Wang%26entry.1292438233%3D%2520%2520The%2520limitations%2520of%2520existing%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520Automatic%2520Target%250ARecognition%2520%2528ATR%2529%2520methods%2520lie%2520in%2520their%2520confinement%2520by%2520the%2520closed-environment%250Aassumption%252C%2520hindering%2520their%2520effective%2520and%2520robust%2520handling%2520of%2520unknown%2520target%250Acategories%2520in%2520open%2520environments.%2520Open%2520Set%2520Recognition%2520%2528OSR%2529%252C%2520a%2520pivotal%2520facet%250Afor%2520algorithmic%2520practicality%252C%2520intends%2520to%2520categorize%2520known%2520classes%2520while%250Adenoting%2520unknown%2520ones%2520as%2520%2522unknown.%2522%2520The%2520chief%2520challenge%2520in%2520OSR%2520involves%250Aconcurrently%2520mitigating%2520risks%2520associated%2520with%2520generalizing%2520features%2520from%2520a%250Arestricted%2520set%2520of%2520known%2520classes%2520to%2520numerous%2520unknown%2520samples%2520and%2520the%2520open%2520space%250Aexposure%2520to%2520potential%2520unknown%2520data.%2520To%2520enhance%2520open-set%2520SAR%2520classification%252C%2520a%250Amethod%2520called%2520scattering%2520kernel%2520with%2520reciprocal%2520learning%2520network%2520is%2520proposed.%250AInitially%252C%2520a%2520feature%2520learning%2520framework%2520is%2520constructed%2520based%2520on%2520reciprocal%250Apoint%2520learning%2520%2528RPL%2529%252C%2520establishing%2520a%2520bounded%2520space%2520for%2520potential%2520unknown%250Aclasses.%2520This%2520approach%2520indirectly%2520introduces%2520unknown%2520information%2520into%2520a%2520learner%250Aconfined%2520to%2520known%2520classes%252C%2520thereby%2520acquiring%2520more%2520concise%2520and%2520discriminative%250Arepresentations.%2520Subsequently%252C%2520considering%2520the%2520variability%2520in%2520the%2520imaging%2520of%250Atargets%2520at%2520different%2520angles%2520and%2520the%2520discreteness%2520of%2520components%2520in%2520SAR%2520images%252C%2520a%250Aproposal%2520is%2520made%2520to%2520design%2520convolutional%2520kernels%2520based%2520on%2520large-sized%2520attribute%250Ascattering%2520center%2520models.%2520This%2520enhances%2520the%2520ability%2520to%2520extract%2520intrinsic%250Anon-linear%2520features%2520and%2520specific%2520scattering%2520characteristics%2520in%2520SAR%2520images%252C%250Athereby%2520improving%2520the%2520discriminative%2520features%2520of%2520the%2520model%2520and%2520mitigating%2520the%250Aimpact%2520of%2520imaging%2520variations%2520on%2520classification%2520performance.%2520Experiments%2520on%2520the%250AMSTAR%2520datasets%2520substantiate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520approach%250Acalled%2520ASC-RPL%2520over%2520mainstream%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reciprocal%20Point%20Learning%20Network%20with%20Large%20Electromagnetic%20Kernel%20for%0A%20%20SAR%20Open-Set%20Recognition&entry.906535625=Xiayang%20Xiao%20and%20Zhuoxuan%20Li%20and%20Ruyi%20Zhang%20and%20Jiacheng%20Chen%20and%20Haipeng%20Wang&entry.1292438233=%20%20The%20limitations%20of%20existing%20Synthetic%20Aperture%20Radar%20%28SAR%29%20Automatic%20Target%0ARecognition%20%28ATR%29%20methods%20lie%20in%20their%20confinement%20by%20the%20closed-environment%0Aassumption%2C%20hindering%20their%20effective%20and%20robust%20handling%20of%20unknown%20target%0Acategories%20in%20open%20environments.%20Open%20Set%20Recognition%20%28OSR%29%2C%20a%20pivotal%20facet%0Afor%20algorithmic%20practicality%2C%20intends%20to%20categorize%20known%20classes%20while%0Adenoting%20unknown%20ones%20as%20%22unknown.%22%20The%20chief%20challenge%20in%20OSR%20involves%0Aconcurrently%20mitigating%20risks%20associated%20with%20generalizing%20features%20from%20a%0Arestricted%20set%20of%20known%20classes%20to%20numerous%20unknown%20samples%20and%20the%20open%20space%0Aexposure%20to%20potential%20unknown%20data.%20To%20enhance%20open-set%20SAR%20classification%2C%20a%0Amethod%20called%20scattering%20kernel%20with%20reciprocal%20learning%20network%20is%20proposed.%0AInitially%2C%20a%20feature%20learning%20framework%20is%20constructed%20based%20on%20reciprocal%0Apoint%20learning%20%28RPL%29%2C%20establishing%20a%20bounded%20space%20for%20potential%20unknown%0Aclasses.%20This%20approach%20indirectly%20introduces%20unknown%20information%20into%20a%20learner%0Aconfined%20to%20known%20classes%2C%20thereby%20acquiring%20more%20concise%20and%20discriminative%0Arepresentations.%20Subsequently%2C%20considering%20the%20variability%20in%20the%20imaging%20of%0Atargets%20at%20different%20angles%20and%20the%20discreteness%20of%20components%20in%20SAR%20images%2C%20a%0Aproposal%20is%20made%20to%20design%20convolutional%20kernels%20based%20on%20large-sized%20attribute%0Ascattering%20center%20models.%20This%20enhances%20the%20ability%20to%20extract%20intrinsic%0Anon-linear%20features%20and%20specific%20scattering%20characteristics%20in%20SAR%20images%2C%0Athereby%20improving%20the%20discriminative%20features%20of%20the%20model%20and%20mitigating%20the%0Aimpact%20of%20imaging%20variations%20on%20classification%20performance.%20Experiments%20on%20the%0AMSTAR%20datasets%20substantiate%20the%20superior%20performance%20of%20the%20proposed%20approach%0Acalled%20ASC-RPL%20over%20mainstream%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04693v1&entry.124074799=Read"},
{"title": "Fed-LDR: Federated Local Data-infused Graph Creation with Node-centric\n  Model Refinement", "author": "Jiechao Gao and Yuangang Li and Syeda Faiza Ahmed", "abstract": "  The rapid acceleration of global urbanization has introduced novel challenges\nin enhancing urban infrastructure and services. Spatio-temporal data,\nintegrating spatial and temporal dimensions, has emerged as a critical tool for\nunderstanding urban phenomena and promoting sustainability. In this context,\nFederated Learning (FL) has gained prominence as a distributed learning\nparadigm aligned with the privacy requirements of urban IoT environments.\nHowever, integrating traditional and deep learning models into the FL framework\nposes significant challenges, particularly in capturing complex spatio-temporal\ndependencies and adapting to diverse urban conditions. To address these\nchallenges, we propose the Federated Local Data-Infused Graph Creation with\nNode-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR leverages FL and\nGraph Convolutional Networks (GCN) to enhance spatio-temporal data analysis in\nurban environments. The algorithm comprises two key modules: (1) the Local\nData-Infused Graph Creation (LDIGC) module, which dynamically reconfigures\nadjacency matrices to reflect evolving spatial relationships within urban\nenvironments, and (2) the Node-centric Model Refinement (NoMoR) module, which\ncustomizes model parameters for individual urban nodes to accommodate\nheterogeneity. Evaluations on the PeMSD4 and PeMSD8 datasets demonstrate\nFed-LDR's superior performance over six baseline methods. Fed-LDR achieved the\nlowest Mean Absolute Error (MAE) values of 20.15 and 17.30, and the lowest Root\nMean Square Error (RMSE) values of 32.30 and 27.15, respectively, while\nmaintaining a high correlation coefficient of 0.96 across both datasets.\nNotably, on the PeMSD4 dataset, Fed-LDR reduced MAE and RMSE by up to 81\\% and\n78\\%, respectively, compared to the best-performing baseline FedMedian.\n", "link": "http://arxiv.org/abs/2411.04936v1", "date": "2024-11-07", "relevancy": 2.6463, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5382}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5267}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fed-LDR%3A%20Federated%20Local%20Data-infused%20Graph%20Creation%20with%20Node-centric%0A%20%20Model%20Refinement&body=Title%3A%20Fed-LDR%3A%20Federated%20Local%20Data-infused%20Graph%20Creation%20with%20Node-centric%0A%20%20Model%20Refinement%0AAuthor%3A%20Jiechao%20Gao%20and%20Yuangang%20Li%20and%20Syeda%20Faiza%20Ahmed%0AAbstract%3A%20%20%20The%20rapid%20acceleration%20of%20global%20urbanization%20has%20introduced%20novel%20challenges%0Ain%20enhancing%20urban%20infrastructure%20and%20services.%20Spatio-temporal%20data%2C%0Aintegrating%20spatial%20and%20temporal%20dimensions%2C%20has%20emerged%20as%20a%20critical%20tool%20for%0Aunderstanding%20urban%20phenomena%20and%20promoting%20sustainability.%20In%20this%20context%2C%0AFederated%20Learning%20%28FL%29%20has%20gained%20prominence%20as%20a%20distributed%20learning%0Aparadigm%20aligned%20with%20the%20privacy%20requirements%20of%20urban%20IoT%20environments.%0AHowever%2C%20integrating%20traditional%20and%20deep%20learning%20models%20into%20the%20FL%20framework%0Aposes%20significant%20challenges%2C%20particularly%20in%20capturing%20complex%20spatio-temporal%0Adependencies%20and%20adapting%20to%20diverse%20urban%20conditions.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Federated%20Local%20Data-Infused%20Graph%20Creation%20with%0ANode-centric%20Model%20Refinement%20%28Fed-LDR%29%20algorithm.%20Fed-LDR%20leverages%20FL%20and%0AGraph%20Convolutional%20Networks%20%28GCN%29%20to%20enhance%20spatio-temporal%20data%20analysis%20in%0Aurban%20environments.%20The%20algorithm%20comprises%20two%20key%20modules%3A%20%281%29%20the%20Local%0AData-Infused%20Graph%20Creation%20%28LDIGC%29%20module%2C%20which%20dynamically%20reconfigures%0Aadjacency%20matrices%20to%20reflect%20evolving%20spatial%20relationships%20within%20urban%0Aenvironments%2C%20and%20%282%29%20the%20Node-centric%20Model%20Refinement%20%28NoMoR%29%20module%2C%20which%0Acustomizes%20model%20parameters%20for%20individual%20urban%20nodes%20to%20accommodate%0Aheterogeneity.%20Evaluations%20on%20the%20PeMSD4%20and%20PeMSD8%20datasets%20demonstrate%0AFed-LDR%27s%20superior%20performance%20over%20six%20baseline%20methods.%20Fed-LDR%20achieved%20the%0Alowest%20Mean%20Absolute%20Error%20%28MAE%29%20values%20of%2020.15%20and%2017.30%2C%20and%20the%20lowest%20Root%0AMean%20Square%20Error%20%28RMSE%29%20values%20of%2032.30%20and%2027.15%2C%20respectively%2C%20while%0Amaintaining%20a%20high%20correlation%20coefficient%20of%200.96%20across%20both%20datasets.%0ANotably%2C%20on%20the%20PeMSD4%20dataset%2C%20Fed-LDR%20reduced%20MAE%20and%20RMSE%20by%20up%20to%2081%5C%25%20and%0A78%5C%25%2C%20respectively%2C%20compared%20to%20the%20best-performing%20baseline%20FedMedian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFed-LDR%253A%2520Federated%2520Local%2520Data-infused%2520Graph%2520Creation%2520with%2520Node-centric%250A%2520%2520Model%2520Refinement%26entry.906535625%3DJiechao%2520Gao%2520and%2520Yuangang%2520Li%2520and%2520Syeda%2520Faiza%2520Ahmed%26entry.1292438233%3D%2520%2520The%2520rapid%2520acceleration%2520of%2520global%2520urbanization%2520has%2520introduced%2520novel%2520challenges%250Ain%2520enhancing%2520urban%2520infrastructure%2520and%2520services.%2520Spatio-temporal%2520data%252C%250Aintegrating%2520spatial%2520and%2520temporal%2520dimensions%252C%2520has%2520emerged%2520as%2520a%2520critical%2520tool%2520for%250Aunderstanding%2520urban%2520phenomena%2520and%2520promoting%2520sustainability.%2520In%2520this%2520context%252C%250AFederated%2520Learning%2520%2528FL%2529%2520has%2520gained%2520prominence%2520as%2520a%2520distributed%2520learning%250Aparadigm%2520aligned%2520with%2520the%2520privacy%2520requirements%2520of%2520urban%2520IoT%2520environments.%250AHowever%252C%2520integrating%2520traditional%2520and%2520deep%2520learning%2520models%2520into%2520the%2520FL%2520framework%250Aposes%2520significant%2520challenges%252C%2520particularly%2520in%2520capturing%2520complex%2520spatio-temporal%250Adependencies%2520and%2520adapting%2520to%2520diverse%2520urban%2520conditions.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520Federated%2520Local%2520Data-Infused%2520Graph%2520Creation%2520with%250ANode-centric%2520Model%2520Refinement%2520%2528Fed-LDR%2529%2520algorithm.%2520Fed-LDR%2520leverages%2520FL%2520and%250AGraph%2520Convolutional%2520Networks%2520%2528GCN%2529%2520to%2520enhance%2520spatio-temporal%2520data%2520analysis%2520in%250Aurban%2520environments.%2520The%2520algorithm%2520comprises%2520two%2520key%2520modules%253A%2520%25281%2529%2520the%2520Local%250AData-Infused%2520Graph%2520Creation%2520%2528LDIGC%2529%2520module%252C%2520which%2520dynamically%2520reconfigures%250Aadjacency%2520matrices%2520to%2520reflect%2520evolving%2520spatial%2520relationships%2520within%2520urban%250Aenvironments%252C%2520and%2520%25282%2529%2520the%2520Node-centric%2520Model%2520Refinement%2520%2528NoMoR%2529%2520module%252C%2520which%250Acustomizes%2520model%2520parameters%2520for%2520individual%2520urban%2520nodes%2520to%2520accommodate%250Aheterogeneity.%2520Evaluations%2520on%2520the%2520PeMSD4%2520and%2520PeMSD8%2520datasets%2520demonstrate%250AFed-LDR%2527s%2520superior%2520performance%2520over%2520six%2520baseline%2520methods.%2520Fed-LDR%2520achieved%2520the%250Alowest%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520values%2520of%252020.15%2520and%252017.30%252C%2520and%2520the%2520lowest%2520Root%250AMean%2520Square%2520Error%2520%2528RMSE%2529%2520values%2520of%252032.30%2520and%252027.15%252C%2520respectively%252C%2520while%250Amaintaining%2520a%2520high%2520correlation%2520coefficient%2520of%25200.96%2520across%2520both%2520datasets.%250ANotably%252C%2520on%2520the%2520PeMSD4%2520dataset%252C%2520Fed-LDR%2520reduced%2520MAE%2520and%2520RMSE%2520by%2520up%2520to%252081%255C%2525%2520and%250A78%255C%2525%252C%2520respectively%252C%2520compared%2520to%2520the%2520best-performing%2520baseline%2520FedMedian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fed-LDR%3A%20Federated%20Local%20Data-infused%20Graph%20Creation%20with%20Node-centric%0A%20%20Model%20Refinement&entry.906535625=Jiechao%20Gao%20and%20Yuangang%20Li%20and%20Syeda%20Faiza%20Ahmed&entry.1292438233=%20%20The%20rapid%20acceleration%20of%20global%20urbanization%20has%20introduced%20novel%20challenges%0Ain%20enhancing%20urban%20infrastructure%20and%20services.%20Spatio-temporal%20data%2C%0Aintegrating%20spatial%20and%20temporal%20dimensions%2C%20has%20emerged%20as%20a%20critical%20tool%20for%0Aunderstanding%20urban%20phenomena%20and%20promoting%20sustainability.%20In%20this%20context%2C%0AFederated%20Learning%20%28FL%29%20has%20gained%20prominence%20as%20a%20distributed%20learning%0Aparadigm%20aligned%20with%20the%20privacy%20requirements%20of%20urban%20IoT%20environments.%0AHowever%2C%20integrating%20traditional%20and%20deep%20learning%20models%20into%20the%20FL%20framework%0Aposes%20significant%20challenges%2C%20particularly%20in%20capturing%20complex%20spatio-temporal%0Adependencies%20and%20adapting%20to%20diverse%20urban%20conditions.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Federated%20Local%20Data-Infused%20Graph%20Creation%20with%0ANode-centric%20Model%20Refinement%20%28Fed-LDR%29%20algorithm.%20Fed-LDR%20leverages%20FL%20and%0AGraph%20Convolutional%20Networks%20%28GCN%29%20to%20enhance%20spatio-temporal%20data%20analysis%20in%0Aurban%20environments.%20The%20algorithm%20comprises%20two%20key%20modules%3A%20%281%29%20the%20Local%0AData-Infused%20Graph%20Creation%20%28LDIGC%29%20module%2C%20which%20dynamically%20reconfigures%0Aadjacency%20matrices%20to%20reflect%20evolving%20spatial%20relationships%20within%20urban%0Aenvironments%2C%20and%20%282%29%20the%20Node-centric%20Model%20Refinement%20%28NoMoR%29%20module%2C%20which%0Acustomizes%20model%20parameters%20for%20individual%20urban%20nodes%20to%20accommodate%0Aheterogeneity.%20Evaluations%20on%20the%20PeMSD4%20and%20PeMSD8%20datasets%20demonstrate%0AFed-LDR%27s%20superior%20performance%20over%20six%20baseline%20methods.%20Fed-LDR%20achieved%20the%0Alowest%20Mean%20Absolute%20Error%20%28MAE%29%20values%20of%2020.15%20and%2017.30%2C%20and%20the%20lowest%20Root%0AMean%20Square%20Error%20%28RMSE%29%20values%20of%2032.30%20and%2027.15%2C%20respectively%2C%20while%0Amaintaining%20a%20high%20correlation%20coefficient%20of%200.96%20across%20both%20datasets.%0ANotably%2C%20on%20the%20PeMSD4%20dataset%2C%20Fed-LDR%20reduced%20MAE%20and%20RMSE%20by%20up%20to%2081%5C%25%20and%0A78%5C%25%2C%20respectively%2C%20compared%20to%20the%20best-performing%20baseline%20FedMedian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04936v1&entry.124074799=Read"},
{"title": "The Impact of Semi-Supervised Learning on Line Segment Detection", "author": "Johanna Engman and Karl \u00c5str\u00f6m and Magnus Oskarsson", "abstract": "  In this paper we present a method for line segment detection in images, based\non a semi-supervised framework. Leveraging the use of a consistency loss based\non differently augmented and perturbed unlabeled images with a small amount of\nlabeled data, we show comparable results to fully supervised methods. This\nopens up application scenarios where annotation is difficult or expensive, and\nfor domain specific adaptation of models. We are specifically interested in\nreal-time and online applications, and investigate small and efficient learning\nbackbones. Our method is to our knowledge the first to target line detection\nusing modern state-of-the-art methodologies for semi-supervised learning. We\ntest the method on both standard benchmarks and domain specific scenarios for\nforestry applications, showing the tractability of the proposed method.\n", "link": "http://arxiv.org/abs/2411.04596v1", "date": "2024-11-07", "relevancy": 2.6321, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5647}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5083}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Semi-Supervised%20Learning%20on%20Line%20Segment%20Detection&body=Title%3A%20The%20Impact%20of%20Semi-Supervised%20Learning%20on%20Line%20Segment%20Detection%0AAuthor%3A%20Johanna%20Engman%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20a%20method%20for%20line%20segment%20detection%20in%20images%2C%20based%0Aon%20a%20semi-supervised%20framework.%20Leveraging%20the%20use%20of%20a%20consistency%20loss%20based%0Aon%20differently%20augmented%20and%20perturbed%20unlabeled%20images%20with%20a%20small%20amount%20of%0Alabeled%20data%2C%20we%20show%20comparable%20results%20to%20fully%20supervised%20methods.%20This%0Aopens%20up%20application%20scenarios%20where%20annotation%20is%20difficult%20or%20expensive%2C%20and%0Afor%20domain%20specific%20adaptation%20of%20models.%20We%20are%20specifically%20interested%20in%0Areal-time%20and%20online%20applications%2C%20and%20investigate%20small%20and%20efficient%20learning%0Abackbones.%20Our%20method%20is%20to%20our%20knowledge%20the%20first%20to%20target%20line%20detection%0Ausing%20modern%20state-of-the-art%20methodologies%20for%20semi-supervised%20learning.%20We%0Atest%20the%20method%20on%20both%20standard%20benchmarks%20and%20domain%20specific%20scenarios%20for%0Aforestry%20applications%2C%20showing%20the%20tractability%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Semi-Supervised%2520Learning%2520on%2520Line%2520Segment%2520Detection%26entry.906535625%3DJohanna%2520Engman%2520and%2520Karl%2520%25C3%2585str%25C3%25B6m%2520and%2520Magnus%2520Oskarsson%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520a%2520method%2520for%2520line%2520segment%2520detection%2520in%2520images%252C%2520based%250Aon%2520a%2520semi-supervised%2520framework.%2520Leveraging%2520the%2520use%2520of%2520a%2520consistency%2520loss%2520based%250Aon%2520differently%2520augmented%2520and%2520perturbed%2520unlabeled%2520images%2520with%2520a%2520small%2520amount%2520of%250Alabeled%2520data%252C%2520we%2520show%2520comparable%2520results%2520to%2520fully%2520supervised%2520methods.%2520This%250Aopens%2520up%2520application%2520scenarios%2520where%2520annotation%2520is%2520difficult%2520or%2520expensive%252C%2520and%250Afor%2520domain%2520specific%2520adaptation%2520of%2520models.%2520We%2520are%2520specifically%2520interested%2520in%250Areal-time%2520and%2520online%2520applications%252C%2520and%2520investigate%2520small%2520and%2520efficient%2520learning%250Abackbones.%2520Our%2520method%2520is%2520to%2520our%2520knowledge%2520the%2520first%2520to%2520target%2520line%2520detection%250Ausing%2520modern%2520state-of-the-art%2520methodologies%2520for%2520semi-supervised%2520learning.%2520We%250Atest%2520the%2520method%2520on%2520both%2520standard%2520benchmarks%2520and%2520domain%2520specific%2520scenarios%2520for%250Aforestry%2520applications%252C%2520showing%2520the%2520tractability%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Semi-Supervised%20Learning%20on%20Line%20Segment%20Detection&entry.906535625=Johanna%20Engman%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson&entry.1292438233=%20%20In%20this%20paper%20we%20present%20a%20method%20for%20line%20segment%20detection%20in%20images%2C%20based%0Aon%20a%20semi-supervised%20framework.%20Leveraging%20the%20use%20of%20a%20consistency%20loss%20based%0Aon%20differently%20augmented%20and%20perturbed%20unlabeled%20images%20with%20a%20small%20amount%20of%0Alabeled%20data%2C%20we%20show%20comparable%20results%20to%20fully%20supervised%20methods.%20This%0Aopens%20up%20application%20scenarios%20where%20annotation%20is%20difficult%20or%20expensive%2C%20and%0Afor%20domain%20specific%20adaptation%20of%20models.%20We%20are%20specifically%20interested%20in%0Areal-time%20and%20online%20applications%2C%20and%20investigate%20small%20and%20efficient%20learning%0Abackbones.%20Our%20method%20is%20to%20our%20knowledge%20the%20first%20to%20target%20line%20detection%0Ausing%20modern%20state-of-the-art%20methodologies%20for%20semi-supervised%20learning.%20We%0Atest%20the%20method%20on%20both%20standard%20benchmarks%20and%20domain%20specific%20scenarios%20for%0Aforestry%20applications%2C%20showing%20the%20tractability%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04596v1&entry.124074799=Read"},
{"title": "GUI Agents with Foundation Models: A Comprehensive Survey", "author": "Shuai Wang and Weiwen Liu and Jingxuan Chen and Weinan Gan and Xingshan Zeng and Shuai Yu and Xinlong Hao and Kun Shao and Yasheng Wang and Ruiming Tang", "abstract": "  Recent advances in foundation models, particularly Large Language Models\n(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent\nagents being capable of performing complex tasks. By leveraging the ability of\n(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents\ncan autonomously execute user instructions by simulating human-like\ninteractions such as clicking and typing. This survey consolidates recent\nresearch on (M)LLM-based GUI agents, highlighting key innovations in data,\nframeworks, and applications. We begin by discussing representative datasets\nand benchmarks. Next, we summarize a unified framework that captures the\nessential components used in prior research, accompanied by a taxonomy.\nAdditionally, we explore commercial applications of (M)LLM-based GUI agents.\nDrawing from existing work, we identify several key challenges and propose\nfuture research directions. We hope this paper will inspire further\ndevelopments in the field of (M)LLM-based GUI agents.\n", "link": "http://arxiv.org/abs/2411.04890v1", "date": "2024-11-07", "relevancy": 2.6275, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI%20Agents%20with%20Foundation%20Models%3A%20A%20Comprehensive%20Survey&body=Title%3A%20GUI%20Agents%20with%20Foundation%20Models%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Shuai%20Wang%20and%20Weiwen%20Liu%20and%20Jingxuan%20Chen%20and%20Weinan%20Gan%20and%20Xingshan%20Zeng%20and%20Shuai%20Yu%20and%20Xinlong%20Hao%20and%20Kun%20Shao%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%0AAbstract%3A%20%20%20Recent%20advances%20in%20foundation%20models%2C%20particularly%20Large%20Language%20Models%0A%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20facilitate%20intelligent%0Aagents%20being%20capable%20of%20performing%20complex%20tasks.%20By%20leveraging%20the%20ability%20of%0A%28M%29LLMs%20to%20process%20and%20interpret%20Graphical%20User%20Interfaces%20%28GUIs%29%2C%20these%20agents%0Acan%20autonomously%20execute%20user%20instructions%20by%20simulating%20human-like%0Ainteractions%20such%20as%20clicking%20and%20typing.%20This%20survey%20consolidates%20recent%0Aresearch%20on%20%28M%29LLM-based%20GUI%20agents%2C%20highlighting%20key%20innovations%20in%20data%2C%0Aframeworks%2C%20and%20applications.%20We%20begin%20by%20discussing%20representative%20datasets%0Aand%20benchmarks.%20Next%2C%20we%20summarize%20a%20unified%20framework%20that%20captures%20the%0Aessential%20components%20used%20in%20prior%20research%2C%20accompanied%20by%20a%20taxonomy.%0AAdditionally%2C%20we%20explore%20commercial%20applications%20of%20%28M%29LLM-based%20GUI%20agents.%0ADrawing%20from%20existing%20work%2C%20we%20identify%20several%20key%20challenges%20and%20propose%0Afuture%20research%20directions.%20We%20hope%20this%20paper%20will%20inspire%20further%0Adevelopments%20in%20the%20field%20of%20%28M%29LLM-based%20GUI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI%2520Agents%2520with%2520Foundation%2520Models%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DShuai%2520Wang%2520and%2520Weiwen%2520Liu%2520and%2520Jingxuan%2520Chen%2520and%2520Weinan%2520Gan%2520and%2520Xingshan%2520Zeng%2520and%2520Shuai%2520Yu%2520and%2520Xinlong%2520Hao%2520and%2520Kun%2520Shao%2520and%2520Yasheng%2520Wang%2520and%2520Ruiming%2520Tang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520foundation%2520models%252C%2520particularly%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520facilitate%2520intelligent%250Aagents%2520being%2520capable%2520of%2520performing%2520complex%2520tasks.%2520By%2520leveraging%2520the%2520ability%2520of%250A%2528M%2529LLMs%2520to%2520process%2520and%2520interpret%2520Graphical%2520User%2520Interfaces%2520%2528GUIs%2529%252C%2520these%2520agents%250Acan%2520autonomously%2520execute%2520user%2520instructions%2520by%2520simulating%2520human-like%250Ainteractions%2520such%2520as%2520clicking%2520and%2520typing.%2520This%2520survey%2520consolidates%2520recent%250Aresearch%2520on%2520%2528M%2529LLM-based%2520GUI%2520agents%252C%2520highlighting%2520key%2520innovations%2520in%2520data%252C%250Aframeworks%252C%2520and%2520applications.%2520We%2520begin%2520by%2520discussing%2520representative%2520datasets%250Aand%2520benchmarks.%2520Next%252C%2520we%2520summarize%2520a%2520unified%2520framework%2520that%2520captures%2520the%250Aessential%2520components%2520used%2520in%2520prior%2520research%252C%2520accompanied%2520by%2520a%2520taxonomy.%250AAdditionally%252C%2520we%2520explore%2520commercial%2520applications%2520of%2520%2528M%2529LLM-based%2520GUI%2520agents.%250ADrawing%2520from%2520existing%2520work%252C%2520we%2520identify%2520several%2520key%2520challenges%2520and%2520propose%250Afuture%2520research%2520directions.%2520We%2520hope%2520this%2520paper%2520will%2520inspire%2520further%250Adevelopments%2520in%2520the%2520field%2520of%2520%2528M%2529LLM-based%2520GUI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI%20Agents%20with%20Foundation%20Models%3A%20A%20Comprehensive%20Survey&entry.906535625=Shuai%20Wang%20and%20Weiwen%20Liu%20and%20Jingxuan%20Chen%20and%20Weinan%20Gan%20and%20Xingshan%20Zeng%20and%20Shuai%20Yu%20and%20Xinlong%20Hao%20and%20Kun%20Shao%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang&entry.1292438233=%20%20Recent%20advances%20in%20foundation%20models%2C%20particularly%20Large%20Language%20Models%0A%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20facilitate%20intelligent%0Aagents%20being%20capable%20of%20performing%20complex%20tasks.%20By%20leveraging%20the%20ability%20of%0A%28M%29LLMs%20to%20process%20and%20interpret%20Graphical%20User%20Interfaces%20%28GUIs%29%2C%20these%20agents%0Acan%20autonomously%20execute%20user%20instructions%20by%20simulating%20human-like%0Ainteractions%20such%20as%20clicking%20and%20typing.%20This%20survey%20consolidates%20recent%0Aresearch%20on%20%28M%29LLM-based%20GUI%20agents%2C%20highlighting%20key%20innovations%20in%20data%2C%0Aframeworks%2C%20and%20applications.%20We%20begin%20by%20discussing%20representative%20datasets%0Aand%20benchmarks.%20Next%2C%20we%20summarize%20a%20unified%20framework%20that%20captures%20the%0Aessential%20components%20used%20in%20prior%20research%2C%20accompanied%20by%20a%20taxonomy.%0AAdditionally%2C%20we%20explore%20commercial%20applications%20of%20%28M%29LLM-based%20GUI%20agents.%0ADrawing%20from%20existing%20work%2C%20we%20identify%20several%20key%20challenges%20and%20propose%0Afuture%20research%20directions.%20We%20hope%20this%20paper%20will%20inspire%20further%0Adevelopments%20in%20the%20field%20of%20%28M%29LLM-based%20GUI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04890v1&entry.124074799=Read"},
{"title": "Sampling-guided Heterogeneous Graph Neural Network with Temporal\n  Smoothing for Scalable Longitudinal Data Imputation", "author": "Zhaoyang Zhang and Ziqi Chen and Qiao Liu and Jinhan Xie and Hongtu Zhu", "abstract": "  In this paper, we propose a novel framework, the Sampling-guided\nHeterogeneous Graph Neural Network (SHT-GNN), to effectively tackle the\nchallenge of missing data imputation in longitudinal studies. Unlike\ntraditional methods, which often require extensive preprocessing to handle\nirregular or inconsistent missing data, our approach accommodates arbitrary\nmissing data patterns while maintaining computational efficiency. SHT-GNN\nmodels both observations and covariates as distinct node types, connecting\nobservation nodes at successive time points through subject-specific\nlongitudinal subnetworks, while covariate-observation interactions are\nrepresented by attributed edges within bipartite graphs. By leveraging\nsubject-wise mini-batch sampling and a multi-layer temporal smoothing\nmechanism, SHT-GNN efficiently scales to large datasets, while effectively\nlearning node representations and imputing missing data. Extensive experiments\non both synthetic and real-world datasets, including the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset, demonstrate that SHT-GNN significantly\noutperforms existing imputation methods, even with high missing data rates. The\nempirical results highlight SHT-GNN's robust imputation capabilities and\nsuperior performance, particularly in the context of complex, large-scale\nlongitudinal data.\n", "link": "http://arxiv.org/abs/2411.04899v1", "date": "2024-11-07", "relevancy": 2.5973, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5365}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5211}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-guided%20Heterogeneous%20Graph%20Neural%20Network%20with%20Temporal%0A%20%20Smoothing%20for%20Scalable%20Longitudinal%20Data%20Imputation&body=Title%3A%20Sampling-guided%20Heterogeneous%20Graph%20Neural%20Network%20with%20Temporal%0A%20%20Smoothing%20for%20Scalable%20Longitudinal%20Data%20Imputation%0AAuthor%3A%20Zhaoyang%20Zhang%20and%20Ziqi%20Chen%20and%20Qiao%20Liu%20and%20Jinhan%20Xie%20and%20Hongtu%20Zhu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%20the%20Sampling-guided%0AHeterogeneous%20Graph%20Neural%20Network%20%28SHT-GNN%29%2C%20to%20effectively%20tackle%20the%0Achallenge%20of%20missing%20data%20imputation%20in%20longitudinal%20studies.%20Unlike%0Atraditional%20methods%2C%20which%20often%20require%20extensive%20preprocessing%20to%20handle%0Airregular%20or%20inconsistent%20missing%20data%2C%20our%20approach%20accommodates%20arbitrary%0Amissing%20data%20patterns%20while%20maintaining%20computational%20efficiency.%20SHT-GNN%0Amodels%20both%20observations%20and%20covariates%20as%20distinct%20node%20types%2C%20connecting%0Aobservation%20nodes%20at%20successive%20time%20points%20through%20subject-specific%0Alongitudinal%20subnetworks%2C%20while%20covariate-observation%20interactions%20are%0Arepresented%20by%20attributed%20edges%20within%20bipartite%20graphs.%20By%20leveraging%0Asubject-wise%20mini-batch%20sampling%20and%20a%20multi-layer%20temporal%20smoothing%0Amechanism%2C%20SHT-GNN%20efficiently%20scales%20to%20large%20datasets%2C%20while%20effectively%0Alearning%20node%20representations%20and%20imputing%20missing%20data.%20Extensive%20experiments%0Aon%20both%20synthetic%20and%20real-world%20datasets%2C%20including%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20dataset%2C%20demonstrate%20that%20SHT-GNN%20significantly%0Aoutperforms%20existing%20imputation%20methods%2C%20even%20with%20high%20missing%20data%20rates.%20The%0Aempirical%20results%20highlight%20SHT-GNN%27s%20robust%20imputation%20capabilities%20and%0Asuperior%20performance%2C%20particularly%20in%20the%20context%20of%20complex%2C%20large-scale%0Alongitudinal%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-guided%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520with%2520Temporal%250A%2520%2520Smoothing%2520for%2520Scalable%2520Longitudinal%2520Data%2520Imputation%26entry.906535625%3DZhaoyang%2520Zhang%2520and%2520Ziqi%2520Chen%2520and%2520Qiao%2520Liu%2520and%2520Jinhan%2520Xie%2520and%2520Hongtu%2520Zhu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520the%2520Sampling-guided%250AHeterogeneous%2520Graph%2520Neural%2520Network%2520%2528SHT-GNN%2529%252C%2520to%2520effectively%2520tackle%2520the%250Achallenge%2520of%2520missing%2520data%2520imputation%2520in%2520longitudinal%2520studies.%2520Unlike%250Atraditional%2520methods%252C%2520which%2520often%2520require%2520extensive%2520preprocessing%2520to%2520handle%250Airregular%2520or%2520inconsistent%2520missing%2520data%252C%2520our%2520approach%2520accommodates%2520arbitrary%250Amissing%2520data%2520patterns%2520while%2520maintaining%2520computational%2520efficiency.%2520SHT-GNN%250Amodels%2520both%2520observations%2520and%2520covariates%2520as%2520distinct%2520node%2520types%252C%2520connecting%250Aobservation%2520nodes%2520at%2520successive%2520time%2520points%2520through%2520subject-specific%250Alongitudinal%2520subnetworks%252C%2520while%2520covariate-observation%2520interactions%2520are%250Arepresented%2520by%2520attributed%2520edges%2520within%2520bipartite%2520graphs.%2520By%2520leveraging%250Asubject-wise%2520mini-batch%2520sampling%2520and%2520a%2520multi-layer%2520temporal%2520smoothing%250Amechanism%252C%2520SHT-GNN%2520efficiently%2520scales%2520to%2520large%2520datasets%252C%2520while%2520effectively%250Alearning%2520node%2520representations%2520and%2520imputing%2520missing%2520data.%2520Extensive%2520experiments%250Aon%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520including%2520the%2520Alzheimer%2527s%2520Disease%250ANeuroimaging%2520Initiative%2520%2528ADNI%2529%2520dataset%252C%2520demonstrate%2520that%2520SHT-GNN%2520significantly%250Aoutperforms%2520existing%2520imputation%2520methods%252C%2520even%2520with%2520high%2520missing%2520data%2520rates.%2520The%250Aempirical%2520results%2520highlight%2520SHT-GNN%2527s%2520robust%2520imputation%2520capabilities%2520and%250Asuperior%2520performance%252C%2520particularly%2520in%2520the%2520context%2520of%2520complex%252C%2520large-scale%250Alongitudinal%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-guided%20Heterogeneous%20Graph%20Neural%20Network%20with%20Temporal%0A%20%20Smoothing%20for%20Scalable%20Longitudinal%20Data%20Imputation&entry.906535625=Zhaoyang%20Zhang%20and%20Ziqi%20Chen%20and%20Qiao%20Liu%20and%20Jinhan%20Xie%20and%20Hongtu%20Zhu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%2C%20the%20Sampling-guided%0AHeterogeneous%20Graph%20Neural%20Network%20%28SHT-GNN%29%2C%20to%20effectively%20tackle%20the%0Achallenge%20of%20missing%20data%20imputation%20in%20longitudinal%20studies.%20Unlike%0Atraditional%20methods%2C%20which%20often%20require%20extensive%20preprocessing%20to%20handle%0Airregular%20or%20inconsistent%20missing%20data%2C%20our%20approach%20accommodates%20arbitrary%0Amissing%20data%20patterns%20while%20maintaining%20computational%20efficiency.%20SHT-GNN%0Amodels%20both%20observations%20and%20covariates%20as%20distinct%20node%20types%2C%20connecting%0Aobservation%20nodes%20at%20successive%20time%20points%20through%20subject-specific%0Alongitudinal%20subnetworks%2C%20while%20covariate-observation%20interactions%20are%0Arepresented%20by%20attributed%20edges%20within%20bipartite%20graphs.%20By%20leveraging%0Asubject-wise%20mini-batch%20sampling%20and%20a%20multi-layer%20temporal%20smoothing%0Amechanism%2C%20SHT-GNN%20efficiently%20scales%20to%20large%20datasets%2C%20while%20effectively%0Alearning%20node%20representations%20and%20imputing%20missing%20data.%20Extensive%20experiments%0Aon%20both%20synthetic%20and%20real-world%20datasets%2C%20including%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20dataset%2C%20demonstrate%20that%20SHT-GNN%20significantly%0Aoutperforms%20existing%20imputation%20methods%2C%20even%20with%20high%20missing%20data%20rates.%20The%0Aempirical%20results%20highlight%20SHT-GNN%27s%20robust%20imputation%20capabilities%20and%0Asuperior%20performance%2C%20particularly%20in%20the%20context%20of%20complex%2C%20large-scale%0Alongitudinal%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04899v1&entry.124074799=Read"},
{"title": "Population estimation using 3D city modelling and Carto2S datasets -- A\n  case study", "author": "Jai G Singla", "abstract": "  With the launch of Carto2S series of satellites, high resolution images\n(0.6-1.0 meters) are acquired and available for use. High resolution Digital\nElevation Model (DEM) with better accuracies can be generated using C2S\nmulti-view and multi date datasets. DEMs are further used as an input to derive\nDigital terrain models (DTMs) and to extract accurate heights of the objects\n(building and tree) over the surface of the Earth. Extracted building heights\nare validated with ground control points and can be used for generation of city\nmodelling and resource estimation like population estimation, health planning,\nwater and transport resource estimations. In this study, an attempt is made to\nassess the population of a township using high-resolution Indian remote sensing\nsatellite datasets. We used Carto 2S multi-view data and generated a precise\nDEM and DTM over a city area. Using DEM and DTM datasets, accurate heights of\nthe buildings are extracted which are further validated with ground data.\nAccurate building heights and high resolution imagery are used for generating\naccurate virtual 3D city model and assessing the number of floor and carpet\narea of the houses/ flats/ apartments. Population estimation of the area is\nmade using derived information of no of houses/ flats/ apartments from the\nsatellite datasets. Further, information about number of hospital and schools\naround the residential area is extracted from open street maps (OSM).\nPopulation estimation using satellite data and derived information from OSM\ndatasets can prove to be very good tool for local administrator and decision\nmakers.\n", "link": "http://arxiv.org/abs/2411.04612v1", "date": "2024-11-07", "relevancy": 2.596, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5273}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5273}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Population%20estimation%20using%203D%20city%20modelling%20and%20Carto2S%20datasets%20--%20A%0A%20%20case%20study&body=Title%3A%20Population%20estimation%20using%203D%20city%20modelling%20and%20Carto2S%20datasets%20--%20A%0A%20%20case%20study%0AAuthor%3A%20Jai%20G%20Singla%0AAbstract%3A%20%20%20With%20the%20launch%20of%20Carto2S%20series%20of%20satellites%2C%20high%20resolution%20images%0A%280.6-1.0%20meters%29%20are%20acquired%20and%20available%20for%20use.%20High%20resolution%20Digital%0AElevation%20Model%20%28DEM%29%20with%20better%20accuracies%20can%20be%20generated%20using%20C2S%0Amulti-view%20and%20multi%20date%20datasets.%20DEMs%20are%20further%20used%20as%20an%20input%20to%20derive%0ADigital%20terrain%20models%20%28DTMs%29%20and%20to%20extract%20accurate%20heights%20of%20the%20objects%0A%28building%20and%20tree%29%20over%20the%20surface%20of%20the%20Earth.%20Extracted%20building%20heights%0Aare%20validated%20with%20ground%20control%20points%20and%20can%20be%20used%20for%20generation%20of%20city%0Amodelling%20and%20resource%20estimation%20like%20population%20estimation%2C%20health%20planning%2C%0Awater%20and%20transport%20resource%20estimations.%20In%20this%20study%2C%20an%20attempt%20is%20made%20to%0Aassess%20the%20population%20of%20a%20township%20using%20high-resolution%20Indian%20remote%20sensing%0Asatellite%20datasets.%20We%20used%20Carto%202S%20multi-view%20data%20and%20generated%20a%20precise%0ADEM%20and%20DTM%20over%20a%20city%20area.%20Using%20DEM%20and%20DTM%20datasets%2C%20accurate%20heights%20of%0Athe%20buildings%20are%20extracted%20which%20are%20further%20validated%20with%20ground%20data.%0AAccurate%20building%20heights%20and%20high%20resolution%20imagery%20are%20used%20for%20generating%0Aaccurate%20virtual%203D%20city%20model%20and%20assessing%20the%20number%20of%20floor%20and%20carpet%0Aarea%20of%20the%20houses/%20flats/%20apartments.%20Population%20estimation%20of%20the%20area%20is%0Amade%20using%20derived%20information%20of%20no%20of%20houses/%20flats/%20apartments%20from%20the%0Asatellite%20datasets.%20Further%2C%20information%20about%20number%20of%20hospital%20and%20schools%0Aaround%20the%20residential%20area%20is%20extracted%20from%20open%20street%20maps%20%28OSM%29.%0APopulation%20estimation%20using%20satellite%20data%20and%20derived%20information%20from%20OSM%0Adatasets%20can%20prove%20to%20be%20very%20good%20tool%20for%20local%20administrator%20and%20decision%0Amakers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopulation%2520estimation%2520using%25203D%2520city%2520modelling%2520and%2520Carto2S%2520datasets%2520--%2520A%250A%2520%2520case%2520study%26entry.906535625%3DJai%2520G%2520Singla%26entry.1292438233%3D%2520%2520With%2520the%2520launch%2520of%2520Carto2S%2520series%2520of%2520satellites%252C%2520high%2520resolution%2520images%250A%25280.6-1.0%2520meters%2529%2520are%2520acquired%2520and%2520available%2520for%2520use.%2520High%2520resolution%2520Digital%250AElevation%2520Model%2520%2528DEM%2529%2520with%2520better%2520accuracies%2520can%2520be%2520generated%2520using%2520C2S%250Amulti-view%2520and%2520multi%2520date%2520datasets.%2520DEMs%2520are%2520further%2520used%2520as%2520an%2520input%2520to%2520derive%250ADigital%2520terrain%2520models%2520%2528DTMs%2529%2520and%2520to%2520extract%2520accurate%2520heights%2520of%2520the%2520objects%250A%2528building%2520and%2520tree%2529%2520over%2520the%2520surface%2520of%2520the%2520Earth.%2520Extracted%2520building%2520heights%250Aare%2520validated%2520with%2520ground%2520control%2520points%2520and%2520can%2520be%2520used%2520for%2520generation%2520of%2520city%250Amodelling%2520and%2520resource%2520estimation%2520like%2520population%2520estimation%252C%2520health%2520planning%252C%250Awater%2520and%2520transport%2520resource%2520estimations.%2520In%2520this%2520study%252C%2520an%2520attempt%2520is%2520made%2520to%250Aassess%2520the%2520population%2520of%2520a%2520township%2520using%2520high-resolution%2520Indian%2520remote%2520sensing%250Asatellite%2520datasets.%2520We%2520used%2520Carto%25202S%2520multi-view%2520data%2520and%2520generated%2520a%2520precise%250ADEM%2520and%2520DTM%2520over%2520a%2520city%2520area.%2520Using%2520DEM%2520and%2520DTM%2520datasets%252C%2520accurate%2520heights%2520of%250Athe%2520buildings%2520are%2520extracted%2520which%2520are%2520further%2520validated%2520with%2520ground%2520data.%250AAccurate%2520building%2520heights%2520and%2520high%2520resolution%2520imagery%2520are%2520used%2520for%2520generating%250Aaccurate%2520virtual%25203D%2520city%2520model%2520and%2520assessing%2520the%2520number%2520of%2520floor%2520and%2520carpet%250Aarea%2520of%2520the%2520houses/%2520flats/%2520apartments.%2520Population%2520estimation%2520of%2520the%2520area%2520is%250Amade%2520using%2520derived%2520information%2520of%2520no%2520of%2520houses/%2520flats/%2520apartments%2520from%2520the%250Asatellite%2520datasets.%2520Further%252C%2520information%2520about%2520number%2520of%2520hospital%2520and%2520schools%250Aaround%2520the%2520residential%2520area%2520is%2520extracted%2520from%2520open%2520street%2520maps%2520%2528OSM%2529.%250APopulation%2520estimation%2520using%2520satellite%2520data%2520and%2520derived%2520information%2520from%2520OSM%250Adatasets%2520can%2520prove%2520to%2520be%2520very%2520good%2520tool%2520for%2520local%2520administrator%2520and%2520decision%250Amakers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Population%20estimation%20using%203D%20city%20modelling%20and%20Carto2S%20datasets%20--%20A%0A%20%20case%20study&entry.906535625=Jai%20G%20Singla&entry.1292438233=%20%20With%20the%20launch%20of%20Carto2S%20series%20of%20satellites%2C%20high%20resolution%20images%0A%280.6-1.0%20meters%29%20are%20acquired%20and%20available%20for%20use.%20High%20resolution%20Digital%0AElevation%20Model%20%28DEM%29%20with%20better%20accuracies%20can%20be%20generated%20using%20C2S%0Amulti-view%20and%20multi%20date%20datasets.%20DEMs%20are%20further%20used%20as%20an%20input%20to%20derive%0ADigital%20terrain%20models%20%28DTMs%29%20and%20to%20extract%20accurate%20heights%20of%20the%20objects%0A%28building%20and%20tree%29%20over%20the%20surface%20of%20the%20Earth.%20Extracted%20building%20heights%0Aare%20validated%20with%20ground%20control%20points%20and%20can%20be%20used%20for%20generation%20of%20city%0Amodelling%20and%20resource%20estimation%20like%20population%20estimation%2C%20health%20planning%2C%0Awater%20and%20transport%20resource%20estimations.%20In%20this%20study%2C%20an%20attempt%20is%20made%20to%0Aassess%20the%20population%20of%20a%20township%20using%20high-resolution%20Indian%20remote%20sensing%0Asatellite%20datasets.%20We%20used%20Carto%202S%20multi-view%20data%20and%20generated%20a%20precise%0ADEM%20and%20DTM%20over%20a%20city%20area.%20Using%20DEM%20and%20DTM%20datasets%2C%20accurate%20heights%20of%0Athe%20buildings%20are%20extracted%20which%20are%20further%20validated%20with%20ground%20data.%0AAccurate%20building%20heights%20and%20high%20resolution%20imagery%20are%20used%20for%20generating%0Aaccurate%20virtual%203D%20city%20model%20and%20assessing%20the%20number%20of%20floor%20and%20carpet%0Aarea%20of%20the%20houses/%20flats/%20apartments.%20Population%20estimation%20of%20the%20area%20is%0Amade%20using%20derived%20information%20of%20no%20of%20houses/%20flats/%20apartments%20from%20the%0Asatellite%20datasets.%20Further%2C%20information%20about%20number%20of%20hospital%20and%20schools%0Aaround%20the%20residential%20area%20is%20extracted%20from%20open%20street%20maps%20%28OSM%29.%0APopulation%20estimation%20using%20satellite%20data%20and%20derived%20information%20from%20OSM%0Adatasets%20can%20prove%20to%20be%20very%20good%20tool%20for%20local%20administrator%20and%20decision%0Amakers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04612v1&entry.124074799=Read"},
{"title": "Planar Reflection-Aware Neural Radiance Fields", "author": "Chen Gao and Yipeng Wang and Changil Kim and Jia-Bin Huang and Johannes Kopf", "abstract": "  Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in\nreconstructing complex scenes with high fidelity. However, NeRF's view\ndependency can only handle low-frequency reflections. It falls short when\nhandling complex planar reflections, often interpreting them as erroneous scene\ngeometries and leading to duplicated and inaccurate scene representations. To\naddress this challenge, we introduce a reflection-aware NeRF that jointly\nmodels planar reflectors, such as windows, and explicitly casts reflected rays\nto capture the source of the high-frequency reflections. We query a single\nradiance field to render the primary color and the source of the reflection. We\npropose a sparse edge regularization to help utilize the true sources of\nreflections for rendering planar reflections rather than creating a duplicate\nalong the primary ray at the same depth. As a result, we obtain accurate scene\ngeometry. Rendering along the primary ray results in a clean, reflection-free\nview, while explicitly rendering along the reflected ray allows us to\nreconstruct highly detailed reflections. Our extensive quantitative and\nqualitative evaluations of real-world datasets demonstrate our method's\nenhanced performance in accurately handling reflections.\n", "link": "http://arxiv.org/abs/2411.04984v1", "date": "2024-11-07", "relevancy": 2.59, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planar%20Reflection-Aware%20Neural%20Radiance%20Fields&body=Title%3A%20Planar%20Reflection-Aware%20Neural%20Radiance%20Fields%0AAuthor%3A%20Chen%20Gao%20and%20Yipeng%20Wang%20and%20Changil%20Kim%20and%20Jia-Bin%20Huang%20and%20Johannes%20Kopf%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20demonstrated%20exceptional%20capabilities%20in%0Areconstructing%20complex%20scenes%20with%20high%20fidelity.%20However%2C%20NeRF%27s%20view%0Adependency%20can%20only%20handle%20low-frequency%20reflections.%20It%20falls%20short%20when%0Ahandling%20complex%20planar%20reflections%2C%20often%20interpreting%20them%20as%20erroneous%20scene%0Ageometries%20and%20leading%20to%20duplicated%20and%20inaccurate%20scene%20representations.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20a%20reflection-aware%20NeRF%20that%20jointly%0Amodels%20planar%20reflectors%2C%20such%20as%20windows%2C%20and%20explicitly%20casts%20reflected%20rays%0Ato%20capture%20the%20source%20of%20the%20high-frequency%20reflections.%20We%20query%20a%20single%0Aradiance%20field%20to%20render%20the%20primary%20color%20and%20the%20source%20of%20the%20reflection.%20We%0Apropose%20a%20sparse%20edge%20regularization%20to%20help%20utilize%20the%20true%20sources%20of%0Areflections%20for%20rendering%20planar%20reflections%20rather%20than%20creating%20a%20duplicate%0Aalong%20the%20primary%20ray%20at%20the%20same%20depth.%20As%20a%20result%2C%20we%20obtain%20accurate%20scene%0Ageometry.%20Rendering%20along%20the%20primary%20ray%20results%20in%20a%20clean%2C%20reflection-free%0Aview%2C%20while%20explicitly%20rendering%20along%20the%20reflected%20ray%20allows%20us%20to%0Areconstruct%20highly%20detailed%20reflections.%20Our%20extensive%20quantitative%20and%0Aqualitative%20evaluations%20of%20real-world%20datasets%20demonstrate%20our%20method%27s%0Aenhanced%20performance%20in%20accurately%20handling%20reflections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanar%2520Reflection-Aware%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DChen%2520Gao%2520and%2520Yipeng%2520Wang%2520and%2520Changil%2520Kim%2520and%2520Jia-Bin%2520Huang%2520and%2520Johannes%2520Kopf%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520demonstrated%2520exceptional%2520capabilities%2520in%250Areconstructing%2520complex%2520scenes%2520with%2520high%2520fidelity.%2520However%252C%2520NeRF%2527s%2520view%250Adependency%2520can%2520only%2520handle%2520low-frequency%2520reflections.%2520It%2520falls%2520short%2520when%250Ahandling%2520complex%2520planar%2520reflections%252C%2520often%2520interpreting%2520them%2520as%2520erroneous%2520scene%250Ageometries%2520and%2520leading%2520to%2520duplicated%2520and%2520inaccurate%2520scene%2520representations.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520reflection-aware%2520NeRF%2520that%2520jointly%250Amodels%2520planar%2520reflectors%252C%2520such%2520as%2520windows%252C%2520and%2520explicitly%2520casts%2520reflected%2520rays%250Ato%2520capture%2520the%2520source%2520of%2520the%2520high-frequency%2520reflections.%2520We%2520query%2520a%2520single%250Aradiance%2520field%2520to%2520render%2520the%2520primary%2520color%2520and%2520the%2520source%2520of%2520the%2520reflection.%2520We%250Apropose%2520a%2520sparse%2520edge%2520regularization%2520to%2520help%2520utilize%2520the%2520true%2520sources%2520of%250Areflections%2520for%2520rendering%2520planar%2520reflections%2520rather%2520than%2520creating%2520a%2520duplicate%250Aalong%2520the%2520primary%2520ray%2520at%2520the%2520same%2520depth.%2520As%2520a%2520result%252C%2520we%2520obtain%2520accurate%2520scene%250Ageometry.%2520Rendering%2520along%2520the%2520primary%2520ray%2520results%2520in%2520a%2520clean%252C%2520reflection-free%250Aview%252C%2520while%2520explicitly%2520rendering%2520along%2520the%2520reflected%2520ray%2520allows%2520us%2520to%250Areconstruct%2520highly%2520detailed%2520reflections.%2520Our%2520extensive%2520quantitative%2520and%250Aqualitative%2520evaluations%2520of%2520real-world%2520datasets%2520demonstrate%2520our%2520method%2527s%250Aenhanced%2520performance%2520in%2520accurately%2520handling%2520reflections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planar%20Reflection-Aware%20Neural%20Radiance%20Fields&entry.906535625=Chen%20Gao%20and%20Yipeng%20Wang%20and%20Changil%20Kim%20and%20Jia-Bin%20Huang%20and%20Johannes%20Kopf&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20demonstrated%20exceptional%20capabilities%20in%0Areconstructing%20complex%20scenes%20with%20high%20fidelity.%20However%2C%20NeRF%27s%20view%0Adependency%20can%20only%20handle%20low-frequency%20reflections.%20It%20falls%20short%20when%0Ahandling%20complex%20planar%20reflections%2C%20often%20interpreting%20them%20as%20erroneous%20scene%0Ageometries%20and%20leading%20to%20duplicated%20and%20inaccurate%20scene%20representations.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20a%20reflection-aware%20NeRF%20that%20jointly%0Amodels%20planar%20reflectors%2C%20such%20as%20windows%2C%20and%20explicitly%20casts%20reflected%20rays%0Ato%20capture%20the%20source%20of%20the%20high-frequency%20reflections.%20We%20query%20a%20single%0Aradiance%20field%20to%20render%20the%20primary%20color%20and%20the%20source%20of%20the%20reflection.%20We%0Apropose%20a%20sparse%20edge%20regularization%20to%20help%20utilize%20the%20true%20sources%20of%0Areflections%20for%20rendering%20planar%20reflections%20rather%20than%20creating%20a%20duplicate%0Aalong%20the%20primary%20ray%20at%20the%20same%20depth.%20As%20a%20result%2C%20we%20obtain%20accurate%20scene%0Ageometry.%20Rendering%20along%20the%20primary%20ray%20results%20in%20a%20clean%2C%20reflection-free%0Aview%2C%20while%20explicitly%20rendering%20along%20the%20reflected%20ray%20allows%20us%20to%0Areconstruct%20highly%20detailed%20reflections.%20Our%20extensive%20quantitative%20and%0Aqualitative%20evaluations%20of%20real-world%20datasets%20demonstrate%20our%20method%27s%0Aenhanced%20performance%20in%20accurately%20handling%20reflections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04984v1&entry.124074799=Read"},
{"title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation", "author": "Koichi Namekata and Sherwin Bahmani and Ziyi Wu and Yash Kant and Igor Gilitschenski and David B. Lindell", "abstract": "  Methods for image-to-video generation have achieved impressive,\nphoto-realistic quality. However, adjusting specific elements in generated\nvideos, such as object motion or camera movement, is often a tedious process of\ntrial and error, e.g., involving re-generating videos with different random\nseeds. Recent techniques address this issue by fine-tuning a pre-trained model\nto follow conditioning signals, such as bounding boxes or point trajectories.\nYet, this fine-tuning procedure can be computationally expensive, and it\nrequires datasets with annotated object motion, which can be difficult to\nprocure. In this work, we introduce SG-I2V, a framework for controllable\nimage-to-video generation that is self-guided$\\unicode{x2013}$offering\nzero-shot control by relying solely on the knowledge present in a pre-trained\nimage-to-video diffusion model without the need for fine-tuning or external\nknowledge. Our zero-shot method outperforms unsupervised baselines while being\ncompetitive with supervised models in terms of visual quality and motion\nfidelity.\n", "link": "http://arxiv.org/abs/2411.04989v1", "date": "2024-11-07", "relevancy": 2.5845, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6453}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation&body=Title%3A%20SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation%0AAuthor%3A%20Koichi%20Namekata%20and%20Sherwin%20Bahmani%20and%20Ziyi%20Wu%20and%20Yash%20Kant%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Methods%20for%20image-to-video%20generation%20have%20achieved%20impressive%2C%0Aphoto-realistic%20quality.%20However%2C%20adjusting%20specific%20elements%20in%20generated%0Avideos%2C%20such%20as%20object%20motion%20or%20camera%20movement%2C%20is%20often%20a%20tedious%20process%20of%0Atrial%20and%20error%2C%20e.g.%2C%20involving%20re-generating%20videos%20with%20different%20random%0Aseeds.%20Recent%20techniques%20address%20this%20issue%20by%20fine-tuning%20a%20pre-trained%20model%0Ato%20follow%20conditioning%20signals%2C%20such%20as%20bounding%20boxes%20or%20point%20trajectories.%0AYet%2C%20this%20fine-tuning%20procedure%20can%20be%20computationally%20expensive%2C%20and%20it%0Arequires%20datasets%20with%20annotated%20object%20motion%2C%20which%20can%20be%20difficult%20to%0Aprocure.%20In%20this%20work%2C%20we%20introduce%20SG-I2V%2C%20a%20framework%20for%20controllable%0Aimage-to-video%20generation%20that%20is%20self-guided%24%5Cunicode%7Bx2013%7D%24offering%0Azero-shot%20control%20by%20relying%20solely%20on%20the%20knowledge%20present%20in%20a%20pre-trained%0Aimage-to-video%20diffusion%20model%20without%20the%20need%20for%20fine-tuning%20or%20external%0Aknowledge.%20Our%20zero-shot%20method%20outperforms%20unsupervised%20baselines%20while%20being%0Acompetitive%20with%20supervised%20models%20in%20terms%20of%20visual%20quality%20and%20motion%0Afidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-I2V%253A%2520Self-Guided%2520Trajectory%2520Control%2520in%2520Image-to-Video%2520Generation%26entry.906535625%3DKoichi%2520Namekata%2520and%2520Sherwin%2520Bahmani%2520and%2520Ziyi%2520Wu%2520and%2520Yash%2520Kant%2520and%2520Igor%2520Gilitschenski%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Methods%2520for%2520image-to-video%2520generation%2520have%2520achieved%2520impressive%252C%250Aphoto-realistic%2520quality.%2520However%252C%2520adjusting%2520specific%2520elements%2520in%2520generated%250Avideos%252C%2520such%2520as%2520object%2520motion%2520or%2520camera%2520movement%252C%2520is%2520often%2520a%2520tedious%2520process%2520of%250Atrial%2520and%2520error%252C%2520e.g.%252C%2520involving%2520re-generating%2520videos%2520with%2520different%2520random%250Aseeds.%2520Recent%2520techniques%2520address%2520this%2520issue%2520by%2520fine-tuning%2520a%2520pre-trained%2520model%250Ato%2520follow%2520conditioning%2520signals%252C%2520such%2520as%2520bounding%2520boxes%2520or%2520point%2520trajectories.%250AYet%252C%2520this%2520fine-tuning%2520procedure%2520can%2520be%2520computationally%2520expensive%252C%2520and%2520it%250Arequires%2520datasets%2520with%2520annotated%2520object%2520motion%252C%2520which%2520can%2520be%2520difficult%2520to%250Aprocure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SG-I2V%252C%2520a%2520framework%2520for%2520controllable%250Aimage-to-video%2520generation%2520that%2520is%2520self-guided%2524%255Cunicode%257Bx2013%257D%2524offering%250Azero-shot%2520control%2520by%2520relying%2520solely%2520on%2520the%2520knowledge%2520present%2520in%2520a%2520pre-trained%250Aimage-to-video%2520diffusion%2520model%2520without%2520the%2520need%2520for%2520fine-tuning%2520or%2520external%250Aknowledge.%2520Our%2520zero-shot%2520method%2520outperforms%2520unsupervised%2520baselines%2520while%2520being%250Acompetitive%2520with%2520supervised%2520models%2520in%2520terms%2520of%2520visual%2520quality%2520and%2520motion%250Afidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation&entry.906535625=Koichi%20Namekata%20and%20Sherwin%20Bahmani%20and%20Ziyi%20Wu%20and%20Yash%20Kant%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Methods%20for%20image-to-video%20generation%20have%20achieved%20impressive%2C%0Aphoto-realistic%20quality.%20However%2C%20adjusting%20specific%20elements%20in%20generated%0Avideos%2C%20such%20as%20object%20motion%20or%20camera%20movement%2C%20is%20often%20a%20tedious%20process%20of%0Atrial%20and%20error%2C%20e.g.%2C%20involving%20re-generating%20videos%20with%20different%20random%0Aseeds.%20Recent%20techniques%20address%20this%20issue%20by%20fine-tuning%20a%20pre-trained%20model%0Ato%20follow%20conditioning%20signals%2C%20such%20as%20bounding%20boxes%20or%20point%20trajectories.%0AYet%2C%20this%20fine-tuning%20procedure%20can%20be%20computationally%20expensive%2C%20and%20it%0Arequires%20datasets%20with%20annotated%20object%20motion%2C%20which%20can%20be%20difficult%20to%0Aprocure.%20In%20this%20work%2C%20we%20introduce%20SG-I2V%2C%20a%20framework%20for%20controllable%0Aimage-to-video%20generation%20that%20is%20self-guided%24%5Cunicode%7Bx2013%7D%24offering%0Azero-shot%20control%20by%20relying%20solely%20on%20the%20knowledge%20present%20in%20a%20pre-trained%0Aimage-to-video%20diffusion%20model%20without%20the%20need%20for%20fine-tuning%20or%20external%0Aknowledge.%20Our%20zero-shot%20method%20outperforms%20unsupervised%20baselines%20while%20being%0Acompetitive%20with%20supervised%20models%20in%20terms%20of%20visual%20quality%20and%20motion%0Afidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04989v1&entry.124074799=Read"},
{"title": "Multi-temporal crack segmentation in concrete structure using deep\n  learning approaches", "author": "Said Harb and Pedro Achanccaray and Mehdi Maboudi and Markus Gerke", "abstract": "  Cracks are among the earliest indicators of deterioration in concrete\nstructures. Early automatic detection of these cracks can significantly extend\nthe lifespan of critical infrastructures, such as bridges, buildings, and\ntunnels, while simultaneously reducing maintenance costs and facilitating\nefficient structural health monitoring. This study investigates whether\nleveraging multi-temporal data for crack segmentation can enhance segmentation\nquality. Therefore, we compare a Swin UNETR trained on multi-temporal data with\na U-Net trained on mono-temporal data to assess the effect of temporal\ninformation compared with conventional single-epoch approaches. To this end, a\nmulti-temporal dataset comprising 1356 images, each with 32 sequential crack\npropagation images, was created. After training the models, experiments were\nconducted to analyze their generalization ability, temporal consistency, and\nsegmentation quality. The multi-temporal approach consistently outperformed its\nmono-temporal counterpart, achieving an IoU of $82.72\\%$ and a F1-score of\n$90.54\\%$, representing a significant improvement over the mono-temporal\nmodel's IoU of $76.69\\%$ and F1-score of $86.18\\%$, despite requiring only half\nof the trainable parameters. The multi-temporal model also displayed a more\nconsistent segmentation quality, with reduced noise and fewer errors. These\nresults suggest that temporal information significantly enhances the\nperformance of segmentation models, offering a promising solution for improved\ncrack detection and the long-term monitoring of concrete structures, even with\nlimited sequential data.\n", "link": "http://arxiv.org/abs/2411.04620v1", "date": "2024-11-07", "relevancy": 2.5426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5042}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-temporal%20crack%20segmentation%20in%20concrete%20structure%20using%20deep%0A%20%20learning%20approaches&body=Title%3A%20Multi-temporal%20crack%20segmentation%20in%20concrete%20structure%20using%20deep%0A%20%20learning%20approaches%0AAuthor%3A%20Said%20Harb%20and%20Pedro%20Achanccaray%20and%20Mehdi%20Maboudi%20and%20Markus%20Gerke%0AAbstract%3A%20%20%20Cracks%20are%20among%20the%20earliest%20indicators%20of%20deterioration%20in%20concrete%0Astructures.%20Early%20automatic%20detection%20of%20these%20cracks%20can%20significantly%20extend%0Athe%20lifespan%20of%20critical%20infrastructures%2C%20such%20as%20bridges%2C%20buildings%2C%20and%0Atunnels%2C%20while%20simultaneously%20reducing%20maintenance%20costs%20and%20facilitating%0Aefficient%20structural%20health%20monitoring.%20This%20study%20investigates%20whether%0Aleveraging%20multi-temporal%20data%20for%20crack%20segmentation%20can%20enhance%20segmentation%0Aquality.%20Therefore%2C%20we%20compare%20a%20Swin%20UNETR%20trained%20on%20multi-temporal%20data%20with%0Aa%20U-Net%20trained%20on%20mono-temporal%20data%20to%20assess%20the%20effect%20of%20temporal%0Ainformation%20compared%20with%20conventional%20single-epoch%20approaches.%20To%20this%20end%2C%20a%0Amulti-temporal%20dataset%20comprising%201356%20images%2C%20each%20with%2032%20sequential%20crack%0Apropagation%20images%2C%20was%20created.%20After%20training%20the%20models%2C%20experiments%20were%0Aconducted%20to%20analyze%20their%20generalization%20ability%2C%20temporal%20consistency%2C%20and%0Asegmentation%20quality.%20The%20multi-temporal%20approach%20consistently%20outperformed%20its%0Amono-temporal%20counterpart%2C%20achieving%20an%20IoU%20of%20%2482.72%5C%25%24%20and%20a%20F1-score%20of%0A%2490.54%5C%25%24%2C%20representing%20a%20significant%20improvement%20over%20the%20mono-temporal%0Amodel%27s%20IoU%20of%20%2476.69%5C%25%24%20and%20F1-score%20of%20%2486.18%5C%25%24%2C%20despite%20requiring%20only%20half%0Aof%20the%20trainable%20parameters.%20The%20multi-temporal%20model%20also%20displayed%20a%20more%0Aconsistent%20segmentation%20quality%2C%20with%20reduced%20noise%20and%20fewer%20errors.%20These%0Aresults%20suggest%20that%20temporal%20information%20significantly%20enhances%20the%0Aperformance%20of%20segmentation%20models%2C%20offering%20a%20promising%20solution%20for%20improved%0Acrack%20detection%20and%20the%20long-term%20monitoring%20of%20concrete%20structures%2C%20even%20with%0Alimited%20sequential%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-temporal%2520crack%2520segmentation%2520in%2520concrete%2520structure%2520using%2520deep%250A%2520%2520learning%2520approaches%26entry.906535625%3DSaid%2520Harb%2520and%2520Pedro%2520Achanccaray%2520and%2520Mehdi%2520Maboudi%2520and%2520Markus%2520Gerke%26entry.1292438233%3D%2520%2520Cracks%2520are%2520among%2520the%2520earliest%2520indicators%2520of%2520deterioration%2520in%2520concrete%250Astructures.%2520Early%2520automatic%2520detection%2520of%2520these%2520cracks%2520can%2520significantly%2520extend%250Athe%2520lifespan%2520of%2520critical%2520infrastructures%252C%2520such%2520as%2520bridges%252C%2520buildings%252C%2520and%250Atunnels%252C%2520while%2520simultaneously%2520reducing%2520maintenance%2520costs%2520and%2520facilitating%250Aefficient%2520structural%2520health%2520monitoring.%2520This%2520study%2520investigates%2520whether%250Aleveraging%2520multi-temporal%2520data%2520for%2520crack%2520segmentation%2520can%2520enhance%2520segmentation%250Aquality.%2520Therefore%252C%2520we%2520compare%2520a%2520Swin%2520UNETR%2520trained%2520on%2520multi-temporal%2520data%2520with%250Aa%2520U-Net%2520trained%2520on%2520mono-temporal%2520data%2520to%2520assess%2520the%2520effect%2520of%2520temporal%250Ainformation%2520compared%2520with%2520conventional%2520single-epoch%2520approaches.%2520To%2520this%2520end%252C%2520a%250Amulti-temporal%2520dataset%2520comprising%25201356%2520images%252C%2520each%2520with%252032%2520sequential%2520crack%250Apropagation%2520images%252C%2520was%2520created.%2520After%2520training%2520the%2520models%252C%2520experiments%2520were%250Aconducted%2520to%2520analyze%2520their%2520generalization%2520ability%252C%2520temporal%2520consistency%252C%2520and%250Asegmentation%2520quality.%2520The%2520multi-temporal%2520approach%2520consistently%2520outperformed%2520its%250Amono-temporal%2520counterpart%252C%2520achieving%2520an%2520IoU%2520of%2520%252482.72%255C%2525%2524%2520and%2520a%2520F1-score%2520of%250A%252490.54%255C%2525%2524%252C%2520representing%2520a%2520significant%2520improvement%2520over%2520the%2520mono-temporal%250Amodel%2527s%2520IoU%2520of%2520%252476.69%255C%2525%2524%2520and%2520F1-score%2520of%2520%252486.18%255C%2525%2524%252C%2520despite%2520requiring%2520only%2520half%250Aof%2520the%2520trainable%2520parameters.%2520The%2520multi-temporal%2520model%2520also%2520displayed%2520a%2520more%250Aconsistent%2520segmentation%2520quality%252C%2520with%2520reduced%2520noise%2520and%2520fewer%2520errors.%2520These%250Aresults%2520suggest%2520that%2520temporal%2520information%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520segmentation%2520models%252C%2520offering%2520a%2520promising%2520solution%2520for%2520improved%250Acrack%2520detection%2520and%2520the%2520long-term%2520monitoring%2520of%2520concrete%2520structures%252C%2520even%2520with%250Alimited%2520sequential%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-temporal%20crack%20segmentation%20in%20concrete%20structure%20using%20deep%0A%20%20learning%20approaches&entry.906535625=Said%20Harb%20and%20Pedro%20Achanccaray%20and%20Mehdi%20Maboudi%20and%20Markus%20Gerke&entry.1292438233=%20%20Cracks%20are%20among%20the%20earliest%20indicators%20of%20deterioration%20in%20concrete%0Astructures.%20Early%20automatic%20detection%20of%20these%20cracks%20can%20significantly%20extend%0Athe%20lifespan%20of%20critical%20infrastructures%2C%20such%20as%20bridges%2C%20buildings%2C%20and%0Atunnels%2C%20while%20simultaneously%20reducing%20maintenance%20costs%20and%20facilitating%0Aefficient%20structural%20health%20monitoring.%20This%20study%20investigates%20whether%0Aleveraging%20multi-temporal%20data%20for%20crack%20segmentation%20can%20enhance%20segmentation%0Aquality.%20Therefore%2C%20we%20compare%20a%20Swin%20UNETR%20trained%20on%20multi-temporal%20data%20with%0Aa%20U-Net%20trained%20on%20mono-temporal%20data%20to%20assess%20the%20effect%20of%20temporal%0Ainformation%20compared%20with%20conventional%20single-epoch%20approaches.%20To%20this%20end%2C%20a%0Amulti-temporal%20dataset%20comprising%201356%20images%2C%20each%20with%2032%20sequential%20crack%0Apropagation%20images%2C%20was%20created.%20After%20training%20the%20models%2C%20experiments%20were%0Aconducted%20to%20analyze%20their%20generalization%20ability%2C%20temporal%20consistency%2C%20and%0Asegmentation%20quality.%20The%20multi-temporal%20approach%20consistently%20outperformed%20its%0Amono-temporal%20counterpart%2C%20achieving%20an%20IoU%20of%20%2482.72%5C%25%24%20and%20a%20F1-score%20of%0A%2490.54%5C%25%24%2C%20representing%20a%20significant%20improvement%20over%20the%20mono-temporal%0Amodel%27s%20IoU%20of%20%2476.69%5C%25%24%20and%20F1-score%20of%20%2486.18%5C%25%24%2C%20despite%20requiring%20only%20half%0Aof%20the%20trainable%20parameters.%20The%20multi-temporal%20model%20also%20displayed%20a%20more%0Aconsistent%20segmentation%20quality%2C%20with%20reduced%20noise%20and%20fewer%20errors.%20These%0Aresults%20suggest%20that%20temporal%20information%20significantly%20enhances%20the%0Aperformance%20of%20segmentation%20models%2C%20offering%20a%20promising%20solution%20for%20improved%0Acrack%20detection%20and%20the%20long-term%20monitoring%20of%20concrete%20structures%2C%20even%20with%0Alimited%20sequential%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04620v1&entry.124074799=Read"},
{"title": "SPGD: Steepest Perturbed Gradient Descent Optimization", "author": "Amir M. Vahedi and Horea T. Ilies", "abstract": "  Optimization algorithms are pivotal in advancing various scientific and\nindustrial fields but often encounter obstacles such as trapping in local\nminima, saddle points, and plateaus (flat regions), which makes the convergence\nto reasonable or near-optimal solutions particularly challenging. This paper\npresents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that\ninnovatively combines the principles of the gradient descent method with\nperiodic uniform perturbation sampling to effectively circumvent these\nimpediments and lead to better solutions whenever possible. SPGD is\ndistinctively designed to generate a set of candidate solutions and select the\none exhibiting the steepest loss difference relative to the current solution.\nIt enhances the traditional gradient descent approach by integrating a\nstrategic exploration mechanism that significantly increases the likelihood of\nescaping sub-optimal local minima and navigating complex optimization\nlandscapes effectively. Our approach not only retains the directed efficiency\nof gradient descent but also leverages the exploratory benefits of stochastic\nperturbations, thus enabling a more comprehensive search for global optima\nacross diverse problem spaces. We demonstrate the efficacy of SPGD in solving\nthe 3D component packing problem, an NP-hard challenge. Preliminary results\nshow a substantial improvement over four established methods, particularly on\nresponse surfaces with complex topographies and in multidimensional non-convex\ncontinuous optimization problems. Comparative analyses with established 2D\nbenchmark functions highlight SPGD's superior performance, showcasing its\nability to navigate complex optimization landscapes. These results emphasize\nSPGD's potential as a versatile tool for a wide range of optimization problems.\n", "link": "http://arxiv.org/abs/2411.04946v1", "date": "2024-11-07", "relevancy": 2.5422, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5173}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization&body=Title%3A%20SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization%0AAuthor%3A%20Amir%20M.%20Vahedi%20and%20Horea%20T.%20Ilies%0AAbstract%3A%20%20%20Optimization%20algorithms%20are%20pivotal%20in%20advancing%20various%20scientific%20and%0Aindustrial%20fields%20but%20often%20encounter%20obstacles%20such%20as%20trapping%20in%20local%0Aminima%2C%20saddle%20points%2C%20and%20plateaus%20%28flat%20regions%29%2C%20which%20makes%20the%20convergence%0Ato%20reasonable%20or%20near-optimal%20solutions%20particularly%20challenging.%20This%20paper%0Apresents%20the%20Steepest%20Perturbed%20Gradient%20Descent%20%28SPGD%29%2C%20a%20novel%20algorithm%20that%0Ainnovatively%20combines%20the%20principles%20of%20the%20gradient%20descent%20method%20with%0Aperiodic%20uniform%20perturbation%20sampling%20to%20effectively%20circumvent%20these%0Aimpediments%20and%20lead%20to%20better%20solutions%20whenever%20possible.%20SPGD%20is%0Adistinctively%20designed%20to%20generate%20a%20set%20of%20candidate%20solutions%20and%20select%20the%0Aone%20exhibiting%20the%20steepest%20loss%20difference%20relative%20to%20the%20current%20solution.%0AIt%20enhances%20the%20traditional%20gradient%20descent%20approach%20by%20integrating%20a%0Astrategic%20exploration%20mechanism%20that%20significantly%20increases%20the%20likelihood%20of%0Aescaping%20sub-optimal%20local%20minima%20and%20navigating%20complex%20optimization%0Alandscapes%20effectively.%20Our%20approach%20not%20only%20retains%20the%20directed%20efficiency%0Aof%20gradient%20descent%20but%20also%20leverages%20the%20exploratory%20benefits%20of%20stochastic%0Aperturbations%2C%20thus%20enabling%20a%20more%20comprehensive%20search%20for%20global%20optima%0Aacross%20diverse%20problem%20spaces.%20We%20demonstrate%20the%20efficacy%20of%20SPGD%20in%20solving%0Athe%203D%20component%20packing%20problem%2C%20an%20NP-hard%20challenge.%20Preliminary%20results%0Ashow%20a%20substantial%20improvement%20over%20four%20established%20methods%2C%20particularly%20on%0Aresponse%20surfaces%20with%20complex%20topographies%20and%20in%20multidimensional%20non-convex%0Acontinuous%20optimization%20problems.%20Comparative%20analyses%20with%20established%202D%0Abenchmark%20functions%20highlight%20SPGD%27s%20superior%20performance%2C%20showcasing%20its%0Aability%20to%20navigate%20complex%20optimization%20landscapes.%20These%20results%20emphasize%0ASPGD%27s%20potential%20as%20a%20versatile%20tool%20for%20a%20wide%20range%20of%20optimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPGD%253A%2520Steepest%2520Perturbed%2520Gradient%2520Descent%2520Optimization%26entry.906535625%3DAmir%2520M.%2520Vahedi%2520and%2520Horea%2520T.%2520Ilies%26entry.1292438233%3D%2520%2520Optimization%2520algorithms%2520are%2520pivotal%2520in%2520advancing%2520various%2520scientific%2520and%250Aindustrial%2520fields%2520but%2520often%2520encounter%2520obstacles%2520such%2520as%2520trapping%2520in%2520local%250Aminima%252C%2520saddle%2520points%252C%2520and%2520plateaus%2520%2528flat%2520regions%2529%252C%2520which%2520makes%2520the%2520convergence%250Ato%2520reasonable%2520or%2520near-optimal%2520solutions%2520particularly%2520challenging.%2520This%2520paper%250Apresents%2520the%2520Steepest%2520Perturbed%2520Gradient%2520Descent%2520%2528SPGD%2529%252C%2520a%2520novel%2520algorithm%2520that%250Ainnovatively%2520combines%2520the%2520principles%2520of%2520the%2520gradient%2520descent%2520method%2520with%250Aperiodic%2520uniform%2520perturbation%2520sampling%2520to%2520effectively%2520circumvent%2520these%250Aimpediments%2520and%2520lead%2520to%2520better%2520solutions%2520whenever%2520possible.%2520SPGD%2520is%250Adistinctively%2520designed%2520to%2520generate%2520a%2520set%2520of%2520candidate%2520solutions%2520and%2520select%2520the%250Aone%2520exhibiting%2520the%2520steepest%2520loss%2520difference%2520relative%2520to%2520the%2520current%2520solution.%250AIt%2520enhances%2520the%2520traditional%2520gradient%2520descent%2520approach%2520by%2520integrating%2520a%250Astrategic%2520exploration%2520mechanism%2520that%2520significantly%2520increases%2520the%2520likelihood%2520of%250Aescaping%2520sub-optimal%2520local%2520minima%2520and%2520navigating%2520complex%2520optimization%250Alandscapes%2520effectively.%2520Our%2520approach%2520not%2520only%2520retains%2520the%2520directed%2520efficiency%250Aof%2520gradient%2520descent%2520but%2520also%2520leverages%2520the%2520exploratory%2520benefits%2520of%2520stochastic%250Aperturbations%252C%2520thus%2520enabling%2520a%2520more%2520comprehensive%2520search%2520for%2520global%2520optima%250Aacross%2520diverse%2520problem%2520spaces.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520SPGD%2520in%2520solving%250Athe%25203D%2520component%2520packing%2520problem%252C%2520an%2520NP-hard%2520challenge.%2520Preliminary%2520results%250Ashow%2520a%2520substantial%2520improvement%2520over%2520four%2520established%2520methods%252C%2520particularly%2520on%250Aresponse%2520surfaces%2520with%2520complex%2520topographies%2520and%2520in%2520multidimensional%2520non-convex%250Acontinuous%2520optimization%2520problems.%2520Comparative%2520analyses%2520with%2520established%25202D%250Abenchmark%2520functions%2520highlight%2520SPGD%2527s%2520superior%2520performance%252C%2520showcasing%2520its%250Aability%2520to%2520navigate%2520complex%2520optimization%2520landscapes.%2520These%2520results%2520emphasize%250ASPGD%2527s%2520potential%2520as%2520a%2520versatile%2520tool%2520for%2520a%2520wide%2520range%2520of%2520optimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization&entry.906535625=Amir%20M.%20Vahedi%20and%20Horea%20T.%20Ilies&entry.1292438233=%20%20Optimization%20algorithms%20are%20pivotal%20in%20advancing%20various%20scientific%20and%0Aindustrial%20fields%20but%20often%20encounter%20obstacles%20such%20as%20trapping%20in%20local%0Aminima%2C%20saddle%20points%2C%20and%20plateaus%20%28flat%20regions%29%2C%20which%20makes%20the%20convergence%0Ato%20reasonable%20or%20near-optimal%20solutions%20particularly%20challenging.%20This%20paper%0Apresents%20the%20Steepest%20Perturbed%20Gradient%20Descent%20%28SPGD%29%2C%20a%20novel%20algorithm%20that%0Ainnovatively%20combines%20the%20principles%20of%20the%20gradient%20descent%20method%20with%0Aperiodic%20uniform%20perturbation%20sampling%20to%20effectively%20circumvent%20these%0Aimpediments%20and%20lead%20to%20better%20solutions%20whenever%20possible.%20SPGD%20is%0Adistinctively%20designed%20to%20generate%20a%20set%20of%20candidate%20solutions%20and%20select%20the%0Aone%20exhibiting%20the%20steepest%20loss%20difference%20relative%20to%20the%20current%20solution.%0AIt%20enhances%20the%20traditional%20gradient%20descent%20approach%20by%20integrating%20a%0Astrategic%20exploration%20mechanism%20that%20significantly%20increases%20the%20likelihood%20of%0Aescaping%20sub-optimal%20local%20minima%20and%20navigating%20complex%20optimization%0Alandscapes%20effectively.%20Our%20approach%20not%20only%20retains%20the%20directed%20efficiency%0Aof%20gradient%20descent%20but%20also%20leverages%20the%20exploratory%20benefits%20of%20stochastic%0Aperturbations%2C%20thus%20enabling%20a%20more%20comprehensive%20search%20for%20global%20optima%0Aacross%20diverse%20problem%20spaces.%20We%20demonstrate%20the%20efficacy%20of%20SPGD%20in%20solving%0Athe%203D%20component%20packing%20problem%2C%20an%20NP-hard%20challenge.%20Preliminary%20results%0Ashow%20a%20substantial%20improvement%20over%20four%20established%20methods%2C%20particularly%20on%0Aresponse%20surfaces%20with%20complex%20topographies%20and%20in%20multidimensional%20non-convex%0Acontinuous%20optimization%20problems.%20Comparative%20analyses%20with%20established%202D%0Abenchmark%20functions%20highlight%20SPGD%27s%20superior%20performance%2C%20showcasing%20its%0Aability%20to%20navigate%20complex%20optimization%20landscapes.%20These%20results%20emphasize%0ASPGD%27s%20potential%20as%20a%20versatile%20tool%20for%20a%20wide%20range%20of%20optimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04946v1&entry.124074799=Read"},
{"title": "Improved Multi-Task Brain Tumour Segmentation with Synthetic Data\n  Augmentation", "author": "Andr\u00e9 Ferreira and Tiago Jesus and Behrus Puladi and Jens Kleesiek and Victor Alves and Jan Egger", "abstract": "  This paper presents the winning solution of task 1 and the third-placed\nsolution of task 3 of the BraTS challenge. The use of automated tools in\nclinical practice has increased due to the development of more and more\nsophisticated and reliable algorithms. However, achieving clinical standards\nand developing tools for real-life scenarios is a major challenge. To this end,\nBraTS has organised tasks to find the most advanced solutions for specific\npurposes. In this paper, we propose the use of synthetic data to train\nstate-of-the-art frameworks in order to improve the segmentation of adult\ngliomas in a post-treatment scenario, and the segmentation of meningioma for\nradiotherapy planning. Our results suggest that the use of synthetic data leads\nto more robust algorithms, although the synthetic data generation pipeline is\nnot directly suited to the meningioma task. The code for these tasks is\navailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n", "link": "http://arxiv.org/abs/2411.04632v1", "date": "2024-11-07", "relevancy": 2.5, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5256}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4926}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation&body=Title%3A%20Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation%0AAuthor%3A%20Andr%C3%A9%20Ferreira%20and%20Tiago%20Jesus%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20winning%20solution%20of%20task%201%20and%20the%20third-placed%0Asolution%20of%20task%203%20of%20the%20BraTS%20challenge.%20The%20use%20of%20automated%20tools%20in%0Aclinical%20practice%20has%20increased%20due%20to%20the%20development%20of%20more%20and%20more%0Asophisticated%20and%20reliable%20algorithms.%20However%2C%20achieving%20clinical%20standards%0Aand%20developing%20tools%20for%20real-life%20scenarios%20is%20a%20major%20challenge.%20To%20this%20end%2C%0ABraTS%20has%20organised%20tasks%20to%20find%20the%20most%20advanced%20solutions%20for%20specific%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20the%20use%20of%20synthetic%20data%20to%20train%0Astate-of-the-art%20frameworks%20in%20order%20to%20improve%20the%20segmentation%20of%20adult%0Agliomas%20in%20a%20post-treatment%20scenario%2C%20and%20the%20segmentation%20of%20meningioma%20for%0Aradiotherapy%20planning.%20Our%20results%20suggest%20that%20the%20use%20of%20synthetic%20data%20leads%0Ato%20more%20robust%20algorithms%2C%20although%20the%20synthetic%20data%20generation%20pipeline%20is%0Anot%20directly%20suited%20to%20the%20meningioma%20task.%20The%20code%20for%20these%20tasks%20is%0Aavailable%20at%20https%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Multi-Task%2520Brain%2520Tumour%2520Segmentation%2520with%2520Synthetic%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DAndr%25C3%25A9%2520Ferreira%2520and%2520Tiago%2520Jesus%2520and%2520Behrus%2520Puladi%2520and%2520Jens%2520Kleesiek%2520and%2520Victor%2520Alves%2520and%2520Jan%2520Egger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520winning%2520solution%2520of%2520task%25201%2520and%2520the%2520third-placed%250Asolution%2520of%2520task%25203%2520of%2520the%2520BraTS%2520challenge.%2520The%2520use%2520of%2520automated%2520tools%2520in%250Aclinical%2520practice%2520has%2520increased%2520due%2520to%2520the%2520development%2520of%2520more%2520and%2520more%250Asophisticated%2520and%2520reliable%2520algorithms.%2520However%252C%2520achieving%2520clinical%2520standards%250Aand%2520developing%2520tools%2520for%2520real-life%2520scenarios%2520is%2520a%2520major%2520challenge.%2520To%2520this%2520end%252C%250ABraTS%2520has%2520organised%2520tasks%2520to%2520find%2520the%2520most%2520advanced%2520solutions%2520for%2520specific%250Apurposes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520use%2520of%2520synthetic%2520data%2520to%2520train%250Astate-of-the-art%2520frameworks%2520in%2520order%2520to%2520improve%2520the%2520segmentation%2520of%2520adult%250Agliomas%2520in%2520a%2520post-treatment%2520scenario%252C%2520and%2520the%2520segmentation%2520of%2520meningioma%2520for%250Aradiotherapy%2520planning.%2520Our%2520results%2520suggest%2520that%2520the%2520use%2520of%2520synthetic%2520data%2520leads%250Ato%2520more%2520robust%2520algorithms%252C%2520although%2520the%2520synthetic%2520data%2520generation%2520pipeline%2520is%250Anot%2520directly%2520suited%2520to%2520the%2520meningioma%2520task.%2520The%2520code%2520for%2520these%2520tasks%2520is%250Aavailable%2520at%2520https%253A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Multi-Task%20Brain%20Tumour%20Segmentation%20with%20Synthetic%20Data%0A%20%20Augmentation&entry.906535625=Andr%C3%A9%20Ferreira%20and%20Tiago%20Jesus%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger&entry.1292438233=%20%20This%20paper%20presents%20the%20winning%20solution%20of%20task%201%20and%20the%20third-placed%0Asolution%20of%20task%203%20of%20the%20BraTS%20challenge.%20The%20use%20of%20automated%20tools%20in%0Aclinical%20practice%20has%20increased%20due%20to%20the%20development%20of%20more%20and%20more%0Asophisticated%20and%20reliable%20algorithms.%20However%2C%20achieving%20clinical%20standards%0Aand%20developing%20tools%20for%20real-life%20scenarios%20is%20a%20major%20challenge.%20To%20this%20end%2C%0ABraTS%20has%20organised%20tasks%20to%20find%20the%20most%20advanced%20solutions%20for%20specific%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20the%20use%20of%20synthetic%20data%20to%20train%0Astate-of-the-art%20frameworks%20in%20order%20to%20improve%20the%20segmentation%20of%20adult%0Agliomas%20in%20a%20post-treatment%20scenario%2C%20and%20the%20segmentation%20of%20meningioma%20for%0Aradiotherapy%20planning.%20Our%20results%20suggest%20that%20the%20use%20of%20synthetic%20data%20leads%0Ato%20more%20robust%20algorithms%2C%20although%20the%20synthetic%20data%20generation%20pipeline%20is%0Anot%20directly%20suited%20to%20the%20meningioma%20task.%20The%20code%20for%20these%20tasks%20is%0Aavailable%20at%20https%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04632v1&entry.124074799=Read"},
{"title": "DISCO: DISCovering Overfittings as Causal Rules for Text Classification\n  Models", "author": "Zijian Zhang and Vinay Setty and Yumeng Wang and Avishek Anand", "abstract": "  With the rapid advancement of neural language models, the deployment of\nover-parameterized models has surged, increasing the need for interpretable\nexplanations comprehensible to human inspectors. Existing post-hoc\ninterpretability methods, which often focus on unigram features of single input\ntextual instances, fail to capture the models' decision-making process fully.\nAdditionally, many methods do not differentiate between decisions based on\nspurious correlations and those based on a holistic understanding of the input.\nOur paper introduces DISCO, a novel method for discovering global, rule-based\nexplanations by identifying causal n-gram associations with model predictions.\nThis method employs a scalable sequence mining technique to extract relevant\ntext spans from training data, associate them with model predictions, and\nconduct causality checks to distill robust rules that elucidate model behavior.\nThese rules expose potential overfitting and provide insights into misleading\nfeature combinations. We validate DISCO through extensive testing,\ndemonstrating its superiority over existing methods in offering comprehensive\ninsights into complex model behaviors. Our approach successfully identifies all\nshortcuts manually introduced into the training data (100% detection rate on\nthe MultiRC dataset), resulting in an 18.8% regression in model performance --\na capability unmatched by any other method. Furthermore, DISCO supports\ninteractive explanations, enabling human inspectors to distinguish spurious\ncauses in the rule-based output. This alleviates the burden of abundant\ninstance-wise explanations and helps assess the model's risk when encountering\nout-of-distribution (OOD) data.\n", "link": "http://arxiv.org/abs/2411.04649v1", "date": "2024-11-07", "relevancy": 2.4884, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCO%3A%20DISCovering%20Overfittings%20as%20Causal%20Rules%20for%20Text%20Classification%0A%20%20Models&body=Title%3A%20DISCO%3A%20DISCovering%20Overfittings%20as%20Causal%20Rules%20for%20Text%20Classification%0A%20%20Models%0AAuthor%3A%20Zijian%20Zhang%20and%20Vinay%20Setty%20and%20Yumeng%20Wang%20and%20Avishek%20Anand%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20neural%20language%20models%2C%20the%20deployment%20of%0Aover-parameterized%20models%20has%20surged%2C%20increasing%20the%20need%20for%20interpretable%0Aexplanations%20comprehensible%20to%20human%20inspectors.%20Existing%20post-hoc%0Ainterpretability%20methods%2C%20which%20often%20focus%20on%20unigram%20features%20of%20single%20input%0Atextual%20instances%2C%20fail%20to%20capture%20the%20models%27%20decision-making%20process%20fully.%0AAdditionally%2C%20many%20methods%20do%20not%20differentiate%20between%20decisions%20based%20on%0Aspurious%20correlations%20and%20those%20based%20on%20a%20holistic%20understanding%20of%20the%20input.%0AOur%20paper%20introduces%20DISCO%2C%20a%20novel%20method%20for%20discovering%20global%2C%20rule-based%0Aexplanations%20by%20identifying%20causal%20n-gram%20associations%20with%20model%20predictions.%0AThis%20method%20employs%20a%20scalable%20sequence%20mining%20technique%20to%20extract%20relevant%0Atext%20spans%20from%20training%20data%2C%20associate%20them%20with%20model%20predictions%2C%20and%0Aconduct%20causality%20checks%20to%20distill%20robust%20rules%20that%20elucidate%20model%20behavior.%0AThese%20rules%20expose%20potential%20overfitting%20and%20provide%20insights%20into%20misleading%0Afeature%20combinations.%20We%20validate%20DISCO%20through%20extensive%20testing%2C%0Ademonstrating%20its%20superiority%20over%20existing%20methods%20in%20offering%20comprehensive%0Ainsights%20into%20complex%20model%20behaviors.%20Our%20approach%20successfully%20identifies%20all%0Ashortcuts%20manually%20introduced%20into%20the%20training%20data%20%28100%25%20detection%20rate%20on%0Athe%20MultiRC%20dataset%29%2C%20resulting%20in%20an%2018.8%25%20regression%20in%20model%20performance%20--%0Aa%20capability%20unmatched%20by%20any%20other%20method.%20Furthermore%2C%20DISCO%20supports%0Ainteractive%20explanations%2C%20enabling%20human%20inspectors%20to%20distinguish%20spurious%0Acauses%20in%20the%20rule-based%20output.%20This%20alleviates%20the%20burden%20of%20abundant%0Ainstance-wise%20explanations%20and%20helps%20assess%20the%20model%27s%20risk%20when%20encountering%0Aout-of-distribution%20%28OOD%29%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCO%253A%2520DISCovering%2520Overfittings%2520as%2520Causal%2520Rules%2520for%2520Text%2520Classification%250A%2520%2520Models%26entry.906535625%3DZijian%2520Zhang%2520and%2520Vinay%2520Setty%2520and%2520Yumeng%2520Wang%2520and%2520Avishek%2520Anand%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520neural%2520language%2520models%252C%2520the%2520deployment%2520of%250Aover-parameterized%2520models%2520has%2520surged%252C%2520increasing%2520the%2520need%2520for%2520interpretable%250Aexplanations%2520comprehensible%2520to%2520human%2520inspectors.%2520Existing%2520post-hoc%250Ainterpretability%2520methods%252C%2520which%2520often%2520focus%2520on%2520unigram%2520features%2520of%2520single%2520input%250Atextual%2520instances%252C%2520fail%2520to%2520capture%2520the%2520models%2527%2520decision-making%2520process%2520fully.%250AAdditionally%252C%2520many%2520methods%2520do%2520not%2520differentiate%2520between%2520decisions%2520based%2520on%250Aspurious%2520correlations%2520and%2520those%2520based%2520on%2520a%2520holistic%2520understanding%2520of%2520the%2520input.%250AOur%2520paper%2520introduces%2520DISCO%252C%2520a%2520novel%2520method%2520for%2520discovering%2520global%252C%2520rule-based%250Aexplanations%2520by%2520identifying%2520causal%2520n-gram%2520associations%2520with%2520model%2520predictions.%250AThis%2520method%2520employs%2520a%2520scalable%2520sequence%2520mining%2520technique%2520to%2520extract%2520relevant%250Atext%2520spans%2520from%2520training%2520data%252C%2520associate%2520them%2520with%2520model%2520predictions%252C%2520and%250Aconduct%2520causality%2520checks%2520to%2520distill%2520robust%2520rules%2520that%2520elucidate%2520model%2520behavior.%250AThese%2520rules%2520expose%2520potential%2520overfitting%2520and%2520provide%2520insights%2520into%2520misleading%250Afeature%2520combinations.%2520We%2520validate%2520DISCO%2520through%2520extensive%2520testing%252C%250Ademonstrating%2520its%2520superiority%2520over%2520existing%2520methods%2520in%2520offering%2520comprehensive%250Ainsights%2520into%2520complex%2520model%2520behaviors.%2520Our%2520approach%2520successfully%2520identifies%2520all%250Ashortcuts%2520manually%2520introduced%2520into%2520the%2520training%2520data%2520%2528100%2525%2520detection%2520rate%2520on%250Athe%2520MultiRC%2520dataset%2529%252C%2520resulting%2520in%2520an%252018.8%2525%2520regression%2520in%2520model%2520performance%2520--%250Aa%2520capability%2520unmatched%2520by%2520any%2520other%2520method.%2520Furthermore%252C%2520DISCO%2520supports%250Ainteractive%2520explanations%252C%2520enabling%2520human%2520inspectors%2520to%2520distinguish%2520spurious%250Acauses%2520in%2520the%2520rule-based%2520output.%2520This%2520alleviates%2520the%2520burden%2520of%2520abundant%250Ainstance-wise%2520explanations%2520and%2520helps%2520assess%2520the%2520model%2527s%2520risk%2520when%2520encountering%250Aout-of-distribution%2520%2528OOD%2529%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCO%3A%20DISCovering%20Overfittings%20as%20Causal%20Rules%20for%20Text%20Classification%0A%20%20Models&entry.906535625=Zijian%20Zhang%20and%20Vinay%20Setty%20and%20Yumeng%20Wang%20and%20Avishek%20Anand&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20neural%20language%20models%2C%20the%20deployment%20of%0Aover-parameterized%20models%20has%20surged%2C%20increasing%20the%20need%20for%20interpretable%0Aexplanations%20comprehensible%20to%20human%20inspectors.%20Existing%20post-hoc%0Ainterpretability%20methods%2C%20which%20often%20focus%20on%20unigram%20features%20of%20single%20input%0Atextual%20instances%2C%20fail%20to%20capture%20the%20models%27%20decision-making%20process%20fully.%0AAdditionally%2C%20many%20methods%20do%20not%20differentiate%20between%20decisions%20based%20on%0Aspurious%20correlations%20and%20those%20based%20on%20a%20holistic%20understanding%20of%20the%20input.%0AOur%20paper%20introduces%20DISCO%2C%20a%20novel%20method%20for%20discovering%20global%2C%20rule-based%0Aexplanations%20by%20identifying%20causal%20n-gram%20associations%20with%20model%20predictions.%0AThis%20method%20employs%20a%20scalable%20sequence%20mining%20technique%20to%20extract%20relevant%0Atext%20spans%20from%20training%20data%2C%20associate%20them%20with%20model%20predictions%2C%20and%0Aconduct%20causality%20checks%20to%20distill%20robust%20rules%20that%20elucidate%20model%20behavior.%0AThese%20rules%20expose%20potential%20overfitting%20and%20provide%20insights%20into%20misleading%0Afeature%20combinations.%20We%20validate%20DISCO%20through%20extensive%20testing%2C%0Ademonstrating%20its%20superiority%20over%20existing%20methods%20in%20offering%20comprehensive%0Ainsights%20into%20complex%20model%20behaviors.%20Our%20approach%20successfully%20identifies%20all%0Ashortcuts%20manually%20introduced%20into%20the%20training%20data%20%28100%25%20detection%20rate%20on%0Athe%20MultiRC%20dataset%29%2C%20resulting%20in%20an%2018.8%25%20regression%20in%20model%20performance%20--%0Aa%20capability%20unmatched%20by%20any%20other%20method.%20Furthermore%2C%20DISCO%20supports%0Ainteractive%20explanations%2C%20enabling%20human%20inspectors%20to%20distinguish%20spurious%0Acauses%20in%20the%20rule-based%20output.%20This%20alleviates%20the%20burden%20of%20abundant%0Ainstance-wise%20explanations%20and%20helps%20assess%20the%20model%27s%20risk%20when%20encountering%0Aout-of-distribution%20%28OOD%29%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04649v1&entry.124074799=Read"},
{"title": "GD doesn't make the cut: Three ways that non-differentiability affects\n  neural network training", "author": "Siddharth Krishna Kumar", "abstract": "  This paper critically examines the fundamental distinctions between gradient\nmethods applied to non-differentiable functions (NGDMs) and classical gradient\ndescents (GDs) for differentiable functions, revealing significant gaps in\ncurrent deep learning optimization theory. We demonstrate that NGDMs exhibit\nmarkedly different convergence properties compared to GDs, strongly challenging\nthe applicability of extensive neural network convergence literature based on\n$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxical\nbehavior of NDGM solutions for $L_{1}$-regularized problems, where increasing\nregularization counterintuitively leads to larger $L_{1}$ norms of optimal\nsolutions. This finding calls into question widely adopted $L_{1}$ penalization\ntechniques for network pruning. We further challenge the common assumption that\noptimization algorithms like RMSProp behave similarly in differentiable and\nnon-differentiable contexts. Expanding on the Edge of Stability phenomenon, we\ndemonstrate its occurrence in a broader class of functions, including Lipschitz\ncontinuous convex differentiable functions. This finding raises important\nquestions about its relevance and interpretation in non-convex,\nnon-differentiable neural networks, particularly those using ReLU activations.\nOur work identifies critical misunderstandings of NDGMs in influential\nliterature, stemming from an overreliance on strong smoothness assumptions.\nThese findings necessitate a reevaluation of optimization dynamics in deep\nlearning, emphasizing the crucial need for more nuanced theoretical foundations\nin analyzing these complex systems.\n", "link": "http://arxiv.org/abs/2401.08426v5", "date": "2024-11-07", "relevancy": 2.4805, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5401}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4763}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GD%20doesn%27t%20make%20the%20cut%3A%20Three%20ways%20that%20non-differentiability%20affects%0A%20%20neural%20network%20training&body=Title%3A%20GD%20doesn%27t%20make%20the%20cut%3A%20Three%20ways%20that%20non-differentiability%20affects%0A%20%20neural%20network%20training%0AAuthor%3A%20Siddharth%20Krishna%20Kumar%0AAbstract%3A%20%20%20This%20paper%20critically%20examines%20the%20fundamental%20distinctions%20between%20gradient%0Amethods%20applied%20to%20non-differentiable%20functions%20%28NGDMs%29%20and%20classical%20gradient%0Adescents%20%28GDs%29%20for%20differentiable%20functions%2C%20revealing%20significant%20gaps%20in%0Acurrent%20deep%20learning%20optimization%20theory.%20We%20demonstrate%20that%20NGDMs%20exhibit%0Amarkedly%20different%20convergence%20properties%20compared%20to%20GDs%2C%20strongly%20challenging%0Athe%20applicability%20of%20extensive%20neural%20network%20convergence%20literature%20based%20on%0A%24L-smoothness%24%20to%20non-smooth%20neural%20networks.%20Our%20analysis%20reveals%20paradoxical%0Abehavior%20of%20NDGM%20solutions%20for%20%24L_%7B1%7D%24-regularized%20problems%2C%20where%20increasing%0Aregularization%20counterintuitively%20leads%20to%20larger%20%24L_%7B1%7D%24%20norms%20of%20optimal%0Asolutions.%20This%20finding%20calls%20into%20question%20widely%20adopted%20%24L_%7B1%7D%24%20penalization%0Atechniques%20for%20network%20pruning.%20We%20further%20challenge%20the%20common%20assumption%20that%0Aoptimization%20algorithms%20like%20RMSProp%20behave%20similarly%20in%20differentiable%20and%0Anon-differentiable%20contexts.%20Expanding%20on%20the%20Edge%20of%20Stability%20phenomenon%2C%20we%0Ademonstrate%20its%20occurrence%20in%20a%20broader%20class%20of%20functions%2C%20including%20Lipschitz%0Acontinuous%20convex%20differentiable%20functions.%20This%20finding%20raises%20important%0Aquestions%20about%20its%20relevance%20and%20interpretation%20in%20non-convex%2C%0Anon-differentiable%20neural%20networks%2C%20particularly%20those%20using%20ReLU%20activations.%0AOur%20work%20identifies%20critical%20misunderstandings%20of%20NDGMs%20in%20influential%0Aliterature%2C%20stemming%20from%20an%20overreliance%20on%20strong%20smoothness%20assumptions.%0AThese%20findings%20necessitate%20a%20reevaluation%20of%20optimization%20dynamics%20in%20deep%0Alearning%2C%20emphasizing%20the%20crucial%20need%20for%20more%20nuanced%20theoretical%20foundations%0Ain%20analyzing%20these%20complex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08426v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGD%2520doesn%2527t%2520make%2520the%2520cut%253A%2520Three%2520ways%2520that%2520non-differentiability%2520affects%250A%2520%2520neural%2520network%2520training%26entry.906535625%3DSiddharth%2520Krishna%2520Kumar%26entry.1292438233%3D%2520%2520This%2520paper%2520critically%2520examines%2520the%2520fundamental%2520distinctions%2520between%2520gradient%250Amethods%2520applied%2520to%2520non-differentiable%2520functions%2520%2528NGDMs%2529%2520and%2520classical%2520gradient%250Adescents%2520%2528GDs%2529%2520for%2520differentiable%2520functions%252C%2520revealing%2520significant%2520gaps%2520in%250Acurrent%2520deep%2520learning%2520optimization%2520theory.%2520We%2520demonstrate%2520that%2520NGDMs%2520exhibit%250Amarkedly%2520different%2520convergence%2520properties%2520compared%2520to%2520GDs%252C%2520strongly%2520challenging%250Athe%2520applicability%2520of%2520extensive%2520neural%2520network%2520convergence%2520literature%2520based%2520on%250A%2524L-smoothness%2524%2520to%2520non-smooth%2520neural%2520networks.%2520Our%2520analysis%2520reveals%2520paradoxical%250Abehavior%2520of%2520NDGM%2520solutions%2520for%2520%2524L_%257B1%257D%2524-regularized%2520problems%252C%2520where%2520increasing%250Aregularization%2520counterintuitively%2520leads%2520to%2520larger%2520%2524L_%257B1%257D%2524%2520norms%2520of%2520optimal%250Asolutions.%2520This%2520finding%2520calls%2520into%2520question%2520widely%2520adopted%2520%2524L_%257B1%257D%2524%2520penalization%250Atechniques%2520for%2520network%2520pruning.%2520We%2520further%2520challenge%2520the%2520common%2520assumption%2520that%250Aoptimization%2520algorithms%2520like%2520RMSProp%2520behave%2520similarly%2520in%2520differentiable%2520and%250Anon-differentiable%2520contexts.%2520Expanding%2520on%2520the%2520Edge%2520of%2520Stability%2520phenomenon%252C%2520we%250Ademonstrate%2520its%2520occurrence%2520in%2520a%2520broader%2520class%2520of%2520functions%252C%2520including%2520Lipschitz%250Acontinuous%2520convex%2520differentiable%2520functions.%2520This%2520finding%2520raises%2520important%250Aquestions%2520about%2520its%2520relevance%2520and%2520interpretation%2520in%2520non-convex%252C%250Anon-differentiable%2520neural%2520networks%252C%2520particularly%2520those%2520using%2520ReLU%2520activations.%250AOur%2520work%2520identifies%2520critical%2520misunderstandings%2520of%2520NDGMs%2520in%2520influential%250Aliterature%252C%2520stemming%2520from%2520an%2520overreliance%2520on%2520strong%2520smoothness%2520assumptions.%250AThese%2520findings%2520necessitate%2520a%2520reevaluation%2520of%2520optimization%2520dynamics%2520in%2520deep%250Alearning%252C%2520emphasizing%2520the%2520crucial%2520need%2520for%2520more%2520nuanced%2520theoretical%2520foundations%250Ain%2520analyzing%2520these%2520complex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08426v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GD%20doesn%27t%20make%20the%20cut%3A%20Three%20ways%20that%20non-differentiability%20affects%0A%20%20neural%20network%20training&entry.906535625=Siddharth%20Krishna%20Kumar&entry.1292438233=%20%20This%20paper%20critically%20examines%20the%20fundamental%20distinctions%20between%20gradient%0Amethods%20applied%20to%20non-differentiable%20functions%20%28NGDMs%29%20and%20classical%20gradient%0Adescents%20%28GDs%29%20for%20differentiable%20functions%2C%20revealing%20significant%20gaps%20in%0Acurrent%20deep%20learning%20optimization%20theory.%20We%20demonstrate%20that%20NGDMs%20exhibit%0Amarkedly%20different%20convergence%20properties%20compared%20to%20GDs%2C%20strongly%20challenging%0Athe%20applicability%20of%20extensive%20neural%20network%20convergence%20literature%20based%20on%0A%24L-smoothness%24%20to%20non-smooth%20neural%20networks.%20Our%20analysis%20reveals%20paradoxical%0Abehavior%20of%20NDGM%20solutions%20for%20%24L_%7B1%7D%24-regularized%20problems%2C%20where%20increasing%0Aregularization%20counterintuitively%20leads%20to%20larger%20%24L_%7B1%7D%24%20norms%20of%20optimal%0Asolutions.%20This%20finding%20calls%20into%20question%20widely%20adopted%20%24L_%7B1%7D%24%20penalization%0Atechniques%20for%20network%20pruning.%20We%20further%20challenge%20the%20common%20assumption%20that%0Aoptimization%20algorithms%20like%20RMSProp%20behave%20similarly%20in%20differentiable%20and%0Anon-differentiable%20contexts.%20Expanding%20on%20the%20Edge%20of%20Stability%20phenomenon%2C%20we%0Ademonstrate%20its%20occurrence%20in%20a%20broader%20class%20of%20functions%2C%20including%20Lipschitz%0Acontinuous%20convex%20differentiable%20functions.%20This%20finding%20raises%20important%0Aquestions%20about%20its%20relevance%20and%20interpretation%20in%20non-convex%2C%0Anon-differentiable%20neural%20networks%2C%20particularly%20those%20using%20ReLU%20activations.%0AOur%20work%20identifies%20critical%20misunderstandings%20of%20NDGMs%20in%20influential%0Aliterature%2C%20stemming%20from%20an%20overreliance%20on%20strong%20smoothness%20assumptions.%0AThese%20findings%20necessitate%20a%20reevaluation%20of%20optimization%20dynamics%20in%20deep%0Alearning%2C%20emphasizing%20the%20crucial%20need%20for%20more%20nuanced%20theoretical%20foundations%0Ain%20analyzing%20these%20complex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08426v5&entry.124074799=Read"},
{"title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition", "author": "Maximillian Chen and Zhou Yu", "abstract": "  Speech models have long been known to overfit individual speakers for many\nclassification tasks. This leads to poor generalization in settings where the\nspeakers are out-of-domain or out-of-distribution, as is common in production\nenvironments. We view speaker adaptation as a few-shot learning problem and\npropose investigating transfer learning approaches inspired by recent success\nwith pre-trained models in natural language tasks. We propose pre-finetuning\nspeech models on difficult tasks to distill knowledge into few-shot downstream\nclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation of\nfour multiclass emotional speech recognition corpora and evaluate our\npre-finetuned models through 33,600 few-shot fine-tuning trials on the\nEmotional Speech Dataset.\n", "link": "http://arxiv.org/abs/2302.12921v3", "date": "2024-11-07", "relevancy": 2.4733, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4957}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Finetuning%20for%20Few-Shot%20Emotional%20Speech%20Recognition&body=Title%3A%20Pre-Finetuning%20for%20Few-Shot%20Emotional%20Speech%20Recognition%0AAuthor%3A%20Maximillian%20Chen%20and%20Zhou%20Yu%0AAbstract%3A%20%20%20Speech%20models%20have%20long%20been%20known%20to%20overfit%20individual%20speakers%20for%20many%0Aclassification%20tasks.%20This%20leads%20to%20poor%20generalization%20in%20settings%20where%20the%0Aspeakers%20are%20out-of-domain%20or%20out-of-distribution%2C%20as%20is%20common%20in%20production%0Aenvironments.%20We%20view%20speaker%20adaptation%20as%20a%20few-shot%20learning%20problem%20and%0Apropose%20investigating%20transfer%20learning%20approaches%20inspired%20by%20recent%20success%0Awith%20pre-trained%20models%20in%20natural%20language%20tasks.%20We%20propose%20pre-finetuning%0Aspeech%20models%20on%20difficult%20tasks%20to%20distill%20knowledge%20into%20few-shot%20downstream%0Aclassification%20objectives.%20We%20pre-finetune%20Wav2Vec2.0%20on%20every%20permutation%20of%0Afour%20multiclass%20emotional%20speech%20recognition%20corpora%20and%20evaluate%20our%0Apre-finetuned%20models%20through%2033%2C600%20few-shot%20fine-tuning%20trials%20on%20the%0AEmotional%20Speech%20Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.12921v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Finetuning%2520for%2520Few-Shot%2520Emotional%2520Speech%2520Recognition%26entry.906535625%3DMaximillian%2520Chen%2520and%2520Zhou%2520Yu%26entry.1292438233%3D%2520%2520Speech%2520models%2520have%2520long%2520been%2520known%2520to%2520overfit%2520individual%2520speakers%2520for%2520many%250Aclassification%2520tasks.%2520This%2520leads%2520to%2520poor%2520generalization%2520in%2520settings%2520where%2520the%250Aspeakers%2520are%2520out-of-domain%2520or%2520out-of-distribution%252C%2520as%2520is%2520common%2520in%2520production%250Aenvironments.%2520We%2520view%2520speaker%2520adaptation%2520as%2520a%2520few-shot%2520learning%2520problem%2520and%250Apropose%2520investigating%2520transfer%2520learning%2520approaches%2520inspired%2520by%2520recent%2520success%250Awith%2520pre-trained%2520models%2520in%2520natural%2520language%2520tasks.%2520We%2520propose%2520pre-finetuning%250Aspeech%2520models%2520on%2520difficult%2520tasks%2520to%2520distill%2520knowledge%2520into%2520few-shot%2520downstream%250Aclassification%2520objectives.%2520We%2520pre-finetune%2520Wav2Vec2.0%2520on%2520every%2520permutation%2520of%250Afour%2520multiclass%2520emotional%2520speech%2520recognition%2520corpora%2520and%2520evaluate%2520our%250Apre-finetuned%2520models%2520through%252033%252C600%2520few-shot%2520fine-tuning%2520trials%2520on%2520the%250AEmotional%2520Speech%2520Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.12921v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Finetuning%20for%20Few-Shot%20Emotional%20Speech%20Recognition&entry.906535625=Maximillian%20Chen%20and%20Zhou%20Yu&entry.1292438233=%20%20Speech%20models%20have%20long%20been%20known%20to%20overfit%20individual%20speakers%20for%20many%0Aclassification%20tasks.%20This%20leads%20to%20poor%20generalization%20in%20settings%20where%20the%0Aspeakers%20are%20out-of-domain%20or%20out-of-distribution%2C%20as%20is%20common%20in%20production%0Aenvironments.%20We%20view%20speaker%20adaptation%20as%20a%20few-shot%20learning%20problem%20and%0Apropose%20investigating%20transfer%20learning%20approaches%20inspired%20by%20recent%20success%0Awith%20pre-trained%20models%20in%20natural%20language%20tasks.%20We%20propose%20pre-finetuning%0Aspeech%20models%20on%20difficult%20tasks%20to%20distill%20knowledge%20into%20few-shot%20downstream%0Aclassification%20objectives.%20We%20pre-finetune%20Wav2Vec2.0%20on%20every%20permutation%20of%0Afour%20multiclass%20emotional%20speech%20recognition%20corpora%20and%20evaluate%20our%0Apre-finetuned%20models%20through%2033%2C600%20few-shot%20fine-tuning%20trials%20on%20the%0AEmotional%20Speech%20Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.12921v3&entry.124074799=Read"},
{"title": "Structure Matters: Dynamic Policy Gradient", "author": "Sara Klein and Xiangyuan Zhang and Tamer Ba\u015far and Simon Weissmann and Leif D\u00f6ring", "abstract": "  In this work, we study $\\gamma$-discounted infinite-horizon tabular Markov\ndecision processes (MDPs) and introduce a framework called dynamic policy\ngradient (DynPG). The framework directly integrates dynamic programming with\n(any) policy gradient method, explicitly leveraging the Markovian property of\nthe environment. DynPG dynamically adjusts the problem horizon during training,\ndecomposing the original infinite-horizon MDP into a sequence of contextual\nbandit problems. By iteratively solving these contextual bandits, DynPG\nconverges to the stationary optimal policy of the infinite-horizon MDP. To\ndemonstrate the power of DynPG, we establish its non-asymptotic global\nconvergence rate under the tabular softmax parametrization, focusing on the\ndependencies on salient but essential parameters of the MDP. By combining\nclassical arguments from dynamic programming with more recent convergence\narguments of policy gradient schemes, we prove that softmax DynPG scales\npolynomially in the effective horizon $(1-\\gamma)^{-1}$. Our findings contrast\nrecent exponential lower bound examples for vanilla policy gradient.\n", "link": "http://arxiv.org/abs/2411.04913v1", "date": "2024-11-07", "relevancy": 2.4723, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.515}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4943}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20Matters%3A%20Dynamic%20Policy%20Gradient&body=Title%3A%20Structure%20Matters%3A%20Dynamic%20Policy%20Gradient%0AAuthor%3A%20Sara%20Klein%20and%20Xiangyuan%20Zhang%20and%20Tamer%20Ba%C5%9Far%20and%20Simon%20Weissmann%20and%20Leif%20D%C3%B6ring%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20%24%5Cgamma%24-discounted%20infinite-horizon%20tabular%20Markov%0Adecision%20processes%20%28MDPs%29%20and%20introduce%20a%20framework%20called%20dynamic%20policy%0Agradient%20%28DynPG%29.%20The%20framework%20directly%20integrates%20dynamic%20programming%20with%0A%28any%29%20policy%20gradient%20method%2C%20explicitly%20leveraging%20the%20Markovian%20property%20of%0Athe%20environment.%20DynPG%20dynamically%20adjusts%20the%20problem%20horizon%20during%20training%2C%0Adecomposing%20the%20original%20infinite-horizon%20MDP%20into%20a%20sequence%20of%20contextual%0Abandit%20problems.%20By%20iteratively%20solving%20these%20contextual%20bandits%2C%20DynPG%0Aconverges%20to%20the%20stationary%20optimal%20policy%20of%20the%20infinite-horizon%20MDP.%20To%0Ademonstrate%20the%20power%20of%20DynPG%2C%20we%20establish%20its%20non-asymptotic%20global%0Aconvergence%20rate%20under%20the%20tabular%20softmax%20parametrization%2C%20focusing%20on%20the%0Adependencies%20on%20salient%20but%20essential%20parameters%20of%20the%20MDP.%20By%20combining%0Aclassical%20arguments%20from%20dynamic%20programming%20with%20more%20recent%20convergence%0Aarguments%20of%20policy%20gradient%20schemes%2C%20we%20prove%20that%20softmax%20DynPG%20scales%0Apolynomially%20in%20the%20effective%20horizon%20%24%281-%5Cgamma%29%5E%7B-1%7D%24.%20Our%20findings%20contrast%0Arecent%20exponential%20lower%20bound%20examples%20for%20vanilla%20policy%20gradient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520Matters%253A%2520Dynamic%2520Policy%2520Gradient%26entry.906535625%3DSara%2520Klein%2520and%2520Xiangyuan%2520Zhang%2520and%2520Tamer%2520Ba%25C5%259Far%2520and%2520Simon%2520Weissmann%2520and%2520Leif%2520D%25C3%25B6ring%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520%2524%255Cgamma%2524-discounted%2520infinite-horizon%2520tabular%2520Markov%250Adecision%2520processes%2520%2528MDPs%2529%2520and%2520introduce%2520a%2520framework%2520called%2520dynamic%2520policy%250Agradient%2520%2528DynPG%2529.%2520The%2520framework%2520directly%2520integrates%2520dynamic%2520programming%2520with%250A%2528any%2529%2520policy%2520gradient%2520method%252C%2520explicitly%2520leveraging%2520the%2520Markovian%2520property%2520of%250Athe%2520environment.%2520DynPG%2520dynamically%2520adjusts%2520the%2520problem%2520horizon%2520during%2520training%252C%250Adecomposing%2520the%2520original%2520infinite-horizon%2520MDP%2520into%2520a%2520sequence%2520of%2520contextual%250Abandit%2520problems.%2520By%2520iteratively%2520solving%2520these%2520contextual%2520bandits%252C%2520DynPG%250Aconverges%2520to%2520the%2520stationary%2520optimal%2520policy%2520of%2520the%2520infinite-horizon%2520MDP.%2520To%250Ademonstrate%2520the%2520power%2520of%2520DynPG%252C%2520we%2520establish%2520its%2520non-asymptotic%2520global%250Aconvergence%2520rate%2520under%2520the%2520tabular%2520softmax%2520parametrization%252C%2520focusing%2520on%2520the%250Adependencies%2520on%2520salient%2520but%2520essential%2520parameters%2520of%2520the%2520MDP.%2520By%2520combining%250Aclassical%2520arguments%2520from%2520dynamic%2520programming%2520with%2520more%2520recent%2520convergence%250Aarguments%2520of%2520policy%2520gradient%2520schemes%252C%2520we%2520prove%2520that%2520softmax%2520DynPG%2520scales%250Apolynomially%2520in%2520the%2520effective%2520horizon%2520%2524%25281-%255Cgamma%2529%255E%257B-1%257D%2524.%2520Our%2520findings%2520contrast%250Arecent%2520exponential%2520lower%2520bound%2520examples%2520for%2520vanilla%2520policy%2520gradient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20Matters%3A%20Dynamic%20Policy%20Gradient&entry.906535625=Sara%20Klein%20and%20Xiangyuan%20Zhang%20and%20Tamer%20Ba%C5%9Far%20and%20Simon%20Weissmann%20and%20Leif%20D%C3%B6ring&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20%24%5Cgamma%24-discounted%20infinite-horizon%20tabular%20Markov%0Adecision%20processes%20%28MDPs%29%20and%20introduce%20a%20framework%20called%20dynamic%20policy%0Agradient%20%28DynPG%29.%20The%20framework%20directly%20integrates%20dynamic%20programming%20with%0A%28any%29%20policy%20gradient%20method%2C%20explicitly%20leveraging%20the%20Markovian%20property%20of%0Athe%20environment.%20DynPG%20dynamically%20adjusts%20the%20problem%20horizon%20during%20training%2C%0Adecomposing%20the%20original%20infinite-horizon%20MDP%20into%20a%20sequence%20of%20contextual%0Abandit%20problems.%20By%20iteratively%20solving%20these%20contextual%20bandits%2C%20DynPG%0Aconverges%20to%20the%20stationary%20optimal%20policy%20of%20the%20infinite-horizon%20MDP.%20To%0Ademonstrate%20the%20power%20of%20DynPG%2C%20we%20establish%20its%20non-asymptotic%20global%0Aconvergence%20rate%20under%20the%20tabular%20softmax%20parametrization%2C%20focusing%20on%20the%0Adependencies%20on%20salient%20but%20essential%20parameters%20of%20the%20MDP.%20By%20combining%0Aclassical%20arguments%20from%20dynamic%20programming%20with%20more%20recent%20convergence%0Aarguments%20of%20policy%20gradient%20schemes%2C%20we%20prove%20that%20softmax%20DynPG%20scales%0Apolynomially%20in%20the%20effective%20horizon%20%24%281-%5Cgamma%29%5E%7B-1%7D%24.%20Our%20findings%20contrast%0Arecent%20exponential%20lower%20bound%20examples%20for%20vanilla%20policy%20gradient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04913v1&entry.124074799=Read"},
{"title": "Regularized Projection Matrix Approximation with Applications to\n  Community Detection", "author": "Zheng Zhai and Jialu Xu and Mingxin Wu and Xiaohui Li", "abstract": "  This paper introduces a regularized projection matrix approximation framework\ndesigned to recover cluster information from the affinity matrix. The model is\nformulated as a projection approximation problem, incorporating an entry-wise\npenalty function. We investigate three distinct penalty functions, each\nspecifically tailored to address bounded, positive, and sparse scenarios. To\nsolve this problem, we propose direct optimization on the Stiefel manifold,\nutilizing the Cayley transformation along with the Alternating Direction Method\nof Multipliers (ADMM) algorithm. Additionally, we provide a theoretical\nanalysis that establishes the convergence properties of ADMM, demonstrating\nthat the convergence point satisfies the KKT conditions of the original\nproblem. Numerical experiments conducted on both synthetic and real-world\ndatasets reveal that our regularized projection matrix approximation approach\nsignificantly outperforms state-of-the-art methods in clustering performance.\n", "link": "http://arxiv.org/abs/2405.16598v2", "date": "2024-11-07", "relevancy": 2.4704, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4972}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4968}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Projection%20Matrix%20Approximation%20with%20Applications%20to%0A%20%20Community%20Detection&body=Title%3A%20Regularized%20Projection%20Matrix%20Approximation%20with%20Applications%20to%0A%20%20Community%20Detection%0AAuthor%3A%20Zheng%20Zhai%20and%20Jialu%20Xu%20and%20Mingxin%20Wu%20and%20Xiaohui%20Li%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20regularized%20projection%20matrix%20approximation%20framework%0Adesigned%20to%20recover%20cluster%20information%20from%20the%20affinity%20matrix.%20The%20model%20is%0Aformulated%20as%20a%20projection%20approximation%20problem%2C%20incorporating%20an%20entry-wise%0Apenalty%20function.%20We%20investigate%20three%20distinct%20penalty%20functions%2C%20each%0Aspecifically%20tailored%20to%20address%20bounded%2C%20positive%2C%20and%20sparse%20scenarios.%20To%0Asolve%20this%20problem%2C%20we%20propose%20direct%20optimization%20on%20the%20Stiefel%20manifold%2C%0Autilizing%20the%20Cayley%20transformation%20along%20with%20the%20Alternating%20Direction%20Method%0Aof%20Multipliers%20%28ADMM%29%20algorithm.%20Additionally%2C%20we%20provide%20a%20theoretical%0Aanalysis%20that%20establishes%20the%20convergence%20properties%20of%20ADMM%2C%20demonstrating%0Athat%20the%20convergence%20point%20satisfies%20the%20KKT%20conditions%20of%20the%20original%0Aproblem.%20Numerical%20experiments%20conducted%20on%20both%20synthetic%20and%20real-world%0Adatasets%20reveal%20that%20our%20regularized%20projection%20matrix%20approximation%20approach%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%20clustering%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16598v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Projection%2520Matrix%2520Approximation%2520with%2520Applications%2520to%250A%2520%2520Community%2520Detection%26entry.906535625%3DZheng%2520Zhai%2520and%2520Jialu%2520Xu%2520and%2520Mingxin%2520Wu%2520and%2520Xiaohui%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520regularized%2520projection%2520matrix%2520approximation%2520framework%250Adesigned%2520to%2520recover%2520cluster%2520information%2520from%2520the%2520affinity%2520matrix.%2520The%2520model%2520is%250Aformulated%2520as%2520a%2520projection%2520approximation%2520problem%252C%2520incorporating%2520an%2520entry-wise%250Apenalty%2520function.%2520We%2520investigate%2520three%2520distinct%2520penalty%2520functions%252C%2520each%250Aspecifically%2520tailored%2520to%2520address%2520bounded%252C%2520positive%252C%2520and%2520sparse%2520scenarios.%2520To%250Asolve%2520this%2520problem%252C%2520we%2520propose%2520direct%2520optimization%2520on%2520the%2520Stiefel%2520manifold%252C%250Autilizing%2520the%2520Cayley%2520transformation%2520along%2520with%2520the%2520Alternating%2520Direction%2520Method%250Aof%2520Multipliers%2520%2528ADMM%2529%2520algorithm.%2520Additionally%252C%2520we%2520provide%2520a%2520theoretical%250Aanalysis%2520that%2520establishes%2520the%2520convergence%2520properties%2520of%2520ADMM%252C%2520demonstrating%250Athat%2520the%2520convergence%2520point%2520satisfies%2520the%2520KKT%2520conditions%2520of%2520the%2520original%250Aproblem.%2520Numerical%2520experiments%2520conducted%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets%2520reveal%2520that%2520our%2520regularized%2520projection%2520matrix%2520approximation%2520approach%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520clustering%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16598v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Projection%20Matrix%20Approximation%20with%20Applications%20to%0A%20%20Community%20Detection&entry.906535625=Zheng%20Zhai%20and%20Jialu%20Xu%20and%20Mingxin%20Wu%20and%20Xiaohui%20Li&entry.1292438233=%20%20This%20paper%20introduces%20a%20regularized%20projection%20matrix%20approximation%20framework%0Adesigned%20to%20recover%20cluster%20information%20from%20the%20affinity%20matrix.%20The%20model%20is%0Aformulated%20as%20a%20projection%20approximation%20problem%2C%20incorporating%20an%20entry-wise%0Apenalty%20function.%20We%20investigate%20three%20distinct%20penalty%20functions%2C%20each%0Aspecifically%20tailored%20to%20address%20bounded%2C%20positive%2C%20and%20sparse%20scenarios.%20To%0Asolve%20this%20problem%2C%20we%20propose%20direct%20optimization%20on%20the%20Stiefel%20manifold%2C%0Autilizing%20the%20Cayley%20transformation%20along%20with%20the%20Alternating%20Direction%20Method%0Aof%20Multipliers%20%28ADMM%29%20algorithm.%20Additionally%2C%20we%20provide%20a%20theoretical%0Aanalysis%20that%20establishes%20the%20convergence%20properties%20of%20ADMM%2C%20demonstrating%0Athat%20the%20convergence%20point%20satisfies%20the%20KKT%20conditions%20of%20the%20original%0Aproblem.%20Numerical%20experiments%20conducted%20on%20both%20synthetic%20and%20real-world%0Adatasets%20reveal%20that%20our%20regularized%20projection%20matrix%20approximation%20approach%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%20clustering%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16598v2&entry.124074799=Read"},
{"title": "Mining Glitch Tokens in Large Language Models via Gradient-based\n  Discrete Optimization", "author": "Zihui Wu and Haichang Gao and Ping Wang and Shudong Zhang and Zhaoxiang Liu and Shiguo Lian", "abstract": "  Glitch tokens in Large Language Models (LLMs) can trigger unpredictable\nbehaviors, threatening model reliability and safety. Existing detection methods\noften depend on predefined patterns, limiting their adaptability across diverse\nLLM architectures. We propose GlitchMiner, a gradient-based discrete\noptimization framework that efficiently identifies glitch tokens by leveraging\nentropy to quantify prediction uncertainty and a local search strategy for\nexploring the token space. Experiments across multiple LLM architectures show\nthat GlitchMiner outperforms existing methods in both detection accuracy and\nadaptability, achieving over 10% average efficiency improvement. GlitchMiner\nenhances vulnerability assessment in LLMs, contributing to more robust and\nreliable applications. Code is available at\nhttps://github.com/wooozihui/GlitchMiner.\n", "link": "http://arxiv.org/abs/2410.15052v3", "date": "2024-11-07", "relevancy": 2.4631, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5165}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4831}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mining%20Glitch%20Tokens%20in%20Large%20Language%20Models%20via%20Gradient-based%0A%20%20Discrete%20Optimization&body=Title%3A%20Mining%20Glitch%20Tokens%20in%20Large%20Language%20Models%20via%20Gradient-based%0A%20%20Discrete%20Optimization%0AAuthor%3A%20Zihui%20Wu%20and%20Haichang%20Gao%20and%20Ping%20Wang%20and%20Shudong%20Zhang%20and%20Zhaoxiang%20Liu%20and%20Shiguo%20Lian%0AAbstract%3A%20%20%20Glitch%20tokens%20in%20Large%20Language%20Models%20%28LLMs%29%20can%20trigger%20unpredictable%0Abehaviors%2C%20threatening%20model%20reliability%20and%20safety.%20Existing%20detection%20methods%0Aoften%20depend%20on%20predefined%20patterns%2C%20limiting%20their%20adaptability%20across%20diverse%0ALLM%20architectures.%20We%20propose%20GlitchMiner%2C%20a%20gradient-based%20discrete%0Aoptimization%20framework%20that%20efficiently%20identifies%20glitch%20tokens%20by%20leveraging%0Aentropy%20to%20quantify%20prediction%20uncertainty%20and%20a%20local%20search%20strategy%20for%0Aexploring%20the%20token%20space.%20Experiments%20across%20multiple%20LLM%20architectures%20show%0Athat%20GlitchMiner%20outperforms%20existing%20methods%20in%20both%20detection%20accuracy%20and%0Aadaptability%2C%20achieving%20over%2010%25%20average%20efficiency%20improvement.%20GlitchMiner%0Aenhances%20vulnerability%20assessment%20in%20LLMs%2C%20contributing%20to%20more%20robust%20and%0Areliable%20applications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wooozihui/GlitchMiner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15052v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMining%2520Glitch%2520Tokens%2520in%2520Large%2520Language%2520Models%2520via%2520Gradient-based%250A%2520%2520Discrete%2520Optimization%26entry.906535625%3DZihui%2520Wu%2520and%2520Haichang%2520Gao%2520and%2520Ping%2520Wang%2520and%2520Shudong%2520Zhang%2520and%2520Zhaoxiang%2520Liu%2520and%2520Shiguo%2520Lian%26entry.1292438233%3D%2520%2520Glitch%2520tokens%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520trigger%2520unpredictable%250Abehaviors%252C%2520threatening%2520model%2520reliability%2520and%2520safety.%2520Existing%2520detection%2520methods%250Aoften%2520depend%2520on%2520predefined%2520patterns%252C%2520limiting%2520their%2520adaptability%2520across%2520diverse%250ALLM%2520architectures.%2520We%2520propose%2520GlitchMiner%252C%2520a%2520gradient-based%2520discrete%250Aoptimization%2520framework%2520that%2520efficiently%2520identifies%2520glitch%2520tokens%2520by%2520leveraging%250Aentropy%2520to%2520quantify%2520prediction%2520uncertainty%2520and%2520a%2520local%2520search%2520strategy%2520for%250Aexploring%2520the%2520token%2520space.%2520Experiments%2520across%2520multiple%2520LLM%2520architectures%2520show%250Athat%2520GlitchMiner%2520outperforms%2520existing%2520methods%2520in%2520both%2520detection%2520accuracy%2520and%250Aadaptability%252C%2520achieving%2520over%252010%2525%2520average%2520efficiency%2520improvement.%2520GlitchMiner%250Aenhances%2520vulnerability%2520assessment%2520in%2520LLMs%252C%2520contributing%2520to%2520more%2520robust%2520and%250Areliable%2520applications.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/wooozihui/GlitchMiner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15052v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Glitch%20Tokens%20in%20Large%20Language%20Models%20via%20Gradient-based%0A%20%20Discrete%20Optimization&entry.906535625=Zihui%20Wu%20and%20Haichang%20Gao%20and%20Ping%20Wang%20and%20Shudong%20Zhang%20and%20Zhaoxiang%20Liu%20and%20Shiguo%20Lian&entry.1292438233=%20%20Glitch%20tokens%20in%20Large%20Language%20Models%20%28LLMs%29%20can%20trigger%20unpredictable%0Abehaviors%2C%20threatening%20model%20reliability%20and%20safety.%20Existing%20detection%20methods%0Aoften%20depend%20on%20predefined%20patterns%2C%20limiting%20their%20adaptability%20across%20diverse%0ALLM%20architectures.%20We%20propose%20GlitchMiner%2C%20a%20gradient-based%20discrete%0Aoptimization%20framework%20that%20efficiently%20identifies%20glitch%20tokens%20by%20leveraging%0Aentropy%20to%20quantify%20prediction%20uncertainty%20and%20a%20local%20search%20strategy%20for%0Aexploring%20the%20token%20space.%20Experiments%20across%20multiple%20LLM%20architectures%20show%0Athat%20GlitchMiner%20outperforms%20existing%20methods%20in%20both%20detection%20accuracy%20and%0Aadaptability%2C%20achieving%20over%2010%25%20average%20efficiency%20improvement.%20GlitchMiner%0Aenhances%20vulnerability%20assessment%20in%20LLMs%2C%20contributing%20to%20more%20robust%20and%0Areliable%20applications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wooozihui/GlitchMiner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15052v3&entry.124074799=Read"},
{"title": "NeuroFly: A framework for whole-brain single neuron reconstruction", "author": "Rubin Zhao and Yang Liu and Shiqi Zhang and Zijian Yi and Yanyang Xiao and Fang Xu and Yi Yang and Pencheng Zhou", "abstract": "  Neurons, with their elongated, tree-like dendritic and axonal structures,\nenable efficient signal integration and long-range communication across brain\nregions. By reconstructing individual neurons' morphology, we can gain valuable\ninsights into brain connectivity, revealing the structure basis of cognition,\nmovement, and perception. Despite the accumulation of extensive 3D microscopic\nimaging data, progress has been considerably hindered by the absence of\nautomated tools to streamline this process. Here we introduce NeuroFly, a\nvalidated framework for large-scale automatic single neuron reconstruction.\nThis framework breaks down the process into three distinct stages:\nsegmentation, connection, and proofreading. In the segmentation stage, we\nperform automatic segmentation followed by skeletonization to generate\nover-segmented neuronal fragments without branches. During the connection\nstage, we use a 3D image-based path following approach to extend each fragment\nand connect it with other fragments of the same neuron. Finally, human\nannotators are required only to proofread the few unresolved positions. The\nfirst two stages of our process are clearly defined computer vision problems,\nand we have trained robust baseline models to solve them. We validated\nNeuroFly's efficiency using in-house datasets that include a variety of\nchallenging scenarios, such as dense arborizations, weak axons, images with\ncontamination. We will release the datasets along with a suite of visualization\nand annotation tools for better reproducibility. Our goal is to foster\ncollaboration among researchers to address the neuron reconstruction challenge,\nultimately accelerating advancements in neuroscience research. The dataset and\ncode are available at https://github.com/beanli161514/neurofly\n", "link": "http://arxiv.org/abs/2411.04715v1", "date": "2024-11-07", "relevancy": 2.4366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroFly%3A%20A%20framework%20for%20whole-brain%20single%20neuron%20reconstruction&body=Title%3A%20NeuroFly%3A%20A%20framework%20for%20whole-brain%20single%20neuron%20reconstruction%0AAuthor%3A%20Rubin%20Zhao%20and%20Yang%20Liu%20and%20Shiqi%20Zhang%20and%20Zijian%20Yi%20and%20Yanyang%20Xiao%20and%20Fang%20Xu%20and%20Yi%20Yang%20and%20Pencheng%20Zhou%0AAbstract%3A%20%20%20Neurons%2C%20with%20their%20elongated%2C%20tree-like%20dendritic%20and%20axonal%20structures%2C%0Aenable%20efficient%20signal%20integration%20and%20long-range%20communication%20across%20brain%0Aregions.%20By%20reconstructing%20individual%20neurons%27%20morphology%2C%20we%20can%20gain%20valuable%0Ainsights%20into%20brain%20connectivity%2C%20revealing%20the%20structure%20basis%20of%20cognition%2C%0Amovement%2C%20and%20perception.%20Despite%20the%20accumulation%20of%20extensive%203D%20microscopic%0Aimaging%20data%2C%20progress%20has%20been%20considerably%20hindered%20by%20the%20absence%20of%0Aautomated%20tools%20to%20streamline%20this%20process.%20Here%20we%20introduce%20NeuroFly%2C%20a%0Avalidated%20framework%20for%20large-scale%20automatic%20single%20neuron%20reconstruction.%0AThis%20framework%20breaks%20down%20the%20process%20into%20three%20distinct%20stages%3A%0Asegmentation%2C%20connection%2C%20and%20proofreading.%20In%20the%20segmentation%20stage%2C%20we%0Aperform%20automatic%20segmentation%20followed%20by%20skeletonization%20to%20generate%0Aover-segmented%20neuronal%20fragments%20without%20branches.%20During%20the%20connection%0Astage%2C%20we%20use%20a%203D%20image-based%20path%20following%20approach%20to%20extend%20each%20fragment%0Aand%20connect%20it%20with%20other%20fragments%20of%20the%20same%20neuron.%20Finally%2C%20human%0Aannotators%20are%20required%20only%20to%20proofread%20the%20few%20unresolved%20positions.%20The%0Afirst%20two%20stages%20of%20our%20process%20are%20clearly%20defined%20computer%20vision%20problems%2C%0Aand%20we%20have%20trained%20robust%20baseline%20models%20to%20solve%20them.%20We%20validated%0ANeuroFly%27s%20efficiency%20using%20in-house%20datasets%20that%20include%20a%20variety%20of%0Achallenging%20scenarios%2C%20such%20as%20dense%20arborizations%2C%20weak%20axons%2C%20images%20with%0Acontamination.%20We%20will%20release%20the%20datasets%20along%20with%20a%20suite%20of%20visualization%0Aand%20annotation%20tools%20for%20better%20reproducibility.%20Our%20goal%20is%20to%20foster%0Acollaboration%20among%20researchers%20to%20address%20the%20neuron%20reconstruction%20challenge%2C%0Aultimately%20accelerating%20advancements%20in%20neuroscience%20research.%20The%20dataset%20and%0Acode%20are%20available%20at%20https%3A//github.com/beanli161514/neurofly%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroFly%253A%2520A%2520framework%2520for%2520whole-brain%2520single%2520neuron%2520reconstruction%26entry.906535625%3DRubin%2520Zhao%2520and%2520Yang%2520Liu%2520and%2520Shiqi%2520Zhang%2520and%2520Zijian%2520Yi%2520and%2520Yanyang%2520Xiao%2520and%2520Fang%2520Xu%2520and%2520Yi%2520Yang%2520and%2520Pencheng%2520Zhou%26entry.1292438233%3D%2520%2520Neurons%252C%2520with%2520their%2520elongated%252C%2520tree-like%2520dendritic%2520and%2520axonal%2520structures%252C%250Aenable%2520efficient%2520signal%2520integration%2520and%2520long-range%2520communication%2520across%2520brain%250Aregions.%2520By%2520reconstructing%2520individual%2520neurons%2527%2520morphology%252C%2520we%2520can%2520gain%2520valuable%250Ainsights%2520into%2520brain%2520connectivity%252C%2520revealing%2520the%2520structure%2520basis%2520of%2520cognition%252C%250Amovement%252C%2520and%2520perception.%2520Despite%2520the%2520accumulation%2520of%2520extensive%25203D%2520microscopic%250Aimaging%2520data%252C%2520progress%2520has%2520been%2520considerably%2520hindered%2520by%2520the%2520absence%2520of%250Aautomated%2520tools%2520to%2520streamline%2520this%2520process.%2520Here%2520we%2520introduce%2520NeuroFly%252C%2520a%250Avalidated%2520framework%2520for%2520large-scale%2520automatic%2520single%2520neuron%2520reconstruction.%250AThis%2520framework%2520breaks%2520down%2520the%2520process%2520into%2520three%2520distinct%2520stages%253A%250Asegmentation%252C%2520connection%252C%2520and%2520proofreading.%2520In%2520the%2520segmentation%2520stage%252C%2520we%250Aperform%2520automatic%2520segmentation%2520followed%2520by%2520skeletonization%2520to%2520generate%250Aover-segmented%2520neuronal%2520fragments%2520without%2520branches.%2520During%2520the%2520connection%250Astage%252C%2520we%2520use%2520a%25203D%2520image-based%2520path%2520following%2520approach%2520to%2520extend%2520each%2520fragment%250Aand%2520connect%2520it%2520with%2520other%2520fragments%2520of%2520the%2520same%2520neuron.%2520Finally%252C%2520human%250Aannotators%2520are%2520required%2520only%2520to%2520proofread%2520the%2520few%2520unresolved%2520positions.%2520The%250Afirst%2520two%2520stages%2520of%2520our%2520process%2520are%2520clearly%2520defined%2520computer%2520vision%2520problems%252C%250Aand%2520we%2520have%2520trained%2520robust%2520baseline%2520models%2520to%2520solve%2520them.%2520We%2520validated%250ANeuroFly%2527s%2520efficiency%2520using%2520in-house%2520datasets%2520that%2520include%2520a%2520variety%2520of%250Achallenging%2520scenarios%252C%2520such%2520as%2520dense%2520arborizations%252C%2520weak%2520axons%252C%2520images%2520with%250Acontamination.%2520We%2520will%2520release%2520the%2520datasets%2520along%2520with%2520a%2520suite%2520of%2520visualization%250Aand%2520annotation%2520tools%2520for%2520better%2520reproducibility.%2520Our%2520goal%2520is%2520to%2520foster%250Acollaboration%2520among%2520researchers%2520to%2520address%2520the%2520neuron%2520reconstruction%2520challenge%252C%250Aultimately%2520accelerating%2520advancements%2520in%2520neuroscience%2520research.%2520The%2520dataset%2520and%250Acode%2520are%2520available%2520at%2520https%253A//github.com/beanli161514/neurofly%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroFly%3A%20A%20framework%20for%20whole-brain%20single%20neuron%20reconstruction&entry.906535625=Rubin%20Zhao%20and%20Yang%20Liu%20and%20Shiqi%20Zhang%20and%20Zijian%20Yi%20and%20Yanyang%20Xiao%20and%20Fang%20Xu%20and%20Yi%20Yang%20and%20Pencheng%20Zhou&entry.1292438233=%20%20Neurons%2C%20with%20their%20elongated%2C%20tree-like%20dendritic%20and%20axonal%20structures%2C%0Aenable%20efficient%20signal%20integration%20and%20long-range%20communication%20across%20brain%0Aregions.%20By%20reconstructing%20individual%20neurons%27%20morphology%2C%20we%20can%20gain%20valuable%0Ainsights%20into%20brain%20connectivity%2C%20revealing%20the%20structure%20basis%20of%20cognition%2C%0Amovement%2C%20and%20perception.%20Despite%20the%20accumulation%20of%20extensive%203D%20microscopic%0Aimaging%20data%2C%20progress%20has%20been%20considerably%20hindered%20by%20the%20absence%20of%0Aautomated%20tools%20to%20streamline%20this%20process.%20Here%20we%20introduce%20NeuroFly%2C%20a%0Avalidated%20framework%20for%20large-scale%20automatic%20single%20neuron%20reconstruction.%0AThis%20framework%20breaks%20down%20the%20process%20into%20three%20distinct%20stages%3A%0Asegmentation%2C%20connection%2C%20and%20proofreading.%20In%20the%20segmentation%20stage%2C%20we%0Aperform%20automatic%20segmentation%20followed%20by%20skeletonization%20to%20generate%0Aover-segmented%20neuronal%20fragments%20without%20branches.%20During%20the%20connection%0Astage%2C%20we%20use%20a%203D%20image-based%20path%20following%20approach%20to%20extend%20each%20fragment%0Aand%20connect%20it%20with%20other%20fragments%20of%20the%20same%20neuron.%20Finally%2C%20human%0Aannotators%20are%20required%20only%20to%20proofread%20the%20few%20unresolved%20positions.%20The%0Afirst%20two%20stages%20of%20our%20process%20are%20clearly%20defined%20computer%20vision%20problems%2C%0Aand%20we%20have%20trained%20robust%20baseline%20models%20to%20solve%20them.%20We%20validated%0ANeuroFly%27s%20efficiency%20using%20in-house%20datasets%20that%20include%20a%20variety%20of%0Achallenging%20scenarios%2C%20such%20as%20dense%20arborizations%2C%20weak%20axons%2C%20images%20with%0Acontamination.%20We%20will%20release%20the%20datasets%20along%20with%20a%20suite%20of%20visualization%0Aand%20annotation%20tools%20for%20better%20reproducibility.%20Our%20goal%20is%20to%20foster%0Acollaboration%20among%20researchers%20to%20address%20the%20neuron%20reconstruction%20challenge%2C%0Aultimately%20accelerating%20advancements%20in%20neuroscience%20research.%20The%20dataset%20and%0Acode%20are%20available%20at%20https%3A//github.com/beanli161514/neurofly%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04715v1&entry.124074799=Read"},
{"title": "D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic\n  Scenes", "author": "Siyu Chen and Hong Liu and Wenhao Li and Ying Zhu and Guoquan Wang and Jianbing Wu", "abstract": "  Depth estimation is a crucial technology in robotics. Recently,\nself-supervised depth estimation methods have demonstrated great potential as\nthey can efficiently leverage large amounts of unlabelled real-world data.\nHowever, most existing methods are designed under the assumption of static\nscenes, which hinders their adaptability in dynamic environments. To address\nthis issue, we present D$^3$epth, a novel method for self-supervised depth\nestimation in dynamic scenes. It tackles the challenge of dynamic objects from\ntwo key perspectives. First, within the self-supervised framework, we design a\nreprojection constraint to identify regions likely to contain dynamic objects,\nallowing the construction of a dynamic mask that mitigates their impact at the\nloss level. Second, for multi-frame depth estimation, we introduce a cost\nvolume auto-masking strategy that leverages adjacent frames to identify regions\nassociated with dynamic objects and generate corresponding masks. This provides\nguidance for subsequent processes. Furthermore, we propose a spectral entropy\nuncertainty module that incorporates spectral entropy to guide uncertainty\nestimation during depth fusion, effectively addressing issues arising from cost\nvolume computation in dynamic environments. Extensive experiments on KITTI and\nCityscapes datasets demonstrate that the proposed method consistently\noutperforms existing self-supervised monocular depth estimation baselines. Code\nis available at \\url{https://github.com/Csyunling/D3epth}.\n", "link": "http://arxiv.org/abs/2411.04826v1", "date": "2024-11-07", "relevancy": 2.4117, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6172}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6109}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D%24%5E3%24epth%3A%20Self-Supervised%20Depth%20Estimation%20with%20Dynamic%20Mask%20in%20Dynamic%0A%20%20Scenes&body=Title%3A%20D%24%5E3%24epth%3A%20Self-Supervised%20Depth%20Estimation%20with%20Dynamic%20Mask%20in%20Dynamic%0A%20%20Scenes%0AAuthor%3A%20Siyu%20Chen%20and%20Hong%20Liu%20and%20Wenhao%20Li%20and%20Ying%20Zhu%20and%20Guoquan%20Wang%20and%20Jianbing%20Wu%0AAbstract%3A%20%20%20Depth%20estimation%20is%20a%20crucial%20technology%20in%20robotics.%20Recently%2C%0Aself-supervised%20depth%20estimation%20methods%20have%20demonstrated%20great%20potential%20as%0Athey%20can%20efficiently%20leverage%20large%20amounts%20of%20unlabelled%20real-world%20data.%0AHowever%2C%20most%20existing%20methods%20are%20designed%20under%20the%20assumption%20of%20static%0Ascenes%2C%20which%20hinders%20their%20adaptability%20in%20dynamic%20environments.%20To%20address%0Athis%20issue%2C%20we%20present%20D%24%5E3%24epth%2C%20a%20novel%20method%20for%20self-supervised%20depth%0Aestimation%20in%20dynamic%20scenes.%20It%20tackles%20the%20challenge%20of%20dynamic%20objects%20from%0Atwo%20key%20perspectives.%20First%2C%20within%20the%20self-supervised%20framework%2C%20we%20design%20a%0Areprojection%20constraint%20to%20identify%20regions%20likely%20to%20contain%20dynamic%20objects%2C%0Aallowing%20the%20construction%20of%20a%20dynamic%20mask%20that%20mitigates%20their%20impact%20at%20the%0Aloss%20level.%20Second%2C%20for%20multi-frame%20depth%20estimation%2C%20we%20introduce%20a%20cost%0Avolume%20auto-masking%20strategy%20that%20leverages%20adjacent%20frames%20to%20identify%20regions%0Aassociated%20with%20dynamic%20objects%20and%20generate%20corresponding%20masks.%20This%20provides%0Aguidance%20for%20subsequent%20processes.%20Furthermore%2C%20we%20propose%20a%20spectral%20entropy%0Auncertainty%20module%20that%20incorporates%20spectral%20entropy%20to%20guide%20uncertainty%0Aestimation%20during%20depth%20fusion%2C%20effectively%20addressing%20issues%20arising%20from%20cost%0Avolume%20computation%20in%20dynamic%20environments.%20Extensive%20experiments%20on%20KITTI%20and%0ACityscapes%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%0Aoutperforms%20existing%20self-supervised%20monocular%20depth%20estimation%20baselines.%20Code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/Csyunling/D3epth%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD%2524%255E3%2524epth%253A%2520Self-Supervised%2520Depth%2520Estimation%2520with%2520Dynamic%2520Mask%2520in%2520Dynamic%250A%2520%2520Scenes%26entry.906535625%3DSiyu%2520Chen%2520and%2520Hong%2520Liu%2520and%2520Wenhao%2520Li%2520and%2520Ying%2520Zhu%2520and%2520Guoquan%2520Wang%2520and%2520Jianbing%2520Wu%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520is%2520a%2520crucial%2520technology%2520in%2520robotics.%2520Recently%252C%250Aself-supervised%2520depth%2520estimation%2520methods%2520have%2520demonstrated%2520great%2520potential%2520as%250Athey%2520can%2520efficiently%2520leverage%2520large%2520amounts%2520of%2520unlabelled%2520real-world%2520data.%250AHowever%252C%2520most%2520existing%2520methods%2520are%2520designed%2520under%2520the%2520assumption%2520of%2520static%250Ascenes%252C%2520which%2520hinders%2520their%2520adaptability%2520in%2520dynamic%2520environments.%2520To%2520address%250Athis%2520issue%252C%2520we%2520present%2520D%2524%255E3%2524epth%252C%2520a%2520novel%2520method%2520for%2520self-supervised%2520depth%250Aestimation%2520in%2520dynamic%2520scenes.%2520It%2520tackles%2520the%2520challenge%2520of%2520dynamic%2520objects%2520from%250Atwo%2520key%2520perspectives.%2520First%252C%2520within%2520the%2520self-supervised%2520framework%252C%2520we%2520design%2520a%250Areprojection%2520constraint%2520to%2520identify%2520regions%2520likely%2520to%2520contain%2520dynamic%2520objects%252C%250Aallowing%2520the%2520construction%2520of%2520a%2520dynamic%2520mask%2520that%2520mitigates%2520their%2520impact%2520at%2520the%250Aloss%2520level.%2520Second%252C%2520for%2520multi-frame%2520depth%2520estimation%252C%2520we%2520introduce%2520a%2520cost%250Avolume%2520auto-masking%2520strategy%2520that%2520leverages%2520adjacent%2520frames%2520to%2520identify%2520regions%250Aassociated%2520with%2520dynamic%2520objects%2520and%2520generate%2520corresponding%2520masks.%2520This%2520provides%250Aguidance%2520for%2520subsequent%2520processes.%2520Furthermore%252C%2520we%2520propose%2520a%2520spectral%2520entropy%250Auncertainty%2520module%2520that%2520incorporates%2520spectral%2520entropy%2520to%2520guide%2520uncertainty%250Aestimation%2520during%2520depth%2520fusion%252C%2520effectively%2520addressing%2520issues%2520arising%2520from%2520cost%250Avolume%2520computation%2520in%2520dynamic%2520environments.%2520Extensive%2520experiments%2520on%2520KITTI%2520and%250ACityscapes%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%250Aoutperforms%2520existing%2520self-supervised%2520monocular%2520depth%2520estimation%2520baselines.%2520Code%250Ais%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/Csyunling/D3epth%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%24%5E3%24epth%3A%20Self-Supervised%20Depth%20Estimation%20with%20Dynamic%20Mask%20in%20Dynamic%0A%20%20Scenes&entry.906535625=Siyu%20Chen%20and%20Hong%20Liu%20and%20Wenhao%20Li%20and%20Ying%20Zhu%20and%20Guoquan%20Wang%20and%20Jianbing%20Wu&entry.1292438233=%20%20Depth%20estimation%20is%20a%20crucial%20technology%20in%20robotics.%20Recently%2C%0Aself-supervised%20depth%20estimation%20methods%20have%20demonstrated%20great%20potential%20as%0Athey%20can%20efficiently%20leverage%20large%20amounts%20of%20unlabelled%20real-world%20data.%0AHowever%2C%20most%20existing%20methods%20are%20designed%20under%20the%20assumption%20of%20static%0Ascenes%2C%20which%20hinders%20their%20adaptability%20in%20dynamic%20environments.%20To%20address%0Athis%20issue%2C%20we%20present%20D%24%5E3%24epth%2C%20a%20novel%20method%20for%20self-supervised%20depth%0Aestimation%20in%20dynamic%20scenes.%20It%20tackles%20the%20challenge%20of%20dynamic%20objects%20from%0Atwo%20key%20perspectives.%20First%2C%20within%20the%20self-supervised%20framework%2C%20we%20design%20a%0Areprojection%20constraint%20to%20identify%20regions%20likely%20to%20contain%20dynamic%20objects%2C%0Aallowing%20the%20construction%20of%20a%20dynamic%20mask%20that%20mitigates%20their%20impact%20at%20the%0Aloss%20level.%20Second%2C%20for%20multi-frame%20depth%20estimation%2C%20we%20introduce%20a%20cost%0Avolume%20auto-masking%20strategy%20that%20leverages%20adjacent%20frames%20to%20identify%20regions%0Aassociated%20with%20dynamic%20objects%20and%20generate%20corresponding%20masks.%20This%20provides%0Aguidance%20for%20subsequent%20processes.%20Furthermore%2C%20we%20propose%20a%20spectral%20entropy%0Auncertainty%20module%20that%20incorporates%20spectral%20entropy%20to%20guide%20uncertainty%0Aestimation%20during%20depth%20fusion%2C%20effectively%20addressing%20issues%20arising%20from%20cost%0Avolume%20computation%20in%20dynamic%20environments.%20Extensive%20experiments%20on%20KITTI%20and%0ACityscapes%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%0Aoutperforms%20existing%20self-supervised%20monocular%20depth%20estimation%20baselines.%20Code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/Csyunling/D3epth%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04826v1&entry.124074799=Read"},
{"title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational\n  Agents in XR", "author": "Kadir Burak Buldu and S\u00fcleyman \u00d6zdel and Ka Hei Carrie Lau and Mengdi Wang and Daniel Saad and Sofie Sch\u00f6nborn and Auxane Boch and Enkelejda Kasneci and Efe Bozkir", "abstract": "  Recent developments in computer graphics, machine learning, and sensor\ntechnologies enable numerous opportunities for extended reality (XR) setups for\neveryday life, from skills training to entertainment. With large corporations\noffering consumer-grade head-mounted displays (HMDs) in an affordable way, it\nis likely that XR will become pervasive, and HMDs will develop as personal\ndevices like smartphones and tablets. However, having intelligent spaces and\nnaturalistic interactions in XR is as important as technological advances so\nthat users grow their engagement in virtual and augmented spaces. To this end,\nlarge language model (LLM)--powered non-player characters (NPCs) with\nspeech-to-text (STT) and text-to-speech (TTS) models bring significant\nadvantages over conventional or pre-scripted NPCs for facilitating more natural\nconversational user interfaces (CUIs) in XR. In this paper, we provide the\ncommunity with an open-source, customizable, extensible, and privacy-aware\nUnity package, CUIfy, that facilitates speech-based NPC-user interaction with\nvarious LLMs, STT, and TTS models. Our package also supports multiple\nLLM-powered NPCs per environment and minimizes the latency between different\ncomputational models through streaming to achieve usable interactions between\nusers and NPCs. We publish our source code in the following repository:\nhttps://gitlab.lrz.de/hctl/cuify\n", "link": "http://arxiv.org/abs/2411.04671v1", "date": "2024-11-07", "relevancy": 2.4107, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4828}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CUIfy%20the%20XR%3A%20An%20Open-Source%20Package%20to%20Embed%20LLM-powered%20Conversational%0A%20%20Agents%20in%20XR&body=Title%3A%20CUIfy%20the%20XR%3A%20An%20Open-Source%20Package%20to%20Embed%20LLM-powered%20Conversational%0A%20%20Agents%20in%20XR%0AAuthor%3A%20Kadir%20Burak%20Buldu%20and%20S%C3%BCleyman%20%C3%96zdel%20and%20Ka%20Hei%20Carrie%20Lau%20and%20Mengdi%20Wang%20and%20Daniel%20Saad%20and%20Sofie%20Sch%C3%B6nborn%20and%20Auxane%20Boch%20and%20Enkelejda%20Kasneci%20and%20Efe%20Bozkir%0AAbstract%3A%20%20%20Recent%20developments%20in%20computer%20graphics%2C%20machine%20learning%2C%20and%20sensor%0Atechnologies%20enable%20numerous%20opportunities%20for%20extended%20reality%20%28XR%29%20setups%20for%0Aeveryday%20life%2C%20from%20skills%20training%20to%20entertainment.%20With%20large%20corporations%0Aoffering%20consumer-grade%20head-mounted%20displays%20%28HMDs%29%20in%20an%20affordable%20way%2C%20it%0Ais%20likely%20that%20XR%20will%20become%20pervasive%2C%20and%20HMDs%20will%20develop%20as%20personal%0Adevices%20like%20smartphones%20and%20tablets.%20However%2C%20having%20intelligent%20spaces%20and%0Anaturalistic%20interactions%20in%20XR%20is%20as%20important%20as%20technological%20advances%20so%0Athat%20users%20grow%20their%20engagement%20in%20virtual%20and%20augmented%20spaces.%20To%20this%20end%2C%0Alarge%20language%20model%20%28LLM%29--powered%20non-player%20characters%20%28NPCs%29%20with%0Aspeech-to-text%20%28STT%29%20and%20text-to-speech%20%28TTS%29%20models%20bring%20significant%0Aadvantages%20over%20conventional%20or%20pre-scripted%20NPCs%20for%20facilitating%20more%20natural%0Aconversational%20user%20interfaces%20%28CUIs%29%20in%20XR.%20In%20this%20paper%2C%20we%20provide%20the%0Acommunity%20with%20an%20open-source%2C%20customizable%2C%20extensible%2C%20and%20privacy-aware%0AUnity%20package%2C%20CUIfy%2C%20that%20facilitates%20speech-based%20NPC-user%20interaction%20with%0Avarious%20LLMs%2C%20STT%2C%20and%20TTS%20models.%20Our%20package%20also%20supports%20multiple%0ALLM-powered%20NPCs%20per%20environment%20and%20minimizes%20the%20latency%20between%20different%0Acomputational%20models%20through%20streaming%20to%20achieve%20usable%20interactions%20between%0Ausers%20and%20NPCs.%20We%20publish%20our%20source%20code%20in%20the%20following%20repository%3A%0Ahttps%3A//gitlab.lrz.de/hctl/cuify%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCUIfy%2520the%2520XR%253A%2520An%2520Open-Source%2520Package%2520to%2520Embed%2520LLM-powered%2520Conversational%250A%2520%2520Agents%2520in%2520XR%26entry.906535625%3DKadir%2520Burak%2520Buldu%2520and%2520S%25C3%25BCleyman%2520%25C3%2596zdel%2520and%2520Ka%2520Hei%2520Carrie%2520Lau%2520and%2520Mengdi%2520Wang%2520and%2520Daniel%2520Saad%2520and%2520Sofie%2520Sch%25C3%25B6nborn%2520and%2520Auxane%2520Boch%2520and%2520Enkelejda%2520Kasneci%2520and%2520Efe%2520Bozkir%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520computer%2520graphics%252C%2520machine%2520learning%252C%2520and%2520sensor%250Atechnologies%2520enable%2520numerous%2520opportunities%2520for%2520extended%2520reality%2520%2528XR%2529%2520setups%2520for%250Aeveryday%2520life%252C%2520from%2520skills%2520training%2520to%2520entertainment.%2520With%2520large%2520corporations%250Aoffering%2520consumer-grade%2520head-mounted%2520displays%2520%2528HMDs%2529%2520in%2520an%2520affordable%2520way%252C%2520it%250Ais%2520likely%2520that%2520XR%2520will%2520become%2520pervasive%252C%2520and%2520HMDs%2520will%2520develop%2520as%2520personal%250Adevices%2520like%2520smartphones%2520and%2520tablets.%2520However%252C%2520having%2520intelligent%2520spaces%2520and%250Anaturalistic%2520interactions%2520in%2520XR%2520is%2520as%2520important%2520as%2520technological%2520advances%2520so%250Athat%2520users%2520grow%2520their%2520engagement%2520in%2520virtual%2520and%2520augmented%2520spaces.%2520To%2520this%2520end%252C%250Alarge%2520language%2520model%2520%2528LLM%2529--powered%2520non-player%2520characters%2520%2528NPCs%2529%2520with%250Aspeech-to-text%2520%2528STT%2529%2520and%2520text-to-speech%2520%2528TTS%2529%2520models%2520bring%2520significant%250Aadvantages%2520over%2520conventional%2520or%2520pre-scripted%2520NPCs%2520for%2520facilitating%2520more%2520natural%250Aconversational%2520user%2520interfaces%2520%2528CUIs%2529%2520in%2520XR.%2520In%2520this%2520paper%252C%2520we%2520provide%2520the%250Acommunity%2520with%2520an%2520open-source%252C%2520customizable%252C%2520extensible%252C%2520and%2520privacy-aware%250AUnity%2520package%252C%2520CUIfy%252C%2520that%2520facilitates%2520speech-based%2520NPC-user%2520interaction%2520with%250Avarious%2520LLMs%252C%2520STT%252C%2520and%2520TTS%2520models.%2520Our%2520package%2520also%2520supports%2520multiple%250ALLM-powered%2520NPCs%2520per%2520environment%2520and%2520minimizes%2520the%2520latency%2520between%2520different%250Acomputational%2520models%2520through%2520streaming%2520to%2520achieve%2520usable%2520interactions%2520between%250Ausers%2520and%2520NPCs.%2520We%2520publish%2520our%2520source%2520code%2520in%2520the%2520following%2520repository%253A%250Ahttps%253A//gitlab.lrz.de/hctl/cuify%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CUIfy%20the%20XR%3A%20An%20Open-Source%20Package%20to%20Embed%20LLM-powered%20Conversational%0A%20%20Agents%20in%20XR&entry.906535625=Kadir%20Burak%20Buldu%20and%20S%C3%BCleyman%20%C3%96zdel%20and%20Ka%20Hei%20Carrie%20Lau%20and%20Mengdi%20Wang%20and%20Daniel%20Saad%20and%20Sofie%20Sch%C3%B6nborn%20and%20Auxane%20Boch%20and%20Enkelejda%20Kasneci%20and%20Efe%20Bozkir&entry.1292438233=%20%20Recent%20developments%20in%20computer%20graphics%2C%20machine%20learning%2C%20and%20sensor%0Atechnologies%20enable%20numerous%20opportunities%20for%20extended%20reality%20%28XR%29%20setups%20for%0Aeveryday%20life%2C%20from%20skills%20training%20to%20entertainment.%20With%20large%20corporations%0Aoffering%20consumer-grade%20head-mounted%20displays%20%28HMDs%29%20in%20an%20affordable%20way%2C%20it%0Ais%20likely%20that%20XR%20will%20become%20pervasive%2C%20and%20HMDs%20will%20develop%20as%20personal%0Adevices%20like%20smartphones%20and%20tablets.%20However%2C%20having%20intelligent%20spaces%20and%0Anaturalistic%20interactions%20in%20XR%20is%20as%20important%20as%20technological%20advances%20so%0Athat%20users%20grow%20their%20engagement%20in%20virtual%20and%20augmented%20spaces.%20To%20this%20end%2C%0Alarge%20language%20model%20%28LLM%29--powered%20non-player%20characters%20%28NPCs%29%20with%0Aspeech-to-text%20%28STT%29%20and%20text-to-speech%20%28TTS%29%20models%20bring%20significant%0Aadvantages%20over%20conventional%20or%20pre-scripted%20NPCs%20for%20facilitating%20more%20natural%0Aconversational%20user%20interfaces%20%28CUIs%29%20in%20XR.%20In%20this%20paper%2C%20we%20provide%20the%0Acommunity%20with%20an%20open-source%2C%20customizable%2C%20extensible%2C%20and%20privacy-aware%0AUnity%20package%2C%20CUIfy%2C%20that%20facilitates%20speech-based%20NPC-user%20interaction%20with%0Avarious%20LLMs%2C%20STT%2C%20and%20TTS%20models.%20Our%20package%20also%20supports%20multiple%0ALLM-powered%20NPCs%20per%20environment%20and%20minimizes%20the%20latency%20between%20different%0Acomputational%20models%20through%20streaming%20to%20achieve%20usable%20interactions%20between%0Ausers%20and%20NPCs.%20We%20publish%20our%20source%20code%20in%20the%20following%20repository%3A%0Ahttps%3A//gitlab.lrz.de/hctl/cuify%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04671v1&entry.124074799=Read"},
{"title": "A Reinforcement Learning-Based Automatic Video Editing Method Using\n  Pre-trained Vision-Language Model", "author": "Panwen Hu and Nan Xiao and Feifei Li and Yongquan Chen and Rui Huang", "abstract": "  In this era of videos, automatic video editing techniques attract more and\nmore attention from industry and academia since they can reduce workloads and\nlower the requirements for human editors. Existing automatic editing systems\nare mainly scene- or event-specific, e.g., soccer game broadcasting, yet the\nautomatic systems for general editing, e.g., movie or vlog editing which covers\nvarious scenes and events, were rarely studied before, and converting the\nevent-driven editing method to a general scene is nontrivial. In this paper, we\npropose a two-stage scheme for general editing. Firstly, unlike previous works\nthat extract scene-specific features, we leverage the pre-trained\nVision-Language Model (VLM) to extract the editing-relevant representations as\nediting context. Moreover, to close the gap between the professional-looking\nvideos and the automatic productions generated with simple guidelines, we\npropose a Reinforcement Learning (RL)-based editing framework to formulate the\nediting problem and train the virtual editor to make better sequential editing\ndecisions. Finally, we evaluate the proposed method on a more general editing\ntask with a real movie dataset. Experimental results demonstrate the\neffectiveness and benefits of the proposed context representation and the\nlearning ability of our RL-based editing framework.\n", "link": "http://arxiv.org/abs/2411.04942v1", "date": "2024-11-07", "relevancy": 2.3747, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Reinforcement%20Learning-Based%20Automatic%20Video%20Editing%20Method%20Using%0A%20%20Pre-trained%20Vision-Language%20Model&body=Title%3A%20A%20Reinforcement%20Learning-Based%20Automatic%20Video%20Editing%20Method%20Using%0A%20%20Pre-trained%20Vision-Language%20Model%0AAuthor%3A%20Panwen%20Hu%20and%20Nan%20Xiao%20and%20Feifei%20Li%20and%20Yongquan%20Chen%20and%20Rui%20Huang%0AAbstract%3A%20%20%20In%20this%20era%20of%20videos%2C%20automatic%20video%20editing%20techniques%20attract%20more%20and%0Amore%20attention%20from%20industry%20and%20academia%20since%20they%20can%20reduce%20workloads%20and%0Alower%20the%20requirements%20for%20human%20editors.%20Existing%20automatic%20editing%20systems%0Aare%20mainly%20scene-%20or%20event-specific%2C%20e.g.%2C%20soccer%20game%20broadcasting%2C%20yet%20the%0Aautomatic%20systems%20for%20general%20editing%2C%20e.g.%2C%20movie%20or%20vlog%20editing%20which%20covers%0Avarious%20scenes%20and%20events%2C%20were%20rarely%20studied%20before%2C%20and%20converting%20the%0Aevent-driven%20editing%20method%20to%20a%20general%20scene%20is%20nontrivial.%20In%20this%20paper%2C%20we%0Apropose%20a%20two-stage%20scheme%20for%20general%20editing.%20Firstly%2C%20unlike%20previous%20works%0Athat%20extract%20scene-specific%20features%2C%20we%20leverage%20the%20pre-trained%0AVision-Language%20Model%20%28VLM%29%20to%20extract%20the%20editing-relevant%20representations%20as%0Aediting%20context.%20Moreover%2C%20to%20close%20the%20gap%20between%20the%20professional-looking%0Avideos%20and%20the%20automatic%20productions%20generated%20with%20simple%20guidelines%2C%20we%0Apropose%20a%20Reinforcement%20Learning%20%28RL%29-based%20editing%20framework%20to%20formulate%20the%0Aediting%20problem%20and%20train%20the%20virtual%20editor%20to%20make%20better%20sequential%20editing%0Adecisions.%20Finally%2C%20we%20evaluate%20the%20proposed%20method%20on%20a%20more%20general%20editing%0Atask%20with%20a%20real%20movie%20dataset.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20and%20benefits%20of%20the%20proposed%20context%20representation%20and%20the%0Alearning%20ability%20of%20our%20RL-based%20editing%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Reinforcement%2520Learning-Based%2520Automatic%2520Video%2520Editing%2520Method%2520Using%250A%2520%2520Pre-trained%2520Vision-Language%2520Model%26entry.906535625%3DPanwen%2520Hu%2520and%2520Nan%2520Xiao%2520and%2520Feifei%2520Li%2520and%2520Yongquan%2520Chen%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520era%2520of%2520videos%252C%2520automatic%2520video%2520editing%2520techniques%2520attract%2520more%2520and%250Amore%2520attention%2520from%2520industry%2520and%2520academia%2520since%2520they%2520can%2520reduce%2520workloads%2520and%250Alower%2520the%2520requirements%2520for%2520human%2520editors.%2520Existing%2520automatic%2520editing%2520systems%250Aare%2520mainly%2520scene-%2520or%2520event-specific%252C%2520e.g.%252C%2520soccer%2520game%2520broadcasting%252C%2520yet%2520the%250Aautomatic%2520systems%2520for%2520general%2520editing%252C%2520e.g.%252C%2520movie%2520or%2520vlog%2520editing%2520which%2520covers%250Avarious%2520scenes%2520and%2520events%252C%2520were%2520rarely%2520studied%2520before%252C%2520and%2520converting%2520the%250Aevent-driven%2520editing%2520method%2520to%2520a%2520general%2520scene%2520is%2520nontrivial.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520two-stage%2520scheme%2520for%2520general%2520editing.%2520Firstly%252C%2520unlike%2520previous%2520works%250Athat%2520extract%2520scene-specific%2520features%252C%2520we%2520leverage%2520the%2520pre-trained%250AVision-Language%2520Model%2520%2528VLM%2529%2520to%2520extract%2520the%2520editing-relevant%2520representations%2520as%250Aediting%2520context.%2520Moreover%252C%2520to%2520close%2520the%2520gap%2520between%2520the%2520professional-looking%250Avideos%2520and%2520the%2520automatic%2520productions%2520generated%2520with%2520simple%2520guidelines%252C%2520we%250Apropose%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529-based%2520editing%2520framework%2520to%2520formulate%2520the%250Aediting%2520problem%2520and%2520train%2520the%2520virtual%2520editor%2520to%2520make%2520better%2520sequential%2520editing%250Adecisions.%2520Finally%252C%2520we%2520evaluate%2520the%2520proposed%2520method%2520on%2520a%2520more%2520general%2520editing%250Atask%2520with%2520a%2520real%2520movie%2520dataset.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520and%2520benefits%2520of%2520the%2520proposed%2520context%2520representation%2520and%2520the%250Alearning%2520ability%2520of%2520our%2520RL-based%2520editing%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Reinforcement%20Learning-Based%20Automatic%20Video%20Editing%20Method%20Using%0A%20%20Pre-trained%20Vision-Language%20Model&entry.906535625=Panwen%20Hu%20and%20Nan%20Xiao%20and%20Feifei%20Li%20and%20Yongquan%20Chen%20and%20Rui%20Huang&entry.1292438233=%20%20In%20this%20era%20of%20videos%2C%20automatic%20video%20editing%20techniques%20attract%20more%20and%0Amore%20attention%20from%20industry%20and%20academia%20since%20they%20can%20reduce%20workloads%20and%0Alower%20the%20requirements%20for%20human%20editors.%20Existing%20automatic%20editing%20systems%0Aare%20mainly%20scene-%20or%20event-specific%2C%20e.g.%2C%20soccer%20game%20broadcasting%2C%20yet%20the%0Aautomatic%20systems%20for%20general%20editing%2C%20e.g.%2C%20movie%20or%20vlog%20editing%20which%20covers%0Avarious%20scenes%20and%20events%2C%20were%20rarely%20studied%20before%2C%20and%20converting%20the%0Aevent-driven%20editing%20method%20to%20a%20general%20scene%20is%20nontrivial.%20In%20this%20paper%2C%20we%0Apropose%20a%20two-stage%20scheme%20for%20general%20editing.%20Firstly%2C%20unlike%20previous%20works%0Athat%20extract%20scene-specific%20features%2C%20we%20leverage%20the%20pre-trained%0AVision-Language%20Model%20%28VLM%29%20to%20extract%20the%20editing-relevant%20representations%20as%0Aediting%20context.%20Moreover%2C%20to%20close%20the%20gap%20between%20the%20professional-looking%0Avideos%20and%20the%20automatic%20productions%20generated%20with%20simple%20guidelines%2C%20we%0Apropose%20a%20Reinforcement%20Learning%20%28RL%29-based%20editing%20framework%20to%20formulate%20the%0Aediting%20problem%20and%20train%20the%20virtual%20editor%20to%20make%20better%20sequential%20editing%0Adecisions.%20Finally%2C%20we%20evaluate%20the%20proposed%20method%20on%20a%20more%20general%20editing%0Atask%20with%20a%20real%20movie%20dataset.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20and%20benefits%20of%20the%20proposed%20context%20representation%20and%20the%0Alearning%20ability%20of%20our%20RL-based%20editing%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04942v1&entry.124074799=Read"},
{"title": "Higher-Order GNNs Meet Efficiency: Sparse Sobolev Graph Neural Networks", "author": "Jhony H. Giraldo and Aref Einizade and Andjela Todorovic and Jhon A. Castro-Correa and Mohsen Badiey and Thierry Bouwmans and Fragkiskos D. Malliaros", "abstract": "  Graph Neural Networks (GNNs) have shown great promise in modeling\nrelationships between nodes in a graph, but capturing higher-order\nrelationships remains a challenge for large-scale networks. Previous studies\nhave primarily attempted to utilize the information from higher-order neighbors\nin the graph, involving the incorporation of powers of the shift operator, such\nas the graph Laplacian or adjacency matrix. This approach comes with a\ntrade-off in terms of increased computational and memory demands. Relying on\ngraph spectral theory, we make a fundamental observation: the regular and the\nHadamard power of the Laplacian matrix behave similarly in the spectrum. This\nobservation has significant implications for capturing higher-order information\nin GNNs for various tasks such as node classification and semi-supervised\nlearning. Consequently, we propose a novel graph convolutional operator based\non the sparse Sobolev norm of graph signals. Our approach, known as Sparse\nSobolev GNN (S2-GNN), employs Hadamard products between matrices to maintain\nthe sparsity level in graph representations. S2-GNN utilizes a cascade of\nfilters with increasing Hadamard powers to generate a diverse set of functions.\nWe theoretically analyze the stability of S2-GNN to show the robustness of the\nmodel against possible graph perturbations. We also conduct a comprehensive\nevaluation of S2-GNN across various graph mining, semi-supervised node\nclassification, and computer vision tasks. In particular use cases, our\nalgorithm demonstrates competitive performance compared to state-of-the-art\nGNNs in terms of performance and running time.\n", "link": "http://arxiv.org/abs/2411.04570v1", "date": "2024-11-07", "relevancy": 2.3724, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20GNNs%20Meet%20Efficiency%3A%20Sparse%20Sobolev%20Graph%20Neural%20Networks&body=Title%3A%20Higher-Order%20GNNs%20Meet%20Efficiency%3A%20Sparse%20Sobolev%20Graph%20Neural%20Networks%0AAuthor%3A%20Jhony%20H.%20Giraldo%20and%20Aref%20Einizade%20and%20Andjela%20Todorovic%20and%20Jhon%20A.%20Castro-Correa%20and%20Mohsen%20Badiey%20and%20Thierry%20Bouwmans%20and%20Fragkiskos%20D.%20Malliaros%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20modeling%0Arelationships%20between%20nodes%20in%20a%20graph%2C%20but%20capturing%20higher-order%0Arelationships%20remains%20a%20challenge%20for%20large-scale%20networks.%20Previous%20studies%0Ahave%20primarily%20attempted%20to%20utilize%20the%20information%20from%20higher-order%20neighbors%0Ain%20the%20graph%2C%20involving%20the%20incorporation%20of%20powers%20of%20the%20shift%20operator%2C%20such%0Aas%20the%20graph%20Laplacian%20or%20adjacency%20matrix.%20This%20approach%20comes%20with%20a%0Atrade-off%20in%20terms%20of%20increased%20computational%20and%20memory%20demands.%20Relying%20on%0Agraph%20spectral%20theory%2C%20we%20make%20a%20fundamental%20observation%3A%20the%20regular%20and%20the%0AHadamard%20power%20of%20the%20Laplacian%20matrix%20behave%20similarly%20in%20the%20spectrum.%20This%0Aobservation%20has%20significant%20implications%20for%20capturing%20higher-order%20information%0Ain%20GNNs%20for%20various%20tasks%20such%20as%20node%20classification%20and%20semi-supervised%0Alearning.%20Consequently%2C%20we%20propose%20a%20novel%20graph%20convolutional%20operator%20based%0Aon%20the%20sparse%20Sobolev%20norm%20of%20graph%20signals.%20Our%20approach%2C%20known%20as%20Sparse%0ASobolev%20GNN%20%28S2-GNN%29%2C%20employs%20Hadamard%20products%20between%20matrices%20to%20maintain%0Athe%20sparsity%20level%20in%20graph%20representations.%20S2-GNN%20utilizes%20a%20cascade%20of%0Afilters%20with%20increasing%20Hadamard%20powers%20to%20generate%20a%20diverse%20set%20of%20functions.%0AWe%20theoretically%20analyze%20the%20stability%20of%20S2-GNN%20to%20show%20the%20robustness%20of%20the%0Amodel%20against%20possible%20graph%20perturbations.%20We%20also%20conduct%20a%20comprehensive%0Aevaluation%20of%20S2-GNN%20across%20various%20graph%20mining%2C%20semi-supervised%20node%0Aclassification%2C%20and%20computer%20vision%20tasks.%20In%20particular%20use%20cases%2C%20our%0Aalgorithm%20demonstrates%20competitive%20performance%20compared%20to%20state-of-the-art%0AGNNs%20in%20terms%20of%20performance%20and%20running%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520GNNs%2520Meet%2520Efficiency%253A%2520Sparse%2520Sobolev%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJhony%2520H.%2520Giraldo%2520and%2520Aref%2520Einizade%2520and%2520Andjela%2520Todorovic%2520and%2520Jhon%2520A.%2520Castro-Correa%2520and%2520Mohsen%2520Badiey%2520and%2520Thierry%2520Bouwmans%2520and%2520Fragkiskos%2520D.%2520Malliaros%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520great%2520promise%2520in%2520modeling%250Arelationships%2520between%2520nodes%2520in%2520a%2520graph%252C%2520but%2520capturing%2520higher-order%250Arelationships%2520remains%2520a%2520challenge%2520for%2520large-scale%2520networks.%2520Previous%2520studies%250Ahave%2520primarily%2520attempted%2520to%2520utilize%2520the%2520information%2520from%2520higher-order%2520neighbors%250Ain%2520the%2520graph%252C%2520involving%2520the%2520incorporation%2520of%2520powers%2520of%2520the%2520shift%2520operator%252C%2520such%250Aas%2520the%2520graph%2520Laplacian%2520or%2520adjacency%2520matrix.%2520This%2520approach%2520comes%2520with%2520a%250Atrade-off%2520in%2520terms%2520of%2520increased%2520computational%2520and%2520memory%2520demands.%2520Relying%2520on%250Agraph%2520spectral%2520theory%252C%2520we%2520make%2520a%2520fundamental%2520observation%253A%2520the%2520regular%2520and%2520the%250AHadamard%2520power%2520of%2520the%2520Laplacian%2520matrix%2520behave%2520similarly%2520in%2520the%2520spectrum.%2520This%250Aobservation%2520has%2520significant%2520implications%2520for%2520capturing%2520higher-order%2520information%250Ain%2520GNNs%2520for%2520various%2520tasks%2520such%2520as%2520node%2520classification%2520and%2520semi-supervised%250Alearning.%2520Consequently%252C%2520we%2520propose%2520a%2520novel%2520graph%2520convolutional%2520operator%2520based%250Aon%2520the%2520sparse%2520Sobolev%2520norm%2520of%2520graph%2520signals.%2520Our%2520approach%252C%2520known%2520as%2520Sparse%250ASobolev%2520GNN%2520%2528S2-GNN%2529%252C%2520employs%2520Hadamard%2520products%2520between%2520matrices%2520to%2520maintain%250Athe%2520sparsity%2520level%2520in%2520graph%2520representations.%2520S2-GNN%2520utilizes%2520a%2520cascade%2520of%250Afilters%2520with%2520increasing%2520Hadamard%2520powers%2520to%2520generate%2520a%2520diverse%2520set%2520of%2520functions.%250AWe%2520theoretically%2520analyze%2520the%2520stability%2520of%2520S2-GNN%2520to%2520show%2520the%2520robustness%2520of%2520the%250Amodel%2520against%2520possible%2520graph%2520perturbations.%2520We%2520also%2520conduct%2520a%2520comprehensive%250Aevaluation%2520of%2520S2-GNN%2520across%2520various%2520graph%2520mining%252C%2520semi-supervised%2520node%250Aclassification%252C%2520and%2520computer%2520vision%2520tasks.%2520In%2520particular%2520use%2520cases%252C%2520our%250Aalgorithm%2520demonstrates%2520competitive%2520performance%2520compared%2520to%2520state-of-the-art%250AGNNs%2520in%2520terms%2520of%2520performance%2520and%2520running%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20GNNs%20Meet%20Efficiency%3A%20Sparse%20Sobolev%20Graph%20Neural%20Networks&entry.906535625=Jhony%20H.%20Giraldo%20and%20Aref%20Einizade%20and%20Andjela%20Todorovic%20and%20Jhon%20A.%20Castro-Correa%20and%20Mohsen%20Badiey%20and%20Thierry%20Bouwmans%20and%20Fragkiskos%20D.%20Malliaros&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20modeling%0Arelationships%20between%20nodes%20in%20a%20graph%2C%20but%20capturing%20higher-order%0Arelationships%20remains%20a%20challenge%20for%20large-scale%20networks.%20Previous%20studies%0Ahave%20primarily%20attempted%20to%20utilize%20the%20information%20from%20higher-order%20neighbors%0Ain%20the%20graph%2C%20involving%20the%20incorporation%20of%20powers%20of%20the%20shift%20operator%2C%20such%0Aas%20the%20graph%20Laplacian%20or%20adjacency%20matrix.%20This%20approach%20comes%20with%20a%0Atrade-off%20in%20terms%20of%20increased%20computational%20and%20memory%20demands.%20Relying%20on%0Agraph%20spectral%20theory%2C%20we%20make%20a%20fundamental%20observation%3A%20the%20regular%20and%20the%0AHadamard%20power%20of%20the%20Laplacian%20matrix%20behave%20similarly%20in%20the%20spectrum.%20This%0Aobservation%20has%20significant%20implications%20for%20capturing%20higher-order%20information%0Ain%20GNNs%20for%20various%20tasks%20such%20as%20node%20classification%20and%20semi-supervised%0Alearning.%20Consequently%2C%20we%20propose%20a%20novel%20graph%20convolutional%20operator%20based%0Aon%20the%20sparse%20Sobolev%20norm%20of%20graph%20signals.%20Our%20approach%2C%20known%20as%20Sparse%0ASobolev%20GNN%20%28S2-GNN%29%2C%20employs%20Hadamard%20products%20between%20matrices%20to%20maintain%0Athe%20sparsity%20level%20in%20graph%20representations.%20S2-GNN%20utilizes%20a%20cascade%20of%0Afilters%20with%20increasing%20Hadamard%20powers%20to%20generate%20a%20diverse%20set%20of%20functions.%0AWe%20theoretically%20analyze%20the%20stability%20of%20S2-GNN%20to%20show%20the%20robustness%20of%20the%0Amodel%20against%20possible%20graph%20perturbations.%20We%20also%20conduct%20a%20comprehensive%0Aevaluation%20of%20S2-GNN%20across%20various%20graph%20mining%2C%20semi-supervised%20node%0Aclassification%2C%20and%20computer%20vision%20tasks.%20In%20particular%20use%20cases%2C%20our%0Aalgorithm%20demonstrates%20competitive%20performance%20compared%20to%20state-of-the-art%0AGNNs%20in%20terms%20of%20performance%20and%20running%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04570v1&entry.124074799=Read"},
{"title": "VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in\n  Videos", "author": "Shehan Munasinghe and Hanan Gani and Wenqi Zhu and Jiale Cao and Eric Xing and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Fine-grained alignment between videos and text is challenging due to complex\nspatial and temporal dynamics in videos. Existing video-based Large Multimodal\nModels (LMMs) handle basic conversations but struggle with precise pixel-level\ngrounding in videos. To address this, we introduce VideoGLaMM, a LMM designed\nfor fine-grained pixel-level grounding in videos based on user-provided textual\ninputs. Our design seamlessly connects three key components: a Large Language\nModel, a dual vision encoder that emphasizes both spatial and temporal details,\nand a spatio-temporal decoder for accurate mask generation. This connection is\nfacilitated via tunable V-L and L-V adapters that enable close Vision-Language\n(VL) alignment. The architecture is trained to synchronize both spatial and\ntemporal elements of video content with textual instructions. To enable\nfine-grained grounding, we curate a multimodal dataset featuring detailed\nvisually-grounded conversations using a semiautomatic annotation pipeline,\nresulting in a diverse set of 38k video-QA triplets along with 83k objects and\n671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded\nConversation Generation, Visual Grounding, and Referring Video Segmentation.\nExperimental results show that our model consistently outperforms existing\napproaches across all three tasks.\n", "link": "http://arxiv.org/abs/2411.04923v1", "date": "2024-11-07", "relevancy": 2.3718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5813}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGLaMM%3A%20A%20Large%20Multimodal%20Model%20for%20Pixel-Level%20Visual%20Grounding%20in%0A%20%20Videos&body=Title%3A%20VideoGLaMM%3A%20A%20Large%20Multimodal%20Model%20for%20Pixel-Level%20Visual%20Grounding%20in%0A%20%20Videos%0AAuthor%3A%20Shehan%20Munasinghe%20and%20Hanan%20Gani%20and%20Wenqi%20Zhu%20and%20Jiale%20Cao%20and%20Eric%20Xing%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Fine-grained%20alignment%20between%20videos%20and%20text%20is%20challenging%20due%20to%20complex%0Aspatial%20and%20temporal%20dynamics%20in%20videos.%20Existing%20video-based%20Large%20Multimodal%0AModels%20%28LMMs%29%20handle%20basic%20conversations%20but%20struggle%20with%20precise%20pixel-level%0Agrounding%20in%20videos.%20To%20address%20this%2C%20we%20introduce%20VideoGLaMM%2C%20a%20LMM%20designed%0Afor%20fine-grained%20pixel-level%20grounding%20in%20videos%20based%20on%20user-provided%20textual%0Ainputs.%20Our%20design%20seamlessly%20connects%20three%20key%20components%3A%20a%20Large%20Language%0AModel%2C%20a%20dual%20vision%20encoder%20that%20emphasizes%20both%20spatial%20and%20temporal%20details%2C%0Aand%20a%20spatio-temporal%20decoder%20for%20accurate%20mask%20generation.%20This%20connection%20is%0Afacilitated%20via%20tunable%20V-L%20and%20L-V%20adapters%20that%20enable%20close%20Vision-Language%0A%28VL%29%20alignment.%20The%20architecture%20is%20trained%20to%20synchronize%20both%20spatial%20and%0Atemporal%20elements%20of%20video%20content%20with%20textual%20instructions.%20To%20enable%0Afine-grained%20grounding%2C%20we%20curate%20a%20multimodal%20dataset%20featuring%20detailed%0Avisually-grounded%20conversations%20using%20a%20semiautomatic%20annotation%20pipeline%2C%0Aresulting%20in%20a%20diverse%20set%20of%2038k%20video-QA%20triplets%20along%20with%2083k%20objects%20and%0A671k%20masks.%20We%20evaluate%20VideoGLaMM%20on%20three%20challenging%20tasks%3A%20Grounded%0AConversation%20Generation%2C%20Visual%20Grounding%2C%20and%20Referring%20Video%20Segmentation.%0AExperimental%20results%20show%20that%20our%20model%20consistently%20outperforms%20existing%0Aapproaches%20across%20all%20three%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGLaMM%253A%2520A%2520Large%2520Multimodal%2520Model%2520for%2520Pixel-Level%2520Visual%2520Grounding%2520in%250A%2520%2520Videos%26entry.906535625%3DShehan%2520Munasinghe%2520and%2520Hanan%2520Gani%2520and%2520Wenqi%2520Zhu%2520and%2520Jiale%2520Cao%2520and%2520Eric%2520Xing%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Fine-grained%2520alignment%2520between%2520videos%2520and%2520text%2520is%2520challenging%2520due%2520to%2520complex%250Aspatial%2520and%2520temporal%2520dynamics%2520in%2520videos.%2520Existing%2520video-based%2520Large%2520Multimodal%250AModels%2520%2528LMMs%2529%2520handle%2520basic%2520conversations%2520but%2520struggle%2520with%2520precise%2520pixel-level%250Agrounding%2520in%2520videos.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VideoGLaMM%252C%2520a%2520LMM%2520designed%250Afor%2520fine-grained%2520pixel-level%2520grounding%2520in%2520videos%2520based%2520on%2520user-provided%2520textual%250Ainputs.%2520Our%2520design%2520seamlessly%2520connects%2520three%2520key%2520components%253A%2520a%2520Large%2520Language%250AModel%252C%2520a%2520dual%2520vision%2520encoder%2520that%2520emphasizes%2520both%2520spatial%2520and%2520temporal%2520details%252C%250Aand%2520a%2520spatio-temporal%2520decoder%2520for%2520accurate%2520mask%2520generation.%2520This%2520connection%2520is%250Afacilitated%2520via%2520tunable%2520V-L%2520and%2520L-V%2520adapters%2520that%2520enable%2520close%2520Vision-Language%250A%2528VL%2529%2520alignment.%2520The%2520architecture%2520is%2520trained%2520to%2520synchronize%2520both%2520spatial%2520and%250Atemporal%2520elements%2520of%2520video%2520content%2520with%2520textual%2520instructions.%2520To%2520enable%250Afine-grained%2520grounding%252C%2520we%2520curate%2520a%2520multimodal%2520dataset%2520featuring%2520detailed%250Avisually-grounded%2520conversations%2520using%2520a%2520semiautomatic%2520annotation%2520pipeline%252C%250Aresulting%2520in%2520a%2520diverse%2520set%2520of%252038k%2520video-QA%2520triplets%2520along%2520with%252083k%2520objects%2520and%250A671k%2520masks.%2520We%2520evaluate%2520VideoGLaMM%2520on%2520three%2520challenging%2520tasks%253A%2520Grounded%250AConversation%2520Generation%252C%2520Visual%2520Grounding%252C%2520and%2520Referring%2520Video%2520Segmentation.%250AExperimental%2520results%2520show%2520that%2520our%2520model%2520consistently%2520outperforms%2520existing%250Aapproaches%2520across%2520all%2520three%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGLaMM%3A%20A%20Large%20Multimodal%20Model%20for%20Pixel-Level%20Visual%20Grounding%20in%0A%20%20Videos&entry.906535625=Shehan%20Munasinghe%20and%20Hanan%20Gani%20and%20Wenqi%20Zhu%20and%20Jiale%20Cao%20and%20Eric%20Xing%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Fine-grained%20alignment%20between%20videos%20and%20text%20is%20challenging%20due%20to%20complex%0Aspatial%20and%20temporal%20dynamics%20in%20videos.%20Existing%20video-based%20Large%20Multimodal%0AModels%20%28LMMs%29%20handle%20basic%20conversations%20but%20struggle%20with%20precise%20pixel-level%0Agrounding%20in%20videos.%20To%20address%20this%2C%20we%20introduce%20VideoGLaMM%2C%20a%20LMM%20designed%0Afor%20fine-grained%20pixel-level%20grounding%20in%20videos%20based%20on%20user-provided%20textual%0Ainputs.%20Our%20design%20seamlessly%20connects%20three%20key%20components%3A%20a%20Large%20Language%0AModel%2C%20a%20dual%20vision%20encoder%20that%20emphasizes%20both%20spatial%20and%20temporal%20details%2C%0Aand%20a%20spatio-temporal%20decoder%20for%20accurate%20mask%20generation.%20This%20connection%20is%0Afacilitated%20via%20tunable%20V-L%20and%20L-V%20adapters%20that%20enable%20close%20Vision-Language%0A%28VL%29%20alignment.%20The%20architecture%20is%20trained%20to%20synchronize%20both%20spatial%20and%0Atemporal%20elements%20of%20video%20content%20with%20textual%20instructions.%20To%20enable%0Afine-grained%20grounding%2C%20we%20curate%20a%20multimodal%20dataset%20featuring%20detailed%0Avisually-grounded%20conversations%20using%20a%20semiautomatic%20annotation%20pipeline%2C%0Aresulting%20in%20a%20diverse%20set%20of%2038k%20video-QA%20triplets%20along%20with%2083k%20objects%20and%0A671k%20masks.%20We%20evaluate%20VideoGLaMM%20on%20three%20challenging%20tasks%3A%20Grounded%0AConversation%20Generation%2C%20Visual%20Grounding%2C%20and%20Referring%20Video%20Segmentation.%0AExperimental%20results%20show%20that%20our%20model%20consistently%20outperforms%20existing%0Aapproaches%20across%20all%20three%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04923v1&entry.124074799=Read"},
{"title": "DanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for\n  Audio-Driven Dance Motion Reconstruction", "author": "Li Zhao and Zhengmin Lu", "abstract": "  This paper introduces DanceFusion, a novel framework for reconstructing and\ngenerating dance movements synchronized to music, utilizing a Spatio-Temporal\nSkeleton Diffusion Transformer. The framework adeptly handles incomplete and\nnoisy skeletal data common in short-form dance videos on social media platforms\nlike TikTok. DanceFusion incorporates a hierarchical Transformer-based\nVariational Autoencoder (VAE) integrated with a diffusion model, significantly\nenhancing motion realism and accuracy. Our approach introduces sophisticated\nmasking techniques and a unique iterative diffusion process that refines the\nmotion sequences, ensuring high fidelity in both motion generation and\nsynchronization with accompanying audio cues. Comprehensive evaluations\ndemonstrate that DanceFusion surpasses existing methods, providing\nstate-of-the-art performance in generating dynamic, realistic, and\nstylistically diverse dance motions. Potential applications of this framework\nextend to content creation, virtual reality, and interactive entertainment,\npromising substantial advancements in automated dance generation. Visit our\nproject page at https://th-mlab.github.io/DanceFusion/.\n", "link": "http://arxiv.org/abs/2411.04646v1", "date": "2024-11-07", "relevancy": 2.3558, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6163}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceFusion%3A%20A%20Spatio-Temporal%20Skeleton%20Diffusion%20Transformer%20for%0A%20%20Audio-Driven%20Dance%20Motion%20Reconstruction&body=Title%3A%20DanceFusion%3A%20A%20Spatio-Temporal%20Skeleton%20Diffusion%20Transformer%20for%0A%20%20Audio-Driven%20Dance%20Motion%20Reconstruction%0AAuthor%3A%20Li%20Zhao%20and%20Zhengmin%20Lu%0AAbstract%3A%20%20%20This%20paper%20introduces%20DanceFusion%2C%20a%20novel%20framework%20for%20reconstructing%20and%0Agenerating%20dance%20movements%20synchronized%20to%20music%2C%20utilizing%20a%20Spatio-Temporal%0ASkeleton%20Diffusion%20Transformer.%20The%20framework%20adeptly%20handles%20incomplete%20and%0Anoisy%20skeletal%20data%20common%20in%20short-form%20dance%20videos%20on%20social%20media%20platforms%0Alike%20TikTok.%20DanceFusion%20incorporates%20a%20hierarchical%20Transformer-based%0AVariational%20Autoencoder%20%28VAE%29%20integrated%20with%20a%20diffusion%20model%2C%20significantly%0Aenhancing%20motion%20realism%20and%20accuracy.%20Our%20approach%20introduces%20sophisticated%0Amasking%20techniques%20and%20a%20unique%20iterative%20diffusion%20process%20that%20refines%20the%0Amotion%20sequences%2C%20ensuring%20high%20fidelity%20in%20both%20motion%20generation%20and%0Asynchronization%20with%20accompanying%20audio%20cues.%20Comprehensive%20evaluations%0Ademonstrate%20that%20DanceFusion%20surpasses%20existing%20methods%2C%20providing%0Astate-of-the-art%20performance%20in%20generating%20dynamic%2C%20realistic%2C%20and%0Astylistically%20diverse%20dance%20motions.%20Potential%20applications%20of%20this%20framework%0Aextend%20to%20content%20creation%2C%20virtual%20reality%2C%20and%20interactive%20entertainment%2C%0Apromising%20substantial%20advancements%20in%20automated%20dance%20generation.%20Visit%20our%0Aproject%20page%20at%20https%3A//th-mlab.github.io/DanceFusion/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceFusion%253A%2520A%2520Spatio-Temporal%2520Skeleton%2520Diffusion%2520Transformer%2520for%250A%2520%2520Audio-Driven%2520Dance%2520Motion%2520Reconstruction%26entry.906535625%3DLi%2520Zhao%2520and%2520Zhengmin%2520Lu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520DanceFusion%252C%2520a%2520novel%2520framework%2520for%2520reconstructing%2520and%250Agenerating%2520dance%2520movements%2520synchronized%2520to%2520music%252C%2520utilizing%2520a%2520Spatio-Temporal%250ASkeleton%2520Diffusion%2520Transformer.%2520The%2520framework%2520adeptly%2520handles%2520incomplete%2520and%250Anoisy%2520skeletal%2520data%2520common%2520in%2520short-form%2520dance%2520videos%2520on%2520social%2520media%2520platforms%250Alike%2520TikTok.%2520DanceFusion%2520incorporates%2520a%2520hierarchical%2520Transformer-based%250AVariational%2520Autoencoder%2520%2528VAE%2529%2520integrated%2520with%2520a%2520diffusion%2520model%252C%2520significantly%250Aenhancing%2520motion%2520realism%2520and%2520accuracy.%2520Our%2520approach%2520introduces%2520sophisticated%250Amasking%2520techniques%2520and%2520a%2520unique%2520iterative%2520diffusion%2520process%2520that%2520refines%2520the%250Amotion%2520sequences%252C%2520ensuring%2520high%2520fidelity%2520in%2520both%2520motion%2520generation%2520and%250Asynchronization%2520with%2520accompanying%2520audio%2520cues.%2520Comprehensive%2520evaluations%250Ademonstrate%2520that%2520DanceFusion%2520surpasses%2520existing%2520methods%252C%2520providing%250Astate-of-the-art%2520performance%2520in%2520generating%2520dynamic%252C%2520realistic%252C%2520and%250Astylistically%2520diverse%2520dance%2520motions.%2520Potential%2520applications%2520of%2520this%2520framework%250Aextend%2520to%2520content%2520creation%252C%2520virtual%2520reality%252C%2520and%2520interactive%2520entertainment%252C%250Apromising%2520substantial%2520advancements%2520in%2520automated%2520dance%2520generation.%2520Visit%2520our%250Aproject%2520page%2520at%2520https%253A//th-mlab.github.io/DanceFusion/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceFusion%3A%20A%20Spatio-Temporal%20Skeleton%20Diffusion%20Transformer%20for%0A%20%20Audio-Driven%20Dance%20Motion%20Reconstruction&entry.906535625=Li%20Zhao%20and%20Zhengmin%20Lu&entry.1292438233=%20%20This%20paper%20introduces%20DanceFusion%2C%20a%20novel%20framework%20for%20reconstructing%20and%0Agenerating%20dance%20movements%20synchronized%20to%20music%2C%20utilizing%20a%20Spatio-Temporal%0ASkeleton%20Diffusion%20Transformer.%20The%20framework%20adeptly%20handles%20incomplete%20and%0Anoisy%20skeletal%20data%20common%20in%20short-form%20dance%20videos%20on%20social%20media%20platforms%0Alike%20TikTok.%20DanceFusion%20incorporates%20a%20hierarchical%20Transformer-based%0AVariational%20Autoencoder%20%28VAE%29%20integrated%20with%20a%20diffusion%20model%2C%20significantly%0Aenhancing%20motion%20realism%20and%20accuracy.%20Our%20approach%20introduces%20sophisticated%0Amasking%20techniques%20and%20a%20unique%20iterative%20diffusion%20process%20that%20refines%20the%0Amotion%20sequences%2C%20ensuring%20high%20fidelity%20in%20both%20motion%20generation%20and%0Asynchronization%20with%20accompanying%20audio%20cues.%20Comprehensive%20evaluations%0Ademonstrate%20that%20DanceFusion%20surpasses%20existing%20methods%2C%20providing%0Astate-of-the-art%20performance%20in%20generating%20dynamic%2C%20realistic%2C%20and%0Astylistically%20diverse%20dance%20motions.%20Potential%20applications%20of%20this%20framework%0Aextend%20to%20content%20creation%2C%20virtual%20reality%2C%20and%20interactive%20entertainment%2C%0Apromising%20substantial%20advancements%20in%20automated%20dance%20generation.%20Visit%20our%0Aproject%20page%20at%20https%3A//th-mlab.github.io/DanceFusion/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04646v1&entry.124074799=Read"},
{"title": "Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs", "author": "Chengxin Hu and Hao Li", "abstract": "  Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the multi-level nature of\ngraph features. The impact of different feature levels on LLMs and the\nimportance of each level remain unexplored, and it is possible that different\nchemistry tasks require different feature levels. In this work, we first\ninvestigate the effect of feature granularity by fusing GNN-generated feature\ntokens, discovering that even reducing all tokens to a single token does not\nsignificantly impact performance. We then explore the effect of various feature\nlevels on performance, finding that both the quality of LLM-generated molecules\nand performance on different tasks benefit from different feature levels. We\nconclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)\nlack a comprehensive understanding of graph features, and (2) static processing\nis not sufficient for hierarchical graph feature. Our code will be publicly\navailable soon.\n", "link": "http://arxiv.org/abs/2411.04708v1", "date": "2024-11-07", "relevancy": 2.3501, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Hierarchical%20Molecular%20Graph%20Representation%20in%20Multimodal%20LLMs&body=Title%3A%20Exploring%20Hierarchical%20Molecular%20Graph%20Representation%20in%20Multimodal%20LLMs%0AAuthor%3A%20Chengxin%20Hu%20and%20Hao%20Li%0AAbstract%3A%20%20%20Following%20the%20milestones%20in%20large%20language%20models%20%28LLMs%29%20and%20multimodal%0Amodels%2C%20we%20have%20seen%20a%20surge%20in%20applying%20LLMs%20to%20biochemical%20tasks.%20Leveraging%0Agraph%20features%20and%20molecular%20text%20representations%2C%20LLMs%20can%20tackle%20various%0Atasks%2C%20such%20as%20predicting%20chemical%20reaction%20outcomes%20and%20describing%20molecular%0Aproperties.%20However%2C%20most%20current%20work%20overlooks%20the%20multi-level%20nature%20of%0Agraph%20features.%20The%20impact%20of%20different%20feature%20levels%20on%20LLMs%20and%20the%0Aimportance%20of%20each%20level%20remain%20unexplored%2C%20and%20it%20is%20possible%20that%20different%0Achemistry%20tasks%20require%20different%20feature%20levels.%20In%20this%20work%2C%20we%20first%0Ainvestigate%20the%20effect%20of%20feature%20granularity%20by%20fusing%20GNN-generated%20feature%0Atokens%2C%20discovering%20that%20even%20reducing%20all%20tokens%20to%20a%20single%20token%20does%20not%0Asignificantly%20impact%20performance.%20We%20then%20explore%20the%20effect%20of%20various%20feature%0Alevels%20on%20performance%2C%20finding%20that%20both%20the%20quality%20of%20LLM-generated%20molecules%0Aand%20performance%20on%20different%20tasks%20benefit%20from%20different%20feature%20levels.%20We%0Aconclude%20with%20two%20key%20insights%3A%20%281%29%20current%20molecular%20Multimodal%20LLMs%28MLLMs%29%0Alack%20a%20comprehensive%20understanding%20of%20graph%20features%2C%20and%20%282%29%20static%20processing%0Ais%20not%20sufficient%20for%20hierarchical%20graph%20feature.%20Our%20code%20will%20be%20publicly%0Aavailable%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Hierarchical%2520Molecular%2520Graph%2520Representation%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DChengxin%2520Hu%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Following%2520the%2520milestones%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520multimodal%250Amodels%252C%2520we%2520have%2520seen%2520a%2520surge%2520in%2520applying%2520LLMs%2520to%2520biochemical%2520tasks.%2520Leveraging%250Agraph%2520features%2520and%2520molecular%2520text%2520representations%252C%2520LLMs%2520can%2520tackle%2520various%250Atasks%252C%2520such%2520as%2520predicting%2520chemical%2520reaction%2520outcomes%2520and%2520describing%2520molecular%250Aproperties.%2520However%252C%2520most%2520current%2520work%2520overlooks%2520the%2520multi-level%2520nature%2520of%250Agraph%2520features.%2520The%2520impact%2520of%2520different%2520feature%2520levels%2520on%2520LLMs%2520and%2520the%250Aimportance%2520of%2520each%2520level%2520remain%2520unexplored%252C%2520and%2520it%2520is%2520possible%2520that%2520different%250Achemistry%2520tasks%2520require%2520different%2520feature%2520levels.%2520In%2520this%2520work%252C%2520we%2520first%250Ainvestigate%2520the%2520effect%2520of%2520feature%2520granularity%2520by%2520fusing%2520GNN-generated%2520feature%250Atokens%252C%2520discovering%2520that%2520even%2520reducing%2520all%2520tokens%2520to%2520a%2520single%2520token%2520does%2520not%250Asignificantly%2520impact%2520performance.%2520We%2520then%2520explore%2520the%2520effect%2520of%2520various%2520feature%250Alevels%2520on%2520performance%252C%2520finding%2520that%2520both%2520the%2520quality%2520of%2520LLM-generated%2520molecules%250Aand%2520performance%2520on%2520different%2520tasks%2520benefit%2520from%2520different%2520feature%2520levels.%2520We%250Aconclude%2520with%2520two%2520key%2520insights%253A%2520%25281%2529%2520current%2520molecular%2520Multimodal%2520LLMs%2528MLLMs%2529%250Alack%2520a%2520comprehensive%2520understanding%2520of%2520graph%2520features%252C%2520and%2520%25282%2529%2520static%2520processing%250Ais%2520not%2520sufficient%2520for%2520hierarchical%2520graph%2520feature.%2520Our%2520code%2520will%2520be%2520publicly%250Aavailable%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Hierarchical%20Molecular%20Graph%20Representation%20in%20Multimodal%20LLMs&entry.906535625=Chengxin%20Hu%20and%20Hao%20Li&entry.1292438233=%20%20Following%20the%20milestones%20in%20large%20language%20models%20%28LLMs%29%20and%20multimodal%0Amodels%2C%20we%20have%20seen%20a%20surge%20in%20applying%20LLMs%20to%20biochemical%20tasks.%20Leveraging%0Agraph%20features%20and%20molecular%20text%20representations%2C%20LLMs%20can%20tackle%20various%0Atasks%2C%20such%20as%20predicting%20chemical%20reaction%20outcomes%20and%20describing%20molecular%0Aproperties.%20However%2C%20most%20current%20work%20overlooks%20the%20multi-level%20nature%20of%0Agraph%20features.%20The%20impact%20of%20different%20feature%20levels%20on%20LLMs%20and%20the%0Aimportance%20of%20each%20level%20remain%20unexplored%2C%20and%20it%20is%20possible%20that%20different%0Achemistry%20tasks%20require%20different%20feature%20levels.%20In%20this%20work%2C%20we%20first%0Ainvestigate%20the%20effect%20of%20feature%20granularity%20by%20fusing%20GNN-generated%20feature%0Atokens%2C%20discovering%20that%20even%20reducing%20all%20tokens%20to%20a%20single%20token%20does%20not%0Asignificantly%20impact%20performance.%20We%20then%20explore%20the%20effect%20of%20various%20feature%0Alevels%20on%20performance%2C%20finding%20that%20both%20the%20quality%20of%20LLM-generated%20molecules%0Aand%20performance%20on%20different%20tasks%20benefit%20from%20different%20feature%20levels.%20We%0Aconclude%20with%20two%20key%20insights%3A%20%281%29%20current%20molecular%20Multimodal%20LLMs%28MLLMs%29%0Alack%20a%20comprehensive%20understanding%20of%20graph%20features%2C%20and%20%282%29%20static%20processing%0Ais%20not%20sufficient%20for%20hierarchical%20graph%20feature.%20Our%20code%20will%20be%20publicly%0Aavailable%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04708v1&entry.124074799=Read"},
{"title": "ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis\n  Classification Network Using CLIP-guided SAM mechanism", "author": "Xinlei Yu and Ahmed Elazab and Ruiquan Ge and Hui Jin and Xinchen Jiang and Gangyong Jia and Qing Wu and Qinglei Shi and Changmiao Wang", "abstract": "  Intracerebral hemorrhage (ICH) is the most fatal subtype of stroke and is\ncharacterized by a high incidence of disability. Accurate segmentation of the\nICH region and prognosis prediction are critically important for developing and\nrefining treatment plans for post-ICH patients. However, existing approaches\naddress these two tasks independently and predominantly focus on imaging data\nalone, thereby neglecting the intrinsic correlation between the tasks and\nmodalities. This paper introduces a multi-task network, ICH-SCNet, designed for\nboth ICH segmentation and prognosis classification. Specifically, we integrate\na SAM-CLIP cross-modal interaction mechanism that combines medical text and\nsegmentation auxiliary information with neuroimaging data to enhance\ncross-modal feature recognition. Additionally, we develop an effective feature\nfusion module and a multi-task loss function to improve performance further.\nExtensive experiments on an ICH dataset reveal that our approach surpasses\nother state-of-the-art methods. It excels in the overall performance of\nclassification tasks and outperforms competing models in all segmentation task\nmetrics.\n", "link": "http://arxiv.org/abs/2411.04656v1", "date": "2024-11-07", "relevancy": 2.3494, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICH-SCNet%3A%20Intracerebral%20Hemorrhage%20Segmentation%20and%20Prognosis%0A%20%20Classification%20Network%20Using%20CLIP-guided%20SAM%20mechanism&body=Title%3A%20ICH-SCNet%3A%20Intracerebral%20Hemorrhage%20Segmentation%20and%20Prognosis%0A%20%20Classification%20Network%20Using%20CLIP-guided%20SAM%20mechanism%0AAuthor%3A%20Xinlei%20Yu%20and%20Ahmed%20Elazab%20and%20Ruiquan%20Ge%20and%20Hui%20Jin%20and%20Xinchen%20Jiang%20and%20Gangyong%20Jia%20and%20Qing%20Wu%20and%20Qinglei%20Shi%20and%20Changmiao%20Wang%0AAbstract%3A%20%20%20Intracerebral%20hemorrhage%20%28ICH%29%20is%20the%20most%20fatal%20subtype%20of%20stroke%20and%20is%0Acharacterized%20by%20a%20high%20incidence%20of%20disability.%20Accurate%20segmentation%20of%20the%0AICH%20region%20and%20prognosis%20prediction%20are%20critically%20important%20for%20developing%20and%0Arefining%20treatment%20plans%20for%20post-ICH%20patients.%20However%2C%20existing%20approaches%0Aaddress%20these%20two%20tasks%20independently%20and%20predominantly%20focus%20on%20imaging%20data%0Aalone%2C%20thereby%20neglecting%20the%20intrinsic%20correlation%20between%20the%20tasks%20and%0Amodalities.%20This%20paper%20introduces%20a%20multi-task%20network%2C%20ICH-SCNet%2C%20designed%20for%0Aboth%20ICH%20segmentation%20and%20prognosis%20classification.%20Specifically%2C%20we%20integrate%0Aa%20SAM-CLIP%20cross-modal%20interaction%20mechanism%20that%20combines%20medical%20text%20and%0Asegmentation%20auxiliary%20information%20with%20neuroimaging%20data%20to%20enhance%0Across-modal%20feature%20recognition.%20Additionally%2C%20we%20develop%20an%20effective%20feature%0Afusion%20module%20and%20a%20multi-task%20loss%20function%20to%20improve%20performance%20further.%0AExtensive%20experiments%20on%20an%20ICH%20dataset%20reveal%20that%20our%20approach%20surpasses%0Aother%20state-of-the-art%20methods.%20It%20excels%20in%20the%20overall%20performance%20of%0Aclassification%20tasks%20and%20outperforms%20competing%20models%20in%20all%20segmentation%20task%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICH-SCNet%253A%2520Intracerebral%2520Hemorrhage%2520Segmentation%2520and%2520Prognosis%250A%2520%2520Classification%2520Network%2520Using%2520CLIP-guided%2520SAM%2520mechanism%26entry.906535625%3DXinlei%2520Yu%2520and%2520Ahmed%2520Elazab%2520and%2520Ruiquan%2520Ge%2520and%2520Hui%2520Jin%2520and%2520Xinchen%2520Jiang%2520and%2520Gangyong%2520Jia%2520and%2520Qing%2520Wu%2520and%2520Qinglei%2520Shi%2520and%2520Changmiao%2520Wang%26entry.1292438233%3D%2520%2520Intracerebral%2520hemorrhage%2520%2528ICH%2529%2520is%2520the%2520most%2520fatal%2520subtype%2520of%2520stroke%2520and%2520is%250Acharacterized%2520by%2520a%2520high%2520incidence%2520of%2520disability.%2520Accurate%2520segmentation%2520of%2520the%250AICH%2520region%2520and%2520prognosis%2520prediction%2520are%2520critically%2520important%2520for%2520developing%2520and%250Arefining%2520treatment%2520plans%2520for%2520post-ICH%2520patients.%2520However%252C%2520existing%2520approaches%250Aaddress%2520these%2520two%2520tasks%2520independently%2520and%2520predominantly%2520focus%2520on%2520imaging%2520data%250Aalone%252C%2520thereby%2520neglecting%2520the%2520intrinsic%2520correlation%2520between%2520the%2520tasks%2520and%250Amodalities.%2520This%2520paper%2520introduces%2520a%2520multi-task%2520network%252C%2520ICH-SCNet%252C%2520designed%2520for%250Aboth%2520ICH%2520segmentation%2520and%2520prognosis%2520classification.%2520Specifically%252C%2520we%2520integrate%250Aa%2520SAM-CLIP%2520cross-modal%2520interaction%2520mechanism%2520that%2520combines%2520medical%2520text%2520and%250Asegmentation%2520auxiliary%2520information%2520with%2520neuroimaging%2520data%2520to%2520enhance%250Across-modal%2520feature%2520recognition.%2520Additionally%252C%2520we%2520develop%2520an%2520effective%2520feature%250Afusion%2520module%2520and%2520a%2520multi-task%2520loss%2520function%2520to%2520improve%2520performance%2520further.%250AExtensive%2520experiments%2520on%2520an%2520ICH%2520dataset%2520reveal%2520that%2520our%2520approach%2520surpasses%250Aother%2520state-of-the-art%2520methods.%2520It%2520excels%2520in%2520the%2520overall%2520performance%2520of%250Aclassification%2520tasks%2520and%2520outperforms%2520competing%2520models%2520in%2520all%2520segmentation%2520task%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICH-SCNet%3A%20Intracerebral%20Hemorrhage%20Segmentation%20and%20Prognosis%0A%20%20Classification%20Network%20Using%20CLIP-guided%20SAM%20mechanism&entry.906535625=Xinlei%20Yu%20and%20Ahmed%20Elazab%20and%20Ruiquan%20Ge%20and%20Hui%20Jin%20and%20Xinchen%20Jiang%20and%20Gangyong%20Jia%20and%20Qing%20Wu%20and%20Qinglei%20Shi%20and%20Changmiao%20Wang&entry.1292438233=%20%20Intracerebral%20hemorrhage%20%28ICH%29%20is%20the%20most%20fatal%20subtype%20of%20stroke%20and%20is%0Acharacterized%20by%20a%20high%20incidence%20of%20disability.%20Accurate%20segmentation%20of%20the%0AICH%20region%20and%20prognosis%20prediction%20are%20critically%20important%20for%20developing%20and%0Arefining%20treatment%20plans%20for%20post-ICH%20patients.%20However%2C%20existing%20approaches%0Aaddress%20these%20two%20tasks%20independently%20and%20predominantly%20focus%20on%20imaging%20data%0Aalone%2C%20thereby%20neglecting%20the%20intrinsic%20correlation%20between%20the%20tasks%20and%0Amodalities.%20This%20paper%20introduces%20a%20multi-task%20network%2C%20ICH-SCNet%2C%20designed%20for%0Aboth%20ICH%20segmentation%20and%20prognosis%20classification.%20Specifically%2C%20we%20integrate%0Aa%20SAM-CLIP%20cross-modal%20interaction%20mechanism%20that%20combines%20medical%20text%20and%0Asegmentation%20auxiliary%20information%20with%20neuroimaging%20data%20to%20enhance%0Across-modal%20feature%20recognition.%20Additionally%2C%20we%20develop%20an%20effective%20feature%0Afusion%20module%20and%20a%20multi-task%20loss%20function%20to%20improve%20performance%20further.%0AExtensive%20experiments%20on%20an%20ICH%20dataset%20reveal%20that%20our%20approach%20surpasses%0Aother%20state-of-the-art%20methods.%20It%20excels%20in%20the%20overall%20performance%20of%0Aclassification%20tasks%20and%20outperforms%20competing%20models%20in%20all%20segmentation%20task%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04656v1&entry.124074799=Read"},
{"title": "VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal\n  Transparent Surface Reconstruction in Indoor Scenes", "author": "Advaith V. Sethuraman and Onur Bagoren and Harikrishnan Seetharaman and Dalton Richardson and Joseph Taylor and Katherine A. Skinner", "abstract": "  Mobile robots operating indoors must be prepared to navigate challenging\nscenes that contain transparent surfaces. This paper proposes a novel method\nfor the fusion of acoustic and visual sensing modalities through implicit\nneural representations to enable dense reconstruction of transparent surfaces\nin indoor scenes. We propose a novel model that leverages generative latent\noptimization to learn an implicit representation of indoor scenes consisting of\ntransparent surfaces. We demonstrate that we can query the implicit\nrepresentation to enable volumetric rendering in image space or 3D geometry\nreconstruction (point clouds or mesh) with transparent surface prediction. We\nevaluate our method's effectiveness qualitatively and quantitatively on a new\ndataset collected using a custom, low-cost sensing platform featuring RGB-D\ncameras and ultrasonic sensors. Our method exhibits significant improvement\nover state-of-the-art for transparent surface reconstruction.\n", "link": "http://arxiv.org/abs/2411.04963v1", "date": "2024-11-07", "relevancy": 2.3426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6028}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAIR%3A%20Visuo-Acoustic%20Implicit%20Representations%20for%20Low-Cost%2C%20Multi-Modal%0A%20%20Transparent%20Surface%20Reconstruction%20in%20Indoor%20Scenes&body=Title%3A%20VAIR%3A%20Visuo-Acoustic%20Implicit%20Representations%20for%20Low-Cost%2C%20Multi-Modal%0A%20%20Transparent%20Surface%20Reconstruction%20in%20Indoor%20Scenes%0AAuthor%3A%20Advaith%20V.%20Sethuraman%20and%20Onur%20Bagoren%20and%20Harikrishnan%20Seetharaman%20and%20Dalton%20Richardson%20and%20Joseph%20Taylor%20and%20Katherine%20A.%20Skinner%0AAbstract%3A%20%20%20Mobile%20robots%20operating%20indoors%20must%20be%20prepared%20to%20navigate%20challenging%0Ascenes%20that%20contain%20transparent%20surfaces.%20This%20paper%20proposes%20a%20novel%20method%0Afor%20the%20fusion%20of%20acoustic%20and%20visual%20sensing%20modalities%20through%20implicit%0Aneural%20representations%20to%20enable%20dense%20reconstruction%20of%20transparent%20surfaces%0Ain%20indoor%20scenes.%20We%20propose%20a%20novel%20model%20that%20leverages%20generative%20latent%0Aoptimization%20to%20learn%20an%20implicit%20representation%20of%20indoor%20scenes%20consisting%20of%0Atransparent%20surfaces.%20We%20demonstrate%20that%20we%20can%20query%20the%20implicit%0Arepresentation%20to%20enable%20volumetric%20rendering%20in%20image%20space%20or%203D%20geometry%0Areconstruction%20%28point%20clouds%20or%20mesh%29%20with%20transparent%20surface%20prediction.%20We%0Aevaluate%20our%20method%27s%20effectiveness%20qualitatively%20and%20quantitatively%20on%20a%20new%0Adataset%20collected%20using%20a%20custom%2C%20low-cost%20sensing%20platform%20featuring%20RGB-D%0Acameras%20and%20ultrasonic%20sensors.%20Our%20method%20exhibits%20significant%20improvement%0Aover%20state-of-the-art%20for%20transparent%20surface%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAIR%253A%2520Visuo-Acoustic%2520Implicit%2520Representations%2520for%2520Low-Cost%252C%2520Multi-Modal%250A%2520%2520Transparent%2520Surface%2520Reconstruction%2520in%2520Indoor%2520Scenes%26entry.906535625%3DAdvaith%2520V.%2520Sethuraman%2520and%2520Onur%2520Bagoren%2520and%2520Harikrishnan%2520Seetharaman%2520and%2520Dalton%2520Richardson%2520and%2520Joseph%2520Taylor%2520and%2520Katherine%2520A.%2520Skinner%26entry.1292438233%3D%2520%2520Mobile%2520robots%2520operating%2520indoors%2520must%2520be%2520prepared%2520to%2520navigate%2520challenging%250Ascenes%2520that%2520contain%2520transparent%2520surfaces.%2520This%2520paper%2520proposes%2520a%2520novel%2520method%250Afor%2520the%2520fusion%2520of%2520acoustic%2520and%2520visual%2520sensing%2520modalities%2520through%2520implicit%250Aneural%2520representations%2520to%2520enable%2520dense%2520reconstruction%2520of%2520transparent%2520surfaces%250Ain%2520indoor%2520scenes.%2520We%2520propose%2520a%2520novel%2520model%2520that%2520leverages%2520generative%2520latent%250Aoptimization%2520to%2520learn%2520an%2520implicit%2520representation%2520of%2520indoor%2520scenes%2520consisting%2520of%250Atransparent%2520surfaces.%2520We%2520demonstrate%2520that%2520we%2520can%2520query%2520the%2520implicit%250Arepresentation%2520to%2520enable%2520volumetric%2520rendering%2520in%2520image%2520space%2520or%25203D%2520geometry%250Areconstruction%2520%2528point%2520clouds%2520or%2520mesh%2529%2520with%2520transparent%2520surface%2520prediction.%2520We%250Aevaluate%2520our%2520method%2527s%2520effectiveness%2520qualitatively%2520and%2520quantitatively%2520on%2520a%2520new%250Adataset%2520collected%2520using%2520a%2520custom%252C%2520low-cost%2520sensing%2520platform%2520featuring%2520RGB-D%250Acameras%2520and%2520ultrasonic%2520sensors.%2520Our%2520method%2520exhibits%2520significant%2520improvement%250Aover%2520state-of-the-art%2520for%2520transparent%2520surface%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAIR%3A%20Visuo-Acoustic%20Implicit%20Representations%20for%20Low-Cost%2C%20Multi-Modal%0A%20%20Transparent%20Surface%20Reconstruction%20in%20Indoor%20Scenes&entry.906535625=Advaith%20V.%20Sethuraman%20and%20Onur%20Bagoren%20and%20Harikrishnan%20Seetharaman%20and%20Dalton%20Richardson%20and%20Joseph%20Taylor%20and%20Katherine%20A.%20Skinner&entry.1292438233=%20%20Mobile%20robots%20operating%20indoors%20must%20be%20prepared%20to%20navigate%20challenging%0Ascenes%20that%20contain%20transparent%20surfaces.%20This%20paper%20proposes%20a%20novel%20method%0Afor%20the%20fusion%20of%20acoustic%20and%20visual%20sensing%20modalities%20through%20implicit%0Aneural%20representations%20to%20enable%20dense%20reconstruction%20of%20transparent%20surfaces%0Ain%20indoor%20scenes.%20We%20propose%20a%20novel%20model%20that%20leverages%20generative%20latent%0Aoptimization%20to%20learn%20an%20implicit%20representation%20of%20indoor%20scenes%20consisting%20of%0Atransparent%20surfaces.%20We%20demonstrate%20that%20we%20can%20query%20the%20implicit%0Arepresentation%20to%20enable%20volumetric%20rendering%20in%20image%20space%20or%203D%20geometry%0Areconstruction%20%28point%20clouds%20or%20mesh%29%20with%20transparent%20surface%20prediction.%20We%0Aevaluate%20our%20method%27s%20effectiveness%20qualitatively%20and%20quantitatively%20on%20a%20new%0Adataset%20collected%20using%20a%20custom%2C%20low-cost%20sensing%20platform%20featuring%20RGB-D%0Acameras%20and%20ultrasonic%20sensors.%20Our%20method%20exhibits%20significant%20improvement%0Aover%20state-of-the-art%20for%20transparent%20surface%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04963v1&entry.124074799=Read"},
{"title": "DomainGallery: Few-shot Domain-driven Image Generation by\n  Attribute-centric Finetuning", "author": "Yuxuan Duan and Yan Hong and Bo Zhang and Jun Lan and Huijia Zhu and Weiqiang Wang and Jianfu Zhang and Li Niu and Liqing Zhang", "abstract": "  The recent progress in text-to-image models pretrained on large-scale\ndatasets has enabled us to generate various images as long as we provide a text\nprompt describing what we want. Nevertheless, the availability of these models\nis still limited when we expect to generate images that fall into a specific\ndomain either hard to describe or just unseen to the models. In this work, we\npropose DomainGallery, a few-shot domain-driven image generation method which\naims at finetuning pretrained Stable Diffusion on few-shot target datasets in\nan attribute-centric manner. Specifically, DomainGallery features prior\nattribute erasure, attribute disentanglement, regularization and enhancement.\nThese techniques are tailored to few-shot domain-driven generation in order to\nsolve key issues that previous works have failed to settle. Extensive\nexperiments are given to validate the superior performance of DomainGallery on\na variety of domain-driven generation scenarios. Codes are available at\nhttps://github.com/Ldhlwh/DomainGallery.\n", "link": "http://arxiv.org/abs/2411.04571v1", "date": "2024-11-07", "relevancy": 2.3375, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5987}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5832}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DomainGallery%3A%20Few-shot%20Domain-driven%20Image%20Generation%20by%0A%20%20Attribute-centric%20Finetuning&body=Title%3A%20DomainGallery%3A%20Few-shot%20Domain-driven%20Image%20Generation%20by%0A%20%20Attribute-centric%20Finetuning%0AAuthor%3A%20Yuxuan%20Duan%20and%20Yan%20Hong%20and%20Bo%20Zhang%20and%20Jun%20Lan%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jianfu%20Zhang%20and%20Li%20Niu%20and%20Liqing%20Zhang%0AAbstract%3A%20%20%20The%20recent%20progress%20in%20text-to-image%20models%20pretrained%20on%20large-scale%0Adatasets%20has%20enabled%20us%20to%20generate%20various%20images%20as%20long%20as%20we%20provide%20a%20text%0Aprompt%20describing%20what%20we%20want.%20Nevertheless%2C%20the%20availability%20of%20these%20models%0Ais%20still%20limited%20when%20we%20expect%20to%20generate%20images%20that%20fall%20into%20a%20specific%0Adomain%20either%20hard%20to%20describe%20or%20just%20unseen%20to%20the%20models.%20In%20this%20work%2C%20we%0Apropose%20DomainGallery%2C%20a%20few-shot%20domain-driven%20image%20generation%20method%20which%0Aaims%20at%20finetuning%20pretrained%20Stable%20Diffusion%20on%20few-shot%20target%20datasets%20in%0Aan%20attribute-centric%20manner.%20Specifically%2C%20DomainGallery%20features%20prior%0Aattribute%20erasure%2C%20attribute%20disentanglement%2C%20regularization%20and%20enhancement.%0AThese%20techniques%20are%20tailored%20to%20few-shot%20domain-driven%20generation%20in%20order%20to%0Asolve%20key%20issues%20that%20previous%20works%20have%20failed%20to%20settle.%20Extensive%0Aexperiments%20are%20given%20to%20validate%20the%20superior%20performance%20of%20DomainGallery%20on%0Aa%20variety%20of%20domain-driven%20generation%20scenarios.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Ldhlwh/DomainGallery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomainGallery%253A%2520Few-shot%2520Domain-driven%2520Image%2520Generation%2520by%250A%2520%2520Attribute-centric%2520Finetuning%26entry.906535625%3DYuxuan%2520Duan%2520and%2520Yan%2520Hong%2520and%2520Bo%2520Zhang%2520and%2520Jun%2520Lan%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Jianfu%2520Zhang%2520and%2520Li%2520Niu%2520and%2520Liqing%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520progress%2520in%2520text-to-image%2520models%2520pretrained%2520on%2520large-scale%250Adatasets%2520has%2520enabled%2520us%2520to%2520generate%2520various%2520images%2520as%2520long%2520as%2520we%2520provide%2520a%2520text%250Aprompt%2520describing%2520what%2520we%2520want.%2520Nevertheless%252C%2520the%2520availability%2520of%2520these%2520models%250Ais%2520still%2520limited%2520when%2520we%2520expect%2520to%2520generate%2520images%2520that%2520fall%2520into%2520a%2520specific%250Adomain%2520either%2520hard%2520to%2520describe%2520or%2520just%2520unseen%2520to%2520the%2520models.%2520In%2520this%2520work%252C%2520we%250Apropose%2520DomainGallery%252C%2520a%2520few-shot%2520domain-driven%2520image%2520generation%2520method%2520which%250Aaims%2520at%2520finetuning%2520pretrained%2520Stable%2520Diffusion%2520on%2520few-shot%2520target%2520datasets%2520in%250Aan%2520attribute-centric%2520manner.%2520Specifically%252C%2520DomainGallery%2520features%2520prior%250Aattribute%2520erasure%252C%2520attribute%2520disentanglement%252C%2520regularization%2520and%2520enhancement.%250AThese%2520techniques%2520are%2520tailored%2520to%2520few-shot%2520domain-driven%2520generation%2520in%2520order%2520to%250Asolve%2520key%2520issues%2520that%2520previous%2520works%2520have%2520failed%2520to%2520settle.%2520Extensive%250Aexperiments%2520are%2520given%2520to%2520validate%2520the%2520superior%2520performance%2520of%2520DomainGallery%2520on%250Aa%2520variety%2520of%2520domain-driven%2520generation%2520scenarios.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Ldhlwh/DomainGallery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DomainGallery%3A%20Few-shot%20Domain-driven%20Image%20Generation%20by%0A%20%20Attribute-centric%20Finetuning&entry.906535625=Yuxuan%20Duan%20and%20Yan%20Hong%20and%20Bo%20Zhang%20and%20Jun%20Lan%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Jianfu%20Zhang%20and%20Li%20Niu%20and%20Liqing%20Zhang&entry.1292438233=%20%20The%20recent%20progress%20in%20text-to-image%20models%20pretrained%20on%20large-scale%0Adatasets%20has%20enabled%20us%20to%20generate%20various%20images%20as%20long%20as%20we%20provide%20a%20text%0Aprompt%20describing%20what%20we%20want.%20Nevertheless%2C%20the%20availability%20of%20these%20models%0Ais%20still%20limited%20when%20we%20expect%20to%20generate%20images%20that%20fall%20into%20a%20specific%0Adomain%20either%20hard%20to%20describe%20or%20just%20unseen%20to%20the%20models.%20In%20this%20work%2C%20we%0Apropose%20DomainGallery%2C%20a%20few-shot%20domain-driven%20image%20generation%20method%20which%0Aaims%20at%20finetuning%20pretrained%20Stable%20Diffusion%20on%20few-shot%20target%20datasets%20in%0Aan%20attribute-centric%20manner.%20Specifically%2C%20DomainGallery%20features%20prior%0Aattribute%20erasure%2C%20attribute%20disentanglement%2C%20regularization%20and%20enhancement.%0AThese%20techniques%20are%20tailored%20to%20few-shot%20domain-driven%20generation%20in%20order%20to%0Asolve%20key%20issues%20that%20previous%20works%20have%20failed%20to%20settle.%20Extensive%0Aexperiments%20are%20given%20to%20validate%20the%20superior%20performance%20of%20DomainGallery%20on%0Aa%20variety%20of%20domain-driven%20generation%20scenarios.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Ldhlwh/DomainGallery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04571v1&entry.124074799=Read"},
{"title": "StoryAgent: Customized Storytelling Video Generation via Multi-Agent\n  Collaboration", "author": "Panwen Hu and Jin Jiang and Jianqi Chen and Mingfei Han and Shengcai Liao and Xiaojun Chang and Xiaodan Liang", "abstract": "  The advent of AI-Generated Content (AIGC) has spurred research into automated\nvideo generation to streamline conventional processes. However, automating\nstorytelling video production, particularly for customized narratives, remains\nchallenging due to the complexity of maintaining subject consistency across\nshots. While existing approaches like Mora and AesopAgent integrate multiple\nagents for Story-to-Video (S2V) generation, they fall short in preserving\nprotagonist consistency and supporting Customized Storytelling Video Generation\n(CSVG). To address these limitations, we propose StoryAgent, a multi-agent\nframework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks\nassigned to specialized agents, mirroring the professional production process.\nNotably, our framework includes agents for story design, storyboard generation,\nvideo creation, agent coordination, and result evaluation. Leveraging the\nstrengths of different models, StoryAgent enhances control over the generation\nprocess, significantly improving character consistency. Specifically, we\nintroduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance\nintra-shot temporal consistency, while a novel storyboard generation pipeline\nis proposed to maintain subject consistency across shots. Extensive experiments\ndemonstrate the effectiveness of our approach in synthesizing highly consistent\nstorytelling videos, outperforming state-of-the-art methods. Our contributions\ninclude the introduction of StoryAgent, a versatile framework for video\ngeneration tasks, and novel techniques for preserving protagonist consistency.\n", "link": "http://arxiv.org/abs/2411.04925v1", "date": "2024-11-07", "relevancy": 2.3301, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6047}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6013}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration&body=Title%3A%20StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration%0AAuthor%3A%20Panwen%20Hu%20and%20Jin%20Jiang%20and%20Jianqi%20Chen%20and%20Mingfei%20Han%20and%20Shengcai%20Liao%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20The%20advent%20of%20AI-Generated%20Content%20%28AIGC%29%20has%20spurred%20research%20into%20automated%0Avideo%20generation%20to%20streamline%20conventional%20processes.%20However%2C%20automating%0Astorytelling%20video%20production%2C%20particularly%20for%20customized%20narratives%2C%20remains%0Achallenging%20due%20to%20the%20complexity%20of%20maintaining%20subject%20consistency%20across%0Ashots.%20While%20existing%20approaches%20like%20Mora%20and%20AesopAgent%20integrate%20multiple%0Aagents%20for%20Story-to-Video%20%28S2V%29%20generation%2C%20they%20fall%20short%20in%20preserving%0Aprotagonist%20consistency%20and%20supporting%20Customized%20Storytelling%20Video%20Generation%0A%28CSVG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20StoryAgent%2C%20a%20multi-agent%0Aframework%20designed%20for%20CSVG.%20StoryAgent%20decomposes%20CSVG%20into%20distinct%20subtasks%0Aassigned%20to%20specialized%20agents%2C%20mirroring%20the%20professional%20production%20process.%0ANotably%2C%20our%20framework%20includes%20agents%20for%20story%20design%2C%20storyboard%20generation%2C%0Avideo%20creation%2C%20agent%20coordination%2C%20and%20result%20evaluation.%20Leveraging%20the%0Astrengths%20of%20different%20models%2C%20StoryAgent%20enhances%20control%20over%20the%20generation%0Aprocess%2C%20significantly%20improving%20character%20consistency.%20Specifically%2C%20we%0Aintroduce%20a%20customized%20Image-to-Video%20%28I2V%29%20method%2C%20LoRA-BE%2C%20to%20enhance%0Aintra-shot%20temporal%20consistency%2C%20while%20a%20novel%20storyboard%20generation%20pipeline%0Ais%20proposed%20to%20maintain%20subject%20consistency%20across%20shots.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20synthesizing%20highly%20consistent%0Astorytelling%20videos%2C%20outperforming%20state-of-the-art%20methods.%20Our%20contributions%0Ainclude%20the%20introduction%20of%20StoryAgent%2C%20a%20versatile%20framework%20for%20video%0Ageneration%20tasks%2C%20and%20novel%20techniques%20for%20preserving%20protagonist%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryAgent%253A%2520Customized%2520Storytelling%2520Video%2520Generation%2520via%2520Multi-Agent%250A%2520%2520Collaboration%26entry.906535625%3DPanwen%2520Hu%2520and%2520Jin%2520Jiang%2520and%2520Jianqi%2520Chen%2520and%2520Mingfei%2520Han%2520and%2520Shengcai%2520Liao%2520and%2520Xiaojun%2520Chang%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520AI-Generated%2520Content%2520%2528AIGC%2529%2520has%2520spurred%2520research%2520into%2520automated%250Avideo%2520generation%2520to%2520streamline%2520conventional%2520processes.%2520However%252C%2520automating%250Astorytelling%2520video%2520production%252C%2520particularly%2520for%2520customized%2520narratives%252C%2520remains%250Achallenging%2520due%2520to%2520the%2520complexity%2520of%2520maintaining%2520subject%2520consistency%2520across%250Ashots.%2520While%2520existing%2520approaches%2520like%2520Mora%2520and%2520AesopAgent%2520integrate%2520multiple%250Aagents%2520for%2520Story-to-Video%2520%2528S2V%2529%2520generation%252C%2520they%2520fall%2520short%2520in%2520preserving%250Aprotagonist%2520consistency%2520and%2520supporting%2520Customized%2520Storytelling%2520Video%2520Generation%250A%2528CSVG%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520StoryAgent%252C%2520a%2520multi-agent%250Aframework%2520designed%2520for%2520CSVG.%2520StoryAgent%2520decomposes%2520CSVG%2520into%2520distinct%2520subtasks%250Aassigned%2520to%2520specialized%2520agents%252C%2520mirroring%2520the%2520professional%2520production%2520process.%250ANotably%252C%2520our%2520framework%2520includes%2520agents%2520for%2520story%2520design%252C%2520storyboard%2520generation%252C%250Avideo%2520creation%252C%2520agent%2520coordination%252C%2520and%2520result%2520evaluation.%2520Leveraging%2520the%250Astrengths%2520of%2520different%2520models%252C%2520StoryAgent%2520enhances%2520control%2520over%2520the%2520generation%250Aprocess%252C%2520significantly%2520improving%2520character%2520consistency.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520customized%2520Image-to-Video%2520%2528I2V%2529%2520method%252C%2520LoRA-BE%252C%2520to%2520enhance%250Aintra-shot%2520temporal%2520consistency%252C%2520while%2520a%2520novel%2520storyboard%2520generation%2520pipeline%250Ais%2520proposed%2520to%2520maintain%2520subject%2520consistency%2520across%2520shots.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520synthesizing%2520highly%2520consistent%250Astorytelling%2520videos%252C%2520outperforming%2520state-of-the-art%2520methods.%2520Our%2520contributions%250Ainclude%2520the%2520introduction%2520of%2520StoryAgent%252C%2520a%2520versatile%2520framework%2520for%2520video%250Ageneration%2520tasks%252C%2520and%2520novel%2520techniques%2520for%2520preserving%2520protagonist%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryAgent%3A%20Customized%20Storytelling%20Video%20Generation%20via%20Multi-Agent%0A%20%20Collaboration&entry.906535625=Panwen%20Hu%20and%20Jin%20Jiang%20and%20Jianqi%20Chen%20and%20Mingfei%20Han%20and%20Shengcai%20Liao%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang&entry.1292438233=%20%20The%20advent%20of%20AI-Generated%20Content%20%28AIGC%29%20has%20spurred%20research%20into%20automated%0Avideo%20generation%20to%20streamline%20conventional%20processes.%20However%2C%20automating%0Astorytelling%20video%20production%2C%20particularly%20for%20customized%20narratives%2C%20remains%0Achallenging%20due%20to%20the%20complexity%20of%20maintaining%20subject%20consistency%20across%0Ashots.%20While%20existing%20approaches%20like%20Mora%20and%20AesopAgent%20integrate%20multiple%0Aagents%20for%20Story-to-Video%20%28S2V%29%20generation%2C%20they%20fall%20short%20in%20preserving%0Aprotagonist%20consistency%20and%20supporting%20Customized%20Storytelling%20Video%20Generation%0A%28CSVG%29.%20To%20address%20these%20limitations%2C%20we%20propose%20StoryAgent%2C%20a%20multi-agent%0Aframework%20designed%20for%20CSVG.%20StoryAgent%20decomposes%20CSVG%20into%20distinct%20subtasks%0Aassigned%20to%20specialized%20agents%2C%20mirroring%20the%20professional%20production%20process.%0ANotably%2C%20our%20framework%20includes%20agents%20for%20story%20design%2C%20storyboard%20generation%2C%0Avideo%20creation%2C%20agent%20coordination%2C%20and%20result%20evaluation.%20Leveraging%20the%0Astrengths%20of%20different%20models%2C%20StoryAgent%20enhances%20control%20over%20the%20generation%0Aprocess%2C%20significantly%20improving%20character%20consistency.%20Specifically%2C%20we%0Aintroduce%20a%20customized%20Image-to-Video%20%28I2V%29%20method%2C%20LoRA-BE%2C%20to%20enhance%0Aintra-shot%20temporal%20consistency%2C%20while%20a%20novel%20storyboard%20generation%20pipeline%0Ais%20proposed%20to%20maintain%20subject%20consistency%20across%20shots.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20synthesizing%20highly%20consistent%0Astorytelling%20videos%2C%20outperforming%20state-of-the-art%20methods.%20Our%20contributions%0Ainclude%20the%20introduction%20of%20StoryAgent%2C%20a%20versatile%20framework%20for%20video%0Ageneration%20tasks%2C%20and%20novel%20techniques%20for%20preserving%20protagonist%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04925v1&entry.124074799=Read"},
{"title": "C3T: Cross-modal Transfer Through Time for Human Action Recognition", "author": "Abhi Kamboj and Anh Duy Nguyen and Minh Do", "abstract": "  In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between modalities using the structure of a unified\nmultimodal representation space for Human Action Recognition (HAR). We\nformalize and explore an understudied cross-modal transfer setting we term\nUnsupervised Modality Adaptation (UMA), where the modality used in testing is\nnot used in supervised training, i.e. zero labeled instances of the test\nmodality are available during training. We develop three methods to perform\nUMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer\nThrough Time (C3T). Our extensive experiments on various camera+IMU datasets\ncompare these methods to each other in the UMA setting, and to their empirical\nupper bound in the supervised setting. The results indicate C3T is the most\nrobust and highest performing by at least a margin of 8%, and nears the\nsupervised setting performance even in the presence of temporal noise. This\nmethod introduces a novel mechanism for aligning signals across time-varying\nlatent vectors, extracted from the receptive field of temporal convolutions.\nOur findings suggest that C3T has significant potential for developing\ngeneralizable models for time-series sensor data, opening new avenues for\nmulti-modal learning in various applications.\n", "link": "http://arxiv.org/abs/2407.16803v2", "date": "2024-11-07", "relevancy": 2.3239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Human%20Action%20Recognition&body=Title%3A%20C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Human%20Action%20Recognition%0AAuthor%3A%20Abhi%20Kamboj%20and%20Anh%20Duy%20Nguyen%20and%20Minh%20Do%0AAbstract%3A%20%20%20In%20order%20to%20unlock%20the%20potential%20of%20diverse%20sensors%2C%20we%20investigate%20a%20method%0Ato%20transfer%20knowledge%20between%20modalities%20using%20the%20structure%20of%20a%20unified%0Amultimodal%20representation%20space%20for%20Human%20Action%20Recognition%20%28HAR%29.%20We%0Aformalize%20and%20explore%20an%20understudied%20cross-modal%20transfer%20setting%20we%20term%0AUnsupervised%20Modality%20Adaptation%20%28UMA%29%2C%20where%20the%20modality%20used%20in%20testing%20is%0Anot%20used%20in%20supervised%20training%2C%20i.e.%20zero%20labeled%20instances%20of%20the%20test%0Amodality%20are%20available%20during%20training.%20We%20develop%20three%20methods%20to%20perform%0AUMA%3A%20Student-Teacher%20%28ST%29%2C%20Contrastive%20Alignment%20%28CA%29%2C%20and%20Cross-modal%20Transfer%0AThrough%20Time%20%28C3T%29.%20Our%20extensive%20experiments%20on%20various%20camera%2BIMU%20datasets%0Acompare%20these%20methods%20to%20each%20other%20in%20the%20UMA%20setting%2C%20and%20to%20their%20empirical%0Aupper%20bound%20in%20the%20supervised%20setting.%20The%20results%20indicate%20C3T%20is%20the%20most%0Arobust%20and%20highest%20performing%20by%20at%20least%20a%20margin%20of%208%25%2C%20and%20nears%20the%0Asupervised%20setting%20performance%20even%20in%20the%20presence%20of%20temporal%20noise.%20This%0Amethod%20introduces%20a%20novel%20mechanism%20for%20aligning%20signals%20across%20time-varying%0Alatent%20vectors%2C%20extracted%20from%20the%20receptive%20field%20of%20temporal%20convolutions.%0AOur%20findings%20suggest%20that%20C3T%20has%20significant%20potential%20for%20developing%0Ageneralizable%20models%20for%20time-series%20sensor%20data%2C%20opening%20new%20avenues%20for%0Amulti-modal%20learning%20in%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3T%253A%2520Cross-modal%2520Transfer%2520Through%2520Time%2520for%2520Human%2520Action%2520Recognition%26entry.906535625%3DAbhi%2520Kamboj%2520and%2520Anh%2520Duy%2520Nguyen%2520and%2520Minh%2520Do%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520unlock%2520the%2520potential%2520of%2520diverse%2520sensors%252C%2520we%2520investigate%2520a%2520method%250Ato%2520transfer%2520knowledge%2520between%2520modalities%2520using%2520the%2520structure%2520of%2520a%2520unified%250Amultimodal%2520representation%2520space%2520for%2520Human%2520Action%2520Recognition%2520%2528HAR%2529.%2520We%250Aformalize%2520and%2520explore%2520an%2520understudied%2520cross-modal%2520transfer%2520setting%2520we%2520term%250AUnsupervised%2520Modality%2520Adaptation%2520%2528UMA%2529%252C%2520where%2520the%2520modality%2520used%2520in%2520testing%2520is%250Anot%2520used%2520in%2520supervised%2520training%252C%2520i.e.%2520zero%2520labeled%2520instances%2520of%2520the%2520test%250Amodality%2520are%2520available%2520during%2520training.%2520We%2520develop%2520three%2520methods%2520to%2520perform%250AUMA%253A%2520Student-Teacher%2520%2528ST%2529%252C%2520Contrastive%2520Alignment%2520%2528CA%2529%252C%2520and%2520Cross-modal%2520Transfer%250AThrough%2520Time%2520%2528C3T%2529.%2520Our%2520extensive%2520experiments%2520on%2520various%2520camera%252BIMU%2520datasets%250Acompare%2520these%2520methods%2520to%2520each%2520other%2520in%2520the%2520UMA%2520setting%252C%2520and%2520to%2520their%2520empirical%250Aupper%2520bound%2520in%2520the%2520supervised%2520setting.%2520The%2520results%2520indicate%2520C3T%2520is%2520the%2520most%250Arobust%2520and%2520highest%2520performing%2520by%2520at%2520least%2520a%2520margin%2520of%25208%2525%252C%2520and%2520nears%2520the%250Asupervised%2520setting%2520performance%2520even%2520in%2520the%2520presence%2520of%2520temporal%2520noise.%2520This%250Amethod%2520introduces%2520a%2520novel%2520mechanism%2520for%2520aligning%2520signals%2520across%2520time-varying%250Alatent%2520vectors%252C%2520extracted%2520from%2520the%2520receptive%2520field%2520of%2520temporal%2520convolutions.%250AOur%2520findings%2520suggest%2520that%2520C3T%2520has%2520significant%2520potential%2520for%2520developing%250Ageneralizable%2520models%2520for%2520time-series%2520sensor%2520data%252C%2520opening%2520new%2520avenues%2520for%250Amulti-modal%2520learning%2520in%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3T%3A%20Cross-modal%20Transfer%20Through%20Time%20for%20Human%20Action%20Recognition&entry.906535625=Abhi%20Kamboj%20and%20Anh%20Duy%20Nguyen%20and%20Minh%20Do&entry.1292438233=%20%20In%20order%20to%20unlock%20the%20potential%20of%20diverse%20sensors%2C%20we%20investigate%20a%20method%0Ato%20transfer%20knowledge%20between%20modalities%20using%20the%20structure%20of%20a%20unified%0Amultimodal%20representation%20space%20for%20Human%20Action%20Recognition%20%28HAR%29.%20We%0Aformalize%20and%20explore%20an%20understudied%20cross-modal%20transfer%20setting%20we%20term%0AUnsupervised%20Modality%20Adaptation%20%28UMA%29%2C%20where%20the%20modality%20used%20in%20testing%20is%0Anot%20used%20in%20supervised%20training%2C%20i.e.%20zero%20labeled%20instances%20of%20the%20test%0Amodality%20are%20available%20during%20training.%20We%20develop%20three%20methods%20to%20perform%0AUMA%3A%20Student-Teacher%20%28ST%29%2C%20Contrastive%20Alignment%20%28CA%29%2C%20and%20Cross-modal%20Transfer%0AThrough%20Time%20%28C3T%29.%20Our%20extensive%20experiments%20on%20various%20camera%2BIMU%20datasets%0Acompare%20these%20methods%20to%20each%20other%20in%20the%20UMA%20setting%2C%20and%20to%20their%20empirical%0Aupper%20bound%20in%20the%20supervised%20setting.%20The%20results%20indicate%20C3T%20is%20the%20most%0Arobust%20and%20highest%20performing%20by%20at%20least%20a%20margin%20of%208%25%2C%20and%20nears%20the%0Asupervised%20setting%20performance%20even%20in%20the%20presence%20of%20temporal%20noise.%20This%0Amethod%20introduces%20a%20novel%20mechanism%20for%20aligning%20signals%20across%20time-varying%0Alatent%20vectors%2C%20extracted%20from%20the%20receptive%20field%20of%20temporal%20convolutions.%0AOur%20findings%20suggest%20that%20C3T%20has%20significant%20potential%20for%20developing%0Ageneralizable%20models%20for%20time-series%20sensor%20data%2C%20opening%20new%20avenues%20for%0Amulti-modal%20learning%20in%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16803v2&entry.124074799=Read"},
{"title": "MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation", "author": "Sayan Paul and Ruddra dev Roychoudhury and Brojeshwar Bhowmick", "abstract": "  Visual odometry (VO) is essential for enabling accurate point-goal navigation\nof embodied agents in indoor environments where GPS and compass sensors are\nunreliable and inaccurate. However, traditional VO methods face challenges in\nwide-baseline scenarios, where fast robot motions and low frames per second\n(FPS) during inference hinder their performance, leading to drift and\ncatastrophic failures in point-goal navigation. Recent deep-learned VO methods\nshow robust performance but suffer from sample inefficiency during training;\nhence, they require huge datasets and compute resources. So, we propose a\nrobust and sample-efficient VO pipeline based on motion priors available while\nan agent is navigating an environment. It consists of a training-free\naction-prior based geometric VO module that estimates a coarse relative pose\nwhich is further consumed as a motion prior by a deep-learned VO model, which\nfinally produces a fine relative pose to be used by the navigation policy. This\nstrategy helps our pipeline achieve up to 2x sample efficiency during training\nand demonstrates superior accuracy and robustness in point-goal navigation\ntasks compared to state-of-the-art VO method(s). Realistic indoor environments\nof the Gibson dataset is used in the AI-Habitat simulator to evaluate the\nproposed approach using navigation metrics (like success/SPL) and pose metrics\n(like RPE/ATE). We hope this method further opens a direction of work where\nmotion priors from various sources can be utilized to improve VO estimates and\nachieve better results in embodied navigation tasks.\n", "link": "http://arxiv.org/abs/2411.04796v1", "date": "2024-11-07", "relevancy": 2.3186, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5924}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPVO%3A%20Motion-Prior%20based%20Visual%20Odometry%20for%20PointGoal%20Navigation&body=Title%3A%20MPVO%3A%20Motion-Prior%20based%20Visual%20Odometry%20for%20PointGoal%20Navigation%0AAuthor%3A%20Sayan%20Paul%20and%20Ruddra%20dev%20Roychoudhury%20and%20Brojeshwar%20Bhowmick%0AAbstract%3A%20%20%20Visual%20odometry%20%28VO%29%20is%20essential%20for%20enabling%20accurate%20point-goal%20navigation%0Aof%20embodied%20agents%20in%20indoor%20environments%20where%20GPS%20and%20compass%20sensors%20are%0Aunreliable%20and%20inaccurate.%20However%2C%20traditional%20VO%20methods%20face%20challenges%20in%0Awide-baseline%20scenarios%2C%20where%20fast%20robot%20motions%20and%20low%20frames%20per%20second%0A%28FPS%29%20during%20inference%20hinder%20their%20performance%2C%20leading%20to%20drift%20and%0Acatastrophic%20failures%20in%20point-goal%20navigation.%20Recent%20deep-learned%20VO%20methods%0Ashow%20robust%20performance%20but%20suffer%20from%20sample%20inefficiency%20during%20training%3B%0Ahence%2C%20they%20require%20huge%20datasets%20and%20compute%20resources.%20So%2C%20we%20propose%20a%0Arobust%20and%20sample-efficient%20VO%20pipeline%20based%20on%20motion%20priors%20available%20while%0Aan%20agent%20is%20navigating%20an%20environment.%20It%20consists%20of%20a%20training-free%0Aaction-prior%20based%20geometric%20VO%20module%20that%20estimates%20a%20coarse%20relative%20pose%0Awhich%20is%20further%20consumed%20as%20a%20motion%20prior%20by%20a%20deep-learned%20VO%20model%2C%20which%0Afinally%20produces%20a%20fine%20relative%20pose%20to%20be%20used%20by%20the%20navigation%20policy.%20This%0Astrategy%20helps%20our%20pipeline%20achieve%20up%20to%202x%20sample%20efficiency%20during%20training%0Aand%20demonstrates%20superior%20accuracy%20and%20robustness%20in%20point-goal%20navigation%0Atasks%20compared%20to%20state-of-the-art%20VO%20method%28s%29.%20Realistic%20indoor%20environments%0Aof%20the%20Gibson%20dataset%20is%20used%20in%20the%20AI-Habitat%20simulator%20to%20evaluate%20the%0Aproposed%20approach%20using%20navigation%20metrics%20%28like%20success/SPL%29%20and%20pose%20metrics%0A%28like%20RPE/ATE%29.%20We%20hope%20this%20method%20further%20opens%20a%20direction%20of%20work%20where%0Amotion%20priors%20from%20various%20sources%20can%20be%20utilized%20to%20improve%20VO%20estimates%20and%0Aachieve%20better%20results%20in%20embodied%20navigation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPVO%253A%2520Motion-Prior%2520based%2520Visual%2520Odometry%2520for%2520PointGoal%2520Navigation%26entry.906535625%3DSayan%2520Paul%2520and%2520Ruddra%2520dev%2520Roychoudhury%2520and%2520Brojeshwar%2520Bhowmick%26entry.1292438233%3D%2520%2520Visual%2520odometry%2520%2528VO%2529%2520is%2520essential%2520for%2520enabling%2520accurate%2520point-goal%2520navigation%250Aof%2520embodied%2520agents%2520in%2520indoor%2520environments%2520where%2520GPS%2520and%2520compass%2520sensors%2520are%250Aunreliable%2520and%2520inaccurate.%2520However%252C%2520traditional%2520VO%2520methods%2520face%2520challenges%2520in%250Awide-baseline%2520scenarios%252C%2520where%2520fast%2520robot%2520motions%2520and%2520low%2520frames%2520per%2520second%250A%2528FPS%2529%2520during%2520inference%2520hinder%2520their%2520performance%252C%2520leading%2520to%2520drift%2520and%250Acatastrophic%2520failures%2520in%2520point-goal%2520navigation.%2520Recent%2520deep-learned%2520VO%2520methods%250Ashow%2520robust%2520performance%2520but%2520suffer%2520from%2520sample%2520inefficiency%2520during%2520training%253B%250Ahence%252C%2520they%2520require%2520huge%2520datasets%2520and%2520compute%2520resources.%2520So%252C%2520we%2520propose%2520a%250Arobust%2520and%2520sample-efficient%2520VO%2520pipeline%2520based%2520on%2520motion%2520priors%2520available%2520while%250Aan%2520agent%2520is%2520navigating%2520an%2520environment.%2520It%2520consists%2520of%2520a%2520training-free%250Aaction-prior%2520based%2520geometric%2520VO%2520module%2520that%2520estimates%2520a%2520coarse%2520relative%2520pose%250Awhich%2520is%2520further%2520consumed%2520as%2520a%2520motion%2520prior%2520by%2520a%2520deep-learned%2520VO%2520model%252C%2520which%250Afinally%2520produces%2520a%2520fine%2520relative%2520pose%2520to%2520be%2520used%2520by%2520the%2520navigation%2520policy.%2520This%250Astrategy%2520helps%2520our%2520pipeline%2520achieve%2520up%2520to%25202x%2520sample%2520efficiency%2520during%2520training%250Aand%2520demonstrates%2520superior%2520accuracy%2520and%2520robustness%2520in%2520point-goal%2520navigation%250Atasks%2520compared%2520to%2520state-of-the-art%2520VO%2520method%2528s%2529.%2520Realistic%2520indoor%2520environments%250Aof%2520the%2520Gibson%2520dataset%2520is%2520used%2520in%2520the%2520AI-Habitat%2520simulator%2520to%2520evaluate%2520the%250Aproposed%2520approach%2520using%2520navigation%2520metrics%2520%2528like%2520success/SPL%2529%2520and%2520pose%2520metrics%250A%2528like%2520RPE/ATE%2529.%2520We%2520hope%2520this%2520method%2520further%2520opens%2520a%2520direction%2520of%2520work%2520where%250Amotion%2520priors%2520from%2520various%2520sources%2520can%2520be%2520utilized%2520to%2520improve%2520VO%2520estimates%2520and%250Aachieve%2520better%2520results%2520in%2520embodied%2520navigation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPVO%3A%20Motion-Prior%20based%20Visual%20Odometry%20for%20PointGoal%20Navigation&entry.906535625=Sayan%20Paul%20and%20Ruddra%20dev%20Roychoudhury%20and%20Brojeshwar%20Bhowmick&entry.1292438233=%20%20Visual%20odometry%20%28VO%29%20is%20essential%20for%20enabling%20accurate%20point-goal%20navigation%0Aof%20embodied%20agents%20in%20indoor%20environments%20where%20GPS%20and%20compass%20sensors%20are%0Aunreliable%20and%20inaccurate.%20However%2C%20traditional%20VO%20methods%20face%20challenges%20in%0Awide-baseline%20scenarios%2C%20where%20fast%20robot%20motions%20and%20low%20frames%20per%20second%0A%28FPS%29%20during%20inference%20hinder%20their%20performance%2C%20leading%20to%20drift%20and%0Acatastrophic%20failures%20in%20point-goal%20navigation.%20Recent%20deep-learned%20VO%20methods%0Ashow%20robust%20performance%20but%20suffer%20from%20sample%20inefficiency%20during%20training%3B%0Ahence%2C%20they%20require%20huge%20datasets%20and%20compute%20resources.%20So%2C%20we%20propose%20a%0Arobust%20and%20sample-efficient%20VO%20pipeline%20based%20on%20motion%20priors%20available%20while%0Aan%20agent%20is%20navigating%20an%20environment.%20It%20consists%20of%20a%20training-free%0Aaction-prior%20based%20geometric%20VO%20module%20that%20estimates%20a%20coarse%20relative%20pose%0Awhich%20is%20further%20consumed%20as%20a%20motion%20prior%20by%20a%20deep-learned%20VO%20model%2C%20which%0Afinally%20produces%20a%20fine%20relative%20pose%20to%20be%20used%20by%20the%20navigation%20policy.%20This%0Astrategy%20helps%20our%20pipeline%20achieve%20up%20to%202x%20sample%20efficiency%20during%20training%0Aand%20demonstrates%20superior%20accuracy%20and%20robustness%20in%20point-goal%20navigation%0Atasks%20compared%20to%20state-of-the-art%20VO%20method%28s%29.%20Realistic%20indoor%20environments%0Aof%20the%20Gibson%20dataset%20is%20used%20in%20the%20AI-Habitat%20simulator%20to%20evaluate%20the%0Aproposed%20approach%20using%20navigation%20metrics%20%28like%20success/SPL%29%20and%20pose%20metrics%0A%28like%20RPE/ATE%29.%20We%20hope%20this%20method%20further%20opens%20a%20direction%20of%20work%20where%0Amotion%20priors%20from%20various%20sources%20can%20be%20utilized%20to%20improve%20VO%20estimates%20and%0Aachieve%20better%20results%20in%20embodied%20navigation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04796v1&entry.124074799=Read"},
{"title": "ProEdit: Simple Progression is All You Need for High-Quality 3D Scene\n  Editing", "author": "Jun-Kun Chen and Yu-Xiong Wang", "abstract": "  This paper proposes ProEdit - a simple yet effective framework for\nhigh-quality 3D scene editing guided by diffusion distillation in a novel\nprogressive manner. Inspired by the crucial observation that multi-view\ninconsistency in scene editing is rooted in the diffusion model's large\nfeasible output space (FOS), our framework controls the size of FOS and reduces\ninconsistency by decomposing the overall editing task into several subtasks,\nwhich are then executed progressively on the scene. Within this framework, we\ndesign a difficulty-aware subtask decomposition scheduler and an adaptive 3D\nGaussian splatting (3DGS) training strategy, ensuring high quality and\nefficiency in performing each subtask. Extensive evaluation shows that our\nProEdit achieves state-of-the-art results in various scenes and challenging\nediting tasks, all through a simple framework without any expensive or\nsophisticated add-ons like distillation losses, components, or training\nprocedures. Notably, ProEdit also provides a new way to control, preview, and\nselect the \"aggressivity\" of editing operation during the editing process.\n", "link": "http://arxiv.org/abs/2411.05006v1", "date": "2024-11-07", "relevancy": 2.3152, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5821}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProEdit%3A%20Simple%20Progression%20is%20All%20You%20Need%20for%20High-Quality%203D%20Scene%0A%20%20Editing&body=Title%3A%20ProEdit%3A%20Simple%20Progression%20is%20All%20You%20Need%20for%20High-Quality%203D%20Scene%0A%20%20Editing%0AAuthor%3A%20Jun-Kun%20Chen%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20This%20paper%20proposes%20ProEdit%20-%20a%20simple%20yet%20effective%20framework%20for%0Ahigh-quality%203D%20scene%20editing%20guided%20by%20diffusion%20distillation%20in%20a%20novel%0Aprogressive%20manner.%20Inspired%20by%20the%20crucial%20observation%20that%20multi-view%0Ainconsistency%20in%20scene%20editing%20is%20rooted%20in%20the%20diffusion%20model%27s%20large%0Afeasible%20output%20space%20%28FOS%29%2C%20our%20framework%20controls%20the%20size%20of%20FOS%20and%20reduces%0Ainconsistency%20by%20decomposing%20the%20overall%20editing%20task%20into%20several%20subtasks%2C%0Awhich%20are%20then%20executed%20progressively%20on%20the%20scene.%20Within%20this%20framework%2C%20we%0Adesign%20a%20difficulty-aware%20subtask%20decomposition%20scheduler%20and%20an%20adaptive%203D%0AGaussian%20splatting%20%283DGS%29%20training%20strategy%2C%20ensuring%20high%20quality%20and%0Aefficiency%20in%20performing%20each%20subtask.%20Extensive%20evaluation%20shows%20that%20our%0AProEdit%20achieves%20state-of-the-art%20results%20in%20various%20scenes%20and%20challenging%0Aediting%20tasks%2C%20all%20through%20a%20simple%20framework%20without%20any%20expensive%20or%0Asophisticated%20add-ons%20like%20distillation%20losses%2C%20components%2C%20or%20training%0Aprocedures.%20Notably%2C%20ProEdit%20also%20provides%20a%20new%20way%20to%20control%2C%20preview%2C%20and%0Aselect%20the%20%22aggressivity%22%20of%20editing%20operation%20during%20the%20editing%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProEdit%253A%2520Simple%2520Progression%2520is%2520All%2520You%2520Need%2520for%2520High-Quality%25203D%2520Scene%250A%2520%2520Editing%26entry.906535625%3DJun-Kun%2520Chen%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520ProEdit%2520-%2520a%2520simple%2520yet%2520effective%2520framework%2520for%250Ahigh-quality%25203D%2520scene%2520editing%2520guided%2520by%2520diffusion%2520distillation%2520in%2520a%2520novel%250Aprogressive%2520manner.%2520Inspired%2520by%2520the%2520crucial%2520observation%2520that%2520multi-view%250Ainconsistency%2520in%2520scene%2520editing%2520is%2520rooted%2520in%2520the%2520diffusion%2520model%2527s%2520large%250Afeasible%2520output%2520space%2520%2528FOS%2529%252C%2520our%2520framework%2520controls%2520the%2520size%2520of%2520FOS%2520and%2520reduces%250Ainconsistency%2520by%2520decomposing%2520the%2520overall%2520editing%2520task%2520into%2520several%2520subtasks%252C%250Awhich%2520are%2520then%2520executed%2520progressively%2520on%2520the%2520scene.%2520Within%2520this%2520framework%252C%2520we%250Adesign%2520a%2520difficulty-aware%2520subtask%2520decomposition%2520scheduler%2520and%2520an%2520adaptive%25203D%250AGaussian%2520splatting%2520%25283DGS%2529%2520training%2520strategy%252C%2520ensuring%2520high%2520quality%2520and%250Aefficiency%2520in%2520performing%2520each%2520subtask.%2520Extensive%2520evaluation%2520shows%2520that%2520our%250AProEdit%2520achieves%2520state-of-the-art%2520results%2520in%2520various%2520scenes%2520and%2520challenging%250Aediting%2520tasks%252C%2520all%2520through%2520a%2520simple%2520framework%2520without%2520any%2520expensive%2520or%250Asophisticated%2520add-ons%2520like%2520distillation%2520losses%252C%2520components%252C%2520or%2520training%250Aprocedures.%2520Notably%252C%2520ProEdit%2520also%2520provides%2520a%2520new%2520way%2520to%2520control%252C%2520preview%252C%2520and%250Aselect%2520the%2520%2522aggressivity%2522%2520of%2520editing%2520operation%2520during%2520the%2520editing%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProEdit%3A%20Simple%20Progression%20is%20All%20You%20Need%20for%20High-Quality%203D%20Scene%0A%20%20Editing&entry.906535625=Jun-Kun%20Chen%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20This%20paper%20proposes%20ProEdit%20-%20a%20simple%20yet%20effective%20framework%20for%0Ahigh-quality%203D%20scene%20editing%20guided%20by%20diffusion%20distillation%20in%20a%20novel%0Aprogressive%20manner.%20Inspired%20by%20the%20crucial%20observation%20that%20multi-view%0Ainconsistency%20in%20scene%20editing%20is%20rooted%20in%20the%20diffusion%20model%27s%20large%0Afeasible%20output%20space%20%28FOS%29%2C%20our%20framework%20controls%20the%20size%20of%20FOS%20and%20reduces%0Ainconsistency%20by%20decomposing%20the%20overall%20editing%20task%20into%20several%20subtasks%2C%0Awhich%20are%20then%20executed%20progressively%20on%20the%20scene.%20Within%20this%20framework%2C%20we%0Adesign%20a%20difficulty-aware%20subtask%20decomposition%20scheduler%20and%20an%20adaptive%203D%0AGaussian%20splatting%20%283DGS%29%20training%20strategy%2C%20ensuring%20high%20quality%20and%0Aefficiency%20in%20performing%20each%20subtask.%20Extensive%20evaluation%20shows%20that%20our%0AProEdit%20achieves%20state-of-the-art%20results%20in%20various%20scenes%20and%20challenging%0Aediting%20tasks%2C%20all%20through%20a%20simple%20framework%20without%20any%20expensive%20or%0Asophisticated%20add-ons%20like%20distillation%20losses%2C%20components%2C%20or%20training%0Aprocedures.%20Notably%2C%20ProEdit%20also%20provides%20a%20new%20way%20to%20control%2C%20preview%2C%20and%0Aselect%20the%20%22aggressivity%22%20of%20editing%20operation%20during%20the%20editing%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05006v1&entry.124074799=Read"},
{"title": "Social EgoMesh Estimation", "author": "Luca Scofano and Alessio Sampieri and Edoardo De Matteis and Indro Spinelli and Fabio Galasso", "abstract": "  Accurately estimating the 3D pose of the camera wearer in egocentric video\nsequences is crucial to modeling human behavior in virtual and augmented\nreality applications. The task presents unique challenges due to the limited\nvisibility of the user's body caused by the front-facing camera mounted on\ntheir head. Recent research has explored the utilization of the scene and\nego-motion, but it has overlooked humans' interactive nature. We propose a\nnovel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our\napproach is the first to estimate the wearer's mesh using only a latent\nprobabilistic diffusion model, which we condition on the scene and, for the\nfirst time, on the social wearer-interactee interactions. Our in-depth study\nsheds light on when social interaction matters most for ego-mesh estimation; it\nquantifies the impact of interpersonal distance and gaze direction. Overall,\nSEE-ME surpasses the current best technique, reducing the pose estimation error\n(MPJPE) by 53%. The code is available at https://github.com/L-Scofano/SEEME.\n", "link": "http://arxiv.org/abs/2411.04598v1", "date": "2024-11-07", "relevancy": 2.3084, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5861}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5715}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20EgoMesh%20Estimation&body=Title%3A%20Social%20EgoMesh%20Estimation%0AAuthor%3A%20Luca%20Scofano%20and%20Alessio%20Sampieri%20and%20Edoardo%20De%20Matteis%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso%0AAbstract%3A%20%20%20Accurately%20estimating%20the%203D%20pose%20of%20the%20camera%20wearer%20in%20egocentric%20video%0Asequences%20is%20crucial%20to%20modeling%20human%20behavior%20in%20virtual%20and%20augmented%0Areality%20applications.%20The%20task%20presents%20unique%20challenges%20due%20to%20the%20limited%0Avisibility%20of%20the%20user%27s%20body%20caused%20by%20the%20front-facing%20camera%20mounted%20on%0Atheir%20head.%20Recent%20research%20has%20explored%20the%20utilization%20of%20the%20scene%20and%0Aego-motion%2C%20but%20it%20has%20overlooked%20humans%27%20interactive%20nature.%20We%20propose%20a%0Anovel%20framework%20for%20Social%20Egocentric%20Estimation%20of%20body%20MEshes%20%28SEE-ME%29.%20Our%0Aapproach%20is%20the%20first%20to%20estimate%20the%20wearer%27s%20mesh%20using%20only%20a%20latent%0Aprobabilistic%20diffusion%20model%2C%20which%20we%20condition%20on%20the%20scene%20and%2C%20for%20the%0Afirst%20time%2C%20on%20the%20social%20wearer-interactee%20interactions.%20Our%20in-depth%20study%0Asheds%20light%20on%20when%20social%20interaction%20matters%20most%20for%20ego-mesh%20estimation%3B%20it%0Aquantifies%20the%20impact%20of%20interpersonal%20distance%20and%20gaze%20direction.%20Overall%2C%0ASEE-ME%20surpasses%20the%20current%20best%20technique%2C%20reducing%20the%20pose%20estimation%20error%0A%28MPJPE%29%20by%2053%25.%20The%20code%20is%20available%20at%20https%3A//github.com/L-Scofano/SEEME.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520EgoMesh%2520Estimation%26entry.906535625%3DLuca%2520Scofano%2520and%2520Alessio%2520Sampieri%2520and%2520Edoardo%2520De%2520Matteis%2520and%2520Indro%2520Spinelli%2520and%2520Fabio%2520Galasso%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520the%25203D%2520pose%2520of%2520the%2520camera%2520wearer%2520in%2520egocentric%2520video%250Asequences%2520is%2520crucial%2520to%2520modeling%2520human%2520behavior%2520in%2520virtual%2520and%2520augmented%250Areality%2520applications.%2520The%2520task%2520presents%2520unique%2520challenges%2520due%2520to%2520the%2520limited%250Avisibility%2520of%2520the%2520user%2527s%2520body%2520caused%2520by%2520the%2520front-facing%2520camera%2520mounted%2520on%250Atheir%2520head.%2520Recent%2520research%2520has%2520explored%2520the%2520utilization%2520of%2520the%2520scene%2520and%250Aego-motion%252C%2520but%2520it%2520has%2520overlooked%2520humans%2527%2520interactive%2520nature.%2520We%2520propose%2520a%250Anovel%2520framework%2520for%2520Social%2520Egocentric%2520Estimation%2520of%2520body%2520MEshes%2520%2528SEE-ME%2529.%2520Our%250Aapproach%2520is%2520the%2520first%2520to%2520estimate%2520the%2520wearer%2527s%2520mesh%2520using%2520only%2520a%2520latent%250Aprobabilistic%2520diffusion%2520model%252C%2520which%2520we%2520condition%2520on%2520the%2520scene%2520and%252C%2520for%2520the%250Afirst%2520time%252C%2520on%2520the%2520social%2520wearer-interactee%2520interactions.%2520Our%2520in-depth%2520study%250Asheds%2520light%2520on%2520when%2520social%2520interaction%2520matters%2520most%2520for%2520ego-mesh%2520estimation%253B%2520it%250Aquantifies%2520the%2520impact%2520of%2520interpersonal%2520distance%2520and%2520gaze%2520direction.%2520Overall%252C%250ASEE-ME%2520surpasses%2520the%2520current%2520best%2520technique%252C%2520reducing%2520the%2520pose%2520estimation%2520error%250A%2528MPJPE%2529%2520by%252053%2525.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/L-Scofano/SEEME.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20EgoMesh%20Estimation&entry.906535625=Luca%20Scofano%20and%20Alessio%20Sampieri%20and%20Edoardo%20De%20Matteis%20and%20Indro%20Spinelli%20and%20Fabio%20Galasso&entry.1292438233=%20%20Accurately%20estimating%20the%203D%20pose%20of%20the%20camera%20wearer%20in%20egocentric%20video%0Asequences%20is%20crucial%20to%20modeling%20human%20behavior%20in%20virtual%20and%20augmented%0Areality%20applications.%20The%20task%20presents%20unique%20challenges%20due%20to%20the%20limited%0Avisibility%20of%20the%20user%27s%20body%20caused%20by%20the%20front-facing%20camera%20mounted%20on%0Atheir%20head.%20Recent%20research%20has%20explored%20the%20utilization%20of%20the%20scene%20and%0Aego-motion%2C%20but%20it%20has%20overlooked%20humans%27%20interactive%20nature.%20We%20propose%20a%0Anovel%20framework%20for%20Social%20Egocentric%20Estimation%20of%20body%20MEshes%20%28SEE-ME%29.%20Our%0Aapproach%20is%20the%20first%20to%20estimate%20the%20wearer%27s%20mesh%20using%20only%20a%20latent%0Aprobabilistic%20diffusion%20model%2C%20which%20we%20condition%20on%20the%20scene%20and%2C%20for%20the%0Afirst%20time%2C%20on%20the%20social%20wearer-interactee%20interactions.%20Our%20in-depth%20study%0Asheds%20light%20on%20when%20social%20interaction%20matters%20most%20for%20ego-mesh%20estimation%3B%20it%0Aquantifies%20the%20impact%20of%20interpersonal%20distance%20and%20gaze%20direction.%20Overall%2C%0ASEE-ME%20surpasses%20the%20current%20best%20technique%2C%20reducing%20the%20pose%20estimation%20error%0A%28MPJPE%29%20by%2053%25.%20The%20code%20is%20available%20at%20https%3A//github.com/L-Scofano/SEEME.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04598v1&entry.124074799=Read"},
{"title": "An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation", "author": "Quan Huu Cap", "abstract": "  Whole-slide images (WSI) glomerulus segmentation is essential for accurately\ndiagnosing kidney diseases. In this work, we propose a practical pipeline for\nglomerulus segmentation that effectively enhances both patch-level and\nWSI-level segmentation tasks. Our approach leverages stitching on overlapping\npatches, increasing the detection coverage, especially when glomeruli are\nlocated near patch image borders. In addition, we conduct comprehensive\nevaluations from different segmentation models across two large and diverse\ndatasets with over 30K glomerulus annotations. Experimental results demonstrate\nthat models using our pipeline outperform the previous state-of-the-art method,\nachieving superior results across both datasets and setting a new benchmark for\nglomerulus segmentation in WSIs. The code and pre-trained models are available\nat https://github.com/huuquan1994/wsi_glomerulus_seg.\n", "link": "http://arxiv.org/abs/2411.04782v1", "date": "2024-11-07", "relevancy": 2.307, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Effective%20Pipeline%20for%20Whole-Slide%20Image%20Glomerulus%20Segmentation&body=Title%3A%20An%20Effective%20Pipeline%20for%20Whole-Slide%20Image%20Glomerulus%20Segmentation%0AAuthor%3A%20Quan%20Huu%20Cap%0AAbstract%3A%20%20%20Whole-slide%20images%20%28WSI%29%20glomerulus%20segmentation%20is%20essential%20for%20accurately%0Adiagnosing%20kidney%20diseases.%20In%20this%20work%2C%20we%20propose%20a%20practical%20pipeline%20for%0Aglomerulus%20segmentation%20that%20effectively%20enhances%20both%20patch-level%20and%0AWSI-level%20segmentation%20tasks.%20Our%20approach%20leverages%20stitching%20on%20overlapping%0Apatches%2C%20increasing%20the%20detection%20coverage%2C%20especially%20when%20glomeruli%20are%0Alocated%20near%20patch%20image%20borders.%20In%20addition%2C%20we%20conduct%20comprehensive%0Aevaluations%20from%20different%20segmentation%20models%20across%20two%20large%20and%20diverse%0Adatasets%20with%20over%2030K%20glomerulus%20annotations.%20Experimental%20results%20demonstrate%0Athat%20models%20using%20our%20pipeline%20outperform%20the%20previous%20state-of-the-art%20method%2C%0Aachieving%20superior%20results%20across%20both%20datasets%20and%20setting%20a%20new%20benchmark%20for%0Aglomerulus%20segmentation%20in%20WSIs.%20The%20code%20and%20pre-trained%20models%20are%20available%0Aat%20https%3A//github.com/huuquan1994/wsi_glomerulus_seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Effective%2520Pipeline%2520for%2520Whole-Slide%2520Image%2520Glomerulus%2520Segmentation%26entry.906535625%3DQuan%2520Huu%2520Cap%26entry.1292438233%3D%2520%2520Whole-slide%2520images%2520%2528WSI%2529%2520glomerulus%2520segmentation%2520is%2520essential%2520for%2520accurately%250Adiagnosing%2520kidney%2520diseases.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520practical%2520pipeline%2520for%250Aglomerulus%2520segmentation%2520that%2520effectively%2520enhances%2520both%2520patch-level%2520and%250AWSI-level%2520segmentation%2520tasks.%2520Our%2520approach%2520leverages%2520stitching%2520on%2520overlapping%250Apatches%252C%2520increasing%2520the%2520detection%2520coverage%252C%2520especially%2520when%2520glomeruli%2520are%250Alocated%2520near%2520patch%2520image%2520borders.%2520In%2520addition%252C%2520we%2520conduct%2520comprehensive%250Aevaluations%2520from%2520different%2520segmentation%2520models%2520across%2520two%2520large%2520and%2520diverse%250Adatasets%2520with%2520over%252030K%2520glomerulus%2520annotations.%2520Experimental%2520results%2520demonstrate%250Athat%2520models%2520using%2520our%2520pipeline%2520outperform%2520the%2520previous%2520state-of-the-art%2520method%252C%250Aachieving%2520superior%2520results%2520across%2520both%2520datasets%2520and%2520setting%2520a%2520new%2520benchmark%2520for%250Aglomerulus%2520segmentation%2520in%2520WSIs.%2520The%2520code%2520and%2520pre-trained%2520models%2520are%2520available%250Aat%2520https%253A//github.com/huuquan1994/wsi_glomerulus_seg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Effective%20Pipeline%20for%20Whole-Slide%20Image%20Glomerulus%20Segmentation&entry.906535625=Quan%20Huu%20Cap&entry.1292438233=%20%20Whole-slide%20images%20%28WSI%29%20glomerulus%20segmentation%20is%20essential%20for%20accurately%0Adiagnosing%20kidney%20diseases.%20In%20this%20work%2C%20we%20propose%20a%20practical%20pipeline%20for%0Aglomerulus%20segmentation%20that%20effectively%20enhances%20both%20patch-level%20and%0AWSI-level%20segmentation%20tasks.%20Our%20approach%20leverages%20stitching%20on%20overlapping%0Apatches%2C%20increasing%20the%20detection%20coverage%2C%20especially%20when%20glomeruli%20are%0Alocated%20near%20patch%20image%20borders.%20In%20addition%2C%20we%20conduct%20comprehensive%0Aevaluations%20from%20different%20segmentation%20models%20across%20two%20large%20and%20diverse%0Adatasets%20with%20over%2030K%20glomerulus%20annotations.%20Experimental%20results%20demonstrate%0Athat%20models%20using%20our%20pipeline%20outperform%20the%20previous%20state-of-the-art%20method%2C%0Aachieving%20superior%20results%20across%20both%20datasets%20and%20setting%20a%20new%20benchmark%20for%0Aglomerulus%20segmentation%20in%20WSIs.%20The%20code%20and%20pre-trained%20models%20are%20available%0Aat%20https%3A//github.com/huuquan1994/wsi_glomerulus_seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04782v1&entry.124074799=Read"},
{"title": "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM", "author": "Jingwei Xu and Chenyu Wang and Zibo Zhao and Wen Liu and Yi Ma and Shenghua Gao", "abstract": "  This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/\n", "link": "http://arxiv.org/abs/2411.04954v1", "date": "2024-11-07", "relevancy": 2.3021, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5829}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-MLLM%3A%20Unifying%20Multimodality-Conditioned%20CAD%20Generation%20With%20MLLM&body=Title%3A%20CAD-MLLM%3A%20Unifying%20Multimodality-Conditioned%20CAD%20Generation%20With%20MLLM%0AAuthor%3A%20Jingwei%20Xu%20and%20Chenyu%20Wang%20and%20Zibo%20Zhao%20and%20Wen%20Liu%20and%20Yi%20Ma%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20design%20a%20unified%20Computer-Aided%20Design%20%28CAD%29%20generation%0Asystem%20that%20can%20easily%20generate%20CAD%20models%20based%20on%20the%20user%27s%20inputs%20in%20the%0Aform%20of%20textual%20description%2C%20images%2C%20point%20clouds%2C%20or%20even%20a%20combination%20of%0Athem.%20Towards%20this%20goal%2C%20we%20introduce%20the%20CAD-MLLM%2C%20the%20first%20system%20capable%20of%0Agenerating%20parametric%20CAD%20models%20conditioned%20on%20the%20multimodal%20input.%0ASpecifically%2C%20within%20the%20CAD-MLLM%20framework%2C%20we%20leverage%20the%20command%20sequences%0Aof%20CAD%20models%20and%20then%20employ%20advanced%20large%20language%20models%20%28LLMs%29%20to%20align%0Athe%20feature%20space%20across%20these%20diverse%20multi-modalities%20data%20and%20CAD%20models%27%0Avectorized%20representations.%20To%20facilitate%20the%20model%20training%2C%20we%20design%20a%0Acomprehensive%20data%20construction%20and%20annotation%20pipeline%20that%20equips%20each%20CAD%0Amodel%20with%20corresponding%20multimodal%20data.%20Our%20resulting%20dataset%2C%20named%0AOmni-CAD%2C%20is%20the%20first%20multimodal%20CAD%20dataset%20that%20contains%20textual%0Adescription%2C%20multi-view%20images%2C%20points%2C%20and%20command%20sequence%20for%20each%20CAD%0Amodel.%20It%20contains%20approximately%20450K%20instances%20and%20their%20CAD%20construction%0Asequences.%20To%20thoroughly%20evaluate%20the%20quality%20of%20our%20generated%20CAD%20models%2C%20we%0Ago%20beyond%20current%20evaluation%20metrics%20that%20focus%20on%20reconstruction%20quality%20by%0Aintroducing%20additional%20metrics%20that%20assess%20topology%20quality%20and%20surface%0Aenclosure%20extent.%20Extensive%20experimental%20results%20demonstrate%20that%20CAD-MLLM%0Asignificantly%20outperforms%20existing%20conditional%20generative%20methods%20and%20remains%0Ahighly%20robust%20to%20noises%20and%20missing%20points.%20The%20project%20page%20and%20more%0Avisualizations%20can%20be%20found%20at%3A%20https%3A//cad-mllm.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-MLLM%253A%2520Unifying%2520Multimodality-Conditioned%2520CAD%2520Generation%2520With%2520MLLM%26entry.906535625%3DJingwei%2520Xu%2520and%2520Chenyu%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Wen%2520Liu%2520and%2520Yi%2520Ma%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520design%2520a%2520unified%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520generation%250Asystem%2520that%2520can%2520easily%2520generate%2520CAD%2520models%2520based%2520on%2520the%2520user%2527s%2520inputs%2520in%2520the%250Aform%2520of%2520textual%2520description%252C%2520images%252C%2520point%2520clouds%252C%2520or%2520even%2520a%2520combination%2520of%250Athem.%2520Towards%2520this%2520goal%252C%2520we%2520introduce%2520the%2520CAD-MLLM%252C%2520the%2520first%2520system%2520capable%2520of%250Agenerating%2520parametric%2520CAD%2520models%2520conditioned%2520on%2520the%2520multimodal%2520input.%250ASpecifically%252C%2520within%2520the%2520CAD-MLLM%2520framework%252C%2520we%2520leverage%2520the%2520command%2520sequences%250Aof%2520CAD%2520models%2520and%2520then%2520employ%2520advanced%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520align%250Athe%2520feature%2520space%2520across%2520these%2520diverse%2520multi-modalities%2520data%2520and%2520CAD%2520models%2527%250Avectorized%2520representations.%2520To%2520facilitate%2520the%2520model%2520training%252C%2520we%2520design%2520a%250Acomprehensive%2520data%2520construction%2520and%2520annotation%2520pipeline%2520that%2520equips%2520each%2520CAD%250Amodel%2520with%2520corresponding%2520multimodal%2520data.%2520Our%2520resulting%2520dataset%252C%2520named%250AOmni-CAD%252C%2520is%2520the%2520first%2520multimodal%2520CAD%2520dataset%2520that%2520contains%2520textual%250Adescription%252C%2520multi-view%2520images%252C%2520points%252C%2520and%2520command%2520sequence%2520for%2520each%2520CAD%250Amodel.%2520It%2520contains%2520approximately%2520450K%2520instances%2520and%2520their%2520CAD%2520construction%250Asequences.%2520To%2520thoroughly%2520evaluate%2520the%2520quality%2520of%2520our%2520generated%2520CAD%2520models%252C%2520we%250Ago%2520beyond%2520current%2520evaluation%2520metrics%2520that%2520focus%2520on%2520reconstruction%2520quality%2520by%250Aintroducing%2520additional%2520metrics%2520that%2520assess%2520topology%2520quality%2520and%2520surface%250Aenclosure%2520extent.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520CAD-MLLM%250Asignificantly%2520outperforms%2520existing%2520conditional%2520generative%2520methods%2520and%2520remains%250Ahighly%2520robust%2520to%2520noises%2520and%2520missing%2520points.%2520The%2520project%2520page%2520and%2520more%250Avisualizations%2520can%2520be%2520found%2520at%253A%2520https%253A//cad-mllm.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-MLLM%3A%20Unifying%20Multimodality-Conditioned%20CAD%20Generation%20With%20MLLM&entry.906535625=Jingwei%20Xu%20and%20Chenyu%20Wang%20and%20Zibo%20Zhao%20and%20Wen%20Liu%20and%20Yi%20Ma%20and%20Shenghua%20Gao&entry.1292438233=%20%20This%20paper%20aims%20to%20design%20a%20unified%20Computer-Aided%20Design%20%28CAD%29%20generation%0Asystem%20that%20can%20easily%20generate%20CAD%20models%20based%20on%20the%20user%27s%20inputs%20in%20the%0Aform%20of%20textual%20description%2C%20images%2C%20point%20clouds%2C%20or%20even%20a%20combination%20of%0Athem.%20Towards%20this%20goal%2C%20we%20introduce%20the%20CAD-MLLM%2C%20the%20first%20system%20capable%20of%0Agenerating%20parametric%20CAD%20models%20conditioned%20on%20the%20multimodal%20input.%0ASpecifically%2C%20within%20the%20CAD-MLLM%20framework%2C%20we%20leverage%20the%20command%20sequences%0Aof%20CAD%20models%20and%20then%20employ%20advanced%20large%20language%20models%20%28LLMs%29%20to%20align%0Athe%20feature%20space%20across%20these%20diverse%20multi-modalities%20data%20and%20CAD%20models%27%0Avectorized%20representations.%20To%20facilitate%20the%20model%20training%2C%20we%20design%20a%0Acomprehensive%20data%20construction%20and%20annotation%20pipeline%20that%20equips%20each%20CAD%0Amodel%20with%20corresponding%20multimodal%20data.%20Our%20resulting%20dataset%2C%20named%0AOmni-CAD%2C%20is%20the%20first%20multimodal%20CAD%20dataset%20that%20contains%20textual%0Adescription%2C%20multi-view%20images%2C%20points%2C%20and%20command%20sequence%20for%20each%20CAD%0Amodel.%20It%20contains%20approximately%20450K%20instances%20and%20their%20CAD%20construction%0Asequences.%20To%20thoroughly%20evaluate%20the%20quality%20of%20our%20generated%20CAD%20models%2C%20we%0Ago%20beyond%20current%20evaluation%20metrics%20that%20focus%20on%20reconstruction%20quality%20by%0Aintroducing%20additional%20metrics%20that%20assess%20topology%20quality%20and%20surface%0Aenclosure%20extent.%20Extensive%20experimental%20results%20demonstrate%20that%20CAD-MLLM%0Asignificantly%20outperforms%20existing%20conditional%20generative%20methods%20and%20remains%0Ahighly%20robust%20to%20noises%20and%20missing%20points.%20The%20project%20page%20and%20more%0Avisualizations%20can%20be%20found%20at%3A%20https%3A//cad-mllm.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04954v1&entry.124074799=Read"},
{"title": "Development of a Service Robot for Hospital Environments in\n  Rehabilitation Medicine with LiDAR Based Simultaneous Localization and\n  Mapping", "author": "Sayat Ibrayev and Arman Ibrayeva and Bekzat Amanov and Serik Tolenov", "abstract": "  This paper presents the development and evaluation of a medical service robot\nequipped with 3D LiDAR and advanced localization capabilities for use in\nhospital environments. The robot employs LiDAR-based Simultaneous Localization\nand Mapping SLAM to navigate autonomously and interact effectively within\ncomplex and dynamic healthcare settings. A comparative analysis with\nestablished 3D SLAM technology in Autoware version 1.14.0, under a Linux ROS\nframework, provided a benchmark for evaluating our system performance. The\nadaptation of Normal Distribution Transform NDT Matching to indoor navigation\nallowed for precise real-time mapping and enhanced obstacle avoidance\ncapabilities. Empirical validation was conducted through manual maneuvers in\nvarious environments, supplemented by ROS simulations to test the system\nresponse to simulated challenges. The findings demonstrate that the robot\nintegration of 3D LiDAR and NDT Matching significantly improves navigation\naccuracy and operational reliability in a healthcare context. This study\nhighlights the robot ability to perform essential tasks with high efficiency\nand identifies potential areas for further improvement, particularly in sensor\nperformance under diverse environmental conditions. The successful deployment\nof this technology in a hospital setting illustrates its potential to support\nmedical staff and contribute to patient care, suggesting a promising direction\nfor future research and development in healthcare robotics.\n", "link": "http://arxiv.org/abs/2411.04797v1", "date": "2024-11-07", "relevancy": 2.2917, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5643}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20a%20Service%20Robot%20for%20Hospital%20Environments%20in%0A%20%20Rehabilitation%20Medicine%20with%20LiDAR%20Based%20Simultaneous%20Localization%20and%0A%20%20Mapping&body=Title%3A%20Development%20of%20a%20Service%20Robot%20for%20Hospital%20Environments%20in%0A%20%20Rehabilitation%20Medicine%20with%20LiDAR%20Based%20Simultaneous%20Localization%20and%0A%20%20Mapping%0AAuthor%3A%20Sayat%20Ibrayev%20and%20Arman%20Ibrayeva%20and%20Bekzat%20Amanov%20and%20Serik%20Tolenov%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20development%20and%20evaluation%20of%20a%20medical%20service%20robot%0Aequipped%20with%203D%20LiDAR%20and%20advanced%20localization%20capabilities%20for%20use%20in%0Ahospital%20environments.%20The%20robot%20employs%20LiDAR-based%20Simultaneous%20Localization%0Aand%20Mapping%20SLAM%20to%20navigate%20autonomously%20and%20interact%20effectively%20within%0Acomplex%20and%20dynamic%20healthcare%20settings.%20A%20comparative%20analysis%20with%0Aestablished%203D%20SLAM%20technology%20in%20Autoware%20version%201.14.0%2C%20under%20a%20Linux%20ROS%0Aframework%2C%20provided%20a%20benchmark%20for%20evaluating%20our%20system%20performance.%20The%0Aadaptation%20of%20Normal%20Distribution%20Transform%20NDT%20Matching%20to%20indoor%20navigation%0Aallowed%20for%20precise%20real-time%20mapping%20and%20enhanced%20obstacle%20avoidance%0Acapabilities.%20Empirical%20validation%20was%20conducted%20through%20manual%20maneuvers%20in%0Avarious%20environments%2C%20supplemented%20by%20ROS%20simulations%20to%20test%20the%20system%0Aresponse%20to%20simulated%20challenges.%20The%20findings%20demonstrate%20that%20the%20robot%0Aintegration%20of%203D%20LiDAR%20and%20NDT%20Matching%20significantly%20improves%20navigation%0Aaccuracy%20and%20operational%20reliability%20in%20a%20healthcare%20context.%20This%20study%0Ahighlights%20the%20robot%20ability%20to%20perform%20essential%20tasks%20with%20high%20efficiency%0Aand%20identifies%20potential%20areas%20for%20further%20improvement%2C%20particularly%20in%20sensor%0Aperformance%20under%20diverse%20environmental%20conditions.%20The%20successful%20deployment%0Aof%20this%20technology%20in%20a%20hospital%20setting%20illustrates%20its%20potential%20to%20support%0Amedical%20staff%20and%20contribute%20to%20patient%20care%2C%20suggesting%20a%20promising%20direction%0Afor%20future%20research%20and%20development%20in%20healthcare%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520a%2520Service%2520Robot%2520for%2520Hospital%2520Environments%2520in%250A%2520%2520Rehabilitation%2520Medicine%2520with%2520LiDAR%2520Based%2520Simultaneous%2520Localization%2520and%250A%2520%2520Mapping%26entry.906535625%3DSayat%2520Ibrayev%2520and%2520Arman%2520Ibrayeva%2520and%2520Bekzat%2520Amanov%2520and%2520Serik%2520Tolenov%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520development%2520and%2520evaluation%2520of%2520a%2520medical%2520service%2520robot%250Aequipped%2520with%25203D%2520LiDAR%2520and%2520advanced%2520localization%2520capabilities%2520for%2520use%2520in%250Ahospital%2520environments.%2520The%2520robot%2520employs%2520LiDAR-based%2520Simultaneous%2520Localization%250Aand%2520Mapping%2520SLAM%2520to%2520navigate%2520autonomously%2520and%2520interact%2520effectively%2520within%250Acomplex%2520and%2520dynamic%2520healthcare%2520settings.%2520A%2520comparative%2520analysis%2520with%250Aestablished%25203D%2520SLAM%2520technology%2520in%2520Autoware%2520version%25201.14.0%252C%2520under%2520a%2520Linux%2520ROS%250Aframework%252C%2520provided%2520a%2520benchmark%2520for%2520evaluating%2520our%2520system%2520performance.%2520The%250Aadaptation%2520of%2520Normal%2520Distribution%2520Transform%2520NDT%2520Matching%2520to%2520indoor%2520navigation%250Aallowed%2520for%2520precise%2520real-time%2520mapping%2520and%2520enhanced%2520obstacle%2520avoidance%250Acapabilities.%2520Empirical%2520validation%2520was%2520conducted%2520through%2520manual%2520maneuvers%2520in%250Avarious%2520environments%252C%2520supplemented%2520by%2520ROS%2520simulations%2520to%2520test%2520the%2520system%250Aresponse%2520to%2520simulated%2520challenges.%2520The%2520findings%2520demonstrate%2520that%2520the%2520robot%250Aintegration%2520of%25203D%2520LiDAR%2520and%2520NDT%2520Matching%2520significantly%2520improves%2520navigation%250Aaccuracy%2520and%2520operational%2520reliability%2520in%2520a%2520healthcare%2520context.%2520This%2520study%250Ahighlights%2520the%2520robot%2520ability%2520to%2520perform%2520essential%2520tasks%2520with%2520high%2520efficiency%250Aand%2520identifies%2520potential%2520areas%2520for%2520further%2520improvement%252C%2520particularly%2520in%2520sensor%250Aperformance%2520under%2520diverse%2520environmental%2520conditions.%2520The%2520successful%2520deployment%250Aof%2520this%2520technology%2520in%2520a%2520hospital%2520setting%2520illustrates%2520its%2520potential%2520to%2520support%250Amedical%2520staff%2520and%2520contribute%2520to%2520patient%2520care%252C%2520suggesting%2520a%2520promising%2520direction%250Afor%2520future%2520research%2520and%2520development%2520in%2520healthcare%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20a%20Service%20Robot%20for%20Hospital%20Environments%20in%0A%20%20Rehabilitation%20Medicine%20with%20LiDAR%20Based%20Simultaneous%20Localization%20and%0A%20%20Mapping&entry.906535625=Sayat%20Ibrayev%20and%20Arman%20Ibrayeva%20and%20Bekzat%20Amanov%20and%20Serik%20Tolenov&entry.1292438233=%20%20This%20paper%20presents%20the%20development%20and%20evaluation%20of%20a%20medical%20service%20robot%0Aequipped%20with%203D%20LiDAR%20and%20advanced%20localization%20capabilities%20for%20use%20in%0Ahospital%20environments.%20The%20robot%20employs%20LiDAR-based%20Simultaneous%20Localization%0Aand%20Mapping%20SLAM%20to%20navigate%20autonomously%20and%20interact%20effectively%20within%0Acomplex%20and%20dynamic%20healthcare%20settings.%20A%20comparative%20analysis%20with%0Aestablished%203D%20SLAM%20technology%20in%20Autoware%20version%201.14.0%2C%20under%20a%20Linux%20ROS%0Aframework%2C%20provided%20a%20benchmark%20for%20evaluating%20our%20system%20performance.%20The%0Aadaptation%20of%20Normal%20Distribution%20Transform%20NDT%20Matching%20to%20indoor%20navigation%0Aallowed%20for%20precise%20real-time%20mapping%20and%20enhanced%20obstacle%20avoidance%0Acapabilities.%20Empirical%20validation%20was%20conducted%20through%20manual%20maneuvers%20in%0Avarious%20environments%2C%20supplemented%20by%20ROS%20simulations%20to%20test%20the%20system%0Aresponse%20to%20simulated%20challenges.%20The%20findings%20demonstrate%20that%20the%20robot%0Aintegration%20of%203D%20LiDAR%20and%20NDT%20Matching%20significantly%20improves%20navigation%0Aaccuracy%20and%20operational%20reliability%20in%20a%20healthcare%20context.%20This%20study%0Ahighlights%20the%20robot%20ability%20to%20perform%20essential%20tasks%20with%20high%20efficiency%0Aand%20identifies%20potential%20areas%20for%20further%20improvement%2C%20particularly%20in%20sensor%0Aperformance%20under%20diverse%20environmental%20conditions.%20The%20successful%20deployment%0Aof%20this%20technology%20in%20a%20hospital%20setting%20illustrates%20its%20potential%20to%20support%0Amedical%20staff%20and%20contribute%20to%20patient%20care%2C%20suggesting%20a%20promising%20direction%0Afor%20future%20research%20and%20development%20in%20healthcare%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04797v1&entry.124074799=Read"},
{"title": "Explainable Search and Discovery of Visual Cultural Heritage Collections\n  with Multimodal Large Language Models", "author": "Taylor Arnold and Lauren Tilton", "abstract": "  Many cultural institutions have made large digitized visual collections\navailable online, often under permissible re-use licences. Creating interfaces\nfor exploring and searching these collections is difficult, particularly in the\nabsence of granular metadata. In this paper, we introduce a method for using\nstate-of-the-art multimodal large language models (LLMs) to enable an\nopen-ended, explainable search and discovery interface for visual collections.\nWe show how our approach can create novel clustering and recommendation systems\nthat avoid common pitfalls of methods based directly on visual embeddings. Of\nparticular interest is the ability to offer concrete textual explanations of\neach recommendation without the need to preselect the features of interest.\nTogether, these features can create a digital interface that is more open-ended\nand flexible while also being better suited to addressing privacy and ethical\nconcerns. Through a case study using a collection of documentary photographs,\nwe provide several metrics showing the efficacy and possibilities of our\napproach.\n", "link": "http://arxiv.org/abs/2411.04663v1", "date": "2024-11-07", "relevancy": 2.2606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Search%20and%20Discovery%20of%20Visual%20Cultural%20Heritage%20Collections%0A%20%20with%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Explainable%20Search%20and%20Discovery%20of%20Visual%20Cultural%20Heritage%20Collections%0A%20%20with%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Taylor%20Arnold%20and%20Lauren%20Tilton%0AAbstract%3A%20%20%20Many%20cultural%20institutions%20have%20made%20large%20digitized%20visual%20collections%0Aavailable%20online%2C%20often%20under%20permissible%20re-use%20licences.%20Creating%20interfaces%0Afor%20exploring%20and%20searching%20these%20collections%20is%20difficult%2C%20particularly%20in%20the%0Aabsence%20of%20granular%20metadata.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20for%20using%0Astate-of-the-art%20multimodal%20large%20language%20models%20%28LLMs%29%20to%20enable%20an%0Aopen-ended%2C%20explainable%20search%20and%20discovery%20interface%20for%20visual%20collections.%0AWe%20show%20how%20our%20approach%20can%20create%20novel%20clustering%20and%20recommendation%20systems%0Athat%20avoid%20common%20pitfalls%20of%20methods%20based%20directly%20on%20visual%20embeddings.%20Of%0Aparticular%20interest%20is%20the%20ability%20to%20offer%20concrete%20textual%20explanations%20of%0Aeach%20recommendation%20without%20the%20need%20to%20preselect%20the%20features%20of%20interest.%0ATogether%2C%20these%20features%20can%20create%20a%20digital%20interface%20that%20is%20more%20open-ended%0Aand%20flexible%20while%20also%20being%20better%20suited%20to%20addressing%20privacy%20and%20ethical%0Aconcerns.%20Through%20a%20case%20study%20using%20a%20collection%20of%20documentary%20photographs%2C%0Awe%20provide%20several%20metrics%20showing%20the%20efficacy%20and%20possibilities%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Search%2520and%2520Discovery%2520of%2520Visual%2520Cultural%2520Heritage%2520Collections%250A%2520%2520with%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DTaylor%2520Arnold%2520and%2520Lauren%2520Tilton%26entry.1292438233%3D%2520%2520Many%2520cultural%2520institutions%2520have%2520made%2520large%2520digitized%2520visual%2520collections%250Aavailable%2520online%252C%2520often%2520under%2520permissible%2520re-use%2520licences.%2520Creating%2520interfaces%250Afor%2520exploring%2520and%2520searching%2520these%2520collections%2520is%2520difficult%252C%2520particularly%2520in%2520the%250Aabsence%2520of%2520granular%2520metadata.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520for%2520using%250Astate-of-the-art%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520enable%2520an%250Aopen-ended%252C%2520explainable%2520search%2520and%2520discovery%2520interface%2520for%2520visual%2520collections.%250AWe%2520show%2520how%2520our%2520approach%2520can%2520create%2520novel%2520clustering%2520and%2520recommendation%2520systems%250Athat%2520avoid%2520common%2520pitfalls%2520of%2520methods%2520based%2520directly%2520on%2520visual%2520embeddings.%2520Of%250Aparticular%2520interest%2520is%2520the%2520ability%2520to%2520offer%2520concrete%2520textual%2520explanations%2520of%250Aeach%2520recommendation%2520without%2520the%2520need%2520to%2520preselect%2520the%2520features%2520of%2520interest.%250ATogether%252C%2520these%2520features%2520can%2520create%2520a%2520digital%2520interface%2520that%2520is%2520more%2520open-ended%250Aand%2520flexible%2520while%2520also%2520being%2520better%2520suited%2520to%2520addressing%2520privacy%2520and%2520ethical%250Aconcerns.%2520Through%2520a%2520case%2520study%2520using%2520a%2520collection%2520of%2520documentary%2520photographs%252C%250Awe%2520provide%2520several%2520metrics%2520showing%2520the%2520efficacy%2520and%2520possibilities%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Search%20and%20Discovery%20of%20Visual%20Cultural%20Heritage%20Collections%0A%20%20with%20Multimodal%20Large%20Language%20Models&entry.906535625=Taylor%20Arnold%20and%20Lauren%20Tilton&entry.1292438233=%20%20Many%20cultural%20institutions%20have%20made%20large%20digitized%20visual%20collections%0Aavailable%20online%2C%20often%20under%20permissible%20re-use%20licences.%20Creating%20interfaces%0Afor%20exploring%20and%20searching%20these%20collections%20is%20difficult%2C%20particularly%20in%20the%0Aabsence%20of%20granular%20metadata.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20for%20using%0Astate-of-the-art%20multimodal%20large%20language%20models%20%28LLMs%29%20to%20enable%20an%0Aopen-ended%2C%20explainable%20search%20and%20discovery%20interface%20for%20visual%20collections.%0AWe%20show%20how%20our%20approach%20can%20create%20novel%20clustering%20and%20recommendation%20systems%0Athat%20avoid%20common%20pitfalls%20of%20methods%20based%20directly%20on%20visual%20embeddings.%20Of%0Aparticular%20interest%20is%20the%20ability%20to%20offer%20concrete%20textual%20explanations%20of%0Aeach%20recommendation%20without%20the%20need%20to%20preselect%20the%20features%20of%20interest.%0ATogether%2C%20these%20features%20can%20create%20a%20digital%20interface%20that%20is%20more%20open-ended%0Aand%20flexible%20while%20also%20being%20better%20suited%20to%20addressing%20privacy%20and%20ethical%0Aconcerns.%20Through%20a%20case%20study%20using%20a%20collection%20of%20documentary%20photographs%2C%0Awe%20provide%20several%20metrics%20showing%20the%20efficacy%20and%20possibilities%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04663v1&entry.124074799=Read"},
{"title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language", "author": "Anthony Costarelli and Mat Allen and Severin Field", "abstract": "  As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area. Our code is available at\nhttps://github.com/acostarelli/meta-models-public .\n", "link": "http://arxiv.org/abs/2410.02472v3", "date": "2024-11-07", "relevancy": 2.2473, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Models%3A%20An%20Architecture%20for%20Decoding%20LLM%20Behaviors%20Through%0A%20%20Interpreted%20Embeddings%20and%20Natural%20Language&body=Title%3A%20Meta-Models%3A%20An%20Architecture%20for%20Decoding%20LLM%20Behaviors%20Through%0A%20%20Interpreted%20Embeddings%20and%20Natural%20Language%0AAuthor%3A%20Anthony%20Costarelli%20and%20Mat%20Allen%20and%20Severin%20Field%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20integrated%20into%20our%20daily%0Alives%2C%20the%20potential%20harms%20from%20deceptive%20behavior%20underlie%20the%20need%20for%0Afaithfully%20interpreting%20their%20decision-making.%20While%20traditional%20probing%0Amethods%20have%20shown%20some%20effectiveness%2C%20they%20remain%20best%20for%20narrowly%20scoped%0Atasks%20while%20more%20comprehensive%20explanations%20are%20still%20necessary.%20To%20this%20end%2C%0Awe%20investigate%20meta-models-an%20architecture%20using%20a%20%22meta-model%22%20that%20takes%0Aactivations%20from%20an%20%22input-model%22%20and%20answers%20natural%20language%20questions%20about%0Athe%20input-model%27s%20behaviors.%20We%20evaluate%20the%20meta-model%27s%20ability%20to%20generalize%0Aby%20training%20them%20on%20selected%20task%20types%20and%20assessing%20their%20out-of-distribution%0Aperformance%20in%20deceptive%20scenarios.%20Our%20findings%20show%20that%20meta-models%0Ageneralize%20well%20to%20out-of-distribution%20tasks%20and%20point%20towards%20opportunities%0Afor%20future%20research%20in%20this%20area.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/acostarelli/meta-models-public%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Models%253A%2520An%2520Architecture%2520for%2520Decoding%2520LLM%2520Behaviors%2520Through%250A%2520%2520Interpreted%2520Embeddings%2520and%2520Natural%2520Language%26entry.906535625%3DAnthony%2520Costarelli%2520and%2520Mat%2520Allen%2520and%2520Severin%2520Field%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520increasingly%2520integrated%2520into%2520our%2520daily%250Alives%252C%2520the%2520potential%2520harms%2520from%2520deceptive%2520behavior%2520underlie%2520the%2520need%2520for%250Afaithfully%2520interpreting%2520their%2520decision-making.%2520While%2520traditional%2520probing%250Amethods%2520have%2520shown%2520some%2520effectiveness%252C%2520they%2520remain%2520best%2520for%2520narrowly%2520scoped%250Atasks%2520while%2520more%2520comprehensive%2520explanations%2520are%2520still%2520necessary.%2520To%2520this%2520end%252C%250Awe%2520investigate%2520meta-models-an%2520architecture%2520using%2520a%2520%2522meta-model%2522%2520that%2520takes%250Aactivations%2520from%2520an%2520%2522input-model%2522%2520and%2520answers%2520natural%2520language%2520questions%2520about%250Athe%2520input-model%2527s%2520behaviors.%2520We%2520evaluate%2520the%2520meta-model%2527s%2520ability%2520to%2520generalize%250Aby%2520training%2520them%2520on%2520selected%2520task%2520types%2520and%2520assessing%2520their%2520out-of-distribution%250Aperformance%2520in%2520deceptive%2520scenarios.%2520Our%2520findings%2520show%2520that%2520meta-models%250Ageneralize%2520well%2520to%2520out-of-distribution%2520tasks%2520and%2520point%2520towards%2520opportunities%250Afor%2520future%2520research%2520in%2520this%2520area.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/acostarelli/meta-models-public%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Models%3A%20An%20Architecture%20for%20Decoding%20LLM%20Behaviors%20Through%0A%20%20Interpreted%20Embeddings%20and%20Natural%20Language&entry.906535625=Anthony%20Costarelli%20and%20Mat%20Allen%20and%20Severin%20Field&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20integrated%20into%20our%20daily%0Alives%2C%20the%20potential%20harms%20from%20deceptive%20behavior%20underlie%20the%20need%20for%0Afaithfully%20interpreting%20their%20decision-making.%20While%20traditional%20probing%0Amethods%20have%20shown%20some%20effectiveness%2C%20they%20remain%20best%20for%20narrowly%20scoped%0Atasks%20while%20more%20comprehensive%20explanations%20are%20still%20necessary.%20To%20this%20end%2C%0Awe%20investigate%20meta-models-an%20architecture%20using%20a%20%22meta-model%22%20that%20takes%0Aactivations%20from%20an%20%22input-model%22%20and%20answers%20natural%20language%20questions%20about%0Athe%20input-model%27s%20behaviors.%20We%20evaluate%20the%20meta-model%27s%20ability%20to%20generalize%0Aby%20training%20them%20on%20selected%20task%20types%20and%20assessing%20their%20out-of-distribution%0Aperformance%20in%20deceptive%20scenarios.%20Our%20findings%20show%20that%20meta-models%0Ageneralize%20well%20to%20out-of-distribution%20tasks%20and%20point%20towards%20opportunities%0Afor%20future%20research%20in%20this%20area.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/acostarelli/meta-models-public%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02472v3&entry.124074799=Read"},
{"title": "Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities\n  of Neurosymbolic AI", "author": "Ruwan Wickramarachchi and Cory Henson and Amit Sheth", "abstract": "  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful\napproach for tasks spanning from perception to cognition. The use of\nNeurosymbolic AI has been shown to achieve enhanced capabilities, including\nimproved grounding, alignment, explainability, and reliability. However, due to\nits nascent stage, there is a lack of widely available real-world benchmark\ndatasets tailored to Neurosymbolic AI tasks. To address this gap and support\nthe evaluation of current and future methods, we introduce DSceneKG -- a suite\nof knowledge graphs of driving scenes built from real-world, high-quality\nscenes from multiple open autonomous driving datasets. In this article, we\ndetail the construction process of DSceneKG and highlight its application in\nseven different tasks. DSceneKG is publicly accessible at:\nhttps://github.com/ruwantw/DSceneKG\n", "link": "http://arxiv.org/abs/2411.03225v2", "date": "2024-11-07", "relevancy": 2.2422, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI&body=Title%3A%20Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI%0AAuthor%3A%20Ruwan%20Wickramarachchi%20and%20Cory%20Henson%20and%20Amit%20Sheth%0AAbstract%3A%20%20%20In%20the%20era%20of%20Generative%20AI%2C%20Neurosymbolic%20AI%20is%20emerging%20as%20a%20powerful%0Aapproach%20for%20tasks%20spanning%20from%20perception%20to%20cognition.%20The%20use%20of%0ANeurosymbolic%20AI%20has%20been%20shown%20to%20achieve%20enhanced%20capabilities%2C%20including%0Aimproved%20grounding%2C%20alignment%2C%20explainability%2C%20and%20reliability.%20However%2C%20due%20to%0Aits%20nascent%20stage%2C%20there%20is%20a%20lack%20of%20widely%20available%20real-world%20benchmark%0Adatasets%20tailored%20to%20Neurosymbolic%20AI%20tasks.%20To%20address%20this%20gap%20and%20support%0Athe%20evaluation%20of%20current%20and%20future%20methods%2C%20we%20introduce%20DSceneKG%20--%20a%20suite%0Aof%20knowledge%20graphs%20of%20driving%20scenes%20built%20from%20real-world%2C%20high-quality%0Ascenes%20from%20multiple%20open%20autonomous%20driving%20datasets.%20In%20this%20article%2C%20we%0Adetail%20the%20construction%20process%20of%20DSceneKG%20and%20highlight%20its%20application%20in%0Aseven%20different%20tasks.%20DSceneKG%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/ruwantw/DSceneKG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graphs%2520of%2520Driving%2520Scenes%2520to%2520Empower%2520the%2520Emerging%2520Capabilities%250A%2520%2520of%2520Neurosymbolic%2520AI%26entry.906535625%3DRuwan%2520Wickramarachchi%2520and%2520Cory%2520Henson%2520and%2520Amit%2520Sheth%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520Generative%2520AI%252C%2520Neurosymbolic%2520AI%2520is%2520emerging%2520as%2520a%2520powerful%250Aapproach%2520for%2520tasks%2520spanning%2520from%2520perception%2520to%2520cognition.%2520The%2520use%2520of%250ANeurosymbolic%2520AI%2520has%2520been%2520shown%2520to%2520achieve%2520enhanced%2520capabilities%252C%2520including%250Aimproved%2520grounding%252C%2520alignment%252C%2520explainability%252C%2520and%2520reliability.%2520However%252C%2520due%2520to%250Aits%2520nascent%2520stage%252C%2520there%2520is%2520a%2520lack%2520of%2520widely%2520available%2520real-world%2520benchmark%250Adatasets%2520tailored%2520to%2520Neurosymbolic%2520AI%2520tasks.%2520To%2520address%2520this%2520gap%2520and%2520support%250Athe%2520evaluation%2520of%2520current%2520and%2520future%2520methods%252C%2520we%2520introduce%2520DSceneKG%2520--%2520a%2520suite%250Aof%2520knowledge%2520graphs%2520of%2520driving%2520scenes%2520built%2520from%2520real-world%252C%2520high-quality%250Ascenes%2520from%2520multiple%2520open%2520autonomous%2520driving%2520datasets.%2520In%2520this%2520article%252C%2520we%250Adetail%2520the%2520construction%2520process%2520of%2520DSceneKG%2520and%2520highlight%2520its%2520application%2520in%250Aseven%2520different%2520tasks.%2520DSceneKG%2520is%2520publicly%2520accessible%2520at%253A%250Ahttps%253A//github.com/ruwantw/DSceneKG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graphs%20of%20Driving%20Scenes%20to%20Empower%20the%20Emerging%20Capabilities%0A%20%20of%20Neurosymbolic%20AI&entry.906535625=Ruwan%20Wickramarachchi%20and%20Cory%20Henson%20and%20Amit%20Sheth&entry.1292438233=%20%20In%20the%20era%20of%20Generative%20AI%2C%20Neurosymbolic%20AI%20is%20emerging%20as%20a%20powerful%0Aapproach%20for%20tasks%20spanning%20from%20perception%20to%20cognition.%20The%20use%20of%0ANeurosymbolic%20AI%20has%20been%20shown%20to%20achieve%20enhanced%20capabilities%2C%20including%0Aimproved%20grounding%2C%20alignment%2C%20explainability%2C%20and%20reliability.%20However%2C%20due%20to%0Aits%20nascent%20stage%2C%20there%20is%20a%20lack%20of%20widely%20available%20real-world%20benchmark%0Adatasets%20tailored%20to%20Neurosymbolic%20AI%20tasks.%20To%20address%20this%20gap%20and%20support%0Athe%20evaluation%20of%20current%20and%20future%20methods%2C%20we%20introduce%20DSceneKG%20--%20a%20suite%0Aof%20knowledge%20graphs%20of%20driving%20scenes%20built%20from%20real-world%2C%20high-quality%0Ascenes%20from%20multiple%20open%20autonomous%20driving%20datasets.%20In%20this%20article%2C%20we%0Adetail%20the%20construction%20process%20of%20DSceneKG%20and%20highlight%20its%20application%20in%0Aseven%20different%20tasks.%20DSceneKG%20is%20publicly%20accessible%20at%3A%0Ahttps%3A//github.com/ruwantw/DSceneKG%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03225v2&entry.124074799=Read"},
{"title": "Few-Shot Task Learning through Inverse Generative Modeling", "author": "Aviv Netanyahu and Yilun Du and Antonia Bronars and Jyothish Pari and Joshua Tenenbaum and Tianmin Shu and Pulkit Agrawal", "abstract": "  Learning the intents of an agent, defined by its goals or motion style, is\noften extremely challenging from just a few examples. We refer to this problem\nas task concept learning and present our approach, Few-Shot Task Learning\nthrough Inverse Generative Modeling (FTL-IGM), which learns new task concepts\nby leveraging invertible neural generative models. The core idea is to pretrain\na generative model on a set of basic concepts and their demonstrations. Then,\ngiven a few demonstrations of a new concept (such as a new goal or a new\naction), our method learns the underlying concepts through backpropagation\nwithout updating the model weights, thanks to the invertibility of the\ngenerative model. We evaluate our method in five domains -- object\nrearrangement, goal-oriented navigation, motion caption of human actions,\nautonomous driving, and real-world table-top manipulation. Our experimental\nresults demonstrate that via the pretrained generative model, we successfully\nlearn novel concepts and generate agent plans or motion corresponding to these\nconcepts in (1) unseen environments and (2) in composition with training\nconcepts.\n", "link": "http://arxiv.org/abs/2411.04987v1", "date": "2024-11-07", "relevancy": 2.2367, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6066}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling&body=Title%3A%20Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling%0AAuthor%3A%20Aviv%20Netanyahu%20and%20Yilun%20Du%20and%20Antonia%20Bronars%20and%20Jyothish%20Pari%20and%20Joshua%20Tenenbaum%20and%20Tianmin%20Shu%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Learning%20the%20intents%20of%20an%20agent%2C%20defined%20by%20its%20goals%20or%20motion%20style%2C%20is%0Aoften%20extremely%20challenging%20from%20just%20a%20few%20examples.%20We%20refer%20to%20this%20problem%0Aas%20task%20concept%20learning%20and%20present%20our%20approach%2C%20Few-Shot%20Task%20Learning%0Athrough%20Inverse%20Generative%20Modeling%20%28FTL-IGM%29%2C%20which%20learns%20new%20task%20concepts%0Aby%20leveraging%20invertible%20neural%20generative%20models.%20The%20core%20idea%20is%20to%20pretrain%0Aa%20generative%20model%20on%20a%20set%20of%20basic%20concepts%20and%20their%20demonstrations.%20Then%2C%0Agiven%20a%20few%20demonstrations%20of%20a%20new%20concept%20%28such%20as%20a%20new%20goal%20or%20a%20new%0Aaction%29%2C%20our%20method%20learns%20the%20underlying%20concepts%20through%20backpropagation%0Awithout%20updating%20the%20model%20weights%2C%20thanks%20to%20the%20invertibility%20of%20the%0Agenerative%20model.%20We%20evaluate%20our%20method%20in%20five%20domains%20--%20object%0Arearrangement%2C%20goal-oriented%20navigation%2C%20motion%20caption%20of%20human%20actions%2C%0Aautonomous%20driving%2C%20and%20real-world%20table-top%20manipulation.%20Our%20experimental%0Aresults%20demonstrate%20that%20via%20the%20pretrained%20generative%20model%2C%20we%20successfully%0Alearn%20novel%20concepts%20and%20generate%20agent%20plans%20or%20motion%20corresponding%20to%20these%0Aconcepts%20in%20%281%29%20unseen%20environments%20and%20%282%29%20in%20composition%20with%20training%0Aconcepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Task%2520Learning%2520through%2520Inverse%2520Generative%2520Modeling%26entry.906535625%3DAviv%2520Netanyahu%2520and%2520Yilun%2520Du%2520and%2520Antonia%2520Bronars%2520and%2520Jyothish%2520Pari%2520and%2520Joshua%2520Tenenbaum%2520and%2520Tianmin%2520Shu%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Learning%2520the%2520intents%2520of%2520an%2520agent%252C%2520defined%2520by%2520its%2520goals%2520or%2520motion%2520style%252C%2520is%250Aoften%2520extremely%2520challenging%2520from%2520just%2520a%2520few%2520examples.%2520We%2520refer%2520to%2520this%2520problem%250Aas%2520task%2520concept%2520learning%2520and%2520present%2520our%2520approach%252C%2520Few-Shot%2520Task%2520Learning%250Athrough%2520Inverse%2520Generative%2520Modeling%2520%2528FTL-IGM%2529%252C%2520which%2520learns%2520new%2520task%2520concepts%250Aby%2520leveraging%2520invertible%2520neural%2520generative%2520models.%2520The%2520core%2520idea%2520is%2520to%2520pretrain%250Aa%2520generative%2520model%2520on%2520a%2520set%2520of%2520basic%2520concepts%2520and%2520their%2520demonstrations.%2520Then%252C%250Agiven%2520a%2520few%2520demonstrations%2520of%2520a%2520new%2520concept%2520%2528such%2520as%2520a%2520new%2520goal%2520or%2520a%2520new%250Aaction%2529%252C%2520our%2520method%2520learns%2520the%2520underlying%2520concepts%2520through%2520backpropagation%250Awithout%2520updating%2520the%2520model%2520weights%252C%2520thanks%2520to%2520the%2520invertibility%2520of%2520the%250Agenerative%2520model.%2520We%2520evaluate%2520our%2520method%2520in%2520five%2520domains%2520--%2520object%250Arearrangement%252C%2520goal-oriented%2520navigation%252C%2520motion%2520caption%2520of%2520human%2520actions%252C%250Aautonomous%2520driving%252C%2520and%2520real-world%2520table-top%2520manipulation.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520via%2520the%2520pretrained%2520generative%2520model%252C%2520we%2520successfully%250Alearn%2520novel%2520concepts%2520and%2520generate%2520agent%2520plans%2520or%2520motion%2520corresponding%2520to%2520these%250Aconcepts%2520in%2520%25281%2529%2520unseen%2520environments%2520and%2520%25282%2529%2520in%2520composition%2520with%2520training%250Aconcepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling&entry.906535625=Aviv%20Netanyahu%20and%20Yilun%20Du%20and%20Antonia%20Bronars%20and%20Jyothish%20Pari%20and%20Joshua%20Tenenbaum%20and%20Tianmin%20Shu%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Learning%20the%20intents%20of%20an%20agent%2C%20defined%20by%20its%20goals%20or%20motion%20style%2C%20is%0Aoften%20extremely%20challenging%20from%20just%20a%20few%20examples.%20We%20refer%20to%20this%20problem%0Aas%20task%20concept%20learning%20and%20present%20our%20approach%2C%20Few-Shot%20Task%20Learning%0Athrough%20Inverse%20Generative%20Modeling%20%28FTL-IGM%29%2C%20which%20learns%20new%20task%20concepts%0Aby%20leveraging%20invertible%20neural%20generative%20models.%20The%20core%20idea%20is%20to%20pretrain%0Aa%20generative%20model%20on%20a%20set%20of%20basic%20concepts%20and%20their%20demonstrations.%20Then%2C%0Agiven%20a%20few%20demonstrations%20of%20a%20new%20concept%20%28such%20as%20a%20new%20goal%20or%20a%20new%0Aaction%29%2C%20our%20method%20learns%20the%20underlying%20concepts%20through%20backpropagation%0Awithout%20updating%20the%20model%20weights%2C%20thanks%20to%20the%20invertibility%20of%20the%0Agenerative%20model.%20We%20evaluate%20our%20method%20in%20five%20domains%20--%20object%0Arearrangement%2C%20goal-oriented%20navigation%2C%20motion%20caption%20of%20human%20actions%2C%0Aautonomous%20driving%2C%20and%20real-world%20table-top%20manipulation.%20Our%20experimental%0Aresults%20demonstrate%20that%20via%20the%20pretrained%20generative%20model%2C%20we%20successfully%0Alearn%20novel%20concepts%20and%20generate%20agent%20plans%20or%20motion%20corresponding%20to%20these%0Aconcepts%20in%20%281%29%20unseen%20environments%20and%20%282%29%20in%20composition%20with%20training%0Aconcepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04987v1&entry.124074799=Read"},
{"title": "EffiCANet: Efficient Time Series Forecasting with Convolutional\n  Attention", "author": "Xinxing Zhou and Jiaqi Ye and Shubao Zhao and Ming Jin and Chengyi Yang and Yanlong Wen and Xiaojie Yuan", "abstract": "  The exponential growth of multivariate time series data from sensor networks\nin domains like industrial monitoring and smart cities requires efficient and\naccurate forecasting models. Current deep learning methods often fail to\nadequately capture long-range dependencies and complex inter-variable\nrelationships, especially under real-time processing constraints. These\nlimitations arise as many models are optimized for either short-term\nforecasting with limited receptive fields or long-term accuracy at the cost of\nefficiency. Additionally, dynamic and intricate interactions between variables\nin real-world data further complicate modeling efforts. To address these\nlimitations, we propose EffiCANet, an Efficient Convolutional Attention Network\ndesigned to enhance forecasting accuracy while maintaining computational\nefficiency. EffiCANet integrates three key components: (1) a Temporal\nLarge-kernel Decomposed Convolution (TLDC) module that captures long-term\ntemporal dependencies while reducing computational overhead; (2) an\nInter-Variable Group Convolution (IVGC) module that captures complex and\nevolving relationships among variables; and (3) a Global Temporal-Variable\nAttention (GTVA) mechanism that prioritizes critical temporal and\ninter-variable features. Extensive evaluations across nine benchmark datasets\nshow that EffiCANet achieves the maximum reduction of 10.02% in MAE over\nstate-of-the-art models, while cutting computational costs by 26.2% relative to\nconventional large-kernel convolution methods, thanks to its efficient\ndecomposition strategy.\n", "link": "http://arxiv.org/abs/2411.04669v1", "date": "2024-11-07", "relevancy": 2.2289, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.585}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5443}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EffiCANet%3A%20Efficient%20Time%20Series%20Forecasting%20with%20Convolutional%0A%20%20Attention&body=Title%3A%20EffiCANet%3A%20Efficient%20Time%20Series%20Forecasting%20with%20Convolutional%0A%20%20Attention%0AAuthor%3A%20Xinxing%20Zhou%20and%20Jiaqi%20Ye%20and%20Shubao%20Zhao%20and%20Ming%20Jin%20and%20Chengyi%20Yang%20and%20Yanlong%20Wen%20and%20Xiaojie%20Yuan%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20multivariate%20time%20series%20data%20from%20sensor%20networks%0Ain%20domains%20like%20industrial%20monitoring%20and%20smart%20cities%20requires%20efficient%20and%0Aaccurate%20forecasting%20models.%20Current%20deep%20learning%20methods%20often%20fail%20to%0Aadequately%20capture%20long-range%20dependencies%20and%20complex%20inter-variable%0Arelationships%2C%20especially%20under%20real-time%20processing%20constraints.%20These%0Alimitations%20arise%20as%20many%20models%20are%20optimized%20for%20either%20short-term%0Aforecasting%20with%20limited%20receptive%20fields%20or%20long-term%20accuracy%20at%20the%20cost%20of%0Aefficiency.%20Additionally%2C%20dynamic%20and%20intricate%20interactions%20between%20variables%0Ain%20real-world%20data%20further%20complicate%20modeling%20efforts.%20To%20address%20these%0Alimitations%2C%20we%20propose%20EffiCANet%2C%20an%20Efficient%20Convolutional%20Attention%20Network%0Adesigned%20to%20enhance%20forecasting%20accuracy%20while%20maintaining%20computational%0Aefficiency.%20EffiCANet%20integrates%20three%20key%20components%3A%20%281%29%20a%20Temporal%0ALarge-kernel%20Decomposed%20Convolution%20%28TLDC%29%20module%20that%20captures%20long-term%0Atemporal%20dependencies%20while%20reducing%20computational%20overhead%3B%20%282%29%20an%0AInter-Variable%20Group%20Convolution%20%28IVGC%29%20module%20that%20captures%20complex%20and%0Aevolving%20relationships%20among%20variables%3B%20and%20%283%29%20a%20Global%20Temporal-Variable%0AAttention%20%28GTVA%29%20mechanism%20that%20prioritizes%20critical%20temporal%20and%0Ainter-variable%20features.%20Extensive%20evaluations%20across%20nine%20benchmark%20datasets%0Ashow%20that%20EffiCANet%20achieves%20the%20maximum%20reduction%20of%2010.02%25%20in%20MAE%20over%0Astate-of-the-art%20models%2C%20while%20cutting%20computational%20costs%20by%2026.2%25%20relative%20to%0Aconventional%20large-kernel%20convolution%20methods%2C%20thanks%20to%20its%20efficient%0Adecomposition%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffiCANet%253A%2520Efficient%2520Time%2520Series%2520Forecasting%2520with%2520Convolutional%250A%2520%2520Attention%26entry.906535625%3DXinxing%2520Zhou%2520and%2520Jiaqi%2520Ye%2520and%2520Shubao%2520Zhao%2520and%2520Ming%2520Jin%2520and%2520Chengyi%2520Yang%2520and%2520Yanlong%2520Wen%2520and%2520Xiaojie%2520Yuan%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520multivariate%2520time%2520series%2520data%2520from%2520sensor%2520networks%250Ain%2520domains%2520like%2520industrial%2520monitoring%2520and%2520smart%2520cities%2520requires%2520efficient%2520and%250Aaccurate%2520forecasting%2520models.%2520Current%2520deep%2520learning%2520methods%2520often%2520fail%2520to%250Aadequately%2520capture%2520long-range%2520dependencies%2520and%2520complex%2520inter-variable%250Arelationships%252C%2520especially%2520under%2520real-time%2520processing%2520constraints.%2520These%250Alimitations%2520arise%2520as%2520many%2520models%2520are%2520optimized%2520for%2520either%2520short-term%250Aforecasting%2520with%2520limited%2520receptive%2520fields%2520or%2520long-term%2520accuracy%2520at%2520the%2520cost%2520of%250Aefficiency.%2520Additionally%252C%2520dynamic%2520and%2520intricate%2520interactions%2520between%2520variables%250Ain%2520real-world%2520data%2520further%2520complicate%2520modeling%2520efforts.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520EffiCANet%252C%2520an%2520Efficient%2520Convolutional%2520Attention%2520Network%250Adesigned%2520to%2520enhance%2520forecasting%2520accuracy%2520while%2520maintaining%2520computational%250Aefficiency.%2520EffiCANet%2520integrates%2520three%2520key%2520components%253A%2520%25281%2529%2520a%2520Temporal%250ALarge-kernel%2520Decomposed%2520Convolution%2520%2528TLDC%2529%2520module%2520that%2520captures%2520long-term%250Atemporal%2520dependencies%2520while%2520reducing%2520computational%2520overhead%253B%2520%25282%2529%2520an%250AInter-Variable%2520Group%2520Convolution%2520%2528IVGC%2529%2520module%2520that%2520captures%2520complex%2520and%250Aevolving%2520relationships%2520among%2520variables%253B%2520and%2520%25283%2529%2520a%2520Global%2520Temporal-Variable%250AAttention%2520%2528GTVA%2529%2520mechanism%2520that%2520prioritizes%2520critical%2520temporal%2520and%250Ainter-variable%2520features.%2520Extensive%2520evaluations%2520across%2520nine%2520benchmark%2520datasets%250Ashow%2520that%2520EffiCANet%2520achieves%2520the%2520maximum%2520reduction%2520of%252010.02%2525%2520in%2520MAE%2520over%250Astate-of-the-art%2520models%252C%2520while%2520cutting%2520computational%2520costs%2520by%252026.2%2525%2520relative%2520to%250Aconventional%2520large-kernel%2520convolution%2520methods%252C%2520thanks%2520to%2520its%2520efficient%250Adecomposition%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EffiCANet%3A%20Efficient%20Time%20Series%20Forecasting%20with%20Convolutional%0A%20%20Attention&entry.906535625=Xinxing%20Zhou%20and%20Jiaqi%20Ye%20and%20Shubao%20Zhao%20and%20Ming%20Jin%20and%20Chengyi%20Yang%20and%20Yanlong%20Wen%20and%20Xiaojie%20Yuan&entry.1292438233=%20%20The%20exponential%20growth%20of%20multivariate%20time%20series%20data%20from%20sensor%20networks%0Ain%20domains%20like%20industrial%20monitoring%20and%20smart%20cities%20requires%20efficient%20and%0Aaccurate%20forecasting%20models.%20Current%20deep%20learning%20methods%20often%20fail%20to%0Aadequately%20capture%20long-range%20dependencies%20and%20complex%20inter-variable%0Arelationships%2C%20especially%20under%20real-time%20processing%20constraints.%20These%0Alimitations%20arise%20as%20many%20models%20are%20optimized%20for%20either%20short-term%0Aforecasting%20with%20limited%20receptive%20fields%20or%20long-term%20accuracy%20at%20the%20cost%20of%0Aefficiency.%20Additionally%2C%20dynamic%20and%20intricate%20interactions%20between%20variables%0Ain%20real-world%20data%20further%20complicate%20modeling%20efforts.%20To%20address%20these%0Alimitations%2C%20we%20propose%20EffiCANet%2C%20an%20Efficient%20Convolutional%20Attention%20Network%0Adesigned%20to%20enhance%20forecasting%20accuracy%20while%20maintaining%20computational%0Aefficiency.%20EffiCANet%20integrates%20three%20key%20components%3A%20%281%29%20a%20Temporal%0ALarge-kernel%20Decomposed%20Convolution%20%28TLDC%29%20module%20that%20captures%20long-term%0Atemporal%20dependencies%20while%20reducing%20computational%20overhead%3B%20%282%29%20an%0AInter-Variable%20Group%20Convolution%20%28IVGC%29%20module%20that%20captures%20complex%20and%0Aevolving%20relationships%20among%20variables%3B%20and%20%283%29%20a%20Global%20Temporal-Variable%0AAttention%20%28GTVA%29%20mechanism%20that%20prioritizes%20critical%20temporal%20and%0Ainter-variable%20features.%20Extensive%20evaluations%20across%20nine%20benchmark%20datasets%0Ashow%20that%20EffiCANet%20achieves%20the%20maximum%20reduction%20of%2010.02%25%20in%20MAE%20over%0Astate-of-the-art%20models%2C%20while%20cutting%20computational%20costs%20by%2026.2%25%20relative%20to%0Aconventional%20large-kernel%20convolution%20methods%2C%20thanks%20to%20its%20efficient%0Adecomposition%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04669v1&entry.124074799=Read"},
{"title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition", "author": "Candida M. Greco and Lucio La Cava and Andrea Tagarelli", "abstract": "  Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.\n", "link": "http://arxiv.org/abs/2406.14894v2", "date": "2024-11-07", "relevancy": 2.2157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talking%20the%20Talk%20Does%20Not%20Entail%20Walking%20the%20Walk%3A%20On%20the%20Limits%20of%0A%20%20Large%20Language%20Models%20in%20Lexical%20Entailment%20Recognition&body=Title%3A%20Talking%20the%20Talk%20Does%20Not%20Entail%20Walking%20the%20Walk%3A%20On%20the%20Limits%20of%0A%20%20Large%20Language%20Models%20in%20Lexical%20Entailment%20Recognition%0AAuthor%3A%20Candida%20M.%20Greco%20and%20Lucio%20La%20Cava%20and%20Andrea%20Tagarelli%0AAbstract%3A%20%20%20Verbs%20form%20the%20backbone%20of%20language%2C%20providing%20the%20structure%20and%20meaning%20to%0Asentences.%20Yet%2C%20their%20intricate%20semantic%20nuances%20pose%20a%20longstanding%20challenge.%0AUnderstanding%20verb%20relations%20through%20the%20concept%20of%20lexical%20entailment%20is%0Acrucial%20for%20comprehending%20sentence%20meanings%20and%20grasping%20verb%20dynamics.%20This%0Awork%20investigates%20the%20capabilities%20of%20eight%20Large%20Language%20Models%20in%0Arecognizing%20lexical%20entailment%20relations%20among%20verbs%20through%20differently%0Adevised%20prompting%20strategies%20and%20zero-/few-shot%20settings%20over%20verb%20pairs%20from%0Atwo%20lexical%20databases%2C%20namely%20WordNet%20and%20HyperLex.%20Our%20findings%20unveil%20that%0Athe%20models%20can%20tackle%20the%20lexical%20entailment%20recognition%20task%20with%20moderately%0Agood%20performance%2C%20although%20at%20varying%20degree%20of%20effectiveness%20and%20under%0Adifferent%20conditions.%20Also%2C%20utilizing%20few-shot%20prompting%20can%20enhance%20the%0Amodels%27%20performance.%20However%2C%20perfectly%20solving%20the%20task%20arises%20as%20an%20unmet%0Achallenge%20for%20all%20examined%20LLMs%2C%20which%20raises%20an%20emergence%20for%20further%20research%0Adevelopments%20on%20this%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalking%2520the%2520Talk%2520Does%2520Not%2520Entail%2520Walking%2520the%2520Walk%253A%2520On%2520the%2520Limits%2520of%250A%2520%2520Large%2520Language%2520Models%2520in%2520Lexical%2520Entailment%2520Recognition%26entry.906535625%3DCandida%2520M.%2520Greco%2520and%2520Lucio%2520La%2520Cava%2520and%2520Andrea%2520Tagarelli%26entry.1292438233%3D%2520%2520Verbs%2520form%2520the%2520backbone%2520of%2520language%252C%2520providing%2520the%2520structure%2520and%2520meaning%2520to%250Asentences.%2520Yet%252C%2520their%2520intricate%2520semantic%2520nuances%2520pose%2520a%2520longstanding%2520challenge.%250AUnderstanding%2520verb%2520relations%2520through%2520the%2520concept%2520of%2520lexical%2520entailment%2520is%250Acrucial%2520for%2520comprehending%2520sentence%2520meanings%2520and%2520grasping%2520verb%2520dynamics.%2520This%250Awork%2520investigates%2520the%2520capabilities%2520of%2520eight%2520Large%2520Language%2520Models%2520in%250Arecognizing%2520lexical%2520entailment%2520relations%2520among%2520verbs%2520through%2520differently%250Adevised%2520prompting%2520strategies%2520and%2520zero-/few-shot%2520settings%2520over%2520verb%2520pairs%2520from%250Atwo%2520lexical%2520databases%252C%2520namely%2520WordNet%2520and%2520HyperLex.%2520Our%2520findings%2520unveil%2520that%250Athe%2520models%2520can%2520tackle%2520the%2520lexical%2520entailment%2520recognition%2520task%2520with%2520moderately%250Agood%2520performance%252C%2520although%2520at%2520varying%2520degree%2520of%2520effectiveness%2520and%2520under%250Adifferent%2520conditions.%2520Also%252C%2520utilizing%2520few-shot%2520prompting%2520can%2520enhance%2520the%250Amodels%2527%2520performance.%2520However%252C%2520perfectly%2520solving%2520the%2520task%2520arises%2520as%2520an%2520unmet%250Achallenge%2520for%2520all%2520examined%2520LLMs%252C%2520which%2520raises%2520an%2520emergence%2520for%2520further%2520research%250Adevelopments%2520on%2520this%2520topic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talking%20the%20Talk%20Does%20Not%20Entail%20Walking%20the%20Walk%3A%20On%20the%20Limits%20of%0A%20%20Large%20Language%20Models%20in%20Lexical%20Entailment%20Recognition&entry.906535625=Candida%20M.%20Greco%20and%20Lucio%20La%20Cava%20and%20Andrea%20Tagarelli&entry.1292438233=%20%20Verbs%20form%20the%20backbone%20of%20language%2C%20providing%20the%20structure%20and%20meaning%20to%0Asentences.%20Yet%2C%20their%20intricate%20semantic%20nuances%20pose%20a%20longstanding%20challenge.%0AUnderstanding%20verb%20relations%20through%20the%20concept%20of%20lexical%20entailment%20is%0Acrucial%20for%20comprehending%20sentence%20meanings%20and%20grasping%20verb%20dynamics.%20This%0Awork%20investigates%20the%20capabilities%20of%20eight%20Large%20Language%20Models%20in%0Arecognizing%20lexical%20entailment%20relations%20among%20verbs%20through%20differently%0Adevised%20prompting%20strategies%20and%20zero-/few-shot%20settings%20over%20verb%20pairs%20from%0Atwo%20lexical%20databases%2C%20namely%20WordNet%20and%20HyperLex.%20Our%20findings%20unveil%20that%0Athe%20models%20can%20tackle%20the%20lexical%20entailment%20recognition%20task%20with%20moderately%0Agood%20performance%2C%20although%20at%20varying%20degree%20of%20effectiveness%20and%20under%0Adifferent%20conditions.%20Also%2C%20utilizing%20few-shot%20prompting%20can%20enhance%20the%0Amodels%27%20performance.%20However%2C%20perfectly%20solving%20the%20task%20arises%20as%20an%20unmet%0Achallenge%20for%20all%20examined%20LLMs%2C%20which%20raises%20an%20emergence%20for%20further%20research%0Adevelopments%20on%20this%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14894v2&entry.124074799=Read"},
{"title": "DNN-based 3D Cloud Retrieval for Variable Solar Illumination and\n  Multiview Spaceborne Imaging", "author": "Tamar Klein and Tom Aizenberg and Roi Ronen", "abstract": "  Climate studies often rely on remotely sensed images to retrieve\ntwo-dimensional maps of cloud properties. To advance volumetric analysis, we\nfocus on recovering the three-dimensional (3D) heterogeneous extinction\ncoefficient field of shallow clouds using multiview remote sensing data.\nClimate research requires large-scale worldwide statistics. To enable scalable\ndata processing, previous deep neural networks (DNNs) can infer at spaceborne\nremote sensing downlink rates. However, prior methods are limited to a fixed\nsolar illumination direction. In this work, we introduce the first scalable\nDNN-based system for 3D cloud retrieval that accommodates varying camera poses\nand solar directions. By integrating multiview cloud intensity images with\ncamera poses and solar direction data, we achieve greater flexibility in\nrecovery. Training of the DNN is performed by a novel two-stage scheme to\naddress the high number of degrees of freedom in this problem. Our approach\nshows substantial improvements over previous state-of-the-art, particularly in\nhandling variations in the sun's zenith angle.\n", "link": "http://arxiv.org/abs/2411.04682v1", "date": "2024-11-07", "relevancy": 2.2052, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5532}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNN-based%203D%20Cloud%20Retrieval%20for%20Variable%20Solar%20Illumination%20and%0A%20%20Multiview%20Spaceborne%20Imaging&body=Title%3A%20DNN-based%203D%20Cloud%20Retrieval%20for%20Variable%20Solar%20Illumination%20and%0A%20%20Multiview%20Spaceborne%20Imaging%0AAuthor%3A%20Tamar%20Klein%20and%20Tom%20Aizenberg%20and%20Roi%20Ronen%0AAbstract%3A%20%20%20Climate%20studies%20often%20rely%20on%20remotely%20sensed%20images%20to%20retrieve%0Atwo-dimensional%20maps%20of%20cloud%20properties.%20To%20advance%20volumetric%20analysis%2C%20we%0Afocus%20on%20recovering%20the%20three-dimensional%20%283D%29%20heterogeneous%20extinction%0Acoefficient%20field%20of%20shallow%20clouds%20using%20multiview%20remote%20sensing%20data.%0AClimate%20research%20requires%20large-scale%20worldwide%20statistics.%20To%20enable%20scalable%0Adata%20processing%2C%20previous%20deep%20neural%20networks%20%28DNNs%29%20can%20infer%20at%20spaceborne%0Aremote%20sensing%20downlink%20rates.%20However%2C%20prior%20methods%20are%20limited%20to%20a%20fixed%0Asolar%20illumination%20direction.%20In%20this%20work%2C%20we%20introduce%20the%20first%20scalable%0ADNN-based%20system%20for%203D%20cloud%20retrieval%20that%20accommodates%20varying%20camera%20poses%0Aand%20solar%20directions.%20By%20integrating%20multiview%20cloud%20intensity%20images%20with%0Acamera%20poses%20and%20solar%20direction%20data%2C%20we%20achieve%20greater%20flexibility%20in%0Arecovery.%20Training%20of%20the%20DNN%20is%20performed%20by%20a%20novel%20two-stage%20scheme%20to%0Aaddress%20the%20high%20number%20of%20degrees%20of%20freedom%20in%20this%20problem.%20Our%20approach%0Ashows%20substantial%20improvements%20over%20previous%20state-of-the-art%2C%20particularly%20in%0Ahandling%20variations%20in%20the%20sun%27s%20zenith%20angle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNN-based%25203D%2520Cloud%2520Retrieval%2520for%2520Variable%2520Solar%2520Illumination%2520and%250A%2520%2520Multiview%2520Spaceborne%2520Imaging%26entry.906535625%3DTamar%2520Klein%2520and%2520Tom%2520Aizenberg%2520and%2520Roi%2520Ronen%26entry.1292438233%3D%2520%2520Climate%2520studies%2520often%2520rely%2520on%2520remotely%2520sensed%2520images%2520to%2520retrieve%250Atwo-dimensional%2520maps%2520of%2520cloud%2520properties.%2520To%2520advance%2520volumetric%2520analysis%252C%2520we%250Afocus%2520on%2520recovering%2520the%2520three-dimensional%2520%25283D%2529%2520heterogeneous%2520extinction%250Acoefficient%2520field%2520of%2520shallow%2520clouds%2520using%2520multiview%2520remote%2520sensing%2520data.%250AClimate%2520research%2520requires%2520large-scale%2520worldwide%2520statistics.%2520To%2520enable%2520scalable%250Adata%2520processing%252C%2520previous%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520can%2520infer%2520at%2520spaceborne%250Aremote%2520sensing%2520downlink%2520rates.%2520However%252C%2520prior%2520methods%2520are%2520limited%2520to%2520a%2520fixed%250Asolar%2520illumination%2520direction.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520first%2520scalable%250ADNN-based%2520system%2520for%25203D%2520cloud%2520retrieval%2520that%2520accommodates%2520varying%2520camera%2520poses%250Aand%2520solar%2520directions.%2520By%2520integrating%2520multiview%2520cloud%2520intensity%2520images%2520with%250Acamera%2520poses%2520and%2520solar%2520direction%2520data%252C%2520we%2520achieve%2520greater%2520flexibility%2520in%250Arecovery.%2520Training%2520of%2520the%2520DNN%2520is%2520performed%2520by%2520a%2520novel%2520two-stage%2520scheme%2520to%250Aaddress%2520the%2520high%2520number%2520of%2520degrees%2520of%2520freedom%2520in%2520this%2520problem.%2520Our%2520approach%250Ashows%2520substantial%2520improvements%2520over%2520previous%2520state-of-the-art%252C%2520particularly%2520in%250Ahandling%2520variations%2520in%2520the%2520sun%2527s%2520zenith%2520angle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNN-based%203D%20Cloud%20Retrieval%20for%20Variable%20Solar%20Illumination%20and%0A%20%20Multiview%20Spaceborne%20Imaging&entry.906535625=Tamar%20Klein%20and%20Tom%20Aizenberg%20and%20Roi%20Ronen&entry.1292438233=%20%20Climate%20studies%20often%20rely%20on%20remotely%20sensed%20images%20to%20retrieve%0Atwo-dimensional%20maps%20of%20cloud%20properties.%20To%20advance%20volumetric%20analysis%2C%20we%0Afocus%20on%20recovering%20the%20three-dimensional%20%283D%29%20heterogeneous%20extinction%0Acoefficient%20field%20of%20shallow%20clouds%20using%20multiview%20remote%20sensing%20data.%0AClimate%20research%20requires%20large-scale%20worldwide%20statistics.%20To%20enable%20scalable%0Adata%20processing%2C%20previous%20deep%20neural%20networks%20%28DNNs%29%20can%20infer%20at%20spaceborne%0Aremote%20sensing%20downlink%20rates.%20However%2C%20prior%20methods%20are%20limited%20to%20a%20fixed%0Asolar%20illumination%20direction.%20In%20this%20work%2C%20we%20introduce%20the%20first%20scalable%0ADNN-based%20system%20for%203D%20cloud%20retrieval%20that%20accommodates%20varying%20camera%20poses%0Aand%20solar%20directions.%20By%20integrating%20multiview%20cloud%20intensity%20images%20with%0Acamera%20poses%20and%20solar%20direction%20data%2C%20we%20achieve%20greater%20flexibility%20in%0Arecovery.%20Training%20of%20the%20DNN%20is%20performed%20by%20a%20novel%20two-stage%20scheme%20to%0Aaddress%20the%20high%20number%20of%20degrees%20of%20freedom%20in%20this%20problem.%20Our%20approach%0Ashows%20substantial%20improvements%20over%20previous%20state-of-the-art%2C%20particularly%20in%0Ahandling%20variations%20in%20the%20sun%27s%20zenith%20angle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04682v1&entry.124074799=Read"},
{"title": "HourVideo: 1-Hour Video-Language Understanding", "author": "Keshigeyan Chandrasegaran and Agrim Gupta and Lea M. Hadzic and Taran Kota and Jimming He and Crist\u00f3bal Eyzaguirre and Zane Durante and Manling Li and Jiajun Wu and Li Fei-Fei", "abstract": "  We present HourVideo, a benchmark dataset for hour-long video-language\nunderstanding. Our dataset consists of a novel task suite comprising\nsummarization, perception (recall, tracking), visual reasoning (spatial,\ntemporal, predictive, causal, counterfactual), and navigation (room-to-room,\nobject retrieval) tasks. HourVideo includes 500 manually curated egocentric\nvideos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and\nfeatures 12,976 high-quality, five-way multiple-choice questions. Benchmarking\nresults reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve\nmarginal improvements over random chance. In stark contrast, human experts\nsignificantly outperform the state-of-the-art long-context multimodal model,\nGemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal\ncapabilities. Our benchmark, evaluation toolkit, prompts, and documentation are\navailable at https://hourvideo.stanford.edu\n", "link": "http://arxiv.org/abs/2411.04998v1", "date": "2024-11-07", "relevancy": 2.1927, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HourVideo%3A%201-Hour%20Video-Language%20Understanding&body=Title%3A%20HourVideo%3A%201-Hour%20Video-Language%20Understanding%0AAuthor%3A%20Keshigeyan%20Chandrasegaran%20and%20Agrim%20Gupta%20and%20Lea%20M.%20Hadzic%20and%20Taran%20Kota%20and%20Jimming%20He%20and%20Crist%C3%B3bal%20Eyzaguirre%20and%20Zane%20Durante%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Li%20Fei-Fei%0AAbstract%3A%20%20%20We%20present%20HourVideo%2C%20a%20benchmark%20dataset%20for%20hour-long%20video-language%0Aunderstanding.%20Our%20dataset%20consists%20of%20a%20novel%20task%20suite%20comprising%0Asummarization%2C%20perception%20%28recall%2C%20tracking%29%2C%20visual%20reasoning%20%28spatial%2C%0Atemporal%2C%20predictive%2C%20causal%2C%20counterfactual%29%2C%20and%20navigation%20%28room-to-room%2C%0Aobject%20retrieval%29%20tasks.%20HourVideo%20includes%20500%20manually%20curated%20egocentric%0Avideos%20from%20the%20Ego4D%20dataset%2C%20spanning%20durations%20of%2020%20to%20120%20minutes%2C%20and%0Afeatures%2012%2C976%20high-quality%2C%20five-way%20multiple-choice%20questions.%20Benchmarking%0Aresults%20reveal%20that%20multimodal%20models%2C%20including%20GPT-4%20and%20LLaVA-NeXT%2C%20achieve%0Amarginal%20improvements%20over%20random%20chance.%20In%20stark%20contrast%2C%20human%20experts%0Asignificantly%20outperform%20the%20state-of-the-art%20long-context%20multimodal%20model%2C%0AGemini%20Pro%201.5%20%2885.0%25%20vs.%2037.3%25%29%2C%20highlighting%20a%20substantial%20gap%20in%20multimodal%0Acapabilities.%20Our%20benchmark%2C%20evaluation%20toolkit%2C%20prompts%2C%20and%20documentation%20are%0Aavailable%20at%20https%3A//hourvideo.stanford.edu%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHourVideo%253A%25201-Hour%2520Video-Language%2520Understanding%26entry.906535625%3DKeshigeyan%2520Chandrasegaran%2520and%2520Agrim%2520Gupta%2520and%2520Lea%2520M.%2520Hadzic%2520and%2520Taran%2520Kota%2520and%2520Jimming%2520He%2520and%2520Crist%25C3%25B3bal%2520Eyzaguirre%2520and%2520Zane%2520Durante%2520and%2520Manling%2520Li%2520and%2520Jiajun%2520Wu%2520and%2520Li%2520Fei-Fei%26entry.1292438233%3D%2520%2520We%2520present%2520HourVideo%252C%2520a%2520benchmark%2520dataset%2520for%2520hour-long%2520video-language%250Aunderstanding.%2520Our%2520dataset%2520consists%2520of%2520a%2520novel%2520task%2520suite%2520comprising%250Asummarization%252C%2520perception%2520%2528recall%252C%2520tracking%2529%252C%2520visual%2520reasoning%2520%2528spatial%252C%250Atemporal%252C%2520predictive%252C%2520causal%252C%2520counterfactual%2529%252C%2520and%2520navigation%2520%2528room-to-room%252C%250Aobject%2520retrieval%2529%2520tasks.%2520HourVideo%2520includes%2520500%2520manually%2520curated%2520egocentric%250Avideos%2520from%2520the%2520Ego4D%2520dataset%252C%2520spanning%2520durations%2520of%252020%2520to%2520120%2520minutes%252C%2520and%250Afeatures%252012%252C976%2520high-quality%252C%2520five-way%2520multiple-choice%2520questions.%2520Benchmarking%250Aresults%2520reveal%2520that%2520multimodal%2520models%252C%2520including%2520GPT-4%2520and%2520LLaVA-NeXT%252C%2520achieve%250Amarginal%2520improvements%2520over%2520random%2520chance.%2520In%2520stark%2520contrast%252C%2520human%2520experts%250Asignificantly%2520outperform%2520the%2520state-of-the-art%2520long-context%2520multimodal%2520model%252C%250AGemini%2520Pro%25201.5%2520%252885.0%2525%2520vs.%252037.3%2525%2529%252C%2520highlighting%2520a%2520substantial%2520gap%2520in%2520multimodal%250Acapabilities.%2520Our%2520benchmark%252C%2520evaluation%2520toolkit%252C%2520prompts%252C%2520and%2520documentation%2520are%250Aavailable%2520at%2520https%253A//hourvideo.stanford.edu%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HourVideo%3A%201-Hour%20Video-Language%20Understanding&entry.906535625=Keshigeyan%20Chandrasegaran%20and%20Agrim%20Gupta%20and%20Lea%20M.%20Hadzic%20and%20Taran%20Kota%20and%20Jimming%20He%20and%20Crist%C3%B3bal%20Eyzaguirre%20and%20Zane%20Durante%20and%20Manling%20Li%20and%20Jiajun%20Wu%20and%20Li%20Fei-Fei&entry.1292438233=%20%20We%20present%20HourVideo%2C%20a%20benchmark%20dataset%20for%20hour-long%20video-language%0Aunderstanding.%20Our%20dataset%20consists%20of%20a%20novel%20task%20suite%20comprising%0Asummarization%2C%20perception%20%28recall%2C%20tracking%29%2C%20visual%20reasoning%20%28spatial%2C%0Atemporal%2C%20predictive%2C%20causal%2C%20counterfactual%29%2C%20and%20navigation%20%28room-to-room%2C%0Aobject%20retrieval%29%20tasks.%20HourVideo%20includes%20500%20manually%20curated%20egocentric%0Avideos%20from%20the%20Ego4D%20dataset%2C%20spanning%20durations%20of%2020%20to%20120%20minutes%2C%20and%0Afeatures%2012%2C976%20high-quality%2C%20five-way%20multiple-choice%20questions.%20Benchmarking%0Aresults%20reveal%20that%20multimodal%20models%2C%20including%20GPT-4%20and%20LLaVA-NeXT%2C%20achieve%0Amarginal%20improvements%20over%20random%20chance.%20In%20stark%20contrast%2C%20human%20experts%0Asignificantly%20outperform%20the%20state-of-the-art%20long-context%20multimodal%20model%2C%0AGemini%20Pro%201.5%20%2885.0%25%20vs.%2037.3%25%29%2C%20highlighting%20a%20substantial%20gap%20in%20multimodal%0Acapabilities.%20Our%20benchmark%2C%20evaluation%20toolkit%2C%20prompts%2C%20and%20documentation%20are%0Aavailable%20at%20https%3A//hourvideo.stanford.edu%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04998v1&entry.124074799=Read"},
{"title": "Local Padding in Patch-Based GANs for Seamless Infinite-Sized Texture\n  Synthesis", "author": "Alhasan Abdellatif and Ahmed H. Elsheikh and Hannah P. Menke", "abstract": "  Texture models based on Generative Adversarial Networks (GANs) use\nzero-padding to implicitly encode positional information of the image features.\nHowever, when extending the spatial input to generate images at large sizes,\nzero-padding can often lead to degradation in image quality due to the\nincorrect positional information at the center of the image. Moreover,\nzero-padding can limit the diversity within the generated large images. In this\npaper, we propose a novel approach for generating stochastic texture images at\nlarge arbitrary sizes using GANs based on patch-by-patch generation. Instead of\nzero-padding, the model uses \\textit{local padding} in the generator that\nshares border features between the generated patches; providing positional\ncontext and ensuring consistency at the boundaries. The proposed models are\ntrainable on a single texture image and have a constant GPU scalability with\nrespect to the output image size, and hence can generate images of infinite\nsizes. We show in the experiments that our method has a significant advancement\nbeyond existing GANs-based texture models in terms of the quality and diversity\nof the generated textures. Furthermore, the implementation of local padding in\nthe state-of-the-art super-resolution models effectively eliminates tiling\nartifacts enabling large-scale super-resolution. Our code is available at\n\\url{https://github.com/ai4netzero/Infinite_Texture_GANs}.\n", "link": "http://arxiv.org/abs/2309.02340v5", "date": "2024-11-07", "relevancy": 2.1824, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5496}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Padding%20in%20Patch-Based%20GANs%20for%20Seamless%20Infinite-Sized%20Texture%0A%20%20Synthesis&body=Title%3A%20Local%20Padding%20in%20Patch-Based%20GANs%20for%20Seamless%20Infinite-Sized%20Texture%0A%20%20Synthesis%0AAuthor%3A%20Alhasan%20Abdellatif%20and%20Ahmed%20H.%20Elsheikh%20and%20Hannah%20P.%20Menke%0AAbstract%3A%20%20%20Texture%20models%20based%20on%20Generative%20Adversarial%20Networks%20%28GANs%29%20use%0Azero-padding%20to%20implicitly%20encode%20positional%20information%20of%20the%20image%20features.%0AHowever%2C%20when%20extending%20the%20spatial%20input%20to%20generate%20images%20at%20large%20sizes%2C%0Azero-padding%20can%20often%20lead%20to%20degradation%20in%20image%20quality%20due%20to%20the%0Aincorrect%20positional%20information%20at%20the%20center%20of%20the%20image.%20Moreover%2C%0Azero-padding%20can%20limit%20the%20diversity%20within%20the%20generated%20large%20images.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20for%20generating%20stochastic%20texture%20images%20at%0Alarge%20arbitrary%20sizes%20using%20GANs%20based%20on%20patch-by-patch%20generation.%20Instead%20of%0Azero-padding%2C%20the%20model%20uses%20%5Ctextit%7Blocal%20padding%7D%20in%20the%20generator%20that%0Ashares%20border%20features%20between%20the%20generated%20patches%3B%20providing%20positional%0Acontext%20and%20ensuring%20consistency%20at%20the%20boundaries.%20The%20proposed%20models%20are%0Atrainable%20on%20a%20single%20texture%20image%20and%20have%20a%20constant%20GPU%20scalability%20with%0Arespect%20to%20the%20output%20image%20size%2C%20and%20hence%20can%20generate%20images%20of%20infinite%0Asizes.%20We%20show%20in%20the%20experiments%20that%20our%20method%20has%20a%20significant%20advancement%0Abeyond%20existing%20GANs-based%20texture%20models%20in%20terms%20of%20the%20quality%20and%20diversity%0Aof%20the%20generated%20textures.%20Furthermore%2C%20the%20implementation%20of%20local%20padding%20in%0Athe%20state-of-the-art%20super-resolution%20models%20effectively%20eliminates%20tiling%0Aartifacts%20enabling%20large-scale%20super-resolution.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ai4netzero/Infinite_Texture_GANs%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.02340v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Padding%2520in%2520Patch-Based%2520GANs%2520for%2520Seamless%2520Infinite-Sized%2520Texture%250A%2520%2520Synthesis%26entry.906535625%3DAlhasan%2520Abdellatif%2520and%2520Ahmed%2520H.%2520Elsheikh%2520and%2520Hannah%2520P.%2520Menke%26entry.1292438233%3D%2520%2520Texture%2520models%2520based%2520on%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520use%250Azero-padding%2520to%2520implicitly%2520encode%2520positional%2520information%2520of%2520the%2520image%2520features.%250AHowever%252C%2520when%2520extending%2520the%2520spatial%2520input%2520to%2520generate%2520images%2520at%2520large%2520sizes%252C%250Azero-padding%2520can%2520often%2520lead%2520to%2520degradation%2520in%2520image%2520quality%2520due%2520to%2520the%250Aincorrect%2520positional%2520information%2520at%2520the%2520center%2520of%2520the%2520image.%2520Moreover%252C%250Azero-padding%2520can%2520limit%2520the%2520diversity%2520within%2520the%2520generated%2520large%2520images.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520generating%2520stochastic%2520texture%2520images%2520at%250Alarge%2520arbitrary%2520sizes%2520using%2520GANs%2520based%2520on%2520patch-by-patch%2520generation.%2520Instead%2520of%250Azero-padding%252C%2520the%2520model%2520uses%2520%255Ctextit%257Blocal%2520padding%257D%2520in%2520the%2520generator%2520that%250Ashares%2520border%2520features%2520between%2520the%2520generated%2520patches%253B%2520providing%2520positional%250Acontext%2520and%2520ensuring%2520consistency%2520at%2520the%2520boundaries.%2520The%2520proposed%2520models%2520are%250Atrainable%2520on%2520a%2520single%2520texture%2520image%2520and%2520have%2520a%2520constant%2520GPU%2520scalability%2520with%250Arespect%2520to%2520the%2520output%2520image%2520size%252C%2520and%2520hence%2520can%2520generate%2520images%2520of%2520infinite%250Asizes.%2520We%2520show%2520in%2520the%2520experiments%2520that%2520our%2520method%2520has%2520a%2520significant%2520advancement%250Abeyond%2520existing%2520GANs-based%2520texture%2520models%2520in%2520terms%2520of%2520the%2520quality%2520and%2520diversity%250Aof%2520the%2520generated%2520textures.%2520Furthermore%252C%2520the%2520implementation%2520of%2520local%2520padding%2520in%250Athe%2520state-of-the-art%2520super-resolution%2520models%2520effectively%2520eliminates%2520tiling%250Aartifacts%2520enabling%2520large-scale%2520super-resolution.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/ai4netzero/Infinite_Texture_GANs%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.02340v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Padding%20in%20Patch-Based%20GANs%20for%20Seamless%20Infinite-Sized%20Texture%0A%20%20Synthesis&entry.906535625=Alhasan%20Abdellatif%20and%20Ahmed%20H.%20Elsheikh%20and%20Hannah%20P.%20Menke&entry.1292438233=%20%20Texture%20models%20based%20on%20Generative%20Adversarial%20Networks%20%28GANs%29%20use%0Azero-padding%20to%20implicitly%20encode%20positional%20information%20of%20the%20image%20features.%0AHowever%2C%20when%20extending%20the%20spatial%20input%20to%20generate%20images%20at%20large%20sizes%2C%0Azero-padding%20can%20often%20lead%20to%20degradation%20in%20image%20quality%20due%20to%20the%0Aincorrect%20positional%20information%20at%20the%20center%20of%20the%20image.%20Moreover%2C%0Azero-padding%20can%20limit%20the%20diversity%20within%20the%20generated%20large%20images.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20for%20generating%20stochastic%20texture%20images%20at%0Alarge%20arbitrary%20sizes%20using%20GANs%20based%20on%20patch-by-patch%20generation.%20Instead%20of%0Azero-padding%2C%20the%20model%20uses%20%5Ctextit%7Blocal%20padding%7D%20in%20the%20generator%20that%0Ashares%20border%20features%20between%20the%20generated%20patches%3B%20providing%20positional%0Acontext%20and%20ensuring%20consistency%20at%20the%20boundaries.%20The%20proposed%20models%20are%0Atrainable%20on%20a%20single%20texture%20image%20and%20have%20a%20constant%20GPU%20scalability%20with%0Arespect%20to%20the%20output%20image%20size%2C%20and%20hence%20can%20generate%20images%20of%20infinite%0Asizes.%20We%20show%20in%20the%20experiments%20that%20our%20method%20has%20a%20significant%20advancement%0Abeyond%20existing%20GANs-based%20texture%20models%20in%20terms%20of%20the%20quality%20and%20diversity%0Aof%20the%20generated%20textures.%20Furthermore%2C%20the%20implementation%20of%20local%20padding%20in%0Athe%20state-of-the-art%20super-resolution%20models%20effectively%20eliminates%20tiling%0Aartifacts%20enabling%20large-scale%20super-resolution.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ai4netzero/Infinite_Texture_GANs%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.02340v5&entry.124074799=Read"},
{"title": "Learning from Demonstration with Hierarchical Policy Abstractions Toward\n  High-Performance and Courteous Autonomous Racing", "author": "Chanyoung Chung and Hyunki Seong and David Hyunchul Shim", "abstract": "  Fully autonomous racing demands not only high-speed driving but also fair and\ncourteous maneuvers. In this paper, we propose an autonomous racing framework\nthat learns complex racing behaviors from expert demonstrations using\nhierarchical policy abstractions. At the trajectory level, our policy model\npredicts a dense distribution map indicating the likelihood of trajectories\nlearned from offline demonstrations. The maximum likelihood trajectory is then\npassed to the control-level policy, which generates control inputs in a\nresidual fashion, considering vehicle dynamics at the limits of performance. We\nevaluate our framework in a high-fidelity racing simulator and compare it\nagainst competing baselines in challenging multi-agent adversarial scenarios.\nQuantitative and qualitative results show that our trajectory planning policy\nsignificantly outperforms the baselines, and the residual control policy\nimproves lap time and tracking accuracy. Moreover, challenging closed-loop\nexperiments with ten opponents show that our framework can overtake other\nvehicles by understanding nuanced interactions, effectively balancing\nperformance and courtesy like professional drivers.\n", "link": "http://arxiv.org/abs/2411.04735v1", "date": "2024-11-07", "relevancy": 2.168, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5602}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Demonstration%20with%20Hierarchical%20Policy%20Abstractions%20Toward%0A%20%20High-Performance%20and%20Courteous%20Autonomous%20Racing&body=Title%3A%20Learning%20from%20Demonstration%20with%20Hierarchical%20Policy%20Abstractions%20Toward%0A%20%20High-Performance%20and%20Courteous%20Autonomous%20Racing%0AAuthor%3A%20Chanyoung%20Chung%20and%20Hyunki%20Seong%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20%20%20Fully%20autonomous%20racing%20demands%20not%20only%20high-speed%20driving%20but%20also%20fair%20and%0Acourteous%20maneuvers.%20In%20this%20paper%2C%20we%20propose%20an%20autonomous%20racing%20framework%0Athat%20learns%20complex%20racing%20behaviors%20from%20expert%20demonstrations%20using%0Ahierarchical%20policy%20abstractions.%20At%20the%20trajectory%20level%2C%20our%20policy%20model%0Apredicts%20a%20dense%20distribution%20map%20indicating%20the%20likelihood%20of%20trajectories%0Alearned%20from%20offline%20demonstrations.%20The%20maximum%20likelihood%20trajectory%20is%20then%0Apassed%20to%20the%20control-level%20policy%2C%20which%20generates%20control%20inputs%20in%20a%0Aresidual%20fashion%2C%20considering%20vehicle%20dynamics%20at%20the%20limits%20of%20performance.%20We%0Aevaluate%20our%20framework%20in%20a%20high-fidelity%20racing%20simulator%20and%20compare%20it%0Aagainst%20competing%20baselines%20in%20challenging%20multi-agent%20adversarial%20scenarios.%0AQuantitative%20and%20qualitative%20results%20show%20that%20our%20trajectory%20planning%20policy%0Asignificantly%20outperforms%20the%20baselines%2C%20and%20the%20residual%20control%20policy%0Aimproves%20lap%20time%20and%20tracking%20accuracy.%20Moreover%2C%20challenging%20closed-loop%0Aexperiments%20with%20ten%20opponents%20show%20that%20our%20framework%20can%20overtake%20other%0Avehicles%20by%20understanding%20nuanced%20interactions%2C%20effectively%20balancing%0Aperformance%20and%20courtesy%20like%20professional%20drivers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Demonstration%2520with%2520Hierarchical%2520Policy%2520Abstractions%2520Toward%250A%2520%2520High-Performance%2520and%2520Courteous%2520Autonomous%2520Racing%26entry.906535625%3DChanyoung%2520Chung%2520and%2520Hyunki%2520Seong%2520and%2520David%2520Hyunchul%2520Shim%26entry.1292438233%3D%2520%2520Fully%2520autonomous%2520racing%2520demands%2520not%2520only%2520high-speed%2520driving%2520but%2520also%2520fair%2520and%250Acourteous%2520maneuvers.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520autonomous%2520racing%2520framework%250Athat%2520learns%2520complex%2520racing%2520behaviors%2520from%2520expert%2520demonstrations%2520using%250Ahierarchical%2520policy%2520abstractions.%2520At%2520the%2520trajectory%2520level%252C%2520our%2520policy%2520model%250Apredicts%2520a%2520dense%2520distribution%2520map%2520indicating%2520the%2520likelihood%2520of%2520trajectories%250Alearned%2520from%2520offline%2520demonstrations.%2520The%2520maximum%2520likelihood%2520trajectory%2520is%2520then%250Apassed%2520to%2520the%2520control-level%2520policy%252C%2520which%2520generates%2520control%2520inputs%2520in%2520a%250Aresidual%2520fashion%252C%2520considering%2520vehicle%2520dynamics%2520at%2520the%2520limits%2520of%2520performance.%2520We%250Aevaluate%2520our%2520framework%2520in%2520a%2520high-fidelity%2520racing%2520simulator%2520and%2520compare%2520it%250Aagainst%2520competing%2520baselines%2520in%2520challenging%2520multi-agent%2520adversarial%2520scenarios.%250AQuantitative%2520and%2520qualitative%2520results%2520show%2520that%2520our%2520trajectory%2520planning%2520policy%250Asignificantly%2520outperforms%2520the%2520baselines%252C%2520and%2520the%2520residual%2520control%2520policy%250Aimproves%2520lap%2520time%2520and%2520tracking%2520accuracy.%2520Moreover%252C%2520challenging%2520closed-loop%250Aexperiments%2520with%2520ten%2520opponents%2520show%2520that%2520our%2520framework%2520can%2520overtake%2520other%250Avehicles%2520by%2520understanding%2520nuanced%2520interactions%252C%2520effectively%2520balancing%250Aperformance%2520and%2520courtesy%2520like%2520professional%2520drivers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Demonstration%20with%20Hierarchical%20Policy%20Abstractions%20Toward%0A%20%20High-Performance%20and%20Courteous%20Autonomous%20Racing&entry.906535625=Chanyoung%20Chung%20and%20Hyunki%20Seong%20and%20David%20Hyunchul%20Shim&entry.1292438233=%20%20Fully%20autonomous%20racing%20demands%20not%20only%20high-speed%20driving%20but%20also%20fair%20and%0Acourteous%20maneuvers.%20In%20this%20paper%2C%20we%20propose%20an%20autonomous%20racing%20framework%0Athat%20learns%20complex%20racing%20behaviors%20from%20expert%20demonstrations%20using%0Ahierarchical%20policy%20abstractions.%20At%20the%20trajectory%20level%2C%20our%20policy%20model%0Apredicts%20a%20dense%20distribution%20map%20indicating%20the%20likelihood%20of%20trajectories%0Alearned%20from%20offline%20demonstrations.%20The%20maximum%20likelihood%20trajectory%20is%20then%0Apassed%20to%20the%20control-level%20policy%2C%20which%20generates%20control%20inputs%20in%20a%0Aresidual%20fashion%2C%20considering%20vehicle%20dynamics%20at%20the%20limits%20of%20performance.%20We%0Aevaluate%20our%20framework%20in%20a%20high-fidelity%20racing%20simulator%20and%20compare%20it%0Aagainst%20competing%20baselines%20in%20challenging%20multi-agent%20adversarial%20scenarios.%0AQuantitative%20and%20qualitative%20results%20show%20that%20our%20trajectory%20planning%20policy%0Asignificantly%20outperforms%20the%20baselines%2C%20and%20the%20residual%20control%20policy%0Aimproves%20lap%20time%20and%20tracking%20accuracy.%20Moreover%2C%20challenging%20closed-loop%0Aexperiments%20with%20ten%20opponents%20show%20that%20our%20framework%20can%20overtake%20other%0Avehicles%20by%20understanding%20nuanced%20interactions%2C%20effectively%20balancing%0Aperformance%20and%20courtesy%20like%20professional%20drivers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04735v1&entry.124074799=Read"},
{"title": "Cross- and Intra-image Prototypical Learning for Multi-label Disease\n  Diagnosis and Interpretation", "author": "Chong Wang and Fengbei Liu and Yuanhong Chen and Helen Frazer and Gustavo Carneiro", "abstract": "  Recent advances in prototypical learning have shown remarkable potential to\nprovide useful decision interpretations associating activation maps and\npredictions with class-specific training prototypes. Such prototypical learning\nhas been well-studied for various single-label diseases, but for quite relevant\nand more challenging multi-label diagnosis, where multiple diseases are often\nconcurrent within an image, existing prototypical learning models struggle to\nobtain meaningful activation maps and effective class prototypes due to the\nentanglement of the multiple diseases. In this paper, we present a novel Cross-\nand Intra-image Prototypical Learning (CIPL) framework, for accurate\nmulti-label disease diagnosis and interpretation from medical images. CIPL\ntakes advantage of common cross-image semantics to disentangle the multiple\ndiseases when learning the prototypes, allowing a comprehensive understanding\nof complicated pathological lesions. Furthermore, we propose a new two-level\nalignment-based regularisation strategy that effectively leverages consistent\nintra-image information to enhance interpretation robustness and predictive\nperformance. Extensive experiments show that our CIPL attains the\nstate-of-the-art (SOTA) classification accuracy in two public multi-label\nbenchmarks of disease diagnosis: thoracic radiography and fundus images.\nQuantitative interpretability results show that CIPL also has superiority in\nweakly-supervised thoracic disease localisation over other leading saliency-\nand prototype-based explanation methods.\n", "link": "http://arxiv.org/abs/2411.04607v1", "date": "2024-11-07", "relevancy": 2.1506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5117}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-%20and%20Intra-image%20Prototypical%20Learning%20for%20Multi-label%20Disease%0A%20%20Diagnosis%20and%20Interpretation&body=Title%3A%20Cross-%20and%20Intra-image%20Prototypical%20Learning%20for%20Multi-label%20Disease%0A%20%20Diagnosis%20and%20Interpretation%0AAuthor%3A%20Chong%20Wang%20and%20Fengbei%20Liu%20and%20Yuanhong%20Chen%20and%20Helen%20Frazer%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20Recent%20advances%20in%20prototypical%20learning%20have%20shown%20remarkable%20potential%20to%0Aprovide%20useful%20decision%20interpretations%20associating%20activation%20maps%20and%0Apredictions%20with%20class-specific%20training%20prototypes.%20Such%20prototypical%20learning%0Ahas%20been%20well-studied%20for%20various%20single-label%20diseases%2C%20but%20for%20quite%20relevant%0Aand%20more%20challenging%20multi-label%20diagnosis%2C%20where%20multiple%20diseases%20are%20often%0Aconcurrent%20within%20an%20image%2C%20existing%20prototypical%20learning%20models%20struggle%20to%0Aobtain%20meaningful%20activation%20maps%20and%20effective%20class%20prototypes%20due%20to%20the%0Aentanglement%20of%20the%20multiple%20diseases.%20In%20this%20paper%2C%20we%20present%20a%20novel%20Cross-%0Aand%20Intra-image%20Prototypical%20Learning%20%28CIPL%29%20framework%2C%20for%20accurate%0Amulti-label%20disease%20diagnosis%20and%20interpretation%20from%20medical%20images.%20CIPL%0Atakes%20advantage%20of%20common%20cross-image%20semantics%20to%20disentangle%20the%20multiple%0Adiseases%20when%20learning%20the%20prototypes%2C%20allowing%20a%20comprehensive%20understanding%0Aof%20complicated%20pathological%20lesions.%20Furthermore%2C%20we%20propose%20a%20new%20two-level%0Aalignment-based%20regularisation%20strategy%20that%20effectively%20leverages%20consistent%0Aintra-image%20information%20to%20enhance%20interpretation%20robustness%20and%20predictive%0Aperformance.%20Extensive%20experiments%20show%20that%20our%20CIPL%20attains%20the%0Astate-of-the-art%20%28SOTA%29%20classification%20accuracy%20in%20two%20public%20multi-label%0Abenchmarks%20of%20disease%20diagnosis%3A%20thoracic%20radiography%20and%20fundus%20images.%0AQuantitative%20interpretability%20results%20show%20that%20CIPL%20also%20has%20superiority%20in%0Aweakly-supervised%20thoracic%20disease%20localisation%20over%20other%20leading%20saliency-%0Aand%20prototype-based%20explanation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-%2520and%2520Intra-image%2520Prototypical%2520Learning%2520for%2520Multi-label%2520Disease%250A%2520%2520Diagnosis%2520and%2520Interpretation%26entry.906535625%3DChong%2520Wang%2520and%2520Fengbei%2520Liu%2520and%2520Yuanhong%2520Chen%2520and%2520Helen%2520Frazer%2520and%2520Gustavo%2520Carneiro%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520prototypical%2520learning%2520have%2520shown%2520remarkable%2520potential%2520to%250Aprovide%2520useful%2520decision%2520interpretations%2520associating%2520activation%2520maps%2520and%250Apredictions%2520with%2520class-specific%2520training%2520prototypes.%2520Such%2520prototypical%2520learning%250Ahas%2520been%2520well-studied%2520for%2520various%2520single-label%2520diseases%252C%2520but%2520for%2520quite%2520relevant%250Aand%2520more%2520challenging%2520multi-label%2520diagnosis%252C%2520where%2520multiple%2520diseases%2520are%2520often%250Aconcurrent%2520within%2520an%2520image%252C%2520existing%2520prototypical%2520learning%2520models%2520struggle%2520to%250Aobtain%2520meaningful%2520activation%2520maps%2520and%2520effective%2520class%2520prototypes%2520due%2520to%2520the%250Aentanglement%2520of%2520the%2520multiple%2520diseases.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520Cross-%250Aand%2520Intra-image%2520Prototypical%2520Learning%2520%2528CIPL%2529%2520framework%252C%2520for%2520accurate%250Amulti-label%2520disease%2520diagnosis%2520and%2520interpretation%2520from%2520medical%2520images.%2520CIPL%250Atakes%2520advantage%2520of%2520common%2520cross-image%2520semantics%2520to%2520disentangle%2520the%2520multiple%250Adiseases%2520when%2520learning%2520the%2520prototypes%252C%2520allowing%2520a%2520comprehensive%2520understanding%250Aof%2520complicated%2520pathological%2520lesions.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%2520two-level%250Aalignment-based%2520regularisation%2520strategy%2520that%2520effectively%2520leverages%2520consistent%250Aintra-image%2520information%2520to%2520enhance%2520interpretation%2520robustness%2520and%2520predictive%250Aperformance.%2520Extensive%2520experiments%2520show%2520that%2520our%2520CIPL%2520attains%2520the%250Astate-of-the-art%2520%2528SOTA%2529%2520classification%2520accuracy%2520in%2520two%2520public%2520multi-label%250Abenchmarks%2520of%2520disease%2520diagnosis%253A%2520thoracic%2520radiography%2520and%2520fundus%2520images.%250AQuantitative%2520interpretability%2520results%2520show%2520that%2520CIPL%2520also%2520has%2520superiority%2520in%250Aweakly-supervised%2520thoracic%2520disease%2520localisation%2520over%2520other%2520leading%2520saliency-%250Aand%2520prototype-based%2520explanation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-%20and%20Intra-image%20Prototypical%20Learning%20for%20Multi-label%20Disease%0A%20%20Diagnosis%20and%20Interpretation&entry.906535625=Chong%20Wang%20and%20Fengbei%20Liu%20and%20Yuanhong%20Chen%20and%20Helen%20Frazer%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20Recent%20advances%20in%20prototypical%20learning%20have%20shown%20remarkable%20potential%20to%0Aprovide%20useful%20decision%20interpretations%20associating%20activation%20maps%20and%0Apredictions%20with%20class-specific%20training%20prototypes.%20Such%20prototypical%20learning%0Ahas%20been%20well-studied%20for%20various%20single-label%20diseases%2C%20but%20for%20quite%20relevant%0Aand%20more%20challenging%20multi-label%20diagnosis%2C%20where%20multiple%20diseases%20are%20often%0Aconcurrent%20within%20an%20image%2C%20existing%20prototypical%20learning%20models%20struggle%20to%0Aobtain%20meaningful%20activation%20maps%20and%20effective%20class%20prototypes%20due%20to%20the%0Aentanglement%20of%20the%20multiple%20diseases.%20In%20this%20paper%2C%20we%20present%20a%20novel%20Cross-%0Aand%20Intra-image%20Prototypical%20Learning%20%28CIPL%29%20framework%2C%20for%20accurate%0Amulti-label%20disease%20diagnosis%20and%20interpretation%20from%20medical%20images.%20CIPL%0Atakes%20advantage%20of%20common%20cross-image%20semantics%20to%20disentangle%20the%20multiple%0Adiseases%20when%20learning%20the%20prototypes%2C%20allowing%20a%20comprehensive%20understanding%0Aof%20complicated%20pathological%20lesions.%20Furthermore%2C%20we%20propose%20a%20new%20two-level%0Aalignment-based%20regularisation%20strategy%20that%20effectively%20leverages%20consistent%0Aintra-image%20information%20to%20enhance%20interpretation%20robustness%20and%20predictive%0Aperformance.%20Extensive%20experiments%20show%20that%20our%20CIPL%20attains%20the%0Astate-of-the-art%20%28SOTA%29%20classification%20accuracy%20in%20two%20public%20multi-label%0Abenchmarks%20of%20disease%20diagnosis%3A%20thoracic%20radiography%20and%20fundus%20images.%0AQuantitative%20interpretability%20results%20show%20that%20CIPL%20also%20has%20superiority%20in%0Aweakly-supervised%20thoracic%20disease%20localisation%20over%20other%20leading%20saliency-%0Aand%20prototype-based%20explanation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04607v1&entry.124074799=Read"},
{"title": "Brain Tumour Removing and Missing Modality Generation using 3D WDM", "author": "Andr\u00e9 Ferreira and Gijs Luijten and Behrus Puladi and Jens Kleesiek and Victor Alves and Jan Egger", "abstract": "  This paper presents the second-placed solution for task 8 and the\nparticipation solution for task 7 of BraTS 2024. The adoption of automated\nbrain analysis algorithms to support clinical practice is increasing. However,\nmany of these algorithms struggle with the presence of brain lesions or the\nabsence of certain MRI modalities. The alterations in the brain's morphology\nleads to high variability and thus poor performance of predictive models that\nwere trained only on healthy brains. The lack of information that is usually\nprovided by some of the missing MRI modalities also reduces the reliability of\nthe prediction models trained with all modalities. In order to improve the\nperformance of these models, we propose the use of conditional 3D wavelet\ndiffusion models. The wavelet transform enabled full-resolution image training\nand prediction on a GPU with 48 GB VRAM, without patching or downsampling,\npreserving all information for prediction. For the inpainting task of BraTS\n2024, the use of a large and variable number of healthy masks and the stability\nand efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and\n0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE,\nPSNR and SSIM respectively). The code for these tasks is available at\nhttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n", "link": "http://arxiv.org/abs/2411.04630v1", "date": "2024-11-07", "relevancy": 2.1496, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5489}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5351}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM&body=Title%3A%20Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM%0AAuthor%3A%20Andr%C3%A9%20Ferreira%20and%20Gijs%20Luijten%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20second-placed%20solution%20for%20task%208%20and%20the%0Aparticipation%20solution%20for%20task%207%20of%20BraTS%202024.%20The%20adoption%20of%20automated%0Abrain%20analysis%20algorithms%20to%20support%20clinical%20practice%20is%20increasing.%20However%2C%0Amany%20of%20these%20algorithms%20struggle%20with%20the%20presence%20of%20brain%20lesions%20or%20the%0Aabsence%20of%20certain%20MRI%20modalities.%20The%20alterations%20in%20the%20brain%27s%20morphology%0Aleads%20to%20high%20variability%20and%20thus%20poor%20performance%20of%20predictive%20models%20that%0Awere%20trained%20only%20on%20healthy%20brains.%20The%20lack%20of%20information%20that%20is%20usually%0Aprovided%20by%20some%20of%20the%20missing%20MRI%20modalities%20also%20reduces%20the%20reliability%20of%0Athe%20prediction%20models%20trained%20with%20all%20modalities.%20In%20order%20to%20improve%20the%0Aperformance%20of%20these%20models%2C%20we%20propose%20the%20use%20of%20conditional%203D%20wavelet%0Adiffusion%20models.%20The%20wavelet%20transform%20enabled%20full-resolution%20image%20training%0Aand%20prediction%20on%20a%20GPU%20with%2048%20GB%20VRAM%2C%20without%20patching%20or%20downsampling%2C%0Apreserving%20all%20information%20for%20prediction.%20For%20the%20inpainting%20task%20of%20BraTS%0A2024%2C%20the%20use%20of%20a%20large%20and%20variable%20number%20of%20healthy%20masks%20and%20the%20stability%0Aand%20efficiency%20of%20the%203D%20wavelet%20diffusion%20model%20resulted%20in%200.007%2C%2022.61%20and%0A0.842%20in%20the%20validation%20set%20and%200.07%20%2C%2022.8%20and%200.91%20in%20the%20testing%20set%20%28MSE%2C%0APSNR%20and%20SSIM%20respectively%29.%20The%20code%20for%20these%20tasks%20is%20available%20at%0Ahttps%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Tumour%2520Removing%2520and%2520Missing%2520Modality%2520Generation%2520using%25203D%2520WDM%26entry.906535625%3DAndr%25C3%25A9%2520Ferreira%2520and%2520Gijs%2520Luijten%2520and%2520Behrus%2520Puladi%2520and%2520Jens%2520Kleesiek%2520and%2520Victor%2520Alves%2520and%2520Jan%2520Egger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520second-placed%2520solution%2520for%2520task%25208%2520and%2520the%250Aparticipation%2520solution%2520for%2520task%25207%2520of%2520BraTS%25202024.%2520The%2520adoption%2520of%2520automated%250Abrain%2520analysis%2520algorithms%2520to%2520support%2520clinical%2520practice%2520is%2520increasing.%2520However%252C%250Amany%2520of%2520these%2520algorithms%2520struggle%2520with%2520the%2520presence%2520of%2520brain%2520lesions%2520or%2520the%250Aabsence%2520of%2520certain%2520MRI%2520modalities.%2520The%2520alterations%2520in%2520the%2520brain%2527s%2520morphology%250Aleads%2520to%2520high%2520variability%2520and%2520thus%2520poor%2520performance%2520of%2520predictive%2520models%2520that%250Awere%2520trained%2520only%2520on%2520healthy%2520brains.%2520The%2520lack%2520of%2520information%2520that%2520is%2520usually%250Aprovided%2520by%2520some%2520of%2520the%2520missing%2520MRI%2520modalities%2520also%2520reduces%2520the%2520reliability%2520of%250Athe%2520prediction%2520models%2520trained%2520with%2520all%2520modalities.%2520In%2520order%2520to%2520improve%2520the%250Aperformance%2520of%2520these%2520models%252C%2520we%2520propose%2520the%2520use%2520of%2520conditional%25203D%2520wavelet%250Adiffusion%2520models.%2520The%2520wavelet%2520transform%2520enabled%2520full-resolution%2520image%2520training%250Aand%2520prediction%2520on%2520a%2520GPU%2520with%252048%2520GB%2520VRAM%252C%2520without%2520patching%2520or%2520downsampling%252C%250Apreserving%2520all%2520information%2520for%2520prediction.%2520For%2520the%2520inpainting%2520task%2520of%2520BraTS%250A2024%252C%2520the%2520use%2520of%2520a%2520large%2520and%2520variable%2520number%2520of%2520healthy%2520masks%2520and%2520the%2520stability%250Aand%2520efficiency%2520of%2520the%25203D%2520wavelet%2520diffusion%2520model%2520resulted%2520in%25200.007%252C%252022.61%2520and%250A0.842%2520in%2520the%2520validation%2520set%2520and%25200.07%2520%252C%252022.8%2520and%25200.91%2520in%2520the%2520testing%2520set%2520%2528MSE%252C%250APSNR%2520and%2520SSIM%2520respectively%2529.%2520The%2520code%2520for%2520these%2520tasks%2520is%2520available%2520at%250Ahttps%253A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Tumour%20Removing%20and%20Missing%20Modality%20Generation%20using%203D%20WDM&entry.906535625=Andr%C3%A9%20Ferreira%20and%20Gijs%20Luijten%20and%20Behrus%20Puladi%20and%20Jens%20Kleesiek%20and%20Victor%20Alves%20and%20Jan%20Egger&entry.1292438233=%20%20This%20paper%20presents%20the%20second-placed%20solution%20for%20task%208%20and%20the%0Aparticipation%20solution%20for%20task%207%20of%20BraTS%202024.%20The%20adoption%20of%20automated%0Abrain%20analysis%20algorithms%20to%20support%20clinical%20practice%20is%20increasing.%20However%2C%0Amany%20of%20these%20algorithms%20struggle%20with%20the%20presence%20of%20brain%20lesions%20or%20the%0Aabsence%20of%20certain%20MRI%20modalities.%20The%20alterations%20in%20the%20brain%27s%20morphology%0Aleads%20to%20high%20variability%20and%20thus%20poor%20performance%20of%20predictive%20models%20that%0Awere%20trained%20only%20on%20healthy%20brains.%20The%20lack%20of%20information%20that%20is%20usually%0Aprovided%20by%20some%20of%20the%20missing%20MRI%20modalities%20also%20reduces%20the%20reliability%20of%0Athe%20prediction%20models%20trained%20with%20all%20modalities.%20In%20order%20to%20improve%20the%0Aperformance%20of%20these%20models%2C%20we%20propose%20the%20use%20of%20conditional%203D%20wavelet%0Adiffusion%20models.%20The%20wavelet%20transform%20enabled%20full-resolution%20image%20training%0Aand%20prediction%20on%20a%20GPU%20with%2048%20GB%20VRAM%2C%20without%20patching%20or%20downsampling%2C%0Apreserving%20all%20information%20for%20prediction.%20For%20the%20inpainting%20task%20of%20BraTS%0A2024%2C%20the%20use%20of%20a%20large%20and%20variable%20number%20of%20healthy%20masks%20and%20the%20stability%0Aand%20efficiency%20of%20the%203D%20wavelet%20diffusion%20model%20resulted%20in%200.007%2C%2022.61%20and%0A0.842%20in%20the%20validation%20set%20and%200.07%20%2C%2022.8%20and%200.91%20in%20the%20testing%20set%20%28MSE%2C%0APSNR%20and%20SSIM%20respectively%29.%20The%20code%20for%20these%20tasks%20is%20available%20at%0Ahttps%3A//github.com/ShadowTwin41/BraTS_2023_2024_solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04630v1&entry.124074799=Read"},
{"title": "Perceptions of Linguistic Uncertainty by Language Models and Humans", "author": "Catarina G Belem and Markelle Kelly and Mark Steyvers and Sameer Singh and Padhraic Smyth", "abstract": "  _Uncertainty expressions_ such as \"probably\" or \"highly unlikely\" are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans quantitatively interpret\nthese expressions, there has been little inquiry into the abilities of language\nmodels in the same context. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We find that 7\nout of 10 models are able to map uncertainty expressions to probabilistic\nresponses in a human-like manner. However, we observe systematically different\nbehavior depending on whether a statement is actually true or false. This\nsensitivity indicates that language models are substantially more susceptible\nto bias based on their prior knowledge (as compared to humans). These findings\nraise important questions and have broad implications for human-AI and AI-AI\ncommunication.\n", "link": "http://arxiv.org/abs/2407.15814v2", "date": "2024-11-07", "relevancy": 2.1387, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5493}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans&body=Title%3A%20Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans%0AAuthor%3A%20Catarina%20G%20Belem%20and%20Markelle%20Kelly%20and%20Mark%20Steyvers%20and%20Sameer%20Singh%20and%20Padhraic%20Smyth%0AAbstract%3A%20%20%20_Uncertainty%20expressions_%20such%20as%20%22probably%22%20or%20%22highly%20unlikely%22%20are%0Apervasive%20in%20human%20language.%20While%20prior%20work%20has%20established%20that%20there%20is%0Apopulation-level%20agreement%20in%20terms%20of%20how%20humans%20quantitatively%20interpret%0Athese%20expressions%2C%20there%20has%20been%20little%20inquiry%20into%20the%20abilities%20of%20language%0Amodels%20in%20the%20same%20context.%20In%20this%20paper%2C%20we%20investigate%20how%20language%20models%0Amap%20linguistic%20expressions%20of%20uncertainty%20to%20numerical%20responses.%20Our%20approach%0Aassesses%20whether%20language%20models%20can%20employ%20theory%20of%20mind%20in%20this%20setting%3A%0Aunderstanding%20the%20uncertainty%20of%20another%20agent%20about%20a%20particular%20statement%2C%0Aindependently%20of%20the%20model%27s%20own%20certainty%20about%20that%20statement.%20We%20find%20that%207%0Aout%20of%2010%20models%20are%20able%20to%20map%20uncertainty%20expressions%20to%20probabilistic%0Aresponses%20in%20a%20human-like%20manner.%20However%2C%20we%20observe%20systematically%20different%0Abehavior%20depending%20on%20whether%20a%20statement%20is%20actually%20true%20or%20false.%20This%0Asensitivity%20indicates%20that%20language%20models%20are%20substantially%20more%20susceptible%0Ato%20bias%20based%20on%20their%20prior%20knowledge%20%28as%20compared%20to%20humans%29.%20These%20findings%0Araise%20important%20questions%20and%20have%20broad%20implications%20for%20human-AI%20and%20AI-AI%0Acommunication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceptions%2520of%2520Linguistic%2520Uncertainty%2520by%2520Language%2520Models%2520and%2520Humans%26entry.906535625%3DCatarina%2520G%2520Belem%2520and%2520Markelle%2520Kelly%2520and%2520Mark%2520Steyvers%2520and%2520Sameer%2520Singh%2520and%2520Padhraic%2520Smyth%26entry.1292438233%3D%2520%2520_Uncertainty%2520expressions_%2520such%2520as%2520%2522probably%2522%2520or%2520%2522highly%2520unlikely%2522%2520are%250Apervasive%2520in%2520human%2520language.%2520While%2520prior%2520work%2520has%2520established%2520that%2520there%2520is%250Apopulation-level%2520agreement%2520in%2520terms%2520of%2520how%2520humans%2520quantitatively%2520interpret%250Athese%2520expressions%252C%2520there%2520has%2520been%2520little%2520inquiry%2520into%2520the%2520abilities%2520of%2520language%250Amodels%2520in%2520the%2520same%2520context.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520language%2520models%250Amap%2520linguistic%2520expressions%2520of%2520uncertainty%2520to%2520numerical%2520responses.%2520Our%2520approach%250Aassesses%2520whether%2520language%2520models%2520can%2520employ%2520theory%2520of%2520mind%2520in%2520this%2520setting%253A%250Aunderstanding%2520the%2520uncertainty%2520of%2520another%2520agent%2520about%2520a%2520particular%2520statement%252C%250Aindependently%2520of%2520the%2520model%2527s%2520own%2520certainty%2520about%2520that%2520statement.%2520We%2520find%2520that%25207%250Aout%2520of%252010%2520models%2520are%2520able%2520to%2520map%2520uncertainty%2520expressions%2520to%2520probabilistic%250Aresponses%2520in%2520a%2520human-like%2520manner.%2520However%252C%2520we%2520observe%2520systematically%2520different%250Abehavior%2520depending%2520on%2520whether%2520a%2520statement%2520is%2520actually%2520true%2520or%2520false.%2520This%250Asensitivity%2520indicates%2520that%2520language%2520models%2520are%2520substantially%2520more%2520susceptible%250Ato%2520bias%2520based%2520on%2520their%2520prior%2520knowledge%2520%2528as%2520compared%2520to%2520humans%2529.%2520These%2520findings%250Araise%2520important%2520questions%2520and%2520have%2520broad%2520implications%2520for%2520human-AI%2520and%2520AI-AI%250Acommunication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceptions%20of%20Linguistic%20Uncertainty%20by%20Language%20Models%20and%20Humans&entry.906535625=Catarina%20G%20Belem%20and%20Markelle%20Kelly%20and%20Mark%20Steyvers%20and%20Sameer%20Singh%20and%20Padhraic%20Smyth&entry.1292438233=%20%20_Uncertainty%20expressions_%20such%20as%20%22probably%22%20or%20%22highly%20unlikely%22%20are%0Apervasive%20in%20human%20language.%20While%20prior%20work%20has%20established%20that%20there%20is%0Apopulation-level%20agreement%20in%20terms%20of%20how%20humans%20quantitatively%20interpret%0Athese%20expressions%2C%20there%20has%20been%20little%20inquiry%20into%20the%20abilities%20of%20language%0Amodels%20in%20the%20same%20context.%20In%20this%20paper%2C%20we%20investigate%20how%20language%20models%0Amap%20linguistic%20expressions%20of%20uncertainty%20to%20numerical%20responses.%20Our%20approach%0Aassesses%20whether%20language%20models%20can%20employ%20theory%20of%20mind%20in%20this%20setting%3A%0Aunderstanding%20the%20uncertainty%20of%20another%20agent%20about%20a%20particular%20statement%2C%0Aindependently%20of%20the%20model%27s%20own%20certainty%20about%20that%20statement.%20We%20find%20that%207%0Aout%20of%2010%20models%20are%20able%20to%20map%20uncertainty%20expressions%20to%20probabilistic%0Aresponses%20in%20a%20human-like%20manner.%20However%2C%20we%20observe%20systematically%20different%0Abehavior%20depending%20on%20whether%20a%20statement%20is%20actually%20true%20or%20false.%20This%0Asensitivity%20indicates%20that%20language%20models%20are%20substantially%20more%20susceptible%0Ato%20bias%20based%20on%20their%20prior%20knowledge%20%28as%20compared%20to%20humans%29.%20These%20findings%0Araise%20important%20questions%20and%20have%20broad%20implications%20for%20human-AI%20and%20AI-AI%0Acommunication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15814v2&entry.124074799=Read"},
{"title": "AsCAN: Asymmetric Convolution-Attention Networks for Efficient\n  Recognition and Generation", "author": "Anil Kag and Huseyin Coskun and Jierun Chen and Junli Cao and Willi Menapace and Aliaksandr Siarohin and Sergey Tulyakov and Jian Ren", "abstract": "  Neural network architecture design requires making many crucial decisions.\nThe common desiderata is that similar decisions, with little modifications, can\nbe reused in a variety of tasks and applications. To satisfy that,\narchitectures must provide promising latency and performance trade-offs,\nsupport a variety of tasks, scale efficiently with respect to the amounts of\ndata and compute, leverage available data from other tasks, and efficiently\nsupport various hardware. To this end, we introduce AsCAN -- a hybrid\narchitecture, combining both convolutional and transformer blocks. We revisit\nthe key design principles of hybrid architectures and propose a simple and\neffective \\emph{asymmetric} architecture, where the distribution of\nconvolutional and transformer blocks is \\emph{asymmetric}, containing more\nconvolutional blocks in the earlier stages, followed by more transformer blocks\nin later stages. AsCAN supports a variety of tasks: recognition, segmentation,\nclass-conditional image generation, and features a superior trade-off between\nperformance and latency. We then scale the same architecture to solve a\nlarge-scale text-to-image task and show state-of-the-art performance compared\nto the most recent public and commercial models. Notably, even without any\ncomputation optimization for transformer blocks, our models still yield faster\ninference speed than existing works featuring efficient attention mechanisms,\nhighlighting the advantages and the value of our approach.\n", "link": "http://arxiv.org/abs/2411.04967v1", "date": "2024-11-07", "relevancy": 2.1371, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5499}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5478}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AsCAN%3A%20Asymmetric%20Convolution-Attention%20Networks%20for%20Efficient%0A%20%20Recognition%20and%20Generation&body=Title%3A%20AsCAN%3A%20Asymmetric%20Convolution-Attention%20Networks%20for%20Efficient%0A%20%20Recognition%20and%20Generation%0AAuthor%3A%20Anil%20Kag%20and%20Huseyin%20Coskun%20and%20Jierun%20Chen%20and%20Junli%20Cao%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren%0AAbstract%3A%20%20%20Neural%20network%20architecture%20design%20requires%20making%20many%20crucial%20decisions.%0AThe%20common%20desiderata%20is%20that%20similar%20decisions%2C%20with%20little%20modifications%2C%20can%0Abe%20reused%20in%20a%20variety%20of%20tasks%20and%20applications.%20To%20satisfy%20that%2C%0Aarchitectures%20must%20provide%20promising%20latency%20and%20performance%20trade-offs%2C%0Asupport%20a%20variety%20of%20tasks%2C%20scale%20efficiently%20with%20respect%20to%20the%20amounts%20of%0Adata%20and%20compute%2C%20leverage%20available%20data%20from%20other%20tasks%2C%20and%20efficiently%0Asupport%20various%20hardware.%20To%20this%20end%2C%20we%20introduce%20AsCAN%20--%20a%20hybrid%0Aarchitecture%2C%20combining%20both%20convolutional%20and%20transformer%20blocks.%20We%20revisit%0Athe%20key%20design%20principles%20of%20hybrid%20architectures%20and%20propose%20a%20simple%20and%0Aeffective%20%5Cemph%7Basymmetric%7D%20architecture%2C%20where%20the%20distribution%20of%0Aconvolutional%20and%20transformer%20blocks%20is%20%5Cemph%7Basymmetric%7D%2C%20containing%20more%0Aconvolutional%20blocks%20in%20the%20earlier%20stages%2C%20followed%20by%20more%20transformer%20blocks%0Ain%20later%20stages.%20AsCAN%20supports%20a%20variety%20of%20tasks%3A%20recognition%2C%20segmentation%2C%0Aclass-conditional%20image%20generation%2C%20and%20features%20a%20superior%20trade-off%20between%0Aperformance%20and%20latency.%20We%20then%20scale%20the%20same%20architecture%20to%20solve%20a%0Alarge-scale%20text-to-image%20task%20and%20show%20state-of-the-art%20performance%20compared%0Ato%20the%20most%20recent%20public%20and%20commercial%20models.%20Notably%2C%20even%20without%20any%0Acomputation%20optimization%20for%20transformer%20blocks%2C%20our%20models%20still%20yield%20faster%0Ainference%20speed%20than%20existing%20works%20featuring%20efficient%20attention%20mechanisms%2C%0Ahighlighting%20the%20advantages%20and%20the%20value%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsCAN%253A%2520Asymmetric%2520Convolution-Attention%2520Networks%2520for%2520Efficient%250A%2520%2520Recognition%2520and%2520Generation%26entry.906535625%3DAnil%2520Kag%2520and%2520Huseyin%2520Coskun%2520and%2520Jierun%2520Chen%2520and%2520Junli%2520Cao%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Sergey%2520Tulyakov%2520and%2520Jian%2520Ren%26entry.1292438233%3D%2520%2520Neural%2520network%2520architecture%2520design%2520requires%2520making%2520many%2520crucial%2520decisions.%250AThe%2520common%2520desiderata%2520is%2520that%2520similar%2520decisions%252C%2520with%2520little%2520modifications%252C%2520can%250Abe%2520reused%2520in%2520a%2520variety%2520of%2520tasks%2520and%2520applications.%2520To%2520satisfy%2520that%252C%250Aarchitectures%2520must%2520provide%2520promising%2520latency%2520and%2520performance%2520trade-offs%252C%250Asupport%2520a%2520variety%2520of%2520tasks%252C%2520scale%2520efficiently%2520with%2520respect%2520to%2520the%2520amounts%2520of%250Adata%2520and%2520compute%252C%2520leverage%2520available%2520data%2520from%2520other%2520tasks%252C%2520and%2520efficiently%250Asupport%2520various%2520hardware.%2520To%2520this%2520end%252C%2520we%2520introduce%2520AsCAN%2520--%2520a%2520hybrid%250Aarchitecture%252C%2520combining%2520both%2520convolutional%2520and%2520transformer%2520blocks.%2520We%2520revisit%250Athe%2520key%2520design%2520principles%2520of%2520hybrid%2520architectures%2520and%2520propose%2520a%2520simple%2520and%250Aeffective%2520%255Cemph%257Basymmetric%257D%2520architecture%252C%2520where%2520the%2520distribution%2520of%250Aconvolutional%2520and%2520transformer%2520blocks%2520is%2520%255Cemph%257Basymmetric%257D%252C%2520containing%2520more%250Aconvolutional%2520blocks%2520in%2520the%2520earlier%2520stages%252C%2520followed%2520by%2520more%2520transformer%2520blocks%250Ain%2520later%2520stages.%2520AsCAN%2520supports%2520a%2520variety%2520of%2520tasks%253A%2520recognition%252C%2520segmentation%252C%250Aclass-conditional%2520image%2520generation%252C%2520and%2520features%2520a%2520superior%2520trade-off%2520between%250Aperformance%2520and%2520latency.%2520We%2520then%2520scale%2520the%2520same%2520architecture%2520to%2520solve%2520a%250Alarge-scale%2520text-to-image%2520task%2520and%2520show%2520state-of-the-art%2520performance%2520compared%250Ato%2520the%2520most%2520recent%2520public%2520and%2520commercial%2520models.%2520Notably%252C%2520even%2520without%2520any%250Acomputation%2520optimization%2520for%2520transformer%2520blocks%252C%2520our%2520models%2520still%2520yield%2520faster%250Ainference%2520speed%2520than%2520existing%2520works%2520featuring%2520efficient%2520attention%2520mechanisms%252C%250Ahighlighting%2520the%2520advantages%2520and%2520the%2520value%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AsCAN%3A%20Asymmetric%20Convolution-Attention%20Networks%20for%20Efficient%0A%20%20Recognition%20and%20Generation&entry.906535625=Anil%20Kag%20and%20Huseyin%20Coskun%20and%20Jierun%20Chen%20and%20Junli%20Cao%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren&entry.1292438233=%20%20Neural%20network%20architecture%20design%20requires%20making%20many%20crucial%20decisions.%0AThe%20common%20desiderata%20is%20that%20similar%20decisions%2C%20with%20little%20modifications%2C%20can%0Abe%20reused%20in%20a%20variety%20of%20tasks%20and%20applications.%20To%20satisfy%20that%2C%0Aarchitectures%20must%20provide%20promising%20latency%20and%20performance%20trade-offs%2C%0Asupport%20a%20variety%20of%20tasks%2C%20scale%20efficiently%20with%20respect%20to%20the%20amounts%20of%0Adata%20and%20compute%2C%20leverage%20available%20data%20from%20other%20tasks%2C%20and%20efficiently%0Asupport%20various%20hardware.%20To%20this%20end%2C%20we%20introduce%20AsCAN%20--%20a%20hybrid%0Aarchitecture%2C%20combining%20both%20convolutional%20and%20transformer%20blocks.%20We%20revisit%0Athe%20key%20design%20principles%20of%20hybrid%20architectures%20and%20propose%20a%20simple%20and%0Aeffective%20%5Cemph%7Basymmetric%7D%20architecture%2C%20where%20the%20distribution%20of%0Aconvolutional%20and%20transformer%20blocks%20is%20%5Cemph%7Basymmetric%7D%2C%20containing%20more%0Aconvolutional%20blocks%20in%20the%20earlier%20stages%2C%20followed%20by%20more%20transformer%20blocks%0Ain%20later%20stages.%20AsCAN%20supports%20a%20variety%20of%20tasks%3A%20recognition%2C%20segmentation%2C%0Aclass-conditional%20image%20generation%2C%20and%20features%20a%20superior%20trade-off%20between%0Aperformance%20and%20latency.%20We%20then%20scale%20the%20same%20architecture%20to%20solve%20a%0Alarge-scale%20text-to-image%20task%20and%20show%20state-of-the-art%20performance%20compared%0Ato%20the%20most%20recent%20public%20and%20commercial%20models.%20Notably%2C%20even%20without%20any%0Acomputation%20optimization%20for%20transformer%20blocks%2C%20our%20models%20still%20yield%20faster%0Ainference%20speed%20than%20existing%20works%20featuring%20efficient%20attention%20mechanisms%2C%0Ahighlighting%20the%20advantages%20and%20the%20value%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04967v1&entry.124074799=Read"},
{"title": "Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural\n  Networks", "author": "Sanja Karilanova and Maxime Fabre and Emre Neftci and Ay\u00e7a \u00d6z\u00e7elikkale", "abstract": "  Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks\nthat efficiently extract temporal information while offering promising gains in\nterms of energy efficiency and latency when deployed on neuromorphic devices.\nHowever, SNN model parameters are sensitive to temporal resolution, leading to\nsignificant performance drops when the temporal resolution of target data at\nthe edge is not the same with that of the pre-deployment source data used for\ntraining, especially when fine-tuning is not possible at the edge. To address\nthis challenge, we propose three novel domain adaptation methods for adapting\nneuron parameters to account for the change in time resolution without\nre-training on target time-resolution. The proposed methods are based on a\nmapping between neuron dynamics in SNNs and State Space Models (SSMs); and are\napplicable to general neuron models. We evaluate the proposed methods under\nspatio-temporal data tasks, namely the audio keyword spotting datasets SHD and\nMSWC as well as the image classification NMINST dataset. Our methods provide an\nalternative to - and in majority of the cases significantly outperform - the\nexisting reference method that simply scales the time constant. Moreover, our\nresults show that high accuracy on high temporal resolution data can be\nobtained by time efficient training on lower temporal resolution data and model\nadaptation.\n", "link": "http://arxiv.org/abs/2411.04760v1", "date": "2024-11-07", "relevancy": 2.135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5598}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Sanja%20Karilanova%20and%20Maxime%20Fabre%20and%20Emre%20Neftci%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically-inspired%20deep%20neural%20networks%0Athat%20efficiently%20extract%20temporal%20information%20while%20offering%20promising%20gains%20in%0Aterms%20of%20energy%20efficiency%20and%20latency%20when%20deployed%20on%20neuromorphic%20devices.%0AHowever%2C%20SNN%20model%20parameters%20are%20sensitive%20to%20temporal%20resolution%2C%20leading%20to%0Asignificant%20performance%20drops%20when%20the%20temporal%20resolution%20of%20target%20data%20at%0Athe%20edge%20is%20not%20the%20same%20with%20that%20of%20the%20pre-deployment%20source%20data%20used%20for%0Atraining%2C%20especially%20when%20fine-tuning%20is%20not%20possible%20at%20the%20edge.%20To%20address%0Athis%20challenge%2C%20we%20propose%20three%20novel%20domain%20adaptation%20methods%20for%20adapting%0Aneuron%20parameters%20to%20account%20for%20the%20change%20in%20time%20resolution%20without%0Are-training%20on%20target%20time-resolution.%20The%20proposed%20methods%20are%20based%20on%20a%0Amapping%20between%20neuron%20dynamics%20in%20SNNs%20and%20State%20Space%20Models%20%28SSMs%29%3B%20and%20are%0Aapplicable%20to%20general%20neuron%20models.%20We%20evaluate%20the%20proposed%20methods%20under%0Aspatio-temporal%20data%20tasks%2C%20namely%20the%20audio%20keyword%20spotting%20datasets%20SHD%20and%0AMSWC%20as%20well%20as%20the%20image%20classification%20NMINST%20dataset.%20Our%20methods%20provide%20an%0Aalternative%20to%20-%20and%20in%20majority%20of%20the%20cases%20significantly%20outperform%20-%20the%0Aexisting%20reference%20method%20that%20simply%20scales%20the%20time%20constant.%20Moreover%2C%20our%0Aresults%20show%20that%20high%20accuracy%20on%20high%20temporal%20resolution%20data%20can%20be%0Aobtained%20by%20time%20efficient%20training%20on%20lower%20temporal%20resolution%20data%20and%20model%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Temporal%2520Resolution%2520Domain%2520Adaptation%2520for%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSanja%2520Karilanova%2520and%2520Maxime%2520Fabre%2520and%2520Emre%2520Neftci%2520and%2520Ay%25C3%25A7a%2520%25C3%2596z%25C3%25A7elikkale%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520biologically-inspired%2520deep%2520neural%2520networks%250Athat%2520efficiently%2520extract%2520temporal%2520information%2520while%2520offering%2520promising%2520gains%2520in%250Aterms%2520of%2520energy%2520efficiency%2520and%2520latency%2520when%2520deployed%2520on%2520neuromorphic%2520devices.%250AHowever%252C%2520SNN%2520model%2520parameters%2520are%2520sensitive%2520to%2520temporal%2520resolution%252C%2520leading%2520to%250Asignificant%2520performance%2520drops%2520when%2520the%2520temporal%2520resolution%2520of%2520target%2520data%2520at%250Athe%2520edge%2520is%2520not%2520the%2520same%2520with%2520that%2520of%2520the%2520pre-deployment%2520source%2520data%2520used%2520for%250Atraining%252C%2520especially%2520when%2520fine-tuning%2520is%2520not%2520possible%2520at%2520the%2520edge.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520three%2520novel%2520domain%2520adaptation%2520methods%2520for%2520adapting%250Aneuron%2520parameters%2520to%2520account%2520for%2520the%2520change%2520in%2520time%2520resolution%2520without%250Are-training%2520on%2520target%2520time-resolution.%2520The%2520proposed%2520methods%2520are%2520based%2520on%2520a%250Amapping%2520between%2520neuron%2520dynamics%2520in%2520SNNs%2520and%2520State%2520Space%2520Models%2520%2528SSMs%2529%253B%2520and%2520are%250Aapplicable%2520to%2520general%2520neuron%2520models.%2520We%2520evaluate%2520the%2520proposed%2520methods%2520under%250Aspatio-temporal%2520data%2520tasks%252C%2520namely%2520the%2520audio%2520keyword%2520spotting%2520datasets%2520SHD%2520and%250AMSWC%2520as%2520well%2520as%2520the%2520image%2520classification%2520NMINST%2520dataset.%2520Our%2520methods%2520provide%2520an%250Aalternative%2520to%2520-%2520and%2520in%2520majority%2520of%2520the%2520cases%2520significantly%2520outperform%2520-%2520the%250Aexisting%2520reference%2520method%2520that%2520simply%2520scales%2520the%2520time%2520constant.%2520Moreover%252C%2520our%250Aresults%2520show%2520that%2520high%2520accuracy%2520on%2520high%2520temporal%2520resolution%2520data%2520can%2520be%250Aobtained%2520by%2520time%2520efficient%2520training%2520on%2520lower%2520temporal%2520resolution%2520data%2520and%2520model%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Temporal%20Resolution%20Domain%20Adaptation%20for%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Sanja%20Karilanova%20and%20Maxime%20Fabre%20and%20Emre%20Neftci%20and%20Ay%C3%A7a%20%C3%96z%C3%A7elikkale&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20biologically-inspired%20deep%20neural%20networks%0Athat%20efficiently%20extract%20temporal%20information%20while%20offering%20promising%20gains%20in%0Aterms%20of%20energy%20efficiency%20and%20latency%20when%20deployed%20on%20neuromorphic%20devices.%0AHowever%2C%20SNN%20model%20parameters%20are%20sensitive%20to%20temporal%20resolution%2C%20leading%20to%0Asignificant%20performance%20drops%20when%20the%20temporal%20resolution%20of%20target%20data%20at%0Athe%20edge%20is%20not%20the%20same%20with%20that%20of%20the%20pre-deployment%20source%20data%20used%20for%0Atraining%2C%20especially%20when%20fine-tuning%20is%20not%20possible%20at%20the%20edge.%20To%20address%0Athis%20challenge%2C%20we%20propose%20three%20novel%20domain%20adaptation%20methods%20for%20adapting%0Aneuron%20parameters%20to%20account%20for%20the%20change%20in%20time%20resolution%20without%0Are-training%20on%20target%20time-resolution.%20The%20proposed%20methods%20are%20based%20on%20a%0Amapping%20between%20neuron%20dynamics%20in%20SNNs%20and%20State%20Space%20Models%20%28SSMs%29%3B%20and%20are%0Aapplicable%20to%20general%20neuron%20models.%20We%20evaluate%20the%20proposed%20methods%20under%0Aspatio-temporal%20data%20tasks%2C%20namely%20the%20audio%20keyword%20spotting%20datasets%20SHD%20and%0AMSWC%20as%20well%20as%20the%20image%20classification%20NMINST%20dataset.%20Our%20methods%20provide%20an%0Aalternative%20to%20-%20and%20in%20majority%20of%20the%20cases%20significantly%20outperform%20-%20the%0Aexisting%20reference%20method%20that%20simply%20scales%20the%20time%20constant.%20Moreover%2C%20our%0Aresults%20show%20that%20high%20accuracy%20on%20high%20temporal%20resolution%20data%20can%20be%0Aobtained%20by%20time%20efficient%20training%20on%20lower%20temporal%20resolution%20data%20and%20model%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04760v1&entry.124074799=Read"},
{"title": "Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs", "author": "Ahmad Naser Eddin and Jacopo Bono and David Apar\u00edcio and Hugo Ferreira and Pedro Ribeiro and Pedro Bizarro", "abstract": "  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art (SOTA) feature\nengineering and graph neural network methods using five diverse datasets. The\nresults indicate that DGS achieves competitive performance while inference\nspeed improves between 4x and 12x compared to other deep learning approaches on\nour benchmark datasets. Our method effectively bridges the gap between deep\nrepresentation learning and low-latency application requirements for CTDGs.\n", "link": "http://arxiv.org/abs/2407.07712v3", "date": "2024-11-07", "relevancy": 2.1297, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5434}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5337}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&body=Title%3A%20Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs%0AAuthor%3A%20Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro%0AAbstract%3A%20%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20%28SOTA%29%20feature%0Aengineering%20and%20graph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%0Aresults%20indicate%20that%20DGS%20achieves%20competitive%20performance%20while%20inference%0Aspeed%20improves%20between%204x%20and%2012x%20compared%20to%20other%20deep%20learning%20approaches%20on%0Aour%20benchmark%20datasets.%20Our%20method%20effectively%20bridges%20the%20gap%20between%20deep%0Arepresentation%20learning%20and%20low-latency%20application%20requirements%20for%20CTDGs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-Graph-Sprints%253A%2520Accelerated%2520Representation%2520Learning%2520in%250A%2520%2520Continuous-Time%2520Dynamic%2520Graphs%26entry.906535625%3DAhmad%2520Naser%2520Eddin%2520and%2520Jacopo%2520Bono%2520and%2520David%2520Apar%25C3%25ADcio%2520and%2520Hugo%2520Ferreira%2520and%2520Pedro%2520Ribeiro%2520and%2520Pedro%2520Bizarro%26entry.1292438233%3D%2520%2520Continuous-time%2520dynamic%2520graphs%2520%2528CTDGs%2529%2520are%2520essential%2520for%2520modeling%250Ainterconnected%252C%2520evolving%2520systems.%2520Traditional%2520methods%2520for%2520extracting%2520knowledge%250Afrom%2520these%2520graphs%2520often%2520depend%2520on%2520feature%2520engineering%2520or%2520deep%2520learning.%2520Feature%250Aengineering%2520is%2520limited%2520by%2520the%2520manual%2520and%2520time-intensive%2520nature%2520of%2520crafting%250Afeatures%252C%2520while%2520deep%2520learning%2520approaches%2520suffer%2520from%2520high%2520inference%2520latency%252C%250Amaking%2520them%2520impractical%2520for%2520real-time%2520applications.%2520This%2520paper%2520introduces%250ADeep-Graph-Sprints%2520%2528DGS%2529%252C%2520a%2520novel%2520deep%2520learning%2520architecture%2520designed%2520for%250Aefficient%2520representation%2520learning%2520on%2520CTDGs%2520with%2520low-latency%2520inference%250Arequirements.%2520We%2520benchmark%2520DGS%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520feature%250Aengineering%2520and%2520graph%2520neural%2520network%2520methods%2520using%2520five%2520diverse%2520datasets.%2520The%250Aresults%2520indicate%2520that%2520DGS%2520achieves%2520competitive%2520performance%2520while%2520inference%250Aspeed%2520improves%2520between%25204x%2520and%252012x%2520compared%2520to%2520other%2520deep%2520learning%2520approaches%2520on%250Aour%2520benchmark%2520datasets.%2520Our%2520method%2520effectively%2520bridges%2520the%2520gap%2520between%2520deep%250Arepresentation%2520learning%2520and%2520low-latency%2520application%2520requirements%2520for%2520CTDGs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-Graph-Sprints%3A%20Accelerated%20Representation%20Learning%20in%0A%20%20Continuous-Time%20Dynamic%20Graphs&entry.906535625=Ahmad%20Naser%20Eddin%20and%20Jacopo%20Bono%20and%20David%20Apar%C3%ADcio%20and%20Hugo%20Ferreira%20and%20Pedro%20Ribeiro%20and%20Pedro%20Bizarro&entry.1292438233=%20%20Continuous-time%20dynamic%20graphs%20%28CTDGs%29%20are%20essential%20for%20modeling%0Ainterconnected%2C%20evolving%20systems.%20Traditional%20methods%20for%20extracting%20knowledge%0Afrom%20these%20graphs%20often%20depend%20on%20feature%20engineering%20or%20deep%20learning.%20Feature%0Aengineering%20is%20limited%20by%20the%20manual%20and%20time-intensive%20nature%20of%20crafting%0Afeatures%2C%20while%20deep%20learning%20approaches%20suffer%20from%20high%20inference%20latency%2C%0Amaking%20them%20impractical%20for%20real-time%20applications.%20This%20paper%20introduces%0ADeep-Graph-Sprints%20%28DGS%29%2C%20a%20novel%20deep%20learning%20architecture%20designed%20for%0Aefficient%20representation%20learning%20on%20CTDGs%20with%20low-latency%20inference%0Arequirements.%20We%20benchmark%20DGS%20against%20state-of-the-art%20%28SOTA%29%20feature%0Aengineering%20and%20graph%20neural%20network%20methods%20using%20five%20diverse%20datasets.%20The%0Aresults%20indicate%20that%20DGS%20achieves%20competitive%20performance%20while%20inference%0Aspeed%20improves%20between%204x%20and%2012x%20compared%20to%20other%20deep%20learning%20approaches%20on%0Aour%20benchmark%20datasets.%20Our%20method%20effectively%20bridges%20the%20gap%20between%20deep%0Arepresentation%20learning%20and%20low-latency%20application%20requirements%20for%20CTDGs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07712v3&entry.124074799=Read"},
{"title": "NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory", "author": "Navami Kairanda and Marc Habermann and Christian Theobalt and Vladislav Golyanik", "abstract": "  Despite existing 3D cloth simulators producing realistic results, they\npredominantly operate on discrete surface representations (e.g. points and\nmeshes) with a fixed spatial resolution, which often leads to large memory\nconsumption and resolution-dependent simulations. Moreover, back-propagating\ngradients through the existing solvers is difficult, and they cannot be easily\nintegrated into modern neural architectures. In response, this paper re-thinks\nphysically plausible cloth simulation: We propose NeuralClothSim, i.e., a new\nquasistatic cloth simulator using thin shells, in which surface deformation is\nencoded in neural network weights in the form of a neural field. Our\nmemory-efficient solver operates on a new continuous coordinate-based surface\nrepresentation called neural deformation fields (NDFs); it supervises NDF\nequilibria with the laws of the non-linear Kirchhoff-Love shell theory with a\nnon-linear anisotropic material model. NDFs are adaptive: They 1) allocate\ntheir capacity to the deformation details and 2) allow surface state queries at\narbitrary spatial resolutions without re-training. We show how to train\nNeuralClothSim while imposing hard boundary conditions and demonstrate multiple\napplications, such as material interpolation and simulation editing. The\nexperimental results highlight the effectiveness of our continuous neural\nformulation. See our project page: https://4dqv.mpi-inf.mpg.de/NeuralClothSim/.\n", "link": "http://arxiv.org/abs/2308.12970v3", "date": "2024-11-07", "relevancy": 2.1227, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5161}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory&body=Title%3A%20NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory%0AAuthor%3A%20Navami%20Kairanda%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Despite%20existing%203D%20cloth%20simulators%20producing%20realistic%20results%2C%20they%0Apredominantly%20operate%20on%20discrete%20surface%20representations%20%28e.g.%20points%20and%0Ameshes%29%20with%20a%20fixed%20spatial%20resolution%2C%20which%20often%20leads%20to%20large%20memory%0Aconsumption%20and%20resolution-dependent%20simulations.%20Moreover%2C%20back-propagating%0Agradients%20through%20the%20existing%20solvers%20is%20difficult%2C%20and%20they%20cannot%20be%20easily%0Aintegrated%20into%20modern%20neural%20architectures.%20In%20response%2C%20this%20paper%20re-thinks%0Aphysically%20plausible%20cloth%20simulation%3A%20We%20propose%20NeuralClothSim%2C%20i.e.%2C%20a%20new%0Aquasistatic%20cloth%20simulator%20using%20thin%20shells%2C%20in%20which%20surface%20deformation%20is%0Aencoded%20in%20neural%20network%20weights%20in%20the%20form%20of%20a%20neural%20field.%20Our%0Amemory-efficient%20solver%20operates%20on%20a%20new%20continuous%20coordinate-based%20surface%0Arepresentation%20called%20neural%20deformation%20fields%20%28NDFs%29%3B%20it%20supervises%20NDF%0Aequilibria%20with%20the%20laws%20of%20the%20non-linear%20Kirchhoff-Love%20shell%20theory%20with%20a%0Anon-linear%20anisotropic%20material%20model.%20NDFs%20are%20adaptive%3A%20They%201%29%20allocate%0Atheir%20capacity%20to%20the%20deformation%20details%20and%202%29%20allow%20surface%20state%20queries%20at%0Aarbitrary%20spatial%20resolutions%20without%20re-training.%20We%20show%20how%20to%20train%0ANeuralClothSim%20while%20imposing%20hard%20boundary%20conditions%20and%20demonstrate%20multiple%0Aapplications%2C%20such%20as%20material%20interpolation%20and%20simulation%20editing.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20continuous%20neural%0Aformulation.%20See%20our%20project%20page%3A%20https%3A//4dqv.mpi-inf.mpg.de/NeuralClothSim/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12970v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralClothSim%253A%2520Neural%2520Deformation%2520Fields%2520Meet%2520the%2520Thin%2520Shell%2520Theory%26entry.906535625%3DNavami%2520Kairanda%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Despite%2520existing%25203D%2520cloth%2520simulators%2520producing%2520realistic%2520results%252C%2520they%250Apredominantly%2520operate%2520on%2520discrete%2520surface%2520representations%2520%2528e.g.%2520points%2520and%250Ameshes%2529%2520with%2520a%2520fixed%2520spatial%2520resolution%252C%2520which%2520often%2520leads%2520to%2520large%2520memory%250Aconsumption%2520and%2520resolution-dependent%2520simulations.%2520Moreover%252C%2520back-propagating%250Agradients%2520through%2520the%2520existing%2520solvers%2520is%2520difficult%252C%2520and%2520they%2520cannot%2520be%2520easily%250Aintegrated%2520into%2520modern%2520neural%2520architectures.%2520In%2520response%252C%2520this%2520paper%2520re-thinks%250Aphysically%2520plausible%2520cloth%2520simulation%253A%2520We%2520propose%2520NeuralClothSim%252C%2520i.e.%252C%2520a%2520new%250Aquasistatic%2520cloth%2520simulator%2520using%2520thin%2520shells%252C%2520in%2520which%2520surface%2520deformation%2520is%250Aencoded%2520in%2520neural%2520network%2520weights%2520in%2520the%2520form%2520of%2520a%2520neural%2520field.%2520Our%250Amemory-efficient%2520solver%2520operates%2520on%2520a%2520new%2520continuous%2520coordinate-based%2520surface%250Arepresentation%2520called%2520neural%2520deformation%2520fields%2520%2528NDFs%2529%253B%2520it%2520supervises%2520NDF%250Aequilibria%2520with%2520the%2520laws%2520of%2520the%2520non-linear%2520Kirchhoff-Love%2520shell%2520theory%2520with%2520a%250Anon-linear%2520anisotropic%2520material%2520model.%2520NDFs%2520are%2520adaptive%253A%2520They%25201%2529%2520allocate%250Atheir%2520capacity%2520to%2520the%2520deformation%2520details%2520and%25202%2529%2520allow%2520surface%2520state%2520queries%2520at%250Aarbitrary%2520spatial%2520resolutions%2520without%2520re-training.%2520We%2520show%2520how%2520to%2520train%250ANeuralClothSim%2520while%2520imposing%2520hard%2520boundary%2520conditions%2520and%2520demonstrate%2520multiple%250Aapplications%252C%2520such%2520as%2520material%2520interpolation%2520and%2520simulation%2520editing.%2520The%250Aexperimental%2520results%2520highlight%2520the%2520effectiveness%2520of%2520our%2520continuous%2520neural%250Aformulation.%2520See%2520our%2520project%2520page%253A%2520https%253A//4dqv.mpi-inf.mpg.de/NeuralClothSim/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12970v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory&entry.906535625=Navami%20Kairanda%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Despite%20existing%203D%20cloth%20simulators%20producing%20realistic%20results%2C%20they%0Apredominantly%20operate%20on%20discrete%20surface%20representations%20%28e.g.%20points%20and%0Ameshes%29%20with%20a%20fixed%20spatial%20resolution%2C%20which%20often%20leads%20to%20large%20memory%0Aconsumption%20and%20resolution-dependent%20simulations.%20Moreover%2C%20back-propagating%0Agradients%20through%20the%20existing%20solvers%20is%20difficult%2C%20and%20they%20cannot%20be%20easily%0Aintegrated%20into%20modern%20neural%20architectures.%20In%20response%2C%20this%20paper%20re-thinks%0Aphysically%20plausible%20cloth%20simulation%3A%20We%20propose%20NeuralClothSim%2C%20i.e.%2C%20a%20new%0Aquasistatic%20cloth%20simulator%20using%20thin%20shells%2C%20in%20which%20surface%20deformation%20is%0Aencoded%20in%20neural%20network%20weights%20in%20the%20form%20of%20a%20neural%20field.%20Our%0Amemory-efficient%20solver%20operates%20on%20a%20new%20continuous%20coordinate-based%20surface%0Arepresentation%20called%20neural%20deformation%20fields%20%28NDFs%29%3B%20it%20supervises%20NDF%0Aequilibria%20with%20the%20laws%20of%20the%20non-linear%20Kirchhoff-Love%20shell%20theory%20with%20a%0Anon-linear%20anisotropic%20material%20model.%20NDFs%20are%20adaptive%3A%20They%201%29%20allocate%0Atheir%20capacity%20to%20the%20deformation%20details%20and%202%29%20allow%20surface%20state%20queries%20at%0Aarbitrary%20spatial%20resolutions%20without%20re-training.%20We%20show%20how%20to%20train%0ANeuralClothSim%20while%20imposing%20hard%20boundary%20conditions%20and%20demonstrate%20multiple%0Aapplications%2C%20such%20as%20material%20interpolation%20and%20simulation%20editing.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20continuous%20neural%0Aformulation.%20See%20our%20project%20page%3A%20https%3A//4dqv.mpi-inf.mpg.de/NeuralClothSim/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12970v3&entry.124074799=Read"},
{"title": "Dynamic Brightness Adaptation for Robust Multi-modal Image Fusion", "author": "Yiming Sun and Bing Cao and Pengfei Zhu and Qinghua Hu", "abstract": "  Infrared and visible image fusion aim to integrate modality strengths for\nvisually enhanced, informative images. Visible imaging in real-world scenarios\nis susceptible to dynamic environmental brightness fluctuations, leading to\ntexture degradation. Existing fusion methods lack robustness against such\nbrightness perturbations, significantly compromising the visual fidelity of the\nfused imagery. To address this challenge, we propose the Brightness Adaptive\nmultimodal dynamic fusion framework (BA-Fusion), which achieves robust image\nfusion despite dynamic brightness fluctuations. Specifically, we introduce a\nBrightness Adaptive Gate (BAG) module, which is designed to dynamically select\nfeatures from brightness-related channels for normalization, while preserving\nbrightness-independent structural information within the source images.\nFurthermore, we propose a brightness consistency loss function to optimize the\nBAG module. The entire framework is tuned via alternating training strategies.\nExtensive experiments validate that our method surpasses state-of-the-art\nmethods in preserving multi-modal image information and visual fidelity, while\nexhibiting remarkable robustness across varying brightness levels. Our code is\navailable: https://github.com/SunYM2020/BA-Fusion.\n", "link": "http://arxiv.org/abs/2411.04697v1", "date": "2024-11-07", "relevancy": 2.1169, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5449}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5344}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Brightness%20Adaptation%20for%20Robust%20Multi-modal%20Image%20Fusion&body=Title%3A%20Dynamic%20Brightness%20Adaptation%20for%20Robust%20Multi-modal%20Image%20Fusion%0AAuthor%3A%20Yiming%20Sun%20and%20Bing%20Cao%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu%0AAbstract%3A%20%20%20Infrared%20and%20visible%20image%20fusion%20aim%20to%20integrate%20modality%20strengths%20for%0Avisually%20enhanced%2C%20informative%20images.%20Visible%20imaging%20in%20real-world%20scenarios%0Ais%20susceptible%20to%20dynamic%20environmental%20brightness%20fluctuations%2C%20leading%20to%0Atexture%20degradation.%20Existing%20fusion%20methods%20lack%20robustness%20against%20such%0Abrightness%20perturbations%2C%20significantly%20compromising%20the%20visual%20fidelity%20of%20the%0Afused%20imagery.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Brightness%20Adaptive%0Amultimodal%20dynamic%20fusion%20framework%20%28BA-Fusion%29%2C%20which%20achieves%20robust%20image%0Afusion%20despite%20dynamic%20brightness%20fluctuations.%20Specifically%2C%20we%20introduce%20a%0ABrightness%20Adaptive%20Gate%20%28BAG%29%20module%2C%20which%20is%20designed%20to%20dynamically%20select%0Afeatures%20from%20brightness-related%20channels%20for%20normalization%2C%20while%20preserving%0Abrightness-independent%20structural%20information%20within%20the%20source%20images.%0AFurthermore%2C%20we%20propose%20a%20brightness%20consistency%20loss%20function%20to%20optimize%20the%0ABAG%20module.%20The%20entire%20framework%20is%20tuned%20via%20alternating%20training%20strategies.%0AExtensive%20experiments%20validate%20that%20our%20method%20surpasses%20state-of-the-art%0Amethods%20in%20preserving%20multi-modal%20image%20information%20and%20visual%20fidelity%2C%20while%0Aexhibiting%20remarkable%20robustness%20across%20varying%20brightness%20levels.%20Our%20code%20is%0Aavailable%3A%20https%3A//github.com/SunYM2020/BA-Fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Brightness%2520Adaptation%2520for%2520Robust%2520Multi-modal%2520Image%2520Fusion%26entry.906535625%3DYiming%2520Sun%2520and%2520Bing%2520Cao%2520and%2520Pengfei%2520Zhu%2520and%2520Qinghua%2520Hu%26entry.1292438233%3D%2520%2520Infrared%2520and%2520visible%2520image%2520fusion%2520aim%2520to%2520integrate%2520modality%2520strengths%2520for%250Avisually%2520enhanced%252C%2520informative%2520images.%2520Visible%2520imaging%2520in%2520real-world%2520scenarios%250Ais%2520susceptible%2520to%2520dynamic%2520environmental%2520brightness%2520fluctuations%252C%2520leading%2520to%250Atexture%2520degradation.%2520Existing%2520fusion%2520methods%2520lack%2520robustness%2520against%2520such%250Abrightness%2520perturbations%252C%2520significantly%2520compromising%2520the%2520visual%2520fidelity%2520of%2520the%250Afused%2520imagery.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Brightness%2520Adaptive%250Amultimodal%2520dynamic%2520fusion%2520framework%2520%2528BA-Fusion%2529%252C%2520which%2520achieves%2520robust%2520image%250Afusion%2520despite%2520dynamic%2520brightness%2520fluctuations.%2520Specifically%252C%2520we%2520introduce%2520a%250ABrightness%2520Adaptive%2520Gate%2520%2528BAG%2529%2520module%252C%2520which%2520is%2520designed%2520to%2520dynamically%2520select%250Afeatures%2520from%2520brightness-related%2520channels%2520for%2520normalization%252C%2520while%2520preserving%250Abrightness-independent%2520structural%2520information%2520within%2520the%2520source%2520images.%250AFurthermore%252C%2520we%2520propose%2520a%2520brightness%2520consistency%2520loss%2520function%2520to%2520optimize%2520the%250ABAG%2520module.%2520The%2520entire%2520framework%2520is%2520tuned%2520via%2520alternating%2520training%2520strategies.%250AExtensive%2520experiments%2520validate%2520that%2520our%2520method%2520surpasses%2520state-of-the-art%250Amethods%2520in%2520preserving%2520multi-modal%2520image%2520information%2520and%2520visual%2520fidelity%252C%2520while%250Aexhibiting%2520remarkable%2520robustness%2520across%2520varying%2520brightness%2520levels.%2520Our%2520code%2520is%250Aavailable%253A%2520https%253A//github.com/SunYM2020/BA-Fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Brightness%20Adaptation%20for%20Robust%20Multi-modal%20Image%20Fusion&entry.906535625=Yiming%20Sun%20and%20Bing%20Cao%20and%20Pengfei%20Zhu%20and%20Qinghua%20Hu&entry.1292438233=%20%20Infrared%20and%20visible%20image%20fusion%20aim%20to%20integrate%20modality%20strengths%20for%0Avisually%20enhanced%2C%20informative%20images.%20Visible%20imaging%20in%20real-world%20scenarios%0Ais%20susceptible%20to%20dynamic%20environmental%20brightness%20fluctuations%2C%20leading%20to%0Atexture%20degradation.%20Existing%20fusion%20methods%20lack%20robustness%20against%20such%0Abrightness%20perturbations%2C%20significantly%20compromising%20the%20visual%20fidelity%20of%20the%0Afused%20imagery.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Brightness%20Adaptive%0Amultimodal%20dynamic%20fusion%20framework%20%28BA-Fusion%29%2C%20which%20achieves%20robust%20image%0Afusion%20despite%20dynamic%20brightness%20fluctuations.%20Specifically%2C%20we%20introduce%20a%0ABrightness%20Adaptive%20Gate%20%28BAG%29%20module%2C%20which%20is%20designed%20to%20dynamically%20select%0Afeatures%20from%20brightness-related%20channels%20for%20normalization%2C%20while%20preserving%0Abrightness-independent%20structural%20information%20within%20the%20source%20images.%0AFurthermore%2C%20we%20propose%20a%20brightness%20consistency%20loss%20function%20to%20optimize%20the%0ABAG%20module.%20The%20entire%20framework%20is%20tuned%20via%20alternating%20training%20strategies.%0AExtensive%20experiments%20validate%20that%20our%20method%20surpasses%20state-of-the-art%0Amethods%20in%20preserving%20multi-modal%20image%20information%20and%20visual%20fidelity%2C%20while%0Aexhibiting%20remarkable%20robustness%20across%20varying%20brightness%20levels.%20Our%20code%20is%0Aavailable%3A%20https%3A//github.com/SunYM2020/BA-Fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04697v1&entry.124074799=Read"},
{"title": "TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency\n  Perception for Enhanced Liver Tumor Segmentation", "author": "Xiaoyan Jiang and Zhi Zhou and Hailing Wang and Guozhong Wang and Zhijun Fang", "abstract": "  Integrating textual data with imaging in liver tumor segmentation is\nessential for enhancing diagnostic accuracy. However, current multi-modal\nmedical datasets offer only general text annotations, lacking lesion-specific\ndetails critical for extracting nuanced features, especially for fine-grained\nsegmentation of tumor boundaries and small lesions. To address these\nlimitations, we developed datasets with lesion-specific text annotations for\nliver tumors and introduced the TexLiverNet model. TexLiverNet employs an\nagent-based cross-attention module that integrates text features efficiently\nwith visual features, significantly reducing computational costs. Additionally,\nenhanced spatial and adaptive frequency domain perception is proposed to\nprecisely delineate lesion boundaries, reduce background interference, and\nrecover fine details in small lesions. Comprehensive evaluations on public and\nprivate datasets demonstrate that TexLiverNet achieves superior performance\ncompared to current state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.04595v1", "date": "2024-11-07", "relevancy": 2.1075, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5172}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexLiverNet%3A%20Leveraging%20Medical%20Knowledge%20and%20Spatial-Frequency%0A%20%20Perception%20for%20Enhanced%20Liver%20Tumor%20Segmentation&body=Title%3A%20TexLiverNet%3A%20Leveraging%20Medical%20Knowledge%20and%20Spatial-Frequency%0A%20%20Perception%20for%20Enhanced%20Liver%20Tumor%20Segmentation%0AAuthor%3A%20Xiaoyan%20Jiang%20and%20Zhi%20Zhou%20and%20Hailing%20Wang%20and%20Guozhong%20Wang%20and%20Zhijun%20Fang%0AAbstract%3A%20%20%20Integrating%20textual%20data%20with%20imaging%20in%20liver%20tumor%20segmentation%20is%0Aessential%20for%20enhancing%20diagnostic%20accuracy.%20However%2C%20current%20multi-modal%0Amedical%20datasets%20offer%20only%20general%20text%20annotations%2C%20lacking%20lesion-specific%0Adetails%20critical%20for%20extracting%20nuanced%20features%2C%20especially%20for%20fine-grained%0Asegmentation%20of%20tumor%20boundaries%20and%20small%20lesions.%20To%20address%20these%0Alimitations%2C%20we%20developed%20datasets%20with%20lesion-specific%20text%20annotations%20for%0Aliver%20tumors%20and%20introduced%20the%20TexLiverNet%20model.%20TexLiverNet%20employs%20an%0Aagent-based%20cross-attention%20module%20that%20integrates%20text%20features%20efficiently%0Awith%20visual%20features%2C%20significantly%20reducing%20computational%20costs.%20Additionally%2C%0Aenhanced%20spatial%20and%20adaptive%20frequency%20domain%20perception%20is%20proposed%20to%0Aprecisely%20delineate%20lesion%20boundaries%2C%20reduce%20background%20interference%2C%20and%0Arecover%20fine%20details%20in%20small%20lesions.%20Comprehensive%20evaluations%20on%20public%20and%0Aprivate%20datasets%20demonstrate%20that%20TexLiverNet%20achieves%20superior%20performance%0Acompared%20to%20current%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexLiverNet%253A%2520Leveraging%2520Medical%2520Knowledge%2520and%2520Spatial-Frequency%250A%2520%2520Perception%2520for%2520Enhanced%2520Liver%2520Tumor%2520Segmentation%26entry.906535625%3DXiaoyan%2520Jiang%2520and%2520Zhi%2520Zhou%2520and%2520Hailing%2520Wang%2520and%2520Guozhong%2520Wang%2520and%2520Zhijun%2520Fang%26entry.1292438233%3D%2520%2520Integrating%2520textual%2520data%2520with%2520imaging%2520in%2520liver%2520tumor%2520segmentation%2520is%250Aessential%2520for%2520enhancing%2520diagnostic%2520accuracy.%2520However%252C%2520current%2520multi-modal%250Amedical%2520datasets%2520offer%2520only%2520general%2520text%2520annotations%252C%2520lacking%2520lesion-specific%250Adetails%2520critical%2520for%2520extracting%2520nuanced%2520features%252C%2520especially%2520for%2520fine-grained%250Asegmentation%2520of%2520tumor%2520boundaries%2520and%2520small%2520lesions.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520developed%2520datasets%2520with%2520lesion-specific%2520text%2520annotations%2520for%250Aliver%2520tumors%2520and%2520introduced%2520the%2520TexLiverNet%2520model.%2520TexLiverNet%2520employs%2520an%250Aagent-based%2520cross-attention%2520module%2520that%2520integrates%2520text%2520features%2520efficiently%250Awith%2520visual%2520features%252C%2520significantly%2520reducing%2520computational%2520costs.%2520Additionally%252C%250Aenhanced%2520spatial%2520and%2520adaptive%2520frequency%2520domain%2520perception%2520is%2520proposed%2520to%250Aprecisely%2520delineate%2520lesion%2520boundaries%252C%2520reduce%2520background%2520interference%252C%2520and%250Arecover%2520fine%2520details%2520in%2520small%2520lesions.%2520Comprehensive%2520evaluations%2520on%2520public%2520and%250Aprivate%2520datasets%2520demonstrate%2520that%2520TexLiverNet%2520achieves%2520superior%2520performance%250Acompared%2520to%2520current%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexLiverNet%3A%20Leveraging%20Medical%20Knowledge%20and%20Spatial-Frequency%0A%20%20Perception%20for%20Enhanced%20Liver%20Tumor%20Segmentation&entry.906535625=Xiaoyan%20Jiang%20and%20Zhi%20Zhou%20and%20Hailing%20Wang%20and%20Guozhong%20Wang%20and%20Zhijun%20Fang&entry.1292438233=%20%20Integrating%20textual%20data%20with%20imaging%20in%20liver%20tumor%20segmentation%20is%0Aessential%20for%20enhancing%20diagnostic%20accuracy.%20However%2C%20current%20multi-modal%0Amedical%20datasets%20offer%20only%20general%20text%20annotations%2C%20lacking%20lesion-specific%0Adetails%20critical%20for%20extracting%20nuanced%20features%2C%20especially%20for%20fine-grained%0Asegmentation%20of%20tumor%20boundaries%20and%20small%20lesions.%20To%20address%20these%0Alimitations%2C%20we%20developed%20datasets%20with%20lesion-specific%20text%20annotations%20for%0Aliver%20tumors%20and%20introduced%20the%20TexLiverNet%20model.%20TexLiverNet%20employs%20an%0Aagent-based%20cross-attention%20module%20that%20integrates%20text%20features%20efficiently%0Awith%20visual%20features%2C%20significantly%20reducing%20computational%20costs.%20Additionally%2C%0Aenhanced%20spatial%20and%20adaptive%20frequency%20domain%20perception%20is%20proposed%20to%0Aprecisely%20delineate%20lesion%20boundaries%2C%20reduce%20background%20interference%2C%20and%0Arecover%20fine%20details%20in%20small%20lesions.%20Comprehensive%20evaluations%20on%20public%20and%0Aprivate%20datasets%20demonstrate%20that%20TexLiverNet%20achieves%20superior%20performance%0Acompared%20to%20current%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04595v1&entry.124074799=Read"},
{"title": "Learning Latent Space Dynamics with Model-Form Uncertainties: A\n  Stochastic Reduced-Order Modeling Approach", "author": "Jin Yi Yong and Rudy Geelen and Johann Guilleminot", "abstract": "  This paper presents a probabilistic approach to represent and quantify\nmodel-form uncertainties in the reduced-order modeling of complex systems using\noperator inference techniques. Such uncertainties can arise in the selection of\nan appropriate state-space representation, in the projection step that\nunderlies many reduced-order modeling methods, or as a byproduct of\nconsiderations made during training, to name a few. Following previous works in\nthe literature, the proposed method captures these uncertainties by expanding\nthe approximation space through the randomization of the projection matrix.\nThis is achieved by combining Riemannian projection and retraction operators -\nacting on a subset of the Stiefel manifold - with an information-theoretic\nformulation. The efficacy of the approach is assessed on canonical problems in\nfluid mechanics by identifying and quantifying the impact of model-form\nuncertainties on the inferred operators.\n", "link": "http://arxiv.org/abs/2409.00220v2", "date": "2024-11-07", "relevancy": 2.0963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5304}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Latent%20Space%20Dynamics%20with%20Model-Form%20Uncertainties%3A%20A%0A%20%20Stochastic%20Reduced-Order%20Modeling%20Approach&body=Title%3A%20Learning%20Latent%20Space%20Dynamics%20with%20Model-Form%20Uncertainties%3A%20A%0A%20%20Stochastic%20Reduced-Order%20Modeling%20Approach%0AAuthor%3A%20Jin%20Yi%20Yong%20and%20Rudy%20Geelen%20and%20Johann%20Guilleminot%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20probabilistic%20approach%20to%20represent%20and%20quantify%0Amodel-form%20uncertainties%20in%20the%20reduced-order%20modeling%20of%20complex%20systems%20using%0Aoperator%20inference%20techniques.%20Such%20uncertainties%20can%20arise%20in%20the%20selection%20of%0Aan%20appropriate%20state-space%20representation%2C%20in%20the%20projection%20step%20that%0Aunderlies%20many%20reduced-order%20modeling%20methods%2C%20or%20as%20a%20byproduct%20of%0Aconsiderations%20made%20during%20training%2C%20to%20name%20a%20few.%20Following%20previous%20works%20in%0Athe%20literature%2C%20the%20proposed%20method%20captures%20these%20uncertainties%20by%20expanding%0Athe%20approximation%20space%20through%20the%20randomization%20of%20the%20projection%20matrix.%0AThis%20is%20achieved%20by%20combining%20Riemannian%20projection%20and%20retraction%20operators%20-%0Aacting%20on%20a%20subset%20of%20the%20Stiefel%20manifold%20-%20with%20an%20information-theoretic%0Aformulation.%20The%20efficacy%20of%20the%20approach%20is%20assessed%20on%20canonical%20problems%20in%0Afluid%20mechanics%20by%20identifying%20and%20quantifying%20the%20impact%20of%20model-form%0Auncertainties%20on%20the%20inferred%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Latent%2520Space%2520Dynamics%2520with%2520Model-Form%2520Uncertainties%253A%2520A%250A%2520%2520Stochastic%2520Reduced-Order%2520Modeling%2520Approach%26entry.906535625%3DJin%2520Yi%2520Yong%2520and%2520Rudy%2520Geelen%2520and%2520Johann%2520Guilleminot%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520probabilistic%2520approach%2520to%2520represent%2520and%2520quantify%250Amodel-form%2520uncertainties%2520in%2520the%2520reduced-order%2520modeling%2520of%2520complex%2520systems%2520using%250Aoperator%2520inference%2520techniques.%2520Such%2520uncertainties%2520can%2520arise%2520in%2520the%2520selection%2520of%250Aan%2520appropriate%2520state-space%2520representation%252C%2520in%2520the%2520projection%2520step%2520that%250Aunderlies%2520many%2520reduced-order%2520modeling%2520methods%252C%2520or%2520as%2520a%2520byproduct%2520of%250Aconsiderations%2520made%2520during%2520training%252C%2520to%2520name%2520a%2520few.%2520Following%2520previous%2520works%2520in%250Athe%2520literature%252C%2520the%2520proposed%2520method%2520captures%2520these%2520uncertainties%2520by%2520expanding%250Athe%2520approximation%2520space%2520through%2520the%2520randomization%2520of%2520the%2520projection%2520matrix.%250AThis%2520is%2520achieved%2520by%2520combining%2520Riemannian%2520projection%2520and%2520retraction%2520operators%2520-%250Aacting%2520on%2520a%2520subset%2520of%2520the%2520Stiefel%2520manifold%2520-%2520with%2520an%2520information-theoretic%250Aformulation.%2520The%2520efficacy%2520of%2520the%2520approach%2520is%2520assessed%2520on%2520canonical%2520problems%2520in%250Afluid%2520mechanics%2520by%2520identifying%2520and%2520quantifying%2520the%2520impact%2520of%2520model-form%250Auncertainties%2520on%2520the%2520inferred%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Latent%20Space%20Dynamics%20with%20Model-Form%20Uncertainties%3A%20A%0A%20%20Stochastic%20Reduced-Order%20Modeling%20Approach&entry.906535625=Jin%20Yi%20Yong%20and%20Rudy%20Geelen%20and%20Johann%20Guilleminot&entry.1292438233=%20%20This%20paper%20presents%20a%20probabilistic%20approach%20to%20represent%20and%20quantify%0Amodel-form%20uncertainties%20in%20the%20reduced-order%20modeling%20of%20complex%20systems%20using%0Aoperator%20inference%20techniques.%20Such%20uncertainties%20can%20arise%20in%20the%20selection%20of%0Aan%20appropriate%20state-space%20representation%2C%20in%20the%20projection%20step%20that%0Aunderlies%20many%20reduced-order%20modeling%20methods%2C%20or%20as%20a%20byproduct%20of%0Aconsiderations%20made%20during%20training%2C%20to%20name%20a%20few.%20Following%20previous%20works%20in%0Athe%20literature%2C%20the%20proposed%20method%20captures%20these%20uncertainties%20by%20expanding%0Athe%20approximation%20space%20through%20the%20randomization%20of%20the%20projection%20matrix.%0AThis%20is%20achieved%20by%20combining%20Riemannian%20projection%20and%20retraction%20operators%20-%0Aacting%20on%20a%20subset%20of%20the%20Stiefel%20manifold%20-%20with%20an%20information-theoretic%0Aformulation.%20The%20efficacy%20of%20the%20approach%20is%20assessed%20on%20canonical%20problems%20in%0Afluid%20mechanics%20by%20identifying%20and%20quantifying%20the%20impact%20of%20model-form%0Auncertainties%20on%20the%20inferred%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00220v2&entry.124074799=Read"},
{"title": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference", "author": "Gabriele Oliaro and Zhihao Jia and Daniel Campos and Aurick Qiao", "abstract": "  We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated.\n", "link": "http://arxiv.org/abs/2411.04975v1", "date": "2024-11-07", "relevancy": 2.0953, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuffixDecoding%3A%20A%20Model-Free%20Approach%20to%20Speeding%20Up%20Large%20Language%0A%20%20Model%20Inference&body=Title%3A%20SuffixDecoding%3A%20A%20Model-Free%20Approach%20to%20Speeding%20Up%20Large%20Language%0A%20%20Model%20Inference%0AAuthor%3A%20Gabriele%20Oliaro%20and%20Zhihao%20Jia%20and%20Daniel%20Campos%20and%20Aurick%20Qiao%0AAbstract%3A%20%20%20We%20present%20SuffixDecoding%2C%20a%20novel%20model-free%20approach%20to%20accelerating%20large%0Alanguage%20model%20%28LLM%29%20inference%20through%20speculative%20decoding.%20Unlike%20existing%0Amethods%20that%20rely%20on%20draft%20models%20or%20specialized%20decoding%20heads%2C%20SuffixDecoding%0Aleverages%20suffix%20trees%20built%20from%20previously%20generated%20outputs%20to%20efficiently%0Apredict%20candidate%20token%20sequences.%20Our%20approach%20enables%20flexible%0Atree-structured%20speculation%20without%20the%20overhead%20of%20maintaining%20and%0Aorchestrating%20additional%20models.%20SuffixDecoding%20builds%20and%20dynamically%20updates%0Asuffix%20trees%20to%20capture%20patterns%20in%20the%20generated%20text%2C%20using%20them%20to%20construct%0Aspeculation%20trees%20through%20a%20principled%20scoring%20mechanism%20based%20on%20empirical%0Atoken%20frequencies.%20SuffixDecoding%20requires%20only%20CPU%20memory%20which%20is%20plentiful%0Aand%20underutilized%20on%20typical%20LLM%20serving%20nodes.%20We%20demonstrate%20that%0ASuffixDecoding%20achieves%20competitive%20speedups%20compared%20to%20model-based%20approaches%0Aacross%20diverse%20workloads%20including%20open-domain%20chat%2C%20code%20generation%2C%20and%0Atext-to-SQL%20tasks.%20For%20open-ended%20chat%20and%20code%20generation%20tasks%2C%0ASuffixDecoding%20achieves%20up%20to%20%241.4%5Ctimes%24%20higher%20output%20throughput%20than%0ASpecInfer%20and%20up%20to%20%241.1%5Ctimes%24%20lower%20time-per-token%20%28TPOT%29%20latency.%20For%20a%0Aproprietary%20multi-LLM%20text-to-SQL%20application%2C%20SuffixDecoding%20achieves%20up%20to%0A%242.9%5Ctimes%24%20higher%20output%20throughput%20and%20%243%5Ctimes%24%20lower%20latency%20than%0Aspeculative%20decoding.%20Our%20evaluation%20shows%20that%20SuffixDecoding%20maintains%20high%0Aacceptance%20rates%20even%20with%20small%20reference%20corpora%20of%20256%20examples%2C%20while%0Acontinuing%20to%20improve%20performance%20as%20more%20historical%20outputs%20are%20incorporated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuffixDecoding%253A%2520A%2520Model-Free%2520Approach%2520to%2520Speeding%2520Up%2520Large%2520Language%250A%2520%2520Model%2520Inference%26entry.906535625%3DGabriele%2520Oliaro%2520and%2520Zhihao%2520Jia%2520and%2520Daniel%2520Campos%2520and%2520Aurick%2520Qiao%26entry.1292438233%3D%2520%2520We%2520present%2520SuffixDecoding%252C%2520a%2520novel%2520model-free%2520approach%2520to%2520accelerating%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520inference%2520through%2520speculative%2520decoding.%2520Unlike%2520existing%250Amethods%2520that%2520rely%2520on%2520draft%2520models%2520or%2520specialized%2520decoding%2520heads%252C%2520SuffixDecoding%250Aleverages%2520suffix%2520trees%2520built%2520from%2520previously%2520generated%2520outputs%2520to%2520efficiently%250Apredict%2520candidate%2520token%2520sequences.%2520Our%2520approach%2520enables%2520flexible%250Atree-structured%2520speculation%2520without%2520the%2520overhead%2520of%2520maintaining%2520and%250Aorchestrating%2520additional%2520models.%2520SuffixDecoding%2520builds%2520and%2520dynamically%2520updates%250Asuffix%2520trees%2520to%2520capture%2520patterns%2520in%2520the%2520generated%2520text%252C%2520using%2520them%2520to%2520construct%250Aspeculation%2520trees%2520through%2520a%2520principled%2520scoring%2520mechanism%2520based%2520on%2520empirical%250Atoken%2520frequencies.%2520SuffixDecoding%2520requires%2520only%2520CPU%2520memory%2520which%2520is%2520plentiful%250Aand%2520underutilized%2520on%2520typical%2520LLM%2520serving%2520nodes.%2520We%2520demonstrate%2520that%250ASuffixDecoding%2520achieves%2520competitive%2520speedups%2520compared%2520to%2520model-based%2520approaches%250Aacross%2520diverse%2520workloads%2520including%2520open-domain%2520chat%252C%2520code%2520generation%252C%2520and%250Atext-to-SQL%2520tasks.%2520For%2520open-ended%2520chat%2520and%2520code%2520generation%2520tasks%252C%250ASuffixDecoding%2520achieves%2520up%2520to%2520%25241.4%255Ctimes%2524%2520higher%2520output%2520throughput%2520than%250ASpecInfer%2520and%2520up%2520to%2520%25241.1%255Ctimes%2524%2520lower%2520time-per-token%2520%2528TPOT%2529%2520latency.%2520For%2520a%250Aproprietary%2520multi-LLM%2520text-to-SQL%2520application%252C%2520SuffixDecoding%2520achieves%2520up%2520to%250A%25242.9%255Ctimes%2524%2520higher%2520output%2520throughput%2520and%2520%25243%255Ctimes%2524%2520lower%2520latency%2520than%250Aspeculative%2520decoding.%2520Our%2520evaluation%2520shows%2520that%2520SuffixDecoding%2520maintains%2520high%250Aacceptance%2520rates%2520even%2520with%2520small%2520reference%2520corpora%2520of%2520256%2520examples%252C%2520while%250Acontinuing%2520to%2520improve%2520performance%2520as%2520more%2520historical%2520outputs%2520are%2520incorporated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuffixDecoding%3A%20A%20Model-Free%20Approach%20to%20Speeding%20Up%20Large%20Language%0A%20%20Model%20Inference&entry.906535625=Gabriele%20Oliaro%20and%20Zhihao%20Jia%20and%20Daniel%20Campos%20and%20Aurick%20Qiao&entry.1292438233=%20%20We%20present%20SuffixDecoding%2C%20a%20novel%20model-free%20approach%20to%20accelerating%20large%0Alanguage%20model%20%28LLM%29%20inference%20through%20speculative%20decoding.%20Unlike%20existing%0Amethods%20that%20rely%20on%20draft%20models%20or%20specialized%20decoding%20heads%2C%20SuffixDecoding%0Aleverages%20suffix%20trees%20built%20from%20previously%20generated%20outputs%20to%20efficiently%0Apredict%20candidate%20token%20sequences.%20Our%20approach%20enables%20flexible%0Atree-structured%20speculation%20without%20the%20overhead%20of%20maintaining%20and%0Aorchestrating%20additional%20models.%20SuffixDecoding%20builds%20and%20dynamically%20updates%0Asuffix%20trees%20to%20capture%20patterns%20in%20the%20generated%20text%2C%20using%20them%20to%20construct%0Aspeculation%20trees%20through%20a%20principled%20scoring%20mechanism%20based%20on%20empirical%0Atoken%20frequencies.%20SuffixDecoding%20requires%20only%20CPU%20memory%20which%20is%20plentiful%0Aand%20underutilized%20on%20typical%20LLM%20serving%20nodes.%20We%20demonstrate%20that%0ASuffixDecoding%20achieves%20competitive%20speedups%20compared%20to%20model-based%20approaches%0Aacross%20diverse%20workloads%20including%20open-domain%20chat%2C%20code%20generation%2C%20and%0Atext-to-SQL%20tasks.%20For%20open-ended%20chat%20and%20code%20generation%20tasks%2C%0ASuffixDecoding%20achieves%20up%20to%20%241.4%5Ctimes%24%20higher%20output%20throughput%20than%0ASpecInfer%20and%20up%20to%20%241.1%5Ctimes%24%20lower%20time-per-token%20%28TPOT%29%20latency.%20For%20a%0Aproprietary%20multi-LLM%20text-to-SQL%20application%2C%20SuffixDecoding%20achieves%20up%20to%0A%242.9%5Ctimes%24%20higher%20output%20throughput%20and%20%243%5Ctimes%24%20lower%20latency%20than%0Aspeculative%20decoding.%20Our%20evaluation%20shows%20that%20SuffixDecoding%20maintains%20high%0Aacceptance%20rates%20even%20with%20small%20reference%20corpora%20of%20256%20examples%2C%20while%0Acontinuing%20to%20improve%20performance%20as%20more%20historical%20outputs%20are%20incorporated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04975v1&entry.124074799=Read"},
{"title": "Occam Gradient Descent", "author": "B. N. Kausik", "abstract": "  Deep learning neural network models must be large enough to adapt to their\nproblem domain, while small enough to avoid overfitting training data during\ngradient descent. To balance these competing demands, overprovisioned deep\nlearning models such as transformers are trained for a single epoch on large\ndata sets, and hence inefficient with both computing resources and training\ndata. In response to these inefficiencies, we exploit learning theory to derive\nOccam Gradient Descent, an algorithm that interleaves adaptive reduction of\nmodel size to minimize generalization error, with gradient descent on model\nweights to minimize fitting error. In contrast, traditional gradient descent\ngreedily minimizes fitting error without regard to generalization error. Our\nalgorithm simultaneously descends the space of weights and topological size of\nany neural network without modification. With respect to loss, compute and\nmodel size, our experiments show (a) on image classification benchmarks, linear\nand convolutional neural networks trained with Occam Gradient Descent\noutperform traditional gradient descent with or without post-train pruning; (b)\non a range of tabular data classification tasks, neural networks trained with\nOccam Gradient Descent outperform traditional gradient descent, as well as\nRandom Forests; (c) on natural language transformers, Occam Gradient Descent\noutperforms traditional gradient descent.\n", "link": "http://arxiv.org/abs/2405.20194v5", "date": "2024-11-07", "relevancy": 2.0874, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5311}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occam%20Gradient%20Descent&body=Title%3A%20Occam%20Gradient%20Descent%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20194v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccam%2520Gradient%2520Descent%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520neural%2520network%2520models%2520must%2520be%2520large%2520enough%2520to%2520adapt%2520to%2520their%250Aproblem%2520domain%252C%2520while%2520small%2520enough%2520to%2520avoid%2520overfitting%2520training%2520data%2520during%250Agradient%2520descent.%2520To%2520balance%2520these%2520competing%2520demands%252C%2520overprovisioned%2520deep%250Alearning%2520models%2520such%2520as%2520transformers%2520are%2520trained%2520for%2520a%2520single%2520epoch%2520on%2520large%250Adata%2520sets%252C%2520and%2520hence%2520inefficient%2520with%2520both%2520computing%2520resources%2520and%2520training%250Adata.%2520In%2520response%2520to%2520these%2520inefficiencies%252C%2520we%2520exploit%2520learning%2520theory%2520to%2520derive%250AOccam%2520Gradient%2520Descent%252C%2520an%2520algorithm%2520that%2520interleaves%2520adaptive%2520reduction%2520of%250Amodel%2520size%2520to%2520minimize%2520generalization%2520error%252C%2520with%2520gradient%2520descent%2520on%2520model%250Aweights%2520to%2520minimize%2520fitting%2520error.%2520In%2520contrast%252C%2520traditional%2520gradient%2520descent%250Agreedily%2520minimizes%2520fitting%2520error%2520without%2520regard%2520to%2520generalization%2520error.%2520Our%250Aalgorithm%2520simultaneously%2520descends%2520the%2520space%2520of%2520weights%2520and%2520topological%2520size%2520of%250Aany%2520neural%2520network%2520without%2520modification.%2520With%2520respect%2520to%2520loss%252C%2520compute%2520and%250Amodel%2520size%252C%2520our%2520experiments%2520show%2520%2528a%2529%2520on%2520image%2520classification%2520benchmarks%252C%2520linear%250Aand%2520convolutional%2520neural%2520networks%2520trained%2520with%2520Occam%2520Gradient%2520Descent%250Aoutperform%2520traditional%2520gradient%2520descent%2520with%2520or%2520without%2520post-train%2520pruning%253B%2520%2528b%2529%250Aon%2520a%2520range%2520of%2520tabular%2520data%2520classification%2520tasks%252C%2520neural%2520networks%2520trained%2520with%250AOccam%2520Gradient%2520Descent%2520outperform%2520traditional%2520gradient%2520descent%252C%2520as%2520well%2520as%250ARandom%2520Forests%253B%2520%2528c%2529%2520on%2520natural%2520language%2520transformers%252C%2520Occam%2520Gradient%2520Descent%250Aoutperforms%2520traditional%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20194v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occam%20Gradient%20Descent&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20194v5&entry.124074799=Read"},
{"title": "Field Assessment of Force Torque Sensors for Planetary Rover Navigation", "author": "Levin Gerdes and Carlos P\u00e9rez del Pulgar and Ra\u00fal Castilla Arquillo and Martin Azkarate", "abstract": "  Proprioceptive sensors on planetary rovers serve for state estimation and for\nunderstanding terrain and locomotion performance. While inertial measurement\nunits (IMUs) are widely used to this effect, force-torque sensors are less\nexplored for planetary navigation despite their potential to directly measure\ninteraction forces and provide insights into traction performance. This paper\npresents an evaluation of the performance and use cases of force-torque sensors\nbased on data collected from a six-wheeled rover during tests over varying\nterrains, speeds, and slopes. We discuss challenges, such as sensor signal\nreliability and terrain response accuracy, and identify opportunities regarding\nthe use of these sensors. The data is openly accessible and includes\nforce-torque measurements from each of the six-wheel assemblies as well as IMU\ndata from within the rover chassis. This paper aims to inform the design of\nfuture studies and rover upgrades, particularly in sensor integration and\ncontrol algorithms, to improve navigation capabilities.\n", "link": "http://arxiv.org/abs/2411.04700v1", "date": "2024-11-07", "relevancy": 2.0694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5563}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5423}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Field%20Assessment%20of%20Force%20Torque%20Sensors%20for%20Planetary%20Rover%20Navigation&body=Title%3A%20Field%20Assessment%20of%20Force%20Torque%20Sensors%20for%20Planetary%20Rover%20Navigation%0AAuthor%3A%20Levin%20Gerdes%20and%20Carlos%20P%C3%A9rez%20del%20Pulgar%20and%20Ra%C3%BAl%20Castilla%20Arquillo%20and%20Martin%20Azkarate%0AAbstract%3A%20%20%20Proprioceptive%20sensors%20on%20planetary%20rovers%20serve%20for%20state%20estimation%20and%20for%0Aunderstanding%20terrain%20and%20locomotion%20performance.%20While%20inertial%20measurement%0Aunits%20%28IMUs%29%20are%20widely%20used%20to%20this%20effect%2C%20force-torque%20sensors%20are%20less%0Aexplored%20for%20planetary%20navigation%20despite%20their%20potential%20to%20directly%20measure%0Ainteraction%20forces%20and%20provide%20insights%20into%20traction%20performance.%20This%20paper%0Apresents%20an%20evaluation%20of%20the%20performance%20and%20use%20cases%20of%20force-torque%20sensors%0Abased%20on%20data%20collected%20from%20a%20six-wheeled%20rover%20during%20tests%20over%20varying%0Aterrains%2C%20speeds%2C%20and%20slopes.%20We%20discuss%20challenges%2C%20such%20as%20sensor%20signal%0Areliability%20and%20terrain%20response%20accuracy%2C%20and%20identify%20opportunities%20regarding%0Athe%20use%20of%20these%20sensors.%20The%20data%20is%20openly%20accessible%20and%20includes%0Aforce-torque%20measurements%20from%20each%20of%20the%20six-wheel%20assemblies%20as%20well%20as%20IMU%0Adata%20from%20within%20the%20rover%20chassis.%20This%20paper%20aims%20to%20inform%20the%20design%20of%0Afuture%20studies%20and%20rover%20upgrades%2C%20particularly%20in%20sensor%20integration%20and%0Acontrol%20algorithms%2C%20to%20improve%20navigation%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DField%2520Assessment%2520of%2520Force%2520Torque%2520Sensors%2520for%2520Planetary%2520Rover%2520Navigation%26entry.906535625%3DLevin%2520Gerdes%2520and%2520Carlos%2520P%25C3%25A9rez%2520del%2520Pulgar%2520and%2520Ra%25C3%25BAl%2520Castilla%2520Arquillo%2520and%2520Martin%2520Azkarate%26entry.1292438233%3D%2520%2520Proprioceptive%2520sensors%2520on%2520planetary%2520rovers%2520serve%2520for%2520state%2520estimation%2520and%2520for%250Aunderstanding%2520terrain%2520and%2520locomotion%2520performance.%2520While%2520inertial%2520measurement%250Aunits%2520%2528IMUs%2529%2520are%2520widely%2520used%2520to%2520this%2520effect%252C%2520force-torque%2520sensors%2520are%2520less%250Aexplored%2520for%2520planetary%2520navigation%2520despite%2520their%2520potential%2520to%2520directly%2520measure%250Ainteraction%2520forces%2520and%2520provide%2520insights%2520into%2520traction%2520performance.%2520This%2520paper%250Apresents%2520an%2520evaluation%2520of%2520the%2520performance%2520and%2520use%2520cases%2520of%2520force-torque%2520sensors%250Abased%2520on%2520data%2520collected%2520from%2520a%2520six-wheeled%2520rover%2520during%2520tests%2520over%2520varying%250Aterrains%252C%2520speeds%252C%2520and%2520slopes.%2520We%2520discuss%2520challenges%252C%2520such%2520as%2520sensor%2520signal%250Areliability%2520and%2520terrain%2520response%2520accuracy%252C%2520and%2520identify%2520opportunities%2520regarding%250Athe%2520use%2520of%2520these%2520sensors.%2520The%2520data%2520is%2520openly%2520accessible%2520and%2520includes%250Aforce-torque%2520measurements%2520from%2520each%2520of%2520the%2520six-wheel%2520assemblies%2520as%2520well%2520as%2520IMU%250Adata%2520from%2520within%2520the%2520rover%2520chassis.%2520This%2520paper%2520aims%2520to%2520inform%2520the%2520design%2520of%250Afuture%2520studies%2520and%2520rover%2520upgrades%252C%2520particularly%2520in%2520sensor%2520integration%2520and%250Acontrol%2520algorithms%252C%2520to%2520improve%2520navigation%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Field%20Assessment%20of%20Force%20Torque%20Sensors%20for%20Planetary%20Rover%20Navigation&entry.906535625=Levin%20Gerdes%20and%20Carlos%20P%C3%A9rez%20del%20Pulgar%20and%20Ra%C3%BAl%20Castilla%20Arquillo%20and%20Martin%20Azkarate&entry.1292438233=%20%20Proprioceptive%20sensors%20on%20planetary%20rovers%20serve%20for%20state%20estimation%20and%20for%0Aunderstanding%20terrain%20and%20locomotion%20performance.%20While%20inertial%20measurement%0Aunits%20%28IMUs%29%20are%20widely%20used%20to%20this%20effect%2C%20force-torque%20sensors%20are%20less%0Aexplored%20for%20planetary%20navigation%20despite%20their%20potential%20to%20directly%20measure%0Ainteraction%20forces%20and%20provide%20insights%20into%20traction%20performance.%20This%20paper%0Apresents%20an%20evaluation%20of%20the%20performance%20and%20use%20cases%20of%20force-torque%20sensors%0Abased%20on%20data%20collected%20from%20a%20six-wheeled%20rover%20during%20tests%20over%20varying%0Aterrains%2C%20speeds%2C%20and%20slopes.%20We%20discuss%20challenges%2C%20such%20as%20sensor%20signal%0Areliability%20and%20terrain%20response%20accuracy%2C%20and%20identify%20opportunities%20regarding%0Athe%20use%20of%20these%20sensors.%20The%20data%20is%20openly%20accessible%20and%20includes%0Aforce-torque%20measurements%20from%20each%20of%20the%20six-wheel%20assemblies%20as%20well%20as%20IMU%0Adata%20from%20within%20the%20rover%20chassis.%20This%20paper%20aims%20to%20inform%20the%20design%20of%0Afuture%20studies%20and%20rover%20upgrades%2C%20particularly%20in%20sensor%20integration%20and%0Acontrol%20algorithms%2C%20to%20improve%20navigation%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04700v1&entry.124074799=Read"},
{"title": "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing\n  Audio-Visual Question Answering", "author": "ianyu Yang and Yiyang Nan and Lisen Dai and Zhenwen Liang and Yapeng Tian and Xiangliang Zhang", "abstract": "  Audio-Visual Question Answering (AVQA) is a challenging task that involves\nanswering questions based on both auditory and visual information in videos. A\nsignificant challenge is interpreting complex multi-modal scenes, which include\nboth visual objects and sound sources, and connecting them to the given\nquestion. In this paper, we introduce the Source-aware Semantic Representation\nNetwork (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes\nsource-wise learnable tokens to efficiently capture and align audio-visual\nelements with the corresponding question. It streamlines the fusion of audio\nand visual information using spatial and temporal attention mechanisms to\nidentify answers in multi-modal scenes. Extensive experiments on the Music-AVQA\nand AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA\nmethods.\n", "link": "http://arxiv.org/abs/2411.04933v1", "date": "2024-11-07", "relevancy": 2.0661, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SaSR-Net%3A%20Source-Aware%20Semantic%20Representation%20Network%20for%20Enhancing%0A%20%20Audio-Visual%20Question%20Answering&body=Title%3A%20SaSR-Net%3A%20Source-Aware%20Semantic%20Representation%20Network%20for%20Enhancing%0A%20%20Audio-Visual%20Question%20Answering%0AAuthor%3A%20ianyu%20Yang%20and%20Yiyang%20Nan%20and%20Lisen%20Dai%20and%20Zhenwen%20Liang%20and%20Yapeng%20Tian%20and%20Xiangliang%20Zhang%0AAbstract%3A%20%20%20Audio-Visual%20Question%20Answering%20%28AVQA%29%20is%20a%20challenging%20task%20that%20involves%0Aanswering%20questions%20based%20on%20both%20auditory%20and%20visual%20information%20in%20videos.%20A%0Asignificant%20challenge%20is%20interpreting%20complex%20multi-modal%20scenes%2C%20which%20include%0Aboth%20visual%20objects%20and%20sound%20sources%2C%20and%20connecting%20them%20to%20the%20given%0Aquestion.%20In%20this%20paper%2C%20we%20introduce%20the%20Source-aware%20Semantic%20Representation%0ANetwork%20%28SaSR-Net%29%2C%20a%20novel%20model%20designed%20for%20AVQA.%20SaSR-Net%20utilizes%0Asource-wise%20learnable%20tokens%20to%20efficiently%20capture%20and%20align%20audio-visual%0Aelements%20with%20the%20corresponding%20question.%20It%20streamlines%20the%20fusion%20of%20audio%0Aand%20visual%20information%20using%20spatial%20and%20temporal%20attention%20mechanisms%20to%0Aidentify%20answers%20in%20multi-modal%20scenes.%20Extensive%20experiments%20on%20the%20Music-AVQA%0Aand%20AVQA-Yang%20datasets%20show%20that%20SaSR-Net%20outperforms%20state-of-the-art%20AVQA%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaSR-Net%253A%2520Source-Aware%2520Semantic%2520Representation%2520Network%2520for%2520Enhancing%250A%2520%2520Audio-Visual%2520Question%2520Answering%26entry.906535625%3Dianyu%2520Yang%2520and%2520Yiyang%2520Nan%2520and%2520Lisen%2520Dai%2520and%2520Zhenwen%2520Liang%2520and%2520Yapeng%2520Tian%2520and%2520Xiangliang%2520Zhang%26entry.1292438233%3D%2520%2520Audio-Visual%2520Question%2520Answering%2520%2528AVQA%2529%2520is%2520a%2520challenging%2520task%2520that%2520involves%250Aanswering%2520questions%2520based%2520on%2520both%2520auditory%2520and%2520visual%2520information%2520in%2520videos.%2520A%250Asignificant%2520challenge%2520is%2520interpreting%2520complex%2520multi-modal%2520scenes%252C%2520which%2520include%250Aboth%2520visual%2520objects%2520and%2520sound%2520sources%252C%2520and%2520connecting%2520them%2520to%2520the%2520given%250Aquestion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Source-aware%2520Semantic%2520Representation%250ANetwork%2520%2528SaSR-Net%2529%252C%2520a%2520novel%2520model%2520designed%2520for%2520AVQA.%2520SaSR-Net%2520utilizes%250Asource-wise%2520learnable%2520tokens%2520to%2520efficiently%2520capture%2520and%2520align%2520audio-visual%250Aelements%2520with%2520the%2520corresponding%2520question.%2520It%2520streamlines%2520the%2520fusion%2520of%2520audio%250Aand%2520visual%2520information%2520using%2520spatial%2520and%2520temporal%2520attention%2520mechanisms%2520to%250Aidentify%2520answers%2520in%2520multi-modal%2520scenes.%2520Extensive%2520experiments%2520on%2520the%2520Music-AVQA%250Aand%2520AVQA-Yang%2520datasets%2520show%2520that%2520SaSR-Net%2520outperforms%2520state-of-the-art%2520AVQA%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SaSR-Net%3A%20Source-Aware%20Semantic%20Representation%20Network%20for%20Enhancing%0A%20%20Audio-Visual%20Question%20Answering&entry.906535625=ianyu%20Yang%20and%20Yiyang%20Nan%20and%20Lisen%20Dai%20and%20Zhenwen%20Liang%20and%20Yapeng%20Tian%20and%20Xiangliang%20Zhang&entry.1292438233=%20%20Audio-Visual%20Question%20Answering%20%28AVQA%29%20is%20a%20challenging%20task%20that%20involves%0Aanswering%20questions%20based%20on%20both%20auditory%20and%20visual%20information%20in%20videos.%20A%0Asignificant%20challenge%20is%20interpreting%20complex%20multi-modal%20scenes%2C%20which%20include%0Aboth%20visual%20objects%20and%20sound%20sources%2C%20and%20connecting%20them%20to%20the%20given%0Aquestion.%20In%20this%20paper%2C%20we%20introduce%20the%20Source-aware%20Semantic%20Representation%0ANetwork%20%28SaSR-Net%29%2C%20a%20novel%20model%20designed%20for%20AVQA.%20SaSR-Net%20utilizes%0Asource-wise%20learnable%20tokens%20to%20efficiently%20capture%20and%20align%20audio-visual%0Aelements%20with%20the%20corresponding%20question.%20It%20streamlines%20the%20fusion%20of%20audio%0Aand%20visual%20information%20using%20spatial%20and%20temporal%20attention%20mechanisms%20to%0Aidentify%20answers%20in%20multi-modal%20scenes.%20Extensive%20experiments%20on%20the%20Music-AVQA%0Aand%20AVQA-Yang%20datasets%20show%20that%20SaSR-Net%20outperforms%20state-of-the-art%20AVQA%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04933v1&entry.124074799=Read"},
{"title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in\n  Low-resource Languages", "author": "Leena G Pillai and Kavya Manohar and Basil K Raju and Elizabeth Sherly", "abstract": "  This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.\n", "link": "http://arxiv.org/abs/2411.04573v1", "date": "2024-11-07", "relevancy": 2.0643, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4191}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4098}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multistage%20Fine-tuning%20Strategies%20for%20Automatic%20Speech%20Recognition%20in%0A%20%20Low-resource%20Languages&body=Title%3A%20Multistage%20Fine-tuning%20Strategies%20for%20Automatic%20Speech%20Recognition%20in%0A%20%20Low-resource%20Languages%0AAuthor%3A%20Leena%20G%20Pillai%20and%20Kavya%20Manohar%20and%20Basil%20K%20Raju%20and%20Elizabeth%20Sherly%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20multistage%20fine-tuning%20strategy%20designed%20to%0Aenhance%20automatic%20speech%20recognition%20%28ASR%29%20performance%20in%20low-resource%0Alanguages%20using%20OpenAI%27s%20Whisper%20model.%20In%20this%20approach%20we%20aim%20to%20build%20ASR%0Amodel%20for%20languages%20with%20limited%20digital%20resources%20by%20sequentially%20adapting%20the%0Amodel%20across%20linguistically%20similar%20languages.%20We%20experimented%20this%20on%20the%0AMalasar%20language%2C%20a%20Dravidian%20language%20spoken%20by%20approximately%20ten%20thousand%0Apeople%20in%20the%20Western%20Ghats%20of%20South%20India.%20Malasar%20language%20faces%20critical%0Achallenges%20for%20technological%20intervention%20due%20to%20its%20lack%20of%20a%20native%20script%0Aand%20absence%20of%20digital%20or%20spoken%20data%20resources.%20Working%20in%20collaboration%20with%0AWycliffe%20India%20and%20Malasar%20community%20members%2C%20we%20created%20a%20spoken%20Malasar%0Acorpus%20paired%20with%20transcription%20in%20Tamil%20script%2C%20a%20closely%20related%20major%0Alanguage.%20In%20our%20approach%20to%20build%20ASR%20model%20for%20Malasar%2C%20we%20first%20build%20an%0Aintermediate%20Tamil%20ASR%2C%20leveraging%20higher%20data%20availability%20for%20Tamil%20annotated%0Aspeech.%20This%20intermediate%20model%20is%20subsequently%20fine-tuned%20on%20Malasar%20data%2C%0Aallowing%20for%20more%20effective%20ASR%20adaptation%20despite%20limited%20resources.%20The%0Amultistage%20fine-tuning%20strategy%20demonstrated%20significant%20improvements%20over%0Adirect%20fine-tuning%20on%20Malasar%20data%20alone%2C%20achieving%20a%20word%20error%20rate%20%28WER%29%20of%0A51.9%25%2C%20which%20is%204.5%25%20absolute%20reduction%20when%20compared%20to%20the%20direct%20fine-tuning%0Amethod.%20Further%20a%20WER%20reduction%20to%2047.3%25%20was%20achieved%20through%20punctuation%0Aremoval%20in%20post-processing%2C%20which%20addresses%20formatting%20inconsistencies%20that%0Aimpact%20evaluation.%20Our%20results%20underscore%20the%20effectiveness%20of%20sequential%0Amultistage%20fine-tuning%20combined%20with%20targeted%20post-processing%20as%20a%20scalable%0Astrategy%20for%20ASR%20system%20development%20in%20low-resource%20languages%2C%20especially%20where%0Alinguistic%20similarities%20can%20be%20leveraged%20to%20bridge%20gaps%20in%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultistage%2520Fine-tuning%2520Strategies%2520for%2520Automatic%2520Speech%2520Recognition%2520in%250A%2520%2520Low-resource%2520Languages%26entry.906535625%3DLeena%2520G%2520Pillai%2520and%2520Kavya%2520Manohar%2520and%2520Basil%2520K%2520Raju%2520and%2520Elizabeth%2520Sherly%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520multistage%2520fine-tuning%2520strategy%2520designed%2520to%250Aenhance%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520performance%2520in%2520low-resource%250Alanguages%2520using%2520OpenAI%2527s%2520Whisper%2520model.%2520In%2520this%2520approach%2520we%2520aim%2520to%2520build%2520ASR%250Amodel%2520for%2520languages%2520with%2520limited%2520digital%2520resources%2520by%2520sequentially%2520adapting%2520the%250Amodel%2520across%2520linguistically%2520similar%2520languages.%2520We%2520experimented%2520this%2520on%2520the%250AMalasar%2520language%252C%2520a%2520Dravidian%2520language%2520spoken%2520by%2520approximately%2520ten%2520thousand%250Apeople%2520in%2520the%2520Western%2520Ghats%2520of%2520South%2520India.%2520Malasar%2520language%2520faces%2520critical%250Achallenges%2520for%2520technological%2520intervention%2520due%2520to%2520its%2520lack%2520of%2520a%2520native%2520script%250Aand%2520absence%2520of%2520digital%2520or%2520spoken%2520data%2520resources.%2520Working%2520in%2520collaboration%2520with%250AWycliffe%2520India%2520and%2520Malasar%2520community%2520members%252C%2520we%2520created%2520a%2520spoken%2520Malasar%250Acorpus%2520paired%2520with%2520transcription%2520in%2520Tamil%2520script%252C%2520a%2520closely%2520related%2520major%250Alanguage.%2520In%2520our%2520approach%2520to%2520build%2520ASR%2520model%2520for%2520Malasar%252C%2520we%2520first%2520build%2520an%250Aintermediate%2520Tamil%2520ASR%252C%2520leveraging%2520higher%2520data%2520availability%2520for%2520Tamil%2520annotated%250Aspeech.%2520This%2520intermediate%2520model%2520is%2520subsequently%2520fine-tuned%2520on%2520Malasar%2520data%252C%250Aallowing%2520for%2520more%2520effective%2520ASR%2520adaptation%2520despite%2520limited%2520resources.%2520The%250Amultistage%2520fine-tuning%2520strategy%2520demonstrated%2520significant%2520improvements%2520over%250Adirect%2520fine-tuning%2520on%2520Malasar%2520data%2520alone%252C%2520achieving%2520a%2520word%2520error%2520rate%2520%2528WER%2529%2520of%250A51.9%2525%252C%2520which%2520is%25204.5%2525%2520absolute%2520reduction%2520when%2520compared%2520to%2520the%2520direct%2520fine-tuning%250Amethod.%2520Further%2520a%2520WER%2520reduction%2520to%252047.3%2525%2520was%2520achieved%2520through%2520punctuation%250Aremoval%2520in%2520post-processing%252C%2520which%2520addresses%2520formatting%2520inconsistencies%2520that%250Aimpact%2520evaluation.%2520Our%2520results%2520underscore%2520the%2520effectiveness%2520of%2520sequential%250Amultistage%2520fine-tuning%2520combined%2520with%2520targeted%2520post-processing%2520as%2520a%2520scalable%250Astrategy%2520for%2520ASR%2520system%2520development%2520in%2520low-resource%2520languages%252C%2520especially%2520where%250Alinguistic%2520similarities%2520can%2520be%2520leveraged%2520to%2520bridge%2520gaps%2520in%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multistage%20Fine-tuning%20Strategies%20for%20Automatic%20Speech%20Recognition%20in%0A%20%20Low-resource%20Languages&entry.906535625=Leena%20G%20Pillai%20and%20Kavya%20Manohar%20and%20Basil%20K%20Raju%20and%20Elizabeth%20Sherly&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20multistage%20fine-tuning%20strategy%20designed%20to%0Aenhance%20automatic%20speech%20recognition%20%28ASR%29%20performance%20in%20low-resource%0Alanguages%20using%20OpenAI%27s%20Whisper%20model.%20In%20this%20approach%20we%20aim%20to%20build%20ASR%0Amodel%20for%20languages%20with%20limited%20digital%20resources%20by%20sequentially%20adapting%20the%0Amodel%20across%20linguistically%20similar%20languages.%20We%20experimented%20this%20on%20the%0AMalasar%20language%2C%20a%20Dravidian%20language%20spoken%20by%20approximately%20ten%20thousand%0Apeople%20in%20the%20Western%20Ghats%20of%20South%20India.%20Malasar%20language%20faces%20critical%0Achallenges%20for%20technological%20intervention%20due%20to%20its%20lack%20of%20a%20native%20script%0Aand%20absence%20of%20digital%20or%20spoken%20data%20resources.%20Working%20in%20collaboration%20with%0AWycliffe%20India%20and%20Malasar%20community%20members%2C%20we%20created%20a%20spoken%20Malasar%0Acorpus%20paired%20with%20transcription%20in%20Tamil%20script%2C%20a%20closely%20related%20major%0Alanguage.%20In%20our%20approach%20to%20build%20ASR%20model%20for%20Malasar%2C%20we%20first%20build%20an%0Aintermediate%20Tamil%20ASR%2C%20leveraging%20higher%20data%20availability%20for%20Tamil%20annotated%0Aspeech.%20This%20intermediate%20model%20is%20subsequently%20fine-tuned%20on%20Malasar%20data%2C%0Aallowing%20for%20more%20effective%20ASR%20adaptation%20despite%20limited%20resources.%20The%0Amultistage%20fine-tuning%20strategy%20demonstrated%20significant%20improvements%20over%0Adirect%20fine-tuning%20on%20Malasar%20data%20alone%2C%20achieving%20a%20word%20error%20rate%20%28WER%29%20of%0A51.9%25%2C%20which%20is%204.5%25%20absolute%20reduction%20when%20compared%20to%20the%20direct%20fine-tuning%0Amethod.%20Further%20a%20WER%20reduction%20to%2047.3%25%20was%20achieved%20through%20punctuation%0Aremoval%20in%20post-processing%2C%20which%20addresses%20formatting%20inconsistencies%20that%0Aimpact%20evaluation.%20Our%20results%20underscore%20the%20effectiveness%20of%20sequential%0Amultistage%20fine-tuning%20combined%20with%20targeted%20post-processing%20as%20a%20scalable%0Astrategy%20for%20ASR%20system%20development%20in%20low-resource%20languages%2C%20especially%20where%0Alinguistic%20similarities%20can%20be%20leveraged%20to%20bridge%20gaps%20in%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04573v1&entry.124074799=Read"},
{"title": "LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics", "author": "Thomas Robert and Mher Safaryan and Ionut-Vlad Modoranu and Dan Alistarh", "abstract": "  We introduce LDAdam, a memory-efficient optimizer for training large models,\nthat performs adaptive optimization steps within lower dimensional subspaces,\nwhile consistently exploring the full parameter space during training. This\nstrategy keeps the optimizer's memory footprint to a fraction of the model\nsize. LDAdam relies on a new projection-aware update rule for the optimizer\nstates that allows for transitioning between subspaces, i.e., estimation of the\nstatistics of the projected gradients. To mitigate the errors due to low-rank\nprojection, LDAdam integrates a new generalized error feedback mechanism, which\nexplicitly accounts for both gradient and optimizer state compression. We prove\nthe convergence of LDAdam under standard assumptions, and show that LDAdam\nallows for accurate and efficient fine-tuning and pre-training of language\nmodels.\n", "link": "http://arxiv.org/abs/2410.16103v3", "date": "2024-11-07", "relevancy": 2.0548, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5181}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics&body=Title%3A%20LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics%0AAuthor%3A%20Thomas%20Robert%20and%20Mher%20Safaryan%20and%20Ionut-Vlad%20Modoranu%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20We%20introduce%20LDAdam%2C%20a%20memory-efficient%20optimizer%20for%20training%20large%20models%2C%0Athat%20performs%20adaptive%20optimization%20steps%20within%20lower%20dimensional%20subspaces%2C%0Awhile%20consistently%20exploring%20the%20full%20parameter%20space%20during%20training.%20This%0Astrategy%20keeps%20the%20optimizer%27s%20memory%20footprint%20to%20a%20fraction%20of%20the%20model%0Asize.%20LDAdam%20relies%20on%20a%20new%20projection-aware%20update%20rule%20for%20the%20optimizer%0Astates%20that%20allows%20for%20transitioning%20between%20subspaces%2C%20i.e.%2C%20estimation%20of%20the%0Astatistics%20of%20the%20projected%20gradients.%20To%20mitigate%20the%20errors%20due%20to%20low-rank%0Aprojection%2C%20LDAdam%20integrates%20a%20new%20generalized%20error%20feedback%20mechanism%2C%20which%0Aexplicitly%20accounts%20for%20both%20gradient%20and%20optimizer%20state%20compression.%20We%20prove%0Athe%20convergence%20of%20LDAdam%20under%20standard%20assumptions%2C%20and%20show%20that%20LDAdam%0Aallows%20for%20accurate%20and%20efficient%20fine-tuning%20and%20pre-training%20of%20language%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16103v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLDAdam%253A%2520Adaptive%2520Optimization%2520from%2520Low-Dimensional%2520Gradient%2520Statistics%26entry.906535625%3DThomas%2520Robert%2520and%2520Mher%2520Safaryan%2520and%2520Ionut-Vlad%2520Modoranu%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520We%2520introduce%2520LDAdam%252C%2520a%2520memory-efficient%2520optimizer%2520for%2520training%2520large%2520models%252C%250Athat%2520performs%2520adaptive%2520optimization%2520steps%2520within%2520lower%2520dimensional%2520subspaces%252C%250Awhile%2520consistently%2520exploring%2520the%2520full%2520parameter%2520space%2520during%2520training.%2520This%250Astrategy%2520keeps%2520the%2520optimizer%2527s%2520memory%2520footprint%2520to%2520a%2520fraction%2520of%2520the%2520model%250Asize.%2520LDAdam%2520relies%2520on%2520a%2520new%2520projection-aware%2520update%2520rule%2520for%2520the%2520optimizer%250Astates%2520that%2520allows%2520for%2520transitioning%2520between%2520subspaces%252C%2520i.e.%252C%2520estimation%2520of%2520the%250Astatistics%2520of%2520the%2520projected%2520gradients.%2520To%2520mitigate%2520the%2520errors%2520due%2520to%2520low-rank%250Aprojection%252C%2520LDAdam%2520integrates%2520a%2520new%2520generalized%2520error%2520feedback%2520mechanism%252C%2520which%250Aexplicitly%2520accounts%2520for%2520both%2520gradient%2520and%2520optimizer%2520state%2520compression.%2520We%2520prove%250Athe%2520convergence%2520of%2520LDAdam%2520under%2520standard%2520assumptions%252C%2520and%2520show%2520that%2520LDAdam%250Aallows%2520for%2520accurate%2520and%2520efficient%2520fine-tuning%2520and%2520pre-training%2520of%2520language%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16103v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics&entry.906535625=Thomas%20Robert%20and%20Mher%20Safaryan%20and%20Ionut-Vlad%20Modoranu%20and%20Dan%20Alistarh&entry.1292438233=%20%20We%20introduce%20LDAdam%2C%20a%20memory-efficient%20optimizer%20for%20training%20large%20models%2C%0Athat%20performs%20adaptive%20optimization%20steps%20within%20lower%20dimensional%20subspaces%2C%0Awhile%20consistently%20exploring%20the%20full%20parameter%20space%20during%20training.%20This%0Astrategy%20keeps%20the%20optimizer%27s%20memory%20footprint%20to%20a%20fraction%20of%20the%20model%0Asize.%20LDAdam%20relies%20on%20a%20new%20projection-aware%20update%20rule%20for%20the%20optimizer%0Astates%20that%20allows%20for%20transitioning%20between%20subspaces%2C%20i.e.%2C%20estimation%20of%20the%0Astatistics%20of%20the%20projected%20gradients.%20To%20mitigate%20the%20errors%20due%20to%20low-rank%0Aprojection%2C%20LDAdam%20integrates%20a%20new%20generalized%20error%20feedback%20mechanism%2C%20which%0Aexplicitly%20accounts%20for%20both%20gradient%20and%20optimizer%20state%20compression.%20We%20prove%0Athe%20convergence%20of%20LDAdam%20under%20standard%20assumptions%2C%20and%20show%20that%20LDAdam%0Aallows%20for%20accurate%20and%20efficient%20fine-tuning%20and%20pre-training%20of%20language%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16103v3&entry.124074799=Read"},
{"title": "AllGaits: Learning All Quadruped Gaits and Transitions", "author": "Guillaume Bellegarda and Milad Shafiee and Auke Ijspeert", "abstract": "  We present a framework for learning a single policy capable of producing all\nquadruped gaits and transitions. The framework consists of a policy trained\nwith deep reinforcement learning (DRL) to modulate the parameters of a system\nof abstract oscillators (i.e. Central Pattern Generator), whose output is\nmapped to joint commands through a pattern formation layer that sets the gait\nstyle, i.e. body height, swing foot ground clearance height, and foot offset.\nDifferent gaits are formed by changing the coupling between different\noscillators, which can be instantaneously selected at any velocity by a user.\nWith this framework, we systematically investigate which gait should be used at\nwhich velocity, and when gait transitions should occur from a Cost of Transport\n(COT), i.e. energy-efficiency, point of view. Additionally, we note how gait\nstyle changes as a function of locomotion speed for each gait to keep the most\nenergy-efficient locomotion. While the currently most popular gait (trot) does\nnot result in the lowest COT, we find that considering different co-dependent\nmetrics such as mean base velocity and joint acceleration result in different\n`optimal' gaits than those that minimize COT. We deploy our controller in\nvarious hardware experiments, showing all 9 typical quadruped animal gaits, and\ndemonstrate generalizability to unseen gaits during training, and robustness to\nleg failures. Video results can be found at https://youtu.be/OLoWSX_R868.\n", "link": "http://arxiv.org/abs/2411.04787v1", "date": "2024-11-07", "relevancy": 2.0441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5236}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AllGaits%3A%20Learning%20All%20Quadruped%20Gaits%20and%20Transitions&body=Title%3A%20AllGaits%3A%20Learning%20All%20Quadruped%20Gaits%20and%20Transitions%0AAuthor%3A%20Guillaume%20Bellegarda%20and%20Milad%20Shafiee%20and%20Auke%20Ijspeert%0AAbstract%3A%20%20%20We%20present%20a%20framework%20for%20learning%20a%20single%20policy%20capable%20of%20producing%20all%0Aquadruped%20gaits%20and%20transitions.%20The%20framework%20consists%20of%20a%20policy%20trained%0Awith%20deep%20reinforcement%20learning%20%28DRL%29%20to%20modulate%20the%20parameters%20of%20a%20system%0Aof%20abstract%20oscillators%20%28i.e.%20Central%20Pattern%20Generator%29%2C%20whose%20output%20is%0Amapped%20to%20joint%20commands%20through%20a%20pattern%20formation%20layer%20that%20sets%20the%20gait%0Astyle%2C%20i.e.%20body%20height%2C%20swing%20foot%20ground%20clearance%20height%2C%20and%20foot%20offset.%0ADifferent%20gaits%20are%20formed%20by%20changing%20the%20coupling%20between%20different%0Aoscillators%2C%20which%20can%20be%20instantaneously%20selected%20at%20any%20velocity%20by%20a%20user.%0AWith%20this%20framework%2C%20we%20systematically%20investigate%20which%20gait%20should%20be%20used%20at%0Awhich%20velocity%2C%20and%20when%20gait%20transitions%20should%20occur%20from%20a%20Cost%20of%20Transport%0A%28COT%29%2C%20i.e.%20energy-efficiency%2C%20point%20of%20view.%20Additionally%2C%20we%20note%20how%20gait%0Astyle%20changes%20as%20a%20function%20of%20locomotion%20speed%20for%20each%20gait%20to%20keep%20the%20most%0Aenergy-efficient%20locomotion.%20While%20the%20currently%20most%20popular%20gait%20%28trot%29%20does%0Anot%20result%20in%20the%20lowest%20COT%2C%20we%20find%20that%20considering%20different%20co-dependent%0Ametrics%20such%20as%20mean%20base%20velocity%20and%20joint%20acceleration%20result%20in%20different%0A%60optimal%27%20gaits%20than%20those%20that%20minimize%20COT.%20We%20deploy%20our%20controller%20in%0Avarious%20hardware%20experiments%2C%20showing%20all%209%20typical%20quadruped%20animal%20gaits%2C%20and%0Ademonstrate%20generalizability%20to%20unseen%20gaits%20during%20training%2C%20and%20robustness%20to%0Aleg%20failures.%20Video%20results%20can%20be%20found%20at%20https%3A//youtu.be/OLoWSX_R868.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAllGaits%253A%2520Learning%2520All%2520Quadruped%2520Gaits%2520and%2520Transitions%26entry.906535625%3DGuillaume%2520Bellegarda%2520and%2520Milad%2520Shafiee%2520and%2520Auke%2520Ijspeert%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520framework%2520for%2520learning%2520a%2520single%2520policy%2520capable%2520of%2520producing%2520all%250Aquadruped%2520gaits%2520and%2520transitions.%2520The%2520framework%2520consists%2520of%2520a%2520policy%2520trained%250Awith%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520to%2520modulate%2520the%2520parameters%2520of%2520a%2520system%250Aof%2520abstract%2520oscillators%2520%2528i.e.%2520Central%2520Pattern%2520Generator%2529%252C%2520whose%2520output%2520is%250Amapped%2520to%2520joint%2520commands%2520through%2520a%2520pattern%2520formation%2520layer%2520that%2520sets%2520the%2520gait%250Astyle%252C%2520i.e.%2520body%2520height%252C%2520swing%2520foot%2520ground%2520clearance%2520height%252C%2520and%2520foot%2520offset.%250ADifferent%2520gaits%2520are%2520formed%2520by%2520changing%2520the%2520coupling%2520between%2520different%250Aoscillators%252C%2520which%2520can%2520be%2520instantaneously%2520selected%2520at%2520any%2520velocity%2520by%2520a%2520user.%250AWith%2520this%2520framework%252C%2520we%2520systematically%2520investigate%2520which%2520gait%2520should%2520be%2520used%2520at%250Awhich%2520velocity%252C%2520and%2520when%2520gait%2520transitions%2520should%2520occur%2520from%2520a%2520Cost%2520of%2520Transport%250A%2528COT%2529%252C%2520i.e.%2520energy-efficiency%252C%2520point%2520of%2520view.%2520Additionally%252C%2520we%2520note%2520how%2520gait%250Astyle%2520changes%2520as%2520a%2520function%2520of%2520locomotion%2520speed%2520for%2520each%2520gait%2520to%2520keep%2520the%2520most%250Aenergy-efficient%2520locomotion.%2520While%2520the%2520currently%2520most%2520popular%2520gait%2520%2528trot%2529%2520does%250Anot%2520result%2520in%2520the%2520lowest%2520COT%252C%2520we%2520find%2520that%2520considering%2520different%2520co-dependent%250Ametrics%2520such%2520as%2520mean%2520base%2520velocity%2520and%2520joint%2520acceleration%2520result%2520in%2520different%250A%2560optimal%2527%2520gaits%2520than%2520those%2520that%2520minimize%2520COT.%2520We%2520deploy%2520our%2520controller%2520in%250Avarious%2520hardware%2520experiments%252C%2520showing%2520all%25209%2520typical%2520quadruped%2520animal%2520gaits%252C%2520and%250Ademonstrate%2520generalizability%2520to%2520unseen%2520gaits%2520during%2520training%252C%2520and%2520robustness%2520to%250Aleg%2520failures.%2520Video%2520results%2520can%2520be%2520found%2520at%2520https%253A//youtu.be/OLoWSX_R868.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AllGaits%3A%20Learning%20All%20Quadruped%20Gaits%20and%20Transitions&entry.906535625=Guillaume%20Bellegarda%20and%20Milad%20Shafiee%20and%20Auke%20Ijspeert&entry.1292438233=%20%20We%20present%20a%20framework%20for%20learning%20a%20single%20policy%20capable%20of%20producing%20all%0Aquadruped%20gaits%20and%20transitions.%20The%20framework%20consists%20of%20a%20policy%20trained%0Awith%20deep%20reinforcement%20learning%20%28DRL%29%20to%20modulate%20the%20parameters%20of%20a%20system%0Aof%20abstract%20oscillators%20%28i.e.%20Central%20Pattern%20Generator%29%2C%20whose%20output%20is%0Amapped%20to%20joint%20commands%20through%20a%20pattern%20formation%20layer%20that%20sets%20the%20gait%0Astyle%2C%20i.e.%20body%20height%2C%20swing%20foot%20ground%20clearance%20height%2C%20and%20foot%20offset.%0ADifferent%20gaits%20are%20formed%20by%20changing%20the%20coupling%20between%20different%0Aoscillators%2C%20which%20can%20be%20instantaneously%20selected%20at%20any%20velocity%20by%20a%20user.%0AWith%20this%20framework%2C%20we%20systematically%20investigate%20which%20gait%20should%20be%20used%20at%0Awhich%20velocity%2C%20and%20when%20gait%20transitions%20should%20occur%20from%20a%20Cost%20of%20Transport%0A%28COT%29%2C%20i.e.%20energy-efficiency%2C%20point%20of%20view.%20Additionally%2C%20we%20note%20how%20gait%0Astyle%20changes%20as%20a%20function%20of%20locomotion%20speed%20for%20each%20gait%20to%20keep%20the%20most%0Aenergy-efficient%20locomotion.%20While%20the%20currently%20most%20popular%20gait%20%28trot%29%20does%0Anot%20result%20in%20the%20lowest%20COT%2C%20we%20find%20that%20considering%20different%20co-dependent%0Ametrics%20such%20as%20mean%20base%20velocity%20and%20joint%20acceleration%20result%20in%20different%0A%60optimal%27%20gaits%20than%20those%20that%20minimize%20COT.%20We%20deploy%20our%20controller%20in%0Avarious%20hardware%20experiments%2C%20showing%20all%209%20typical%20quadruped%20animal%20gaits%2C%20and%0Ademonstrate%20generalizability%20to%20unseen%20gaits%20during%20training%2C%20and%20robustness%20to%0Aleg%20failures.%20Video%20results%20can%20be%20found%20at%20https%3A//youtu.be/OLoWSX_R868.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04787v1&entry.124074799=Read"},
{"title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large\n  Language Models for Vulnerability Analysis in Decompiled Binaries", "author": "Dylan Manuel and Nafis Tanveer Islam and Joseph Khoury and Ana Nunez and Elias Bou-Harb and Peyman Najafirad", "abstract": "  Security experts reverse engineer (decompile) binary code to identify\ncritical security vulnerabilities. The limited access to source code in vital\nsystems - such as firmware, drivers, and proprietary software used in Critical\nInfrastructures (CI) - makes this analysis even more crucial on the binary\nlevel. Even with available source code, a semantic gap persists after\ncompilation between the source and the binary code executed by the processor.\nThis gap may hinder the detection of vulnerabilities in source code. That being\nsaid, current research on Large Language Models (LLMs) overlooks the\nsignificance of decompiled binaries in this area by focusing solely on source\ncode. In this work, we are the first to empirically uncover the substantial\nsemantic limitations of state-of-the-art LLMs when it comes to analyzing\nvulnerabilities in decompiled binaries, largely due to the absence of relevant\ndatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary\ncode vulnerability dataset. Our dataset is multi-architecture and\nmulti-optimization, focusing on C/C++ due to their wide usage in CI and\nassociation with numerous vulnerabilities. Specifically, we curate 150,872\nsamples of vulnerable and non-vulnerable decompiled binary code for the task of\n(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)\nrecovering function names in the domain of decompiled binaries. Subsequently,\nwe fine-tune state-of-the-art LLMs using DeBinVul and report on a performance\nincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and\nCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,\nusing DeBinVul, we report a high performance of 80-90% on the vulnerability\nclassification task. Furthermore, we report improved performance in function\nname recovery and vulnerability description tasks.\n", "link": "http://arxiv.org/abs/2411.04981v1", "date": "2024-11-07", "relevancy": 2.0366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Reverse%20Engineering%3A%20Investigating%20and%20Benchmarking%20Large%0A%20%20Language%20Models%20for%20Vulnerability%20Analysis%20in%20Decompiled%20Binaries&body=Title%3A%20Enhancing%20Reverse%20Engineering%3A%20Investigating%20and%20Benchmarking%20Large%0A%20%20Language%20Models%20for%20Vulnerability%20Analysis%20in%20Decompiled%20Binaries%0AAuthor%3A%20Dylan%20Manuel%20and%20Nafis%20Tanveer%20Islam%20and%20Joseph%20Khoury%20and%20Ana%20Nunez%20and%20Elias%20Bou-Harb%20and%20Peyman%20Najafirad%0AAbstract%3A%20%20%20Security%20experts%20reverse%20engineer%20%28decompile%29%20binary%20code%20to%20identify%0Acritical%20security%20vulnerabilities.%20The%20limited%20access%20to%20source%20code%20in%20vital%0Asystems%20-%20such%20as%20firmware%2C%20drivers%2C%20and%20proprietary%20software%20used%20in%20Critical%0AInfrastructures%20%28CI%29%20-%20makes%20this%20analysis%20even%20more%20crucial%20on%20the%20binary%0Alevel.%20Even%20with%20available%20source%20code%2C%20a%20semantic%20gap%20persists%20after%0Acompilation%20between%20the%20source%20and%20the%20binary%20code%20executed%20by%20the%20processor.%0AThis%20gap%20may%20hinder%20the%20detection%20of%20vulnerabilities%20in%20source%20code.%20That%20being%0Asaid%2C%20current%20research%20on%20Large%20Language%20Models%20%28LLMs%29%20overlooks%20the%0Asignificance%20of%20decompiled%20binaries%20in%20this%20area%20by%20focusing%20solely%20on%20source%0Acode.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20empirically%20uncover%20the%20substantial%0Asemantic%20limitations%20of%20state-of-the-art%20LLMs%20when%20it%20comes%20to%20analyzing%0Avulnerabilities%20in%20decompiled%20binaries%2C%20largely%20due%20to%20the%20absence%20of%20relevant%0Adatasets.%20To%20bridge%20the%20gap%2C%20we%20introduce%20DeBinVul%2C%20a%20novel%20decompiled%20binary%0Acode%20vulnerability%20dataset.%20Our%20dataset%20is%20multi-architecture%20and%0Amulti-optimization%2C%20focusing%20on%20C/C%2B%2B%20due%20to%20their%20wide%20usage%20in%20CI%20and%0Aassociation%20with%20numerous%20vulnerabilities.%20Specifically%2C%20we%20curate%20150%2C872%0Asamples%20of%20vulnerable%20and%20non-vulnerable%20decompiled%20binary%20code%20for%20the%20task%20of%0A%28i%29%20identifying%3B%20%28ii%29%20classifying%3B%20%28iii%29%20describing%20vulnerabilities%3B%20and%20%28iv%29%0Arecovering%20function%20names%20in%20the%20domain%20of%20decompiled%20binaries.%20Subsequently%2C%0Awe%20fine-tune%20state-of-the-art%20LLMs%20using%20DeBinVul%20and%20report%20on%20a%20performance%0Aincrease%20of%2019%25%2C%2024%25%2C%20and%2021%25%20in%20the%20capabilities%20of%20CodeLlama%2C%20Llama3%2C%20and%0ACodeGen2%20respectively%2C%20in%20detecting%20binary%20code%20vulnerabilities.%20Additionally%2C%0Ausing%20DeBinVul%2C%20we%20report%20a%20high%20performance%20of%2080-90%25%20on%20the%20vulnerability%0Aclassification%20task.%20Furthermore%2C%20we%20report%20improved%20performance%20in%20function%0Aname%20recovery%20and%20vulnerability%20description%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Reverse%2520Engineering%253A%2520Investigating%2520and%2520Benchmarking%2520Large%250A%2520%2520Language%2520Models%2520for%2520Vulnerability%2520Analysis%2520in%2520Decompiled%2520Binaries%26entry.906535625%3DDylan%2520Manuel%2520and%2520Nafis%2520Tanveer%2520Islam%2520and%2520Joseph%2520Khoury%2520and%2520Ana%2520Nunez%2520and%2520Elias%2520Bou-Harb%2520and%2520Peyman%2520Najafirad%26entry.1292438233%3D%2520%2520Security%2520experts%2520reverse%2520engineer%2520%2528decompile%2529%2520binary%2520code%2520to%2520identify%250Acritical%2520security%2520vulnerabilities.%2520The%2520limited%2520access%2520to%2520source%2520code%2520in%2520vital%250Asystems%2520-%2520such%2520as%2520firmware%252C%2520drivers%252C%2520and%2520proprietary%2520software%2520used%2520in%2520Critical%250AInfrastructures%2520%2528CI%2529%2520-%2520makes%2520this%2520analysis%2520even%2520more%2520crucial%2520on%2520the%2520binary%250Alevel.%2520Even%2520with%2520available%2520source%2520code%252C%2520a%2520semantic%2520gap%2520persists%2520after%250Acompilation%2520between%2520the%2520source%2520and%2520the%2520binary%2520code%2520executed%2520by%2520the%2520processor.%250AThis%2520gap%2520may%2520hinder%2520the%2520detection%2520of%2520vulnerabilities%2520in%2520source%2520code.%2520That%2520being%250Asaid%252C%2520current%2520research%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520overlooks%2520the%250Asignificance%2520of%2520decompiled%2520binaries%2520in%2520this%2520area%2520by%2520focusing%2520solely%2520on%2520source%250Acode.%2520In%2520this%2520work%252C%2520we%2520are%2520the%2520first%2520to%2520empirically%2520uncover%2520the%2520substantial%250Asemantic%2520limitations%2520of%2520state-of-the-art%2520LLMs%2520when%2520it%2520comes%2520to%2520analyzing%250Avulnerabilities%2520in%2520decompiled%2520binaries%252C%2520largely%2520due%2520to%2520the%2520absence%2520of%2520relevant%250Adatasets.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520introduce%2520DeBinVul%252C%2520a%2520novel%2520decompiled%2520binary%250Acode%2520vulnerability%2520dataset.%2520Our%2520dataset%2520is%2520multi-architecture%2520and%250Amulti-optimization%252C%2520focusing%2520on%2520C/C%252B%252B%2520due%2520to%2520their%2520wide%2520usage%2520in%2520CI%2520and%250Aassociation%2520with%2520numerous%2520vulnerabilities.%2520Specifically%252C%2520we%2520curate%2520150%252C872%250Asamples%2520of%2520vulnerable%2520and%2520non-vulnerable%2520decompiled%2520binary%2520code%2520for%2520the%2520task%2520of%250A%2528i%2529%2520identifying%253B%2520%2528ii%2529%2520classifying%253B%2520%2528iii%2529%2520describing%2520vulnerabilities%253B%2520and%2520%2528iv%2529%250Arecovering%2520function%2520names%2520in%2520the%2520domain%2520of%2520decompiled%2520binaries.%2520Subsequently%252C%250Awe%2520fine-tune%2520state-of-the-art%2520LLMs%2520using%2520DeBinVul%2520and%2520report%2520on%2520a%2520performance%250Aincrease%2520of%252019%2525%252C%252024%2525%252C%2520and%252021%2525%2520in%2520the%2520capabilities%2520of%2520CodeLlama%252C%2520Llama3%252C%2520and%250ACodeGen2%2520respectively%252C%2520in%2520detecting%2520binary%2520code%2520vulnerabilities.%2520Additionally%252C%250Ausing%2520DeBinVul%252C%2520we%2520report%2520a%2520high%2520performance%2520of%252080-90%2525%2520on%2520the%2520vulnerability%250Aclassification%2520task.%2520Furthermore%252C%2520we%2520report%2520improved%2520performance%2520in%2520function%250Aname%2520recovery%2520and%2520vulnerability%2520description%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Reverse%20Engineering%3A%20Investigating%20and%20Benchmarking%20Large%0A%20%20Language%20Models%20for%20Vulnerability%20Analysis%20in%20Decompiled%20Binaries&entry.906535625=Dylan%20Manuel%20and%20Nafis%20Tanveer%20Islam%20and%20Joseph%20Khoury%20and%20Ana%20Nunez%20and%20Elias%20Bou-Harb%20and%20Peyman%20Najafirad&entry.1292438233=%20%20Security%20experts%20reverse%20engineer%20%28decompile%29%20binary%20code%20to%20identify%0Acritical%20security%20vulnerabilities.%20The%20limited%20access%20to%20source%20code%20in%20vital%0Asystems%20-%20such%20as%20firmware%2C%20drivers%2C%20and%20proprietary%20software%20used%20in%20Critical%0AInfrastructures%20%28CI%29%20-%20makes%20this%20analysis%20even%20more%20crucial%20on%20the%20binary%0Alevel.%20Even%20with%20available%20source%20code%2C%20a%20semantic%20gap%20persists%20after%0Acompilation%20between%20the%20source%20and%20the%20binary%20code%20executed%20by%20the%20processor.%0AThis%20gap%20may%20hinder%20the%20detection%20of%20vulnerabilities%20in%20source%20code.%20That%20being%0Asaid%2C%20current%20research%20on%20Large%20Language%20Models%20%28LLMs%29%20overlooks%20the%0Asignificance%20of%20decompiled%20binaries%20in%20this%20area%20by%20focusing%20solely%20on%20source%0Acode.%20In%20this%20work%2C%20we%20are%20the%20first%20to%20empirically%20uncover%20the%20substantial%0Asemantic%20limitations%20of%20state-of-the-art%20LLMs%20when%20it%20comes%20to%20analyzing%0Avulnerabilities%20in%20decompiled%20binaries%2C%20largely%20due%20to%20the%20absence%20of%20relevant%0Adatasets.%20To%20bridge%20the%20gap%2C%20we%20introduce%20DeBinVul%2C%20a%20novel%20decompiled%20binary%0Acode%20vulnerability%20dataset.%20Our%20dataset%20is%20multi-architecture%20and%0Amulti-optimization%2C%20focusing%20on%20C/C%2B%2B%20due%20to%20their%20wide%20usage%20in%20CI%20and%0Aassociation%20with%20numerous%20vulnerabilities.%20Specifically%2C%20we%20curate%20150%2C872%0Asamples%20of%20vulnerable%20and%20non-vulnerable%20decompiled%20binary%20code%20for%20the%20task%20of%0A%28i%29%20identifying%3B%20%28ii%29%20classifying%3B%20%28iii%29%20describing%20vulnerabilities%3B%20and%20%28iv%29%0Arecovering%20function%20names%20in%20the%20domain%20of%20decompiled%20binaries.%20Subsequently%2C%0Awe%20fine-tune%20state-of-the-art%20LLMs%20using%20DeBinVul%20and%20report%20on%20a%20performance%0Aincrease%20of%2019%25%2C%2024%25%2C%20and%2021%25%20in%20the%20capabilities%20of%20CodeLlama%2C%20Llama3%2C%20and%0ACodeGen2%20respectively%2C%20in%20detecting%20binary%20code%20vulnerabilities.%20Additionally%2C%0Ausing%20DeBinVul%2C%20we%20report%20a%20high%20performance%20of%2080-90%25%20on%20the%20vulnerability%0Aclassification%20task.%20Furthermore%2C%20we%20report%20improved%20performance%20in%20function%0Aname%20recovery%20and%20vulnerability%20description%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04981v1&entry.124074799=Read"},
{"title": "Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data", "author": "Nikita Tsoy and Nikola Konstantinov", "abstract": "  Simplicity bias, the propensity of deep models to over-rely on simple\nfeatures, has been identified as a potential reason for limited\nout-of-distribution generalization of neural networks (Shah et al., 2020).\nDespite the important implications, this phenomenon has been theoretically\nconfirmed and characterized only under strong dataset assumptions, such as\nlinear separability (Lyu et al., 2021). In this work, we characterize\nsimplicity bias for general datasets in the context of two-layer neural\nnetworks initialized with small weights and trained with gradient flow.\nSpecifically, we prove that in the early training phases, network features\ncluster around a few directions that do not depend on the size of the hidden\nlayer. Furthermore, for datasets with an XOR-like pattern, we precisely\nidentify the learned features and demonstrate that simplicity bias intensifies\nduring later training stages. These results indicate that features learned in\nthe middle stages of training may be more useful for OOD transfer. We support\nthis hypothesis with experiments on image data.\n", "link": "http://arxiv.org/abs/2405.17299v2", "date": "2024-11-07", "relevancy": 2.0323, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5062}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplicity%20Bias%20of%20Two-Layer%20Networks%20beyond%20Linearly%20Separable%20Data&body=Title%3A%20Simplicity%20Bias%20of%20Two-Layer%20Networks%20beyond%20Linearly%20Separable%20Data%0AAuthor%3A%20Nikita%20Tsoy%20and%20Nikola%20Konstantinov%0AAbstract%3A%20%20%20Simplicity%20bias%2C%20the%20propensity%20of%20deep%20models%20to%20over-rely%20on%20simple%0Afeatures%2C%20has%20been%20identified%20as%20a%20potential%20reason%20for%20limited%0Aout-of-distribution%20generalization%20of%20neural%20networks%20%28Shah%20et%20al.%2C%202020%29.%0ADespite%20the%20important%20implications%2C%20this%20phenomenon%20has%20been%20theoretically%0Aconfirmed%20and%20characterized%20only%20under%20strong%20dataset%20assumptions%2C%20such%20as%0Alinear%20separability%20%28Lyu%20et%20al.%2C%202021%29.%20In%20this%20work%2C%20we%20characterize%0Asimplicity%20bias%20for%20general%20datasets%20in%20the%20context%20of%20two-layer%20neural%0Anetworks%20initialized%20with%20small%20weights%20and%20trained%20with%20gradient%20flow.%0ASpecifically%2C%20we%20prove%20that%20in%20the%20early%20training%20phases%2C%20network%20features%0Acluster%20around%20a%20few%20directions%20that%20do%20not%20depend%20on%20the%20size%20of%20the%20hidden%0Alayer.%20Furthermore%2C%20for%20datasets%20with%20an%20XOR-like%20pattern%2C%20we%20precisely%0Aidentify%20the%20learned%20features%20and%20demonstrate%20that%20simplicity%20bias%20intensifies%0Aduring%20later%20training%20stages.%20These%20results%20indicate%20that%20features%20learned%20in%0Athe%20middle%20stages%20of%20training%20may%20be%20more%20useful%20for%20OOD%20transfer.%20We%20support%0Athis%20hypothesis%20with%20experiments%20on%20image%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplicity%2520Bias%2520of%2520Two-Layer%2520Networks%2520beyond%2520Linearly%2520Separable%2520Data%26entry.906535625%3DNikita%2520Tsoy%2520and%2520Nikola%2520Konstantinov%26entry.1292438233%3D%2520%2520Simplicity%2520bias%252C%2520the%2520propensity%2520of%2520deep%2520models%2520to%2520over-rely%2520on%2520simple%250Afeatures%252C%2520has%2520been%2520identified%2520as%2520a%2520potential%2520reason%2520for%2520limited%250Aout-of-distribution%2520generalization%2520of%2520neural%2520networks%2520%2528Shah%2520et%2520al.%252C%25202020%2529.%250ADespite%2520the%2520important%2520implications%252C%2520this%2520phenomenon%2520has%2520been%2520theoretically%250Aconfirmed%2520and%2520characterized%2520only%2520under%2520strong%2520dataset%2520assumptions%252C%2520such%2520as%250Alinear%2520separability%2520%2528Lyu%2520et%2520al.%252C%25202021%2529.%2520In%2520this%2520work%252C%2520we%2520characterize%250Asimplicity%2520bias%2520for%2520general%2520datasets%2520in%2520the%2520context%2520of%2520two-layer%2520neural%250Anetworks%2520initialized%2520with%2520small%2520weights%2520and%2520trained%2520with%2520gradient%2520flow.%250ASpecifically%252C%2520we%2520prove%2520that%2520in%2520the%2520early%2520training%2520phases%252C%2520network%2520features%250Acluster%2520around%2520a%2520few%2520directions%2520that%2520do%2520not%2520depend%2520on%2520the%2520size%2520of%2520the%2520hidden%250Alayer.%2520Furthermore%252C%2520for%2520datasets%2520with%2520an%2520XOR-like%2520pattern%252C%2520we%2520precisely%250Aidentify%2520the%2520learned%2520features%2520and%2520demonstrate%2520that%2520simplicity%2520bias%2520intensifies%250Aduring%2520later%2520training%2520stages.%2520These%2520results%2520indicate%2520that%2520features%2520learned%2520in%250Athe%2520middle%2520stages%2520of%2520training%2520may%2520be%2520more%2520useful%2520for%2520OOD%2520transfer.%2520We%2520support%250Athis%2520hypothesis%2520with%2520experiments%2520on%2520image%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplicity%20Bias%20of%20Two-Layer%20Networks%20beyond%20Linearly%20Separable%20Data&entry.906535625=Nikita%20Tsoy%20and%20Nikola%20Konstantinov&entry.1292438233=%20%20Simplicity%20bias%2C%20the%20propensity%20of%20deep%20models%20to%20over-rely%20on%20simple%0Afeatures%2C%20has%20been%20identified%20as%20a%20potential%20reason%20for%20limited%0Aout-of-distribution%20generalization%20of%20neural%20networks%20%28Shah%20et%20al.%2C%202020%29.%0ADespite%20the%20important%20implications%2C%20this%20phenomenon%20has%20been%20theoretically%0Aconfirmed%20and%20characterized%20only%20under%20strong%20dataset%20assumptions%2C%20such%20as%0Alinear%20separability%20%28Lyu%20et%20al.%2C%202021%29.%20In%20this%20work%2C%20we%20characterize%0Asimplicity%20bias%20for%20general%20datasets%20in%20the%20context%20of%20two-layer%20neural%0Anetworks%20initialized%20with%20small%20weights%20and%20trained%20with%20gradient%20flow.%0ASpecifically%2C%20we%20prove%20that%20in%20the%20early%20training%20phases%2C%20network%20features%0Acluster%20around%20a%20few%20directions%20that%20do%20not%20depend%20on%20the%20size%20of%20the%20hidden%0Alayer.%20Furthermore%2C%20for%20datasets%20with%20an%20XOR-like%20pattern%2C%20we%20precisely%0Aidentify%20the%20learned%20features%20and%20demonstrate%20that%20simplicity%20bias%20intensifies%0Aduring%20later%20training%20stages.%20These%20results%20indicate%20that%20features%20learned%20in%0Athe%20middle%20stages%20of%20training%20may%20be%20more%20useful%20for%20OOD%20transfer.%20We%20support%0Athis%20hypothesis%20with%20experiments%20on%20image%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17299v2&entry.124074799=Read"},
{"title": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling:\n  Foundations, Theory, and Alternatives", "author": "Hao Sun and Yunyi Shen and Jean-Francois Ton", "abstract": "  The Bradley-Terry (BT) model is a common and successful practice in reward\nmodeling for Large Language Model (LLM) alignment. However, it remains unclear\nwhy this model -- originally developed for multi-player stochastic game\nmatching -- can be adopted to convert pairwise response comparisons to reward\nvalues and make predictions. Especially given the fact that only a limited\nnumber of prompt-response pairs are sparsely compared with others. In this\npaper, we first revisit the foundations of using BT models in reward modeling,\nand establish the convergence rate of BT reward models based on deep neural\nnetworks using embeddings, providing a theoretical foundation for their use.\nDespite theoretically sound, we argue that the BT model is not a necessary\nchoice from the perspective of downstream optimization. This is because a\nreward model only needs to preserve the correct ranking predictions through a\nmonotonic transformation of the true reward. We highlight the critical concept\nof order consistency in reward modeling and demonstrate that the BT model\npossesses this property. Consequently, we propose a simple and straightforward\nupper-bound algorithm, compatible with off-the-shelf binary classifiers, as an\nalternative order-consistent reward modeling objective. To offer practical\ninsights, we empirically evaluate the performance of these different reward\nmodeling approaches across more than 12,000 experimental setups, using $6$ base\nLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,\nquality, and pairing choices in preference annotations.\n", "link": "http://arxiv.org/abs/2411.04991v1", "date": "2024-11-07", "relevancy": 2.0212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Bradley-Terry%20Models%20in%20Preference-Based%20Reward%20Modeling%3A%0A%20%20Foundations%2C%20Theory%2C%20and%20Alternatives&body=Title%3A%20Rethinking%20Bradley-Terry%20Models%20in%20Preference-Based%20Reward%20Modeling%3A%0A%20%20Foundations%2C%20Theory%2C%20and%20Alternatives%0AAuthor%3A%20Hao%20Sun%20and%20Yunyi%20Shen%20and%20Jean-Francois%20Ton%0AAbstract%3A%20%20%20The%20Bradley-Terry%20%28BT%29%20model%20is%20a%20common%20and%20successful%20practice%20in%20reward%0Amodeling%20for%20Large%20Language%20Model%20%28LLM%29%20alignment.%20However%2C%20it%20remains%20unclear%0Awhy%20this%20model%20--%20originally%20developed%20for%20multi-player%20stochastic%20game%0Amatching%20--%20can%20be%20adopted%20to%20convert%20pairwise%20response%20comparisons%20to%20reward%0Avalues%20and%20make%20predictions.%20Especially%20given%20the%20fact%20that%20only%20a%20limited%0Anumber%20of%20prompt-response%20pairs%20are%20sparsely%20compared%20with%20others.%20In%20this%0Apaper%2C%20we%20first%20revisit%20the%20foundations%20of%20using%20BT%20models%20in%20reward%20modeling%2C%0Aand%20establish%20the%20convergence%20rate%20of%20BT%20reward%20models%20based%20on%20deep%20neural%0Anetworks%20using%20embeddings%2C%20providing%20a%20theoretical%20foundation%20for%20their%20use.%0ADespite%20theoretically%20sound%2C%20we%20argue%20that%20the%20BT%20model%20is%20not%20a%20necessary%0Achoice%20from%20the%20perspective%20of%20downstream%20optimization.%20This%20is%20because%20a%0Areward%20model%20only%20needs%20to%20preserve%20the%20correct%20ranking%20predictions%20through%20a%0Amonotonic%20transformation%20of%20the%20true%20reward.%20We%20highlight%20the%20critical%20concept%0Aof%20order%20consistency%20in%20reward%20modeling%20and%20demonstrate%20that%20the%20BT%20model%0Apossesses%20this%20property.%20Consequently%2C%20we%20propose%20a%20simple%20and%20straightforward%0Aupper-bound%20algorithm%2C%20compatible%20with%20off-the-shelf%20binary%20classifiers%2C%20as%20an%0Aalternative%20order-consistent%20reward%20modeling%20objective.%20To%20offer%20practical%0Ainsights%2C%20we%20empirically%20evaluate%20the%20performance%20of%20these%20different%20reward%0Amodeling%20approaches%20across%20more%20than%2012%2C000%20experimental%20setups%2C%20using%20%246%24%20base%0ALLMs%2C%20%242%24%20datasets%2C%20and%20diverse%20annotation%20designs%20that%20vary%20in%20quantity%2C%0Aquality%2C%20and%20pairing%20choices%20in%20preference%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Bradley-Terry%2520Models%2520in%2520Preference-Based%2520Reward%2520Modeling%253A%250A%2520%2520Foundations%252C%2520Theory%252C%2520and%2520Alternatives%26entry.906535625%3DHao%2520Sun%2520and%2520Yunyi%2520Shen%2520and%2520Jean-Francois%2520Ton%26entry.1292438233%3D%2520%2520The%2520Bradley-Terry%2520%2528BT%2529%2520model%2520is%2520a%2520common%2520and%2520successful%2520practice%2520in%2520reward%250Amodeling%2520for%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520alignment.%2520However%252C%2520it%2520remains%2520unclear%250Awhy%2520this%2520model%2520--%2520originally%2520developed%2520for%2520multi-player%2520stochastic%2520game%250Amatching%2520--%2520can%2520be%2520adopted%2520to%2520convert%2520pairwise%2520response%2520comparisons%2520to%2520reward%250Avalues%2520and%2520make%2520predictions.%2520Especially%2520given%2520the%2520fact%2520that%2520only%2520a%2520limited%250Anumber%2520of%2520prompt-response%2520pairs%2520are%2520sparsely%2520compared%2520with%2520others.%2520In%2520this%250Apaper%252C%2520we%2520first%2520revisit%2520the%2520foundations%2520of%2520using%2520BT%2520models%2520in%2520reward%2520modeling%252C%250Aand%2520establish%2520the%2520convergence%2520rate%2520of%2520BT%2520reward%2520models%2520based%2520on%2520deep%2520neural%250Anetworks%2520using%2520embeddings%252C%2520providing%2520a%2520theoretical%2520foundation%2520for%2520their%2520use.%250ADespite%2520theoretically%2520sound%252C%2520we%2520argue%2520that%2520the%2520BT%2520model%2520is%2520not%2520a%2520necessary%250Achoice%2520from%2520the%2520perspective%2520of%2520downstream%2520optimization.%2520This%2520is%2520because%2520a%250Areward%2520model%2520only%2520needs%2520to%2520preserve%2520the%2520correct%2520ranking%2520predictions%2520through%2520a%250Amonotonic%2520transformation%2520of%2520the%2520true%2520reward.%2520We%2520highlight%2520the%2520critical%2520concept%250Aof%2520order%2520consistency%2520in%2520reward%2520modeling%2520and%2520demonstrate%2520that%2520the%2520BT%2520model%250Apossesses%2520this%2520property.%2520Consequently%252C%2520we%2520propose%2520a%2520simple%2520and%2520straightforward%250Aupper-bound%2520algorithm%252C%2520compatible%2520with%2520off-the-shelf%2520binary%2520classifiers%252C%2520as%2520an%250Aalternative%2520order-consistent%2520reward%2520modeling%2520objective.%2520To%2520offer%2520practical%250Ainsights%252C%2520we%2520empirically%2520evaluate%2520the%2520performance%2520of%2520these%2520different%2520reward%250Amodeling%2520approaches%2520across%2520more%2520than%252012%252C000%2520experimental%2520setups%252C%2520using%2520%25246%2524%2520base%250ALLMs%252C%2520%25242%2524%2520datasets%252C%2520and%2520diverse%2520annotation%2520designs%2520that%2520vary%2520in%2520quantity%252C%250Aquality%252C%2520and%2520pairing%2520choices%2520in%2520preference%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Bradley-Terry%20Models%20in%20Preference-Based%20Reward%20Modeling%3A%0A%20%20Foundations%2C%20Theory%2C%20and%20Alternatives&entry.906535625=Hao%20Sun%20and%20Yunyi%20Shen%20and%20Jean-Francois%20Ton&entry.1292438233=%20%20The%20Bradley-Terry%20%28BT%29%20model%20is%20a%20common%20and%20successful%20practice%20in%20reward%0Amodeling%20for%20Large%20Language%20Model%20%28LLM%29%20alignment.%20However%2C%20it%20remains%20unclear%0Awhy%20this%20model%20--%20originally%20developed%20for%20multi-player%20stochastic%20game%0Amatching%20--%20can%20be%20adopted%20to%20convert%20pairwise%20response%20comparisons%20to%20reward%0Avalues%20and%20make%20predictions.%20Especially%20given%20the%20fact%20that%20only%20a%20limited%0Anumber%20of%20prompt-response%20pairs%20are%20sparsely%20compared%20with%20others.%20In%20this%0Apaper%2C%20we%20first%20revisit%20the%20foundations%20of%20using%20BT%20models%20in%20reward%20modeling%2C%0Aand%20establish%20the%20convergence%20rate%20of%20BT%20reward%20models%20based%20on%20deep%20neural%0Anetworks%20using%20embeddings%2C%20providing%20a%20theoretical%20foundation%20for%20their%20use.%0ADespite%20theoretically%20sound%2C%20we%20argue%20that%20the%20BT%20model%20is%20not%20a%20necessary%0Achoice%20from%20the%20perspective%20of%20downstream%20optimization.%20This%20is%20because%20a%0Areward%20model%20only%20needs%20to%20preserve%20the%20correct%20ranking%20predictions%20through%20a%0Amonotonic%20transformation%20of%20the%20true%20reward.%20We%20highlight%20the%20critical%20concept%0Aof%20order%20consistency%20in%20reward%20modeling%20and%20demonstrate%20that%20the%20BT%20model%0Apossesses%20this%20property.%20Consequently%2C%20we%20propose%20a%20simple%20and%20straightforward%0Aupper-bound%20algorithm%2C%20compatible%20with%20off-the-shelf%20binary%20classifiers%2C%20as%20an%0Aalternative%20order-consistent%20reward%20modeling%20objective.%20To%20offer%20practical%0Ainsights%2C%20we%20empirically%20evaluate%20the%20performance%20of%20these%20different%20reward%0Amodeling%20approaches%20across%20more%20than%2012%2C000%20experimental%20setups%2C%20using%20%246%24%20base%0ALLMs%2C%20%242%24%20datasets%2C%20and%20diverse%20annotation%20designs%20that%20vary%20in%20quantity%2C%0Aquality%2C%20and%20pairing%20choices%20in%20preference%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04991v1&entry.124074799=Read"},
{"title": "ESC-MISR: Enhancing Spatial Correlations for Multi-Image\n  Super-Resolution in Remote Sensing", "author": "Zhihui Zhang and Jinhui Pang and Jianan Li and Xiaoshuai Hao", "abstract": "  Multi-Image Super-Resolution (MISR) is a crucial yet challenging research\ntask in the remote sensing community. In this paper, we address the challenging\ntask of Multi-Image Super-Resolution in Remote Sensing (MISR-RS), aiming to\ngenerate a High-Resolution (HR) image from multiple Low-Resolution (LR) images\nobtained by satellites. Recently, the weak temporal correlations among LR\nimages have attracted increasing attention in the MISR-RS task. However,\nexisting MISR methods treat the LR images as sequences with strong temporal\ncorrelations, overlooking spatial correlations and imposing temporal\ndependencies. To address this problem, we propose a novel end-to-end framework\nnamed Enhancing Spatial Correlations in MISR (ESC-MISR), which fully exploits\nthe spatial-temporal relations of multiple images for HR image reconstruction.\nSpecifically, we first introduce a novel fusion module named Multi-Image\nSpatial Transformer (MIST), which emphasizes parts with clearer global spatial\nfeatures and enhances the spatial correlations between LR images. Besides, we\nperform a random shuffle strategy for the sequential inputs of LR images to\nattenuate temporal dependencies and capture weak temporal correlations in the\ntraining stage. Compared with the state-of-the-art methods, our ESC-MISR\nachieves 0.70dB and 0.76dB cPSNR improvements on the two bands of the PROBA-V\ndataset respectively, demonstrating the superiority of our method.\n", "link": "http://arxiv.org/abs/2411.04706v1", "date": "2024-11-07", "relevancy": 2.0168, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.51}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5049}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESC-MISR%3A%20Enhancing%20Spatial%20Correlations%20for%20Multi-Image%0A%20%20Super-Resolution%20in%20Remote%20Sensing&body=Title%3A%20ESC-MISR%3A%20Enhancing%20Spatial%20Correlations%20for%20Multi-Image%0A%20%20Super-Resolution%20in%20Remote%20Sensing%0AAuthor%3A%20Zhihui%20Zhang%20and%20Jinhui%20Pang%20and%20Jianan%20Li%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Multi-Image%20Super-Resolution%20%28MISR%29%20is%20a%20crucial%20yet%20challenging%20research%0Atask%20in%20the%20remote%20sensing%20community.%20In%20this%20paper%2C%20we%20address%20the%20challenging%0Atask%20of%20Multi-Image%20Super-Resolution%20in%20Remote%20Sensing%20%28MISR-RS%29%2C%20aiming%20to%0Agenerate%20a%20High-Resolution%20%28HR%29%20image%20from%20multiple%20Low-Resolution%20%28LR%29%20images%0Aobtained%20by%20satellites.%20Recently%2C%20the%20weak%20temporal%20correlations%20among%20LR%0Aimages%20have%20attracted%20increasing%20attention%20in%20the%20MISR-RS%20task.%20However%2C%0Aexisting%20MISR%20methods%20treat%20the%20LR%20images%20as%20sequences%20with%20strong%20temporal%0Acorrelations%2C%20overlooking%20spatial%20correlations%20and%20imposing%20temporal%0Adependencies.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20end-to-end%20framework%0Anamed%20Enhancing%20Spatial%20Correlations%20in%20MISR%20%28ESC-MISR%29%2C%20which%20fully%20exploits%0Athe%20spatial-temporal%20relations%20of%20multiple%20images%20for%20HR%20image%20reconstruction.%0ASpecifically%2C%20we%20first%20introduce%20a%20novel%20fusion%20module%20named%20Multi-Image%0ASpatial%20Transformer%20%28MIST%29%2C%20which%20emphasizes%20parts%20with%20clearer%20global%20spatial%0Afeatures%20and%20enhances%20the%20spatial%20correlations%20between%20LR%20images.%20Besides%2C%20we%0Aperform%20a%20random%20shuffle%20strategy%20for%20the%20sequential%20inputs%20of%20LR%20images%20to%0Aattenuate%20temporal%20dependencies%20and%20capture%20weak%20temporal%20correlations%20in%20the%0Atraining%20stage.%20Compared%20with%20the%20state-of-the-art%20methods%2C%20our%20ESC-MISR%0Aachieves%200.70dB%20and%200.76dB%20cPSNR%20improvements%20on%20the%20two%20bands%20of%20the%20PROBA-V%0Adataset%20respectively%2C%20demonstrating%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESC-MISR%253A%2520Enhancing%2520Spatial%2520Correlations%2520for%2520Multi-Image%250A%2520%2520Super-Resolution%2520in%2520Remote%2520Sensing%26entry.906535625%3DZhihui%2520Zhang%2520and%2520Jinhui%2520Pang%2520and%2520Jianan%2520Li%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Multi-Image%2520Super-Resolution%2520%2528MISR%2529%2520is%2520a%2520crucial%2520yet%2520challenging%2520research%250Atask%2520in%2520the%2520remote%2520sensing%2520community.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenging%250Atask%2520of%2520Multi-Image%2520Super-Resolution%2520in%2520Remote%2520Sensing%2520%2528MISR-RS%2529%252C%2520aiming%2520to%250Agenerate%2520a%2520High-Resolution%2520%2528HR%2529%2520image%2520from%2520multiple%2520Low-Resolution%2520%2528LR%2529%2520images%250Aobtained%2520by%2520satellites.%2520Recently%252C%2520the%2520weak%2520temporal%2520correlations%2520among%2520LR%250Aimages%2520have%2520attracted%2520increasing%2520attention%2520in%2520the%2520MISR-RS%2520task.%2520However%252C%250Aexisting%2520MISR%2520methods%2520treat%2520the%2520LR%2520images%2520as%2520sequences%2520with%2520strong%2520temporal%250Acorrelations%252C%2520overlooking%2520spatial%2520correlations%2520and%2520imposing%2520temporal%250Adependencies.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520framework%250Anamed%2520Enhancing%2520Spatial%2520Correlations%2520in%2520MISR%2520%2528ESC-MISR%2529%252C%2520which%2520fully%2520exploits%250Athe%2520spatial-temporal%2520relations%2520of%2520multiple%2520images%2520for%2520HR%2520image%2520reconstruction.%250ASpecifically%252C%2520we%2520first%2520introduce%2520a%2520novel%2520fusion%2520module%2520named%2520Multi-Image%250ASpatial%2520Transformer%2520%2528MIST%2529%252C%2520which%2520emphasizes%2520parts%2520with%2520clearer%2520global%2520spatial%250Afeatures%2520and%2520enhances%2520the%2520spatial%2520correlations%2520between%2520LR%2520images.%2520Besides%252C%2520we%250Aperform%2520a%2520random%2520shuffle%2520strategy%2520for%2520the%2520sequential%2520inputs%2520of%2520LR%2520images%2520to%250Aattenuate%2520temporal%2520dependencies%2520and%2520capture%2520weak%2520temporal%2520correlations%2520in%2520the%250Atraining%2520stage.%2520Compared%2520with%2520the%2520state-of-the-art%2520methods%252C%2520our%2520ESC-MISR%250Aachieves%25200.70dB%2520and%25200.76dB%2520cPSNR%2520improvements%2520on%2520the%2520two%2520bands%2520of%2520the%2520PROBA-V%250Adataset%2520respectively%252C%2520demonstrating%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESC-MISR%3A%20Enhancing%20Spatial%20Correlations%20for%20Multi-Image%0A%20%20Super-Resolution%20in%20Remote%20Sensing&entry.906535625=Zhihui%20Zhang%20and%20Jinhui%20Pang%20and%20Jianan%20Li%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Multi-Image%20Super-Resolution%20%28MISR%29%20is%20a%20crucial%20yet%20challenging%20research%0Atask%20in%20the%20remote%20sensing%20community.%20In%20this%20paper%2C%20we%20address%20the%20challenging%0Atask%20of%20Multi-Image%20Super-Resolution%20in%20Remote%20Sensing%20%28MISR-RS%29%2C%20aiming%20to%0Agenerate%20a%20High-Resolution%20%28HR%29%20image%20from%20multiple%20Low-Resolution%20%28LR%29%20images%0Aobtained%20by%20satellites.%20Recently%2C%20the%20weak%20temporal%20correlations%20among%20LR%0Aimages%20have%20attracted%20increasing%20attention%20in%20the%20MISR-RS%20task.%20However%2C%0Aexisting%20MISR%20methods%20treat%20the%20LR%20images%20as%20sequences%20with%20strong%20temporal%0Acorrelations%2C%20overlooking%20spatial%20correlations%20and%20imposing%20temporal%0Adependencies.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20end-to-end%20framework%0Anamed%20Enhancing%20Spatial%20Correlations%20in%20MISR%20%28ESC-MISR%29%2C%20which%20fully%20exploits%0Athe%20spatial-temporal%20relations%20of%20multiple%20images%20for%20HR%20image%20reconstruction.%0ASpecifically%2C%20we%20first%20introduce%20a%20novel%20fusion%20module%20named%20Multi-Image%0ASpatial%20Transformer%20%28MIST%29%2C%20which%20emphasizes%20parts%20with%20clearer%20global%20spatial%0Afeatures%20and%20enhances%20the%20spatial%20correlations%20between%20LR%20images.%20Besides%2C%20we%0Aperform%20a%20random%20shuffle%20strategy%20for%20the%20sequential%20inputs%20of%20LR%20images%20to%0Aattenuate%20temporal%20dependencies%20and%20capture%20weak%20temporal%20correlations%20in%20the%0Atraining%20stage.%20Compared%20with%20the%20state-of-the-art%20methods%2C%20our%20ESC-MISR%0Aachieves%200.70dB%20and%200.76dB%20cPSNR%20improvements%20on%20the%20two%20bands%20of%20the%20PROBA-V%0Adataset%20respectively%2C%20demonstrating%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04706v1&entry.124074799=Read"},
{"title": "Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent\n  Reinforcement Learning", "author": "Zhiyu Shao and Qiong Wu and Pingyi Fan and Kezhi Wang and Qiang Fan and Wen Chen and Khaled B. Letaief", "abstract": "  This paper presents a semantic-aware multi-modal resource allocation (SAMRA)\nfor multi-task using multi-agent reinforcement learning (MARL), termed\nSAMRAMARL, utilizing in platoon systems where cellular vehicle-to-everything\n(C-V2X) communication is employed. The proposed approach leverages the semantic\ninformation to optimize the allocation of communication resources. By\nintegrating a distributed multi-agent reinforcement learning (MARL) algorithm,\nSAMRAMARL enables autonomous decision-making for each vehicle, channel\nassignment optimization, power allocation, and semantic symbol length based on\nthe contextual importance of the transmitted information. This\nsemantic-awareness ensures that both vehicle-to-vehicle (V2V) and\nvehicle-to-infrastructure (V2I) communications prioritize data that is critical\nfor maintaining safe and efficient platoon operations. The framework also\nintroduces a tailored quality of experience (QoE) metric for semantic\ncommunication, aiming to maximize QoE in V2V links while improving the success\nrate of semantic information transmission (SRS). Extensive simulations has\ndemonstrated that SAMRAMARL outperforms existing methods, achieving significant\ngains in QoE and communication efficiency in C-V2X platooning scenarios.\n", "link": "http://arxiv.org/abs/2411.04672v1", "date": "2024-11-07", "relevancy": 2.0144, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5602}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4939}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Aware%20Resource%20Management%20for%20C-V2X%20Platooning%20via%20Multi-Agent%0A%20%20Reinforcement%20Learning&body=Title%3A%20Semantic-Aware%20Resource%20Management%20for%20C-V2X%20Platooning%20via%20Multi-Agent%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhiyu%20Shao%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang%20and%20Qiang%20Fan%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20semantic-aware%20multi-modal%20resource%20allocation%20%28SAMRA%29%0Afor%20multi-task%20using%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%20termed%0ASAMRAMARL%2C%20utilizing%20in%20platoon%20systems%20where%20cellular%20vehicle-to-everything%0A%28C-V2X%29%20communication%20is%20employed.%20The%20proposed%20approach%20leverages%20the%20semantic%0Ainformation%20to%20optimize%20the%20allocation%20of%20communication%20resources.%20By%0Aintegrating%20a%20distributed%20multi-agent%20reinforcement%20learning%20%28MARL%29%20algorithm%2C%0ASAMRAMARL%20enables%20autonomous%20decision-making%20for%20each%20vehicle%2C%20channel%0Aassignment%20optimization%2C%20power%20allocation%2C%20and%20semantic%20symbol%20length%20based%20on%0Athe%20contextual%20importance%20of%20the%20transmitted%20information.%20This%0Asemantic-awareness%20ensures%20that%20both%20vehicle-to-vehicle%20%28V2V%29%20and%0Avehicle-to-infrastructure%20%28V2I%29%20communications%20prioritize%20data%20that%20is%20critical%0Afor%20maintaining%20safe%20and%20efficient%20platoon%20operations.%20The%20framework%20also%0Aintroduces%20a%20tailored%20quality%20of%20experience%20%28QoE%29%20metric%20for%20semantic%0Acommunication%2C%20aiming%20to%20maximize%20QoE%20in%20V2V%20links%20while%20improving%20the%20success%0Arate%20of%20semantic%20information%20transmission%20%28SRS%29.%20Extensive%20simulations%20has%0Ademonstrated%20that%20SAMRAMARL%20outperforms%20existing%20methods%2C%20achieving%20significant%0Agains%20in%20QoE%20and%20communication%20efficiency%20in%20C-V2X%20platooning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Aware%2520Resource%2520Management%2520for%2520C-V2X%2520Platooning%2520via%2520Multi-Agent%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhiyu%2520Shao%2520and%2520Qiong%2520Wu%2520and%2520Pingyi%2520Fan%2520and%2520Kezhi%2520Wang%2520and%2520Qiang%2520Fan%2520and%2520Wen%2520Chen%2520and%2520Khaled%2520B.%2520Letaief%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520semantic-aware%2520multi-modal%2520resource%2520allocation%2520%2528SAMRA%2529%250Afor%2520multi-task%2520using%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%252C%2520termed%250ASAMRAMARL%252C%2520utilizing%2520in%2520platoon%2520systems%2520where%2520cellular%2520vehicle-to-everything%250A%2528C-V2X%2529%2520communication%2520is%2520employed.%2520The%2520proposed%2520approach%2520leverages%2520the%2520semantic%250Ainformation%2520to%2520optimize%2520the%2520allocation%2520of%2520communication%2520resources.%2520By%250Aintegrating%2520a%2520distributed%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520algorithm%252C%250ASAMRAMARL%2520enables%2520autonomous%2520decision-making%2520for%2520each%2520vehicle%252C%2520channel%250Aassignment%2520optimization%252C%2520power%2520allocation%252C%2520and%2520semantic%2520symbol%2520length%2520based%2520on%250Athe%2520contextual%2520importance%2520of%2520the%2520transmitted%2520information.%2520This%250Asemantic-awareness%2520ensures%2520that%2520both%2520vehicle-to-vehicle%2520%2528V2V%2529%2520and%250Avehicle-to-infrastructure%2520%2528V2I%2529%2520communications%2520prioritize%2520data%2520that%2520is%2520critical%250Afor%2520maintaining%2520safe%2520and%2520efficient%2520platoon%2520operations.%2520The%2520framework%2520also%250Aintroduces%2520a%2520tailored%2520quality%2520of%2520experience%2520%2528QoE%2529%2520metric%2520for%2520semantic%250Acommunication%252C%2520aiming%2520to%2520maximize%2520QoE%2520in%2520V2V%2520links%2520while%2520improving%2520the%2520success%250Arate%2520of%2520semantic%2520information%2520transmission%2520%2528SRS%2529.%2520Extensive%2520simulations%2520has%250Ademonstrated%2520that%2520SAMRAMARL%2520outperforms%2520existing%2520methods%252C%2520achieving%2520significant%250Agains%2520in%2520QoE%2520and%2520communication%2520efficiency%2520in%2520C-V2X%2520platooning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Aware%20Resource%20Management%20for%20C-V2X%20Platooning%20via%20Multi-Agent%0A%20%20Reinforcement%20Learning&entry.906535625=Zhiyu%20Shao%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang%20and%20Qiang%20Fan%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief&entry.1292438233=%20%20This%20paper%20presents%20a%20semantic-aware%20multi-modal%20resource%20allocation%20%28SAMRA%29%0Afor%20multi-task%20using%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%20termed%0ASAMRAMARL%2C%20utilizing%20in%20platoon%20systems%20where%20cellular%20vehicle-to-everything%0A%28C-V2X%29%20communication%20is%20employed.%20The%20proposed%20approach%20leverages%20the%20semantic%0Ainformation%20to%20optimize%20the%20allocation%20of%20communication%20resources.%20By%0Aintegrating%20a%20distributed%20multi-agent%20reinforcement%20learning%20%28MARL%29%20algorithm%2C%0ASAMRAMARL%20enables%20autonomous%20decision-making%20for%20each%20vehicle%2C%20channel%0Aassignment%20optimization%2C%20power%20allocation%2C%20and%20semantic%20symbol%20length%20based%20on%0Athe%20contextual%20importance%20of%20the%20transmitted%20information.%20This%0Asemantic-awareness%20ensures%20that%20both%20vehicle-to-vehicle%20%28V2V%29%20and%0Avehicle-to-infrastructure%20%28V2I%29%20communications%20prioritize%20data%20that%20is%20critical%0Afor%20maintaining%20safe%20and%20efficient%20platoon%20operations.%20The%20framework%20also%0Aintroduces%20a%20tailored%20quality%20of%20experience%20%28QoE%29%20metric%20for%20semantic%0Acommunication%2C%20aiming%20to%20maximize%20QoE%20in%20V2V%20links%20while%20improving%20the%20success%0Arate%20of%20semantic%20information%20transmission%20%28SRS%29.%20Extensive%20simulations%20has%0Ademonstrated%20that%20SAMRAMARL%20outperforms%20existing%20methods%2C%20achieving%20significant%0Agains%20in%20QoE%20and%20communication%20efficiency%20in%20C-V2X%20platooning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04672v1&entry.124074799=Read"},
{"title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile\n  Manipulation", "author": "Peiqi Liu and Zhanqiu Guo and Mohit Warke and Soumith Chintala and Chris Paxton and Nur Muhammad Mahi Shafiullah and Lerrel Pinto", "abstract": "  Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/\n", "link": "http://arxiv.org/abs/2411.04999v1", "date": "2024-11-07", "relevancy": 1.7826, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaMem%3A%20Online%20Dynamic%20Spatio-Semantic%20Memory%20for%20Open%20World%20Mobile%0A%20%20Manipulation&body=Title%3A%20DynaMem%3A%20Online%20Dynamic%20Spatio-Semantic%20Memory%20for%20Open%20World%20Mobile%0A%20%20Manipulation%0AAuthor%3A%20Peiqi%20Liu%20and%20Zhanqiu%20Guo%20and%20Mohit%20Warke%20and%20Soumith%20Chintala%20and%20Chris%20Paxton%20and%20Nur%20Muhammad%20Mahi%20Shafiullah%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20Significant%20progress%20has%20been%20made%20in%20open-vocabulary%20mobile%20manipulation%2C%0Awhere%20the%20goal%20is%20for%20a%20robot%20to%20perform%20tasks%20in%20any%20environment%20given%20a%0Anatural%20language%20description.%20However%2C%20most%20current%20systems%20assume%20a%20static%0Aenvironment%2C%20which%20limits%20the%20system%27s%20applicability%20in%20real-world%20scenarios%0Awhere%20environments%20frequently%20change%20due%20to%20human%20intervention%20or%20the%20robot%27s%0Aown%20actions.%20In%20this%20work%2C%20we%20present%20DynaMem%2C%20a%20new%20approach%20to%20open-world%0Amobile%20manipulation%20that%20uses%20a%20dynamic%20spatio-semantic%20memory%20to%20represent%20a%0Arobot%27s%20environment.%20DynaMem%20constructs%20a%203D%20data%20structure%20to%20maintain%20a%0Adynamic%20memory%20of%20point%20clouds%2C%20and%20answers%20open-vocabulary%20object%20localization%0Aqueries%20using%20multimodal%20LLMs%20or%20open-vocabulary%20features%20generated%20by%0Astate-of-the-art%20vision-language%20models.%20Powered%20by%20DynaMem%2C%20our%20robots%20can%0Aexplore%20novel%20environments%2C%20search%20for%20objects%20not%20found%20in%20memory%2C%20and%0Acontinuously%20update%20the%20memory%20as%20objects%20move%2C%20appear%2C%20or%20disappear%20in%20the%0Ascene.%20We%20run%20extensive%20experiments%20on%20the%20Stretch%20SE3%20robots%20in%20three%20real%20and%0Anine%20offline%20scenes%2C%20and%20achieve%20an%20average%20pick-and-drop%20success%20rate%20of%2070%25%0Aon%20non-stationary%20objects%2C%20which%20is%20more%20than%20a%202x%20improvement%20over%0Astate-of-the-art%20static%20systems.%20Our%20code%20as%20well%20as%20our%20experiment%20and%0Adeployment%20videos%20are%20open%20sourced%20and%20can%20be%20found%20on%20our%20project%20website%3A%0Ahttps%3A//dynamem.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaMem%253A%2520Online%2520Dynamic%2520Spatio-Semantic%2520Memory%2520for%2520Open%2520World%2520Mobile%250A%2520%2520Manipulation%26entry.906535625%3DPeiqi%2520Liu%2520and%2520Zhanqiu%2520Guo%2520and%2520Mohit%2520Warke%2520and%2520Soumith%2520Chintala%2520and%2520Chris%2520Paxton%2520and%2520Nur%2520Muhammad%2520Mahi%2520Shafiullah%2520and%2520Lerrel%2520Pinto%26entry.1292438233%3D%2520%2520Significant%2520progress%2520has%2520been%2520made%2520in%2520open-vocabulary%2520mobile%2520manipulation%252C%250Awhere%2520the%2520goal%2520is%2520for%2520a%2520robot%2520to%2520perform%2520tasks%2520in%2520any%2520environment%2520given%2520a%250Anatural%2520language%2520description.%2520However%252C%2520most%2520current%2520systems%2520assume%2520a%2520static%250Aenvironment%252C%2520which%2520limits%2520the%2520system%2527s%2520applicability%2520in%2520real-world%2520scenarios%250Awhere%2520environments%2520frequently%2520change%2520due%2520to%2520human%2520intervention%2520or%2520the%2520robot%2527s%250Aown%2520actions.%2520In%2520this%2520work%252C%2520we%2520present%2520DynaMem%252C%2520a%2520new%2520approach%2520to%2520open-world%250Amobile%2520manipulation%2520that%2520uses%2520a%2520dynamic%2520spatio-semantic%2520memory%2520to%2520represent%2520a%250Arobot%2527s%2520environment.%2520DynaMem%2520constructs%2520a%25203D%2520data%2520structure%2520to%2520maintain%2520a%250Adynamic%2520memory%2520of%2520point%2520clouds%252C%2520and%2520answers%2520open-vocabulary%2520object%2520localization%250Aqueries%2520using%2520multimodal%2520LLMs%2520or%2520open-vocabulary%2520features%2520generated%2520by%250Astate-of-the-art%2520vision-language%2520models.%2520Powered%2520by%2520DynaMem%252C%2520our%2520robots%2520can%250Aexplore%2520novel%2520environments%252C%2520search%2520for%2520objects%2520not%2520found%2520in%2520memory%252C%2520and%250Acontinuously%2520update%2520the%2520memory%2520as%2520objects%2520move%252C%2520appear%252C%2520or%2520disappear%2520in%2520the%250Ascene.%2520We%2520run%2520extensive%2520experiments%2520on%2520the%2520Stretch%2520SE3%2520robots%2520in%2520three%2520real%2520and%250Anine%2520offline%2520scenes%252C%2520and%2520achieve%2520an%2520average%2520pick-and-drop%2520success%2520rate%2520of%252070%2525%250Aon%2520non-stationary%2520objects%252C%2520which%2520is%2520more%2520than%2520a%25202x%2520improvement%2520over%250Astate-of-the-art%2520static%2520systems.%2520Our%2520code%2520as%2520well%2520as%2520our%2520experiment%2520and%250Adeployment%2520videos%2520are%2520open%2520sourced%2520and%2520can%2520be%2520found%2520on%2520our%2520project%2520website%253A%250Ahttps%253A//dynamem.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMem%3A%20Online%20Dynamic%20Spatio-Semantic%20Memory%20for%20Open%20World%20Mobile%0A%20%20Manipulation&entry.906535625=Peiqi%20Liu%20and%20Zhanqiu%20Guo%20and%20Mohit%20Warke%20and%20Soumith%20Chintala%20and%20Chris%20Paxton%20and%20Nur%20Muhammad%20Mahi%20Shafiullah%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Significant%20progress%20has%20been%20made%20in%20open-vocabulary%20mobile%20manipulation%2C%0Awhere%20the%20goal%20is%20for%20a%20robot%20to%20perform%20tasks%20in%20any%20environment%20given%20a%0Anatural%20language%20description.%20However%2C%20most%20current%20systems%20assume%20a%20static%0Aenvironment%2C%20which%20limits%20the%20system%27s%20applicability%20in%20real-world%20scenarios%0Awhere%20environments%20frequently%20change%20due%20to%20human%20intervention%20or%20the%20robot%27s%0Aown%20actions.%20In%20this%20work%2C%20we%20present%20DynaMem%2C%20a%20new%20approach%20to%20open-world%0Amobile%20manipulation%20that%20uses%20a%20dynamic%20spatio-semantic%20memory%20to%20represent%20a%0Arobot%27s%20environment.%20DynaMem%20constructs%20a%203D%20data%20structure%20to%20maintain%20a%0Adynamic%20memory%20of%20point%20clouds%2C%20and%20answers%20open-vocabulary%20object%20localization%0Aqueries%20using%20multimodal%20LLMs%20or%20open-vocabulary%20features%20generated%20by%0Astate-of-the-art%20vision-language%20models.%20Powered%20by%20DynaMem%2C%20our%20robots%20can%0Aexplore%20novel%20environments%2C%20search%20for%20objects%20not%20found%20in%20memory%2C%20and%0Acontinuously%20update%20the%20memory%20as%20objects%20move%2C%20appear%2C%20or%20disappear%20in%20the%0Ascene.%20We%20run%20extensive%20experiments%20on%20the%20Stretch%20SE3%20robots%20in%20three%20real%20and%0Anine%20offline%20scenes%2C%20and%20achieve%20an%20average%20pick-and-drop%20success%20rate%20of%2070%25%0Aon%20non-stationary%20objects%2C%20which%20is%20more%20than%20a%202x%20improvement%20over%0Astate-of-the-art%20static%20systems.%20Our%20code%20as%20well%20as%20our%20experiment%20and%0Adeployment%20videos%20are%20open%20sourced%20and%20can%20be%20found%20on%20our%20project%20website%3A%0Ahttps%3A//dynamem.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04999v1&entry.124074799=Read"},
{"title": "Spatial Transformers for Radio Map Estimation", "author": "Pham Q. Viet and Daniel Romero", "abstract": "  Radio map estimation (RME) involves spatial interpolation of radio\nmeasurements to predict metrics such as the received signal strength at\nlocations where no measurements were collected. The most popular estimators\nnowadays project the measurement locations to a regular grid and complete the\nresulting measurement tensor with a convolutional deep neural network.\nUnfortunately, these approaches suffer from poor spatial resolution and require\na great number of parameters. The first contribution of this paper addresses\nthese limitations by means of an attention-based estimator named Spatial\nTransfOrmer for Radio Map estimation (STORM). This scheme not only outperforms\nthe existing estimators, but also exhibits lower computational complexity,\ntranslation equivariance, rotation equivariance, and full spatial resolution.\nThe second contribution is an extended transformer architecture that allows\nSTORM to perform active sensing, by which the next measurement location is\nselected based on the previous measurements. This is particularly useful for\nminimization of drive tests (MDT) in cellular networks, where operators request\nuser equipment to collect measurements. Finally, STORM is extensively validated\nby experiments with one ray-tracing and two real-measurement datasets.\n", "link": "http://arxiv.org/abs/2411.01211v2", "date": "2024-11-07", "relevancy": 1.9401, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4842}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Transformers%20for%20Radio%20Map%20Estimation&body=Title%3A%20Spatial%20Transformers%20for%20Radio%20Map%20Estimation%0AAuthor%3A%20Pham%20Q.%20Viet%20and%20Daniel%20Romero%0AAbstract%3A%20%20%20Radio%20map%20estimation%20%28RME%29%20involves%20spatial%20interpolation%20of%20radio%0Ameasurements%20to%20predict%20metrics%20such%20as%20the%20received%20signal%20strength%20at%0Alocations%20where%20no%20measurements%20were%20collected.%20The%20most%20popular%20estimators%0Anowadays%20project%20the%20measurement%20locations%20to%20a%20regular%20grid%20and%20complete%20the%0Aresulting%20measurement%20tensor%20with%20a%20convolutional%20deep%20neural%20network.%0AUnfortunately%2C%20these%20approaches%20suffer%20from%20poor%20spatial%20resolution%20and%20require%0Aa%20great%20number%20of%20parameters.%20The%20first%20contribution%20of%20this%20paper%20addresses%0Athese%20limitations%20by%20means%20of%20an%20attention-based%20estimator%20named%20Spatial%0ATransfOrmer%20for%20Radio%20Map%20estimation%20%28STORM%29.%20This%20scheme%20not%20only%20outperforms%0Athe%20existing%20estimators%2C%20but%20also%20exhibits%20lower%20computational%20complexity%2C%0Atranslation%20equivariance%2C%20rotation%20equivariance%2C%20and%20full%20spatial%20resolution.%0AThe%20second%20contribution%20is%20an%20extended%20transformer%20architecture%20that%20allows%0ASTORM%20to%20perform%20active%20sensing%2C%20by%20which%20the%20next%20measurement%20location%20is%0Aselected%20based%20on%20the%20previous%20measurements.%20This%20is%20particularly%20useful%20for%0Aminimization%20of%20drive%20tests%20%28MDT%29%20in%20cellular%20networks%2C%20where%20operators%20request%0Auser%20equipment%20to%20collect%20measurements.%20Finally%2C%20STORM%20is%20extensively%20validated%0Aby%20experiments%20with%20one%20ray-tracing%20and%20two%20real-measurement%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Transformers%2520for%2520Radio%2520Map%2520Estimation%26entry.906535625%3DPham%2520Q.%2520Viet%2520and%2520Daniel%2520Romero%26entry.1292438233%3D%2520%2520Radio%2520map%2520estimation%2520%2528RME%2529%2520involves%2520spatial%2520interpolation%2520of%2520radio%250Ameasurements%2520to%2520predict%2520metrics%2520such%2520as%2520the%2520received%2520signal%2520strength%2520at%250Alocations%2520where%2520no%2520measurements%2520were%2520collected.%2520The%2520most%2520popular%2520estimators%250Anowadays%2520project%2520the%2520measurement%2520locations%2520to%2520a%2520regular%2520grid%2520and%2520complete%2520the%250Aresulting%2520measurement%2520tensor%2520with%2520a%2520convolutional%2520deep%2520neural%2520network.%250AUnfortunately%252C%2520these%2520approaches%2520suffer%2520from%2520poor%2520spatial%2520resolution%2520and%2520require%250Aa%2520great%2520number%2520of%2520parameters.%2520The%2520first%2520contribution%2520of%2520this%2520paper%2520addresses%250Athese%2520limitations%2520by%2520means%2520of%2520an%2520attention-based%2520estimator%2520named%2520Spatial%250ATransfOrmer%2520for%2520Radio%2520Map%2520estimation%2520%2528STORM%2529.%2520This%2520scheme%2520not%2520only%2520outperforms%250Athe%2520existing%2520estimators%252C%2520but%2520also%2520exhibits%2520lower%2520computational%2520complexity%252C%250Atranslation%2520equivariance%252C%2520rotation%2520equivariance%252C%2520and%2520full%2520spatial%2520resolution.%250AThe%2520second%2520contribution%2520is%2520an%2520extended%2520transformer%2520architecture%2520that%2520allows%250ASTORM%2520to%2520perform%2520active%2520sensing%252C%2520by%2520which%2520the%2520next%2520measurement%2520location%2520is%250Aselected%2520based%2520on%2520the%2520previous%2520measurements.%2520This%2520is%2520particularly%2520useful%2520for%250Aminimization%2520of%2520drive%2520tests%2520%2528MDT%2529%2520in%2520cellular%2520networks%252C%2520where%2520operators%2520request%250Auser%2520equipment%2520to%2520collect%2520measurements.%2520Finally%252C%2520STORM%2520is%2520extensively%2520validated%250Aby%2520experiments%2520with%2520one%2520ray-tracing%2520and%2520two%2520real-measurement%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Transformers%20for%20Radio%20Map%20Estimation&entry.906535625=Pham%20Q.%20Viet%20and%20Daniel%20Romero&entry.1292438233=%20%20Radio%20map%20estimation%20%28RME%29%20involves%20spatial%20interpolation%20of%20radio%0Ameasurements%20to%20predict%20metrics%20such%20as%20the%20received%20signal%20strength%20at%0Alocations%20where%20no%20measurements%20were%20collected.%20The%20most%20popular%20estimators%0Anowadays%20project%20the%20measurement%20locations%20to%20a%20regular%20grid%20and%20complete%20the%0Aresulting%20measurement%20tensor%20with%20a%20convolutional%20deep%20neural%20network.%0AUnfortunately%2C%20these%20approaches%20suffer%20from%20poor%20spatial%20resolution%20and%20require%0Aa%20great%20number%20of%20parameters.%20The%20first%20contribution%20of%20this%20paper%20addresses%0Athese%20limitations%20by%20means%20of%20an%20attention-based%20estimator%20named%20Spatial%0ATransfOrmer%20for%20Radio%20Map%20estimation%20%28STORM%29.%20This%20scheme%20not%20only%20outperforms%0Athe%20existing%20estimators%2C%20but%20also%20exhibits%20lower%20computational%20complexity%2C%0Atranslation%20equivariance%2C%20rotation%20equivariance%2C%20and%20full%20spatial%20resolution.%0AThe%20second%20contribution%20is%20an%20extended%20transformer%20architecture%20that%20allows%0ASTORM%20to%20perform%20active%20sensing%2C%20by%20which%20the%20next%20measurement%20location%20is%0Aselected%20based%20on%20the%20previous%20measurements.%20This%20is%20particularly%20useful%20for%0Aminimization%20of%20drive%20tests%20%28MDT%29%20in%20cellular%20networks%2C%20where%20operators%20request%0Auser%20equipment%20to%20collect%20measurements.%20Finally%2C%20STORM%20is%20extensively%20validated%0Aby%20experiments%20with%20one%20ray-tracing%20and%20two%20real-measurement%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01211v2&entry.124074799=Read"},
{"title": "Clustering in Causal Attention Masking", "author": "Nikita Karagodin and Yury Polyanskiy and Philippe Rigollet", "abstract": "  This work presents a modification of the self-attention dynamics proposed by\nGeshkovski et al. (arXiv:2312.10794) to better reflect the practically\nrelevant, causally masked attention used in transformer architectures for\ngenerative AI. This modification translates into an interacting particle system\nthat cannot be interpreted as a mean-field gradient flow. Despite this loss of\nstructure, we significantly strengthen the results of Geshkovski et al.\n(arXiv:2312.10794) in this context: While previous rigorous results focused on\ncases where all three matrices (Key, Query, and Value) were scaled identities,\nwe prove asymptotic convergence to a single cluster for arbitrary key-query\nmatrices and a value matrix equal to the identity. Additionally, we establish a\nconnection to the classical R\\'enyi parking problem from combinatorial geometry\nto make initial theoretical steps towards demonstrating the existence of\nmeta-stable states.\n", "link": "http://arxiv.org/abs/2411.04990v1", "date": "2024-11-07", "relevancy": 1.4182, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4739}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20in%20Causal%20Attention%20Masking&body=Title%3A%20Clustering%20in%20Causal%20Attention%20Masking%0AAuthor%3A%20Nikita%20Karagodin%20and%20Yury%20Polyanskiy%20and%20Philippe%20Rigollet%0AAbstract%3A%20%20%20This%20work%20presents%20a%20modification%20of%20the%20self-attention%20dynamics%20proposed%20by%0AGeshkovski%20et%20al.%20%28arXiv%3A2312.10794%29%20to%20better%20reflect%20the%20practically%0Arelevant%2C%20causally%20masked%20attention%20used%20in%20transformer%20architectures%20for%0Agenerative%20AI.%20This%20modification%20translates%20into%20an%20interacting%20particle%20system%0Athat%20cannot%20be%20interpreted%20as%20a%20mean-field%20gradient%20flow.%20Despite%20this%20loss%20of%0Astructure%2C%20we%20significantly%20strengthen%20the%20results%20of%20Geshkovski%20et%20al.%0A%28arXiv%3A2312.10794%29%20in%20this%20context%3A%20While%20previous%20rigorous%20results%20focused%20on%0Acases%20where%20all%20three%20matrices%20%28Key%2C%20Query%2C%20and%20Value%29%20were%20scaled%20identities%2C%0Awe%20prove%20asymptotic%20convergence%20to%20a%20single%20cluster%20for%20arbitrary%20key-query%0Amatrices%20and%20a%20value%20matrix%20equal%20to%20the%20identity.%20Additionally%2C%20we%20establish%20a%0Aconnection%20to%20the%20classical%20R%5C%27enyi%20parking%20problem%20from%20combinatorial%20geometry%0Ato%20make%20initial%20theoretical%20steps%20towards%20demonstrating%20the%20existence%20of%0Ameta-stable%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520in%2520Causal%2520Attention%2520Masking%26entry.906535625%3DNikita%2520Karagodin%2520and%2520Yury%2520Polyanskiy%2520and%2520Philippe%2520Rigollet%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520modification%2520of%2520the%2520self-attention%2520dynamics%2520proposed%2520by%250AGeshkovski%2520et%2520al.%2520%2528arXiv%253A2312.10794%2529%2520to%2520better%2520reflect%2520the%2520practically%250Arelevant%252C%2520causally%2520masked%2520attention%2520used%2520in%2520transformer%2520architectures%2520for%250Agenerative%2520AI.%2520This%2520modification%2520translates%2520into%2520an%2520interacting%2520particle%2520system%250Athat%2520cannot%2520be%2520interpreted%2520as%2520a%2520mean-field%2520gradient%2520flow.%2520Despite%2520this%2520loss%2520of%250Astructure%252C%2520we%2520significantly%2520strengthen%2520the%2520results%2520of%2520Geshkovski%2520et%2520al.%250A%2528arXiv%253A2312.10794%2529%2520in%2520this%2520context%253A%2520While%2520previous%2520rigorous%2520results%2520focused%2520on%250Acases%2520where%2520all%2520three%2520matrices%2520%2528Key%252C%2520Query%252C%2520and%2520Value%2529%2520were%2520scaled%2520identities%252C%250Awe%2520prove%2520asymptotic%2520convergence%2520to%2520a%2520single%2520cluster%2520for%2520arbitrary%2520key-query%250Amatrices%2520and%2520a%2520value%2520matrix%2520equal%2520to%2520the%2520identity.%2520Additionally%252C%2520we%2520establish%2520a%250Aconnection%2520to%2520the%2520classical%2520R%255C%2527enyi%2520parking%2520problem%2520from%2520combinatorial%2520geometry%250Ato%2520make%2520initial%2520theoretical%2520steps%2520towards%2520demonstrating%2520the%2520existence%2520of%250Ameta-stable%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20in%20Causal%20Attention%20Masking&entry.906535625=Nikita%20Karagodin%20and%20Yury%20Polyanskiy%20and%20Philippe%20Rigollet&entry.1292438233=%20%20This%20work%20presents%20a%20modification%20of%20the%20self-attention%20dynamics%20proposed%20by%0AGeshkovski%20et%20al.%20%28arXiv%3A2312.10794%29%20to%20better%20reflect%20the%20practically%0Arelevant%2C%20causally%20masked%20attention%20used%20in%20transformer%20architectures%20for%0Agenerative%20AI.%20This%20modification%20translates%20into%20an%20interacting%20particle%20system%0Athat%20cannot%20be%20interpreted%20as%20a%20mean-field%20gradient%20flow.%20Despite%20this%20loss%20of%0Astructure%2C%20we%20significantly%20strengthen%20the%20results%20of%20Geshkovski%20et%20al.%0A%28arXiv%3A2312.10794%29%20in%20this%20context%3A%20While%20previous%20rigorous%20results%20focused%20on%0Acases%20where%20all%20three%20matrices%20%28Key%2C%20Query%2C%20and%20Value%29%20were%20scaled%20identities%2C%0Awe%20prove%20asymptotic%20convergence%20to%20a%20single%20cluster%20for%20arbitrary%20key-query%0Amatrices%20and%20a%20value%20matrix%20equal%20to%20the%20identity.%20Additionally%2C%20we%20establish%20a%0Aconnection%20to%20the%20classical%20R%5C%27enyi%20parking%20problem%20from%20combinatorial%20geometry%0Ato%20make%20initial%20theoretical%20steps%20towards%20demonstrating%20the%20existence%20of%0Ameta-stable%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04990v1&entry.124074799=Read"},
{"title": "A Simple Packing Algorithm for Optimized Mapping of Artificial Neural\n  Networks onto Non-Volatile Memory Cross-Bar Arrays", "author": "W. Haensch", "abstract": "  Neuromorphic computing with crossbar arrays has emerged as a promising\nalternative to improve computing efficiency for machine learning. Previous work\nhas focused on implementing crossbar arrays to perform basic mathematical\noperations. However, in this paper, we explore the impact of mapping the layers\nof an artificial neural network onto physical cross-bar arrays arranged in\ntiles across a chip. We have developed a simplified mapping algorithm to\ndetermine the number of physical tiles, with fixed optimal array dimensions,\nand to estimate the minimum area occupied by these tiles for a given design\nobjective. This simplified algorithm is compared with conventional binary\nlinear optimization, which solves the equivalent bin-packing problem. We have\nfound that the optimum solution is not necessarily related to the minimum\nnumber of tiles; rather, it is shown to be an interaction between tile array\ncapacity and the scaling properties of its peripheral circuits. Additionally,\nwe have discovered that square arrays are not always the best choice for\noptimal mapping, and that performance optimization comes at the cost of total\ntile area\n", "link": "http://arxiv.org/abs/2411.04814v1", "date": "2024-11-07", "relevancy": 1.8888, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4735}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Packing%20Algorithm%20for%20Optimized%20Mapping%20of%20Artificial%20Neural%0A%20%20Networks%20onto%20Non-Volatile%20Memory%20Cross-Bar%20Arrays&body=Title%3A%20A%20Simple%20Packing%20Algorithm%20for%20Optimized%20Mapping%20of%20Artificial%20Neural%0A%20%20Networks%20onto%20Non-Volatile%20Memory%20Cross-Bar%20Arrays%0AAuthor%3A%20W.%20Haensch%0AAbstract%3A%20%20%20Neuromorphic%20computing%20with%20crossbar%20arrays%20has%20emerged%20as%20a%20promising%0Aalternative%20to%20improve%20computing%20efficiency%20for%20machine%20learning.%20Previous%20work%0Ahas%20focused%20on%20implementing%20crossbar%20arrays%20to%20perform%20basic%20mathematical%0Aoperations.%20However%2C%20in%20this%20paper%2C%20we%20explore%20the%20impact%20of%20mapping%20the%20layers%0Aof%20an%20artificial%20neural%20network%20onto%20physical%20cross-bar%20arrays%20arranged%20in%0Atiles%20across%20a%20chip.%20We%20have%20developed%20a%20simplified%20mapping%20algorithm%20to%0Adetermine%20the%20number%20of%20physical%20tiles%2C%20with%20fixed%20optimal%20array%20dimensions%2C%0Aand%20to%20estimate%20the%20minimum%20area%20occupied%20by%20these%20tiles%20for%20a%20given%20design%0Aobjective.%20This%20simplified%20algorithm%20is%20compared%20with%20conventional%20binary%0Alinear%20optimization%2C%20which%20solves%20the%20equivalent%20bin-packing%20problem.%20We%20have%0Afound%20that%20the%20optimum%20solution%20is%20not%20necessarily%20related%20to%20the%20minimum%0Anumber%20of%20tiles%3B%20rather%2C%20it%20is%20shown%20to%20be%20an%20interaction%20between%20tile%20array%0Acapacity%20and%20the%20scaling%20properties%20of%20its%20peripheral%20circuits.%20Additionally%2C%0Awe%20have%20discovered%20that%20square%20arrays%20are%20not%20always%20the%20best%20choice%20for%0Aoptimal%20mapping%2C%20and%20that%20performance%20optimization%20comes%20at%20the%20cost%20of%20total%0Atile%20area%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Packing%2520Algorithm%2520for%2520Optimized%2520Mapping%2520of%2520Artificial%2520Neural%250A%2520%2520Networks%2520onto%2520Non-Volatile%2520Memory%2520Cross-Bar%2520Arrays%26entry.906535625%3DW.%2520Haensch%26entry.1292438233%3D%2520%2520Neuromorphic%2520computing%2520with%2520crossbar%2520arrays%2520has%2520emerged%2520as%2520a%2520promising%250Aalternative%2520to%2520improve%2520computing%2520efficiency%2520for%2520machine%2520learning.%2520Previous%2520work%250Ahas%2520focused%2520on%2520implementing%2520crossbar%2520arrays%2520to%2520perform%2520basic%2520mathematical%250Aoperations.%2520However%252C%2520in%2520this%2520paper%252C%2520we%2520explore%2520the%2520impact%2520of%2520mapping%2520the%2520layers%250Aof%2520an%2520artificial%2520neural%2520network%2520onto%2520physical%2520cross-bar%2520arrays%2520arranged%2520in%250Atiles%2520across%2520a%2520chip.%2520We%2520have%2520developed%2520a%2520simplified%2520mapping%2520algorithm%2520to%250Adetermine%2520the%2520number%2520of%2520physical%2520tiles%252C%2520with%2520fixed%2520optimal%2520array%2520dimensions%252C%250Aand%2520to%2520estimate%2520the%2520minimum%2520area%2520occupied%2520by%2520these%2520tiles%2520for%2520a%2520given%2520design%250Aobjective.%2520This%2520simplified%2520algorithm%2520is%2520compared%2520with%2520conventional%2520binary%250Alinear%2520optimization%252C%2520which%2520solves%2520the%2520equivalent%2520bin-packing%2520problem.%2520We%2520have%250Afound%2520that%2520the%2520optimum%2520solution%2520is%2520not%2520necessarily%2520related%2520to%2520the%2520minimum%250Anumber%2520of%2520tiles%253B%2520rather%252C%2520it%2520is%2520shown%2520to%2520be%2520an%2520interaction%2520between%2520tile%2520array%250Acapacity%2520and%2520the%2520scaling%2520properties%2520of%2520its%2520peripheral%2520circuits.%2520Additionally%252C%250Awe%2520have%2520discovered%2520that%2520square%2520arrays%2520are%2520not%2520always%2520the%2520best%2520choice%2520for%250Aoptimal%2520mapping%252C%2520and%2520that%2520performance%2520optimization%2520comes%2520at%2520the%2520cost%2520of%2520total%250Atile%2520area%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Packing%20Algorithm%20for%20Optimized%20Mapping%20of%20Artificial%20Neural%0A%20%20Networks%20onto%20Non-Volatile%20Memory%20Cross-Bar%20Arrays&entry.906535625=W.%20Haensch&entry.1292438233=%20%20Neuromorphic%20computing%20with%20crossbar%20arrays%20has%20emerged%20as%20a%20promising%0Aalternative%20to%20improve%20computing%20efficiency%20for%20machine%20learning.%20Previous%20work%0Ahas%20focused%20on%20implementing%20crossbar%20arrays%20to%20perform%20basic%20mathematical%0Aoperations.%20However%2C%20in%20this%20paper%2C%20we%20explore%20the%20impact%20of%20mapping%20the%20layers%0Aof%20an%20artificial%20neural%20network%20onto%20physical%20cross-bar%20arrays%20arranged%20in%0Atiles%20across%20a%20chip.%20We%20have%20developed%20a%20simplified%20mapping%20algorithm%20to%0Adetermine%20the%20number%20of%20physical%20tiles%2C%20with%20fixed%20optimal%20array%20dimensions%2C%0Aand%20to%20estimate%20the%20minimum%20area%20occupied%20by%20these%20tiles%20for%20a%20given%20design%0Aobjective.%20This%20simplified%20algorithm%20is%20compared%20with%20conventional%20binary%0Alinear%20optimization%2C%20which%20solves%20the%20equivalent%20bin-packing%20problem.%20We%20have%0Afound%20that%20the%20optimum%20solution%20is%20not%20necessarily%20related%20to%20the%20minimum%0Anumber%20of%20tiles%3B%20rather%2C%20it%20is%20shown%20to%20be%20an%20interaction%20between%20tile%20array%0Acapacity%20and%20the%20scaling%20properties%20of%20its%20peripheral%20circuits.%20Additionally%2C%0Awe%20have%20discovered%20that%20square%20arrays%20are%20not%20always%20the%20best%20choice%20for%0Aoptimal%20mapping%2C%20and%20that%20performance%20optimization%20comes%20at%20the%20cost%20of%20total%0Atile%20area%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04814v1&entry.124074799=Read"},
{"title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering", "author": "Laura Cabello and Carmen Martin-Turrero and Uchenna Akujuobi and Anders S\u00f8gaard and Carlos Bobed", "abstract": "  Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.\n", "link": "http://arxiv.org/abs/2411.03883v2", "date": "2024-11-07", "relevancy": 2.0075, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering&body=Title%3A%20MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering%0AAuthor%3A%20Laura%20Cabello%20and%20Carmen%20Martin-Turrero%20and%20Uchenna%20Akujuobi%20and%20Anders%20S%C3%B8gaard%20and%20Carlos%20Bobed%0AAbstract%3A%20%20%20Question%20answering%20is%20a%20natural%20language%20understanding%20task%20that%20involves%0Areasoning%20over%20both%20explicit%20context%20and%20unstated%2C%20relevant%20domain%20knowledge.%0ALarge%20language%20models%20%28LLMs%29%2C%20which%20underpin%20most%20contemporary%20question%0Aanswering%20systems%2C%20struggle%20to%20induce%20how%20concepts%20relate%20in%20specialized%0Adomains%20such%20as%20medicine.%20Existing%20medical%20LLMs%20are%20also%20costly%20to%20train.%20In%0Athis%20work%2C%20we%20present%20MEG%2C%20a%20parameter-efficient%20approach%20for%20medical%0Aknowledge-augmented%20LLMs.%20MEG%20uses%20a%20lightweight%20mapping%20network%20to%20integrate%0Agraph%20embeddings%20into%20the%20LLM%2C%20enabling%20it%20to%20leverage%20external%20knowledge%20in%20a%0Acost-effective%20way.%20We%20evaluate%20our%20method%20on%20four%20popular%20medical%0Amultiple-choice%20datasets%20and%20show%20that%20LLMs%20greatly%20benefit%20from%20the%20factual%0Agrounding%20provided%20by%20knowledge%20graph%20embeddings.%20MEG%20attains%20an%20average%20of%0A%2B10.2%25%20accuracy%20over%20the%20Mistral-Instruct%20baseline%2C%20and%20%2B6.7%25%20over%20specialized%0Amodels%20like%20BioMistral.%20We%20also%20show%20results%20based%20on%20Llama-3.%20Finally%2C%20we%20show%0Athat%20MEG%27s%20performance%20remains%20robust%20to%20the%20choice%20of%20graph%20encoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEG%253A%2520Medical%2520Knowledge-Augmented%2520Large%2520Language%2520Models%2520for%2520Question%250A%2520%2520Answering%26entry.906535625%3DLaura%2520Cabello%2520and%2520Carmen%2520Martin-Turrero%2520and%2520Uchenna%2520Akujuobi%2520and%2520Anders%2520S%25C3%25B8gaard%2520and%2520Carlos%2520Bobed%26entry.1292438233%3D%2520%2520Question%2520answering%2520is%2520a%2520natural%2520language%2520understanding%2520task%2520that%2520involves%250Areasoning%2520over%2520both%2520explicit%2520context%2520and%2520unstated%252C%2520relevant%2520domain%2520knowledge.%250ALarge%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520underpin%2520most%2520contemporary%2520question%250Aanswering%2520systems%252C%2520struggle%2520to%2520induce%2520how%2520concepts%2520relate%2520in%2520specialized%250Adomains%2520such%2520as%2520medicine.%2520Existing%2520medical%2520LLMs%2520are%2520also%2520costly%2520to%2520train.%2520In%250Athis%2520work%252C%2520we%2520present%2520MEG%252C%2520a%2520parameter-efficient%2520approach%2520for%2520medical%250Aknowledge-augmented%2520LLMs.%2520MEG%2520uses%2520a%2520lightweight%2520mapping%2520network%2520to%2520integrate%250Agraph%2520embeddings%2520into%2520the%2520LLM%252C%2520enabling%2520it%2520to%2520leverage%2520external%2520knowledge%2520in%2520a%250Acost-effective%2520way.%2520We%2520evaluate%2520our%2520method%2520on%2520four%2520popular%2520medical%250Amultiple-choice%2520datasets%2520and%2520show%2520that%2520LLMs%2520greatly%2520benefit%2520from%2520the%2520factual%250Agrounding%2520provided%2520by%2520knowledge%2520graph%2520embeddings.%2520MEG%2520attains%2520an%2520average%2520of%250A%252B10.2%2525%2520accuracy%2520over%2520the%2520Mistral-Instruct%2520baseline%252C%2520and%2520%252B6.7%2525%2520over%2520specialized%250Amodels%2520like%2520BioMistral.%2520We%2520also%2520show%2520results%2520based%2520on%2520Llama-3.%2520Finally%252C%2520we%2520show%250Athat%2520MEG%2527s%2520performance%2520remains%2520robust%2520to%2520the%2520choice%2520of%2520graph%2520encoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering&entry.906535625=Laura%20Cabello%20and%20Carmen%20Martin-Turrero%20and%20Uchenna%20Akujuobi%20and%20Anders%20S%C3%B8gaard%20and%20Carlos%20Bobed&entry.1292438233=%20%20Question%20answering%20is%20a%20natural%20language%20understanding%20task%20that%20involves%0Areasoning%20over%20both%20explicit%20context%20and%20unstated%2C%20relevant%20domain%20knowledge.%0ALarge%20language%20models%20%28LLMs%29%2C%20which%20underpin%20most%20contemporary%20question%0Aanswering%20systems%2C%20struggle%20to%20induce%20how%20concepts%20relate%20in%20specialized%0Adomains%20such%20as%20medicine.%20Existing%20medical%20LLMs%20are%20also%20costly%20to%20train.%20In%0Athis%20work%2C%20we%20present%20MEG%2C%20a%20parameter-efficient%20approach%20for%20medical%0Aknowledge-augmented%20LLMs.%20MEG%20uses%20a%20lightweight%20mapping%20network%20to%20integrate%0Agraph%20embeddings%20into%20the%20LLM%2C%20enabling%20it%20to%20leverage%20external%20knowledge%20in%20a%0Acost-effective%20way.%20We%20evaluate%20our%20method%20on%20four%20popular%20medical%0Amultiple-choice%20datasets%20and%20show%20that%20LLMs%20greatly%20benefit%20from%20the%20factual%0Agrounding%20provided%20by%20knowledge%20graph%20embeddings.%20MEG%20attains%20an%20average%20of%0A%2B10.2%25%20accuracy%20over%20the%20Mistral-Instruct%20baseline%2C%20and%20%2B6.7%25%20over%20specialized%0Amodels%20like%20BioMistral.%20We%20also%20show%20results%20based%20on%20Llama-3.%20Finally%2C%20we%20show%0Athat%20MEG%27s%20performance%20remains%20robust%20to%20the%20choice%20of%20graph%20encoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03883v2&entry.124074799=Read"},
{"title": "Asymptotic regularity of a generalised stochastic Halpern scheme with\n  applications", "author": "Nicholas Pischke and Thomas Powell", "abstract": "  We provide abstract, general and highly uniform rates of asymptotic\nregularity for a generalized stochastic Halpern-style iteration, which\nincorporates a second mapping in the style of a Krasnoselskii-Mann iteration.\nThis iteration is general in two ways: First, it incorporates stochasticity in\na completely abstract way rather than fixing a sampling method; secondly, it\nincludes as special cases stochastic versions of various schemes from the\noptimization literature, including Halpern's iteration as well as a\nKrasnoselskii-Mann iteration with Tikhonov regularization terms in the sense of\nBo\\c{t}, Csetnek and Meier. For these particular cases, we in particular obtain\nlinear rates of asymptotic regularity, matching (or improving) the currently\nbest known rates for these iterations in stochastic optimization, and quadratic\nrates of asymptotic regularity are obtained in the context of inner product\nspaces for the general iteration. We utilize these rates to give bounds on the\noracle complexity of such iterations under suitable variance assumptions and\nbatching strategies, again presented in an abstract style. Finally, we sketch\nhow the schemes presented here can be instantiated in the context of\nreinforcement learning to yield novel methods for Q-learning.\n", "link": "http://arxiv.org/abs/2411.04845v1", "date": "2024-11-07", "relevancy": 1.6484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4259}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4101}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotic%20regularity%20of%20a%20generalised%20stochastic%20Halpern%20scheme%20with%0A%20%20applications&body=Title%3A%20Asymptotic%20regularity%20of%20a%20generalised%20stochastic%20Halpern%20scheme%20with%0A%20%20applications%0AAuthor%3A%20Nicholas%20Pischke%20and%20Thomas%20Powell%0AAbstract%3A%20%20%20We%20provide%20abstract%2C%20general%20and%20highly%20uniform%20rates%20of%20asymptotic%0Aregularity%20for%20a%20generalized%20stochastic%20Halpern-style%20iteration%2C%20which%0Aincorporates%20a%20second%20mapping%20in%20the%20style%20of%20a%20Krasnoselskii-Mann%20iteration.%0AThis%20iteration%20is%20general%20in%20two%20ways%3A%20First%2C%20it%20incorporates%20stochasticity%20in%0Aa%20completely%20abstract%20way%20rather%20than%20fixing%20a%20sampling%20method%3B%20secondly%2C%20it%0Aincludes%20as%20special%20cases%20stochastic%20versions%20of%20various%20schemes%20from%20the%0Aoptimization%20literature%2C%20including%20Halpern%27s%20iteration%20as%20well%20as%20a%0AKrasnoselskii-Mann%20iteration%20with%20Tikhonov%20regularization%20terms%20in%20the%20sense%20of%0ABo%5Cc%7Bt%7D%2C%20Csetnek%20and%20Meier.%20For%20these%20particular%20cases%2C%20we%20in%20particular%20obtain%0Alinear%20rates%20of%20asymptotic%20regularity%2C%20matching%20%28or%20improving%29%20the%20currently%0Abest%20known%20rates%20for%20these%20iterations%20in%20stochastic%20optimization%2C%20and%20quadratic%0Arates%20of%20asymptotic%20regularity%20are%20obtained%20in%20the%20context%20of%20inner%20product%0Aspaces%20for%20the%20general%20iteration.%20We%20utilize%20these%20rates%20to%20give%20bounds%20on%20the%0Aoracle%20complexity%20of%20such%20iterations%20under%20suitable%20variance%20assumptions%20and%0Abatching%20strategies%2C%20again%20presented%20in%20an%20abstract%20style.%20Finally%2C%20we%20sketch%0Ahow%20the%20schemes%20presented%20here%20can%20be%20instantiated%20in%20the%20context%20of%0Areinforcement%20learning%20to%20yield%20novel%20methods%20for%20Q-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotic%2520regularity%2520of%2520a%2520generalised%2520stochastic%2520Halpern%2520scheme%2520with%250A%2520%2520applications%26entry.906535625%3DNicholas%2520Pischke%2520and%2520Thomas%2520Powell%26entry.1292438233%3D%2520%2520We%2520provide%2520abstract%252C%2520general%2520and%2520highly%2520uniform%2520rates%2520of%2520asymptotic%250Aregularity%2520for%2520a%2520generalized%2520stochastic%2520Halpern-style%2520iteration%252C%2520which%250Aincorporates%2520a%2520second%2520mapping%2520in%2520the%2520style%2520of%2520a%2520Krasnoselskii-Mann%2520iteration.%250AThis%2520iteration%2520is%2520general%2520in%2520two%2520ways%253A%2520First%252C%2520it%2520incorporates%2520stochasticity%2520in%250Aa%2520completely%2520abstract%2520way%2520rather%2520than%2520fixing%2520a%2520sampling%2520method%253B%2520secondly%252C%2520it%250Aincludes%2520as%2520special%2520cases%2520stochastic%2520versions%2520of%2520various%2520schemes%2520from%2520the%250Aoptimization%2520literature%252C%2520including%2520Halpern%2527s%2520iteration%2520as%2520well%2520as%2520a%250AKrasnoselskii-Mann%2520iteration%2520with%2520Tikhonov%2520regularization%2520terms%2520in%2520the%2520sense%2520of%250ABo%255Cc%257Bt%257D%252C%2520Csetnek%2520and%2520Meier.%2520For%2520these%2520particular%2520cases%252C%2520we%2520in%2520particular%2520obtain%250Alinear%2520rates%2520of%2520asymptotic%2520regularity%252C%2520matching%2520%2528or%2520improving%2529%2520the%2520currently%250Abest%2520known%2520rates%2520for%2520these%2520iterations%2520in%2520stochastic%2520optimization%252C%2520and%2520quadratic%250Arates%2520of%2520asymptotic%2520regularity%2520are%2520obtained%2520in%2520the%2520context%2520of%2520inner%2520product%250Aspaces%2520for%2520the%2520general%2520iteration.%2520We%2520utilize%2520these%2520rates%2520to%2520give%2520bounds%2520on%2520the%250Aoracle%2520complexity%2520of%2520such%2520iterations%2520under%2520suitable%2520variance%2520assumptions%2520and%250Abatching%2520strategies%252C%2520again%2520presented%2520in%2520an%2520abstract%2520style.%2520Finally%252C%2520we%2520sketch%250Ahow%2520the%2520schemes%2520presented%2520here%2520can%2520be%2520instantiated%2520in%2520the%2520context%2520of%250Areinforcement%2520learning%2520to%2520yield%2520novel%2520methods%2520for%2520Q-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotic%20regularity%20of%20a%20generalised%20stochastic%20Halpern%20scheme%20with%0A%20%20applications&entry.906535625=Nicholas%20Pischke%20and%20Thomas%20Powell&entry.1292438233=%20%20We%20provide%20abstract%2C%20general%20and%20highly%20uniform%20rates%20of%20asymptotic%0Aregularity%20for%20a%20generalized%20stochastic%20Halpern-style%20iteration%2C%20which%0Aincorporates%20a%20second%20mapping%20in%20the%20style%20of%20a%20Krasnoselskii-Mann%20iteration.%0AThis%20iteration%20is%20general%20in%20two%20ways%3A%20First%2C%20it%20incorporates%20stochasticity%20in%0Aa%20completely%20abstract%20way%20rather%20than%20fixing%20a%20sampling%20method%3B%20secondly%2C%20it%0Aincludes%20as%20special%20cases%20stochastic%20versions%20of%20various%20schemes%20from%20the%0Aoptimization%20literature%2C%20including%20Halpern%27s%20iteration%20as%20well%20as%20a%0AKrasnoselskii-Mann%20iteration%20with%20Tikhonov%20regularization%20terms%20in%20the%20sense%20of%0ABo%5Cc%7Bt%7D%2C%20Csetnek%20and%20Meier.%20For%20these%20particular%20cases%2C%20we%20in%20particular%20obtain%0Alinear%20rates%20of%20asymptotic%20regularity%2C%20matching%20%28or%20improving%29%20the%20currently%0Abest%20known%20rates%20for%20these%20iterations%20in%20stochastic%20optimization%2C%20and%20quadratic%0Arates%20of%20asymptotic%20regularity%20are%20obtained%20in%20the%20context%20of%20inner%20product%0Aspaces%20for%20the%20general%20iteration.%20We%20utilize%20these%20rates%20to%20give%20bounds%20on%20the%0Aoracle%20complexity%20of%20such%20iterations%20under%20suitable%20variance%20assumptions%20and%0Abatching%20strategies%2C%20again%20presented%20in%20an%20abstract%20style.%20Finally%2C%20we%20sketch%0Ahow%20the%20schemes%20presented%20here%20can%20be%20instantiated%20in%20the%20context%20of%0Areinforcement%20learning%20to%20yield%20novel%20methods%20for%20Q-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04845v1&entry.124074799=Read"},
{"title": "SpikeBottleNet: Spike-Driven Feature Compression Architecture for\n  Edge-Cloud Co-Inference", "author": "Maruf Hassan and Steven Davy", "abstract": "  Edge-cloud co-inference enables efficient deep neural network (DNN)\ndeployment by splitting the architecture between an edge device and cloud\nserver, crucial for resource-constraint edge devices. This approach requires\nbalancing on-device computations and communication costs, often achieved\nthrough compressed intermediate feature transmission. Conventional DNN\narchitectures require continuous data processing and floating point\nactivations, leading to considerable energy consumption and increased feature\nsizes, thus raising transmission costs. This challenge motivates exploring\nbinary, event-driven activations using spiking neural networks (SNNs), known\nfor their extreme energy efficiency. In this research, we propose\nSpikeBottleNet, a novel architecture for edge-cloud co-inference systems that\nintegrates a spiking neuron model to significantly reduce energy consumption on\nedge devices. A key innovation of our study is an intermediate feature\ncompression technique tailored for SNNs for efficient feature transmission.\nThis technique leverages a split computing approach to strategically place\nencoder-decoder bottleneck units within complex deep architectures like ResNet\nand MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up\nto 256x bit compression in the final convolutional layer of ResNet, with\nminimal accuracy loss (0.16%). Additionally, our approach enhances edge device\nenergy efficiency by up to 144x compared to the baseline BottleNet, making it\nideal for resource-limited edge devices.\n", "link": "http://arxiv.org/abs/2410.08673v2", "date": "2024-11-07", "relevancy": 1.9948, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5207}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.493}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeBottleNet%3A%20Spike-Driven%20Feature%20Compression%20Architecture%20for%0A%20%20Edge-Cloud%20Co-Inference&body=Title%3A%20SpikeBottleNet%3A%20Spike-Driven%20Feature%20Compression%20Architecture%20for%0A%20%20Edge-Cloud%20Co-Inference%0AAuthor%3A%20Maruf%20Hassan%20and%20Steven%20Davy%0AAbstract%3A%20%20%20Edge-cloud%20co-inference%20enables%20efficient%20deep%20neural%20network%20%28DNN%29%0Adeployment%20by%20splitting%20the%20architecture%20between%20an%20edge%20device%20and%20cloud%0Aserver%2C%20crucial%20for%20resource-constraint%20edge%20devices.%20This%20approach%20requires%0Abalancing%20on-device%20computations%20and%20communication%20costs%2C%20often%20achieved%0Athrough%20compressed%20intermediate%20feature%20transmission.%20Conventional%20DNN%0Aarchitectures%20require%20continuous%20data%20processing%20and%20floating%20point%0Aactivations%2C%20leading%20to%20considerable%20energy%20consumption%20and%20increased%20feature%0Asizes%2C%20thus%20raising%20transmission%20costs.%20This%20challenge%20motivates%20exploring%0Abinary%2C%20event-driven%20activations%20using%20spiking%20neural%20networks%20%28SNNs%29%2C%20known%0Afor%20their%20extreme%20energy%20efficiency.%20In%20this%20research%2C%20we%20propose%0ASpikeBottleNet%2C%20a%20novel%20architecture%20for%20edge-cloud%20co-inference%20systems%20that%0Aintegrates%20a%20spiking%20neuron%20model%20to%20significantly%20reduce%20energy%20consumption%20on%0Aedge%20devices.%20A%20key%20innovation%20of%20our%20study%20is%20an%20intermediate%20feature%0Acompression%20technique%20tailored%20for%20SNNs%20for%20efficient%20feature%20transmission.%0AThis%20technique%20leverages%20a%20split%20computing%20approach%20to%20strategically%20place%0Aencoder-decoder%20bottleneck%20units%20within%20complex%20deep%20architectures%20like%20ResNet%0Aand%20MobileNet.%20Experimental%20results%20demonstrate%20that%20SpikeBottleNet%20achieves%20up%0Ato%20256x%20bit%20compression%20in%20the%20final%20convolutional%20layer%20of%20ResNet%2C%20with%0Aminimal%20accuracy%20loss%20%280.16%25%29.%20Additionally%2C%20our%20approach%20enhances%20edge%20device%0Aenergy%20efficiency%20by%20up%20to%20144x%20compared%20to%20the%20baseline%20BottleNet%2C%20making%20it%0Aideal%20for%20resource-limited%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeBottleNet%253A%2520Spike-Driven%2520Feature%2520Compression%2520Architecture%2520for%250A%2520%2520Edge-Cloud%2520Co-Inference%26entry.906535625%3DMaruf%2520Hassan%2520and%2520Steven%2520Davy%26entry.1292438233%3D%2520%2520Edge-cloud%2520co-inference%2520enables%2520efficient%2520deep%2520neural%2520network%2520%2528DNN%2529%250Adeployment%2520by%2520splitting%2520the%2520architecture%2520between%2520an%2520edge%2520device%2520and%2520cloud%250Aserver%252C%2520crucial%2520for%2520resource-constraint%2520edge%2520devices.%2520This%2520approach%2520requires%250Abalancing%2520on-device%2520computations%2520and%2520communication%2520costs%252C%2520often%2520achieved%250Athrough%2520compressed%2520intermediate%2520feature%2520transmission.%2520Conventional%2520DNN%250Aarchitectures%2520require%2520continuous%2520data%2520processing%2520and%2520floating%2520point%250Aactivations%252C%2520leading%2520to%2520considerable%2520energy%2520consumption%2520and%2520increased%2520feature%250Asizes%252C%2520thus%2520raising%2520transmission%2520costs.%2520This%2520challenge%2520motivates%2520exploring%250Abinary%252C%2520event-driven%2520activations%2520using%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520known%250Afor%2520their%2520extreme%2520energy%2520efficiency.%2520In%2520this%2520research%252C%2520we%2520propose%250ASpikeBottleNet%252C%2520a%2520novel%2520architecture%2520for%2520edge-cloud%2520co-inference%2520systems%2520that%250Aintegrates%2520a%2520spiking%2520neuron%2520model%2520to%2520significantly%2520reduce%2520energy%2520consumption%2520on%250Aedge%2520devices.%2520A%2520key%2520innovation%2520of%2520our%2520study%2520is%2520an%2520intermediate%2520feature%250Acompression%2520technique%2520tailored%2520for%2520SNNs%2520for%2520efficient%2520feature%2520transmission.%250AThis%2520technique%2520leverages%2520a%2520split%2520computing%2520approach%2520to%2520strategically%2520place%250Aencoder-decoder%2520bottleneck%2520units%2520within%2520complex%2520deep%2520architectures%2520like%2520ResNet%250Aand%2520MobileNet.%2520Experimental%2520results%2520demonstrate%2520that%2520SpikeBottleNet%2520achieves%2520up%250Ato%2520256x%2520bit%2520compression%2520in%2520the%2520final%2520convolutional%2520layer%2520of%2520ResNet%252C%2520with%250Aminimal%2520accuracy%2520loss%2520%25280.16%2525%2529.%2520Additionally%252C%2520our%2520approach%2520enhances%2520edge%2520device%250Aenergy%2520efficiency%2520by%2520up%2520to%2520144x%2520compared%2520to%2520the%2520baseline%2520BottleNet%252C%2520making%2520it%250Aideal%2520for%2520resource-limited%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeBottleNet%3A%20Spike-Driven%20Feature%20Compression%20Architecture%20for%0A%20%20Edge-Cloud%20Co-Inference&entry.906535625=Maruf%20Hassan%20and%20Steven%20Davy&entry.1292438233=%20%20Edge-cloud%20co-inference%20enables%20efficient%20deep%20neural%20network%20%28DNN%29%0Adeployment%20by%20splitting%20the%20architecture%20between%20an%20edge%20device%20and%20cloud%0Aserver%2C%20crucial%20for%20resource-constraint%20edge%20devices.%20This%20approach%20requires%0Abalancing%20on-device%20computations%20and%20communication%20costs%2C%20often%20achieved%0Athrough%20compressed%20intermediate%20feature%20transmission.%20Conventional%20DNN%0Aarchitectures%20require%20continuous%20data%20processing%20and%20floating%20point%0Aactivations%2C%20leading%20to%20considerable%20energy%20consumption%20and%20increased%20feature%0Asizes%2C%20thus%20raising%20transmission%20costs.%20This%20challenge%20motivates%20exploring%0Abinary%2C%20event-driven%20activations%20using%20spiking%20neural%20networks%20%28SNNs%29%2C%20known%0Afor%20their%20extreme%20energy%20efficiency.%20In%20this%20research%2C%20we%20propose%0ASpikeBottleNet%2C%20a%20novel%20architecture%20for%20edge-cloud%20co-inference%20systems%20that%0Aintegrates%20a%20spiking%20neuron%20model%20to%20significantly%20reduce%20energy%20consumption%20on%0Aedge%20devices.%20A%20key%20innovation%20of%20our%20study%20is%20an%20intermediate%20feature%0Acompression%20technique%20tailored%20for%20SNNs%20for%20efficient%20feature%20transmission.%0AThis%20technique%20leverages%20a%20split%20computing%20approach%20to%20strategically%20place%0Aencoder-decoder%20bottleneck%20units%20within%20complex%20deep%20architectures%20like%20ResNet%0Aand%20MobileNet.%20Experimental%20results%20demonstrate%20that%20SpikeBottleNet%20achieves%20up%0Ato%20256x%20bit%20compression%20in%20the%20final%20convolutional%20layer%20of%20ResNet%2C%20with%0Aminimal%20accuracy%20loss%20%280.16%25%29.%20Additionally%2C%20our%20approach%20enhances%20edge%20device%0Aenergy%20efficiency%20by%20up%20to%20144x%20compared%20to%20the%20baseline%20BottleNet%2C%20making%20it%0Aideal%20for%20resource-limited%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08673v2&entry.124074799=Read"},
{"title": "Personalized Large Language Models", "author": "Stanis\u0142aw Wo\u017aniak and Bart\u0142omiej Koptyra and Arkadiusz Janz and Przemys\u0142aw Kazienko and Jan Koco\u0144", "abstract": "  Large language models (LLMs) have significantly advanced Natural Language\nProcessing (NLP) tasks in recent years. However, their universal nature poses\nlimitations in scenarios requiring personalized responses, such as\nrecommendation systems and chatbots. This paper investigates methods to\npersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on\nsubjective tasks. Results demonstrate that personalized fine-tuning improves\nmodel reasoning compared to non-personalized models. Experiments on datasets\nfor emotion recognition and hate speech detection show consistent performance\ngains with personalized methods across different LLM architectures. These\nfindings underscore the importance of personalization for enhancing LLM\ncapabilities in subjective text perception tasks.\n", "link": "http://arxiv.org/abs/2402.09269v2", "date": "2024-11-07", "relevancy": 1.975, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Large%20Language%20Models&body=Title%3A%20Personalized%20Large%20Language%20Models%0AAuthor%3A%20Stanis%C5%82aw%20Wo%C5%BAniak%20and%20Bart%C5%82omiej%20Koptyra%20and%20Arkadiusz%20Janz%20and%20Przemys%C5%82aw%20Kazienko%20and%20Jan%20Koco%C5%84%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20Natural%20Language%0AProcessing%20%28NLP%29%20tasks%20in%20recent%20years.%20However%2C%20their%20universal%20nature%20poses%0Alimitations%20in%20scenarios%20requiring%20personalized%20responses%2C%20such%20as%0Arecommendation%20systems%20and%20chatbots.%20This%20paper%20investigates%20methods%20to%0Apersonalize%20LLMs%2C%20comparing%20fine-tuning%20and%20zero-shot%20reasoning%20approaches%20on%0Asubjective%20tasks.%20Results%20demonstrate%20that%20personalized%20fine-tuning%20improves%0Amodel%20reasoning%20compared%20to%20non-personalized%20models.%20Experiments%20on%20datasets%0Afor%20emotion%20recognition%20and%20hate%20speech%20detection%20show%20consistent%20performance%0Agains%20with%20personalized%20methods%20across%20different%20LLM%20architectures.%20These%0Afindings%20underscore%20the%20importance%20of%20personalization%20for%20enhancing%20LLM%0Acapabilities%20in%20subjective%20text%20perception%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Large%2520Language%2520Models%26entry.906535625%3DStanis%25C5%2582aw%2520Wo%25C5%25BAniak%2520and%2520Bart%25C5%2582omiej%2520Koptyra%2520and%2520Arkadiusz%2520Janz%2520and%2520Przemys%25C5%2582aw%2520Kazienko%2520and%2520Jan%2520Koco%25C5%2584%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529%2520tasks%2520in%2520recent%2520years.%2520However%252C%2520their%2520universal%2520nature%2520poses%250Alimitations%2520in%2520scenarios%2520requiring%2520personalized%2520responses%252C%2520such%2520as%250Arecommendation%2520systems%2520and%2520chatbots.%2520This%2520paper%2520investigates%2520methods%2520to%250Apersonalize%2520LLMs%252C%2520comparing%2520fine-tuning%2520and%2520zero-shot%2520reasoning%2520approaches%2520on%250Asubjective%2520tasks.%2520Results%2520demonstrate%2520that%2520personalized%2520fine-tuning%2520improves%250Amodel%2520reasoning%2520compared%2520to%2520non-personalized%2520models.%2520Experiments%2520on%2520datasets%250Afor%2520emotion%2520recognition%2520and%2520hate%2520speech%2520detection%2520show%2520consistent%2520performance%250Agains%2520with%2520personalized%2520methods%2520across%2520different%2520LLM%2520architectures.%2520These%250Afindings%2520underscore%2520the%2520importance%2520of%2520personalization%2520for%2520enhancing%2520LLM%250Acapabilities%2520in%2520subjective%2520text%2520perception%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Large%20Language%20Models&entry.906535625=Stanis%C5%82aw%20Wo%C5%BAniak%20and%20Bart%C5%82omiej%20Koptyra%20and%20Arkadiusz%20Janz%20and%20Przemys%C5%82aw%20Kazienko%20and%20Jan%20Koco%C5%84&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20Natural%20Language%0AProcessing%20%28NLP%29%20tasks%20in%20recent%20years.%20However%2C%20their%20universal%20nature%20poses%0Alimitations%20in%20scenarios%20requiring%20personalized%20responses%2C%20such%20as%0Arecommendation%20systems%20and%20chatbots.%20This%20paper%20investigates%20methods%20to%0Apersonalize%20LLMs%2C%20comparing%20fine-tuning%20and%20zero-shot%20reasoning%20approaches%20on%0Asubjective%20tasks.%20Results%20demonstrate%20that%20personalized%20fine-tuning%20improves%0Amodel%20reasoning%20compared%20to%20non-personalized%20models.%20Experiments%20on%20datasets%0Afor%20emotion%20recognition%20and%20hate%20speech%20detection%20show%20consistent%20performance%0Agains%20with%20personalized%20methods%20across%20different%20LLM%20architectures.%20These%0Afindings%20underscore%20the%20importance%20of%20personalization%20for%20enhancing%20LLM%0Acapabilities%20in%20subjective%20text%20perception%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09269v2&entry.124074799=Read"},
{"title": "Latent Diffusion Model for Conditional Reservoir Facies Generation", "author": "Daesoo Lee and Oscar Ovanger and Jo Eidsvik and Erlend Aune and Jacob Skauvold and Ragnar Hauge", "abstract": "  Creating accurate and geologically realistic reservoir facies based on\nlimited measurements is crucial for field development and reservoir management,\nespecially in the oil and gas sector. Traditional two-point geostatistics,\nwhile foundational, often struggle to capture complex geological patterns.\nMulti-point statistics offers more flexibility, but comes with its own\nchallenges related to pattern configurations and storage limits. With the rise\nof Generative Adversarial Networks (GANs) and their success in various fields,\nthere has been a shift towards using them for facies generation. However,\nrecent advances in the computer vision domain have shown the superiority of\ndiffusion models over GANs. Motivated by this, a novel Latent Diffusion Model\nis proposed, which is specifically designed for conditional generation of\nreservoir facies. The proposed model produces high-fidelity facies realizations\nthat rigorously preserve conditioning data. It significantly outperforms a\nGAN-based alternative. Our implementation on GitHub:\n\\url{https://github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation}.\n", "link": "http://arxiv.org/abs/2311.01968v2", "date": "2024-11-07", "relevancy": 1.1866, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6119}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5901}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Diffusion%20Model%20for%20Conditional%20Reservoir%20Facies%20Generation&body=Title%3A%20Latent%20Diffusion%20Model%20for%20Conditional%20Reservoir%20Facies%20Generation%0AAuthor%3A%20Daesoo%20Lee%20and%20Oscar%20Ovanger%20and%20Jo%20Eidsvik%20and%20Erlend%20Aune%20and%20Jacob%20Skauvold%20and%20Ragnar%20Hauge%0AAbstract%3A%20%20%20Creating%20accurate%20and%20geologically%20realistic%20reservoir%20facies%20based%20on%0Alimited%20measurements%20is%20crucial%20for%20field%20development%20and%20reservoir%20management%2C%0Aespecially%20in%20the%20oil%20and%20gas%20sector.%20Traditional%20two-point%20geostatistics%2C%0Awhile%20foundational%2C%20often%20struggle%20to%20capture%20complex%20geological%20patterns.%0AMulti-point%20statistics%20offers%20more%20flexibility%2C%20but%20comes%20with%20its%20own%0Achallenges%20related%20to%20pattern%20configurations%20and%20storage%20limits.%20With%20the%20rise%0Aof%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20their%20success%20in%20various%20fields%2C%0Athere%20has%20been%20a%20shift%20towards%20using%20them%20for%20facies%20generation.%20However%2C%0Arecent%20advances%20in%20the%20computer%20vision%20domain%20have%20shown%20the%20superiority%20of%0Adiffusion%20models%20over%20GANs.%20Motivated%20by%20this%2C%20a%20novel%20Latent%20Diffusion%20Model%0Ais%20proposed%2C%20which%20is%20specifically%20designed%20for%20conditional%20generation%20of%0Areservoir%20facies.%20The%20proposed%20model%20produces%20high-fidelity%20facies%20realizations%0Athat%20rigorously%20preserve%20conditioning%20data.%20It%20significantly%20outperforms%20a%0AGAN-based%20alternative.%20Our%20implementation%20on%20GitHub%3A%0A%5Curl%7Bhttps%3A//github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Diffusion%2520Model%2520for%2520Conditional%2520Reservoir%2520Facies%2520Generation%26entry.906535625%3DDaesoo%2520Lee%2520and%2520Oscar%2520Ovanger%2520and%2520Jo%2520Eidsvik%2520and%2520Erlend%2520Aune%2520and%2520Jacob%2520Skauvold%2520and%2520Ragnar%2520Hauge%26entry.1292438233%3D%2520%2520Creating%2520accurate%2520and%2520geologically%2520realistic%2520reservoir%2520facies%2520based%2520on%250Alimited%2520measurements%2520is%2520crucial%2520for%2520field%2520development%2520and%2520reservoir%2520management%252C%250Aespecially%2520in%2520the%2520oil%2520and%2520gas%2520sector.%2520Traditional%2520two-point%2520geostatistics%252C%250Awhile%2520foundational%252C%2520often%2520struggle%2520to%2520capture%2520complex%2520geological%2520patterns.%250AMulti-point%2520statistics%2520offers%2520more%2520flexibility%252C%2520but%2520comes%2520with%2520its%2520own%250Achallenges%2520related%2520to%2520pattern%2520configurations%2520and%2520storage%2520limits.%2520With%2520the%2520rise%250Aof%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%2520their%2520success%2520in%2520various%2520fields%252C%250Athere%2520has%2520been%2520a%2520shift%2520towards%2520using%2520them%2520for%2520facies%2520generation.%2520However%252C%250Arecent%2520advances%2520in%2520the%2520computer%2520vision%2520domain%2520have%2520shown%2520the%2520superiority%2520of%250Adiffusion%2520models%2520over%2520GANs.%2520Motivated%2520by%2520this%252C%2520a%2520novel%2520Latent%2520Diffusion%2520Model%250Ais%2520proposed%252C%2520which%2520is%2520specifically%2520designed%2520for%2520conditional%2520generation%2520of%250Areservoir%2520facies.%2520The%2520proposed%2520model%2520produces%2520high-fidelity%2520facies%2520realizations%250Athat%2520rigorously%2520preserve%2520conditioning%2520data.%2520It%2520significantly%2520outperforms%2520a%250AGAN-based%2520alternative.%2520Our%2520implementation%2520on%2520GitHub%253A%250A%255Curl%257Bhttps%253A//github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Diffusion%20Model%20for%20Conditional%20Reservoir%20Facies%20Generation&entry.906535625=Daesoo%20Lee%20and%20Oscar%20Ovanger%20and%20Jo%20Eidsvik%20and%20Erlend%20Aune%20and%20Jacob%20Skauvold%20and%20Ragnar%20Hauge&entry.1292438233=%20%20Creating%20accurate%20and%20geologically%20realistic%20reservoir%20facies%20based%20on%0Alimited%20measurements%20is%20crucial%20for%20field%20development%20and%20reservoir%20management%2C%0Aespecially%20in%20the%20oil%20and%20gas%20sector.%20Traditional%20two-point%20geostatistics%2C%0Awhile%20foundational%2C%20often%20struggle%20to%20capture%20complex%20geological%20patterns.%0AMulti-point%20statistics%20offers%20more%20flexibility%2C%20but%20comes%20with%20its%20own%0Achallenges%20related%20to%20pattern%20configurations%20and%20storage%20limits.%20With%20the%20rise%0Aof%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20their%20success%20in%20various%20fields%2C%0Athere%20has%20been%20a%20shift%20towards%20using%20them%20for%20facies%20generation.%20However%2C%0Arecent%20advances%20in%20the%20computer%20vision%20domain%20have%20shown%20the%20superiority%20of%0Adiffusion%20models%20over%20GANs.%20Motivated%20by%20this%2C%20a%20novel%20Latent%20Diffusion%20Model%0Ais%20proposed%2C%20which%20is%20specifically%20designed%20for%20conditional%20generation%20of%0Areservoir%20facies.%20The%20proposed%20model%20produces%20high-fidelity%20facies%20realizations%0Athat%20rigorously%20preserve%20conditioning%20data.%20It%20significantly%20outperforms%20a%0AGAN-based%20alternative.%20Our%20implementation%20on%20GitHub%3A%0A%5Curl%7Bhttps%3A//github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01968v2&entry.124074799=Read"},
{"title": "On the Inherent Robustness of One-Stage Object Detection against\n  Out-of-Distribution Data", "author": "Aitor Martinez-Seras and Javier Del Ser and Alain Andres and Pablo Garcia-Bringas", "abstract": "  Robustness is a fundamental aspect for developing safe and trustworthy\nmodels, particularly when they are deployed in the open world. In this work we\nanalyze the inherent capability of one-stage object detectors to robustly\noperate in the presence of out-of-distribution (OoD) data. Specifically, we\npropose a novel detection algorithm for detecting unknown objects in image\ndata, which leverages the features extracted by the model from each sample.\nDifferently from other recent approaches in the literature, our proposal does\nnot require retraining the object detector, thereby allowing for the use of\npretrained models. Our proposed OoD detector exploits the application of\nsupervised dimensionality reduction techniques to mitigate the effects of the\ncurse of dimensionality on the features extracted by the model. Furthermore, it\nutilizes high-resolution feature maps to identify potential unknown objects in\nan unsupervised fashion. Our experiments analyze the Pareto trade-off between\nthe performance detecting known and unknown objects resulting from different\nalgorithmic configurations and inference confidence thresholds. We also compare\nthe performance of our proposed algorithm to that of logits-based post-hoc OoD\nmethods, as well as possible fusion strategies. Finally, we discuss on the\ncompetitiveness of all tested methods against state-of-the-art OoD approaches\nfor object detection models over the recently published Unknown Object\nDetection benchmark. The obtained results verify that the performance of\navant-garde post-hoc OoD detectors can be further improved when combined with\nour proposed algorithm.\n", "link": "http://arxiv.org/abs/2411.04586v1", "date": "2024-11-07", "relevancy": 1.58, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5498}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Inherent%20Robustness%20of%20One-Stage%20Object%20Detection%20against%0A%20%20Out-of-Distribution%20Data&body=Title%3A%20On%20the%20Inherent%20Robustness%20of%20One-Stage%20Object%20Detection%20against%0A%20%20Out-of-Distribution%20Data%0AAuthor%3A%20Aitor%20Martinez-Seras%20and%20Javier%20Del%20Ser%20and%20Alain%20Andres%20and%20Pablo%20Garcia-Bringas%0AAbstract%3A%20%20%20Robustness%20is%20a%20fundamental%20aspect%20for%20developing%20safe%20and%20trustworthy%0Amodels%2C%20particularly%20when%20they%20are%20deployed%20in%20the%20open%20world.%20In%20this%20work%20we%0Aanalyze%20the%20inherent%20capability%20of%20one-stage%20object%20detectors%20to%20robustly%0Aoperate%20in%20the%20presence%20of%20out-of-distribution%20%28OoD%29%20data.%20Specifically%2C%20we%0Apropose%20a%20novel%20detection%20algorithm%20for%20detecting%20unknown%20objects%20in%20image%0Adata%2C%20which%20leverages%20the%20features%20extracted%20by%20the%20model%20from%20each%20sample.%0ADifferently%20from%20other%20recent%20approaches%20in%20the%20literature%2C%20our%20proposal%20does%0Anot%20require%20retraining%20the%20object%20detector%2C%20thereby%20allowing%20for%20the%20use%20of%0Apretrained%20models.%20Our%20proposed%20OoD%20detector%20exploits%20the%20application%20of%0Asupervised%20dimensionality%20reduction%20techniques%20to%20mitigate%20the%20effects%20of%20the%0Acurse%20of%20dimensionality%20on%20the%20features%20extracted%20by%20the%20model.%20Furthermore%2C%20it%0Autilizes%20high-resolution%20feature%20maps%20to%20identify%20potential%20unknown%20objects%20in%0Aan%20unsupervised%20fashion.%20Our%20experiments%20analyze%20the%20Pareto%20trade-off%20between%0Athe%20performance%20detecting%20known%20and%20unknown%20objects%20resulting%20from%20different%0Aalgorithmic%20configurations%20and%20inference%20confidence%20thresholds.%20We%20also%20compare%0Athe%20performance%20of%20our%20proposed%20algorithm%20to%20that%20of%20logits-based%20post-hoc%20OoD%0Amethods%2C%20as%20well%20as%20possible%20fusion%20strategies.%20Finally%2C%20we%20discuss%20on%20the%0Acompetitiveness%20of%20all%20tested%20methods%20against%20state-of-the-art%20OoD%20approaches%0Afor%20object%20detection%20models%20over%20the%20recently%20published%20Unknown%20Object%0ADetection%20benchmark.%20The%20obtained%20results%20verify%20that%20the%20performance%20of%0Aavant-garde%20post-hoc%20OoD%20detectors%20can%20be%20further%20improved%20when%20combined%20with%0Aour%20proposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Inherent%2520Robustness%2520of%2520One-Stage%2520Object%2520Detection%2520against%250A%2520%2520Out-of-Distribution%2520Data%26entry.906535625%3DAitor%2520Martinez-Seras%2520and%2520Javier%2520Del%2520Ser%2520and%2520Alain%2520Andres%2520and%2520Pablo%2520Garcia-Bringas%26entry.1292438233%3D%2520%2520Robustness%2520is%2520a%2520fundamental%2520aspect%2520for%2520developing%2520safe%2520and%2520trustworthy%250Amodels%252C%2520particularly%2520when%2520they%2520are%2520deployed%2520in%2520the%2520open%2520world.%2520In%2520this%2520work%2520we%250Aanalyze%2520the%2520inherent%2520capability%2520of%2520one-stage%2520object%2520detectors%2520to%2520robustly%250Aoperate%2520in%2520the%2520presence%2520of%2520out-of-distribution%2520%2528OoD%2529%2520data.%2520Specifically%252C%2520we%250Apropose%2520a%2520novel%2520detection%2520algorithm%2520for%2520detecting%2520unknown%2520objects%2520in%2520image%250Adata%252C%2520which%2520leverages%2520the%2520features%2520extracted%2520by%2520the%2520model%2520from%2520each%2520sample.%250ADifferently%2520from%2520other%2520recent%2520approaches%2520in%2520the%2520literature%252C%2520our%2520proposal%2520does%250Anot%2520require%2520retraining%2520the%2520object%2520detector%252C%2520thereby%2520allowing%2520for%2520the%2520use%2520of%250Apretrained%2520models.%2520Our%2520proposed%2520OoD%2520detector%2520exploits%2520the%2520application%2520of%250Asupervised%2520dimensionality%2520reduction%2520techniques%2520to%2520mitigate%2520the%2520effects%2520of%2520the%250Acurse%2520of%2520dimensionality%2520on%2520the%2520features%2520extracted%2520by%2520the%2520model.%2520Furthermore%252C%2520it%250Autilizes%2520high-resolution%2520feature%2520maps%2520to%2520identify%2520potential%2520unknown%2520objects%2520in%250Aan%2520unsupervised%2520fashion.%2520Our%2520experiments%2520analyze%2520the%2520Pareto%2520trade-off%2520between%250Athe%2520performance%2520detecting%2520known%2520and%2520unknown%2520objects%2520resulting%2520from%2520different%250Aalgorithmic%2520configurations%2520and%2520inference%2520confidence%2520thresholds.%2520We%2520also%2520compare%250Athe%2520performance%2520of%2520our%2520proposed%2520algorithm%2520to%2520that%2520of%2520logits-based%2520post-hoc%2520OoD%250Amethods%252C%2520as%2520well%2520as%2520possible%2520fusion%2520strategies.%2520Finally%252C%2520we%2520discuss%2520on%2520the%250Acompetitiveness%2520of%2520all%2520tested%2520methods%2520against%2520state-of-the-art%2520OoD%2520approaches%250Afor%2520object%2520detection%2520models%2520over%2520the%2520recently%2520published%2520Unknown%2520Object%250ADetection%2520benchmark.%2520The%2520obtained%2520results%2520verify%2520that%2520the%2520performance%2520of%250Aavant-garde%2520post-hoc%2520OoD%2520detectors%2520can%2520be%2520further%2520improved%2520when%2520combined%2520with%250Aour%2520proposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Inherent%20Robustness%20of%20One-Stage%20Object%20Detection%20against%0A%20%20Out-of-Distribution%20Data&entry.906535625=Aitor%20Martinez-Seras%20and%20Javier%20Del%20Ser%20and%20Alain%20Andres%20and%20Pablo%20Garcia-Bringas&entry.1292438233=%20%20Robustness%20is%20a%20fundamental%20aspect%20for%20developing%20safe%20and%20trustworthy%0Amodels%2C%20particularly%20when%20they%20are%20deployed%20in%20the%20open%20world.%20In%20this%20work%20we%0Aanalyze%20the%20inherent%20capability%20of%20one-stage%20object%20detectors%20to%20robustly%0Aoperate%20in%20the%20presence%20of%20out-of-distribution%20%28OoD%29%20data.%20Specifically%2C%20we%0Apropose%20a%20novel%20detection%20algorithm%20for%20detecting%20unknown%20objects%20in%20image%0Adata%2C%20which%20leverages%20the%20features%20extracted%20by%20the%20model%20from%20each%20sample.%0ADifferently%20from%20other%20recent%20approaches%20in%20the%20literature%2C%20our%20proposal%20does%0Anot%20require%20retraining%20the%20object%20detector%2C%20thereby%20allowing%20for%20the%20use%20of%0Apretrained%20models.%20Our%20proposed%20OoD%20detector%20exploits%20the%20application%20of%0Asupervised%20dimensionality%20reduction%20techniques%20to%20mitigate%20the%20effects%20of%20the%0Acurse%20of%20dimensionality%20on%20the%20features%20extracted%20by%20the%20model.%20Furthermore%2C%20it%0Autilizes%20high-resolution%20feature%20maps%20to%20identify%20potential%20unknown%20objects%20in%0Aan%20unsupervised%20fashion.%20Our%20experiments%20analyze%20the%20Pareto%20trade-off%20between%0Athe%20performance%20detecting%20known%20and%20unknown%20objects%20resulting%20from%20different%0Aalgorithmic%20configurations%20and%20inference%20confidence%20thresholds.%20We%20also%20compare%0Athe%20performance%20of%20our%20proposed%20algorithm%20to%20that%20of%20logits-based%20post-hoc%20OoD%0Amethods%2C%20as%20well%20as%20possible%20fusion%20strategies.%20Finally%2C%20we%20discuss%20on%20the%0Acompetitiveness%20of%20all%20tested%20methods%20against%20state-of-the-art%20OoD%20approaches%0Afor%20object%20detection%20models%20over%20the%20recently%20published%20Unknown%20Object%0ADetection%20benchmark.%20The%20obtained%20results%20verify%20that%20the%20performance%20of%0Aavant-garde%20post-hoc%20OoD%20detectors%20can%20be%20further%20improved%20when%20combined%20with%0Aour%20proposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04586v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


