<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241219.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SqueezeMe: Efficient Gaussian Avatars for VR", "author": "Shunsuke Saito and Stanislav Pidhorskyi and Igor Santesteban and Forrest Iandola and Divam Gupta and Anuj Pahuja and Nemanja Bartolovic and Frank Yu and Emanuel Garbin and Tomas Simon", "abstract": "  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.\n", "link": "http://arxiv.org/abs/2412.15171v1", "date": "2024-12-19", "relevancy": 3.4666, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7032}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7032}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SqueezeMe%3A%20Efficient%20Gaussian%20Avatars%20for%20VR&body=Title%3A%20SqueezeMe%3A%20Efficient%20Gaussian%20Avatars%20for%20VR%0AAuthor%3A%20Shunsuke%20Saito%20and%20Stanislav%20Pidhorskyi%20and%20Igor%20Santesteban%20and%20Forrest%20Iandola%20and%20Divam%20Gupta%20and%20Anuj%20Pahuja%20and%20Nemanja%20Bartolovic%20and%20Frank%20Yu%20and%20Emanuel%20Garbin%20and%20Tomas%20Simon%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20enabled%20real-time%203D%20human%20avatars%20with%20unprecedented%0Alevels%20of%20visual%20quality.%20While%20previous%20methods%20require%20a%20desktop%20GPU%20for%0Areal-time%20inference%20of%20a%20single%20avatar%2C%20we%20aim%20to%20squeeze%20multiple%20Gaussian%0Aavatars%20onto%20a%20portable%20virtual%20reality%20headset%20with%20real-time%20drivable%0Ainference.%20We%20begin%20by%20training%20a%20previous%20work%2C%20Animatable%20Gaussians%2C%20on%20a%0Ahigh%20quality%20dataset%20captured%20with%20512%20cameras.%20The%20Gaussians%20are%20animated%20by%0Acontrolling%20base%20set%20of%20Gaussians%20with%20linear%20blend%20skinning%20%28LBS%29%20motion%20and%0Athen%20further%20adjusting%20the%20Gaussians%20with%20a%20neural%20network%20decoder%20to%20correct%0Atheir%20appearance.%20When%20deploying%20the%20model%20on%20a%20Meta%20Quest%203%20VR%20headset%2C%20we%0Afind%20two%20major%20computational%20bottlenecks%3A%20the%20decoder%20and%20the%20rendering.%20To%0Aaccelerate%20the%20decoder%2C%20we%20train%20the%20Gaussians%20in%20UV-space%20instead%20of%0Apixel-space%2C%20and%20we%20distill%20the%20decoder%20to%20a%20single%20neural%20network%20layer.%0AFurther%2C%20we%20discover%20that%20neighborhoods%20of%20Gaussians%20can%20share%20a%20single%0Acorrective%20from%20the%20decoder%2C%20which%20provides%20an%20additional%20speedup.%20To%0Aaccelerate%20the%20rendering%2C%20we%20develop%20a%20custom%20pipeline%20in%20Vulkan%20that%20runs%20on%0Athe%20mobile%20GPU.%20Putting%20it%20all%20together%2C%20we%20run%203%20Gaussian%20avatars%20concurrently%0Aat%2072%20FPS%20on%20a%20VR%20headset.%20Demo%20videos%20are%20at%0Ahttps%3A//forresti.github.io/squeezeme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSqueezeMe%253A%2520Efficient%2520Gaussian%2520Avatars%2520for%2520VR%26entry.906535625%3DShunsuke%2520Saito%2520and%2520Stanislav%2520Pidhorskyi%2520and%2520Igor%2520Santesteban%2520and%2520Forrest%2520Iandola%2520and%2520Divam%2520Gupta%2520and%2520Anuj%2520Pahuja%2520and%2520Nemanja%2520Bartolovic%2520and%2520Frank%2520Yu%2520and%2520Emanuel%2520Garbin%2520and%2520Tomas%2520Simon%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520enabled%2520real-time%25203D%2520human%2520avatars%2520with%2520unprecedented%250Alevels%2520of%2520visual%2520quality.%2520While%2520previous%2520methods%2520require%2520a%2520desktop%2520GPU%2520for%250Areal-time%2520inference%2520of%2520a%2520single%2520avatar%252C%2520we%2520aim%2520to%2520squeeze%2520multiple%2520Gaussian%250Aavatars%2520onto%2520a%2520portable%2520virtual%2520reality%2520headset%2520with%2520real-time%2520drivable%250Ainference.%2520We%2520begin%2520by%2520training%2520a%2520previous%2520work%252C%2520Animatable%2520Gaussians%252C%2520on%2520a%250Ahigh%2520quality%2520dataset%2520captured%2520with%2520512%2520cameras.%2520The%2520Gaussians%2520are%2520animated%2520by%250Acontrolling%2520base%2520set%2520of%2520Gaussians%2520with%2520linear%2520blend%2520skinning%2520%2528LBS%2529%2520motion%2520and%250Athen%2520further%2520adjusting%2520the%2520Gaussians%2520with%2520a%2520neural%2520network%2520decoder%2520to%2520correct%250Atheir%2520appearance.%2520When%2520deploying%2520the%2520model%2520on%2520a%2520Meta%2520Quest%25203%2520VR%2520headset%252C%2520we%250Afind%2520two%2520major%2520computational%2520bottlenecks%253A%2520the%2520decoder%2520and%2520the%2520rendering.%2520To%250Aaccelerate%2520the%2520decoder%252C%2520we%2520train%2520the%2520Gaussians%2520in%2520UV-space%2520instead%2520of%250Apixel-space%252C%2520and%2520we%2520distill%2520the%2520decoder%2520to%2520a%2520single%2520neural%2520network%2520layer.%250AFurther%252C%2520we%2520discover%2520that%2520neighborhoods%2520of%2520Gaussians%2520can%2520share%2520a%2520single%250Acorrective%2520from%2520the%2520decoder%252C%2520which%2520provides%2520an%2520additional%2520speedup.%2520To%250Aaccelerate%2520the%2520rendering%252C%2520we%2520develop%2520a%2520custom%2520pipeline%2520in%2520Vulkan%2520that%2520runs%2520on%250Athe%2520mobile%2520GPU.%2520Putting%2520it%2520all%2520together%252C%2520we%2520run%25203%2520Gaussian%2520avatars%2520concurrently%250Aat%252072%2520FPS%2520on%2520a%2520VR%2520headset.%2520Demo%2520videos%2520are%2520at%250Ahttps%253A//forresti.github.io/squeezeme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SqueezeMe%3A%20Efficient%20Gaussian%20Avatars%20for%20VR&entry.906535625=Shunsuke%20Saito%20and%20Stanislav%20Pidhorskyi%20and%20Igor%20Santesteban%20and%20Forrest%20Iandola%20and%20Divam%20Gupta%20and%20Anuj%20Pahuja%20and%20Nemanja%20Bartolovic%20and%20Frank%20Yu%20and%20Emanuel%20Garbin%20and%20Tomas%20Simon&entry.1292438233=%20%20Gaussian%20Splatting%20has%20enabled%20real-time%203D%20human%20avatars%20with%20unprecedented%0Alevels%20of%20visual%20quality.%20While%20previous%20methods%20require%20a%20desktop%20GPU%20for%0Areal-time%20inference%20of%20a%20single%20avatar%2C%20we%20aim%20to%20squeeze%20multiple%20Gaussian%0Aavatars%20onto%20a%20portable%20virtual%20reality%20headset%20with%20real-time%20drivable%0Ainference.%20We%20begin%20by%20training%20a%20previous%20work%2C%20Animatable%20Gaussians%2C%20on%20a%0Ahigh%20quality%20dataset%20captured%20with%20512%20cameras.%20The%20Gaussians%20are%20animated%20by%0Acontrolling%20base%20set%20of%20Gaussians%20with%20linear%20blend%20skinning%20%28LBS%29%20motion%20and%0Athen%20further%20adjusting%20the%20Gaussians%20with%20a%20neural%20network%20decoder%20to%20correct%0Atheir%20appearance.%20When%20deploying%20the%20model%20on%20a%20Meta%20Quest%203%20VR%20headset%2C%20we%0Afind%20two%20major%20computational%20bottlenecks%3A%20the%20decoder%20and%20the%20rendering.%20To%0Aaccelerate%20the%20decoder%2C%20we%20train%20the%20Gaussians%20in%20UV-space%20instead%20of%0Apixel-space%2C%20and%20we%20distill%20the%20decoder%20to%20a%20single%20neural%20network%20layer.%0AFurther%2C%20we%20discover%20that%20neighborhoods%20of%20Gaussians%20can%20share%20a%20single%0Acorrective%20from%20the%20decoder%2C%20which%20provides%20an%20additional%20speedup.%20To%0Aaccelerate%20the%20rendering%2C%20we%20develop%20a%20custom%20pipeline%20in%20Vulkan%20that%20runs%20on%0Athe%20mobile%20GPU.%20Putting%20it%20all%20together%2C%20we%20run%203%20Gaussian%20avatars%20concurrently%0Aat%2072%20FPS%20on%20a%20VR%20headset.%20Demo%20videos%20are%20at%0Ahttps%3A//forresti.github.io/squeezeme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15171v1&entry.124074799=Read"},
{"title": "SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos", "author": "Yuzheng Liu and Siyan Dong and Shuzhe Wang and Yanchao Yang and Qingnan Fan and Baoquan Chen", "abstract": "  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM\nsystem for real-time and high-quality dense 3D reconstruction. SLAM3R provides\nan end-to-end solution by seamlessly integrating local 3D reconstruction and\nglobal coordinate registration through feed-forward neural networks. Given an\ninput video, the system first converts it into overlapping clips using a\nsliding window mechanism. Unlike traditional pose optimization-based methods,\nSLAM3R directly regresses 3D pointmaps from RGB images in each window and\nprogressively aligns and deforms these local pointmaps to create a globally\nconsistent scene reconstruction - all without explicitly solving any camera\nparameters. Experiments across datasets consistently show that SLAM3R achieves\nstate-of-the-art reconstruction accuracy and completeness while maintaining\nreal-time performance at 20+ FPS. Code and weights at:\nhttps://github.com/PKU-VCL-3DV/SLAM3R.\n", "link": "http://arxiv.org/abs/2412.09401v2", "date": "2024-12-19", "relevancy": 3.3191, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7656}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos&body=Title%3A%20SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos%0AAuthor%3A%20Yuzheng%20Liu%20and%20Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Yanchao%20Yang%20and%20Qingnan%20Fan%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20SLAM3R%2C%20a%20novel%20and%20effective%20monocular%20RGB%20SLAM%0Asystem%20for%20real-time%20and%20high-quality%20dense%203D%20reconstruction.%20SLAM3R%20provides%0Aan%20end-to-end%20solution%20by%20seamlessly%20integrating%20local%203D%20reconstruction%20and%0Aglobal%20coordinate%20registration%20through%20feed-forward%20neural%20networks.%20Given%20an%0Ainput%20video%2C%20the%20system%20first%20converts%20it%20into%20overlapping%20clips%20using%20a%0Asliding%20window%20mechanism.%20Unlike%20traditional%20pose%20optimization-based%20methods%2C%0ASLAM3R%20directly%20regresses%203D%20pointmaps%20from%20RGB%20images%20in%20each%20window%20and%0Aprogressively%20aligns%20and%20deforms%20these%20local%20pointmaps%20to%20create%20a%20globally%0Aconsistent%20scene%20reconstruction%20-%20all%20without%20explicitly%20solving%20any%20camera%0Aparameters.%20Experiments%20across%20datasets%20consistently%20show%20that%20SLAM3R%20achieves%0Astate-of-the-art%20reconstruction%20accuracy%20and%20completeness%20while%20maintaining%0Areal-time%20performance%20at%2020%2B%20FPS.%20Code%20and%20weights%20at%3A%0Ahttps%3A//github.com/PKU-VCL-3DV/SLAM3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09401v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM3R%253A%2520Real-Time%2520Dense%2520Scene%2520Reconstruction%2520from%2520Monocular%2520RGB%2520Videos%26entry.906535625%3DYuzheng%2520Liu%2520and%2520Siyan%2520Dong%2520and%2520Shuzhe%2520Wang%2520and%2520Yanchao%2520Yang%2520and%2520Qingnan%2520Fan%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SLAM3R%252C%2520a%2520novel%2520and%2520effective%2520monocular%2520RGB%2520SLAM%250Asystem%2520for%2520real-time%2520and%2520high-quality%2520dense%25203D%2520reconstruction.%2520SLAM3R%2520provides%250Aan%2520end-to-end%2520solution%2520by%2520seamlessly%2520integrating%2520local%25203D%2520reconstruction%2520and%250Aglobal%2520coordinate%2520registration%2520through%2520feed-forward%2520neural%2520networks.%2520Given%2520an%250Ainput%2520video%252C%2520the%2520system%2520first%2520converts%2520it%2520into%2520overlapping%2520clips%2520using%2520a%250Asliding%2520window%2520mechanism.%2520Unlike%2520traditional%2520pose%2520optimization-based%2520methods%252C%250ASLAM3R%2520directly%2520regresses%25203D%2520pointmaps%2520from%2520RGB%2520images%2520in%2520each%2520window%2520and%250Aprogressively%2520aligns%2520and%2520deforms%2520these%2520local%2520pointmaps%2520to%2520create%2520a%2520globally%250Aconsistent%2520scene%2520reconstruction%2520-%2520all%2520without%2520explicitly%2520solving%2520any%2520camera%250Aparameters.%2520Experiments%2520across%2520datasets%2520consistently%2520show%2520that%2520SLAM3R%2520achieves%250Astate-of-the-art%2520reconstruction%2520accuracy%2520and%2520completeness%2520while%2520maintaining%250Areal-time%2520performance%2520at%252020%252B%2520FPS.%2520Code%2520and%2520weights%2520at%253A%250Ahttps%253A//github.com/PKU-VCL-3DV/SLAM3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09401v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM3R%3A%20Real-Time%20Dense%20Scene%20Reconstruction%20from%20Monocular%20RGB%20Videos&entry.906535625=Yuzheng%20Liu%20and%20Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Yanchao%20Yang%20and%20Qingnan%20Fan%20and%20Baoquan%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20SLAM3R%2C%20a%20novel%20and%20effective%20monocular%20RGB%20SLAM%0Asystem%20for%20real-time%20and%20high-quality%20dense%203D%20reconstruction.%20SLAM3R%20provides%0Aan%20end-to-end%20solution%20by%20seamlessly%20integrating%20local%203D%20reconstruction%20and%0Aglobal%20coordinate%20registration%20through%20feed-forward%20neural%20networks.%20Given%20an%0Ainput%20video%2C%20the%20system%20first%20converts%20it%20into%20overlapping%20clips%20using%20a%0Asliding%20window%20mechanism.%20Unlike%20traditional%20pose%20optimization-based%20methods%2C%0ASLAM3R%20directly%20regresses%203D%20pointmaps%20from%20RGB%20images%20in%20each%20window%20and%0Aprogressively%20aligns%20and%20deforms%20these%20local%20pointmaps%20to%20create%20a%20globally%0Aconsistent%20scene%20reconstruction%20-%20all%20without%20explicitly%20solving%20any%20camera%0Aparameters.%20Experiments%20across%20datasets%20consistently%20show%20that%20SLAM3R%20achieves%0Astate-of-the-art%20reconstruction%20accuracy%20and%20completeness%20while%20maintaining%0Areal-time%20performance%20at%2020%2B%20FPS.%20Code%20and%20weights%20at%3A%0Ahttps%3A//github.com/PKU-VCL-3DV/SLAM3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09401v2&entry.124074799=Read"},
{"title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image", "author": "Yiyu Zhuang and Jiaxi Lv and Hao Wen and Qing Shuai and Ailing Zeng and Hao Zhu and Shifeng Chen and Yujiu Yang and Xun Cao and Wei Liu", "abstract": "  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n", "link": "http://arxiv.org/abs/2412.14963v1", "date": "2024-12-19", "relevancy": 3.3172, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7066}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6418}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDOL%3A%20Instant%20Photorealistic%203D%20Human%20Creation%20from%20a%20Single%20Image&body=Title%3A%20IDOL%3A%20Instant%20Photorealistic%203D%20Human%20Creation%20from%20a%20Single%20Image%0AAuthor%3A%20Yiyu%20Zhuang%20and%20Jiaxi%20Lv%20and%20Hao%20Wen%20and%20Qing%20Shuai%20and%20Ailing%20Zeng%20and%20Hao%20Zhu%20and%20Shifeng%20Chen%20and%20Yujiu%20Yang%20and%20Xun%20Cao%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Creating%20a%20high-fidelity%2C%20animatable%203D%20full-body%20avatar%20from%20a%20single%20image%0Ais%20a%20challenging%20task%20due%20to%20the%20diverse%20appearance%20and%20poses%20of%20humans%20and%20the%0Alimited%20availability%20of%20high-quality%20training%20data.%20To%20achieve%20fast%20and%0Ahigh-quality%20human%20reconstruction%2C%20this%20work%20rethinks%20the%20task%20from%20the%0Aperspectives%20of%20dataset%2C%20model%2C%20and%20representation.%20First%2C%20we%20introduce%20a%0Alarge-scale%20HUman-centric%20GEnerated%20dataset%2C%20HuGe100K%2C%20consisting%20of%20100K%0Adiverse%2C%20photorealistic%20sets%20of%20human%20images.%20Each%20set%20contains%2024-view%20frames%0Ain%20specific%20human%20poses%2C%20generated%20using%20a%20pose-controllable%0Aimage-to-multi-view%20model.%20Next%2C%20leveraging%20the%20diversity%20in%20views%2C%20poses%2C%20and%0Aappearances%20within%20HuGe100K%2C%20we%20develop%20a%20scalable%20feed-forward%20transformer%0Amodel%20to%20predict%20a%203D%20human%20Gaussian%20representation%20in%20a%20uniform%20space%20from%20a%0Agiven%20human%20image.%20This%20model%20is%20trained%20to%20disentangle%20human%20pose%2C%20body%20shape%2C%0Aclothing%20geometry%2C%20and%20texture.%20The%20estimated%20Gaussians%20can%20be%20animated%20without%0Apost-processing.%20We%20conduct%20comprehensive%20experiments%20to%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20dataset%20and%20method.%20Our%20model%20demonstrates%20the%0Aability%20to%20efficiently%20reconstruct%20photorealistic%20humans%20at%201K%20resolution%20from%0Aa%20single%20input%20image%20using%20a%20single%20GPU%20instantly.%20Additionally%2C%20it%20seamlessly%0Asupports%20various%20applications%2C%20as%20well%20as%20shape%20and%20texture%20editing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDOL%253A%2520Instant%2520Photorealistic%25203D%2520Human%2520Creation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYiyu%2520Zhuang%2520and%2520Jiaxi%2520Lv%2520and%2520Hao%2520Wen%2520and%2520Qing%2520Shuai%2520and%2520Ailing%2520Zeng%2520and%2520Hao%2520Zhu%2520and%2520Shifeng%2520Chen%2520and%2520Yujiu%2520Yang%2520and%2520Xun%2520Cao%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Creating%2520a%2520high-fidelity%252C%2520animatable%25203D%2520full-body%2520avatar%2520from%2520a%2520single%2520image%250Ais%2520a%2520challenging%2520task%2520due%2520to%2520the%2520diverse%2520appearance%2520and%2520poses%2520of%2520humans%2520and%2520the%250Alimited%2520availability%2520of%2520high-quality%2520training%2520data.%2520To%2520achieve%2520fast%2520and%250Ahigh-quality%2520human%2520reconstruction%252C%2520this%2520work%2520rethinks%2520the%2520task%2520from%2520the%250Aperspectives%2520of%2520dataset%252C%2520model%252C%2520and%2520representation.%2520First%252C%2520we%2520introduce%2520a%250Alarge-scale%2520HUman-centric%2520GEnerated%2520dataset%252C%2520HuGe100K%252C%2520consisting%2520of%2520100K%250Adiverse%252C%2520photorealistic%2520sets%2520of%2520human%2520images.%2520Each%2520set%2520contains%252024-view%2520frames%250Ain%2520specific%2520human%2520poses%252C%2520generated%2520using%2520a%2520pose-controllable%250Aimage-to-multi-view%2520model.%2520Next%252C%2520leveraging%2520the%2520diversity%2520in%2520views%252C%2520poses%252C%2520and%250Aappearances%2520within%2520HuGe100K%252C%2520we%2520develop%2520a%2520scalable%2520feed-forward%2520transformer%250Amodel%2520to%2520predict%2520a%25203D%2520human%2520Gaussian%2520representation%2520in%2520a%2520uniform%2520space%2520from%2520a%250Agiven%2520human%2520image.%2520This%2520model%2520is%2520trained%2520to%2520disentangle%2520human%2520pose%252C%2520body%2520shape%252C%250Aclothing%2520geometry%252C%2520and%2520texture.%2520The%2520estimated%2520Gaussians%2520can%2520be%2520animated%2520without%250Apost-processing.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520dataset%2520and%2520method.%2520Our%2520model%2520demonstrates%2520the%250Aability%2520to%2520efficiently%2520reconstruct%2520photorealistic%2520humans%2520at%25201K%2520resolution%2520from%250Aa%2520single%2520input%2520image%2520using%2520a%2520single%2520GPU%2520instantly.%2520Additionally%252C%2520it%2520seamlessly%250Asupports%2520various%2520applications%252C%2520as%2520well%2520as%2520shape%2520and%2520texture%2520editing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDOL%3A%20Instant%20Photorealistic%203D%20Human%20Creation%20from%20a%20Single%20Image&entry.906535625=Yiyu%20Zhuang%20and%20Jiaxi%20Lv%20and%20Hao%20Wen%20and%20Qing%20Shuai%20and%20Ailing%20Zeng%20and%20Hao%20Zhu%20and%20Shifeng%20Chen%20and%20Yujiu%20Yang%20and%20Xun%20Cao%20and%20Wei%20Liu&entry.1292438233=%20%20Creating%20a%20high-fidelity%2C%20animatable%203D%20full-body%20avatar%20from%20a%20single%20image%0Ais%20a%20challenging%20task%20due%20to%20the%20diverse%20appearance%20and%20poses%20of%20humans%20and%20the%0Alimited%20availability%20of%20high-quality%20training%20data.%20To%20achieve%20fast%20and%0Ahigh-quality%20human%20reconstruction%2C%20this%20work%20rethinks%20the%20task%20from%20the%0Aperspectives%20of%20dataset%2C%20model%2C%20and%20representation.%20First%2C%20we%20introduce%20a%0Alarge-scale%20HUman-centric%20GEnerated%20dataset%2C%20HuGe100K%2C%20consisting%20of%20100K%0Adiverse%2C%20photorealistic%20sets%20of%20human%20images.%20Each%20set%20contains%2024-view%20frames%0Ain%20specific%20human%20poses%2C%20generated%20using%20a%20pose-controllable%0Aimage-to-multi-view%20model.%20Next%2C%20leveraging%20the%20diversity%20in%20views%2C%20poses%2C%20and%0Aappearances%20within%20HuGe100K%2C%20we%20develop%20a%20scalable%20feed-forward%20transformer%0Amodel%20to%20predict%20a%203D%20human%20Gaussian%20representation%20in%20a%20uniform%20space%20from%20a%0Agiven%20human%20image.%20This%20model%20is%20trained%20to%20disentangle%20human%20pose%2C%20body%20shape%2C%0Aclothing%20geometry%2C%20and%20texture.%20The%20estimated%20Gaussians%20can%20be%20animated%20without%0Apost-processing.%20We%20conduct%20comprehensive%20experiments%20to%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20dataset%20and%20method.%20Our%20model%20demonstrates%20the%0Aability%20to%20efficiently%20reconstruct%20photorealistic%20humans%20at%201K%20resolution%20from%0Aa%20single%20input%20image%20using%20a%20single%20GPU%20instantly.%20Additionally%2C%20it%20seamlessly%0Asupports%20various%20applications%2C%20as%20well%20as%20shape%20and%20texture%20editing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14963v1&entry.124074799=Read"},
{"title": "EnvGS: Modeling View-Dependent Appearance with Environment Gaussian", "author": "Tao Xie and Xi Chen and Zhen Xu and Yiman Xie and Yudong Jin and Yujun Shen and Sida Peng and Hujun Bao and Xiaowei Zhou", "abstract": "  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis.\n", "link": "http://arxiv.org/abs/2412.15215v1", "date": "2024-12-19", "relevancy": 3.177, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnvGS%3A%20Modeling%20View-Dependent%20Appearance%20with%20Environment%20Gaussian&body=Title%3A%20EnvGS%3A%20Modeling%20View-Dependent%20Appearance%20with%20Environment%20Gaussian%0AAuthor%3A%20Tao%20Xie%20and%20Xi%20Chen%20and%20Zhen%20Xu%20and%20Yiman%20Xie%20and%20Yudong%20Jin%20and%20Yujun%20Shen%20and%20Sida%20Peng%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20Reconstructing%20complex%20reflections%20in%20real-world%20scenes%20from%202D%20images%20is%0Aessential%20for%20achieving%20photorealistic%20novel%20view%20synthesis.%20Existing%20methods%0Athat%20utilize%20environment%20maps%20to%20model%20reflections%20from%20distant%20lighting%20often%0Astruggle%20with%20high-frequency%20reflection%20details%20and%20fail%20to%20account%20for%0Anear-field%20reflections.%20In%20this%20work%2C%20we%20introduce%20EnvGS%2C%20a%20novel%20approach%20that%0Aemploys%20a%20set%20of%20Gaussian%20primitives%20as%20an%20explicit%203D%20representation%20for%0Acapturing%20reflections%20of%20environments.%20These%20environment%20Gaussian%20primitives%0Aare%20incorporated%20with%20base%20Gaussian%20primitives%20to%20model%20the%20appearance%20of%20the%0Awhole%20scene.%20To%20efficiently%20render%20these%20environment%20Gaussian%20primitives%2C%20we%0Adeveloped%20a%20ray-tracing-based%20renderer%20that%20leverages%20the%20GPU%27s%20RT%20core%20for%0Afast%20rendering.%20This%20allows%20us%20to%20jointly%20optimize%20our%20model%20for%20high-quality%0Areconstruction%20while%20maintaining%20real-time%20rendering%20speeds.%20Results%20from%0Amultiple%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20our%20method%20produces%0Asignificantly%20more%20detailed%20reflections%2C%20achieving%20the%20best%20rendering%20quality%0Ain%20real-time%20novel%20view%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvGS%253A%2520Modeling%2520View-Dependent%2520Appearance%2520with%2520Environment%2520Gaussian%26entry.906535625%3DTao%2520Xie%2520and%2520Xi%2520Chen%2520and%2520Zhen%2520Xu%2520and%2520Yiman%2520Xie%2520and%2520Yudong%2520Jin%2520and%2520Yujun%2520Shen%2520and%2520Sida%2520Peng%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520Reconstructing%2520complex%2520reflections%2520in%2520real-world%2520scenes%2520from%25202D%2520images%2520is%250Aessential%2520for%2520achieving%2520photorealistic%2520novel%2520view%2520synthesis.%2520Existing%2520methods%250Athat%2520utilize%2520environment%2520maps%2520to%2520model%2520reflections%2520from%2520distant%2520lighting%2520often%250Astruggle%2520with%2520high-frequency%2520reflection%2520details%2520and%2520fail%2520to%2520account%2520for%250Anear-field%2520reflections.%2520In%2520this%2520work%252C%2520we%2520introduce%2520EnvGS%252C%2520a%2520novel%2520approach%2520that%250Aemploys%2520a%2520set%2520of%2520Gaussian%2520primitives%2520as%2520an%2520explicit%25203D%2520representation%2520for%250Acapturing%2520reflections%2520of%2520environments.%2520These%2520environment%2520Gaussian%2520primitives%250Aare%2520incorporated%2520with%2520base%2520Gaussian%2520primitives%2520to%2520model%2520the%2520appearance%2520of%2520the%250Awhole%2520scene.%2520To%2520efficiently%2520render%2520these%2520environment%2520Gaussian%2520primitives%252C%2520we%250Adeveloped%2520a%2520ray-tracing-based%2520renderer%2520that%2520leverages%2520the%2520GPU%2527s%2520RT%2520core%2520for%250Afast%2520rendering.%2520This%2520allows%2520us%2520to%2520jointly%2520optimize%2520our%2520model%2520for%2520high-quality%250Areconstruction%2520while%2520maintaining%2520real-time%2520rendering%2520speeds.%2520Results%2520from%250Amultiple%2520real-world%2520and%2520synthetic%2520datasets%2520demonstrate%2520that%2520our%2520method%2520produces%250Asignificantly%2520more%2520detailed%2520reflections%252C%2520achieving%2520the%2520best%2520rendering%2520quality%250Ain%2520real-time%2520novel%2520view%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnvGS%3A%20Modeling%20View-Dependent%20Appearance%20with%20Environment%20Gaussian&entry.906535625=Tao%20Xie%20and%20Xi%20Chen%20and%20Zhen%20Xu%20and%20Yiman%20Xie%20and%20Yudong%20Jin%20and%20Yujun%20Shen%20and%20Sida%20Peng%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20Reconstructing%20complex%20reflections%20in%20real-world%20scenes%20from%202D%20images%20is%0Aessential%20for%20achieving%20photorealistic%20novel%20view%20synthesis.%20Existing%20methods%0Athat%20utilize%20environment%20maps%20to%20model%20reflections%20from%20distant%20lighting%20often%0Astruggle%20with%20high-frequency%20reflection%20details%20and%20fail%20to%20account%20for%0Anear-field%20reflections.%20In%20this%20work%2C%20we%20introduce%20EnvGS%2C%20a%20novel%20approach%20that%0Aemploys%20a%20set%20of%20Gaussian%20primitives%20as%20an%20explicit%203D%20representation%20for%0Acapturing%20reflections%20of%20environments.%20These%20environment%20Gaussian%20primitives%0Aare%20incorporated%20with%20base%20Gaussian%20primitives%20to%20model%20the%20appearance%20of%20the%0Awhole%20scene.%20To%20efficiently%20render%20these%20environment%20Gaussian%20primitives%2C%20we%0Adeveloped%20a%20ray-tracing-based%20renderer%20that%20leverages%20the%20GPU%27s%20RT%20core%20for%0Afast%20rendering.%20This%20allows%20us%20to%20jointly%20optimize%20our%20model%20for%20high-quality%0Areconstruction%20while%20maintaining%20real-time%20rendering%20speeds.%20Results%20from%0Amultiple%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20our%20method%20produces%0Asignificantly%20more%20detailed%20reflections%2C%20achieving%20the%20best%20rendering%20quality%0Ain%20real-time%20novel%20view%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15215v1&entry.124074799=Read"},
{"title": "Tiled Diffusion", "author": "Or Madar and Ohad Fried", "abstract": "  Image tiling -- the seamless connection of disparate images to create a\ncoherent visual field -- is crucial for applications such as texture creation,\nvideo game asset development, and digital art. Traditionally, tiles have been\nconstructed manually, a method that poses significant limitations in\nscalability and flexibility. Recent research has attempted to automate this\nprocess using generative models. However, current approaches primarily focus on\ntiling textures and manipulating models for single-image generation, without\ninherently supporting the creation of multiple interconnected tiles across\ndiverse domains. This paper presents Tiled Diffusion, a novel approach that\nextends the capabilities of diffusion models to accommodate the generation of\ncohesive tiling patterns across various domains of image synthesis that require\ntiling. Our method supports a wide range of tiling scenarios, from self-tiling\nto complex many-to-many connections, enabling seamless integration of multiple\nimages. Tiled Diffusion automates the tiling process, eliminating the need for\nmanual intervention and enhancing creative possibilities in various\napplications, such as seamlessly tiling of existing images, tiled texture\ncreation, and 360{\\deg} synthesis.\n", "link": "http://arxiv.org/abs/2412.15185v1", "date": "2024-12-19", "relevancy": 3.1064, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6078}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiled%20Diffusion&body=Title%3A%20Tiled%20Diffusion%0AAuthor%3A%20Or%20Madar%20and%20Ohad%20Fried%0AAbstract%3A%20%20%20Image%20tiling%20--%20the%20seamless%20connection%20of%20disparate%20images%20to%20create%20a%0Acoherent%20visual%20field%20--%20is%20crucial%20for%20applications%20such%20as%20texture%20creation%2C%0Avideo%20game%20asset%20development%2C%20and%20digital%20art.%20Traditionally%2C%20tiles%20have%20been%0Aconstructed%20manually%2C%20a%20method%20that%20poses%20significant%20limitations%20in%0Ascalability%20and%20flexibility.%20Recent%20research%20has%20attempted%20to%20automate%20this%0Aprocess%20using%20generative%20models.%20However%2C%20current%20approaches%20primarily%20focus%20on%0Atiling%20textures%20and%20manipulating%20models%20for%20single-image%20generation%2C%20without%0Ainherently%20supporting%20the%20creation%20of%20multiple%20interconnected%20tiles%20across%0Adiverse%20domains.%20This%20paper%20presents%20Tiled%20Diffusion%2C%20a%20novel%20approach%20that%0Aextends%20the%20capabilities%20of%20diffusion%20models%20to%20accommodate%20the%20generation%20of%0Acohesive%20tiling%20patterns%20across%20various%20domains%20of%20image%20synthesis%20that%20require%0Atiling.%20Our%20method%20supports%20a%20wide%20range%20of%20tiling%20scenarios%2C%20from%20self-tiling%0Ato%20complex%20many-to-many%20connections%2C%20enabling%20seamless%20integration%20of%20multiple%0Aimages.%20Tiled%20Diffusion%20automates%20the%20tiling%20process%2C%20eliminating%20the%20need%20for%0Amanual%20intervention%20and%20enhancing%20creative%20possibilities%20in%20various%0Aapplications%2C%20such%20as%20seamlessly%20tiling%20of%20existing%20images%2C%20tiled%20texture%0Acreation%2C%20and%20360%7B%5Cdeg%7D%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiled%2520Diffusion%26entry.906535625%3DOr%2520Madar%2520and%2520Ohad%2520Fried%26entry.1292438233%3D%2520%2520Image%2520tiling%2520--%2520the%2520seamless%2520connection%2520of%2520disparate%2520images%2520to%2520create%2520a%250Acoherent%2520visual%2520field%2520--%2520is%2520crucial%2520for%2520applications%2520such%2520as%2520texture%2520creation%252C%250Avideo%2520game%2520asset%2520development%252C%2520and%2520digital%2520art.%2520Traditionally%252C%2520tiles%2520have%2520been%250Aconstructed%2520manually%252C%2520a%2520method%2520that%2520poses%2520significant%2520limitations%2520in%250Ascalability%2520and%2520flexibility.%2520Recent%2520research%2520has%2520attempted%2520to%2520automate%2520this%250Aprocess%2520using%2520generative%2520models.%2520However%252C%2520current%2520approaches%2520primarily%2520focus%2520on%250Atiling%2520textures%2520and%2520manipulating%2520models%2520for%2520single-image%2520generation%252C%2520without%250Ainherently%2520supporting%2520the%2520creation%2520of%2520multiple%2520interconnected%2520tiles%2520across%250Adiverse%2520domains.%2520This%2520paper%2520presents%2520Tiled%2520Diffusion%252C%2520a%2520novel%2520approach%2520that%250Aextends%2520the%2520capabilities%2520of%2520diffusion%2520models%2520to%2520accommodate%2520the%2520generation%2520of%250Acohesive%2520tiling%2520patterns%2520across%2520various%2520domains%2520of%2520image%2520synthesis%2520that%2520require%250Atiling.%2520Our%2520method%2520supports%2520a%2520wide%2520range%2520of%2520tiling%2520scenarios%252C%2520from%2520self-tiling%250Ato%2520complex%2520many-to-many%2520connections%252C%2520enabling%2520seamless%2520integration%2520of%2520multiple%250Aimages.%2520Tiled%2520Diffusion%2520automates%2520the%2520tiling%2520process%252C%2520eliminating%2520the%2520need%2520for%250Amanual%2520intervention%2520and%2520enhancing%2520creative%2520possibilities%2520in%2520various%250Aapplications%252C%2520such%2520as%2520seamlessly%2520tiling%2520of%2520existing%2520images%252C%2520tiled%2520texture%250Acreation%252C%2520and%2520360%257B%255Cdeg%257D%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiled%20Diffusion&entry.906535625=Or%20Madar%20and%20Ohad%20Fried&entry.1292438233=%20%20Image%20tiling%20--%20the%20seamless%20connection%20of%20disparate%20images%20to%20create%20a%0Acoherent%20visual%20field%20--%20is%20crucial%20for%20applications%20such%20as%20texture%20creation%2C%0Avideo%20game%20asset%20development%2C%20and%20digital%20art.%20Traditionally%2C%20tiles%20have%20been%0Aconstructed%20manually%2C%20a%20method%20that%20poses%20significant%20limitations%20in%0Ascalability%20and%20flexibility.%20Recent%20research%20has%20attempted%20to%20automate%20this%0Aprocess%20using%20generative%20models.%20However%2C%20current%20approaches%20primarily%20focus%20on%0Atiling%20textures%20and%20manipulating%20models%20for%20single-image%20generation%2C%20without%0Ainherently%20supporting%20the%20creation%20of%20multiple%20interconnected%20tiles%20across%0Adiverse%20domains.%20This%20paper%20presents%20Tiled%20Diffusion%2C%20a%20novel%20approach%20that%0Aextends%20the%20capabilities%20of%20diffusion%20models%20to%20accommodate%20the%20generation%20of%0Acohesive%20tiling%20patterns%20across%20various%20domains%20of%20image%20synthesis%20that%20require%0Atiling.%20Our%20method%20supports%20a%20wide%20range%20of%20tiling%20scenarios%2C%20from%20self-tiling%0Ato%20complex%20many-to-many%20connections%2C%20enabling%20seamless%20integration%20of%20multiple%0Aimages.%20Tiled%20Diffusion%20automates%20the%20tiling%20process%2C%20eliminating%20the%20need%20for%0Amanual%20intervention%20and%20enhancing%20creative%20possibilities%20in%20various%0Aapplications%2C%20such%20as%20seamlessly%20tiling%20of%20existing%20images%2C%20tiled%20texture%0Acreation%2C%20and%20360%7B%5Cdeg%7D%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15185v1&entry.124074799=Read"},
{"title": "LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation", "author": "Chenxu Zhou and Lvchang Fu and Sida Peng and Yunzhi Yan and Zhanhua Zhang and Yong Chen and Jiazhi Xia and Xiaowei Zhou", "abstract": "  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n", "link": "http://arxiv.org/abs/2412.15199v1", "date": "2024-12-19", "relevancy": 3.1053, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7061}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5957}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-RT%3A%20Gaussian-based%20Ray%20Tracing%20for%20Dynamic%20LiDAR%20Re-simulation&body=Title%3A%20LiDAR-RT%3A%20Gaussian-based%20Ray%20Tracing%20for%20Dynamic%20LiDAR%20Re-simulation%0AAuthor%3A%20Chenxu%20Zhou%20and%20Lvchang%20Fu%20and%20Sida%20Peng%20and%20Yunzhi%20Yan%20and%20Zhanhua%20Zhang%20and%20Yong%20Chen%20and%20Jiazhi%20Xia%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20targets%20the%20challenge%20of%20real-time%20LiDAR%20re-simulation%20in%20dynamic%0Adriving%20scenarios.%20Recent%20approaches%20utilize%20neural%20radiance%20fields%20combined%0Awith%20the%20physical%20modeling%20of%20LiDAR%20sensors%20to%20achieve%20high-fidelity%0Are-simulation%20results.%20Unfortunately%2C%20these%20methods%20face%20limitations%20due%20to%0Ahigh%20computational%20demands%20in%20large-scale%20scenes%20and%20cannot%20perform%20real-time%0ALiDAR%20rendering.%20To%20overcome%20these%20constraints%2C%20we%20propose%20LiDAR-RT%2C%20a%20novel%0Aframework%20that%20supports%20real-time%2C%20physically%20accurate%20LiDAR%20re-simulation%20for%0Adriving%20scenes.%20Our%20primary%20contribution%20is%20the%20development%20of%20an%20efficient%20and%0Aeffective%20rendering%20pipeline%2C%20which%20integrates%20Gaussian%20primitives%20and%0Ahardware-accelerated%20ray%20tracing%20technology.%20Specifically%2C%20we%20model%20the%0Aphysical%20properties%20of%20LiDAR%20sensors%20using%20Gaussian%20primitives%20with%20learnable%0Aparameters%20and%20incorporate%20scene%20graphs%20to%20handle%20scene%20dynamics.%20Building%20upon%0Athis%20scene%20representation%2C%20our%20framework%20first%20constructs%20a%20bounding%20volume%0Ahierarchy%20%28BVH%29%2C%20then%20casts%20rays%20for%20each%20pixel%20and%20generates%20novel%20LiDAR%20views%0Athrough%20a%20differentiable%20rendering%20algorithm.%20Importantly%2C%20our%20framework%0Asupports%20realistic%20rendering%20with%20flexible%20scene%20editing%20operations%20and%20various%0Asensor%20configurations.%20Extensive%20experiments%20across%20multiple%20public%20benchmarks%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%0Arendering%20quality%20and%20efficiency.%20Our%20project%20page%20is%20at%0Ahttps%3A//zju3dv.github.io/lidar-rt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-RT%253A%2520Gaussian-based%2520Ray%2520Tracing%2520for%2520Dynamic%2520LiDAR%2520Re-simulation%26entry.906535625%3DChenxu%2520Zhou%2520and%2520Lvchang%2520Fu%2520and%2520Sida%2520Peng%2520and%2520Yunzhi%2520Yan%2520and%2520Zhanhua%2520Zhang%2520and%2520Yong%2520Chen%2520and%2520Jiazhi%2520Xia%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520targets%2520the%2520challenge%2520of%2520real-time%2520LiDAR%2520re-simulation%2520in%2520dynamic%250Adriving%2520scenarios.%2520Recent%2520approaches%2520utilize%2520neural%2520radiance%2520fields%2520combined%250Awith%2520the%2520physical%2520modeling%2520of%2520LiDAR%2520sensors%2520to%2520achieve%2520high-fidelity%250Are-simulation%2520results.%2520Unfortunately%252C%2520these%2520methods%2520face%2520limitations%2520due%2520to%250Ahigh%2520computational%2520demands%2520in%2520large-scale%2520scenes%2520and%2520cannot%2520perform%2520real-time%250ALiDAR%2520rendering.%2520To%2520overcome%2520these%2520constraints%252C%2520we%2520propose%2520LiDAR-RT%252C%2520a%2520novel%250Aframework%2520that%2520supports%2520real-time%252C%2520physically%2520accurate%2520LiDAR%2520re-simulation%2520for%250Adriving%2520scenes.%2520Our%2520primary%2520contribution%2520is%2520the%2520development%2520of%2520an%2520efficient%2520and%250Aeffective%2520rendering%2520pipeline%252C%2520which%2520integrates%2520Gaussian%2520primitives%2520and%250Ahardware-accelerated%2520ray%2520tracing%2520technology.%2520Specifically%252C%2520we%2520model%2520the%250Aphysical%2520properties%2520of%2520LiDAR%2520sensors%2520using%2520Gaussian%2520primitives%2520with%2520learnable%250Aparameters%2520and%2520incorporate%2520scene%2520graphs%2520to%2520handle%2520scene%2520dynamics.%2520Building%2520upon%250Athis%2520scene%2520representation%252C%2520our%2520framework%2520first%2520constructs%2520a%2520bounding%2520volume%250Ahierarchy%2520%2528BVH%2529%252C%2520then%2520casts%2520rays%2520for%2520each%2520pixel%2520and%2520generates%2520novel%2520LiDAR%2520views%250Athrough%2520a%2520differentiable%2520rendering%2520algorithm.%2520Importantly%252C%2520our%2520framework%250Asupports%2520realistic%2520rendering%2520with%2520flexible%2520scene%2520editing%2520operations%2520and%2520various%250Asensor%2520configurations.%2520Extensive%2520experiments%2520across%2520multiple%2520public%2520benchmarks%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%250Arendering%2520quality%2520and%2520efficiency.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//zju3dv.github.io/lidar-rt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-RT%3A%20Gaussian-based%20Ray%20Tracing%20for%20Dynamic%20LiDAR%20Re-simulation&entry.906535625=Chenxu%20Zhou%20and%20Lvchang%20Fu%20and%20Sida%20Peng%20and%20Yunzhi%20Yan%20and%20Zhanhua%20Zhang%20and%20Yong%20Chen%20and%20Jiazhi%20Xia%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20targets%20the%20challenge%20of%20real-time%20LiDAR%20re-simulation%20in%20dynamic%0Adriving%20scenarios.%20Recent%20approaches%20utilize%20neural%20radiance%20fields%20combined%0Awith%20the%20physical%20modeling%20of%20LiDAR%20sensors%20to%20achieve%20high-fidelity%0Are-simulation%20results.%20Unfortunately%2C%20these%20methods%20face%20limitations%20due%20to%0Ahigh%20computational%20demands%20in%20large-scale%20scenes%20and%20cannot%20perform%20real-time%0ALiDAR%20rendering.%20To%20overcome%20these%20constraints%2C%20we%20propose%20LiDAR-RT%2C%20a%20novel%0Aframework%20that%20supports%20real-time%2C%20physically%20accurate%20LiDAR%20re-simulation%20for%0Adriving%20scenes.%20Our%20primary%20contribution%20is%20the%20development%20of%20an%20efficient%20and%0Aeffective%20rendering%20pipeline%2C%20which%20integrates%20Gaussian%20primitives%20and%0Ahardware-accelerated%20ray%20tracing%20technology.%20Specifically%2C%20we%20model%20the%0Aphysical%20properties%20of%20LiDAR%20sensors%20using%20Gaussian%20primitives%20with%20learnable%0Aparameters%20and%20incorporate%20scene%20graphs%20to%20handle%20scene%20dynamics.%20Building%20upon%0Athis%20scene%20representation%2C%20our%20framework%20first%20constructs%20a%20bounding%20volume%0Ahierarchy%20%28BVH%29%2C%20then%20casts%20rays%20for%20each%20pixel%20and%20generates%20novel%20LiDAR%20views%0Athrough%20a%20differentiable%20rendering%20algorithm.%20Importantly%2C%20our%20framework%0Asupports%20realistic%20rendering%20with%20flexible%20scene%20editing%20operations%20and%20various%0Asensor%20configurations.%20Extensive%20experiments%20across%20multiple%20public%20benchmarks%0Ademonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%20of%0Arendering%20quality%20and%20efficiency.%20Our%20project%20page%20is%20at%0Ahttps%3A//zju3dv.github.io/lidar-rt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15199v1&entry.124074799=Read"},
{"title": "Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization", "author": "Zhongwei Chen and Zhao-Xu Yang and Hai-Jun Rong", "abstract": "  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. The codes will be released soon.\n", "link": "http://arxiv.org/abs/2412.14819v1", "date": "2024-12-19", "relevancy": 3.0727, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Level%20Embedding%20and%20Alignment%20Network%20with%20Consistency%20and%0A%20%20Invariance%20Learning%20for%20Cross-View%20Geo-Localization&body=Title%3A%20Multi-Level%20Embedding%20and%20Alignment%20Network%20with%20Consistency%20and%0A%20%20Invariance%20Learning%20for%20Cross-View%20Geo-Localization%0AAuthor%3A%20Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong%0AAbstract%3A%20%20%20Cross-View%20Geo-Localization%20%28CVGL%29%20involves%20determining%20the%20localization%20of%0Adrone%20images%20by%20retrieving%20the%20most%20similar%20GPS-tagged%20satellite%20images.%0AHowever%2C%20the%20imaging%20gaps%20between%20platforms%20are%20often%20significant%20and%20the%0Avariations%20in%20viewpoints%20are%20substantial%2C%20which%20limits%20the%20ability%20of%20existing%0Amethods%20to%20effectively%20associate%20cross-view%20features%20and%20extract%20consistent%20and%0Ainvariant%20characteristics.%20Moreover%2C%20existing%20methods%20often%20overlook%20the%0Aproblem%20of%20increased%20computational%20and%20storage%20requirements%20when%20improving%0Amodel%20performance.%20To%20handle%20these%20limitations%2C%20we%20propose%20a%20lightweight%0Aenhanced%20alignment%20network%2C%20called%20the%20Multi-Level%20Embedding%20and%20Alignment%0ANetwork%20%28MEAN%29.%20The%20MEAN%20network%20uses%20a%20progressive%20multi-level%20enhancement%0Astrategy%2C%20global-to-local%20associations%2C%20and%20cross-domain%20alignment%2C%20enabling%0Afeature%20communication%20across%20levels.%20This%20allows%20MEAN%20to%20effectively%20connect%0Afeatures%20at%20different%20levels%20and%20learn%20robust%20cross-view%20consistent%20mappings%0Aand%20modality-invariant%20features.%20Moreover%2C%20MEAN%20adopts%20a%20shallow%20backbone%0Anetwork%20combined%20with%20a%20lightweight%20branch%20design%2C%20effectively%20reducing%0Aparameter%20count%20and%20computational%20complexity.%20Experimental%20results%20on%20the%0AUniversity-1652%20and%20SUES-200%20datasets%20demonstrate%20that%20MEAN%20reduces%20parameter%0Acount%20by%2062.17%25%20and%20computational%20complexity%20by%2070.99%25%20compared%20to%0Astate-of-the-art%20models%2C%20while%20maintaining%20competitive%20or%20even%20superior%0Aperformance.%20The%20codes%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Level%2520Embedding%2520and%2520Alignment%2520Network%2520with%2520Consistency%2520and%250A%2520%2520Invariance%2520Learning%2520for%2520Cross-View%2520Geo-Localization%26entry.906535625%3DZhongwei%2520Chen%2520and%2520Zhao-Xu%2520Yang%2520and%2520Hai-Jun%2520Rong%26entry.1292438233%3D%2520%2520Cross-View%2520Geo-Localization%2520%2528CVGL%2529%2520involves%2520determining%2520the%2520localization%2520of%250Adrone%2520images%2520by%2520retrieving%2520the%2520most%2520similar%2520GPS-tagged%2520satellite%2520images.%250AHowever%252C%2520the%2520imaging%2520gaps%2520between%2520platforms%2520are%2520often%2520significant%2520and%2520the%250Avariations%2520in%2520viewpoints%2520are%2520substantial%252C%2520which%2520limits%2520the%2520ability%2520of%2520existing%250Amethods%2520to%2520effectively%2520associate%2520cross-view%2520features%2520and%2520extract%2520consistent%2520and%250Ainvariant%2520characteristics.%2520Moreover%252C%2520existing%2520methods%2520often%2520overlook%2520the%250Aproblem%2520of%2520increased%2520computational%2520and%2520storage%2520requirements%2520when%2520improving%250Amodel%2520performance.%2520To%2520handle%2520these%2520limitations%252C%2520we%2520propose%2520a%2520lightweight%250Aenhanced%2520alignment%2520network%252C%2520called%2520the%2520Multi-Level%2520Embedding%2520and%2520Alignment%250ANetwork%2520%2528MEAN%2529.%2520The%2520MEAN%2520network%2520uses%2520a%2520progressive%2520multi-level%2520enhancement%250Astrategy%252C%2520global-to-local%2520associations%252C%2520and%2520cross-domain%2520alignment%252C%2520enabling%250Afeature%2520communication%2520across%2520levels.%2520This%2520allows%2520MEAN%2520to%2520effectively%2520connect%250Afeatures%2520at%2520different%2520levels%2520and%2520learn%2520robust%2520cross-view%2520consistent%2520mappings%250Aand%2520modality-invariant%2520features.%2520Moreover%252C%2520MEAN%2520adopts%2520a%2520shallow%2520backbone%250Anetwork%2520combined%2520with%2520a%2520lightweight%2520branch%2520design%252C%2520effectively%2520reducing%250Aparameter%2520count%2520and%2520computational%2520complexity.%2520Experimental%2520results%2520on%2520the%250AUniversity-1652%2520and%2520SUES-200%2520datasets%2520demonstrate%2520that%2520MEAN%2520reduces%2520parameter%250Acount%2520by%252062.17%2525%2520and%2520computational%2520complexity%2520by%252070.99%2525%2520compared%2520to%250Astate-of-the-art%2520models%252C%2520while%2520maintaining%2520competitive%2520or%2520even%2520superior%250Aperformance.%2520The%2520codes%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Level%20Embedding%20and%20Alignment%20Network%20with%20Consistency%20and%0A%20%20Invariance%20Learning%20for%20Cross-View%20Geo-Localization&entry.906535625=Zhongwei%20Chen%20and%20Zhao-Xu%20Yang%20and%20Hai-Jun%20Rong&entry.1292438233=%20%20Cross-View%20Geo-Localization%20%28CVGL%29%20involves%20determining%20the%20localization%20of%0Adrone%20images%20by%20retrieving%20the%20most%20similar%20GPS-tagged%20satellite%20images.%0AHowever%2C%20the%20imaging%20gaps%20between%20platforms%20are%20often%20significant%20and%20the%0Avariations%20in%20viewpoints%20are%20substantial%2C%20which%20limits%20the%20ability%20of%20existing%0Amethods%20to%20effectively%20associate%20cross-view%20features%20and%20extract%20consistent%20and%0Ainvariant%20characteristics.%20Moreover%2C%20existing%20methods%20often%20overlook%20the%0Aproblem%20of%20increased%20computational%20and%20storage%20requirements%20when%20improving%0Amodel%20performance.%20To%20handle%20these%20limitations%2C%20we%20propose%20a%20lightweight%0Aenhanced%20alignment%20network%2C%20called%20the%20Multi-Level%20Embedding%20and%20Alignment%0ANetwork%20%28MEAN%29.%20The%20MEAN%20network%20uses%20a%20progressive%20multi-level%20enhancement%0Astrategy%2C%20global-to-local%20associations%2C%20and%20cross-domain%20alignment%2C%20enabling%0Afeature%20communication%20across%20levels.%20This%20allows%20MEAN%20to%20effectively%20connect%0Afeatures%20at%20different%20levels%20and%20learn%20robust%20cross-view%20consistent%20mappings%0Aand%20modality-invariant%20features.%20Moreover%2C%20MEAN%20adopts%20a%20shallow%20backbone%0Anetwork%20combined%20with%20a%20lightweight%20branch%20design%2C%20effectively%20reducing%0Aparameter%20count%20and%20computational%20complexity.%20Experimental%20results%20on%20the%0AUniversity-1652%20and%20SUES-200%20datasets%20demonstrate%20that%20MEAN%20reduces%20parameter%0Acount%20by%2062.17%25%20and%20computational%20complexity%20by%2070.99%25%20compared%20to%0Astate-of-the-art%20models%2C%20while%20maintaining%20competitive%20or%20even%20superior%0Aperformance.%20The%20codes%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14819v1&entry.124074799=Read"},
{"title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation", "author": "Muntasir Wahed and Kiet A. Nguyen and Adheesh Sunil Juvekar and Xinzhuo Li and Xiaona Zhou and Vedant Shah and Tianjiao Yu and Pinar Yanardag and Ismini Lourentzou", "abstract": "  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2412.15209v1", "date": "2024-12-19", "relevancy": 3.059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMA%3A%20Multi-Image%20Vision-Language%20Models%20for%20Reasoning%20Segmentation&body=Title%3A%20PRIMA%3A%20Multi-Image%20Vision-Language%20Models%20for%20Reasoning%20Segmentation%0AAuthor%3A%20Muntasir%20Wahed%20and%20Kiet%20A.%20Nguyen%20and%20Adheesh%20Sunil%20Juvekar%20and%20Xinzhuo%20Li%20and%20Xiaona%20Zhou%20and%20Vedant%20Shah%20and%20Tianjiao%20Yu%20and%20Pinar%20Yanardag%20and%20Ismini%20Lourentzou%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%0Aexisting%20pixel-grounding%20models%20operate%20on%20single-image%20settings%2C%20limiting%0Atheir%20ability%20to%20perform%20detailed%2C%20fine-grained%20comparisons%20across%20multiple%0Aimages.%20Conversely%2C%20current%20multi-image%20understanding%20models%20lack%20pixel-level%0Agrounding.%20Our%20work%20addresses%20this%20gap%20by%20introducing%20the%20task%20of%20multi-image%0Apixel-grounded%20reasoning%20segmentation%2C%20and%20PRIMA%2C%20a%20novel%20LVLM%20that%20integrates%0Apixel-level%20grounding%20with%20robust%20multi-image%20reasoning%20capabilities%20to%20produce%0Acontextually%20rich%2C%20pixel-grounded%20explanations.%20Central%20to%20PRIMA%20is%20an%0Aefficient%20vision%20module%20that%20queries%20fine-grained%20visual%20representations%20across%0Amultiple%20images%2C%20reducing%20TFLOPs%20by%20%2425.3%5C%25%24.%20To%20support%20training%20and%0Aevaluation%2C%20we%20curate%20%24M%5E4Seg%24%2C%20a%20new%20reasoning%20segmentation%20benchmark%0Aconsisting%20of%20%24%5Csim%24224K%20question-answer%20pairs%20that%20require%20fine-grained%20visual%0Aunderstanding%20across%20multiple%20images.%20Experimental%20results%20demonstrate%20PRIMA%0Aoutperforms%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMA%253A%2520Multi-Image%2520Vision-Language%2520Models%2520for%2520Reasoning%2520Segmentation%26entry.906535625%3DMuntasir%2520Wahed%2520and%2520Kiet%2520A.%2520Nguyen%2520and%2520Adheesh%2520Sunil%2520Juvekar%2520and%2520Xinzhuo%2520Li%2520and%2520Xiaona%2520Zhou%2520and%2520Vedant%2520Shah%2520and%2520Tianjiao%2520Yu%2520and%2520Pinar%2520Yanardag%2520and%2520Ismini%2520Lourentzou%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%250Aexisting%2520pixel-grounding%2520models%2520operate%2520on%2520single-image%2520settings%252C%2520limiting%250Atheir%2520ability%2520to%2520perform%2520detailed%252C%2520fine-grained%2520comparisons%2520across%2520multiple%250Aimages.%2520Conversely%252C%2520current%2520multi-image%2520understanding%2520models%2520lack%2520pixel-level%250Agrounding.%2520Our%2520work%2520addresses%2520this%2520gap%2520by%2520introducing%2520the%2520task%2520of%2520multi-image%250Apixel-grounded%2520reasoning%2520segmentation%252C%2520and%2520PRIMA%252C%2520a%2520novel%2520LVLM%2520that%2520integrates%250Apixel-level%2520grounding%2520with%2520robust%2520multi-image%2520reasoning%2520capabilities%2520to%2520produce%250Acontextually%2520rich%252C%2520pixel-grounded%2520explanations.%2520Central%2520to%2520PRIMA%2520is%2520an%250Aefficient%2520vision%2520module%2520that%2520queries%2520fine-grained%2520visual%2520representations%2520across%250Amultiple%2520images%252C%2520reducing%2520TFLOPs%2520by%2520%252425.3%255C%2525%2524.%2520To%2520support%2520training%2520and%250Aevaluation%252C%2520we%2520curate%2520%2524M%255E4Seg%2524%252C%2520a%2520new%2520reasoning%2520segmentation%2520benchmark%250Aconsisting%2520of%2520%2524%255Csim%2524224K%2520question-answer%2520pairs%2520that%2520require%2520fine-grained%2520visual%250Aunderstanding%2520across%2520multiple%2520images.%2520Experimental%2520results%2520demonstrate%2520PRIMA%250Aoutperforms%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMA%3A%20Multi-Image%20Vision-Language%20Models%20for%20Reasoning%20Segmentation&entry.906535625=Muntasir%20Wahed%20and%20Kiet%20A.%20Nguyen%20and%20Adheesh%20Sunil%20Juvekar%20and%20Xinzhuo%20Li%20and%20Xiaona%20Zhou%20and%20Vedant%20Shah%20and%20Tianjiao%20Yu%20and%20Pinar%20Yanardag%20and%20Ismini%20Lourentzou&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%0Aexisting%20pixel-grounding%20models%20operate%20on%20single-image%20settings%2C%20limiting%0Atheir%20ability%20to%20perform%20detailed%2C%20fine-grained%20comparisons%20across%20multiple%0Aimages.%20Conversely%2C%20current%20multi-image%20understanding%20models%20lack%20pixel-level%0Agrounding.%20Our%20work%20addresses%20this%20gap%20by%20introducing%20the%20task%20of%20multi-image%0Apixel-grounded%20reasoning%20segmentation%2C%20and%20PRIMA%2C%20a%20novel%20LVLM%20that%20integrates%0Apixel-level%20grounding%20with%20robust%20multi-image%20reasoning%20capabilities%20to%20produce%0Acontextually%20rich%2C%20pixel-grounded%20explanations.%20Central%20to%20PRIMA%20is%20an%0Aefficient%20vision%20module%20that%20queries%20fine-grained%20visual%20representations%20across%0Amultiple%20images%2C%20reducing%20TFLOPs%20by%20%2425.3%5C%25%24.%20To%20support%20training%20and%0Aevaluation%2C%20we%20curate%20%24M%5E4Seg%24%2C%20a%20new%20reasoning%20segmentation%20benchmark%0Aconsisting%20of%20%24%5Csim%24224K%20question-answer%20pairs%20that%20require%20fine-grained%20visual%0Aunderstanding%20across%20multiple%20images.%20Experimental%20results%20demonstrate%20PRIMA%0Aoutperforms%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15209v1&entry.124074799=Read"},
{"title": "Diffusion priors for Bayesian 3D reconstruction from incomplete\n  measurements", "author": "Julian L. M\u00f6bius and Michael Habeck", "abstract": "  Many inverse problems are ill-posed and need to be complemented by prior\ninformation that restricts the class of admissible models. Bayesian approaches\nencode this information as prior distributions that impose generic properties\non the model such as sparsity, non-negativity or smoothness. However, in case\nof complex structured models such as images, graphs or three-dimensional (3D)\nobjects,generic prior distributions tend to favor models that differ largely\nfrom those observed in the real world. Here we explore the use of diffusion\nmodels as priors that are combined with experimental data within a Bayesian\nframework. We use 3D point clouds to represent 3D objects such as household\nitems or biomolecular complexes formed from proteins and nucleic acids. We\ntrain diffusion models that generate coarse-grained 3D structures at a medium\nresolution and integrate these with incomplete and noisy experimental data. To\ndemonstrate the power of our approach, we focus on the reconstruction of\nbiomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which\nis an important inverse problem in structural biology. We find that posterior\nsampling with diffusion model priors allows for 3D reconstruction from very\nsparse, low-resolution and partial observations.\n", "link": "http://arxiv.org/abs/2412.14897v1", "date": "2024-12-19", "relevancy": 3.0245, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6262}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6262}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20priors%20for%20Bayesian%203D%20reconstruction%20from%20incomplete%0A%20%20measurements&body=Title%3A%20Diffusion%20priors%20for%20Bayesian%203D%20reconstruction%20from%20incomplete%0A%20%20measurements%0AAuthor%3A%20Julian%20L.%20M%C3%B6bius%20and%20Michael%20Habeck%0AAbstract%3A%20%20%20Many%20inverse%20problems%20are%20ill-posed%20and%20need%20to%20be%20complemented%20by%20prior%0Ainformation%20that%20restricts%20the%20class%20of%20admissible%20models.%20Bayesian%20approaches%0Aencode%20this%20information%20as%20prior%20distributions%20that%20impose%20generic%20properties%0Aon%20the%20model%20such%20as%20sparsity%2C%20non-negativity%20or%20smoothness.%20However%2C%20in%20case%0Aof%20complex%20structured%20models%20such%20as%20images%2C%20graphs%20or%20three-dimensional%20%283D%29%0Aobjects%2Cgeneric%20prior%20distributions%20tend%20to%20favor%20models%20that%20differ%20largely%0Afrom%20those%20observed%20in%20the%20real%20world.%20Here%20we%20explore%20the%20use%20of%20diffusion%0Amodels%20as%20priors%20that%20are%20combined%20with%20experimental%20data%20within%20a%20Bayesian%0Aframework.%20We%20use%203D%20point%20clouds%20to%20represent%203D%20objects%20such%20as%20household%0Aitems%20or%20biomolecular%20complexes%20formed%20from%20proteins%20and%20nucleic%20acids.%20We%0Atrain%20diffusion%20models%20that%20generate%20coarse-grained%203D%20structures%20at%20a%20medium%0Aresolution%20and%20integrate%20these%20with%20incomplete%20and%20noisy%20experimental%20data.%20To%0Ademonstrate%20the%20power%20of%20our%20approach%2C%20we%20focus%20on%20the%20reconstruction%20of%0Abiomolecular%20assemblies%20from%20cryo-electron%20microscopy%20%28cryo-EM%29%20images%2C%20which%0Ais%20an%20important%20inverse%20problem%20in%20structural%20biology.%20We%20find%20that%20posterior%0Asampling%20with%20diffusion%20model%20priors%20allows%20for%203D%20reconstruction%20from%20very%0Asparse%2C%20low-resolution%20and%20partial%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520priors%2520for%2520Bayesian%25203D%2520reconstruction%2520from%2520incomplete%250A%2520%2520measurements%26entry.906535625%3DJulian%2520L.%2520M%25C3%25B6bius%2520and%2520Michael%2520Habeck%26entry.1292438233%3D%2520%2520Many%2520inverse%2520problems%2520are%2520ill-posed%2520and%2520need%2520to%2520be%2520complemented%2520by%2520prior%250Ainformation%2520that%2520restricts%2520the%2520class%2520of%2520admissible%2520models.%2520Bayesian%2520approaches%250Aencode%2520this%2520information%2520as%2520prior%2520distributions%2520that%2520impose%2520generic%2520properties%250Aon%2520the%2520model%2520such%2520as%2520sparsity%252C%2520non-negativity%2520or%2520smoothness.%2520However%252C%2520in%2520case%250Aof%2520complex%2520structured%2520models%2520such%2520as%2520images%252C%2520graphs%2520or%2520three-dimensional%2520%25283D%2529%250Aobjects%252Cgeneric%2520prior%2520distributions%2520tend%2520to%2520favor%2520models%2520that%2520differ%2520largely%250Afrom%2520those%2520observed%2520in%2520the%2520real%2520world.%2520Here%2520we%2520explore%2520the%2520use%2520of%2520diffusion%250Amodels%2520as%2520priors%2520that%2520are%2520combined%2520with%2520experimental%2520data%2520within%2520a%2520Bayesian%250Aframework.%2520We%2520use%25203D%2520point%2520clouds%2520to%2520represent%25203D%2520objects%2520such%2520as%2520household%250Aitems%2520or%2520biomolecular%2520complexes%2520formed%2520from%2520proteins%2520and%2520nucleic%2520acids.%2520We%250Atrain%2520diffusion%2520models%2520that%2520generate%2520coarse-grained%25203D%2520structures%2520at%2520a%2520medium%250Aresolution%2520and%2520integrate%2520these%2520with%2520incomplete%2520and%2520noisy%2520experimental%2520data.%2520To%250Ademonstrate%2520the%2520power%2520of%2520our%2520approach%252C%2520we%2520focus%2520on%2520the%2520reconstruction%2520of%250Abiomolecular%2520assemblies%2520from%2520cryo-electron%2520microscopy%2520%2528cryo-EM%2529%2520images%252C%2520which%250Ais%2520an%2520important%2520inverse%2520problem%2520in%2520structural%2520biology.%2520We%2520find%2520that%2520posterior%250Asampling%2520with%2520diffusion%2520model%2520priors%2520allows%2520for%25203D%2520reconstruction%2520from%2520very%250Asparse%252C%2520low-resolution%2520and%2520partial%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20priors%20for%20Bayesian%203D%20reconstruction%20from%20incomplete%0A%20%20measurements&entry.906535625=Julian%20L.%20M%C3%B6bius%20and%20Michael%20Habeck&entry.1292438233=%20%20Many%20inverse%20problems%20are%20ill-posed%20and%20need%20to%20be%20complemented%20by%20prior%0Ainformation%20that%20restricts%20the%20class%20of%20admissible%20models.%20Bayesian%20approaches%0Aencode%20this%20information%20as%20prior%20distributions%20that%20impose%20generic%20properties%0Aon%20the%20model%20such%20as%20sparsity%2C%20non-negativity%20or%20smoothness.%20However%2C%20in%20case%0Aof%20complex%20structured%20models%20such%20as%20images%2C%20graphs%20or%20three-dimensional%20%283D%29%0Aobjects%2Cgeneric%20prior%20distributions%20tend%20to%20favor%20models%20that%20differ%20largely%0Afrom%20those%20observed%20in%20the%20real%20world.%20Here%20we%20explore%20the%20use%20of%20diffusion%0Amodels%20as%20priors%20that%20are%20combined%20with%20experimental%20data%20within%20a%20Bayesian%0Aframework.%20We%20use%203D%20point%20clouds%20to%20represent%203D%20objects%20such%20as%20household%0Aitems%20or%20biomolecular%20complexes%20formed%20from%20proteins%20and%20nucleic%20acids.%20We%0Atrain%20diffusion%20models%20that%20generate%20coarse-grained%203D%20structures%20at%20a%20medium%0Aresolution%20and%20integrate%20these%20with%20incomplete%20and%20noisy%20experimental%20data.%20To%0Ademonstrate%20the%20power%20of%20our%20approach%2C%20we%20focus%20on%20the%20reconstruction%20of%0Abiomolecular%20assemblies%20from%20cryo-electron%20microscopy%20%28cryo-EM%29%20images%2C%20which%0Ais%20an%20important%20inverse%20problem%20in%20structural%20biology.%20We%20find%20that%20posterior%0Asampling%20with%20diffusion%20model%20priors%20allows%20for%203D%20reconstruction%20from%20very%0Asparse%2C%20low-resolution%20and%20partial%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14897v1&entry.124074799=Read"},
{"title": "From Training-Free to Adaptive: Empirical Insights into MLLMs'\n  Understanding of Detection Information", "author": "Qirui Jiao and Daoyuan Chen and Yilun Huang and Yaliang Li and Ying Shen", "abstract": "  Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. Vision detection models excel\nat recognizing fine-grained image details, prompting researchers to use them to\nenhance MLLMs. One effective strategy is to infuse detection information in\ntext format, which has proven simple and effective. However, most studies\nutilize this method without training, leaving the potential of adaptive\ntraining largely unexplored. Adaptive training could significantly enhance\nMLLMs' comprehension of unique inputs while filtering out irrelevant\ninformation. This paper addresses the crucial question: How does training\nimpact MLLMs' understanding of infused textual detection information? We\nsystematically experiment with various representative models to evaluate the\neffects of training-free, retraining, and fine-tuning strategies. We also\nexamine the influence of training on MLLMs' original abilities and the\ninterchangeability of detection models. Our findings indicate that fine-tuning\na pre-trained MLLM to incorporate textual detection information delivers\nsuperior results compared to training-free and retraining methods, improving\nperformance by 6.71% across 10 widely recognized benchmarks. Furthermore,\nfine-tuning enables MLLMs to retain performance enhancements even when\ndetection models are swapped, indicating improved understanding of formatted\ntextual data. We release our codes to support further exploration of fusion\nstrategies for vision detection models and the enhancement of MLLMs'\nfine-grained multimodal capabilities.\n", "link": "http://arxiv.org/abs/2401.17981v3", "date": "2024-12-19", "relevancy": 2.9567, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Training-Free%20to%20Adaptive%3A%20Empirical%20Insights%20into%20MLLMs%27%0A%20%20Understanding%20of%20Detection%20Information&body=Title%3A%20From%20Training-Free%20to%20Adaptive%3A%20Empirical%20Insights%20into%20MLLMs%27%0A%20%20Understanding%20of%20Detection%20Information%0AAuthor%3A%20Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen%0AAbstract%3A%20%20%20Despite%20the%20impressive%20capabilities%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20in%20integrating%20text%20and%20image%20modalities%2C%20challenges%20remain%20in%0Aaccurately%20interpreting%20detailed%20visual%20elements.%20Vision%20detection%20models%20excel%0Aat%20recognizing%20fine-grained%20image%20details%2C%20prompting%20researchers%20to%20use%20them%20to%0Aenhance%20MLLMs.%20One%20effective%20strategy%20is%20to%20infuse%20detection%20information%20in%0Atext%20format%2C%20which%20has%20proven%20simple%20and%20effective.%20However%2C%20most%20studies%0Autilize%20this%20method%20without%20training%2C%20leaving%20the%20potential%20of%20adaptive%0Atraining%20largely%20unexplored.%20Adaptive%20training%20could%20significantly%20enhance%0AMLLMs%27%20comprehension%20of%20unique%20inputs%20while%20filtering%20out%20irrelevant%0Ainformation.%20This%20paper%20addresses%20the%20crucial%20question%3A%20How%20does%20training%0Aimpact%20MLLMs%27%20understanding%20of%20infused%20textual%20detection%20information%3F%20We%0Asystematically%20experiment%20with%20various%20representative%20models%20to%20evaluate%20the%0Aeffects%20of%20training-free%2C%20retraining%2C%20and%20fine-tuning%20strategies.%20We%20also%0Aexamine%20the%20influence%20of%20training%20on%20MLLMs%27%20original%20abilities%20and%20the%0Ainterchangeability%20of%20detection%20models.%20Our%20findings%20indicate%20that%20fine-tuning%0Aa%20pre-trained%20MLLM%20to%20incorporate%20textual%20detection%20information%20delivers%0Asuperior%20results%20compared%20to%20training-free%20and%20retraining%20methods%2C%20improving%0Aperformance%20by%206.71%25%20across%2010%20widely%20recognized%20benchmarks.%20Furthermore%2C%0Afine-tuning%20enables%20MLLMs%20to%20retain%20performance%20enhancements%20even%20when%0Adetection%20models%20are%20swapped%2C%20indicating%20improved%20understanding%20of%20formatted%0Atextual%20data.%20We%20release%20our%20codes%20to%20support%20further%20exploration%20of%20fusion%0Astrategies%20for%20vision%20detection%20models%20and%20the%20enhancement%20of%20MLLMs%27%0Afine-grained%20multimodal%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17981v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Training-Free%2520to%2520Adaptive%253A%2520Empirical%2520Insights%2520into%2520MLLMs%2527%250A%2520%2520Understanding%2520of%2520Detection%2520Information%26entry.906535625%3DQirui%2520Jiao%2520and%2520Daoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Yaliang%2520Li%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520in%2520integrating%2520text%2520and%2520image%2520modalities%252C%2520challenges%2520remain%2520in%250Aaccurately%2520interpreting%2520detailed%2520visual%2520elements.%2520Vision%2520detection%2520models%2520excel%250Aat%2520recognizing%2520fine-grained%2520image%2520details%252C%2520prompting%2520researchers%2520to%2520use%2520them%2520to%250Aenhance%2520MLLMs.%2520One%2520effective%2520strategy%2520is%2520to%2520infuse%2520detection%2520information%2520in%250Atext%2520format%252C%2520which%2520has%2520proven%2520simple%2520and%2520effective.%2520However%252C%2520most%2520studies%250Autilize%2520this%2520method%2520without%2520training%252C%2520leaving%2520the%2520potential%2520of%2520adaptive%250Atraining%2520largely%2520unexplored.%2520Adaptive%2520training%2520could%2520significantly%2520enhance%250AMLLMs%2527%2520comprehension%2520of%2520unique%2520inputs%2520while%2520filtering%2520out%2520irrelevant%250Ainformation.%2520This%2520paper%2520addresses%2520the%2520crucial%2520question%253A%2520How%2520does%2520training%250Aimpact%2520MLLMs%2527%2520understanding%2520of%2520infused%2520textual%2520detection%2520information%253F%2520We%250Asystematically%2520experiment%2520with%2520various%2520representative%2520models%2520to%2520evaluate%2520the%250Aeffects%2520of%2520training-free%252C%2520retraining%252C%2520and%2520fine-tuning%2520strategies.%2520We%2520also%250Aexamine%2520the%2520influence%2520of%2520training%2520on%2520MLLMs%2527%2520original%2520abilities%2520and%2520the%250Ainterchangeability%2520of%2520detection%2520models.%2520Our%2520findings%2520indicate%2520that%2520fine-tuning%250Aa%2520pre-trained%2520MLLM%2520to%2520incorporate%2520textual%2520detection%2520information%2520delivers%250Asuperior%2520results%2520compared%2520to%2520training-free%2520and%2520retraining%2520methods%252C%2520improving%250Aperformance%2520by%25206.71%2525%2520across%252010%2520widely%2520recognized%2520benchmarks.%2520Furthermore%252C%250Afine-tuning%2520enables%2520MLLMs%2520to%2520retain%2520performance%2520enhancements%2520even%2520when%250Adetection%2520models%2520are%2520swapped%252C%2520indicating%2520improved%2520understanding%2520of%2520formatted%250Atextual%2520data.%2520We%2520release%2520our%2520codes%2520to%2520support%2520further%2520exploration%2520of%2520fusion%250Astrategies%2520for%2520vision%2520detection%2520models%2520and%2520the%2520enhancement%2520of%2520MLLMs%2527%250Afine-grained%2520multimodal%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17981v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Training-Free%20to%20Adaptive%3A%20Empirical%20Insights%20into%20MLLMs%27%0A%20%20Understanding%20of%20Detection%20Information&entry.906535625=Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen&entry.1292438233=%20%20Despite%20the%20impressive%20capabilities%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20in%20integrating%20text%20and%20image%20modalities%2C%20challenges%20remain%20in%0Aaccurately%20interpreting%20detailed%20visual%20elements.%20Vision%20detection%20models%20excel%0Aat%20recognizing%20fine-grained%20image%20details%2C%20prompting%20researchers%20to%20use%20them%20to%0Aenhance%20MLLMs.%20One%20effective%20strategy%20is%20to%20infuse%20detection%20information%20in%0Atext%20format%2C%20which%20has%20proven%20simple%20and%20effective.%20However%2C%20most%20studies%0Autilize%20this%20method%20without%20training%2C%20leaving%20the%20potential%20of%20adaptive%0Atraining%20largely%20unexplored.%20Adaptive%20training%20could%20significantly%20enhance%0AMLLMs%27%20comprehension%20of%20unique%20inputs%20while%20filtering%20out%20irrelevant%0Ainformation.%20This%20paper%20addresses%20the%20crucial%20question%3A%20How%20does%20training%0Aimpact%20MLLMs%27%20understanding%20of%20infused%20textual%20detection%20information%3F%20We%0Asystematically%20experiment%20with%20various%20representative%20models%20to%20evaluate%20the%0Aeffects%20of%20training-free%2C%20retraining%2C%20and%20fine-tuning%20strategies.%20We%20also%0Aexamine%20the%20influence%20of%20training%20on%20MLLMs%27%20original%20abilities%20and%20the%0Ainterchangeability%20of%20detection%20models.%20Our%20findings%20indicate%20that%20fine-tuning%0Aa%20pre-trained%20MLLM%20to%20incorporate%20textual%20detection%20information%20delivers%0Asuperior%20results%20compared%20to%20training-free%20and%20retraining%20methods%2C%20improving%0Aperformance%20by%206.71%25%20across%2010%20widely%20recognized%20benchmarks.%20Furthermore%2C%0Afine-tuning%20enables%20MLLMs%20to%20retain%20performance%20enhancements%20even%20when%0Adetection%20models%20are%20swapped%2C%20indicating%20improved%20understanding%20of%20formatted%0Atextual%20data.%20We%20release%20our%20codes%20to%20support%20further%20exploration%20of%20fusion%0Astrategies%20for%20vision%20detection%20models%20and%20the%20enhancement%20of%20MLLMs%27%0Afine-grained%20multimodal%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17981v3&entry.124074799=Read"},
{"title": "VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis", "author": "Chao Pang and Xingxing Weng and Jiang Wu and Jiayu Li and Yi Liu and Jiaxing Sun and Weijia Li and Shuai Wang and Litong Feng and Gui-Song Xia and Conghui He", "abstract": "  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n", "link": "http://arxiv.org/abs/2403.20213v4", "date": "2024-12-19", "relevancy": 2.9103, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.588}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VHM%3A%20Versatile%20and%20Honest%20Vision%20Language%20Model%20for%20Remote%20Sensing%20Image%0A%20%20Analysis&body=Title%3A%20VHM%3A%20Versatile%20and%20Honest%20Vision%20Language%20Model%20for%20Remote%20Sensing%20Image%0A%20%20Analysis%0AAuthor%3A%20Chao%20Pang%20and%20Xingxing%20Weng%20and%20Jiang%20Wu%20and%20Jiayu%20Li%20and%20Yi%20Liu%20and%20Jiaxing%20Sun%20and%20Weijia%20Li%20and%20Shuai%20Wang%20and%20Litong%20Feng%20and%20Gui-Song%20Xia%20and%20Conghui%20He%0AAbstract%3A%20%20%20This%20paper%20develops%20a%20Versatile%20and%20Honest%20vision%20language%20Model%20%28VHM%29%20for%0Aremote%20sensing%20image%20analysis.%20VHM%20is%20built%20on%20a%20large-scale%20remote%20sensing%0Aimage-text%20dataset%20with%20rich-content%20captions%20%28VersaD%29%2C%20and%20an%20honest%0Ainstruction%20dataset%20comprising%20both%20factual%20and%20deceptive%20questions%20%28HnstD%29.%0AUnlike%20prevailing%20remote%20sensing%20image-text%20datasets%2C%20in%20which%20image%20captions%0Afocus%20on%20a%20few%20prominent%20objects%20and%20their%20relationships%2C%20VersaD%20captions%0Aprovide%20detailed%20information%20about%20image%20properties%2C%20object%20attributes%2C%20and%20the%0Aoverall%20scene.%20This%20comprehensive%20captioning%20enables%20VHM%20to%20thoroughly%0Aunderstand%20remote%20sensing%20images%20and%20perform%20diverse%20remote%20sensing%20tasks.%0AMoreover%2C%20different%20from%20existing%20remote%20sensing%20instruction%20datasets%20that%20only%0Ainclude%20factual%20questions%2C%20HnstD%20contains%20additional%20deceptive%20questions%0Astemming%20from%20the%20non-existence%20of%20objects.%20This%20feature%20prevents%20VHM%20from%0Aproducing%20affirmative%20answers%20to%20nonsense%20queries%2C%20thereby%20ensuring%20its%0Ahonesty.%20In%20our%20experiments%2C%20VHM%20significantly%20outperforms%20various%20vision%0Alanguage%20models%20on%20common%20tasks%20of%20scene%20classification%2C%20visual%20question%0Aanswering%2C%20and%20visual%20grounding.%20Additionally%2C%20VHM%20achieves%20competent%0Aperformance%20on%20several%20unexplored%20tasks%2C%20such%20as%20building%20vectorizing%2C%0Amulti-label%20classification%20and%20honest%20question%20answering.%20We%20will%20release%20the%0Acode%2C%20data%20and%20model%20weights%20at%20https%3A//github.com/opendatalab/VHM%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20213v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVHM%253A%2520Versatile%2520and%2520Honest%2520Vision%2520Language%2520Model%2520for%2520Remote%2520Sensing%2520Image%250A%2520%2520Analysis%26entry.906535625%3DChao%2520Pang%2520and%2520Xingxing%2520Weng%2520and%2520Jiang%2520Wu%2520and%2520Jiayu%2520Li%2520and%2520Yi%2520Liu%2520and%2520Jiaxing%2520Sun%2520and%2520Weijia%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Litong%2520Feng%2520and%2520Gui-Song%2520Xia%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520This%2520paper%2520develops%2520a%2520Versatile%2520and%2520Honest%2520vision%2520language%2520Model%2520%2528VHM%2529%2520for%250Aremote%2520sensing%2520image%2520analysis.%2520VHM%2520is%2520built%2520on%2520a%2520large-scale%2520remote%2520sensing%250Aimage-text%2520dataset%2520with%2520rich-content%2520captions%2520%2528VersaD%2529%252C%2520and%2520an%2520honest%250Ainstruction%2520dataset%2520comprising%2520both%2520factual%2520and%2520deceptive%2520questions%2520%2528HnstD%2529.%250AUnlike%2520prevailing%2520remote%2520sensing%2520image-text%2520datasets%252C%2520in%2520which%2520image%2520captions%250Afocus%2520on%2520a%2520few%2520prominent%2520objects%2520and%2520their%2520relationships%252C%2520VersaD%2520captions%250Aprovide%2520detailed%2520information%2520about%2520image%2520properties%252C%2520object%2520attributes%252C%2520and%2520the%250Aoverall%2520scene.%2520This%2520comprehensive%2520captioning%2520enables%2520VHM%2520to%2520thoroughly%250Aunderstand%2520remote%2520sensing%2520images%2520and%2520perform%2520diverse%2520remote%2520sensing%2520tasks.%250AMoreover%252C%2520different%2520from%2520existing%2520remote%2520sensing%2520instruction%2520datasets%2520that%2520only%250Ainclude%2520factual%2520questions%252C%2520HnstD%2520contains%2520additional%2520deceptive%2520questions%250Astemming%2520from%2520the%2520non-existence%2520of%2520objects.%2520This%2520feature%2520prevents%2520VHM%2520from%250Aproducing%2520affirmative%2520answers%2520to%2520nonsense%2520queries%252C%2520thereby%2520ensuring%2520its%250Ahonesty.%2520In%2520our%2520experiments%252C%2520VHM%2520significantly%2520outperforms%2520various%2520vision%250Alanguage%2520models%2520on%2520common%2520tasks%2520of%2520scene%2520classification%252C%2520visual%2520question%250Aanswering%252C%2520and%2520visual%2520grounding.%2520Additionally%252C%2520VHM%2520achieves%2520competent%250Aperformance%2520on%2520several%2520unexplored%2520tasks%252C%2520such%2520as%2520building%2520vectorizing%252C%250Amulti-label%2520classification%2520and%2520honest%2520question%2520answering.%2520We%2520will%2520release%2520the%250Acode%252C%2520data%2520and%2520model%2520weights%2520at%2520https%253A//github.com/opendatalab/VHM%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20213v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VHM%3A%20Versatile%20and%20Honest%20Vision%20Language%20Model%20for%20Remote%20Sensing%20Image%0A%20%20Analysis&entry.906535625=Chao%20Pang%20and%20Xingxing%20Weng%20and%20Jiang%20Wu%20and%20Jiayu%20Li%20and%20Yi%20Liu%20and%20Jiaxing%20Sun%20and%20Weijia%20Li%20and%20Shuai%20Wang%20and%20Litong%20Feng%20and%20Gui-Song%20Xia%20and%20Conghui%20He&entry.1292438233=%20%20This%20paper%20develops%20a%20Versatile%20and%20Honest%20vision%20language%20Model%20%28VHM%29%20for%0Aremote%20sensing%20image%20analysis.%20VHM%20is%20built%20on%20a%20large-scale%20remote%20sensing%0Aimage-text%20dataset%20with%20rich-content%20captions%20%28VersaD%29%2C%20and%20an%20honest%0Ainstruction%20dataset%20comprising%20both%20factual%20and%20deceptive%20questions%20%28HnstD%29.%0AUnlike%20prevailing%20remote%20sensing%20image-text%20datasets%2C%20in%20which%20image%20captions%0Afocus%20on%20a%20few%20prominent%20objects%20and%20their%20relationships%2C%20VersaD%20captions%0Aprovide%20detailed%20information%20about%20image%20properties%2C%20object%20attributes%2C%20and%20the%0Aoverall%20scene.%20This%20comprehensive%20captioning%20enables%20VHM%20to%20thoroughly%0Aunderstand%20remote%20sensing%20images%20and%20perform%20diverse%20remote%20sensing%20tasks.%0AMoreover%2C%20different%20from%20existing%20remote%20sensing%20instruction%20datasets%20that%20only%0Ainclude%20factual%20questions%2C%20HnstD%20contains%20additional%20deceptive%20questions%0Astemming%20from%20the%20non-existence%20of%20objects.%20This%20feature%20prevents%20VHM%20from%0Aproducing%20affirmative%20answers%20to%20nonsense%20queries%2C%20thereby%20ensuring%20its%0Ahonesty.%20In%20our%20experiments%2C%20VHM%20significantly%20outperforms%20various%20vision%0Alanguage%20models%20on%20common%20tasks%20of%20scene%20classification%2C%20visual%20question%0Aanswering%2C%20and%20visual%20grounding.%20Additionally%2C%20VHM%20achieves%20competent%0Aperformance%20on%20several%20unexplored%20tasks%2C%20such%20as%20building%20vectorizing%2C%0Amulti-label%20classification%20and%20honest%20question%20answering.%20We%20will%20release%20the%0Acode%2C%20data%20and%20model%20weights%20at%20https%3A//github.com/opendatalab/VHM%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20213v4&entry.124074799=Read"},
{"title": "TDCNet: Transparent Objects Depth Completion with CNN-Transformer\n  Dual-Branch Parallel Network", "author": "Xianghui Fan and Chao Ye and Anping Deng and Xiaotian Wu and Mengyang Pan and Hang Yang", "abstract": "  The sensing and manipulation of transparent objects present a critical\nchallenge in industrial and laboratory robotics. Conventional sensors face\nchallenges in obtaining the full depth of transparent objects due to the\nrefraction and reflection of light on their surfaces and their lack of visible\ntexture. Previous research has attempted to obtain complete depth maps of\ntransparent objects from RGB and damaged depth maps (collected by depth sensor)\nusing deep learning models. However, existing methods fail to fully utilize the\noriginal depth map, resulting in limited accuracy for deep completion. To solve\nthis problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel\nnetwork for transparent object depth completion. The proposed framework\nconsists of two different branches: one extracts features from partial depth\nmaps, while the other processes RGB-D images. Experimental results demonstrate\nthat our model achieves state-of-the-art performance across multiple public\ndatasets. Our code and the pre-trained model are publicly available at\nhttps://github.com/XianghuiFan/TDCNet.\n", "link": "http://arxiv.org/abs/2412.14961v1", "date": "2024-12-19", "relevancy": 2.8449, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDCNet%3A%20Transparent%20Objects%20Depth%20Completion%20with%20CNN-Transformer%0A%20%20Dual-Branch%20Parallel%20Network&body=Title%3A%20TDCNet%3A%20Transparent%20Objects%20Depth%20Completion%20with%20CNN-Transformer%0A%20%20Dual-Branch%20Parallel%20Network%0AAuthor%3A%20Xianghui%20Fan%20and%20Chao%20Ye%20and%20Anping%20Deng%20and%20Xiaotian%20Wu%20and%20Mengyang%20Pan%20and%20Hang%20Yang%0AAbstract%3A%20%20%20The%20sensing%20and%20manipulation%20of%20transparent%20objects%20present%20a%20critical%0Achallenge%20in%20industrial%20and%20laboratory%20robotics.%20Conventional%20sensors%20face%0Achallenges%20in%20obtaining%20the%20full%20depth%20of%20transparent%20objects%20due%20to%20the%0Arefraction%20and%20reflection%20of%20light%20on%20their%20surfaces%20and%20their%20lack%20of%20visible%0Atexture.%20Previous%20research%20has%20attempted%20to%20obtain%20complete%20depth%20maps%20of%0Atransparent%20objects%20from%20RGB%20and%20damaged%20depth%20maps%20%28collected%20by%20depth%20sensor%29%0Ausing%20deep%20learning%20models.%20However%2C%20existing%20methods%20fail%20to%20fully%20utilize%20the%0Aoriginal%20depth%20map%2C%20resulting%20in%20limited%20accuracy%20for%20deep%20completion.%20To%20solve%0Athis%20problem%2C%20we%20propose%20TDCNet%2C%20a%20novel%20dual-branch%20CNN-Transformer%20parallel%0Anetwork%20for%20transparent%20object%20depth%20completion.%20The%20proposed%20framework%0Aconsists%20of%20two%20different%20branches%3A%20one%20extracts%20features%20from%20partial%20depth%0Amaps%2C%20while%20the%20other%20processes%20RGB-D%20images.%20Experimental%20results%20demonstrate%0Athat%20our%20model%20achieves%20state-of-the-art%20performance%20across%20multiple%20public%0Adatasets.%20Our%20code%20and%20the%20pre-trained%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/XianghuiFan/TDCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDCNet%253A%2520Transparent%2520Objects%2520Depth%2520Completion%2520with%2520CNN-Transformer%250A%2520%2520Dual-Branch%2520Parallel%2520Network%26entry.906535625%3DXianghui%2520Fan%2520and%2520Chao%2520Ye%2520and%2520Anping%2520Deng%2520and%2520Xiaotian%2520Wu%2520and%2520Mengyang%2520Pan%2520and%2520Hang%2520Yang%26entry.1292438233%3D%2520%2520The%2520sensing%2520and%2520manipulation%2520of%2520transparent%2520objects%2520present%2520a%2520critical%250Achallenge%2520in%2520industrial%2520and%2520laboratory%2520robotics.%2520Conventional%2520sensors%2520face%250Achallenges%2520in%2520obtaining%2520the%2520full%2520depth%2520of%2520transparent%2520objects%2520due%2520to%2520the%250Arefraction%2520and%2520reflection%2520of%2520light%2520on%2520their%2520surfaces%2520and%2520their%2520lack%2520of%2520visible%250Atexture.%2520Previous%2520research%2520has%2520attempted%2520to%2520obtain%2520complete%2520depth%2520maps%2520of%250Atransparent%2520objects%2520from%2520RGB%2520and%2520damaged%2520depth%2520maps%2520%2528collected%2520by%2520depth%2520sensor%2529%250Ausing%2520deep%2520learning%2520models.%2520However%252C%2520existing%2520methods%2520fail%2520to%2520fully%2520utilize%2520the%250Aoriginal%2520depth%2520map%252C%2520resulting%2520in%2520limited%2520accuracy%2520for%2520deep%2520completion.%2520To%2520solve%250Athis%2520problem%252C%2520we%2520propose%2520TDCNet%252C%2520a%2520novel%2520dual-branch%2520CNN-Transformer%2520parallel%250Anetwork%2520for%2520transparent%2520object%2520depth%2520completion.%2520The%2520proposed%2520framework%250Aconsists%2520of%2520two%2520different%2520branches%253A%2520one%2520extracts%2520features%2520from%2520partial%2520depth%250Amaps%252C%2520while%2520the%2520other%2520processes%2520RGB-D%2520images.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520public%250Adatasets.%2520Our%2520code%2520and%2520the%2520pre-trained%2520model%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/XianghuiFan/TDCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDCNet%3A%20Transparent%20Objects%20Depth%20Completion%20with%20CNN-Transformer%0A%20%20Dual-Branch%20Parallel%20Network&entry.906535625=Xianghui%20Fan%20and%20Chao%20Ye%20and%20Anping%20Deng%20and%20Xiaotian%20Wu%20and%20Mengyang%20Pan%20and%20Hang%20Yang&entry.1292438233=%20%20The%20sensing%20and%20manipulation%20of%20transparent%20objects%20present%20a%20critical%0Achallenge%20in%20industrial%20and%20laboratory%20robotics.%20Conventional%20sensors%20face%0Achallenges%20in%20obtaining%20the%20full%20depth%20of%20transparent%20objects%20due%20to%20the%0Arefraction%20and%20reflection%20of%20light%20on%20their%20surfaces%20and%20their%20lack%20of%20visible%0Atexture.%20Previous%20research%20has%20attempted%20to%20obtain%20complete%20depth%20maps%20of%0Atransparent%20objects%20from%20RGB%20and%20damaged%20depth%20maps%20%28collected%20by%20depth%20sensor%29%0Ausing%20deep%20learning%20models.%20However%2C%20existing%20methods%20fail%20to%20fully%20utilize%20the%0Aoriginal%20depth%20map%2C%20resulting%20in%20limited%20accuracy%20for%20deep%20completion.%20To%20solve%0Athis%20problem%2C%20we%20propose%20TDCNet%2C%20a%20novel%20dual-branch%20CNN-Transformer%20parallel%0Anetwork%20for%20transparent%20object%20depth%20completion.%20The%20proposed%20framework%0Aconsists%20of%20two%20different%20branches%3A%20one%20extracts%20features%20from%20partial%20depth%0Amaps%2C%20while%20the%20other%20processes%20RGB-D%20images.%20Experimental%20results%20demonstrate%0Athat%20our%20model%20achieves%20state-of-the-art%20performance%20across%20multiple%20public%0Adatasets.%20Our%20code%20and%20the%20pre-trained%20model%20are%20publicly%20available%20at%0Ahttps%3A//github.com/XianghuiFan/TDCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14961v1&entry.124074799=Read"},
{"title": "Prediction-Feedback DETR for Temporal Action Detection", "author": "Jihwan Kim and Miso Lee and Cheol-Ho Cho and Jihyun Lee and Jae-Pil Heo", "abstract": "  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n", "link": "http://arxiv.org/abs/2408.16729v3", "date": "2024-12-19", "relevancy": 2.8419, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&body=Title%3A%20Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection%0AAuthor%3A%20Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16729v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction-Feedback%2520DETR%2520for%2520Temporal%2520Action%2520Detection%26entry.906535625%3DJihwan%2520Kim%2520and%2520Miso%2520Lee%2520and%2520Cheol-Ho%2520Cho%2520and%2520Jihyun%2520Lee%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%2520is%2520fundamental%2520yet%2520challenging%2520for%2520real-world%250Avideo%2520applications.%2520Leveraging%2520the%2520unique%2520benefits%2520of%2520transformers%252C%2520various%250ADETR-based%2520approaches%2520have%2520been%2520adopted%2520in%2520TAD.%2520However%252C%2520it%2520has%2520recently%2520been%250Aidentified%2520that%2520the%2520attention%2520collapse%2520in%2520self-attention%2520causes%2520the%2520performance%250Adegradation%2520of%2520DETR%2520for%2520TAD.%2520Building%2520upon%2520previous%2520research%252C%2520this%2520paper%2520newly%250Aaddresses%2520the%2520attention%2520collapse%2520problem%2520in%2520cross-attention%2520within%2520DETR-based%250ATAD%2520methods.%2520Moreover%252C%2520our%2520findings%2520reveal%2520that%2520cross-attention%2520exhibits%250Apatterns%2520distinct%2520from%2520predictions%252C%2520indicating%2520a%2520short-cut%2520phenomenon.%2520To%250Aresolve%2520this%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520Prediction-Feedback%2520DETR%2520%2528Pred-DETR%2529%252C%250Awhich%2520utilizes%2520predictions%2520to%2520restore%2520the%2520collapse%2520and%2520align%2520the%2520cross-%2520and%250Aself-attention%2520with%2520predictions.%2520Specifically%252C%2520we%2520devise%2520novel%250Aprediction-feedback%2520objectives%2520using%2520guidance%2520from%2520the%2520relations%2520of%2520the%250Apredictions.%2520As%2520a%2520result%252C%2520Pred-DETR%2520significantly%2520alleviates%2520the%2520collapse%2520and%250Aachieves%2520state-of-the-art%2520performance%2520among%2520DETR-based%2520methods%2520on%2520various%250Achallenging%2520benchmarks%2520including%2520THUMOS14%252C%2520ActivityNet-v1.3%252C%2520HACS%252C%2520and%250AFineAction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16729v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction-Feedback%20DETR%20for%20Temporal%20Action%20Detection&entry.906535625=Jihwan%20Kim%20and%20Miso%20Lee%20and%20Cheol-Ho%20Cho%20and%20Jihyun%20Lee%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Temporal%20Action%20Detection%20%28TAD%29%20is%20fundamental%20yet%20challenging%20for%20real-world%0Avideo%20applications.%20Leveraging%20the%20unique%20benefits%20of%20transformers%2C%20various%0ADETR-based%20approaches%20have%20been%20adopted%20in%20TAD.%20However%2C%20it%20has%20recently%20been%0Aidentified%20that%20the%20attention%20collapse%20in%20self-attention%20causes%20the%20performance%0Adegradation%20of%20DETR%20for%20TAD.%20Building%20upon%20previous%20research%2C%20this%20paper%20newly%0Aaddresses%20the%20attention%20collapse%20problem%20in%20cross-attention%20within%20DETR-based%0ATAD%20methods.%20Moreover%2C%20our%20findings%20reveal%20that%20cross-attention%20exhibits%0Apatterns%20distinct%20from%20predictions%2C%20indicating%20a%20short-cut%20phenomenon.%20To%0Aresolve%20this%2C%20we%20propose%20a%20new%20framework%2C%20Prediction-Feedback%20DETR%20%28Pred-DETR%29%2C%0Awhich%20utilizes%20predictions%20to%20restore%20the%20collapse%20and%20align%20the%20cross-%20and%0Aself-attention%20with%20predictions.%20Specifically%2C%20we%20devise%20novel%0Aprediction-feedback%20objectives%20using%20guidance%20from%20the%20relations%20of%20the%0Apredictions.%20As%20a%20result%2C%20Pred-DETR%20significantly%20alleviates%20the%20collapse%20and%0Aachieves%20state-of-the-art%20performance%20among%20DETR-based%20methods%20on%20various%0Achallenging%20benchmarks%20including%20THUMOS14%2C%20ActivityNet-v1.3%2C%20HACS%2C%20and%0AFineAction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16729v3&entry.124074799=Read"},
{"title": "MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance", "author": "Hallee E. Wong and Jose Javier Gonzalez Ortiz and John Guttag and Adrian V. Dalca", "abstract": "  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n", "link": "http://arxiv.org/abs/2412.15058v1", "date": "2024-12-19", "relevancy": 2.7653, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5608}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiverSeg%3A%20Scalable%20Interactive%20Segmentation%20of%20Biomedical%20Imaging%0A%20%20Datasets%20with%20In-Context%20Guidance&body=Title%3A%20MultiverSeg%3A%20Scalable%20Interactive%20Segmentation%20of%20Biomedical%20Imaging%0A%20%20Datasets%20with%20In-Context%20Guidance%0AAuthor%3A%20Hallee%20E.%20Wong%20and%20Jose%20Javier%20Gonzalez%20Ortiz%20and%20John%20Guttag%20and%20Adrian%20V.%20Dalca%0AAbstract%3A%20%20%20Medical%20researchers%20and%20clinicians%20often%20need%20to%20perform%20novel%20segmentation%0Atasks%20on%20a%20set%20of%20related%20images.%20Existing%20methods%20for%20segmenting%20a%20new%20dataset%0Aare%20either%20interactive%2C%20requiring%20substantial%20human%20effort%20for%20each%20image%2C%20or%0Arequire%20an%20existing%20set%20of%20manually%20labeled%20images.%20We%20introduce%20a%20system%2C%0AMultiverSeg%2C%20that%20enables%20practitioners%20to%20rapidly%20segment%20an%20entire%20new%0Adataset%20without%20requiring%20access%20to%20any%20existing%20labeled%20data%20from%20that%20task%20or%0Adomain.%20Along%20with%20the%20image%20to%20segment%2C%20the%20model%20takes%20user%20interactions%20such%0Aas%20clicks%2C%20bounding%20boxes%20or%20scribbles%20as%20input%2C%20and%20predicts%20a%20segmentation.%0AAs%20the%20user%20segments%20more%20images%2C%20those%20images%20and%20segmentations%20become%0Aadditional%20inputs%20to%20the%20model%2C%20providing%20context.%20As%20the%20context%20set%20of%0Alabeled%20images%20grows%2C%20the%20number%20of%20interactions%20required%20to%20segment%20each%20new%0Aimage%20decreases.%20We%20demonstrate%20that%20MultiverSeg%20enables%20users%20to%20interactively%0Asegment%20new%20datasets%20efficiently%2C%20by%20amortizing%20the%20number%20of%20interactions%20per%0Aimage%20to%20achieve%20an%20accurate%20segmentation.%20Compared%20to%20using%20a%20state-of-the-art%0Ainteractive%20segmentation%20method%2C%20using%20MultiverSeg%20reduced%20the%20total%20number%20of%0Ascribble%20steps%20by%2053%25%20and%20clicks%20by%2036%25%20to%20achieve%2090%25%20Dice%20on%20sets%20of%20images%0Afrom%20unseen%20tasks.%20We%20release%20code%20and%20model%20weights%20at%0Ahttps%3A//multiverseg.csail.mit.edu%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiverSeg%253A%2520Scalable%2520Interactive%2520Segmentation%2520of%2520Biomedical%2520Imaging%250A%2520%2520Datasets%2520with%2520In-Context%2520Guidance%26entry.906535625%3DHallee%2520E.%2520Wong%2520and%2520Jose%2520Javier%2520Gonzalez%2520Ortiz%2520and%2520John%2520Guttag%2520and%2520Adrian%2520V.%2520Dalca%26entry.1292438233%3D%2520%2520Medical%2520researchers%2520and%2520clinicians%2520often%2520need%2520to%2520perform%2520novel%2520segmentation%250Atasks%2520on%2520a%2520set%2520of%2520related%2520images.%2520Existing%2520methods%2520for%2520segmenting%2520a%2520new%2520dataset%250Aare%2520either%2520interactive%252C%2520requiring%2520substantial%2520human%2520effort%2520for%2520each%2520image%252C%2520or%250Arequire%2520an%2520existing%2520set%2520of%2520manually%2520labeled%2520images.%2520We%2520introduce%2520a%2520system%252C%250AMultiverSeg%252C%2520that%2520enables%2520practitioners%2520to%2520rapidly%2520segment%2520an%2520entire%2520new%250Adataset%2520without%2520requiring%2520access%2520to%2520any%2520existing%2520labeled%2520data%2520from%2520that%2520task%2520or%250Adomain.%2520Along%2520with%2520the%2520image%2520to%2520segment%252C%2520the%2520model%2520takes%2520user%2520interactions%2520such%250Aas%2520clicks%252C%2520bounding%2520boxes%2520or%2520scribbles%2520as%2520input%252C%2520and%2520predicts%2520a%2520segmentation.%250AAs%2520the%2520user%2520segments%2520more%2520images%252C%2520those%2520images%2520and%2520segmentations%2520become%250Aadditional%2520inputs%2520to%2520the%2520model%252C%2520providing%2520context.%2520As%2520the%2520context%2520set%2520of%250Alabeled%2520images%2520grows%252C%2520the%2520number%2520of%2520interactions%2520required%2520to%2520segment%2520each%2520new%250Aimage%2520decreases.%2520We%2520demonstrate%2520that%2520MultiverSeg%2520enables%2520users%2520to%2520interactively%250Asegment%2520new%2520datasets%2520efficiently%252C%2520by%2520amortizing%2520the%2520number%2520of%2520interactions%2520per%250Aimage%2520to%2520achieve%2520an%2520accurate%2520segmentation.%2520Compared%2520to%2520using%2520a%2520state-of-the-art%250Ainteractive%2520segmentation%2520method%252C%2520using%2520MultiverSeg%2520reduced%2520the%2520total%2520number%2520of%250Ascribble%2520steps%2520by%252053%2525%2520and%2520clicks%2520by%252036%2525%2520to%2520achieve%252090%2525%2520Dice%2520on%2520sets%2520of%2520images%250Afrom%2520unseen%2520tasks.%2520We%2520release%2520code%2520and%2520model%2520weights%2520at%250Ahttps%253A//multiverseg.csail.mit.edu%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiverSeg%3A%20Scalable%20Interactive%20Segmentation%20of%20Biomedical%20Imaging%0A%20%20Datasets%20with%20In-Context%20Guidance&entry.906535625=Hallee%20E.%20Wong%20and%20Jose%20Javier%20Gonzalez%20Ortiz%20and%20John%20Guttag%20and%20Adrian%20V.%20Dalca&entry.1292438233=%20%20Medical%20researchers%20and%20clinicians%20often%20need%20to%20perform%20novel%20segmentation%0Atasks%20on%20a%20set%20of%20related%20images.%20Existing%20methods%20for%20segmenting%20a%20new%20dataset%0Aare%20either%20interactive%2C%20requiring%20substantial%20human%20effort%20for%20each%20image%2C%20or%0Arequire%20an%20existing%20set%20of%20manually%20labeled%20images.%20We%20introduce%20a%20system%2C%0AMultiverSeg%2C%20that%20enables%20practitioners%20to%20rapidly%20segment%20an%20entire%20new%0Adataset%20without%20requiring%20access%20to%20any%20existing%20labeled%20data%20from%20that%20task%20or%0Adomain.%20Along%20with%20the%20image%20to%20segment%2C%20the%20model%20takes%20user%20interactions%20such%0Aas%20clicks%2C%20bounding%20boxes%20or%20scribbles%20as%20input%2C%20and%20predicts%20a%20segmentation.%0AAs%20the%20user%20segments%20more%20images%2C%20those%20images%20and%20segmentations%20become%0Aadditional%20inputs%20to%20the%20model%2C%20providing%20context.%20As%20the%20context%20set%20of%0Alabeled%20images%20grows%2C%20the%20number%20of%20interactions%20required%20to%20segment%20each%20new%0Aimage%20decreases.%20We%20demonstrate%20that%20MultiverSeg%20enables%20users%20to%20interactively%0Asegment%20new%20datasets%20efficiently%2C%20by%20amortizing%20the%20number%20of%20interactions%20per%0Aimage%20to%20achieve%20an%20accurate%20segmentation.%20Compared%20to%20using%20a%20state-of-the-art%0Ainteractive%20segmentation%20method%2C%20using%20MultiverSeg%20reduced%20the%20total%20number%20of%0Ascribble%20steps%20by%2053%25%20and%20clicks%20by%2036%25%20to%20achieve%2090%25%20Dice%20on%20sets%20of%20images%0Afrom%20unseen%20tasks.%20We%20release%20code%20and%20model%20weights%20at%0Ahttps%3A//multiverseg.csail.mit.edu%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15058v1&entry.124074799=Read"},
{"title": "DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with\n  Selective State Space Models", "author": "Haonan Yuan and Qingyun Sun and Zhaonan Wang and Xingcheng Fu and Cheng Ji and Yongjian Wang and Bo Jin and Jianxin Li", "abstract": "  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,\nwidely existing in the real world. Nevertheless, the structure incompleteness,\nnoise, and redundancy result in poor robustness for Dynamic Graph Neural\nNetworks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising\nway to optimize graph structures. However, aside from encountering unacceptable\nquadratic complexity, it overly relies on heuristic priors, making it hard to\ndiscover underlying predictive patterns. How to efficiently refine the dynamic\nstructures, capture intrinsic dependencies, and learn robust representations,\nremains under-explored. In this work, we propose the novel DG-Mamba, a robust\nand efficient Dynamic Graph structure learning framework with the Selective\nState Space Models (Mamba). To accelerate the spatio-temporal structure\nlearning, we propose a kernelized dynamic message-passing operator that reduces\nthe quadratic time complexity to linear. To capture global intrinsic dynamics,\nwe establish the dynamic graph as a self-contained system with State Space\nModel. By discretizing the system states with the cross-snapshot graph\nadjacency, we enable the long-distance dependencies capturing with the\nselective snapshot scan. To endow learned dynamic structures more expressive\nwith informativeness, we propose the self-supervised Principle of Relevant\nInformation for DGSL to regularize the most relevant yet least redundant\ninformation, enhancing global robustness. Extensive experiments demonstrate the\nsuperiority of the robustness and efficiency of our DG-Mamba compared with the\nstate-of-the-art baselines against adversarial attacks.\n", "link": "http://arxiv.org/abs/2412.08160v4", "date": "2024-12-19", "relevancy": 2.7628, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5898}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5359}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DG-Mamba%3A%20Robust%20and%20Efficient%20Dynamic%20Graph%20Structure%20Learning%20with%0A%20%20Selective%20State%20Space%20Models&body=Title%3A%20DG-Mamba%3A%20Robust%20and%20Efficient%20Dynamic%20Graph%20Structure%20Learning%20with%0A%20%20Selective%20State%20Space%20Models%0AAuthor%3A%20Haonan%20Yuan%20and%20Qingyun%20Sun%20and%20Zhaonan%20Wang%20and%20Xingcheng%20Fu%20and%20Cheng%20Ji%20and%20Yongjian%20Wang%20and%20Bo%20Jin%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Dynamic%20graphs%20exhibit%20intertwined%20spatio-temporal%20evolutionary%20patterns%2C%0Awidely%20existing%20in%20the%20real%20world.%20Nevertheless%2C%20the%20structure%20incompleteness%2C%0Anoise%2C%20and%20redundancy%20result%20in%20poor%20robustness%20for%20Dynamic%20Graph%20Neural%0ANetworks%20%28DGNNs%29.%20Dynamic%20Graph%20Structure%20Learning%20%28DGSL%29%20offers%20a%20promising%0Away%20to%20optimize%20graph%20structures.%20However%2C%20aside%20from%20encountering%20unacceptable%0Aquadratic%20complexity%2C%20it%20overly%20relies%20on%20heuristic%20priors%2C%20making%20it%20hard%20to%0Adiscover%20underlying%20predictive%20patterns.%20How%20to%20efficiently%20refine%20the%20dynamic%0Astructures%2C%20capture%20intrinsic%20dependencies%2C%20and%20learn%20robust%20representations%2C%0Aremains%20under-explored.%20In%20this%20work%2C%20we%20propose%20the%20novel%20DG-Mamba%2C%20a%20robust%0Aand%20efficient%20Dynamic%20Graph%20structure%20learning%20framework%20with%20the%20Selective%0AState%20Space%20Models%20%28Mamba%29.%20To%20accelerate%20the%20spatio-temporal%20structure%0Alearning%2C%20we%20propose%20a%20kernelized%20dynamic%20message-passing%20operator%20that%20reduces%0Athe%20quadratic%20time%20complexity%20to%20linear.%20To%20capture%20global%20intrinsic%20dynamics%2C%0Awe%20establish%20the%20dynamic%20graph%20as%20a%20self-contained%20system%20with%20State%20Space%0AModel.%20By%20discretizing%20the%20system%20states%20with%20the%20cross-snapshot%20graph%0Aadjacency%2C%20we%20enable%20the%20long-distance%20dependencies%20capturing%20with%20the%0Aselective%20snapshot%20scan.%20To%20endow%20learned%20dynamic%20structures%20more%20expressive%0Awith%20informativeness%2C%20we%20propose%20the%20self-supervised%20Principle%20of%20Relevant%0AInformation%20for%20DGSL%20to%20regularize%20the%20most%20relevant%20yet%20least%20redundant%0Ainformation%2C%20enhancing%20global%20robustness.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20the%20robustness%20and%20efficiency%20of%20our%20DG-Mamba%20compared%20with%20the%0Astate-of-the-art%20baselines%20against%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08160v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDG-Mamba%253A%2520Robust%2520and%2520Efficient%2520Dynamic%2520Graph%2520Structure%2520Learning%2520with%250A%2520%2520Selective%2520State%2520Space%2520Models%26entry.906535625%3DHaonan%2520Yuan%2520and%2520Qingyun%2520Sun%2520and%2520Zhaonan%2520Wang%2520and%2520Xingcheng%2520Fu%2520and%2520Cheng%2520Ji%2520and%2520Yongjian%2520Wang%2520and%2520Bo%2520Jin%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Dynamic%2520graphs%2520exhibit%2520intertwined%2520spatio-temporal%2520evolutionary%2520patterns%252C%250Awidely%2520existing%2520in%2520the%2520real%2520world.%2520Nevertheless%252C%2520the%2520structure%2520incompleteness%252C%250Anoise%252C%2520and%2520redundancy%2520result%2520in%2520poor%2520robustness%2520for%2520Dynamic%2520Graph%2520Neural%250ANetworks%2520%2528DGNNs%2529.%2520Dynamic%2520Graph%2520Structure%2520Learning%2520%2528DGSL%2529%2520offers%2520a%2520promising%250Away%2520to%2520optimize%2520graph%2520structures.%2520However%252C%2520aside%2520from%2520encountering%2520unacceptable%250Aquadratic%2520complexity%252C%2520it%2520overly%2520relies%2520on%2520heuristic%2520priors%252C%2520making%2520it%2520hard%2520to%250Adiscover%2520underlying%2520predictive%2520patterns.%2520How%2520to%2520efficiently%2520refine%2520the%2520dynamic%250Astructures%252C%2520capture%2520intrinsic%2520dependencies%252C%2520and%2520learn%2520robust%2520representations%252C%250Aremains%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520novel%2520DG-Mamba%252C%2520a%2520robust%250Aand%2520efficient%2520Dynamic%2520Graph%2520structure%2520learning%2520framework%2520with%2520the%2520Selective%250AState%2520Space%2520Models%2520%2528Mamba%2529.%2520To%2520accelerate%2520the%2520spatio-temporal%2520structure%250Alearning%252C%2520we%2520propose%2520a%2520kernelized%2520dynamic%2520message-passing%2520operator%2520that%2520reduces%250Athe%2520quadratic%2520time%2520complexity%2520to%2520linear.%2520To%2520capture%2520global%2520intrinsic%2520dynamics%252C%250Awe%2520establish%2520the%2520dynamic%2520graph%2520as%2520a%2520self-contained%2520system%2520with%2520State%2520Space%250AModel.%2520By%2520discretizing%2520the%2520system%2520states%2520with%2520the%2520cross-snapshot%2520graph%250Aadjacency%252C%2520we%2520enable%2520the%2520long-distance%2520dependencies%2520capturing%2520with%2520the%250Aselective%2520snapshot%2520scan.%2520To%2520endow%2520learned%2520dynamic%2520structures%2520more%2520expressive%250Awith%2520informativeness%252C%2520we%2520propose%2520the%2520self-supervised%2520Principle%2520of%2520Relevant%250AInformation%2520for%2520DGSL%2520to%2520regularize%2520the%2520most%2520relevant%2520yet%2520least%2520redundant%250Ainformation%252C%2520enhancing%2520global%2520robustness.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520robustness%2520and%2520efficiency%2520of%2520our%2520DG-Mamba%2520compared%2520with%2520the%250Astate-of-the-art%2520baselines%2520against%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08160v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DG-Mamba%3A%20Robust%20and%20Efficient%20Dynamic%20Graph%20Structure%20Learning%20with%0A%20%20Selective%20State%20Space%20Models&entry.906535625=Haonan%20Yuan%20and%20Qingyun%20Sun%20and%20Zhaonan%20Wang%20and%20Xingcheng%20Fu%20and%20Cheng%20Ji%20and%20Yongjian%20Wang%20and%20Bo%20Jin%20and%20Jianxin%20Li&entry.1292438233=%20%20Dynamic%20graphs%20exhibit%20intertwined%20spatio-temporal%20evolutionary%20patterns%2C%0Awidely%20existing%20in%20the%20real%20world.%20Nevertheless%2C%20the%20structure%20incompleteness%2C%0Anoise%2C%20and%20redundancy%20result%20in%20poor%20robustness%20for%20Dynamic%20Graph%20Neural%0ANetworks%20%28DGNNs%29.%20Dynamic%20Graph%20Structure%20Learning%20%28DGSL%29%20offers%20a%20promising%0Away%20to%20optimize%20graph%20structures.%20However%2C%20aside%20from%20encountering%20unacceptable%0Aquadratic%20complexity%2C%20it%20overly%20relies%20on%20heuristic%20priors%2C%20making%20it%20hard%20to%0Adiscover%20underlying%20predictive%20patterns.%20How%20to%20efficiently%20refine%20the%20dynamic%0Astructures%2C%20capture%20intrinsic%20dependencies%2C%20and%20learn%20robust%20representations%2C%0Aremains%20under-explored.%20In%20this%20work%2C%20we%20propose%20the%20novel%20DG-Mamba%2C%20a%20robust%0Aand%20efficient%20Dynamic%20Graph%20structure%20learning%20framework%20with%20the%20Selective%0AState%20Space%20Models%20%28Mamba%29.%20To%20accelerate%20the%20spatio-temporal%20structure%0Alearning%2C%20we%20propose%20a%20kernelized%20dynamic%20message-passing%20operator%20that%20reduces%0Athe%20quadratic%20time%20complexity%20to%20linear.%20To%20capture%20global%20intrinsic%20dynamics%2C%0Awe%20establish%20the%20dynamic%20graph%20as%20a%20self-contained%20system%20with%20State%20Space%0AModel.%20By%20discretizing%20the%20system%20states%20with%20the%20cross-snapshot%20graph%0Aadjacency%2C%20we%20enable%20the%20long-distance%20dependencies%20capturing%20with%20the%0Aselective%20snapshot%20scan.%20To%20endow%20learned%20dynamic%20structures%20more%20expressive%0Awith%20informativeness%2C%20we%20propose%20the%20self-supervised%20Principle%20of%20Relevant%0AInformation%20for%20DGSL%20to%20regularize%20the%20most%20relevant%20yet%20least%20redundant%0Ainformation%2C%20enhancing%20global%20robustness.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20the%20robustness%20and%20efficiency%20of%20our%20DG-Mamba%20compared%20with%20the%0Astate-of-the-art%20baselines%20against%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08160v4&entry.124074799=Read"},
{"title": "G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o", "author": "Tony Cheng Tong and Sirui He and Zhiwen Shao and Dit-Yan Yeung", "abstract": "  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n", "link": "http://arxiv.org/abs/2412.13647v2", "date": "2024-12-19", "relevancy": 2.7403, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-VEval%3A%20A%20Versatile%20Metric%20for%20Evaluating%20Image%20and%20Video%20Captions%0A%20%20Using%20GPT-4o&body=Title%3A%20G-VEval%3A%20A%20Versatile%20Metric%20for%20Evaluating%20Image%20and%20Video%20Captions%0A%20%20Using%20GPT-4o%0AAuthor%3A%20Tony%20Cheng%20Tong%20and%20Sirui%20He%20and%20Zhiwen%20Shao%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Evaluation%20metric%20of%20visual%20captioning%20is%20important%20yet%20not%20thoroughly%0Aexplored.%20Traditional%20metrics%20like%20BLEU%2C%20METEOR%2C%20CIDEr%2C%20and%20ROUGE%20often%20miss%0Asemantic%20depth%2C%20while%20trained%20metrics%20such%20as%20CLIP-Score%2C%20PAC-S%2C%20and%20Polos%20are%0Alimited%20in%20zero-shot%20scenarios.%20Advanced%20Language%20Model-based%20metrics%20also%0Astruggle%20with%20aligning%20to%20nuanced%20human%20preferences.%20To%20address%20these%20issues%2C%0Awe%20introduce%20G-VEval%2C%20a%20novel%20metric%20inspired%20by%20G-Eval%20and%20powered%20by%20the%20new%0AGPT-4o.%20G-VEval%20uses%20chain-of-thought%20reasoning%20in%20large%20multimodal%20models%20and%0Asupports%20three%20modes%3A%20reference-free%2C%20reference-only%2C%20and%20combined%2C%0Aaccommodating%20both%20video%20and%20image%20inputs.%20We%20also%20propose%20MSVD-Eval%2C%20a%20new%0Adataset%20for%20video%20captioning%20evaluation%2C%20to%20establish%20a%20more%20transparent%20and%0Aconsistent%20framework%20for%20both%20human%20experts%20and%20evaluation%20metrics.%20It%20is%0Adesigned%20to%20address%20the%20lack%20of%20clear%20criteria%20in%20existing%20datasets%20by%0Aintroducing%20distinct%20dimensions%20of%20Accuracy%2C%20Completeness%2C%20Conciseness%2C%20and%0ARelevance%20%28ACCR%29.%20Extensive%20results%20show%20that%20G-VEval%20outperforms%20existing%0Amethods%20in%20correlation%20with%20human%20annotations%2C%20as%20measured%20by%20Kendall%20tau-b%20and%0AKendall%20tau-c.%20This%20provides%20a%20flexible%20solution%20for%20diverse%20captioning%20tasks%0Aand%20suggests%20a%20straightforward%20yet%20effective%20approach%20for%20large%20language%20models%0Ato%20understand%20video%20content%2C%20paving%20the%20way%20for%20advancements%20in%20automated%0Acaptioning.%20Codes%20are%20available%20at%20https%3A//github.com/ztangaj/gveval%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-VEval%253A%2520A%2520Versatile%2520Metric%2520for%2520Evaluating%2520Image%2520and%2520Video%2520Captions%250A%2520%2520Using%2520GPT-4o%26entry.906535625%3DTony%2520Cheng%2520Tong%2520and%2520Sirui%2520He%2520and%2520Zhiwen%2520Shao%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Evaluation%2520metric%2520of%2520visual%2520captioning%2520is%2520important%2520yet%2520not%2520thoroughly%250Aexplored.%2520Traditional%2520metrics%2520like%2520BLEU%252C%2520METEOR%252C%2520CIDEr%252C%2520and%2520ROUGE%2520often%2520miss%250Asemantic%2520depth%252C%2520while%2520trained%2520metrics%2520such%2520as%2520CLIP-Score%252C%2520PAC-S%252C%2520and%2520Polos%2520are%250Alimited%2520in%2520zero-shot%2520scenarios.%2520Advanced%2520Language%2520Model-based%2520metrics%2520also%250Astruggle%2520with%2520aligning%2520to%2520nuanced%2520human%2520preferences.%2520To%2520address%2520these%2520issues%252C%250Awe%2520introduce%2520G-VEval%252C%2520a%2520novel%2520metric%2520inspired%2520by%2520G-Eval%2520and%2520powered%2520by%2520the%2520new%250AGPT-4o.%2520G-VEval%2520uses%2520chain-of-thought%2520reasoning%2520in%2520large%2520multimodal%2520models%2520and%250Asupports%2520three%2520modes%253A%2520reference-free%252C%2520reference-only%252C%2520and%2520combined%252C%250Aaccommodating%2520both%2520video%2520and%2520image%2520inputs.%2520We%2520also%2520propose%2520MSVD-Eval%252C%2520a%2520new%250Adataset%2520for%2520video%2520captioning%2520evaluation%252C%2520to%2520establish%2520a%2520more%2520transparent%2520and%250Aconsistent%2520framework%2520for%2520both%2520human%2520experts%2520and%2520evaluation%2520metrics.%2520It%2520is%250Adesigned%2520to%2520address%2520the%2520lack%2520of%2520clear%2520criteria%2520in%2520existing%2520datasets%2520by%250Aintroducing%2520distinct%2520dimensions%2520of%2520Accuracy%252C%2520Completeness%252C%2520Conciseness%252C%2520and%250ARelevance%2520%2528ACCR%2529.%2520Extensive%2520results%2520show%2520that%2520G-VEval%2520outperforms%2520existing%250Amethods%2520in%2520correlation%2520with%2520human%2520annotations%252C%2520as%2520measured%2520by%2520Kendall%2520tau-b%2520and%250AKendall%2520tau-c.%2520This%2520provides%2520a%2520flexible%2520solution%2520for%2520diverse%2520captioning%2520tasks%250Aand%2520suggests%2520a%2520straightforward%2520yet%2520effective%2520approach%2520for%2520large%2520language%2520models%250Ato%2520understand%2520video%2520content%252C%2520paving%2520the%2520way%2520for%2520advancements%2520in%2520automated%250Acaptioning.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/ztangaj/gveval%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-VEval%3A%20A%20Versatile%20Metric%20for%20Evaluating%20Image%20and%20Video%20Captions%0A%20%20Using%20GPT-4o&entry.906535625=Tony%20Cheng%20Tong%20and%20Sirui%20He%20and%20Zhiwen%20Shao%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Evaluation%20metric%20of%20visual%20captioning%20is%20important%20yet%20not%20thoroughly%0Aexplored.%20Traditional%20metrics%20like%20BLEU%2C%20METEOR%2C%20CIDEr%2C%20and%20ROUGE%20often%20miss%0Asemantic%20depth%2C%20while%20trained%20metrics%20such%20as%20CLIP-Score%2C%20PAC-S%2C%20and%20Polos%20are%0Alimited%20in%20zero-shot%20scenarios.%20Advanced%20Language%20Model-based%20metrics%20also%0Astruggle%20with%20aligning%20to%20nuanced%20human%20preferences.%20To%20address%20these%20issues%2C%0Awe%20introduce%20G-VEval%2C%20a%20novel%20metric%20inspired%20by%20G-Eval%20and%20powered%20by%20the%20new%0AGPT-4o.%20G-VEval%20uses%20chain-of-thought%20reasoning%20in%20large%20multimodal%20models%20and%0Asupports%20three%20modes%3A%20reference-free%2C%20reference-only%2C%20and%20combined%2C%0Aaccommodating%20both%20video%20and%20image%20inputs.%20We%20also%20propose%20MSVD-Eval%2C%20a%20new%0Adataset%20for%20video%20captioning%20evaluation%2C%20to%20establish%20a%20more%20transparent%20and%0Aconsistent%20framework%20for%20both%20human%20experts%20and%20evaluation%20metrics.%20It%20is%0Adesigned%20to%20address%20the%20lack%20of%20clear%20criteria%20in%20existing%20datasets%20by%0Aintroducing%20distinct%20dimensions%20of%20Accuracy%2C%20Completeness%2C%20Conciseness%2C%20and%0ARelevance%20%28ACCR%29.%20Extensive%20results%20show%20that%20G-VEval%20outperforms%20existing%0Amethods%20in%20correlation%20with%20human%20annotations%2C%20as%20measured%20by%20Kendall%20tau-b%20and%0AKendall%20tau-c.%20This%20provides%20a%20flexible%20solution%20for%20diverse%20captioning%20tasks%0Aand%20suggests%20a%20straightforward%20yet%20effective%20approach%20for%20large%20language%20models%0Ato%20understand%20video%20content%2C%20paving%20the%20way%20for%20advancements%20in%20automated%0Acaptioning.%20Codes%20are%20available%20at%20https%3A//github.com/ztangaj/gveval%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13647v2&entry.124074799=Read"},
{"title": "Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos", "author": "Haitao Tian and Pierre Payeur", "abstract": "  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2412.14988v1", "date": "2024-12-19", "relevancy": 2.7354, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.541}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stitch%20Contrast%20and%20Segment_Learning%20a%20Human%20Action%20Segmentation%20Model%0A%20%20Using%20Trimmed%20Skeleton%20Videos&body=Title%3A%20Stitch%20Contrast%20and%20Segment_Learning%20a%20Human%20Action%20Segmentation%20Model%0A%20%20Using%20Trimmed%20Skeleton%20Videos%0AAuthor%3A%20Haitao%20Tian%20and%20Pierre%20Payeur%0AAbstract%3A%20%20%20Existing%20skeleton-based%20human%20action%20classification%20models%20rely%20on%0Awell-trimmed%20action-specific%20skeleton%20videos%20for%20both%20training%20and%20testing%2C%0Aprecluding%20their%20scalability%20to%20real-world%20applications%20where%20untrimmed%20videos%0Aexhibiting%20concatenated%20actions%20are%20predominant.%20To%20overcome%20this%20limitation%2C%0Arecently%20introduced%20skeleton%20action%20segmentation%20models%20involve%20un-trimmed%0Askeleton%20videos%20into%20end-to-end%20training.%20The%20model%20is%20optimized%20to%20provide%0Aframe-wise%20predictions%20for%20any%20length%20of%20testing%20videos%2C%20simultaneously%0Arealizing%20action%20localization%20and%20classification.%20Yet%2C%20achieving%20such%20an%0Aimprovement%20im-poses%20frame-wise%20annotated%20skeleton%20videos%2C%20which%20remains%0Atime-consuming%20in%20practice.%20This%20paper%20features%20a%20novel%20framework%20for%0Askeleton-based%20action%20segmentation%20trained%20on%20short%20trimmed%20skeleton%20videos%2C%0Abut%20that%20can%20run%20on%20longer%20un-trimmed%20videos.%20The%20approach%20is%20implemented%20in%0Athree%20steps%3A%20Stitch%2C%20Contrast%2C%20and%20Segment.%20First%2C%20Stitch%20proposes%20a%20tem-poral%0Askeleton%20stitching%20scheme%20that%20treats%20trimmed%20skeleton%20videos%20as%20elementary%0Ahuman%20motions%20that%20compose%20a%20semantic%20space%20and%20can%20be%20sampled%20to%20generate%0Amulti-action%20stitched%20se-quences.%20Contrast%20learns%20contrastive%20representations%0Afrom%20stitched%20sequences%20with%20a%20novel%20discrimination%20pretext%20task%20that%20enables%20a%0Askeleton%20encoder%20to%20learn%20meaningful%20action-temporal%20contexts%20to%20improve%20action%0Asegmentation.%20Finally%2C%20Segment%20relates%20the%20proposed%20method%20to%20action%0Asegmentation%20by%20learning%20a%20segmentation%20layer%20while%20handling%20particular%20da-ta%0Aavailability.%20Experiments%20involve%20a%20trimmed%20source%20dataset%20and%20an%20untrimmed%0Atarget%20dataset%20in%20an%20adaptation%20formulation%20for%20real-world%20skeleton-based%20human%0Aaction%20segmentation%20to%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStitch%2520Contrast%2520and%2520Segment_Learning%2520a%2520Human%2520Action%2520Segmentation%2520Model%250A%2520%2520Using%2520Trimmed%2520Skeleton%2520Videos%26entry.906535625%3DHaitao%2520Tian%2520and%2520Pierre%2520Payeur%26entry.1292438233%3D%2520%2520Existing%2520skeleton-based%2520human%2520action%2520classification%2520models%2520rely%2520on%250Awell-trimmed%2520action-specific%2520skeleton%2520videos%2520for%2520both%2520training%2520and%2520testing%252C%250Aprecluding%2520their%2520scalability%2520to%2520real-world%2520applications%2520where%2520untrimmed%2520videos%250Aexhibiting%2520concatenated%2520actions%2520are%2520predominant.%2520To%2520overcome%2520this%2520limitation%252C%250Arecently%2520introduced%2520skeleton%2520action%2520segmentation%2520models%2520involve%2520un-trimmed%250Askeleton%2520videos%2520into%2520end-to-end%2520training.%2520The%2520model%2520is%2520optimized%2520to%2520provide%250Aframe-wise%2520predictions%2520for%2520any%2520length%2520of%2520testing%2520videos%252C%2520simultaneously%250Arealizing%2520action%2520localization%2520and%2520classification.%2520Yet%252C%2520achieving%2520such%2520an%250Aimprovement%2520im-poses%2520frame-wise%2520annotated%2520skeleton%2520videos%252C%2520which%2520remains%250Atime-consuming%2520in%2520practice.%2520This%2520paper%2520features%2520a%2520novel%2520framework%2520for%250Askeleton-based%2520action%2520segmentation%2520trained%2520on%2520short%2520trimmed%2520skeleton%2520videos%252C%250Abut%2520that%2520can%2520run%2520on%2520longer%2520un-trimmed%2520videos.%2520The%2520approach%2520is%2520implemented%2520in%250Athree%2520steps%253A%2520Stitch%252C%2520Contrast%252C%2520and%2520Segment.%2520First%252C%2520Stitch%2520proposes%2520a%2520tem-poral%250Askeleton%2520stitching%2520scheme%2520that%2520treats%2520trimmed%2520skeleton%2520videos%2520as%2520elementary%250Ahuman%2520motions%2520that%2520compose%2520a%2520semantic%2520space%2520and%2520can%2520be%2520sampled%2520to%2520generate%250Amulti-action%2520stitched%2520se-quences.%2520Contrast%2520learns%2520contrastive%2520representations%250Afrom%2520stitched%2520sequences%2520with%2520a%2520novel%2520discrimination%2520pretext%2520task%2520that%2520enables%2520a%250Askeleton%2520encoder%2520to%2520learn%2520meaningful%2520action-temporal%2520contexts%2520to%2520improve%2520action%250Asegmentation.%2520Finally%252C%2520Segment%2520relates%2520the%2520proposed%2520method%2520to%2520action%250Asegmentation%2520by%2520learning%2520a%2520segmentation%2520layer%2520while%2520handling%2520particular%2520da-ta%250Aavailability.%2520Experiments%2520involve%2520a%2520trimmed%2520source%2520dataset%2520and%2520an%2520untrimmed%250Atarget%2520dataset%2520in%2520an%2520adaptation%2520formulation%2520for%2520real-world%2520skeleton-based%2520human%250Aaction%2520segmentation%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stitch%20Contrast%20and%20Segment_Learning%20a%20Human%20Action%20Segmentation%20Model%0A%20%20Using%20Trimmed%20Skeleton%20Videos&entry.906535625=Haitao%20Tian%20and%20Pierre%20Payeur&entry.1292438233=%20%20Existing%20skeleton-based%20human%20action%20classification%20models%20rely%20on%0Awell-trimmed%20action-specific%20skeleton%20videos%20for%20both%20training%20and%20testing%2C%0Aprecluding%20their%20scalability%20to%20real-world%20applications%20where%20untrimmed%20videos%0Aexhibiting%20concatenated%20actions%20are%20predominant.%20To%20overcome%20this%20limitation%2C%0Arecently%20introduced%20skeleton%20action%20segmentation%20models%20involve%20un-trimmed%0Askeleton%20videos%20into%20end-to-end%20training.%20The%20model%20is%20optimized%20to%20provide%0Aframe-wise%20predictions%20for%20any%20length%20of%20testing%20videos%2C%20simultaneously%0Arealizing%20action%20localization%20and%20classification.%20Yet%2C%20achieving%20such%20an%0Aimprovement%20im-poses%20frame-wise%20annotated%20skeleton%20videos%2C%20which%20remains%0Atime-consuming%20in%20practice.%20This%20paper%20features%20a%20novel%20framework%20for%0Askeleton-based%20action%20segmentation%20trained%20on%20short%20trimmed%20skeleton%20videos%2C%0Abut%20that%20can%20run%20on%20longer%20un-trimmed%20videos.%20The%20approach%20is%20implemented%20in%0Athree%20steps%3A%20Stitch%2C%20Contrast%2C%20and%20Segment.%20First%2C%20Stitch%20proposes%20a%20tem-poral%0Askeleton%20stitching%20scheme%20that%20treats%20trimmed%20skeleton%20videos%20as%20elementary%0Ahuman%20motions%20that%20compose%20a%20semantic%20space%20and%20can%20be%20sampled%20to%20generate%0Amulti-action%20stitched%20se-quences.%20Contrast%20learns%20contrastive%20representations%0Afrom%20stitched%20sequences%20with%20a%20novel%20discrimination%20pretext%20task%20that%20enables%20a%0Askeleton%20encoder%20to%20learn%20meaningful%20action-temporal%20contexts%20to%20improve%20action%0Asegmentation.%20Finally%2C%20Segment%20relates%20the%20proposed%20method%20to%20action%0Asegmentation%20by%20learning%20a%20segmentation%20layer%20while%20handling%20particular%20da-ta%0Aavailability.%20Experiments%20involve%20a%20trimmed%20source%20dataset%20and%20an%20untrimmed%0Atarget%20dataset%20in%20an%20adaptation%20formulation%20for%20real-world%20skeleton-based%20human%0Aaction%20segmentation%20to%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14988v1&entry.124074799=Read"},
{"title": "Does VLM Classification Benefit from LLM Description Semantics?", "author": "Pingchuan Ma and Lennart Rietdorf and Dmytro Kotovenko and Vincent Tao Hu and Bj\u00f6rn Ommer", "abstract": "  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n", "link": "http://arxiv.org/abs/2412.11917v3", "date": "2024-12-19", "relevancy": 2.7198, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F&body=Title%3A%20Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F%0AAuthor%3A%20Pingchuan%20Ma%20and%20Lennart%20Rietdorf%20and%20Dmytro%20Kotovenko%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Accurately%20describing%20images%20with%20text%20is%20a%20foundation%20of%20explainable%20AI.%0AVision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20recently%20addressed%20this%20by%0Aaligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space%2C%20expressing%20semantic%0Asimilarities%20between%20vision%20and%20language%20embeddings.%20VLM%20classification%20can%20be%0Aimproved%20with%20descriptions%20generated%20by%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Ait%20is%20difficult%20to%20determine%20the%20contribution%20of%20actual%20description%20semantics%2C%0Aas%20the%20performance%20gain%20may%20also%20stem%20from%20a%20semantic-agnostic%20ensembling%0Aeffect%2C%20where%20multiple%20modified%20text%20prompts%20act%20as%20a%20noisy%20test-time%0Aaugmentation%20for%20the%20original%20one.%20We%20propose%20an%20alternative%20evaluation%0Ascenario%20to%20decide%20if%20a%20performance%20boost%20of%20LLM-generated%20descriptions%20is%0Acaused%20by%20such%20a%20noise%20augmentation%20effect%20or%20rather%20by%20genuine%20description%0Asemantics.%20The%20proposed%20scenario%20avoids%20noisy%20test-time%20augmentation%20and%0Aensures%20that%20genuine%2C%20distinctive%20descriptions%20cause%20the%20performance%20boost.%0AFurthermore%2C%20we%20propose%20a%20training-free%20method%20for%20selecting%20discriminative%0Adescriptions%20that%20work%20independently%20of%20classname-ensembling%20effects.%20Our%0Aapproach%20identifies%20descriptions%20that%20effectively%20differentiate%20classes%20within%0Aa%20local%20CLIP%20label%20neighborhood%2C%20improving%20classification%20accuracy%20across%20seven%0Adatasets.%20Additionally%2C%20we%20provide%20insights%20into%20the%20explainability%20of%0Adescription-based%20image%20classification%20with%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520VLM%2520Classification%2520Benefit%2520from%2520LLM%2520Description%2520Semantics%253F%26entry.906535625%3DPingchuan%2520Ma%2520and%2520Lennart%2520Rietdorf%2520and%2520Dmytro%2520Kotovenko%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520Accurately%2520describing%2520images%2520with%2520text%2520is%2520a%2520foundation%2520of%2520explainable%2520AI.%250AVision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520have%2520recently%2520addressed%2520this%2520by%250Aaligning%2520images%2520and%2520texts%2520in%2520a%2520shared%2520embedding%2520space%252C%2520expressing%2520semantic%250Asimilarities%2520between%2520vision%2520and%2520language%2520embeddings.%2520VLM%2520classification%2520can%2520be%250Aimproved%2520with%2520descriptions%2520generated%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520determine%2520the%2520contribution%2520of%2520actual%2520description%2520semantics%252C%250Aas%2520the%2520performance%2520gain%2520may%2520also%2520stem%2520from%2520a%2520semantic-agnostic%2520ensembling%250Aeffect%252C%2520where%2520multiple%2520modified%2520text%2520prompts%2520act%2520as%2520a%2520noisy%2520test-time%250Aaugmentation%2520for%2520the%2520original%2520one.%2520We%2520propose%2520an%2520alternative%2520evaluation%250Ascenario%2520to%2520decide%2520if%2520a%2520performance%2520boost%2520of%2520LLM-generated%2520descriptions%2520is%250Acaused%2520by%2520such%2520a%2520noise%2520augmentation%2520effect%2520or%2520rather%2520by%2520genuine%2520description%250Asemantics.%2520The%2520proposed%2520scenario%2520avoids%2520noisy%2520test-time%2520augmentation%2520and%250Aensures%2520that%2520genuine%252C%2520distinctive%2520descriptions%2520cause%2520the%2520performance%2520boost.%250AFurthermore%252C%2520we%2520propose%2520a%2520training-free%2520method%2520for%2520selecting%2520discriminative%250Adescriptions%2520that%2520work%2520independently%2520of%2520classname-ensembling%2520effects.%2520Our%250Aapproach%2520identifies%2520descriptions%2520that%2520effectively%2520differentiate%2520classes%2520within%250Aa%2520local%2520CLIP%2520label%2520neighborhood%252C%2520improving%2520classification%2520accuracy%2520across%2520seven%250Adatasets.%2520Additionally%252C%2520we%2520provide%2520insights%2520into%2520the%2520explainability%2520of%250Adescription-based%2520image%2520classification%2520with%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20VLM%20Classification%20Benefit%20from%20LLM%20Description%20Semantics%3F&entry.906535625=Pingchuan%20Ma%20and%20Lennart%20Rietdorf%20and%20Dmytro%20Kotovenko%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Accurately%20describing%20images%20with%20text%20is%20a%20foundation%20of%20explainable%20AI.%0AVision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20recently%20addressed%20this%20by%0Aaligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space%2C%20expressing%20semantic%0Asimilarities%20between%20vision%20and%20language%20embeddings.%20VLM%20classification%20can%20be%0Aimproved%20with%20descriptions%20generated%20by%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%0Ait%20is%20difficult%20to%20determine%20the%20contribution%20of%20actual%20description%20semantics%2C%0Aas%20the%20performance%20gain%20may%20also%20stem%20from%20a%20semantic-agnostic%20ensembling%0Aeffect%2C%20where%20multiple%20modified%20text%20prompts%20act%20as%20a%20noisy%20test-time%0Aaugmentation%20for%20the%20original%20one.%20We%20propose%20an%20alternative%20evaluation%0Ascenario%20to%20decide%20if%20a%20performance%20boost%20of%20LLM-generated%20descriptions%20is%0Acaused%20by%20such%20a%20noise%20augmentation%20effect%20or%20rather%20by%20genuine%20description%0Asemantics.%20The%20proposed%20scenario%20avoids%20noisy%20test-time%20augmentation%20and%0Aensures%20that%20genuine%2C%20distinctive%20descriptions%20cause%20the%20performance%20boost.%0AFurthermore%2C%20we%20propose%20a%20training-free%20method%20for%20selecting%20discriminative%0Adescriptions%20that%20work%20independently%20of%20classname-ensembling%20effects.%20Our%0Aapproach%20identifies%20descriptions%20that%20effectively%20differentiate%20classes%20within%0Aa%20local%20CLIP%20label%20neighborhood%2C%20improving%20classification%20accuracy%20across%20seven%0Adatasets.%20Additionally%2C%20we%20provide%20insights%20into%20the%20explainability%20of%0Adescription-based%20image%20classification%20with%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11917v3&entry.124074799=Read"},
{"title": "Knowing Where to Focus: Attention-Guided Alignment for Text-based Person\n  Search", "author": "Lei Tan and Weihao Li and Pingyang Dai and Jie Chen and Liujuan Cao and Rongrong Ji", "abstract": "  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to\nexplore more efficient interaction frameworks between text descriptions and\nvisual data. However, recent approaches encounter two principal challenges.\nFirstly, the widely used random-based Masked Language Modeling (MLM) considers\nall the words in the text equally during training. However, massive\nsemantically vacuous words ('with', 'the', etc.) be masked fail to contribute\nefficient interaction in the cross-modal MLM and hampers the representation\nalignment. Secondly, manual descriptions in TBPS datasets are tedious and\ninevitably contain several inaccuracies. To address these issues, we introduce\nan Attention-Guided Alignment (AGA) framework featuring two innovative\ncomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module\n(TEM). AGM dynamically masks semantically meaningful words by aggregating the\nattention weight derived from the text encoding process, thereby cross-modal\nMLM can capture information related to the masked word from text context and\nimages and align their representations. Meanwhile, TEM alleviates low-quality\nrepresentations caused by repetitive and erroneous text descriptions by\nreplacing those semantically meaningful words with MLM's prediction. It not\nonly enriches text descriptions but also prevents overfitting. Extensive\nexperiments across three challenging benchmarks demonstrate the effectiveness\nof our AGA, achieving new state-of-the-art results with Rank-1 accuracy\nreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nrespectively.\n", "link": "http://arxiv.org/abs/2412.15106v1", "date": "2024-12-19", "relevancy": 2.7174, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowing%20Where%20to%20Focus%3A%20Attention-Guided%20Alignment%20for%20Text-based%20Person%0A%20%20Search&body=Title%3A%20Knowing%20Where%20to%20Focus%3A%20Attention-Guided%20Alignment%20for%20Text-based%20Person%0A%20%20Search%0AAuthor%3A%20Lei%20Tan%20and%20Weihao%20Li%20and%20Pingyang%20Dai%20and%20Jie%20Chen%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20In%20the%20realm%20of%20Text-Based%20Person%20Search%20%28TBPS%29%2C%20mainstream%20methods%20aim%20to%0Aexplore%20more%20efficient%20interaction%20frameworks%20between%20text%20descriptions%20and%0Avisual%20data.%20However%2C%20recent%20approaches%20encounter%20two%20principal%20challenges.%0AFirstly%2C%20the%20widely%20used%20random-based%20Masked%20Language%20Modeling%20%28MLM%29%20considers%0Aall%20the%20words%20in%20the%20text%20equally%20during%20training.%20However%2C%20massive%0Asemantically%20vacuous%20words%20%28%27with%27%2C%20%27the%27%2C%20etc.%29%20be%20masked%20fail%20to%20contribute%0Aefficient%20interaction%20in%20the%20cross-modal%20MLM%20and%20hampers%20the%20representation%0Aalignment.%20Secondly%2C%20manual%20descriptions%20in%20TBPS%20datasets%20are%20tedious%20and%0Ainevitably%20contain%20several%20inaccuracies.%20To%20address%20these%20issues%2C%20we%20introduce%0Aan%20Attention-Guided%20Alignment%20%28AGA%29%20framework%20featuring%20two%20innovative%0Acomponents%3A%20Attention-Guided%20Mask%20%28AGM%29%20Modeling%20and%20Text%20Enrichment%20Module%0A%28TEM%29.%20AGM%20dynamically%20masks%20semantically%20meaningful%20words%20by%20aggregating%20the%0Aattention%20weight%20derived%20from%20the%20text%20encoding%20process%2C%20thereby%20cross-modal%0AMLM%20can%20capture%20information%20related%20to%20the%20masked%20word%20from%20text%20context%20and%0Aimages%20and%20align%20their%20representations.%20Meanwhile%2C%20TEM%20alleviates%20low-quality%0Arepresentations%20caused%20by%20repetitive%20and%20erroneous%20text%20descriptions%20by%0Areplacing%20those%20semantically%20meaningful%20words%20with%20MLM%27s%20prediction.%20It%20not%0Aonly%20enriches%20text%20descriptions%20but%20also%20prevents%20overfitting.%20Extensive%0Aexperiments%20across%20three%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%0Aof%20our%20AGA%2C%20achieving%20new%20state-of-the-art%20results%20with%20Rank-1%20accuracy%0Areaching%2078.36%25%2C%2067.31%25%2C%20and%2067.4%25%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowing%2520Where%2520to%2520Focus%253A%2520Attention-Guided%2520Alignment%2520for%2520Text-based%2520Person%250A%2520%2520Search%26entry.906535625%3DLei%2520Tan%2520and%2520Weihao%2520Li%2520and%2520Pingyang%2520Dai%2520and%2520Jie%2520Chen%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520Text-Based%2520Person%2520Search%2520%2528TBPS%2529%252C%2520mainstream%2520methods%2520aim%2520to%250Aexplore%2520more%2520efficient%2520interaction%2520frameworks%2520between%2520text%2520descriptions%2520and%250Avisual%2520data.%2520However%252C%2520recent%2520approaches%2520encounter%2520two%2520principal%2520challenges.%250AFirstly%252C%2520the%2520widely%2520used%2520random-based%2520Masked%2520Language%2520Modeling%2520%2528MLM%2529%2520considers%250Aall%2520the%2520words%2520in%2520the%2520text%2520equally%2520during%2520training.%2520However%252C%2520massive%250Asemantically%2520vacuous%2520words%2520%2528%2527with%2527%252C%2520%2527the%2527%252C%2520etc.%2529%2520be%2520masked%2520fail%2520to%2520contribute%250Aefficient%2520interaction%2520in%2520the%2520cross-modal%2520MLM%2520and%2520hampers%2520the%2520representation%250Aalignment.%2520Secondly%252C%2520manual%2520descriptions%2520in%2520TBPS%2520datasets%2520are%2520tedious%2520and%250Ainevitably%2520contain%2520several%2520inaccuracies.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250Aan%2520Attention-Guided%2520Alignment%2520%2528AGA%2529%2520framework%2520featuring%2520two%2520innovative%250Acomponents%253A%2520Attention-Guided%2520Mask%2520%2528AGM%2529%2520Modeling%2520and%2520Text%2520Enrichment%2520Module%250A%2528TEM%2529.%2520AGM%2520dynamically%2520masks%2520semantically%2520meaningful%2520words%2520by%2520aggregating%2520the%250Aattention%2520weight%2520derived%2520from%2520the%2520text%2520encoding%2520process%252C%2520thereby%2520cross-modal%250AMLM%2520can%2520capture%2520information%2520related%2520to%2520the%2520masked%2520word%2520from%2520text%2520context%2520and%250Aimages%2520and%2520align%2520their%2520representations.%2520Meanwhile%252C%2520TEM%2520alleviates%2520low-quality%250Arepresentations%2520caused%2520by%2520repetitive%2520and%2520erroneous%2520text%2520descriptions%2520by%250Areplacing%2520those%2520semantically%2520meaningful%2520words%2520with%2520MLM%2527s%2520prediction.%2520It%2520not%250Aonly%2520enriches%2520text%2520descriptions%2520but%2520also%2520prevents%2520overfitting.%2520Extensive%250Aexperiments%2520across%2520three%2520challenging%2520benchmarks%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520AGA%252C%2520achieving%2520new%2520state-of-the-art%2520results%2520with%2520Rank-1%2520accuracy%250Areaching%252078.36%2525%252C%252067.31%2525%252C%2520and%252067.4%2525%2520on%2520CUHK-PEDES%252C%2520ICFG-PEDES%252C%2520and%2520RSTPReid%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowing%20Where%20to%20Focus%3A%20Attention-Guided%20Alignment%20for%20Text-based%20Person%0A%20%20Search&entry.906535625=Lei%20Tan%20and%20Weihao%20Li%20and%20Pingyang%20Dai%20and%20Jie%20Chen%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20In%20the%20realm%20of%20Text-Based%20Person%20Search%20%28TBPS%29%2C%20mainstream%20methods%20aim%20to%0Aexplore%20more%20efficient%20interaction%20frameworks%20between%20text%20descriptions%20and%0Avisual%20data.%20However%2C%20recent%20approaches%20encounter%20two%20principal%20challenges.%0AFirstly%2C%20the%20widely%20used%20random-based%20Masked%20Language%20Modeling%20%28MLM%29%20considers%0Aall%20the%20words%20in%20the%20text%20equally%20during%20training.%20However%2C%20massive%0Asemantically%20vacuous%20words%20%28%27with%27%2C%20%27the%27%2C%20etc.%29%20be%20masked%20fail%20to%20contribute%0Aefficient%20interaction%20in%20the%20cross-modal%20MLM%20and%20hampers%20the%20representation%0Aalignment.%20Secondly%2C%20manual%20descriptions%20in%20TBPS%20datasets%20are%20tedious%20and%0Ainevitably%20contain%20several%20inaccuracies.%20To%20address%20these%20issues%2C%20we%20introduce%0Aan%20Attention-Guided%20Alignment%20%28AGA%29%20framework%20featuring%20two%20innovative%0Acomponents%3A%20Attention-Guided%20Mask%20%28AGM%29%20Modeling%20and%20Text%20Enrichment%20Module%0A%28TEM%29.%20AGM%20dynamically%20masks%20semantically%20meaningful%20words%20by%20aggregating%20the%0Aattention%20weight%20derived%20from%20the%20text%20encoding%20process%2C%20thereby%20cross-modal%0AMLM%20can%20capture%20information%20related%20to%20the%20masked%20word%20from%20text%20context%20and%0Aimages%20and%20align%20their%20representations.%20Meanwhile%2C%20TEM%20alleviates%20low-quality%0Arepresentations%20caused%20by%20repetitive%20and%20erroneous%20text%20descriptions%20by%0Areplacing%20those%20semantically%20meaningful%20words%20with%20MLM%27s%20prediction.%20It%20not%0Aonly%20enriches%20text%20descriptions%20but%20also%20prevents%20overfitting.%20Extensive%0Aexperiments%20across%20three%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%0Aof%20our%20AGA%2C%20achieving%20new%20state-of-the-art%20results%20with%20Rank-1%20accuracy%0Areaching%2078.36%25%2C%2067.31%25%2C%20and%2067.4%25%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15106v1&entry.124074799=Read"},
{"title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture", "author": "Thomas F Burns and Tomoki Fukai and Christopher J Earls", "abstract": "  Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million parameters,\nfocusing on attention head values, with results also indicating improved ICL\nperformance at this larger and more naturalistic scale.\n", "link": "http://arxiv.org/abs/2412.15113v1", "date": "2024-12-19", "relevancy": 2.7127, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture&body=Title%3A%20Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture%0AAuthor%3A%20Thomas%20F%20Burns%20and%20Tomoki%20Fukai%20and%20Christopher%20J%20Earls%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20an%20impressive%20ability%20to%20utilise%0Ainformation%20within%20the%20context%20of%20their%20input%20sequences%20to%20appropriately%0Arespond%20to%20data%20unseen%20by%20the%20LLM%20during%20its%20training%20procedure.%20This%20ability%0Ais%20known%20as%20in-context%20learning%20%28ICL%29.%20Humans%20and%20non-human%20animals%20demonstrate%0Asimilar%20abilities%2C%20however%20their%20neural%20architectures%20differ%20substantially%20from%0ALLMs.%20Despite%20this%2C%20a%20critical%20component%20within%20LLMs%2C%20the%20attention%20mechanism%2C%0Aresembles%20modern%20associative%20memory%20models%2C%20widely%20used%20in%20and%20influenced%20by%0Athe%20computational%20neuroscience%20community%20to%20model%20biological%20memory%20systems.%0AUsing%20this%20connection%2C%20we%20introduce%20an%20associative%20memory%20model%20capable%20of%0Aperforming%20ICL.%20We%20use%20this%20as%20inspiration%20for%20a%20novel%20residual%20stream%0Aarchitecture%20which%20allows%20information%20to%20directly%20flow%20between%20attention%20heads.%0AWe%20test%20this%20architecture%20during%20training%20within%20a%20two-layer%20Transformer%20and%0Ashow%20its%20ICL%20abilities%20manifest%20more%20quickly%20than%20without%20this%20modification.%20We%0Athen%20apply%20our%20architecture%20in%20small%20language%20models%20with%208%20million%20parameters%2C%0Afocusing%20on%20attention%20head%20values%2C%20with%20results%20also%20indicating%20improved%20ICL%0Aperformance%20at%20this%20larger%20and%20more%20naturalistic%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssociative%2520memory%2520inspires%2520improvements%2520for%2520in-context%2520learning%2520using%2520a%250A%2520%2520novel%2520attention%2520residual%2520stream%2520architecture%26entry.906535625%3DThomas%2520F%2520Burns%2520and%2520Tomoki%2520Fukai%2520and%2520Christopher%2520J%2520Earls%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520an%2520impressive%2520ability%2520to%2520utilise%250Ainformation%2520within%2520the%2520context%2520of%2520their%2520input%2520sequences%2520to%2520appropriately%250Arespond%2520to%2520data%2520unseen%2520by%2520the%2520LLM%2520during%2520its%2520training%2520procedure.%2520This%2520ability%250Ais%2520known%2520as%2520in-context%2520learning%2520%2528ICL%2529.%2520Humans%2520and%2520non-human%2520animals%2520demonstrate%250Asimilar%2520abilities%252C%2520however%2520their%2520neural%2520architectures%2520differ%2520substantially%2520from%250ALLMs.%2520Despite%2520this%252C%2520a%2520critical%2520component%2520within%2520LLMs%252C%2520the%2520attention%2520mechanism%252C%250Aresembles%2520modern%2520associative%2520memory%2520models%252C%2520widely%2520used%2520in%2520and%2520influenced%2520by%250Athe%2520computational%2520neuroscience%2520community%2520to%2520model%2520biological%2520memory%2520systems.%250AUsing%2520this%2520connection%252C%2520we%2520introduce%2520an%2520associative%2520memory%2520model%2520capable%2520of%250Aperforming%2520ICL.%2520We%2520use%2520this%2520as%2520inspiration%2520for%2520a%2520novel%2520residual%2520stream%250Aarchitecture%2520which%2520allows%2520information%2520to%2520directly%2520flow%2520between%2520attention%2520heads.%250AWe%2520test%2520this%2520architecture%2520during%2520training%2520within%2520a%2520two-layer%2520Transformer%2520and%250Ashow%2520its%2520ICL%2520abilities%2520manifest%2520more%2520quickly%2520than%2520without%2520this%2520modification.%2520We%250Athen%2520apply%2520our%2520architecture%2520in%2520small%2520language%2520models%2520with%25208%2520million%2520parameters%252C%250Afocusing%2520on%2520attention%2520head%2520values%252C%2520with%2520results%2520also%2520indicating%2520improved%2520ICL%250Aperformance%2520at%2520this%2520larger%2520and%2520more%2520naturalistic%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Associative%20memory%20inspires%20improvements%20for%20in-context%20learning%20using%20a%0A%20%20novel%20attention%20residual%20stream%20architecture&entry.906535625=Thomas%20F%20Burns%20and%20Tomoki%20Fukai%20and%20Christopher%20J%20Earls&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20an%20impressive%20ability%20to%20utilise%0Ainformation%20within%20the%20context%20of%20their%20input%20sequences%20to%20appropriately%0Arespond%20to%20data%20unseen%20by%20the%20LLM%20during%20its%20training%20procedure.%20This%20ability%0Ais%20known%20as%20in-context%20learning%20%28ICL%29.%20Humans%20and%20non-human%20animals%20demonstrate%0Asimilar%20abilities%2C%20however%20their%20neural%20architectures%20differ%20substantially%20from%0ALLMs.%20Despite%20this%2C%20a%20critical%20component%20within%20LLMs%2C%20the%20attention%20mechanism%2C%0Aresembles%20modern%20associative%20memory%20models%2C%20widely%20used%20in%20and%20influenced%20by%0Athe%20computational%20neuroscience%20community%20to%20model%20biological%20memory%20systems.%0AUsing%20this%20connection%2C%20we%20introduce%20an%20associative%20memory%20model%20capable%20of%0Aperforming%20ICL.%20We%20use%20this%20as%20inspiration%20for%20a%20novel%20residual%20stream%0Aarchitecture%20which%20allows%20information%20to%20directly%20flow%20between%20attention%20heads.%0AWe%20test%20this%20architecture%20during%20training%20within%20a%20two-layer%20Transformer%20and%0Ashow%20its%20ICL%20abilities%20manifest%20more%20quickly%20than%20without%20this%20modification.%20We%0Athen%20apply%20our%20architecture%20in%20small%20language%20models%20with%208%20million%20parameters%2C%0Afocusing%20on%20attention%20head%20values%2C%20with%20results%20also%20indicating%20improved%20ICL%0Aperformance%20at%20this%20larger%20and%20more%20naturalistic%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15113v1&entry.124074799=Read"},
{"title": "Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation", "author": "J\u00e9r\u00e9my Lebreton and Ingo Ahrns and Roland Brochard and Christoph Haskamp and Hans Kr\u00fcger and Matthieu Le Goff and Nicolas Menga and Nicolas Ollagnier and Ralf Regele and Francesco Capolupo and Massimo Casasco", "abstract": "  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n", "link": "http://arxiv.org/abs/2409.11383v2", "date": "2024-12-19", "relevancy": 2.71, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation&body=Title%3A%20Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation%0AAuthor%3A%20J%C3%A9r%C3%A9my%20Lebreton%20and%20Ingo%20Ahrns%20and%20Roland%20Brochard%20and%20Christoph%20Haskamp%20and%20Hans%20Kr%C3%BCger%20and%20Matthieu%20Le%20Goff%20and%20Nicolas%20Menga%20and%20Nicolas%20Ollagnier%20and%20Ralf%20Regele%20and%20Francesco%20Capolupo%20and%20Massimo%20Casasco%0AAbstract%3A%20%20%20Vision%20Based%20Navigation%20consists%20in%20utilizing%20cameras%20as%20precision%20sensors%0Afor%20GNC%20after%20extracting%20information%20from%20images.%20To%20enable%20the%20adoption%20of%0Amachine%20learning%20for%20space%20applications%2C%20one%20of%20obstacles%20is%20the%20demonstration%0Athat%20available%20training%20datasets%20are%20adequate%20to%20validate%20the%20algorithms.%20The%0Aobjective%20of%20the%20study%20is%20to%20generate%20datasets%20of%20images%20and%20metadata%20suitable%0Afor%20training%20machine%20learning%20algorithms.%20Two%20use%20cases%20were%20selected%20and%20a%0Arobust%20methodology%20was%20developed%20to%20validate%20the%20datasets%20including%20the%20ground%0Atruth.%20The%20first%20use%20case%20is%20in-orbit%20rendezvous%20with%20a%20man-made%20object%3A%20a%0Amockup%20of%20satellite%20ENVISAT.%20The%20second%20use%20case%20is%20a%20Lunar%20landing%20scenario.%0ADatasets%20were%20produced%20from%20archival%20datasets%20%28Chang%27e%203%29%2C%20from%20the%20laboratory%0Aat%20DLR%20TRON%20facility%20and%20at%20Airbus%20Robotic%20laboratory%2C%20from%20SurRender%20software%0Ahigh%20fidelity%20image%20simulator%20using%20Model%20Capture%20and%20from%20Generative%0AAdversarial%20Networks.%20The%20use%20case%20definition%20included%20the%20selection%20of%0Aalgorithms%20as%20benchmark%3A%20an%20AI-based%20pose%20estimation%20algorithm%20and%20a%20dense%0Aoptical%20flow%20algorithm%20were%20selected.%20Eventually%20it%20is%20demonstrated%20that%0Adatasets%20produced%20with%20SurRender%20and%20selected%20laboratory%20facilities%20are%0Aadequate%20to%20train%20machine%20learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Datasets%2520Generation%2520for%2520Machine%2520Learning%253A%2520Application%2520to%2520Vision%250A%2520%2520Based%2520Navigation%26entry.906535625%3DJ%25C3%25A9r%25C3%25A9my%2520Lebreton%2520and%2520Ingo%2520Ahrns%2520and%2520Roland%2520Brochard%2520and%2520Christoph%2520Haskamp%2520and%2520Hans%2520Kr%25C3%25BCger%2520and%2520Matthieu%2520Le%2520Goff%2520and%2520Nicolas%2520Menga%2520and%2520Nicolas%2520Ollagnier%2520and%2520Ralf%2520Regele%2520and%2520Francesco%2520Capolupo%2520and%2520Massimo%2520Casasco%26entry.1292438233%3D%2520%2520Vision%2520Based%2520Navigation%2520consists%2520in%2520utilizing%2520cameras%2520as%2520precision%2520sensors%250Afor%2520GNC%2520after%2520extracting%2520information%2520from%2520images.%2520To%2520enable%2520the%2520adoption%2520of%250Amachine%2520learning%2520for%2520space%2520applications%252C%2520one%2520of%2520obstacles%2520is%2520the%2520demonstration%250Athat%2520available%2520training%2520datasets%2520are%2520adequate%2520to%2520validate%2520the%2520algorithms.%2520The%250Aobjective%2520of%2520the%2520study%2520is%2520to%2520generate%2520datasets%2520of%2520images%2520and%2520metadata%2520suitable%250Afor%2520training%2520machine%2520learning%2520algorithms.%2520Two%2520use%2520cases%2520were%2520selected%2520and%2520a%250Arobust%2520methodology%2520was%2520developed%2520to%2520validate%2520the%2520datasets%2520including%2520the%2520ground%250Atruth.%2520The%2520first%2520use%2520case%2520is%2520in-orbit%2520rendezvous%2520with%2520a%2520man-made%2520object%253A%2520a%250Amockup%2520of%2520satellite%2520ENVISAT.%2520The%2520second%2520use%2520case%2520is%2520a%2520Lunar%2520landing%2520scenario.%250ADatasets%2520were%2520produced%2520from%2520archival%2520datasets%2520%2528Chang%2527e%25203%2529%252C%2520from%2520the%2520laboratory%250Aat%2520DLR%2520TRON%2520facility%2520and%2520at%2520Airbus%2520Robotic%2520laboratory%252C%2520from%2520SurRender%2520software%250Ahigh%2520fidelity%2520image%2520simulator%2520using%2520Model%2520Capture%2520and%2520from%2520Generative%250AAdversarial%2520Networks.%2520The%2520use%2520case%2520definition%2520included%2520the%2520selection%2520of%250Aalgorithms%2520as%2520benchmark%253A%2520an%2520AI-based%2520pose%2520estimation%2520algorithm%2520and%2520a%2520dense%250Aoptical%2520flow%2520algorithm%2520were%2520selected.%2520Eventually%2520it%2520is%2520demonstrated%2520that%250Adatasets%2520produced%2520with%2520SurRender%2520and%2520selected%2520laboratory%2520facilities%2520are%250Aadequate%2520to%2520train%2520machine%2520learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Datasets%20Generation%20for%20Machine%20Learning%3A%20Application%20to%20Vision%0A%20%20Based%20Navigation&entry.906535625=J%C3%A9r%C3%A9my%20Lebreton%20and%20Ingo%20Ahrns%20and%20Roland%20Brochard%20and%20Christoph%20Haskamp%20and%20Hans%20Kr%C3%BCger%20and%20Matthieu%20Le%20Goff%20and%20Nicolas%20Menga%20and%20Nicolas%20Ollagnier%20and%20Ralf%20Regele%20and%20Francesco%20Capolupo%20and%20Massimo%20Casasco&entry.1292438233=%20%20Vision%20Based%20Navigation%20consists%20in%20utilizing%20cameras%20as%20precision%20sensors%0Afor%20GNC%20after%20extracting%20information%20from%20images.%20To%20enable%20the%20adoption%20of%0Amachine%20learning%20for%20space%20applications%2C%20one%20of%20obstacles%20is%20the%20demonstration%0Athat%20available%20training%20datasets%20are%20adequate%20to%20validate%20the%20algorithms.%20The%0Aobjective%20of%20the%20study%20is%20to%20generate%20datasets%20of%20images%20and%20metadata%20suitable%0Afor%20training%20machine%20learning%20algorithms.%20Two%20use%20cases%20were%20selected%20and%20a%0Arobust%20methodology%20was%20developed%20to%20validate%20the%20datasets%20including%20the%20ground%0Atruth.%20The%20first%20use%20case%20is%20in-orbit%20rendezvous%20with%20a%20man-made%20object%3A%20a%0Amockup%20of%20satellite%20ENVISAT.%20The%20second%20use%20case%20is%20a%20Lunar%20landing%20scenario.%0ADatasets%20were%20produced%20from%20archival%20datasets%20%28Chang%27e%203%29%2C%20from%20the%20laboratory%0Aat%20DLR%20TRON%20facility%20and%20at%20Airbus%20Robotic%20laboratory%2C%20from%20SurRender%20software%0Ahigh%20fidelity%20image%20simulator%20using%20Model%20Capture%20and%20from%20Generative%0AAdversarial%20Networks.%20The%20use%20case%20definition%20included%20the%20selection%20of%0Aalgorithms%20as%20benchmark%3A%20an%20AI-based%20pose%20estimation%20algorithm%20and%20a%20dense%0Aoptical%20flow%20algorithm%20were%20selected.%20Eventually%20it%20is%20demonstrated%20that%0Adatasets%20produced%20with%20SurRender%20and%20selected%20laboratory%20facilities%20are%0Aadequate%20to%20train%20machine%20learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11383v2&entry.124074799=Read"},
{"title": "3D Registration in 30 Years: A Survey", "author": "Jiaqi Yang and Chu'ai Zhang and Zhengbao Wang and Xinyue Cao and Xuan Ouyang and Xiyu Zhang and Zhenxuan Zeng and Zhao Zeng and Borui Lu and Zhiyi Xia and Qian Zhang and Yulan Guo and Yanning Zhang", "abstract": "  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n", "link": "http://arxiv.org/abs/2412.13735v2", "date": "2024-12-19", "relevancy": 2.7034, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Registration%20in%2030%20Years%3A%20A%20Survey&body=Title%3A%203D%20Registration%20in%2030%20Years%3A%20A%20Survey%0AAuthor%3A%20Jiaqi%20Yang%20and%20Chu%27ai%20Zhang%20and%20Zhengbao%20Wang%20and%20Xinyue%20Cao%20and%20Xuan%20Ouyang%20and%20Xiyu%20Zhang%20and%20Zhenxuan%20Zeng%20and%20Zhao%20Zeng%20and%20Borui%20Lu%20and%20Zhiyi%20Xia%20and%20Qian%20Zhang%20and%20Yulan%20Guo%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%203D%20point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%0Acomputer%20graphics%2C%20robotics%2C%20remote%20sensing%2C%20and%20etc.%20Over%20the%20last%20thirty%0Ayears%2C%20we%20have%20witnessed%20the%20amazing%20advancement%20in%20this%20area%20with%20numerous%0Akinds%20of%20solutions.%20Although%20a%20handful%20of%20relevant%20surveys%20have%20been%20conducted%2C%0Atheir%20coverage%20is%20still%20limited.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%0Asurvey%20on%203D%20point%20cloud%20registration%2C%20covering%20a%20set%20of%20sub-areas%20such%20as%0Apairwise%20coarse%20registration%2C%20pairwise%20fine%20registration%2C%20multi-view%0Aregistration%2C%20cross-scale%20registration%2C%20and%20multi-instance%20registration.%20The%0Adatasets%2C%20evaluation%20metrics%2C%20method%20taxonomy%2C%20discussions%20of%20the%20merits%20and%0Ademerits%2C%20insightful%20thoughts%20of%20future%20directions%20are%20comprehensively%0Apresented%20in%20this%20survey.%20The%20regularly%20updated%20project%20page%20of%20the%20survey%20is%0Aavailable%20at%20https%3A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Registration%2520in%252030%2520Years%253A%2520A%2520Survey%26entry.906535625%3DJiaqi%2520Yang%2520and%2520Chu%2527ai%2520Zhang%2520and%2520Zhengbao%2520Wang%2520and%2520Xinyue%2520Cao%2520and%2520Xuan%2520Ouyang%2520and%2520Xiyu%2520Zhang%2520and%2520Zhenxuan%2520Zeng%2520and%2520Zhao%2520Zeng%2520and%2520Borui%2520Lu%2520and%2520Zhiyi%2520Xia%2520and%2520Qian%2520Zhang%2520and%2520Yulan%2520Guo%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%25203D%2520point%2520cloud%2520registration%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%252C%250Acomputer%2520graphics%252C%2520robotics%252C%2520remote%2520sensing%252C%2520and%2520etc.%2520Over%2520the%2520last%2520thirty%250Ayears%252C%2520we%2520have%2520witnessed%2520the%2520amazing%2520advancement%2520in%2520this%2520area%2520with%2520numerous%250Akinds%2520of%2520solutions.%2520Although%2520a%2520handful%2520of%2520relevant%2520surveys%2520have%2520been%2520conducted%252C%250Atheir%2520coverage%2520is%2520still%2520limited.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%250Asurvey%2520on%25203D%2520point%2520cloud%2520registration%252C%2520covering%2520a%2520set%2520of%2520sub-areas%2520such%2520as%250Apairwise%2520coarse%2520registration%252C%2520pairwise%2520fine%2520registration%252C%2520multi-view%250Aregistration%252C%2520cross-scale%2520registration%252C%2520and%2520multi-instance%2520registration.%2520The%250Adatasets%252C%2520evaluation%2520metrics%252C%2520method%2520taxonomy%252C%2520discussions%2520of%2520the%2520merits%2520and%250Ademerits%252C%2520insightful%2520thoughts%2520of%2520future%2520directions%2520are%2520comprehensively%250Apresented%2520in%2520this%2520survey.%2520The%2520regularly%2520updated%2520project%2520page%2520of%2520the%2520survey%2520is%250Aavailable%2520at%2520https%253A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Registration%20in%2030%20Years%3A%20A%20Survey&entry.906535625=Jiaqi%20Yang%20and%20Chu%27ai%20Zhang%20and%20Zhengbao%20Wang%20and%20Xinyue%20Cao%20and%20Xuan%20Ouyang%20and%20Xiyu%20Zhang%20and%20Zhenxuan%20Zeng%20and%20Zhao%20Zeng%20and%20Borui%20Lu%20and%20Zhiyi%20Xia%20and%20Qian%20Zhang%20and%20Yulan%20Guo%20and%20Yanning%20Zhang&entry.1292438233=%20%203D%20point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%0Acomputer%20graphics%2C%20robotics%2C%20remote%20sensing%2C%20and%20etc.%20Over%20the%20last%20thirty%0Ayears%2C%20we%20have%20witnessed%20the%20amazing%20advancement%20in%20this%20area%20with%20numerous%0Akinds%20of%20solutions.%20Although%20a%20handful%20of%20relevant%20surveys%20have%20been%20conducted%2C%0Atheir%20coverage%20is%20still%20limited.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%0Asurvey%20on%203D%20point%20cloud%20registration%2C%20covering%20a%20set%20of%20sub-areas%20such%20as%0Apairwise%20coarse%20registration%2C%20pairwise%20fine%20registration%2C%20multi-view%0Aregistration%2C%20cross-scale%20registration%2C%20and%20multi-instance%20registration.%20The%0Adatasets%2C%20evaluation%20metrics%2C%20method%20taxonomy%2C%20discussions%20of%20the%20merits%20and%0Ademerits%2C%20insightful%20thoughts%20of%20future%20directions%20are%20comprehensively%0Apresented%20in%20this%20survey.%20The%20regularly%20updated%20project%20page%20of%20the%20survey%20is%0Aavailable%20at%20https%3A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13735v2&entry.124074799=Read"},
{"title": "ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset", "author": "Olaf Wysocki and Yue Tan and Thomas Froech and Yan Xia and Magdalena Wysocki and Ludwig Hoegner and Daniel Cremers and Christoph Holst", "abstract": "  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n", "link": "http://arxiv.org/abs/2411.04865v4", "date": "2024-12-19", "relevancy": 2.6939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&body=Title%3A%20ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset%0AAuthor%3A%20Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst%0AAbstract%3A%20%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04865v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZAHA%253A%2520Introducing%2520the%2520Level%2520of%2520Facade%2520Generalization%2520and%2520the%2520Large-Scale%250A%2520%2520Point%2520Cloud%2520Facade%2520Semantic%2520Segmentation%2520Benchmark%2520Dataset%26entry.906535625%3DOlaf%2520Wysocki%2520and%2520Yue%2520Tan%2520and%2520Thomas%2520Froech%2520and%2520Yan%2520Xia%2520and%2520Magdalena%2520Wysocki%2520and%2520Ludwig%2520Hoegner%2520and%2520Daniel%2520Cremers%2520and%2520Christoph%2520Holst%26entry.1292438233%3D%2520%2520Facade%2520semantic%2520segmentation%2520is%2520a%2520long-standing%2520challenge%2520in%2520photogrammetry%250Aand%2520computer%2520vision.%2520Although%2520the%2520last%2520decades%2520have%2520witnessed%2520the%2520influx%2520of%250Afacade%2520segmentation%2520methods%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%2520facade%2520classes%250Aand%2520data%2520covering%2520the%2520architectural%2520variability.%2520In%2520ZAHA%252C%2520we%2520introduce%2520Level%2520of%250AFacade%2520Generalization%2520%2528LoFG%2529%252C%2520novel%2520hierarchical%2520facade%2520classes%2520designed%2520based%250Aon%2520international%2520urban%2520modeling%2520standards%252C%2520ensuring%2520compatibility%2520with%250Areal-world%2520challenging%2520classes%2520and%2520uniform%2520methods%2527%2520comparison.%2520Realizing%2520the%250ALoFG%252C%2520we%2520present%2520to%2520date%2520the%2520largest%2520semantic%25203D%2520facade%2520segmentation%2520dataset%252C%250Aproviding%2520601%2520million%2520annotated%2520points%2520at%2520five%2520and%252015%2520classes%2520of%2520LoFG2%2520and%250ALoFG3%252C%2520respectively.%2520Moreover%252C%2520we%2520analyze%2520the%2520performance%2520of%2520baseline%2520semantic%250Asegmentation%2520methods%2520on%2520our%2520introduced%2520LoFG%2520classes%2520and%2520data%252C%2520complementing%2520it%250Awith%2520a%2520discussion%2520on%2520the%2520unresolved%2520challenges%2520for%2520facade%2520segmentation.%2520We%250Afirmly%2520believe%2520that%2520ZAHA%2520shall%2520facilitate%2520further%2520development%2520of%25203D%2520facade%250Asemantic%2520segmentation%2520methods%252C%2520enabling%2520robust%2520segmentation%2520indispensable%2520in%250Acreating%2520urban%2520digital%2520twins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04865v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZAHA%3A%20Introducing%20the%20Level%20of%20Facade%20Generalization%20and%20the%20Large-Scale%0A%20%20Point%20Cloud%20Facade%20Semantic%20Segmentation%20Benchmark%20Dataset&entry.906535625=Olaf%20Wysocki%20and%20Yue%20Tan%20and%20Thomas%20Froech%20and%20Yan%20Xia%20and%20Magdalena%20Wysocki%20and%20Ludwig%20Hoegner%20and%20Daniel%20Cremers%20and%20Christoph%20Holst&entry.1292438233=%20%20Facade%20semantic%20segmentation%20is%20a%20long-standing%20challenge%20in%20photogrammetry%0Aand%20computer%20vision.%20Although%20the%20last%20decades%20have%20witnessed%20the%20influx%20of%0Afacade%20segmentation%20methods%2C%20there%20is%20a%20lack%20of%20comprehensive%20facade%20classes%0Aand%20data%20covering%20the%20architectural%20variability.%20In%20ZAHA%2C%20we%20introduce%20Level%20of%0AFacade%20Generalization%20%28LoFG%29%2C%20novel%20hierarchical%20facade%20classes%20designed%20based%0Aon%20international%20urban%20modeling%20standards%2C%20ensuring%20compatibility%20with%0Areal-world%20challenging%20classes%20and%20uniform%20methods%27%20comparison.%20Realizing%20the%0ALoFG%2C%20we%20present%20to%20date%20the%20largest%20semantic%203D%20facade%20segmentation%20dataset%2C%0Aproviding%20601%20million%20annotated%20points%20at%20five%20and%2015%20classes%20of%20LoFG2%20and%0ALoFG3%2C%20respectively.%20Moreover%2C%20we%20analyze%20the%20performance%20of%20baseline%20semantic%0Asegmentation%20methods%20on%20our%20introduced%20LoFG%20classes%20and%20data%2C%20complementing%20it%0Awith%20a%20discussion%20on%20the%20unresolved%20challenges%20for%20facade%20segmentation.%20We%0Afirmly%20believe%20that%20ZAHA%20shall%20facilitate%20further%20development%20of%203D%20facade%0Asemantic%20segmentation%20methods%2C%20enabling%20robust%20segmentation%20indispensable%20in%0Acreating%20urban%20digital%20twins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04865v4&entry.124074799=Read"},
{"title": "Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action\n  Recognition", "author": "Hao Huang and Yujie Lin and Siyu Chen and Haiyang Liu", "abstract": "  Skeleton-based action recognition using GCNs has achieved remarkable\nperformance, but recognizing ambiguous actions, such as \"waving\" and\n\"saluting\", remains a significant challenge. Existing methods typically rely on\na serial combination of GCNs and TCNs, where spatial and temporal features are\nextracted independently, leading to an unbalanced spatial-temporal information,\nwhich hinders accurate action recognition. Moreover, existing methods for\nambiguous actions often overemphasize local details, resulting in the loss of\ncrucial global context, which further complicates the task of differentiating\nambiguous actions. To address these challenges, we propose a lightweight\nplug-and-play module called Synchronized and Fine-grained Head (SF-Head),\ninserted between GCN and TCN layers. SF-Head first conducts Synchronized\nSpatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),\nensuring a balanced interaction between the two types of features. It then\nperforms Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature\nConsistency Loss (F-CL), which aligns the aggregated feature with their\noriginal spatial-temporal feature. This aggregation step effectively combines\nboth global context and local details. Experimental results on NTU RGB+D 60,\nNTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in\ndistinguishing ambiguous actions. Our code will be made available at\nhttps://github.com/HaoHuang2003/SFHead.\n", "link": "http://arxiv.org/abs/2412.14833v1", "date": "2024-12-19", "relevancy": 2.6822, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5449}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronized%20and%20Fine-Grained%20Head%20for%20Skeleton-Based%20Ambiguous%20Action%0A%20%20Recognition&body=Title%3A%20Synchronized%20and%20Fine-Grained%20Head%20for%20Skeleton-Based%20Ambiguous%20Action%0A%20%20Recognition%0AAuthor%3A%20Hao%20Huang%20and%20Yujie%20Lin%20and%20Siyu%20Chen%20and%20Haiyang%20Liu%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20using%20GCNs%20has%20achieved%20remarkable%0Aperformance%2C%20but%20recognizing%20ambiguous%20actions%2C%20such%20as%20%22waving%22%20and%0A%22saluting%22%2C%20remains%20a%20significant%20challenge.%20Existing%20methods%20typically%20rely%20on%0Aa%20serial%20combination%20of%20GCNs%20and%20TCNs%2C%20where%20spatial%20and%20temporal%20features%20are%0Aextracted%20independently%2C%20leading%20to%20an%20unbalanced%20spatial-temporal%20information%2C%0Awhich%20hinders%20accurate%20action%20recognition.%20Moreover%2C%20existing%20methods%20for%0Aambiguous%20actions%20often%20overemphasize%20local%20details%2C%20resulting%20in%20the%20loss%20of%0Acrucial%20global%20context%2C%20which%20further%20complicates%20the%20task%20of%20differentiating%0Aambiguous%20actions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%0Aplug-and-play%20module%20called%20Synchronized%20and%20Fine-grained%20Head%20%28SF-Head%29%2C%0Ainserted%20between%20GCN%20and%20TCN%20layers.%20SF-Head%20first%20conducts%20Synchronized%0ASpatial-Temporal%20Extraction%20%28SSTE%29%20with%20a%20Feature%20Redundancy%20Loss%20%28F-RL%29%2C%0Aensuring%20a%20balanced%20interaction%20between%20the%20two%20types%20of%20features.%20It%20then%0Aperforms%20Adaptive%20Cross-dimensional%20Feature%20Aggregation%20%28AC-FA%29%2C%20with%20a%20Feature%0AConsistency%20Loss%20%28F-CL%29%2C%20which%20aligns%20the%20aggregated%20feature%20with%20their%0Aoriginal%20spatial-temporal%20feature.%20This%20aggregation%20step%20effectively%20combines%0Aboth%20global%20context%20and%20local%20details.%20Experimental%20results%20on%20NTU%20RGB%2BD%2060%2C%0ANTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets%20demonstrate%20significant%20improvements%20in%0Adistinguishing%20ambiguous%20actions.%20Our%20code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/HaoHuang2003/SFHead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronized%2520and%2520Fine-Grained%2520Head%2520for%2520Skeleton-Based%2520Ambiguous%2520Action%250A%2520%2520Recognition%26entry.906535625%3DHao%2520Huang%2520and%2520Yujie%2520Lin%2520and%2520Siyu%2520Chen%2520and%2520Haiyang%2520Liu%26entry.1292438233%3D%2520%2520Skeleton-based%2520action%2520recognition%2520using%2520GCNs%2520has%2520achieved%2520remarkable%250Aperformance%252C%2520but%2520recognizing%2520ambiguous%2520actions%252C%2520such%2520as%2520%2522waving%2522%2520and%250A%2522saluting%2522%252C%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520methods%2520typically%2520rely%2520on%250Aa%2520serial%2520combination%2520of%2520GCNs%2520and%2520TCNs%252C%2520where%2520spatial%2520and%2520temporal%2520features%2520are%250Aextracted%2520independently%252C%2520leading%2520to%2520an%2520unbalanced%2520spatial-temporal%2520information%252C%250Awhich%2520hinders%2520accurate%2520action%2520recognition.%2520Moreover%252C%2520existing%2520methods%2520for%250Aambiguous%2520actions%2520often%2520overemphasize%2520local%2520details%252C%2520resulting%2520in%2520the%2520loss%2520of%250Acrucial%2520global%2520context%252C%2520which%2520further%2520complicates%2520the%2520task%2520of%2520differentiating%250Aambiguous%2520actions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520lightweight%250Aplug-and-play%2520module%2520called%2520Synchronized%2520and%2520Fine-grained%2520Head%2520%2528SF-Head%2529%252C%250Ainserted%2520between%2520GCN%2520and%2520TCN%2520layers.%2520SF-Head%2520first%2520conducts%2520Synchronized%250ASpatial-Temporal%2520Extraction%2520%2528SSTE%2529%2520with%2520a%2520Feature%2520Redundancy%2520Loss%2520%2528F-RL%2529%252C%250Aensuring%2520a%2520balanced%2520interaction%2520between%2520the%2520two%2520types%2520of%2520features.%2520It%2520then%250Aperforms%2520Adaptive%2520Cross-dimensional%2520Feature%2520Aggregation%2520%2528AC-FA%2529%252C%2520with%2520a%2520Feature%250AConsistency%2520Loss%2520%2528F-CL%2529%252C%2520which%2520aligns%2520the%2520aggregated%2520feature%2520with%2520their%250Aoriginal%2520spatial-temporal%2520feature.%2520This%2520aggregation%2520step%2520effectively%2520combines%250Aboth%2520global%2520context%2520and%2520local%2520details.%2520Experimental%2520results%2520on%2520NTU%2520RGB%252BD%252060%252C%250ANTU%2520RGB%252BD%2520120%252C%2520and%2520NW-UCLA%2520datasets%2520demonstrate%2520significant%2520improvements%2520in%250Adistinguishing%2520ambiguous%2520actions.%2520Our%2520code%2520will%2520be%2520made%2520available%2520at%250Ahttps%253A//github.com/HaoHuang2003/SFHead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronized%20and%20Fine-Grained%20Head%20for%20Skeleton-Based%20Ambiguous%20Action%0A%20%20Recognition&entry.906535625=Hao%20Huang%20and%20Yujie%20Lin%20and%20Siyu%20Chen%20and%20Haiyang%20Liu&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20using%20GCNs%20has%20achieved%20remarkable%0Aperformance%2C%20but%20recognizing%20ambiguous%20actions%2C%20such%20as%20%22waving%22%20and%0A%22saluting%22%2C%20remains%20a%20significant%20challenge.%20Existing%20methods%20typically%20rely%20on%0Aa%20serial%20combination%20of%20GCNs%20and%20TCNs%2C%20where%20spatial%20and%20temporal%20features%20are%0Aextracted%20independently%2C%20leading%20to%20an%20unbalanced%20spatial-temporal%20information%2C%0Awhich%20hinders%20accurate%20action%20recognition.%20Moreover%2C%20existing%20methods%20for%0Aambiguous%20actions%20often%20overemphasize%20local%20details%2C%20resulting%20in%20the%20loss%20of%0Acrucial%20global%20context%2C%20which%20further%20complicates%20the%20task%20of%20differentiating%0Aambiguous%20actions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20lightweight%0Aplug-and-play%20module%20called%20Synchronized%20and%20Fine-grained%20Head%20%28SF-Head%29%2C%0Ainserted%20between%20GCN%20and%20TCN%20layers.%20SF-Head%20first%20conducts%20Synchronized%0ASpatial-Temporal%20Extraction%20%28SSTE%29%20with%20a%20Feature%20Redundancy%20Loss%20%28F-RL%29%2C%0Aensuring%20a%20balanced%20interaction%20between%20the%20two%20types%20of%20features.%20It%20then%0Aperforms%20Adaptive%20Cross-dimensional%20Feature%20Aggregation%20%28AC-FA%29%2C%20with%20a%20Feature%0AConsistency%20Loss%20%28F-CL%29%2C%20which%20aligns%20the%20aggregated%20feature%20with%20their%0Aoriginal%20spatial-temporal%20feature.%20This%20aggregation%20step%20effectively%20combines%0Aboth%20global%20context%20and%20local%20details.%20Experimental%20results%20on%20NTU%20RGB%2BD%2060%2C%0ANTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets%20demonstrate%20significant%20improvements%20in%0Adistinguishing%20ambiguous%20actions.%20Our%20code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/HaoHuang2003/SFHead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14833v1&entry.124074799=Read"},
{"title": "ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects", "author": "Qihang Cao and Huangxun Chen", "abstract": "  3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.\n", "link": "http://arxiv.org/abs/2412.14837v1", "date": "2024-12-19", "relevancy": 2.6707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6837}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjVariantEnsemble%3A%20Advancing%20Point%20Cloud%20LLM%20Evaluation%20in%20Challenging%0A%20%20Scenes%20with%20Subtly%20Distinguished%20Objects&body=Title%3A%20ObjVariantEnsemble%3A%20Advancing%20Point%20Cloud%20LLM%20Evaluation%20in%20Challenging%0A%20%20Scenes%20with%20Subtly%20Distinguished%20Objects%0AAuthor%3A%20Qihang%20Cao%20and%20Huangxun%20Chen%0AAbstract%3A%20%20%203D%20scene%20understanding%20is%20an%20important%20task%2C%20and%20there%20has%20been%20a%20recent%0Asurge%20of%20research%20interest%20in%20aligning%203D%20representations%20of%20point%20clouds%20with%0Atext%20to%20empower%20embodied%20AI.%20However%2C%20due%20to%20the%20lack%20of%20comprehensive%203D%0Abenchmarks%2C%20the%20capabilities%20of%203D%20models%20in%20real-world%20scenes%2C%20particularly%0Athose%20that%20are%20challenging%20with%20subtly%20distinguished%20objects%2C%20remain%0Ainsufficiently%20investigated.%20To%20facilitate%20a%20more%20thorough%20evaluation%20of%203D%0Amodels%27%20capabilities%2C%20we%20propose%20a%20scheme%2C%20ObjVariantEnsemble%2C%20to%0Asystematically%20introduce%20more%20scenes%20with%20specified%20object%20classes%2C%20colors%2C%0Ashapes%2C%20quantities%2C%20and%20spatial%20relationships%20to%20meet%20model%20evaluation%20needs.%0AMore%20importantly%2C%20we%20intentionally%20construct%20scenes%20with%20similar%20objects%20to%20a%0Acertain%20degree%20and%20design%20an%20LLM-VLM-cooperated%20annotator%20to%20capture%20key%0Adistinctions%20as%20annotations.%20The%20resultant%20benchmark%20can%20better%20challenge%203D%0Amodels%2C%20reveal%20their%20shortcomings%20in%20understanding%2C%20and%20potentially%20aid%20in%20the%0Afurther%20development%20of%203D%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjVariantEnsemble%253A%2520Advancing%2520Point%2520Cloud%2520LLM%2520Evaluation%2520in%2520Challenging%250A%2520%2520Scenes%2520with%2520Subtly%2520Distinguished%2520Objects%26entry.906535625%3DQihang%2520Cao%2520and%2520Huangxun%2520Chen%26entry.1292438233%3D%2520%25203D%2520scene%2520understanding%2520is%2520an%2520important%2520task%252C%2520and%2520there%2520has%2520been%2520a%2520recent%250Asurge%2520of%2520research%2520interest%2520in%2520aligning%25203D%2520representations%2520of%2520point%2520clouds%2520with%250Atext%2520to%2520empower%2520embodied%2520AI.%2520However%252C%2520due%2520to%2520the%2520lack%2520of%2520comprehensive%25203D%250Abenchmarks%252C%2520the%2520capabilities%2520of%25203D%2520models%2520in%2520real-world%2520scenes%252C%2520particularly%250Athose%2520that%2520are%2520challenging%2520with%2520subtly%2520distinguished%2520objects%252C%2520remain%250Ainsufficiently%2520investigated.%2520To%2520facilitate%2520a%2520more%2520thorough%2520evaluation%2520of%25203D%250Amodels%2527%2520capabilities%252C%2520we%2520propose%2520a%2520scheme%252C%2520ObjVariantEnsemble%252C%2520to%250Asystematically%2520introduce%2520more%2520scenes%2520with%2520specified%2520object%2520classes%252C%2520colors%252C%250Ashapes%252C%2520quantities%252C%2520and%2520spatial%2520relationships%2520to%2520meet%2520model%2520evaluation%2520needs.%250AMore%2520importantly%252C%2520we%2520intentionally%2520construct%2520scenes%2520with%2520similar%2520objects%2520to%2520a%250Acertain%2520degree%2520and%2520design%2520an%2520LLM-VLM-cooperated%2520annotator%2520to%2520capture%2520key%250Adistinctions%2520as%2520annotations.%2520The%2520resultant%2520benchmark%2520can%2520better%2520challenge%25203D%250Amodels%252C%2520reveal%2520their%2520shortcomings%2520in%2520understanding%252C%2520and%2520potentially%2520aid%2520in%2520the%250Afurther%2520development%2520of%25203D%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjVariantEnsemble%3A%20Advancing%20Point%20Cloud%20LLM%20Evaluation%20in%20Challenging%0A%20%20Scenes%20with%20Subtly%20Distinguished%20Objects&entry.906535625=Qihang%20Cao%20and%20Huangxun%20Chen&entry.1292438233=%20%203D%20scene%20understanding%20is%20an%20important%20task%2C%20and%20there%20has%20been%20a%20recent%0Asurge%20of%20research%20interest%20in%20aligning%203D%20representations%20of%20point%20clouds%20with%0Atext%20to%20empower%20embodied%20AI.%20However%2C%20due%20to%20the%20lack%20of%20comprehensive%203D%0Abenchmarks%2C%20the%20capabilities%20of%203D%20models%20in%20real-world%20scenes%2C%20particularly%0Athose%20that%20are%20challenging%20with%20subtly%20distinguished%20objects%2C%20remain%0Ainsufficiently%20investigated.%20To%20facilitate%20a%20more%20thorough%20evaluation%20of%203D%0Amodels%27%20capabilities%2C%20we%20propose%20a%20scheme%2C%20ObjVariantEnsemble%2C%20to%0Asystematically%20introduce%20more%20scenes%20with%20specified%20object%20classes%2C%20colors%2C%0Ashapes%2C%20quantities%2C%20and%20spatial%20relationships%20to%20meet%20model%20evaluation%20needs.%0AMore%20importantly%2C%20we%20intentionally%20construct%20scenes%20with%20similar%20objects%20to%20a%0Acertain%20degree%20and%20design%20an%20LLM-VLM-cooperated%20annotator%20to%20capture%20key%0Adistinctions%20as%20annotations.%20The%20resultant%20benchmark%20can%20better%20challenge%203D%0Amodels%2C%20reveal%20their%20shortcomings%20in%20understanding%2C%20and%20potentially%20aid%20in%20the%0Afurther%20development%20of%203D%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14837v1&entry.124074799=Read"},
{"title": "Smoothness Really Matters: A Simple Yet Effective Approach for\n  Unsupervised Graph Domain Adaptation", "author": "Wei Chen and Guo Ye and Yakun Wang and Zhao Zhang and Libang Zhang and Daxin Wang and Zhiqiang Zhang and Fuzhen Zhuang", "abstract": "  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.\n", "link": "http://arxiv.org/abs/2412.11654v2", "date": "2024-12-19", "relevancy": 2.6707, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5486}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5408}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothness%20Really%20Matters%3A%20A%20Simple%20Yet%20Effective%20Approach%20for%0A%20%20Unsupervised%20Graph%20Domain%20Adaptation&body=Title%3A%20Smoothness%20Really%20Matters%3A%20A%20Simple%20Yet%20Effective%20Approach%20for%0A%20%20Unsupervised%20Graph%20Domain%20Adaptation%0AAuthor%3A%20Wei%20Chen%20and%20Guo%20Ye%20and%20Yakun%20Wang%20and%20Zhao%20Zhang%20and%20Libang%20Zhang%20and%20Daxin%20Wang%20and%20Zhiqiang%20Zhang%20and%20Fuzhen%20Zhuang%0AAbstract%3A%20%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20seeks%20to%20bridge%20distribution%0Ashifts%20between%20domains%20by%20transferring%20knowledge%20from%20labeled%20source%20graphs%20to%0Agiven%20unlabeled%20target%20graphs.%20Existing%20UGDA%20methods%20primarily%20focus%20on%0Aaligning%20features%20in%20the%20latent%20space%20learned%20by%20graph%20neural%20networks%20%28GNNs%29%0Aacross%20domains%2C%20often%20overlooking%20structural%20shifts%2C%20resulting%20in%20limited%0Aeffectiveness%20when%20addressing%20structurally%20complex%20transfer%20scenarios.%20Given%0Athe%20sensitivity%20of%20GNNs%20to%20local%20structural%20features%2C%20even%20slight%20discrepancies%0Abetween%20source%20and%20target%20graphs%20could%20lead%20to%20significant%20shifts%20in%20node%0Aembeddings%2C%20thereby%20reducing%20the%20effectiveness%20of%20knowledge%20transfer.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20novel%20approach%20for%20UGDA%20called%20Target-Domain%0AStructural%20Smoothing%20%28TDSS%29.%20TDSS%20is%20a%20simple%20and%20effective%20method%20designed%20to%0Aperform%20structural%20smoothing%20directly%20on%20the%20target%20graph%2C%20thereby%20mitigating%0Astructural%20distribution%20shifts%20and%20ensuring%20the%20consistency%20of%20node%0Arepresentations.%20Specifically%2C%20by%20integrating%20smoothing%20techniques%20with%0Aneighborhood%20sampling%2C%20TDSS%20maintains%20the%20structural%20coherence%20of%20the%20target%0Agraph%20while%20mitigating%20the%20risk%20of%20over-smoothing.%20Our%20theoretical%20analysis%0Ashows%20that%20TDSS%20effectively%20reduces%20target%20risk%20by%20improving%20model%20smoothness.%0AEmpirical%20results%20on%20three%20real-world%20datasets%20demonstrate%20that%20TDSS%0Aoutperforms%20recent%20state-of-the-art%20baselines%2C%20achieving%20significant%0Aimprovements%20across%20six%20transfer%20scenarios.%20The%20code%20is%20available%20in%0Ahttps%3A//github.com/cwei01/TDSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothness%2520Really%2520Matters%253A%2520A%2520Simple%2520Yet%2520Effective%2520Approach%2520for%250A%2520%2520Unsupervised%2520Graph%2520Domain%2520Adaptation%26entry.906535625%3DWei%2520Chen%2520and%2520Guo%2520Ye%2520and%2520Yakun%2520Wang%2520and%2520Zhao%2520Zhang%2520and%2520Libang%2520Zhang%2520and%2520Daxin%2520Wang%2520and%2520Zhiqiang%2520Zhang%2520and%2520Fuzhen%2520Zhuang%26entry.1292438233%3D%2520%2520Unsupervised%2520Graph%2520Domain%2520Adaptation%2520%2528UGDA%2529%2520seeks%2520to%2520bridge%2520distribution%250Ashifts%2520between%2520domains%2520by%2520transferring%2520knowledge%2520from%2520labeled%2520source%2520graphs%2520to%250Agiven%2520unlabeled%2520target%2520graphs.%2520Existing%2520UGDA%2520methods%2520primarily%2520focus%2520on%250Aaligning%2520features%2520in%2520the%2520latent%2520space%2520learned%2520by%2520graph%2520neural%2520networks%2520%2528GNNs%2529%250Aacross%2520domains%252C%2520often%2520overlooking%2520structural%2520shifts%252C%2520resulting%2520in%2520limited%250Aeffectiveness%2520when%2520addressing%2520structurally%2520complex%2520transfer%2520scenarios.%2520Given%250Athe%2520sensitivity%2520of%2520GNNs%2520to%2520local%2520structural%2520features%252C%2520even%2520slight%2520discrepancies%250Abetween%2520source%2520and%2520target%2520graphs%2520could%2520lead%2520to%2520significant%2520shifts%2520in%2520node%250Aembeddings%252C%2520thereby%2520reducing%2520the%2520effectiveness%2520of%2520knowledge%2520transfer.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520UGDA%2520called%2520Target-Domain%250AStructural%2520Smoothing%2520%2528TDSS%2529.%2520TDSS%2520is%2520a%2520simple%2520and%2520effective%2520method%2520designed%2520to%250Aperform%2520structural%2520smoothing%2520directly%2520on%2520the%2520target%2520graph%252C%2520thereby%2520mitigating%250Astructural%2520distribution%2520shifts%2520and%2520ensuring%2520the%2520consistency%2520of%2520node%250Arepresentations.%2520Specifically%252C%2520by%2520integrating%2520smoothing%2520techniques%2520with%250Aneighborhood%2520sampling%252C%2520TDSS%2520maintains%2520the%2520structural%2520coherence%2520of%2520the%2520target%250Agraph%2520while%2520mitigating%2520the%2520risk%2520of%2520over-smoothing.%2520Our%2520theoretical%2520analysis%250Ashows%2520that%2520TDSS%2520effectively%2520reduces%2520target%2520risk%2520by%2520improving%2520model%2520smoothness.%250AEmpirical%2520results%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520TDSS%250Aoutperforms%2520recent%2520state-of-the-art%2520baselines%252C%2520achieving%2520significant%250Aimprovements%2520across%2520six%2520transfer%2520scenarios.%2520The%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/cwei01/TDSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothness%20Really%20Matters%3A%20A%20Simple%20Yet%20Effective%20Approach%20for%0A%20%20Unsupervised%20Graph%20Domain%20Adaptation&entry.906535625=Wei%20Chen%20and%20Guo%20Ye%20and%20Yakun%20Wang%20and%20Zhao%20Zhang%20and%20Libang%20Zhang%20and%20Daxin%20Wang%20and%20Zhiqiang%20Zhang%20and%20Fuzhen%20Zhuang&entry.1292438233=%20%20Unsupervised%20Graph%20Domain%20Adaptation%20%28UGDA%29%20seeks%20to%20bridge%20distribution%0Ashifts%20between%20domains%20by%20transferring%20knowledge%20from%20labeled%20source%20graphs%20to%0Agiven%20unlabeled%20target%20graphs.%20Existing%20UGDA%20methods%20primarily%20focus%20on%0Aaligning%20features%20in%20the%20latent%20space%20learned%20by%20graph%20neural%20networks%20%28GNNs%29%0Aacross%20domains%2C%20often%20overlooking%20structural%20shifts%2C%20resulting%20in%20limited%0Aeffectiveness%20when%20addressing%20structurally%20complex%20transfer%20scenarios.%20Given%0Athe%20sensitivity%20of%20GNNs%20to%20local%20structural%20features%2C%20even%20slight%20discrepancies%0Abetween%20source%20and%20target%20graphs%20could%20lead%20to%20significant%20shifts%20in%20node%0Aembeddings%2C%20thereby%20reducing%20the%20effectiveness%20of%20knowledge%20transfer.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20a%20novel%20approach%20for%20UGDA%20called%20Target-Domain%0AStructural%20Smoothing%20%28TDSS%29.%20TDSS%20is%20a%20simple%20and%20effective%20method%20designed%20to%0Aperform%20structural%20smoothing%20directly%20on%20the%20target%20graph%2C%20thereby%20mitigating%0Astructural%20distribution%20shifts%20and%20ensuring%20the%20consistency%20of%20node%0Arepresentations.%20Specifically%2C%20by%20integrating%20smoothing%20techniques%20with%0Aneighborhood%20sampling%2C%20TDSS%20maintains%20the%20structural%20coherence%20of%20the%20target%0Agraph%20while%20mitigating%20the%20risk%20of%20over-smoothing.%20Our%20theoretical%20analysis%0Ashows%20that%20TDSS%20effectively%20reduces%20target%20risk%20by%20improving%20model%20smoothness.%0AEmpirical%20results%20on%20three%20real-world%20datasets%20demonstrate%20that%20TDSS%0Aoutperforms%20recent%20state-of-the-art%20baselines%2C%20achieving%20significant%0Aimprovements%20across%20six%20transfer%20scenarios.%20The%20code%20is%20available%20in%0Ahttps%3A//github.com/cwei01/TDSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11654v2&entry.124074799=Read"},
{"title": "Large-scale School Mapping using Weakly Supervised Deep Learning for\n  Universal School Connectivity", "author": "Isabelle Tingzon and Utku Can Ozturk and Ivan Dotu", "abstract": "  Improving global school connectivity is critical for ensuring inclusive and\nequitable quality education. To reliably estimate the cost of connecting\nschools, governments and connectivity providers require complete and accurate\nschool location data - a resource that is often scarce in many low- and\nmiddle-income countries. To address this challenge, we propose a\ncost-effective, scalable approach to locating schools in high-resolution\nsatellite images using weakly supervised deep learning techniques. Our best\nmodels, which combine vision transformers and convolutional neural networks,\nachieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging\nexplainable AI techniques, our approach can approximate the precise\ngeographical coordinates of the school locations using only low-cost,\nclassification-level annotations. To demonstrate the scalability of our method,\nwe generate nationwide maps of school location predictions in African countries\nand present a detailed analysis of our results, using Senegal as our case\nstudy. Finally, we demonstrate the immediate usability of our work by\nintroducing an interactive web mapping tool to streamline human-in-the-loop\nmodel validation efforts by government partners. This work successfully\nshowcases the real-world utility of deep learning and satellite images for\nplanning regional infrastructure and accelerating universal school\nconnectivity.\n", "link": "http://arxiv.org/abs/2412.14870v1", "date": "2024-12-19", "relevancy": 2.6579, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5695}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5166}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20School%20Mapping%20using%20Weakly%20Supervised%20Deep%20Learning%20for%0A%20%20Universal%20School%20Connectivity&body=Title%3A%20Large-scale%20School%20Mapping%20using%20Weakly%20Supervised%20Deep%20Learning%20for%0A%20%20Universal%20School%20Connectivity%0AAuthor%3A%20Isabelle%20Tingzon%20and%20Utku%20Can%20Ozturk%20and%20Ivan%20Dotu%0AAbstract%3A%20%20%20Improving%20global%20school%20connectivity%20is%20critical%20for%20ensuring%20inclusive%20and%0Aequitable%20quality%20education.%20To%20reliably%20estimate%20the%20cost%20of%20connecting%0Aschools%2C%20governments%20and%20connectivity%20providers%20require%20complete%20and%20accurate%0Aschool%20location%20data%20-%20a%20resource%20that%20is%20often%20scarce%20in%20many%20low-%20and%0Amiddle-income%20countries.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Acost-effective%2C%20scalable%20approach%20to%20locating%20schools%20in%20high-resolution%0Asatellite%20images%20using%20weakly%20supervised%20deep%20learning%20techniques.%20Our%20best%0Amodels%2C%20which%20combine%20vision%20transformers%20and%20convolutional%20neural%20networks%2C%0Aachieve%20AUPRC%20values%20above%200.96%20across%2010%20pilot%20African%20countries.%20Leveraging%0Aexplainable%20AI%20techniques%2C%20our%20approach%20can%20approximate%20the%20precise%0Ageographical%20coordinates%20of%20the%20school%20locations%20using%20only%20low-cost%2C%0Aclassification-level%20annotations.%20To%20demonstrate%20the%20scalability%20of%20our%20method%2C%0Awe%20generate%20nationwide%20maps%20of%20school%20location%20predictions%20in%20African%20countries%0Aand%20present%20a%20detailed%20analysis%20of%20our%20results%2C%20using%20Senegal%20as%20our%20case%0Astudy.%20Finally%2C%20we%20demonstrate%20the%20immediate%20usability%20of%20our%20work%20by%0Aintroducing%20an%20interactive%20web%20mapping%20tool%20to%20streamline%20human-in-the-loop%0Amodel%20validation%20efforts%20by%20government%20partners.%20This%20work%20successfully%0Ashowcases%20the%20real-world%20utility%20of%20deep%20learning%20and%20satellite%20images%20for%0Aplanning%20regional%20infrastructure%20and%20accelerating%20universal%20school%0Aconnectivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520School%2520Mapping%2520using%2520Weakly%2520Supervised%2520Deep%2520Learning%2520for%250A%2520%2520Universal%2520School%2520Connectivity%26entry.906535625%3DIsabelle%2520Tingzon%2520and%2520Utku%2520Can%2520Ozturk%2520and%2520Ivan%2520Dotu%26entry.1292438233%3D%2520%2520Improving%2520global%2520school%2520connectivity%2520is%2520critical%2520for%2520ensuring%2520inclusive%2520and%250Aequitable%2520quality%2520education.%2520To%2520reliably%2520estimate%2520the%2520cost%2520of%2520connecting%250Aschools%252C%2520governments%2520and%2520connectivity%2520providers%2520require%2520complete%2520and%2520accurate%250Aschool%2520location%2520data%2520-%2520a%2520resource%2520that%2520is%2520often%2520scarce%2520in%2520many%2520low-%2520and%250Amiddle-income%2520countries.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Acost-effective%252C%2520scalable%2520approach%2520to%2520locating%2520schools%2520in%2520high-resolution%250Asatellite%2520images%2520using%2520weakly%2520supervised%2520deep%2520learning%2520techniques.%2520Our%2520best%250Amodels%252C%2520which%2520combine%2520vision%2520transformers%2520and%2520convolutional%2520neural%2520networks%252C%250Aachieve%2520AUPRC%2520values%2520above%25200.96%2520across%252010%2520pilot%2520African%2520countries.%2520Leveraging%250Aexplainable%2520AI%2520techniques%252C%2520our%2520approach%2520can%2520approximate%2520the%2520precise%250Ageographical%2520coordinates%2520of%2520the%2520school%2520locations%2520using%2520only%2520low-cost%252C%250Aclassification-level%2520annotations.%2520To%2520demonstrate%2520the%2520scalability%2520of%2520our%2520method%252C%250Awe%2520generate%2520nationwide%2520maps%2520of%2520school%2520location%2520predictions%2520in%2520African%2520countries%250Aand%2520present%2520a%2520detailed%2520analysis%2520of%2520our%2520results%252C%2520using%2520Senegal%2520as%2520our%2520case%250Astudy.%2520Finally%252C%2520we%2520demonstrate%2520the%2520immediate%2520usability%2520of%2520our%2520work%2520by%250Aintroducing%2520an%2520interactive%2520web%2520mapping%2520tool%2520to%2520streamline%2520human-in-the-loop%250Amodel%2520validation%2520efforts%2520by%2520government%2520partners.%2520This%2520work%2520successfully%250Ashowcases%2520the%2520real-world%2520utility%2520of%2520deep%2520learning%2520and%2520satellite%2520images%2520for%250Aplanning%2520regional%2520infrastructure%2520and%2520accelerating%2520universal%2520school%250Aconnectivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20School%20Mapping%20using%20Weakly%20Supervised%20Deep%20Learning%20for%0A%20%20Universal%20School%20Connectivity&entry.906535625=Isabelle%20Tingzon%20and%20Utku%20Can%20Ozturk%20and%20Ivan%20Dotu&entry.1292438233=%20%20Improving%20global%20school%20connectivity%20is%20critical%20for%20ensuring%20inclusive%20and%0Aequitable%20quality%20education.%20To%20reliably%20estimate%20the%20cost%20of%20connecting%0Aschools%2C%20governments%20and%20connectivity%20providers%20require%20complete%20and%20accurate%0Aschool%20location%20data%20-%20a%20resource%20that%20is%20often%20scarce%20in%20many%20low-%20and%0Amiddle-income%20countries.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Acost-effective%2C%20scalable%20approach%20to%20locating%20schools%20in%20high-resolution%0Asatellite%20images%20using%20weakly%20supervised%20deep%20learning%20techniques.%20Our%20best%0Amodels%2C%20which%20combine%20vision%20transformers%20and%20convolutional%20neural%20networks%2C%0Aachieve%20AUPRC%20values%20above%200.96%20across%2010%20pilot%20African%20countries.%20Leveraging%0Aexplainable%20AI%20techniques%2C%20our%20approach%20can%20approximate%20the%20precise%0Ageographical%20coordinates%20of%20the%20school%20locations%20using%20only%20low-cost%2C%0Aclassification-level%20annotations.%20To%20demonstrate%20the%20scalability%20of%20our%20method%2C%0Awe%20generate%20nationwide%20maps%20of%20school%20location%20predictions%20in%20African%20countries%0Aand%20present%20a%20detailed%20analysis%20of%20our%20results%2C%20using%20Senegal%20as%20our%20case%0Astudy.%20Finally%2C%20we%20demonstrate%20the%20immediate%20usability%20of%20our%20work%20by%0Aintroducing%20an%20interactive%20web%20mapping%20tool%20to%20streamline%20human-in-the-loop%0Amodel%20validation%20efforts%20by%20government%20partners.%20This%20work%20successfully%0Ashowcases%20the%20real-world%20utility%20of%20deep%20learning%20and%20satellite%20images%20for%0Aplanning%20regional%20infrastructure%20and%20accelerating%20universal%20school%0Aconnectivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14870v1&entry.124074799=Read"},
{"title": "Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval", "author": "Seonguk Seo and Mustafa Gokhan Uzunbas and Bohyung Han and Sara Cao and Ser-Nam Lim", "abstract": "  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n", "link": "http://arxiv.org/abs/2301.03767v2", "date": "2024-12-19", "relevancy": 2.6471, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.531}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5296}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20Compatible%20Training%20for%20Online%20Backfilling%20in%20Large-Scale%0A%20%20Retrieval&body=Title%3A%20Metric%20Compatible%20Training%20for%20Online%20Backfilling%20in%20Large-Scale%0A%20%20Retrieval%0AAuthor%3A%20Seonguk%20Seo%20and%20Mustafa%20Gokhan%20Uzunbas%20and%20Bohyung%20Han%20and%20Sara%20Cao%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Backfilling%20is%20the%20process%20of%20re-extracting%20all%20gallery%20embeddings%20from%0Aupgraded%20models%20in%20image%20retrieval%20systems.%20It%20inevitably%20requires%20a%0Aprohibitively%20large%20amount%20of%20computational%20cost%20and%20even%20entails%20the%20downtime%0Aof%20the%20service.%20Although%20backward-compatible%20learning%20sidesteps%20this%20challenge%0Aby%20tackling%20query-side%20representations%2C%20this%20leads%20to%20suboptimal%20solutions%20in%0Aprinciple%20because%20gallery%20embeddings%20cannot%20benefit%20from%20model%20upgrades.%20We%0Aaddress%20this%20dilemma%20by%20introducing%20an%20online%20backfilling%20algorithm%2C%20which%0Aenables%20us%20to%20achieve%20a%20progressive%20performance%20improvement%20during%20the%0Abackfilling%20process%20while%20not%20sacrificing%20the%20final%20performance%20of%20new%20model%0Aafter%20the%20completion%20of%20backfilling.%20To%20this%20end%2C%20we%20first%20propose%20a%20simple%0Adistance%20rank%20merge%20technique%20for%20online%20backfilling.%20Then%2C%20we%20incorporate%20a%0Areverse%20transformation%20module%20for%20more%20effective%20and%20efficient%20merging%2C%20which%0Ais%20further%20enhanced%20by%20adopting%20a%20metric-compatible%20contrastive%20learning%0Aapproach.%20These%20two%20components%20help%20to%20make%20the%20distances%20of%20old%20and%20new%20models%0Acompatible%2C%20resulting%20in%20desirable%20merge%20results%20during%20backfilling%20with%20no%0Aextra%20computational%20overhead.%20Extensive%20experiments%20show%20the%20effectiveness%20of%0Aour%20framework%20on%20four%20standard%20benchmarks%20in%20various%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.03767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520Compatible%2520Training%2520for%2520Online%2520Backfilling%2520in%2520Large-Scale%250A%2520%2520Retrieval%26entry.906535625%3DSeonguk%2520Seo%2520and%2520Mustafa%2520Gokhan%2520Uzunbas%2520and%2520Bohyung%2520Han%2520and%2520Sara%2520Cao%2520and%2520Ser-Nam%2520Lim%26entry.1292438233%3D%2520%2520Backfilling%2520is%2520the%2520process%2520of%2520re-extracting%2520all%2520gallery%2520embeddings%2520from%250Aupgraded%2520models%2520in%2520image%2520retrieval%2520systems.%2520It%2520inevitably%2520requires%2520a%250Aprohibitively%2520large%2520amount%2520of%2520computational%2520cost%2520and%2520even%2520entails%2520the%2520downtime%250Aof%2520the%2520service.%2520Although%2520backward-compatible%2520learning%2520sidesteps%2520this%2520challenge%250Aby%2520tackling%2520query-side%2520representations%252C%2520this%2520leads%2520to%2520suboptimal%2520solutions%2520in%250Aprinciple%2520because%2520gallery%2520embeddings%2520cannot%2520benefit%2520from%2520model%2520upgrades.%2520We%250Aaddress%2520this%2520dilemma%2520by%2520introducing%2520an%2520online%2520backfilling%2520algorithm%252C%2520which%250Aenables%2520us%2520to%2520achieve%2520a%2520progressive%2520performance%2520improvement%2520during%2520the%250Abackfilling%2520process%2520while%2520not%2520sacrificing%2520the%2520final%2520performance%2520of%2520new%2520model%250Aafter%2520the%2520completion%2520of%2520backfilling.%2520To%2520this%2520end%252C%2520we%2520first%2520propose%2520a%2520simple%250Adistance%2520rank%2520merge%2520technique%2520for%2520online%2520backfilling.%2520Then%252C%2520we%2520incorporate%2520a%250Areverse%2520transformation%2520module%2520for%2520more%2520effective%2520and%2520efficient%2520merging%252C%2520which%250Ais%2520further%2520enhanced%2520by%2520adopting%2520a%2520metric-compatible%2520contrastive%2520learning%250Aapproach.%2520These%2520two%2520components%2520help%2520to%2520make%2520the%2520distances%2520of%2520old%2520and%2520new%2520models%250Acompatible%252C%2520resulting%2520in%2520desirable%2520merge%2520results%2520during%2520backfilling%2520with%2520no%250Aextra%2520computational%2520overhead.%2520Extensive%2520experiments%2520show%2520the%2520effectiveness%2520of%250Aour%2520framework%2520on%2520four%2520standard%2520benchmarks%2520in%2520various%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.03767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20Compatible%20Training%20for%20Online%20Backfilling%20in%20Large-Scale%0A%20%20Retrieval&entry.906535625=Seonguk%20Seo%20and%20Mustafa%20Gokhan%20Uzunbas%20and%20Bohyung%20Han%20and%20Sara%20Cao%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Backfilling%20is%20the%20process%20of%20re-extracting%20all%20gallery%20embeddings%20from%0Aupgraded%20models%20in%20image%20retrieval%20systems.%20It%20inevitably%20requires%20a%0Aprohibitively%20large%20amount%20of%20computational%20cost%20and%20even%20entails%20the%20downtime%0Aof%20the%20service.%20Although%20backward-compatible%20learning%20sidesteps%20this%20challenge%0Aby%20tackling%20query-side%20representations%2C%20this%20leads%20to%20suboptimal%20solutions%20in%0Aprinciple%20because%20gallery%20embeddings%20cannot%20benefit%20from%20model%20upgrades.%20We%0Aaddress%20this%20dilemma%20by%20introducing%20an%20online%20backfilling%20algorithm%2C%20which%0Aenables%20us%20to%20achieve%20a%20progressive%20performance%20improvement%20during%20the%0Abackfilling%20process%20while%20not%20sacrificing%20the%20final%20performance%20of%20new%20model%0Aafter%20the%20completion%20of%20backfilling.%20To%20this%20end%2C%20we%20first%20propose%20a%20simple%0Adistance%20rank%20merge%20technique%20for%20online%20backfilling.%20Then%2C%20we%20incorporate%20a%0Areverse%20transformation%20module%20for%20more%20effective%20and%20efficient%20merging%2C%20which%0Ais%20further%20enhanced%20by%20adopting%20a%20metric-compatible%20contrastive%20learning%0Aapproach.%20These%20two%20components%20help%20to%20make%20the%20distances%20of%20old%20and%20new%20models%0Acompatible%2C%20resulting%20in%20desirable%20merge%20results%20during%20backfilling%20with%20no%0Aextra%20computational%20overhead.%20Extensive%20experiments%20show%20the%20effectiveness%20of%0Aour%20framework%20on%20four%20standard%20benchmarks%20in%20various%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.03767v2&entry.124074799=Read"},
{"title": "Explicit Relational Reasoning Network for Scene Text Detection", "author": "Yuchen Su and Zhineng Chen and Yongkun Du and Zhilong Ji and Kai Hu and Jinfeng Bai and Xieping Gao", "abstract": "  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n", "link": "http://arxiv.org/abs/2412.14692v1", "date": "2024-12-19", "relevancy": 2.6233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection&body=Title%3A%20Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection%0AAuthor%3A%20Yuchen%20Su%20and%20Zhineng%20Chen%20and%20Yongkun%20Du%20and%20Zhilong%20Ji%20and%20Kai%20Hu%20and%20Jinfeng%20Bai%20and%20Xieping%20Gao%0AAbstract%3A%20%20%20Connected%20component%20%28CC%29%20is%20a%20proper%20text%20shape%20representation%20that%20aligns%0Awith%20human%20reading%20intuition.%20However%2C%20CC-based%20text%20detection%20methods%20have%0Arecently%20faced%20a%20developmental%20bottleneck%20that%20their%20time-consuming%0Apost-processing%20is%20difficult%20to%20eliminate.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20explicit%20relational%20reasoning%20network%20%28ERRNet%29%20to%20elegantly%20model%20the%0Acomponent%20relationships%20without%20post-processing.%20Concretely%2C%20we%20first%20represent%0Aeach%20text%20instance%20as%20multiple%20ordered%20text%20components%2C%20and%20then%20treat%20these%0Acomponents%20as%20objects%20in%20sequential%20movement.%20In%20this%20way%2C%20scene%20text%20detection%0Acan%20be%20innovatively%20viewed%20as%20a%20tracking%20problem.%20From%20this%20perspective%2C%20we%0Adesign%20an%20end-to-end%20tracking%20decoder%20to%20achieve%20a%20CC-based%20method%20dispensing%0Awith%20post-processing%20entirely.%20Additionally%2C%20we%20observe%20that%20there%20is%20an%0Ainconsistency%20between%20classification%20confidence%20and%20localization%20quality%2C%20so%20we%0Apropose%20a%20Polygon%20Monte-Carlo%20method%20to%20quickly%20and%20accurately%20evaluate%20the%0Alocalization%20quality.%20Based%20on%20this%2C%20we%20introduce%20a%20position-supervised%0Aclassification%20loss%20to%20guide%20the%20task-aligned%20learning%20of%20ERRNet.%20Experiments%0Aon%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ERRNet.%20It%0Aconsistently%20achieves%20state-of-the-art%20accuracy%20while%20holding%20highly%0Acompetitive%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Relational%2520Reasoning%2520Network%2520for%2520Scene%2520Text%2520Detection%26entry.906535625%3DYuchen%2520Su%2520and%2520Zhineng%2520Chen%2520and%2520Yongkun%2520Du%2520and%2520Zhilong%2520Ji%2520and%2520Kai%2520Hu%2520and%2520Jinfeng%2520Bai%2520and%2520Xieping%2520Gao%26entry.1292438233%3D%2520%2520Connected%2520component%2520%2528CC%2529%2520is%2520a%2520proper%2520text%2520shape%2520representation%2520that%2520aligns%250Awith%2520human%2520reading%2520intuition.%2520However%252C%2520CC-based%2520text%2520detection%2520methods%2520have%250Arecently%2520faced%2520a%2520developmental%2520bottleneck%2520that%2520their%2520time-consuming%250Apost-processing%2520is%2520difficult%2520to%2520eliminate.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%250Aan%2520explicit%2520relational%2520reasoning%2520network%2520%2528ERRNet%2529%2520to%2520elegantly%2520model%2520the%250Acomponent%2520relationships%2520without%2520post-processing.%2520Concretely%252C%2520we%2520first%2520represent%250Aeach%2520text%2520instance%2520as%2520multiple%2520ordered%2520text%2520components%252C%2520and%2520then%2520treat%2520these%250Acomponents%2520as%2520objects%2520in%2520sequential%2520movement.%2520In%2520this%2520way%252C%2520scene%2520text%2520detection%250Acan%2520be%2520innovatively%2520viewed%2520as%2520a%2520tracking%2520problem.%2520From%2520this%2520perspective%252C%2520we%250Adesign%2520an%2520end-to-end%2520tracking%2520decoder%2520to%2520achieve%2520a%2520CC-based%2520method%2520dispensing%250Awith%2520post-processing%2520entirely.%2520Additionally%252C%2520we%2520observe%2520that%2520there%2520is%2520an%250Ainconsistency%2520between%2520classification%2520confidence%2520and%2520localization%2520quality%252C%2520so%2520we%250Apropose%2520a%2520Polygon%2520Monte-Carlo%2520method%2520to%2520quickly%2520and%2520accurately%2520evaluate%2520the%250Alocalization%2520quality.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520a%2520position-supervised%250Aclassification%2520loss%2520to%2520guide%2520the%2520task-aligned%2520learning%2520of%2520ERRNet.%2520Experiments%250Aon%2520challenging%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520ERRNet.%2520It%250Aconsistently%2520achieves%2520state-of-the-art%2520accuracy%2520while%2520holding%2520highly%250Acompetitive%2520inference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Relational%20Reasoning%20Network%20for%20Scene%20Text%20Detection&entry.906535625=Yuchen%20Su%20and%20Zhineng%20Chen%20and%20Yongkun%20Du%20and%20Zhilong%20Ji%20and%20Kai%20Hu%20and%20Jinfeng%20Bai%20and%20Xieping%20Gao&entry.1292438233=%20%20Connected%20component%20%28CC%29%20is%20a%20proper%20text%20shape%20representation%20that%20aligns%0Awith%20human%20reading%20intuition.%20However%2C%20CC-based%20text%20detection%20methods%20have%0Arecently%20faced%20a%20developmental%20bottleneck%20that%20their%20time-consuming%0Apost-processing%20is%20difficult%20to%20eliminate.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20explicit%20relational%20reasoning%20network%20%28ERRNet%29%20to%20elegantly%20model%20the%0Acomponent%20relationships%20without%20post-processing.%20Concretely%2C%20we%20first%20represent%0Aeach%20text%20instance%20as%20multiple%20ordered%20text%20components%2C%20and%20then%20treat%20these%0Acomponents%20as%20objects%20in%20sequential%20movement.%20In%20this%20way%2C%20scene%20text%20detection%0Acan%20be%20innovatively%20viewed%20as%20a%20tracking%20problem.%20From%20this%20perspective%2C%20we%0Adesign%20an%20end-to-end%20tracking%20decoder%20to%20achieve%20a%20CC-based%20method%20dispensing%0Awith%20post-processing%20entirely.%20Additionally%2C%20we%20observe%20that%20there%20is%20an%0Ainconsistency%20between%20classification%20confidence%20and%20localization%20quality%2C%20so%20we%0Apropose%20a%20Polygon%20Monte-Carlo%20method%20to%20quickly%20and%20accurately%20evaluate%20the%0Alocalization%20quality.%20Based%20on%20this%2C%20we%20introduce%20a%20position-supervised%0Aclassification%20loss%20to%20guide%20the%20task-aligned%20learning%20of%20ERRNet.%20Experiments%0Aon%20challenging%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ERRNet.%20It%0Aconsistently%20achieves%20state-of-the-art%20accuracy%20while%20holding%20highly%0Acompetitive%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14692v1&entry.124074799=Read"},
{"title": "FashionComposer: Compositional Fashion Image Generation", "author": "Sihui Ji and Yiyang Wang and Xi Chen and Xiaogang Xu and Hao Luo and Hengshuang Zhao", "abstract": "  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n", "link": "http://arxiv.org/abs/2412.14168v2", "date": "2024-12-19", "relevancy": 2.5828, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6915}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6581}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation&body=Title%3A%20FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation%0AAuthor%3A%20Sihui%20Ji%20and%20Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Hao%20Luo%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20We%20present%20FashionComposer%20for%20compositional%20fashion%20image%20generation.%20Unlike%0Aprevious%20methods%2C%20FashionComposer%20is%20highly%20flexible.%20It%20takes%20multi-modal%0Ainput%20%28i.e.%2C%20text%20prompt%2C%20parametric%20human%20model%2C%20garment%20image%2C%20and%20face%0Aimage%29%20and%20supports%20personalizing%20the%20appearance%2C%20pose%2C%20and%20figure%20of%20the%20human%0Aand%20assigning%20multiple%20garments%20in%20one%20pass.%20To%20achieve%20this%2C%20we%20first%20develop%0Aa%20universal%20framework%20capable%20of%20handling%20diverse%20input%20modalities.%20We%0Aconstruct%20scaled%20training%20data%20to%20enhance%20the%20model%27s%20robust%20compositional%0Acapabilities.%20To%20accommodate%20multiple%20reference%20images%20%28garments%20and%20faces%29%0Aseamlessly%2C%20we%20organize%20these%20references%20in%20a%20single%20image%20as%20an%20%22asset%0Alibrary%22%20and%20employ%20a%20reference%20UNet%20to%20extract%20appearance%20features.%20To%20inject%0Athe%20appearance%20features%20into%20the%20correct%20pixels%20in%20the%20generated%20result%2C%20we%0Apropose%20subject-binding%20attention.%20It%20binds%20the%20appearance%20features%20from%0Adifferent%20%22assets%22%20with%20the%20corresponding%20text%20features.%20In%20this%20way%2C%20the%20model%0Acould%20understand%20each%20asset%20according%20to%20their%20semantics%2C%20supporting%20arbitrary%0Anumbers%20and%20types%20of%20reference%20images.%20As%20a%20comprehensive%20solution%2C%0AFashionComposer%20also%20supports%20many%20other%20applications%20like%20human%20album%0Ageneration%2C%20diverse%20virtual%20try-on%20tasks%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionComposer%253A%2520Compositional%2520Fashion%2520Image%2520Generation%26entry.906535625%3DSihui%2520Ji%2520and%2520Yiyang%2520Wang%2520and%2520Xi%2520Chen%2520and%2520Xiaogang%2520Xu%2520and%2520Hao%2520Luo%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520We%2520present%2520FashionComposer%2520for%2520compositional%2520fashion%2520image%2520generation.%2520Unlike%250Aprevious%2520methods%252C%2520FashionComposer%2520is%2520highly%2520flexible.%2520It%2520takes%2520multi-modal%250Ainput%2520%2528i.e.%252C%2520text%2520prompt%252C%2520parametric%2520human%2520model%252C%2520garment%2520image%252C%2520and%2520face%250Aimage%2529%2520and%2520supports%2520personalizing%2520the%2520appearance%252C%2520pose%252C%2520and%2520figure%2520of%2520the%2520human%250Aand%2520assigning%2520multiple%2520garments%2520in%2520one%2520pass.%2520To%2520achieve%2520this%252C%2520we%2520first%2520develop%250Aa%2520universal%2520framework%2520capable%2520of%2520handling%2520diverse%2520input%2520modalities.%2520We%250Aconstruct%2520scaled%2520training%2520data%2520to%2520enhance%2520the%2520model%2527s%2520robust%2520compositional%250Acapabilities.%2520To%2520accommodate%2520multiple%2520reference%2520images%2520%2528garments%2520and%2520faces%2529%250Aseamlessly%252C%2520we%2520organize%2520these%2520references%2520in%2520a%2520single%2520image%2520as%2520an%2520%2522asset%250Alibrary%2522%2520and%2520employ%2520a%2520reference%2520UNet%2520to%2520extract%2520appearance%2520features.%2520To%2520inject%250Athe%2520appearance%2520features%2520into%2520the%2520correct%2520pixels%2520in%2520the%2520generated%2520result%252C%2520we%250Apropose%2520subject-binding%2520attention.%2520It%2520binds%2520the%2520appearance%2520features%2520from%250Adifferent%2520%2522assets%2522%2520with%2520the%2520corresponding%2520text%2520features.%2520In%2520this%2520way%252C%2520the%2520model%250Acould%2520understand%2520each%2520asset%2520according%2520to%2520their%2520semantics%252C%2520supporting%2520arbitrary%250Anumbers%2520and%2520types%2520of%2520reference%2520images.%2520As%2520a%2520comprehensive%2520solution%252C%250AFashionComposer%2520also%2520supports%2520many%2520other%2520applications%2520like%2520human%2520album%250Ageneration%252C%2520diverse%2520virtual%2520try-on%2520tasks%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation&entry.906535625=Sihui%20Ji%20and%20Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Hao%20Luo%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20We%20present%20FashionComposer%20for%20compositional%20fashion%20image%20generation.%20Unlike%0Aprevious%20methods%2C%20FashionComposer%20is%20highly%20flexible.%20It%20takes%20multi-modal%0Ainput%20%28i.e.%2C%20text%20prompt%2C%20parametric%20human%20model%2C%20garment%20image%2C%20and%20face%0Aimage%29%20and%20supports%20personalizing%20the%20appearance%2C%20pose%2C%20and%20figure%20of%20the%20human%0Aand%20assigning%20multiple%20garments%20in%20one%20pass.%20To%20achieve%20this%2C%20we%20first%20develop%0Aa%20universal%20framework%20capable%20of%20handling%20diverse%20input%20modalities.%20We%0Aconstruct%20scaled%20training%20data%20to%20enhance%20the%20model%27s%20robust%20compositional%0Acapabilities.%20To%20accommodate%20multiple%20reference%20images%20%28garments%20and%20faces%29%0Aseamlessly%2C%20we%20organize%20these%20references%20in%20a%20single%20image%20as%20an%20%22asset%0Alibrary%22%20and%20employ%20a%20reference%20UNet%20to%20extract%20appearance%20features.%20To%20inject%0Athe%20appearance%20features%20into%20the%20correct%20pixels%20in%20the%20generated%20result%2C%20we%0Apropose%20subject-binding%20attention.%20It%20binds%20the%20appearance%20features%20from%0Adifferent%20%22assets%22%20with%20the%20corresponding%20text%20features.%20In%20this%20way%2C%20the%20model%0Acould%20understand%20each%20asset%20according%20to%20their%20semantics%2C%20supporting%20arbitrary%0Anumbers%20and%20types%20of%20reference%20images.%20As%20a%20comprehensive%20solution%2C%0AFashionComposer%20also%20supports%20many%20other%20applications%20like%20human%20album%0Ageneration%2C%20diverse%20virtual%20try-on%20tasks%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14168v2&entry.124074799=Read"},
{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment", "author": "Shaolei Zhang and Kehao Zhang and Qingkai Fang and Shoutao Guo and Yan Zhou and Xiaodong Liu and Yang Feng", "abstract": "  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n", "link": "http://arxiv.org/abs/2411.16300v3", "date": "2024-12-19", "relevancy": 2.5794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&body=Title%3A%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment%0AAuthor%3A%20Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayLing%25202%253A%2520A%2520Multilingual%2520Large%2520Language%2520Model%2520with%2520Efficient%2520Language%250A%2520%2520Alignment%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Kehao%2520Zhang%2520and%2520Qingkai%2520Fang%2520and%2520Shoutao%2520Guo%2520and%2520Yan%2520Zhou%2520and%2520Xiaodong%2520Liu%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520their%2520powerful%2520generative%2520capabilities%2520and%250Avast%2520knowledge%252C%2520empower%2520various%2520tasks%2520in%2520everyday%2520life.%2520However%252C%2520these%250Aabilities%2520are%2520primarily%2520concentrated%2520in%2520high-resource%2520languages%252C%2520leaving%250Alow-resource%2520languages%2520with%2520weaker%2520generative%2520capabilities%2520and%2520relatively%250Alimited%2520knowledge.%2520Enhancing%2520the%2520multilingual%2520capabilities%2520of%2520LLMs%2520is%2520therefore%250Acrucial%2520for%2520serving%2520over%2520100%2520linguistic%2520communities%2520worldwide.%2520An%2520intuitive%250Aapproach%2520to%2520enhance%2520the%2520multilingual%2520capabilities%2520would%2520be%2520to%2520construct%250Ainstruction%2520data%2520for%2520various%2520languages%252C%2520but%2520constructing%2520instruction%2520data%2520for%250Aover%2520100%2520languages%2520is%2520prohibitively%2520costly.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520BayLing%250A2%252C%2520which%2520efficiently%2520transfers%2520generative%2520capabilities%2520and%2520knowledge%2520from%250Ahigh-resource%2520languages%2520to%2520low-resource%2520languages%2520through%2520language%2520alignment.%250ATo%2520achieve%2520this%252C%2520we%2520constructed%2520a%2520dataset%2520of%25203.2%2520million%2520instructions%252C%250Acomprising%2520high-resource%2520language%2520instructions%2520%2528Chinese%2520and%2520English%2529%2520and%250Across-lingual%2520instructions%2520for%2520100%252B%2520languages%2520and%2520performed%2520instruction%2520tuning%250Abased%2520on%2520the%2520dataset%2520to%2520facilitate%2520the%2520capability%2520transfer%2520between%2520languages.%250AUsing%2520Llama%2520as%2520the%2520foundation%2520model%252C%2520we%2520developed%2520BayLing-2-7B%252C%2520BayLing-2-13B%252C%250Aand%2520BayLing-2-8B%252C%2520and%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520BayLing.%2520For%250Amultilingual%2520translation%2520across%2520100%252B%2520languages%252C%2520BayLing%2520shows%2520superior%250Aperformance%2520compared%2520to%2520open-source%2520models%2520of%2520similar%2520scale.%2520For%2520multilingual%250Aknowledge%2520and%2520understanding%2520benchmarks%252C%2520BayLing%2520achieves%2520significant%250Aimprovements%2520across%2520over%252020%2520low-resource%2520languages%252C%2520demonstrating%2520its%250Acapability%2520of%2520effective%2520knowledge%2520transfer%2520from%2520high-resource%2520to%2520low-resource%250Alanguages.%2520Furthermore%252C%2520results%2520on%2520English%2520benchmarks%2520indicate%2520that%2520BayLing%250Amaintains%2520high%2520performance%2520in%2520highresource%2520languages%2520while%2520enhancing%2520the%250Aperformance%2520in%2520low-resource%2520languages.%2520Demo%252C%2520homepage%252C%2520code%2520and%2520models%2520of%250ABayLing%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&entry.906535625=Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-2-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16300v3&entry.124074799=Read"},
{"title": "SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and\n  BEV Paradigm", "author": "Junyan Ye and Jun He and Weijia Li and Zhutao Lv and Yi Lin and Jinhua Yu and Haote Yang and Conghui He", "abstract": "  Ground-to-aerial image synthesis focuses on generating realistic aerial\nimages from corresponding ground street view images while maintaining\nconsistent content layout, simulating a top-down view. The significant\nviewpoint difference leads to domain gaps between views, and dense urban scenes\nlimit the visible range of street views, making this cross-view generation task\nparticularly challenging. In this paper, we introduce SkyDiffusion, a novel\ncross-view generation method for synthesizing aerial images from street view\nimages, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The\nCurved-BEV method in SkyDiffusion converts street-view images into a BEV\nperspective, effectively bridging the domain gap, and employs a \"multi-to-one\"\nmapping strategy to address occlusion issues in dense urban scenes. Next,\nSkyDiffusion designed a BEV-guided diffusion model to generate\ncontent-consistent and realistic aerial images. Additionally, we introduce a\nnovel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image\nsynthesis applications, including disaster scene aerial synthesis, historical\nhigh-resolution satellite image synthesis, and low-altitude UAV image synthesis\ntasks. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on cross-view datasets across natural (CVUSA),\nsuburban (CVACT), urban (VIGOR-Chicago), and various application scenarios\n(G2A-3), achieving realistic and content-consistent aerial image generation.\nMore result and dataset information can be found at\nhttps://opendatalab.github.io/skydiffusion/ .\n", "link": "http://arxiv.org/abs/2408.01812v3", "date": "2024-12-19", "relevancy": 2.5632, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6458}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6458}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkyDiffusion%3A%20Ground-to-Aerial%20Image%20Synthesis%20with%20Diffusion%20Models%20and%0A%20%20BEV%20Paradigm&body=Title%3A%20SkyDiffusion%3A%20Ground-to-Aerial%20Image%20Synthesis%20with%20Diffusion%20Models%20and%0A%20%20BEV%20Paradigm%0AAuthor%3A%20Junyan%20Ye%20and%20Jun%20He%20and%20Weijia%20Li%20and%20Zhutao%20Lv%20and%20Yi%20Lin%20and%20Jinhua%20Yu%20and%20Haote%20Yang%20and%20Conghui%20He%0AAbstract%3A%20%20%20Ground-to-aerial%20image%20synthesis%20focuses%20on%20generating%20realistic%20aerial%0Aimages%20from%20corresponding%20ground%20street%20view%20images%20while%20maintaining%0Aconsistent%20content%20layout%2C%20simulating%20a%20top-down%20view.%20The%20significant%0Aviewpoint%20difference%20leads%20to%20domain%20gaps%20between%20views%2C%20and%20dense%20urban%20scenes%0Alimit%20the%20visible%20range%20of%20street%20views%2C%20making%20this%20cross-view%20generation%20task%0Aparticularly%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SkyDiffusion%2C%20a%20novel%0Across-view%20generation%20method%20for%20synthesizing%20aerial%20images%20from%20street%20view%0Aimages%2C%20utilizing%20a%20diffusion%20model%20and%20the%20Bird%27s-Eye%20View%20%28BEV%29%20paradigm.%20The%0ACurved-BEV%20method%20in%20SkyDiffusion%20converts%20street-view%20images%20into%20a%20BEV%0Aperspective%2C%20effectively%20bridging%20the%20domain%20gap%2C%20and%20employs%20a%20%22multi-to-one%22%0Amapping%20strategy%20to%20address%20occlusion%20issues%20in%20dense%20urban%20scenes.%20Next%2C%0ASkyDiffusion%20designed%20a%20BEV-guided%20diffusion%20model%20to%20generate%0Acontent-consistent%20and%20realistic%20aerial%20images.%20Additionally%2C%20we%20introduce%20a%0Anovel%20dataset%2C%20Ground2Aerial-3%2C%20designed%20for%20diverse%20ground-to-aerial%20image%0Asynthesis%20applications%2C%20including%20disaster%20scene%20aerial%20synthesis%2C%20historical%0Ahigh-resolution%20satellite%20image%20synthesis%2C%20and%20low-altitude%20UAV%20image%20synthesis%0Atasks.%20Experimental%20results%20demonstrate%20that%20SkyDiffusion%20outperforms%0Astate-of-the-art%20methods%20on%20cross-view%20datasets%20across%20natural%20%28CVUSA%29%2C%0Asuburban%20%28CVACT%29%2C%20urban%20%28VIGOR-Chicago%29%2C%20and%20various%20application%20scenarios%0A%28G2A-3%29%2C%20achieving%20realistic%20and%20content-consistent%20aerial%20image%20generation.%0AMore%20result%20and%20dataset%20information%20can%20be%20found%20at%0Ahttps%3A//opendatalab.github.io/skydiffusion/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyDiffusion%253A%2520Ground-to-Aerial%2520Image%2520Synthesis%2520with%2520Diffusion%2520Models%2520and%250A%2520%2520BEV%2520Paradigm%26entry.906535625%3DJunyan%2520Ye%2520and%2520Jun%2520He%2520and%2520Weijia%2520Li%2520and%2520Zhutao%2520Lv%2520and%2520Yi%2520Lin%2520and%2520Jinhua%2520Yu%2520and%2520Haote%2520Yang%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520Ground-to-aerial%2520image%2520synthesis%2520focuses%2520on%2520generating%2520realistic%2520aerial%250Aimages%2520from%2520corresponding%2520ground%2520street%2520view%2520images%2520while%2520maintaining%250Aconsistent%2520content%2520layout%252C%2520simulating%2520a%2520top-down%2520view.%2520The%2520significant%250Aviewpoint%2520difference%2520leads%2520to%2520domain%2520gaps%2520between%2520views%252C%2520and%2520dense%2520urban%2520scenes%250Alimit%2520the%2520visible%2520range%2520of%2520street%2520views%252C%2520making%2520this%2520cross-view%2520generation%2520task%250Aparticularly%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SkyDiffusion%252C%2520a%2520novel%250Across-view%2520generation%2520method%2520for%2520synthesizing%2520aerial%2520images%2520from%2520street%2520view%250Aimages%252C%2520utilizing%2520a%2520diffusion%2520model%2520and%2520the%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520paradigm.%2520The%250ACurved-BEV%2520method%2520in%2520SkyDiffusion%2520converts%2520street-view%2520images%2520into%2520a%2520BEV%250Aperspective%252C%2520effectively%2520bridging%2520the%2520domain%2520gap%252C%2520and%2520employs%2520a%2520%2522multi-to-one%2522%250Amapping%2520strategy%2520to%2520address%2520occlusion%2520issues%2520in%2520dense%2520urban%2520scenes.%2520Next%252C%250ASkyDiffusion%2520designed%2520a%2520BEV-guided%2520diffusion%2520model%2520to%2520generate%250Acontent-consistent%2520and%2520realistic%2520aerial%2520images.%2520Additionally%252C%2520we%2520introduce%2520a%250Anovel%2520dataset%252C%2520Ground2Aerial-3%252C%2520designed%2520for%2520diverse%2520ground-to-aerial%2520image%250Asynthesis%2520applications%252C%2520including%2520disaster%2520scene%2520aerial%2520synthesis%252C%2520historical%250Ahigh-resolution%2520satellite%2520image%2520synthesis%252C%2520and%2520low-altitude%2520UAV%2520image%2520synthesis%250Atasks.%2520Experimental%2520results%2520demonstrate%2520that%2520SkyDiffusion%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520cross-view%2520datasets%2520across%2520natural%2520%2528CVUSA%2529%252C%250Asuburban%2520%2528CVACT%2529%252C%2520urban%2520%2528VIGOR-Chicago%2529%252C%2520and%2520various%2520application%2520scenarios%250A%2528G2A-3%2529%252C%2520achieving%2520realistic%2520and%2520content-consistent%2520aerial%2520image%2520generation.%250AMore%2520result%2520and%2520dataset%2520information%2520can%2520be%2520found%2520at%250Ahttps%253A//opendatalab.github.io/skydiffusion/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkyDiffusion%3A%20Ground-to-Aerial%20Image%20Synthesis%20with%20Diffusion%20Models%20and%0A%20%20BEV%20Paradigm&entry.906535625=Junyan%20Ye%20and%20Jun%20He%20and%20Weijia%20Li%20and%20Zhutao%20Lv%20and%20Yi%20Lin%20and%20Jinhua%20Yu%20and%20Haote%20Yang%20and%20Conghui%20He&entry.1292438233=%20%20Ground-to-aerial%20image%20synthesis%20focuses%20on%20generating%20realistic%20aerial%0Aimages%20from%20corresponding%20ground%20street%20view%20images%20while%20maintaining%0Aconsistent%20content%20layout%2C%20simulating%20a%20top-down%20view.%20The%20significant%0Aviewpoint%20difference%20leads%20to%20domain%20gaps%20between%20views%2C%20and%20dense%20urban%20scenes%0Alimit%20the%20visible%20range%20of%20street%20views%2C%20making%20this%20cross-view%20generation%20task%0Aparticularly%20challenging.%20In%20this%20paper%2C%20we%20introduce%20SkyDiffusion%2C%20a%20novel%0Across-view%20generation%20method%20for%20synthesizing%20aerial%20images%20from%20street%20view%0Aimages%2C%20utilizing%20a%20diffusion%20model%20and%20the%20Bird%27s-Eye%20View%20%28BEV%29%20paradigm.%20The%0ACurved-BEV%20method%20in%20SkyDiffusion%20converts%20street-view%20images%20into%20a%20BEV%0Aperspective%2C%20effectively%20bridging%20the%20domain%20gap%2C%20and%20employs%20a%20%22multi-to-one%22%0Amapping%20strategy%20to%20address%20occlusion%20issues%20in%20dense%20urban%20scenes.%20Next%2C%0ASkyDiffusion%20designed%20a%20BEV-guided%20diffusion%20model%20to%20generate%0Acontent-consistent%20and%20realistic%20aerial%20images.%20Additionally%2C%20we%20introduce%20a%0Anovel%20dataset%2C%20Ground2Aerial-3%2C%20designed%20for%20diverse%20ground-to-aerial%20image%0Asynthesis%20applications%2C%20including%20disaster%20scene%20aerial%20synthesis%2C%20historical%0Ahigh-resolution%20satellite%20image%20synthesis%2C%20and%20low-altitude%20UAV%20image%20synthesis%0Atasks.%20Experimental%20results%20demonstrate%20that%20SkyDiffusion%20outperforms%0Astate-of-the-art%20methods%20on%20cross-view%20datasets%20across%20natural%20%28CVUSA%29%2C%0Asuburban%20%28CVACT%29%2C%20urban%20%28VIGOR-Chicago%29%2C%20and%20various%20application%20scenarios%0A%28G2A-3%29%2C%20achieving%20realistic%20and%20content-consistent%20aerial%20image%20generation.%0AMore%20result%20and%20dataset%20information%20can%20be%20found%20at%0Ahttps%3A//opendatalab.github.io/skydiffusion/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01812v3&entry.124074799=Read"},
{"title": "Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness", "author": "Haotian Zheng and Jinke Ren and Yushan Sun and Ruichen Zhang and Wenbo Zhang and Zhen Li and Dusit Niyato and Shuguang Cui and Yatong Han", "abstract": "  The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.\n", "link": "http://arxiv.org/abs/2412.15127v1", "date": "2024-12-19", "relevancy": 2.5553, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Pruning%20for%20Large%20Language%20Models%20with%20Structural%20Importance%0A%20%20Awareness&body=Title%3A%20Adaptive%20Pruning%20for%20Large%20Language%20Models%20with%20Structural%20Importance%0A%20%20Awareness%0AAuthor%3A%20Haotian%20Zheng%20and%20Jinke%20Ren%20and%20Yushan%20Sun%20and%20Ruichen%20Zhang%20and%20Wenbo%20Zhang%20and%20Zhen%20Li%20and%20Dusit%20Niyato%20and%20Shuguang%20Cui%20and%20Yatong%20Han%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20language%20understanding%20and%20generation%20capabilities.%20However%2C%20it%20is%0Adifficult%20to%20deploy%20LLMs%20on%20resource-constrained%20edge%20devices%20due%20to%20their%20high%0Acomputational%20and%20storage%20resource%20demands.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20LLM%20model%20pruning%20method%2C%20namely%20structurally-aware%20adaptive%20pruning%0A%28SAAP%29%2C%20to%20significantly%20reduce%20the%20computational%20and%20memory%20costs%20while%0Amaintaining%20model%20performance.%20We%20first%20define%20an%20adaptive%20importance%20fusion%0Ametric%20to%20evaluate%20the%20importance%20of%20all%20coupled%20structures%20in%20LLMs%20by%0Aconsidering%20their%20homoscedastic%20uncertainty.%20Then%2C%20we%20rank%20the%20importance%20of%0Aall%20modules%20to%20determine%20the%20specific%20layers%20that%20should%20be%20pruned%20to%20meet%0Aparticular%20performance%20requirements.%20Furthermore%2C%20we%20develop%20a%20new%20group%0Afine-tuning%20strategy%20to%20improve%20the%20inference%20efficiency%20of%20LLMs.%20Finally%2C%20we%0Aevaluate%20the%20proposed%20SAAP%20method%20on%20multiple%20LLMs%20across%20two%20common%20tasks%2C%0Ai.e.%2C%20zero-shot%20classification%20and%20text%20generation.%20Experimental%20results%20show%0Athat%20our%20SAAP%20method%20outperforms%20several%20state-of-the-art%20baseline%20methods%2C%0Aachieving%202.17%25%2C%202.37%25%2C%20and%202.39%25%20accuracy%20gains%20on%20LLaMA-7B%2C%20Vicuna-7B%2C%20and%0ALLaMA-13B.%20Additionally%2C%20SAAP%20improves%20the%20token%20generation%20speed%20by%205%25%2C%0Ashowcasing%20its%20practical%20advantages%20in%20resource-constrained%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Pruning%2520for%2520Large%2520Language%2520Models%2520with%2520Structural%2520Importance%250A%2520%2520Awareness%26entry.906535625%3DHaotian%2520Zheng%2520and%2520Jinke%2520Ren%2520and%2520Yushan%2520Sun%2520and%2520Ruichen%2520Zhang%2520and%2520Wenbo%2520Zhang%2520and%2520Zhen%2520Li%2520and%2520Dusit%2520Niyato%2520and%2520Shuguang%2520Cui%2520and%2520Yatong%2520Han%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%250Aimproved%2520language%2520understanding%2520and%2520generation%2520capabilities.%2520However%252C%2520it%2520is%250Adifficult%2520to%2520deploy%2520LLMs%2520on%2520resource-constrained%2520edge%2520devices%2520due%2520to%2520their%2520high%250Acomputational%2520and%2520storage%2520resource%2520demands.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anovel%2520LLM%2520model%2520pruning%2520method%252C%2520namely%2520structurally-aware%2520adaptive%2520pruning%250A%2528SAAP%2529%252C%2520to%2520significantly%2520reduce%2520the%2520computational%2520and%2520memory%2520costs%2520while%250Amaintaining%2520model%2520performance.%2520We%2520first%2520define%2520an%2520adaptive%2520importance%2520fusion%250Ametric%2520to%2520evaluate%2520the%2520importance%2520of%2520all%2520coupled%2520structures%2520in%2520LLMs%2520by%250Aconsidering%2520their%2520homoscedastic%2520uncertainty.%2520Then%252C%2520we%2520rank%2520the%2520importance%2520of%250Aall%2520modules%2520to%2520determine%2520the%2520specific%2520layers%2520that%2520should%2520be%2520pruned%2520to%2520meet%250Aparticular%2520performance%2520requirements.%2520Furthermore%252C%2520we%2520develop%2520a%2520new%2520group%250Afine-tuning%2520strategy%2520to%2520improve%2520the%2520inference%2520efficiency%2520of%2520LLMs.%2520Finally%252C%2520we%250Aevaluate%2520the%2520proposed%2520SAAP%2520method%2520on%2520multiple%2520LLMs%2520across%2520two%2520common%2520tasks%252C%250Ai.e.%252C%2520zero-shot%2520classification%2520and%2520text%2520generation.%2520Experimental%2520results%2520show%250Athat%2520our%2520SAAP%2520method%2520outperforms%2520several%2520state-of-the-art%2520baseline%2520methods%252C%250Aachieving%25202.17%2525%252C%25202.37%2525%252C%2520and%25202.39%2525%2520accuracy%2520gains%2520on%2520LLaMA-7B%252C%2520Vicuna-7B%252C%2520and%250ALLaMA-13B.%2520Additionally%252C%2520SAAP%2520improves%2520the%2520token%2520generation%2520speed%2520by%25205%2525%252C%250Ashowcasing%2520its%2520practical%2520advantages%2520in%2520resource-constrained%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Pruning%20for%20Large%20Language%20Models%20with%20Structural%20Importance%0A%20%20Awareness&entry.906535625=Haotian%20Zheng%20and%20Jinke%20Ren%20and%20Yushan%20Sun%20and%20Ruichen%20Zhang%20and%20Wenbo%20Zhang%20and%20Zhen%20Li%20and%20Dusit%20Niyato%20and%20Shuguang%20Cui%20and%20Yatong%20Han&entry.1292438233=%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aimproved%20language%20understanding%20and%20generation%20capabilities.%20However%2C%20it%20is%0Adifficult%20to%20deploy%20LLMs%20on%20resource-constrained%20edge%20devices%20due%20to%20their%20high%0Acomputational%20and%20storage%20resource%20demands.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20LLM%20model%20pruning%20method%2C%20namely%20structurally-aware%20adaptive%20pruning%0A%28SAAP%29%2C%20to%20significantly%20reduce%20the%20computational%20and%20memory%20costs%20while%0Amaintaining%20model%20performance.%20We%20first%20define%20an%20adaptive%20importance%20fusion%0Ametric%20to%20evaluate%20the%20importance%20of%20all%20coupled%20structures%20in%20LLMs%20by%0Aconsidering%20their%20homoscedastic%20uncertainty.%20Then%2C%20we%20rank%20the%20importance%20of%0Aall%20modules%20to%20determine%20the%20specific%20layers%20that%20should%20be%20pruned%20to%20meet%0Aparticular%20performance%20requirements.%20Furthermore%2C%20we%20develop%20a%20new%20group%0Afine-tuning%20strategy%20to%20improve%20the%20inference%20efficiency%20of%20LLMs.%20Finally%2C%20we%0Aevaluate%20the%20proposed%20SAAP%20method%20on%20multiple%20LLMs%20across%20two%20common%20tasks%2C%0Ai.e.%2C%20zero-shot%20classification%20and%20text%20generation.%20Experimental%20results%20show%0Athat%20our%20SAAP%20method%20outperforms%20several%20state-of-the-art%20baseline%20methods%2C%0Aachieving%202.17%25%2C%202.37%25%2C%20and%202.39%25%20accuracy%20gains%20on%20LLaMA-7B%2C%20Vicuna-7B%2C%20and%0ALLaMA-13B.%20Additionally%2C%20SAAP%20improves%20the%20token%20generation%20speed%20by%205%25%2C%0Ashowcasing%20its%20practical%20advantages%20in%20resource-constrained%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15127v1&entry.124074799=Read"},
{"title": "MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in\n  T2I Diffusion Models", "author": "Jing Zhao and Heliang Zheng and Chaoyue Wang and Long Lan and Wanrong Hunag and Yuhua Tang", "abstract": "  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable\nof generating famous persons by simply referring to their names. Is it possible\nto make such models generate generic identities as simple as the famous ones,\ne.g., just use a name? In this paper, we explore the existence of a \"Name\nSpace\", where any point in the space corresponds to a specific identity.\nFortunately, we find some clues in the feature space spanned by text embedding\nof celebrities' names. Specifically, we first extract the embeddings of\ncelebrities' names in the Laion5B dataset with the text encoder of diffusion\nmodels. Such embeddings are used as supervision to learn an encoder that can\npredict the name (actually an embedding) of a given face image. We\nexperimentally find that such name embeddings work well in promising the\ngenerated image with good identity consistency. Note that like the names of\ncelebrities, our predicted name embeddings are disentangled from the semantics\nof text inputs, making the original generation capability of text-to-image\nmodels well-preserved. Moreover, by simply plugging such name embeddings, all\nvariants (e.g., from Civitai) derived from the same base model (i.e., SDXL)\nreadily become identity-aware text-to-image models. Project homepage:\n\\url{https://magicfusion.github.io/MagicNaming/}.\n", "link": "http://arxiv.org/abs/2412.14902v1", "date": "2024-12-19", "relevancy": 2.5553, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.737}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5876}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicNaming%3A%20Consistent%20Identity%20Generation%20by%20Finding%20a%20%22Name%20Space%22%20in%0A%20%20T2I%20Diffusion%20Models&body=Title%3A%20MagicNaming%3A%20Consistent%20Identity%20Generation%20by%20Finding%20a%20%22Name%20Space%22%20in%0A%20%20T2I%20Diffusion%20Models%0AAuthor%3A%20Jing%20Zhao%20and%20Heliang%20Zheng%20and%20Chaoyue%20Wang%20and%20Long%20Lan%20and%20Wanrong%20Hunag%20and%20Yuhua%20Tang%0AAbstract%3A%20%20%20Large-scale%20text-to-image%20diffusion%20models%2C%20%28e.g.%2C%20DALL-E%2C%20SDXL%29%20are%20capable%0Aof%20generating%20famous%20persons%20by%20simply%20referring%20to%20their%20names.%20Is%20it%20possible%0Ato%20make%20such%20models%20generate%20generic%20identities%20as%20simple%20as%20the%20famous%20ones%2C%0Ae.g.%2C%20just%20use%20a%20name%3F%20In%20this%20paper%2C%20we%20explore%20the%20existence%20of%20a%20%22Name%0ASpace%22%2C%20where%20any%20point%20in%20the%20space%20corresponds%20to%20a%20specific%20identity.%0AFortunately%2C%20we%20find%20some%20clues%20in%20the%20feature%20space%20spanned%20by%20text%20embedding%0Aof%20celebrities%27%20names.%20Specifically%2C%20we%20first%20extract%20the%20embeddings%20of%0Acelebrities%27%20names%20in%20the%20Laion5B%20dataset%20with%20the%20text%20encoder%20of%20diffusion%0Amodels.%20Such%20embeddings%20are%20used%20as%20supervision%20to%20learn%20an%20encoder%20that%20can%0Apredict%20the%20name%20%28actually%20an%20embedding%29%20of%20a%20given%20face%20image.%20We%0Aexperimentally%20find%20that%20such%20name%20embeddings%20work%20well%20in%20promising%20the%0Agenerated%20image%20with%20good%20identity%20consistency.%20Note%20that%20like%20the%20names%20of%0Acelebrities%2C%20our%20predicted%20name%20embeddings%20are%20disentangled%20from%20the%20semantics%0Aof%20text%20inputs%2C%20making%20the%20original%20generation%20capability%20of%20text-to-image%0Amodels%20well-preserved.%20Moreover%2C%20by%20simply%20plugging%20such%20name%20embeddings%2C%20all%0Avariants%20%28e.g.%2C%20from%20Civitai%29%20derived%20from%20the%20same%20base%20model%20%28i.e.%2C%20SDXL%29%0Areadily%20become%20identity-aware%20text-to-image%20models.%20Project%20homepage%3A%0A%5Curl%7Bhttps%3A//magicfusion.github.io/MagicNaming/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicNaming%253A%2520Consistent%2520Identity%2520Generation%2520by%2520Finding%2520a%2520%2522Name%2520Space%2522%2520in%250A%2520%2520T2I%2520Diffusion%2520Models%26entry.906535625%3DJing%2520Zhao%2520and%2520Heliang%2520Zheng%2520and%2520Chaoyue%2520Wang%2520and%2520Long%2520Lan%2520and%2520Wanrong%2520Hunag%2520and%2520Yuhua%2520Tang%26entry.1292438233%3D%2520%2520Large-scale%2520text-to-image%2520diffusion%2520models%252C%2520%2528e.g.%252C%2520DALL-E%252C%2520SDXL%2529%2520are%2520capable%250Aof%2520generating%2520famous%2520persons%2520by%2520simply%2520referring%2520to%2520their%2520names.%2520Is%2520it%2520possible%250Ato%2520make%2520such%2520models%2520generate%2520generic%2520identities%2520as%2520simple%2520as%2520the%2520famous%2520ones%252C%250Ae.g.%252C%2520just%2520use%2520a%2520name%253F%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520existence%2520of%2520a%2520%2522Name%250ASpace%2522%252C%2520where%2520any%2520point%2520in%2520the%2520space%2520corresponds%2520to%2520a%2520specific%2520identity.%250AFortunately%252C%2520we%2520find%2520some%2520clues%2520in%2520the%2520feature%2520space%2520spanned%2520by%2520text%2520embedding%250Aof%2520celebrities%2527%2520names.%2520Specifically%252C%2520we%2520first%2520extract%2520the%2520embeddings%2520of%250Acelebrities%2527%2520names%2520in%2520the%2520Laion5B%2520dataset%2520with%2520the%2520text%2520encoder%2520of%2520diffusion%250Amodels.%2520Such%2520embeddings%2520are%2520used%2520as%2520supervision%2520to%2520learn%2520an%2520encoder%2520that%2520can%250Apredict%2520the%2520name%2520%2528actually%2520an%2520embedding%2529%2520of%2520a%2520given%2520face%2520image.%2520We%250Aexperimentally%2520find%2520that%2520such%2520name%2520embeddings%2520work%2520well%2520in%2520promising%2520the%250Agenerated%2520image%2520with%2520good%2520identity%2520consistency.%2520Note%2520that%2520like%2520the%2520names%2520of%250Acelebrities%252C%2520our%2520predicted%2520name%2520embeddings%2520are%2520disentangled%2520from%2520the%2520semantics%250Aof%2520text%2520inputs%252C%2520making%2520the%2520original%2520generation%2520capability%2520of%2520text-to-image%250Amodels%2520well-preserved.%2520Moreover%252C%2520by%2520simply%2520plugging%2520such%2520name%2520embeddings%252C%2520all%250Avariants%2520%2528e.g.%252C%2520from%2520Civitai%2529%2520derived%2520from%2520the%2520same%2520base%2520model%2520%2528i.e.%252C%2520SDXL%2529%250Areadily%2520become%2520identity-aware%2520text-to-image%2520models.%2520Project%2520homepage%253A%250A%255Curl%257Bhttps%253A//magicfusion.github.io/MagicNaming/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicNaming%3A%20Consistent%20Identity%20Generation%20by%20Finding%20a%20%22Name%20Space%22%20in%0A%20%20T2I%20Diffusion%20Models&entry.906535625=Jing%20Zhao%20and%20Heliang%20Zheng%20and%20Chaoyue%20Wang%20and%20Long%20Lan%20and%20Wanrong%20Hunag%20and%20Yuhua%20Tang&entry.1292438233=%20%20Large-scale%20text-to-image%20diffusion%20models%2C%20%28e.g.%2C%20DALL-E%2C%20SDXL%29%20are%20capable%0Aof%20generating%20famous%20persons%20by%20simply%20referring%20to%20their%20names.%20Is%20it%20possible%0Ato%20make%20such%20models%20generate%20generic%20identities%20as%20simple%20as%20the%20famous%20ones%2C%0Ae.g.%2C%20just%20use%20a%20name%3F%20In%20this%20paper%2C%20we%20explore%20the%20existence%20of%20a%20%22Name%0ASpace%22%2C%20where%20any%20point%20in%20the%20space%20corresponds%20to%20a%20specific%20identity.%0AFortunately%2C%20we%20find%20some%20clues%20in%20the%20feature%20space%20spanned%20by%20text%20embedding%0Aof%20celebrities%27%20names.%20Specifically%2C%20we%20first%20extract%20the%20embeddings%20of%0Acelebrities%27%20names%20in%20the%20Laion5B%20dataset%20with%20the%20text%20encoder%20of%20diffusion%0Amodels.%20Such%20embeddings%20are%20used%20as%20supervision%20to%20learn%20an%20encoder%20that%20can%0Apredict%20the%20name%20%28actually%20an%20embedding%29%20of%20a%20given%20face%20image.%20We%0Aexperimentally%20find%20that%20such%20name%20embeddings%20work%20well%20in%20promising%20the%0Agenerated%20image%20with%20good%20identity%20consistency.%20Note%20that%20like%20the%20names%20of%0Acelebrities%2C%20our%20predicted%20name%20embeddings%20are%20disentangled%20from%20the%20semantics%0Aof%20text%20inputs%2C%20making%20the%20original%20generation%20capability%20of%20text-to-image%0Amodels%20well-preserved.%20Moreover%2C%20by%20simply%20plugging%20such%20name%20embeddings%2C%20all%0Avariants%20%28e.g.%2C%20from%20Civitai%29%20derived%20from%20the%20same%20base%20model%20%28i.e.%2C%20SDXL%29%0Areadily%20become%20identity-aware%20text-to-image%20models.%20Project%20homepage%3A%0A%5Curl%7Bhttps%3A//magicfusion.github.io/MagicNaming/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14902v1&entry.124074799=Read"},
{"title": "Generative Multiview Relighting for 3D Reconstruction under Extreme\n  Illumination Variation", "author": "Hadi Alzayer and Philipp Henzler and Jonathan T. Barron and Jia-Bin Huang and Pratul P. Srinivasan and Dor Verbin", "abstract": "  Reconstructing the geometry and appearance of objects from photographs taken\nin different environments is difficult as the illumination and therefore the\nobject appearance vary across captured images. This is particularly challenging\nfor more specular objects whose appearance strongly depends on the viewing\ndirection. Some prior approaches model appearance variation across images using\na per-image embedding vector, while others use physically-based rendering to\nrecover the materials and per-image illumination. Such approaches fail at\nfaithfully recovering view-dependent appearance given significant variation in\ninput illumination and tend to produce mostly diffuse results. We present an\napproach that reconstructs objects from images taken under different\nilluminations by first relighting the images under a single reference\nillumination with a multiview relighting diffusion model and then\nreconstructing the object's geometry and appearance with a radiance field\narchitecture that is robust to the small remaining inconsistencies among the\nrelit images. We validate our proposed approach on both synthetic and real\ndatasets and demonstrate that it greatly outperforms existing techniques at\nreconstructing high-fidelity appearance from images taken under extreme\nillumination variation. Moreover, our approach is particularly effective at\nrecovering view-dependent \"shiny\" appearance which cannot be reconstructed by\nprior methods.\n", "link": "http://arxiv.org/abs/2412.15211v1", "date": "2024-12-19", "relevancy": 2.5488, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6414}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6414}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Multiview%20Relighting%20for%203D%20Reconstruction%20under%20Extreme%0A%20%20Illumination%20Variation&body=Title%3A%20Generative%20Multiview%20Relighting%20for%203D%20Reconstruction%20under%20Extreme%0A%20%20Illumination%20Variation%0AAuthor%3A%20Hadi%20Alzayer%20and%20Philipp%20Henzler%20and%20Jonathan%20T.%20Barron%20and%20Jia-Bin%20Huang%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin%0AAbstract%3A%20%20%20Reconstructing%20the%20geometry%20and%20appearance%20of%20objects%20from%20photographs%20taken%0Ain%20different%20environments%20is%20difficult%20as%20the%20illumination%20and%20therefore%20the%0Aobject%20appearance%20vary%20across%20captured%20images.%20This%20is%20particularly%20challenging%0Afor%20more%20specular%20objects%20whose%20appearance%20strongly%20depends%20on%20the%20viewing%0Adirection.%20Some%20prior%20approaches%20model%20appearance%20variation%20across%20images%20using%0Aa%20per-image%20embedding%20vector%2C%20while%20others%20use%20physically-based%20rendering%20to%0Arecover%20the%20materials%20and%20per-image%20illumination.%20Such%20approaches%20fail%20at%0Afaithfully%20recovering%20view-dependent%20appearance%20given%20significant%20variation%20in%0Ainput%20illumination%20and%20tend%20to%20produce%20mostly%20diffuse%20results.%20We%20present%20an%0Aapproach%20that%20reconstructs%20objects%20from%20images%20taken%20under%20different%0Ailluminations%20by%20first%20relighting%20the%20images%20under%20a%20single%20reference%0Aillumination%20with%20a%20multiview%20relighting%20diffusion%20model%20and%20then%0Areconstructing%20the%20object%27s%20geometry%20and%20appearance%20with%20a%20radiance%20field%0Aarchitecture%20that%20is%20robust%20to%20the%20small%20remaining%20inconsistencies%20among%20the%0Arelit%20images.%20We%20validate%20our%20proposed%20approach%20on%20both%20synthetic%20and%20real%0Adatasets%20and%20demonstrate%20that%20it%20greatly%20outperforms%20existing%20techniques%20at%0Areconstructing%20high-fidelity%20appearance%20from%20images%20taken%20under%20extreme%0Aillumination%20variation.%20Moreover%2C%20our%20approach%20is%20particularly%20effective%20at%0Arecovering%20view-dependent%20%22shiny%22%20appearance%20which%20cannot%20be%20reconstructed%20by%0Aprior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Multiview%2520Relighting%2520for%25203D%2520Reconstruction%2520under%2520Extreme%250A%2520%2520Illumination%2520Variation%26entry.906535625%3DHadi%2520Alzayer%2520and%2520Philipp%2520Henzler%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Jia-Bin%2520Huang%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Dor%2520Verbin%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520geometry%2520and%2520appearance%2520of%2520objects%2520from%2520photographs%2520taken%250Ain%2520different%2520environments%2520is%2520difficult%2520as%2520the%2520illumination%2520and%2520therefore%2520the%250Aobject%2520appearance%2520vary%2520across%2520captured%2520images.%2520This%2520is%2520particularly%2520challenging%250Afor%2520more%2520specular%2520objects%2520whose%2520appearance%2520strongly%2520depends%2520on%2520the%2520viewing%250Adirection.%2520Some%2520prior%2520approaches%2520model%2520appearance%2520variation%2520across%2520images%2520using%250Aa%2520per-image%2520embedding%2520vector%252C%2520while%2520others%2520use%2520physically-based%2520rendering%2520to%250Arecover%2520the%2520materials%2520and%2520per-image%2520illumination.%2520Such%2520approaches%2520fail%2520at%250Afaithfully%2520recovering%2520view-dependent%2520appearance%2520given%2520significant%2520variation%2520in%250Ainput%2520illumination%2520and%2520tend%2520to%2520produce%2520mostly%2520diffuse%2520results.%2520We%2520present%2520an%250Aapproach%2520that%2520reconstructs%2520objects%2520from%2520images%2520taken%2520under%2520different%250Ailluminations%2520by%2520first%2520relighting%2520the%2520images%2520under%2520a%2520single%2520reference%250Aillumination%2520with%2520a%2520multiview%2520relighting%2520diffusion%2520model%2520and%2520then%250Areconstructing%2520the%2520object%2527s%2520geometry%2520and%2520appearance%2520with%2520a%2520radiance%2520field%250Aarchitecture%2520that%2520is%2520robust%2520to%2520the%2520small%2520remaining%2520inconsistencies%2520among%2520the%250Arelit%2520images.%2520We%2520validate%2520our%2520proposed%2520approach%2520on%2520both%2520synthetic%2520and%2520real%250Adatasets%2520and%2520demonstrate%2520that%2520it%2520greatly%2520outperforms%2520existing%2520techniques%2520at%250Areconstructing%2520high-fidelity%2520appearance%2520from%2520images%2520taken%2520under%2520extreme%250Aillumination%2520variation.%2520Moreover%252C%2520our%2520approach%2520is%2520particularly%2520effective%2520at%250Arecovering%2520view-dependent%2520%2522shiny%2522%2520appearance%2520which%2520cannot%2520be%2520reconstructed%2520by%250Aprior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Multiview%20Relighting%20for%203D%20Reconstruction%20under%20Extreme%0A%20%20Illumination%20Variation&entry.906535625=Hadi%20Alzayer%20and%20Philipp%20Henzler%20and%20Jonathan%20T.%20Barron%20and%20Jia-Bin%20Huang%20and%20Pratul%20P.%20Srinivasan%20and%20Dor%20Verbin&entry.1292438233=%20%20Reconstructing%20the%20geometry%20and%20appearance%20of%20objects%20from%20photographs%20taken%0Ain%20different%20environments%20is%20difficult%20as%20the%20illumination%20and%20therefore%20the%0Aobject%20appearance%20vary%20across%20captured%20images.%20This%20is%20particularly%20challenging%0Afor%20more%20specular%20objects%20whose%20appearance%20strongly%20depends%20on%20the%20viewing%0Adirection.%20Some%20prior%20approaches%20model%20appearance%20variation%20across%20images%20using%0Aa%20per-image%20embedding%20vector%2C%20while%20others%20use%20physically-based%20rendering%20to%0Arecover%20the%20materials%20and%20per-image%20illumination.%20Such%20approaches%20fail%20at%0Afaithfully%20recovering%20view-dependent%20appearance%20given%20significant%20variation%20in%0Ainput%20illumination%20and%20tend%20to%20produce%20mostly%20diffuse%20results.%20We%20present%20an%0Aapproach%20that%20reconstructs%20objects%20from%20images%20taken%20under%20different%0Ailluminations%20by%20first%20relighting%20the%20images%20under%20a%20single%20reference%0Aillumination%20with%20a%20multiview%20relighting%20diffusion%20model%20and%20then%0Areconstructing%20the%20object%27s%20geometry%20and%20appearance%20with%20a%20radiance%20field%0Aarchitecture%20that%20is%20robust%20to%20the%20small%20remaining%20inconsistencies%20among%20the%0Arelit%20images.%20We%20validate%20our%20proposed%20approach%20on%20both%20synthetic%20and%20real%0Adatasets%20and%20demonstrate%20that%20it%20greatly%20outperforms%20existing%20techniques%20at%0Areconstructing%20high-fidelity%20appearance%20from%20images%20taken%20under%20extreme%0Aillumination%20variation.%20Moreover%2C%20our%20approach%20is%20particularly%20effective%20at%0Arecovering%20view-dependent%20%22shiny%22%20appearance%20which%20cannot%20be%20reconstructed%20by%0Aprior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15211v1&entry.124074799=Read"},
{"title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings", "author": "Duo Wang and Yuan Zuo and Fengzhi Li and Junjie Wu", "abstract": "  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n", "link": "http://arxiv.org/abs/2408.14512v3", "date": "2024-12-19", "relevancy": 2.5479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings&body=Title%3A%20LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings%0AAuthor%3A%20Duo%20Wang%20and%20Yuan%20Zuo%20and%20Fengzhi%20Li%20and%20Junjie%20Wu%0AAbstract%3A%20%20%20Zero-shot%20graph%20machine%20learning%2C%20especially%20with%20graph%20neural%20networks%0A%28GNNs%29%2C%20has%20garnered%20significant%20interest%20due%20to%20the%20challenge%20of%20scarce%0Alabeled%20data.%20While%20methods%20like%20self-supervised%20learning%20and%20graph%20prompt%0Alearning%20have%20been%20extensively%20explored%2C%20they%20often%20rely%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20in%20zero-shot%20scenarios.%0AInspired%20by%20the%20zero-shot%20capabilities%20of%20instruction-fine-tuned%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20introduce%20a%20novel%20framework%20named%20Token%20Embedding-Aligned%0AGraph%20Language%20Model%20%28TEA-GLM%29%20that%20leverages%20LLMs%20as%20cross-dataset%20and%0Across-task%20zero-shot%20learners%20for%20graph%20machine%20learning.%20Concretely%2C%20we%0Apretrain%20a%20GNN%2C%20aligning%20its%20representations%20with%20token%20embeddings%20of%20an%20LLM.%0AWe%20then%20train%20a%20linear%20projector%20that%20transforms%20the%20GNN%27s%20representations%20into%0Aa%20fixed%20number%20of%20graph%20token%20embeddings%20without%20tuning%20the%20LLM.%20A%20unified%0Ainstruction%20is%20designed%20for%20various%20graph%20tasks%20at%20different%20levels%2C%20such%20as%0Anode%20classification%20%28node-level%29%20and%20link%20prediction%20%28edge-level%29.%20These%20design%0Achoices%20collectively%20enhance%20our%20method%27s%20effectiveness%20in%20zero-shot%20learning%2C%0Asetting%20it%20apart%20from%20existing%20methods.%20Experiments%20show%20that%20our%20graph%20token%0Aembeddings%20help%20the%20LLM%20predictor%20achieve%20state-of-the-art%20performance%20on%0Aunseen%20datasets%20and%20tasks%20compared%20to%20other%20methods%20using%20LLMs%20as%20predictors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14512v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Zero-shot%2520Graph%2520Learners%253A%2520Alignment%2520of%2520GNN%2520Representations%2520with%250A%2520%2520LLM%2520Token%2520Embeddings%26entry.906535625%3DDuo%2520Wang%2520and%2520Yuan%2520Zuo%2520and%2520Fengzhi%2520Li%2520and%2520Junjie%2520Wu%26entry.1292438233%3D%2520%2520Zero-shot%2520graph%2520machine%2520learning%252C%2520especially%2520with%2520graph%2520neural%2520networks%250A%2528GNNs%2529%252C%2520has%2520garnered%2520significant%2520interest%2520due%2520to%2520the%2520challenge%2520of%2520scarce%250Alabeled%2520data.%2520While%2520methods%2520like%2520self-supervised%2520learning%2520and%2520graph%2520prompt%250Alearning%2520have%2520been%2520extensively%2520explored%252C%2520they%2520often%2520rely%2520on%2520fine-tuning%2520with%250Atask-specific%2520labels%252C%2520limiting%2520their%2520effectiveness%2520in%2520zero-shot%2520scenarios.%250AInspired%2520by%2520the%2520zero-shot%2520capabilities%2520of%2520instruction-fine-tuned%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520named%2520Token%2520Embedding-Aligned%250AGraph%2520Language%2520Model%2520%2528TEA-GLM%2529%2520that%2520leverages%2520LLMs%2520as%2520cross-dataset%2520and%250Across-task%2520zero-shot%2520learners%2520for%2520graph%2520machine%2520learning.%2520Concretely%252C%2520we%250Apretrain%2520a%2520GNN%252C%2520aligning%2520its%2520representations%2520with%2520token%2520embeddings%2520of%2520an%2520LLM.%250AWe%2520then%2520train%2520a%2520linear%2520projector%2520that%2520transforms%2520the%2520GNN%2527s%2520representations%2520into%250Aa%2520fixed%2520number%2520of%2520graph%2520token%2520embeddings%2520without%2520tuning%2520the%2520LLM.%2520A%2520unified%250Ainstruction%2520is%2520designed%2520for%2520various%2520graph%2520tasks%2520at%2520different%2520levels%252C%2520such%2520as%250Anode%2520classification%2520%2528node-level%2529%2520and%2520link%2520prediction%2520%2528edge-level%2529.%2520These%2520design%250Achoices%2520collectively%2520enhance%2520our%2520method%2527s%2520effectiveness%2520in%2520zero-shot%2520learning%252C%250Asetting%2520it%2520apart%2520from%2520existing%2520methods.%2520Experiments%2520show%2520that%2520our%2520graph%2520token%250Aembeddings%2520help%2520the%2520LLM%2520predictor%2520achieve%2520state-of-the-art%2520performance%2520on%250Aunseen%2520datasets%2520and%2520tasks%2520compared%2520to%2520other%2520methods%2520using%2520LLMs%2520as%2520predictors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14512v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Zero-shot%20Graph%20Learners%3A%20Alignment%20of%20GNN%20Representations%20with%0A%20%20LLM%20Token%20Embeddings&entry.906535625=Duo%20Wang%20and%20Yuan%20Zuo%20and%20Fengzhi%20Li%20and%20Junjie%20Wu&entry.1292438233=%20%20Zero-shot%20graph%20machine%20learning%2C%20especially%20with%20graph%20neural%20networks%0A%28GNNs%29%2C%20has%20garnered%20significant%20interest%20due%20to%20the%20challenge%20of%20scarce%0Alabeled%20data.%20While%20methods%20like%20self-supervised%20learning%20and%20graph%20prompt%0Alearning%20have%20been%20extensively%20explored%2C%20they%20often%20rely%20on%20fine-tuning%20with%0Atask-specific%20labels%2C%20limiting%20their%20effectiveness%20in%20zero-shot%20scenarios.%0AInspired%20by%20the%20zero-shot%20capabilities%20of%20instruction-fine-tuned%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20introduce%20a%20novel%20framework%20named%20Token%20Embedding-Aligned%0AGraph%20Language%20Model%20%28TEA-GLM%29%20that%20leverages%20LLMs%20as%20cross-dataset%20and%0Across-task%20zero-shot%20learners%20for%20graph%20machine%20learning.%20Concretely%2C%20we%0Apretrain%20a%20GNN%2C%20aligning%20its%20representations%20with%20token%20embeddings%20of%20an%20LLM.%0AWe%20then%20train%20a%20linear%20projector%20that%20transforms%20the%20GNN%27s%20representations%20into%0Aa%20fixed%20number%20of%20graph%20token%20embeddings%20without%20tuning%20the%20LLM.%20A%20unified%0Ainstruction%20is%20designed%20for%20various%20graph%20tasks%20at%20different%20levels%2C%20such%20as%0Anode%20classification%20%28node-level%29%20and%20link%20prediction%20%28edge-level%29.%20These%20design%0Achoices%20collectively%20enhance%20our%20method%27s%20effectiveness%20in%20zero-shot%20learning%2C%0Asetting%20it%20apart%20from%20existing%20methods.%20Experiments%20show%20that%20our%20graph%20token%0Aembeddings%20help%20the%20LLM%20predictor%20achieve%20state-of-the-art%20performance%20on%0Aunseen%20datasets%20and%20tasks%20compared%20to%20other%20methods%20using%20LLMs%20as%20predictors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14512v3&entry.124074799=Read"},
{"title": "OnlineVPO: Align Video Diffusion Model with Online Video-Centric\n  Preference Optimization", "author": "Jiacheng Zhang and Jie Wu and Weifeng Chen and Yatai Ji and Xuefeng Xiao and Weilin Huang and Kai Han", "abstract": "  In recent years, the field of text-to-video (T2V) generation has made\nsignificant strides. Despite this progress, there is still a gap between\ntheoretical advancements and practical application, amplified by issues like\ndegraded image quality and flickering artifacts. Recent advancements in\nenhancing the video diffusion model (VDM) through feedback learning have shown\npromising results. However, these methods still exhibit notable limitations,\nsuch as misaligned feedback and inferior scalability. To tackle these issues,\nwe introduce OnlineVPO, a more efficient preference learning approach tailored\nspecifically for video diffusion models. Our method features two novel designs,\nfirstly, instead of directly using image-based reward feedback, we leverage the\nvideo quality assessment (VQA) model trained on synthetic data as the reward\nmodel to provide distribution and modality-aligned feedback on the video\ndiffusion model. Additionally, we introduce an online DPO algorithm to address\nthe off-policy optimization and scalability issue in existing video preference\nlearning frameworks. By employing the video reward model to offer concise video\nfeedback on the fly, OnlineVPO offers effective and efficient preference\nguidance. Extensive experiments on the open-source video-diffusion model\ndemonstrate OnlineVPO as a simple yet effective and more importantly scalable\npreference learning algorithm for video diffusion models, offering valuable\ninsights for future advancements in this domain.\n", "link": "http://arxiv.org/abs/2412.15159v1", "date": "2024-12-19", "relevancy": 2.5459, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6719}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6446}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OnlineVPO%3A%20Align%20Video%20Diffusion%20Model%20with%20Online%20Video-Centric%0A%20%20Preference%20Optimization&body=Title%3A%20OnlineVPO%3A%20Align%20Video%20Diffusion%20Model%20with%20Online%20Video-Centric%0A%20%20Preference%20Optimization%0AAuthor%3A%20Jiacheng%20Zhang%20and%20Jie%20Wu%20and%20Weifeng%20Chen%20and%20Yatai%20Ji%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Kai%20Han%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20field%20of%20text-to-video%20%28T2V%29%20generation%20has%20made%0Asignificant%20strides.%20Despite%20this%20progress%2C%20there%20is%20still%20a%20gap%20between%0Atheoretical%20advancements%20and%20practical%20application%2C%20amplified%20by%20issues%20like%0Adegraded%20image%20quality%20and%20flickering%20artifacts.%20Recent%20advancements%20in%0Aenhancing%20the%20video%20diffusion%20model%20%28VDM%29%20through%20feedback%20learning%20have%20shown%0Apromising%20results.%20However%2C%20these%20methods%20still%20exhibit%20notable%20limitations%2C%0Asuch%20as%20misaligned%20feedback%20and%20inferior%20scalability.%20To%20tackle%20these%20issues%2C%0Awe%20introduce%20OnlineVPO%2C%20a%20more%20efficient%20preference%20learning%20approach%20tailored%0Aspecifically%20for%20video%20diffusion%20models.%20Our%20method%20features%20two%20novel%20designs%2C%0Afirstly%2C%20instead%20of%20directly%20using%20image-based%20reward%20feedback%2C%20we%20leverage%20the%0Avideo%20quality%20assessment%20%28VQA%29%20model%20trained%20on%20synthetic%20data%20as%20the%20reward%0Amodel%20to%20provide%20distribution%20and%20modality-aligned%20feedback%20on%20the%20video%0Adiffusion%20model.%20Additionally%2C%20we%20introduce%20an%20online%20DPO%20algorithm%20to%20address%0Athe%20off-policy%20optimization%20and%20scalability%20issue%20in%20existing%20video%20preference%0Alearning%20frameworks.%20By%20employing%20the%20video%20reward%20model%20to%20offer%20concise%20video%0Afeedback%20on%20the%20fly%2C%20OnlineVPO%20offers%20effective%20and%20efficient%20preference%0Aguidance.%20Extensive%20experiments%20on%20the%20open-source%20video-diffusion%20model%0Ademonstrate%20OnlineVPO%20as%20a%20simple%20yet%20effective%20and%20more%20importantly%20scalable%0Apreference%20learning%20algorithm%20for%20video%20diffusion%20models%2C%20offering%20valuable%0Ainsights%20for%20future%20advancements%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnlineVPO%253A%2520Align%2520Video%2520Diffusion%2520Model%2520with%2520Online%2520Video-Centric%250A%2520%2520Preference%2520Optimization%26entry.906535625%3DJiacheng%2520Zhang%2520and%2520Jie%2520Wu%2520and%2520Weifeng%2520Chen%2520and%2520Yatai%2520Ji%2520and%2520Xuefeng%2520Xiao%2520and%2520Weilin%2520Huang%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520field%2520of%2520text-to-video%2520%2528T2V%2529%2520generation%2520has%2520made%250Asignificant%2520strides.%2520Despite%2520this%2520progress%252C%2520there%2520is%2520still%2520a%2520gap%2520between%250Atheoretical%2520advancements%2520and%2520practical%2520application%252C%2520amplified%2520by%2520issues%2520like%250Adegraded%2520image%2520quality%2520and%2520flickering%2520artifacts.%2520Recent%2520advancements%2520in%250Aenhancing%2520the%2520video%2520diffusion%2520model%2520%2528VDM%2529%2520through%2520feedback%2520learning%2520have%2520shown%250Apromising%2520results.%2520However%252C%2520these%2520methods%2520still%2520exhibit%2520notable%2520limitations%252C%250Asuch%2520as%2520misaligned%2520feedback%2520and%2520inferior%2520scalability.%2520To%2520tackle%2520these%2520issues%252C%250Awe%2520introduce%2520OnlineVPO%252C%2520a%2520more%2520efficient%2520preference%2520learning%2520approach%2520tailored%250Aspecifically%2520for%2520video%2520diffusion%2520models.%2520Our%2520method%2520features%2520two%2520novel%2520designs%252C%250Afirstly%252C%2520instead%2520of%2520directly%2520using%2520image-based%2520reward%2520feedback%252C%2520we%2520leverage%2520the%250Avideo%2520quality%2520assessment%2520%2528VQA%2529%2520model%2520trained%2520on%2520synthetic%2520data%2520as%2520the%2520reward%250Amodel%2520to%2520provide%2520distribution%2520and%2520modality-aligned%2520feedback%2520on%2520the%2520video%250Adiffusion%2520model.%2520Additionally%252C%2520we%2520introduce%2520an%2520online%2520DPO%2520algorithm%2520to%2520address%250Athe%2520off-policy%2520optimization%2520and%2520scalability%2520issue%2520in%2520existing%2520video%2520preference%250Alearning%2520frameworks.%2520By%2520employing%2520the%2520video%2520reward%2520model%2520to%2520offer%2520concise%2520video%250Afeedback%2520on%2520the%2520fly%252C%2520OnlineVPO%2520offers%2520effective%2520and%2520efficient%2520preference%250Aguidance.%2520Extensive%2520experiments%2520on%2520the%2520open-source%2520video-diffusion%2520model%250Ademonstrate%2520OnlineVPO%2520as%2520a%2520simple%2520yet%2520effective%2520and%2520more%2520importantly%2520scalable%250Apreference%2520learning%2520algorithm%2520for%2520video%2520diffusion%2520models%252C%2520offering%2520valuable%250Ainsights%2520for%2520future%2520advancements%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OnlineVPO%3A%20Align%20Video%20Diffusion%20Model%20with%20Online%20Video-Centric%0A%20%20Preference%20Optimization&entry.906535625=Jiacheng%20Zhang%20and%20Jie%20Wu%20and%20Weifeng%20Chen%20and%20Yatai%20Ji%20and%20Xuefeng%20Xiao%20and%20Weilin%20Huang%20and%20Kai%20Han&entry.1292438233=%20%20In%20recent%20years%2C%20the%20field%20of%20text-to-video%20%28T2V%29%20generation%20has%20made%0Asignificant%20strides.%20Despite%20this%20progress%2C%20there%20is%20still%20a%20gap%20between%0Atheoretical%20advancements%20and%20practical%20application%2C%20amplified%20by%20issues%20like%0Adegraded%20image%20quality%20and%20flickering%20artifacts.%20Recent%20advancements%20in%0Aenhancing%20the%20video%20diffusion%20model%20%28VDM%29%20through%20feedback%20learning%20have%20shown%0Apromising%20results.%20However%2C%20these%20methods%20still%20exhibit%20notable%20limitations%2C%0Asuch%20as%20misaligned%20feedback%20and%20inferior%20scalability.%20To%20tackle%20these%20issues%2C%0Awe%20introduce%20OnlineVPO%2C%20a%20more%20efficient%20preference%20learning%20approach%20tailored%0Aspecifically%20for%20video%20diffusion%20models.%20Our%20method%20features%20two%20novel%20designs%2C%0Afirstly%2C%20instead%20of%20directly%20using%20image-based%20reward%20feedback%2C%20we%20leverage%20the%0Avideo%20quality%20assessment%20%28VQA%29%20model%20trained%20on%20synthetic%20data%20as%20the%20reward%0Amodel%20to%20provide%20distribution%20and%20modality-aligned%20feedback%20on%20the%20video%0Adiffusion%20model.%20Additionally%2C%20we%20introduce%20an%20online%20DPO%20algorithm%20to%20address%0Athe%20off-policy%20optimization%20and%20scalability%20issue%20in%20existing%20video%20preference%0Alearning%20frameworks.%20By%20employing%20the%20video%20reward%20model%20to%20offer%20concise%20video%0Afeedback%20on%20the%20fly%2C%20OnlineVPO%20offers%20effective%20and%20efficient%20preference%0Aguidance.%20Extensive%20experiments%20on%20the%20open-source%20video-diffusion%20model%0Ademonstrate%20OnlineVPO%20as%20a%20simple%20yet%20effective%20and%20more%20importantly%20scalable%0Apreference%20learning%20algorithm%20for%20video%20diffusion%20models%2C%20offering%20valuable%0Ainsights%20for%20future%20advancements%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15159v1&entry.124074799=Read"},
{"title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization", "author": "Tian Qin and Naomi Saphra and David Alvarez-Melis", "abstract": "  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n", "link": "http://arxiv.org/abs/2412.04619v3", "date": "2024-12-19", "relevancy": 2.535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization&body=Title%3A%20Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization%0AAuthor%3A%20Tian%20Qin%20and%20Naomi%20Saphra%20and%20David%20Alvarez-Melis%0AAbstract%3A%20%20%20Language%20models%20%28LMs%29%2C%20like%20other%20neural%20networks%2C%20often%20favor%20shortcut%0Aheuristics%20based%20on%20surface-level%20patterns.%20Although%20LMs%20behave%20like%20n-gram%0Amodels%20early%20in%20training%2C%20they%20must%20eventually%20learn%20hierarchical%20syntactic%0Arepresentations%20to%20correctly%20apply%20grammatical%20rules%20out-of-distribution%20%28OOD%29.%0AIn%20this%20work%2C%20we%20use%20case%20studies%20of%20English%20grammar%20to%20explore%20how%20complex%2C%0Adiverse%20training%20data%20drives%20models%20to%20generalize%20OOD.%20We%20construct%20a%20framework%0Athat%20unifies%20our%20understanding%20of%20random%20variation%20with%20training%20dynamics%2C%20rule%0Aselection%20with%20memorization%2C%20and%20data%20diversity%20with%20complexity.%20We%20show%20that%0Athese%20factors%20are%20nuanced%2C%20and%20that%20intermediate%20levels%20of%20diversity%20and%0Acomplexity%20lead%20to%20inconsistent%20behavior%20across%20random%20seeds%20and%20to%20unstable%0Atraining%20dynamics.%20Our%20findings%20emphasize%20the%20critical%20role%20of%20training%20data%20in%0Ashaping%20generalization%20patterns%20and%20illuminate%20how%20competing%20model%20strategies%0Alead%20to%20inconsistent%20generalization%20outcomes%20across%20random%20seeds.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sunnytqin/concept_comp.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04619v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSometimes%2520I%2520am%2520a%2520Tree%253A%2520Data%2520Drives%2520Unstable%2520Hierarchical%2520Generalization%26entry.906535625%3DTian%2520Qin%2520and%2520Naomi%2520Saphra%2520and%2520David%2520Alvarez-Melis%26entry.1292438233%3D%2520%2520Language%2520models%2520%2528LMs%2529%252C%2520like%2520other%2520neural%2520networks%252C%2520often%2520favor%2520shortcut%250Aheuristics%2520based%2520on%2520surface-level%2520patterns.%2520Although%2520LMs%2520behave%2520like%2520n-gram%250Amodels%2520early%2520in%2520training%252C%2520they%2520must%2520eventually%2520learn%2520hierarchical%2520syntactic%250Arepresentations%2520to%2520correctly%2520apply%2520grammatical%2520rules%2520out-of-distribution%2520%2528OOD%2529.%250AIn%2520this%2520work%252C%2520we%2520use%2520case%2520studies%2520of%2520English%2520grammar%2520to%2520explore%2520how%2520complex%252C%250Adiverse%2520training%2520data%2520drives%2520models%2520to%2520generalize%2520OOD.%2520We%2520construct%2520a%2520framework%250Athat%2520unifies%2520our%2520understanding%2520of%2520random%2520variation%2520with%2520training%2520dynamics%252C%2520rule%250Aselection%2520with%2520memorization%252C%2520and%2520data%2520diversity%2520with%2520complexity.%2520We%2520show%2520that%250Athese%2520factors%2520are%2520nuanced%252C%2520and%2520that%2520intermediate%2520levels%2520of%2520diversity%2520and%250Acomplexity%2520lead%2520to%2520inconsistent%2520behavior%2520across%2520random%2520seeds%2520and%2520to%2520unstable%250Atraining%2520dynamics.%2520Our%2520findings%2520emphasize%2520the%2520critical%2520role%2520of%2520training%2520data%2520in%250Ashaping%2520generalization%2520patterns%2520and%2520illuminate%2520how%2520competing%2520model%2520strategies%250Alead%2520to%2520inconsistent%2520generalization%2520outcomes%2520across%2520random%2520seeds.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/sunnytqin/concept_comp.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04619v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sometimes%20I%20am%20a%20Tree%3A%20Data%20Drives%20Unstable%20Hierarchical%20Generalization&entry.906535625=Tian%20Qin%20and%20Naomi%20Saphra%20and%20David%20Alvarez-Melis&entry.1292438233=%20%20Language%20models%20%28LMs%29%2C%20like%20other%20neural%20networks%2C%20often%20favor%20shortcut%0Aheuristics%20based%20on%20surface-level%20patterns.%20Although%20LMs%20behave%20like%20n-gram%0Amodels%20early%20in%20training%2C%20they%20must%20eventually%20learn%20hierarchical%20syntactic%0Arepresentations%20to%20correctly%20apply%20grammatical%20rules%20out-of-distribution%20%28OOD%29.%0AIn%20this%20work%2C%20we%20use%20case%20studies%20of%20English%20grammar%20to%20explore%20how%20complex%2C%0Adiverse%20training%20data%20drives%20models%20to%20generalize%20OOD.%20We%20construct%20a%20framework%0Athat%20unifies%20our%20understanding%20of%20random%20variation%20with%20training%20dynamics%2C%20rule%0Aselection%20with%20memorization%2C%20and%20data%20diversity%20with%20complexity.%20We%20show%20that%0Athese%20factors%20are%20nuanced%2C%20and%20that%20intermediate%20levels%20of%20diversity%20and%0Acomplexity%20lead%20to%20inconsistent%20behavior%20across%20random%20seeds%20and%20to%20unstable%0Atraining%20dynamics.%20Our%20findings%20emphasize%20the%20critical%20role%20of%20training%20data%20in%0Ashaping%20generalization%20patterns%20and%20illuminate%20how%20competing%20model%20strategies%0Alead%20to%20inconsistent%20generalization%20outcomes%20across%20random%20seeds.%20Code%20is%0Aavailable%20at%20https%3A//github.com/sunnytqin/concept_comp.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04619v3&entry.124074799=Read"},
{"title": "Zero-Shot Artifact2Artifact: Self-incentive artifact removal for\n  photoacoustic imaging without any data", "author": "Shuang Li and Qian Chen and Chulhong Kim and Seongwook Choi and Yibing Wang and Yu Zhang and Changhui Li", "abstract": "  Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.\n", "link": "http://arxiv.org/abs/2412.14873v1", "date": "2024-12-19", "relevancy": 2.5117, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5139}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5024}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Artifact2Artifact%3A%20Self-incentive%20artifact%20removal%20for%0A%20%20photoacoustic%20imaging%20without%20any%20data&body=Title%3A%20Zero-Shot%20Artifact2Artifact%3A%20Self-incentive%20artifact%20removal%20for%0A%20%20photoacoustic%20imaging%20without%20any%20data%0AAuthor%3A%20Shuang%20Li%20and%20Qian%20Chen%20and%20Chulhong%20Kim%20and%20Seongwook%20Choi%20and%20Yibing%20Wang%20and%20Yu%20Zhang%20and%20Changhui%20Li%0AAbstract%3A%20%20%20Photoacoustic%20imaging%20%28PAI%29%20uniquely%20combines%20optical%20contrast%20with%20the%0Apenetration%20depth%20of%20ultrasound%2C%20making%20it%20critical%20for%20clinical%20applications.%0AHowever%2C%20the%20quality%20of%203D%20PAI%20is%20often%20degraded%20due%20to%20reconstruction%0Aartifacts%20caused%20by%20the%20sparse%20and%20angle-limited%20configuration%20of%20detector%0Aarrays.%20Existing%20iterative%20or%20deep%20learning-based%20methods%20are%20either%0Atime-consuming%20or%20require%20large%20training%20datasets%2C%20significantly%20limiting%20their%0Apractical%20application.%20Here%2C%20we%20propose%20Zero-Shot%20Artifact2Artifact%20%28ZS-A2A%29%2C%20a%0Azero-shot%20self-supervised%20artifact%20removal%20method%20based%20on%20a%20super-lightweight%0Anetwork%2C%20which%20leverages%20the%20fact%20that%20reconstruction%20artifacts%20are%20sensitive%0Ato%20irregularities%20caused%20by%20data%20loss.%20By%20introducing%20random%20perturbations%20to%0Athe%20acquired%20PA%20data%2C%20it%20spontaneously%20generates%20subset%20data%2C%20which%20in%20turn%0Astimulates%20the%20network%20to%20learn%20the%20artifact%20patterns%20in%20the%20reconstruction%0Aresults%2C%20thus%20enabling%20zero-shot%20artifact%20removal.%20This%20approach%20requires%0Aneither%20training%20data%20nor%20prior%20knowledge%20of%20the%20artifacts%2C%20and%20is%20capable%20of%0Aartifact%20removal%20for%203D%20PAI.%20For%20maximum%20amplitude%20projection%20%28MAP%29%20images%20or%0Aslice%20images%20in%203D%20PAI%20acquired%20with%20arbitrarily%20sparse%20or%20angle-limited%0Adetector%20arrays%2C%20ZS-A2A%20employs%20a%20self-incentive%20strategy%20to%20complete%20artifact%0Aremoval%20and%20improves%20the%20Contrast-to-Noise%20Ratio%20%28CNR%29.%20We%20validated%20ZS-A2A%20in%0Aboth%20simulation%20study%20and%20%24%20in%5C%20vivo%20%24%20animal%20experiments.%20Results%20demonstrate%0Athat%20ZS-A2A%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%0Azero-shot%20methods%2C%20and%20for%20the%20%24%20in%5C%20vivo%20%24%20rat%20liver%2C%20ZS-A2A%20improves%20CNR%20from%0A17.48%20to%2043.46%20in%20just%208%20seconds.%20The%20project%20for%20ZS-A2A%20will%20be%20available%20in%0Athe%20following%20GitHub%20repository%3A%20https%3A//github.com/JaegerCQ/ZS-A2A.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Artifact2Artifact%253A%2520Self-incentive%2520artifact%2520removal%2520for%250A%2520%2520photoacoustic%2520imaging%2520without%2520any%2520data%26entry.906535625%3DShuang%2520Li%2520and%2520Qian%2520Chen%2520and%2520Chulhong%2520Kim%2520and%2520Seongwook%2520Choi%2520and%2520Yibing%2520Wang%2520and%2520Yu%2520Zhang%2520and%2520Changhui%2520Li%26entry.1292438233%3D%2520%2520Photoacoustic%2520imaging%2520%2528PAI%2529%2520uniquely%2520combines%2520optical%2520contrast%2520with%2520the%250Apenetration%2520depth%2520of%2520ultrasound%252C%2520making%2520it%2520critical%2520for%2520clinical%2520applications.%250AHowever%252C%2520the%2520quality%2520of%25203D%2520PAI%2520is%2520often%2520degraded%2520due%2520to%2520reconstruction%250Aartifacts%2520caused%2520by%2520the%2520sparse%2520and%2520angle-limited%2520configuration%2520of%2520detector%250Aarrays.%2520Existing%2520iterative%2520or%2520deep%2520learning-based%2520methods%2520are%2520either%250Atime-consuming%2520or%2520require%2520large%2520training%2520datasets%252C%2520significantly%2520limiting%2520their%250Apractical%2520application.%2520Here%252C%2520we%2520propose%2520Zero-Shot%2520Artifact2Artifact%2520%2528ZS-A2A%2529%252C%2520a%250Azero-shot%2520self-supervised%2520artifact%2520removal%2520method%2520based%2520on%2520a%2520super-lightweight%250Anetwork%252C%2520which%2520leverages%2520the%2520fact%2520that%2520reconstruction%2520artifacts%2520are%2520sensitive%250Ato%2520irregularities%2520caused%2520by%2520data%2520loss.%2520By%2520introducing%2520random%2520perturbations%2520to%250Athe%2520acquired%2520PA%2520data%252C%2520it%2520spontaneously%2520generates%2520subset%2520data%252C%2520which%2520in%2520turn%250Astimulates%2520the%2520network%2520to%2520learn%2520the%2520artifact%2520patterns%2520in%2520the%2520reconstruction%250Aresults%252C%2520thus%2520enabling%2520zero-shot%2520artifact%2520removal.%2520This%2520approach%2520requires%250Aneither%2520training%2520data%2520nor%2520prior%2520knowledge%2520of%2520the%2520artifacts%252C%2520and%2520is%2520capable%2520of%250Aartifact%2520removal%2520for%25203D%2520PAI.%2520For%2520maximum%2520amplitude%2520projection%2520%2528MAP%2529%2520images%2520or%250Aslice%2520images%2520in%25203D%2520PAI%2520acquired%2520with%2520arbitrarily%2520sparse%2520or%2520angle-limited%250Adetector%2520arrays%252C%2520ZS-A2A%2520employs%2520a%2520self-incentive%2520strategy%2520to%2520complete%2520artifact%250Aremoval%2520and%2520improves%2520the%2520Contrast-to-Noise%2520Ratio%2520%2528CNR%2529.%2520We%2520validated%2520ZS-A2A%2520in%250Aboth%2520simulation%2520study%2520and%2520%2524%2520in%255C%2520vivo%2520%2524%2520animal%2520experiments.%2520Results%2520demonstrate%250Athat%2520ZS-A2A%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520compared%2520to%2520existing%250Azero-shot%2520methods%252C%2520and%2520for%2520the%2520%2524%2520in%255C%2520vivo%2520%2524%2520rat%2520liver%252C%2520ZS-A2A%2520improves%2520CNR%2520from%250A17.48%2520to%252043.46%2520in%2520just%25208%2520seconds.%2520The%2520project%2520for%2520ZS-A2A%2520will%2520be%2520available%2520in%250Athe%2520following%2520GitHub%2520repository%253A%2520https%253A//github.com/JaegerCQ/ZS-A2A.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Artifact2Artifact%3A%20Self-incentive%20artifact%20removal%20for%0A%20%20photoacoustic%20imaging%20without%20any%20data&entry.906535625=Shuang%20Li%20and%20Qian%20Chen%20and%20Chulhong%20Kim%20and%20Seongwook%20Choi%20and%20Yibing%20Wang%20and%20Yu%20Zhang%20and%20Changhui%20Li&entry.1292438233=%20%20Photoacoustic%20imaging%20%28PAI%29%20uniquely%20combines%20optical%20contrast%20with%20the%0Apenetration%20depth%20of%20ultrasound%2C%20making%20it%20critical%20for%20clinical%20applications.%0AHowever%2C%20the%20quality%20of%203D%20PAI%20is%20often%20degraded%20due%20to%20reconstruction%0Aartifacts%20caused%20by%20the%20sparse%20and%20angle-limited%20configuration%20of%20detector%0Aarrays.%20Existing%20iterative%20or%20deep%20learning-based%20methods%20are%20either%0Atime-consuming%20or%20require%20large%20training%20datasets%2C%20significantly%20limiting%20their%0Apractical%20application.%20Here%2C%20we%20propose%20Zero-Shot%20Artifact2Artifact%20%28ZS-A2A%29%2C%20a%0Azero-shot%20self-supervised%20artifact%20removal%20method%20based%20on%20a%20super-lightweight%0Anetwork%2C%20which%20leverages%20the%20fact%20that%20reconstruction%20artifacts%20are%20sensitive%0Ato%20irregularities%20caused%20by%20data%20loss.%20By%20introducing%20random%20perturbations%20to%0Athe%20acquired%20PA%20data%2C%20it%20spontaneously%20generates%20subset%20data%2C%20which%20in%20turn%0Astimulates%20the%20network%20to%20learn%20the%20artifact%20patterns%20in%20the%20reconstruction%0Aresults%2C%20thus%20enabling%20zero-shot%20artifact%20removal.%20This%20approach%20requires%0Aneither%20training%20data%20nor%20prior%20knowledge%20of%20the%20artifacts%2C%20and%20is%20capable%20of%0Aartifact%20removal%20for%203D%20PAI.%20For%20maximum%20amplitude%20projection%20%28MAP%29%20images%20or%0Aslice%20images%20in%203D%20PAI%20acquired%20with%20arbitrarily%20sparse%20or%20angle-limited%0Adetector%20arrays%2C%20ZS-A2A%20employs%20a%20self-incentive%20strategy%20to%20complete%20artifact%0Aremoval%20and%20improves%20the%20Contrast-to-Noise%20Ratio%20%28CNR%29.%20We%20validated%20ZS-A2A%20in%0Aboth%20simulation%20study%20and%20%24%20in%5C%20vivo%20%24%20animal%20experiments.%20Results%20demonstrate%0Athat%20ZS-A2A%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20compared%20to%20existing%0Azero-shot%20methods%2C%20and%20for%20the%20%24%20in%5C%20vivo%20%24%20rat%20liver%2C%20ZS-A2A%20improves%20CNR%20from%0A17.48%20to%2043.46%20in%20just%208%20seconds.%20The%20project%20for%20ZS-A2A%20will%20be%20available%20in%0Athe%20following%20GitHub%20repository%3A%20https%3A//github.com/JaegerCQ/ZS-A2A.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14873v1&entry.124074799=Read"},
{"title": "GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction", "author": "Zesong Yang and Ru Zhang and Jiale Shi and Zixiang Ai and Boming Zhao and Hujun Bao and Luwei Yang and Zhaopeng Cui", "abstract": "  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n", "link": "http://arxiv.org/abs/2412.14939v1", "date": "2024-12-19", "relevancy": 2.5002, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6233}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&body=Title%3A%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGURecon%253A%2520Learning%2520Detailed%25203D%2520Geometric%2520Uncertainties%2520for%2520Neural%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DZesong%2520Yang%2520and%2520Ru%2520Zhang%2520and%2520Jiale%2520Shi%2520and%2520Zixiang%2520Ai%2520and%2520Boming%2520Zhao%2520and%2520Hujun%2520Bao%2520and%2520Luwei%2520Yang%2520and%2520Zhaopeng%2520Cui%26entry.1292438233%3D%2520%2520Neural%2520surface%2520representation%2520has%2520demonstrated%2520remarkable%2520success%2520in%2520the%250Aareas%2520of%2520novel%2520view%2520synthesis%2520and%25203D%2520reconstruction.%2520However%252C%2520assessing%2520the%250Ageometric%2520quality%2520of%25203D%2520reconstructions%2520in%2520the%2520absence%2520of%2520ground%2520truth%2520mesh%250Aremains%2520a%2520significant%2520challenge%252C%2520due%2520to%2520its%2520rendering-based%2520optimization%250Aprocess%2520and%2520entangled%2520learning%2520of%2520appearance%2520and%2520geometry%2520with%2520photometric%250Alosses.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520i.e%252C%2520GURecon%252C%2520which%250Aestablishes%2520a%2520geometric%2520uncertainty%2520field%2520for%2520the%2520neural%2520surface%2520based%2520on%250Ageometric%2520consistency.%2520Different%2520from%2520existing%2520methods%2520that%2520rely%2520on%250Arendering-based%2520measurement%252C%2520GURecon%2520models%2520a%2520continuous%25203D%2520uncertainty%2520field%250Afor%2520the%2520reconstructed%2520surface%252C%2520and%2520is%2520learned%2520by%2520an%2520online%2520distillation%250Aapproach%2520without%2520introducing%2520real%2520geometric%2520information%2520for%2520supervision.%250AMoreover%252C%2520in%2520order%2520to%2520mitigate%2520the%2520interference%2520of%2520illumination%2520on%2520geometric%250Aconsistency%252C%2520a%2520decoupled%2520field%2520is%2520learned%2520and%2520exploited%2520to%2520finetune%2520the%250Auncertainty%2520field.%2520Experiments%2520on%2520various%2520datasets%2520demonstrate%2520the%2520superiority%250Aof%2520GURecon%2520in%2520modeling%25203D%2520geometric%2520uncertainty%252C%2520as%2520well%2520as%2520its%2520plug-and-play%250Aextension%2520to%2520various%2520neural%2520surface%2520representations%2520and%2520improvement%2520on%250Adownstream%2520tasks%2520such%2520as%2520incremental%2520reconstruction.%2520The%2520code%2520and%2520supplementary%250Amaterial%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//zju3dv.github.io/GURecon/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&entry.906535625=Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14939v1&entry.124074799=Read"},
{"title": "M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation", "author": "Zixuan Chen and Jiaxin Li and Liming Tan and Yejie Guo and Junxuan Liang and Cewu Lu and Yong-Lu Li", "abstract": "  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.\n", "link": "http://arxiv.org/abs/2412.13803v2", "date": "2024-12-19", "relevancy": 2.4909, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation&body=Title%3A%20M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation%0AAuthor%3A%20Zixuan%20Chen%20and%20Jiaxin%20Li%20and%20Liming%20Tan%20and%20Yejie%20Guo%20and%20Junxuan%20Liang%20and%20Cewu%20Lu%20and%20Yong-Lu%20Li%0AAbstract%3A%20%20%20Intelligent%20robots%20need%20to%20interact%20with%20diverse%20objects%20across%20various%0Aenvironments.%20The%20appearance%20and%20state%20of%20objects%20frequently%20undergo%20complex%0Atransformations%20depending%20on%20the%20object%20properties%2C%20e.g.%2C%20phase%20transitions.%0AHowever%2C%20in%20the%20vision%20community%2C%20segmenting%20dynamic%20objects%20with%20phase%0Atransitions%20is%20overlooked.%20In%20light%20of%20this%2C%20we%20introduce%20the%20concept%20of%20phase%0Ain%20segmentation%2C%20which%20categorizes%20real-world%20objects%20based%20on%20their%20visual%0Acharacteristics%20and%20potential%20morphological%20and%20appearance%20changes.%20Then%2C%20we%0Apresent%20a%20new%20benchmark%2C%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%0AObject%20Segmentation%20%28M%24%5E3%24-VOS%29%2C%20to%20verify%20the%20ability%20of%20models%20to%20understand%0Aobject%20phases%2C%20which%20consists%20of%20479%20high-resolution%20videos%20spanning%20over%2010%0Adistinct%20everyday%20scenarios.%20It%20provides%20dense%20instance%20mask%20annotations%20that%0Acapture%20both%20object%20phases%20and%20their%20transitions.%20We%20evaluate%20state-of-the-art%0Amethods%20on%20M%24%5E3%24-VOS%2C%20yielding%20several%20key%20insights.%20Notably%2C%20current%0Aappearancebased%20approaches%20show%20significant%20room%20for%20improvement%20when%20handling%0Aobjects%20with%20phase%20transitions.%20The%20inherent%20changes%20in%20disorder%20suggest%20that%0Athe%20predictive%20performance%20of%20the%20forward%20entropy-increasing%20process%20can%20be%0Aimproved%20through%20a%20reverse%20entropy-reducing%20process.%20These%20findings%20lead%20us%20to%0Apropose%20ReVOS%2C%20a%20new%20plug-andplay%20model%20that%20improves%20its%20performance%20by%0Areversal%20refinement.%20Our%20data%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//zixuan-chen.github.io/M-cubeVOS.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E3%2524-VOS%253A%2520Multi-Phase%252C%2520Multi-Transition%252C%2520and%2520Multi-Scenery%2520Video%2520Object%250A%2520%2520Segmentation%26entry.906535625%3DZixuan%2520Chen%2520and%2520Jiaxin%2520Li%2520and%2520Liming%2520Tan%2520and%2520Yejie%2520Guo%2520and%2520Junxuan%2520Liang%2520and%2520Cewu%2520Lu%2520and%2520Yong-Lu%2520Li%26entry.1292438233%3D%2520%2520Intelligent%2520robots%2520need%2520to%2520interact%2520with%2520diverse%2520objects%2520across%2520various%250Aenvironments.%2520The%2520appearance%2520and%2520state%2520of%2520objects%2520frequently%2520undergo%2520complex%250Atransformations%2520depending%2520on%2520the%2520object%2520properties%252C%2520e.g.%252C%2520phase%2520transitions.%250AHowever%252C%2520in%2520the%2520vision%2520community%252C%2520segmenting%2520dynamic%2520objects%2520with%2520phase%250Atransitions%2520is%2520overlooked.%2520In%2520light%2520of%2520this%252C%2520we%2520introduce%2520the%2520concept%2520of%2520phase%250Ain%2520segmentation%252C%2520which%2520categorizes%2520real-world%2520objects%2520based%2520on%2520their%2520visual%250Acharacteristics%2520and%2520potential%2520morphological%2520and%2520appearance%2520changes.%2520Then%252C%2520we%250Apresent%2520a%2520new%2520benchmark%252C%2520Multi-Phase%252C%2520Multi-Transition%252C%2520and%2520Multi-Scenery%2520Video%250AObject%2520Segmentation%2520%2528M%2524%255E3%2524-VOS%2529%252C%2520to%2520verify%2520the%2520ability%2520of%2520models%2520to%2520understand%250Aobject%2520phases%252C%2520which%2520consists%2520of%2520479%2520high-resolution%2520videos%2520spanning%2520over%252010%250Adistinct%2520everyday%2520scenarios.%2520It%2520provides%2520dense%2520instance%2520mask%2520annotations%2520that%250Acapture%2520both%2520object%2520phases%2520and%2520their%2520transitions.%2520We%2520evaluate%2520state-of-the-art%250Amethods%2520on%2520M%2524%255E3%2524-VOS%252C%2520yielding%2520several%2520key%2520insights.%2520Notably%252C%2520current%250Aappearancebased%2520approaches%2520show%2520significant%2520room%2520for%2520improvement%2520when%2520handling%250Aobjects%2520with%2520phase%2520transitions.%2520The%2520inherent%2520changes%2520in%2520disorder%2520suggest%2520that%250Athe%2520predictive%2520performance%2520of%2520the%2520forward%2520entropy-increasing%2520process%2520can%2520be%250Aimproved%2520through%2520a%2520reverse%2520entropy-reducing%2520process.%2520These%2520findings%2520lead%2520us%2520to%250Apropose%2520ReVOS%252C%2520a%2520new%2520plug-andplay%2520model%2520that%2520improves%2520its%2520performance%2520by%250Areversal%2520refinement.%2520Our%2520data%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//zixuan-chen.github.io/M-cubeVOS.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation&entry.906535625=Zixuan%20Chen%20and%20Jiaxin%20Li%20and%20Liming%20Tan%20and%20Yejie%20Guo%20and%20Junxuan%20Liang%20and%20Cewu%20Lu%20and%20Yong-Lu%20Li&entry.1292438233=%20%20Intelligent%20robots%20need%20to%20interact%20with%20diverse%20objects%20across%20various%0Aenvironments.%20The%20appearance%20and%20state%20of%20objects%20frequently%20undergo%20complex%0Atransformations%20depending%20on%20the%20object%20properties%2C%20e.g.%2C%20phase%20transitions.%0AHowever%2C%20in%20the%20vision%20community%2C%20segmenting%20dynamic%20objects%20with%20phase%0Atransitions%20is%20overlooked.%20In%20light%20of%20this%2C%20we%20introduce%20the%20concept%20of%20phase%0Ain%20segmentation%2C%20which%20categorizes%20real-world%20objects%20based%20on%20their%20visual%0Acharacteristics%20and%20potential%20morphological%20and%20appearance%20changes.%20Then%2C%20we%0Apresent%20a%20new%20benchmark%2C%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%0AObject%20Segmentation%20%28M%24%5E3%24-VOS%29%2C%20to%20verify%20the%20ability%20of%20models%20to%20understand%0Aobject%20phases%2C%20which%20consists%20of%20479%20high-resolution%20videos%20spanning%20over%2010%0Adistinct%20everyday%20scenarios.%20It%20provides%20dense%20instance%20mask%20annotations%20that%0Acapture%20both%20object%20phases%20and%20their%20transitions.%20We%20evaluate%20state-of-the-art%0Amethods%20on%20M%24%5E3%24-VOS%2C%20yielding%20several%20key%20insights.%20Notably%2C%20current%0Aappearancebased%20approaches%20show%20significant%20room%20for%20improvement%20when%20handling%0Aobjects%20with%20phase%20transitions.%20The%20inherent%20changes%20in%20disorder%20suggest%20that%0Athe%20predictive%20performance%20of%20the%20forward%20entropy-increasing%20process%20can%20be%0Aimproved%20through%20a%20reverse%20entropy-reducing%20process.%20These%20findings%20lead%20us%20to%0Apropose%20ReVOS%2C%20a%20new%20plug-andplay%20model%20that%20improves%20its%20performance%20by%0Areversal%20refinement.%20Our%20data%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//zixuan-chen.github.io/M-cubeVOS.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13803v2&entry.124074799=Read"},
{"title": "Beyond the Hype: A Comprehensive Review of Current Trends in Generative\n  AI Research, Teaching Practices, and Tools", "author": "James Prather and Juho Leinonen and Natalie Kiesler and Jamie Gorson Benario and Sam Lau and Stephen MacNeil and Narges Norouzi and Simone Opel and Vee Pettit and Leo Porter and Brent N. Reeves and Jaromir Savelka and David H. Smith IV and Sven Strickroth and Daniel Zingaro", "abstract": "  Generative AI (GenAI) is advancing rapidly, and the literature in computing\neducation is expanding almost as quickly. Initial responses to GenAI tools were\nmixed between panic and utopian optimism. Many were fast to point out the\nopportunities and challenges of GenAI. Researchers reported that these new\ntools are capable of solving most introductory programming tasks and are\ncausing disruptions throughout the curriculum. These tools can write and\nexplain code, enhance error messages, create resources for instructors, and\neven provide feedback and help for students like a traditional teaching\nassistant. In 2024, new research started to emerge on the effects of GenAI\nusage in the computing classroom. These new data involve the use of GenAI to\nsupport classroom instruction at scale and to teach students how to code with\nGenAI. In support of the former, a new class of tools is emerging that can\nprovide personalized feedback to students on their programming assignments or\nteach both programming and prompting skills at the same time. With the\nliterature expanding so rapidly, this report aims to summarize and explain what\nis happening on the ground in computing classrooms. We provide a systematic\nliterature review; a survey of educators and industry professionals; and\ninterviews with educators using GenAI in their courses, educators studying\nGenAI, and researchers who create GenAI tools to support computing education.\nThe triangulation of these methods and data sources expands the understanding\nof GenAI usage and perceptions at this critical moment for our community.\n", "link": "http://arxiv.org/abs/2412.14732v1", "date": "2024-12-19", "relevancy": 2.4856, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.543}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4894}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Hype%3A%20A%20Comprehensive%20Review%20of%20Current%20Trends%20in%20Generative%0A%20%20AI%20Research%2C%20Teaching%20Practices%2C%20and%20Tools&body=Title%3A%20Beyond%20the%20Hype%3A%20A%20Comprehensive%20Review%20of%20Current%20Trends%20in%20Generative%0A%20%20AI%20Research%2C%20Teaching%20Practices%2C%20and%20Tools%0AAuthor%3A%20James%20Prather%20and%20Juho%20Leinonen%20and%20Natalie%20Kiesler%20and%20Jamie%20Gorson%20Benario%20and%20Sam%20Lau%20and%20Stephen%20MacNeil%20and%20Narges%20Norouzi%20and%20Simone%20Opel%20and%20Vee%20Pettit%20and%20Leo%20Porter%20and%20Brent%20N.%20Reeves%20and%20Jaromir%20Savelka%20and%20David%20H.%20Smith%20IV%20and%20Sven%20Strickroth%20and%20Daniel%20Zingaro%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20is%20advancing%20rapidly%2C%20and%20the%20literature%20in%20computing%0Aeducation%20is%20expanding%20almost%20as%20quickly.%20Initial%20responses%20to%20GenAI%20tools%20were%0Amixed%20between%20panic%20and%20utopian%20optimism.%20Many%20were%20fast%20to%20point%20out%20the%0Aopportunities%20and%20challenges%20of%20GenAI.%20Researchers%20reported%20that%20these%20new%0Atools%20are%20capable%20of%20solving%20most%20introductory%20programming%20tasks%20and%20are%0Acausing%20disruptions%20throughout%20the%20curriculum.%20These%20tools%20can%20write%20and%0Aexplain%20code%2C%20enhance%20error%20messages%2C%20create%20resources%20for%20instructors%2C%20and%0Aeven%20provide%20feedback%20and%20help%20for%20students%20like%20a%20traditional%20teaching%0Aassistant.%20In%202024%2C%20new%20research%20started%20to%20emerge%20on%20the%20effects%20of%20GenAI%0Ausage%20in%20the%20computing%20classroom.%20These%20new%20data%20involve%20the%20use%20of%20GenAI%20to%0Asupport%20classroom%20instruction%20at%20scale%20and%20to%20teach%20students%20how%20to%20code%20with%0AGenAI.%20In%20support%20of%20the%20former%2C%20a%20new%20class%20of%20tools%20is%20emerging%20that%20can%0Aprovide%20personalized%20feedback%20to%20students%20on%20their%20programming%20assignments%20or%0Ateach%20both%20programming%20and%20prompting%20skills%20at%20the%20same%20time.%20With%20the%0Aliterature%20expanding%20so%20rapidly%2C%20this%20report%20aims%20to%20summarize%20and%20explain%20what%0Ais%20happening%20on%20the%20ground%20in%20computing%20classrooms.%20We%20provide%20a%20systematic%0Aliterature%20review%3B%20a%20survey%20of%20educators%20and%20industry%20professionals%3B%20and%0Ainterviews%20with%20educators%20using%20GenAI%20in%20their%20courses%2C%20educators%20studying%0AGenAI%2C%20and%20researchers%20who%20create%20GenAI%20tools%20to%20support%20computing%20education.%0AThe%20triangulation%20of%20these%20methods%20and%20data%20sources%20expands%20the%20understanding%0Aof%20GenAI%20usage%20and%20perceptions%20at%20this%20critical%20moment%20for%20our%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Hype%253A%2520A%2520Comprehensive%2520Review%2520of%2520Current%2520Trends%2520in%2520Generative%250A%2520%2520AI%2520Research%252C%2520Teaching%2520Practices%252C%2520and%2520Tools%26entry.906535625%3DJames%2520Prather%2520and%2520Juho%2520Leinonen%2520and%2520Natalie%2520Kiesler%2520and%2520Jamie%2520Gorson%2520Benario%2520and%2520Sam%2520Lau%2520and%2520Stephen%2520MacNeil%2520and%2520Narges%2520Norouzi%2520and%2520Simone%2520Opel%2520and%2520Vee%2520Pettit%2520and%2520Leo%2520Porter%2520and%2520Brent%2520N.%2520Reeves%2520and%2520Jaromir%2520Savelka%2520and%2520David%2520H.%2520Smith%2520IV%2520and%2520Sven%2520Strickroth%2520and%2520Daniel%2520Zingaro%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520is%2520advancing%2520rapidly%252C%2520and%2520the%2520literature%2520in%2520computing%250Aeducation%2520is%2520expanding%2520almost%2520as%2520quickly.%2520Initial%2520responses%2520to%2520GenAI%2520tools%2520were%250Amixed%2520between%2520panic%2520and%2520utopian%2520optimism.%2520Many%2520were%2520fast%2520to%2520point%2520out%2520the%250Aopportunities%2520and%2520challenges%2520of%2520GenAI.%2520Researchers%2520reported%2520that%2520these%2520new%250Atools%2520are%2520capable%2520of%2520solving%2520most%2520introductory%2520programming%2520tasks%2520and%2520are%250Acausing%2520disruptions%2520throughout%2520the%2520curriculum.%2520These%2520tools%2520can%2520write%2520and%250Aexplain%2520code%252C%2520enhance%2520error%2520messages%252C%2520create%2520resources%2520for%2520instructors%252C%2520and%250Aeven%2520provide%2520feedback%2520and%2520help%2520for%2520students%2520like%2520a%2520traditional%2520teaching%250Aassistant.%2520In%25202024%252C%2520new%2520research%2520started%2520to%2520emerge%2520on%2520the%2520effects%2520of%2520GenAI%250Ausage%2520in%2520the%2520computing%2520classroom.%2520These%2520new%2520data%2520involve%2520the%2520use%2520of%2520GenAI%2520to%250Asupport%2520classroom%2520instruction%2520at%2520scale%2520and%2520to%2520teach%2520students%2520how%2520to%2520code%2520with%250AGenAI.%2520In%2520support%2520of%2520the%2520former%252C%2520a%2520new%2520class%2520of%2520tools%2520is%2520emerging%2520that%2520can%250Aprovide%2520personalized%2520feedback%2520to%2520students%2520on%2520their%2520programming%2520assignments%2520or%250Ateach%2520both%2520programming%2520and%2520prompting%2520skills%2520at%2520the%2520same%2520time.%2520With%2520the%250Aliterature%2520expanding%2520so%2520rapidly%252C%2520this%2520report%2520aims%2520to%2520summarize%2520and%2520explain%2520what%250Ais%2520happening%2520on%2520the%2520ground%2520in%2520computing%2520classrooms.%2520We%2520provide%2520a%2520systematic%250Aliterature%2520review%253B%2520a%2520survey%2520of%2520educators%2520and%2520industry%2520professionals%253B%2520and%250Ainterviews%2520with%2520educators%2520using%2520GenAI%2520in%2520their%2520courses%252C%2520educators%2520studying%250AGenAI%252C%2520and%2520researchers%2520who%2520create%2520GenAI%2520tools%2520to%2520support%2520computing%2520education.%250AThe%2520triangulation%2520of%2520these%2520methods%2520and%2520data%2520sources%2520expands%2520the%2520understanding%250Aof%2520GenAI%2520usage%2520and%2520perceptions%2520at%2520this%2520critical%2520moment%2520for%2520our%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Hype%3A%20A%20Comprehensive%20Review%20of%20Current%20Trends%20in%20Generative%0A%20%20AI%20Research%2C%20Teaching%20Practices%2C%20and%20Tools&entry.906535625=James%20Prather%20and%20Juho%20Leinonen%20and%20Natalie%20Kiesler%20and%20Jamie%20Gorson%20Benario%20and%20Sam%20Lau%20and%20Stephen%20MacNeil%20and%20Narges%20Norouzi%20and%20Simone%20Opel%20and%20Vee%20Pettit%20and%20Leo%20Porter%20and%20Brent%20N.%20Reeves%20and%20Jaromir%20Savelka%20and%20David%20H.%20Smith%20IV%20and%20Sven%20Strickroth%20and%20Daniel%20Zingaro&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20is%20advancing%20rapidly%2C%20and%20the%20literature%20in%20computing%0Aeducation%20is%20expanding%20almost%20as%20quickly.%20Initial%20responses%20to%20GenAI%20tools%20were%0Amixed%20between%20panic%20and%20utopian%20optimism.%20Many%20were%20fast%20to%20point%20out%20the%0Aopportunities%20and%20challenges%20of%20GenAI.%20Researchers%20reported%20that%20these%20new%0Atools%20are%20capable%20of%20solving%20most%20introductory%20programming%20tasks%20and%20are%0Acausing%20disruptions%20throughout%20the%20curriculum.%20These%20tools%20can%20write%20and%0Aexplain%20code%2C%20enhance%20error%20messages%2C%20create%20resources%20for%20instructors%2C%20and%0Aeven%20provide%20feedback%20and%20help%20for%20students%20like%20a%20traditional%20teaching%0Aassistant.%20In%202024%2C%20new%20research%20started%20to%20emerge%20on%20the%20effects%20of%20GenAI%0Ausage%20in%20the%20computing%20classroom.%20These%20new%20data%20involve%20the%20use%20of%20GenAI%20to%0Asupport%20classroom%20instruction%20at%20scale%20and%20to%20teach%20students%20how%20to%20code%20with%0AGenAI.%20In%20support%20of%20the%20former%2C%20a%20new%20class%20of%20tools%20is%20emerging%20that%20can%0Aprovide%20personalized%20feedback%20to%20students%20on%20their%20programming%20assignments%20or%0Ateach%20both%20programming%20and%20prompting%20skills%20at%20the%20same%20time.%20With%20the%0Aliterature%20expanding%20so%20rapidly%2C%20this%20report%20aims%20to%20summarize%20and%20explain%20what%0Ais%20happening%20on%20the%20ground%20in%20computing%20classrooms.%20We%20provide%20a%20systematic%0Aliterature%20review%3B%20a%20survey%20of%20educators%20and%20industry%20professionals%3B%20and%0Ainterviews%20with%20educators%20using%20GenAI%20in%20their%20courses%2C%20educators%20studying%0AGenAI%2C%20and%20researchers%20who%20create%20GenAI%20tools%20to%20support%20computing%20education.%0AThe%20triangulation%20of%20these%20methods%20and%20data%20sources%20expands%20the%20understanding%0Aof%20GenAI%20usage%20and%20perceptions%20at%20this%20critical%20moment%20for%20our%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14732v1&entry.124074799=Read"},
{"title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\n  for High-quality 3D Asset Creation", "author": "Wang Zhao and Yan-Pei Cao and Jiale Xu and Yuejiang Dong and Ying Shan", "abstract": "  Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.\n", "link": "http://arxiv.org/abs/2412.15200v1", "date": "2024-12-19", "relevancy": 2.4767, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6235}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6183}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DI-PCG%3A%20Diffusion-based%20Efficient%20Inverse%20Procedural%20Content%20Generation%0A%20%20for%20High-quality%203D%20Asset%20Creation&body=Title%3A%20DI-PCG%3A%20Diffusion-based%20Efficient%20Inverse%20Procedural%20Content%20Generation%0A%20%20for%20High-quality%203D%20Asset%20Creation%0AAuthor%3A%20Wang%20Zhao%20and%20Yan-Pei%20Cao%20and%20Jiale%20Xu%20and%20Yuejiang%20Dong%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Procedural%20Content%20Generation%20%28PCG%29%20is%20powerful%20in%20creating%20high-quality%203D%0Acontents%2C%20yet%20controlling%20it%20to%20produce%20desired%20shapes%20is%20difficult%20and%20often%0Arequires%20extensive%20parameter%20tuning.%20Inverse%20Procedural%20Content%20Generation%20aims%0Ato%20automatically%20find%20the%20best%20parameters%20under%20the%20input%20condition.%20However%2C%0Aexisting%20sampling-based%20and%20neural%20network-based%20methods%20still%20suffer%20from%0Anumerous%20sample%20iterations%20or%20limited%20controllability.%20In%20this%20work%2C%20we%20present%0ADI-PCG%2C%20a%20novel%20and%20efficient%20method%20for%20Inverse%20PCG%20from%20general%20image%0Aconditions.%20At%20its%20core%20is%20a%20lightweight%20diffusion%20transformer%20model%2C%20where%20PCG%0Aparameters%20are%20directly%20treated%20as%20the%20denoising%20target%20and%20the%20observed%20images%0Aas%20conditions%20to%20control%20parameter%20generation.%20DI-PCG%20is%20efficient%20and%0Aeffective.%20With%20only%207.6M%20network%20parameters%20and%2030%20GPU%20hours%20to%20train%2C%20it%0Ademonstrates%20superior%20performance%20in%20recovering%20parameters%20accurately%2C%20and%0Ageneralizing%20well%20to%20in-the-wild%20images.%20Quantitative%20and%20qualitative%0Aexperiment%20results%20validate%20the%20effectiveness%20of%20DI-PCG%20in%20inverse%20PCG%20and%0Aimage-to-3D%20generation%20tasks.%20DI-PCG%20offers%20a%20promising%20approach%20for%20efficient%0Ainverse%20PCG%20and%20represents%20a%20valuable%20exploration%20step%20towards%20a%203D%20generation%0Apath%20that%20models%20how%20to%20construct%20a%203D%20asset%20using%20parametric%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDI-PCG%253A%2520Diffusion-based%2520Efficient%2520Inverse%2520Procedural%2520Content%2520Generation%250A%2520%2520for%2520High-quality%25203D%2520Asset%2520Creation%26entry.906535625%3DWang%2520Zhao%2520and%2520Yan-Pei%2520Cao%2520and%2520Jiale%2520Xu%2520and%2520Yuejiang%2520Dong%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Procedural%2520Content%2520Generation%2520%2528PCG%2529%2520is%2520powerful%2520in%2520creating%2520high-quality%25203D%250Acontents%252C%2520yet%2520controlling%2520it%2520to%2520produce%2520desired%2520shapes%2520is%2520difficult%2520and%2520often%250Arequires%2520extensive%2520parameter%2520tuning.%2520Inverse%2520Procedural%2520Content%2520Generation%2520aims%250Ato%2520automatically%2520find%2520the%2520best%2520parameters%2520under%2520the%2520input%2520condition.%2520However%252C%250Aexisting%2520sampling-based%2520and%2520neural%2520network-based%2520methods%2520still%2520suffer%2520from%250Anumerous%2520sample%2520iterations%2520or%2520limited%2520controllability.%2520In%2520this%2520work%252C%2520we%2520present%250ADI-PCG%252C%2520a%2520novel%2520and%2520efficient%2520method%2520for%2520Inverse%2520PCG%2520from%2520general%2520image%250Aconditions.%2520At%2520its%2520core%2520is%2520a%2520lightweight%2520diffusion%2520transformer%2520model%252C%2520where%2520PCG%250Aparameters%2520are%2520directly%2520treated%2520as%2520the%2520denoising%2520target%2520and%2520the%2520observed%2520images%250Aas%2520conditions%2520to%2520control%2520parameter%2520generation.%2520DI-PCG%2520is%2520efficient%2520and%250Aeffective.%2520With%2520only%25207.6M%2520network%2520parameters%2520and%252030%2520GPU%2520hours%2520to%2520train%252C%2520it%250Ademonstrates%2520superior%2520performance%2520in%2520recovering%2520parameters%2520accurately%252C%2520and%250Ageneralizing%2520well%2520to%2520in-the-wild%2520images.%2520Quantitative%2520and%2520qualitative%250Aexperiment%2520results%2520validate%2520the%2520effectiveness%2520of%2520DI-PCG%2520in%2520inverse%2520PCG%2520and%250Aimage-to-3D%2520generation%2520tasks.%2520DI-PCG%2520offers%2520a%2520promising%2520approach%2520for%2520efficient%250Ainverse%2520PCG%2520and%2520represents%2520a%2520valuable%2520exploration%2520step%2520towards%2520a%25203D%2520generation%250Apath%2520that%2520models%2520how%2520to%2520construct%2520a%25203D%2520asset%2520using%2520parametric%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DI-PCG%3A%20Diffusion-based%20Efficient%20Inverse%20Procedural%20Content%20Generation%0A%20%20for%20High-quality%203D%20Asset%20Creation&entry.906535625=Wang%20Zhao%20and%20Yan-Pei%20Cao%20and%20Jiale%20Xu%20and%20Yuejiang%20Dong%20and%20Ying%20Shan&entry.1292438233=%20%20Procedural%20Content%20Generation%20%28PCG%29%20is%20powerful%20in%20creating%20high-quality%203D%0Acontents%2C%20yet%20controlling%20it%20to%20produce%20desired%20shapes%20is%20difficult%20and%20often%0Arequires%20extensive%20parameter%20tuning.%20Inverse%20Procedural%20Content%20Generation%20aims%0Ato%20automatically%20find%20the%20best%20parameters%20under%20the%20input%20condition.%20However%2C%0Aexisting%20sampling-based%20and%20neural%20network-based%20methods%20still%20suffer%20from%0Anumerous%20sample%20iterations%20or%20limited%20controllability.%20In%20this%20work%2C%20we%20present%0ADI-PCG%2C%20a%20novel%20and%20efficient%20method%20for%20Inverse%20PCG%20from%20general%20image%0Aconditions.%20At%20its%20core%20is%20a%20lightweight%20diffusion%20transformer%20model%2C%20where%20PCG%0Aparameters%20are%20directly%20treated%20as%20the%20denoising%20target%20and%20the%20observed%20images%0Aas%20conditions%20to%20control%20parameter%20generation.%20DI-PCG%20is%20efficient%20and%0Aeffective.%20With%20only%207.6M%20network%20parameters%20and%2030%20GPU%20hours%20to%20train%2C%20it%0Ademonstrates%20superior%20performance%20in%20recovering%20parameters%20accurately%2C%20and%0Ageneralizing%20well%20to%20in-the-wild%20images.%20Quantitative%20and%20qualitative%0Aexperiment%20results%20validate%20the%20effectiveness%20of%20DI-PCG%20in%20inverse%20PCG%20and%0Aimage-to-3D%20generation%20tasks.%20DI-PCG%20offers%20a%20promising%20approach%20for%20efficient%0Ainverse%20PCG%20and%20represents%20a%20valuable%20exploration%20step%20towards%20a%203D%20generation%0Apath%20that%20models%20how%20to%20construct%20a%203D%20asset%20using%20parametric%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15200v1&entry.124074799=Read"},
{"title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling", "author": "Zihan Liu and Yang Chen and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping", "abstract": "  In this paper, we introduce AceMath, a suite of frontier math models that\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop the instruction-tuned math models, we propose a\nsupervised fine-tuning (SFT) process that first achieves competitive\nperformance across general domains, followed by targeted fine-tuning for the\nmath domain using a carefully curated set of prompts and synthetically\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\nmath-specialized reward model, we first construct AceMath-RewardBench, a\ncomprehensive and robust benchmark for evaluating math reward models across\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-art reward models. Furthermore, when\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\naverage rm@8 score across the math reasoning benchmarks. We will release model\nweights, training data, and evaluation benchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemath\n", "link": "http://arxiv.org/abs/2412.15084v1", "date": "2024-12-19", "relevancy": 2.4622, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AceMath%3A%20Advancing%20Frontier%20Math%20Reasoning%20with%20Post-Training%20and%20Reward%0A%20%20Modeling&body=Title%3A%20AceMath%3A%20Advancing%20Frontier%20Math%20Reasoning%20with%20Post-Training%20and%20Reward%0A%20%20Modeling%0AAuthor%3A%20Zihan%20Liu%20and%20Yang%20Chen%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20AceMath%2C%20a%20suite%20of%20frontier%20math%20models%20that%0Aexcel%20in%20solving%20complex%20math%20problems%2C%20along%20with%20highly%20effective%20reward%0Amodels%20capable%20of%20evaluating%20generated%20solutions%20and%20reliably%20identifying%20the%0Acorrect%20ones.%20To%20develop%20the%20instruction-tuned%20math%20models%2C%20we%20propose%20a%0Asupervised%20fine-tuning%20%28SFT%29%20process%20that%20first%20achieves%20competitive%0Aperformance%20across%20general%20domains%2C%20followed%20by%20targeted%20fine-tuning%20for%20the%0Amath%20domain%20using%20a%20carefully%20curated%20set%20of%20prompts%20and%20synthetically%0Agenerated%20responses.%20The%20resulting%20model%2C%20AceMath-72B-Instruct%20greatly%0Aoutperforms%20Qwen2.5-Math-72B-Instruct%2C%20GPT-4o%20and%20Claude-3.5%20Sonnet.%20To%20develop%0Amath-specialized%20reward%20model%2C%20we%20first%20construct%20AceMath-RewardBench%2C%20a%0Acomprehensive%20and%20robust%20benchmark%20for%20evaluating%20math%20reward%20models%20across%0Adiverse%20problems%20and%20difficulty%20levels.%20After%20that%2C%20we%20present%20a%20systematic%0Aapproach%20to%20build%20our%20math%20reward%20models.%20The%20resulting%20model%2C%20AceMath-72B-RM%2C%0Aconsistently%20outperforms%20state-of-the-art%20reward%20models.%20Furthermore%2C%20when%0Acombining%20AceMath-72B-Instruct%20with%20AceMath-72B-RM%2C%20we%20achieve%20the%20highest%0Aaverage%20rm%408%20score%20across%20the%20math%20reasoning%20benchmarks.%20We%20will%20release%20model%0Aweights%2C%20training%20data%2C%20and%20evaluation%20benchmarks%20at%3A%0Ahttps%3A//research.nvidia.com/labs/adlr/acemath%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAceMath%253A%2520Advancing%2520Frontier%2520Math%2520Reasoning%2520with%2520Post-Training%2520and%2520Reward%250A%2520%2520Modeling%26entry.906535625%3DZihan%2520Liu%2520and%2520Yang%2520Chen%2520and%2520Mohammad%2520Shoeybi%2520and%2520Bryan%2520Catanzaro%2520and%2520Wei%2520Ping%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520AceMath%252C%2520a%2520suite%2520of%2520frontier%2520math%2520models%2520that%250Aexcel%2520in%2520solving%2520complex%2520math%2520problems%252C%2520along%2520with%2520highly%2520effective%2520reward%250Amodels%2520capable%2520of%2520evaluating%2520generated%2520solutions%2520and%2520reliably%2520identifying%2520the%250Acorrect%2520ones.%2520To%2520develop%2520the%2520instruction-tuned%2520math%2520models%252C%2520we%2520propose%2520a%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520process%2520that%2520first%2520achieves%2520competitive%250Aperformance%2520across%2520general%2520domains%252C%2520followed%2520by%2520targeted%2520fine-tuning%2520for%2520the%250Amath%2520domain%2520using%2520a%2520carefully%2520curated%2520set%2520of%2520prompts%2520and%2520synthetically%250Agenerated%2520responses.%2520The%2520resulting%2520model%252C%2520AceMath-72B-Instruct%2520greatly%250Aoutperforms%2520Qwen2.5-Math-72B-Instruct%252C%2520GPT-4o%2520and%2520Claude-3.5%2520Sonnet.%2520To%2520develop%250Amath-specialized%2520reward%2520model%252C%2520we%2520first%2520construct%2520AceMath-RewardBench%252C%2520a%250Acomprehensive%2520and%2520robust%2520benchmark%2520for%2520evaluating%2520math%2520reward%2520models%2520across%250Adiverse%2520problems%2520and%2520difficulty%2520levels.%2520After%2520that%252C%2520we%2520present%2520a%2520systematic%250Aapproach%2520to%2520build%2520our%2520math%2520reward%2520models.%2520The%2520resulting%2520model%252C%2520AceMath-72B-RM%252C%250Aconsistently%2520outperforms%2520state-of-the-art%2520reward%2520models.%2520Furthermore%252C%2520when%250Acombining%2520AceMath-72B-Instruct%2520with%2520AceMath-72B-RM%252C%2520we%2520achieve%2520the%2520highest%250Aaverage%2520rm%25408%2520score%2520across%2520the%2520math%2520reasoning%2520benchmarks.%2520We%2520will%2520release%2520model%250Aweights%252C%2520training%2520data%252C%2520and%2520evaluation%2520benchmarks%2520at%253A%250Ahttps%253A//research.nvidia.com/labs/adlr/acemath%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AceMath%3A%20Advancing%20Frontier%20Math%20Reasoning%20with%20Post-Training%20and%20Reward%0A%20%20Modeling&entry.906535625=Zihan%20Liu%20and%20Yang%20Chen%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20AceMath%2C%20a%20suite%20of%20frontier%20math%20models%20that%0Aexcel%20in%20solving%20complex%20math%20problems%2C%20along%20with%20highly%20effective%20reward%0Amodels%20capable%20of%20evaluating%20generated%20solutions%20and%20reliably%20identifying%20the%0Acorrect%20ones.%20To%20develop%20the%20instruction-tuned%20math%20models%2C%20we%20propose%20a%0Asupervised%20fine-tuning%20%28SFT%29%20process%20that%20first%20achieves%20competitive%0Aperformance%20across%20general%20domains%2C%20followed%20by%20targeted%20fine-tuning%20for%20the%0Amath%20domain%20using%20a%20carefully%20curated%20set%20of%20prompts%20and%20synthetically%0Agenerated%20responses.%20The%20resulting%20model%2C%20AceMath-72B-Instruct%20greatly%0Aoutperforms%20Qwen2.5-Math-72B-Instruct%2C%20GPT-4o%20and%20Claude-3.5%20Sonnet.%20To%20develop%0Amath-specialized%20reward%20model%2C%20we%20first%20construct%20AceMath-RewardBench%2C%20a%0Acomprehensive%20and%20robust%20benchmark%20for%20evaluating%20math%20reward%20models%20across%0Adiverse%20problems%20and%20difficulty%20levels.%20After%20that%2C%20we%20present%20a%20systematic%0Aapproach%20to%20build%20our%20math%20reward%20models.%20The%20resulting%20model%2C%20AceMath-72B-RM%2C%0Aconsistently%20outperforms%20state-of-the-art%20reward%20models.%20Furthermore%2C%20when%0Acombining%20AceMath-72B-Instruct%20with%20AceMath-72B-RM%2C%20we%20achieve%20the%20highest%0Aaverage%20rm%408%20score%20across%20the%20math%20reasoning%20benchmarks.%20We%20will%20release%20model%0Aweights%2C%20training%20data%2C%20and%20evaluation%20benchmarks%20at%3A%0Ahttps%3A//research.nvidia.com/labs/adlr/acemath%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15084v1&entry.124074799=Read"},
{"title": "Boosting GNN Performance via Training Sample Selection Based on\n  Adversarial Robustness Evaluation", "author": "Yongyu Wang", "abstract": "  Graph Neural Networks (GNNs) have established themselves as one of the most\npowerful neural network architectures, excelling in leveraging graph topology\nand node features for various tasks. However, GNNs are inherently vulnerable to\nnoise in their inputs. Such noise can significantly degrade their performance.\nTo address this challenge, we propose a novel approach that employs adversarial\nrobustness evaluation techniques to identify nodes in the graph that are most\nsusceptible to noise. By selecting and constructing a training set composed of\nthese particularly noise-prone nodes, we then use them to train a Graph\nConvolutional Network (GCN). Our experimental results demonstrate that this\nstrategy leads to substantial improvements in the GCN's performance.\n", "link": "http://arxiv.org/abs/2412.14738v1", "date": "2024-12-19", "relevancy": 2.4528, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5508}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4858}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20GNN%20Performance%20via%20Training%20Sample%20Selection%20Based%20on%0A%20%20Adversarial%20Robustness%20Evaluation&body=Title%3A%20Boosting%20GNN%20Performance%20via%20Training%20Sample%20Selection%20Based%20on%0A%20%20Adversarial%20Robustness%20Evaluation%0AAuthor%3A%20Yongyu%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20established%20themselves%20as%20one%20of%20the%20most%0Apowerful%20neural%20network%20architectures%2C%20excelling%20in%20leveraging%20graph%20topology%0Aand%20node%20features%20for%20various%20tasks.%20However%2C%20GNNs%20are%20inherently%20vulnerable%20to%0Anoise%20in%20their%20inputs.%20Such%20noise%20can%20significantly%20degrade%20their%20performance.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20that%20employs%20adversarial%0Arobustness%20evaluation%20techniques%20to%20identify%20nodes%20in%20the%20graph%20that%20are%20most%0Asusceptible%20to%20noise.%20By%20selecting%20and%20constructing%20a%20training%20set%20composed%20of%0Athese%20particularly%20noise-prone%20nodes%2C%20we%20then%20use%20them%20to%20train%20a%20Graph%0AConvolutional%20Network%20%28GCN%29.%20Our%20experimental%20results%20demonstrate%20that%20this%0Astrategy%20leads%20to%20substantial%20improvements%20in%20the%20GCN%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520GNN%2520Performance%2520via%2520Training%2520Sample%2520Selection%2520Based%2520on%250A%2520%2520Adversarial%2520Robustness%2520Evaluation%26entry.906535625%3DYongyu%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520established%2520themselves%2520as%2520one%2520of%2520the%2520most%250Apowerful%2520neural%2520network%2520architectures%252C%2520excelling%2520in%2520leveraging%2520graph%2520topology%250Aand%2520node%2520features%2520for%2520various%2520tasks.%2520However%252C%2520GNNs%2520are%2520inherently%2520vulnerable%2520to%250Anoise%2520in%2520their%2520inputs.%2520Such%2520noise%2520can%2520significantly%2520degrade%2520their%2520performance.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520employs%2520adversarial%250Arobustness%2520evaluation%2520techniques%2520to%2520identify%2520nodes%2520in%2520the%2520graph%2520that%2520are%2520most%250Asusceptible%2520to%2520noise.%2520By%2520selecting%2520and%2520constructing%2520a%2520training%2520set%2520composed%2520of%250Athese%2520particularly%2520noise-prone%2520nodes%252C%2520we%2520then%2520use%2520them%2520to%2520train%2520a%2520Graph%250AConvolutional%2520Network%2520%2528GCN%2529.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520this%250Astrategy%2520leads%2520to%2520substantial%2520improvements%2520in%2520the%2520GCN%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20GNN%20Performance%20via%20Training%20Sample%20Selection%20Based%20on%0A%20%20Adversarial%20Robustness%20Evaluation&entry.906535625=Yongyu%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20established%20themselves%20as%20one%20of%20the%20most%0Apowerful%20neural%20network%20architectures%2C%20excelling%20in%20leveraging%20graph%20topology%0Aand%20node%20features%20for%20various%20tasks.%20However%2C%20GNNs%20are%20inherently%20vulnerable%20to%0Anoise%20in%20their%20inputs.%20Such%20noise%20can%20significantly%20degrade%20their%20performance.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20that%20employs%20adversarial%0Arobustness%20evaluation%20techniques%20to%20identify%20nodes%20in%20the%20graph%20that%20are%20most%0Asusceptible%20to%20noise.%20By%20selecting%20and%20constructing%20a%20training%20set%20composed%20of%0Athese%20particularly%20noise-prone%20nodes%2C%20we%20then%20use%20them%20to%20train%20a%20Graph%0AConvolutional%20Network%20%28GCN%29.%20Our%20experimental%20results%20demonstrate%20that%20this%0Astrategy%20leads%20to%20substantial%20improvements%20in%20the%20GCN%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14738v1&entry.124074799=Read"},
{"title": "Autonomous Navigation in Dynamic Human Environments with an Embedded 2D\n  LiDAR-based Person Tracker", "author": "Davide Plozza and Steven Marty and Cyril Scherrer and Simon Schwartz and Stefan Zihlmann and Michele Magno", "abstract": "  In the rapidly evolving landscape of autonomous mobile robots, the emphasis\non seamless human-robot interactions has shifted towards autonomous\ndecision-making. This paper delves into the intricate challenges associated\nwith robotic autonomy, focusing on navigation in dynamic environments shared\nwith humans. It introduces an embedded real-time tracking pipeline, integrated\ninto a navigation planning framework for effective person tracking and\navoidance, adapting a state-of-the-art 2D LiDAR-based human detection network\nand an efficient multi-object tracker. By addressing the key components of\ndetection, tracking, and planning separately, the proposed approach highlights\nthe modularity and transferability of each component to other applications. Our\ntracking approach is validated on a quadruped robot equipped with 270{\\deg}\n2D-LiDAR against motion capture system data, with the preferred configuration\nachieving an average MOTA of 85.45% in three newly recorded datasets, while\nreliably running in real-time at 20 Hz on the NVIDIA Jetson Xavier NX embedded\nGPU-accelerated platform. Furthermore, the integrated tracking and avoidance\nsystem is evaluated in real-world navigation experiments, demonstrating how\naccurate person tracking benefits the planner in optimizing the generated\ntrajectories, enhancing its collision avoidance capabilities. This paper\ncontributes to safer human-robot cohabitation, blending recent advances in\nhuman detection with responsive planning to navigate shared spaces effectively\nand securely.\n", "link": "http://arxiv.org/abs/2412.15000v1", "date": "2024-12-19", "relevancy": 2.4316, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6202}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6011}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Navigation%20in%20Dynamic%20Human%20Environments%20with%20an%20Embedded%202D%0A%20%20LiDAR-based%20Person%20Tracker&body=Title%3A%20Autonomous%20Navigation%20in%20Dynamic%20Human%20Environments%20with%20an%20Embedded%202D%0A%20%20LiDAR-based%20Person%20Tracker%0AAuthor%3A%20Davide%20Plozza%20and%20Steven%20Marty%20and%20Cyril%20Scherrer%20and%20Simon%20Schwartz%20and%20Stefan%20Zihlmann%20and%20Michele%20Magno%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20mobile%20robots%2C%20the%20emphasis%0Aon%20seamless%20human-robot%20interactions%20has%20shifted%20towards%20autonomous%0Adecision-making.%20This%20paper%20delves%20into%20the%20intricate%20challenges%20associated%0Awith%20robotic%20autonomy%2C%20focusing%20on%20navigation%20in%20dynamic%20environments%20shared%0Awith%20humans.%20It%20introduces%20an%20embedded%20real-time%20tracking%20pipeline%2C%20integrated%0Ainto%20a%20navigation%20planning%20framework%20for%20effective%20person%20tracking%20and%0Aavoidance%2C%20adapting%20a%20state-of-the-art%202D%20LiDAR-based%20human%20detection%20network%0Aand%20an%20efficient%20multi-object%20tracker.%20By%20addressing%20the%20key%20components%20of%0Adetection%2C%20tracking%2C%20and%20planning%20separately%2C%20the%20proposed%20approach%20highlights%0Athe%20modularity%20and%20transferability%20of%20each%20component%20to%20other%20applications.%20Our%0Atracking%20approach%20is%20validated%20on%20a%20quadruped%20robot%20equipped%20with%20270%7B%5Cdeg%7D%0A2D-LiDAR%20against%20motion%20capture%20system%20data%2C%20with%20the%20preferred%20configuration%0Aachieving%20an%20average%20MOTA%20of%2085.45%25%20in%20three%20newly%20recorded%20datasets%2C%20while%0Areliably%20running%20in%20real-time%20at%2020%20Hz%20on%20the%20NVIDIA%20Jetson%20Xavier%20NX%20embedded%0AGPU-accelerated%20platform.%20Furthermore%2C%20the%20integrated%20tracking%20and%20avoidance%0Asystem%20is%20evaluated%20in%20real-world%20navigation%20experiments%2C%20demonstrating%20how%0Aaccurate%20person%20tracking%20benefits%20the%20planner%20in%20optimizing%20the%20generated%0Atrajectories%2C%20enhancing%20its%20collision%20avoidance%20capabilities.%20This%20paper%0Acontributes%20to%20safer%20human-robot%20cohabitation%2C%20blending%20recent%20advances%20in%0Ahuman%20detection%20with%20responsive%20planning%20to%20navigate%20shared%20spaces%20effectively%0Aand%20securely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Navigation%2520in%2520Dynamic%2520Human%2520Environments%2520with%2520an%2520Embedded%25202D%250A%2520%2520LiDAR-based%2520Person%2520Tracker%26entry.906535625%3DDavide%2520Plozza%2520and%2520Steven%2520Marty%2520and%2520Cyril%2520Scherrer%2520and%2520Simon%2520Schwartz%2520and%2520Stefan%2520Zihlmann%2520and%2520Michele%2520Magno%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520landscape%2520of%2520autonomous%2520mobile%2520robots%252C%2520the%2520emphasis%250Aon%2520seamless%2520human-robot%2520interactions%2520has%2520shifted%2520towards%2520autonomous%250Adecision-making.%2520This%2520paper%2520delves%2520into%2520the%2520intricate%2520challenges%2520associated%250Awith%2520robotic%2520autonomy%252C%2520focusing%2520on%2520navigation%2520in%2520dynamic%2520environments%2520shared%250Awith%2520humans.%2520It%2520introduces%2520an%2520embedded%2520real-time%2520tracking%2520pipeline%252C%2520integrated%250Ainto%2520a%2520navigation%2520planning%2520framework%2520for%2520effective%2520person%2520tracking%2520and%250Aavoidance%252C%2520adapting%2520a%2520state-of-the-art%25202D%2520LiDAR-based%2520human%2520detection%2520network%250Aand%2520an%2520efficient%2520multi-object%2520tracker.%2520By%2520addressing%2520the%2520key%2520components%2520of%250Adetection%252C%2520tracking%252C%2520and%2520planning%2520separately%252C%2520the%2520proposed%2520approach%2520highlights%250Athe%2520modularity%2520and%2520transferability%2520of%2520each%2520component%2520to%2520other%2520applications.%2520Our%250Atracking%2520approach%2520is%2520validated%2520on%2520a%2520quadruped%2520robot%2520equipped%2520with%2520270%257B%255Cdeg%257D%250A2D-LiDAR%2520against%2520motion%2520capture%2520system%2520data%252C%2520with%2520the%2520preferred%2520configuration%250Aachieving%2520an%2520average%2520MOTA%2520of%252085.45%2525%2520in%2520three%2520newly%2520recorded%2520datasets%252C%2520while%250Areliably%2520running%2520in%2520real-time%2520at%252020%2520Hz%2520on%2520the%2520NVIDIA%2520Jetson%2520Xavier%2520NX%2520embedded%250AGPU-accelerated%2520platform.%2520Furthermore%252C%2520the%2520integrated%2520tracking%2520and%2520avoidance%250Asystem%2520is%2520evaluated%2520in%2520real-world%2520navigation%2520experiments%252C%2520demonstrating%2520how%250Aaccurate%2520person%2520tracking%2520benefits%2520the%2520planner%2520in%2520optimizing%2520the%2520generated%250Atrajectories%252C%2520enhancing%2520its%2520collision%2520avoidance%2520capabilities.%2520This%2520paper%250Acontributes%2520to%2520safer%2520human-robot%2520cohabitation%252C%2520blending%2520recent%2520advances%2520in%250Ahuman%2520detection%2520with%2520responsive%2520planning%2520to%2520navigate%2520shared%2520spaces%2520effectively%250Aand%2520securely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Navigation%20in%20Dynamic%20Human%20Environments%20with%20an%20Embedded%202D%0A%20%20LiDAR-based%20Person%20Tracker&entry.906535625=Davide%20Plozza%20and%20Steven%20Marty%20and%20Cyril%20Scherrer%20and%20Simon%20Schwartz%20and%20Stefan%20Zihlmann%20and%20Michele%20Magno&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20landscape%20of%20autonomous%20mobile%20robots%2C%20the%20emphasis%0Aon%20seamless%20human-robot%20interactions%20has%20shifted%20towards%20autonomous%0Adecision-making.%20This%20paper%20delves%20into%20the%20intricate%20challenges%20associated%0Awith%20robotic%20autonomy%2C%20focusing%20on%20navigation%20in%20dynamic%20environments%20shared%0Awith%20humans.%20It%20introduces%20an%20embedded%20real-time%20tracking%20pipeline%2C%20integrated%0Ainto%20a%20navigation%20planning%20framework%20for%20effective%20person%20tracking%20and%0Aavoidance%2C%20adapting%20a%20state-of-the-art%202D%20LiDAR-based%20human%20detection%20network%0Aand%20an%20efficient%20multi-object%20tracker.%20By%20addressing%20the%20key%20components%20of%0Adetection%2C%20tracking%2C%20and%20planning%20separately%2C%20the%20proposed%20approach%20highlights%0Athe%20modularity%20and%20transferability%20of%20each%20component%20to%20other%20applications.%20Our%0Atracking%20approach%20is%20validated%20on%20a%20quadruped%20robot%20equipped%20with%20270%7B%5Cdeg%7D%0A2D-LiDAR%20against%20motion%20capture%20system%20data%2C%20with%20the%20preferred%20configuration%0Aachieving%20an%20average%20MOTA%20of%2085.45%25%20in%20three%20newly%20recorded%20datasets%2C%20while%0Areliably%20running%20in%20real-time%20at%2020%20Hz%20on%20the%20NVIDIA%20Jetson%20Xavier%20NX%20embedded%0AGPU-accelerated%20platform.%20Furthermore%2C%20the%20integrated%20tracking%20and%20avoidance%0Asystem%20is%20evaluated%20in%20real-world%20navigation%20experiments%2C%20demonstrating%20how%0Aaccurate%20person%20tracking%20benefits%20the%20planner%20in%20optimizing%20the%20generated%0Atrajectories%2C%20enhancing%20its%20collision%20avoidance%20capabilities.%20This%20paper%0Acontributes%20to%20safer%20human-robot%20cohabitation%2C%20blending%20recent%20advances%20in%0Ahuman%20detection%20with%20responsive%20planning%20to%20navigate%20shared%20spaces%20effectively%0Aand%20securely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15000v1&entry.124074799=Read"},
{"title": "DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation", "author": "Hourun Li and Yifan Wang and Zhiping Xiao and Jia Yang and Changling Zhou and Ming Zhang and Wei Ju", "abstract": "  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n", "link": "http://arxiv.org/abs/2412.15005v1", "date": "2024-12-19", "relevancy": 2.4307, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4909}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.487}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCo%3A%20Graph-Based%20Disentangled%20Contrastive%20Learning%20for%20Cold-Start%0A%20%20Cross-Domain%20Recommendation&body=Title%3A%20DisCo%3A%20Graph-Based%20Disentangled%20Contrastive%20Learning%20for%20Cold-Start%0A%20%20Cross-Domain%20Recommendation%0AAuthor%3A%20Hourun%20Li%20and%20Yifan%20Wang%20and%20Zhiping%20Xiao%20and%20Jia%20Yang%20and%20Changling%20Zhou%20and%20Ming%20Zhang%20and%20Wei%20Ju%0AAbstract%3A%20%20%20Recommender%20systems%20are%20widely%20used%20in%20various%20real-world%20applications%2C%20but%0Athey%20often%20encounter%20the%20persistent%20challenge%20of%20the%20user%20cold-start%20problem.%0ACross-domain%20recommendation%20%28CDR%29%2C%20which%20leverages%20user%20interactions%20from%20one%0Adomain%20to%20improve%20prediction%20performance%20in%20another%2C%20has%20emerged%20as%20a%20promising%0Asolution.%20However%2C%20users%20with%20similar%20preferences%20in%20the%20source%20domain%20may%0Aexhibit%20different%20interests%20in%20the%20target%20domain.%20Therefore%2C%20directly%0Atransferring%20embeddings%20may%20introduce%20irrelevant%20source-domain%20collaborative%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20graph-based%20disentangled%0Acontrastive%20learning%20framework%20to%20capture%20fine-grained%20user%20intent%20and%20filter%0Aout%20irrelevant%20collaborative%20information%2C%20thereby%20avoiding%20negative%20transfer.%0ASpecifically%2C%20for%20each%20domain%2C%20we%20use%20a%20multi-channel%20graph%20encoder%20to%20capture%0Adiverse%20user%20intents.%20We%20then%20construct%20the%20affinity%20graph%20in%20the%20embedding%0Aspace%20and%20perform%20multi-step%20random%20walks%20to%20capture%20high-order%20user%20similarity%0Arelationships.%20Treating%20one%20domain%20as%20the%20target%2C%20we%20propose%20a%20disentangled%0Aintent-wise%20contrastive%20learning%20approach%2C%20guided%20by%20user%20similarity%2C%20to%20refine%0Athe%20bridging%20of%20user%20intents%20across%20domains.%20Extensive%20experiments%20on%20four%0Abenchmark%20CDR%20datasets%20demonstrate%20that%20DisCo%20consistently%20outperforms%20existing%0Astate-of-the-art%20baselines%2C%20thereby%20validating%20the%20effectiveness%20of%20both%20DisCo%0Aand%20its%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCo%253A%2520Graph-Based%2520Disentangled%2520Contrastive%2520Learning%2520for%2520Cold-Start%250A%2520%2520Cross-Domain%2520Recommendation%26entry.906535625%3DHourun%2520Li%2520and%2520Yifan%2520Wang%2520and%2520Zhiping%2520Xiao%2520and%2520Jia%2520Yang%2520and%2520Changling%2520Zhou%2520and%2520Ming%2520Zhang%2520and%2520Wei%2520Ju%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520widely%2520used%2520in%2520various%2520real-world%2520applications%252C%2520but%250Athey%2520often%2520encounter%2520the%2520persistent%2520challenge%2520of%2520the%2520user%2520cold-start%2520problem.%250ACross-domain%2520recommendation%2520%2528CDR%2529%252C%2520which%2520leverages%2520user%2520interactions%2520from%2520one%250Adomain%2520to%2520improve%2520prediction%2520performance%2520in%2520another%252C%2520has%2520emerged%2520as%2520a%2520promising%250Asolution.%2520However%252C%2520users%2520with%2520similar%2520preferences%2520in%2520the%2520source%2520domain%2520may%250Aexhibit%2520different%2520interests%2520in%2520the%2520target%2520domain.%2520Therefore%252C%2520directly%250Atransferring%2520embeddings%2520may%2520introduce%2520irrelevant%2520source-domain%2520collaborative%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520graph-based%2520disentangled%250Acontrastive%2520learning%2520framework%2520to%2520capture%2520fine-grained%2520user%2520intent%2520and%2520filter%250Aout%2520irrelevant%2520collaborative%2520information%252C%2520thereby%2520avoiding%2520negative%2520transfer.%250ASpecifically%252C%2520for%2520each%2520domain%252C%2520we%2520use%2520a%2520multi-channel%2520graph%2520encoder%2520to%2520capture%250Adiverse%2520user%2520intents.%2520We%2520then%2520construct%2520the%2520affinity%2520graph%2520in%2520the%2520embedding%250Aspace%2520and%2520perform%2520multi-step%2520random%2520walks%2520to%2520capture%2520high-order%2520user%2520similarity%250Arelationships.%2520Treating%2520one%2520domain%2520as%2520the%2520target%252C%2520we%2520propose%2520a%2520disentangled%250Aintent-wise%2520contrastive%2520learning%2520approach%252C%2520guided%2520by%2520user%2520similarity%252C%2520to%2520refine%250Athe%2520bridging%2520of%2520user%2520intents%2520across%2520domains.%2520Extensive%2520experiments%2520on%2520four%250Abenchmark%2520CDR%2520datasets%2520demonstrate%2520that%2520DisCo%2520consistently%2520outperforms%2520existing%250Astate-of-the-art%2520baselines%252C%2520thereby%2520validating%2520the%2520effectiveness%2520of%2520both%2520DisCo%250Aand%2520its%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCo%3A%20Graph-Based%20Disentangled%20Contrastive%20Learning%20for%20Cold-Start%0A%20%20Cross-Domain%20Recommendation&entry.906535625=Hourun%20Li%20and%20Yifan%20Wang%20and%20Zhiping%20Xiao%20and%20Jia%20Yang%20and%20Changling%20Zhou%20and%20Ming%20Zhang%20and%20Wei%20Ju&entry.1292438233=%20%20Recommender%20systems%20are%20widely%20used%20in%20various%20real-world%20applications%2C%20but%0Athey%20often%20encounter%20the%20persistent%20challenge%20of%20the%20user%20cold-start%20problem.%0ACross-domain%20recommendation%20%28CDR%29%2C%20which%20leverages%20user%20interactions%20from%20one%0Adomain%20to%20improve%20prediction%20performance%20in%20another%2C%20has%20emerged%20as%20a%20promising%0Asolution.%20However%2C%20users%20with%20similar%20preferences%20in%20the%20source%20domain%20may%0Aexhibit%20different%20interests%20in%20the%20target%20domain.%20Therefore%2C%20directly%0Atransferring%20embeddings%20may%20introduce%20irrelevant%20source-domain%20collaborative%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20graph-based%20disentangled%0Acontrastive%20learning%20framework%20to%20capture%20fine-grained%20user%20intent%20and%20filter%0Aout%20irrelevant%20collaborative%20information%2C%20thereby%20avoiding%20negative%20transfer.%0ASpecifically%2C%20for%20each%20domain%2C%20we%20use%20a%20multi-channel%20graph%20encoder%20to%20capture%0Adiverse%20user%20intents.%20We%20then%20construct%20the%20affinity%20graph%20in%20the%20embedding%0Aspace%20and%20perform%20multi-step%20random%20walks%20to%20capture%20high-order%20user%20similarity%0Arelationships.%20Treating%20one%20domain%20as%20the%20target%2C%20we%20propose%20a%20disentangled%0Aintent-wise%20contrastive%20learning%20approach%2C%20guided%20by%20user%20similarity%2C%20to%20refine%0Athe%20bridging%20of%20user%20intents%20across%20domains.%20Extensive%20experiments%20on%20four%0Abenchmark%20CDR%20datasets%20demonstrate%20that%20DisCo%20consistently%20outperforms%20existing%0Astate-of-the-art%20baselines%2C%20thereby%20validating%20the%20effectiveness%20of%20both%20DisCo%0Aand%20its%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15005v1&entry.124074799=Read"},
{"title": "Holistic Adversarially Robust Pruning", "author": "Qi Zhao and Christian Wressnegger", "abstract": "  Neural networks can be drastically shrunk in size by removing redundant\nparameters. While crucial for the deployment on resource-constraint hardware,\noftentimes, compression comes with a severe drop in accuracy and lack of\nadversarial robustness. Despite recent advances, counteracting both aspects has\nonly succeeded for moderate compression rates so far. We propose a novel\nmethod, HARP, that copes with aggressive pruning significantly better than\nprior work. For this, we consider the network holistically. We learn a global\ncompression strategy that optimizes how many parameters (compression rate) and\nwhich parameters (scoring connections) to prune specific to each layer\nindividually. Our method fine-tunes an existing model with dynamic\nregularization, that follows a step-wise incremental function balancing the\ndifferent objectives. It starts by favoring robustness before shifting focus on\nreaching the target compression rate and only then handles the objectives\nequally. The learned compression strategies allow us to maintain the\npre-trained model natural accuracy and its adversarial robustness for a\nreduction by 99% of the network original size. Moreover, we observe a crucial\ninfluence of non-uniform compression across layers.\n", "link": "http://arxiv.org/abs/2412.14714v1", "date": "2024-12-19", "relevancy": 2.4289, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4921}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Adversarially%20Robust%20Pruning&body=Title%3A%20Holistic%20Adversarially%20Robust%20Pruning%0AAuthor%3A%20Qi%20Zhao%20and%20Christian%20Wressnegger%0AAbstract%3A%20%20%20Neural%20networks%20can%20be%20drastically%20shrunk%20in%20size%20by%20removing%20redundant%0Aparameters.%20While%20crucial%20for%20the%20deployment%20on%20resource-constraint%20hardware%2C%0Aoftentimes%2C%20compression%20comes%20with%20a%20severe%20drop%20in%20accuracy%20and%20lack%20of%0Aadversarial%20robustness.%20Despite%20recent%20advances%2C%20counteracting%20both%20aspects%20has%0Aonly%20succeeded%20for%20moderate%20compression%20rates%20so%20far.%20We%20propose%20a%20novel%0Amethod%2C%20HARP%2C%20that%20copes%20with%20aggressive%20pruning%20significantly%20better%20than%0Aprior%20work.%20For%20this%2C%20we%20consider%20the%20network%20holistically.%20We%20learn%20a%20global%0Acompression%20strategy%20that%20optimizes%20how%20many%20parameters%20%28compression%20rate%29%20and%0Awhich%20parameters%20%28scoring%20connections%29%20to%20prune%20specific%20to%20each%20layer%0Aindividually.%20Our%20method%20fine-tunes%20an%20existing%20model%20with%20dynamic%0Aregularization%2C%20that%20follows%20a%20step-wise%20incremental%20function%20balancing%20the%0Adifferent%20objectives.%20It%20starts%20by%20favoring%20robustness%20before%20shifting%20focus%20on%0Areaching%20the%20target%20compression%20rate%20and%20only%20then%20handles%20the%20objectives%0Aequally.%20The%20learned%20compression%20strategies%20allow%20us%20to%20maintain%20the%0Apre-trained%20model%20natural%20accuracy%20and%20its%20adversarial%20robustness%20for%20a%0Areduction%20by%2099%25%20of%20the%20network%20original%20size.%20Moreover%2C%20we%20observe%20a%20crucial%0Ainfluence%20of%20non-uniform%20compression%20across%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Adversarially%2520Robust%2520Pruning%26entry.906535625%3DQi%2520Zhao%2520and%2520Christian%2520Wressnegger%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520be%2520drastically%2520shrunk%2520in%2520size%2520by%2520removing%2520redundant%250Aparameters.%2520While%2520crucial%2520for%2520the%2520deployment%2520on%2520resource-constraint%2520hardware%252C%250Aoftentimes%252C%2520compression%2520comes%2520with%2520a%2520severe%2520drop%2520in%2520accuracy%2520and%2520lack%2520of%250Aadversarial%2520robustness.%2520Despite%2520recent%2520advances%252C%2520counteracting%2520both%2520aspects%2520has%250Aonly%2520succeeded%2520for%2520moderate%2520compression%2520rates%2520so%2520far.%2520We%2520propose%2520a%2520novel%250Amethod%252C%2520HARP%252C%2520that%2520copes%2520with%2520aggressive%2520pruning%2520significantly%2520better%2520than%250Aprior%2520work.%2520For%2520this%252C%2520we%2520consider%2520the%2520network%2520holistically.%2520We%2520learn%2520a%2520global%250Acompression%2520strategy%2520that%2520optimizes%2520how%2520many%2520parameters%2520%2528compression%2520rate%2529%2520and%250Awhich%2520parameters%2520%2528scoring%2520connections%2529%2520to%2520prune%2520specific%2520to%2520each%2520layer%250Aindividually.%2520Our%2520method%2520fine-tunes%2520an%2520existing%2520model%2520with%2520dynamic%250Aregularization%252C%2520that%2520follows%2520a%2520step-wise%2520incremental%2520function%2520balancing%2520the%250Adifferent%2520objectives.%2520It%2520starts%2520by%2520favoring%2520robustness%2520before%2520shifting%2520focus%2520on%250Areaching%2520the%2520target%2520compression%2520rate%2520and%2520only%2520then%2520handles%2520the%2520objectives%250Aequally.%2520The%2520learned%2520compression%2520strategies%2520allow%2520us%2520to%2520maintain%2520the%250Apre-trained%2520model%2520natural%2520accuracy%2520and%2520its%2520adversarial%2520robustness%2520for%2520a%250Areduction%2520by%252099%2525%2520of%2520the%2520network%2520original%2520size.%2520Moreover%252C%2520we%2520observe%2520a%2520crucial%250Ainfluence%2520of%2520non-uniform%2520compression%2520across%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Adversarially%20Robust%20Pruning&entry.906535625=Qi%20Zhao%20and%20Christian%20Wressnegger&entry.1292438233=%20%20Neural%20networks%20can%20be%20drastically%20shrunk%20in%20size%20by%20removing%20redundant%0Aparameters.%20While%20crucial%20for%20the%20deployment%20on%20resource-constraint%20hardware%2C%0Aoftentimes%2C%20compression%20comes%20with%20a%20severe%20drop%20in%20accuracy%20and%20lack%20of%0Aadversarial%20robustness.%20Despite%20recent%20advances%2C%20counteracting%20both%20aspects%20has%0Aonly%20succeeded%20for%20moderate%20compression%20rates%20so%20far.%20We%20propose%20a%20novel%0Amethod%2C%20HARP%2C%20that%20copes%20with%20aggressive%20pruning%20significantly%20better%20than%0Aprior%20work.%20For%20this%2C%20we%20consider%20the%20network%20holistically.%20We%20learn%20a%20global%0Acompression%20strategy%20that%20optimizes%20how%20many%20parameters%20%28compression%20rate%29%20and%0Awhich%20parameters%20%28scoring%20connections%29%20to%20prune%20specific%20to%20each%20layer%0Aindividually.%20Our%20method%20fine-tunes%20an%20existing%20model%20with%20dynamic%0Aregularization%2C%20that%20follows%20a%20step-wise%20incremental%20function%20balancing%20the%0Adifferent%20objectives.%20It%20starts%20by%20favoring%20robustness%20before%20shifting%20focus%20on%0Areaching%20the%20target%20compression%20rate%20and%20only%20then%20handles%20the%20objectives%0Aequally.%20The%20learned%20compression%20strategies%20allow%20us%20to%20maintain%20the%0Apre-trained%20model%20natural%20accuracy%20and%20its%20adversarial%20robustness%20for%20a%0Areduction%20by%2099%25%20of%20the%20network%20original%20size.%20Moreover%2C%20we%20observe%20a%20crucial%0Ainfluence%20of%20non-uniform%20compression%20across%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14714v1&entry.124074799=Read"},
{"title": "ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait\n  Image", "author": "Jinkun Hao and Junshu Tang and Jiangning Zhang and Ran Yi and Yijia Hong and Moran Li and Weijian Cao and Yating Wang and Chengjie Wang and Lizhuang Ma", "abstract": "  While recent works have achieved great success on image-to-3D object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, ID-Sculpt, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the ID-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from a single in-the-wild portrait\nimage.\n", "link": "http://arxiv.org/abs/2406.16710v2", "date": "2024-12-19", "relevancy": 2.4192, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6251}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.605}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ID-Sculpt%3A%20ID-aware%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%0A%20%20Image&body=Title%3A%20ID-Sculpt%3A%20ID-aware%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%0A%20%20Image%0AAuthor%3A%20Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20image-to-3D%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20ID-Sculpt%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20ID-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20a%20single%20in-the-wild%20portrait%0Aimage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DID-Sculpt%253A%2520ID-aware%25203D%2520Head%2520Generation%2520from%2520Single%2520In-the-wild%2520Portrait%250A%2520%2520Image%26entry.906535625%3DJinkun%2520Hao%2520and%2520Junshu%2520Tang%2520and%2520Jiangning%2520Zhang%2520and%2520Ran%2520Yi%2520and%2520Yijia%2520Hong%2520and%2520Moran%2520Li%2520and%2520Weijian%2520Cao%2520and%2520Yating%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520While%2520recent%2520works%2520have%2520achieved%2520great%2520success%2520on%2520image-to-3D%2520object%250Ageneration%252C%2520high%2520quality%2520and%2520fidelity%25203D%2520head%2520generation%2520from%2520a%2520single%2520image%250Aremains%2520a%2520great%2520challenge.%2520Previous%2520text-based%2520methods%2520for%2520generating%25203D%2520heads%250Awere%2520limited%2520by%2520text%2520descriptions%2520and%2520image-based%2520methods%2520struggled%2520to%2520produce%250Ahigh-quality%2520head%2520geometry.%2520To%2520handle%2520this%2520challenging%2520problem%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520ID-Sculpt%252C%2520to%2520generate%2520high-quality%25203D%2520heads%2520while%2520preserving%250Atheir%2520identities.%2520Our%2520work%2520incorporates%2520the%2520identity%2520information%2520of%2520the%250Aportrait%2520image%2520into%2520three%2520parts%253A%25201%2529%2520geometry%2520initialization%252C%25202%2529%2520geometry%250Asculpting%252C%2520and%25203%2529%2520texture%2520generation%2520stages.%2520Given%2520a%2520reference%2520portrait%2520image%252C%250Awe%2520first%2520align%2520the%2520identity%2520features%2520with%2520text%2520features%2520to%2520realize%2520ID-aware%250Aguidance%2520enhancement%252C%2520which%2520contains%2520the%2520control%2520signals%2520representing%2520the%2520face%250Ainformation.%2520We%2520then%2520use%2520the%2520canny%2520map%252C%2520ID%2520features%2520of%2520the%2520portrait%2520image%252C%2520and%250Aa%2520pre-trained%2520text-to-normal/depth%2520diffusion%2520model%2520to%2520generate%2520ID-aware%250Ageometry%2520supervision%252C%2520and%25203D-GAN%2520inversion%2520is%2520employed%2520to%2520generate%2520ID-aware%250Ageometry%2520initialization.%2520Furthermore%252C%2520with%2520the%2520ability%2520to%2520inject%2520identity%250Ainformation%2520into%25203D%2520head%2520generation%252C%2520we%2520use%2520ID-aware%2520guidance%2520to%2520calculate%250AID-aware%2520Score%2520Distillation%2520%2528ISD%2529%2520for%2520geometry%2520sculpting.%2520For%2520texture%250Ageneration%252C%2520we%2520adopt%2520the%2520ID%2520Consistent%2520Texture%2520Inpainting%2520and%2520Refinement%2520which%250Aprogressively%2520expands%2520the%2520view%2520for%2520texture%2520inpainting%2520to%2520obtain%2520an%250Ainitialization%2520UV%2520texture%2520map.%2520We%2520then%2520use%2520the%2520ID-aware%2520guidance%2520to%2520provide%250Aimage-level%2520supervision%2520for%2520noisy%2520multi-view%2520images%2520to%2520obtain%2520a%2520refined%2520texture%250Amap.%2520Extensive%2520experiments%2520demonstrate%2520that%2520we%2520can%2520generate%2520high-quality%25203D%250Aheads%2520with%2520accurate%2520geometry%2520and%2520texture%2520from%2520a%2520single%2520in-the-wild%2520portrait%250Aimage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Sculpt%3A%20ID-aware%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%0A%20%20Image&entry.906535625=Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20image-to-3D%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20ID-Sculpt%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20ID-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20a%20single%20in-the-wild%20portrait%0Aimage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16710v2&entry.124074799=Read"},
{"title": "Learning from Linear Algebra: A Graph Neural Network Approach to\n  Preconditioner Design for Conjugate Gradient Solvers", "author": "Vladislav Trifonov and Alexander Rudikov and Oleg Iliev and Yuri M. Laevsky and Ivan Oseledets and Ekaterina Muravleva", "abstract": "  Large linear systems are ubiquitous in modern computational science and\nengineering. The main recipe for solving them is the use of Krylov subspace\niterative methods with well-designed preconditioners. Deep learning models can\nbe used as nonlinear preconditioners during the iteration of linear solvers\nsuch as the conjugate gradient (CG) method. Neural network models require an\nenormous number of parameters to approximate well in this setup. Another\napproach is to take advantage of small graph neural networks (GNNs) to\nconstruct preconditioners with predefined sparsity patterns. Recently, GNNs\nhave been shown to be a promising tool for designing preconditioners to reduce\nthe overall computational cost of iterative methods by constructing them more\nefficiently than with classical linear algebra techniques. However,\npreconditioners designed with these approaches cannot outperform those designed\nwith classical methods in terms of the number of iterations in CG. In our work,\nwe recall well-established preconditioners from linear algebra and use them as\na starting point for training the GNN to obtain preconditioners that reduce the\ncondition number of the system more significantly. Numerical experiments show\nthat our approach outperforms both classical and neural network-based methods\nfor an important class of parametric partial differential equations. We also\nprovide a heuristic justification for the loss function used and show that\npreconditioners obtained by learning with this loss function reduce the\ncondition number in a more desirable way for CG.\n", "link": "http://arxiv.org/abs/2405.15557v2", "date": "2024-12-19", "relevancy": 2.4188, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4911}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&body=Title%3A%20Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers%0AAuthor%3A%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva%0AAbstract%3A%20%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science%20and%0Aengineering.%20The%20main%20recipe%20for%20solving%20them%20is%20the%20use%20of%20Krylov%20subspace%0Aiterative%20methods%20with%20well-designed%20preconditioners.%20Deep%20learning%20models%20can%0Abe%20used%20as%20nonlinear%20preconditioners%20during%20the%20iteration%20of%20linear%20solvers%0Asuch%20as%20the%20conjugate%20gradient%20%28CG%29%20method.%20Neural%20network%20models%20require%20an%0Aenormous%20number%20of%20parameters%20to%20approximate%20well%20in%20this%20setup.%20Another%0Aapproach%20is%20to%20take%20advantage%20of%20small%20graph%20neural%20networks%20%28GNNs%29%20to%0Aconstruct%20preconditioners%20with%20predefined%20sparsity%20patterns.%20Recently%2C%20GNNs%0Ahave%20been%20shown%20to%20be%20a%20promising%20tool%20for%20designing%20preconditioners%20to%20reduce%0Athe%20overall%20computational%20cost%20of%20iterative%20methods%20by%20constructing%20them%20more%0Aefficiently%20than%20with%20classical%20linear%20algebra%20techniques.%20However%2C%0Apreconditioners%20designed%20with%20these%20approaches%20cannot%20outperform%20those%20designed%0Awith%20classical%20methods%20in%20terms%20of%20the%20number%20of%20iterations%20in%20CG.%20In%20our%20work%2C%0Awe%20recall%20well-established%20preconditioners%20from%20linear%20algebra%20and%20use%20them%20as%0Aa%20starting%20point%20for%20training%20the%20GNN%20to%20obtain%20preconditioners%20that%20reduce%20the%0Acondition%20number%20of%20the%20system%20more%20significantly.%20Numerical%20experiments%20show%0Athat%20our%20approach%20outperforms%20both%20classical%20and%20neural%20network-based%20methods%0Afor%20an%20important%20class%20of%20parametric%20partial%20differential%20equations.%20We%20also%0Aprovide%20a%20heuristic%20justification%20for%20the%20loss%20function%20used%20and%20show%20that%0Apreconditioners%20obtained%20by%20learning%20with%20this%20loss%20function%20reduce%20the%0Acondition%20number%20in%20a%20more%20desirable%20way%20for%20CG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Linear%2520Algebra%253A%2520A%2520Graph%2520Neural%2520Network%2520Approach%2520to%250A%2520%2520Preconditioner%2520Design%2520for%2520Conjugate%2520Gradient%2520Solvers%26entry.906535625%3DVladislav%2520Trifonov%2520and%2520Alexander%2520Rudikov%2520and%2520Oleg%2520Iliev%2520and%2520Yuri%2520M.%2520Laevsky%2520and%2520Ivan%2520Oseledets%2520and%2520Ekaterina%2520Muravleva%26entry.1292438233%3D%2520%2520Large%2520linear%2520systems%2520are%2520ubiquitous%2520in%2520modern%2520computational%2520science%2520and%250Aengineering.%2520The%2520main%2520recipe%2520for%2520solving%2520them%2520is%2520the%2520use%2520of%2520Krylov%2520subspace%250Aiterative%2520methods%2520with%2520well-designed%2520preconditioners.%2520Deep%2520learning%2520models%2520can%250Abe%2520used%2520as%2520nonlinear%2520preconditioners%2520during%2520the%2520iteration%2520of%2520linear%2520solvers%250Asuch%2520as%2520the%2520conjugate%2520gradient%2520%2528CG%2529%2520method.%2520Neural%2520network%2520models%2520require%2520an%250Aenormous%2520number%2520of%2520parameters%2520to%2520approximate%2520well%2520in%2520this%2520setup.%2520Another%250Aapproach%2520is%2520to%2520take%2520advantage%2520of%2520small%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%250Aconstruct%2520preconditioners%2520with%2520predefined%2520sparsity%2520patterns.%2520Recently%252C%2520GNNs%250Ahave%2520been%2520shown%2520to%2520be%2520a%2520promising%2520tool%2520for%2520designing%2520preconditioners%2520to%2520reduce%250Athe%2520overall%2520computational%2520cost%2520of%2520iterative%2520methods%2520by%2520constructing%2520them%2520more%250Aefficiently%2520than%2520with%2520classical%2520linear%2520algebra%2520techniques.%2520However%252C%250Apreconditioners%2520designed%2520with%2520these%2520approaches%2520cannot%2520outperform%2520those%2520designed%250Awith%2520classical%2520methods%2520in%2520terms%2520of%2520the%2520number%2520of%2520iterations%2520in%2520CG.%2520In%2520our%2520work%252C%250Awe%2520recall%2520well-established%2520preconditioners%2520from%2520linear%2520algebra%2520and%2520use%2520them%2520as%250Aa%2520starting%2520point%2520for%2520training%2520the%2520GNN%2520to%2520obtain%2520preconditioners%2520that%2520reduce%2520the%250Acondition%2520number%2520of%2520the%2520system%2520more%2520significantly.%2520Numerical%2520experiments%2520show%250Athat%2520our%2520approach%2520outperforms%2520both%2520classical%2520and%2520neural%2520network-based%2520methods%250Afor%2520an%2520important%2520class%2520of%2520parametric%2520partial%2520differential%2520equations.%2520We%2520also%250Aprovide%2520a%2520heuristic%2520justification%2520for%2520the%2520loss%2520function%2520used%2520and%2520show%2520that%250Apreconditioners%2520obtained%2520by%2520learning%2520with%2520this%2520loss%2520function%2520reduce%2520the%250Acondition%2520number%2520in%2520a%2520more%2520desirable%2520way%2520for%2520CG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Linear%20Algebra%3A%20A%20Graph%20Neural%20Network%20Approach%20to%0A%20%20Preconditioner%20Design%20for%20Conjugate%20Gradient%20Solvers&entry.906535625=Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Oleg%20Iliev%20and%20Yuri%20M.%20Laevsky%20and%20Ivan%20Oseledets%20and%20Ekaterina%20Muravleva&entry.1292438233=%20%20Large%20linear%20systems%20are%20ubiquitous%20in%20modern%20computational%20science%20and%0Aengineering.%20The%20main%20recipe%20for%20solving%20them%20is%20the%20use%20of%20Krylov%20subspace%0Aiterative%20methods%20with%20well-designed%20preconditioners.%20Deep%20learning%20models%20can%0Abe%20used%20as%20nonlinear%20preconditioners%20during%20the%20iteration%20of%20linear%20solvers%0Asuch%20as%20the%20conjugate%20gradient%20%28CG%29%20method.%20Neural%20network%20models%20require%20an%0Aenormous%20number%20of%20parameters%20to%20approximate%20well%20in%20this%20setup.%20Another%0Aapproach%20is%20to%20take%20advantage%20of%20small%20graph%20neural%20networks%20%28GNNs%29%20to%0Aconstruct%20preconditioners%20with%20predefined%20sparsity%20patterns.%20Recently%2C%20GNNs%0Ahave%20been%20shown%20to%20be%20a%20promising%20tool%20for%20designing%20preconditioners%20to%20reduce%0Athe%20overall%20computational%20cost%20of%20iterative%20methods%20by%20constructing%20them%20more%0Aefficiently%20than%20with%20classical%20linear%20algebra%20techniques.%20However%2C%0Apreconditioners%20designed%20with%20these%20approaches%20cannot%20outperform%20those%20designed%0Awith%20classical%20methods%20in%20terms%20of%20the%20number%20of%20iterations%20in%20CG.%20In%20our%20work%2C%0Awe%20recall%20well-established%20preconditioners%20from%20linear%20algebra%20and%20use%20them%20as%0Aa%20starting%20point%20for%20training%20the%20GNN%20to%20obtain%20preconditioners%20that%20reduce%20the%0Acondition%20number%20of%20the%20system%20more%20significantly.%20Numerical%20experiments%20show%0Athat%20our%20approach%20outperforms%20both%20classical%20and%20neural%20network-based%20methods%0Afor%20an%20important%20class%20of%20parametric%20partial%20differential%20equations.%20We%20also%0Aprovide%20a%20heuristic%20justification%20for%20the%20loss%20function%20used%20and%20show%20that%0Apreconditioners%20obtained%20by%20learning%20with%20this%20loss%20function%20reduce%20the%0Acondition%20number%20in%20a%20more%20desirable%20way%20for%20CG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15557v2&entry.124074799=Read"},
{"title": "TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated\n  Learning", "author": "Gangqiang Hu and Jianfeng Lu and Jianmin Han and Shuqin Cao and Jing Liu and Hao Fu", "abstract": "  Due to the sensitivity of data, Federated Learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized FL, clients' communication and training states are dynamic.\nThis variability arises from local training fluctuations, heterogeneous data\ndistributions, and intermittent client participation. Most existing studies\nprimarily focus on stable client states, neglecting the dynamic challenges\ninherent in real-world scenarios. To tackle this issue, we propose a\nTRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client\nstates and contributions, enhancing model training efficiency through selective\nclient participation. We focus on a semi-decentralized FL framework where edge\nservers and clients train a shared global model using unreliable intra-cluster\nmodel aggregation and inter-cluster model consensus. First, we propose an\nadaptive hidden semi-Markov model to estimate clients' communication states and\ncontributions. Next, we address a client-server association optimization\nproblem to minimize global training loss. Using convergence analysis, we\npropose a greedy client scheduling algorithm. Finally, our experiments\nconducted on real-world datasets demonstrate that TRAIL outperforms\nstate-of-the-art baselines, achieving an improvement of 8.7% in test accuracy\nand a reduction of 15.3% in training loss.\n", "link": "http://arxiv.org/abs/2412.11448v3", "date": "2024-12-19", "relevancy": 2.3978, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4789}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRAIL%3A%20Trust-Aware%20Client%20Scheduling%20for%20Semi-Decentralized%20Federated%0A%20%20Learning&body=Title%3A%20TRAIL%3A%20Trust-Aware%20Client%20Scheduling%20for%20Semi-Decentralized%20Federated%0A%20%20Learning%0AAuthor%3A%20Gangqiang%20Hu%20and%20Jianfeng%20Lu%20and%20Jianmin%20Han%20and%20Shuqin%20Cao%20and%20Jing%20Liu%20and%20Hao%20Fu%0AAbstract%3A%20%20%20Due%20to%20the%20sensitivity%20of%20data%2C%20Federated%20Learning%20%28FL%29%20is%20employed%20to%20enable%0Adistributed%20machine%20learning%20while%20safeguarding%20data%20privacy%20and%20accommodating%0Athe%20requirements%20of%20various%20devices.%20However%2C%20in%20the%20context%20of%0Asemi-decentralized%20FL%2C%20clients%27%20communication%20and%20training%20states%20are%20dynamic.%0AThis%20variability%20arises%20from%20local%20training%20fluctuations%2C%20heterogeneous%20data%0Adistributions%2C%20and%20intermittent%20client%20participation.%20Most%20existing%20studies%0Aprimarily%20focus%20on%20stable%20client%20states%2C%20neglecting%20the%20dynamic%20challenges%0Ainherent%20in%20real-world%20scenarios.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%0ATRust-Aware%20clIent%20scheduLing%20mechanism%20called%20TRAIL%2C%20which%20assesses%20client%0Astates%20and%20contributions%2C%20enhancing%20model%20training%20efficiency%20through%20selective%0Aclient%20participation.%20We%20focus%20on%20a%20semi-decentralized%20FL%20framework%20where%20edge%0Aservers%20and%20clients%20train%20a%20shared%20global%20model%20using%20unreliable%20intra-cluster%0Amodel%20aggregation%20and%20inter-cluster%20model%20consensus.%20First%2C%20we%20propose%20an%0Aadaptive%20hidden%20semi-Markov%20model%20to%20estimate%20clients%27%20communication%20states%20and%0Acontributions.%20Next%2C%20we%20address%20a%20client-server%20association%20optimization%0Aproblem%20to%20minimize%20global%20training%20loss.%20Using%20convergence%20analysis%2C%20we%0Apropose%20a%20greedy%20client%20scheduling%20algorithm.%20Finally%2C%20our%20experiments%0Aconducted%20on%20real-world%20datasets%20demonstrate%20that%20TRAIL%20outperforms%0Astate-of-the-art%20baselines%2C%20achieving%20an%20improvement%20of%208.7%25%20in%20test%20accuracy%0Aand%20a%20reduction%20of%2015.3%25%20in%20training%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11448v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRAIL%253A%2520Trust-Aware%2520Client%2520Scheduling%2520for%2520Semi-Decentralized%2520Federated%250A%2520%2520Learning%26entry.906535625%3DGangqiang%2520Hu%2520and%2520Jianfeng%2520Lu%2520and%2520Jianmin%2520Han%2520and%2520Shuqin%2520Cao%2520and%2520Jing%2520Liu%2520and%2520Hao%2520Fu%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520sensitivity%2520of%2520data%252C%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520employed%2520to%2520enable%250Adistributed%2520machine%2520learning%2520while%2520safeguarding%2520data%2520privacy%2520and%2520accommodating%250Athe%2520requirements%2520of%2520various%2520devices.%2520However%252C%2520in%2520the%2520context%2520of%250Asemi-decentralized%2520FL%252C%2520clients%2527%2520communication%2520and%2520training%2520states%2520are%2520dynamic.%250AThis%2520variability%2520arises%2520from%2520local%2520training%2520fluctuations%252C%2520heterogeneous%2520data%250Adistributions%252C%2520and%2520intermittent%2520client%2520participation.%2520Most%2520existing%2520studies%250Aprimarily%2520focus%2520on%2520stable%2520client%2520states%252C%2520neglecting%2520the%2520dynamic%2520challenges%250Ainherent%2520in%2520real-world%2520scenarios.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%250ATRust-Aware%2520clIent%2520scheduLing%2520mechanism%2520called%2520TRAIL%252C%2520which%2520assesses%2520client%250Astates%2520and%2520contributions%252C%2520enhancing%2520model%2520training%2520efficiency%2520through%2520selective%250Aclient%2520participation.%2520We%2520focus%2520on%2520a%2520semi-decentralized%2520FL%2520framework%2520where%2520edge%250Aservers%2520and%2520clients%2520train%2520a%2520shared%2520global%2520model%2520using%2520unreliable%2520intra-cluster%250Amodel%2520aggregation%2520and%2520inter-cluster%2520model%2520consensus.%2520First%252C%2520we%2520propose%2520an%250Aadaptive%2520hidden%2520semi-Markov%2520model%2520to%2520estimate%2520clients%2527%2520communication%2520states%2520and%250Acontributions.%2520Next%252C%2520we%2520address%2520a%2520client-server%2520association%2520optimization%250Aproblem%2520to%2520minimize%2520global%2520training%2520loss.%2520Using%2520convergence%2520analysis%252C%2520we%250Apropose%2520a%2520greedy%2520client%2520scheduling%2520algorithm.%2520Finally%252C%2520our%2520experiments%250Aconducted%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520TRAIL%2520outperforms%250Astate-of-the-art%2520baselines%252C%2520achieving%2520an%2520improvement%2520of%25208.7%2525%2520in%2520test%2520accuracy%250Aand%2520a%2520reduction%2520of%252015.3%2525%2520in%2520training%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11448v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRAIL%3A%20Trust-Aware%20Client%20Scheduling%20for%20Semi-Decentralized%20Federated%0A%20%20Learning&entry.906535625=Gangqiang%20Hu%20and%20Jianfeng%20Lu%20and%20Jianmin%20Han%20and%20Shuqin%20Cao%20and%20Jing%20Liu%20and%20Hao%20Fu&entry.1292438233=%20%20Due%20to%20the%20sensitivity%20of%20data%2C%20Federated%20Learning%20%28FL%29%20is%20employed%20to%20enable%0Adistributed%20machine%20learning%20while%20safeguarding%20data%20privacy%20and%20accommodating%0Athe%20requirements%20of%20various%20devices.%20However%2C%20in%20the%20context%20of%0Asemi-decentralized%20FL%2C%20clients%27%20communication%20and%20training%20states%20are%20dynamic.%0AThis%20variability%20arises%20from%20local%20training%20fluctuations%2C%20heterogeneous%20data%0Adistributions%2C%20and%20intermittent%20client%20participation.%20Most%20existing%20studies%0Aprimarily%20focus%20on%20stable%20client%20states%2C%20neglecting%20the%20dynamic%20challenges%0Ainherent%20in%20real-world%20scenarios.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%0ATRust-Aware%20clIent%20scheduLing%20mechanism%20called%20TRAIL%2C%20which%20assesses%20client%0Astates%20and%20contributions%2C%20enhancing%20model%20training%20efficiency%20through%20selective%0Aclient%20participation.%20We%20focus%20on%20a%20semi-decentralized%20FL%20framework%20where%20edge%0Aservers%20and%20clients%20train%20a%20shared%20global%20model%20using%20unreliable%20intra-cluster%0Amodel%20aggregation%20and%20inter-cluster%20model%20consensus.%20First%2C%20we%20propose%20an%0Aadaptive%20hidden%20semi-Markov%20model%20to%20estimate%20clients%27%20communication%20states%20and%0Acontributions.%20Next%2C%20we%20address%20a%20client-server%20association%20optimization%0Aproblem%20to%20minimize%20global%20training%20loss.%20Using%20convergence%20analysis%2C%20we%0Apropose%20a%20greedy%20client%20scheduling%20algorithm.%20Finally%2C%20our%20experiments%0Aconducted%20on%20real-world%20datasets%20demonstrate%20that%20TRAIL%20outperforms%0Astate-of-the-art%20baselines%2C%20achieving%20an%20improvement%20of%208.7%25%20in%20test%20accuracy%0Aand%20a%20reduction%20of%2015.3%25%20in%20training%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11448v3&entry.124074799=Read"},
{"title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis", "author": "Hanlin Wang and Hao Ouyang and Qiuyu Wang and Wen Wang and Ka Leong Cheng and Qifeng Chen and Yujun Shen and Limin Wang", "abstract": "  The intuitive nature of drag-based interaction has led to its growing\nadoption for controlling object trajectories in image-to-video synthesis.\nStill, existing methods that perform dragging in the 2D space usually face\nambiguity when handling out-of-plane movements. In this work, we augment the\ninteraction with a new dimension, i.e., the depth dimension, such that users\nare allowed to assign a relative depth for each point on the trajectory. That\nway, our new interaction paradigm not only inherits the convenience from 2D\ndragging, but facilitates trajectory control in the 3D space, broadening the\nscope of creativity. We propose a pioneering method for 3D trajectory control\nin image-to-video synthesis by abstracting object masks into a few cluster\npoints. These points, accompanied by the depth information and the instance\ninformation, are finally fed into a video diffusion model as the control\nsignal. Extensive experiments validate the effectiveness of our approach,\ndubbed LeviTor, in precisely manipulating the object movements when producing\nphoto-realistic videos from static images. Project page:\nhttps://ppetrichor.github.io/levitor.github.io/\n", "link": "http://arxiv.org/abs/2412.15214v1", "date": "2024-12-19", "relevancy": 2.3975, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6003}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5994}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeviTor%3A%203D%20Trajectory%20Oriented%20Image-to-Video%20Synthesis&body=Title%3A%20LeviTor%3A%203D%20Trajectory%20Oriented%20Image-to-Video%20Synthesis%0AAuthor%3A%20Hanlin%20Wang%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Qifeng%20Chen%20and%20Yujun%20Shen%20and%20Limin%20Wang%0AAbstract%3A%20%20%20The%20intuitive%20nature%20of%20drag-based%20interaction%20has%20led%20to%20its%20growing%0Aadoption%20for%20controlling%20object%20trajectories%20in%20image-to-video%20synthesis.%0AStill%2C%20existing%20methods%20that%20perform%20dragging%20in%20the%202D%20space%20usually%20face%0Aambiguity%20when%20handling%20out-of-plane%20movements.%20In%20this%20work%2C%20we%20augment%20the%0Ainteraction%20with%20a%20new%20dimension%2C%20i.e.%2C%20the%20depth%20dimension%2C%20such%20that%20users%0Aare%20allowed%20to%20assign%20a%20relative%20depth%20for%20each%20point%20on%20the%20trajectory.%20That%0Away%2C%20our%20new%20interaction%20paradigm%20not%20only%20inherits%20the%20convenience%20from%202D%0Adragging%2C%20but%20facilitates%20trajectory%20control%20in%20the%203D%20space%2C%20broadening%20the%0Ascope%20of%20creativity.%20We%20propose%20a%20pioneering%20method%20for%203D%20trajectory%20control%0Ain%20image-to-video%20synthesis%20by%20abstracting%20object%20masks%20into%20a%20few%20cluster%0Apoints.%20These%20points%2C%20accompanied%20by%20the%20depth%20information%20and%20the%20instance%0Ainformation%2C%20are%20finally%20fed%20into%20a%20video%20diffusion%20model%20as%20the%20control%0Asignal.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%2C%0Adubbed%20LeviTor%2C%20in%20precisely%20manipulating%20the%20object%20movements%20when%20producing%0Aphoto-realistic%20videos%20from%20static%20images.%20Project%20page%3A%0Ahttps%3A//ppetrichor.github.io/levitor.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeviTor%253A%25203D%2520Trajectory%2520Oriented%2520Image-to-Video%2520Synthesis%26entry.906535625%3DHanlin%2520Wang%2520and%2520Hao%2520Ouyang%2520and%2520Qiuyu%2520Wang%2520and%2520Wen%2520Wang%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Qifeng%2520Chen%2520and%2520Yujun%2520Shen%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520The%2520intuitive%2520nature%2520of%2520drag-based%2520interaction%2520has%2520led%2520to%2520its%2520growing%250Aadoption%2520for%2520controlling%2520object%2520trajectories%2520in%2520image-to-video%2520synthesis.%250AStill%252C%2520existing%2520methods%2520that%2520perform%2520dragging%2520in%2520the%25202D%2520space%2520usually%2520face%250Aambiguity%2520when%2520handling%2520out-of-plane%2520movements.%2520In%2520this%2520work%252C%2520we%2520augment%2520the%250Ainteraction%2520with%2520a%2520new%2520dimension%252C%2520i.e.%252C%2520the%2520depth%2520dimension%252C%2520such%2520that%2520users%250Aare%2520allowed%2520to%2520assign%2520a%2520relative%2520depth%2520for%2520each%2520point%2520on%2520the%2520trajectory.%2520That%250Away%252C%2520our%2520new%2520interaction%2520paradigm%2520not%2520only%2520inherits%2520the%2520convenience%2520from%25202D%250Adragging%252C%2520but%2520facilitates%2520trajectory%2520control%2520in%2520the%25203D%2520space%252C%2520broadening%2520the%250Ascope%2520of%2520creativity.%2520We%2520propose%2520a%2520pioneering%2520method%2520for%25203D%2520trajectory%2520control%250Ain%2520image-to-video%2520synthesis%2520by%2520abstracting%2520object%2520masks%2520into%2520a%2520few%2520cluster%250Apoints.%2520These%2520points%252C%2520accompanied%2520by%2520the%2520depth%2520information%2520and%2520the%2520instance%250Ainformation%252C%2520are%2520finally%2520fed%2520into%2520a%2520video%2520diffusion%2520model%2520as%2520the%2520control%250Asignal.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Adubbed%2520LeviTor%252C%2520in%2520precisely%2520manipulating%2520the%2520object%2520movements%2520when%2520producing%250Aphoto-realistic%2520videos%2520from%2520static%2520images.%2520Project%2520page%253A%250Ahttps%253A//ppetrichor.github.io/levitor.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeviTor%3A%203D%20Trajectory%20Oriented%20Image-to-Video%20Synthesis&entry.906535625=Hanlin%20Wang%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Qifeng%20Chen%20and%20Yujun%20Shen%20and%20Limin%20Wang&entry.1292438233=%20%20The%20intuitive%20nature%20of%20drag-based%20interaction%20has%20led%20to%20its%20growing%0Aadoption%20for%20controlling%20object%20trajectories%20in%20image-to-video%20synthesis.%0AStill%2C%20existing%20methods%20that%20perform%20dragging%20in%20the%202D%20space%20usually%20face%0Aambiguity%20when%20handling%20out-of-plane%20movements.%20In%20this%20work%2C%20we%20augment%20the%0Ainteraction%20with%20a%20new%20dimension%2C%20i.e.%2C%20the%20depth%20dimension%2C%20such%20that%20users%0Aare%20allowed%20to%20assign%20a%20relative%20depth%20for%20each%20point%20on%20the%20trajectory.%20That%0Away%2C%20our%20new%20interaction%20paradigm%20not%20only%20inherits%20the%20convenience%20from%202D%0Adragging%2C%20but%20facilitates%20trajectory%20control%20in%20the%203D%20space%2C%20broadening%20the%0Ascope%20of%20creativity.%20We%20propose%20a%20pioneering%20method%20for%203D%20trajectory%20control%0Ain%20image-to-video%20synthesis%20by%20abstracting%20object%20masks%20into%20a%20few%20cluster%0Apoints.%20These%20points%2C%20accompanied%20by%20the%20depth%20information%20and%20the%20instance%0Ainformation%2C%20are%20finally%20fed%20into%20a%20video%20diffusion%20model%20as%20the%20control%0Asignal.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%2C%0Adubbed%20LeviTor%2C%20in%20precisely%20manipulating%20the%20object%20movements%20when%20producing%0Aphoto-realistic%20videos%20from%20static%20images.%20Project%20page%3A%0Ahttps%3A//ppetrichor.github.io/levitor.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15214v1&entry.124074799=Read"},
{"title": "Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision", "author": "Chenxiang Jin and Jiajun Zhou and Chenxuan Xie and Shanqing Yu and Qi Xuan and Xiaoniu Yang", "abstract": "  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be available in\nhttps://github.com/GISec-Team/Meta-IFD.\n", "link": "http://arxiv.org/abs/2408.00641v2", "date": "2024-12-19", "relevancy": 2.3961, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5002}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.469}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision&body=Title%3A%20Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision%0AAuthor%3A%20Chenxiang%20Jin%20and%20Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang%0AAbstract%3A%20%20%20The%20rampant%20fraudulent%20activities%20on%20Ethereum%20hinder%20the%20healthy%20development%0Aof%20the%20blockchain%20ecosystem%2C%20necessitating%20the%20reinforcement%20of%20regulations.%0AHowever%2C%20multiple%20imbalances%20involving%20account%20interaction%20frequencies%20and%0Ainteraction%20types%20in%20the%20Ethereum%20transaction%20environment%20pose%20significant%0Achallenges%20to%20data%20mining-based%20fraud%20detection%20research.%20To%20address%20this%2C%20we%0Afirst%20propose%20the%20concept%20of%20meta-interactions%20to%20refine%20interaction%20behaviors%0Ain%20Ethereum%2C%20and%20based%20on%20this%2C%20we%20present%20a%20dual%20self-supervision%20enhanced%0AEthereum%20fraud%20detection%20framework%2C%20named%20Meta-IFD.%20This%20framework%20initially%0Aintroduces%20a%20generative%20self-supervision%20mechanism%20to%20augment%20the%20interaction%0Afeatures%20of%20accounts%2C%20followed%20by%20a%20contrastive%20self-supervision%20mechanism%20to%0Adifferentiate%20various%20behavior%20patterns%2C%20and%20ultimately%20characterizes%20the%0Abehavioral%20representations%20of%20accounts%20and%20mines%20potential%20fraud%20risks%20through%0Amulti-view%20interaction%20feature%20learning.%20Extensive%20experiments%20on%20real%20Ethereum%0Adatasets%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20framework%20in%0Adetecting%20common%20Ethereum%20fraud%20behaviors%20such%20as%20Ponzi%20schemes%20and%20phishing%0Ascams.%20Additionally%2C%20the%20generative%20module%20can%20effectively%20alleviate%20the%0Ainteraction%20distribution%20imbalance%20in%20Ethereum%20data%2C%20while%20the%20contrastive%0Amodule%20significantly%20enhances%20the%20framework%27s%20ability%20to%20distinguish%20different%0Abehavior%20patterns.%20The%20source%20code%20will%20be%20available%20in%0Ahttps%3A//github.com/GISec-Team/Meta-IFD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Ethereum%2520Fraud%2520Detection%2520via%2520Generative%2520and%2520Contrastive%250A%2520%2520Self-supervision%26entry.906535625%3DChenxiang%2520Jin%2520and%2520Jiajun%2520Zhou%2520and%2520Chenxuan%2520Xie%2520and%2520Shanqing%2520Yu%2520and%2520Qi%2520Xuan%2520and%2520Xiaoniu%2520Yang%26entry.1292438233%3D%2520%2520The%2520rampant%2520fraudulent%2520activities%2520on%2520Ethereum%2520hinder%2520the%2520healthy%2520development%250Aof%2520the%2520blockchain%2520ecosystem%252C%2520necessitating%2520the%2520reinforcement%2520of%2520regulations.%250AHowever%252C%2520multiple%2520imbalances%2520involving%2520account%2520interaction%2520frequencies%2520and%250Ainteraction%2520types%2520in%2520the%2520Ethereum%2520transaction%2520environment%2520pose%2520significant%250Achallenges%2520to%2520data%2520mining-based%2520fraud%2520detection%2520research.%2520To%2520address%2520this%252C%2520we%250Afirst%2520propose%2520the%2520concept%2520of%2520meta-interactions%2520to%2520refine%2520interaction%2520behaviors%250Ain%2520Ethereum%252C%2520and%2520based%2520on%2520this%252C%2520we%2520present%2520a%2520dual%2520self-supervision%2520enhanced%250AEthereum%2520fraud%2520detection%2520framework%252C%2520named%2520Meta-IFD.%2520This%2520framework%2520initially%250Aintroduces%2520a%2520generative%2520self-supervision%2520mechanism%2520to%2520augment%2520the%2520interaction%250Afeatures%2520of%2520accounts%252C%2520followed%2520by%2520a%2520contrastive%2520self-supervision%2520mechanism%2520to%250Adifferentiate%2520various%2520behavior%2520patterns%252C%2520and%2520ultimately%2520characterizes%2520the%250Abehavioral%2520representations%2520of%2520accounts%2520and%2520mines%2520potential%2520fraud%2520risks%2520through%250Amulti-view%2520interaction%2520feature%2520learning.%2520Extensive%2520experiments%2520on%2520real%2520Ethereum%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520framework%2520in%250Adetecting%2520common%2520Ethereum%2520fraud%2520behaviors%2520such%2520as%2520Ponzi%2520schemes%2520and%2520phishing%250Ascams.%2520Additionally%252C%2520the%2520generative%2520module%2520can%2520effectively%2520alleviate%2520the%250Ainteraction%2520distribution%2520imbalance%2520in%2520Ethereum%2520data%252C%2520while%2520the%2520contrastive%250Amodule%2520significantly%2520enhances%2520the%2520framework%2527s%2520ability%2520to%2520distinguish%2520different%250Abehavior%2520patterns.%2520The%2520source%2520code%2520will%2520be%2520available%2520in%250Ahttps%253A//github.com/GISec-Team/Meta-IFD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision&entry.906535625=Chenxiang%20Jin%20and%20Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang&entry.1292438233=%20%20The%20rampant%20fraudulent%20activities%20on%20Ethereum%20hinder%20the%20healthy%20development%0Aof%20the%20blockchain%20ecosystem%2C%20necessitating%20the%20reinforcement%20of%20regulations.%0AHowever%2C%20multiple%20imbalances%20involving%20account%20interaction%20frequencies%20and%0Ainteraction%20types%20in%20the%20Ethereum%20transaction%20environment%20pose%20significant%0Achallenges%20to%20data%20mining-based%20fraud%20detection%20research.%20To%20address%20this%2C%20we%0Afirst%20propose%20the%20concept%20of%20meta-interactions%20to%20refine%20interaction%20behaviors%0Ain%20Ethereum%2C%20and%20based%20on%20this%2C%20we%20present%20a%20dual%20self-supervision%20enhanced%0AEthereum%20fraud%20detection%20framework%2C%20named%20Meta-IFD.%20This%20framework%20initially%0Aintroduces%20a%20generative%20self-supervision%20mechanism%20to%20augment%20the%20interaction%0Afeatures%20of%20accounts%2C%20followed%20by%20a%20contrastive%20self-supervision%20mechanism%20to%0Adifferentiate%20various%20behavior%20patterns%2C%20and%20ultimately%20characterizes%20the%0Abehavioral%20representations%20of%20accounts%20and%20mines%20potential%20fraud%20risks%20through%0Amulti-view%20interaction%20feature%20learning.%20Extensive%20experiments%20on%20real%20Ethereum%0Adatasets%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20framework%20in%0Adetecting%20common%20Ethereum%20fraud%20behaviors%20such%20as%20Ponzi%20schemes%20and%20phishing%0Ascams.%20Additionally%2C%20the%20generative%20module%20can%20effectively%20alleviate%20the%0Ainteraction%20distribution%20imbalance%20in%20Ethereum%20data%2C%20while%20the%20contrastive%0Amodule%20significantly%20enhances%20the%20framework%27s%20ability%20to%20distinguish%20different%0Abehavior%20patterns.%20The%20source%20code%20will%20be%20available%20in%0Ahttps%3A//github.com/GISec-Team/Meta-IFD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00641v2&entry.124074799=Read"},
{"title": "AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation", "author": "Moayed Haji-Ali and Willi Menapace and Aliaksandr Siarohin and Ivan Skorokhodov and Alper Canberk and Kwot Sin Lee and Vicente Ordonez and Sergey Tulyakov", "abstract": "  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n", "link": "http://arxiv.org/abs/2412.15191v1", "date": "2024-12-19", "relevancy": 2.3888, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6216}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5961}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AV-Link%3A%20Temporally-Aligned%20Diffusion%20Features%20for%20Cross-Modal%0A%20%20Audio-Video%20Generation&body=Title%3A%20AV-Link%3A%20Temporally-Aligned%20Diffusion%20Features%20for%20Cross-Modal%0A%20%20Audio-Video%20Generation%0AAuthor%3A%20Moayed%20Haji-Ali%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Ivan%20Skorokhodov%20and%20Alper%20Canberk%20and%20Kwot%20Sin%20Lee%20and%20Vicente%20Ordonez%20and%20Sergey%20Tulyakov%0AAbstract%3A%20%20%20We%20propose%20AV-Link%2C%20a%20unified%20framework%20for%20Video-to-Audio%20and%20Audio-to-Video%0Ageneration%20that%20leverages%20the%20activations%20of%20frozen%20video%20and%20audio%20diffusion%0Amodels%20for%20temporally-aligned%20cross-modal%20conditioning.%20The%20key%20to%20our%0Aframework%20is%20a%20Fusion%20Block%20that%20enables%20bidirectional%20information%20exchange%0Abetween%20our%20backbone%20video%20and%20audio%20diffusion%20models%20through%20a%0Atemporally-aligned%20self%20attention%20operation.%20Unlike%20prior%20work%20that%20uses%0Afeature%20extractors%20pretrained%20for%20other%20tasks%20for%20the%20conditioning%20signal%2C%0AAV-Link%20can%20directly%20leverage%20features%20obtained%20by%20the%20complementary%20modality%0Ain%20a%20single%20framework%20i.e.%20video%20features%20to%20generate%20audio%2C%20or%20audio%20features%0Ato%20generate%20video.%20We%20extensively%20evaluate%20our%20design%20choices%20and%20demonstrate%0Athe%20ability%20of%20our%20method%20to%20achieve%20synchronized%20and%20high-quality%20audiovisual%0Acontent%2C%20showcasing%20its%20potential%20for%20applications%20in%20immersive%20media%0Ageneration.%20Project%20Page%3A%20snap-research.github.io/AVLink/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAV-Link%253A%2520Temporally-Aligned%2520Diffusion%2520Features%2520for%2520Cross-Modal%250A%2520%2520Audio-Video%2520Generation%26entry.906535625%3DMoayed%2520Haji-Ali%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Ivan%2520Skorokhodov%2520and%2520Alper%2520Canberk%2520and%2520Kwot%2520Sin%2520Lee%2520and%2520Vicente%2520Ordonez%2520and%2520Sergey%2520Tulyakov%26entry.1292438233%3D%2520%2520We%2520propose%2520AV-Link%252C%2520a%2520unified%2520framework%2520for%2520Video-to-Audio%2520and%2520Audio-to-Video%250Ageneration%2520that%2520leverages%2520the%2520activations%2520of%2520frozen%2520video%2520and%2520audio%2520diffusion%250Amodels%2520for%2520temporally-aligned%2520cross-modal%2520conditioning.%2520The%2520key%2520to%2520our%250Aframework%2520is%2520a%2520Fusion%2520Block%2520that%2520enables%2520bidirectional%2520information%2520exchange%250Abetween%2520our%2520backbone%2520video%2520and%2520audio%2520diffusion%2520models%2520through%2520a%250Atemporally-aligned%2520self%2520attention%2520operation.%2520Unlike%2520prior%2520work%2520that%2520uses%250Afeature%2520extractors%2520pretrained%2520for%2520other%2520tasks%2520for%2520the%2520conditioning%2520signal%252C%250AAV-Link%2520can%2520directly%2520leverage%2520features%2520obtained%2520by%2520the%2520complementary%2520modality%250Ain%2520a%2520single%2520framework%2520i.e.%2520video%2520features%2520to%2520generate%2520audio%252C%2520or%2520audio%2520features%250Ato%2520generate%2520video.%2520We%2520extensively%2520evaluate%2520our%2520design%2520choices%2520and%2520demonstrate%250Athe%2520ability%2520of%2520our%2520method%2520to%2520achieve%2520synchronized%2520and%2520high-quality%2520audiovisual%250Acontent%252C%2520showcasing%2520its%2520potential%2520for%2520applications%2520in%2520immersive%2520media%250Ageneration.%2520Project%2520Page%253A%2520snap-research.github.io/AVLink/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AV-Link%3A%20Temporally-Aligned%20Diffusion%20Features%20for%20Cross-Modal%0A%20%20Audio-Video%20Generation&entry.906535625=Moayed%20Haji-Ali%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Ivan%20Skorokhodov%20and%20Alper%20Canberk%20and%20Kwot%20Sin%20Lee%20and%20Vicente%20Ordonez%20and%20Sergey%20Tulyakov&entry.1292438233=%20%20We%20propose%20AV-Link%2C%20a%20unified%20framework%20for%20Video-to-Audio%20and%20Audio-to-Video%0Ageneration%20that%20leverages%20the%20activations%20of%20frozen%20video%20and%20audio%20diffusion%0Amodels%20for%20temporally-aligned%20cross-modal%20conditioning.%20The%20key%20to%20our%0Aframework%20is%20a%20Fusion%20Block%20that%20enables%20bidirectional%20information%20exchange%0Abetween%20our%20backbone%20video%20and%20audio%20diffusion%20models%20through%20a%0Atemporally-aligned%20self%20attention%20operation.%20Unlike%20prior%20work%20that%20uses%0Afeature%20extractors%20pretrained%20for%20other%20tasks%20for%20the%20conditioning%20signal%2C%0AAV-Link%20can%20directly%20leverage%20features%20obtained%20by%20the%20complementary%20modality%0Ain%20a%20single%20framework%20i.e.%20video%20features%20to%20generate%20audio%2C%20or%20audio%20features%0Ato%20generate%20video.%20We%20extensively%20evaluate%20our%20design%20choices%20and%20demonstrate%0Athe%20ability%20of%20our%20method%20to%20achieve%20synchronized%20and%20high-quality%20audiovisual%0Acontent%2C%20showcasing%20its%20potential%20for%20applications%20in%20immersive%20media%0Ageneration.%20Project%20Page%3A%20snap-research.github.io/AVLink/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15191v1&entry.124074799=Read"},
{"title": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages", "author": "Aman Chaturvedi and Daniel Nichols and Siddharth Singh and Abhinav Bhatele", "abstract": "  Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date.\n", "link": "http://arxiv.org/abs/2412.15178v1", "date": "2024-12-19", "relevancy": 2.3831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HPC-Coder-V2%3A%20Studying%20Code%20LLMs%20Across%20Low-Resource%20Parallel%20Languages&body=Title%3A%20HPC-Coder-V2%3A%20Studying%20Code%20LLMs%20Across%20Low-Resource%20Parallel%20Languages%0AAuthor%3A%20Aman%20Chaturvedi%20and%20Daniel%20Nichols%20and%20Siddharth%20Singh%20and%20Abhinav%20Bhatele%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20based%20coding%20tools%20have%20been%20tremendously%0Asuccessful%20as%20software%20development%20assistants%2C%20yet%20they%20are%20often%20designed%20for%0Ageneral%20purpose%20programming%20tasks%20and%20perform%20poorly%20for%20more%20specialized%0Adomains%20such%20as%20high%20performance%20computing.%20Creating%20specialized%20models%20and%0Atools%20for%20these%20domains%20is%20crucial%20towards%20gaining%20the%20benefits%20of%20LLMs%20in%0Aareas%20such%20as%20HPC.%20While%20previous%20work%20has%20explored%20HPC-specific%20models%2C%20LLMs%0Astill%20struggle%20to%20generate%20parallel%20code%20and%20it%20is%20not%20at%20all%20clear%20what%0Ahurdles%20are%20still%20holding%20back%20these%20LLMs%20and%20what%20must%20be%20done%20to%20overcome%0Athem.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20study%20along%20the%20many%20axes%20of%0Afine-tuning%20a%20specialized%20HPC%20LLM%20in%20order%20to%20better%20understand%20the%20challenges.%0ABased%20on%20our%20findings%20we%20fine-tune%20and%20evaluate%20a%20specialized%20HPC%20LLM%20that%20is%0Ashown%20to%20be%20the%20best%20performing%20open-source%20code%20LLM%20for%20parallel%20code%0Ageneration%20to%20date.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHPC-Coder-V2%253A%2520Studying%2520Code%2520LLMs%2520Across%2520Low-Resource%2520Parallel%2520Languages%26entry.906535625%3DAman%2520Chaturvedi%2520and%2520Daniel%2520Nichols%2520and%2520Siddharth%2520Singh%2520and%2520Abhinav%2520Bhatele%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520coding%2520tools%2520have%2520been%2520tremendously%250Asuccessful%2520as%2520software%2520development%2520assistants%252C%2520yet%2520they%2520are%2520often%2520designed%2520for%250Ageneral%2520purpose%2520programming%2520tasks%2520and%2520perform%2520poorly%2520for%2520more%2520specialized%250Adomains%2520such%2520as%2520high%2520performance%2520computing.%2520Creating%2520specialized%2520models%2520and%250Atools%2520for%2520these%2520domains%2520is%2520crucial%2520towards%2520gaining%2520the%2520benefits%2520of%2520LLMs%2520in%250Aareas%2520such%2520as%2520HPC.%2520While%2520previous%2520work%2520has%2520explored%2520HPC-specific%2520models%252C%2520LLMs%250Astill%2520struggle%2520to%2520generate%2520parallel%2520code%2520and%2520it%2520is%2520not%2520at%2520all%2520clear%2520what%250Ahurdles%2520are%2520still%2520holding%2520back%2520these%2520LLMs%2520and%2520what%2520must%2520be%2520done%2520to%2520overcome%250Athem.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520in-depth%2520study%2520along%2520the%2520many%2520axes%2520of%250Afine-tuning%2520a%2520specialized%2520HPC%2520LLM%2520in%2520order%2520to%2520better%2520understand%2520the%2520challenges.%250ABased%2520on%2520our%2520findings%2520we%2520fine-tune%2520and%2520evaluate%2520a%2520specialized%2520HPC%2520LLM%2520that%2520is%250Ashown%2520to%2520be%2520the%2520best%2520performing%2520open-source%2520code%2520LLM%2520for%2520parallel%2520code%250Ageneration%2520to%2520date.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HPC-Coder-V2%3A%20Studying%20Code%20LLMs%20Across%20Low-Resource%20Parallel%20Languages&entry.906535625=Aman%20Chaturvedi%20and%20Daniel%20Nichols%20and%20Siddharth%20Singh%20and%20Abhinav%20Bhatele&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20based%20coding%20tools%20have%20been%20tremendously%0Asuccessful%20as%20software%20development%20assistants%2C%20yet%20they%20are%20often%20designed%20for%0Ageneral%20purpose%20programming%20tasks%20and%20perform%20poorly%20for%20more%20specialized%0Adomains%20such%20as%20high%20performance%20computing.%20Creating%20specialized%20models%20and%0Atools%20for%20these%20domains%20is%20crucial%20towards%20gaining%20the%20benefits%20of%20LLMs%20in%0Aareas%20such%20as%20HPC.%20While%20previous%20work%20has%20explored%20HPC-specific%20models%2C%20LLMs%0Astill%20struggle%20to%20generate%20parallel%20code%20and%20it%20is%20not%20at%20all%20clear%20what%0Ahurdles%20are%20still%20holding%20back%20these%20LLMs%20and%20what%20must%20be%20done%20to%20overcome%0Athem.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20study%20along%20the%20many%20axes%20of%0Afine-tuning%20a%20specialized%20HPC%20LLM%20in%20order%20to%20better%20understand%20the%20challenges.%0ABased%20on%20our%20findings%20we%20fine-tune%20and%20evaluate%20a%20specialized%20HPC%20LLM%20that%20is%0Ashown%20to%20be%20the%20best%20performing%20open-source%20code%20LLM%20for%20parallel%20code%0Ageneration%20to%20date.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15178v1&entry.124074799=Read"},
{"title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models", "author": "Qirui Jiao and Daoyuan Chen and Yilun Huang and Bolin Ding and Yaliang Li and Ying Shen", "abstract": "  High-performance Multimodal Large Language Models (MLLMs) are heavily\ndependent on data quality. To advance fine-grained image recognition within\nMLLMs, we introduce a novel data synthesis method inspired by contrastive\nlearning and image difference captioning. Our key idea involves challenging the\nmodel to discern both matching and distinct elements by scrutinizing object\ndifferences in detailed regions across similar images. We begin by generating\npairs of similar images that emphasize object variations. Following this, we\nemploy a Difference Area Generator to pinpoint object differences, and\nsubsequently, a Difference Captions Generator to articulate these differences.\nThis process results in a high-quality dataset of \"object replacement\" samples,\ntermed Img-Diff, which can be scaled as needed due to its automated nature. We\nleverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,\nsuch as InternVL2, achieving substantial improvements across various image\ndifference and Visual Question Answering tasks. Notably, the trained models\nsignificantly outperform existing SOTA models like GPT-4V and Gemini on the\nMMVP benchmark. Additionally, we conduct comprehensive evaluations to validate\nthe dataset's diversity, quality, and robustness, offering several insights\ninto the synthesis of such contrastive datasets. We release our codes and\ndataset to encourage further research on multimodal data synthesis and MLLMs'\nfundamental capabilities for image understanding.\n", "link": "http://arxiv.org/abs/2408.04594v3", "date": "2024-12-19", "relevancy": 2.382, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6134}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Ying%20Shen%0AAbstract%3A%20%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20heavily%0Adependent%20on%20data%20quality.%20To%20advance%20fine-grained%20image%20recognition%20within%0AMLLMs%2C%20we%20introduce%20a%20novel%20data%20synthesis%20method%20inspired%20by%20contrastive%0Alearning%20and%20image%20difference%20captioning.%20Our%20key%20idea%20involves%20challenging%20the%0Amodel%20to%20discern%20both%20matching%20and%20distinct%20elements%20by%20scrutinizing%20object%0Adifferences%20in%20detailed%20regions%20across%20similar%20images.%20We%20begin%20by%20generating%0Apairs%20of%20similar%20images%20that%20emphasize%20object%20variations.%20Following%20this%2C%20we%0Aemploy%20a%20Difference%20Area%20Generator%20to%20pinpoint%20object%20differences%2C%20and%0Asubsequently%2C%20a%20Difference%20Captions%20Generator%20to%20articulate%20these%20differences.%0AThis%20process%20results%20in%20a%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples%2C%0Atermed%20Img-Diff%2C%20which%20can%20be%20scaled%20as%20needed%20due%20to%20its%20automated%20nature.%20We%0Aleverage%20this%20generated%20dataset%20to%20fine-tune%20state-of-the-art%20%28SOTA%29%20MLLMs%2C%0Asuch%20as%20InternVL2%2C%20achieving%20substantial%20improvements%20across%20various%20image%0Adifference%20and%20Visual%20Question%20Answering%20tasks.%20Notably%2C%20the%20trained%20models%0Asignificantly%20outperform%20existing%20SOTA%20models%20like%20GPT-4V%20and%20Gemini%20on%20the%0AMMVP%20benchmark.%20Additionally%2C%20we%20conduct%20comprehensive%20evaluations%20to%20validate%0Athe%20dataset%27s%20diversity%2C%20quality%2C%20and%20robustness%2C%20offering%20several%20insights%0Ainto%20the%20synthesis%20of%20such%20contrastive%20datasets.%20We%20release%20our%20codes%20and%0Adataset%20to%20encourage%20further%20research%20on%20multimodal%20data%20synthesis%20and%20MLLMs%27%0Afundamental%20capabilities%20for%20image%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04594v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImg-Diff%253A%2520Contrastive%2520Data%2520Synthesis%2520for%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DQirui%2520Jiao%2520and%2520Daoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Bolin%2520Ding%2520and%2520Yaliang%2520Li%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520High-performance%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520heavily%250Adependent%2520on%2520data%2520quality.%2520To%2520advance%2520fine-grained%2520image%2520recognition%2520within%250AMLLMs%252C%2520we%2520introduce%2520a%2520novel%2520data%2520synthesis%2520method%2520inspired%2520by%2520contrastive%250Alearning%2520and%2520image%2520difference%2520captioning.%2520Our%2520key%2520idea%2520involves%2520challenging%2520the%250Amodel%2520to%2520discern%2520both%2520matching%2520and%2520distinct%2520elements%2520by%2520scrutinizing%2520object%250Adifferences%2520in%2520detailed%2520regions%2520across%2520similar%2520images.%2520We%2520begin%2520by%2520generating%250Apairs%2520of%2520similar%2520images%2520that%2520emphasize%2520object%2520variations.%2520Following%2520this%252C%2520we%250Aemploy%2520a%2520Difference%2520Area%2520Generator%2520to%2520pinpoint%2520object%2520differences%252C%2520and%250Asubsequently%252C%2520a%2520Difference%2520Captions%2520Generator%2520to%2520articulate%2520these%2520differences.%250AThis%2520process%2520results%2520in%2520a%2520high-quality%2520dataset%2520of%2520%2522object%2520replacement%2522%2520samples%252C%250Atermed%2520Img-Diff%252C%2520which%2520can%2520be%2520scaled%2520as%2520needed%2520due%2520to%2520its%2520automated%2520nature.%2520We%250Aleverage%2520this%2520generated%2520dataset%2520to%2520fine-tune%2520state-of-the-art%2520%2528SOTA%2529%2520MLLMs%252C%250Asuch%2520as%2520InternVL2%252C%2520achieving%2520substantial%2520improvements%2520across%2520various%2520image%250Adifference%2520and%2520Visual%2520Question%2520Answering%2520tasks.%2520Notably%252C%2520the%2520trained%2520models%250Asignificantly%2520outperform%2520existing%2520SOTA%2520models%2520like%2520GPT-4V%2520and%2520Gemini%2520on%2520the%250AMMVP%2520benchmark.%2520Additionally%252C%2520we%2520conduct%2520comprehensive%2520evaluations%2520to%2520validate%250Athe%2520dataset%2527s%2520diversity%252C%2520quality%252C%2520and%2520robustness%252C%2520offering%2520several%2520insights%250Ainto%2520the%2520synthesis%2520of%2520such%2520contrastive%2520datasets.%2520We%2520release%2520our%2520codes%2520and%250Adataset%2520to%2520encourage%2520further%2520research%2520on%2520multimodal%2520data%2520synthesis%2520and%2520MLLMs%2527%250Afundamental%2520capabilities%2520for%2520image%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04594v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Ying%20Shen&entry.1292438233=%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20heavily%0Adependent%20on%20data%20quality.%20To%20advance%20fine-grained%20image%20recognition%20within%0AMLLMs%2C%20we%20introduce%20a%20novel%20data%20synthesis%20method%20inspired%20by%20contrastive%0Alearning%20and%20image%20difference%20captioning.%20Our%20key%20idea%20involves%20challenging%20the%0Amodel%20to%20discern%20both%20matching%20and%20distinct%20elements%20by%20scrutinizing%20object%0Adifferences%20in%20detailed%20regions%20across%20similar%20images.%20We%20begin%20by%20generating%0Apairs%20of%20similar%20images%20that%20emphasize%20object%20variations.%20Following%20this%2C%20we%0Aemploy%20a%20Difference%20Area%20Generator%20to%20pinpoint%20object%20differences%2C%20and%0Asubsequently%2C%20a%20Difference%20Captions%20Generator%20to%20articulate%20these%20differences.%0AThis%20process%20results%20in%20a%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples%2C%0Atermed%20Img-Diff%2C%20which%20can%20be%20scaled%20as%20needed%20due%20to%20its%20automated%20nature.%20We%0Aleverage%20this%20generated%20dataset%20to%20fine-tune%20state-of-the-art%20%28SOTA%29%20MLLMs%2C%0Asuch%20as%20InternVL2%2C%20achieving%20substantial%20improvements%20across%20various%20image%0Adifference%20and%20Visual%20Question%20Answering%20tasks.%20Notably%2C%20the%20trained%20models%0Asignificantly%20outperform%20existing%20SOTA%20models%20like%20GPT-4V%20and%20Gemini%20on%20the%0AMMVP%20benchmark.%20Additionally%2C%20we%20conduct%20comprehensive%20evaluations%20to%20validate%0Athe%20dataset%27s%20diversity%2C%20quality%2C%20and%20robustness%2C%20offering%20several%20insights%0Ainto%20the%20synthesis%20of%20such%20contrastive%20datasets.%20We%20release%20our%20codes%20and%0Adataset%20to%20encourage%20further%20research%20on%20multimodal%20data%20synthesis%20and%20MLLMs%27%0Afundamental%20capabilities%20for%20image%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04594v3&entry.124074799=Read"},
{"title": "TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for\n  Lazy Clients", "author": "Mengdi Wang and Anna Bodonhelyi and Efe Bozkir and Enkelejda Kasneci", "abstract": "  Federated learning is a distributed collaborative machine learning paradigm\nthat has gained strong momentum in recent years. In federated learning, a\ncentral server periodically coordinates models with clients and aggregates the\nmodels trained locally by clients without necessitating access to local data.\nDespite its potential, the implementation of federated learning continues to\nencounter several challenges, predominantly the slow convergence that is\nlargely due to data heterogeneity. The slow convergence becomes particularly\nproblematic in cross-device federated learning scenarios where clients may be\nstrongly limited by computing power and storage space, and hence counteracting\nmethods that induce additional computation or memory cost on the client side\nsuch as auxiliary objective terms and larger training iterations can be\nimpractical. In this paper, we propose a novel federated aggregation strategy,\nTurboSVM-FL, that poses no additional computation burden on the client side and\ncan significantly accelerate convergence for federated classification task,\nespecially when clients are \"lazy\" and train their models solely for few epochs\nfor next global aggregation. TurboSVM-FL extensively utilizes support vector\nmachine to conduct selective aggregation and max-margin spread-out\nregularization on class embeddings. We evaluate TurboSVM-FL on multiple\ndatasets including FEMNIST, CelebA, and Shakespeare using user-independent\nvalidation with non-iid data distribution. Our results show that TurboSVM-FL\ncan significantly outperform existing popular algorithms on convergence rate\nand reduce communication rounds while delivering better test metrics including\naccuracy, F1 score, and MCC.\n", "link": "http://arxiv.org/abs/2401.12012v5", "date": "2024-12-19", "relevancy": 2.3655, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4672}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients&body=Title%3A%20TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients%0AAuthor%3A%20Mengdi%20Wang%20and%20Anna%20Bodonhelyi%20and%20Efe%20Bozkir%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20distributed%20collaborative%20machine%20learning%20paradigm%0Athat%20has%20gained%20strong%20momentum%20in%20recent%20years.%20In%20federated%20learning%2C%20a%0Acentral%20server%20periodically%20coordinates%20models%20with%20clients%20and%20aggregates%20the%0Amodels%20trained%20locally%20by%20clients%20without%20necessitating%20access%20to%20local%20data.%0ADespite%20its%20potential%2C%20the%20implementation%20of%20federated%20learning%20continues%20to%0Aencounter%20several%20challenges%2C%20predominantly%20the%20slow%20convergence%20that%20is%0Alargely%20due%20to%20data%20heterogeneity.%20The%20slow%20convergence%20becomes%20particularly%0Aproblematic%20in%20cross-device%20federated%20learning%20scenarios%20where%20clients%20may%20be%0Astrongly%20limited%20by%20computing%20power%20and%20storage%20space%2C%20and%20hence%20counteracting%0Amethods%20that%20induce%20additional%20computation%20or%20memory%20cost%20on%20the%20client%20side%0Asuch%20as%20auxiliary%20objective%20terms%20and%20larger%20training%20iterations%20can%20be%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20federated%20aggregation%20strategy%2C%0ATurboSVM-FL%2C%20that%20poses%20no%20additional%20computation%20burden%20on%20the%20client%20side%20and%0Acan%20significantly%20accelerate%20convergence%20for%20federated%20classification%20task%2C%0Aespecially%20when%20clients%20are%20%22lazy%22%20and%20train%20their%20models%20solely%20for%20few%20epochs%0Afor%20next%20global%20aggregation.%20TurboSVM-FL%20extensively%20utilizes%20support%20vector%0Amachine%20to%20conduct%20selective%20aggregation%20and%20max-margin%20spread-out%0Aregularization%20on%20class%20embeddings.%20We%20evaluate%20TurboSVM-FL%20on%20multiple%0Adatasets%20including%20FEMNIST%2C%20CelebA%2C%20and%20Shakespeare%20using%20user-independent%0Avalidation%20with%20non-iid%20data%20distribution.%20Our%20results%20show%20that%20TurboSVM-FL%0Acan%20significantly%20outperform%20existing%20popular%20algorithms%20on%20convergence%20rate%0Aand%20reduce%20communication%20rounds%20while%20delivering%20better%20test%20metrics%20including%0Aaccuracy%2C%20F1%20score%2C%20and%20MCC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12012v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboSVM-FL%253A%2520Boosting%2520Federated%2520Learning%2520through%2520SVM%2520Aggregation%2520for%250A%2520%2520Lazy%2520Clients%26entry.906535625%3DMengdi%2520Wang%2520and%2520Anna%2520Bodonhelyi%2520and%2520Efe%2520Bozkir%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520Federated%2520learning%2520is%2520a%2520distributed%2520collaborative%2520machine%2520learning%2520paradigm%250Athat%2520has%2520gained%2520strong%2520momentum%2520in%2520recent%2520years.%2520In%2520federated%2520learning%252C%2520a%250Acentral%2520server%2520periodically%2520coordinates%2520models%2520with%2520clients%2520and%2520aggregates%2520the%250Amodels%2520trained%2520locally%2520by%2520clients%2520without%2520necessitating%2520access%2520to%2520local%2520data.%250ADespite%2520its%2520potential%252C%2520the%2520implementation%2520of%2520federated%2520learning%2520continues%2520to%250Aencounter%2520several%2520challenges%252C%2520predominantly%2520the%2520slow%2520convergence%2520that%2520is%250Alargely%2520due%2520to%2520data%2520heterogeneity.%2520The%2520slow%2520convergence%2520becomes%2520particularly%250Aproblematic%2520in%2520cross-device%2520federated%2520learning%2520scenarios%2520where%2520clients%2520may%2520be%250Astrongly%2520limited%2520by%2520computing%2520power%2520and%2520storage%2520space%252C%2520and%2520hence%2520counteracting%250Amethods%2520that%2520induce%2520additional%2520computation%2520or%2520memory%2520cost%2520on%2520the%2520client%2520side%250Asuch%2520as%2520auxiliary%2520objective%2520terms%2520and%2520larger%2520training%2520iterations%2520can%2520be%250Aimpractical.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520federated%2520aggregation%2520strategy%252C%250ATurboSVM-FL%252C%2520that%2520poses%2520no%2520additional%2520computation%2520burden%2520on%2520the%2520client%2520side%2520and%250Acan%2520significantly%2520accelerate%2520convergence%2520for%2520federated%2520classification%2520task%252C%250Aespecially%2520when%2520clients%2520are%2520%2522lazy%2522%2520and%2520train%2520their%2520models%2520solely%2520for%2520few%2520epochs%250Afor%2520next%2520global%2520aggregation.%2520TurboSVM-FL%2520extensively%2520utilizes%2520support%2520vector%250Amachine%2520to%2520conduct%2520selective%2520aggregation%2520and%2520max-margin%2520spread-out%250Aregularization%2520on%2520class%2520embeddings.%2520We%2520evaluate%2520TurboSVM-FL%2520on%2520multiple%250Adatasets%2520including%2520FEMNIST%252C%2520CelebA%252C%2520and%2520Shakespeare%2520using%2520user-independent%250Avalidation%2520with%2520non-iid%2520data%2520distribution.%2520Our%2520results%2520show%2520that%2520TurboSVM-FL%250Acan%2520significantly%2520outperform%2520existing%2520popular%2520algorithms%2520on%2520convergence%2520rate%250Aand%2520reduce%2520communication%2520rounds%2520while%2520delivering%2520better%2520test%2520metrics%2520including%250Aaccuracy%252C%2520F1%2520score%252C%2520and%2520MCC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12012v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboSVM-FL%3A%20Boosting%20Federated%20Learning%20through%20SVM%20Aggregation%20for%0A%20%20Lazy%20Clients&entry.906535625=Mengdi%20Wang%20and%20Anna%20Bodonhelyi%20and%20Efe%20Bozkir%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20Federated%20learning%20is%20a%20distributed%20collaborative%20machine%20learning%20paradigm%0Athat%20has%20gained%20strong%20momentum%20in%20recent%20years.%20In%20federated%20learning%2C%20a%0Acentral%20server%20periodically%20coordinates%20models%20with%20clients%20and%20aggregates%20the%0Amodels%20trained%20locally%20by%20clients%20without%20necessitating%20access%20to%20local%20data.%0ADespite%20its%20potential%2C%20the%20implementation%20of%20federated%20learning%20continues%20to%0Aencounter%20several%20challenges%2C%20predominantly%20the%20slow%20convergence%20that%20is%0Alargely%20due%20to%20data%20heterogeneity.%20The%20slow%20convergence%20becomes%20particularly%0Aproblematic%20in%20cross-device%20federated%20learning%20scenarios%20where%20clients%20may%20be%0Astrongly%20limited%20by%20computing%20power%20and%20storage%20space%2C%20and%20hence%20counteracting%0Amethods%20that%20induce%20additional%20computation%20or%20memory%20cost%20on%20the%20client%20side%0Asuch%20as%20auxiliary%20objective%20terms%20and%20larger%20training%20iterations%20can%20be%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20federated%20aggregation%20strategy%2C%0ATurboSVM-FL%2C%20that%20poses%20no%20additional%20computation%20burden%20on%20the%20client%20side%20and%0Acan%20significantly%20accelerate%20convergence%20for%20federated%20classification%20task%2C%0Aespecially%20when%20clients%20are%20%22lazy%22%20and%20train%20their%20models%20solely%20for%20few%20epochs%0Afor%20next%20global%20aggregation.%20TurboSVM-FL%20extensively%20utilizes%20support%20vector%0Amachine%20to%20conduct%20selective%20aggregation%20and%20max-margin%20spread-out%0Aregularization%20on%20class%20embeddings.%20We%20evaluate%20TurboSVM-FL%20on%20multiple%0Adatasets%20including%20FEMNIST%2C%20CelebA%2C%20and%20Shakespeare%20using%20user-independent%0Avalidation%20with%20non-iid%20data%20distribution.%20Our%20results%20show%20that%20TurboSVM-FL%0Acan%20significantly%20outperform%20existing%20popular%20algorithms%20on%20convergence%20rate%0Aand%20reduce%20communication%20rounds%20while%20delivering%20better%20test%20metrics%20including%0Aaccuracy%2C%20F1%20score%2C%20and%20MCC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12012v5&entry.124074799=Read"},
{"title": "Parallelized Autoregressive Visual Generation", "author": "Yuqing Wang and Shuhuai Ren and Zhijie Lin and Yujin Han and Haoyuan Guo and Zhenheng Yang and Difan Zou and Jiashi Feng and Xihui Liu", "abstract": "  Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.\n", "link": "http://arxiv.org/abs/2412.15119v1", "date": "2024-12-19", "relevancy": 2.3244, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5975}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5748}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallelized%20Autoregressive%20Visual%20Generation&body=Title%3A%20Parallelized%20Autoregressive%20Visual%20Generation%0AAuthor%3A%20Yuqing%20Wang%20and%20Shuhuai%20Ren%20and%20Zhijie%20Lin%20and%20Yujin%20Han%20and%20Haoyuan%20Guo%20and%20Zhenheng%20Yang%20and%20Difan%20Zou%20and%20Jiashi%20Feng%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Autoregressive%20models%20have%20emerged%20as%20a%20powerful%20approach%20for%20visual%0Ageneration%20but%20suffer%20from%20slow%20inference%20speed%20due%20to%20their%20sequential%0Atoken-by-token%20prediction%20process.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20approach%20for%20parallelized%20autoregressive%20visual%20generation%20that%0Aimproves%20generation%20efficiency%20while%20preserving%20the%20advantages%20of%0Aautoregressive%20modeling.%20Our%20key%20insight%20is%20that%20parallel%20generation%20depends%20on%0Avisual%20token%20dependencies-tokens%20with%20weak%20dependencies%20can%20be%20generated%20in%0Aparallel%2C%20while%20strongly%20dependent%20adjacent%20tokens%20are%20difficult%20to%20generate%0Atogether%2C%20as%20their%20independent%20sampling%20may%20lead%20to%20inconsistencies.%20Based%20on%0Athis%20observation%2C%20we%20develop%20a%20parallel%20generation%20strategy%20that%20generates%0Adistant%20tokens%20with%20weak%20dependencies%20in%20parallel%20while%20maintaining%20sequential%0Ageneration%20for%20strongly%20dependent%20local%20tokens.%20Our%20approach%20can%20be%20seamlessly%0Aintegrated%20into%20standard%20autoregressive%20models%20without%20modifying%20the%0Aarchitecture%20or%20tokenizer.%20Experiments%20on%20ImageNet%20and%20UCF-101%20demonstrate%20that%0Aour%20method%20achieves%20a%203.6x%20speedup%20with%20comparable%20quality%20and%20up%20to%209.5x%0Aspeedup%20with%20minimal%20quality%20degradation%20across%20both%20image%20and%20video%20generation%0Atasks.%20We%20hope%20this%20work%20will%20inspire%20future%20research%20in%20efficient%20visual%0Ageneration%20and%20unified%20autoregressive%20modeling.%20Project%20page%3A%0Ahttps%3A//epiphqny.github.io/PAR-project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelized%2520Autoregressive%2520Visual%2520Generation%26entry.906535625%3DYuqing%2520Wang%2520and%2520Shuhuai%2520Ren%2520and%2520Zhijie%2520Lin%2520and%2520Yujin%2520Han%2520and%2520Haoyuan%2520Guo%2520and%2520Zhenheng%2520Yang%2520and%2520Difan%2520Zou%2520and%2520Jiashi%2520Feng%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Autoregressive%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520approach%2520for%2520visual%250Ageneration%2520but%2520suffer%2520from%2520slow%2520inference%2520speed%2520due%2520to%2520their%2520sequential%250Atoken-by-token%2520prediction%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aeffective%2520approach%2520for%2520parallelized%2520autoregressive%2520visual%2520generation%2520that%250Aimproves%2520generation%2520efficiency%2520while%2520preserving%2520the%2520advantages%2520of%250Aautoregressive%2520modeling.%2520Our%2520key%2520insight%2520is%2520that%2520parallel%2520generation%2520depends%2520on%250Avisual%2520token%2520dependencies-tokens%2520with%2520weak%2520dependencies%2520can%2520be%2520generated%2520in%250Aparallel%252C%2520while%2520strongly%2520dependent%2520adjacent%2520tokens%2520are%2520difficult%2520to%2520generate%250Atogether%252C%2520as%2520their%2520independent%2520sampling%2520may%2520lead%2520to%2520inconsistencies.%2520Based%2520on%250Athis%2520observation%252C%2520we%2520develop%2520a%2520parallel%2520generation%2520strategy%2520that%2520generates%250Adistant%2520tokens%2520with%2520weak%2520dependencies%2520in%2520parallel%2520while%2520maintaining%2520sequential%250Ageneration%2520for%2520strongly%2520dependent%2520local%2520tokens.%2520Our%2520approach%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520standard%2520autoregressive%2520models%2520without%2520modifying%2520the%250Aarchitecture%2520or%2520tokenizer.%2520Experiments%2520on%2520ImageNet%2520and%2520UCF-101%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520a%25203.6x%2520speedup%2520with%2520comparable%2520quality%2520and%2520up%2520to%25209.5x%250Aspeedup%2520with%2520minimal%2520quality%2520degradation%2520across%2520both%2520image%2520and%2520video%2520generation%250Atasks.%2520We%2520hope%2520this%2520work%2520will%2520inspire%2520future%2520research%2520in%2520efficient%2520visual%250Ageneration%2520and%2520unified%2520autoregressive%2520modeling.%2520Project%2520page%253A%250Ahttps%253A//epiphqny.github.io/PAR-project.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelized%20Autoregressive%20Visual%20Generation&entry.906535625=Yuqing%20Wang%20and%20Shuhuai%20Ren%20and%20Zhijie%20Lin%20and%20Yujin%20Han%20and%20Haoyuan%20Guo%20and%20Zhenheng%20Yang%20and%20Difan%20Zou%20and%20Jiashi%20Feng%20and%20Xihui%20Liu&entry.1292438233=%20%20Autoregressive%20models%20have%20emerged%20as%20a%20powerful%20approach%20for%20visual%0Ageneration%20but%20suffer%20from%20slow%20inference%20speed%20due%20to%20their%20sequential%0Atoken-by-token%20prediction%20process.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20approach%20for%20parallelized%20autoregressive%20visual%20generation%20that%0Aimproves%20generation%20efficiency%20while%20preserving%20the%20advantages%20of%0Aautoregressive%20modeling.%20Our%20key%20insight%20is%20that%20parallel%20generation%20depends%20on%0Avisual%20token%20dependencies-tokens%20with%20weak%20dependencies%20can%20be%20generated%20in%0Aparallel%2C%20while%20strongly%20dependent%20adjacent%20tokens%20are%20difficult%20to%20generate%0Atogether%2C%20as%20their%20independent%20sampling%20may%20lead%20to%20inconsistencies.%20Based%20on%0Athis%20observation%2C%20we%20develop%20a%20parallel%20generation%20strategy%20that%20generates%0Adistant%20tokens%20with%20weak%20dependencies%20in%20parallel%20while%20maintaining%20sequential%0Ageneration%20for%20strongly%20dependent%20local%20tokens.%20Our%20approach%20can%20be%20seamlessly%0Aintegrated%20into%20standard%20autoregressive%20models%20without%20modifying%20the%0Aarchitecture%20or%20tokenizer.%20Experiments%20on%20ImageNet%20and%20UCF-101%20demonstrate%20that%0Aour%20method%20achieves%20a%203.6x%20speedup%20with%20comparable%20quality%20and%20up%20to%209.5x%0Aspeedup%20with%20minimal%20quality%20degradation%20across%20both%20image%20and%20video%20generation%0Atasks.%20We%20hope%20this%20work%20will%20inspire%20future%20research%20in%20efficient%20visual%0Ageneration%20and%20unified%20autoregressive%20modeling.%20Project%20page%3A%0Ahttps%3A//epiphqny.github.io/PAR-project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15119v1&entry.124074799=Read"},
{"title": "Scaling 4D Representations", "author": "Jo\u00e3o Carreira and Dilara Gokay and Michael King and Chuhan Zhang and Ignacio Rocco and Aravindh Mahendran and Thomas Albert Keck and Joseph Heyward and Skanda Koppula and Etienne Pot and Goker Erdogan and Yana Hasson and Yi Yang and Klaus Greff and Guillaume Le Moing and Sjoerd van Steenkiste and Daniel Zoran and Drew A. Hudson and Pedro V\u00e9lez and Luisa Polan\u00eda and Luke Friedman and Chris Duvarney and Ross Goroshin and Kelsey Allen and Jacob Walker and Rishabh Kabra and Eric Aboussouan and Jennifer Sun and Thomas Kipf and Carl Doersch and Viorica P\u0103tr\u0103ucean and Dima Damen and Pauline Luc and Mehdi S. M. Sajjadi and Andrew Zisserman", "abstract": "  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n", "link": "http://arxiv.org/abs/2412.15212v1", "date": "2024-12-19", "relevancy": 2.3243, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.596}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%204D%20Representations&body=Title%3A%20Scaling%204D%20Representations%0AAuthor%3A%20Jo%C3%A3o%20Carreira%20and%20Dilara%20Gokay%20and%20Michael%20King%20and%20Chuhan%20Zhang%20and%20Ignacio%20Rocco%20and%20Aravindh%20Mahendran%20and%20Thomas%20Albert%20Keck%20and%20Joseph%20Heyward%20and%20Skanda%20Koppula%20and%20Etienne%20Pot%20and%20Goker%20Erdogan%20and%20Yana%20Hasson%20and%20Yi%20Yang%20and%20Klaus%20Greff%20and%20Guillaume%20Le%20Moing%20and%20Sjoerd%20van%20Steenkiste%20and%20Daniel%20Zoran%20and%20Drew%20A.%20Hudson%20and%20Pedro%20V%C3%A9lez%20and%20Luisa%20Polan%C3%ADa%20and%20Luke%20Friedman%20and%20Chris%20Duvarney%20and%20Ross%20Goroshin%20and%20Kelsey%20Allen%20and%20Jacob%20Walker%20and%20Rishabh%20Kabra%20and%20Eric%20Aboussouan%20and%20Jennifer%20Sun%20and%20Thomas%20Kipf%20and%20Carl%20Doersch%20and%20Viorica%20P%C4%83tr%C4%83ucean%20and%20Dima%20Damen%20and%20Pauline%20Luc%20and%20Mehdi%20S.%20M.%20Sajjadi%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20Scaling%20has%20not%20yet%20been%20convincingly%20demonstrated%20for%20pure%20self-supervised%0Alearning%20from%20video.%20However%2C%20prior%20work%20has%20focused%20evaluations%20on%0Asemantic-related%20tasks%20%24%5Cunicode%7Bx2013%7D%24%20action%20classification%2C%20ImageNet%0Aclassification%2C%20etc.%20In%20this%20paper%20we%20focus%20on%20evaluating%20self-supervised%0Alearning%20on%20non-semantic%20vision%20tasks%20that%20are%20more%20spatial%20%283D%29%20and%20temporal%0A%28%2B1D%20%3D%204D%29%2C%20such%20as%20camera%20pose%20estimation%2C%20point%20and%20object%20tracking%2C%20and%0Adepth%20estimation.%20We%20show%20that%20by%20learning%20from%20very%20large%20video%20datasets%2C%0Amasked%20auto-encoding%20%28MAE%29%20with%20transformer%20video%20models%20actually%20scales%2C%0Aconsistently%20improving%20performance%20on%20these%204D%20tasks%2C%20as%20model%20size%20increases%0Afrom%2020M%20all%20the%20way%20to%20the%20largest%20by%20far%20reported%20self-supervised%20video%20model%0A%24%5Cunicode%7Bx2013%7D%24%2022B%20parameters.%20Rigorous%20apples-to-apples%20comparison%20with%0Amany%20recent%20image%20and%20video%20models%20demonstrates%20the%20benefits%20of%20scaling%204D%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%25204D%2520Representations%26entry.906535625%3DJo%25C3%25A3o%2520Carreira%2520and%2520Dilara%2520Gokay%2520and%2520Michael%2520King%2520and%2520Chuhan%2520Zhang%2520and%2520Ignacio%2520Rocco%2520and%2520Aravindh%2520Mahendran%2520and%2520Thomas%2520Albert%2520Keck%2520and%2520Joseph%2520Heyward%2520and%2520Skanda%2520Koppula%2520and%2520Etienne%2520Pot%2520and%2520Goker%2520Erdogan%2520and%2520Yana%2520Hasson%2520and%2520Yi%2520Yang%2520and%2520Klaus%2520Greff%2520and%2520Guillaume%2520Le%2520Moing%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Daniel%2520Zoran%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Pedro%2520V%25C3%25A9lez%2520and%2520Luisa%2520Polan%25C3%25ADa%2520and%2520Luke%2520Friedman%2520and%2520Chris%2520Duvarney%2520and%2520Ross%2520Goroshin%2520and%2520Kelsey%2520Allen%2520and%2520Jacob%2520Walker%2520and%2520Rishabh%2520Kabra%2520and%2520Eric%2520Aboussouan%2520and%2520Jennifer%2520Sun%2520and%2520Thomas%2520Kipf%2520and%2520Carl%2520Doersch%2520and%2520Viorica%2520P%25C4%2583tr%25C4%2583ucean%2520and%2520Dima%2520Damen%2520and%2520Pauline%2520Luc%2520and%2520Mehdi%2520S.%2520M.%2520Sajjadi%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520Scaling%2520has%2520not%2520yet%2520been%2520convincingly%2520demonstrated%2520for%2520pure%2520self-supervised%250Alearning%2520from%2520video.%2520However%252C%2520prior%2520work%2520has%2520focused%2520evaluations%2520on%250Asemantic-related%2520tasks%2520%2524%255Cunicode%257Bx2013%257D%2524%2520action%2520classification%252C%2520ImageNet%250Aclassification%252C%2520etc.%2520In%2520this%2520paper%2520we%2520focus%2520on%2520evaluating%2520self-supervised%250Alearning%2520on%2520non-semantic%2520vision%2520tasks%2520that%2520are%2520more%2520spatial%2520%25283D%2529%2520and%2520temporal%250A%2528%252B1D%2520%253D%25204D%2529%252C%2520such%2520as%2520camera%2520pose%2520estimation%252C%2520point%2520and%2520object%2520tracking%252C%2520and%250Adepth%2520estimation.%2520We%2520show%2520that%2520by%2520learning%2520from%2520very%2520large%2520video%2520datasets%252C%250Amasked%2520auto-encoding%2520%2528MAE%2529%2520with%2520transformer%2520video%2520models%2520actually%2520scales%252C%250Aconsistently%2520improving%2520performance%2520on%2520these%25204D%2520tasks%252C%2520as%2520model%2520size%2520increases%250Afrom%252020M%2520all%2520the%2520way%2520to%2520the%2520largest%2520by%2520far%2520reported%2520self-supervised%2520video%2520model%250A%2524%255Cunicode%257Bx2013%257D%2524%252022B%2520parameters.%2520Rigorous%2520apples-to-apples%2520comparison%2520with%250Amany%2520recent%2520image%2520and%2520video%2520models%2520demonstrates%2520the%2520benefits%2520of%2520scaling%25204D%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%204D%20Representations&entry.906535625=Jo%C3%A3o%20Carreira%20and%20Dilara%20Gokay%20and%20Michael%20King%20and%20Chuhan%20Zhang%20and%20Ignacio%20Rocco%20and%20Aravindh%20Mahendran%20and%20Thomas%20Albert%20Keck%20and%20Joseph%20Heyward%20and%20Skanda%20Koppula%20and%20Etienne%20Pot%20and%20Goker%20Erdogan%20and%20Yana%20Hasson%20and%20Yi%20Yang%20and%20Klaus%20Greff%20and%20Guillaume%20Le%20Moing%20and%20Sjoerd%20van%20Steenkiste%20and%20Daniel%20Zoran%20and%20Drew%20A.%20Hudson%20and%20Pedro%20V%C3%A9lez%20and%20Luisa%20Polan%C3%ADa%20and%20Luke%20Friedman%20and%20Chris%20Duvarney%20and%20Ross%20Goroshin%20and%20Kelsey%20Allen%20and%20Jacob%20Walker%20and%20Rishabh%20Kabra%20and%20Eric%20Aboussouan%20and%20Jennifer%20Sun%20and%20Thomas%20Kipf%20and%20Carl%20Doersch%20and%20Viorica%20P%C4%83tr%C4%83ucean%20and%20Dima%20Damen%20and%20Pauline%20Luc%20and%20Mehdi%20S.%20M.%20Sajjadi%20and%20Andrew%20Zisserman&entry.1292438233=%20%20Scaling%20has%20not%20yet%20been%20convincingly%20demonstrated%20for%20pure%20self-supervised%0Alearning%20from%20video.%20However%2C%20prior%20work%20has%20focused%20evaluations%20on%0Asemantic-related%20tasks%20%24%5Cunicode%7Bx2013%7D%24%20action%20classification%2C%20ImageNet%0Aclassification%2C%20etc.%20In%20this%20paper%20we%20focus%20on%20evaluating%20self-supervised%0Alearning%20on%20non-semantic%20vision%20tasks%20that%20are%20more%20spatial%20%283D%29%20and%20temporal%0A%28%2B1D%20%3D%204D%29%2C%20such%20as%20camera%20pose%20estimation%2C%20point%20and%20object%20tracking%2C%20and%0Adepth%20estimation.%20We%20show%20that%20by%20learning%20from%20very%20large%20video%20datasets%2C%0Amasked%20auto-encoding%20%28MAE%29%20with%20transformer%20video%20models%20actually%20scales%2C%0Aconsistently%20improving%20performance%20on%20these%204D%20tasks%2C%20as%20model%20size%20increases%0Afrom%2020M%20all%20the%20way%20to%20the%20largest%20by%20far%20reported%20self-supervised%20video%20model%0A%24%5Cunicode%7Bx2013%7D%24%2022B%20parameters.%20Rigorous%20apples-to-apples%20comparison%20with%0Amany%20recent%20image%20and%20video%20models%20demonstrates%20the%20benefits%20of%20scaling%204D%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15212v1&entry.124074799=Read"},
{"title": "SCB-dataset: A Dataset for Detecting Student Classroom Behavior", "author": "Fan Yang", "abstract": "  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n", "link": "http://arxiv.org/abs/2304.02488v4", "date": "2024-12-19", "relevancy": 2.3083, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4813}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior&body=Title%3A%20SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior%0AAuthor%3A%20Fan%20Yang%0AAbstract%3A%20%20%20The%20use%20of%20deep%20learning%20methods%20for%20automatic%20detection%20of%20students%27%0Aclassroom%20behavior%20is%20a%20promising%20approach%20to%20analyze%20their%20class%20performance%0Aand%20enhance%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset%29%20that%20reflects%20real-life%20scenarios.%20Our%20dataset%20includes%2011%2C248%0Alabels%20and%204%2C003%20images%2C%20with%20a%20focus%20on%20hand-raising%20behavior.%20We%20evaluated%0Athe%20dataset%20using%20the%20YOLOv7%20algorithm%2C%20achieving%20a%20mean%20average%20precision%0A%28map%29%20of%20up%20to%2085.3%25.%20We%20believe%20that%20our%20dataset%20can%20serve%20as%20a%20robust%0Afoundation%20for%20future%20research%20in%20the%20field%20of%20student%20behavior%20detection%20and%0Apromote%20further%20advancements%20in%20this%20area.Our%20SCB-dataset%20can%20be%20downloaded%0Afrom%3A%20https%3A//github.com/Whiffe/SCB-dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02488v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCB-dataset%253A%2520A%2520Dataset%2520for%2520Detecting%2520Student%2520Classroom%2520Behavior%26entry.906535625%3DFan%2520Yang%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520deep%2520learning%2520methods%2520for%2520automatic%2520detection%2520of%2520students%2527%250Aclassroom%2520behavior%2520is%2520a%2520promising%2520approach%2520to%2520analyze%2520their%2520class%2520performance%250Aand%2520enhance%2520teaching%2520effectiveness.%2520However%252C%2520the%2520lack%2520of%2520publicly%2520available%250Adatasets%2520on%2520student%2520behavior%2520poses%2520a%2520challenge%2520for%2520researchers%2520in%2520this%2520field.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Student%2520Classroom%2520Behavior%2520dataset%250A%2528SCB-dataset%2529%2520that%2520reflects%2520real-life%2520scenarios.%2520Our%2520dataset%2520includes%252011%252C248%250Alabels%2520and%25204%252C003%2520images%252C%2520with%2520a%2520focus%2520on%2520hand-raising%2520behavior.%2520We%2520evaluated%250Athe%2520dataset%2520using%2520the%2520YOLOv7%2520algorithm%252C%2520achieving%2520a%2520mean%2520average%2520precision%250A%2528map%2529%2520of%2520up%2520to%252085.3%2525.%2520We%2520believe%2520that%2520our%2520dataset%2520can%2520serve%2520as%2520a%2520robust%250Afoundation%2520for%2520future%2520research%2520in%2520the%2520field%2520of%2520student%2520behavior%2520detection%2520and%250Apromote%2520further%2520advancements%2520in%2520this%2520area.Our%2520SCB-dataset%2520can%2520be%2520downloaded%250Afrom%253A%2520https%253A//github.com/Whiffe/SCB-dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02488v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCB-dataset%3A%20A%20Dataset%20for%20Detecting%20Student%20Classroom%20Behavior&entry.906535625=Fan%20Yang&entry.1292438233=%20%20The%20use%20of%20deep%20learning%20methods%20for%20automatic%20detection%20of%20students%27%0Aclassroom%20behavior%20is%20a%20promising%20approach%20to%20analyze%20their%20class%20performance%0Aand%20enhance%20teaching%20effectiveness.%20However%2C%20the%20lack%20of%20publicly%20available%0Adatasets%20on%20student%20behavior%20poses%20a%20challenge%20for%20researchers%20in%20this%20field.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20Student%20Classroom%20Behavior%20dataset%0A%28SCB-dataset%29%20that%20reflects%20real-life%20scenarios.%20Our%20dataset%20includes%2011%2C248%0Alabels%20and%204%2C003%20images%2C%20with%20a%20focus%20on%20hand-raising%20behavior.%20We%20evaluated%0Athe%20dataset%20using%20the%20YOLOv7%20algorithm%2C%20achieving%20a%20mean%20average%20precision%0A%28map%29%20of%20up%20to%2085.3%25.%20We%20believe%20that%20our%20dataset%20can%20serve%20as%20a%20robust%0Afoundation%20for%20future%20research%20in%20the%20field%20of%20student%20behavior%20detection%20and%0Apromote%20further%20advancements%20in%20this%20area.Our%20SCB-dataset%20can%20be%20downloaded%0Afrom%3A%20https%3A//github.com/Whiffe/SCB-dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02488v4&entry.124074799=Read"},
{"title": "GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images", "author": "Ziyang Xu and Huangxuan Zhao and Wenyu Liu and Xinggang Wang", "abstract": "  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n", "link": "http://arxiv.org/abs/2412.14118v2", "date": "2024-12-19", "relevancy": 2.2926, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5952}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5603}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images&body=Title%3A%20GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images%0AAuthor%3A%20Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20The%20rapid%20and%20accurate%20direct%20multi-frame%20interpolation%20method%20for%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20images%20is%20crucial%20for%20reducing%20radiation%20and%0Aproviding%20real-time%20assistance%20to%20physicians%20for%20precise%20diagnostics%20and%0Atreatment.%20DSA%20images%20contain%20complex%20vascular%20structures%20and%20various%20motions.%0AApplying%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%20methods%20results%20in%0Amotion%20artifacts%2C%20structural%20dissipation%2C%20and%20blurriness.%20Recently%2C%20MoSt-DSA%0Ahas%20specifically%20addressed%20these%20issues%20for%20the%20first%20time%20and%20achieved%20SOTA%0Aresults.%20However%2C%20MoSt-DSA%27s%20focus%20on%20real-time%20performance%20leads%20to%0Ainsufficient%20suppression%20of%20high-frequency%20noise%20and%20incomplete%20filtering%20of%0Alow-frequency%20noise%20in%20the%20generated%20images.%20To%20address%20these%20issues%20within%20the%0Asame%20computational%20time%20scale%2C%20we%20propose%20GaraMoSt.%20Specifically%2C%20we%20optimize%0Athe%20network%20pipeline%20with%20a%20parallel%20design%20and%20propose%20a%20module%20named%20MG-MSFE.%0AMG-MSFE%20extracts%20frame-relative%20motion%20and%20structural%20features%20at%20various%0Agranularities%20in%20a%20fully%20convolutional%20parallel%20manner%20and%20supports%0Aindependent%2C%20flexible%20adjustment%20of%20context-aware%20granularity%20at%20different%0Ascales%2C%20thus%20enhancing%20computational%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20demonstrate%20that%20GaraMoSt%20achieves%20the%20SOTA%20performance%20in%0Aaccuracy%2C%20robustness%2C%20visual%20effects%2C%20and%20noise%20suppression%2C%20comprehensively%0Asurpassing%20MoSt-DSA%20and%20other%20natural%20scene%20VFI%20methods.%20The%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/ZyoungXu/GaraMoSt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaraMoSt%253A%2520Parallel%2520Multi-Granularity%2520Motion%2520and%2520Structural%2520Modeling%2520for%250A%2520%2520Efficient%2520Multi-Frame%2520Interpolation%2520in%2520DSA%2520Images%26entry.906535625%3DZiyang%2520Xu%2520and%2520Huangxuan%2520Zhao%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520and%2520accurate%2520direct%2520multi-frame%2520interpolation%2520method%2520for%2520Digital%250ASubtraction%2520Angiography%2520%2528DSA%2529%2520images%2520is%2520crucial%2520for%2520reducing%2520radiation%2520and%250Aproviding%2520real-time%2520assistance%2520to%2520physicians%2520for%2520precise%2520diagnostics%2520and%250Atreatment.%2520DSA%2520images%2520contain%2520complex%2520vascular%2520structures%2520and%2520various%2520motions.%250AApplying%2520natural%2520scene%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520methods%2520results%2520in%250Amotion%2520artifacts%252C%2520structural%2520dissipation%252C%2520and%2520blurriness.%2520Recently%252C%2520MoSt-DSA%250Ahas%2520specifically%2520addressed%2520these%2520issues%2520for%2520the%2520first%2520time%2520and%2520achieved%2520SOTA%250Aresults.%2520However%252C%2520MoSt-DSA%2527s%2520focus%2520on%2520real-time%2520performance%2520leads%2520to%250Ainsufficient%2520suppression%2520of%2520high-frequency%2520noise%2520and%2520incomplete%2520filtering%2520of%250Alow-frequency%2520noise%2520in%2520the%2520generated%2520images.%2520To%2520address%2520these%2520issues%2520within%2520the%250Asame%2520computational%2520time%2520scale%252C%2520we%2520propose%2520GaraMoSt.%2520Specifically%252C%2520we%2520optimize%250Athe%2520network%2520pipeline%2520with%2520a%2520parallel%2520design%2520and%2520propose%2520a%2520module%2520named%2520MG-MSFE.%250AMG-MSFE%2520extracts%2520frame-relative%2520motion%2520and%2520structural%2520features%2520at%2520various%250Agranularities%2520in%2520a%2520fully%2520convolutional%2520parallel%2520manner%2520and%2520supports%250Aindependent%252C%2520flexible%2520adjustment%2520of%2520context-aware%2520granularity%2520at%2520different%250Ascales%252C%2520thus%2520enhancing%2520computational%2520efficiency%2520and%2520accuracy.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520GaraMoSt%2520achieves%2520the%2520SOTA%2520performance%2520in%250Aaccuracy%252C%2520robustness%252C%2520visual%2520effects%252C%2520and%2520noise%2520suppression%252C%2520comprehensively%250Asurpassing%2520MoSt-DSA%2520and%2520other%2520natural%2520scene%2520VFI%2520methods.%2520The%2520code%2520and%2520models%250Aare%2520available%2520at%2520https%253A//github.com/ZyoungXu/GaraMoSt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images&entry.906535625=Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20The%20rapid%20and%20accurate%20direct%20multi-frame%20interpolation%20method%20for%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20images%20is%20crucial%20for%20reducing%20radiation%20and%0Aproviding%20real-time%20assistance%20to%20physicians%20for%20precise%20diagnostics%20and%0Atreatment.%20DSA%20images%20contain%20complex%20vascular%20structures%20and%20various%20motions.%0AApplying%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%20methods%20results%20in%0Amotion%20artifacts%2C%20structural%20dissipation%2C%20and%20blurriness.%20Recently%2C%20MoSt-DSA%0Ahas%20specifically%20addressed%20these%20issues%20for%20the%20first%20time%20and%20achieved%20SOTA%0Aresults.%20However%2C%20MoSt-DSA%27s%20focus%20on%20real-time%20performance%20leads%20to%0Ainsufficient%20suppression%20of%20high-frequency%20noise%20and%20incomplete%20filtering%20of%0Alow-frequency%20noise%20in%20the%20generated%20images.%20To%20address%20these%20issues%20within%20the%0Asame%20computational%20time%20scale%2C%20we%20propose%20GaraMoSt.%20Specifically%2C%20we%20optimize%0Athe%20network%20pipeline%20with%20a%20parallel%20design%20and%20propose%20a%20module%20named%20MG-MSFE.%0AMG-MSFE%20extracts%20frame-relative%20motion%20and%20structural%20features%20at%20various%0Agranularities%20in%20a%20fully%20convolutional%20parallel%20manner%20and%20supports%0Aindependent%2C%20flexible%20adjustment%20of%20context-aware%20granularity%20at%20different%0Ascales%2C%20thus%20enhancing%20computational%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20demonstrate%20that%20GaraMoSt%20achieves%20the%20SOTA%20performance%20in%0Aaccuracy%2C%20robustness%2C%20visual%20effects%2C%20and%20noise%20suppression%2C%20comprehensively%0Asurpassing%20MoSt-DSA%20and%20other%20natural%20scene%20VFI%20methods.%20The%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/ZyoungXu/GaraMoSt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14118v2&entry.124074799=Read"},
{"title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization", "author": "Jeongseok Hyun and Su Ho Han and Hyolim Kang and Joon-Young Lee and Seon Joo Kim", "abstract": "  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n", "link": "http://arxiv.org/abs/2407.07024v3", "date": "2024-12-19", "relevancy": 2.287, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20limited%20by%20the%0Ascarcity%20of%20large-scale%20annotated%20datasets.%20To%20overcome%20this%2C%20recent%20works%0Aintegrate%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20open-vocabulary%20TAL%0A%28OV-TAL%29.%20However%2C%20despite%20the%20success%20of%20VLMs%20trained%20on%20extensive%20datasets%2C%0Aexisting%20OV-TAL%20methods%20still%20rely%20on%20human-labeled%20TAL%20datasets%20of%20limited%0Asize%20to%20train%20action%20localizers%2C%20limiting%20their%20generalizability.%20In%20this%0Apaper%2C%20we%20explore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%0Avideos%20for%20OV-TAL.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20class-agnostic%0Aaction%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20to%20generate%0Apseudo-labels%20for%20unlabeled%20videos%2C%20and%20%282%29%20the%20large-scale%20pseudo-labeled%0Adataset%20is%20then%20used%20to%20train%20the%20localizer.%20Extensive%20experiments%20demonstrate%0Athat%20leveraging%20web-scale%20videos%20in%20self-training%20significantly%20enhances%20the%0Ageneralizability%20of%20an%20action%20localizer.%20Additionally%2C%20we%20identify%20limitations%0Ain%20existing%20OV-TAL%20evaluation%20schemes%20and%20propose%20a%20new%20benchmark%20for%20thorough%0Aassessment.%20Finally%2C%20we%20showcase%20the%20TAL%20performance%20of%20the%20large%20multimodal%0Amodel%20Gemini-1.5%20on%20our%20new%20benchmark.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07024v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Scalability%2520of%2520Self-Training%2520for%2520Open-Vocabulary%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DJeongseok%2520Hyun%2520and%2520Su%2520Ho%2520Han%2520and%2520Hyolim%2520Kang%2520and%2520Joon-Young%2520Lee%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520The%2520vocabulary%2520size%2520in%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520is%2520limited%2520by%2520the%250Ascarcity%2520of%2520large-scale%2520annotated%2520datasets.%2520To%2520overcome%2520this%252C%2520recent%2520works%250Aintegrate%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520for%2520open-vocabulary%2520TAL%250A%2528OV-TAL%2529.%2520However%252C%2520despite%2520the%2520success%2520of%2520VLMs%2520trained%2520on%2520extensive%2520datasets%252C%250Aexisting%2520OV-TAL%2520methods%2520still%2520rely%2520on%2520human-labeled%2520TAL%2520datasets%2520of%2520limited%250Asize%2520to%2520train%2520action%2520localizers%252C%2520limiting%2520their%2520generalizability.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520the%2520scalability%2520of%2520self-training%2520with%2520unlabeled%2520YouTube%250Avideos%2520for%2520OV-TAL.%2520Our%2520approach%2520consists%2520of%2520two%2520stages%253A%2520%25281%2529%2520a%2520class-agnostic%250Aaction%2520localizer%2520is%2520trained%2520on%2520a%2520human-labeled%2520TAL%2520dataset%2520to%2520generate%250Apseudo-labels%2520for%2520unlabeled%2520videos%252C%2520and%2520%25282%2529%2520the%2520large-scale%2520pseudo-labeled%250Adataset%2520is%2520then%2520used%2520to%2520train%2520the%2520localizer.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520leveraging%2520web-scale%2520videos%2520in%2520self-training%2520significantly%2520enhances%2520the%250Ageneralizability%2520of%2520an%2520action%2520localizer.%2520Additionally%252C%2520we%2520identify%2520limitations%250Ain%2520existing%2520OV-TAL%2520evaluation%2520schemes%2520and%2520propose%2520a%2520new%2520benchmark%2520for%2520thorough%250Aassessment.%2520Finally%252C%2520we%2520showcase%2520the%2520TAL%2520performance%2520of%2520the%2520large%2520multimodal%250Amodel%2520Gemini-1.5%2520on%2520our%2520new%2520benchmark.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/HYUNJS/STOV-TAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07024v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&entry.906535625=Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20limited%20by%20the%0Ascarcity%20of%20large-scale%20annotated%20datasets.%20To%20overcome%20this%2C%20recent%20works%0Aintegrate%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20open-vocabulary%20TAL%0A%28OV-TAL%29.%20However%2C%20despite%20the%20success%20of%20VLMs%20trained%20on%20extensive%20datasets%2C%0Aexisting%20OV-TAL%20methods%20still%20rely%20on%20human-labeled%20TAL%20datasets%20of%20limited%0Asize%20to%20train%20action%20localizers%2C%20limiting%20their%20generalizability.%20In%20this%0Apaper%2C%20we%20explore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%0Avideos%20for%20OV-TAL.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20class-agnostic%0Aaction%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20to%20generate%0Apseudo-labels%20for%20unlabeled%20videos%2C%20and%20%282%29%20the%20large-scale%20pseudo-labeled%0Adataset%20is%20then%20used%20to%20train%20the%20localizer.%20Extensive%20experiments%20demonstrate%0Athat%20leveraging%20web-scale%20videos%20in%20self-training%20significantly%20enhances%20the%0Ageneralizability%20of%20an%20action%20localizer.%20Additionally%2C%20we%20identify%20limitations%0Ain%20existing%20OV-TAL%20evaluation%20schemes%20and%20propose%20a%20new%20benchmark%20for%20thorough%0Aassessment.%20Finally%2C%20we%20showcase%20the%20TAL%20performance%20of%20the%20large%20multimodal%0Amodel%20Gemini-1.5%20on%20our%20new%20benchmark.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07024v3&entry.124074799=Read"},
{"title": "GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation", "author": "G. Andrade-Miranda and K. Chatzipapas and J. D. Arias-Londo\u00f1o and J. I. Godino-Llorente", "abstract": "  The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.\n", "link": "http://arxiv.org/abs/2412.15054v1", "date": "2024-12-19", "relevancy": 2.2814, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4671}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIRAFE%3A%20Glottal%20Imaging%20Dataset%20for%20Advanced%20Segmentation%2C%20Analysis%2C%20and%0A%20%20Facilitative%20Playbacks%20Evaluation&body=Title%3A%20GIRAFE%3A%20Glottal%20Imaging%20Dataset%20for%20Advanced%20Segmentation%2C%20Analysis%2C%20and%0A%20%20Facilitative%20Playbacks%20Evaluation%0AAuthor%3A%20G.%20Andrade-Miranda%20and%20K.%20Chatzipapas%20and%20J.%20D.%20Arias-Londo%C3%B1o%20and%20J.%20I.%20Godino-Llorente%0AAbstract%3A%20%20%20The%20advances%20in%20the%20development%20of%20Facilitative%20Playbacks%20extracted%20from%0AHigh-Speed%20videoendoscopic%20sequences%20of%20the%20vocal%20folds%20are%20hindered%20by%20a%0Anotable%20lack%20of%20publicly%20available%20datasets%20annotated%20with%20the%20semantic%0Asegmentations%20corresponding%20to%20the%20area%20of%20the%20glottal%20gap.%20This%20fact%20also%0Alimits%20the%20reproducibility%20and%20further%20exploration%20of%20existing%20research%20in%20this%0Afield.%0A%20%20To%20address%20this%20gap%2C%20GIRAFE%20is%20a%20data%20repository%20designed%20to%20facilitate%20the%0Adevelopment%20of%20advanced%20techniques%20for%20the%20semantic%20segmentation%2C%20analysis%2C%20and%0Afast%20evaluation%20of%20High-Speed%20videoendoscopic%20sequences%20of%20the%20vocal%20folds.%20The%0Arepository%20includes%2065%20high-speed%20videoendoscopic%20recordings%20from%20a%20cohort%20of%0A50%20patients%20%2830%20female%2C%2020%20male%29.%20The%20dataset%20comprises%2015%20recordings%20from%0Ahealthy%20controls%2C%2026%20from%20patients%20with%20diagnosed%20voice%20disorders%2C%20and%2024%20with%0Aan%20unknown%20health%20condition.%20All%20of%20them%20were%20manually%20annotated%20by%20an%20expert%2C%0Aincluding%20the%20masks%20corresponding%20to%20the%20semantic%20segmentation%20of%20the%20glottal%0Agap.%20The%20repository%20is%20also%20complemented%20with%20the%20automatic%20segmentation%20of%20the%0Aglottal%20area%20using%20different%20state-of-the-art%20approaches.%0A%20%20This%20data%20set%20has%20already%20supported%20several%20studies%2C%20which%20demonstrates%20its%0Ausefulness%20for%20the%20development%20of%20new%20glottal%20gap%20segmentation%20algorithms%20from%0AHigh-Speed-Videoendoscopic%20sequences%20to%20improve%20or%20create%20new%20Facilitative%0APlaybacks.%20Despite%20these%20advances%20and%20others%20in%20the%20field%2C%20the%20broader%0Achallenge%20of%20performing%20an%20accurate%20and%20completely%20automatic%20semantic%0Asegmentation%20method%20of%20the%20glottal%20area%20remains%20open.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIRAFE%253A%2520Glottal%2520Imaging%2520Dataset%2520for%2520Advanced%2520Segmentation%252C%2520Analysis%252C%2520and%250A%2520%2520Facilitative%2520Playbacks%2520Evaluation%26entry.906535625%3DG.%2520Andrade-Miranda%2520and%2520K.%2520Chatzipapas%2520and%2520J.%2520D.%2520Arias-Londo%25C3%25B1o%2520and%2520J.%2520I.%2520Godino-Llorente%26entry.1292438233%3D%2520%2520The%2520advances%2520in%2520the%2520development%2520of%2520Facilitative%2520Playbacks%2520extracted%2520from%250AHigh-Speed%2520videoendoscopic%2520sequences%2520of%2520the%2520vocal%2520folds%2520are%2520hindered%2520by%2520a%250Anotable%2520lack%2520of%2520publicly%2520available%2520datasets%2520annotated%2520with%2520the%2520semantic%250Asegmentations%2520corresponding%2520to%2520the%2520area%2520of%2520the%2520glottal%2520gap.%2520This%2520fact%2520also%250Alimits%2520the%2520reproducibility%2520and%2520further%2520exploration%2520of%2520existing%2520research%2520in%2520this%250Afield.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520GIRAFE%2520is%2520a%2520data%2520repository%2520designed%2520to%2520facilitate%2520the%250Adevelopment%2520of%2520advanced%2520techniques%2520for%2520the%2520semantic%2520segmentation%252C%2520analysis%252C%2520and%250Afast%2520evaluation%2520of%2520High-Speed%2520videoendoscopic%2520sequences%2520of%2520the%2520vocal%2520folds.%2520The%250Arepository%2520includes%252065%2520high-speed%2520videoendoscopic%2520recordings%2520from%2520a%2520cohort%2520of%250A50%2520patients%2520%252830%2520female%252C%252020%2520male%2529.%2520The%2520dataset%2520comprises%252015%2520recordings%2520from%250Ahealthy%2520controls%252C%252026%2520from%2520patients%2520with%2520diagnosed%2520voice%2520disorders%252C%2520and%252024%2520with%250Aan%2520unknown%2520health%2520condition.%2520All%2520of%2520them%2520were%2520manually%2520annotated%2520by%2520an%2520expert%252C%250Aincluding%2520the%2520masks%2520corresponding%2520to%2520the%2520semantic%2520segmentation%2520of%2520the%2520glottal%250Agap.%2520The%2520repository%2520is%2520also%2520complemented%2520with%2520the%2520automatic%2520segmentation%2520of%2520the%250Aglottal%2520area%2520using%2520different%2520state-of-the-art%2520approaches.%250A%2520%2520This%2520data%2520set%2520has%2520already%2520supported%2520several%2520studies%252C%2520which%2520demonstrates%2520its%250Ausefulness%2520for%2520the%2520development%2520of%2520new%2520glottal%2520gap%2520segmentation%2520algorithms%2520from%250AHigh-Speed-Videoendoscopic%2520sequences%2520to%2520improve%2520or%2520create%2520new%2520Facilitative%250APlaybacks.%2520Despite%2520these%2520advances%2520and%2520others%2520in%2520the%2520field%252C%2520the%2520broader%250Achallenge%2520of%2520performing%2520an%2520accurate%2520and%2520completely%2520automatic%2520semantic%250Asegmentation%2520method%2520of%2520the%2520glottal%2520area%2520remains%2520open.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIRAFE%3A%20Glottal%20Imaging%20Dataset%20for%20Advanced%20Segmentation%2C%20Analysis%2C%20and%0A%20%20Facilitative%20Playbacks%20Evaluation&entry.906535625=G.%20Andrade-Miranda%20and%20K.%20Chatzipapas%20and%20J.%20D.%20Arias-Londo%C3%B1o%20and%20J.%20I.%20Godino-Llorente&entry.1292438233=%20%20The%20advances%20in%20the%20development%20of%20Facilitative%20Playbacks%20extracted%20from%0AHigh-Speed%20videoendoscopic%20sequences%20of%20the%20vocal%20folds%20are%20hindered%20by%20a%0Anotable%20lack%20of%20publicly%20available%20datasets%20annotated%20with%20the%20semantic%0Asegmentations%20corresponding%20to%20the%20area%20of%20the%20glottal%20gap.%20This%20fact%20also%0Alimits%20the%20reproducibility%20and%20further%20exploration%20of%20existing%20research%20in%20this%0Afield.%0A%20%20To%20address%20this%20gap%2C%20GIRAFE%20is%20a%20data%20repository%20designed%20to%20facilitate%20the%0Adevelopment%20of%20advanced%20techniques%20for%20the%20semantic%20segmentation%2C%20analysis%2C%20and%0Afast%20evaluation%20of%20High-Speed%20videoendoscopic%20sequences%20of%20the%20vocal%20folds.%20The%0Arepository%20includes%2065%20high-speed%20videoendoscopic%20recordings%20from%20a%20cohort%20of%0A50%20patients%20%2830%20female%2C%2020%20male%29.%20The%20dataset%20comprises%2015%20recordings%20from%0Ahealthy%20controls%2C%2026%20from%20patients%20with%20diagnosed%20voice%20disorders%2C%20and%2024%20with%0Aan%20unknown%20health%20condition.%20All%20of%20them%20were%20manually%20annotated%20by%20an%20expert%2C%0Aincluding%20the%20masks%20corresponding%20to%20the%20semantic%20segmentation%20of%20the%20glottal%0Agap.%20The%20repository%20is%20also%20complemented%20with%20the%20automatic%20segmentation%20of%20the%0Aglottal%20area%20using%20different%20state-of-the-art%20approaches.%0A%20%20This%20data%20set%20has%20already%20supported%20several%20studies%2C%20which%20demonstrates%20its%0Ausefulness%20for%20the%20development%20of%20new%20glottal%20gap%20segmentation%20algorithms%20from%0AHigh-Speed-Videoendoscopic%20sequences%20to%20improve%20or%20create%20new%20Facilitative%0APlaybacks.%20Despite%20these%20advances%20and%20others%20in%20the%20field%2C%20the%20broader%0Achallenge%20of%20performing%20an%20accurate%20and%20completely%20automatic%20semantic%0Asegmentation%20method%20of%20the%20glottal%20area%20remains%20open.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15054v1&entry.124074799=Read"},
{"title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension", "author": "Yongdong Luo and Xiawu Zheng and Xiao Yang and Guilin Li and Haojia Lin and Jinfa Huang and Jiayi Ji and Fei Chao and Jiebo Luo and Rongrong Ji", "abstract": "  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n", "link": "http://arxiv.org/abs/2411.13093v2", "date": "2024-12-19", "relevancy": 2.2775, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5729}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension&body=Title%3A%20Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension%0AAuthor%3A%20Yongdong%20Luo%20and%20Xiawu%20Zheng%20and%20Xiao%20Yang%20and%20Guilin%20Li%20and%20Haojia%20Lin%20and%20Jinfa%20Huang%20and%20Jiayi%20Ji%20and%20Fei%20Chao%20and%20Jiebo%20Luo%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Existing%20large%20video-language%20models%20%28LVLMs%29%20struggle%20to%20comprehend%20long%0Avideos%20correctly%20due%20to%20limited%20context.%20To%20address%20this%20problem%2C%20fine-tuning%0Along-context%20LVLMs%20and%20employing%20GPT-based%20agents%20have%20emerged%20as%20promising%0Asolutions.%20However%2C%20fine-tuning%20LVLMs%20would%20require%20extensive%20high-quality%20data%0Aand%20substantial%20GPU%20resources%2C%20while%20GPT-based%20agents%20would%20rely%20on%20proprietary%0Amodels%20%28e.g.%2C%20GPT-4o%29.%20In%20this%20paper%2C%20we%20propose%20Video%20Retrieval-Augmented%0AGeneration%20%28Video-RAG%29%2C%20a%20training-free%20and%20cost-effective%20pipeline%20that%0Aemploys%20visually-aligned%20auxiliary%20texts%20to%20help%20facilitate%20cross-modality%0Aalignment%20while%20providing%20additional%20information%20beyond%20the%20visual%20content.%0ASpecifically%2C%20we%20leverage%20open-source%20external%20tools%20to%20extract%0Avisually-aligned%20information%20from%20pure%20video%20data%20%28e.g.%2C%20audio%2C%20optical%0Acharacter%2C%20and%20object%20detection%29%2C%20and%20incorporate%20the%20extracted%20information%0Ainto%20an%20existing%20LVLM%20as%20auxiliary%20texts%2C%20alongside%20video%20frames%20and%20queries%2C%0Ain%20a%20plug-and-play%20manner.%20Our%20Video-RAG%20offers%20several%20key%20advantages%3A%20%28i%29%0Alightweight%20with%20low%20computing%20overhead%20due%20to%20single-turn%20retrieval%3B%20%28ii%29%20easy%0Aimplementation%20and%20compatibility%20with%20any%20LVLM%3B%20and%20%28iii%29%20significant%2C%0Aconsistent%20performance%20gains%20across%20long%20video%20understanding%20benchmarks%2C%0Aincluding%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench.%20Notably%2C%20our%20model%20demonstrates%0Asuperior%20performance%20over%20proprietary%20models%20like%20Gemini-1.5-Pro%20and%20GPT-4o%0Awhen%20utilized%20with%20a%2072B%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-RAG%253A%2520Visually-aligned%2520Retrieval-Augmented%2520Long%2520Video%2520Comprehension%26entry.906535625%3DYongdong%2520Luo%2520and%2520Xiawu%2520Zheng%2520and%2520Xiao%2520Yang%2520and%2520Guilin%2520Li%2520and%2520Haojia%2520Lin%2520and%2520Jinfa%2520Huang%2520and%2520Jiayi%2520Ji%2520and%2520Fei%2520Chao%2520and%2520Jiebo%2520Luo%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Existing%2520large%2520video-language%2520models%2520%2528LVLMs%2529%2520struggle%2520to%2520comprehend%2520long%250Avideos%2520correctly%2520due%2520to%2520limited%2520context.%2520To%2520address%2520this%2520problem%252C%2520fine-tuning%250Along-context%2520LVLMs%2520and%2520employing%2520GPT-based%2520agents%2520have%2520emerged%2520as%2520promising%250Asolutions.%2520However%252C%2520fine-tuning%2520LVLMs%2520would%2520require%2520extensive%2520high-quality%2520data%250Aand%2520substantial%2520GPU%2520resources%252C%2520while%2520GPT-based%2520agents%2520would%2520rely%2520on%2520proprietary%250Amodels%2520%2528e.g.%252C%2520GPT-4o%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Video%2520Retrieval-Augmented%250AGeneration%2520%2528Video-RAG%2529%252C%2520a%2520training-free%2520and%2520cost-effective%2520pipeline%2520that%250Aemploys%2520visually-aligned%2520auxiliary%2520texts%2520to%2520help%2520facilitate%2520cross-modality%250Aalignment%2520while%2520providing%2520additional%2520information%2520beyond%2520the%2520visual%2520content.%250ASpecifically%252C%2520we%2520leverage%2520open-source%2520external%2520tools%2520to%2520extract%250Avisually-aligned%2520information%2520from%2520pure%2520video%2520data%2520%2528e.g.%252C%2520audio%252C%2520optical%250Acharacter%252C%2520and%2520object%2520detection%2529%252C%2520and%2520incorporate%2520the%2520extracted%2520information%250Ainto%2520an%2520existing%2520LVLM%2520as%2520auxiliary%2520texts%252C%2520alongside%2520video%2520frames%2520and%2520queries%252C%250Ain%2520a%2520plug-and-play%2520manner.%2520Our%2520Video-RAG%2520offers%2520several%2520key%2520advantages%253A%2520%2528i%2529%250Alightweight%2520with%2520low%2520computing%2520overhead%2520due%2520to%2520single-turn%2520retrieval%253B%2520%2528ii%2529%2520easy%250Aimplementation%2520and%2520compatibility%2520with%2520any%2520LVLM%253B%2520and%2520%2528iii%2529%2520significant%252C%250Aconsistent%2520performance%2520gains%2520across%2520long%2520video%2520understanding%2520benchmarks%252C%250Aincluding%2520Video-MME%252C%2520MLVU%252C%2520and%2520LongVideoBench.%2520Notably%252C%2520our%2520model%2520demonstrates%250Asuperior%2520performance%2520over%2520proprietary%2520models%2520like%2520Gemini-1.5-Pro%2520and%2520GPT-4o%250Awhen%2520utilized%2520with%2520a%252072B%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-RAG%3A%20Visually-aligned%20Retrieval-Augmented%20Long%20Video%20Comprehension&entry.906535625=Yongdong%20Luo%20and%20Xiawu%20Zheng%20and%20Xiao%20Yang%20and%20Guilin%20Li%20and%20Haojia%20Lin%20and%20Jinfa%20Huang%20and%20Jiayi%20Ji%20and%20Fei%20Chao%20and%20Jiebo%20Luo%20and%20Rongrong%20Ji&entry.1292438233=%20%20Existing%20large%20video-language%20models%20%28LVLMs%29%20struggle%20to%20comprehend%20long%0Avideos%20correctly%20due%20to%20limited%20context.%20To%20address%20this%20problem%2C%20fine-tuning%0Along-context%20LVLMs%20and%20employing%20GPT-based%20agents%20have%20emerged%20as%20promising%0Asolutions.%20However%2C%20fine-tuning%20LVLMs%20would%20require%20extensive%20high-quality%20data%0Aand%20substantial%20GPU%20resources%2C%20while%20GPT-based%20agents%20would%20rely%20on%20proprietary%0Amodels%20%28e.g.%2C%20GPT-4o%29.%20In%20this%20paper%2C%20we%20propose%20Video%20Retrieval-Augmented%0AGeneration%20%28Video-RAG%29%2C%20a%20training-free%20and%20cost-effective%20pipeline%20that%0Aemploys%20visually-aligned%20auxiliary%20texts%20to%20help%20facilitate%20cross-modality%0Aalignment%20while%20providing%20additional%20information%20beyond%20the%20visual%20content.%0ASpecifically%2C%20we%20leverage%20open-source%20external%20tools%20to%20extract%0Avisually-aligned%20information%20from%20pure%20video%20data%20%28e.g.%2C%20audio%2C%20optical%0Acharacter%2C%20and%20object%20detection%29%2C%20and%20incorporate%20the%20extracted%20information%0Ainto%20an%20existing%20LVLM%20as%20auxiliary%20texts%2C%20alongside%20video%20frames%20and%20queries%2C%0Ain%20a%20plug-and-play%20manner.%20Our%20Video-RAG%20offers%20several%20key%20advantages%3A%20%28i%29%0Alightweight%20with%20low%20computing%20overhead%20due%20to%20single-turn%20retrieval%3B%20%28ii%29%20easy%0Aimplementation%20and%20compatibility%20with%20any%20LVLM%3B%20and%20%28iii%29%20significant%2C%0Aconsistent%20performance%20gains%20across%20long%20video%20understanding%20benchmarks%2C%0Aincluding%20Video-MME%2C%20MLVU%2C%20and%20LongVideoBench.%20Notably%2C%20our%20model%20demonstrates%0Asuperior%20performance%20over%20proprietary%20models%20like%20Gemini-1.5-Pro%20and%20GPT-4o%0Awhen%20utilized%20with%20a%2072B%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13093v2&entry.124074799=Read"},
{"title": "Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection", "author": "Bastian J\u00e4ckl and Yannick Metz and Udo Schlegel and Daniel A. Keim and Maximilian T. Fischer", "abstract": "  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n", "link": "http://arxiv.org/abs/2412.15150v1", "date": "2024-12-19", "relevancy": 2.2645, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5723}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Color%20Channel%20Independence%20for%20Improved%20Unsupervised%20Object%0A%20%20Detection&body=Title%3A%20Leveraging%20Color%20Channel%20Independence%20for%20Improved%20Unsupervised%20Object%0A%20%20Detection%0AAuthor%3A%20Bastian%20J%C3%A4ckl%20and%20Yannick%20Metz%20and%20Udo%20Schlegel%20and%20Daniel%20A.%20Keim%20and%20Maximilian%20T.%20Fischer%0AAbstract%3A%20%20%20Object-centric%20architectures%20can%20learn%20to%20extract%20distinct%20object%0Arepresentations%20from%20visual%20scenes%2C%20enabling%20downstream%20applications%20on%20the%0Aobject%20level.%20Similarly%20to%20autoencoder-based%20image%20models%2C%20object-centric%0Aapproaches%20have%20been%20trained%20on%20the%20unsupervised%20reconstruction%20loss%20of%20images%0Aencoded%20by%20RGB%20color%20spaces.%20In%20our%20work%2C%20we%20challenge%20the%20common%20assumption%0Athat%20RGB%20images%20are%20the%20optimal%20color%20space%20for%20unsupervised%20learning%20in%0Acomputer%20vision.%20We%20discuss%20conceptually%20and%20empirically%20that%20other%20color%0Aspaces%2C%20such%20as%20HSV%2C%20bear%20essential%20characteristics%20for%20object-centric%0Arepresentation%20learning%2C%20like%20robustness%20to%20lighting%20conditions.%20We%20further%0Ashow%20that%20models%20improve%20when%20requiring%20them%20to%20predict%20additional%20color%0Achannels.%20Specifically%2C%20we%20propose%20to%20transform%20the%20predicted%20targets%20to%20the%0ARGB-S%20space%2C%20which%20extends%20RGB%20with%20HSV%27s%20saturation%20component%20and%20leads%20to%0Amarkedly%20better%20reconstruction%20and%20disentanglement%20for%20five%20common%20evaluation%0Adatasets.%20The%20use%20of%20composite%20color%20spaces%20can%20be%20implemented%20with%20basically%0Ano%20computational%20overhead%2C%20is%20agnostic%20of%20the%20models%27%20architecture%2C%20and%20is%0Auniversally%20applicable%20across%20a%20wide%20range%20of%20visual%20computing%20tasks%20and%0Atraining%20types.%20The%20findings%20of%20our%20approach%20encourage%20additional%0Ainvestigations%20in%20computer%20vision%20tasks%20beyond%20object-centric%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Color%2520Channel%2520Independence%2520for%2520Improved%2520Unsupervised%2520Object%250A%2520%2520Detection%26entry.906535625%3DBastian%2520J%25C3%25A4ckl%2520and%2520Yannick%2520Metz%2520and%2520Udo%2520Schlegel%2520and%2520Daniel%2520A.%2520Keim%2520and%2520Maximilian%2520T.%2520Fischer%26entry.1292438233%3D%2520%2520Object-centric%2520architectures%2520can%2520learn%2520to%2520extract%2520distinct%2520object%250Arepresentations%2520from%2520visual%2520scenes%252C%2520enabling%2520downstream%2520applications%2520on%2520the%250Aobject%2520level.%2520Similarly%2520to%2520autoencoder-based%2520image%2520models%252C%2520object-centric%250Aapproaches%2520have%2520been%2520trained%2520on%2520the%2520unsupervised%2520reconstruction%2520loss%2520of%2520images%250Aencoded%2520by%2520RGB%2520color%2520spaces.%2520In%2520our%2520work%252C%2520we%2520challenge%2520the%2520common%2520assumption%250Athat%2520RGB%2520images%2520are%2520the%2520optimal%2520color%2520space%2520for%2520unsupervised%2520learning%2520in%250Acomputer%2520vision.%2520We%2520discuss%2520conceptually%2520and%2520empirically%2520that%2520other%2520color%250Aspaces%252C%2520such%2520as%2520HSV%252C%2520bear%2520essential%2520characteristics%2520for%2520object-centric%250Arepresentation%2520learning%252C%2520like%2520robustness%2520to%2520lighting%2520conditions.%2520We%2520further%250Ashow%2520that%2520models%2520improve%2520when%2520requiring%2520them%2520to%2520predict%2520additional%2520color%250Achannels.%2520Specifically%252C%2520we%2520propose%2520to%2520transform%2520the%2520predicted%2520targets%2520to%2520the%250ARGB-S%2520space%252C%2520which%2520extends%2520RGB%2520with%2520HSV%2527s%2520saturation%2520component%2520and%2520leads%2520to%250Amarkedly%2520better%2520reconstruction%2520and%2520disentanglement%2520for%2520five%2520common%2520evaluation%250Adatasets.%2520The%2520use%2520of%2520composite%2520color%2520spaces%2520can%2520be%2520implemented%2520with%2520basically%250Ano%2520computational%2520overhead%252C%2520is%2520agnostic%2520of%2520the%2520models%2527%2520architecture%252C%2520and%2520is%250Auniversally%2520applicable%2520across%2520a%2520wide%2520range%2520of%2520visual%2520computing%2520tasks%2520and%250Atraining%2520types.%2520The%2520findings%2520of%2520our%2520approach%2520encourage%2520additional%250Ainvestigations%2520in%2520computer%2520vision%2520tasks%2520beyond%2520object-centric%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Color%20Channel%20Independence%20for%20Improved%20Unsupervised%20Object%0A%20%20Detection&entry.906535625=Bastian%20J%C3%A4ckl%20and%20Yannick%20Metz%20and%20Udo%20Schlegel%20and%20Daniel%20A.%20Keim%20and%20Maximilian%20T.%20Fischer&entry.1292438233=%20%20Object-centric%20architectures%20can%20learn%20to%20extract%20distinct%20object%0Arepresentations%20from%20visual%20scenes%2C%20enabling%20downstream%20applications%20on%20the%0Aobject%20level.%20Similarly%20to%20autoencoder-based%20image%20models%2C%20object-centric%0Aapproaches%20have%20been%20trained%20on%20the%20unsupervised%20reconstruction%20loss%20of%20images%0Aencoded%20by%20RGB%20color%20spaces.%20In%20our%20work%2C%20we%20challenge%20the%20common%20assumption%0Athat%20RGB%20images%20are%20the%20optimal%20color%20space%20for%20unsupervised%20learning%20in%0Acomputer%20vision.%20We%20discuss%20conceptually%20and%20empirically%20that%20other%20color%0Aspaces%2C%20such%20as%20HSV%2C%20bear%20essential%20characteristics%20for%20object-centric%0Arepresentation%20learning%2C%20like%20robustness%20to%20lighting%20conditions.%20We%20further%0Ashow%20that%20models%20improve%20when%20requiring%20them%20to%20predict%20additional%20color%0Achannels.%20Specifically%2C%20we%20propose%20to%20transform%20the%20predicted%20targets%20to%20the%0ARGB-S%20space%2C%20which%20extends%20RGB%20with%20HSV%27s%20saturation%20component%20and%20leads%20to%0Amarkedly%20better%20reconstruction%20and%20disentanglement%20for%20five%20common%20evaluation%0Adatasets.%20The%20use%20of%20composite%20color%20spaces%20can%20be%20implemented%20with%20basically%0Ano%20computational%20overhead%2C%20is%20agnostic%20of%20the%20models%27%20architecture%2C%20and%20is%0Auniversally%20applicable%20across%20a%20wide%20range%20of%20visual%20computing%20tasks%20and%0Atraining%20types.%20The%20findings%20of%20our%20approach%20encourage%20additional%0Ainvestigations%20in%20computer%20vision%20tasks%20beyond%20object-centric%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15150v1&entry.124074799=Read"},
{"title": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation", "author": "Shoumeng Qiu and Xinrun Li and XiangYang Xue and Jian Pu", "abstract": "  Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}\n", "link": "http://arxiv.org/abs/2412.14821v1", "date": "2024-12-19", "relevancy": 2.2481, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PC-BEV%3A%20An%20Efficient%20Polar-Cartesian%20BEV%20Fusion%20Framework%20for%20LiDAR%0A%20%20Semantic%20Segmentation&body=Title%3A%20PC-BEV%3A%20An%20Efficient%20Polar-Cartesian%20BEV%20Fusion%20Framework%20for%20LiDAR%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Shoumeng%20Qiu%20and%20Xinrun%20Li%20and%20XiangYang%20Xue%20and%20Jian%20Pu%0AAbstract%3A%20%20%20Although%20multiview%20fusion%20has%20demonstrated%20potential%20in%20LiDAR%20segmentation%2C%0Aits%20dependence%20on%20computationally%20intensive%20point-based%20interactions%2C%20arising%0Afrom%20the%20lack%20of%20fixed%20correspondences%20between%20views%20such%20as%20range%20view%20and%0ABird%27s-Eye%20View%20%28BEV%29%2C%20hinders%20its%20practical%20deployment.%20This%20paper%20challenges%0Athe%20prevailing%20notion%20that%20multiview%20fusion%20is%20essential%20for%20achieving%20high%0Aperformance.%20We%20demonstrate%20that%20significant%20gains%20can%20be%20realized%20by%20directly%0Afusing%20Polar%20and%20Cartesian%20partitioning%20strategies%20within%20the%20BEV%20space.%20Our%0Aproposed%20BEV-only%20segmentation%20model%20leverages%20the%20inherent%20fixed%20grid%0Acorrespondences%20between%20these%20partitioning%20schemes%2C%20enabling%20a%20fusion%20process%0Athat%20is%20orders%20of%20magnitude%20faster%20%28170%24%5Ctimes%24%20speedup%29%20than%20conventional%0Apoint-based%20methods.%20Furthermore%2C%20our%20approach%20facilitates%20dense%20feature%0Afusion%2C%20preserving%20richer%20contextual%20information%20compared%20to%20sparse%20point-based%0Aalternatives.%20To%20enhance%20scene%20understanding%20while%20maintaining%20inference%0Aefficiency%2C%20we%20also%20introduce%20a%20hybrid%20Transformer-CNN%20architecture.%20Extensive%0Aevaluation%20on%20the%20SemanticKITTI%20and%20nuScenes%20datasets%20provides%20compelling%0Aevidence%20that%20our%20method%20outperforms%20previous%20multiview%20fusion%20approaches%20in%0Aterms%20of%20both%20performance%20and%20inference%20speed%2C%20highlighting%20the%20potential%20of%0ABEV-based%20fusion%20for%20LiDAR%20segmentation.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/skyshoumeng/PC-BEV.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPC-BEV%253A%2520An%2520Efficient%2520Polar-Cartesian%2520BEV%2520Fusion%2520Framework%2520for%2520LiDAR%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DShoumeng%2520Qiu%2520and%2520Xinrun%2520Li%2520and%2520XiangYang%2520Xue%2520and%2520Jian%2520Pu%26entry.1292438233%3D%2520%2520Although%2520multiview%2520fusion%2520has%2520demonstrated%2520potential%2520in%2520LiDAR%2520segmentation%252C%250Aits%2520dependence%2520on%2520computationally%2520intensive%2520point-based%2520interactions%252C%2520arising%250Afrom%2520the%2520lack%2520of%2520fixed%2520correspondences%2520between%2520views%2520such%2520as%2520range%2520view%2520and%250ABird%2527s-Eye%2520View%2520%2528BEV%2529%252C%2520hinders%2520its%2520practical%2520deployment.%2520This%2520paper%2520challenges%250Athe%2520prevailing%2520notion%2520that%2520multiview%2520fusion%2520is%2520essential%2520for%2520achieving%2520high%250Aperformance.%2520We%2520demonstrate%2520that%2520significant%2520gains%2520can%2520be%2520realized%2520by%2520directly%250Afusing%2520Polar%2520and%2520Cartesian%2520partitioning%2520strategies%2520within%2520the%2520BEV%2520space.%2520Our%250Aproposed%2520BEV-only%2520segmentation%2520model%2520leverages%2520the%2520inherent%2520fixed%2520grid%250Acorrespondences%2520between%2520these%2520partitioning%2520schemes%252C%2520enabling%2520a%2520fusion%2520process%250Athat%2520is%2520orders%2520of%2520magnitude%2520faster%2520%2528170%2524%255Ctimes%2524%2520speedup%2529%2520than%2520conventional%250Apoint-based%2520methods.%2520Furthermore%252C%2520our%2520approach%2520facilitates%2520dense%2520feature%250Afusion%252C%2520preserving%2520richer%2520contextual%2520information%2520compared%2520to%2520sparse%2520point-based%250Aalternatives.%2520To%2520enhance%2520scene%2520understanding%2520while%2520maintaining%2520inference%250Aefficiency%252C%2520we%2520also%2520introduce%2520a%2520hybrid%2520Transformer-CNN%2520architecture.%2520Extensive%250Aevaluation%2520on%2520the%2520SemanticKITTI%2520and%2520nuScenes%2520datasets%2520provides%2520compelling%250Aevidence%2520that%2520our%2520method%2520outperforms%2520previous%2520multiview%2520fusion%2520approaches%2520in%250Aterms%2520of%2520both%2520performance%2520and%2520inference%2520speed%252C%2520highlighting%2520the%2520potential%2520of%250ABEV-based%2520fusion%2520for%2520LiDAR%2520segmentation.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/skyshoumeng/PC-BEV.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PC-BEV%3A%20An%20Efficient%20Polar-Cartesian%20BEV%20Fusion%20Framework%20for%20LiDAR%0A%20%20Semantic%20Segmentation&entry.906535625=Shoumeng%20Qiu%20and%20Xinrun%20Li%20and%20XiangYang%20Xue%20and%20Jian%20Pu&entry.1292438233=%20%20Although%20multiview%20fusion%20has%20demonstrated%20potential%20in%20LiDAR%20segmentation%2C%0Aits%20dependence%20on%20computationally%20intensive%20point-based%20interactions%2C%20arising%0Afrom%20the%20lack%20of%20fixed%20correspondences%20between%20views%20such%20as%20range%20view%20and%0ABird%27s-Eye%20View%20%28BEV%29%2C%20hinders%20its%20practical%20deployment.%20This%20paper%20challenges%0Athe%20prevailing%20notion%20that%20multiview%20fusion%20is%20essential%20for%20achieving%20high%0Aperformance.%20We%20demonstrate%20that%20significant%20gains%20can%20be%20realized%20by%20directly%0Afusing%20Polar%20and%20Cartesian%20partitioning%20strategies%20within%20the%20BEV%20space.%20Our%0Aproposed%20BEV-only%20segmentation%20model%20leverages%20the%20inherent%20fixed%20grid%0Acorrespondences%20between%20these%20partitioning%20schemes%2C%20enabling%20a%20fusion%20process%0Athat%20is%20orders%20of%20magnitude%20faster%20%28170%24%5Ctimes%24%20speedup%29%20than%20conventional%0Apoint-based%20methods.%20Furthermore%2C%20our%20approach%20facilitates%20dense%20feature%0Afusion%2C%20preserving%20richer%20contextual%20information%20compared%20to%20sparse%20point-based%0Aalternatives.%20To%20enhance%20scene%20understanding%20while%20maintaining%20inference%0Aefficiency%2C%20we%20also%20introduce%20a%20hybrid%20Transformer-CNN%20architecture.%20Extensive%0Aevaluation%20on%20the%20SemanticKITTI%20and%20nuScenes%20datasets%20provides%20compelling%0Aevidence%20that%20our%20method%20outperforms%20previous%20multiview%20fusion%20approaches%20in%0Aterms%20of%20both%20performance%20and%20inference%20speed%2C%20highlighting%20the%20potential%20of%0ABEV-based%20fusion%20for%20LiDAR%20segmentation.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/skyshoumeng/PC-BEV.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14821v1&entry.124074799=Read"},
{"title": "Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text", "author": "Kangning Li and Zheyang Jia and Anyu Ying", "abstract": "  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n", "link": "http://arxiv.org/abs/2412.14965v1", "date": "2024-12-19", "relevancy": 2.2372, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5881}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Movie2Story%3A%20A%20framework%20for%20understanding%20videos%20and%20telling%20stories%20in%0A%20%20the%20form%20of%20novel%20text&body=Title%3A%20Movie2Story%3A%20A%20framework%20for%20understanding%20videos%20and%20telling%20stories%20in%0A%20%20the%20form%20of%20novel%20text%0AAuthor%3A%20Kangning%20Li%20and%20Zheyang%20Jia%20and%20Anyu%20Ying%0AAbstract%3A%20%20%20Multimodal%20video-to-text%20models%20have%20made%20considerable%20progress%2C%20primarily%20in%0Agenerating%20brief%20descriptions%20of%20video%20content.%20However%2C%20there%20is%20still%20a%0Adeficiency%20in%20generating%20rich%20long-form%20text%20descriptions%20that%20integrate%20both%0Avideo%20and%20audio.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20called%20M2S%2C%20designed%0Ato%20generate%20novel-length%20text%20by%20combining%20audio%2C%20video%2C%20and%20character%0Arecognition.%20M2S%20includes%20modules%20for%20video%20long-form%20text%20description%20and%0Acomprehension%2C%20audio-based%20analysis%20of%20emotion%2C%20speech%20rate%2C%20and%20character%0Aalignment%2C%20and%20visual-based%20character%20recognition%20alignment.%20By%20integrating%0Amultimodal%20information%20using%20the%20large%20language%20model%20GPT4o%2C%20M2S%20stands%20out%20in%0Athe%20field%20of%20multimodal%20text%20generation.%20We%20demonstrate%20the%20effectiveness%20and%0Aaccuracy%20of%20M2S%20through%20comparative%20experiments%20and%20human%20evaluation.%0AAdditionally%2C%20the%20model%20framework%20has%20good%20scalability%20and%20significant%0Apotential%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovie2Story%253A%2520A%2520framework%2520for%2520understanding%2520videos%2520and%2520telling%2520stories%2520in%250A%2520%2520the%2520form%2520of%2520novel%2520text%26entry.906535625%3DKangning%2520Li%2520and%2520Zheyang%2520Jia%2520and%2520Anyu%2520Ying%26entry.1292438233%3D%2520%2520Multimodal%2520video-to-text%2520models%2520have%2520made%2520considerable%2520progress%252C%2520primarily%2520in%250Agenerating%2520brief%2520descriptions%2520of%2520video%2520content.%2520However%252C%2520there%2520is%2520still%2520a%250Adeficiency%2520in%2520generating%2520rich%2520long-form%2520text%2520descriptions%2520that%2520integrate%2520both%250Avideo%2520and%2520audio.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520framework%2520called%2520M2S%252C%2520designed%250Ato%2520generate%2520novel-length%2520text%2520by%2520combining%2520audio%252C%2520video%252C%2520and%2520character%250Arecognition.%2520M2S%2520includes%2520modules%2520for%2520video%2520long-form%2520text%2520description%2520and%250Acomprehension%252C%2520audio-based%2520analysis%2520of%2520emotion%252C%2520speech%2520rate%252C%2520and%2520character%250Aalignment%252C%2520and%2520visual-based%2520character%2520recognition%2520alignment.%2520By%2520integrating%250Amultimodal%2520information%2520using%2520the%2520large%2520language%2520model%2520GPT4o%252C%2520M2S%2520stands%2520out%2520in%250Athe%2520field%2520of%2520multimodal%2520text%2520generation.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%250Aaccuracy%2520of%2520M2S%2520through%2520comparative%2520experiments%2520and%2520human%2520evaluation.%250AAdditionally%252C%2520the%2520model%2520framework%2520has%2520good%2520scalability%2520and%2520significant%250Apotential%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Movie2Story%3A%20A%20framework%20for%20understanding%20videos%20and%20telling%20stories%20in%0A%20%20the%20form%20of%20novel%20text&entry.906535625=Kangning%20Li%20and%20Zheyang%20Jia%20and%20Anyu%20Ying&entry.1292438233=%20%20Multimodal%20video-to-text%20models%20have%20made%20considerable%20progress%2C%20primarily%20in%0Agenerating%20brief%20descriptions%20of%20video%20content.%20However%2C%20there%20is%20still%20a%0Adeficiency%20in%20generating%20rich%20long-form%20text%20descriptions%20that%20integrate%20both%0Avideo%20and%20audio.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20called%20M2S%2C%20designed%0Ato%20generate%20novel-length%20text%20by%20combining%20audio%2C%20video%2C%20and%20character%0Arecognition.%20M2S%20includes%20modules%20for%20video%20long-form%20text%20description%20and%0Acomprehension%2C%20audio-based%20analysis%20of%20emotion%2C%20speech%20rate%2C%20and%20character%0Aalignment%2C%20and%20visual-based%20character%20recognition%20alignment.%20By%20integrating%0Amultimodal%20information%20using%20the%20large%20language%20model%20GPT4o%2C%20M2S%20stands%20out%20in%0Athe%20field%20of%20multimodal%20text%20generation.%20We%20demonstrate%20the%20effectiveness%20and%0Aaccuracy%20of%20M2S%20through%20comparative%20experiments%20and%20human%20evaluation.%0AAdditionally%2C%20the%20model%20framework%20has%20good%20scalability%20and%20significant%0Apotential%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14965v1&entry.124074799=Read"},
{"title": "Progressive Multimodal Reasoning via Active Retrieval", "author": "Guanting Dong and Chenghao Zhang and Mengjie Deng and Yutao Zhu and Zhicheng Dou and Ji-Rong Wen", "abstract": "  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n", "link": "http://arxiv.org/abs/2412.14835v1", "date": "2024-12-19", "relevancy": 2.232, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Multimodal%20Reasoning%20via%20Active%20Retrieval&body=Title%3A%20Progressive%20Multimodal%20Reasoning%20via%20Active%20Retrieval%0AAuthor%3A%20Guanting%20Dong%20and%20Chenghao%20Zhang%20and%20Mengjie%20Deng%20and%20Yutao%20Zhu%20and%20Zhicheng%20Dou%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Multi-step%20multimodal%20reasoning%20tasks%20pose%20significant%20challenges%20for%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20and%20finding%20effective%20ways%20to%20enhance%0Atheir%20performance%20in%20such%20scenarios%20remains%20an%20unresolved%20issue.%20In%20this%20paper%2C%0Awe%20propose%20AR-MCTS%2C%20a%20universal%20framework%20designed%20to%20progressively%20improve%20the%0Areasoning%20capabilities%20of%20MLLMs%20through%20Active%20Retrieval%20%28AR%29%20and%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29.%20Our%20approach%20begins%20with%20the%20development%20of%20a%20unified%0Aretrieval%20module%20that%20retrieves%20key%20supporting%20insights%20for%20solving%20complex%0Areasoning%20problems%20from%20a%20hybrid-modal%20retrieval%20corpus.%20To%20bridge%20the%20gap%20in%0Aautomated%20multimodal%20reasoning%20verification%2C%20we%20employ%20the%20MCTS%20algorithm%0Acombined%20with%20an%20active%20retrieval%20mechanism%2C%20which%20enables%20the%20automatic%0Ageneration%20of%20step-wise%20annotations.%20This%20strategy%20dynamically%20retrieves%20key%0Ainsights%20for%20each%20reasoning%20step%2C%20moving%20beyond%20traditional%20beam%20search%0Asampling%20to%20improve%20the%20diversity%20and%20reliability%20of%20the%20reasoning%20space.%0AAdditionally%2C%20we%20introduce%20a%20process%20reward%20model%20that%20aligns%20progressively%20to%0Asupport%20the%20automatic%20verification%20of%20multimodal%20reasoning%20tasks.%20Experimental%0Aresults%20across%20three%20complex%20multimodal%20reasoning%20benchmarks%20confirm%20the%0Aeffectiveness%20of%20the%20AR-MCTS%20framework%20in%20enhancing%20the%20performance%20of%20various%0Amultimodal%20models.%20Further%20analysis%20demonstrates%20that%20AR-MCTS%20can%20optimize%0Asampling%20diversity%20and%20accuracy%2C%20yielding%20reliable%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Multimodal%2520Reasoning%2520via%2520Active%2520Retrieval%26entry.906535625%3DGuanting%2520Dong%2520and%2520Chenghao%2520Zhang%2520and%2520Mengjie%2520Deng%2520and%2520Yutao%2520Zhu%2520and%2520Zhicheng%2520Dou%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Multi-step%2520multimodal%2520reasoning%2520tasks%2520pose%2520significant%2520challenges%2520for%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520and%2520finding%2520effective%2520ways%2520to%2520enhance%250Atheir%2520performance%2520in%2520such%2520scenarios%2520remains%2520an%2520unresolved%2520issue.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520AR-MCTS%252C%2520a%2520universal%2520framework%2520designed%2520to%2520progressively%2520improve%2520the%250Areasoning%2520capabilities%2520of%2520MLLMs%2520through%2520Active%2520Retrieval%2520%2528AR%2529%2520and%2520Monte%2520Carlo%250ATree%2520Search%2520%2528MCTS%2529.%2520Our%2520approach%2520begins%2520with%2520the%2520development%2520of%2520a%2520unified%250Aretrieval%2520module%2520that%2520retrieves%2520key%2520supporting%2520insights%2520for%2520solving%2520complex%250Areasoning%2520problems%2520from%2520a%2520hybrid-modal%2520retrieval%2520corpus.%2520To%2520bridge%2520the%2520gap%2520in%250Aautomated%2520multimodal%2520reasoning%2520verification%252C%2520we%2520employ%2520the%2520MCTS%2520algorithm%250Acombined%2520with%2520an%2520active%2520retrieval%2520mechanism%252C%2520which%2520enables%2520the%2520automatic%250Ageneration%2520of%2520step-wise%2520annotations.%2520This%2520strategy%2520dynamically%2520retrieves%2520key%250Ainsights%2520for%2520each%2520reasoning%2520step%252C%2520moving%2520beyond%2520traditional%2520beam%2520search%250Asampling%2520to%2520improve%2520the%2520diversity%2520and%2520reliability%2520of%2520the%2520reasoning%2520space.%250AAdditionally%252C%2520we%2520introduce%2520a%2520process%2520reward%2520model%2520that%2520aligns%2520progressively%2520to%250Asupport%2520the%2520automatic%2520verification%2520of%2520multimodal%2520reasoning%2520tasks.%2520Experimental%250Aresults%2520across%2520three%2520complex%2520multimodal%2520reasoning%2520benchmarks%2520confirm%2520the%250Aeffectiveness%2520of%2520the%2520AR-MCTS%2520framework%2520in%2520enhancing%2520the%2520performance%2520of%2520various%250Amultimodal%2520models.%2520Further%2520analysis%2520demonstrates%2520that%2520AR-MCTS%2520can%2520optimize%250Asampling%2520diversity%2520and%2520accuracy%252C%2520yielding%2520reliable%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Multimodal%20Reasoning%20via%20Active%20Retrieval&entry.906535625=Guanting%20Dong%20and%20Chenghao%20Zhang%20and%20Mengjie%20Deng%20and%20Yutao%20Zhu%20and%20Zhicheng%20Dou%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Multi-step%20multimodal%20reasoning%20tasks%20pose%20significant%20challenges%20for%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20and%20finding%20effective%20ways%20to%20enhance%0Atheir%20performance%20in%20such%20scenarios%20remains%20an%20unresolved%20issue.%20In%20this%20paper%2C%0Awe%20propose%20AR-MCTS%2C%20a%20universal%20framework%20designed%20to%20progressively%20improve%20the%0Areasoning%20capabilities%20of%20MLLMs%20through%20Active%20Retrieval%20%28AR%29%20and%20Monte%20Carlo%0ATree%20Search%20%28MCTS%29.%20Our%20approach%20begins%20with%20the%20development%20of%20a%20unified%0Aretrieval%20module%20that%20retrieves%20key%20supporting%20insights%20for%20solving%20complex%0Areasoning%20problems%20from%20a%20hybrid-modal%20retrieval%20corpus.%20To%20bridge%20the%20gap%20in%0Aautomated%20multimodal%20reasoning%20verification%2C%20we%20employ%20the%20MCTS%20algorithm%0Acombined%20with%20an%20active%20retrieval%20mechanism%2C%20which%20enables%20the%20automatic%0Ageneration%20of%20step-wise%20annotations.%20This%20strategy%20dynamically%20retrieves%20key%0Ainsights%20for%20each%20reasoning%20step%2C%20moving%20beyond%20traditional%20beam%20search%0Asampling%20to%20improve%20the%20diversity%20and%20reliability%20of%20the%20reasoning%20space.%0AAdditionally%2C%20we%20introduce%20a%20process%20reward%20model%20that%20aligns%20progressively%20to%0Asupport%20the%20automatic%20verification%20of%20multimodal%20reasoning%20tasks.%20Experimental%0Aresults%20across%20three%20complex%20multimodal%20reasoning%20benchmarks%20confirm%20the%0Aeffectiveness%20of%20the%20AR-MCTS%20framework%20in%20enhancing%20the%20performance%20of%20various%0Amultimodal%20models.%20Further%20analysis%20demonstrates%20that%20AR-MCTS%20can%20optimize%0Asampling%20diversity%20and%20accuracy%2C%20yielding%20reliable%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14835v1&entry.124074799=Read"},
{"title": "Union-over-Intersections: Object Detection beyond Winner-Takes-All", "author": "Aritra Bhowmik and Pascal Mettes and Martin R. Oswald and Cees G. M. Snoek", "abstract": "  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n", "link": "http://arxiv.org/abs/2311.18512v2", "date": "2024-12-19", "relevancy": 2.229, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5813}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Union-over-Intersections%3A%20Object%20Detection%20beyond%20Winner-Takes-All&body=Title%3A%20Union-over-Intersections%3A%20Object%20Detection%20beyond%20Winner-Takes-All%0AAuthor%3A%20Aritra%20Bhowmik%20and%20Pascal%20Mettes%20and%20Martin%20R.%20Oswald%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20This%20paper%20revisits%20the%20problem%20of%20predicting%20box%20locations%20in%20object%0Adetection%20architectures.%20Typically%2C%20each%20box%20proposal%20or%20box%20query%20aims%20to%0Adirectly%20maximize%20the%20intersection-over-union%20score%20with%20the%20ground%20truth%2C%0Afollowed%20by%20a%20winner-takes-all%20non-maximum%20suppression%20where%20only%20the%20highest%0Ascoring%20box%20in%20each%20region%20is%20retained.%20We%20observe%20that%20both%20steps%20are%0Asub-optimal%3A%20the%20first%20involves%20regressing%20proposals%20to%20the%20entire%20ground%0Atruth%2C%20which%20is%20a%20difficult%20task%20even%20with%20large%20receptive%20fields%2C%20and%20the%0Asecond%20neglects%20valuable%20information%20from%20boxes%20other%20than%20the%20top%20candidate.%0AInstead%20of%20regressing%20proposals%20to%20the%20whole%20ground%20truth%2C%20we%20propose%20a%20simpler%0Aapproach%3A%20regress%20only%20to%20the%20area%20of%20intersection%20between%20the%20proposal%20and%20the%0Aground%20truth.%20This%20avoids%20the%20need%20for%20proposals%20to%20extrapolate%20beyond%20their%0Avisual%20scope%2C%20improving%20localization%20accuracy.%20Rather%20than%20adopting%20a%0Awinner-takes-all%20strategy%2C%20we%20take%20the%20union%20over%20the%20regressed%20intersections%0Aof%20all%20boxes%20in%20a%20region%20to%20generate%20the%20final%20box%20outputs.%20Our%20plug-and-play%0Amethod%20integrates%20seamlessly%20into%20proposal-based%2C%20grid-based%2C%20and%20query-based%0Adetection%20architectures%20with%20minimal%20modifications%2C%20consistently%20improving%0Aobject%20localization%20and%20instance%20segmentation.%20We%20demonstrate%20its%20broad%0Aapplicability%20and%20versatility%20across%20various%20detection%20and%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnion-over-Intersections%253A%2520Object%2520Detection%2520beyond%2520Winner-Takes-All%26entry.906535625%3DAritra%2520Bhowmik%2520and%2520Pascal%2520Mettes%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Cees%2520G.%2520M.%2520Snoek%26entry.1292438233%3D%2520%2520This%2520paper%2520revisits%2520the%2520problem%2520of%2520predicting%2520box%2520locations%2520in%2520object%250Adetection%2520architectures.%2520Typically%252C%2520each%2520box%2520proposal%2520or%2520box%2520query%2520aims%2520to%250Adirectly%2520maximize%2520the%2520intersection-over-union%2520score%2520with%2520the%2520ground%2520truth%252C%250Afollowed%2520by%2520a%2520winner-takes-all%2520non-maximum%2520suppression%2520where%2520only%2520the%2520highest%250Ascoring%2520box%2520in%2520each%2520region%2520is%2520retained.%2520We%2520observe%2520that%2520both%2520steps%2520are%250Asub-optimal%253A%2520the%2520first%2520involves%2520regressing%2520proposals%2520to%2520the%2520entire%2520ground%250Atruth%252C%2520which%2520is%2520a%2520difficult%2520task%2520even%2520with%2520large%2520receptive%2520fields%252C%2520and%2520the%250Asecond%2520neglects%2520valuable%2520information%2520from%2520boxes%2520other%2520than%2520the%2520top%2520candidate.%250AInstead%2520of%2520regressing%2520proposals%2520to%2520the%2520whole%2520ground%2520truth%252C%2520we%2520propose%2520a%2520simpler%250Aapproach%253A%2520regress%2520only%2520to%2520the%2520area%2520of%2520intersection%2520between%2520the%2520proposal%2520and%2520the%250Aground%2520truth.%2520This%2520avoids%2520the%2520need%2520for%2520proposals%2520to%2520extrapolate%2520beyond%2520their%250Avisual%2520scope%252C%2520improving%2520localization%2520accuracy.%2520Rather%2520than%2520adopting%2520a%250Awinner-takes-all%2520strategy%252C%2520we%2520take%2520the%2520union%2520over%2520the%2520regressed%2520intersections%250Aof%2520all%2520boxes%2520in%2520a%2520region%2520to%2520generate%2520the%2520final%2520box%2520outputs.%2520Our%2520plug-and-play%250Amethod%2520integrates%2520seamlessly%2520into%2520proposal-based%252C%2520grid-based%252C%2520and%2520query-based%250Adetection%2520architectures%2520with%2520minimal%2520modifications%252C%2520consistently%2520improving%250Aobject%2520localization%2520and%2520instance%2520segmentation.%2520We%2520demonstrate%2520its%2520broad%250Aapplicability%2520and%2520versatility%2520across%2520various%2520detection%2520and%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Union-over-Intersections%3A%20Object%20Detection%20beyond%20Winner-Takes-All&entry.906535625=Aritra%20Bhowmik%20and%20Pascal%20Mettes%20and%20Martin%20R.%20Oswald%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20This%20paper%20revisits%20the%20problem%20of%20predicting%20box%20locations%20in%20object%0Adetection%20architectures.%20Typically%2C%20each%20box%20proposal%20or%20box%20query%20aims%20to%0Adirectly%20maximize%20the%20intersection-over-union%20score%20with%20the%20ground%20truth%2C%0Afollowed%20by%20a%20winner-takes-all%20non-maximum%20suppression%20where%20only%20the%20highest%0Ascoring%20box%20in%20each%20region%20is%20retained.%20We%20observe%20that%20both%20steps%20are%0Asub-optimal%3A%20the%20first%20involves%20regressing%20proposals%20to%20the%20entire%20ground%0Atruth%2C%20which%20is%20a%20difficult%20task%20even%20with%20large%20receptive%20fields%2C%20and%20the%0Asecond%20neglects%20valuable%20information%20from%20boxes%20other%20than%20the%20top%20candidate.%0AInstead%20of%20regressing%20proposals%20to%20the%20whole%20ground%20truth%2C%20we%20propose%20a%20simpler%0Aapproach%3A%20regress%20only%20to%20the%20area%20of%20intersection%20between%20the%20proposal%20and%20the%0Aground%20truth.%20This%20avoids%20the%20need%20for%20proposals%20to%20extrapolate%20beyond%20their%0Avisual%20scope%2C%20improving%20localization%20accuracy.%20Rather%20than%20adopting%20a%0Awinner-takes-all%20strategy%2C%20we%20take%20the%20union%20over%20the%20regressed%20intersections%0Aof%20all%20boxes%20in%20a%20region%20to%20generate%20the%20final%20box%20outputs.%20Our%20plug-and-play%0Amethod%20integrates%20seamlessly%20into%20proposal-based%2C%20grid-based%2C%20and%20query-based%0Adetection%20architectures%20with%20minimal%20modifications%2C%20consistently%20improving%0Aobject%20localization%20and%20instance%20segmentation.%20We%20demonstrate%20its%20broad%0Aapplicability%20and%20versatility%20across%20various%20detection%20and%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18512v2&entry.124074799=Read"},
{"title": "Point Cloud Semantic Segmentation with Sparse and Inhomogeneous\n  Annotations", "author": "Zhiyi Pan and Nan Zhang and Wei Gao and Shan Liu and Ge Li", "abstract": "  Utilizing uniformly distributed sparse annotations, weakly supervised\nlearning alleviates the heavy reliance on fine-grained annotations in point\ncloud semantic segmentation tasks. However, few works discuss the inhomogeneity\nof sparse annotations, albeit it is common in real-world scenarios. Therefore,\nthis work introduces the probability density function into the gradient\nsampling approximation method to qualitatively analyze the impact of annotation\nsparsity and inhomogeneity under weakly supervised learning. Based on our\nanalysis, we propose an Adaptive Annotation Distribution Network (AADNet)\ncapable of robust learning on arbitrarily distributed sparse annotations.\nSpecifically, we propose a label-aware point cloud downsampling strategy to\nincrease the proportion of annotations involved in the training stage.\nFurthermore, we design the multiplicative dynamic entropy as the gradient\ncalibration function to mitigate the gradient bias caused by non-uniformly\ndistributed sparse annotations and explicitly reduce the epistemic uncertainty.\nWithout any prior restrictions and additional information, our proposed method\nachieves comprehensive performance improvements at multiple label rates and\ndifferent annotation distributions.\n", "link": "http://arxiv.org/abs/2312.06259v2", "date": "2024-12-19", "relevancy": 2.2262, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5621}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5539}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Semantic%20Segmentation%20with%20Sparse%20and%20Inhomogeneous%0A%20%20Annotations&body=Title%3A%20Point%20Cloud%20Semantic%20Segmentation%20with%20Sparse%20and%20Inhomogeneous%0A%20%20Annotations%0AAuthor%3A%20Zhiyi%20Pan%20and%20Nan%20Zhang%20and%20Wei%20Gao%20and%20Shan%20Liu%20and%20Ge%20Li%0AAbstract%3A%20%20%20Utilizing%20uniformly%20distributed%20sparse%20annotations%2C%20weakly%20supervised%0Alearning%20alleviates%20the%20heavy%20reliance%20on%20fine-grained%20annotations%20in%20point%0Acloud%20semantic%20segmentation%20tasks.%20However%2C%20few%20works%20discuss%20the%20inhomogeneity%0Aof%20sparse%20annotations%2C%20albeit%20it%20is%20common%20in%20real-world%20scenarios.%20Therefore%2C%0Athis%20work%20introduces%20the%20probability%20density%20function%20into%20the%20gradient%0Asampling%20approximation%20method%20to%20qualitatively%20analyze%20the%20impact%20of%20annotation%0Asparsity%20and%20inhomogeneity%20under%20weakly%20supervised%20learning.%20Based%20on%20our%0Aanalysis%2C%20we%20propose%20an%20Adaptive%20Annotation%20Distribution%20Network%20%28AADNet%29%0Acapable%20of%20robust%20learning%20on%20arbitrarily%20distributed%20sparse%20annotations.%0ASpecifically%2C%20we%20propose%20a%20label-aware%20point%20cloud%20downsampling%20strategy%20to%0Aincrease%20the%20proportion%20of%20annotations%20involved%20in%20the%20training%20stage.%0AFurthermore%2C%20we%20design%20the%20multiplicative%20dynamic%20entropy%20as%20the%20gradient%0Acalibration%20function%20to%20mitigate%20the%20gradient%20bias%20caused%20by%20non-uniformly%0Adistributed%20sparse%20annotations%20and%20explicitly%20reduce%20the%20epistemic%20uncertainty.%0AWithout%20any%20prior%20restrictions%20and%20additional%20information%2C%20our%20proposed%20method%0Aachieves%20comprehensive%20performance%20improvements%20at%20multiple%20label%20rates%20and%0Adifferent%20annotation%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Cloud%2520Semantic%2520Segmentation%2520with%2520Sparse%2520and%2520Inhomogeneous%250A%2520%2520Annotations%26entry.906535625%3DZhiyi%2520Pan%2520and%2520Nan%2520Zhang%2520and%2520Wei%2520Gao%2520and%2520Shan%2520Liu%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Utilizing%2520uniformly%2520distributed%2520sparse%2520annotations%252C%2520weakly%2520supervised%250Alearning%2520alleviates%2520the%2520heavy%2520reliance%2520on%2520fine-grained%2520annotations%2520in%2520point%250Acloud%2520semantic%2520segmentation%2520tasks.%2520However%252C%2520few%2520works%2520discuss%2520the%2520inhomogeneity%250Aof%2520sparse%2520annotations%252C%2520albeit%2520it%2520is%2520common%2520in%2520real-world%2520scenarios.%2520Therefore%252C%250Athis%2520work%2520introduces%2520the%2520probability%2520density%2520function%2520into%2520the%2520gradient%250Asampling%2520approximation%2520method%2520to%2520qualitatively%2520analyze%2520the%2520impact%2520of%2520annotation%250Asparsity%2520and%2520inhomogeneity%2520under%2520weakly%2520supervised%2520learning.%2520Based%2520on%2520our%250Aanalysis%252C%2520we%2520propose%2520an%2520Adaptive%2520Annotation%2520Distribution%2520Network%2520%2528AADNet%2529%250Acapable%2520of%2520robust%2520learning%2520on%2520arbitrarily%2520distributed%2520sparse%2520annotations.%250ASpecifically%252C%2520we%2520propose%2520a%2520label-aware%2520point%2520cloud%2520downsampling%2520strategy%2520to%250Aincrease%2520the%2520proportion%2520of%2520annotations%2520involved%2520in%2520the%2520training%2520stage.%250AFurthermore%252C%2520we%2520design%2520the%2520multiplicative%2520dynamic%2520entropy%2520as%2520the%2520gradient%250Acalibration%2520function%2520to%2520mitigate%2520the%2520gradient%2520bias%2520caused%2520by%2520non-uniformly%250Adistributed%2520sparse%2520annotations%2520and%2520explicitly%2520reduce%2520the%2520epistemic%2520uncertainty.%250AWithout%2520any%2520prior%2520restrictions%2520and%2520additional%2520information%252C%2520our%2520proposed%2520method%250Aachieves%2520comprehensive%2520performance%2520improvements%2520at%2520multiple%2520label%2520rates%2520and%250Adifferent%2520annotation%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Semantic%20Segmentation%20with%20Sparse%20and%20Inhomogeneous%0A%20%20Annotations&entry.906535625=Zhiyi%20Pan%20and%20Nan%20Zhang%20and%20Wei%20Gao%20and%20Shan%20Liu%20and%20Ge%20Li&entry.1292438233=%20%20Utilizing%20uniformly%20distributed%20sparse%20annotations%2C%20weakly%20supervised%0Alearning%20alleviates%20the%20heavy%20reliance%20on%20fine-grained%20annotations%20in%20point%0Acloud%20semantic%20segmentation%20tasks.%20However%2C%20few%20works%20discuss%20the%20inhomogeneity%0Aof%20sparse%20annotations%2C%20albeit%20it%20is%20common%20in%20real-world%20scenarios.%20Therefore%2C%0Athis%20work%20introduces%20the%20probability%20density%20function%20into%20the%20gradient%0Asampling%20approximation%20method%20to%20qualitatively%20analyze%20the%20impact%20of%20annotation%0Asparsity%20and%20inhomogeneity%20under%20weakly%20supervised%20learning.%20Based%20on%20our%0Aanalysis%2C%20we%20propose%20an%20Adaptive%20Annotation%20Distribution%20Network%20%28AADNet%29%0Acapable%20of%20robust%20learning%20on%20arbitrarily%20distributed%20sparse%20annotations.%0ASpecifically%2C%20we%20propose%20a%20label-aware%20point%20cloud%20downsampling%20strategy%20to%0Aincrease%20the%20proportion%20of%20annotations%20involved%20in%20the%20training%20stage.%0AFurthermore%2C%20we%20design%20the%20multiplicative%20dynamic%20entropy%20as%20the%20gradient%0Acalibration%20function%20to%20mitigate%20the%20gradient%20bias%20caused%20by%20non-uniformly%0Adistributed%20sparse%20annotations%20and%20explicitly%20reduce%20the%20epistemic%20uncertainty.%0AWithout%20any%20prior%20restrictions%20and%20additional%20information%2C%20our%20proposed%20method%0Aachieves%20comprehensive%20performance%20improvements%20at%20multiple%20label%20rates%20and%0Adifferent%20annotation%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06259v2&entry.124074799=Read"},
{"title": "EarthDial: Turning Multi-sensory Earth Observations to Interactive\n  Dialogues", "author": "Sagar Soni and Akshay Dudhane and Hiyam Debary and Mustansar Fiaz and Muhammad Akhtar Munir and Muhammad Sohail Danish and Paolo Fraccaro and Campbell D Watson and Levente J Klein and Fahad Shahbaz Khan and Salman Khan", "abstract": "  Automated analysis of vast Earth observation data via interactive\nVision-Language Models (VLMs) can unlock new opportunities for environmental\nmonitoring, disaster response, and resource management. Existing generic VLMs\ndo not perform well on Remote Sensing data, while the recent Geo-spatial VLMs\nremain restricted to a fixed resolution and few sensor modalities. In this\npaper, we introduce EarthDial, a conversational assistant specifically designed\nfor Earth Observation (EO) data, transforming complex, multi-sensory Earth\nobservations into interactive, natural language dialogues. EarthDial supports\nmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a wide\nrange of remote sensing tasks, including classification, detection, captioning,\nquestion answering, visual reasoning, and visual grounding. To achieve this, we\nintroduce an extensive instruction tuning dataset comprising over 11.11M\ninstruction pairs covering RGB, Synthetic Aperture Radar (SAR), and\nmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,\nEarthDial handles bi-temporal and multi-temporal sequence analysis for\napplications like change detection. Our extensive experimental results on 37\ndownstream applications demonstrate that EarthDial outperforms existing generic\nand domain-specific models, achieving better generalization across various EO\ntasks.\n", "link": "http://arxiv.org/abs/2412.15190v1", "date": "2024-12-19", "relevancy": 2.1926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthDial%3A%20Turning%20Multi-sensory%20Earth%20Observations%20to%20Interactive%0A%20%20Dialogues&body=Title%3A%20EarthDial%3A%20Turning%20Multi-sensory%20Earth%20Observations%20to%20Interactive%0A%20%20Dialogues%0AAuthor%3A%20Sagar%20Soni%20and%20Akshay%20Dudhane%20and%20Hiyam%20Debary%20and%20Mustansar%20Fiaz%20and%20Muhammad%20Akhtar%20Munir%20and%20Muhammad%20Sohail%20Danish%20and%20Paolo%20Fraccaro%20and%20Campbell%20D%20Watson%20and%20Levente%20J%20Klein%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Automated%20analysis%20of%20vast%20Earth%20observation%20data%20via%20interactive%0AVision-Language%20Models%20%28VLMs%29%20can%20unlock%20new%20opportunities%20for%20environmental%0Amonitoring%2C%20disaster%20response%2C%20and%20resource%20management.%20Existing%20generic%20VLMs%0Ado%20not%20perform%20well%20on%20Remote%20Sensing%20data%2C%20while%20the%20recent%20Geo-spatial%20VLMs%0Aremain%20restricted%20to%20a%20fixed%20resolution%20and%20few%20sensor%20modalities.%20In%20this%0Apaper%2C%20we%20introduce%20EarthDial%2C%20a%20conversational%20assistant%20specifically%20designed%0Afor%20Earth%20Observation%20%28EO%29%20data%2C%20transforming%20complex%2C%20multi-sensory%20Earth%0Aobservations%20into%20interactive%2C%20natural%20language%20dialogues.%20EarthDial%20supports%0Amulti-spectral%2C%20multi-temporal%2C%20and%20multi-resolution%20imagery%2C%20enabling%20a%20wide%0Arange%20of%20remote%20sensing%20tasks%2C%20including%20classification%2C%20detection%2C%20captioning%2C%0Aquestion%20answering%2C%20visual%20reasoning%2C%20and%20visual%20grounding.%20To%20achieve%20this%2C%20we%0Aintroduce%20an%20extensive%20instruction%20tuning%20dataset%20comprising%20over%2011.11M%0Ainstruction%20pairs%20covering%20RGB%2C%20Synthetic%20Aperture%20Radar%20%28SAR%29%2C%20and%0Amultispectral%20modalities%20such%20as%20Near-Infrared%20%28NIR%29%20and%20infrared.%20Furthermore%2C%0AEarthDial%20handles%20bi-temporal%20and%20multi-temporal%20sequence%20analysis%20for%0Aapplications%20like%20change%20detection.%20Our%20extensive%20experimental%20results%20on%2037%0Adownstream%20applications%20demonstrate%20that%20EarthDial%20outperforms%20existing%20generic%0Aand%20domain-specific%20models%2C%20achieving%20better%20generalization%20across%20various%20EO%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthDial%253A%2520Turning%2520Multi-sensory%2520Earth%2520Observations%2520to%2520Interactive%250A%2520%2520Dialogues%26entry.906535625%3DSagar%2520Soni%2520and%2520Akshay%2520Dudhane%2520and%2520Hiyam%2520Debary%2520and%2520Mustansar%2520Fiaz%2520and%2520Muhammad%2520Akhtar%2520Munir%2520and%2520Muhammad%2520Sohail%2520Danish%2520and%2520Paolo%2520Fraccaro%2520and%2520Campbell%2520D%2520Watson%2520and%2520Levente%2520J%2520Klein%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Automated%2520analysis%2520of%2520vast%2520Earth%2520observation%2520data%2520via%2520interactive%250AVision-Language%2520Models%2520%2528VLMs%2529%2520can%2520unlock%2520new%2520opportunities%2520for%2520environmental%250Amonitoring%252C%2520disaster%2520response%252C%2520and%2520resource%2520management.%2520Existing%2520generic%2520VLMs%250Ado%2520not%2520perform%2520well%2520on%2520Remote%2520Sensing%2520data%252C%2520while%2520the%2520recent%2520Geo-spatial%2520VLMs%250Aremain%2520restricted%2520to%2520a%2520fixed%2520resolution%2520and%2520few%2520sensor%2520modalities.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520EarthDial%252C%2520a%2520conversational%2520assistant%2520specifically%2520designed%250Afor%2520Earth%2520Observation%2520%2528EO%2529%2520data%252C%2520transforming%2520complex%252C%2520multi-sensory%2520Earth%250Aobservations%2520into%2520interactive%252C%2520natural%2520language%2520dialogues.%2520EarthDial%2520supports%250Amulti-spectral%252C%2520multi-temporal%252C%2520and%2520multi-resolution%2520imagery%252C%2520enabling%2520a%2520wide%250Arange%2520of%2520remote%2520sensing%2520tasks%252C%2520including%2520classification%252C%2520detection%252C%2520captioning%252C%250Aquestion%2520answering%252C%2520visual%2520reasoning%252C%2520and%2520visual%2520grounding.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520an%2520extensive%2520instruction%2520tuning%2520dataset%2520comprising%2520over%252011.11M%250Ainstruction%2520pairs%2520covering%2520RGB%252C%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%252C%2520and%250Amultispectral%2520modalities%2520such%2520as%2520Near-Infrared%2520%2528NIR%2529%2520and%2520infrared.%2520Furthermore%252C%250AEarthDial%2520handles%2520bi-temporal%2520and%2520multi-temporal%2520sequence%2520analysis%2520for%250Aapplications%2520like%2520change%2520detection.%2520Our%2520extensive%2520experimental%2520results%2520on%252037%250Adownstream%2520applications%2520demonstrate%2520that%2520EarthDial%2520outperforms%2520existing%2520generic%250Aand%2520domain-specific%2520models%252C%2520achieving%2520better%2520generalization%2520across%2520various%2520EO%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthDial%3A%20Turning%20Multi-sensory%20Earth%20Observations%20to%20Interactive%0A%20%20Dialogues&entry.906535625=Sagar%20Soni%20and%20Akshay%20Dudhane%20and%20Hiyam%20Debary%20and%20Mustansar%20Fiaz%20and%20Muhammad%20Akhtar%20Munir%20and%20Muhammad%20Sohail%20Danish%20and%20Paolo%20Fraccaro%20and%20Campbell%20D%20Watson%20and%20Levente%20J%20Klein%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan&entry.1292438233=%20%20Automated%20analysis%20of%20vast%20Earth%20observation%20data%20via%20interactive%0AVision-Language%20Models%20%28VLMs%29%20can%20unlock%20new%20opportunities%20for%20environmental%0Amonitoring%2C%20disaster%20response%2C%20and%20resource%20management.%20Existing%20generic%20VLMs%0Ado%20not%20perform%20well%20on%20Remote%20Sensing%20data%2C%20while%20the%20recent%20Geo-spatial%20VLMs%0Aremain%20restricted%20to%20a%20fixed%20resolution%20and%20few%20sensor%20modalities.%20In%20this%0Apaper%2C%20we%20introduce%20EarthDial%2C%20a%20conversational%20assistant%20specifically%20designed%0Afor%20Earth%20Observation%20%28EO%29%20data%2C%20transforming%20complex%2C%20multi-sensory%20Earth%0Aobservations%20into%20interactive%2C%20natural%20language%20dialogues.%20EarthDial%20supports%0Amulti-spectral%2C%20multi-temporal%2C%20and%20multi-resolution%20imagery%2C%20enabling%20a%20wide%0Arange%20of%20remote%20sensing%20tasks%2C%20including%20classification%2C%20detection%2C%20captioning%2C%0Aquestion%20answering%2C%20visual%20reasoning%2C%20and%20visual%20grounding.%20To%20achieve%20this%2C%20we%0Aintroduce%20an%20extensive%20instruction%20tuning%20dataset%20comprising%20over%2011.11M%0Ainstruction%20pairs%20covering%20RGB%2C%20Synthetic%20Aperture%20Radar%20%28SAR%29%2C%20and%0Amultispectral%20modalities%20such%20as%20Near-Infrared%20%28NIR%29%20and%20infrared.%20Furthermore%2C%0AEarthDial%20handles%20bi-temporal%20and%20multi-temporal%20sequence%20analysis%20for%0Aapplications%20like%20change%20detection.%20Our%20extensive%20experimental%20results%20on%2037%0Adownstream%20applications%20demonstrate%20that%20EarthDial%20outperforms%20existing%20generic%0Aand%20domain-specific%20models%2C%20achieving%20better%20generalization%20across%20various%20EO%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15190v1&entry.124074799=Read"},
{"title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks", "author": "Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li", "abstract": "  This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.\n", "link": "http://arxiv.org/abs/2412.15204v1", "date": "2024-12-19", "relevancy": 2.1866, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongBench%20v2%3A%20Towards%20Deeper%20Understanding%20and%20Reasoning%20on%20Realistic%0A%20%20Long-context%20Multitasks&body=Title%3A%20LongBench%20v2%3A%20Towards%20Deeper%20Understanding%20and%20Reasoning%20on%20Realistic%0A%20%20Long-context%20Multitasks%0AAuthor%3A%20Yushi%20Bai%20and%20Shangqing%20Tu%20and%20Jiajie%20Zhang%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Xin%20Lv%20and%20Shulin%20Cao%20and%20Jiazheng%20Xu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20This%20paper%20introduces%20LongBench%20v2%2C%20a%20benchmark%20designed%20to%20assess%20the%0Aability%20of%20LLMs%20to%20handle%20long-context%20problems%20requiring%20deep%20understanding%0Aand%20reasoning%20across%20real-world%20multitasks.%20LongBench%20v2%20consists%20of%20503%0Achallenging%20multiple-choice%20questions%2C%20with%20contexts%20ranging%20from%208k%20to%202M%0Awords%2C%20across%20six%20major%20task%20categories%3A%20single-document%20QA%2C%20multi-document%20QA%2C%0Along%20in-context%20learning%2C%20long-dialogue%20history%20understanding%2C%20code%20repository%0Aunderstanding%2C%20and%20long%20structured%20data%20understanding.%20To%20ensure%20the%20breadth%0Aand%20the%20practicality%2C%20we%20collect%20data%20from%20nearly%20100%20highly%20educated%0Aindividuals%20with%20diverse%20professional%20backgrounds.%20We%20employ%20both%20automated%20and%0Amanual%20review%20processes%20to%20maintain%20high%20quality%20and%20difficulty%2C%20resulting%20in%0Ahuman%20experts%20achieving%20only%2053.7%25%20accuracy%20under%20a%2015-minute%20time%20constraint.%0AOur%20evaluation%20reveals%20that%20the%20best-performing%20model%2C%20when%20directly%20answers%0Athe%20questions%2C%20achieves%20only%2050.1%25%20accuracy.%20In%20contrast%2C%20the%20o1-preview%20model%2C%0Awhich%20includes%20longer%20reasoning%2C%20achieves%2057.7%25%2C%20surpassing%20the%20human%20baseline%0Aby%204%25.%20These%20results%20highlight%20the%20importance%20of%20enhanced%20reasoning%20ability%20and%0Ascaling%20inference-time%20compute%20to%20tackle%20the%20long-context%20challenges%20in%0ALongBench%20v2.%20The%20project%20is%20available%20at%20https%3A//longbench2.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongBench%2520v2%253A%2520Towards%2520Deeper%2520Understanding%2520and%2520Reasoning%2520on%2520Realistic%250A%2520%2520Long-context%2520Multitasks%26entry.906535625%3DYushi%2520Bai%2520and%2520Shangqing%2520Tu%2520and%2520Jiajie%2520Zhang%2520and%2520Hao%2520Peng%2520and%2520Xiaozhi%2520Wang%2520and%2520Xin%2520Lv%2520and%2520Shulin%2520Cao%2520and%2520Jiazheng%2520Xu%2520and%2520Lei%2520Hou%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520LongBench%2520v2%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520the%250Aability%2520of%2520LLMs%2520to%2520handle%2520long-context%2520problems%2520requiring%2520deep%2520understanding%250Aand%2520reasoning%2520across%2520real-world%2520multitasks.%2520LongBench%2520v2%2520consists%2520of%2520503%250Achallenging%2520multiple-choice%2520questions%252C%2520with%2520contexts%2520ranging%2520from%25208k%2520to%25202M%250Awords%252C%2520across%2520six%2520major%2520task%2520categories%253A%2520single-document%2520QA%252C%2520multi-document%2520QA%252C%250Along%2520in-context%2520learning%252C%2520long-dialogue%2520history%2520understanding%252C%2520code%2520repository%250Aunderstanding%252C%2520and%2520long%2520structured%2520data%2520understanding.%2520To%2520ensure%2520the%2520breadth%250Aand%2520the%2520practicality%252C%2520we%2520collect%2520data%2520from%2520nearly%2520100%2520highly%2520educated%250Aindividuals%2520with%2520diverse%2520professional%2520backgrounds.%2520We%2520employ%2520both%2520automated%2520and%250Amanual%2520review%2520processes%2520to%2520maintain%2520high%2520quality%2520and%2520difficulty%252C%2520resulting%2520in%250Ahuman%2520experts%2520achieving%2520only%252053.7%2525%2520accuracy%2520under%2520a%252015-minute%2520time%2520constraint.%250AOur%2520evaluation%2520reveals%2520that%2520the%2520best-performing%2520model%252C%2520when%2520directly%2520answers%250Athe%2520questions%252C%2520achieves%2520only%252050.1%2525%2520accuracy.%2520In%2520contrast%252C%2520the%2520o1-preview%2520model%252C%250Awhich%2520includes%2520longer%2520reasoning%252C%2520achieves%252057.7%2525%252C%2520surpassing%2520the%2520human%2520baseline%250Aby%25204%2525.%2520These%2520results%2520highlight%2520the%2520importance%2520of%2520enhanced%2520reasoning%2520ability%2520and%250Ascaling%2520inference-time%2520compute%2520to%2520tackle%2520the%2520long-context%2520challenges%2520in%250ALongBench%2520v2.%2520The%2520project%2520is%2520available%2520at%2520https%253A//longbench2.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongBench%20v2%3A%20Towards%20Deeper%20Understanding%20and%20Reasoning%20on%20Realistic%0A%20%20Long-context%20Multitasks&entry.906535625=Yushi%20Bai%20and%20Shangqing%20Tu%20and%20Jiajie%20Zhang%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Xin%20Lv%20and%20Shulin%20Cao%20and%20Jiazheng%20Xu%20and%20Lei%20Hou%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%20and%20Juanzi%20Li&entry.1292438233=%20%20This%20paper%20introduces%20LongBench%20v2%2C%20a%20benchmark%20designed%20to%20assess%20the%0Aability%20of%20LLMs%20to%20handle%20long-context%20problems%20requiring%20deep%20understanding%0Aand%20reasoning%20across%20real-world%20multitasks.%20LongBench%20v2%20consists%20of%20503%0Achallenging%20multiple-choice%20questions%2C%20with%20contexts%20ranging%20from%208k%20to%202M%0Awords%2C%20across%20six%20major%20task%20categories%3A%20single-document%20QA%2C%20multi-document%20QA%2C%0Along%20in-context%20learning%2C%20long-dialogue%20history%20understanding%2C%20code%20repository%0Aunderstanding%2C%20and%20long%20structured%20data%20understanding.%20To%20ensure%20the%20breadth%0Aand%20the%20practicality%2C%20we%20collect%20data%20from%20nearly%20100%20highly%20educated%0Aindividuals%20with%20diverse%20professional%20backgrounds.%20We%20employ%20both%20automated%20and%0Amanual%20review%20processes%20to%20maintain%20high%20quality%20and%20difficulty%2C%20resulting%20in%0Ahuman%20experts%20achieving%20only%2053.7%25%20accuracy%20under%20a%2015-minute%20time%20constraint.%0AOur%20evaluation%20reveals%20that%20the%20best-performing%20model%2C%20when%20directly%20answers%0Athe%20questions%2C%20achieves%20only%2050.1%25%20accuracy.%20In%20contrast%2C%20the%20o1-preview%20model%2C%0Awhich%20includes%20longer%20reasoning%2C%20achieves%2057.7%25%2C%20surpassing%20the%20human%20baseline%0Aby%204%25.%20These%20results%20highlight%20the%20importance%20of%20enhanced%20reasoning%20ability%20and%0Ascaling%20inference-time%20compute%20to%20tackle%20the%20long-context%20challenges%20in%0ALongBench%20v2.%20The%20project%20is%20available%20at%20https%3A//longbench2.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15204v1&entry.124074799=Read"},
{"title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature\n  Enhancement Network", "author": "Guipeng Xin and Duanfeng Chu and Liping Lu and Zejian Deng and Yuang Lu and Xigang Wu", "abstract": "  Trajectory prediction is crucial for autonomous driving as it aims to\nforecast the future movements of traffic participants. Traditional methods\nusually perform holistic inference on the trajectories of agents, neglecting\nthe differences in prediction difficulty among agents. This paper proposes a\nnovel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages\nthe prediction difficulty differences among agents for multi-agent trajectory\nprediction. Firstly, we employ spatio-temporal feature encoding and interaction\nto capture rich spatio-temporal features. Secondly, a difficulty-guided decoder\ncontrols the flow of future trajectories into subsequent modules, obtaining\nreliable future trajectories. Then, feature interaction and fusion are\nperformed through the future feature interaction module. Finally, the fused\nagent features are fed into the final predictor to generate the predicted\ntrajectory distributions for multiple participants. Experimental results\ndemonstrate that our DGFNet achieves state-of-the-art performance on the\nArgoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate\nthe effectiveness of each module. Moreover, compared with SOTA methods, our\nmethod balances trajectory prediction accuracy and real-time inference speed.\n", "link": "http://arxiv.org/abs/2407.18551v3", "date": "2024-12-19", "relevancy": 2.1812, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Trajectory%20Prediction%20with%20Difficulty-Guided%20Feature%0A%20%20Enhancement%20Network&body=Title%3A%20Multi-Agent%20Trajectory%20Prediction%20with%20Difficulty-Guided%20Feature%0A%20%20Enhancement%20Network%0AAuthor%3A%20Guipeng%20Xin%20and%20Duanfeng%20Chu%20and%20Liping%20Lu%20and%20Zejian%20Deng%20and%20Yuang%20Lu%20and%20Xigang%20Wu%0AAbstract%3A%20%20%20Trajectory%20prediction%20is%20crucial%20for%20autonomous%20driving%20as%20it%20aims%20to%0Aforecast%20the%20future%20movements%20of%20traffic%20participants.%20Traditional%20methods%0Ausually%20perform%20holistic%20inference%20on%20the%20trajectories%20of%20agents%2C%20neglecting%0Athe%20differences%20in%20prediction%20difficulty%20among%20agents.%20This%20paper%20proposes%20a%0Anovel%20Difficulty-Guided%20Feature%20Enhancement%20Network%20%28DGFNet%29%2C%20which%20leverages%0Athe%20prediction%20difficulty%20differences%20among%20agents%20for%20multi-agent%20trajectory%0Aprediction.%20Firstly%2C%20we%20employ%20spatio-temporal%20feature%20encoding%20and%20interaction%0Ato%20capture%20rich%20spatio-temporal%20features.%20Secondly%2C%20a%20difficulty-guided%20decoder%0Acontrols%20the%20flow%20of%20future%20trajectories%20into%20subsequent%20modules%2C%20obtaining%0Areliable%20future%20trajectories.%20Then%2C%20feature%20interaction%20and%20fusion%20are%0Aperformed%20through%20the%20future%20feature%20interaction%20module.%20Finally%2C%20the%20fused%0Aagent%20features%20are%20fed%20into%20the%20final%20predictor%20to%20generate%20the%20predicted%0Atrajectory%20distributions%20for%20multiple%20participants.%20Experimental%20results%0Ademonstrate%20that%20our%20DGFNet%20achieves%20state-of-the-art%20performance%20on%20the%0AArgoverse%201%5C%262%20motion%20forecasting%20benchmarks.%20Ablation%20studies%20further%20validate%0Athe%20effectiveness%20of%20each%20module.%20Moreover%2C%20compared%20with%20SOTA%20methods%2C%20our%0Amethod%20balances%20trajectory%20prediction%20accuracy%20and%20real-time%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18551v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Trajectory%2520Prediction%2520with%2520Difficulty-Guided%2520Feature%250A%2520%2520Enhancement%2520Network%26entry.906535625%3DGuipeng%2520Xin%2520and%2520Duanfeng%2520Chu%2520and%2520Liping%2520Lu%2520and%2520Zejian%2520Deng%2520and%2520Yuang%2520Lu%2520and%2520Xigang%2520Wu%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520is%2520crucial%2520for%2520autonomous%2520driving%2520as%2520it%2520aims%2520to%250Aforecast%2520the%2520future%2520movements%2520of%2520traffic%2520participants.%2520Traditional%2520methods%250Ausually%2520perform%2520holistic%2520inference%2520on%2520the%2520trajectories%2520of%2520agents%252C%2520neglecting%250Athe%2520differences%2520in%2520prediction%2520difficulty%2520among%2520agents.%2520This%2520paper%2520proposes%2520a%250Anovel%2520Difficulty-Guided%2520Feature%2520Enhancement%2520Network%2520%2528DGFNet%2529%252C%2520which%2520leverages%250Athe%2520prediction%2520difficulty%2520differences%2520among%2520agents%2520for%2520multi-agent%2520trajectory%250Aprediction.%2520Firstly%252C%2520we%2520employ%2520spatio-temporal%2520feature%2520encoding%2520and%2520interaction%250Ato%2520capture%2520rich%2520spatio-temporal%2520features.%2520Secondly%252C%2520a%2520difficulty-guided%2520decoder%250Acontrols%2520the%2520flow%2520of%2520future%2520trajectories%2520into%2520subsequent%2520modules%252C%2520obtaining%250Areliable%2520future%2520trajectories.%2520Then%252C%2520feature%2520interaction%2520and%2520fusion%2520are%250Aperformed%2520through%2520the%2520future%2520feature%2520interaction%2520module.%2520Finally%252C%2520the%2520fused%250Aagent%2520features%2520are%2520fed%2520into%2520the%2520final%2520predictor%2520to%2520generate%2520the%2520predicted%250Atrajectory%2520distributions%2520for%2520multiple%2520participants.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520DGFNet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250AArgoverse%25201%255C%25262%2520motion%2520forecasting%2520benchmarks.%2520Ablation%2520studies%2520further%2520validate%250Athe%2520effectiveness%2520of%2520each%2520module.%2520Moreover%252C%2520compared%2520with%2520SOTA%2520methods%252C%2520our%250Amethod%2520balances%2520trajectory%2520prediction%2520accuracy%2520and%2520real-time%2520inference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18551v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Trajectory%20Prediction%20with%20Difficulty-Guided%20Feature%0A%20%20Enhancement%20Network&entry.906535625=Guipeng%20Xin%20and%20Duanfeng%20Chu%20and%20Liping%20Lu%20and%20Zejian%20Deng%20and%20Yuang%20Lu%20and%20Xigang%20Wu&entry.1292438233=%20%20Trajectory%20prediction%20is%20crucial%20for%20autonomous%20driving%20as%20it%20aims%20to%0Aforecast%20the%20future%20movements%20of%20traffic%20participants.%20Traditional%20methods%0Ausually%20perform%20holistic%20inference%20on%20the%20trajectories%20of%20agents%2C%20neglecting%0Athe%20differences%20in%20prediction%20difficulty%20among%20agents.%20This%20paper%20proposes%20a%0Anovel%20Difficulty-Guided%20Feature%20Enhancement%20Network%20%28DGFNet%29%2C%20which%20leverages%0Athe%20prediction%20difficulty%20differences%20among%20agents%20for%20multi-agent%20trajectory%0Aprediction.%20Firstly%2C%20we%20employ%20spatio-temporal%20feature%20encoding%20and%20interaction%0Ato%20capture%20rich%20spatio-temporal%20features.%20Secondly%2C%20a%20difficulty-guided%20decoder%0Acontrols%20the%20flow%20of%20future%20trajectories%20into%20subsequent%20modules%2C%20obtaining%0Areliable%20future%20trajectories.%20Then%2C%20feature%20interaction%20and%20fusion%20are%0Aperformed%20through%20the%20future%20feature%20interaction%20module.%20Finally%2C%20the%20fused%0Aagent%20features%20are%20fed%20into%20the%20final%20predictor%20to%20generate%20the%20predicted%0Atrajectory%20distributions%20for%20multiple%20participants.%20Experimental%20results%0Ademonstrate%20that%20our%20DGFNet%20achieves%20state-of-the-art%20performance%20on%20the%0AArgoverse%201%5C%262%20motion%20forecasting%20benchmarks.%20Ablation%20studies%20further%20validate%0Athe%20effectiveness%20of%20each%20module.%20Moreover%2C%20compared%20with%20SOTA%20methods%2C%20our%0Amethod%20balances%20trajectory%20prediction%20accuracy%20and%20real-time%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18551v3&entry.124074799=Read"},
{"title": "DCL-Sparse: Distributed Range-only Cooperative Localization of\n  Multi-Robots in Noisy and Sparse Sensing Graphs", "author": "Atharva Sagale and Tohid Kargar Tasooji and Ramviyas Parasuraman", "abstract": "  This paper presents a novel approach to range-based cooperative localization\nfor robot swarms in GPS-denied environments, addressing the limitations of\ncurrent methods in noisy and sparse settings. We propose a robust multi-layered\nlocalization framework that combines shadow edge localization techniques with\nthe strategic deployment of UAVs. This approach not only addresses the\nchallenges associated with nonrigid and poorly connected graphs but also\nenhances the convergence rate of the localization process. We introduce two key\nconcepts: the S1-Edge approach in our distributed protocol to address the\nrigidity problem of sparse graphs and the concept of a powerful UAV node to\nincrease the sensing and localization capability of the multi-robot system. Our\napproach leverages the advantages of the distributed localization methods,\nenhancing scalability and adaptability in large robot networks. We establish\ntheoretical conditions for the new S1-Edge that ensure solutions exist even in\nthe presence of noise, thereby validating the effectiveness of shadow edge\nlocalization. Extensive simulation experiments confirm the superior performance\nof our method compared to state-of-the-art techniques, resulting in up to 95\\%\nreduction in localization error, demonstrating substantial improvements in\nlocalization accuracy and robustness to sparse graphs. This work provides a\ndecisive advancement in the field of multi-robot localization, offering a\npowerful tool for high-performance and reliable operations in challenging\nenvironments.\n", "link": "http://arxiv.org/abs/2412.14793v1", "date": "2024-12-19", "relevancy": 2.1784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCL-Sparse%3A%20Distributed%20Range-only%20Cooperative%20Localization%20of%0A%20%20Multi-Robots%20in%20Noisy%20and%20Sparse%20Sensing%20Graphs&body=Title%3A%20DCL-Sparse%3A%20Distributed%20Range-only%20Cooperative%20Localization%20of%0A%20%20Multi-Robots%20in%20Noisy%20and%20Sparse%20Sensing%20Graphs%0AAuthor%3A%20Atharva%20Sagale%20and%20Tohid%20Kargar%20Tasooji%20and%20Ramviyas%20Parasuraman%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20range-based%20cooperative%20localization%0Afor%20robot%20swarms%20in%20GPS-denied%20environments%2C%20addressing%20the%20limitations%20of%0Acurrent%20methods%20in%20noisy%20and%20sparse%20settings.%20We%20propose%20a%20robust%20multi-layered%0Alocalization%20framework%20that%20combines%20shadow%20edge%20localization%20techniques%20with%0Athe%20strategic%20deployment%20of%20UAVs.%20This%20approach%20not%20only%20addresses%20the%0Achallenges%20associated%20with%20nonrigid%20and%20poorly%20connected%20graphs%20but%20also%0Aenhances%20the%20convergence%20rate%20of%20the%20localization%20process.%20We%20introduce%20two%20key%0Aconcepts%3A%20the%20S1-Edge%20approach%20in%20our%20distributed%20protocol%20to%20address%20the%0Arigidity%20problem%20of%20sparse%20graphs%20and%20the%20concept%20of%20a%20powerful%20UAV%20node%20to%0Aincrease%20the%20sensing%20and%20localization%20capability%20of%20the%20multi-robot%20system.%20Our%0Aapproach%20leverages%20the%20advantages%20of%20the%20distributed%20localization%20methods%2C%0Aenhancing%20scalability%20and%20adaptability%20in%20large%20robot%20networks.%20We%20establish%0Atheoretical%20conditions%20for%20the%20new%20S1-Edge%20that%20ensure%20solutions%20exist%20even%20in%0Athe%20presence%20of%20noise%2C%20thereby%20validating%20the%20effectiveness%20of%20shadow%20edge%0Alocalization.%20Extensive%20simulation%20experiments%20confirm%20the%20superior%20performance%0Aof%20our%20method%20compared%20to%20state-of-the-art%20techniques%2C%20resulting%20in%20up%20to%2095%5C%25%0Areduction%20in%20localization%20error%2C%20demonstrating%20substantial%20improvements%20in%0Alocalization%20accuracy%20and%20robustness%20to%20sparse%20graphs.%20This%20work%20provides%20a%0Adecisive%20advancement%20in%20the%20field%20of%20multi-robot%20localization%2C%20offering%20a%0Apowerful%20tool%20for%20high-performance%20and%20reliable%20operations%20in%20challenging%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCL-Sparse%253A%2520Distributed%2520Range-only%2520Cooperative%2520Localization%2520of%250A%2520%2520Multi-Robots%2520in%2520Noisy%2520and%2520Sparse%2520Sensing%2520Graphs%26entry.906535625%3DAtharva%2520Sagale%2520and%2520Tohid%2520Kargar%2520Tasooji%2520and%2520Ramviyas%2520Parasuraman%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520range-based%2520cooperative%2520localization%250Afor%2520robot%2520swarms%2520in%2520GPS-denied%2520environments%252C%2520addressing%2520the%2520limitations%2520of%250Acurrent%2520methods%2520in%2520noisy%2520and%2520sparse%2520settings.%2520We%2520propose%2520a%2520robust%2520multi-layered%250Alocalization%2520framework%2520that%2520combines%2520shadow%2520edge%2520localization%2520techniques%2520with%250Athe%2520strategic%2520deployment%2520of%2520UAVs.%2520This%2520approach%2520not%2520only%2520addresses%2520the%250Achallenges%2520associated%2520with%2520nonrigid%2520and%2520poorly%2520connected%2520graphs%2520but%2520also%250Aenhances%2520the%2520convergence%2520rate%2520of%2520the%2520localization%2520process.%2520We%2520introduce%2520two%2520key%250Aconcepts%253A%2520the%2520S1-Edge%2520approach%2520in%2520our%2520distributed%2520protocol%2520to%2520address%2520the%250Arigidity%2520problem%2520of%2520sparse%2520graphs%2520and%2520the%2520concept%2520of%2520a%2520powerful%2520UAV%2520node%2520to%250Aincrease%2520the%2520sensing%2520and%2520localization%2520capability%2520of%2520the%2520multi-robot%2520system.%2520Our%250Aapproach%2520leverages%2520the%2520advantages%2520of%2520the%2520distributed%2520localization%2520methods%252C%250Aenhancing%2520scalability%2520and%2520adaptability%2520in%2520large%2520robot%2520networks.%2520We%2520establish%250Atheoretical%2520conditions%2520for%2520the%2520new%2520S1-Edge%2520that%2520ensure%2520solutions%2520exist%2520even%2520in%250Athe%2520presence%2520of%2520noise%252C%2520thereby%2520validating%2520the%2520effectiveness%2520of%2520shadow%2520edge%250Alocalization.%2520Extensive%2520simulation%2520experiments%2520confirm%2520the%2520superior%2520performance%250Aof%2520our%2520method%2520compared%2520to%2520state-of-the-art%2520techniques%252C%2520resulting%2520in%2520up%2520to%252095%255C%2525%250Areduction%2520in%2520localization%2520error%252C%2520demonstrating%2520substantial%2520improvements%2520in%250Alocalization%2520accuracy%2520and%2520robustness%2520to%2520sparse%2520graphs.%2520This%2520work%2520provides%2520a%250Adecisive%2520advancement%2520in%2520the%2520field%2520of%2520multi-robot%2520localization%252C%2520offering%2520a%250Apowerful%2520tool%2520for%2520high-performance%2520and%2520reliable%2520operations%2520in%2520challenging%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCL-Sparse%3A%20Distributed%20Range-only%20Cooperative%20Localization%20of%0A%20%20Multi-Robots%20in%20Noisy%20and%20Sparse%20Sensing%20Graphs&entry.906535625=Atharva%20Sagale%20and%20Tohid%20Kargar%20Tasooji%20and%20Ramviyas%20Parasuraman&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20range-based%20cooperative%20localization%0Afor%20robot%20swarms%20in%20GPS-denied%20environments%2C%20addressing%20the%20limitations%20of%0Acurrent%20methods%20in%20noisy%20and%20sparse%20settings.%20We%20propose%20a%20robust%20multi-layered%0Alocalization%20framework%20that%20combines%20shadow%20edge%20localization%20techniques%20with%0Athe%20strategic%20deployment%20of%20UAVs.%20This%20approach%20not%20only%20addresses%20the%0Achallenges%20associated%20with%20nonrigid%20and%20poorly%20connected%20graphs%20but%20also%0Aenhances%20the%20convergence%20rate%20of%20the%20localization%20process.%20We%20introduce%20two%20key%0Aconcepts%3A%20the%20S1-Edge%20approach%20in%20our%20distributed%20protocol%20to%20address%20the%0Arigidity%20problem%20of%20sparse%20graphs%20and%20the%20concept%20of%20a%20powerful%20UAV%20node%20to%0Aincrease%20the%20sensing%20and%20localization%20capability%20of%20the%20multi-robot%20system.%20Our%0Aapproach%20leverages%20the%20advantages%20of%20the%20distributed%20localization%20methods%2C%0Aenhancing%20scalability%20and%20adaptability%20in%20large%20robot%20networks.%20We%20establish%0Atheoretical%20conditions%20for%20the%20new%20S1-Edge%20that%20ensure%20solutions%20exist%20even%20in%0Athe%20presence%20of%20noise%2C%20thereby%20validating%20the%20effectiveness%20of%20shadow%20edge%0Alocalization.%20Extensive%20simulation%20experiments%20confirm%20the%20superior%20performance%0Aof%20our%20method%20compared%20to%20state-of-the-art%20techniques%2C%20resulting%20in%20up%20to%2095%5C%25%0Areduction%20in%20localization%20error%2C%20demonstrating%20substantial%20improvements%20in%0Alocalization%20accuracy%20and%20robustness%20to%20sparse%20graphs.%20This%20work%20provides%20a%0Adecisive%20advancement%20in%20the%20field%20of%20multi-robot%20localization%2C%20offering%20a%0Apowerful%20tool%20for%20high-performance%20and%20reliable%20operations%20in%20challenging%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14793v1&entry.124074799=Read"},
{"title": "Noise Analysis and Modeling of the PMD Flexx2 Depth Camera for Robotic\n  Applications", "author": "Yuke Cai and Davide Plozza and Steven Marty and Paul Joseph and Michele Magno", "abstract": "  Time of Flight ToF cameras renowned for their ability to capture realtime 3D\ninformation have become indispensable for agile mobile robotics These cameras\nutilize light signals to accurately measure distances enabling robots to\nnavigate complex environments with precision Innovative depth cameras\ncharacterized by their compact size and lightweight design such as the recently\nreleased PMD Flexx2 are particularly suited for mobile robots Capable of\nachieving high frame rates while capturing depth information this innovative\nsensor is suitable for tasks such as robot navigation and terrain mapping\nOperating on the ToF measurement principle the sensor offers multiple benefits\nover classic stereobased depth cameras However the depth images produced by the\ncamera are subject to noise from multiple sources complicating their simulation\nThis paper proposes an accurate quantification and modeling of the\nnonsystematic noise of the PMD Flexx2 We propose models for both axial and\nlateral noise across various camera modes assuming Gaussian distributions Axial\nnoise modeled as a function of distance and incidence angle demonstrated a low\naverage KullbackLeibler KL divergence of 0015 nats reflecting precise noise\ncharacterization Lateral noise deviating from a Gaussian distribution was\nmodeled conservatively yielding a satisfactory KL divergence of 0868 nats These\nresults validate our noise models crucial for accurately simulating sensor\nbehavior in virtual environments and reducing the simtoreal gap in\nlearningbased control approaches\n", "link": "http://arxiv.org/abs/2412.15040v1", "date": "2024-12-19", "relevancy": 2.1646, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise%20Analysis%20and%20Modeling%20of%20the%20PMD%20Flexx2%20Depth%20Camera%20for%20Robotic%0A%20%20Applications&body=Title%3A%20Noise%20Analysis%20and%20Modeling%20of%20the%20PMD%20Flexx2%20Depth%20Camera%20for%20Robotic%0A%20%20Applications%0AAuthor%3A%20Yuke%20Cai%20and%20Davide%20Plozza%20and%20Steven%20Marty%20and%20Paul%20Joseph%20and%20Michele%20Magno%0AAbstract%3A%20%20%20Time%20of%20Flight%20ToF%20cameras%20renowned%20for%20their%20ability%20to%20capture%20realtime%203D%0Ainformation%20have%20become%20indispensable%20for%20agile%20mobile%20robotics%20These%20cameras%0Autilize%20light%20signals%20to%20accurately%20measure%20distances%20enabling%20robots%20to%0Anavigate%20complex%20environments%20with%20precision%20Innovative%20depth%20cameras%0Acharacterized%20by%20their%20compact%20size%20and%20lightweight%20design%20such%20as%20the%20recently%0Areleased%20PMD%20Flexx2%20are%20particularly%20suited%20for%20mobile%20robots%20Capable%20of%0Aachieving%20high%20frame%20rates%20while%20capturing%20depth%20information%20this%20innovative%0Asensor%20is%20suitable%20for%20tasks%20such%20as%20robot%20navigation%20and%20terrain%20mapping%0AOperating%20on%20the%20ToF%20measurement%20principle%20the%20sensor%20offers%20multiple%20benefits%0Aover%20classic%20stereobased%20depth%20cameras%20However%20the%20depth%20images%20produced%20by%20the%0Acamera%20are%20subject%20to%20noise%20from%20multiple%20sources%20complicating%20their%20simulation%0AThis%20paper%20proposes%20an%20accurate%20quantification%20and%20modeling%20of%20the%0Anonsystematic%20noise%20of%20the%20PMD%20Flexx2%20We%20propose%20models%20for%20both%20axial%20and%0Alateral%20noise%20across%20various%20camera%20modes%20assuming%20Gaussian%20distributions%20Axial%0Anoise%20modeled%20as%20a%20function%20of%20distance%20and%20incidence%20angle%20demonstrated%20a%20low%0Aaverage%20KullbackLeibler%20KL%20divergence%20of%200015%20nats%20reflecting%20precise%20noise%0Acharacterization%20Lateral%20noise%20deviating%20from%20a%20Gaussian%20distribution%20was%0Amodeled%20conservatively%20yielding%20a%20satisfactory%20KL%20divergence%20of%200868%20nats%20These%0Aresults%20validate%20our%20noise%20models%20crucial%20for%20accurately%20simulating%20sensor%0Abehavior%20in%20virtual%20environments%20and%20reducing%20the%20simtoreal%20gap%20in%0Alearningbased%20control%20approaches%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise%2520Analysis%2520and%2520Modeling%2520of%2520the%2520PMD%2520Flexx2%2520Depth%2520Camera%2520for%2520Robotic%250A%2520%2520Applications%26entry.906535625%3DYuke%2520Cai%2520and%2520Davide%2520Plozza%2520and%2520Steven%2520Marty%2520and%2520Paul%2520Joseph%2520and%2520Michele%2520Magno%26entry.1292438233%3D%2520%2520Time%2520of%2520Flight%2520ToF%2520cameras%2520renowned%2520for%2520their%2520ability%2520to%2520capture%2520realtime%25203D%250Ainformation%2520have%2520become%2520indispensable%2520for%2520agile%2520mobile%2520robotics%2520These%2520cameras%250Autilize%2520light%2520signals%2520to%2520accurately%2520measure%2520distances%2520enabling%2520robots%2520to%250Anavigate%2520complex%2520environments%2520with%2520precision%2520Innovative%2520depth%2520cameras%250Acharacterized%2520by%2520their%2520compact%2520size%2520and%2520lightweight%2520design%2520such%2520as%2520the%2520recently%250Areleased%2520PMD%2520Flexx2%2520are%2520particularly%2520suited%2520for%2520mobile%2520robots%2520Capable%2520of%250Aachieving%2520high%2520frame%2520rates%2520while%2520capturing%2520depth%2520information%2520this%2520innovative%250Asensor%2520is%2520suitable%2520for%2520tasks%2520such%2520as%2520robot%2520navigation%2520and%2520terrain%2520mapping%250AOperating%2520on%2520the%2520ToF%2520measurement%2520principle%2520the%2520sensor%2520offers%2520multiple%2520benefits%250Aover%2520classic%2520stereobased%2520depth%2520cameras%2520However%2520the%2520depth%2520images%2520produced%2520by%2520the%250Acamera%2520are%2520subject%2520to%2520noise%2520from%2520multiple%2520sources%2520complicating%2520their%2520simulation%250AThis%2520paper%2520proposes%2520an%2520accurate%2520quantification%2520and%2520modeling%2520of%2520the%250Anonsystematic%2520noise%2520of%2520the%2520PMD%2520Flexx2%2520We%2520propose%2520models%2520for%2520both%2520axial%2520and%250Alateral%2520noise%2520across%2520various%2520camera%2520modes%2520assuming%2520Gaussian%2520distributions%2520Axial%250Anoise%2520modeled%2520as%2520a%2520function%2520of%2520distance%2520and%2520incidence%2520angle%2520demonstrated%2520a%2520low%250Aaverage%2520KullbackLeibler%2520KL%2520divergence%2520of%25200015%2520nats%2520reflecting%2520precise%2520noise%250Acharacterization%2520Lateral%2520noise%2520deviating%2520from%2520a%2520Gaussian%2520distribution%2520was%250Amodeled%2520conservatively%2520yielding%2520a%2520satisfactory%2520KL%2520divergence%2520of%25200868%2520nats%2520These%250Aresults%2520validate%2520our%2520noise%2520models%2520crucial%2520for%2520accurately%2520simulating%2520sensor%250Abehavior%2520in%2520virtual%2520environments%2520and%2520reducing%2520the%2520simtoreal%2520gap%2520in%250Alearningbased%2520control%2520approaches%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise%20Analysis%20and%20Modeling%20of%20the%20PMD%20Flexx2%20Depth%20Camera%20for%20Robotic%0A%20%20Applications&entry.906535625=Yuke%20Cai%20and%20Davide%20Plozza%20and%20Steven%20Marty%20and%20Paul%20Joseph%20and%20Michele%20Magno&entry.1292438233=%20%20Time%20of%20Flight%20ToF%20cameras%20renowned%20for%20their%20ability%20to%20capture%20realtime%203D%0Ainformation%20have%20become%20indispensable%20for%20agile%20mobile%20robotics%20These%20cameras%0Autilize%20light%20signals%20to%20accurately%20measure%20distances%20enabling%20robots%20to%0Anavigate%20complex%20environments%20with%20precision%20Innovative%20depth%20cameras%0Acharacterized%20by%20their%20compact%20size%20and%20lightweight%20design%20such%20as%20the%20recently%0Areleased%20PMD%20Flexx2%20are%20particularly%20suited%20for%20mobile%20robots%20Capable%20of%0Aachieving%20high%20frame%20rates%20while%20capturing%20depth%20information%20this%20innovative%0Asensor%20is%20suitable%20for%20tasks%20such%20as%20robot%20navigation%20and%20terrain%20mapping%0AOperating%20on%20the%20ToF%20measurement%20principle%20the%20sensor%20offers%20multiple%20benefits%0Aover%20classic%20stereobased%20depth%20cameras%20However%20the%20depth%20images%20produced%20by%20the%0Acamera%20are%20subject%20to%20noise%20from%20multiple%20sources%20complicating%20their%20simulation%0AThis%20paper%20proposes%20an%20accurate%20quantification%20and%20modeling%20of%20the%0Anonsystematic%20noise%20of%20the%20PMD%20Flexx2%20We%20propose%20models%20for%20both%20axial%20and%0Alateral%20noise%20across%20various%20camera%20modes%20assuming%20Gaussian%20distributions%20Axial%0Anoise%20modeled%20as%20a%20function%20of%20distance%20and%20incidence%20angle%20demonstrated%20a%20low%0Aaverage%20KullbackLeibler%20KL%20divergence%20of%200015%20nats%20reflecting%20precise%20noise%0Acharacterization%20Lateral%20noise%20deviating%20from%20a%20Gaussian%20distribution%20was%0Amodeled%20conservatively%20yielding%20a%20satisfactory%20KL%20divergence%20of%200868%20nats%20These%0Aresults%20validate%20our%20noise%20models%20crucial%20for%20accurately%20simulating%20sensor%0Abehavior%20in%20virtual%20environments%20and%20reducing%20the%20simtoreal%20gap%20in%0Alearningbased%20control%20approaches%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15040v1&entry.124074799=Read"},
{"title": "Computing Gram Matrix for SMILES Strings using RDKFingerprint and\n  Sinkhorn-Knopp Algorithm", "author": "Sarwan Ali and Haris Mansoor and Prakash Chourasia and Imdad Ullah Khan and Murray Patterson", "abstract": "  In molecular structure data, SMILES (Simplified Molecular Input Line Entry\nSystem) strings are used to analyze molecular structure design. Numerical\nfeature representation of SMILES strings is a challenging task. This work\nproposes a kernel-based approach for encoding and analyzing molecular\nstructures from SMILES strings. The proposed approach involves computing a\nkernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal\ncomponent analysis (PCA) for dimensionality reduction. The resulting\nlow-dimensional embeddings are then used for classification and regression\nanalysis. The kernel matrix is computed by converting the SMILES strings into\nmolecular structures using the Morgan Fingerprint, which computes a fingerprint\nfor each molecule. The distance matrix is computed using the pairwise kernels\nfunction. The Sinkhorn-Knopp algorithm is used to compute the final kernel\nmatrix that satisfies the constraints of a probability distribution. This is\nachieved by iteratively adjusting the kernel matrix until the marginal\ndistributions of the rows and columns match the desired marginal distributions.\nWe provided a comprehensive empirical analysis of the proposed kernel method to\nevaluate its goodness with greater depth. The suggested method is assessed for\ndrug subcategory prediction (classification task) and solubility AlogPS\n``Aqueous solubility and Octanol/Water partition coefficient\" (regression task)\nusing the benchmark SMILES string dataset. The outcomes show the proposed\nmethod outperforms several baseline methods in terms of supervised analysis and\nhas potential uses in molecular design and drug discovery. Overall, the\nsuggested method is a promising avenue for kernel methods-based molecular\nstructure analysis and design.\n", "link": "http://arxiv.org/abs/2412.14717v1", "date": "2024-12-19", "relevancy": 2.1557, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4354}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4307}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computing%20Gram%20Matrix%20for%20SMILES%20Strings%20using%20RDKFingerprint%20and%0A%20%20Sinkhorn-Knopp%20Algorithm&body=Title%3A%20Computing%20Gram%20Matrix%20for%20SMILES%20Strings%20using%20RDKFingerprint%20and%0A%20%20Sinkhorn-Knopp%20Algorithm%0AAuthor%3A%20Sarwan%20Ali%20and%20Haris%20Mansoor%20and%20Prakash%20Chourasia%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson%0AAbstract%3A%20%20%20In%20molecular%20structure%20data%2C%20SMILES%20%28Simplified%20Molecular%20Input%20Line%20Entry%0ASystem%29%20strings%20are%20used%20to%20analyze%20molecular%20structure%20design.%20Numerical%0Afeature%20representation%20of%20SMILES%20strings%20is%20a%20challenging%20task.%20This%20work%0Aproposes%20a%20kernel-based%20approach%20for%20encoding%20and%20analyzing%20molecular%0Astructures%20from%20SMILES%20strings.%20The%20proposed%20approach%20involves%20computing%20a%0Akernel%20matrix%20using%20the%20Sinkhorn-Knopp%20algorithm%20while%20using%20kernel%20principal%0Acomponent%20analysis%20%28PCA%29%20for%20dimensionality%20reduction.%20The%20resulting%0Alow-dimensional%20embeddings%20are%20then%20used%20for%20classification%20and%20regression%0Aanalysis.%20The%20kernel%20matrix%20is%20computed%20by%20converting%20the%20SMILES%20strings%20into%0Amolecular%20structures%20using%20the%20Morgan%20Fingerprint%2C%20which%20computes%20a%20fingerprint%0Afor%20each%20molecule.%20The%20distance%20matrix%20is%20computed%20using%20the%20pairwise%20kernels%0Afunction.%20The%20Sinkhorn-Knopp%20algorithm%20is%20used%20to%20compute%20the%20final%20kernel%0Amatrix%20that%20satisfies%20the%20constraints%20of%20a%20probability%20distribution.%20This%20is%0Aachieved%20by%20iteratively%20adjusting%20the%20kernel%20matrix%20until%20the%20marginal%0Adistributions%20of%20the%20rows%20and%20columns%20match%20the%20desired%20marginal%20distributions.%0AWe%20provided%20a%20comprehensive%20empirical%20analysis%20of%20the%20proposed%20kernel%20method%20to%0Aevaluate%20its%20goodness%20with%20greater%20depth.%20The%20suggested%20method%20is%20assessed%20for%0Adrug%20subcategory%20prediction%20%28classification%20task%29%20and%20solubility%20AlogPS%0A%60%60Aqueous%20solubility%20and%20Octanol/Water%20partition%20coefficient%22%20%28regression%20task%29%0Ausing%20the%20benchmark%20SMILES%20string%20dataset.%20The%20outcomes%20show%20the%20proposed%0Amethod%20outperforms%20several%20baseline%20methods%20in%20terms%20of%20supervised%20analysis%20and%0Ahas%20potential%20uses%20in%20molecular%20design%20and%20drug%20discovery.%20Overall%2C%20the%0Asuggested%20method%20is%20a%20promising%20avenue%20for%20kernel%20methods-based%20molecular%0Astructure%20analysis%20and%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputing%2520Gram%2520Matrix%2520for%2520SMILES%2520Strings%2520using%2520RDKFingerprint%2520and%250A%2520%2520Sinkhorn-Knopp%2520Algorithm%26entry.906535625%3DSarwan%2520Ali%2520and%2520Haris%2520Mansoor%2520and%2520Prakash%2520Chourasia%2520and%2520Imdad%2520Ullah%2520Khan%2520and%2520Murray%2520Patterson%26entry.1292438233%3D%2520%2520In%2520molecular%2520structure%2520data%252C%2520SMILES%2520%2528Simplified%2520Molecular%2520Input%2520Line%2520Entry%250ASystem%2529%2520strings%2520are%2520used%2520to%2520analyze%2520molecular%2520structure%2520design.%2520Numerical%250Afeature%2520representation%2520of%2520SMILES%2520strings%2520is%2520a%2520challenging%2520task.%2520This%2520work%250Aproposes%2520a%2520kernel-based%2520approach%2520for%2520encoding%2520and%2520analyzing%2520molecular%250Astructures%2520from%2520SMILES%2520strings.%2520The%2520proposed%2520approach%2520involves%2520computing%2520a%250Akernel%2520matrix%2520using%2520the%2520Sinkhorn-Knopp%2520algorithm%2520while%2520using%2520kernel%2520principal%250Acomponent%2520analysis%2520%2528PCA%2529%2520for%2520dimensionality%2520reduction.%2520The%2520resulting%250Alow-dimensional%2520embeddings%2520are%2520then%2520used%2520for%2520classification%2520and%2520regression%250Aanalysis.%2520The%2520kernel%2520matrix%2520is%2520computed%2520by%2520converting%2520the%2520SMILES%2520strings%2520into%250Amolecular%2520structures%2520using%2520the%2520Morgan%2520Fingerprint%252C%2520which%2520computes%2520a%2520fingerprint%250Afor%2520each%2520molecule.%2520The%2520distance%2520matrix%2520is%2520computed%2520using%2520the%2520pairwise%2520kernels%250Afunction.%2520The%2520Sinkhorn-Knopp%2520algorithm%2520is%2520used%2520to%2520compute%2520the%2520final%2520kernel%250Amatrix%2520that%2520satisfies%2520the%2520constraints%2520of%2520a%2520probability%2520distribution.%2520This%2520is%250Aachieved%2520by%2520iteratively%2520adjusting%2520the%2520kernel%2520matrix%2520until%2520the%2520marginal%250Adistributions%2520of%2520the%2520rows%2520and%2520columns%2520match%2520the%2520desired%2520marginal%2520distributions.%250AWe%2520provided%2520a%2520comprehensive%2520empirical%2520analysis%2520of%2520the%2520proposed%2520kernel%2520method%2520to%250Aevaluate%2520its%2520goodness%2520with%2520greater%2520depth.%2520The%2520suggested%2520method%2520is%2520assessed%2520for%250Adrug%2520subcategory%2520prediction%2520%2528classification%2520task%2529%2520and%2520solubility%2520AlogPS%250A%2560%2560Aqueous%2520solubility%2520and%2520Octanol/Water%2520partition%2520coefficient%2522%2520%2528regression%2520task%2529%250Ausing%2520the%2520benchmark%2520SMILES%2520string%2520dataset.%2520The%2520outcomes%2520show%2520the%2520proposed%250Amethod%2520outperforms%2520several%2520baseline%2520methods%2520in%2520terms%2520of%2520supervised%2520analysis%2520and%250Ahas%2520potential%2520uses%2520in%2520molecular%2520design%2520and%2520drug%2520discovery.%2520Overall%252C%2520the%250Asuggested%2520method%2520is%2520a%2520promising%2520avenue%2520for%2520kernel%2520methods-based%2520molecular%250Astructure%2520analysis%2520and%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computing%20Gram%20Matrix%20for%20SMILES%20Strings%20using%20RDKFingerprint%20and%0A%20%20Sinkhorn-Knopp%20Algorithm&entry.906535625=Sarwan%20Ali%20and%20Haris%20Mansoor%20and%20Prakash%20Chourasia%20and%20Imdad%20Ullah%20Khan%20and%20Murray%20Patterson&entry.1292438233=%20%20In%20molecular%20structure%20data%2C%20SMILES%20%28Simplified%20Molecular%20Input%20Line%20Entry%0ASystem%29%20strings%20are%20used%20to%20analyze%20molecular%20structure%20design.%20Numerical%0Afeature%20representation%20of%20SMILES%20strings%20is%20a%20challenging%20task.%20This%20work%0Aproposes%20a%20kernel-based%20approach%20for%20encoding%20and%20analyzing%20molecular%0Astructures%20from%20SMILES%20strings.%20The%20proposed%20approach%20involves%20computing%20a%0Akernel%20matrix%20using%20the%20Sinkhorn-Knopp%20algorithm%20while%20using%20kernel%20principal%0Acomponent%20analysis%20%28PCA%29%20for%20dimensionality%20reduction.%20The%20resulting%0Alow-dimensional%20embeddings%20are%20then%20used%20for%20classification%20and%20regression%0Aanalysis.%20The%20kernel%20matrix%20is%20computed%20by%20converting%20the%20SMILES%20strings%20into%0Amolecular%20structures%20using%20the%20Morgan%20Fingerprint%2C%20which%20computes%20a%20fingerprint%0Afor%20each%20molecule.%20The%20distance%20matrix%20is%20computed%20using%20the%20pairwise%20kernels%0Afunction.%20The%20Sinkhorn-Knopp%20algorithm%20is%20used%20to%20compute%20the%20final%20kernel%0Amatrix%20that%20satisfies%20the%20constraints%20of%20a%20probability%20distribution.%20This%20is%0Aachieved%20by%20iteratively%20adjusting%20the%20kernel%20matrix%20until%20the%20marginal%0Adistributions%20of%20the%20rows%20and%20columns%20match%20the%20desired%20marginal%20distributions.%0AWe%20provided%20a%20comprehensive%20empirical%20analysis%20of%20the%20proposed%20kernel%20method%20to%0Aevaluate%20its%20goodness%20with%20greater%20depth.%20The%20suggested%20method%20is%20assessed%20for%0Adrug%20subcategory%20prediction%20%28classification%20task%29%20and%20solubility%20AlogPS%0A%60%60Aqueous%20solubility%20and%20Octanol/Water%20partition%20coefficient%22%20%28regression%20task%29%0Ausing%20the%20benchmark%20SMILES%20string%20dataset.%20The%20outcomes%20show%20the%20proposed%0Amethod%20outperforms%20several%20baseline%20methods%20in%20terms%20of%20supervised%20analysis%20and%0Ahas%20potential%20uses%20in%20molecular%20design%20and%20drug%20discovery.%20Overall%2C%20the%0Asuggested%20method%20is%20a%20promising%20avenue%20for%20kernel%20methods-based%20molecular%0Astructure%20analysis%20and%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14717v1&entry.124074799=Read"},
{"title": "Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition", "author": "Kun Li and Dan Guo and Guoliang Chen and Chunxiao Fan and Jingyuan Xu and Zhiliang Wu and Hehe Fan and Meng Wang", "abstract": "  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n", "link": "http://arxiv.org/abs/2412.14719v1", "date": "2024-12-19", "relevancy": 2.1447, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5388}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prototypical%20Calibrating%20Ambiguous%20Samples%20for%20Micro-Action%20Recognition&body=Title%3A%20Prototypical%20Calibrating%20Ambiguous%20Samples%20for%20Micro-Action%20Recognition%0AAuthor%3A%20Kun%20Li%20and%20Dan%20Guo%20and%20Guoliang%20Chen%20and%20Chunxiao%20Fan%20and%20Jingyuan%20Xu%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Micro-Action%20Recognition%20%28MAR%29%20has%20gained%20increasing%20attention%20due%20to%20its%0Acrucial%20role%20as%20a%20form%20of%20non-verbal%20communication%20in%20social%20interactions%2C%20with%0Apromising%20potential%20for%20applications%20in%20human%20communication%20and%20emotion%0Aanalysis.%20However%2C%20current%20approaches%20often%20overlook%20the%20inherent%20ambiguity%20in%0Amicro-actions%2C%20which%20arises%20from%20the%20wide%20category%20range%20and%20subtle%20visual%0Adifferences%20between%20categories.%20This%20oversight%20hampers%20the%20accuracy%20of%0Amicro-action%20recognition.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Prototypical%0ACalibrating%20Ambiguous%20Network%20%28%5Ctextbf%7BPCAN%7D%29%20to%20unleash%20and%20mitigate%20the%0Aambiguity%20of%20MAR.%20%5Ctextbf%7BFirstly%7D%2C%20we%20employ%20a%20hierarchical%20action-tree%20to%0Aidentify%20the%20ambiguous%20sample%2C%20categorizing%20them%20into%20distinct%20sets%20of%0Aambiguous%20samples%20of%20false%20negatives%20and%20false%20positives%2C%20considering%20both%0Abody-%20and%20action-level%20categories.%20%5Ctextbf%7BSecondly%7D%2C%20we%20implement%20an%20ambiguous%0Acontrastive%20refinement%20module%20to%20calibrate%20these%20ambiguous%20samples%20by%0Aregulating%20the%20distance%20between%20ambiguous%20samples%20and%20their%20corresponding%0Aprototypes.%20This%20calibration%20process%20aims%20to%20pull%20false%20negative%0A%28%24%5Cmathbb%7BFN%7D%24%29%20samples%20closer%20to%20their%20respective%20prototypes%20and%20push%20false%0Apositive%20%28%24%5Cmathbb%7BFP%7D%24%29%20samples%20apart%20from%20their%20affiliated%20prototypes.%20In%0Aaddition%2C%20we%20propose%20a%20new%20prototypical%20diversity%20amplification%20loss%20to%0Astrengthen%20the%20model%27s%20capacity%20by%20amplifying%20the%20differences%20between%20different%0Aprototypes.%20%5Ctextbf%7BFinally%7D%2C%20we%20propose%20a%20prototype-guided%20rectification%20to%0Arectify%20prediction%20by%20incorporating%20the%20representability%20of%20prototypes.%0AExtensive%20experiments%20conducted%20on%20the%20benchmark%20dataset%20demonstrate%20the%0Asuperior%20performance%20of%20our%20method%20compared%20to%20existing%20approaches.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/kunli-cs/PCAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrototypical%2520Calibrating%2520Ambiguous%2520Samples%2520for%2520Micro-Action%2520Recognition%26entry.906535625%3DKun%2520Li%2520and%2520Dan%2520Guo%2520and%2520Guoliang%2520Chen%2520and%2520Chunxiao%2520Fan%2520and%2520Jingyuan%2520Xu%2520and%2520Zhiliang%2520Wu%2520and%2520Hehe%2520Fan%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Micro-Action%2520Recognition%2520%2528MAR%2529%2520has%2520gained%2520increasing%2520attention%2520due%2520to%2520its%250Acrucial%2520role%2520as%2520a%2520form%2520of%2520non-verbal%2520communication%2520in%2520social%2520interactions%252C%2520with%250Apromising%2520potential%2520for%2520applications%2520in%2520human%2520communication%2520and%2520emotion%250Aanalysis.%2520However%252C%2520current%2520approaches%2520often%2520overlook%2520the%2520inherent%2520ambiguity%2520in%250Amicro-actions%252C%2520which%2520arises%2520from%2520the%2520wide%2520category%2520range%2520and%2520subtle%2520visual%250Adifferences%2520between%2520categories.%2520This%2520oversight%2520hampers%2520the%2520accuracy%2520of%250Amicro-action%2520recognition.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Prototypical%250ACalibrating%2520Ambiguous%2520Network%2520%2528%255Ctextbf%257BPCAN%257D%2529%2520to%2520unleash%2520and%2520mitigate%2520the%250Aambiguity%2520of%2520MAR.%2520%255Ctextbf%257BFirstly%257D%252C%2520we%2520employ%2520a%2520hierarchical%2520action-tree%2520to%250Aidentify%2520the%2520ambiguous%2520sample%252C%2520categorizing%2520them%2520into%2520distinct%2520sets%2520of%250Aambiguous%2520samples%2520of%2520false%2520negatives%2520and%2520false%2520positives%252C%2520considering%2520both%250Abody-%2520and%2520action-level%2520categories.%2520%255Ctextbf%257BSecondly%257D%252C%2520we%2520implement%2520an%2520ambiguous%250Acontrastive%2520refinement%2520module%2520to%2520calibrate%2520these%2520ambiguous%2520samples%2520by%250Aregulating%2520the%2520distance%2520between%2520ambiguous%2520samples%2520and%2520their%2520corresponding%250Aprototypes.%2520This%2520calibration%2520process%2520aims%2520to%2520pull%2520false%2520negative%250A%2528%2524%255Cmathbb%257BFN%257D%2524%2529%2520samples%2520closer%2520to%2520their%2520respective%2520prototypes%2520and%2520push%2520false%250Apositive%2520%2528%2524%255Cmathbb%257BFP%257D%2524%2529%2520samples%2520apart%2520from%2520their%2520affiliated%2520prototypes.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520new%2520prototypical%2520diversity%2520amplification%2520loss%2520to%250Astrengthen%2520the%2520model%2527s%2520capacity%2520by%2520amplifying%2520the%2520differences%2520between%2520different%250Aprototypes.%2520%255Ctextbf%257BFinally%257D%252C%2520we%2520propose%2520a%2520prototype-guided%2520rectification%2520to%250Arectify%2520prediction%2520by%2520incorporating%2520the%2520representability%2520of%2520prototypes.%250AExtensive%2520experiments%2520conducted%2520on%2520the%2520benchmark%2520dataset%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520our%2520method%2520compared%2520to%2520existing%2520approaches.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/kunli-cs/PCAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prototypical%20Calibrating%20Ambiguous%20Samples%20for%20Micro-Action%20Recognition&entry.906535625=Kun%20Li%20and%20Dan%20Guo%20and%20Guoliang%20Chen%20and%20Chunxiao%20Fan%20and%20Jingyuan%20Xu%20and%20Zhiliang%20Wu%20and%20Hehe%20Fan%20and%20Meng%20Wang&entry.1292438233=%20%20Micro-Action%20Recognition%20%28MAR%29%20has%20gained%20increasing%20attention%20due%20to%20its%0Acrucial%20role%20as%20a%20form%20of%20non-verbal%20communication%20in%20social%20interactions%2C%20with%0Apromising%20potential%20for%20applications%20in%20human%20communication%20and%20emotion%0Aanalysis.%20However%2C%20current%20approaches%20often%20overlook%20the%20inherent%20ambiguity%20in%0Amicro-actions%2C%20which%20arises%20from%20the%20wide%20category%20range%20and%20subtle%20visual%0Adifferences%20between%20categories.%20This%20oversight%20hampers%20the%20accuracy%20of%0Amicro-action%20recognition.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Prototypical%0ACalibrating%20Ambiguous%20Network%20%28%5Ctextbf%7BPCAN%7D%29%20to%20unleash%20and%20mitigate%20the%0Aambiguity%20of%20MAR.%20%5Ctextbf%7BFirstly%7D%2C%20we%20employ%20a%20hierarchical%20action-tree%20to%0Aidentify%20the%20ambiguous%20sample%2C%20categorizing%20them%20into%20distinct%20sets%20of%0Aambiguous%20samples%20of%20false%20negatives%20and%20false%20positives%2C%20considering%20both%0Abody-%20and%20action-level%20categories.%20%5Ctextbf%7BSecondly%7D%2C%20we%20implement%20an%20ambiguous%0Acontrastive%20refinement%20module%20to%20calibrate%20these%20ambiguous%20samples%20by%0Aregulating%20the%20distance%20between%20ambiguous%20samples%20and%20their%20corresponding%0Aprototypes.%20This%20calibration%20process%20aims%20to%20pull%20false%20negative%0A%28%24%5Cmathbb%7BFN%7D%24%29%20samples%20closer%20to%20their%20respective%20prototypes%20and%20push%20false%0Apositive%20%28%24%5Cmathbb%7BFP%7D%24%29%20samples%20apart%20from%20their%20affiliated%20prototypes.%20In%0Aaddition%2C%20we%20propose%20a%20new%20prototypical%20diversity%20amplification%20loss%20to%0Astrengthen%20the%20model%27s%20capacity%20by%20amplifying%20the%20differences%20between%20different%0Aprototypes.%20%5Ctextbf%7BFinally%7D%2C%20we%20propose%20a%20prototype-guided%20rectification%20to%0Arectify%20prediction%20by%20incorporating%20the%20representability%20of%20prototypes.%0AExtensive%20experiments%20conducted%20on%20the%20benchmark%20dataset%20demonstrate%20the%0Asuperior%20performance%20of%20our%20method%20compared%20to%20existing%20approaches.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/kunli-cs/PCAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14719v1&entry.124074799=Read"},
{"title": "AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving", "author": "Shuo Xing and Hongyuan Hua and Xiangbo Gao and Shenzhe Zhu and Renjie Li and Kexin Tian and Xiaopeng Li and Heng Huang and Tianbao Yang and Zhangyang Wang and Yang Zhou and Huaxiu Yao and Zhengzhong Tu", "abstract": "  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n", "link": "http://arxiv.org/abs/2412.15206v1", "date": "2024-12-19", "relevancy": 2.1408, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.555}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoTrust%3A%20Benchmarking%20Trustworthiness%20in%20Large%20Vision%20Language%20Models%0A%20%20for%20Autonomous%20Driving&body=Title%3A%20AutoTrust%3A%20Benchmarking%20Trustworthiness%20in%20Large%20Vision%20Language%20Models%0A%20%20for%20Autonomous%20Driving%0AAuthor%3A%20Shuo%20Xing%20and%20Hongyuan%20Hua%20and%20Xiangbo%20Gao%20and%20Shenzhe%20Zhu%20and%20Renjie%20Li%20and%20Kexin%20Tian%20and%20Xiaopeng%20Li%20and%20Heng%20Huang%20and%20Tianbao%20Yang%20and%20Zhangyang%20Wang%20and%20Yang%20Zhou%20and%20Huaxiu%20Yao%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20vision%20language%20models%20%28VLMs%29%20tailored%20for%0Aautonomous%20driving%20%28AD%29%20have%20shown%20strong%20scene%20understanding%20and%20reasoning%0Acapabilities%2C%20making%20them%20undeniable%20candidates%20for%20end-to-end%20driving%20systems.%0AHowever%2C%20limited%20work%20exists%20on%20studying%20the%20trustworthiness%20of%20DriveVLMs%20--%20a%0Acritical%20factor%20that%20directly%20impacts%20public%20transportation%20safety.%20In%20this%0Apaper%2C%20we%20introduce%20AutoTrust%2C%20a%20comprehensive%20trustworthiness%20benchmark%20for%0Alarge%20vision-language%20models%20in%20autonomous%20driving%20%28DriveVLMs%29%2C%20considering%0Adiverse%20perspectives%20--%20including%20trustfulness%2C%20safety%2C%20robustness%2C%20privacy%2C%0Aand%20fairness.%20We%20constructed%20the%20largest%20visual%20question-answering%20dataset%20for%0Ainvestigating%20trustworthiness%20issues%20in%20driving%20scenarios%2C%20comprising%20over%2010k%0Aunique%20scenes%20and%2018k%20queries.%20We%20evaluated%20six%20publicly%20available%20VLMs%2C%0Aspanning%20from%20generalist%20to%20specialist%2C%20from%20open-source%20to%20commercial%20models.%0AOur%20exhaustive%20evaluations%20have%20unveiled%20previously%20undiscovered%0Avulnerabilities%20of%20DriveVLMs%20to%20trustworthiness%20threats.%20Specifically%2C%20we%20found%0Athat%20the%20general%20VLMs%20like%20LLaVA-v1.6%20and%20GPT-4o-mini%20surprisingly%20outperform%0Aspecialized%20models%20fine-tuned%20for%20driving%20in%20terms%20of%20overall%20trustworthiness.%0ADriveVLMs%20like%20DriveLM-Agent%20are%20particularly%20vulnerable%20to%20disclosing%0Asensitive%20information.%20Additionally%2C%20both%20generalist%20and%20specialist%20VLMs%20remain%0Asusceptible%20to%20adversarial%20attacks%20and%20struggle%20to%20ensure%20unbiased%0Adecision-making%20across%20diverse%20environments%20and%20populations.%20Our%20findings%20call%0Afor%20immediate%20and%20decisive%20action%20to%20address%20the%20trustworthiness%20of%20DriveVLMs%0A--%20an%20issue%20of%20critical%20importance%20to%20public%20safety%20and%20the%20welfare%20of%20all%0Acitizens%20relying%20on%20autonomous%20transportation%20systems.%20Our%20benchmark%20is%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/taco-group/AutoTrust%7D%2C%20and%20the%0Aleaderboard%20is%20released%20at%20%5Curl%7Bhttps%3A//taco-group.github.io/AutoTrust/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoTrust%253A%2520Benchmarking%2520Trustworthiness%2520in%2520Large%2520Vision%2520Language%2520Models%250A%2520%2520for%2520Autonomous%2520Driving%26entry.906535625%3DShuo%2520Xing%2520and%2520Hongyuan%2520Hua%2520and%2520Xiangbo%2520Gao%2520and%2520Shenzhe%2520Zhu%2520and%2520Renjie%2520Li%2520and%2520Kexin%2520Tian%2520and%2520Xiaopeng%2520Li%2520and%2520Heng%2520Huang%2520and%2520Tianbao%2520Yang%2520and%2520Zhangyang%2520Wang%2520and%2520Yang%2520Zhou%2520and%2520Huaxiu%2520Yao%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520vision%2520language%2520models%2520%2528VLMs%2529%2520tailored%2520for%250Aautonomous%2520driving%2520%2528AD%2529%2520have%2520shown%2520strong%2520scene%2520understanding%2520and%2520reasoning%250Acapabilities%252C%2520making%2520them%2520undeniable%2520candidates%2520for%2520end-to-end%2520driving%2520systems.%250AHowever%252C%2520limited%2520work%2520exists%2520on%2520studying%2520the%2520trustworthiness%2520of%2520DriveVLMs%2520--%2520a%250Acritical%2520factor%2520that%2520directly%2520impacts%2520public%2520transportation%2520safety.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520AutoTrust%252C%2520a%2520comprehensive%2520trustworthiness%2520benchmark%2520for%250Alarge%2520vision-language%2520models%2520in%2520autonomous%2520driving%2520%2528DriveVLMs%2529%252C%2520considering%250Adiverse%2520perspectives%2520--%2520including%2520trustfulness%252C%2520safety%252C%2520robustness%252C%2520privacy%252C%250Aand%2520fairness.%2520We%2520constructed%2520the%2520largest%2520visual%2520question-answering%2520dataset%2520for%250Ainvestigating%2520trustworthiness%2520issues%2520in%2520driving%2520scenarios%252C%2520comprising%2520over%252010k%250Aunique%2520scenes%2520and%252018k%2520queries.%2520We%2520evaluated%2520six%2520publicly%2520available%2520VLMs%252C%250Aspanning%2520from%2520generalist%2520to%2520specialist%252C%2520from%2520open-source%2520to%2520commercial%2520models.%250AOur%2520exhaustive%2520evaluations%2520have%2520unveiled%2520previously%2520undiscovered%250Avulnerabilities%2520of%2520DriveVLMs%2520to%2520trustworthiness%2520threats.%2520Specifically%252C%2520we%2520found%250Athat%2520the%2520general%2520VLMs%2520like%2520LLaVA-v1.6%2520and%2520GPT-4o-mini%2520surprisingly%2520outperform%250Aspecialized%2520models%2520fine-tuned%2520for%2520driving%2520in%2520terms%2520of%2520overall%2520trustworthiness.%250ADriveVLMs%2520like%2520DriveLM-Agent%2520are%2520particularly%2520vulnerable%2520to%2520disclosing%250Asensitive%2520information.%2520Additionally%252C%2520both%2520generalist%2520and%2520specialist%2520VLMs%2520remain%250Asusceptible%2520to%2520adversarial%2520attacks%2520and%2520struggle%2520to%2520ensure%2520unbiased%250Adecision-making%2520across%2520diverse%2520environments%2520and%2520populations.%2520Our%2520findings%2520call%250Afor%2520immediate%2520and%2520decisive%2520action%2520to%2520address%2520the%2520trustworthiness%2520of%2520DriveVLMs%250A--%2520an%2520issue%2520of%2520critical%2520importance%2520to%2520public%2520safety%2520and%2520the%2520welfare%2520of%2520all%250Acitizens%2520relying%2520on%2520autonomous%2520transportation%2520systems.%2520Our%2520benchmark%2520is%250Apublicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/taco-group/AutoTrust%257D%252C%2520and%2520the%250Aleaderboard%2520is%2520released%2520at%2520%255Curl%257Bhttps%253A//taco-group.github.io/AutoTrust/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoTrust%3A%20Benchmarking%20Trustworthiness%20in%20Large%20Vision%20Language%20Models%0A%20%20for%20Autonomous%20Driving&entry.906535625=Shuo%20Xing%20and%20Hongyuan%20Hua%20and%20Xiangbo%20Gao%20and%20Shenzhe%20Zhu%20and%20Renjie%20Li%20and%20Kexin%20Tian%20and%20Xiaopeng%20Li%20and%20Heng%20Huang%20and%20Tianbao%20Yang%20and%20Zhangyang%20Wang%20and%20Yang%20Zhou%20and%20Huaxiu%20Yao%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Recent%20advancements%20in%20large%20vision%20language%20models%20%28VLMs%29%20tailored%20for%0Aautonomous%20driving%20%28AD%29%20have%20shown%20strong%20scene%20understanding%20and%20reasoning%0Acapabilities%2C%20making%20them%20undeniable%20candidates%20for%20end-to-end%20driving%20systems.%0AHowever%2C%20limited%20work%20exists%20on%20studying%20the%20trustworthiness%20of%20DriveVLMs%20--%20a%0Acritical%20factor%20that%20directly%20impacts%20public%20transportation%20safety.%20In%20this%0Apaper%2C%20we%20introduce%20AutoTrust%2C%20a%20comprehensive%20trustworthiness%20benchmark%20for%0Alarge%20vision-language%20models%20in%20autonomous%20driving%20%28DriveVLMs%29%2C%20considering%0Adiverse%20perspectives%20--%20including%20trustfulness%2C%20safety%2C%20robustness%2C%20privacy%2C%0Aand%20fairness.%20We%20constructed%20the%20largest%20visual%20question-answering%20dataset%20for%0Ainvestigating%20trustworthiness%20issues%20in%20driving%20scenarios%2C%20comprising%20over%2010k%0Aunique%20scenes%20and%2018k%20queries.%20We%20evaluated%20six%20publicly%20available%20VLMs%2C%0Aspanning%20from%20generalist%20to%20specialist%2C%20from%20open-source%20to%20commercial%20models.%0AOur%20exhaustive%20evaluations%20have%20unveiled%20previously%20undiscovered%0Avulnerabilities%20of%20DriveVLMs%20to%20trustworthiness%20threats.%20Specifically%2C%20we%20found%0Athat%20the%20general%20VLMs%20like%20LLaVA-v1.6%20and%20GPT-4o-mini%20surprisingly%20outperform%0Aspecialized%20models%20fine-tuned%20for%20driving%20in%20terms%20of%20overall%20trustworthiness.%0ADriveVLMs%20like%20DriveLM-Agent%20are%20particularly%20vulnerable%20to%20disclosing%0Asensitive%20information.%20Additionally%2C%20both%20generalist%20and%20specialist%20VLMs%20remain%0Asusceptible%20to%20adversarial%20attacks%20and%20struggle%20to%20ensure%20unbiased%0Adecision-making%20across%20diverse%20environments%20and%20populations.%20Our%20findings%20call%0Afor%20immediate%20and%20decisive%20action%20to%20address%20the%20trustworthiness%20of%20DriveVLMs%0A--%20an%20issue%20of%20critical%20importance%20to%20public%20safety%20and%20the%20welfare%20of%20all%0Acitizens%20relying%20on%20autonomous%20transportation%20systems.%20Our%20benchmark%20is%0Apublicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/taco-group/AutoTrust%7D%2C%20and%20the%0Aleaderboard%20is%20released%20at%20%5Curl%7Bhttps%3A//taco-group.github.io/AutoTrust/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15206v1&entry.124074799=Read"},
{"title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport", "author": "Borui Zhang and Wenzhao Zheng and Jie Zhou and Jiwen Lu", "abstract": "  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n", "link": "http://arxiv.org/abs/2412.15195v1", "date": "2024-12-19", "relevancy": 2.1321, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5494}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preventing%20Local%20Pitfalls%20in%20Vector%20Quantization%20via%20Optimal%20Transport&body=Title%3A%20Preventing%20Local%20Pitfalls%20in%20Vector%20Quantization%20via%20Optimal%20Transport%0AAuthor%3A%20Borui%20Zhang%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Vector-quantized%20networks%20%28VQNs%29%20have%20exhibited%20remarkable%20performance%20across%0Avarious%20tasks%2C%20yet%20they%20are%20prone%20to%20training%20instability%2C%20which%20complicates%0Athe%20training%20process%20due%20to%20the%20necessity%20for%20techniques%20such%20as%20subtle%0Ainitialization%20and%20model%20distillation.%20In%20this%20study%2C%20we%20identify%20the%20local%0Aminima%20issue%20as%20the%20primary%20cause%20of%20this%20instability.%20To%20address%20this%2C%20we%0Aintegrate%20an%20optimal%20transport%20method%20in%20place%20of%20the%20nearest%20neighbor%20search%0Ato%20achieve%20a%20more%20globally%20informed%20assignment.%20We%20introduce%20OptVQ%2C%20a%20novel%0Avector%20quantization%20method%20that%20employs%20the%20Sinkhorn%20algorithm%20to%20optimize%20the%0Aoptimal%20transport%20problem%2C%20thereby%20enhancing%20the%20stability%20and%20efficiency%20of%0Athe%20training%20process.%20To%20mitigate%20the%20influence%20of%20diverse%20data%20distributions%0Aon%20the%20Sinkhorn%20algorithm%2C%20we%20implement%20a%20straightforward%20yet%20effective%0Anormalization%20strategy.%20Our%20comprehensive%20experiments%20on%20image%20reconstruction%0Atasks%20demonstrate%20that%20OptVQ%20achieves%20100%25%20codebook%20utilization%20and%20surpasses%0Acurrent%20state-of-the-art%20VQNs%20in%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreventing%2520Local%2520Pitfalls%2520in%2520Vector%2520Quantization%2520via%2520Optimal%2520Transport%26entry.906535625%3DBorui%2520Zhang%2520and%2520Wenzhao%2520Zheng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Vector-quantized%2520networks%2520%2528VQNs%2529%2520have%2520exhibited%2520remarkable%2520performance%2520across%250Avarious%2520tasks%252C%2520yet%2520they%2520are%2520prone%2520to%2520training%2520instability%252C%2520which%2520complicates%250Athe%2520training%2520process%2520due%2520to%2520the%2520necessity%2520for%2520techniques%2520such%2520as%2520subtle%250Ainitialization%2520and%2520model%2520distillation.%2520In%2520this%2520study%252C%2520we%2520identify%2520the%2520local%250Aminima%2520issue%2520as%2520the%2520primary%2520cause%2520of%2520this%2520instability.%2520To%2520address%2520this%252C%2520we%250Aintegrate%2520an%2520optimal%2520transport%2520method%2520in%2520place%2520of%2520the%2520nearest%2520neighbor%2520search%250Ato%2520achieve%2520a%2520more%2520globally%2520informed%2520assignment.%2520We%2520introduce%2520OptVQ%252C%2520a%2520novel%250Avector%2520quantization%2520method%2520that%2520employs%2520the%2520Sinkhorn%2520algorithm%2520to%2520optimize%2520the%250Aoptimal%2520transport%2520problem%252C%2520thereby%2520enhancing%2520the%2520stability%2520and%2520efficiency%2520of%250Athe%2520training%2520process.%2520To%2520mitigate%2520the%2520influence%2520of%2520diverse%2520data%2520distributions%250Aon%2520the%2520Sinkhorn%2520algorithm%252C%2520we%2520implement%2520a%2520straightforward%2520yet%2520effective%250Anormalization%2520strategy.%2520Our%2520comprehensive%2520experiments%2520on%2520image%2520reconstruction%250Atasks%2520demonstrate%2520that%2520OptVQ%2520achieves%2520100%2525%2520codebook%2520utilization%2520and%2520surpasses%250Acurrent%2520state-of-the-art%2520VQNs%2520in%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preventing%20Local%20Pitfalls%20in%20Vector%20Quantization%20via%20Optimal%20Transport&entry.906535625=Borui%20Zhang%20and%20Wenzhao%20Zheng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Vector-quantized%20networks%20%28VQNs%29%20have%20exhibited%20remarkable%20performance%20across%0Avarious%20tasks%2C%20yet%20they%20are%20prone%20to%20training%20instability%2C%20which%20complicates%0Athe%20training%20process%20due%20to%20the%20necessity%20for%20techniques%20such%20as%20subtle%0Ainitialization%20and%20model%20distillation.%20In%20this%20study%2C%20we%20identify%20the%20local%0Aminima%20issue%20as%20the%20primary%20cause%20of%20this%20instability.%20To%20address%20this%2C%20we%0Aintegrate%20an%20optimal%20transport%20method%20in%20place%20of%20the%20nearest%20neighbor%20search%0Ato%20achieve%20a%20more%20globally%20informed%20assignment.%20We%20introduce%20OptVQ%2C%20a%20novel%0Avector%20quantization%20method%20that%20employs%20the%20Sinkhorn%20algorithm%20to%20optimize%20the%0Aoptimal%20transport%20problem%2C%20thereby%20enhancing%20the%20stability%20and%20efficiency%20of%0Athe%20training%20process.%20To%20mitigate%20the%20influence%20of%20diverse%20data%20distributions%0Aon%20the%20Sinkhorn%20algorithm%2C%20we%20implement%20a%20straightforward%20yet%20effective%0Anormalization%20strategy.%20Our%20comprehensive%20experiments%20on%20image%20reconstruction%0Atasks%20demonstrate%20that%20OptVQ%20achieves%20100%25%20codebook%20utilization%20and%20surpasses%0Acurrent%20state-of-the-art%20VQNs%20in%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15195v1&entry.124074799=Read"},
{"title": "Cycle Pixel Difference Network for Crisp Edge Detection", "author": "Changsong Liu and Wei Zhang and Yanyan Liu and Mingyang Li and Wenlin Li and Yimeng Fan and Xiangnan Bai and Liang Zhang", "abstract": "  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.\n", "link": "http://arxiv.org/abs/2409.04272v2", "date": "2024-12-19", "relevancy": 2.1317, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5488}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5471}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection&body=Title%3A%20Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection%0AAuthor%3A%20Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Mingyang%20Li%20and%20Wenlin%20Li%20and%20Yimeng%20Fan%20and%20Xiangnan%20Bai%20and%20Liang%20Zhang%0AAbstract%3A%20%20%20Edge%20detection%2C%20as%20a%20fundamental%20task%20in%20computer%20vision%2C%20has%20garnered%0Aincreasing%20attention.%20The%20advent%20of%20deep%20learning%20has%20significantly%20advanced%0Athis%20field.%20However%2C%20recent%20deep%20learning-based%20methods%20generally%20face%20two%0Asignificant%20issues%3A%201%29%20reliance%20on%20large-scale%20pre-trained%20weights%2C%20and%202%29%0Ageneration%20of%20thick%20edges.%20We%20construct%20a%20U-shape%20encoder-decoder%20model%20named%0ACPD-Net%20that%20successfully%20addresses%20these%20two%20issues%20simultaneously.%20In%0Aresponse%20to%20issue%201%29%2C%20we%20propose%20a%20novel%20cycle%20pixel%20difference%20convolution%0A%28CPDC%29%2C%20which%20effectively%20integrates%20edge%20prior%20knowledge%20with%20modern%0Aconvolution%20operations%2C%20consequently%20successfully%20eliminating%20the%20dependence%20on%0Alarge-scale%20pre-trained%20weights.%20As%20for%20issue%202%29%2C%20we%20construct%20a%20multi-scale%0Ainformation%20enhancement%20module%20%28MSEM%29%20and%20a%20dual%20residual%20connection-based%0A%28DRC%29%20decoder%20to%20enhance%20the%20edge%20location%20ability%20of%20the%20model%2C%20thereby%0Agenerating%20crisp%20and%20clean%20contour%20maps.%20Comprehensive%20experiments%20conducted%20on%0Afour%20standard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%0Aperformance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.813%20and%20AC%3D0.352%29%2C%20NYUD-V2%20%28ODS%3D0.760%0Aand%20AC%3D0.223%29%2C%20BIPED%20dataset%20%28ODS%3D0.898%20and%20AC%3D0.426%29%2C%20and%20CID%20%28ODS%3D0.59%29.%20Our%0Aapproach%20provides%20a%20novel%20perspective%20for%20addressing%20these%20challenges%20in%20edge%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycle%2520Pixel%2520Difference%2520Network%2520for%2520Crisp%2520Edge%2520Detection%26entry.906535625%3DChangsong%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Yanyan%2520Liu%2520and%2520Mingyang%2520Li%2520and%2520Wenlin%2520Li%2520and%2520Yimeng%2520Fan%2520and%2520Xiangnan%2520Bai%2520and%2520Liang%2520Zhang%26entry.1292438233%3D%2520%2520Edge%2520detection%252C%2520as%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%252C%2520has%2520garnered%250Aincreasing%2520attention.%2520The%2520advent%2520of%2520deep%2520learning%2520has%2520significantly%2520advanced%250Athis%2520field.%2520However%252C%2520recent%2520deep%2520learning-based%2520methods%2520generally%2520face%2520two%250Asignificant%2520issues%253A%25201%2529%2520reliance%2520on%2520large-scale%2520pre-trained%2520weights%252C%2520and%25202%2529%250Ageneration%2520of%2520thick%2520edges.%2520We%2520construct%2520a%2520U-shape%2520encoder-decoder%2520model%2520named%250ACPD-Net%2520that%2520successfully%2520addresses%2520these%2520two%2520issues%2520simultaneously.%2520In%250Aresponse%2520to%2520issue%25201%2529%252C%2520we%2520propose%2520a%2520novel%2520cycle%2520pixel%2520difference%2520convolution%250A%2528CPDC%2529%252C%2520which%2520effectively%2520integrates%2520edge%2520prior%2520knowledge%2520with%2520modern%250Aconvolution%2520operations%252C%2520consequently%2520successfully%2520eliminating%2520the%2520dependence%2520on%250Alarge-scale%2520pre-trained%2520weights.%2520As%2520for%2520issue%25202%2529%252C%2520we%2520construct%2520a%2520multi-scale%250Ainformation%2520enhancement%2520module%2520%2528MSEM%2529%2520and%2520a%2520dual%2520residual%2520connection-based%250A%2528DRC%2529%2520decoder%2520to%2520enhance%2520the%2520edge%2520location%2520ability%2520of%2520the%2520model%252C%2520thereby%250Agenerating%2520crisp%2520and%2520clean%2520contour%2520maps.%2520Comprehensive%2520experiments%2520conducted%2520on%250Afour%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%250Aperformance%2520on%2520the%2520BSDS500%2520dataset%2520%2528ODS%253D0.813%2520and%2520AC%253D0.352%2529%252C%2520NYUD-V2%2520%2528ODS%253D0.760%250Aand%2520AC%253D0.223%2529%252C%2520BIPED%2520dataset%2520%2528ODS%253D0.898%2520and%2520AC%253D0.426%2529%252C%2520and%2520CID%2520%2528ODS%253D0.59%2529.%2520Our%250Aapproach%2520provides%2520a%2520novel%2520perspective%2520for%2520addressing%2520these%2520challenges%2520in%2520edge%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle%20Pixel%20Difference%20Network%20for%20Crisp%20Edge%20Detection&entry.906535625=Changsong%20Liu%20and%20Wei%20Zhang%20and%20Yanyan%20Liu%20and%20Mingyang%20Li%20and%20Wenlin%20Li%20and%20Yimeng%20Fan%20and%20Xiangnan%20Bai%20and%20Liang%20Zhang&entry.1292438233=%20%20Edge%20detection%2C%20as%20a%20fundamental%20task%20in%20computer%20vision%2C%20has%20garnered%0Aincreasing%20attention.%20The%20advent%20of%20deep%20learning%20has%20significantly%20advanced%0Athis%20field.%20However%2C%20recent%20deep%20learning-based%20methods%20generally%20face%20two%0Asignificant%20issues%3A%201%29%20reliance%20on%20large-scale%20pre-trained%20weights%2C%20and%202%29%0Ageneration%20of%20thick%20edges.%20We%20construct%20a%20U-shape%20encoder-decoder%20model%20named%0ACPD-Net%20that%20successfully%20addresses%20these%20two%20issues%20simultaneously.%20In%0Aresponse%20to%20issue%201%29%2C%20we%20propose%20a%20novel%20cycle%20pixel%20difference%20convolution%0A%28CPDC%29%2C%20which%20effectively%20integrates%20edge%20prior%20knowledge%20with%20modern%0Aconvolution%20operations%2C%20consequently%20successfully%20eliminating%20the%20dependence%20on%0Alarge-scale%20pre-trained%20weights.%20As%20for%20issue%202%29%2C%20we%20construct%20a%20multi-scale%0Ainformation%20enhancement%20module%20%28MSEM%29%20and%20a%20dual%20residual%20connection-based%0A%28DRC%29%20decoder%20to%20enhance%20the%20edge%20location%20ability%20of%20the%20model%2C%20thereby%0Agenerating%20crisp%20and%20clean%20contour%20maps.%20Comprehensive%20experiments%20conducted%20on%0Afour%20standard%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%0Aperformance%20on%20the%20BSDS500%20dataset%20%28ODS%3D0.813%20and%20AC%3D0.352%29%2C%20NYUD-V2%20%28ODS%3D0.760%0Aand%20AC%3D0.223%29%2C%20BIPED%20dataset%20%28ODS%3D0.898%20and%20AC%3D0.426%29%2C%20and%20CID%20%28ODS%3D0.59%29.%20Our%0Aapproach%20provides%20a%20novel%20perspective%20for%20addressing%20these%20challenges%20in%20edge%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04272v2&entry.124074799=Read"},
{"title": "FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching", "author": "Sucheng Ren and Qihang Yu and Ju He and Xiaohui Shen and Alan Yuille and Liang-Chieh Chen", "abstract": "  Autoregressive (AR) modeling has achieved remarkable success in natural\nlanguage processing by enabling models to generate text with coherence and\ncontextual understanding through next token prediction. Recently, in image\ngeneration, VAR proposes scale-wise autoregressive modeling, which extends the\nnext token prediction to the next scale prediction, preserving the 2D structure\nof images. However, VAR encounters two primary challenges: (1) its complex and\nrigid scale design limits generalization in next scale prediction, and (2) the\ngenerator's dependence on a discrete tokenizer with the same complex scale\nstructure restricts modularity and flexibility in updating the tokenizer. To\naddress these limitations, we introduce FlowAR, a general next scale prediction\nmethod featuring a streamlined scale design, where each subsequent scale is\nsimply double the previous one. This eliminates the need for VAR's intricate\nmulti-scale residual tokenizer and enables the use of any off-the-shelf\nVariational AutoEncoder (VAE). Our simplified design enhances generalization in\nnext scale prediction and facilitates the integration of Flow Matching for\nhigh-quality image synthesis. We validate the effectiveness of FlowAR on the\nchallenging ImageNet-256 benchmark, demonstrating superior generation\nperformance compared to previous methods. Codes will be available at\n\\url{https://github.com/OliverRensu/FlowAR}.\n", "link": "http://arxiv.org/abs/2412.15205v1", "date": "2024-12-19", "relevancy": 2.1297, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6724}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowAR%3A%20Scale-wise%20Autoregressive%20Image%20Generation%20Meets%20Flow%20Matching&body=Title%3A%20FlowAR%3A%20Scale-wise%20Autoregressive%20Image%20Generation%20Meets%20Flow%20Matching%0AAuthor%3A%20Sucheng%20Ren%20and%20Qihang%20Yu%20and%20Ju%20He%20and%20Xiaohui%20Shen%20and%20Alan%20Yuille%20and%20Liang-Chieh%20Chen%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20modeling%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20by%20enabling%20models%20to%20generate%20text%20with%20coherence%20and%0Acontextual%20understanding%20through%20next%20token%20prediction.%20Recently%2C%20in%20image%0Ageneration%2C%20VAR%20proposes%20scale-wise%20autoregressive%20modeling%2C%20which%20extends%20the%0Anext%20token%20prediction%20to%20the%20next%20scale%20prediction%2C%20preserving%20the%202D%20structure%0Aof%20images.%20However%2C%20VAR%20encounters%20two%20primary%20challenges%3A%20%281%29%20its%20complex%20and%0Arigid%20scale%20design%20limits%20generalization%20in%20next%20scale%20prediction%2C%20and%20%282%29%20the%0Agenerator%27s%20dependence%20on%20a%20discrete%20tokenizer%20with%20the%20same%20complex%20scale%0Astructure%20restricts%20modularity%20and%20flexibility%20in%20updating%20the%20tokenizer.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20FlowAR%2C%20a%20general%20next%20scale%20prediction%0Amethod%20featuring%20a%20streamlined%20scale%20design%2C%20where%20each%20subsequent%20scale%20is%0Asimply%20double%20the%20previous%20one.%20This%20eliminates%20the%20need%20for%20VAR%27s%20intricate%0Amulti-scale%20residual%20tokenizer%20and%20enables%20the%20use%20of%20any%20off-the-shelf%0AVariational%20AutoEncoder%20%28VAE%29.%20Our%20simplified%20design%20enhances%20generalization%20in%0Anext%20scale%20prediction%20and%20facilitates%20the%20integration%20of%20Flow%20Matching%20for%0Ahigh-quality%20image%20synthesis.%20We%20validate%20the%20effectiveness%20of%20FlowAR%20on%20the%0Achallenging%20ImageNet-256%20benchmark%2C%20demonstrating%20superior%20generation%0Aperformance%20compared%20to%20previous%20methods.%20Codes%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/FlowAR%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowAR%253A%2520Scale-wise%2520Autoregressive%2520Image%2520Generation%2520Meets%2520Flow%2520Matching%26entry.906535625%3DSucheng%2520Ren%2520and%2520Qihang%2520Yu%2520and%2520Ju%2520He%2520and%2520Xiaohui%2520Shen%2520and%2520Alan%2520Yuille%2520and%2520Liang-Chieh%2520Chen%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520modeling%2520has%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520processing%2520by%2520enabling%2520models%2520to%2520generate%2520text%2520with%2520coherence%2520and%250Acontextual%2520understanding%2520through%2520next%2520token%2520prediction.%2520Recently%252C%2520in%2520image%250Ageneration%252C%2520VAR%2520proposes%2520scale-wise%2520autoregressive%2520modeling%252C%2520which%2520extends%2520the%250Anext%2520token%2520prediction%2520to%2520the%2520next%2520scale%2520prediction%252C%2520preserving%2520the%25202D%2520structure%250Aof%2520images.%2520However%252C%2520VAR%2520encounters%2520two%2520primary%2520challenges%253A%2520%25281%2529%2520its%2520complex%2520and%250Arigid%2520scale%2520design%2520limits%2520generalization%2520in%2520next%2520scale%2520prediction%252C%2520and%2520%25282%2529%2520the%250Agenerator%2527s%2520dependence%2520on%2520a%2520discrete%2520tokenizer%2520with%2520the%2520same%2520complex%2520scale%250Astructure%2520restricts%2520modularity%2520and%2520flexibility%2520in%2520updating%2520the%2520tokenizer.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520FlowAR%252C%2520a%2520general%2520next%2520scale%2520prediction%250Amethod%2520featuring%2520a%2520streamlined%2520scale%2520design%252C%2520where%2520each%2520subsequent%2520scale%2520is%250Asimply%2520double%2520the%2520previous%2520one.%2520This%2520eliminates%2520the%2520need%2520for%2520VAR%2527s%2520intricate%250Amulti-scale%2520residual%2520tokenizer%2520and%2520enables%2520the%2520use%2520of%2520any%2520off-the-shelf%250AVariational%2520AutoEncoder%2520%2528VAE%2529.%2520Our%2520simplified%2520design%2520enhances%2520generalization%2520in%250Anext%2520scale%2520prediction%2520and%2520facilitates%2520the%2520integration%2520of%2520Flow%2520Matching%2520for%250Ahigh-quality%2520image%2520synthesis.%2520We%2520validate%2520the%2520effectiveness%2520of%2520FlowAR%2520on%2520the%250Achallenging%2520ImageNet-256%2520benchmark%252C%2520demonstrating%2520superior%2520generation%250Aperformance%2520compared%2520to%2520previous%2520methods.%2520Codes%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/OliverRensu/FlowAR%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowAR%3A%20Scale-wise%20Autoregressive%20Image%20Generation%20Meets%20Flow%20Matching&entry.906535625=Sucheng%20Ren%20and%20Qihang%20Yu%20and%20Ju%20He%20and%20Xiaohui%20Shen%20and%20Alan%20Yuille%20and%20Liang-Chieh%20Chen&entry.1292438233=%20%20Autoregressive%20%28AR%29%20modeling%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20processing%20by%20enabling%20models%20to%20generate%20text%20with%20coherence%20and%0Acontextual%20understanding%20through%20next%20token%20prediction.%20Recently%2C%20in%20image%0Ageneration%2C%20VAR%20proposes%20scale-wise%20autoregressive%20modeling%2C%20which%20extends%20the%0Anext%20token%20prediction%20to%20the%20next%20scale%20prediction%2C%20preserving%20the%202D%20structure%0Aof%20images.%20However%2C%20VAR%20encounters%20two%20primary%20challenges%3A%20%281%29%20its%20complex%20and%0Arigid%20scale%20design%20limits%20generalization%20in%20next%20scale%20prediction%2C%20and%20%282%29%20the%0Agenerator%27s%20dependence%20on%20a%20discrete%20tokenizer%20with%20the%20same%20complex%20scale%0Astructure%20restricts%20modularity%20and%20flexibility%20in%20updating%20the%20tokenizer.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20FlowAR%2C%20a%20general%20next%20scale%20prediction%0Amethod%20featuring%20a%20streamlined%20scale%20design%2C%20where%20each%20subsequent%20scale%20is%0Asimply%20double%20the%20previous%20one.%20This%20eliminates%20the%20need%20for%20VAR%27s%20intricate%0Amulti-scale%20residual%20tokenizer%20and%20enables%20the%20use%20of%20any%20off-the-shelf%0AVariational%20AutoEncoder%20%28VAE%29.%20Our%20simplified%20design%20enhances%20generalization%20in%0Anext%20scale%20prediction%20and%20facilitates%20the%20integration%20of%20Flow%20Matching%20for%0Ahigh-quality%20image%20synthesis.%20We%20validate%20the%20effectiveness%20of%20FlowAR%20on%20the%0Achallenging%20ImageNet-256%20benchmark%2C%20demonstrating%20superior%20generation%0Aperformance%20compared%20to%20previous%20methods.%20Codes%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/FlowAR%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15205v1&entry.124074799=Read"},
{"title": "Revisiting Machine Unlearning with Dimensional Alignment", "author": "Seonguk Seo and Dongwan Kim and Bohyung Han", "abstract": "  Machine unlearning, an emerging research topic focusing on compliance with\ndata privacy regulations, enables trained models to remove the information\nlearned from specific data. While many existing methods indirectly address this\nissue by intentionally injecting incorrect supervisions, they can drastically\nand unpredictably alter the decision boundaries and feature spaces, leading to\ntraining instability and undesired side effects. To fundamentally approach this\ntask, we first analyze the changes in latent feature spaces between original\nand retrained models, and observe that the feature representations of samples\nnot involved in training are closely aligned with the feature manifolds of\npreviously seen samples in training. Based on these findings, we introduce a\nnovel evaluation metric for machine unlearning, coined dimensional alignment,\nwhich measures the alignment between the eigenspaces of the forget and retain\nset samples. We employ this metric as a regularizer loss to build a robust and\nstable unlearning framework, which is further enhanced by integrating a\nself-distillation loss and an alternating training scheme. Our framework\neffectively eliminates information from the forget set and preserves knowledge\nfrom the retain set. Lastly, we identify critical flaws in established\nevaluation metrics for machine unlearning, and introduce new evaluation tools\nthat more accurately reflect the fundamental goals of machine unlearning.\n", "link": "http://arxiv.org/abs/2407.17710v2", "date": "2024-12-19", "relevancy": 2.1268, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5522}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5311}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Machine%20Unlearning%20with%20Dimensional%20Alignment&body=Title%3A%20Revisiting%20Machine%20Unlearning%20with%20Dimensional%20Alignment%0AAuthor%3A%20Seonguk%20Seo%20and%20Dongwan%20Kim%20and%20Bohyung%20Han%0AAbstract%3A%20%20%20Machine%20unlearning%2C%20an%20emerging%20research%20topic%20focusing%20on%20compliance%20with%0Adata%20privacy%20regulations%2C%20enables%20trained%20models%20to%20remove%20the%20information%0Alearned%20from%20specific%20data.%20While%20many%20existing%20methods%20indirectly%20address%20this%0Aissue%20by%20intentionally%20injecting%20incorrect%20supervisions%2C%20they%20can%20drastically%0Aand%20unpredictably%20alter%20the%20decision%20boundaries%20and%20feature%20spaces%2C%20leading%20to%0Atraining%20instability%20and%20undesired%20side%20effects.%20To%20fundamentally%20approach%20this%0Atask%2C%20we%20first%20analyze%20the%20changes%20in%20latent%20feature%20spaces%20between%20original%0Aand%20retrained%20models%2C%20and%20observe%20that%20the%20feature%20representations%20of%20samples%0Anot%20involved%20in%20training%20are%20closely%20aligned%20with%20the%20feature%20manifolds%20of%0Apreviously%20seen%20samples%20in%20training.%20Based%20on%20these%20findings%2C%20we%20introduce%20a%0Anovel%20evaluation%20metric%20for%20machine%20unlearning%2C%20coined%20dimensional%20alignment%2C%0Awhich%20measures%20the%20alignment%20between%20the%20eigenspaces%20of%20the%20forget%20and%20retain%0Aset%20samples.%20We%20employ%20this%20metric%20as%20a%20regularizer%20loss%20to%20build%20a%20robust%20and%0Astable%20unlearning%20framework%2C%20which%20is%20further%20enhanced%20by%20integrating%20a%0Aself-distillation%20loss%20and%20an%20alternating%20training%20scheme.%20Our%20framework%0Aeffectively%20eliminates%20information%20from%20the%20forget%20set%20and%20preserves%20knowledge%0Afrom%20the%20retain%20set.%20Lastly%2C%20we%20identify%20critical%20flaws%20in%20established%0Aevaluation%20metrics%20for%20machine%20unlearning%2C%20and%20introduce%20new%20evaluation%20tools%0Athat%20more%20accurately%20reflect%20the%20fundamental%20goals%20of%20machine%20unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Machine%2520Unlearning%2520with%2520Dimensional%2520Alignment%26entry.906535625%3DSeonguk%2520Seo%2520and%2520Dongwan%2520Kim%2520and%2520Bohyung%2520Han%26entry.1292438233%3D%2520%2520Machine%2520unlearning%252C%2520an%2520emerging%2520research%2520topic%2520focusing%2520on%2520compliance%2520with%250Adata%2520privacy%2520regulations%252C%2520enables%2520trained%2520models%2520to%2520remove%2520the%2520information%250Alearned%2520from%2520specific%2520data.%2520While%2520many%2520existing%2520methods%2520indirectly%2520address%2520this%250Aissue%2520by%2520intentionally%2520injecting%2520incorrect%2520supervisions%252C%2520they%2520can%2520drastically%250Aand%2520unpredictably%2520alter%2520the%2520decision%2520boundaries%2520and%2520feature%2520spaces%252C%2520leading%2520to%250Atraining%2520instability%2520and%2520undesired%2520side%2520effects.%2520To%2520fundamentally%2520approach%2520this%250Atask%252C%2520we%2520first%2520analyze%2520the%2520changes%2520in%2520latent%2520feature%2520spaces%2520between%2520original%250Aand%2520retrained%2520models%252C%2520and%2520observe%2520that%2520the%2520feature%2520representations%2520of%2520samples%250Anot%2520involved%2520in%2520training%2520are%2520closely%2520aligned%2520with%2520the%2520feature%2520manifolds%2520of%250Apreviously%2520seen%2520samples%2520in%2520training.%2520Based%2520on%2520these%2520findings%252C%2520we%2520introduce%2520a%250Anovel%2520evaluation%2520metric%2520for%2520machine%2520unlearning%252C%2520coined%2520dimensional%2520alignment%252C%250Awhich%2520measures%2520the%2520alignment%2520between%2520the%2520eigenspaces%2520of%2520the%2520forget%2520and%2520retain%250Aset%2520samples.%2520We%2520employ%2520this%2520metric%2520as%2520a%2520regularizer%2520loss%2520to%2520build%2520a%2520robust%2520and%250Astable%2520unlearning%2520framework%252C%2520which%2520is%2520further%2520enhanced%2520by%2520integrating%2520a%250Aself-distillation%2520loss%2520and%2520an%2520alternating%2520training%2520scheme.%2520Our%2520framework%250Aeffectively%2520eliminates%2520information%2520from%2520the%2520forget%2520set%2520and%2520preserves%2520knowledge%250Afrom%2520the%2520retain%2520set.%2520Lastly%252C%2520we%2520identify%2520critical%2520flaws%2520in%2520established%250Aevaluation%2520metrics%2520for%2520machine%2520unlearning%252C%2520and%2520introduce%2520new%2520evaluation%2520tools%250Athat%2520more%2520accurately%2520reflect%2520the%2520fundamental%2520goals%2520of%2520machine%2520unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Machine%20Unlearning%20with%20Dimensional%20Alignment&entry.906535625=Seonguk%20Seo%20and%20Dongwan%20Kim%20and%20Bohyung%20Han&entry.1292438233=%20%20Machine%20unlearning%2C%20an%20emerging%20research%20topic%20focusing%20on%20compliance%20with%0Adata%20privacy%20regulations%2C%20enables%20trained%20models%20to%20remove%20the%20information%0Alearned%20from%20specific%20data.%20While%20many%20existing%20methods%20indirectly%20address%20this%0Aissue%20by%20intentionally%20injecting%20incorrect%20supervisions%2C%20they%20can%20drastically%0Aand%20unpredictably%20alter%20the%20decision%20boundaries%20and%20feature%20spaces%2C%20leading%20to%0Atraining%20instability%20and%20undesired%20side%20effects.%20To%20fundamentally%20approach%20this%0Atask%2C%20we%20first%20analyze%20the%20changes%20in%20latent%20feature%20spaces%20between%20original%0Aand%20retrained%20models%2C%20and%20observe%20that%20the%20feature%20representations%20of%20samples%0Anot%20involved%20in%20training%20are%20closely%20aligned%20with%20the%20feature%20manifolds%20of%0Apreviously%20seen%20samples%20in%20training.%20Based%20on%20these%20findings%2C%20we%20introduce%20a%0Anovel%20evaluation%20metric%20for%20machine%20unlearning%2C%20coined%20dimensional%20alignment%2C%0Awhich%20measures%20the%20alignment%20between%20the%20eigenspaces%20of%20the%20forget%20and%20retain%0Aset%20samples.%20We%20employ%20this%20metric%20as%20a%20regularizer%20loss%20to%20build%20a%20robust%20and%0Astable%20unlearning%20framework%2C%20which%20is%20further%20enhanced%20by%20integrating%20a%0Aself-distillation%20loss%20and%20an%20alternating%20training%20scheme.%20Our%20framework%0Aeffectively%20eliminates%20information%20from%20the%20forget%20set%20and%20preserves%20knowledge%0Afrom%20the%20retain%20set.%20Lastly%2C%20we%20identify%20critical%20flaws%20in%20established%0Aevaluation%20metrics%20for%20machine%20unlearning%2C%20and%20introduce%20new%20evaluation%20tools%0Athat%20more%20accurately%20reflect%20the%20fundamental%20goals%20of%20machine%20unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17710v2&entry.124074799=Read"},
{"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "author": "Kathrin Se\u00dfler and Yao Rong and Emek G\u00f6zl\u00fckl\u00fc and Enkelejda Kasneci", "abstract": "  The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.\n", "link": "http://arxiv.org/abs/2408.10839v2", "date": "2024-12-19", "relevancy": 2.1223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Large%20Language%20Models%20for%20Math%20Reasoning%20Tasks&body=Title%3A%20Benchmarking%20Large%20Language%20Models%20for%20Math%20Reasoning%20Tasks%0AAuthor%3A%20Kathrin%20Se%C3%9Fler%20and%20Yao%20Rong%20and%20Emek%20G%C3%B6zl%C3%BCkl%C3%BC%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20The%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20mathematical%20reasoning%20has%20become%0Aa%20cornerstone%20of%20related%20research%2C%20demonstrating%20the%20intelligence%20of%20these%0Amodels%20and%20enabling%20potential%20practical%20applications%20through%20their%20advanced%0Aperformance%2C%20such%20as%20in%20educational%20settings.%20Despite%20the%20variety%20of%20datasets%0Aand%20in-context%20learning%20algorithms%20designed%20to%20improve%20the%20ability%20of%20LLMs%20to%0Aautomate%20mathematical%20problem%20solving%2C%20the%20lack%20of%20comprehensive%20benchmarking%0Aacross%20different%20datasets%20makes%20it%20complicated%20to%20select%20an%20appropriate%20model%0Afor%20specific%20tasks.%20In%20this%20project%2C%20we%20present%20a%20benchmark%20that%20fairly%0Acompares%20seven%20state-of-the-art%20in-context%20learning%20algorithms%20for%20mathematical%0Aproblem%20solving%20across%20five%20widely%20used%20mathematical%20datasets%20on%20four%20powerful%0Afoundation%20models.%20Furthermore%2C%20we%20explore%20the%20trade-off%20between%20efficiency%20and%0Aperformance%2C%20highlighting%20the%20practical%20applications%20of%20LLMs%20for%20mathematical%0Areasoning.%20Our%20results%20indicate%20that%20larger%20foundation%20models%20like%20GPT-4o%20and%0ALLaMA%203-70B%20can%20solve%20mathematical%20reasoning%20independently%20from%20the%20concrete%0Aprompting%20strategy%2C%20while%20for%20smaller%20models%20the%20in-context%20learning%20approach%0Asignificantly%20influences%20the%20performance.%20Moreover%2C%20the%20optimal%20prompt%20depends%0Aon%20the%20chosen%20foundation%20model.%20We%20open-source%20our%20benchmark%20code%20to%20support%0Athe%20integration%20of%20additional%20models%20in%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Large%2520Language%2520Models%2520for%2520Math%2520Reasoning%2520Tasks%26entry.906535625%3DKathrin%2520Se%25C3%259Fler%2520and%2520Yao%2520Rong%2520and%2520Emek%2520G%25C3%25B6zl%25C3%25BCkl%25C3%25BC%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520mathematical%2520reasoning%2520has%2520become%250Aa%2520cornerstone%2520of%2520related%2520research%252C%2520demonstrating%2520the%2520intelligence%2520of%2520these%250Amodels%2520and%2520enabling%2520potential%2520practical%2520applications%2520through%2520their%2520advanced%250Aperformance%252C%2520such%2520as%2520in%2520educational%2520settings.%2520Despite%2520the%2520variety%2520of%2520datasets%250Aand%2520in-context%2520learning%2520algorithms%2520designed%2520to%2520improve%2520the%2520ability%2520of%2520LLMs%2520to%250Aautomate%2520mathematical%2520problem%2520solving%252C%2520the%2520lack%2520of%2520comprehensive%2520benchmarking%250Aacross%2520different%2520datasets%2520makes%2520it%2520complicated%2520to%2520select%2520an%2520appropriate%2520model%250Afor%2520specific%2520tasks.%2520In%2520this%2520project%252C%2520we%2520present%2520a%2520benchmark%2520that%2520fairly%250Acompares%2520seven%2520state-of-the-art%2520in-context%2520learning%2520algorithms%2520for%2520mathematical%250Aproblem%2520solving%2520across%2520five%2520widely%2520used%2520mathematical%2520datasets%2520on%2520four%2520powerful%250Afoundation%2520models.%2520Furthermore%252C%2520we%2520explore%2520the%2520trade-off%2520between%2520efficiency%2520and%250Aperformance%252C%2520highlighting%2520the%2520practical%2520applications%2520of%2520LLMs%2520for%2520mathematical%250Areasoning.%2520Our%2520results%2520indicate%2520that%2520larger%2520foundation%2520models%2520like%2520GPT-4o%2520and%250ALLaMA%25203-70B%2520can%2520solve%2520mathematical%2520reasoning%2520independently%2520from%2520the%2520concrete%250Aprompting%2520strategy%252C%2520while%2520for%2520smaller%2520models%2520the%2520in-context%2520learning%2520approach%250Asignificantly%2520influences%2520the%2520performance.%2520Moreover%252C%2520the%2520optimal%2520prompt%2520depends%250Aon%2520the%2520chosen%2520foundation%2520model.%2520We%2520open-source%2520our%2520benchmark%2520code%2520to%2520support%250Athe%2520integration%2520of%2520additional%2520models%2520in%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Large%20Language%20Models%20for%20Math%20Reasoning%20Tasks&entry.906535625=Kathrin%20Se%C3%9Fler%20and%20Yao%20Rong%20and%20Emek%20G%C3%B6zl%C3%BCkl%C3%BC%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20The%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20mathematical%20reasoning%20has%20become%0Aa%20cornerstone%20of%20related%20research%2C%20demonstrating%20the%20intelligence%20of%20these%0Amodels%20and%20enabling%20potential%20practical%20applications%20through%20their%20advanced%0Aperformance%2C%20such%20as%20in%20educational%20settings.%20Despite%20the%20variety%20of%20datasets%0Aand%20in-context%20learning%20algorithms%20designed%20to%20improve%20the%20ability%20of%20LLMs%20to%0Aautomate%20mathematical%20problem%20solving%2C%20the%20lack%20of%20comprehensive%20benchmarking%0Aacross%20different%20datasets%20makes%20it%20complicated%20to%20select%20an%20appropriate%20model%0Afor%20specific%20tasks.%20In%20this%20project%2C%20we%20present%20a%20benchmark%20that%20fairly%0Acompares%20seven%20state-of-the-art%20in-context%20learning%20algorithms%20for%20mathematical%0Aproblem%20solving%20across%20five%20widely%20used%20mathematical%20datasets%20on%20four%20powerful%0Afoundation%20models.%20Furthermore%2C%20we%20explore%20the%20trade-off%20between%20efficiency%20and%0Aperformance%2C%20highlighting%20the%20practical%20applications%20of%20LLMs%20for%20mathematical%0Areasoning.%20Our%20results%20indicate%20that%20larger%20foundation%20models%20like%20GPT-4o%20and%0ALLaMA%203-70B%20can%20solve%20mathematical%20reasoning%20independently%20from%20the%20concrete%0Aprompting%20strategy%2C%20while%20for%20smaller%20models%20the%20in-context%20learning%20approach%0Asignificantly%20influences%20the%20performance.%20Moreover%2C%20the%20optimal%20prompt%20depends%0Aon%20the%20chosen%20foundation%20model.%20We%20open-source%20our%20benchmark%20code%20to%20support%0Athe%20integration%20of%20additional%20models%20in%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10839v2&entry.124074799=Read"},
{"title": "MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images", "author": "Zhiwei Wang and Ying Zhou and Shiquan He and Ting Li and Fan Huang and Qiang Ding and Xinxia Feng and Mei Liu and Qiang Li", "abstract": "  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n", "link": "http://arxiv.org/abs/2404.16571v4", "date": "2024-12-19", "relevancy": 2.1206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&body=Title%3A%20MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images%0AAuthor%3A%20Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li%0AAbstract%3A%20%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16571v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoPCC%253A%2520Photometric-invariant%2520Cycle%2520Constraint%2520for%2520Monocular%2520Depth%250A%2520%2520Estimation%2520of%2520Endoscopic%2520Images%26entry.906535625%3DZhiwei%2520Wang%2520and%2520Ying%2520Zhou%2520and%2520Shiquan%2520He%2520and%2520Ting%2520Li%2520and%2520Fan%2520Huang%2520and%2520Qiang%2520Ding%2520and%2520Xinxia%2520Feng%2520and%2520Mei%2520Liu%2520and%2520Qiang%2520Li%26entry.1292438233%3D%2520%2520Photometric%2520constraint%2520is%2520indispensable%2520for%2520self-supervised%2520monocular%2520depth%250Aestimation.%2520It%2520involves%2520warping%2520a%2520source%2520image%2520onto%2520a%2520target%2520view%2520using%250Aestimated%2520depth%2526pose%252C%2520and%2520then%2520minimizing%2520the%2520difference%2520between%2520the%2520warped%2520and%250Atarget%2520images.%2520However%252C%2520the%2520endoscopic%2520built-in%2520light%2520causes%2520significant%250Abrightness%2520fluctuations%252C%2520and%2520thus%2520makes%2520the%2520photometric%2520constraint%2520unreliable.%250APrevious%2520efforts%2520only%2520mitigate%2520this%2520relying%2520on%2520extra%2520models%2520to%2520calibrate%2520image%250Abrightness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MonoPCC%2520to%2520address%2520the%2520brightness%250Ainconsistency%2520radically%2520by%2520reshaping%2520the%2520photometric%2520constraint%2520into%2520a%2520cycle%250Aform.%2520Instead%2520of%2520only%2520warping%2520the%2520source%2520image%252C%2520MonoPCC%2520constructs%2520a%2520closed%250Aloop%2520consisting%2520of%2520two%2520opposite%2520forward-backward%2520warping%2520paths%253A%2520from%2520target%2520to%250Asource%2520and%2520then%2520back%2520to%2520target.%2520Thus%252C%2520the%2520target%2520image%2520finally%2520receives%2520an%250Aimage%2520cycle-warped%2520from%2520itself%252C%2520which%2520naturally%2520makes%2520the%2520constraint%2520invariant%250Ato%2520brightness%2520changes.%2520Moreover%252C%2520MonoPCC%2520transplants%2520the%2520source%2520image%2527s%250Aphase-frequency%2520into%2520the%2520intermediate%2520warped%2520image%2520to%2520avoid%2520structure%2520lost%252C%2520and%250Aalso%2520stabilizes%2520the%2520training%2520via%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520strategy%250Ato%2520avoid%2520frequent%2520changes%2520in%2520the%2520forward%2520warping.%2520The%2520comprehensive%2520and%250Aextensive%2520experimental%2520results%2520on%2520four%2520endoscopic%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520MonoPCC%2520shows%2520a%2520great%2520robustness%2520to%2520the%2520brightness%2520inconsistency%252C%2520and%250Aexceeds%2520other%2520state-of-the-arts%2520by%2520reducing%2520the%2520absolute%2520relative%2520error%2520by%2520at%250Aleast%25207.27%2525%252C%25209.38%2525%252C%25209.90%2525%2520and%25203.17%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16571v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoPCC%3A%20Photometric-invariant%20Cycle%20Constraint%20for%20Monocular%20Depth%0A%20%20Estimation%20of%20Endoscopic%20Images&entry.906535625=Zhiwei%20Wang%20and%20Ying%20Zhou%20and%20Shiquan%20He%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Qiang%20Ding%20and%20Xinxia%20Feng%20and%20Mei%20Liu%20and%20Qiang%20Li&entry.1292438233=%20%20Photometric%20constraint%20is%20indispensable%20for%20self-supervised%20monocular%20depth%0Aestimation.%20It%20involves%20warping%20a%20source%20image%20onto%20a%20target%20view%20using%0Aestimated%20depth%26pose%2C%20and%20then%20minimizing%20the%20difference%20between%20the%20warped%20and%0Atarget%20images.%20However%2C%20the%20endoscopic%20built-in%20light%20causes%20significant%0Abrightness%20fluctuations%2C%20and%20thus%20makes%20the%20photometric%20constraint%20unreliable.%0APrevious%20efforts%20only%20mitigate%20this%20relying%20on%20extra%20models%20to%20calibrate%20image%0Abrightness.%20In%20this%20paper%2C%20we%20propose%20MonoPCC%20to%20address%20the%20brightness%0Ainconsistency%20radically%20by%20reshaping%20the%20photometric%20constraint%20into%20a%20cycle%0Aform.%20Instead%20of%20only%20warping%20the%20source%20image%2C%20MonoPCC%20constructs%20a%20closed%0Aloop%20consisting%20of%20two%20opposite%20forward-backward%20warping%20paths%3A%20from%20target%20to%0Asource%20and%20then%20back%20to%20target.%20Thus%2C%20the%20target%20image%20finally%20receives%20an%0Aimage%20cycle-warped%20from%20itself%2C%20which%20naturally%20makes%20the%20constraint%20invariant%0Ato%20brightness%20changes.%20Moreover%2C%20MonoPCC%20transplants%20the%20source%20image%27s%0Aphase-frequency%20into%20the%20intermediate%20warped%20image%20to%20avoid%20structure%20lost%2C%20and%0Aalso%20stabilizes%20the%20training%20via%20an%20exponential%20moving%20average%20%28EMA%29%20strategy%0Ato%20avoid%20frequent%20changes%20in%20the%20forward%20warping.%20The%20comprehensive%20and%0Aextensive%20experimental%20results%20on%20four%20endoscopic%20datasets%20demonstrate%20that%20our%0Aproposed%20MonoPCC%20shows%20a%20great%20robustness%20to%20the%20brightness%20inconsistency%2C%20and%0Aexceeds%20other%20state-of-the-arts%20by%20reducing%20the%20absolute%20relative%20error%20by%20at%0Aleast%207.27%25%2C%209.38%25%2C%209.90%25%20and%203.17%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16571v4&entry.124074799=Read"},
{"title": "How to Re-enable PDE Loss for Physical Systems Modeling Under Partial\n  Observation", "author": "Haodong Feng and Yue Wang and Dixia Fan", "abstract": "  In science and engineering, machine learning techniques are increasingly\nsuccessful in physical systems modeling (predicting future states of physical\nsystems). Effectively integrating PDE loss as a constraint of system transition\ncan improve the model's prediction by overcoming generalization issues due to\ndata scarcity, especially when data acquisition is costly. However, in many\nreal-world scenarios, due to sensor limitations, the data we can obtain is\noften only partial observation, making the calculation of PDE loss seem to be\ninfeasible, as the PDE loss heavily relies on high-resolution states. We\ncarefully study this problem and propose a novel framework named Re-enable PDE\nLoss under Partial Observation (RPLPO). The key idea is that although enabling\nPDE loss to constrain system transition solely is infeasible, we can re-enable\nPDE loss by reconstructing the learnable high-resolution state and constraining\nsystem transition simultaneously. Specifically, RPLPO combines an encoding\nmodule for reconstructing learnable high-resolution states with a transition\nmodule for predicting future states. The two modules are jointly trained by\ndata and PDE loss. We conduct experiments in various physical systems to\ndemonstrate that RPLPO has significant improvement in generalization, even when\nobservation is sparse, irregular, noisy, and PDE is inaccurate.\n", "link": "http://arxiv.org/abs/2412.09116v3", "date": "2024-12-19", "relevancy": 2.1151, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5253}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Re-enable%20PDE%20Loss%20for%20Physical%20Systems%20Modeling%20Under%20Partial%0A%20%20Observation&body=Title%3A%20How%20to%20Re-enable%20PDE%20Loss%20for%20Physical%20Systems%20Modeling%20Under%20Partial%0A%20%20Observation%0AAuthor%3A%20Haodong%20Feng%20and%20Yue%20Wang%20and%20Dixia%20Fan%0AAbstract%3A%20%20%20In%20science%20and%20engineering%2C%20machine%20learning%20techniques%20are%20increasingly%0Asuccessful%20in%20physical%20systems%20modeling%20%28predicting%20future%20states%20of%20physical%0Asystems%29.%20Effectively%20integrating%20PDE%20loss%20as%20a%20constraint%20of%20system%20transition%0Acan%20improve%20the%20model%27s%20prediction%20by%20overcoming%20generalization%20issues%20due%20to%0Adata%20scarcity%2C%20especially%20when%20data%20acquisition%20is%20costly.%20However%2C%20in%20many%0Areal-world%20scenarios%2C%20due%20to%20sensor%20limitations%2C%20the%20data%20we%20can%20obtain%20is%0Aoften%20only%20partial%20observation%2C%20making%20the%20calculation%20of%20PDE%20loss%20seem%20to%20be%0Ainfeasible%2C%20as%20the%20PDE%20loss%20heavily%20relies%20on%20high-resolution%20states.%20We%0Acarefully%20study%20this%20problem%20and%20propose%20a%20novel%20framework%20named%20Re-enable%20PDE%0ALoss%20under%20Partial%20Observation%20%28RPLPO%29.%20The%20key%20idea%20is%20that%20although%20enabling%0APDE%20loss%20to%20constrain%20system%20transition%20solely%20is%20infeasible%2C%20we%20can%20re-enable%0APDE%20loss%20by%20reconstructing%20the%20learnable%20high-resolution%20state%20and%20constraining%0Asystem%20transition%20simultaneously.%20Specifically%2C%20RPLPO%20combines%20an%20encoding%0Amodule%20for%20reconstructing%20learnable%20high-resolution%20states%20with%20a%20transition%0Amodule%20for%20predicting%20future%20states.%20The%20two%20modules%20are%20jointly%20trained%20by%0Adata%20and%20PDE%20loss.%20We%20conduct%20experiments%20in%20various%20physical%20systems%20to%0Ademonstrate%20that%20RPLPO%20has%20significant%20improvement%20in%20generalization%2C%20even%20when%0Aobservation%20is%20sparse%2C%20irregular%2C%20noisy%2C%20and%20PDE%20is%20inaccurate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09116v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Re-enable%2520PDE%2520Loss%2520for%2520Physical%2520Systems%2520Modeling%2520Under%2520Partial%250A%2520%2520Observation%26entry.906535625%3DHaodong%2520Feng%2520and%2520Yue%2520Wang%2520and%2520Dixia%2520Fan%26entry.1292438233%3D%2520%2520In%2520science%2520and%2520engineering%252C%2520machine%2520learning%2520techniques%2520are%2520increasingly%250Asuccessful%2520in%2520physical%2520systems%2520modeling%2520%2528predicting%2520future%2520states%2520of%2520physical%250Asystems%2529.%2520Effectively%2520integrating%2520PDE%2520loss%2520as%2520a%2520constraint%2520of%2520system%2520transition%250Acan%2520improve%2520the%2520model%2527s%2520prediction%2520by%2520overcoming%2520generalization%2520issues%2520due%2520to%250Adata%2520scarcity%252C%2520especially%2520when%2520data%2520acquisition%2520is%2520costly.%2520However%252C%2520in%2520many%250Areal-world%2520scenarios%252C%2520due%2520to%2520sensor%2520limitations%252C%2520the%2520data%2520we%2520can%2520obtain%2520is%250Aoften%2520only%2520partial%2520observation%252C%2520making%2520the%2520calculation%2520of%2520PDE%2520loss%2520seem%2520to%2520be%250Ainfeasible%252C%2520as%2520the%2520PDE%2520loss%2520heavily%2520relies%2520on%2520high-resolution%2520states.%2520We%250Acarefully%2520study%2520this%2520problem%2520and%2520propose%2520a%2520novel%2520framework%2520named%2520Re-enable%2520PDE%250ALoss%2520under%2520Partial%2520Observation%2520%2528RPLPO%2529.%2520The%2520key%2520idea%2520is%2520that%2520although%2520enabling%250APDE%2520loss%2520to%2520constrain%2520system%2520transition%2520solely%2520is%2520infeasible%252C%2520we%2520can%2520re-enable%250APDE%2520loss%2520by%2520reconstructing%2520the%2520learnable%2520high-resolution%2520state%2520and%2520constraining%250Asystem%2520transition%2520simultaneously.%2520Specifically%252C%2520RPLPO%2520combines%2520an%2520encoding%250Amodule%2520for%2520reconstructing%2520learnable%2520high-resolution%2520states%2520with%2520a%2520transition%250Amodule%2520for%2520predicting%2520future%2520states.%2520The%2520two%2520modules%2520are%2520jointly%2520trained%2520by%250Adata%2520and%2520PDE%2520loss.%2520We%2520conduct%2520experiments%2520in%2520various%2520physical%2520systems%2520to%250Ademonstrate%2520that%2520RPLPO%2520has%2520significant%2520improvement%2520in%2520generalization%252C%2520even%2520when%250Aobservation%2520is%2520sparse%252C%2520irregular%252C%2520noisy%252C%2520and%2520PDE%2520is%2520inaccurate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09116v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Re-enable%20PDE%20Loss%20for%20Physical%20Systems%20Modeling%20Under%20Partial%0A%20%20Observation&entry.906535625=Haodong%20Feng%20and%20Yue%20Wang%20and%20Dixia%20Fan&entry.1292438233=%20%20In%20science%20and%20engineering%2C%20machine%20learning%20techniques%20are%20increasingly%0Asuccessful%20in%20physical%20systems%20modeling%20%28predicting%20future%20states%20of%20physical%0Asystems%29.%20Effectively%20integrating%20PDE%20loss%20as%20a%20constraint%20of%20system%20transition%0Acan%20improve%20the%20model%27s%20prediction%20by%20overcoming%20generalization%20issues%20due%20to%0Adata%20scarcity%2C%20especially%20when%20data%20acquisition%20is%20costly.%20However%2C%20in%20many%0Areal-world%20scenarios%2C%20due%20to%20sensor%20limitations%2C%20the%20data%20we%20can%20obtain%20is%0Aoften%20only%20partial%20observation%2C%20making%20the%20calculation%20of%20PDE%20loss%20seem%20to%20be%0Ainfeasible%2C%20as%20the%20PDE%20loss%20heavily%20relies%20on%20high-resolution%20states.%20We%0Acarefully%20study%20this%20problem%20and%20propose%20a%20novel%20framework%20named%20Re-enable%20PDE%0ALoss%20under%20Partial%20Observation%20%28RPLPO%29.%20The%20key%20idea%20is%20that%20although%20enabling%0APDE%20loss%20to%20constrain%20system%20transition%20solely%20is%20infeasible%2C%20we%20can%20re-enable%0APDE%20loss%20by%20reconstructing%20the%20learnable%20high-resolution%20state%20and%20constraining%0Asystem%20transition%20simultaneously.%20Specifically%2C%20RPLPO%20combines%20an%20encoding%0Amodule%20for%20reconstructing%20learnable%20high-resolution%20states%20with%20a%20transition%0Amodule%20for%20predicting%20future%20states.%20The%20two%20modules%20are%20jointly%20trained%20by%0Adata%20and%20PDE%20loss.%20We%20conduct%20experiments%20in%20various%20physical%20systems%20to%0Ademonstrate%20that%20RPLPO%20has%20significant%20improvement%20in%20generalization%2C%20even%20when%0Aobservation%20is%20sparse%2C%20irregular%2C%20noisy%2C%20and%20PDE%20is%20inaccurate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09116v3&entry.124074799=Read"},
{"title": "FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal\n  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities", "author": "Byeonghun Kim and Byeongjoon Noh", "abstract": "  In smart cities, detecting pedestrian falls is a major challenge to ensure\nthe safety and quality of life of citizens. In this study, we propose a novel\nfall detection system using FLAMe (Federated Learning with Attention\nMechanism), a federated learning (FL) based algorithm. FLAMe trains around\nimportant keypoint information and only transmits the trained important weights\nto the server, reducing communication costs and preserving data privacy.\nFurthermore, the lightweight keypoint transformer model is integrated into the\nFL framework to effectively learn spatio-temporal features. We validated the\nexperiment using 22,672 video samples from the \"Fall Accident Risk Behavior\nVideo-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the\nFLAMe-based system achieved an accuracy of 94.02% with about 190,000\ntransmission parameters, maintaining performance similar to that of existing\ncentralized learning while maximizing efficiency by reducing communication\ncosts by about 40% compared to the existing FL algorithm, FedAvg. Therefore,\nthe FLAMe algorithm has demonstrated that it provides robust performance in the\ndistributed environment of smart cities and is a practical and effective\nsolution for public safety.\n", "link": "http://arxiv.org/abs/2412.14768v1", "date": "2024-12-19", "relevancy": 2.1042, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5403}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAMe%3A%20Federated%20Learning%20with%20Attention%20Mechanism%20using%20Spatio-Temporal%0A%20%20Keypoint%20Transformers%20for%20Pedestrian%20Fall%20Detection%20in%20Smart%20Cities&body=Title%3A%20FLAMe%3A%20Federated%20Learning%20with%20Attention%20Mechanism%20using%20Spatio-Temporal%0A%20%20Keypoint%20Transformers%20for%20Pedestrian%20Fall%20Detection%20in%20Smart%20Cities%0AAuthor%3A%20Byeonghun%20Kim%20and%20Byeongjoon%20Noh%0AAbstract%3A%20%20%20In%20smart%20cities%2C%20detecting%20pedestrian%20falls%20is%20a%20major%20challenge%20to%20ensure%0Athe%20safety%20and%20quality%20of%20life%20of%20citizens.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Afall%20detection%20system%20using%20FLAMe%20%28Federated%20Learning%20with%20Attention%0AMechanism%29%2C%20a%20federated%20learning%20%28FL%29%20based%20algorithm.%20FLAMe%20trains%20around%0Aimportant%20keypoint%20information%20and%20only%20transmits%20the%20trained%20important%20weights%0Ato%20the%20server%2C%20reducing%20communication%20costs%20and%20preserving%20data%20privacy.%0AFurthermore%2C%20the%20lightweight%20keypoint%20transformer%20model%20is%20integrated%20into%20the%0AFL%20framework%20to%20effectively%20learn%20spatio-temporal%20features.%20We%20validated%20the%0Aexperiment%20using%2022%2C672%20video%20samples%20from%20the%20%22Fall%20Accident%20Risk%20Behavior%0AVideo-Sensor%20Pair%20data%22%20dataset%20from%20AI-Hub.%20As%20a%20result%20of%20the%20experiment%2C%20the%0AFLAMe-based%20system%20achieved%20an%20accuracy%20of%2094.02%25%20with%20about%20190%2C000%0Atransmission%20parameters%2C%20maintaining%20performance%20similar%20to%20that%20of%20existing%0Acentralized%20learning%20while%20maximizing%20efficiency%20by%20reducing%20communication%0Acosts%20by%20about%2040%25%20compared%20to%20the%20existing%20FL%20algorithm%2C%20FedAvg.%20Therefore%2C%0Athe%20FLAMe%20algorithm%20has%20demonstrated%20that%20it%20provides%20robust%20performance%20in%20the%0Adistributed%20environment%20of%20smart%20cities%20and%20is%20a%20practical%20and%20effective%0Asolution%20for%20public%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAMe%253A%2520Federated%2520Learning%2520with%2520Attention%2520Mechanism%2520using%2520Spatio-Temporal%250A%2520%2520Keypoint%2520Transformers%2520for%2520Pedestrian%2520Fall%2520Detection%2520in%2520Smart%2520Cities%26entry.906535625%3DByeonghun%2520Kim%2520and%2520Byeongjoon%2520Noh%26entry.1292438233%3D%2520%2520In%2520smart%2520cities%252C%2520detecting%2520pedestrian%2520falls%2520is%2520a%2520major%2520challenge%2520to%2520ensure%250Athe%2520safety%2520and%2520quality%2520of%2520life%2520of%2520citizens.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Afall%2520detection%2520system%2520using%2520FLAMe%2520%2528Federated%2520Learning%2520with%2520Attention%250AMechanism%2529%252C%2520a%2520federated%2520learning%2520%2528FL%2529%2520based%2520algorithm.%2520FLAMe%2520trains%2520around%250Aimportant%2520keypoint%2520information%2520and%2520only%2520transmits%2520the%2520trained%2520important%2520weights%250Ato%2520the%2520server%252C%2520reducing%2520communication%2520costs%2520and%2520preserving%2520data%2520privacy.%250AFurthermore%252C%2520the%2520lightweight%2520keypoint%2520transformer%2520model%2520is%2520integrated%2520into%2520the%250AFL%2520framework%2520to%2520effectively%2520learn%2520spatio-temporal%2520features.%2520We%2520validated%2520the%250Aexperiment%2520using%252022%252C672%2520video%2520samples%2520from%2520the%2520%2522Fall%2520Accident%2520Risk%2520Behavior%250AVideo-Sensor%2520Pair%2520data%2522%2520dataset%2520from%2520AI-Hub.%2520As%2520a%2520result%2520of%2520the%2520experiment%252C%2520the%250AFLAMe-based%2520system%2520achieved%2520an%2520accuracy%2520of%252094.02%2525%2520with%2520about%2520190%252C000%250Atransmission%2520parameters%252C%2520maintaining%2520performance%2520similar%2520to%2520that%2520of%2520existing%250Acentralized%2520learning%2520while%2520maximizing%2520efficiency%2520by%2520reducing%2520communication%250Acosts%2520by%2520about%252040%2525%2520compared%2520to%2520the%2520existing%2520FL%2520algorithm%252C%2520FedAvg.%2520Therefore%252C%250Athe%2520FLAMe%2520algorithm%2520has%2520demonstrated%2520that%2520it%2520provides%2520robust%2520performance%2520in%2520the%250Adistributed%2520environment%2520of%2520smart%2520cities%2520and%2520is%2520a%2520practical%2520and%2520effective%250Asolution%2520for%2520public%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAMe%3A%20Federated%20Learning%20with%20Attention%20Mechanism%20using%20Spatio-Temporal%0A%20%20Keypoint%20Transformers%20for%20Pedestrian%20Fall%20Detection%20in%20Smart%20Cities&entry.906535625=Byeonghun%20Kim%20and%20Byeongjoon%20Noh&entry.1292438233=%20%20In%20smart%20cities%2C%20detecting%20pedestrian%20falls%20is%20a%20major%20challenge%20to%20ensure%0Athe%20safety%20and%20quality%20of%20life%20of%20citizens.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Afall%20detection%20system%20using%20FLAMe%20%28Federated%20Learning%20with%20Attention%0AMechanism%29%2C%20a%20federated%20learning%20%28FL%29%20based%20algorithm.%20FLAMe%20trains%20around%0Aimportant%20keypoint%20information%20and%20only%20transmits%20the%20trained%20important%20weights%0Ato%20the%20server%2C%20reducing%20communication%20costs%20and%20preserving%20data%20privacy.%0AFurthermore%2C%20the%20lightweight%20keypoint%20transformer%20model%20is%20integrated%20into%20the%0AFL%20framework%20to%20effectively%20learn%20spatio-temporal%20features.%20We%20validated%20the%0Aexperiment%20using%2022%2C672%20video%20samples%20from%20the%20%22Fall%20Accident%20Risk%20Behavior%0AVideo-Sensor%20Pair%20data%22%20dataset%20from%20AI-Hub.%20As%20a%20result%20of%20the%20experiment%2C%20the%0AFLAMe-based%20system%20achieved%20an%20accuracy%20of%2094.02%25%20with%20about%20190%2C000%0Atransmission%20parameters%2C%20maintaining%20performance%20similar%20to%20that%20of%20existing%0Acentralized%20learning%20while%20maximizing%20efficiency%20by%20reducing%20communication%0Acosts%20by%20about%2040%25%20compared%20to%20the%20existing%20FL%20algorithm%2C%20FedAvg.%20Therefore%2C%0Athe%20FLAMe%20algorithm%20has%20demonstrated%20that%20it%20provides%20robust%20performance%20in%20the%0Adistributed%20environment%20of%20smart%20cities%20and%20is%20a%20practical%20and%20effective%0Asolution%20for%20public%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14768v1&entry.124074799=Read"},
{"title": "Activity Recognition on Avatar-Anonymized Datasets with Masked\n  Differential Privacy", "author": "David Schneider and Sina Sajadmanesh and Vikash Sehwag and Saquib Sarfraz and Rainer Stiefelhagen and Lingjuan Lyu and Vivek Sharma", "abstract": "  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. Prevalent methods tackling this\nproblem use differential privacy (DP) or obfuscation techniques to protect the\nprivacy of individuals. In both cases, the utility of the trained model is\nsacrificed heavily in this process. In this work, we present an anonymization\npipeline that replaces sensitive human subjects in video datasets with\nsynthetic avatars within context, employing a combined rendering and stable\ndiffusion-based strategy. Additionally we propose masked differential privacy\n({MaskDP}) to protect non-anonymized but privacy sensitive background\ninformation. MaskDP allows for controlling sensitive regions where differential\nprivacy is applied, in contrast to applying DP on the entire input. This\ncombined methodology provides strong privacy protection while minimizing the\nusual performance penalty of privacy preserving methods. Experiments on\nmultiple challenging action recognition datasets demonstrate that our proposed\ntechniques result in better utility-privacy trade-offs compared to standard\ndifferentially private training in the especially demanding $\\epsilon<1$\nregime.\n", "link": "http://arxiv.org/abs/2410.17098v2", "date": "2024-12-19", "relevancy": 2.0949, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5422}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5246}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activity%20Recognition%20on%20Avatar-Anonymized%20Datasets%20with%20Masked%0A%20%20Differential%20Privacy&body=Title%3A%20Activity%20Recognition%20on%20Avatar-Anonymized%20Datasets%20with%20Masked%0A%20%20Differential%20Privacy%0AAuthor%3A%20David%20Schneider%20and%20Sina%20Sajadmanesh%20and%20Vikash%20Sehwag%20and%20Saquib%20Sarfraz%20and%20Rainer%20Stiefelhagen%20and%20Lingjuan%20Lyu%20and%20Vivek%20Sharma%0AAbstract%3A%20%20%20Privacy-preserving%20computer%20vision%20is%20an%20important%20emerging%20problem%20in%0Amachine%20learning%20and%20artificial%20intelligence.%20Prevalent%20methods%20tackling%20this%0Aproblem%20use%20differential%20privacy%20%28DP%29%20or%20obfuscation%20techniques%20to%20protect%20the%0Aprivacy%20of%20individuals.%20In%20both%20cases%2C%20the%20utility%20of%20the%20trained%20model%20is%0Asacrificed%20heavily%20in%20this%20process.%20In%20this%20work%2C%20we%20present%20an%20anonymization%0Apipeline%20that%20replaces%20sensitive%20human%20subjects%20in%20video%20datasets%20with%0Asynthetic%20avatars%20within%20context%2C%20employing%20a%20combined%20rendering%20and%20stable%0Adiffusion-based%20strategy.%20Additionally%20we%20propose%20masked%20differential%20privacy%0A%28%7BMaskDP%7D%29%20to%20protect%20non-anonymized%20but%20privacy%20sensitive%20background%0Ainformation.%20MaskDP%20allows%20for%20controlling%20sensitive%20regions%20where%20differential%0Aprivacy%20is%20applied%2C%20in%20contrast%20to%20applying%20DP%20on%20the%20entire%20input.%20This%0Acombined%20methodology%20provides%20strong%20privacy%20protection%20while%20minimizing%20the%0Ausual%20performance%20penalty%20of%20privacy%20preserving%20methods.%20Experiments%20on%0Amultiple%20challenging%20action%20recognition%20datasets%20demonstrate%20that%20our%20proposed%0Atechniques%20result%20in%20better%20utility-privacy%20trade-offs%20compared%20to%20standard%0Adifferentially%20private%20training%20in%20the%20especially%20demanding%20%24%5Cepsilon%3C1%24%0Aregime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivity%2520Recognition%2520on%2520Avatar-Anonymized%2520Datasets%2520with%2520Masked%250A%2520%2520Differential%2520Privacy%26entry.906535625%3DDavid%2520Schneider%2520and%2520Sina%2520Sajadmanesh%2520and%2520Vikash%2520Sehwag%2520and%2520Saquib%2520Sarfraz%2520and%2520Rainer%2520Stiefelhagen%2520and%2520Lingjuan%2520Lyu%2520and%2520Vivek%2520Sharma%26entry.1292438233%3D%2520%2520Privacy-preserving%2520computer%2520vision%2520is%2520an%2520important%2520emerging%2520problem%2520in%250Amachine%2520learning%2520and%2520artificial%2520intelligence.%2520Prevalent%2520methods%2520tackling%2520this%250Aproblem%2520use%2520differential%2520privacy%2520%2528DP%2529%2520or%2520obfuscation%2520techniques%2520to%2520protect%2520the%250Aprivacy%2520of%2520individuals.%2520In%2520both%2520cases%252C%2520the%2520utility%2520of%2520the%2520trained%2520model%2520is%250Asacrificed%2520heavily%2520in%2520this%2520process.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520anonymization%250Apipeline%2520that%2520replaces%2520sensitive%2520human%2520subjects%2520in%2520video%2520datasets%2520with%250Asynthetic%2520avatars%2520within%2520context%252C%2520employing%2520a%2520combined%2520rendering%2520and%2520stable%250Adiffusion-based%2520strategy.%2520Additionally%2520we%2520propose%2520masked%2520differential%2520privacy%250A%2528%257BMaskDP%257D%2529%2520to%2520protect%2520non-anonymized%2520but%2520privacy%2520sensitive%2520background%250Ainformation.%2520MaskDP%2520allows%2520for%2520controlling%2520sensitive%2520regions%2520where%2520differential%250Aprivacy%2520is%2520applied%252C%2520in%2520contrast%2520to%2520applying%2520DP%2520on%2520the%2520entire%2520input.%2520This%250Acombined%2520methodology%2520provides%2520strong%2520privacy%2520protection%2520while%2520minimizing%2520the%250Ausual%2520performance%2520penalty%2520of%2520privacy%2520preserving%2520methods.%2520Experiments%2520on%250Amultiple%2520challenging%2520action%2520recognition%2520datasets%2520demonstrate%2520that%2520our%2520proposed%250Atechniques%2520result%2520in%2520better%2520utility-privacy%2520trade-offs%2520compared%2520to%2520standard%250Adifferentially%2520private%2520training%2520in%2520the%2520especially%2520demanding%2520%2524%255Cepsilon%253C1%2524%250Aregime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activity%20Recognition%20on%20Avatar-Anonymized%20Datasets%20with%20Masked%0A%20%20Differential%20Privacy&entry.906535625=David%20Schneider%20and%20Sina%20Sajadmanesh%20and%20Vikash%20Sehwag%20and%20Saquib%20Sarfraz%20and%20Rainer%20Stiefelhagen%20and%20Lingjuan%20Lyu%20and%20Vivek%20Sharma&entry.1292438233=%20%20Privacy-preserving%20computer%20vision%20is%20an%20important%20emerging%20problem%20in%0Amachine%20learning%20and%20artificial%20intelligence.%20Prevalent%20methods%20tackling%20this%0Aproblem%20use%20differential%20privacy%20%28DP%29%20or%20obfuscation%20techniques%20to%20protect%20the%0Aprivacy%20of%20individuals.%20In%20both%20cases%2C%20the%20utility%20of%20the%20trained%20model%20is%0Asacrificed%20heavily%20in%20this%20process.%20In%20this%20work%2C%20we%20present%20an%20anonymization%0Apipeline%20that%20replaces%20sensitive%20human%20subjects%20in%20video%20datasets%20with%0Asynthetic%20avatars%20within%20context%2C%20employing%20a%20combined%20rendering%20and%20stable%0Adiffusion-based%20strategy.%20Additionally%20we%20propose%20masked%20differential%20privacy%0A%28%7BMaskDP%7D%29%20to%20protect%20non-anonymized%20but%20privacy%20sensitive%20background%0Ainformation.%20MaskDP%20allows%20for%20controlling%20sensitive%20regions%20where%20differential%0Aprivacy%20is%20applied%2C%20in%20contrast%20to%20applying%20DP%20on%20the%20entire%20input.%20This%0Acombined%20methodology%20provides%20strong%20privacy%20protection%20while%20minimizing%20the%0Ausual%20performance%20penalty%20of%20privacy%20preserving%20methods.%20Experiments%20on%0Amultiple%20challenging%20action%20recognition%20datasets%20demonstrate%20that%20our%20proposed%0Atechniques%20result%20in%20better%20utility-privacy%20trade-offs%20compared%20to%20standard%0Adifferentially%20private%20training%20in%20the%20especially%20demanding%20%24%5Cepsilon%3C1%24%0Aregime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17098v2&entry.124074799=Read"},
{"title": "Event-assisted 12-stop HDR Imaging of Dynamic Scene", "author": "Shi Guo and Zixuan Chen and Ziran Zhang and Yutian Chen and Gangwei Xu and Tianfan Xue", "abstract": "  High dynamic range (HDR) imaging is a crucial task in computational\nphotography, which captures details across diverse lighting conditions.\nTraditional HDR fusion methods face limitations in dynamic scenes with extreme\nexposure differences, as aligning low dynamic range (LDR) frames becomes\nchallenging due to motion and brightness variation. In this work, we propose a\nnovel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera\nsystem with an event camera and an RGB camera. The event camera provides\ntemporally dense, high dynamic range signals that improve alignment between LDR\nframes with large exposure differences, reducing ghosting artifacts caused by\nmotion. Also, a real-world finetuning strategy is proposed to increase the\ngeneralization of alignment module on real-world events. Additionally, we\nintroduce a diffusion-based fusion module that incorporates image priors from\npre-trained diffusion models to address artifacts in high-contrast regions and\nminimize errors from the alignment process. To support this work, we developed\nthe ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized\nevent signals, and validated our approach on both simulated and real-world\ndata. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, successfully extending HDR imaging to 12 stops in\ndynamic scenes.\n", "link": "http://arxiv.org/abs/2412.14705v1", "date": "2024-12-19", "relevancy": 2.0856, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5201}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-assisted%2012-stop%20HDR%20Imaging%20of%20Dynamic%20Scene&body=Title%3A%20Event-assisted%2012-stop%20HDR%20Imaging%20of%20Dynamic%20Scene%0AAuthor%3A%20Shi%20Guo%20and%20Zixuan%20Chen%20and%20Ziran%20Zhang%20and%20Yutian%20Chen%20and%20Gangwei%20Xu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20High%20dynamic%20range%20%28HDR%29%20imaging%20is%20a%20crucial%20task%20in%20computational%0Aphotography%2C%20which%20captures%20details%20across%20diverse%20lighting%20conditions.%0ATraditional%20HDR%20fusion%20methods%20face%20limitations%20in%20dynamic%20scenes%20with%20extreme%0Aexposure%20differences%2C%20as%20aligning%20low%20dynamic%20range%20%28LDR%29%20frames%20becomes%0Achallenging%20due%20to%20motion%20and%20brightness%20variation.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%2012-stop%20HDR%20imaging%20approach%20for%20dynamic%20scenes%2C%20leveraging%20a%20dual-camera%0Asystem%20with%20an%20event%20camera%20and%20an%20RGB%20camera.%20The%20event%20camera%20provides%0Atemporally%20dense%2C%20high%20dynamic%20range%20signals%20that%20improve%20alignment%20between%20LDR%0Aframes%20with%20large%20exposure%20differences%2C%20reducing%20ghosting%20artifacts%20caused%20by%0Amotion.%20Also%2C%20a%20real-world%20finetuning%20strategy%20is%20proposed%20to%20increase%20the%0Ageneralization%20of%20alignment%20module%20on%20real-world%20events.%20Additionally%2C%20we%0Aintroduce%20a%20diffusion-based%20fusion%20module%20that%20incorporates%20image%20priors%20from%0Apre-trained%20diffusion%20models%20to%20address%20artifacts%20in%20high-contrast%20regions%20and%0Aminimize%20errors%20from%20the%20alignment%20process.%20To%20support%20this%20work%2C%20we%20developed%0Athe%20ESHDR%20dataset%2C%20the%20first%20dataset%20for%2012-stop%20HDR%20imaging%20with%20synchronized%0Aevent%20signals%2C%20and%20validated%20our%20approach%20on%20both%20simulated%20and%20real-world%0Adata.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20successfully%20extending%20HDR%20imaging%20to%2012%20stops%20in%0Adynamic%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-assisted%252012-stop%2520HDR%2520Imaging%2520of%2520Dynamic%2520Scene%26entry.906535625%3DShi%2520Guo%2520and%2520Zixuan%2520Chen%2520and%2520Ziran%2520Zhang%2520and%2520Yutian%2520Chen%2520and%2520Gangwei%2520Xu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520High%2520dynamic%2520range%2520%2528HDR%2529%2520imaging%2520is%2520a%2520crucial%2520task%2520in%2520computational%250Aphotography%252C%2520which%2520captures%2520details%2520across%2520diverse%2520lighting%2520conditions.%250ATraditional%2520HDR%2520fusion%2520methods%2520face%2520limitations%2520in%2520dynamic%2520scenes%2520with%2520extreme%250Aexposure%2520differences%252C%2520as%2520aligning%2520low%2520dynamic%2520range%2520%2528LDR%2529%2520frames%2520becomes%250Achallenging%2520due%2520to%2520motion%2520and%2520brightness%2520variation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%252012-stop%2520HDR%2520imaging%2520approach%2520for%2520dynamic%2520scenes%252C%2520leveraging%2520a%2520dual-camera%250Asystem%2520with%2520an%2520event%2520camera%2520and%2520an%2520RGB%2520camera.%2520The%2520event%2520camera%2520provides%250Atemporally%2520dense%252C%2520high%2520dynamic%2520range%2520signals%2520that%2520improve%2520alignment%2520between%2520LDR%250Aframes%2520with%2520large%2520exposure%2520differences%252C%2520reducing%2520ghosting%2520artifacts%2520caused%2520by%250Amotion.%2520Also%252C%2520a%2520real-world%2520finetuning%2520strategy%2520is%2520proposed%2520to%2520increase%2520the%250Ageneralization%2520of%2520alignment%2520module%2520on%2520real-world%2520events.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520diffusion-based%2520fusion%2520module%2520that%2520incorporates%2520image%2520priors%2520from%250Apre-trained%2520diffusion%2520models%2520to%2520address%2520artifacts%2520in%2520high-contrast%2520regions%2520and%250Aminimize%2520errors%2520from%2520the%2520alignment%2520process.%2520To%2520support%2520this%2520work%252C%2520we%2520developed%250Athe%2520ESHDR%2520dataset%252C%2520the%2520first%2520dataset%2520for%252012-stop%2520HDR%2520imaging%2520with%2520synchronized%250Aevent%2520signals%252C%2520and%2520validated%2520our%2520approach%2520on%2520both%2520simulated%2520and%2520real-world%250Adata.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%252C%2520successfully%2520extending%2520HDR%2520imaging%2520to%252012%2520stops%2520in%250Adynamic%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-assisted%2012-stop%20HDR%20Imaging%20of%20Dynamic%20Scene&entry.906535625=Shi%20Guo%20and%20Zixuan%20Chen%20and%20Ziran%20Zhang%20and%20Yutian%20Chen%20and%20Gangwei%20Xu%20and%20Tianfan%20Xue&entry.1292438233=%20%20High%20dynamic%20range%20%28HDR%29%20imaging%20is%20a%20crucial%20task%20in%20computational%0Aphotography%2C%20which%20captures%20details%20across%20diverse%20lighting%20conditions.%0ATraditional%20HDR%20fusion%20methods%20face%20limitations%20in%20dynamic%20scenes%20with%20extreme%0Aexposure%20differences%2C%20as%20aligning%20low%20dynamic%20range%20%28LDR%29%20frames%20becomes%0Achallenging%20due%20to%20motion%20and%20brightness%20variation.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%2012-stop%20HDR%20imaging%20approach%20for%20dynamic%20scenes%2C%20leveraging%20a%20dual-camera%0Asystem%20with%20an%20event%20camera%20and%20an%20RGB%20camera.%20The%20event%20camera%20provides%0Atemporally%20dense%2C%20high%20dynamic%20range%20signals%20that%20improve%20alignment%20between%20LDR%0Aframes%20with%20large%20exposure%20differences%2C%20reducing%20ghosting%20artifacts%20caused%20by%0Amotion.%20Also%2C%20a%20real-world%20finetuning%20strategy%20is%20proposed%20to%20increase%20the%0Ageneralization%20of%20alignment%20module%20on%20real-world%20events.%20Additionally%2C%20we%0Aintroduce%20a%20diffusion-based%20fusion%20module%20that%20incorporates%20image%20priors%20from%0Apre-trained%20diffusion%20models%20to%20address%20artifacts%20in%20high-contrast%20regions%20and%0Aminimize%20errors%20from%20the%20alignment%20process.%20To%20support%20this%20work%2C%20we%20developed%0Athe%20ESHDR%20dataset%2C%20the%20first%20dataset%20for%2012-stop%20HDR%20imaging%20with%20synchronized%0Aevent%20signals%2C%20and%20validated%20our%20approach%20on%20both%20simulated%20and%20real-world%0Adata.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20successfully%20extending%20HDR%20imaging%20to%2012%20stops%20in%0Adynamic%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14705v1&entry.124074799=Read"},
{"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models", "author": "Yuzhe Gu and Ziwei Ji and Wenwei Zhang and Chengqi Lyu and Dahua Lin and Kai Chen", "abstract": "  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n", "link": "http://arxiv.org/abs/2407.04693v2", "date": "2024-12-19", "relevancy": 2.0656, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yuzhe%20Gu%20and%20Ziwei%20Ji%20and%20Wenwei%20Zhang%20and%20Chengqi%20Lyu%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20hallucinations%20in%20long-form%0Aquestion-answering%20tasks%20across%20various%20domains%20and%20wide%20applications.%20Current%0Ahallucination%20detection%20and%20mitigation%20datasets%20are%20limited%20in%20domains%20and%0Asizes%2C%20which%20struggle%20to%20scale%20due%20to%20prohibitive%20labor%20costs%20and%20insufficient%0Areliability%20of%20existing%20hallucination%20annotators.%20To%20facilitate%20the%20scalable%0Aoversight%20of%20LLM%20hallucinations%2C%20this%20paper%20introduces%20an%20iterative%0Aself-training%20framework%20that%20simultaneously%20and%20progressively%20scales%20up%20the%0Ahallucination%20annotation%20dataset%20and%20improves%20the%20accuracy%20of%20the%20hallucination%0Aannotator.%20Based%20on%20the%20Expectation%20Maximization%20%28EM%29%20algorithm%2C%20in%20each%0Aiteration%2C%20the%20framework%20first%20applies%20a%20hallucination%20annotation%20pipeline%20to%0Aannotate%20a%20scaled%20dataset%20and%20then%20trains%20a%20more%20accurate%20hallucination%0Aannotator%20on%20the%20dataset.%20This%20new%20hallucination%20annotator%20is%20adopted%20in%20the%0Ahallucination%20annotation%20pipeline%20used%20for%20the%20next%20iteration.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20finally%20obtained%20hallucination%0Aannotator%20with%20only%207B%20parameters%20surpasses%20the%20performance%20of%20GPT-4%20and%0Aobtains%20new%20state-of-the-art%20hallucination%20detection%20results%20on%20HaluEval%20and%0AHalluQA%20by%20zero-shot%20inference.%20Such%20an%20annotator%20can%20not%20only%20evaluate%20the%0Ahallucination%20levels%20of%20various%20LLMs%20on%20the%20large-scale%20dataset%20but%20also%20help%0Ato%20mitigate%20the%20hallucination%20of%20LLMs%20generations%2C%20with%20the%20Natural%20Language%0AInference%20%28NLI%29%20metric%20increasing%20from%2025%25%20to%2037%25%20on%20HaluEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANAH-v2%253A%2520Scaling%2520Analytical%2520Hallucination%2520Annotation%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYuzhe%2520Gu%2520and%2520Ziwei%2520Ji%2520and%2520Wenwei%2520Zhang%2520and%2520Chengqi%2520Lyu%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520hallucinations%2520in%2520long-form%250Aquestion-answering%2520tasks%2520across%2520various%2520domains%2520and%2520wide%2520applications.%2520Current%250Ahallucination%2520detection%2520and%2520mitigation%2520datasets%2520are%2520limited%2520in%2520domains%2520and%250Asizes%252C%2520which%2520struggle%2520to%2520scale%2520due%2520to%2520prohibitive%2520labor%2520costs%2520and%2520insufficient%250Areliability%2520of%2520existing%2520hallucination%2520annotators.%2520To%2520facilitate%2520the%2520scalable%250Aoversight%2520of%2520LLM%2520hallucinations%252C%2520this%2520paper%2520introduces%2520an%2520iterative%250Aself-training%2520framework%2520that%2520simultaneously%2520and%2520progressively%2520scales%2520up%2520the%250Ahallucination%2520annotation%2520dataset%2520and%2520improves%2520the%2520accuracy%2520of%2520the%2520hallucination%250Aannotator.%2520Based%2520on%2520the%2520Expectation%2520Maximization%2520%2528EM%2529%2520algorithm%252C%2520in%2520each%250Aiteration%252C%2520the%2520framework%2520first%2520applies%2520a%2520hallucination%2520annotation%2520pipeline%2520to%250Aannotate%2520a%2520scaled%2520dataset%2520and%2520then%2520trains%2520a%2520more%2520accurate%2520hallucination%250Aannotator%2520on%2520the%2520dataset.%2520This%2520new%2520hallucination%2520annotator%2520is%2520adopted%2520in%2520the%250Ahallucination%2520annotation%2520pipeline%2520used%2520for%2520the%2520next%2520iteration.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520finally%2520obtained%2520hallucination%250Aannotator%2520with%2520only%25207B%2520parameters%2520surpasses%2520the%2520performance%2520of%2520GPT-4%2520and%250Aobtains%2520new%2520state-of-the-art%2520hallucination%2520detection%2520results%2520on%2520HaluEval%2520and%250AHalluQA%2520by%2520zero-shot%2520inference.%2520Such%2520an%2520annotator%2520can%2520not%2520only%2520evaluate%2520the%250Ahallucination%2520levels%2520of%2520various%2520LLMs%2520on%2520the%2520large-scale%2520dataset%2520but%2520also%2520help%250Ato%2520mitigate%2520the%2520hallucination%2520of%2520LLMs%2520generations%252C%2520with%2520the%2520Natural%2520Language%250AInference%2520%2528NLI%2529%2520metric%2520increasing%2520from%252025%2525%2520to%252037%2525%2520on%2520HaluEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models&entry.906535625=Yuzhe%20Gu%20and%20Ziwei%20Ji%20and%20Wenwei%20Zhang%20and%20Chengqi%20Lyu%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20hallucinations%20in%20long-form%0Aquestion-answering%20tasks%20across%20various%20domains%20and%20wide%20applications.%20Current%0Ahallucination%20detection%20and%20mitigation%20datasets%20are%20limited%20in%20domains%20and%0Asizes%2C%20which%20struggle%20to%20scale%20due%20to%20prohibitive%20labor%20costs%20and%20insufficient%0Areliability%20of%20existing%20hallucination%20annotators.%20To%20facilitate%20the%20scalable%0Aoversight%20of%20LLM%20hallucinations%2C%20this%20paper%20introduces%20an%20iterative%0Aself-training%20framework%20that%20simultaneously%20and%20progressively%20scales%20up%20the%0Ahallucination%20annotation%20dataset%20and%20improves%20the%20accuracy%20of%20the%20hallucination%0Aannotator.%20Based%20on%20the%20Expectation%20Maximization%20%28EM%29%20algorithm%2C%20in%20each%0Aiteration%2C%20the%20framework%20first%20applies%20a%20hallucination%20annotation%20pipeline%20to%0Aannotate%20a%20scaled%20dataset%20and%20then%20trains%20a%20more%20accurate%20hallucination%0Aannotator%20on%20the%20dataset.%20This%20new%20hallucination%20annotator%20is%20adopted%20in%20the%0Ahallucination%20annotation%20pipeline%20used%20for%20the%20next%20iteration.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20finally%20obtained%20hallucination%0Aannotator%20with%20only%207B%20parameters%20surpasses%20the%20performance%20of%20GPT-4%20and%0Aobtains%20new%20state-of-the-art%20hallucination%20detection%20results%20on%20HaluEval%20and%0AHalluQA%20by%20zero-shot%20inference.%20Such%20an%20annotator%20can%20not%20only%20evaluate%20the%0Ahallucination%20levels%20of%20various%20LLMs%20on%20the%20large-scale%20dataset%20but%20also%20help%0Ato%20mitigate%20the%20hallucination%20of%20LLMs%20generations%2C%20with%20the%20Natural%20Language%0AInference%20%28NLI%29%20metric%20increasing%20from%2025%25%20to%2037%25%20on%20HaluEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04693v2&entry.124074799=Read"},
{"title": "One Pixel is All I Need", "author": "Deng Siqin and Zhou Xiaoyi", "abstract": "  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n", "link": "http://arxiv.org/abs/2412.10681v2", "date": "2024-12-19", "relevancy": 2.0629, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5314}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Pixel%20is%20All%20I%20Need&body=Title%3A%20One%20Pixel%20is%20All%20I%20Need%0AAuthor%3A%20Deng%20Siqin%20and%20Zhou%20Xiaoyi%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20record-breaking%20performance%20in%0Avarious%20visual%20tasks.%20However%2C%20concerns%20about%20their%20robustness%20against%20backdoor%0Aattacks%20have%20grown.%20Backdoor%20attacks%20involve%20associating%20a%20specific%20trigger%0Awith%20a%20target%20label%2C%20causing%20the%20model%20to%20predict%20the%20attacker-specified%20label%0Awhen%20the%20trigger%20is%20present%2C%20while%20correctly%20identifying%20clean%20images.We%20found%0Athat%20ViTs%20exhibit%20higher%20attack%20success%20rates%20for%20quasi-triggers%28patterns%0Adifferent%20from%20but%20similar%20to%20the%20original%20training%20triggers%29compared%20to%20CNNs.%0AMoreover%2C%20some%20backdoor%20features%20in%20clean%20samples%20can%20suppress%20the%20original%0Atrigger%2C%20making%20quasi-triggers%20more%20effective.To%20better%20understand%20and%20exploit%0Athese%20vulnerabilities%2C%20we%20developed%20a%20tool%20called%20the%20Perturbation%20Sensitivity%0ADistribution%20Map%20%28PSDM%29.%20PSDM%20computes%20and%20sums%20gradients%20over%20many%20inputs%20to%0Ashow%20how%20sensitive%20the%20model%20is%20to%20small%20changes%20in%20the%20input.%20In%20ViTs%2C%20PSDM%0Areveals%20a%20patch-like%20pattern%20where%20central%20pixels%20are%20more%20sensitive%20than%0Aedges.%20We%20use%20PSDM%20to%20guide%20the%20creation%20of%20quasi-triggers.Based%20on%20these%0Afindings%2C%20we%20designed%20%22WorstVIT%2C%22%20a%20simple%20yet%20effective%20data%20poisoning%0Abackdoor%20for%20ViT%20models.%20This%20attack%20requires%20an%20extremely%20low%20poisoning%20rate%2C%0Atrains%20for%20just%20one%20epoch%2C%20and%20modifies%20a%20single%20pixel%20to%20successfully%20attack%0Aall%20validation%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Pixel%2520is%2520All%2520I%2520Need%26entry.906535625%3DDeng%2520Siqin%2520and%2520Zhou%2520Xiaoyi%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520record-breaking%2520performance%2520in%250Avarious%2520visual%2520tasks.%2520However%252C%2520concerns%2520about%2520their%2520robustness%2520against%2520backdoor%250Aattacks%2520have%2520grown.%2520Backdoor%2520attacks%2520involve%2520associating%2520a%2520specific%2520trigger%250Awith%2520a%2520target%2520label%252C%2520causing%2520the%2520model%2520to%2520predict%2520the%2520attacker-specified%2520label%250Awhen%2520the%2520trigger%2520is%2520present%252C%2520while%2520correctly%2520identifying%2520clean%2520images.We%2520found%250Athat%2520ViTs%2520exhibit%2520higher%2520attack%2520success%2520rates%2520for%2520quasi-triggers%2528patterns%250Adifferent%2520from%2520but%2520similar%2520to%2520the%2520original%2520training%2520triggers%2529compared%2520to%2520CNNs.%250AMoreover%252C%2520some%2520backdoor%2520features%2520in%2520clean%2520samples%2520can%2520suppress%2520the%2520original%250Atrigger%252C%2520making%2520quasi-triggers%2520more%2520effective.To%2520better%2520understand%2520and%2520exploit%250Athese%2520vulnerabilities%252C%2520we%2520developed%2520a%2520tool%2520called%2520the%2520Perturbation%2520Sensitivity%250ADistribution%2520Map%2520%2528PSDM%2529.%2520PSDM%2520computes%2520and%2520sums%2520gradients%2520over%2520many%2520inputs%2520to%250Ashow%2520how%2520sensitive%2520the%2520model%2520is%2520to%2520small%2520changes%2520in%2520the%2520input.%2520In%2520ViTs%252C%2520PSDM%250Areveals%2520a%2520patch-like%2520pattern%2520where%2520central%2520pixels%2520are%2520more%2520sensitive%2520than%250Aedges.%2520We%2520use%2520PSDM%2520to%2520guide%2520the%2520creation%2520of%2520quasi-triggers.Based%2520on%2520these%250Afindings%252C%2520we%2520designed%2520%2522WorstVIT%252C%2522%2520a%2520simple%2520yet%2520effective%2520data%2520poisoning%250Abackdoor%2520for%2520ViT%2520models.%2520This%2520attack%2520requires%2520an%2520extremely%2520low%2520poisoning%2520rate%252C%250Atrains%2520for%2520just%2520one%2520epoch%252C%2520and%2520modifies%2520a%2520single%2520pixel%2520to%2520successfully%2520attack%250Aall%2520validation%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Pixel%20is%20All%20I%20Need&entry.906535625=Deng%20Siqin%20and%20Zhou%20Xiaoyi&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20record-breaking%20performance%20in%0Avarious%20visual%20tasks.%20However%2C%20concerns%20about%20their%20robustness%20against%20backdoor%0Aattacks%20have%20grown.%20Backdoor%20attacks%20involve%20associating%20a%20specific%20trigger%0Awith%20a%20target%20label%2C%20causing%20the%20model%20to%20predict%20the%20attacker-specified%20label%0Awhen%20the%20trigger%20is%20present%2C%20while%20correctly%20identifying%20clean%20images.We%20found%0Athat%20ViTs%20exhibit%20higher%20attack%20success%20rates%20for%20quasi-triggers%28patterns%0Adifferent%20from%20but%20similar%20to%20the%20original%20training%20triggers%29compared%20to%20CNNs.%0AMoreover%2C%20some%20backdoor%20features%20in%20clean%20samples%20can%20suppress%20the%20original%0Atrigger%2C%20making%20quasi-triggers%20more%20effective.To%20better%20understand%20and%20exploit%0Athese%20vulnerabilities%2C%20we%20developed%20a%20tool%20called%20the%20Perturbation%20Sensitivity%0ADistribution%20Map%20%28PSDM%29.%20PSDM%20computes%20and%20sums%20gradients%20over%20many%20inputs%20to%0Ashow%20how%20sensitive%20the%20model%20is%20to%20small%20changes%20in%20the%20input.%20In%20ViTs%2C%20PSDM%0Areveals%20a%20patch-like%20pattern%20where%20central%20pixels%20are%20more%20sensitive%20than%0Aedges.%20We%20use%20PSDM%20to%20guide%20the%20creation%20of%20quasi-triggers.Based%20on%20these%0Afindings%2C%20we%20designed%20%22WorstVIT%2C%22%20a%20simple%20yet%20effective%20data%20poisoning%0Abackdoor%20for%20ViT%20models.%20This%20attack%20requires%20an%20extremely%20low%20poisoning%20rate%2C%0Atrains%20for%20just%20one%20epoch%2C%20and%20modifies%20a%20single%20pixel%20to%20successfully%20attack%0Aall%20validation%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10681v2&entry.124074799=Read"},
{"title": "Task Adaptation of Reinforcement Learning-based NAS Agents through\n  Transfer Learning", "author": "Amber Cassimon and Siegfried Mercelis and Kevin Mets", "abstract": "  Recently, a novel paradigm has been proposed for reinforcement learning-based\nNAS agents, that revolves around the incremental improvement of a given\narchitecture. We assess the abilities of such reinforcement learning agents to\ntransfer between different tasks. We perform our evaluation using the\nTrans-NASBench-101 benchmark, and consider the efficacy of the transferred\nagents, as well as how quickly they can be trained. We find that pretraining an\nagent on one task benefits the performance of the agent in another task in all\nbut 1 task when considering final performance. We also show that the training\nprocedure for an agent can be shortened significantly by pretraining it on\nanother task. Our results indicate that these effects occur regardless of the\nsource or target task, although they are more pronounced for some tasks than\nfor others. Our results show that transfer learning can be an effective tool in\nmitigating the computational cost of the initial training procedure for\nreinforcement learning-based NAS agents.\n", "link": "http://arxiv.org/abs/2412.01420v2", "date": "2024-12-19", "relevancy": 1.9742, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.536}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Adaptation%20of%20Reinforcement%20Learning-based%20NAS%20Agents%20through%0A%20%20Transfer%20Learning&body=Title%3A%20Task%20Adaptation%20of%20Reinforcement%20Learning-based%20NAS%20Agents%20through%0A%20%20Transfer%20Learning%0AAuthor%3A%20Amber%20Cassimon%20and%20Siegfried%20Mercelis%20and%20Kevin%20Mets%0AAbstract%3A%20%20%20Recently%2C%20a%20novel%20paradigm%20has%20been%20proposed%20for%20reinforcement%20learning-based%0ANAS%20agents%2C%20that%20revolves%20around%20the%20incremental%20improvement%20of%20a%20given%0Aarchitecture.%20We%20assess%20the%20abilities%20of%20such%20reinforcement%20learning%20agents%20to%0Atransfer%20between%20different%20tasks.%20We%20perform%20our%20evaluation%20using%20the%0ATrans-NASBench-101%20benchmark%2C%20and%20consider%20the%20efficacy%20of%20the%20transferred%0Aagents%2C%20as%20well%20as%20how%20quickly%20they%20can%20be%20trained.%20We%20find%20that%20pretraining%20an%0Aagent%20on%20one%20task%20benefits%20the%20performance%20of%20the%20agent%20in%20another%20task%20in%20all%0Abut%201%20task%20when%20considering%20final%20performance.%20We%20also%20show%20that%20the%20training%0Aprocedure%20for%20an%20agent%20can%20be%20shortened%20significantly%20by%20pretraining%20it%20on%0Aanother%20task.%20Our%20results%20indicate%20that%20these%20effects%20occur%20regardless%20of%20the%0Asource%20or%20target%20task%2C%20although%20they%20are%20more%20pronounced%20for%20some%20tasks%20than%0Afor%20others.%20Our%20results%20show%20that%20transfer%20learning%20can%20be%20an%20effective%20tool%20in%0Amitigating%20the%20computational%20cost%20of%20the%20initial%20training%20procedure%20for%0Areinforcement%20learning-based%20NAS%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01420v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Adaptation%2520of%2520Reinforcement%2520Learning-based%2520NAS%2520Agents%2520through%250A%2520%2520Transfer%2520Learning%26entry.906535625%3DAmber%2520Cassimon%2520and%2520Siegfried%2520Mercelis%2520and%2520Kevin%2520Mets%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520novel%2520paradigm%2520has%2520been%2520proposed%2520for%2520reinforcement%2520learning-based%250ANAS%2520agents%252C%2520that%2520revolves%2520around%2520the%2520incremental%2520improvement%2520of%2520a%2520given%250Aarchitecture.%2520We%2520assess%2520the%2520abilities%2520of%2520such%2520reinforcement%2520learning%2520agents%2520to%250Atransfer%2520between%2520different%2520tasks.%2520We%2520perform%2520our%2520evaluation%2520using%2520the%250ATrans-NASBench-101%2520benchmark%252C%2520and%2520consider%2520the%2520efficacy%2520of%2520the%2520transferred%250Aagents%252C%2520as%2520well%2520as%2520how%2520quickly%2520they%2520can%2520be%2520trained.%2520We%2520find%2520that%2520pretraining%2520an%250Aagent%2520on%2520one%2520task%2520benefits%2520the%2520performance%2520of%2520the%2520agent%2520in%2520another%2520task%2520in%2520all%250Abut%25201%2520task%2520when%2520considering%2520final%2520performance.%2520We%2520also%2520show%2520that%2520the%2520training%250Aprocedure%2520for%2520an%2520agent%2520can%2520be%2520shortened%2520significantly%2520by%2520pretraining%2520it%2520on%250Aanother%2520task.%2520Our%2520results%2520indicate%2520that%2520these%2520effects%2520occur%2520regardless%2520of%2520the%250Asource%2520or%2520target%2520task%252C%2520although%2520they%2520are%2520more%2520pronounced%2520for%2520some%2520tasks%2520than%250Afor%2520others.%2520Our%2520results%2520show%2520that%2520transfer%2520learning%2520can%2520be%2520an%2520effective%2520tool%2520in%250Amitigating%2520the%2520computational%2520cost%2520of%2520the%2520initial%2520training%2520procedure%2520for%250Areinforcement%2520learning-based%2520NAS%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01420v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Adaptation%20of%20Reinforcement%20Learning-based%20NAS%20Agents%20through%0A%20%20Transfer%20Learning&entry.906535625=Amber%20Cassimon%20and%20Siegfried%20Mercelis%20and%20Kevin%20Mets&entry.1292438233=%20%20Recently%2C%20a%20novel%20paradigm%20has%20been%20proposed%20for%20reinforcement%20learning-based%0ANAS%20agents%2C%20that%20revolves%20around%20the%20incremental%20improvement%20of%20a%20given%0Aarchitecture.%20We%20assess%20the%20abilities%20of%20such%20reinforcement%20learning%20agents%20to%0Atransfer%20between%20different%20tasks.%20We%20perform%20our%20evaluation%20using%20the%0ATrans-NASBench-101%20benchmark%2C%20and%20consider%20the%20efficacy%20of%20the%20transferred%0Aagents%2C%20as%20well%20as%20how%20quickly%20they%20can%20be%20trained.%20We%20find%20that%20pretraining%20an%0Aagent%20on%20one%20task%20benefits%20the%20performance%20of%20the%20agent%20in%20another%20task%20in%20all%0Abut%201%20task%20when%20considering%20final%20performance.%20We%20also%20show%20that%20the%20training%0Aprocedure%20for%20an%20agent%20can%20be%20shortened%20significantly%20by%20pretraining%20it%20on%0Aanother%20task.%20Our%20results%20indicate%20that%20these%20effects%20occur%20regardless%20of%20the%0Asource%20or%20target%20task%2C%20although%20they%20are%20more%20pronounced%20for%20some%20tasks%20than%0Afor%20others.%20Our%20results%20show%20that%20transfer%20learning%20can%20be%20an%20effective%20tool%20in%0Amitigating%20the%20computational%20cost%20of%20the%20initial%20training%20procedure%20for%0Areinforcement%20learning-based%20NAS%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01420v2&entry.124074799=Read"},
{"title": "Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble\n  and Clustering Tensions", "author": "Rahul Shah and Purba Mukherjee and Soumadeep Saha and Utpal Garain and Supratik Pal", "abstract": "  Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on\nestimation of the sound horizon at drag epoch $r_d$ from early universe\nobservations by assuming a cosmological model. We present a recalibration of\ntwo independent BAO datasets, SDSS and DESI, by employing deep learning\ntechniques for model-independent estimation of $r_d$, and explore the impacts\non $\\Lambda$CDM cosmological parameters. Significant reductions in both Hubble\n($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated\ndatasets. Moderate shifts in some other parameters hint towards further\nexploration of such data-driven approaches.\n", "link": "http://arxiv.org/abs/2412.14750v1", "date": "2024-12-19", "relevancy": 1.2832, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4419}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4243}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Based%20Recalibration%20of%20SDSS%20and%20DESI%20BAO%20Alleviates%20Hubble%0A%20%20and%20Clustering%20Tensions&body=Title%3A%20Deep%20Learning%20Based%20Recalibration%20of%20SDSS%20and%20DESI%20BAO%20Alleviates%20Hubble%0A%20%20and%20Clustering%20Tensions%0AAuthor%3A%20Rahul%20Shah%20and%20Purba%20Mukherjee%20and%20Soumadeep%20Saha%20and%20Utpal%20Garain%20and%20Supratik%20Pal%0AAbstract%3A%20%20%20Conventional%20calibration%20of%20Baryon%20Acoustic%20Oscillations%20%28BAO%29%20data%20relies%20on%0Aestimation%20of%20the%20sound%20horizon%20at%20drag%20epoch%20%24r_d%24%20from%20early%20universe%0Aobservations%20by%20assuming%20a%20cosmological%20model.%20We%20present%20a%20recalibration%20of%0Atwo%20independent%20BAO%20datasets%2C%20SDSS%20and%20DESI%2C%20by%20employing%20deep%20learning%0Atechniques%20for%20model-independent%20estimation%20of%20%24r_d%24%2C%20and%20explore%20the%20impacts%0Aon%20%24%5CLambda%24CDM%20cosmological%20parameters.%20Significant%20reductions%20in%20both%20Hubble%0A%28%24H_0%24%29%20and%20clustering%20%28%24S_8%24%29%20tensions%20are%20observed%20for%20both%20the%20recalibrated%0Adatasets.%20Moderate%20shifts%20in%20some%20other%20parameters%20hint%20towards%20further%0Aexploration%20of%20such%20data-driven%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Based%2520Recalibration%2520of%2520SDSS%2520and%2520DESI%2520BAO%2520Alleviates%2520Hubble%250A%2520%2520and%2520Clustering%2520Tensions%26entry.906535625%3DRahul%2520Shah%2520and%2520Purba%2520Mukherjee%2520and%2520Soumadeep%2520Saha%2520and%2520Utpal%2520Garain%2520and%2520Supratik%2520Pal%26entry.1292438233%3D%2520%2520Conventional%2520calibration%2520of%2520Baryon%2520Acoustic%2520Oscillations%2520%2528BAO%2529%2520data%2520relies%2520on%250Aestimation%2520of%2520the%2520sound%2520horizon%2520at%2520drag%2520epoch%2520%2524r_d%2524%2520from%2520early%2520universe%250Aobservations%2520by%2520assuming%2520a%2520cosmological%2520model.%2520We%2520present%2520a%2520recalibration%2520of%250Atwo%2520independent%2520BAO%2520datasets%252C%2520SDSS%2520and%2520DESI%252C%2520by%2520employing%2520deep%2520learning%250Atechniques%2520for%2520model-independent%2520estimation%2520of%2520%2524r_d%2524%252C%2520and%2520explore%2520the%2520impacts%250Aon%2520%2524%255CLambda%2524CDM%2520cosmological%2520parameters.%2520Significant%2520reductions%2520in%2520both%2520Hubble%250A%2528%2524H_0%2524%2529%2520and%2520clustering%2520%2528%2524S_8%2524%2529%2520tensions%2520are%2520observed%2520for%2520both%2520the%2520recalibrated%250Adatasets.%2520Moderate%2520shifts%2520in%2520some%2520other%2520parameters%2520hint%2520towards%2520further%250Aexploration%2520of%2520such%2520data-driven%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Recalibration%20of%20SDSS%20and%20DESI%20BAO%20Alleviates%20Hubble%0A%20%20and%20Clustering%20Tensions&entry.906535625=Rahul%20Shah%20and%20Purba%20Mukherjee%20and%20Soumadeep%20Saha%20and%20Utpal%20Garain%20and%20Supratik%20Pal&entry.1292438233=%20%20Conventional%20calibration%20of%20Baryon%20Acoustic%20Oscillations%20%28BAO%29%20data%20relies%20on%0Aestimation%20of%20the%20sound%20horizon%20at%20drag%20epoch%20%24r_d%24%20from%20early%20universe%0Aobservations%20by%20assuming%20a%20cosmological%20model.%20We%20present%20a%20recalibration%20of%0Atwo%20independent%20BAO%20datasets%2C%20SDSS%20and%20DESI%2C%20by%20employing%20deep%20learning%0Atechniques%20for%20model-independent%20estimation%20of%20%24r_d%24%2C%20and%20explore%20the%20impacts%0Aon%20%24%5CLambda%24CDM%20cosmological%20parameters.%20Significant%20reductions%20in%20both%20Hubble%0A%28%24H_0%24%29%20and%20clustering%20%28%24S_8%24%29%20tensions%20are%20observed%20for%20both%20the%20recalibrated%0Adatasets.%20Moderate%20shifts%20in%20some%20other%20parameters%20hint%20towards%20further%0Aexploration%20of%20such%20data-driven%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14750v1&entry.124074799=Read"},
{"title": "A parametric algorithm is optimal for non-parametric regression of\n  smooth functions", "author": "Davide Maran and Marcello Restelli", "abstract": "  We address the regression problem for a general function $f:[-1,1]^d\\to\n\\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to\nachieve a uniform error bound across the entire domain. In this setting, known\nhistorically as nonparametric regression, we aim to establish a sample\ncomplexity bound that depends solely on the function's degree of smoothness.\nAssuming periodicity at the domain boundaries, we introduce PADUA, an algorithm\nthat, with high probability, provides performance guarantees optimal up to\nconstant or logarithmic factors across all problem parameters. Notably, PADUA\nis the first parametric algorithm with optimal sample complexity for this\nsetting. Due to this feature, we prove that, differently from the\nnon-parametric state of the art, PADUA enjoys optimal space complexity in the\nprediction phase. To validate these results, we perform numerical experiments\nover functions coming from real audio data, where PADUA shows comparable\nperformance to state-of-the-art methods, while requiring only a fraction of the\ncomputational time.\n", "link": "http://arxiv.org/abs/2412.14744v1", "date": "2024-12-19", "relevancy": 1.7016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4403}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4309}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20parametric%20algorithm%20is%20optimal%20for%20non-parametric%20regression%20of%0A%20%20smooth%20functions&body=Title%3A%20A%20parametric%20algorithm%20is%20optimal%20for%20non-parametric%20regression%20of%0A%20%20smooth%20functions%0AAuthor%3A%20Davide%20Maran%20and%20Marcello%20Restelli%0AAbstract%3A%20%20%20We%20address%20the%20regression%20problem%20for%20a%20general%20function%20%24f%3A%5B-1%2C1%5D%5Ed%5Cto%0A%5Cmathbb%20R%24%20when%20the%20learner%20selects%20the%20training%20points%20%24%5C%7Bx_i%5C%7D_%7Bi%3D1%7D%5En%24%20to%0Aachieve%20a%20uniform%20error%20bound%20across%20the%20entire%20domain.%20In%20this%20setting%2C%20known%0Ahistorically%20as%20nonparametric%20regression%2C%20we%20aim%20to%20establish%20a%20sample%0Acomplexity%20bound%20that%20depends%20solely%20on%20the%20function%27s%20degree%20of%20smoothness.%0AAssuming%20periodicity%20at%20the%20domain%20boundaries%2C%20we%20introduce%20PADUA%2C%20an%20algorithm%0Athat%2C%20with%20high%20probability%2C%20provides%20performance%20guarantees%20optimal%20up%20to%0Aconstant%20or%20logarithmic%20factors%20across%20all%20problem%20parameters.%20Notably%2C%20PADUA%0Ais%20the%20first%20parametric%20algorithm%20with%20optimal%20sample%20complexity%20for%20this%0Asetting.%20Due%20to%20this%20feature%2C%20we%20prove%20that%2C%20differently%20from%20the%0Anon-parametric%20state%20of%20the%20art%2C%20PADUA%20enjoys%20optimal%20space%20complexity%20in%20the%0Aprediction%20phase.%20To%20validate%20these%20results%2C%20we%20perform%20numerical%20experiments%0Aover%20functions%20coming%20from%20real%20audio%20data%2C%20where%20PADUA%20shows%20comparable%0Aperformance%20to%20state-of-the-art%20methods%2C%20while%20requiring%20only%20a%20fraction%20of%20the%0Acomputational%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520parametric%2520algorithm%2520is%2520optimal%2520for%2520non-parametric%2520regression%2520of%250A%2520%2520smooth%2520functions%26entry.906535625%3DDavide%2520Maran%2520and%2520Marcello%2520Restelli%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520regression%2520problem%2520for%2520a%2520general%2520function%2520%2524f%253A%255B-1%252C1%255D%255Ed%255Cto%250A%255Cmathbb%2520R%2524%2520when%2520the%2520learner%2520selects%2520the%2520training%2520points%2520%2524%255C%257Bx_i%255C%257D_%257Bi%253D1%257D%255En%2524%2520to%250Aachieve%2520a%2520uniform%2520error%2520bound%2520across%2520the%2520entire%2520domain.%2520In%2520this%2520setting%252C%2520known%250Ahistorically%2520as%2520nonparametric%2520regression%252C%2520we%2520aim%2520to%2520establish%2520a%2520sample%250Acomplexity%2520bound%2520that%2520depends%2520solely%2520on%2520the%2520function%2527s%2520degree%2520of%2520smoothness.%250AAssuming%2520periodicity%2520at%2520the%2520domain%2520boundaries%252C%2520we%2520introduce%2520PADUA%252C%2520an%2520algorithm%250Athat%252C%2520with%2520high%2520probability%252C%2520provides%2520performance%2520guarantees%2520optimal%2520up%2520to%250Aconstant%2520or%2520logarithmic%2520factors%2520across%2520all%2520problem%2520parameters.%2520Notably%252C%2520PADUA%250Ais%2520the%2520first%2520parametric%2520algorithm%2520with%2520optimal%2520sample%2520complexity%2520for%2520this%250Asetting.%2520Due%2520to%2520this%2520feature%252C%2520we%2520prove%2520that%252C%2520differently%2520from%2520the%250Anon-parametric%2520state%2520of%2520the%2520art%252C%2520PADUA%2520enjoys%2520optimal%2520space%2520complexity%2520in%2520the%250Aprediction%2520phase.%2520To%2520validate%2520these%2520results%252C%2520we%2520perform%2520numerical%2520experiments%250Aover%2520functions%2520coming%2520from%2520real%2520audio%2520data%252C%2520where%2520PADUA%2520shows%2520comparable%250Aperformance%2520to%2520state-of-the-art%2520methods%252C%2520while%2520requiring%2520only%2520a%2520fraction%2520of%2520the%250Acomputational%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20parametric%20algorithm%20is%20optimal%20for%20non-parametric%20regression%20of%0A%20%20smooth%20functions&entry.906535625=Davide%20Maran%20and%20Marcello%20Restelli&entry.1292438233=%20%20We%20address%20the%20regression%20problem%20for%20a%20general%20function%20%24f%3A%5B-1%2C1%5D%5Ed%5Cto%0A%5Cmathbb%20R%24%20when%20the%20learner%20selects%20the%20training%20points%20%24%5C%7Bx_i%5C%7D_%7Bi%3D1%7D%5En%24%20to%0Aachieve%20a%20uniform%20error%20bound%20across%20the%20entire%20domain.%20In%20this%20setting%2C%20known%0Ahistorically%20as%20nonparametric%20regression%2C%20we%20aim%20to%20establish%20a%20sample%0Acomplexity%20bound%20that%20depends%20solely%20on%20the%20function%27s%20degree%20of%20smoothness.%0AAssuming%20periodicity%20at%20the%20domain%20boundaries%2C%20we%20introduce%20PADUA%2C%20an%20algorithm%0Athat%2C%20with%20high%20probability%2C%20provides%20performance%20guarantees%20optimal%20up%20to%0Aconstant%20or%20logarithmic%20factors%20across%20all%20problem%20parameters.%20Notably%2C%20PADUA%0Ais%20the%20first%20parametric%20algorithm%20with%20optimal%20sample%20complexity%20for%20this%0Asetting.%20Due%20to%20this%20feature%2C%20we%20prove%20that%2C%20differently%20from%20the%0Anon-parametric%20state%20of%20the%20art%2C%20PADUA%20enjoys%20optimal%20space%20complexity%20in%20the%0Aprediction%20phase.%20To%20validate%20these%20results%2C%20we%20perform%20numerical%20experiments%0Aover%20functions%20coming%20from%20real%20audio%20data%2C%20where%20PADUA%20shows%20comparable%0Aperformance%20to%20state-of-the-art%20methods%2C%20while%20requiring%20only%20a%20fraction%20of%20the%0Acomputational%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14744v1&entry.124074799=Read"},
{"title": "DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space", "author": "Mang Ning and Mingxiao Li and Jianlin Su and Haozhe Jia and Lanmiao Liu and Martin Bene\u0161 and Albert Ali Salah and Itir Onal Ertugrul", "abstract": "  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n", "link": "http://arxiv.org/abs/2412.15032v1", "date": "2024-12-19", "relevancy": 1.8949, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6582}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6302}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCTdiff%3A%20Intriguing%20Properties%20of%20Image%20Generative%20Modeling%20in%20the%20DCT%0A%20%20Space&body=Title%3A%20DCTdiff%3A%20Intriguing%20Properties%20of%20Image%20Generative%20Modeling%20in%20the%20DCT%0A%20%20Space%0AAuthor%3A%20Mang%20Ning%20and%20Mingxiao%20Li%20and%20Jianlin%20Su%20and%20Haozhe%20Jia%20and%20Lanmiao%20Liu%20and%20Martin%20Bene%C5%A1%20and%20Albert%20Ali%20Salah%20and%20Itir%20Onal%20Ertugrul%0AAbstract%3A%20%20%20This%20paper%20explores%20image%20modeling%20from%20the%20frequency%20space%20and%20introduces%0ADCTdiff%2C%20an%20end-to-end%20diffusion%20generative%20paradigm%20that%20efficiently%20models%0Aimages%20in%20the%20discrete%20cosine%20transform%20%28DCT%29%20space.%20We%20investigate%20the%20design%0Aspace%20of%20DCTdiff%20and%20reveal%20the%20key%20design%20factors.%20Experiments%20on%20different%0Aframeworks%20%28UViT%2C%20DiT%29%2C%20generation%20tasks%2C%20and%20various%20diffusion%20samplers%0Ademonstrate%20that%20DCTdiff%20outperforms%20pixel-based%20diffusion%20models%20regarding%0Agenerative%20quality%20and%20training%20efficiency.%20Remarkably%2C%20DCTdiff%20can%20seamlessly%0Ascale%20up%20to%20high-resolution%20generation%20without%20using%20the%20latent%20diffusion%0Aparadigm.%20Finally%2C%20we%20illustrate%20several%20intriguing%20properties%20of%20DCT%20image%0Amodeling.%20For%20example%2C%20we%20provide%20a%20theoretical%20proof%20of%20why%20%60image%20diffusion%0Acan%20be%20seen%20as%20spectral%20autoregression%27%2C%20bridging%20the%20gap%20between%20diffusion%20and%0Aautoregressive%20models.%20The%20effectiveness%20of%20DCTdiff%20and%20the%20introduced%0Aproperties%20suggest%20a%20promising%20direction%20for%20image%20modeling%20in%20the%20frequency%0Aspace.%20The%20code%20is%20at%20%5Curl%7Bhttps%3A//github.com/forever208/DCTdiff%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCTdiff%253A%2520Intriguing%2520Properties%2520of%2520Image%2520Generative%2520Modeling%2520in%2520the%2520DCT%250A%2520%2520Space%26entry.906535625%3DMang%2520Ning%2520and%2520Mingxiao%2520Li%2520and%2520Jianlin%2520Su%2520and%2520Haozhe%2520Jia%2520and%2520Lanmiao%2520Liu%2520and%2520Martin%2520Bene%25C5%25A1%2520and%2520Albert%2520Ali%2520Salah%2520and%2520Itir%2520Onal%2520Ertugrul%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520image%2520modeling%2520from%2520the%2520frequency%2520space%2520and%2520introduces%250ADCTdiff%252C%2520an%2520end-to-end%2520diffusion%2520generative%2520paradigm%2520that%2520efficiently%2520models%250Aimages%2520in%2520the%2520discrete%2520cosine%2520transform%2520%2528DCT%2529%2520space.%2520We%2520investigate%2520the%2520design%250Aspace%2520of%2520DCTdiff%2520and%2520reveal%2520the%2520key%2520design%2520factors.%2520Experiments%2520on%2520different%250Aframeworks%2520%2528UViT%252C%2520DiT%2529%252C%2520generation%2520tasks%252C%2520and%2520various%2520diffusion%2520samplers%250Ademonstrate%2520that%2520DCTdiff%2520outperforms%2520pixel-based%2520diffusion%2520models%2520regarding%250Agenerative%2520quality%2520and%2520training%2520efficiency.%2520Remarkably%252C%2520DCTdiff%2520can%2520seamlessly%250Ascale%2520up%2520to%2520high-resolution%2520generation%2520without%2520using%2520the%2520latent%2520diffusion%250Aparadigm.%2520Finally%252C%2520we%2520illustrate%2520several%2520intriguing%2520properties%2520of%2520DCT%2520image%250Amodeling.%2520For%2520example%252C%2520we%2520provide%2520a%2520theoretical%2520proof%2520of%2520why%2520%2560image%2520diffusion%250Acan%2520be%2520seen%2520as%2520spectral%2520autoregression%2527%252C%2520bridging%2520the%2520gap%2520between%2520diffusion%2520and%250Aautoregressive%2520models.%2520The%2520effectiveness%2520of%2520DCTdiff%2520and%2520the%2520introduced%250Aproperties%2520suggest%2520a%2520promising%2520direction%2520for%2520image%2520modeling%2520in%2520the%2520frequency%250Aspace.%2520The%2520code%2520is%2520at%2520%255Curl%257Bhttps%253A//github.com/forever208/DCTdiff%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCTdiff%3A%20Intriguing%20Properties%20of%20Image%20Generative%20Modeling%20in%20the%20DCT%0A%20%20Space&entry.906535625=Mang%20Ning%20and%20Mingxiao%20Li%20and%20Jianlin%20Su%20and%20Haozhe%20Jia%20and%20Lanmiao%20Liu%20and%20Martin%20Bene%C5%A1%20and%20Albert%20Ali%20Salah%20and%20Itir%20Onal%20Ertugrul&entry.1292438233=%20%20This%20paper%20explores%20image%20modeling%20from%20the%20frequency%20space%20and%20introduces%0ADCTdiff%2C%20an%20end-to-end%20diffusion%20generative%20paradigm%20that%20efficiently%20models%0Aimages%20in%20the%20discrete%20cosine%20transform%20%28DCT%29%20space.%20We%20investigate%20the%20design%0Aspace%20of%20DCTdiff%20and%20reveal%20the%20key%20design%20factors.%20Experiments%20on%20different%0Aframeworks%20%28UViT%2C%20DiT%29%2C%20generation%20tasks%2C%20and%20various%20diffusion%20samplers%0Ademonstrate%20that%20DCTdiff%20outperforms%20pixel-based%20diffusion%20models%20regarding%0Agenerative%20quality%20and%20training%20efficiency.%20Remarkably%2C%20DCTdiff%20can%20seamlessly%0Ascale%20up%20to%20high-resolution%20generation%20without%20using%20the%20latent%20diffusion%0Aparadigm.%20Finally%2C%20we%20illustrate%20several%20intriguing%20properties%20of%20DCT%20image%0Amodeling.%20For%20example%2C%20we%20provide%20a%20theoretical%20proof%20of%20why%20%60image%20diffusion%0Acan%20be%20seen%20as%20spectral%20autoregression%27%2C%20bridging%20the%20gap%20between%20diffusion%20and%0Aautoregressive%20models.%20The%20effectiveness%20of%20DCTdiff%20and%20the%20introduced%0Aproperties%20suggest%20a%20promising%20direction%20for%20image%20modeling%20in%20the%20frequency%0Aspace.%20The%20code%20is%20at%20%5Curl%7Bhttps%3A//github.com/forever208/DCTdiff%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15032v1&entry.124074799=Read"},
{"title": "Mitigating federated learning contribution allocation instability\n  through randomized aggregation", "author": "Arno Geimer and Beltran Fiz and Radu State", "abstract": "  Federated learning (FL) is a collaborative and privacy-preserving Machine\nLearning paradigm, allowing the development of robust models without the need\nto centralise sensitive data. A critical challenge in FL lies in fairly and\naccurately allocating contributions from diverse participants. Inaccurate\nallocation can undermine trust, lead to unfair compensation, and thus\nparticipants may lack the incentive to join or actively contribute to the\nfederation.\n  Various remuneration strategies have been proposed to date, including\nauction-based approaches and Shapley-value based methods, the latter offering a\nmeans to quantify the contribution of each participant. However, little to no\nwork has studied the stability of these contribution evaluation methods.\n  In this paper, we focus on calculating contributions using gradient-based\nmodel reconstruction techniques with Shapley values. We first show that\nbaseline Shapley values do not accurately reflect clients' contributions,\nleading to unstable reward allocations amongst participants in a cross-silo\nfederation. We then introduce \\textsc{FedRandom}, a new method that mitigates\nthese shortcomings with additional data samplings, and show its efficacy at\nincreasing the stability of contribution evaluation in federated learning.\n", "link": "http://arxiv.org/abs/2405.08044v2", "date": "2024-12-19", "relevancy": 1.8394, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.456}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20federated%20learning%20contribution%20allocation%20instability%0A%20%20through%20randomized%20aggregation&body=Title%3A%20Mitigating%20federated%20learning%20contribution%20allocation%20instability%0A%20%20through%20randomized%20aggregation%0AAuthor%3A%20Arno%20Geimer%20and%20Beltran%20Fiz%20and%20Radu%20State%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20collaborative%20and%20privacy-preserving%20Machine%0ALearning%20paradigm%2C%20allowing%20the%20development%20of%20robust%20models%20without%20the%20need%0Ato%20centralise%20sensitive%20data.%20A%20critical%20challenge%20in%20FL%20lies%20in%20fairly%20and%0Aaccurately%20allocating%20contributions%20from%20diverse%20participants.%20Inaccurate%0Aallocation%20can%20undermine%20trust%2C%20lead%20to%20unfair%20compensation%2C%20and%20thus%0Aparticipants%20may%20lack%20the%20incentive%20to%20join%20or%20actively%20contribute%20to%20the%0Afederation.%0A%20%20Various%20remuneration%20strategies%20have%20been%20proposed%20to%20date%2C%20including%0Aauction-based%20approaches%20and%20Shapley-value%20based%20methods%2C%20the%20latter%20offering%20a%0Ameans%20to%20quantify%20the%20contribution%20of%20each%20participant.%20However%2C%20little%20to%20no%0Awork%20has%20studied%20the%20stability%20of%20these%20contribution%20evaluation%20methods.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20calculating%20contributions%20using%20gradient-based%0Amodel%20reconstruction%20techniques%20with%20Shapley%20values.%20We%20first%20show%20that%0Abaseline%20Shapley%20values%20do%20not%20accurately%20reflect%20clients%27%20contributions%2C%0Aleading%20to%20unstable%20reward%20allocations%20amongst%20participants%20in%20a%20cross-silo%0Afederation.%20We%20then%20introduce%20%5Ctextsc%7BFedRandom%7D%2C%20a%20new%20method%20that%20mitigates%0Athese%20shortcomings%20with%20additional%20data%20samplings%2C%20and%20show%20its%20efficacy%20at%0Aincreasing%20the%20stability%20of%20contribution%20evaluation%20in%20federated%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520federated%2520learning%2520contribution%2520allocation%2520instability%250A%2520%2520through%2520randomized%2520aggregation%26entry.906535625%3DArno%2520Geimer%2520and%2520Beltran%2520Fiz%2520and%2520Radu%2520State%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520collaborative%2520and%2520privacy-preserving%2520Machine%250ALearning%2520paradigm%252C%2520allowing%2520the%2520development%2520of%2520robust%2520models%2520without%2520the%2520need%250Ato%2520centralise%2520sensitive%2520data.%2520A%2520critical%2520challenge%2520in%2520FL%2520lies%2520in%2520fairly%2520and%250Aaccurately%2520allocating%2520contributions%2520from%2520diverse%2520participants.%2520Inaccurate%250Aallocation%2520can%2520undermine%2520trust%252C%2520lead%2520to%2520unfair%2520compensation%252C%2520and%2520thus%250Aparticipants%2520may%2520lack%2520the%2520incentive%2520to%2520join%2520or%2520actively%2520contribute%2520to%2520the%250Afederation.%250A%2520%2520Various%2520remuneration%2520strategies%2520have%2520been%2520proposed%2520to%2520date%252C%2520including%250Aauction-based%2520approaches%2520and%2520Shapley-value%2520based%2520methods%252C%2520the%2520latter%2520offering%2520a%250Ameans%2520to%2520quantify%2520the%2520contribution%2520of%2520each%2520participant.%2520However%252C%2520little%2520to%2520no%250Awork%2520has%2520studied%2520the%2520stability%2520of%2520these%2520contribution%2520evaluation%2520methods.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520calculating%2520contributions%2520using%2520gradient-based%250Amodel%2520reconstruction%2520techniques%2520with%2520Shapley%2520values.%2520We%2520first%2520show%2520that%250Abaseline%2520Shapley%2520values%2520do%2520not%2520accurately%2520reflect%2520clients%2527%2520contributions%252C%250Aleading%2520to%2520unstable%2520reward%2520allocations%2520amongst%2520participants%2520in%2520a%2520cross-silo%250Afederation.%2520We%2520then%2520introduce%2520%255Ctextsc%257BFedRandom%257D%252C%2520a%2520new%2520method%2520that%2520mitigates%250Athese%2520shortcomings%2520with%2520additional%2520data%2520samplings%252C%2520and%2520show%2520its%2520efficacy%2520at%250Aincreasing%2520the%2520stability%2520of%2520contribution%2520evaluation%2520in%2520federated%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20federated%20learning%20contribution%20allocation%20instability%0A%20%20through%20randomized%20aggregation&entry.906535625=Arno%20Geimer%20and%20Beltran%20Fiz%20and%20Radu%20State&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20collaborative%20and%20privacy-preserving%20Machine%0ALearning%20paradigm%2C%20allowing%20the%20development%20of%20robust%20models%20without%20the%20need%0Ato%20centralise%20sensitive%20data.%20A%20critical%20challenge%20in%20FL%20lies%20in%20fairly%20and%0Aaccurately%20allocating%20contributions%20from%20diverse%20participants.%20Inaccurate%0Aallocation%20can%20undermine%20trust%2C%20lead%20to%20unfair%20compensation%2C%20and%20thus%0Aparticipants%20may%20lack%20the%20incentive%20to%20join%20or%20actively%20contribute%20to%20the%0Afederation.%0A%20%20Various%20remuneration%20strategies%20have%20been%20proposed%20to%20date%2C%20including%0Aauction-based%20approaches%20and%20Shapley-value%20based%20methods%2C%20the%20latter%20offering%20a%0Ameans%20to%20quantify%20the%20contribution%20of%20each%20participant.%20However%2C%20little%20to%20no%0Awork%20has%20studied%20the%20stability%20of%20these%20contribution%20evaluation%20methods.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20calculating%20contributions%20using%20gradient-based%0Amodel%20reconstruction%20techniques%20with%20Shapley%20values.%20We%20first%20show%20that%0Abaseline%20Shapley%20values%20do%20not%20accurately%20reflect%20clients%27%20contributions%2C%0Aleading%20to%20unstable%20reward%20allocations%20amongst%20participants%20in%20a%20cross-silo%0Afederation.%20We%20then%20introduce%20%5Ctextsc%7BFedRandom%7D%2C%20a%20new%20method%20that%20mitigates%0Athese%20shortcomings%20with%20additional%20data%20samplings%2C%20and%20show%20its%20efficacy%20at%0Aincreasing%20the%20stability%20of%20contribution%20evaluation%20in%20federated%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08044v2&entry.124074799=Read"},
{"title": "DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions", "author": "Aditya Kapoor and Harshad Khadilkar and Jayvardhana Gubbi", "abstract": "  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n", "link": "http://arxiv.org/abs/2407.16302v2", "date": "2024-12-19", "relevancy": 1.7504, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.609}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5557}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepClean%3A%20Integrated%20Distortion%20Identification%20and%20Algorithm%20Selection%0A%20%20for%20Rectifying%20Image%20Corruptions&body=Title%3A%20DeepClean%3A%20Integrated%20Distortion%20Identification%20and%20Algorithm%20Selection%0A%20%20for%20Rectifying%20Image%20Corruptions%0AAuthor%3A%20Aditya%20Kapoor%20and%20Harshad%20Khadilkar%20and%20Jayvardhana%20Gubbi%0AAbstract%3A%20%20%20Distortion%20identification%20and%20rectification%20in%20images%20and%20videos%20is%20vital%20for%0Aachieving%20good%20performance%20in%20downstream%20vision%20applications.%20Instead%20of%0Arelying%20on%20fixed%20trial-and-error%20based%20image%20processing%20pipelines%2C%20we%20propose%20a%0Atwo-level%20sequential%20planning%20approach%20for%20automated%20image%20distortion%0Aclassification%20and%20rectification.%20At%20the%20higher%20level%20it%20detects%20the%20class%20of%0Acorruptions%20present%20in%20the%20input%20image%2C%20if%20any.%20The%20lower%20level%20selects%20a%0Aspecific%20algorithm%20to%20be%20applied%2C%20from%20a%20set%20of%20externally%20provided%20candidate%0Aalgorithms.%20The%20entire%20two-level%20setup%20runs%20in%20the%20form%20of%20a%20single%20forward%0Apass%20during%20inference%20and%20it%20is%20to%20be%20queried%20iteratively%20until%20the%20retrieval%0Aof%20the%20original%20image.%20We%20demonstrate%20improvements%20compared%20to%20three%20baselines%0Aon%20the%20object%20detection%20task%20on%20COCO%20image%20dataset%20with%20rich%20set%20of%0Adistortions.%20The%20advantage%20of%20our%20approach%20is%20its%20dynamic%20reconfiguration%2C%0Aconditioned%20on%20the%20input%20image%20and%20generalisability%20to%20unseen%20candidate%0Aalgorithms%20at%20inference%20time%2C%20since%20it%20relies%20only%20on%20the%20comparison%20of%20their%0Aoutput%20of%20the%20image%20embeddings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepClean%253A%2520Integrated%2520Distortion%2520Identification%2520and%2520Algorithm%2520Selection%250A%2520%2520for%2520Rectifying%2520Image%2520Corruptions%26entry.906535625%3DAditya%2520Kapoor%2520and%2520Harshad%2520Khadilkar%2520and%2520Jayvardhana%2520Gubbi%26entry.1292438233%3D%2520%2520Distortion%2520identification%2520and%2520rectification%2520in%2520images%2520and%2520videos%2520is%2520vital%2520for%250Aachieving%2520good%2520performance%2520in%2520downstream%2520vision%2520applications.%2520Instead%2520of%250Arelying%2520on%2520fixed%2520trial-and-error%2520based%2520image%2520processing%2520pipelines%252C%2520we%2520propose%2520a%250Atwo-level%2520sequential%2520planning%2520approach%2520for%2520automated%2520image%2520distortion%250Aclassification%2520and%2520rectification.%2520At%2520the%2520higher%2520level%2520it%2520detects%2520the%2520class%2520of%250Acorruptions%2520present%2520in%2520the%2520input%2520image%252C%2520if%2520any.%2520The%2520lower%2520level%2520selects%2520a%250Aspecific%2520algorithm%2520to%2520be%2520applied%252C%2520from%2520a%2520set%2520of%2520externally%2520provided%2520candidate%250Aalgorithms.%2520The%2520entire%2520two-level%2520setup%2520runs%2520in%2520the%2520form%2520of%2520a%2520single%2520forward%250Apass%2520during%2520inference%2520and%2520it%2520is%2520to%2520be%2520queried%2520iteratively%2520until%2520the%2520retrieval%250Aof%2520the%2520original%2520image.%2520We%2520demonstrate%2520improvements%2520compared%2520to%2520three%2520baselines%250Aon%2520the%2520object%2520detection%2520task%2520on%2520COCO%2520image%2520dataset%2520with%2520rich%2520set%2520of%250Adistortions.%2520The%2520advantage%2520of%2520our%2520approach%2520is%2520its%2520dynamic%2520reconfiguration%252C%250Aconditioned%2520on%2520the%2520input%2520image%2520and%2520generalisability%2520to%2520unseen%2520candidate%250Aalgorithms%2520at%2520inference%2520time%252C%2520since%2520it%2520relies%2520only%2520on%2520the%2520comparison%2520of%2520their%250Aoutput%2520of%2520the%2520image%2520embeddings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepClean%3A%20Integrated%20Distortion%20Identification%20and%20Algorithm%20Selection%0A%20%20for%20Rectifying%20Image%20Corruptions&entry.906535625=Aditya%20Kapoor%20and%20Harshad%20Khadilkar%20and%20Jayvardhana%20Gubbi&entry.1292438233=%20%20Distortion%20identification%20and%20rectification%20in%20images%20and%20videos%20is%20vital%20for%0Aachieving%20good%20performance%20in%20downstream%20vision%20applications.%20Instead%20of%0Arelying%20on%20fixed%20trial-and-error%20based%20image%20processing%20pipelines%2C%20we%20propose%20a%0Atwo-level%20sequential%20planning%20approach%20for%20automated%20image%20distortion%0Aclassification%20and%20rectification.%20At%20the%20higher%20level%20it%20detects%20the%20class%20of%0Acorruptions%20present%20in%20the%20input%20image%2C%20if%20any.%20The%20lower%20level%20selects%20a%0Aspecific%20algorithm%20to%20be%20applied%2C%20from%20a%20set%20of%20externally%20provided%20candidate%0Aalgorithms.%20The%20entire%20two-level%20setup%20runs%20in%20the%20form%20of%20a%20single%20forward%0Apass%20during%20inference%20and%20it%20is%20to%20be%20queried%20iteratively%20until%20the%20retrieval%0Aof%20the%20original%20image.%20We%20demonstrate%20improvements%20compared%20to%20three%20baselines%0Aon%20the%20object%20detection%20task%20on%20COCO%20image%20dataset%20with%20rich%20set%20of%0Adistortions.%20The%20advantage%20of%20our%20approach%20is%20its%20dynamic%20reconfiguration%2C%0Aconditioned%20on%20the%20input%20image%20and%20generalisability%20to%20unseen%20candidate%0Aalgorithms%20at%20inference%20time%2C%20since%20it%20relies%20only%20on%20the%20comparison%20of%20their%0Aoutput%20of%20the%20image%20embeddings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16302v2&entry.124074799=Read"},
{"title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion", "author": "Zhifei Chen and Tianshuo Xu and Wenhang Ge and Leyi Wu and Dongyu Yan and Jing He and Luozhou Wang and Lu Zeng and Shunsi Zhang and Yingcong Chen", "abstract": "  Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.\n", "link": "http://arxiv.org/abs/2412.15050v1", "date": "2024-12-19", "relevancy": 1.2242, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6579}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5986}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-Renderer%3A%20Unifying%20Rendering%20and%20Inverse%20Rendering%20Via%20Dual%20Stream%0A%20%20Diffusion&body=Title%3A%20Uni-Renderer%3A%20Unifying%20Rendering%20and%20Inverse%20Rendering%20Via%20Dual%20Stream%0A%20%20Diffusion%0AAuthor%3A%20Zhifei%20Chen%20and%20Tianshuo%20Xu%20and%20Wenhang%20Ge%20and%20Leyi%20Wu%20and%20Dongyu%20Yan%20and%20Jing%20He%20and%20Luozhou%20Wang%20and%20Lu%20Zeng%20and%20Shunsi%20Zhang%20and%20Yingcong%20Chen%0AAbstract%3A%20%20%20Rendering%20and%20inverse%20rendering%20are%20pivotal%20tasks%20in%20both%20computer%20vision%20and%0Agraphics.%20The%20rendering%20equation%20is%20the%20core%20of%20the%20two%20tasks%2C%20as%20an%20ideal%0Aconditional%20distribution%20transfer%20function%20from%20intrinsic%20properties%20to%20RGB%0Aimages.%20Despite%20achieving%20promising%20results%20of%20existing%20rendering%20methods%2C%20they%0Amerely%20approximate%20the%20ideal%20estimation%20for%20a%20specific%20scene%20and%20come%20with%20a%0Ahigh%20computational%20cost.%20Additionally%2C%20the%20inverse%20conditional%20distribution%0Atransfer%20is%20intractable%20due%20to%20the%20inherent%20ambiguity.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20data-driven%20method%20that%20jointly%20models%20rendering%20and%0Ainverse%20rendering%20as%20two%20conditional%20generation%20tasks%20within%20a%20single%20diffusion%0Aframework.%20Inspired%20by%20UniDiffuser%2C%20we%20utilize%20two%20distinct%20time%20schedules%20to%0Amodel%20both%20tasks%2C%20and%20with%20a%20tailored%20dual%20streaming%20module%2C%20we%20achieve%0Across-conditioning%20of%20two%20pre-trained%20diffusion%20models.%20This%20unified%20approach%2C%0Anamed%20Uni-Renderer%2C%20allows%20the%20two%20processes%20to%20facilitate%20each%20other%20through%20a%0Acycle-consistent%20constrain%2C%20mitigating%20ambiguity%20by%20enforcing%20consistency%0Abetween%20intrinsic%20properties%20and%20rendered%20images.%20Combined%20with%20a%20meticulously%0Aprepared%20dataset%2C%20our%20method%20effectively%20decomposition%20of%20intrinsic%20properties%0Aand%20demonstrates%20a%20strong%20capability%20to%20recognize%20changes%20during%20rendering.%20We%0Awill%20open-source%20our%20training%20and%20inference%20code%20to%20the%20public%2C%20fostering%0Afurther%20research%20and%20development%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-Renderer%253A%2520Unifying%2520Rendering%2520and%2520Inverse%2520Rendering%2520Via%2520Dual%2520Stream%250A%2520%2520Diffusion%26entry.906535625%3DZhifei%2520Chen%2520and%2520Tianshuo%2520Xu%2520and%2520Wenhang%2520Ge%2520and%2520Leyi%2520Wu%2520and%2520Dongyu%2520Yan%2520and%2520Jing%2520He%2520and%2520Luozhou%2520Wang%2520and%2520Lu%2520Zeng%2520and%2520Shunsi%2520Zhang%2520and%2520Yingcong%2520Chen%26entry.1292438233%3D%2520%2520Rendering%2520and%2520inverse%2520rendering%2520are%2520pivotal%2520tasks%2520in%2520both%2520computer%2520vision%2520and%250Agraphics.%2520The%2520rendering%2520equation%2520is%2520the%2520core%2520of%2520the%2520two%2520tasks%252C%2520as%2520an%2520ideal%250Aconditional%2520distribution%2520transfer%2520function%2520from%2520intrinsic%2520properties%2520to%2520RGB%250Aimages.%2520Despite%2520achieving%2520promising%2520results%2520of%2520existing%2520rendering%2520methods%252C%2520they%250Amerely%2520approximate%2520the%2520ideal%2520estimation%2520for%2520a%2520specific%2520scene%2520and%2520come%2520with%2520a%250Ahigh%2520computational%2520cost.%2520Additionally%252C%2520the%2520inverse%2520conditional%2520distribution%250Atransfer%2520is%2520intractable%2520due%2520to%2520the%2520inherent%2520ambiguity.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520data-driven%2520method%2520that%2520jointly%2520models%2520rendering%2520and%250Ainverse%2520rendering%2520as%2520two%2520conditional%2520generation%2520tasks%2520within%2520a%2520single%2520diffusion%250Aframework.%2520Inspired%2520by%2520UniDiffuser%252C%2520we%2520utilize%2520two%2520distinct%2520time%2520schedules%2520to%250Amodel%2520both%2520tasks%252C%2520and%2520with%2520a%2520tailored%2520dual%2520streaming%2520module%252C%2520we%2520achieve%250Across-conditioning%2520of%2520two%2520pre-trained%2520diffusion%2520models.%2520This%2520unified%2520approach%252C%250Anamed%2520Uni-Renderer%252C%2520allows%2520the%2520two%2520processes%2520to%2520facilitate%2520each%2520other%2520through%2520a%250Acycle-consistent%2520constrain%252C%2520mitigating%2520ambiguity%2520by%2520enforcing%2520consistency%250Abetween%2520intrinsic%2520properties%2520and%2520rendered%2520images.%2520Combined%2520with%2520a%2520meticulously%250Aprepared%2520dataset%252C%2520our%2520method%2520effectively%2520decomposition%2520of%2520intrinsic%2520properties%250Aand%2520demonstrates%2520a%2520strong%2520capability%2520to%2520recognize%2520changes%2520during%2520rendering.%2520We%250Awill%2520open-source%2520our%2520training%2520and%2520inference%2520code%2520to%2520the%2520public%252C%2520fostering%250Afurther%2520research%2520and%2520development%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-Renderer%3A%20Unifying%20Rendering%20and%20Inverse%20Rendering%20Via%20Dual%20Stream%0A%20%20Diffusion&entry.906535625=Zhifei%20Chen%20and%20Tianshuo%20Xu%20and%20Wenhang%20Ge%20and%20Leyi%20Wu%20and%20Dongyu%20Yan%20and%20Jing%20He%20and%20Luozhou%20Wang%20and%20Lu%20Zeng%20and%20Shunsi%20Zhang%20and%20Yingcong%20Chen&entry.1292438233=%20%20Rendering%20and%20inverse%20rendering%20are%20pivotal%20tasks%20in%20both%20computer%20vision%20and%0Agraphics.%20The%20rendering%20equation%20is%20the%20core%20of%20the%20two%20tasks%2C%20as%20an%20ideal%0Aconditional%20distribution%20transfer%20function%20from%20intrinsic%20properties%20to%20RGB%0Aimages.%20Despite%20achieving%20promising%20results%20of%20existing%20rendering%20methods%2C%20they%0Amerely%20approximate%20the%20ideal%20estimation%20for%20a%20specific%20scene%20and%20come%20with%20a%0Ahigh%20computational%20cost.%20Additionally%2C%20the%20inverse%20conditional%20distribution%0Atransfer%20is%20intractable%20due%20to%20the%20inherent%20ambiguity.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20data-driven%20method%20that%20jointly%20models%20rendering%20and%0Ainverse%20rendering%20as%20two%20conditional%20generation%20tasks%20within%20a%20single%20diffusion%0Aframework.%20Inspired%20by%20UniDiffuser%2C%20we%20utilize%20two%20distinct%20time%20schedules%20to%0Amodel%20both%20tasks%2C%20and%20with%20a%20tailored%20dual%20streaming%20module%2C%20we%20achieve%0Across-conditioning%20of%20two%20pre-trained%20diffusion%20models.%20This%20unified%20approach%2C%0Anamed%20Uni-Renderer%2C%20allows%20the%20two%20processes%20to%20facilitate%20each%20other%20through%20a%0Acycle-consistent%20constrain%2C%20mitigating%20ambiguity%20by%20enforcing%20consistency%0Abetween%20intrinsic%20properties%20and%20rendered%20images.%20Combined%20with%20a%20meticulously%0Aprepared%20dataset%2C%20our%20method%20effectively%20decomposition%20of%20intrinsic%20properties%0Aand%20demonstrates%20a%20strong%20capability%20to%20recognize%20changes%20during%20rendering.%20We%0Awill%20open-source%20our%20training%20and%20inference%20code%20to%20the%20public%2C%20fostering%0Afurther%20research%20and%20development%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15050v1&entry.124074799=Read"},
{"title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine", "author": "Rabee Qasem and Mohannad Hendi and Banan Tantour", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.\n", "link": "http://arxiv.org/abs/2412.14771v1", "date": "2024-12-19", "relevancy": 1.8838, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALKAFI-LLAMA3%3A%20Fine-Tuning%20LLMs%20for%20Precise%20Legal%20Understanding%20in%0A%20%20Palestine&body=Title%3A%20ALKAFI-LLAMA3%3A%20Fine-Tuning%20LLMs%20for%20Precise%20Legal%20Understanding%20in%0A%20%20Palestine%0AAuthor%3A%20Rabee%20Qasem%20and%20Mohannad%20Hendi%20and%20Banan%20Tantour%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Adiverse%20domains%2C%20yet%20their%20application%20in%20the%20legal%20sector%2C%20particularly%20in%0Alow-resource%20contexts%2C%20remains%20limited.%20This%20study%20addresses%20the%20challenges%20of%0Aadapting%20LLMs%20to%20the%20Palestinian%20legal%20domain%2C%20where%20political%20instability%2C%0Afragmented%20legal%20frameworks%2C%20and%20limited%20AI%20resources%20hinder%20effective%0Amachine-learning%20applications.%20We%20present%20a%20fine-tuned%20model%20based%20on%20a%0Aquantized%20version%20of%20Llama-3.2-1B-Instruct%2C%20trained%20on%20a%20synthetic%20data%20set%0Aderived%20from%20Palestinian%20legal%20texts.%20Using%20smaller-scale%20models%20and%0Astrategically%20generated%20question-answer%20pairs%2C%20we%20achieve%20a%20cost-effective%2C%0Alocally%20sustainable%20solution%20that%20provides%20accurate%20and%20contextually%20relevant%0Alegal%20guidance.%20Our%20experiments%20demonstrate%20promising%20performance%20on%20various%0Aquery%20types%2C%20ranging%20from%20yes/no%20questions%20and%20narrative%20explanations%20to%0Acomplex%20legal%20differentiations%2C%20while%20highlighting%20areas%20for%20improvement%2C%20such%0Aas%20handling%20calculation-based%20inquiries%20and%20structured%20list%20formatting.%20This%0Awork%20provides%20a%20pathway%20for%20the%20deployment%20of%20AI-driven%20legal%20assistance%20tools%0Atailored%20to%20the%20needs%20of%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALKAFI-LLAMA3%253A%2520Fine-Tuning%2520LLMs%2520for%2520Precise%2520Legal%2520Understanding%2520in%250A%2520%2520Palestine%26entry.906535625%3DRabee%2520Qasem%2520and%2520Mohannad%2520Hendi%2520and%2520Banan%2520Tantour%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%2520in%250Adiverse%2520domains%252C%2520yet%2520their%2520application%2520in%2520the%2520legal%2520sector%252C%2520particularly%2520in%250Alow-resource%2520contexts%252C%2520remains%2520limited.%2520This%2520study%2520addresses%2520the%2520challenges%2520of%250Aadapting%2520LLMs%2520to%2520the%2520Palestinian%2520legal%2520domain%252C%2520where%2520political%2520instability%252C%250Afragmented%2520legal%2520frameworks%252C%2520and%2520limited%2520AI%2520resources%2520hinder%2520effective%250Amachine-learning%2520applications.%2520We%2520present%2520a%2520fine-tuned%2520model%2520based%2520on%2520a%250Aquantized%2520version%2520of%2520Llama-3.2-1B-Instruct%252C%2520trained%2520on%2520a%2520synthetic%2520data%2520set%250Aderived%2520from%2520Palestinian%2520legal%2520texts.%2520Using%2520smaller-scale%2520models%2520and%250Astrategically%2520generated%2520question-answer%2520pairs%252C%2520we%2520achieve%2520a%2520cost-effective%252C%250Alocally%2520sustainable%2520solution%2520that%2520provides%2520accurate%2520and%2520contextually%2520relevant%250Alegal%2520guidance.%2520Our%2520experiments%2520demonstrate%2520promising%2520performance%2520on%2520various%250Aquery%2520types%252C%2520ranging%2520from%2520yes/no%2520questions%2520and%2520narrative%2520explanations%2520to%250Acomplex%2520legal%2520differentiations%252C%2520while%2520highlighting%2520areas%2520for%2520improvement%252C%2520such%250Aas%2520handling%2520calculation-based%2520inquiries%2520and%2520structured%2520list%2520formatting.%2520This%250Awork%2520provides%2520a%2520pathway%2520for%2520the%2520deployment%2520of%2520AI-driven%2520legal%2520assistance%2520tools%250Atailored%2520to%2520the%2520needs%2520of%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALKAFI-LLAMA3%3A%20Fine-Tuning%20LLMs%20for%20Precise%20Legal%20Understanding%20in%0A%20%20Palestine&entry.906535625=Rabee%20Qasem%20and%20Mohannad%20Hendi%20and%20Banan%20Tantour&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Adiverse%20domains%2C%20yet%20their%20application%20in%20the%20legal%20sector%2C%20particularly%20in%0Alow-resource%20contexts%2C%20remains%20limited.%20This%20study%20addresses%20the%20challenges%20of%0Aadapting%20LLMs%20to%20the%20Palestinian%20legal%20domain%2C%20where%20political%20instability%2C%0Afragmented%20legal%20frameworks%2C%20and%20limited%20AI%20resources%20hinder%20effective%0Amachine-learning%20applications.%20We%20present%20a%20fine-tuned%20model%20based%20on%20a%0Aquantized%20version%20of%20Llama-3.2-1B-Instruct%2C%20trained%20on%20a%20synthetic%20data%20set%0Aderived%20from%20Palestinian%20legal%20texts.%20Using%20smaller-scale%20models%20and%0Astrategically%20generated%20question-answer%20pairs%2C%20we%20achieve%20a%20cost-effective%2C%0Alocally%20sustainable%20solution%20that%20provides%20accurate%20and%20contextually%20relevant%0Alegal%20guidance.%20Our%20experiments%20demonstrate%20promising%20performance%20on%20various%0Aquery%20types%2C%20ranging%20from%20yes/no%20questions%20and%20narrative%20explanations%20to%0Acomplex%20legal%20differentiations%2C%20while%20highlighting%20areas%20for%20improvement%2C%20such%0Aas%20handling%20calculation-based%20inquiries%20and%20structured%20list%20formatting.%20This%0Awork%20provides%20a%20pathway%20for%20the%20deployment%20of%20AI-driven%20legal%20assistance%20tools%0Atailored%20to%20the%20needs%20of%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14771v1&entry.124074799=Read"},
{"title": "DepthFM: Fast Monocular Depth Estimation with Flow Matching", "author": "Ming Gui and Johannes Schusterbauer and Ulrich Prestel and Pingchuan Ma and Dmytro Kotovenko and Olga Grebenkova and Stefan Andreas Baumann and Vincent Tao Hu and Bj\u00f6rn Ommer", "abstract": "  Current discriminative depth estimation methods often produce blurry\nartifacts, while generative approaches suffer from slow sampling due to\ncurvatures in the noise-to-depth transport. Our method addresses these\nchallenges by framing depth estimation as a direct transport between image and\ndepth distributions. We are the first to explore flow matching in this field,\nand we demonstrate that its interpolation trajectories enhance both training\nand sampling efficiency while preserving high performance. While generative\nmodels typically require extensive training data, we mitigate this dependency\nby integrating external knowledge from a pre-trained image diffusion model,\nenabling effective transfer even across differing objectives. To further boost\nour model performance, we employ synthetic data and utilize image-depth pairs\ngenerated by a discriminative model on an in-the-wild image dataset. As a\ngenerative model, our model can reliably estimate depth confidence, which\nprovides an additional advantage. Our approach achieves competitive zero-shot\nperformance on standard benchmarks of complex natural scenes while improving\nsampling efficiency and only requiring minimal synthetic data for training.\n", "link": "http://arxiv.org/abs/2403.13788v2", "date": "2024-12-19", "relevancy": 1.2045, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6277}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5961}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching&body=Title%3A%20DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching%0AAuthor%3A%20Ming%20Gui%20and%20Johannes%20Schusterbauer%20and%20Ulrich%20Prestel%20and%20Pingchuan%20Ma%20and%20Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Stefan%20Andreas%20Baumann%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Current%20discriminative%20depth%20estimation%20methods%20often%20produce%20blurry%0Aartifacts%2C%20while%20generative%20approaches%20suffer%20from%20slow%20sampling%20due%20to%0Acurvatures%20in%20the%20noise-to-depth%20transport.%20Our%20method%20addresses%20these%0Achallenges%20by%20framing%20depth%20estimation%20as%20a%20direct%20transport%20between%20image%20and%0Adepth%20distributions.%20We%20are%20the%20first%20to%20explore%20flow%20matching%20in%20this%20field%2C%0Aand%20we%20demonstrate%20that%20its%20interpolation%20trajectories%20enhance%20both%20training%0Aand%20sampling%20efficiency%20while%20preserving%20high%20performance.%20While%20generative%0Amodels%20typically%20require%20extensive%20training%20data%2C%20we%20mitigate%20this%20dependency%0Aby%20integrating%20external%20knowledge%20from%20a%20pre-trained%20image%20diffusion%20model%2C%0Aenabling%20effective%20transfer%20even%20across%20differing%20objectives.%20To%20further%20boost%0Aour%20model%20performance%2C%20we%20employ%20synthetic%20data%20and%20utilize%20image-depth%20pairs%0Agenerated%20by%20a%20discriminative%20model%20on%20an%20in-the-wild%20image%20dataset.%20As%20a%0Agenerative%20model%2C%20our%20model%20can%20reliably%20estimate%20depth%20confidence%2C%20which%0Aprovides%20an%20additional%20advantage.%20Our%20approach%20achieves%20competitive%20zero-shot%0Aperformance%20on%20standard%20benchmarks%20of%20complex%20natural%20scenes%20while%20improving%0Asampling%20efficiency%20and%20only%20requiring%20minimal%20synthetic%20data%20for%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthFM%253A%2520Fast%2520Monocular%2520Depth%2520Estimation%2520with%2520Flow%2520Matching%26entry.906535625%3DMing%2520Gui%2520and%2520Johannes%2520Schusterbauer%2520and%2520Ulrich%2520Prestel%2520and%2520Pingchuan%2520Ma%2520and%2520Dmytro%2520Kotovenko%2520and%2520Olga%2520Grebenkova%2520and%2520Stefan%2520Andreas%2520Baumann%2520and%2520Vincent%2520Tao%2520Hu%2520and%2520Bj%25C3%25B6rn%2520Ommer%26entry.1292438233%3D%2520%2520Current%2520discriminative%2520depth%2520estimation%2520methods%2520often%2520produce%2520blurry%250Aartifacts%252C%2520while%2520generative%2520approaches%2520suffer%2520from%2520slow%2520sampling%2520due%2520to%250Acurvatures%2520in%2520the%2520noise-to-depth%2520transport.%2520Our%2520method%2520addresses%2520these%250Achallenges%2520by%2520framing%2520depth%2520estimation%2520as%2520a%2520direct%2520transport%2520between%2520image%2520and%250Adepth%2520distributions.%2520We%2520are%2520the%2520first%2520to%2520explore%2520flow%2520matching%2520in%2520this%2520field%252C%250Aand%2520we%2520demonstrate%2520that%2520its%2520interpolation%2520trajectories%2520enhance%2520both%2520training%250Aand%2520sampling%2520efficiency%2520while%2520preserving%2520high%2520performance.%2520While%2520generative%250Amodels%2520typically%2520require%2520extensive%2520training%2520data%252C%2520we%2520mitigate%2520this%2520dependency%250Aby%2520integrating%2520external%2520knowledge%2520from%2520a%2520pre-trained%2520image%2520diffusion%2520model%252C%250Aenabling%2520effective%2520transfer%2520even%2520across%2520differing%2520objectives.%2520To%2520further%2520boost%250Aour%2520model%2520performance%252C%2520we%2520employ%2520synthetic%2520data%2520and%2520utilize%2520image-depth%2520pairs%250Agenerated%2520by%2520a%2520discriminative%2520model%2520on%2520an%2520in-the-wild%2520image%2520dataset.%2520As%2520a%250Agenerative%2520model%252C%2520our%2520model%2520can%2520reliably%2520estimate%2520depth%2520confidence%252C%2520which%250Aprovides%2520an%2520additional%2520advantage.%2520Our%2520approach%2520achieves%2520competitive%2520zero-shot%250Aperformance%2520on%2520standard%2520benchmarks%2520of%2520complex%2520natural%2520scenes%2520while%2520improving%250Asampling%2520efficiency%2520and%2520only%2520requiring%2520minimal%2520synthetic%2520data%2520for%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthFM%3A%20Fast%20Monocular%20Depth%20Estimation%20with%20Flow%20Matching&entry.906535625=Ming%20Gui%20and%20Johannes%20Schusterbauer%20and%20Ulrich%20Prestel%20and%20Pingchuan%20Ma%20and%20Dmytro%20Kotovenko%20and%20Olga%20Grebenkova%20and%20Stefan%20Andreas%20Baumann%20and%20Vincent%20Tao%20Hu%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Current%20discriminative%20depth%20estimation%20methods%20often%20produce%20blurry%0Aartifacts%2C%20while%20generative%20approaches%20suffer%20from%20slow%20sampling%20due%20to%0Acurvatures%20in%20the%20noise-to-depth%20transport.%20Our%20method%20addresses%20these%0Achallenges%20by%20framing%20depth%20estimation%20as%20a%20direct%20transport%20between%20image%20and%0Adepth%20distributions.%20We%20are%20the%20first%20to%20explore%20flow%20matching%20in%20this%20field%2C%0Aand%20we%20demonstrate%20that%20its%20interpolation%20trajectories%20enhance%20both%20training%0Aand%20sampling%20efficiency%20while%20preserving%20high%20performance.%20While%20generative%0Amodels%20typically%20require%20extensive%20training%20data%2C%20we%20mitigate%20this%20dependency%0Aby%20integrating%20external%20knowledge%20from%20a%20pre-trained%20image%20diffusion%20model%2C%0Aenabling%20effective%20transfer%20even%20across%20differing%20objectives.%20To%20further%20boost%0Aour%20model%20performance%2C%20we%20employ%20synthetic%20data%20and%20utilize%20image-depth%20pairs%0Agenerated%20by%20a%20discriminative%20model%20on%20an%20in-the-wild%20image%20dataset.%20As%20a%0Agenerative%20model%2C%20our%20model%20can%20reliably%20estimate%20depth%20confidence%2C%20which%0Aprovides%20an%20additional%20advantage.%20Our%20approach%20achieves%20competitive%20zero-shot%0Aperformance%20on%20standard%20benchmarks%20of%20complex%20natural%20scenes%20while%20improving%0Asampling%20efficiency%20and%20only%20requiring%20minimal%20synthetic%20data%20for%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13788v2&entry.124074799=Read"},
{"title": "Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet", "author": "Litingyu Wang and Wenjun Liao and Shichuan Zhang and Guotai Wang", "abstract": "  Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.\n", "link": "http://arxiv.org/abs/2412.14846v1", "date": "2024-12-19", "relevancy": 2.0003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5006}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Head%20and%20Neck%20Tumor%20Segmentation%20of%20MRI%20from%20Pre-%20and%20Mid-radiotherapy%0A%20%20with%20Pre-training%2C%20Data%20Augmentation%20and%20Dual%20Flow%20UNet&body=Title%3A%20Head%20and%20Neck%20Tumor%20Segmentation%20of%20MRI%20from%20Pre-%20and%20Mid-radiotherapy%0A%20%20with%20Pre-training%2C%20Data%20Augmentation%20and%20Dual%20Flow%20UNet%0AAuthor%3A%20Litingyu%20Wang%20and%20Wenjun%20Liao%20and%20Shichuan%20Zhang%20and%20Guotai%20Wang%0AAbstract%3A%20%20%20Head%20and%20neck%20tumors%20and%20metastatic%20lymph%20nodes%20are%20crucial%20for%20treatment%0Aplanning%20and%20prognostic%20analysis.%20Accurate%20segmentation%20and%20quantitative%0Aanalysis%20of%20these%20structures%20require%20pixel-level%20annotation%2C%20making%20automated%0Asegmentation%20techniques%20essential%20for%20the%20diagnosis%20and%20treatment%20of%20head%20and%0Aneck%20cancer.%20In%20this%20study%2C%20we%20investigated%20the%20effects%20of%20multiple%20strategies%0Aon%20the%20segmentation%20of%20pre-radiotherapy%20%28pre-RT%29%20and%20mid-radiotherapy%20%28mid-RT%29%0Aimages.%20For%20the%20segmentation%20of%20pre-RT%20images%2C%20we%20utilized%3A%201%29%20a%20fully%0Asupervised%20learning%20approach%2C%20and%202%29%20the%20same%20approach%20enhanced%20with%0Apre-trained%20weights%20and%20the%20MixUp%20data%20augmentation%20technique.%20For%20mid-RT%0Aimages%2C%20we%20introduced%20a%20novel%20computational-friendly%20network%20architecture%20that%0Afeatures%20separate%20encoders%20for%20mid-RT%20images%20and%20registered%20pre-RT%20images%20with%0Atheir%20labels.%20The%20mid-RT%20encoder%20branch%20integrates%20information%20from%20pre-RT%0Aimages%20and%20labels%20progressively%20during%20the%20forward%20propagation.%20We%20selected%20the%0Ahighest-performing%20model%20from%20each%20fold%20and%20used%20their%20predictions%20to%20create%20an%0Aensemble%20average%20for%20inference.%20In%20the%20final%20test%2C%20our%20models%20achieved%20a%0Asegmentation%20performance%20of%2082.38%25%20for%20pre-RT%20and%2072.53%25%20for%20mid-RT%20on%0Aaggregated%20Dice%20Similarity%20Coefficient%20%28DSC%29%20as%20HiLab.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WltyBY/HNTS-MRG2024_train_code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHead%2520and%2520Neck%2520Tumor%2520Segmentation%2520of%2520MRI%2520from%2520Pre-%2520and%2520Mid-radiotherapy%250A%2520%2520with%2520Pre-training%252C%2520Data%2520Augmentation%2520and%2520Dual%2520Flow%2520UNet%26entry.906535625%3DLitingyu%2520Wang%2520and%2520Wenjun%2520Liao%2520and%2520Shichuan%2520Zhang%2520and%2520Guotai%2520Wang%26entry.1292438233%3D%2520%2520Head%2520and%2520neck%2520tumors%2520and%2520metastatic%2520lymph%2520nodes%2520are%2520crucial%2520for%2520treatment%250Aplanning%2520and%2520prognostic%2520analysis.%2520Accurate%2520segmentation%2520and%2520quantitative%250Aanalysis%2520of%2520these%2520structures%2520require%2520pixel-level%2520annotation%252C%2520making%2520automated%250Asegmentation%2520techniques%2520essential%2520for%2520the%2520diagnosis%2520and%2520treatment%2520of%2520head%2520and%250Aneck%2520cancer.%2520In%2520this%2520study%252C%2520we%2520investigated%2520the%2520effects%2520of%2520multiple%2520strategies%250Aon%2520the%2520segmentation%2520of%2520pre-radiotherapy%2520%2528pre-RT%2529%2520and%2520mid-radiotherapy%2520%2528mid-RT%2529%250Aimages.%2520For%2520the%2520segmentation%2520of%2520pre-RT%2520images%252C%2520we%2520utilized%253A%25201%2529%2520a%2520fully%250Asupervised%2520learning%2520approach%252C%2520and%25202%2529%2520the%2520same%2520approach%2520enhanced%2520with%250Apre-trained%2520weights%2520and%2520the%2520MixUp%2520data%2520augmentation%2520technique.%2520For%2520mid-RT%250Aimages%252C%2520we%2520introduced%2520a%2520novel%2520computational-friendly%2520network%2520architecture%2520that%250Afeatures%2520separate%2520encoders%2520for%2520mid-RT%2520images%2520and%2520registered%2520pre-RT%2520images%2520with%250Atheir%2520labels.%2520The%2520mid-RT%2520encoder%2520branch%2520integrates%2520information%2520from%2520pre-RT%250Aimages%2520and%2520labels%2520progressively%2520during%2520the%2520forward%2520propagation.%2520We%2520selected%2520the%250Ahighest-performing%2520model%2520from%2520each%2520fold%2520and%2520used%2520their%2520predictions%2520to%2520create%2520an%250Aensemble%2520average%2520for%2520inference.%2520In%2520the%2520final%2520test%252C%2520our%2520models%2520achieved%2520a%250Asegmentation%2520performance%2520of%252082.38%2525%2520for%2520pre-RT%2520and%252072.53%2525%2520for%2520mid-RT%2520on%250Aaggregated%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520as%2520HiLab.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/WltyBY/HNTS-MRG2024_train_code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Head%20and%20Neck%20Tumor%20Segmentation%20of%20MRI%20from%20Pre-%20and%20Mid-radiotherapy%0A%20%20with%20Pre-training%2C%20Data%20Augmentation%20and%20Dual%20Flow%20UNet&entry.906535625=Litingyu%20Wang%20and%20Wenjun%20Liao%20and%20Shichuan%20Zhang%20and%20Guotai%20Wang&entry.1292438233=%20%20Head%20and%20neck%20tumors%20and%20metastatic%20lymph%20nodes%20are%20crucial%20for%20treatment%0Aplanning%20and%20prognostic%20analysis.%20Accurate%20segmentation%20and%20quantitative%0Aanalysis%20of%20these%20structures%20require%20pixel-level%20annotation%2C%20making%20automated%0Asegmentation%20techniques%20essential%20for%20the%20diagnosis%20and%20treatment%20of%20head%20and%0Aneck%20cancer.%20In%20this%20study%2C%20we%20investigated%20the%20effects%20of%20multiple%20strategies%0Aon%20the%20segmentation%20of%20pre-radiotherapy%20%28pre-RT%29%20and%20mid-radiotherapy%20%28mid-RT%29%0Aimages.%20For%20the%20segmentation%20of%20pre-RT%20images%2C%20we%20utilized%3A%201%29%20a%20fully%0Asupervised%20learning%20approach%2C%20and%202%29%20the%20same%20approach%20enhanced%20with%0Apre-trained%20weights%20and%20the%20MixUp%20data%20augmentation%20technique.%20For%20mid-RT%0Aimages%2C%20we%20introduced%20a%20novel%20computational-friendly%20network%20architecture%20that%0Afeatures%20separate%20encoders%20for%20mid-RT%20images%20and%20registered%20pre-RT%20images%20with%0Atheir%20labels.%20The%20mid-RT%20encoder%20branch%20integrates%20information%20from%20pre-RT%0Aimages%20and%20labels%20progressively%20during%20the%20forward%20propagation.%20We%20selected%20the%0Ahighest-performing%20model%20from%20each%20fold%20and%20used%20their%20predictions%20to%20create%20an%0Aensemble%20average%20for%20inference.%20In%20the%20final%20test%2C%20our%20models%20achieved%20a%0Asegmentation%20performance%20of%2082.38%25%20for%20pre-RT%20and%2072.53%25%20for%20mid-RT%20on%0Aaggregated%20Dice%20Similarity%20Coefficient%20%28DSC%29%20as%20HiLab.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/WltyBY/HNTS-MRG2024_train_code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14846v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


