<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241118.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images", "author": "Eunji Hong and Minh Hieu Nguyen and Mikaela Angelina Uy and Minhyuk Sung", "abstract": "  We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view\nimages, not merely as a field or raw geometry but as a sketch-extrude CAD\nmodel. Extracting extrusion cylinders from raw 3D geometry has been extensively\nresearched in computer vision, while the processing of 3D data through neural\nnetworks has remained a bottleneck. Since 3D scans are generally accompanied by\nmulti-view images, leveraging 2D convolutional neural networks allows these\nimages to be exploited as a rich source for extracting extrusion cylinder\ninformation. However, we observe that extracting only the surface information\nof the extrudes and utilizing it results in suboptimal outcomes due to the\nchallenges in the occlusion and surface segmentation. By synergizing with the\nextracted base curve information, we achieve the optimal reconstruction result\nwith the best accuracy in 2D sketch and extrude parameter estimation. Our\nexperiments, comparing our method with previous work that takes a raw 3D point\ncloud as input, demonstrate the effectiveness of our approach by taking\nadvantage of multi-view images. Our project page can be found at\nhttp://mv2cyl.github.io .\n", "link": "http://arxiv.org/abs/2406.10853v3", "date": "2024-11-18", "relevancy": 3.1765, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6643}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6643}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images&body=Title%3A%20MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images%0AAuthor%3A%20Eunji%20Hong%20and%20Minh%20Hieu%20Nguyen%20and%20Mikaela%20Angelina%20Uy%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20present%20MV2Cyl%2C%20a%20novel%20method%20for%20reconstructing%203D%20from%202D%20multi-view%0Aimages%2C%20not%20merely%20as%20a%20field%20or%20raw%20geometry%20but%20as%20a%20sketch-extrude%20CAD%0Amodel.%20Extracting%20extrusion%20cylinders%20from%20raw%203D%20geometry%20has%20been%20extensively%0Aresearched%20in%20computer%20vision%2C%20while%20the%20processing%20of%203D%20data%20through%20neural%0Anetworks%20has%20remained%20a%20bottleneck.%20Since%203D%20scans%20are%20generally%20accompanied%20by%0Amulti-view%20images%2C%20leveraging%202D%20convolutional%20neural%20networks%20allows%20these%0Aimages%20to%20be%20exploited%20as%20a%20rich%20source%20for%20extracting%20extrusion%20cylinder%0Ainformation.%20However%2C%20we%20observe%20that%20extracting%20only%20the%20surface%20information%0Aof%20the%20extrudes%20and%20utilizing%20it%20results%20in%20suboptimal%20outcomes%20due%20to%20the%0Achallenges%20in%20the%20occlusion%20and%20surface%20segmentation.%20By%20synergizing%20with%20the%0Aextracted%20base%20curve%20information%2C%20we%20achieve%20the%20optimal%20reconstruction%20result%0Awith%20the%20best%20accuracy%20in%202D%20sketch%20and%20extrude%20parameter%20estimation.%20Our%0Aexperiments%2C%20comparing%20our%20method%20with%20previous%20work%20that%20takes%20a%20raw%203D%20point%0Acloud%20as%20input%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20taking%0Aadvantage%20of%20multi-view%20images.%20Our%20project%20page%20can%20be%20found%20at%0Ahttp%3A//mv2cyl.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10853v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV2Cyl%253A%2520Reconstructing%25203D%2520Extrusion%2520Cylinders%2520from%2520Multi-View%2520Images%26entry.906535625%3DEunji%2520Hong%2520and%2520Minh%2520Hieu%2520Nguyen%2520and%2520Mikaela%2520Angelina%2520Uy%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3D%2520%2520We%2520present%2520MV2Cyl%252C%2520a%2520novel%2520method%2520for%2520reconstructing%25203D%2520from%25202D%2520multi-view%250Aimages%252C%2520not%2520merely%2520as%2520a%2520field%2520or%2520raw%2520geometry%2520but%2520as%2520a%2520sketch-extrude%2520CAD%250Amodel.%2520Extracting%2520extrusion%2520cylinders%2520from%2520raw%25203D%2520geometry%2520has%2520been%2520extensively%250Aresearched%2520in%2520computer%2520vision%252C%2520while%2520the%2520processing%2520of%25203D%2520data%2520through%2520neural%250Anetworks%2520has%2520remained%2520a%2520bottleneck.%2520Since%25203D%2520scans%2520are%2520generally%2520accompanied%2520by%250Amulti-view%2520images%252C%2520leveraging%25202D%2520convolutional%2520neural%2520networks%2520allows%2520these%250Aimages%2520to%2520be%2520exploited%2520as%2520a%2520rich%2520source%2520for%2520extracting%2520extrusion%2520cylinder%250Ainformation.%2520However%252C%2520we%2520observe%2520that%2520extracting%2520only%2520the%2520surface%2520information%250Aof%2520the%2520extrudes%2520and%2520utilizing%2520it%2520results%2520in%2520suboptimal%2520outcomes%2520due%2520to%2520the%250Achallenges%2520in%2520the%2520occlusion%2520and%2520surface%2520segmentation.%2520By%2520synergizing%2520with%2520the%250Aextracted%2520base%2520curve%2520information%252C%2520we%2520achieve%2520the%2520optimal%2520reconstruction%2520result%250Awith%2520the%2520best%2520accuracy%2520in%25202D%2520sketch%2520and%2520extrude%2520parameter%2520estimation.%2520Our%250Aexperiments%252C%2520comparing%2520our%2520method%2520with%2520previous%2520work%2520that%2520takes%2520a%2520raw%25203D%2520point%250Acloud%2520as%2520input%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520by%2520taking%250Aadvantage%2520of%2520multi-view%2520images.%2520Our%2520project%2520page%2520can%2520be%2520found%2520at%250Ahttp%253A//mv2cyl.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10853v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV2Cyl%3A%20Reconstructing%203D%20Extrusion%20Cylinders%20from%20Multi-View%20Images&entry.906535625=Eunji%20Hong%20and%20Minh%20Hieu%20Nguyen%20and%20Mikaela%20Angelina%20Uy%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20present%20MV2Cyl%2C%20a%20novel%20method%20for%20reconstructing%203D%20from%202D%20multi-view%0Aimages%2C%20not%20merely%20as%20a%20field%20or%20raw%20geometry%20but%20as%20a%20sketch-extrude%20CAD%0Amodel.%20Extracting%20extrusion%20cylinders%20from%20raw%203D%20geometry%20has%20been%20extensively%0Aresearched%20in%20computer%20vision%2C%20while%20the%20processing%20of%203D%20data%20through%20neural%0Anetworks%20has%20remained%20a%20bottleneck.%20Since%203D%20scans%20are%20generally%20accompanied%20by%0Amulti-view%20images%2C%20leveraging%202D%20convolutional%20neural%20networks%20allows%20these%0Aimages%20to%20be%20exploited%20as%20a%20rich%20source%20for%20extracting%20extrusion%20cylinder%0Ainformation.%20However%2C%20we%20observe%20that%20extracting%20only%20the%20surface%20information%0Aof%20the%20extrudes%20and%20utilizing%20it%20results%20in%20suboptimal%20outcomes%20due%20to%20the%0Achallenges%20in%20the%20occlusion%20and%20surface%20segmentation.%20By%20synergizing%20with%20the%0Aextracted%20base%20curve%20information%2C%20we%20achieve%20the%20optimal%20reconstruction%20result%0Awith%20the%20best%20accuracy%20in%202D%20sketch%20and%20extrude%20parameter%20estimation.%20Our%0Aexperiments%2C%20comparing%20our%20method%20with%20previous%20work%20that%20takes%20a%20raw%203D%20point%0Acloud%20as%20input%2C%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20taking%0Aadvantage%20of%20multi-view%20images.%20Our%20project%20page%20can%20be%20found%20at%0Ahttp%3A//mv2cyl.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10853v3&entry.124074799=Read"},
{"title": "MGNiceNet: Unified Monocular Geometric Scene Understanding", "author": "Markus Sch\u00f6n and Michael Buchholz and Klaus Dietmayer", "abstract": "  Monocular geometric scene understanding combines panoptic segmentation and\nself-supervised depth estimation, focusing on real-time application in\nautonomous vehicles. We introduce MGNiceNet, a unified approach that uses a\nlinked kernel formulation for panoptic segmentation and self-supervised depth\nestimation. MGNiceNet is based on the state-of-the-art real-time panoptic\nsegmentation method RT-K-Net and extends the architecture to cover both\npanoptic segmentation and self-supervised monocular depth estimation. To this\nend, we introduce a tightly coupled self-supervised depth estimation predictor\nthat explicitly uses information from the panoptic path for depth prediction.\nFurthermore, we introduce a panoptic-guided motion masking method to improve\ndepth estimation without relying on video panoptic segmentation annotations. We\nevaluate our method on two popular autonomous driving datasets, Cityscapes and\nKITTI. Our model shows state-of-the-art results compared to other real-time\nmethods and closes the gap to computationally more demanding methods. Source\ncode and trained models are available at\nhttps://github.com/markusschoen/MGNiceNet.\n", "link": "http://arxiv.org/abs/2411.11466v1", "date": "2024-11-18", "relevancy": 2.9183, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5918}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5889}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGNiceNet%3A%20Unified%20Monocular%20Geometric%20Scene%20Understanding&body=Title%3A%20MGNiceNet%3A%20Unified%20Monocular%20Geometric%20Scene%20Understanding%0AAuthor%3A%20Markus%20Sch%C3%B6n%20and%20Michael%20Buchholz%20and%20Klaus%20Dietmayer%0AAbstract%3A%20%20%20Monocular%20geometric%20scene%20understanding%20combines%20panoptic%20segmentation%20and%0Aself-supervised%20depth%20estimation%2C%20focusing%20on%20real-time%20application%20in%0Aautonomous%20vehicles.%20We%20introduce%20MGNiceNet%2C%20a%20unified%20approach%20that%20uses%20a%0Alinked%20kernel%20formulation%20for%20panoptic%20segmentation%20and%20self-supervised%20depth%0Aestimation.%20MGNiceNet%20is%20based%20on%20the%20state-of-the-art%20real-time%20panoptic%0Asegmentation%20method%20RT-K-Net%20and%20extends%20the%20architecture%20to%20cover%20both%0Apanoptic%20segmentation%20and%20self-supervised%20monocular%20depth%20estimation.%20To%20this%0Aend%2C%20we%20introduce%20a%20tightly%20coupled%20self-supervised%20depth%20estimation%20predictor%0Athat%20explicitly%20uses%20information%20from%20the%20panoptic%20path%20for%20depth%20prediction.%0AFurthermore%2C%20we%20introduce%20a%20panoptic-guided%20motion%20masking%20method%20to%20improve%0Adepth%20estimation%20without%20relying%20on%20video%20panoptic%20segmentation%20annotations.%20We%0Aevaluate%20our%20method%20on%20two%20popular%20autonomous%20driving%20datasets%2C%20Cityscapes%20and%0AKITTI.%20Our%20model%20shows%20state-of-the-art%20results%20compared%20to%20other%20real-time%0Amethods%20and%20closes%20the%20gap%20to%20computationally%20more%20demanding%20methods.%20Source%0Acode%20and%20trained%20models%20are%20available%20at%0Ahttps%3A//github.com/markusschoen/MGNiceNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGNiceNet%253A%2520Unified%2520Monocular%2520Geometric%2520Scene%2520Understanding%26entry.906535625%3DMarkus%2520Sch%25C3%25B6n%2520and%2520Michael%2520Buchholz%2520and%2520Klaus%2520Dietmayer%26entry.1292438233%3D%2520%2520Monocular%2520geometric%2520scene%2520understanding%2520combines%2520panoptic%2520segmentation%2520and%250Aself-supervised%2520depth%2520estimation%252C%2520focusing%2520on%2520real-time%2520application%2520in%250Aautonomous%2520vehicles.%2520We%2520introduce%2520MGNiceNet%252C%2520a%2520unified%2520approach%2520that%2520uses%2520a%250Alinked%2520kernel%2520formulation%2520for%2520panoptic%2520segmentation%2520and%2520self-supervised%2520depth%250Aestimation.%2520MGNiceNet%2520is%2520based%2520on%2520the%2520state-of-the-art%2520real-time%2520panoptic%250Asegmentation%2520method%2520RT-K-Net%2520and%2520extends%2520the%2520architecture%2520to%2520cover%2520both%250Apanoptic%2520segmentation%2520and%2520self-supervised%2520monocular%2520depth%2520estimation.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520a%2520tightly%2520coupled%2520self-supervised%2520depth%2520estimation%2520predictor%250Athat%2520explicitly%2520uses%2520information%2520from%2520the%2520panoptic%2520path%2520for%2520depth%2520prediction.%250AFurthermore%252C%2520we%2520introduce%2520a%2520panoptic-guided%2520motion%2520masking%2520method%2520to%2520improve%250Adepth%2520estimation%2520without%2520relying%2520on%2520video%2520panoptic%2520segmentation%2520annotations.%2520We%250Aevaluate%2520our%2520method%2520on%2520two%2520popular%2520autonomous%2520driving%2520datasets%252C%2520Cityscapes%2520and%250AKITTI.%2520Our%2520model%2520shows%2520state-of-the-art%2520results%2520compared%2520to%2520other%2520real-time%250Amethods%2520and%2520closes%2520the%2520gap%2520to%2520computationally%2520more%2520demanding%2520methods.%2520Source%250Acode%2520and%2520trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/markusschoen/MGNiceNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGNiceNet%3A%20Unified%20Monocular%20Geometric%20Scene%20Understanding&entry.906535625=Markus%20Sch%C3%B6n%20and%20Michael%20Buchholz%20and%20Klaus%20Dietmayer&entry.1292438233=%20%20Monocular%20geometric%20scene%20understanding%20combines%20panoptic%20segmentation%20and%0Aself-supervised%20depth%20estimation%2C%20focusing%20on%20real-time%20application%20in%0Aautonomous%20vehicles.%20We%20introduce%20MGNiceNet%2C%20a%20unified%20approach%20that%20uses%20a%0Alinked%20kernel%20formulation%20for%20panoptic%20segmentation%20and%20self-supervised%20depth%0Aestimation.%20MGNiceNet%20is%20based%20on%20the%20state-of-the-art%20real-time%20panoptic%0Asegmentation%20method%20RT-K-Net%20and%20extends%20the%20architecture%20to%20cover%20both%0Apanoptic%20segmentation%20and%20self-supervised%20monocular%20depth%20estimation.%20To%20this%0Aend%2C%20we%20introduce%20a%20tightly%20coupled%20self-supervised%20depth%20estimation%20predictor%0Athat%20explicitly%20uses%20information%20from%20the%20panoptic%20path%20for%20depth%20prediction.%0AFurthermore%2C%20we%20introduce%20a%20panoptic-guided%20motion%20masking%20method%20to%20improve%0Adepth%20estimation%20without%20relying%20on%20video%20panoptic%20segmentation%20annotations.%20We%0Aevaluate%20our%20method%20on%20two%20popular%20autonomous%20driving%20datasets%2C%20Cityscapes%20and%0AKITTI.%20Our%20model%20shows%20state-of-the-art%20results%20compared%20to%20other%20real-time%0Amethods%20and%20closes%20the%20gap%20to%20computationally%20more%20demanding%20methods.%20Source%0Acode%20and%20trained%20models%20are%20available%20at%0Ahttps%3A//github.com/markusschoen/MGNiceNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11466v1&entry.124074799=Read"},
{"title": "Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in\n  the Wild", "author": "Donggyun Kim and Seongwoong Cho and Semin Kim and Chong Luo and Seunghoon Hong", "abstract": "  Large language models have evolved data-efficient generalists, benefiting\nfrom the universal language interface and large-scale pre-training. However,\nconstructing a data-efficient generalist for dense visual prediction presents a\ndistinct challenge due to the variation in label structures across different\ntasks. Consequently, generalization to unseen dense prediction tasks in the\nlow-data regime is not straightforward and has received less attention from\nprevious vision generalists. In this study, we explore a universal model that\ncan flexibly adapt to unseen dense label structures with a few examples,\nenabling it to serve as a data-efficient vision generalist in diverse\nreal-world scenarios. To this end, we base our method on a powerful\nmeta-learning framework and explore several axes to improve its performance and\nversatility for real-world problems, such as flexible adaptation mechanisms and\nscalability. We evaluate our model across a spectrum of unseen real-world\nscenarios where low-shot learning is desirable, including video, 3D, medical,\nbiological, and user-interactive tasks. Equipped with a generic architecture\nand an effective adaptation mechanism, our model flexibly adapts to all of\nthese tasks with at most 50 labeled images, showcasing a significant\nadvancement over existing data-efficient generalist approaches. Codes are\navailable at https://github.com/GitGyun/chameleon.\n", "link": "http://arxiv.org/abs/2404.18459v2", "date": "2024-11-18", "relevancy": 2.9095, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chameleon%3A%20A%20Data-Efficient%20Generalist%20for%20Dense%20Visual%20Prediction%20in%0A%20%20the%20Wild&body=Title%3A%20Chameleon%3A%20A%20Data-Efficient%20Generalist%20for%20Dense%20Visual%20Prediction%20in%0A%20%20the%20Wild%0AAuthor%3A%20Donggyun%20Kim%20and%20Seongwoong%20Cho%20and%20Semin%20Kim%20and%20Chong%20Luo%20and%20Seunghoon%20Hong%0AAbstract%3A%20%20%20Large%20language%20models%20have%20evolved%20data-efficient%20generalists%2C%20benefiting%0Afrom%20the%20universal%20language%20interface%20and%20large-scale%20pre-training.%20However%2C%0Aconstructing%20a%20data-efficient%20generalist%20for%20dense%20visual%20prediction%20presents%20a%0Adistinct%20challenge%20due%20to%20the%20variation%20in%20label%20structures%20across%20different%0Atasks.%20Consequently%2C%20generalization%20to%20unseen%20dense%20prediction%20tasks%20in%20the%0Alow-data%20regime%20is%20not%20straightforward%20and%20has%20received%20less%20attention%20from%0Aprevious%20vision%20generalists.%20In%20this%20study%2C%20we%20explore%20a%20universal%20model%20that%0Acan%20flexibly%20adapt%20to%20unseen%20dense%20label%20structures%20with%20a%20few%20examples%2C%0Aenabling%20it%20to%20serve%20as%20a%20data-efficient%20vision%20generalist%20in%20diverse%0Areal-world%20scenarios.%20To%20this%20end%2C%20we%20base%20our%20method%20on%20a%20powerful%0Ameta-learning%20framework%20and%20explore%20several%20axes%20to%20improve%20its%20performance%20and%0Aversatility%20for%20real-world%20problems%2C%20such%20as%20flexible%20adaptation%20mechanisms%20and%0Ascalability.%20We%20evaluate%20our%20model%20across%20a%20spectrum%20of%20unseen%20real-world%0Ascenarios%20where%20low-shot%20learning%20is%20desirable%2C%20including%20video%2C%203D%2C%20medical%2C%0Abiological%2C%20and%20user-interactive%20tasks.%20Equipped%20with%20a%20generic%20architecture%0Aand%20an%20effective%20adaptation%20mechanism%2C%20our%20model%20flexibly%20adapts%20to%20all%20of%0Athese%20tasks%20with%20at%20most%2050%20labeled%20images%2C%20showcasing%20a%20significant%0Aadvancement%20over%20existing%20data-efficient%20generalist%20approaches.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/GitGyun/chameleon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChameleon%253A%2520A%2520Data-Efficient%2520Generalist%2520for%2520Dense%2520Visual%2520Prediction%2520in%250A%2520%2520the%2520Wild%26entry.906535625%3DDonggyun%2520Kim%2520and%2520Seongwoong%2520Cho%2520and%2520Semin%2520Kim%2520and%2520Chong%2520Luo%2520and%2520Seunghoon%2520Hong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520evolved%2520data-efficient%2520generalists%252C%2520benefiting%250Afrom%2520the%2520universal%2520language%2520interface%2520and%2520large-scale%2520pre-training.%2520However%252C%250Aconstructing%2520a%2520data-efficient%2520generalist%2520for%2520dense%2520visual%2520prediction%2520presents%2520a%250Adistinct%2520challenge%2520due%2520to%2520the%2520variation%2520in%2520label%2520structures%2520across%2520different%250Atasks.%2520Consequently%252C%2520generalization%2520to%2520unseen%2520dense%2520prediction%2520tasks%2520in%2520the%250Alow-data%2520regime%2520is%2520not%2520straightforward%2520and%2520has%2520received%2520less%2520attention%2520from%250Aprevious%2520vision%2520generalists.%2520In%2520this%2520study%252C%2520we%2520explore%2520a%2520universal%2520model%2520that%250Acan%2520flexibly%2520adapt%2520to%2520unseen%2520dense%2520label%2520structures%2520with%2520a%2520few%2520examples%252C%250Aenabling%2520it%2520to%2520serve%2520as%2520a%2520data-efficient%2520vision%2520generalist%2520in%2520diverse%250Areal-world%2520scenarios.%2520To%2520this%2520end%252C%2520we%2520base%2520our%2520method%2520on%2520a%2520powerful%250Ameta-learning%2520framework%2520and%2520explore%2520several%2520axes%2520to%2520improve%2520its%2520performance%2520and%250Aversatility%2520for%2520real-world%2520problems%252C%2520such%2520as%2520flexible%2520adaptation%2520mechanisms%2520and%250Ascalability.%2520We%2520evaluate%2520our%2520model%2520across%2520a%2520spectrum%2520of%2520unseen%2520real-world%250Ascenarios%2520where%2520low-shot%2520learning%2520is%2520desirable%252C%2520including%2520video%252C%25203D%252C%2520medical%252C%250Abiological%252C%2520and%2520user-interactive%2520tasks.%2520Equipped%2520with%2520a%2520generic%2520architecture%250Aand%2520an%2520effective%2520adaptation%2520mechanism%252C%2520our%2520model%2520flexibly%2520adapts%2520to%2520all%2520of%250Athese%2520tasks%2520with%2520at%2520most%252050%2520labeled%2520images%252C%2520showcasing%2520a%2520significant%250Aadvancement%2520over%2520existing%2520data-efficient%2520generalist%2520approaches.%2520Codes%2520are%250Aavailable%2520at%2520https%253A//github.com/GitGyun/chameleon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chameleon%3A%20A%20Data-Efficient%20Generalist%20for%20Dense%20Visual%20Prediction%20in%0A%20%20the%20Wild&entry.906535625=Donggyun%20Kim%20and%20Seongwoong%20Cho%20and%20Semin%20Kim%20and%20Chong%20Luo%20and%20Seunghoon%20Hong&entry.1292438233=%20%20Large%20language%20models%20have%20evolved%20data-efficient%20generalists%2C%20benefiting%0Afrom%20the%20universal%20language%20interface%20and%20large-scale%20pre-training.%20However%2C%0Aconstructing%20a%20data-efficient%20generalist%20for%20dense%20visual%20prediction%20presents%20a%0Adistinct%20challenge%20due%20to%20the%20variation%20in%20label%20structures%20across%20different%0Atasks.%20Consequently%2C%20generalization%20to%20unseen%20dense%20prediction%20tasks%20in%20the%0Alow-data%20regime%20is%20not%20straightforward%20and%20has%20received%20less%20attention%20from%0Aprevious%20vision%20generalists.%20In%20this%20study%2C%20we%20explore%20a%20universal%20model%20that%0Acan%20flexibly%20adapt%20to%20unseen%20dense%20label%20structures%20with%20a%20few%20examples%2C%0Aenabling%20it%20to%20serve%20as%20a%20data-efficient%20vision%20generalist%20in%20diverse%0Areal-world%20scenarios.%20To%20this%20end%2C%20we%20base%20our%20method%20on%20a%20powerful%0Ameta-learning%20framework%20and%20explore%20several%20axes%20to%20improve%20its%20performance%20and%0Aversatility%20for%20real-world%20problems%2C%20such%20as%20flexible%20adaptation%20mechanisms%20and%0Ascalability.%20We%20evaluate%20our%20model%20across%20a%20spectrum%20of%20unseen%20real-world%0Ascenarios%20where%20low-shot%20learning%20is%20desirable%2C%20including%20video%2C%203D%2C%20medical%2C%0Abiological%2C%20and%20user-interactive%20tasks.%20Equipped%20with%20a%20generic%20architecture%0Aand%20an%20effective%20adaptation%20mechanism%2C%20our%20model%20flexibly%20adapts%20to%20all%20of%0Athese%20tasks%20with%20at%20most%2050%20labeled%20images%2C%20showcasing%20a%20significant%0Aadvancement%20over%20existing%20data-efficient%20generalist%20approaches.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/GitGyun/chameleon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18459v2&entry.124074799=Read"},
{"title": "Exploring Emerging Trends and Research Opportunities in Visual Place\n  Recognition", "author": "Antonios Gasteratos and Konstantinos A. Tsintotas and Tobias Fischer and Yiannis Aloimonos and Michael Milford", "abstract": "  Visual-based recognition, e.g., image classification, object detection, etc.,\nis a long-standing challenge in computer vision and robotics communities.\nConcerning the roboticists, since the knowledge of the environment is a\nprerequisite for complex navigation tasks, visual place recognition is vital\nfor most localization implementations or re-localization and loop closure\ndetection pipelines within simultaneous localization and mapping (SLAM). More\nspecifically, it corresponds to the system's ability to identify and match a\npreviously visited location using computer vision tools. Towards developing\nnovel techniques with enhanced accuracy and robustness, while motivated by the\nsuccess presented in natural language processing methods, researchers have\nrecently turned their attention to vision-language models, which integrate\nvisual and textual data.\n", "link": "http://arxiv.org/abs/2411.11481v1", "date": "2024-11-18", "relevancy": 2.8977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Emerging%20Trends%20and%20Research%20Opportunities%20in%20Visual%20Place%0A%20%20Recognition&body=Title%3A%20Exploring%20Emerging%20Trends%20and%20Research%20Opportunities%20in%20Visual%20Place%0A%20%20Recognition%0AAuthor%3A%20Antonios%20Gasteratos%20and%20Konstantinos%20A.%20Tsintotas%20and%20Tobias%20Fischer%20and%20Yiannis%20Aloimonos%20and%20Michael%20Milford%0AAbstract%3A%20%20%20Visual-based%20recognition%2C%20e.g.%2C%20image%20classification%2C%20object%20detection%2C%20etc.%2C%0Ais%20a%20long-standing%20challenge%20in%20computer%20vision%20and%20robotics%20communities.%0AConcerning%20the%20roboticists%2C%20since%20the%20knowledge%20of%20the%20environment%20is%20a%0Aprerequisite%20for%20complex%20navigation%20tasks%2C%20visual%20place%20recognition%20is%20vital%0Afor%20most%20localization%20implementations%20or%20re-localization%20and%20loop%20closure%0Adetection%20pipelines%20within%20simultaneous%20localization%20and%20mapping%20%28SLAM%29.%20More%0Aspecifically%2C%20it%20corresponds%20to%20the%20system%27s%20ability%20to%20identify%20and%20match%20a%0Apreviously%20visited%20location%20using%20computer%20vision%20tools.%20Towards%20developing%0Anovel%20techniques%20with%20enhanced%20accuracy%20and%20robustness%2C%20while%20motivated%20by%20the%0Asuccess%20presented%20in%20natural%20language%20processing%20methods%2C%20researchers%20have%0Arecently%20turned%20their%20attention%20to%20vision-language%20models%2C%20which%20integrate%0Avisual%20and%20textual%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Emerging%2520Trends%2520and%2520Research%2520Opportunities%2520in%2520Visual%2520Place%250A%2520%2520Recognition%26entry.906535625%3DAntonios%2520Gasteratos%2520and%2520Konstantinos%2520A.%2520Tsintotas%2520and%2520Tobias%2520Fischer%2520and%2520Yiannis%2520Aloimonos%2520and%2520Michael%2520Milford%26entry.1292438233%3D%2520%2520Visual-based%2520recognition%252C%2520e.g.%252C%2520image%2520classification%252C%2520object%2520detection%252C%2520etc.%252C%250Ais%2520a%2520long-standing%2520challenge%2520in%2520computer%2520vision%2520and%2520robotics%2520communities.%250AConcerning%2520the%2520roboticists%252C%2520since%2520the%2520knowledge%2520of%2520the%2520environment%2520is%2520a%250Aprerequisite%2520for%2520complex%2520navigation%2520tasks%252C%2520visual%2520place%2520recognition%2520is%2520vital%250Afor%2520most%2520localization%2520implementations%2520or%2520re-localization%2520and%2520loop%2520closure%250Adetection%2520pipelines%2520within%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529.%2520More%250Aspecifically%252C%2520it%2520corresponds%2520to%2520the%2520system%2527s%2520ability%2520to%2520identify%2520and%2520match%2520a%250Apreviously%2520visited%2520location%2520using%2520computer%2520vision%2520tools.%2520Towards%2520developing%250Anovel%2520techniques%2520with%2520enhanced%2520accuracy%2520and%2520robustness%252C%2520while%2520motivated%2520by%2520the%250Asuccess%2520presented%2520in%2520natural%2520language%2520processing%2520methods%252C%2520researchers%2520have%250Arecently%2520turned%2520their%2520attention%2520to%2520vision-language%2520models%252C%2520which%2520integrate%250Avisual%2520and%2520textual%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Emerging%20Trends%20and%20Research%20Opportunities%20in%20Visual%20Place%0A%20%20Recognition&entry.906535625=Antonios%20Gasteratos%20and%20Konstantinos%20A.%20Tsintotas%20and%20Tobias%20Fischer%20and%20Yiannis%20Aloimonos%20and%20Michael%20Milford&entry.1292438233=%20%20Visual-based%20recognition%2C%20e.g.%2C%20image%20classification%2C%20object%20detection%2C%20etc.%2C%0Ais%20a%20long-standing%20challenge%20in%20computer%20vision%20and%20robotics%20communities.%0AConcerning%20the%20roboticists%2C%20since%20the%20knowledge%20of%20the%20environment%20is%20a%0Aprerequisite%20for%20complex%20navigation%20tasks%2C%20visual%20place%20recognition%20is%20vital%0Afor%20most%20localization%20implementations%20or%20re-localization%20and%20loop%20closure%0Adetection%20pipelines%20within%20simultaneous%20localization%20and%20mapping%20%28SLAM%29.%20More%0Aspecifically%2C%20it%20corresponds%20to%20the%20system%27s%20ability%20to%20identify%20and%20match%20a%0Apreviously%20visited%20location%20using%20computer%20vision%20tools.%20Towards%20developing%0Anovel%20techniques%20with%20enhanced%20accuracy%20and%20robustness%2C%20while%20motivated%20by%20the%0Asuccess%20presented%20in%20natural%20language%20processing%20methods%2C%20researchers%20have%0Arecently%20turned%20their%20attention%20to%20vision-language%20models%2C%20which%20integrate%0Avisual%20and%20textual%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11481v1&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment", "author": "Zhendong Liu and Yuanbi Nie and Yingshui Tan and Xiangyu Yue and Qiushi Cui and Chongjun Wang and Xiaoyong Zhu and Bo Zheng", "abstract": "  Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.\n", "link": "http://arxiv.org/abs/2411.11543v1", "date": "2024-11-18", "relevancy": 2.8443, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&body=Title%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment%0AAuthor%3A%20Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Model%2520Safety%2520through%2520Progressive%250A%2520%2520Concept-Bottleneck-Driven%2520Alignment%26entry.906535625%3DZhendong%2520Liu%2520and%2520Yuanbi%2520Nie%2520and%2520Yingshui%2520Tan%2520and%2520Xiangyu%2520Yue%2520and%2520Qiushi%2520Cui%2520and%2520Chongjun%2520Wang%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520the%2520powerful%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Apre-trained%2520visual%2520encoder%2520models%2520connected%2520to%2520LLMs%2520form%2520Vision%2520Language%2520Models%250A%2528VLMs%2529.%2520However%252C%2520recent%2520research%2520shows%2520that%2520the%2520visual%2520modality%2520in%2520VLMs%2520is%250Ahighly%2520vulnerable%252C%2520allowing%2520attackers%2520to%2520bypass%2520safety%2520alignment%2520in%2520LLMs%250Athrough%2520visually%2520transmitted%2520content%252C%2520launching%2520harmful%2520attacks.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520progressive%2520concept-based%2520alignment%2520strategy%252C%250APSA-VLM%252C%2520which%2520incorporates%2520safety%2520modules%2520as%2520concept%2520bottlenecks%2520to%2520enhance%250Avisual%2520modality%2520safety%2520alignment.%2520By%2520aligning%2520model%2520predictions%2520with%2520specific%250Asafety%2520concepts%252C%2520we%2520improve%2520defenses%2520against%2520risky%2520images%252C%2520enhancing%250Aexplainability%2520and%2520controllability%2520while%2520minimally%2520impacting%2520general%250Aperformance.%2520Our%2520method%2520is%2520obtained%2520through%2520two-stage%2520training.%2520The%2520low%250Acomputational%2520cost%2520of%2520the%2520first%2520stage%2520brings%2520very%2520effective%2520performance%250Aimprovement%252C%2520and%2520the%2520fine-tuning%2520of%2520the%2520language%2520model%2520in%2520the%2520second%2520stage%250Afurther%2520improves%2520the%2520safety%2520performance.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aresults%2520on%2520popular%2520VLM%2520safety%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&entry.906535625=Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11543v1&entry.124074799=Read"},
{"title": "Generalizable Person Re-identification via Balancing Alignment and\n  Uniformity", "author": "Yoonki Cho and Jaeyoon Kim and Woo Jae Kim and Junsik Jung and Sung-eui Yoon", "abstract": "  Domain generalizable person re-identification (DG re-ID) aims to learn\ndiscriminative representations that are robust to distributional shifts. While\ndata augmentation is a straightforward solution to improve generalization,\ncertain augmentations exhibit a polarized effect in this task, enhancing\nin-distribution performance while deteriorating out-of-distribution\nperformance. In this paper, we investigate this phenomenon and reveal that it\nleads to sparse representation spaces with reduced uniformity. To address this\nissue, we propose a novel framework, Balancing Alignment and Uniformity (BAU),\nwhich effectively mitigates this effect by maintaining a balance between\nalignment and uniformity. Specifically, BAU incorporates alignment and\nuniformity losses applied to both original and augmented images and integrates\na weighting strategy to assess the reliability of augmented samples, further\nimproving the alignment loss. Additionally, we introduce a domain-specific\nuniformity loss that promotes uniformity within each source domain, thereby\nenhancing the learning of domain-invariant features. Extensive experimental\nresults demonstrate that BAU effectively exploits the advantages of data\naugmentation, which previous studies could not fully utilize, and achieves\nstate-of-the-art performance without requiring complex training procedures. The\ncode is available at \\url{https://github.com/yoonkicho/BAU}.\n", "link": "http://arxiv.org/abs/2411.11471v1", "date": "2024-11-18", "relevancy": 2.7586, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.556}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5514}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Person%20Re-identification%20via%20Balancing%20Alignment%20and%0A%20%20Uniformity&body=Title%3A%20Generalizable%20Person%20Re-identification%20via%20Balancing%20Alignment%20and%0A%20%20Uniformity%0AAuthor%3A%20Yoonki%20Cho%20and%20Jaeyoon%20Kim%20and%20Woo%20Jae%20Kim%20and%20Junsik%20Jung%20and%20Sung-eui%20Yoon%0AAbstract%3A%20%20%20Domain%20generalizable%20person%20re-identification%20%28DG%20re-ID%29%20aims%20to%20learn%0Adiscriminative%20representations%20that%20are%20robust%20to%20distributional%20shifts.%20While%0Adata%20augmentation%20is%20a%20straightforward%20solution%20to%20improve%20generalization%2C%0Acertain%20augmentations%20exhibit%20a%20polarized%20effect%20in%20this%20task%2C%20enhancing%0Ain-distribution%20performance%20while%20deteriorating%20out-of-distribution%0Aperformance.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20and%20reveal%20that%20it%0Aleads%20to%20sparse%20representation%20spaces%20with%20reduced%20uniformity.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20framework%2C%20Balancing%20Alignment%20and%20Uniformity%20%28BAU%29%2C%0Awhich%20effectively%20mitigates%20this%20effect%20by%20maintaining%20a%20balance%20between%0Aalignment%20and%20uniformity.%20Specifically%2C%20BAU%20incorporates%20alignment%20and%0Auniformity%20losses%20applied%20to%20both%20original%20and%20augmented%20images%20and%20integrates%0Aa%20weighting%20strategy%20to%20assess%20the%20reliability%20of%20augmented%20samples%2C%20further%0Aimproving%20the%20alignment%20loss.%20Additionally%2C%20we%20introduce%20a%20domain-specific%0Auniformity%20loss%20that%20promotes%20uniformity%20within%20each%20source%20domain%2C%20thereby%0Aenhancing%20the%20learning%20of%20domain-invariant%20features.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20BAU%20effectively%20exploits%20the%20advantages%20of%20data%0Aaugmentation%2C%20which%20previous%20studies%20could%20not%20fully%20utilize%2C%20and%20achieves%0Astate-of-the-art%20performance%20without%20requiring%20complex%20training%20procedures.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/yoonkicho/BAU%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Person%2520Re-identification%2520via%2520Balancing%2520Alignment%2520and%250A%2520%2520Uniformity%26entry.906535625%3DYoonki%2520Cho%2520and%2520Jaeyoon%2520Kim%2520and%2520Woo%2520Jae%2520Kim%2520and%2520Junsik%2520Jung%2520and%2520Sung-eui%2520Yoon%26entry.1292438233%3D%2520%2520Domain%2520generalizable%2520person%2520re-identification%2520%2528DG%2520re-ID%2529%2520aims%2520to%2520learn%250Adiscriminative%2520representations%2520that%2520are%2520robust%2520to%2520distributional%2520shifts.%2520While%250Adata%2520augmentation%2520is%2520a%2520straightforward%2520solution%2520to%2520improve%2520generalization%252C%250Acertain%2520augmentations%2520exhibit%2520a%2520polarized%2520effect%2520in%2520this%2520task%252C%2520enhancing%250Ain-distribution%2520performance%2520while%2520deteriorating%2520out-of-distribution%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520phenomenon%2520and%2520reveal%2520that%2520it%250Aleads%2520to%2520sparse%2520representation%2520spaces%2520with%2520reduced%2520uniformity.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520Balancing%2520Alignment%2520and%2520Uniformity%2520%2528BAU%2529%252C%250Awhich%2520effectively%2520mitigates%2520this%2520effect%2520by%2520maintaining%2520a%2520balance%2520between%250Aalignment%2520and%2520uniformity.%2520Specifically%252C%2520BAU%2520incorporates%2520alignment%2520and%250Auniformity%2520losses%2520applied%2520to%2520both%2520original%2520and%2520augmented%2520images%2520and%2520integrates%250Aa%2520weighting%2520strategy%2520to%2520assess%2520the%2520reliability%2520of%2520augmented%2520samples%252C%2520further%250Aimproving%2520the%2520alignment%2520loss.%2520Additionally%252C%2520we%2520introduce%2520a%2520domain-specific%250Auniformity%2520loss%2520that%2520promotes%2520uniformity%2520within%2520each%2520source%2520domain%252C%2520thereby%250Aenhancing%2520the%2520learning%2520of%2520domain-invariant%2520features.%2520Extensive%2520experimental%250Aresults%2520demonstrate%2520that%2520BAU%2520effectively%2520exploits%2520the%2520advantages%2520of%2520data%250Aaugmentation%252C%2520which%2520previous%2520studies%2520could%2520not%2520fully%2520utilize%252C%2520and%2520achieves%250Astate-of-the-art%2520performance%2520without%2520requiring%2520complex%2520training%2520procedures.%2520The%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/yoonkicho/BAU%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Person%20Re-identification%20via%20Balancing%20Alignment%20and%0A%20%20Uniformity&entry.906535625=Yoonki%20Cho%20and%20Jaeyoon%20Kim%20and%20Woo%20Jae%20Kim%20and%20Junsik%20Jung%20and%20Sung-eui%20Yoon&entry.1292438233=%20%20Domain%20generalizable%20person%20re-identification%20%28DG%20re-ID%29%20aims%20to%20learn%0Adiscriminative%20representations%20that%20are%20robust%20to%20distributional%20shifts.%20While%0Adata%20augmentation%20is%20a%20straightforward%20solution%20to%20improve%20generalization%2C%0Acertain%20augmentations%20exhibit%20a%20polarized%20effect%20in%20this%20task%2C%20enhancing%0Ain-distribution%20performance%20while%20deteriorating%20out-of-distribution%0Aperformance.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20and%20reveal%20that%20it%0Aleads%20to%20sparse%20representation%20spaces%20with%20reduced%20uniformity.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20framework%2C%20Balancing%20Alignment%20and%20Uniformity%20%28BAU%29%2C%0Awhich%20effectively%20mitigates%20this%20effect%20by%20maintaining%20a%20balance%20between%0Aalignment%20and%20uniformity.%20Specifically%2C%20BAU%20incorporates%20alignment%20and%0Auniformity%20losses%20applied%20to%20both%20original%20and%20augmented%20images%20and%20integrates%0Aa%20weighting%20strategy%20to%20assess%20the%20reliability%20of%20augmented%20samples%2C%20further%0Aimproving%20the%20alignment%20loss.%20Additionally%2C%20we%20introduce%20a%20domain-specific%0Auniformity%20loss%20that%20promotes%20uniformity%20within%20each%20source%20domain%2C%20thereby%0Aenhancing%20the%20learning%20of%20domain-invariant%20features.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20BAU%20effectively%20exploits%20the%20advantages%20of%20data%0Aaugmentation%2C%20which%20previous%20studies%20could%20not%20fully%20utilize%2C%20and%20achieves%0Astate-of-the-art%20performance%20without%20requiring%20complex%20training%20procedures.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/yoonkicho/BAU%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11471v1&entry.124074799=Read"},
{"title": "Search, Verify and Feedback: Towards Next Generation Post-training\n  Paradigm of Foundation Models via Verifier Engineering", "author": "Xinyan Guan and Yanjiang Liu and Xinyu Lu and Boxi Cao and Ben He and Xianpei Han and Le Sun and Jie Lou and Bowen Yu and Yaojie Lu and Hongyu Lin", "abstract": "  The evolution of machine learning has increasingly prioritized the\ndevelopment of powerful models and more scalable supervision signals. However,\nthe emergence of foundation models presents significant challenges in providing\neffective supervision signals necessary for further enhancing their\ncapabilities. Consequently, there is an urgent need to explore novel\nsupervision signals and technical approaches. In this paper, we propose\nverifier engineering, a novel post-training paradigm specifically designed for\nthe era of foundation models. The core of verifier engineering involves\nleveraging a suite of automated verifiers to perform verification tasks and\ndeliver meaningful feedback to foundation models. We systematically categorize\nthe verifier engineering process into three essential stages: search, verify,\nand feedback, and provide a comprehensive review of state-of-the-art research\ndevelopments within each stage. We believe that verifier engineering\nconstitutes a fundamental pathway toward achieving Artificial General\nIntelligence.\n", "link": "http://arxiv.org/abs/2411.11504v1", "date": "2024-11-18", "relevancy": 2.7492, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search%2C%20Verify%20and%20Feedback%3A%20Towards%20Next%20Generation%20Post-training%0A%20%20Paradigm%20of%20Foundation%20Models%20via%20Verifier%20Engineering&body=Title%3A%20Search%2C%20Verify%20and%20Feedback%3A%20Towards%20Next%20Generation%20Post-training%0A%20%20Paradigm%20of%20Foundation%20Models%20via%20Verifier%20Engineering%0AAuthor%3A%20Xinyan%20Guan%20and%20Yanjiang%20Liu%20and%20Xinyu%20Lu%20and%20Boxi%20Cao%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Jie%20Lou%20and%20Bowen%20Yu%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin%0AAbstract%3A%20%20%20The%20evolution%20of%20machine%20learning%20has%20increasingly%20prioritized%20the%0Adevelopment%20of%20powerful%20models%20and%20more%20scalable%20supervision%20signals.%20However%2C%0Athe%20emergence%20of%20foundation%20models%20presents%20significant%20challenges%20in%20providing%0Aeffective%20supervision%20signals%20necessary%20for%20further%20enhancing%20their%0Acapabilities.%20Consequently%2C%20there%20is%20an%20urgent%20need%20to%20explore%20novel%0Asupervision%20signals%20and%20technical%20approaches.%20In%20this%20paper%2C%20we%20propose%0Averifier%20engineering%2C%20a%20novel%20post-training%20paradigm%20specifically%20designed%20for%0Athe%20era%20of%20foundation%20models.%20The%20core%20of%20verifier%20engineering%20involves%0Aleveraging%20a%20suite%20of%20automated%20verifiers%20to%20perform%20verification%20tasks%20and%0Adeliver%20meaningful%20feedback%20to%20foundation%20models.%20We%20systematically%20categorize%0Athe%20verifier%20engineering%20process%20into%20three%20essential%20stages%3A%20search%2C%20verify%2C%0Aand%20feedback%2C%20and%20provide%20a%20comprehensive%20review%20of%20state-of-the-art%20research%0Adevelopments%20within%20each%20stage.%20We%20believe%20that%20verifier%20engineering%0Aconstitutes%20a%20fundamental%20pathway%20toward%20achieving%20Artificial%20General%0AIntelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch%252C%2520Verify%2520and%2520Feedback%253A%2520Towards%2520Next%2520Generation%2520Post-training%250A%2520%2520Paradigm%2520of%2520Foundation%2520Models%2520via%2520Verifier%2520Engineering%26entry.906535625%3DXinyan%2520Guan%2520and%2520Yanjiang%2520Liu%2520and%2520Xinyu%2520Lu%2520and%2520Boxi%2520Cao%2520and%2520Ben%2520He%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%2520and%2520Jie%2520Lou%2520and%2520Bowen%2520Yu%2520and%2520Yaojie%2520Lu%2520and%2520Hongyu%2520Lin%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520machine%2520learning%2520has%2520increasingly%2520prioritized%2520the%250Adevelopment%2520of%2520powerful%2520models%2520and%2520more%2520scalable%2520supervision%2520signals.%2520However%252C%250Athe%2520emergence%2520of%2520foundation%2520models%2520presents%2520significant%2520challenges%2520in%2520providing%250Aeffective%2520supervision%2520signals%2520necessary%2520for%2520further%2520enhancing%2520their%250Acapabilities.%2520Consequently%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%2520explore%2520novel%250Asupervision%2520signals%2520and%2520technical%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520propose%250Averifier%2520engineering%252C%2520a%2520novel%2520post-training%2520paradigm%2520specifically%2520designed%2520for%250Athe%2520era%2520of%2520foundation%2520models.%2520The%2520core%2520of%2520verifier%2520engineering%2520involves%250Aleveraging%2520a%2520suite%2520of%2520automated%2520verifiers%2520to%2520perform%2520verification%2520tasks%2520and%250Adeliver%2520meaningful%2520feedback%2520to%2520foundation%2520models.%2520We%2520systematically%2520categorize%250Athe%2520verifier%2520engineering%2520process%2520into%2520three%2520essential%2520stages%253A%2520search%252C%2520verify%252C%250Aand%2520feedback%252C%2520and%2520provide%2520a%2520comprehensive%2520review%2520of%2520state-of-the-art%2520research%250Adevelopments%2520within%2520each%2520stage.%2520We%2520believe%2520that%2520verifier%2520engineering%250Aconstitutes%2520a%2520fundamental%2520pathway%2520toward%2520achieving%2520Artificial%2520General%250AIntelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search%2C%20Verify%20and%20Feedback%3A%20Towards%20Next%20Generation%20Post-training%0A%20%20Paradigm%20of%20Foundation%20Models%20via%20Verifier%20Engineering&entry.906535625=Xinyan%20Guan%20and%20Yanjiang%20Liu%20and%20Xinyu%20Lu%20and%20Boxi%20Cao%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Jie%20Lou%20and%20Bowen%20Yu%20and%20Yaojie%20Lu%20and%20Hongyu%20Lin&entry.1292438233=%20%20The%20evolution%20of%20machine%20learning%20has%20increasingly%20prioritized%20the%0Adevelopment%20of%20powerful%20models%20and%20more%20scalable%20supervision%20signals.%20However%2C%0Athe%20emergence%20of%20foundation%20models%20presents%20significant%20challenges%20in%20providing%0Aeffective%20supervision%20signals%20necessary%20for%20further%20enhancing%20their%0Acapabilities.%20Consequently%2C%20there%20is%20an%20urgent%20need%20to%20explore%20novel%0Asupervision%20signals%20and%20technical%20approaches.%20In%20this%20paper%2C%20we%20propose%0Averifier%20engineering%2C%20a%20novel%20post-training%20paradigm%20specifically%20designed%20for%0Athe%20era%20of%20foundation%20models.%20The%20core%20of%20verifier%20engineering%20involves%0Aleveraging%20a%20suite%20of%20automated%20verifiers%20to%20perform%20verification%20tasks%20and%0Adeliver%20meaningful%20feedback%20to%20foundation%20models.%20We%20systematically%20categorize%0Athe%20verifier%20engineering%20process%20into%20three%20essential%20stages%3A%20search%2C%20verify%2C%0Aand%20feedback%2C%20and%20provide%20a%20comprehensive%20review%20of%20state-of-the-art%20research%0Adevelopments%20within%20each%20stage.%20We%20believe%20that%20verifier%20engineering%0Aconstitutes%20a%20fundamental%20pathway%20toward%20achieving%20Artificial%20General%0AIntelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11504v1&entry.124074799=Read"},
{"title": "Exploring Context Window of Large Language Models via Decomposed\n  Positional Vectors", "author": "Zican Dong and Junyi Li and Xin Men and Wayne Xin Zhao and Bingbing Wang and Zhen Tian and Weipeng Chen and Ji-Rong Wen", "abstract": "  Transformer-based large language models (LLMs) typically have a limited\ncontext window, resulting in significant performance degradation when\nprocessing text beyond the length of the context window. Extensive studies have\nbeen proposed to extend the context window and achieve length extrapolation of\nLLMs, but there is still a lack of in-depth interpretation of these approaches.\nIn this study, we explore the positional information within and beyond the\ncontext window for deciphering the underlying mechanism of LLMs. By using a\nmean-based decomposition method, we disentangle positional vectors from hidden\nstates of LLMs and analyze their formation and effect on attention.\nFurthermore, when texts exceed the context window, we analyze the change of\npositional vectors in two settings, i.e., direct extrapolation and context\nwindow extension. Based on our findings, we design two training-free context\nwindow extension methods, positional vector replacement and attention window\nextension. Experimental results show that our methods can effectively extend\nthe context window length.\n", "link": "http://arxiv.org/abs/2405.18009v2", "date": "2024-11-18", "relevancy": 2.742, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Context%20Window%20of%20Large%20Language%20Models%20via%20Decomposed%0A%20%20Positional%20Vectors&body=Title%3A%20Exploring%20Context%20Window%20of%20Large%20Language%20Models%20via%20Decomposed%0A%20%20Positional%20Vectors%0AAuthor%3A%20Zican%20Dong%20and%20Junyi%20Li%20and%20Xin%20Men%20and%20Wayne%20Xin%20Zhao%20and%20Bingbing%20Wang%20and%20Zhen%20Tian%20and%20Weipeng%20Chen%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20typically%20have%20a%20limited%0Acontext%20window%2C%20resulting%20in%20significant%20performance%20degradation%20when%0Aprocessing%20text%20beyond%20the%20length%20of%20the%20context%20window.%20Extensive%20studies%20have%0Abeen%20proposed%20to%20extend%20the%20context%20window%20and%20achieve%20length%20extrapolation%20of%0ALLMs%2C%20but%20there%20is%20still%20a%20lack%20of%20in-depth%20interpretation%20of%20these%20approaches.%0AIn%20this%20study%2C%20we%20explore%20the%20positional%20information%20within%20and%20beyond%20the%0Acontext%20window%20for%20deciphering%20the%20underlying%20mechanism%20of%20LLMs.%20By%20using%20a%0Amean-based%20decomposition%20method%2C%20we%20disentangle%20positional%20vectors%20from%20hidden%0Astates%20of%20LLMs%20and%20analyze%20their%20formation%20and%20effect%20on%20attention.%0AFurthermore%2C%20when%20texts%20exceed%20the%20context%20window%2C%20we%20analyze%20the%20change%20of%0Apositional%20vectors%20in%20two%20settings%2C%20i.e.%2C%20direct%20extrapolation%20and%20context%0Awindow%20extension.%20Based%20on%20our%20findings%2C%20we%20design%20two%20training-free%20context%0Awindow%20extension%20methods%2C%20positional%20vector%20replacement%20and%20attention%20window%0Aextension.%20Experimental%20results%20show%20that%20our%20methods%20can%20effectively%20extend%0Athe%20context%20window%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Context%2520Window%2520of%2520Large%2520Language%2520Models%2520via%2520Decomposed%250A%2520%2520Positional%2520Vectors%26entry.906535625%3DZican%2520Dong%2520and%2520Junyi%2520Li%2520and%2520Xin%2520Men%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Bingbing%2520Wang%2520and%2520Zhen%2520Tian%2520and%2520Weipeng%2520Chen%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520typically%2520have%2520a%2520limited%250Acontext%2520window%252C%2520resulting%2520in%2520significant%2520performance%2520degradation%2520when%250Aprocessing%2520text%2520beyond%2520the%2520length%2520of%2520the%2520context%2520window.%2520Extensive%2520studies%2520have%250Abeen%2520proposed%2520to%2520extend%2520the%2520context%2520window%2520and%2520achieve%2520length%2520extrapolation%2520of%250ALLMs%252C%2520but%2520there%2520is%2520still%2520a%2520lack%2520of%2520in-depth%2520interpretation%2520of%2520these%2520approaches.%250AIn%2520this%2520study%252C%2520we%2520explore%2520the%2520positional%2520information%2520within%2520and%2520beyond%2520the%250Acontext%2520window%2520for%2520deciphering%2520the%2520underlying%2520mechanism%2520of%2520LLMs.%2520By%2520using%2520a%250Amean-based%2520decomposition%2520method%252C%2520we%2520disentangle%2520positional%2520vectors%2520from%2520hidden%250Astates%2520of%2520LLMs%2520and%2520analyze%2520their%2520formation%2520and%2520effect%2520on%2520attention.%250AFurthermore%252C%2520when%2520texts%2520exceed%2520the%2520context%2520window%252C%2520we%2520analyze%2520the%2520change%2520of%250Apositional%2520vectors%2520in%2520two%2520settings%252C%2520i.e.%252C%2520direct%2520extrapolation%2520and%2520context%250Awindow%2520extension.%2520Based%2520on%2520our%2520findings%252C%2520we%2520design%2520two%2520training-free%2520context%250Awindow%2520extension%2520methods%252C%2520positional%2520vector%2520replacement%2520and%2520attention%2520window%250Aextension.%2520Experimental%2520results%2520show%2520that%2520our%2520methods%2520can%2520effectively%2520extend%250Athe%2520context%2520window%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Context%20Window%20of%20Large%20Language%20Models%20via%20Decomposed%0A%20%20Positional%20Vectors&entry.906535625=Zican%20Dong%20and%20Junyi%20Li%20and%20Xin%20Men%20and%20Wayne%20Xin%20Zhao%20and%20Bingbing%20Wang%20and%20Zhen%20Tian%20and%20Weipeng%20Chen%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20typically%20have%20a%20limited%0Acontext%20window%2C%20resulting%20in%20significant%20performance%20degradation%20when%0Aprocessing%20text%20beyond%20the%20length%20of%20the%20context%20window.%20Extensive%20studies%20have%0Abeen%20proposed%20to%20extend%20the%20context%20window%20and%20achieve%20length%20extrapolation%20of%0ALLMs%2C%20but%20there%20is%20still%20a%20lack%20of%20in-depth%20interpretation%20of%20these%20approaches.%0AIn%20this%20study%2C%20we%20explore%20the%20positional%20information%20within%20and%20beyond%20the%0Acontext%20window%20for%20deciphering%20the%20underlying%20mechanism%20of%20LLMs.%20By%20using%20a%0Amean-based%20decomposition%20method%2C%20we%20disentangle%20positional%20vectors%20from%20hidden%0Astates%20of%20LLMs%20and%20analyze%20their%20formation%20and%20effect%20on%20attention.%0AFurthermore%2C%20when%20texts%20exceed%20the%20context%20window%2C%20we%20analyze%20the%20change%20of%0Apositional%20vectors%20in%20two%20settings%2C%20i.e.%2C%20direct%20extrapolation%20and%20context%0Awindow%20extension.%20Based%20on%20our%20findings%2C%20we%20design%20two%20training-free%20context%0Awindow%20extension%20methods%2C%20positional%20vector%20replacement%20and%20attention%20window%0Aextension.%20Experimental%20results%20show%20that%20our%20methods%20can%20effectively%20extend%0Athe%20context%20window%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18009v2&entry.124074799=Read"},
{"title": "Towards Degradation-Robust Reconstruction in Generalizable NeRF", "author": "Chan Ho Park and Ka Leong Cheng and Zhicheng Wang and Qifeng Chen", "abstract": "  Generalizable Neural Radiance Field (GNeRF) across scenes has been proven to\nbe an effective way to avoid per-scene optimization by representing a scene\nwith deep image features of source images. However, despite its potential for\nreal-world applications, there has been limited research on the robustness of\nGNeRFs to different types of degradation present in the source images. The lack\nof such research is primarily attributed to the absence of a large-scale\ndataset fit for training a degradation-robust generalizable NeRF model. To\naddress this gap and facilitate investigations into the degradation robustness\nof 3D reconstruction tasks, we construct the Objaverse Blur Dataset, comprising\n50,000 images from over 1000 settings featuring multiple levels of blur\ndegradation. In addition, we design a simple and model-agnostic module for\nenhancing the degradation robustness of GNeRFs. Specifically, by extracting\n3D-aware features through a lightweight depth estimator and denoiser, the\nproposed module shows improvement on different popular methods in GNeRFs in\nterms of both quantitative and visual quality over varying degradation types\nand levels. Our dataset and code will be made publicly available.\n", "link": "http://arxiv.org/abs/2411.11691v1", "date": "2024-11-18", "relevancy": 2.7224, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.569}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5385}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Degradation-Robust%20Reconstruction%20in%20Generalizable%20NeRF&body=Title%3A%20Towards%20Degradation-Robust%20Reconstruction%20in%20Generalizable%20NeRF%0AAuthor%3A%20Chan%20Ho%20Park%20and%20Ka%20Leong%20Cheng%20and%20Zhicheng%20Wang%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Generalizable%20Neural%20Radiance%20Field%20%28GNeRF%29%20across%20scenes%20has%20been%20proven%20to%0Abe%20an%20effective%20way%20to%20avoid%20per-scene%20optimization%20by%20representing%20a%20scene%0Awith%20deep%20image%20features%20of%20source%20images.%20However%2C%20despite%20its%20potential%20for%0Areal-world%20applications%2C%20there%20has%20been%20limited%20research%20on%20the%20robustness%20of%0AGNeRFs%20to%20different%20types%20of%20degradation%20present%20in%20the%20source%20images.%20The%20lack%0Aof%20such%20research%20is%20primarily%20attributed%20to%20the%20absence%20of%20a%20large-scale%0Adataset%20fit%20for%20training%20a%20degradation-robust%20generalizable%20NeRF%20model.%20To%0Aaddress%20this%20gap%20and%20facilitate%20investigations%20into%20the%20degradation%20robustness%0Aof%203D%20reconstruction%20tasks%2C%20we%20construct%20the%20Objaverse%20Blur%20Dataset%2C%20comprising%0A50%2C000%20images%20from%20over%201000%20settings%20featuring%20multiple%20levels%20of%20blur%0Adegradation.%20In%20addition%2C%20we%20design%20a%20simple%20and%20model-agnostic%20module%20for%0Aenhancing%20the%20degradation%20robustness%20of%20GNeRFs.%20Specifically%2C%20by%20extracting%0A3D-aware%20features%20through%20a%20lightweight%20depth%20estimator%20and%20denoiser%2C%20the%0Aproposed%20module%20shows%20improvement%20on%20different%20popular%20methods%20in%20GNeRFs%20in%0Aterms%20of%20both%20quantitative%20and%20visual%20quality%20over%20varying%20degradation%20types%0Aand%20levels.%20Our%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Degradation-Robust%2520Reconstruction%2520in%2520Generalizable%2520NeRF%26entry.906535625%3DChan%2520Ho%2520Park%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Zhicheng%2520Wang%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Generalizable%2520Neural%2520Radiance%2520Field%2520%2528GNeRF%2529%2520across%2520scenes%2520has%2520been%2520proven%2520to%250Abe%2520an%2520effective%2520way%2520to%2520avoid%2520per-scene%2520optimization%2520by%2520representing%2520a%2520scene%250Awith%2520deep%2520image%2520features%2520of%2520source%2520images.%2520However%252C%2520despite%2520its%2520potential%2520for%250Areal-world%2520applications%252C%2520there%2520has%2520been%2520limited%2520research%2520on%2520the%2520robustness%2520of%250AGNeRFs%2520to%2520different%2520types%2520of%2520degradation%2520present%2520in%2520the%2520source%2520images.%2520The%2520lack%250Aof%2520such%2520research%2520is%2520primarily%2520attributed%2520to%2520the%2520absence%2520of%2520a%2520large-scale%250Adataset%2520fit%2520for%2520training%2520a%2520degradation-robust%2520generalizable%2520NeRF%2520model.%2520To%250Aaddress%2520this%2520gap%2520and%2520facilitate%2520investigations%2520into%2520the%2520degradation%2520robustness%250Aof%25203D%2520reconstruction%2520tasks%252C%2520we%2520construct%2520the%2520Objaverse%2520Blur%2520Dataset%252C%2520comprising%250A50%252C000%2520images%2520from%2520over%25201000%2520settings%2520featuring%2520multiple%2520levels%2520of%2520blur%250Adegradation.%2520In%2520addition%252C%2520we%2520design%2520a%2520simple%2520and%2520model-agnostic%2520module%2520for%250Aenhancing%2520the%2520degradation%2520robustness%2520of%2520GNeRFs.%2520Specifically%252C%2520by%2520extracting%250A3D-aware%2520features%2520through%2520a%2520lightweight%2520depth%2520estimator%2520and%2520denoiser%252C%2520the%250Aproposed%2520module%2520shows%2520improvement%2520on%2520different%2520popular%2520methods%2520in%2520GNeRFs%2520in%250Aterms%2520of%2520both%2520quantitative%2520and%2520visual%2520quality%2520over%2520varying%2520degradation%2520types%250Aand%2520levels.%2520Our%2520dataset%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Degradation-Robust%20Reconstruction%20in%20Generalizable%20NeRF&entry.906535625=Chan%20Ho%20Park%20and%20Ka%20Leong%20Cheng%20and%20Zhicheng%20Wang%20and%20Qifeng%20Chen&entry.1292438233=%20%20Generalizable%20Neural%20Radiance%20Field%20%28GNeRF%29%20across%20scenes%20has%20been%20proven%20to%0Abe%20an%20effective%20way%20to%20avoid%20per-scene%20optimization%20by%20representing%20a%20scene%0Awith%20deep%20image%20features%20of%20source%20images.%20However%2C%20despite%20its%20potential%20for%0Areal-world%20applications%2C%20there%20has%20been%20limited%20research%20on%20the%20robustness%20of%0AGNeRFs%20to%20different%20types%20of%20degradation%20present%20in%20the%20source%20images.%20The%20lack%0Aof%20such%20research%20is%20primarily%20attributed%20to%20the%20absence%20of%20a%20large-scale%0Adataset%20fit%20for%20training%20a%20degradation-robust%20generalizable%20NeRF%20model.%20To%0Aaddress%20this%20gap%20and%20facilitate%20investigations%20into%20the%20degradation%20robustness%0Aof%203D%20reconstruction%20tasks%2C%20we%20construct%20the%20Objaverse%20Blur%20Dataset%2C%20comprising%0A50%2C000%20images%20from%20over%201000%20settings%20featuring%20multiple%20levels%20of%20blur%0Adegradation.%20In%20addition%2C%20we%20design%20a%20simple%20and%20model-agnostic%20module%20for%0Aenhancing%20the%20degradation%20robustness%20of%20GNeRFs.%20Specifically%2C%20by%20extracting%0A3D-aware%20features%20through%20a%20lightweight%20depth%20estimator%20and%20denoiser%2C%20the%0Aproposed%20module%20shows%20improvement%20on%20different%20popular%20methods%20in%20GNeRFs%20in%0Aterms%20of%20both%20quantitative%20and%20visual%20quality%20over%20varying%20degradation%20types%0Aand%20levels.%20Our%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11691v1&entry.124074799=Read"},
{"title": "MVLight: Relightable Text-to-3D Generation via Light-conditioned\n  Multi-View Diffusion", "author": "Dongseok Shim and Yichun Shi and Kejie Li and H. Jin Kim and Peng Wang", "abstract": "  Recent advancements in text-to-3D generation, building on the success of\nhigh-performance text-to-image generative models, have made it possible to\ncreate imaginative and richly textured 3D objects from textual descriptions.\nHowever, a key challenge remains in effectively decoupling light-independent\nand lighting-dependent components to enhance the quality of generated 3D models\nand their relighting performance. In this paper, we present MVLight, a novel\nlight-conditioned multi-view diffusion model that explicitly integrates\nlighting conditions directly into the generation process. This enables the\nmodel to synthesize high-quality images that faithfully reflect the specified\nlighting environment across multiple camera views. By leveraging this\ncapability to Score Distillation Sampling (SDS), we can effectively synthesize\n3D models with improved geometric precision and relighting capabilities. We\nvalidate the effectiveness of MVLight through extensive experiments and a user\nstudy.\n", "link": "http://arxiv.org/abs/2411.11475v1", "date": "2024-11-18", "relevancy": 2.7192, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.691}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.691}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVLight%3A%20Relightable%20Text-to-3D%20Generation%20via%20Light-conditioned%0A%20%20Multi-View%20Diffusion&body=Title%3A%20MVLight%3A%20Relightable%20Text-to-3D%20Generation%20via%20Light-conditioned%0A%20%20Multi-View%20Diffusion%0AAuthor%3A%20Dongseok%20Shim%20and%20Yichun%20Shi%20and%20Kejie%20Li%20and%20H.%20Jin%20Kim%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-3D%20generation%2C%20building%20on%20the%20success%20of%0Ahigh-performance%20text-to-image%20generative%20models%2C%20have%20made%20it%20possible%20to%0Acreate%20imaginative%20and%20richly%20textured%203D%20objects%20from%20textual%20descriptions.%0AHowever%2C%20a%20key%20challenge%20remains%20in%20effectively%20decoupling%20light-independent%0Aand%20lighting-dependent%20components%20to%20enhance%20the%20quality%20of%20generated%203D%20models%0Aand%20their%20relighting%20performance.%20In%20this%20paper%2C%20we%20present%20MVLight%2C%20a%20novel%0Alight-conditioned%20multi-view%20diffusion%20model%20that%20explicitly%20integrates%0Alighting%20conditions%20directly%20into%20the%20generation%20process.%20This%20enables%20the%0Amodel%20to%20synthesize%20high-quality%20images%20that%20faithfully%20reflect%20the%20specified%0Alighting%20environment%20across%20multiple%20camera%20views.%20By%20leveraging%20this%0Acapability%20to%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20we%20can%20effectively%20synthesize%0A3D%20models%20with%20improved%20geometric%20precision%20and%20relighting%20capabilities.%20We%0Avalidate%20the%20effectiveness%20of%20MVLight%20through%20extensive%20experiments%20and%20a%20user%0Astudy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVLight%253A%2520Relightable%2520Text-to-3D%2520Generation%2520via%2520Light-conditioned%250A%2520%2520Multi-View%2520Diffusion%26entry.906535625%3DDongseok%2520Shim%2520and%2520Yichun%2520Shi%2520and%2520Kejie%2520Li%2520and%2520H.%2520Jin%2520Kim%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-3D%2520generation%252C%2520building%2520on%2520the%2520success%2520of%250Ahigh-performance%2520text-to-image%2520generative%2520models%252C%2520have%2520made%2520it%2520possible%2520to%250Acreate%2520imaginative%2520and%2520richly%2520textured%25203D%2520objects%2520from%2520textual%2520descriptions.%250AHowever%252C%2520a%2520key%2520challenge%2520remains%2520in%2520effectively%2520decoupling%2520light-independent%250Aand%2520lighting-dependent%2520components%2520to%2520enhance%2520the%2520quality%2520of%2520generated%25203D%2520models%250Aand%2520their%2520relighting%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%2520MVLight%252C%2520a%2520novel%250Alight-conditioned%2520multi-view%2520diffusion%2520model%2520that%2520explicitly%2520integrates%250Alighting%2520conditions%2520directly%2520into%2520the%2520generation%2520process.%2520This%2520enables%2520the%250Amodel%2520to%2520synthesize%2520high-quality%2520images%2520that%2520faithfully%2520reflect%2520the%2520specified%250Alighting%2520environment%2520across%2520multiple%2520camera%2520views.%2520By%2520leveraging%2520this%250Acapability%2520to%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%252C%2520we%2520can%2520effectively%2520synthesize%250A3D%2520models%2520with%2520improved%2520geometric%2520precision%2520and%2520relighting%2520capabilities.%2520We%250Avalidate%2520the%2520effectiveness%2520of%2520MVLight%2520through%2520extensive%2520experiments%2520and%2520a%2520user%250Astudy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVLight%3A%20Relightable%20Text-to-3D%20Generation%20via%20Light-conditioned%0A%20%20Multi-View%20Diffusion&entry.906535625=Dongseok%20Shim%20and%20Yichun%20Shi%20and%20Kejie%20Li%20and%20H.%20Jin%20Kim%20and%20Peng%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-3D%20generation%2C%20building%20on%20the%20success%20of%0Ahigh-performance%20text-to-image%20generative%20models%2C%20have%20made%20it%20possible%20to%0Acreate%20imaginative%20and%20richly%20textured%203D%20objects%20from%20textual%20descriptions.%0AHowever%2C%20a%20key%20challenge%20remains%20in%20effectively%20decoupling%20light-independent%0Aand%20lighting-dependent%20components%20to%20enhance%20the%20quality%20of%20generated%203D%20models%0Aand%20their%20relighting%20performance.%20In%20this%20paper%2C%20we%20present%20MVLight%2C%20a%20novel%0Alight-conditioned%20multi-view%20diffusion%20model%20that%20explicitly%20integrates%0Alighting%20conditions%20directly%20into%20the%20generation%20process.%20This%20enables%20the%0Amodel%20to%20synthesize%20high-quality%20images%20that%20faithfully%20reflect%20the%20specified%0Alighting%20environment%20across%20multiple%20camera%20views.%20By%20leveraging%20this%0Acapability%20to%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20we%20can%20effectively%20synthesize%0A3D%20models%20with%20improved%20geometric%20precision%20and%20relighting%20capabilities.%20We%0Avalidate%20the%20effectiveness%20of%20MVLight%20through%20extensive%20experiments%20and%20a%20user%0Astudy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11475v1&entry.124074799=Read"},
{"title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model", "author": "Ruichuan An and Sihan Yang and Ming Lu and Kai Zeng and Yulin Luo and Ying Chen and Jiajun Cao and Hao Liang and Qi She and Shanghang Zhang and Wentao Zhang", "abstract": "  Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks including visual question answering. To enhance user experience\nin practical applications, recent studies investigate VLM personalization to\nunderstand user-provided concepts. However, existing studies mainly focus on\nsingle-concept personalization, neglecting the existence and interplay of\nmultiple concepts, which limits the real-world applicability of personalized\nVLMs. In this paper, we propose the first multi-concept personalization method\nnamed MC-LLaVA along with a high-quality multi-concept personalization dataset.\nSpecifically, MC-LLaVA uses a joint training strategy incorporating multiple\nconcepts in a single training step, allowing VLMs to perform accurately in\nmulti-concept personalization. To reduce the cost of joint training, MC-LLaVA\nleverages visual token information for concept token initialization, yielding\nimproved concept representation and accelerating joint training. To advance\nmulti-concept personalization research, we further contribute a high-quality\ndataset. We carefully collect images from various movies that contain multiple\ncharacters and manually generate the multi-concept question-answer samples. Our\ndataset features diverse movie types and question-answer types. We conduct\ncomprehensive qualitative and quantitative experiments to demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.\n", "link": "http://arxiv.org/abs/2411.11706v1", "date": "2024-11-18", "relevancy": 2.7153, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model&body=Title%3A%20MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model%0AAuthor%3A%20Ruichuan%20An%20and%20Sihan%20Yang%20and%20Ming%20Lu%20and%20Kai%20Zeng%20and%20Yulin%20Luo%20and%20Ying%20Chen%20and%20Jiajun%20Cao%20and%20Hao%20Liang%20and%20Qi%20She%20and%20Shanghang%20Zhang%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Current%20vision-language%20models%20%28VLMs%29%20show%20exceptional%20abilities%20across%0Adiverse%20tasks%20including%20visual%20question%20answering.%20To%20enhance%20user%20experience%0Ain%20practical%20applications%2C%20recent%20studies%20investigate%20VLM%20personalization%20to%0Aunderstand%20user-provided%20concepts.%20However%2C%20existing%20studies%20mainly%20focus%20on%0Asingle-concept%20personalization%2C%20neglecting%20the%20existence%20and%20interplay%20of%0Amultiple%20concepts%2C%20which%20limits%20the%20real-world%20applicability%20of%20personalized%0AVLMs.%20In%20this%20paper%2C%20we%20propose%20the%20first%20multi-concept%20personalization%20method%0Anamed%20MC-LLaVA%20along%20with%20a%20high-quality%20multi-concept%20personalization%20dataset.%0ASpecifically%2C%20MC-LLaVA%20uses%20a%20joint%20training%20strategy%20incorporating%20multiple%0Aconcepts%20in%20a%20single%20training%20step%2C%20allowing%20VLMs%20to%20perform%20accurately%20in%0Amulti-concept%20personalization.%20To%20reduce%20the%20cost%20of%20joint%20training%2C%20MC-LLaVA%0Aleverages%20visual%20token%20information%20for%20concept%20token%20initialization%2C%20yielding%0Aimproved%20concept%20representation%20and%20accelerating%20joint%20training.%20To%20advance%0Amulti-concept%20personalization%20research%2C%20we%20further%20contribute%20a%20high-quality%0Adataset.%20We%20carefully%20collect%20images%20from%20various%20movies%20that%20contain%20multiple%0Acharacters%20and%20manually%20generate%20the%20multi-concept%20question-answer%20samples.%20Our%0Adataset%20features%20diverse%20movie%20types%20and%20question-answer%20types.%20We%20conduct%0Acomprehensive%20qualitative%20and%20quantitative%20experiments%20to%20demonstrate%20that%0AMC-LLaVA%20can%20achieve%20impressive%20multi-concept%20personalized%20responses%2C%20paving%0Athe%20way%20for%20VLMs%20to%20become%20better%20user-specific%20assistants.%20The%20code%20and%0Adataset%20will%20be%20publicly%20available%20at%20https%3A//github.com/arctanxarc/MC-LLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMC-LLaVA%253A%2520Multi-Concept%2520Personalized%2520Vision-Language%2520Model%26entry.906535625%3DRuichuan%2520An%2520and%2520Sihan%2520Yang%2520and%2520Ming%2520Lu%2520and%2520Kai%2520Zeng%2520and%2520Yulin%2520Luo%2520and%2520Ying%2520Chen%2520and%2520Jiajun%2520Cao%2520and%2520Hao%2520Liang%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520vision-language%2520models%2520%2528VLMs%2529%2520show%2520exceptional%2520abilities%2520across%250Adiverse%2520tasks%2520including%2520visual%2520question%2520answering.%2520To%2520enhance%2520user%2520experience%250Ain%2520practical%2520applications%252C%2520recent%2520studies%2520investigate%2520VLM%2520personalization%2520to%250Aunderstand%2520user-provided%2520concepts.%2520However%252C%2520existing%2520studies%2520mainly%2520focus%2520on%250Asingle-concept%2520personalization%252C%2520neglecting%2520the%2520existence%2520and%2520interplay%2520of%250Amultiple%2520concepts%252C%2520which%2520limits%2520the%2520real-world%2520applicability%2520of%2520personalized%250AVLMs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520multi-concept%2520personalization%2520method%250Anamed%2520MC-LLaVA%2520along%2520with%2520a%2520high-quality%2520multi-concept%2520personalization%2520dataset.%250ASpecifically%252C%2520MC-LLaVA%2520uses%2520a%2520joint%2520training%2520strategy%2520incorporating%2520multiple%250Aconcepts%2520in%2520a%2520single%2520training%2520step%252C%2520allowing%2520VLMs%2520to%2520perform%2520accurately%2520in%250Amulti-concept%2520personalization.%2520To%2520reduce%2520the%2520cost%2520of%2520joint%2520training%252C%2520MC-LLaVA%250Aleverages%2520visual%2520token%2520information%2520for%2520concept%2520token%2520initialization%252C%2520yielding%250Aimproved%2520concept%2520representation%2520and%2520accelerating%2520joint%2520training.%2520To%2520advance%250Amulti-concept%2520personalization%2520research%252C%2520we%2520further%2520contribute%2520a%2520high-quality%250Adataset.%2520We%2520carefully%2520collect%2520images%2520from%2520various%2520movies%2520that%2520contain%2520multiple%250Acharacters%2520and%2520manually%2520generate%2520the%2520multi-concept%2520question-answer%2520samples.%2520Our%250Adataset%2520features%2520diverse%2520movie%2520types%2520and%2520question-answer%2520types.%2520We%2520conduct%250Acomprehensive%2520qualitative%2520and%2520quantitative%2520experiments%2520to%2520demonstrate%2520that%250AMC-LLaVA%2520can%2520achieve%2520impressive%2520multi-concept%2520personalized%2520responses%252C%2520paving%250Athe%2520way%2520for%2520VLMs%2520to%2520become%2520better%2520user-specific%2520assistants.%2520The%2520code%2520and%250Adataset%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/arctanxarc/MC-LLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model&entry.906535625=Ruichuan%20An%20and%20Sihan%20Yang%20and%20Ming%20Lu%20and%20Kai%20Zeng%20and%20Yulin%20Luo%20and%20Ying%20Chen%20and%20Jiajun%20Cao%20and%20Hao%20Liang%20and%20Qi%20She%20and%20Shanghang%20Zhang%20and%20Wentao%20Zhang&entry.1292438233=%20%20Current%20vision-language%20models%20%28VLMs%29%20show%20exceptional%20abilities%20across%0Adiverse%20tasks%20including%20visual%20question%20answering.%20To%20enhance%20user%20experience%0Ain%20practical%20applications%2C%20recent%20studies%20investigate%20VLM%20personalization%20to%0Aunderstand%20user-provided%20concepts.%20However%2C%20existing%20studies%20mainly%20focus%20on%0Asingle-concept%20personalization%2C%20neglecting%20the%20existence%20and%20interplay%20of%0Amultiple%20concepts%2C%20which%20limits%20the%20real-world%20applicability%20of%20personalized%0AVLMs.%20In%20this%20paper%2C%20we%20propose%20the%20first%20multi-concept%20personalization%20method%0Anamed%20MC-LLaVA%20along%20with%20a%20high-quality%20multi-concept%20personalization%20dataset.%0ASpecifically%2C%20MC-LLaVA%20uses%20a%20joint%20training%20strategy%20incorporating%20multiple%0Aconcepts%20in%20a%20single%20training%20step%2C%20allowing%20VLMs%20to%20perform%20accurately%20in%0Amulti-concept%20personalization.%20To%20reduce%20the%20cost%20of%20joint%20training%2C%20MC-LLaVA%0Aleverages%20visual%20token%20information%20for%20concept%20token%20initialization%2C%20yielding%0Aimproved%20concept%20representation%20and%20accelerating%20joint%20training.%20To%20advance%0Amulti-concept%20personalization%20research%2C%20we%20further%20contribute%20a%20high-quality%0Adataset.%20We%20carefully%20collect%20images%20from%20various%20movies%20that%20contain%20multiple%0Acharacters%20and%20manually%20generate%20the%20multi-concept%20question-answer%20samples.%20Our%0Adataset%20features%20diverse%20movie%20types%20and%20question-answer%20types.%20We%20conduct%0Acomprehensive%20qualitative%20and%20quantitative%20experiments%20to%20demonstrate%20that%0AMC-LLaVA%20can%20achieve%20impressive%20multi-concept%20personalized%20responses%2C%20paving%0Athe%20way%20for%20VLMs%20to%20become%20better%20user-specific%20assistants.%20The%20code%20and%0Adataset%20will%20be%20publicly%20available%20at%20https%3A//github.com/arctanxarc/MC-LLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11706v1&entry.124074799=Read"},
{"title": "UniHands: Unifying Various Wild-Collected Keypoints for Personalized\n  Hand Reconstruction", "author": "Menghe Zhang and Joonyeoup Kim and Yangwen Liang and Shuangquan Wang and Kee-Bong Song", "abstract": "  Accurate hand motion capture and standardized 3D representation are essential\nfor various hand-related tasks. Collecting keypoints-only data, while efficient\nand cost-effective, results in low-fidelity representations and lacks surface\ninformation. Furthermore, data inconsistencies across sources challenge their\nintegration and use. We present UniHands, a novel method for creating\nstandardized yet personalized hand models from wild-collected keypoints from\ndiverse sources. Unlike existing neural implicit representation methods,\nUniHands uses the widely-adopted parametric models MANO and NIMBLE, providing a\nmore scalable and versatile solution. It also derives unified hand joints from\nthe meshes, which facilitates seamless integration into various hand-related\ntasks. Experiments on the FreiHAND and InterHand2.6M datasets demonstrate its\nability to precisely reconstruct hand mesh vertices and keypoints, effectively\ncapturing high-degree articulation motions. Empirical studies involving nine\nparticipants show a clear preference for our unified joints over existing\nconfigurations for accuracy and naturalism (p-value 0.016).\n", "link": "http://arxiv.org/abs/2411.11845v1", "date": "2024-11-18", "relevancy": 2.7086, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5822}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5236}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniHands%3A%20Unifying%20Various%20Wild-Collected%20Keypoints%20for%20Personalized%0A%20%20Hand%20Reconstruction&body=Title%3A%20UniHands%3A%20Unifying%20Various%20Wild-Collected%20Keypoints%20for%20Personalized%0A%20%20Hand%20Reconstruction%0AAuthor%3A%20Menghe%20Zhang%20and%20Joonyeoup%20Kim%20and%20Yangwen%20Liang%20and%20Shuangquan%20Wang%20and%20Kee-Bong%20Song%0AAbstract%3A%20%20%20Accurate%20hand%20motion%20capture%20and%20standardized%203D%20representation%20are%20essential%0Afor%20various%20hand-related%20tasks.%20Collecting%20keypoints-only%20data%2C%20while%20efficient%0Aand%20cost-effective%2C%20results%20in%20low-fidelity%20representations%20and%20lacks%20surface%0Ainformation.%20Furthermore%2C%20data%20inconsistencies%20across%20sources%20challenge%20their%0Aintegration%20and%20use.%20We%20present%20UniHands%2C%20a%20novel%20method%20for%20creating%0Astandardized%20yet%20personalized%20hand%20models%20from%20wild-collected%20keypoints%20from%0Adiverse%20sources.%20Unlike%20existing%20neural%20implicit%20representation%20methods%2C%0AUniHands%20uses%20the%20widely-adopted%20parametric%20models%20MANO%20and%20NIMBLE%2C%20providing%20a%0Amore%20scalable%20and%20versatile%20solution.%20It%20also%20derives%20unified%20hand%20joints%20from%0Athe%20meshes%2C%20which%20facilitates%20seamless%20integration%20into%20various%20hand-related%0Atasks.%20Experiments%20on%20the%20FreiHAND%20and%20InterHand2.6M%20datasets%20demonstrate%20its%0Aability%20to%20precisely%20reconstruct%20hand%20mesh%20vertices%20and%20keypoints%2C%20effectively%0Acapturing%20high-degree%20articulation%20motions.%20Empirical%20studies%20involving%20nine%0Aparticipants%20show%20a%20clear%20preference%20for%20our%20unified%20joints%20over%20existing%0Aconfigurations%20for%20accuracy%20and%20naturalism%20%28p-value%200.016%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniHands%253A%2520Unifying%2520Various%2520Wild-Collected%2520Keypoints%2520for%2520Personalized%250A%2520%2520Hand%2520Reconstruction%26entry.906535625%3DMenghe%2520Zhang%2520and%2520Joonyeoup%2520Kim%2520and%2520Yangwen%2520Liang%2520and%2520Shuangquan%2520Wang%2520and%2520Kee-Bong%2520Song%26entry.1292438233%3D%2520%2520Accurate%2520hand%2520motion%2520capture%2520and%2520standardized%25203D%2520representation%2520are%2520essential%250Afor%2520various%2520hand-related%2520tasks.%2520Collecting%2520keypoints-only%2520data%252C%2520while%2520efficient%250Aand%2520cost-effective%252C%2520results%2520in%2520low-fidelity%2520representations%2520and%2520lacks%2520surface%250Ainformation.%2520Furthermore%252C%2520data%2520inconsistencies%2520across%2520sources%2520challenge%2520their%250Aintegration%2520and%2520use.%2520We%2520present%2520UniHands%252C%2520a%2520novel%2520method%2520for%2520creating%250Astandardized%2520yet%2520personalized%2520hand%2520models%2520from%2520wild-collected%2520keypoints%2520from%250Adiverse%2520sources.%2520Unlike%2520existing%2520neural%2520implicit%2520representation%2520methods%252C%250AUniHands%2520uses%2520the%2520widely-adopted%2520parametric%2520models%2520MANO%2520and%2520NIMBLE%252C%2520providing%2520a%250Amore%2520scalable%2520and%2520versatile%2520solution.%2520It%2520also%2520derives%2520unified%2520hand%2520joints%2520from%250Athe%2520meshes%252C%2520which%2520facilitates%2520seamless%2520integration%2520into%2520various%2520hand-related%250Atasks.%2520Experiments%2520on%2520the%2520FreiHAND%2520and%2520InterHand2.6M%2520datasets%2520demonstrate%2520its%250Aability%2520to%2520precisely%2520reconstruct%2520hand%2520mesh%2520vertices%2520and%2520keypoints%252C%2520effectively%250Acapturing%2520high-degree%2520articulation%2520motions.%2520Empirical%2520studies%2520involving%2520nine%250Aparticipants%2520show%2520a%2520clear%2520preference%2520for%2520our%2520unified%2520joints%2520over%2520existing%250Aconfigurations%2520for%2520accuracy%2520and%2520naturalism%2520%2528p-value%25200.016%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniHands%3A%20Unifying%20Various%20Wild-Collected%20Keypoints%20for%20Personalized%0A%20%20Hand%20Reconstruction&entry.906535625=Menghe%20Zhang%20and%20Joonyeoup%20Kim%20and%20Yangwen%20Liang%20and%20Shuangquan%20Wang%20and%20Kee-Bong%20Song&entry.1292438233=%20%20Accurate%20hand%20motion%20capture%20and%20standardized%203D%20representation%20are%20essential%0Afor%20various%20hand-related%20tasks.%20Collecting%20keypoints-only%20data%2C%20while%20efficient%0Aand%20cost-effective%2C%20results%20in%20low-fidelity%20representations%20and%20lacks%20surface%0Ainformation.%20Furthermore%2C%20data%20inconsistencies%20across%20sources%20challenge%20their%0Aintegration%20and%20use.%20We%20present%20UniHands%2C%20a%20novel%20method%20for%20creating%0Astandardized%20yet%20personalized%20hand%20models%20from%20wild-collected%20keypoints%20from%0Adiverse%20sources.%20Unlike%20existing%20neural%20implicit%20representation%20methods%2C%0AUniHands%20uses%20the%20widely-adopted%20parametric%20models%20MANO%20and%20NIMBLE%2C%20providing%20a%0Amore%20scalable%20and%20versatile%20solution.%20It%20also%20derives%20unified%20hand%20joints%20from%0Athe%20meshes%2C%20which%20facilitates%20seamless%20integration%20into%20various%20hand-related%0Atasks.%20Experiments%20on%20the%20FreiHAND%20and%20InterHand2.6M%20datasets%20demonstrate%20its%0Aability%20to%20precisely%20reconstruct%20hand%20mesh%20vertices%20and%20keypoints%2C%20effectively%0Acapturing%20high-degree%20articulation%20motions.%20Empirical%20studies%20involving%20nine%0Aparticipants%20show%20a%20clear%20preference%20for%20our%20unified%20joints%20over%20existing%0Aconfigurations%20for%20accuracy%20and%20naturalism%20%28p-value%200.016%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11845v1&entry.124074799=Read"},
{"title": "RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model", "author": "Hongjun Chen and Wencheng Han and Huan Zheng and Jianbing Shen", "abstract": "  Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasized\nmetadata-driven approaches to reconstruct RAW data from sRGB images,\nsupplemented by partial RAW information. In image-based de-rendering, metadata\nis commonly obtained through sampling, whereas in video tasks, it is typically\nderived from the initial frame. The distinct metadata requirements necessitate\nspecialized network architectures, leading to architectural incompatibilities\nthat increase deployment complexity. In this paper, we propose RAWMamba, a\nMamba-based unified framework developed for sRGB-to-RAW de-rendering across\nboth image and video domains. The core of RAWMamba is the Unified Metadata\nEmbedding (UME) module, which harmonizes diverse metadata types into a unified\nrepresentation. In detail, a multi-perspective affinity modeling method is\nproposed to promote the extraction of reference information. In addition, we\nintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captures\nlong-range dependencies to enable effective global propagation of metadata.\nExperimental results demonstrate that the proposed RAWMamba achieves\nstate-of-the-art performance, yielding high-quality RAW data reconstruction.\n", "link": "http://arxiv.org/abs/2411.11717v1", "date": "2024-11-18", "relevancy": 2.6999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.569}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5348}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAWMamba%3A%20Unified%20sRGB-to-RAW%20De-rendering%20With%20State%20Space%20Model&body=Title%3A%20RAWMamba%3A%20Unified%20sRGB-to-RAW%20De-rendering%20With%20State%20Space%20Model%0AAuthor%3A%20Hongjun%20Chen%20and%20Wencheng%20Han%20and%20Huan%20Zheng%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20sRGB-to-RAW%20de-rendering%20have%20increasingly%20emphasized%0Ametadata-driven%20approaches%20to%20reconstruct%20RAW%20data%20from%20sRGB%20images%2C%0Asupplemented%20by%20partial%20RAW%20information.%20In%20image-based%20de-rendering%2C%20metadata%0Ais%20commonly%20obtained%20through%20sampling%2C%20whereas%20in%20video%20tasks%2C%20it%20is%20typically%0Aderived%20from%20the%20initial%20frame.%20The%20distinct%20metadata%20requirements%20necessitate%0Aspecialized%20network%20architectures%2C%20leading%20to%20architectural%20incompatibilities%0Athat%20increase%20deployment%20complexity.%20In%20this%20paper%2C%20we%20propose%20RAWMamba%2C%20a%0AMamba-based%20unified%20framework%20developed%20for%20sRGB-to-RAW%20de-rendering%20across%0Aboth%20image%20and%20video%20domains.%20The%20core%20of%20RAWMamba%20is%20the%20Unified%20Metadata%0AEmbedding%20%28UME%29%20module%2C%20which%20harmonizes%20diverse%20metadata%20types%20into%20a%20unified%0Arepresentation.%20In%20detail%2C%20a%20multi-perspective%20affinity%20modeling%20method%20is%0Aproposed%20to%20promote%20the%20extraction%20of%20reference%20information.%20In%20addition%2C%20we%0Aintroduce%20the%20Local%20Tone-Aware%20Mamba%20%28LTA-Mamba%29%20module%2C%20which%20captures%0Along-range%20dependencies%20to%20enable%20effective%20global%20propagation%20of%20metadata.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20RAWMamba%20achieves%0Astate-of-the-art%20performance%2C%20yielding%20high-quality%20RAW%20data%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAWMamba%253A%2520Unified%2520sRGB-to-RAW%2520De-rendering%2520With%2520State%2520Space%2520Model%26entry.906535625%3DHongjun%2520Chen%2520and%2520Wencheng%2520Han%2520and%2520Huan%2520Zheng%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520sRGB-to-RAW%2520de-rendering%2520have%2520increasingly%2520emphasized%250Ametadata-driven%2520approaches%2520to%2520reconstruct%2520RAW%2520data%2520from%2520sRGB%2520images%252C%250Asupplemented%2520by%2520partial%2520RAW%2520information.%2520In%2520image-based%2520de-rendering%252C%2520metadata%250Ais%2520commonly%2520obtained%2520through%2520sampling%252C%2520whereas%2520in%2520video%2520tasks%252C%2520it%2520is%2520typically%250Aderived%2520from%2520the%2520initial%2520frame.%2520The%2520distinct%2520metadata%2520requirements%2520necessitate%250Aspecialized%2520network%2520architectures%252C%2520leading%2520to%2520architectural%2520incompatibilities%250Athat%2520increase%2520deployment%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RAWMamba%252C%2520a%250AMamba-based%2520unified%2520framework%2520developed%2520for%2520sRGB-to-RAW%2520de-rendering%2520across%250Aboth%2520image%2520and%2520video%2520domains.%2520The%2520core%2520of%2520RAWMamba%2520is%2520the%2520Unified%2520Metadata%250AEmbedding%2520%2528UME%2529%2520module%252C%2520which%2520harmonizes%2520diverse%2520metadata%2520types%2520into%2520a%2520unified%250Arepresentation.%2520In%2520detail%252C%2520a%2520multi-perspective%2520affinity%2520modeling%2520method%2520is%250Aproposed%2520to%2520promote%2520the%2520extraction%2520of%2520reference%2520information.%2520In%2520addition%252C%2520we%250Aintroduce%2520the%2520Local%2520Tone-Aware%2520Mamba%2520%2528LTA-Mamba%2529%2520module%252C%2520which%2520captures%250Along-range%2520dependencies%2520to%2520enable%2520effective%2520global%2520propagation%2520of%2520metadata.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520RAWMamba%2520achieves%250Astate-of-the-art%2520performance%252C%2520yielding%2520high-quality%2520RAW%2520data%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAWMamba%3A%20Unified%20sRGB-to-RAW%20De-rendering%20With%20State%20Space%20Model&entry.906535625=Hongjun%20Chen%20and%20Wencheng%20Han%20and%20Huan%20Zheng%20and%20Jianbing%20Shen&entry.1292438233=%20%20Recent%20advancements%20in%20sRGB-to-RAW%20de-rendering%20have%20increasingly%20emphasized%0Ametadata-driven%20approaches%20to%20reconstruct%20RAW%20data%20from%20sRGB%20images%2C%0Asupplemented%20by%20partial%20RAW%20information.%20In%20image-based%20de-rendering%2C%20metadata%0Ais%20commonly%20obtained%20through%20sampling%2C%20whereas%20in%20video%20tasks%2C%20it%20is%20typically%0Aderived%20from%20the%20initial%20frame.%20The%20distinct%20metadata%20requirements%20necessitate%0Aspecialized%20network%20architectures%2C%20leading%20to%20architectural%20incompatibilities%0Athat%20increase%20deployment%20complexity.%20In%20this%20paper%2C%20we%20propose%20RAWMamba%2C%20a%0AMamba-based%20unified%20framework%20developed%20for%20sRGB-to-RAW%20de-rendering%20across%0Aboth%20image%20and%20video%20domains.%20The%20core%20of%20RAWMamba%20is%20the%20Unified%20Metadata%0AEmbedding%20%28UME%29%20module%2C%20which%20harmonizes%20diverse%20metadata%20types%20into%20a%20unified%0Arepresentation.%20In%20detail%2C%20a%20multi-perspective%20affinity%20modeling%20method%20is%0Aproposed%20to%20promote%20the%20extraction%20of%20reference%20information.%20In%20addition%2C%20we%0Aintroduce%20the%20Local%20Tone-Aware%20Mamba%20%28LTA-Mamba%29%20module%2C%20which%20captures%0Along-range%20dependencies%20to%20enable%20effective%20global%20propagation%20of%20metadata.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20RAWMamba%20achieves%0Astate-of-the-art%20performance%2C%20yielding%20high-quality%20RAW%20data%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11717v1&entry.124074799=Read"},
{"title": "Leveraging Computational Pathology AI for Noninvasive Optical Imaging\n  Analysis Without Retraining", "author": "Danny Barash and Emilie Manning and Aidan Van Vleck and Omri Hirsch and Kyi Lei Aye and Jingxi Li and Philip O. Scumpia and Aydogan Ozcan and Sumaira Aasi and Kerri E. Rieger and Kavita Y. Sarin and Oren Freifeld and Yonatan Winetraub", "abstract": "  Noninvasive optical imaging modalities can probe patient's tissue in 3D and\nover time generate gigabytes of clinically relevant data per sample. There is a\nneed for AI models to analyze this data and assist clinical workflow. The lack\nof expert labelers and the large dataset required (>100,000 images) for model\ntraining and tuning are the main hurdles in creating foundation models. In this\npaper we introduce FoundationShift, a method to apply any AI model from\ncomputational pathology without retraining. We show our method is more accurate\nthan state of the art models (SAM, MedSAM, SAM-Med2D, CellProfiler, Hover-Net,\nPLIP, UNI and ChatGPT), with multiple imaging modalities (OCT and RCM). This is\nachieved without the need for model retraining or fine-tuning. Applying our\nmethod to noninvasive in vivo images could enable physicians to readily\nincorporate optical imaging modalities into their clinical practice, providing\nreal time tissue analysis and improving patient care.\n", "link": "http://arxiv.org/abs/2411.11613v1", "date": "2024-11-18", "relevancy": 2.6172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Computational%20Pathology%20AI%20for%20Noninvasive%20Optical%20Imaging%0A%20%20Analysis%20Without%20Retraining&body=Title%3A%20Leveraging%20Computational%20Pathology%20AI%20for%20Noninvasive%20Optical%20Imaging%0A%20%20Analysis%20Without%20Retraining%0AAuthor%3A%20Danny%20Barash%20and%20Emilie%20Manning%20and%20Aidan%20Van%20Vleck%20and%20Omri%20Hirsch%20and%20Kyi%20Lei%20Aye%20and%20Jingxi%20Li%20and%20Philip%20O.%20Scumpia%20and%20Aydogan%20Ozcan%20and%20Sumaira%20Aasi%20and%20Kerri%20E.%20Rieger%20and%20Kavita%20Y.%20Sarin%20and%20Oren%20Freifeld%20and%20Yonatan%20Winetraub%0AAbstract%3A%20%20%20Noninvasive%20optical%20imaging%20modalities%20can%20probe%20patient%27s%20tissue%20in%203D%20and%0Aover%20time%20generate%20gigabytes%20of%20clinically%20relevant%20data%20per%20sample.%20There%20is%20a%0Aneed%20for%20AI%20models%20to%20analyze%20this%20data%20and%20assist%20clinical%20workflow.%20The%20lack%0Aof%20expert%20labelers%20and%20the%20large%20dataset%20required%20%28%3E100%2C000%20images%29%20for%20model%0Atraining%20and%20tuning%20are%20the%20main%20hurdles%20in%20creating%20foundation%20models.%20In%20this%0Apaper%20we%20introduce%20FoundationShift%2C%20a%20method%20to%20apply%20any%20AI%20model%20from%0Acomputational%20pathology%20without%20retraining.%20We%20show%20our%20method%20is%20more%20accurate%0Athan%20state%20of%20the%20art%20models%20%28SAM%2C%20MedSAM%2C%20SAM-Med2D%2C%20CellProfiler%2C%20Hover-Net%2C%0APLIP%2C%20UNI%20and%20ChatGPT%29%2C%20with%20multiple%20imaging%20modalities%20%28OCT%20and%20RCM%29.%20This%20is%0Aachieved%20without%20the%20need%20for%20model%20retraining%20or%20fine-tuning.%20Applying%20our%0Amethod%20to%20noninvasive%20in%20vivo%20images%20could%20enable%20physicians%20to%20readily%0Aincorporate%20optical%20imaging%20modalities%20into%20their%20clinical%20practice%2C%20providing%0Areal%20time%20tissue%20analysis%20and%20improving%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Computational%2520Pathology%2520AI%2520for%2520Noninvasive%2520Optical%2520Imaging%250A%2520%2520Analysis%2520Without%2520Retraining%26entry.906535625%3DDanny%2520Barash%2520and%2520Emilie%2520Manning%2520and%2520Aidan%2520Van%2520Vleck%2520and%2520Omri%2520Hirsch%2520and%2520Kyi%2520Lei%2520Aye%2520and%2520Jingxi%2520Li%2520and%2520Philip%2520O.%2520Scumpia%2520and%2520Aydogan%2520Ozcan%2520and%2520Sumaira%2520Aasi%2520and%2520Kerri%2520E.%2520Rieger%2520and%2520Kavita%2520Y.%2520Sarin%2520and%2520Oren%2520Freifeld%2520and%2520Yonatan%2520Winetraub%26entry.1292438233%3D%2520%2520Noninvasive%2520optical%2520imaging%2520modalities%2520can%2520probe%2520patient%2527s%2520tissue%2520in%25203D%2520and%250Aover%2520time%2520generate%2520gigabytes%2520of%2520clinically%2520relevant%2520data%2520per%2520sample.%2520There%2520is%2520a%250Aneed%2520for%2520AI%2520models%2520to%2520analyze%2520this%2520data%2520and%2520assist%2520clinical%2520workflow.%2520The%2520lack%250Aof%2520expert%2520labelers%2520and%2520the%2520large%2520dataset%2520required%2520%2528%253E100%252C000%2520images%2529%2520for%2520model%250Atraining%2520and%2520tuning%2520are%2520the%2520main%2520hurdles%2520in%2520creating%2520foundation%2520models.%2520In%2520this%250Apaper%2520we%2520introduce%2520FoundationShift%252C%2520a%2520method%2520to%2520apply%2520any%2520AI%2520model%2520from%250Acomputational%2520pathology%2520without%2520retraining.%2520We%2520show%2520our%2520method%2520is%2520more%2520accurate%250Athan%2520state%2520of%2520the%2520art%2520models%2520%2528SAM%252C%2520MedSAM%252C%2520SAM-Med2D%252C%2520CellProfiler%252C%2520Hover-Net%252C%250APLIP%252C%2520UNI%2520and%2520ChatGPT%2529%252C%2520with%2520multiple%2520imaging%2520modalities%2520%2528OCT%2520and%2520RCM%2529.%2520This%2520is%250Aachieved%2520without%2520the%2520need%2520for%2520model%2520retraining%2520or%2520fine-tuning.%2520Applying%2520our%250Amethod%2520to%2520noninvasive%2520in%2520vivo%2520images%2520could%2520enable%2520physicians%2520to%2520readily%250Aincorporate%2520optical%2520imaging%2520modalities%2520into%2520their%2520clinical%2520practice%252C%2520providing%250Areal%2520time%2520tissue%2520analysis%2520and%2520improving%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Computational%20Pathology%20AI%20for%20Noninvasive%20Optical%20Imaging%0A%20%20Analysis%20Without%20Retraining&entry.906535625=Danny%20Barash%20and%20Emilie%20Manning%20and%20Aidan%20Van%20Vleck%20and%20Omri%20Hirsch%20and%20Kyi%20Lei%20Aye%20and%20Jingxi%20Li%20and%20Philip%20O.%20Scumpia%20and%20Aydogan%20Ozcan%20and%20Sumaira%20Aasi%20and%20Kerri%20E.%20Rieger%20and%20Kavita%20Y.%20Sarin%20and%20Oren%20Freifeld%20and%20Yonatan%20Winetraub&entry.1292438233=%20%20Noninvasive%20optical%20imaging%20modalities%20can%20probe%20patient%27s%20tissue%20in%203D%20and%0Aover%20time%20generate%20gigabytes%20of%20clinically%20relevant%20data%20per%20sample.%20There%20is%20a%0Aneed%20for%20AI%20models%20to%20analyze%20this%20data%20and%20assist%20clinical%20workflow.%20The%20lack%0Aof%20expert%20labelers%20and%20the%20large%20dataset%20required%20%28%3E100%2C000%20images%29%20for%20model%0Atraining%20and%20tuning%20are%20the%20main%20hurdles%20in%20creating%20foundation%20models.%20In%20this%0Apaper%20we%20introduce%20FoundationShift%2C%20a%20method%20to%20apply%20any%20AI%20model%20from%0Acomputational%20pathology%20without%20retraining.%20We%20show%20our%20method%20is%20more%20accurate%0Athan%20state%20of%20the%20art%20models%20%28SAM%2C%20MedSAM%2C%20SAM-Med2D%2C%20CellProfiler%2C%20Hover-Net%2C%0APLIP%2C%20UNI%20and%20ChatGPT%29%2C%20with%20multiple%20imaging%20modalities%20%28OCT%20and%20RCM%29.%20This%20is%0Aachieved%20without%20the%20need%20for%20model%20retraining%20or%20fine-tuning.%20Applying%20our%0Amethod%20to%20noninvasive%20in%20vivo%20images%20could%20enable%20physicians%20to%20readily%0Aincorporate%20optical%20imaging%20modalities%20into%20their%20clinical%20practice%2C%20providing%0Areal%20time%20tissue%20analysis%20and%20improving%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11613v1&entry.124074799=Read"},
{"title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers", "author": "Cl\u00e9ment Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West", "abstract": "  A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.\n", "link": "http://arxiv.org/abs/2411.08745v2", "date": "2024-11-18", "relevancy": 2.6105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers&body=Title%3A%20Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers%0AAuthor%3A%20Cl%C3%A9ment%20Dumas%20and%20Chris%20Wendler%20and%20Veniamin%20Veselovsky%20and%20Giovanni%20Monea%20and%20Robert%20West%0AAbstract%3A%20%20%20A%20central%20question%20in%20multilingual%20language%20modeling%20is%20whether%20large%0Alanguage%20models%20%28LLMs%29%20develop%20a%20universal%20concept%20representation%2C%20disentangled%0Afrom%20specific%20languages.%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20analyzing%0Alatent%20representations%20%28latents%29%20during%20a%20word%20translation%20task%20in%0Atransformer-based%20LLMs.%20We%20strategically%20extract%20latents%20from%20a%20source%0Atranslation%20prompt%20and%20insert%20them%20into%20the%20forward%20pass%20on%20a%20target%0Atranslation%20prompt.%20By%20doing%20so%2C%20we%20find%20that%20the%20output%20language%20is%20encoded%20in%0Athe%20latent%20at%20an%20earlier%20layer%20than%20the%20concept%20to%20be%20translated.%20Building%20on%0Athis%20insight%2C%20we%20conduct%20two%20key%20experiments.%20First%2C%20we%20demonstrate%20that%20we%20can%0Achange%20the%20concept%20without%20changing%20the%20language%20and%20vice%20versa%20through%0Aactivation%20patching%20alone.%20Second%2C%20we%20show%20that%20patching%20with%20the%20mean%20over%0Alatents%20across%20different%20languages%20does%20not%20impair%20and%20instead%20improves%20the%0Amodels%27%20performance%20in%20translating%20the%20concept.%20Our%20results%20provide%20evidence%0Afor%20the%20existence%20of%20language-agnostic%20concept%20representations%20within%20the%0Ainvestigated%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparating%2520Tongue%2520from%2520Thought%253A%2520Activation%2520Patching%2520Reveals%250A%2520%2520Language-Agnostic%2520Concept%2520Representations%2520in%2520Transformers%26entry.906535625%3DCl%25C3%25A9ment%2520Dumas%2520and%2520Chris%2520Wendler%2520and%2520Veniamin%2520Veselovsky%2520and%2520Giovanni%2520Monea%2520and%2520Robert%2520West%26entry.1292438233%3D%2520%2520A%2520central%2520question%2520in%2520multilingual%2520language%2520modeling%2520is%2520whether%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520develop%2520a%2520universal%2520concept%2520representation%252C%2520disentangled%250Afrom%2520specific%2520languages.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520question%2520by%2520analyzing%250Alatent%2520representations%2520%2528latents%2529%2520during%2520a%2520word%2520translation%2520task%2520in%250Atransformer-based%2520LLMs.%2520We%2520strategically%2520extract%2520latents%2520from%2520a%2520source%250Atranslation%2520prompt%2520and%2520insert%2520them%2520into%2520the%2520forward%2520pass%2520on%2520a%2520target%250Atranslation%2520prompt.%2520By%2520doing%2520so%252C%2520we%2520find%2520that%2520the%2520output%2520language%2520is%2520encoded%2520in%250Athe%2520latent%2520at%2520an%2520earlier%2520layer%2520than%2520the%2520concept%2520to%2520be%2520translated.%2520Building%2520on%250Athis%2520insight%252C%2520we%2520conduct%2520two%2520key%2520experiments.%2520First%252C%2520we%2520demonstrate%2520that%2520we%2520can%250Achange%2520the%2520concept%2520without%2520changing%2520the%2520language%2520and%2520vice%2520versa%2520through%250Aactivation%2520patching%2520alone.%2520Second%252C%2520we%2520show%2520that%2520patching%2520with%2520the%2520mean%2520over%250Alatents%2520across%2520different%2520languages%2520does%2520not%2520impair%2520and%2520instead%2520improves%2520the%250Amodels%2527%2520performance%2520in%2520translating%2520the%2520concept.%2520Our%2520results%2520provide%2520evidence%250Afor%2520the%2520existence%2520of%2520language-agnostic%2520concept%2520representations%2520within%2520the%250Ainvestigated%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers&entry.906535625=Cl%C3%A9ment%20Dumas%20and%20Chris%20Wendler%20and%20Veniamin%20Veselovsky%20and%20Giovanni%20Monea%20and%20Robert%20West&entry.1292438233=%20%20A%20central%20question%20in%20multilingual%20language%20modeling%20is%20whether%20large%0Alanguage%20models%20%28LLMs%29%20develop%20a%20universal%20concept%20representation%2C%20disentangled%0Afrom%20specific%20languages.%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20analyzing%0Alatent%20representations%20%28latents%29%20during%20a%20word%20translation%20task%20in%0Atransformer-based%20LLMs.%20We%20strategically%20extract%20latents%20from%20a%20source%0Atranslation%20prompt%20and%20insert%20them%20into%20the%20forward%20pass%20on%20a%20target%0Atranslation%20prompt.%20By%20doing%20so%2C%20we%20find%20that%20the%20output%20language%20is%20encoded%20in%0Athe%20latent%20at%20an%20earlier%20layer%20than%20the%20concept%20to%20be%20translated.%20Building%20on%0Athis%20insight%2C%20we%20conduct%20two%20key%20experiments.%20First%2C%20we%20demonstrate%20that%20we%20can%0Achange%20the%20concept%20without%20changing%20the%20language%20and%20vice%20versa%20through%0Aactivation%20patching%20alone.%20Second%2C%20we%20show%20that%20patching%20with%20the%20mean%20over%0Alatents%20across%20different%20languages%20does%20not%20impair%20and%20instead%20improves%20the%0Amodels%27%20performance%20in%20translating%20the%20concept.%20Our%20results%20provide%20evidence%0Afor%20the%20existence%20of%20language-agnostic%20concept%20representations%20within%20the%0Ainvestigated%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08745v2&entry.124074799=Read"},
{"title": "DARNet: Dual Attention Refinement Network with Spatiotemporal\n  Construction for Auditory Attention Detection", "author": "Sheng Yan and Cunhang fan and Hongyu Zhang and Xiaoke Yang and Jianhua Tao and Zhao Lv", "abstract": "  At a cocktail party, humans exhibit an impressive ability to direct their\nattention. The auditory attention detection (AAD) approach seeks to identify\nthe attended speaker by analyzing brain signals, such as EEG signals. However,\ncurrent AAD algorithms overlook the spatial distribution information within EEG\nsignals and lack the ability to capture long-range latent dependencies,\nlimiting the model's ability to decode brain activity. To address these issues,\nthis paper proposes a dual attention refinement network with spatiotemporal\nconstruction for AAD, named DARNet, which consists of the spatiotemporal\nconstruction module, dual attention refinement module, and feature fusion \\&\nclassifier module. Specifically, the spatiotemporal construction module aims to\nconstruct more expressive spatiotemporal feature representations, by capturing\nthe spatial distribution characteristics of EEG signals. The dual attention\nrefinement module aims to extract different levels of temporal patterns in EEG\nsignals and enhance the model's ability to capture long-range latent\ndependencies. The feature fusion \\& classifier module aims to aggregate\ntemporal patterns and dependencies from different levels and obtain the final\nclassification results. The experimental results indicate that compared to the\nstate-of-the-art models, DARNet achieves an average classification accuracy\nimprovement of 5.9\\% for 0.1s, 4.6\\% for 1s, and 3.9\\% for 2s on the DTU\ndataset. While maintaining excellent classification performance, DARNet\nsignificantly reduces the number of required parameters. Compared to the\nstate-of-the-art models, DARNet reduces the parameter count by 91\\%. Code is\navailable at: https://github.com/fchest/DARNet.git.\n", "link": "http://arxiv.org/abs/2410.11181v2", "date": "2024-11-18", "relevancy": 2.6082, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5305}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5293}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARNet%3A%20Dual%20Attention%20Refinement%20Network%20with%20Spatiotemporal%0A%20%20Construction%20for%20Auditory%20Attention%20Detection&body=Title%3A%20DARNet%3A%20Dual%20Attention%20Refinement%20Network%20with%20Spatiotemporal%0A%20%20Construction%20for%20Auditory%20Attention%20Detection%0AAuthor%3A%20Sheng%20Yan%20and%20Cunhang%20fan%20and%20Hongyu%20Zhang%20and%20Xiaoke%20Yang%20and%20Jianhua%20Tao%20and%20Zhao%20Lv%0AAbstract%3A%20%20%20At%20a%20cocktail%20party%2C%20humans%20exhibit%20an%20impressive%20ability%20to%20direct%20their%0Aattention.%20The%20auditory%20attention%20detection%20%28AAD%29%20approach%20seeks%20to%20identify%0Athe%20attended%20speaker%20by%20analyzing%20brain%20signals%2C%20such%20as%20EEG%20signals.%20However%2C%0Acurrent%20AAD%20algorithms%20overlook%20the%20spatial%20distribution%20information%20within%20EEG%0Asignals%20and%20lack%20the%20ability%20to%20capture%20long-range%20latent%20dependencies%2C%0Alimiting%20the%20model%27s%20ability%20to%20decode%20brain%20activity.%20To%20address%20these%20issues%2C%0Athis%20paper%20proposes%20a%20dual%20attention%20refinement%20network%20with%20spatiotemporal%0Aconstruction%20for%20AAD%2C%20named%20DARNet%2C%20which%20consists%20of%20the%20spatiotemporal%0Aconstruction%20module%2C%20dual%20attention%20refinement%20module%2C%20and%20feature%20fusion%20%5C%26%0Aclassifier%20module.%20Specifically%2C%20the%20spatiotemporal%20construction%20module%20aims%20to%0Aconstruct%20more%20expressive%20spatiotemporal%20feature%20representations%2C%20by%20capturing%0Athe%20spatial%20distribution%20characteristics%20of%20EEG%20signals.%20The%20dual%20attention%0Arefinement%20module%20aims%20to%20extract%20different%20levels%20of%20temporal%20patterns%20in%20EEG%0Asignals%20and%20enhance%20the%20model%27s%20ability%20to%20capture%20long-range%20latent%0Adependencies.%20The%20feature%20fusion%20%5C%26%20classifier%20module%20aims%20to%20aggregate%0Atemporal%20patterns%20and%20dependencies%20from%20different%20levels%20and%20obtain%20the%20final%0Aclassification%20results.%20The%20experimental%20results%20indicate%20that%20compared%20to%20the%0Astate-of-the-art%20models%2C%20DARNet%20achieves%20an%20average%20classification%20accuracy%0Aimprovement%20of%205.9%5C%25%20for%200.1s%2C%204.6%5C%25%20for%201s%2C%20and%203.9%5C%25%20for%202s%20on%20the%20DTU%0Adataset.%20While%20maintaining%20excellent%20classification%20performance%2C%20DARNet%0Asignificantly%20reduces%20the%20number%20of%20required%20parameters.%20Compared%20to%20the%0Astate-of-the-art%20models%2C%20DARNet%20reduces%20the%20parameter%20count%20by%2091%5C%25.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/fchest/DARNet.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARNet%253A%2520Dual%2520Attention%2520Refinement%2520Network%2520with%2520Spatiotemporal%250A%2520%2520Construction%2520for%2520Auditory%2520Attention%2520Detection%26entry.906535625%3DSheng%2520Yan%2520and%2520Cunhang%2520fan%2520and%2520Hongyu%2520Zhang%2520and%2520Xiaoke%2520Yang%2520and%2520Jianhua%2520Tao%2520and%2520Zhao%2520Lv%26entry.1292438233%3D%2520%2520At%2520a%2520cocktail%2520party%252C%2520humans%2520exhibit%2520an%2520impressive%2520ability%2520to%2520direct%2520their%250Aattention.%2520The%2520auditory%2520attention%2520detection%2520%2528AAD%2529%2520approach%2520seeks%2520to%2520identify%250Athe%2520attended%2520speaker%2520by%2520analyzing%2520brain%2520signals%252C%2520such%2520as%2520EEG%2520signals.%2520However%252C%250Acurrent%2520AAD%2520algorithms%2520overlook%2520the%2520spatial%2520distribution%2520information%2520within%2520EEG%250Asignals%2520and%2520lack%2520the%2520ability%2520to%2520capture%2520long-range%2520latent%2520dependencies%252C%250Alimiting%2520the%2520model%2527s%2520ability%2520to%2520decode%2520brain%2520activity.%2520To%2520address%2520these%2520issues%252C%250Athis%2520paper%2520proposes%2520a%2520dual%2520attention%2520refinement%2520network%2520with%2520spatiotemporal%250Aconstruction%2520for%2520AAD%252C%2520named%2520DARNet%252C%2520which%2520consists%2520of%2520the%2520spatiotemporal%250Aconstruction%2520module%252C%2520dual%2520attention%2520refinement%2520module%252C%2520and%2520feature%2520fusion%2520%255C%2526%250Aclassifier%2520module.%2520Specifically%252C%2520the%2520spatiotemporal%2520construction%2520module%2520aims%2520to%250Aconstruct%2520more%2520expressive%2520spatiotemporal%2520feature%2520representations%252C%2520by%2520capturing%250Athe%2520spatial%2520distribution%2520characteristics%2520of%2520EEG%2520signals.%2520The%2520dual%2520attention%250Arefinement%2520module%2520aims%2520to%2520extract%2520different%2520levels%2520of%2520temporal%2520patterns%2520in%2520EEG%250Asignals%2520and%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520capture%2520long-range%2520latent%250Adependencies.%2520The%2520feature%2520fusion%2520%255C%2526%2520classifier%2520module%2520aims%2520to%2520aggregate%250Atemporal%2520patterns%2520and%2520dependencies%2520from%2520different%2520levels%2520and%2520obtain%2520the%2520final%250Aclassification%2520results.%2520The%2520experimental%2520results%2520indicate%2520that%2520compared%2520to%2520the%250Astate-of-the-art%2520models%252C%2520DARNet%2520achieves%2520an%2520average%2520classification%2520accuracy%250Aimprovement%2520of%25205.9%255C%2525%2520for%25200.1s%252C%25204.6%255C%2525%2520for%25201s%252C%2520and%25203.9%255C%2525%2520for%25202s%2520on%2520the%2520DTU%250Adataset.%2520While%2520maintaining%2520excellent%2520classification%2520performance%252C%2520DARNet%250Asignificantly%2520reduces%2520the%2520number%2520of%2520required%2520parameters.%2520Compared%2520to%2520the%250Astate-of-the-art%2520models%252C%2520DARNet%2520reduces%2520the%2520parameter%2520count%2520by%252091%255C%2525.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/fchest/DARNet.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARNet%3A%20Dual%20Attention%20Refinement%20Network%20with%20Spatiotemporal%0A%20%20Construction%20for%20Auditory%20Attention%20Detection&entry.906535625=Sheng%20Yan%20and%20Cunhang%20fan%20and%20Hongyu%20Zhang%20and%20Xiaoke%20Yang%20and%20Jianhua%20Tao%20and%20Zhao%20Lv&entry.1292438233=%20%20At%20a%20cocktail%20party%2C%20humans%20exhibit%20an%20impressive%20ability%20to%20direct%20their%0Aattention.%20The%20auditory%20attention%20detection%20%28AAD%29%20approach%20seeks%20to%20identify%0Athe%20attended%20speaker%20by%20analyzing%20brain%20signals%2C%20such%20as%20EEG%20signals.%20However%2C%0Acurrent%20AAD%20algorithms%20overlook%20the%20spatial%20distribution%20information%20within%20EEG%0Asignals%20and%20lack%20the%20ability%20to%20capture%20long-range%20latent%20dependencies%2C%0Alimiting%20the%20model%27s%20ability%20to%20decode%20brain%20activity.%20To%20address%20these%20issues%2C%0Athis%20paper%20proposes%20a%20dual%20attention%20refinement%20network%20with%20spatiotemporal%0Aconstruction%20for%20AAD%2C%20named%20DARNet%2C%20which%20consists%20of%20the%20spatiotemporal%0Aconstruction%20module%2C%20dual%20attention%20refinement%20module%2C%20and%20feature%20fusion%20%5C%26%0Aclassifier%20module.%20Specifically%2C%20the%20spatiotemporal%20construction%20module%20aims%20to%0Aconstruct%20more%20expressive%20spatiotemporal%20feature%20representations%2C%20by%20capturing%0Athe%20spatial%20distribution%20characteristics%20of%20EEG%20signals.%20The%20dual%20attention%0Arefinement%20module%20aims%20to%20extract%20different%20levels%20of%20temporal%20patterns%20in%20EEG%0Asignals%20and%20enhance%20the%20model%27s%20ability%20to%20capture%20long-range%20latent%0Adependencies.%20The%20feature%20fusion%20%5C%26%20classifier%20module%20aims%20to%20aggregate%0Atemporal%20patterns%20and%20dependencies%20from%20different%20levels%20and%20obtain%20the%20final%0Aclassification%20results.%20The%20experimental%20results%20indicate%20that%20compared%20to%20the%0Astate-of-the-art%20models%2C%20DARNet%20achieves%20an%20average%20classification%20accuracy%0Aimprovement%20of%205.9%5C%25%20for%200.1s%2C%204.6%5C%25%20for%201s%2C%20and%203.9%5C%25%20for%202s%20on%20the%20DTU%0Adataset.%20While%20maintaining%20excellent%20classification%20performance%2C%20DARNet%0Asignificantly%20reduces%20the%20number%20of%20required%20parameters.%20Compared%20to%20the%0Astate-of-the-art%20models%2C%20DARNet%20reduces%20the%20parameter%20count%20by%2091%5C%25.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/fchest/DARNet.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11181v2&entry.124074799=Read"},
{"title": "Partial Scene Text Retrieval", "author": "Hao Wang and Minghui Liao and Zhouyi Xie and Wenyu Liu and Xiang Bai", "abstract": "  The task of partial scene text retrieval involves localizing and searching\nfor text instances that are the same or similar to a given query text from an\nimage gallery. However, existing methods can only handle text-line instances,\nleaving the problem of searching for partial patches within these text-line\ninstances unsolved due to a lack of patch annotations in the training data. To\naddress this issue, we propose a network that can simultaneously retrieve both\ntext-line instances and their partial patches. Our method embeds the two types\nof data (query text and scene text instances) into a shared feature space and\nmeasures their cross-modal similarities. To handle partial patches, our\nproposed approach adopts a Multiple Instance Learning (MIL) approach to learn\ntheir similarities with query text, without requiring extra annotations.\nHowever, constructing bags, which is a standard step of conventional MIL\napproaches, can introduce numerous noisy samples for training, and lower\ninference speed. To address this issue, we propose a Ranking MIL (RankMIL)\napproach to adaptively filter those noisy samples. Additionally, we present a\nDynamic Partial Match Algorithm (DPMA) that can directly search for the target\npartial patch from a text-line instance during the inference stage, without\nrequiring bags. This greatly improves the search efficiency and the performance\nof retrieving partial patches. The source code and dataset are available at\nhttps://github.com/lanfeng4659/PSTR.\n", "link": "http://arxiv.org/abs/2411.10261v2", "date": "2024-11-18", "relevancy": 2.6059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partial%20Scene%20Text%20Retrieval&body=Title%3A%20Partial%20Scene%20Text%20Retrieval%0AAuthor%3A%20Hao%20Wang%20and%20Minghui%20Liao%20and%20Zhouyi%20Xie%20and%20Wenyu%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20task%20of%20partial%20scene%20text%20retrieval%20involves%20localizing%20and%20searching%0Afor%20text%20instances%20that%20are%20the%20same%20or%20similar%20to%20a%20given%20query%20text%20from%20an%0Aimage%20gallery.%20However%2C%20existing%20methods%20can%20only%20handle%20text-line%20instances%2C%0Aleaving%20the%20problem%20of%20searching%20for%20partial%20patches%20within%20these%20text-line%0Ainstances%20unsolved%20due%20to%20a%20lack%20of%20patch%20annotations%20in%20the%20training%20data.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20network%20that%20can%20simultaneously%20retrieve%20both%0Atext-line%20instances%20and%20their%20partial%20patches.%20Our%20method%20embeds%20the%20two%20types%0Aof%20data%20%28query%20text%20and%20scene%20text%20instances%29%20into%20a%20shared%20feature%20space%20and%0Ameasures%20their%20cross-modal%20similarities.%20To%20handle%20partial%20patches%2C%20our%0Aproposed%20approach%20adopts%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20approach%20to%20learn%0Atheir%20similarities%20with%20query%20text%2C%20without%20requiring%20extra%20annotations.%0AHowever%2C%20constructing%20bags%2C%20which%20is%20a%20standard%20step%20of%20conventional%20MIL%0Aapproaches%2C%20can%20introduce%20numerous%20noisy%20samples%20for%20training%2C%20and%20lower%0Ainference%20speed.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Ranking%20MIL%20%28RankMIL%29%0Aapproach%20to%20adaptively%20filter%20those%20noisy%20samples.%20Additionally%2C%20we%20present%20a%0ADynamic%20Partial%20Match%20Algorithm%20%28DPMA%29%20that%20can%20directly%20search%20for%20the%20target%0Apartial%20patch%20from%20a%20text-line%20instance%20during%20the%20inference%20stage%2C%20without%0Arequiring%20bags.%20This%20greatly%20improves%20the%20search%20efficiency%20and%20the%20performance%0Aof%20retrieving%20partial%20patches.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/lanfeng4659/PSTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartial%2520Scene%2520Text%2520Retrieval%26entry.906535625%3DHao%2520Wang%2520and%2520Minghui%2520Liao%2520and%2520Zhouyi%2520Xie%2520and%2520Wenyu%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520partial%2520scene%2520text%2520retrieval%2520involves%2520localizing%2520and%2520searching%250Afor%2520text%2520instances%2520that%2520are%2520the%2520same%2520or%2520similar%2520to%2520a%2520given%2520query%2520text%2520from%2520an%250Aimage%2520gallery.%2520However%252C%2520existing%2520methods%2520can%2520only%2520handle%2520text-line%2520instances%252C%250Aleaving%2520the%2520problem%2520of%2520searching%2520for%2520partial%2520patches%2520within%2520these%2520text-line%250Ainstances%2520unsolved%2520due%2520to%2520a%2520lack%2520of%2520patch%2520annotations%2520in%2520the%2520training%2520data.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520network%2520that%2520can%2520simultaneously%2520retrieve%2520both%250Atext-line%2520instances%2520and%2520their%2520partial%2520patches.%2520Our%2520method%2520embeds%2520the%2520two%2520types%250Aof%2520data%2520%2528query%2520text%2520and%2520scene%2520text%2520instances%2529%2520into%2520a%2520shared%2520feature%2520space%2520and%250Ameasures%2520their%2520cross-modal%2520similarities.%2520To%2520handle%2520partial%2520patches%252C%2520our%250Aproposed%2520approach%2520adopts%2520a%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520approach%2520to%2520learn%250Atheir%2520similarities%2520with%2520query%2520text%252C%2520without%2520requiring%2520extra%2520annotations.%250AHowever%252C%2520constructing%2520bags%252C%2520which%2520is%2520a%2520standard%2520step%2520of%2520conventional%2520MIL%250Aapproaches%252C%2520can%2520introduce%2520numerous%2520noisy%2520samples%2520for%2520training%252C%2520and%2520lower%250Ainference%2520speed.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Ranking%2520MIL%2520%2528RankMIL%2529%250Aapproach%2520to%2520adaptively%2520filter%2520those%2520noisy%2520samples.%2520Additionally%252C%2520we%2520present%2520a%250ADynamic%2520Partial%2520Match%2520Algorithm%2520%2528DPMA%2529%2520that%2520can%2520directly%2520search%2520for%2520the%2520target%250Apartial%2520patch%2520from%2520a%2520text-line%2520instance%2520during%2520the%2520inference%2520stage%252C%2520without%250Arequiring%2520bags.%2520This%2520greatly%2520improves%2520the%2520search%2520efficiency%2520and%2520the%2520performance%250Aof%2520retrieving%2520partial%2520patches.%2520The%2520source%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/lanfeng4659/PSTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partial%20Scene%20Text%20Retrieval&entry.906535625=Hao%20Wang%20and%20Minghui%20Liao%20and%20Zhouyi%20Xie%20and%20Wenyu%20Liu%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20task%20of%20partial%20scene%20text%20retrieval%20involves%20localizing%20and%20searching%0Afor%20text%20instances%20that%20are%20the%20same%20or%20similar%20to%20a%20given%20query%20text%20from%20an%0Aimage%20gallery.%20However%2C%20existing%20methods%20can%20only%20handle%20text-line%20instances%2C%0Aleaving%20the%20problem%20of%20searching%20for%20partial%20patches%20within%20these%20text-line%0Ainstances%20unsolved%20due%20to%20a%20lack%20of%20patch%20annotations%20in%20the%20training%20data.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20network%20that%20can%20simultaneously%20retrieve%20both%0Atext-line%20instances%20and%20their%20partial%20patches.%20Our%20method%20embeds%20the%20two%20types%0Aof%20data%20%28query%20text%20and%20scene%20text%20instances%29%20into%20a%20shared%20feature%20space%20and%0Ameasures%20their%20cross-modal%20similarities.%20To%20handle%20partial%20patches%2C%20our%0Aproposed%20approach%20adopts%20a%20Multiple%20Instance%20Learning%20%28MIL%29%20approach%20to%20learn%0Atheir%20similarities%20with%20query%20text%2C%20without%20requiring%20extra%20annotations.%0AHowever%2C%20constructing%20bags%2C%20which%20is%20a%20standard%20step%20of%20conventional%20MIL%0Aapproaches%2C%20can%20introduce%20numerous%20noisy%20samples%20for%20training%2C%20and%20lower%0Ainference%20speed.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Ranking%20MIL%20%28RankMIL%29%0Aapproach%20to%20adaptively%20filter%20those%20noisy%20samples.%20Additionally%2C%20we%20present%20a%0ADynamic%20Partial%20Match%20Algorithm%20%28DPMA%29%20that%20can%20directly%20search%20for%20the%20target%0Apartial%20patch%20from%20a%20text-line%20instance%20during%20the%20inference%20stage%2C%20without%0Arequiring%20bags.%20This%20greatly%20improves%20the%20search%20efficiency%20and%20the%20performance%0Aof%20retrieving%20partial%20patches.%20The%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/lanfeng4659/PSTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10261v2&entry.124074799=Read"},
{"title": "A Recipe for CAC: Mosaic-based Generalized Loss for Improved\n  Class-Agnostic Counting", "author": "Tsung-Han Chou and Brian Wang and Wei-Chen Chiu and Jun-Cheng Chen", "abstract": "  Class agnostic counting (CAC) is a vision task that can be used to count the\ntotal occurrence number of any given reference objects in the query image. The\ntask is usually formulated as a density map estimation problem through\nsimilarity computation among a few image samples of the reference object and\nthe query image. In this paper, we point out a severe issue of the existing CAC\nframework: Given a multi-class setting, models don't consider reference images\nand instead blindly match all dominant objects in the query image. Moreover,\nthe current evaluation metrics and dataset cannot be used to faithfully assess\nthe model's generalization performance and robustness. To this end, we discover\nthat the combination of mosaic augmentation with generalized loss is essential\nfor addressing the aforementioned issue of CAC models to count objects of\nmajority (i.e. dominant objects) regardless of the references. Furthermore, we\nintroduce a new evaluation protocol and metrics for resolving the problem\nbehind the existing CAC evaluation scheme and better benchmarking CAC models in\na more fair manner. Besides, extensive evaluation results demonstrate that our\nproposed recipe can consistently improve the performance of different CAC\nmodels. The code is available at https://github.com/littlepenguin89106/MGCAC.\n", "link": "http://arxiv.org/abs/2404.09826v2", "date": "2024-11-18", "relevancy": 2.5902, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Recipe%20for%20CAC%3A%20Mosaic-based%20Generalized%20Loss%20for%20Improved%0A%20%20Class-Agnostic%20Counting&body=Title%3A%20A%20Recipe%20for%20CAC%3A%20Mosaic-based%20Generalized%20Loss%20for%20Improved%0A%20%20Class-Agnostic%20Counting%0AAuthor%3A%20Tsung-Han%20Chou%20and%20Brian%20Wang%20and%20Wei-Chen%20Chiu%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20Class%20agnostic%20counting%20%28CAC%29%20is%20a%20vision%20task%20that%20can%20be%20used%20to%20count%20the%0Atotal%20occurrence%20number%20of%20any%20given%20reference%20objects%20in%20the%20query%20image.%20The%0Atask%20is%20usually%20formulated%20as%20a%20density%20map%20estimation%20problem%20through%0Asimilarity%20computation%20among%20a%20few%20image%20samples%20of%20the%20reference%20object%20and%0Athe%20query%20image.%20In%20this%20paper%2C%20we%20point%20out%20a%20severe%20issue%20of%20the%20existing%20CAC%0Aframework%3A%20Given%20a%20multi-class%20setting%2C%20models%20don%27t%20consider%20reference%20images%0Aand%20instead%20blindly%20match%20all%20dominant%20objects%20in%20the%20query%20image.%20Moreover%2C%0Athe%20current%20evaluation%20metrics%20and%20dataset%20cannot%20be%20used%20to%20faithfully%20assess%0Athe%20model%27s%20generalization%20performance%20and%20robustness.%20To%20this%20end%2C%20we%20discover%0Athat%20the%20combination%20of%20mosaic%20augmentation%20with%20generalized%20loss%20is%20essential%0Afor%20addressing%20the%20aforementioned%20issue%20of%20CAC%20models%20to%20count%20objects%20of%0Amajority%20%28i.e.%20dominant%20objects%29%20regardless%20of%20the%20references.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20evaluation%20protocol%20and%20metrics%20for%20resolving%20the%20problem%0Abehind%20the%20existing%20CAC%20evaluation%20scheme%20and%20better%20benchmarking%20CAC%20models%20in%0Aa%20more%20fair%20manner.%20Besides%2C%20extensive%20evaluation%20results%20demonstrate%20that%20our%0Aproposed%20recipe%20can%20consistently%20improve%20the%20performance%20of%20different%20CAC%0Amodels.%20The%20code%20is%20available%20at%20https%3A//github.com/littlepenguin89106/MGCAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Recipe%2520for%2520CAC%253A%2520Mosaic-based%2520Generalized%2520Loss%2520for%2520Improved%250A%2520%2520Class-Agnostic%2520Counting%26entry.906535625%3DTsung-Han%2520Chou%2520and%2520Brian%2520Wang%2520and%2520Wei-Chen%2520Chiu%2520and%2520Jun-Cheng%2520Chen%26entry.1292438233%3D%2520%2520Class%2520agnostic%2520counting%2520%2528CAC%2529%2520is%2520a%2520vision%2520task%2520that%2520can%2520be%2520used%2520to%2520count%2520the%250Atotal%2520occurrence%2520number%2520of%2520any%2520given%2520reference%2520objects%2520in%2520the%2520query%2520image.%2520The%250Atask%2520is%2520usually%2520formulated%2520as%2520a%2520density%2520map%2520estimation%2520problem%2520through%250Asimilarity%2520computation%2520among%2520a%2520few%2520image%2520samples%2520of%2520the%2520reference%2520object%2520and%250Athe%2520query%2520image.%2520In%2520this%2520paper%252C%2520we%2520point%2520out%2520a%2520severe%2520issue%2520of%2520the%2520existing%2520CAC%250Aframework%253A%2520Given%2520a%2520multi-class%2520setting%252C%2520models%2520don%2527t%2520consider%2520reference%2520images%250Aand%2520instead%2520blindly%2520match%2520all%2520dominant%2520objects%2520in%2520the%2520query%2520image.%2520Moreover%252C%250Athe%2520current%2520evaluation%2520metrics%2520and%2520dataset%2520cannot%2520be%2520used%2520to%2520faithfully%2520assess%250Athe%2520model%2527s%2520generalization%2520performance%2520and%2520robustness.%2520To%2520this%2520end%252C%2520we%2520discover%250Athat%2520the%2520combination%2520of%2520mosaic%2520augmentation%2520with%2520generalized%2520loss%2520is%2520essential%250Afor%2520addressing%2520the%2520aforementioned%2520issue%2520of%2520CAC%2520models%2520to%2520count%2520objects%2520of%250Amajority%2520%2528i.e.%2520dominant%2520objects%2529%2520regardless%2520of%2520the%2520references.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520new%2520evaluation%2520protocol%2520and%2520metrics%2520for%2520resolving%2520the%2520problem%250Abehind%2520the%2520existing%2520CAC%2520evaluation%2520scheme%2520and%2520better%2520benchmarking%2520CAC%2520models%2520in%250Aa%2520more%2520fair%2520manner.%2520Besides%252C%2520extensive%2520evaluation%2520results%2520demonstrate%2520that%2520our%250Aproposed%2520recipe%2520can%2520consistently%2520improve%2520the%2520performance%2520of%2520different%2520CAC%250Amodels.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/littlepenguin89106/MGCAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Recipe%20for%20CAC%3A%20Mosaic-based%20Generalized%20Loss%20for%20Improved%0A%20%20Class-Agnostic%20Counting&entry.906535625=Tsung-Han%20Chou%20and%20Brian%20Wang%20and%20Wei-Chen%20Chiu%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20Class%20agnostic%20counting%20%28CAC%29%20is%20a%20vision%20task%20that%20can%20be%20used%20to%20count%20the%0Atotal%20occurrence%20number%20of%20any%20given%20reference%20objects%20in%20the%20query%20image.%20The%0Atask%20is%20usually%20formulated%20as%20a%20density%20map%20estimation%20problem%20through%0Asimilarity%20computation%20among%20a%20few%20image%20samples%20of%20the%20reference%20object%20and%0Athe%20query%20image.%20In%20this%20paper%2C%20we%20point%20out%20a%20severe%20issue%20of%20the%20existing%20CAC%0Aframework%3A%20Given%20a%20multi-class%20setting%2C%20models%20don%27t%20consider%20reference%20images%0Aand%20instead%20blindly%20match%20all%20dominant%20objects%20in%20the%20query%20image.%20Moreover%2C%0Athe%20current%20evaluation%20metrics%20and%20dataset%20cannot%20be%20used%20to%20faithfully%20assess%0Athe%20model%27s%20generalization%20performance%20and%20robustness.%20To%20this%20end%2C%20we%20discover%0Athat%20the%20combination%20of%20mosaic%20augmentation%20with%20generalized%20loss%20is%20essential%0Afor%20addressing%20the%20aforementioned%20issue%20of%20CAC%20models%20to%20count%20objects%20of%0Amajority%20%28i.e.%20dominant%20objects%29%20regardless%20of%20the%20references.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20evaluation%20protocol%20and%20metrics%20for%20resolving%20the%20problem%0Abehind%20the%20existing%20CAC%20evaluation%20scheme%20and%20better%20benchmarking%20CAC%20models%20in%0Aa%20more%20fair%20manner.%20Besides%2C%20extensive%20evaluation%20results%20demonstrate%20that%20our%0Aproposed%20recipe%20can%20consistently%20improve%20the%20performance%20of%20different%20CAC%0Amodels.%20The%20code%20is%20available%20at%20https%3A//github.com/littlepenguin89106/MGCAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09826v2&entry.124074799=Read"},
{"title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion", "author": "Philipp Allgeuer and Kyra Ahrens and Stefan Wermter", "abstract": "  We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.\n", "link": "http://arxiv.org/abs/2407.11211v3", "date": "2024-11-18", "relevancy": 2.5789, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5106}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unconstrained%20Open%20Vocabulary%20Image%20Classification%3A%20Zero-Shot%20Transfer%0A%20%20from%20Text%20to%20Image%20via%20CLIP%20Inversion&body=Title%3A%20Unconstrained%20Open%20Vocabulary%20Image%20Classification%3A%20Zero-Shot%20Transfer%0A%20%20from%20Text%20to%20Image%20via%20CLIP%20Inversion%0AAuthor%3A%20Philipp%20Allgeuer%20and%20Kyra%20Ahrens%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20We%20introduce%20NOVIC%2C%20an%20innovative%20real-time%20uNconstrained%20Open%20Vocabulary%0AImage%20Classifier%20that%20uses%20an%20autoregressive%20transformer%20to%20generatively%20output%0Aclassification%20labels%20as%20language.%20Leveraging%20the%20extensive%20knowledge%20of%20CLIP%0Amodels%2C%20NOVIC%20harnesses%20the%20embedding%20space%20to%20enable%20zero-shot%20transfer%20from%0Apure%20text%20to%20images.%20Traditional%20CLIP%20models%2C%20despite%20their%20ability%20for%20open%0Avocabulary%20classification%2C%20require%20an%20exhaustive%20prompt%20of%20potential%20class%0Alabels%2C%20restricting%20their%20application%20to%20images%20of%20known%20content%20or%20context.%20To%0Aaddress%20this%2C%20we%20propose%20an%20%22object%20decoder%22%20model%20that%20is%20trained%20on%20a%0Alarge-scale%2092M-target%20dataset%20of%20templated%20object%20noun%20sets%20and%20LLM-generated%0Acaptions%20to%20always%20output%20the%20object%20noun%20in%20question.%20This%20effectively%20inverts%0Athe%20CLIP%20text%20encoder%20and%20allows%20textual%20object%20labels%20from%20essentially%20the%0Aentire%20English%20language%20to%20be%20generated%20directly%20from%20image-derived%20embedding%0Avectors%2C%20without%20requiring%20any%20a%20priori%20knowledge%20of%20the%20potential%20content%20of%0Aan%20image%2C%20and%20without%20any%20label%20biases.%20The%20trained%20decoders%20are%20tested%20on%20a%0Amix%20of%20manually%20and%20web-curated%20datasets%2C%20as%20well%20as%20standard%20image%0Aclassification%20benchmarks%2C%20and%20achieve%20fine-grained%20prompt-free%20prediction%0Ascores%20of%20up%20to%2087.5%25%2C%20a%20strong%20result%20considering%20the%20model%20must%20work%20for%20any%0Aconceivable%20image%20and%20without%20any%20contextual%20clues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11211v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnconstrained%2520Open%2520Vocabulary%2520Image%2520Classification%253A%2520Zero-Shot%2520Transfer%250A%2520%2520from%2520Text%2520to%2520Image%2520via%2520CLIP%2520Inversion%26entry.906535625%3DPhilipp%2520Allgeuer%2520and%2520Kyra%2520Ahrens%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520We%2520introduce%2520NOVIC%252C%2520an%2520innovative%2520real-time%2520uNconstrained%2520Open%2520Vocabulary%250AImage%2520Classifier%2520that%2520uses%2520an%2520autoregressive%2520transformer%2520to%2520generatively%2520output%250Aclassification%2520labels%2520as%2520language.%2520Leveraging%2520the%2520extensive%2520knowledge%2520of%2520CLIP%250Amodels%252C%2520NOVIC%2520harnesses%2520the%2520embedding%2520space%2520to%2520enable%2520zero-shot%2520transfer%2520from%250Apure%2520text%2520to%2520images.%2520Traditional%2520CLIP%2520models%252C%2520despite%2520their%2520ability%2520for%2520open%250Avocabulary%2520classification%252C%2520require%2520an%2520exhaustive%2520prompt%2520of%2520potential%2520class%250Alabels%252C%2520restricting%2520their%2520application%2520to%2520images%2520of%2520known%2520content%2520or%2520context.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520an%2520%2522object%2520decoder%2522%2520model%2520that%2520is%2520trained%2520on%2520a%250Alarge-scale%252092M-target%2520dataset%2520of%2520templated%2520object%2520noun%2520sets%2520and%2520LLM-generated%250Acaptions%2520to%2520always%2520output%2520the%2520object%2520noun%2520in%2520question.%2520This%2520effectively%2520inverts%250Athe%2520CLIP%2520text%2520encoder%2520and%2520allows%2520textual%2520object%2520labels%2520from%2520essentially%2520the%250Aentire%2520English%2520language%2520to%2520be%2520generated%2520directly%2520from%2520image-derived%2520embedding%250Avectors%252C%2520without%2520requiring%2520any%2520a%2520priori%2520knowledge%2520of%2520the%2520potential%2520content%2520of%250Aan%2520image%252C%2520and%2520without%2520any%2520label%2520biases.%2520The%2520trained%2520decoders%2520are%2520tested%2520on%2520a%250Amix%2520of%2520manually%2520and%2520web-curated%2520datasets%252C%2520as%2520well%2520as%2520standard%2520image%250Aclassification%2520benchmarks%252C%2520and%2520achieve%2520fine-grained%2520prompt-free%2520prediction%250Ascores%2520of%2520up%2520to%252087.5%2525%252C%2520a%2520strong%2520result%2520considering%2520the%2520model%2520must%2520work%2520for%2520any%250Aconceivable%2520image%2520and%2520without%2520any%2520contextual%2520clues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11211v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconstrained%20Open%20Vocabulary%20Image%20Classification%3A%20Zero-Shot%20Transfer%0A%20%20from%20Text%20to%20Image%20via%20CLIP%20Inversion&entry.906535625=Philipp%20Allgeuer%20and%20Kyra%20Ahrens%20and%20Stefan%20Wermter&entry.1292438233=%20%20We%20introduce%20NOVIC%2C%20an%20innovative%20real-time%20uNconstrained%20Open%20Vocabulary%0AImage%20Classifier%20that%20uses%20an%20autoregressive%20transformer%20to%20generatively%20output%0Aclassification%20labels%20as%20language.%20Leveraging%20the%20extensive%20knowledge%20of%20CLIP%0Amodels%2C%20NOVIC%20harnesses%20the%20embedding%20space%20to%20enable%20zero-shot%20transfer%20from%0Apure%20text%20to%20images.%20Traditional%20CLIP%20models%2C%20despite%20their%20ability%20for%20open%0Avocabulary%20classification%2C%20require%20an%20exhaustive%20prompt%20of%20potential%20class%0Alabels%2C%20restricting%20their%20application%20to%20images%20of%20known%20content%20or%20context.%20To%0Aaddress%20this%2C%20we%20propose%20an%20%22object%20decoder%22%20model%20that%20is%20trained%20on%20a%0Alarge-scale%2092M-target%20dataset%20of%20templated%20object%20noun%20sets%20and%20LLM-generated%0Acaptions%20to%20always%20output%20the%20object%20noun%20in%20question.%20This%20effectively%20inverts%0Athe%20CLIP%20text%20encoder%20and%20allows%20textual%20object%20labels%20from%20essentially%20the%0Aentire%20English%20language%20to%20be%20generated%20directly%20from%20image-derived%20embedding%0Avectors%2C%20without%20requiring%20any%20a%20priori%20knowledge%20of%20the%20potential%20content%20of%0Aan%20image%2C%20and%20without%20any%20label%20biases.%20The%20trained%20decoders%20are%20tested%20on%20a%0Amix%20of%20manually%20and%20web-curated%20datasets%2C%20as%20well%20as%20standard%20image%0Aclassification%20benchmarks%2C%20and%20achieve%20fine-grained%20prompt-free%20prediction%0Ascores%20of%20up%20to%2087.5%25%2C%20a%20strong%20result%20considering%20the%20model%20must%20work%20for%20any%0Aconceivable%20image%20and%20without%20any%20contextual%20clues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11211v3&entry.124074799=Read"},
{"title": "Hierarchical Structure Enhances the Convergence and Generalizability of\n  Linear Molecular Representation", "author": "Juan-Ni Wu and Tong Wang and Li-Juan Tang and Hai-Long Wu and Ru-Qin Yu", "abstract": "  Language models demonstrate fundamental abilities in syntax, semantics, and\nreasoning, though their performance often depends significantly on the inputs\nthey process. This study introduces TSIS (Simplified TSID) and its\nvariants:TSISD (TSIS with Depth-First Search), TSISO (TSIS in Order), and TSISR\n(TSIS in Random), as integral components of the t-SMILES framework. These\nadditions complete the framework's design, providing diverse approaches to\nmolecular representation. Through comprehensive analysis and experiments\nemploying deep generative models, including GPT, diffusion models, and\nreinforcement learning, the findings reveal that the hierarchical structure of\nt-SMILES is more straightforward to parse than initially anticipated.\nFurthermore, t-SMILES consistently outperforms other linear representations\nsuch as SMILES, SELFIES, and SAFE, demonstrating superior convergence speed and\nenhanced generalization capabilities.\n", "link": "http://arxiv.org/abs/2402.02164v4", "date": "2024-11-18", "relevancy": 2.5636, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Structure%20Enhances%20the%20Convergence%20and%20Generalizability%20of%0A%20%20Linear%20Molecular%20Representation&body=Title%3A%20Hierarchical%20Structure%20Enhances%20the%20Convergence%20and%20Generalizability%20of%0A%20%20Linear%20Molecular%20Representation%0AAuthor%3A%20Juan-Ni%20Wu%20and%20Tong%20Wang%20and%20Li-Juan%20Tang%20and%20Hai-Long%20Wu%20and%20Ru-Qin%20Yu%0AAbstract%3A%20%20%20Language%20models%20demonstrate%20fundamental%20abilities%20in%20syntax%2C%20semantics%2C%20and%0Areasoning%2C%20though%20their%20performance%20often%20depends%20significantly%20on%20the%20inputs%0Athey%20process.%20This%20study%20introduces%20TSIS%20%28Simplified%20TSID%29%20and%20its%0Avariants%3ATSISD%20%28TSIS%20with%20Depth-First%20Search%29%2C%20TSISO%20%28TSIS%20in%20Order%29%2C%20and%20TSISR%0A%28TSIS%20in%20Random%29%2C%20as%20integral%20components%20of%20the%20t-SMILES%20framework.%20These%0Aadditions%20complete%20the%20framework%27s%20design%2C%20providing%20diverse%20approaches%20to%0Amolecular%20representation.%20Through%20comprehensive%20analysis%20and%20experiments%0Aemploying%20deep%20generative%20models%2C%20including%20GPT%2C%20diffusion%20models%2C%20and%0Areinforcement%20learning%2C%20the%20findings%20reveal%20that%20the%20hierarchical%20structure%20of%0At-SMILES%20is%20more%20straightforward%20to%20parse%20than%20initially%20anticipated.%0AFurthermore%2C%20t-SMILES%20consistently%20outperforms%20other%20linear%20representations%0Asuch%20as%20SMILES%2C%20SELFIES%2C%20and%20SAFE%2C%20demonstrating%20superior%20convergence%20speed%20and%0Aenhanced%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02164v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Structure%2520Enhances%2520the%2520Convergence%2520and%2520Generalizability%2520of%250A%2520%2520Linear%2520Molecular%2520Representation%26entry.906535625%3DJuan-Ni%2520Wu%2520and%2520Tong%2520Wang%2520and%2520Li-Juan%2520Tang%2520and%2520Hai-Long%2520Wu%2520and%2520Ru-Qin%2520Yu%26entry.1292438233%3D%2520%2520Language%2520models%2520demonstrate%2520fundamental%2520abilities%2520in%2520syntax%252C%2520semantics%252C%2520and%250Areasoning%252C%2520though%2520their%2520performance%2520often%2520depends%2520significantly%2520on%2520the%2520inputs%250Athey%2520process.%2520This%2520study%2520introduces%2520TSIS%2520%2528Simplified%2520TSID%2529%2520and%2520its%250Avariants%253ATSISD%2520%2528TSIS%2520with%2520Depth-First%2520Search%2529%252C%2520TSISO%2520%2528TSIS%2520in%2520Order%2529%252C%2520and%2520TSISR%250A%2528TSIS%2520in%2520Random%2529%252C%2520as%2520integral%2520components%2520of%2520the%2520t-SMILES%2520framework.%2520These%250Aadditions%2520complete%2520the%2520framework%2527s%2520design%252C%2520providing%2520diverse%2520approaches%2520to%250Amolecular%2520representation.%2520Through%2520comprehensive%2520analysis%2520and%2520experiments%250Aemploying%2520deep%2520generative%2520models%252C%2520including%2520GPT%252C%2520diffusion%2520models%252C%2520and%250Areinforcement%2520learning%252C%2520the%2520findings%2520reveal%2520that%2520the%2520hierarchical%2520structure%2520of%250At-SMILES%2520is%2520more%2520straightforward%2520to%2520parse%2520than%2520initially%2520anticipated.%250AFurthermore%252C%2520t-SMILES%2520consistently%2520outperforms%2520other%2520linear%2520representations%250Asuch%2520as%2520SMILES%252C%2520SELFIES%252C%2520and%2520SAFE%252C%2520demonstrating%2520superior%2520convergence%2520speed%2520and%250Aenhanced%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02164v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Structure%20Enhances%20the%20Convergence%20and%20Generalizability%20of%0A%20%20Linear%20Molecular%20Representation&entry.906535625=Juan-Ni%20Wu%20and%20Tong%20Wang%20and%20Li-Juan%20Tang%20and%20Hai-Long%20Wu%20and%20Ru-Qin%20Yu&entry.1292438233=%20%20Language%20models%20demonstrate%20fundamental%20abilities%20in%20syntax%2C%20semantics%2C%20and%0Areasoning%2C%20though%20their%20performance%20often%20depends%20significantly%20on%20the%20inputs%0Athey%20process.%20This%20study%20introduces%20TSIS%20%28Simplified%20TSID%29%20and%20its%0Avariants%3ATSISD%20%28TSIS%20with%20Depth-First%20Search%29%2C%20TSISO%20%28TSIS%20in%20Order%29%2C%20and%20TSISR%0A%28TSIS%20in%20Random%29%2C%20as%20integral%20components%20of%20the%20t-SMILES%20framework.%20These%0Aadditions%20complete%20the%20framework%27s%20design%2C%20providing%20diverse%20approaches%20to%0Amolecular%20representation.%20Through%20comprehensive%20analysis%20and%20experiments%0Aemploying%20deep%20generative%20models%2C%20including%20GPT%2C%20diffusion%20models%2C%20and%0Areinforcement%20learning%2C%20the%20findings%20reveal%20that%20the%20hierarchical%20structure%20of%0At-SMILES%20is%20more%20straightforward%20to%20parse%20than%20initially%20anticipated.%0AFurthermore%2C%20t-SMILES%20consistently%20outperforms%20other%20linear%20representations%0Asuch%20as%20SMILES%2C%20SELFIES%2C%20and%20SAFE%2C%20demonstrating%20superior%20convergence%20speed%20and%0Aenhanced%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02164v4&entry.124074799=Read"},
{"title": "Differentiable GPU-Parallelized Task and Motion Planning", "author": "William Shen and Caelan Garrett and Ankit Goyal and Tucker Hermans and Fabio Ramos", "abstract": "  We present a differentiable optimization-based framework for Task and Motion\nPlanning (TAMP) that is massively parallelizable on GPUs, enabling thousands of\nsampled seeds to be optimized simultaneously. Existing sampling-based\napproaches inherently disconnect the parameters by generating samples for each\nindependently and combining them through composition and rejection, while\noptimization-based methods struggle with highly non-convex constraints and\nlocal optima. Our method treats TAMP constraint satisfaction as optimizing a\nbatch of particles, each representing an assignment to a plan skeleton's\ncontinuous parameters. We represent the plan skeleton's constraints using\ndifferentiable cost functions, enabling us to compute the gradient of each\nparticle and update it toward satisfying solutions. Our use of GPU parallelism\nbetter covers the parameter space through scale, increasing the likelihood of\nfinding the global optima by exploring multiple basins through global sampling.\nWe demonstrate that our algorithm can effectively solve a highly constrained\nTetris packing problem using a Franka arm in simulation and deploy our planner\non a real robot arm. Website: https://williamshen-nz.github.io/gpu-tamp\n", "link": "http://arxiv.org/abs/2411.11833v1", "date": "2024-11-18", "relevancy": 2.5634, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5313}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5053}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning&body=Title%3A%20Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning%0AAuthor%3A%20William%20Shen%20and%20Caelan%20Garrett%20and%20Ankit%20Goyal%20and%20Tucker%20Hermans%20and%20Fabio%20Ramos%0AAbstract%3A%20%20%20We%20present%20a%20differentiable%20optimization-based%20framework%20for%20Task%20and%20Motion%0APlanning%20%28TAMP%29%20that%20is%20massively%20parallelizable%20on%20GPUs%2C%20enabling%20thousands%20of%0Asampled%20seeds%20to%20be%20optimized%20simultaneously.%20Existing%20sampling-based%0Aapproaches%20inherently%20disconnect%20the%20parameters%20by%20generating%20samples%20for%20each%0Aindependently%20and%20combining%20them%20through%20composition%20and%20rejection%2C%20while%0Aoptimization-based%20methods%20struggle%20with%20highly%20non-convex%20constraints%20and%0Alocal%20optima.%20Our%20method%20treats%20TAMP%20constraint%20satisfaction%20as%20optimizing%20a%0Abatch%20of%20particles%2C%20each%20representing%20an%20assignment%20to%20a%20plan%20skeleton%27s%0Acontinuous%20parameters.%20We%20represent%20the%20plan%20skeleton%27s%20constraints%20using%0Adifferentiable%20cost%20functions%2C%20enabling%20us%20to%20compute%20the%20gradient%20of%20each%0Aparticle%20and%20update%20it%20toward%20satisfying%20solutions.%20Our%20use%20of%20GPU%20parallelism%0Abetter%20covers%20the%20parameter%20space%20through%20scale%2C%20increasing%20the%20likelihood%20of%0Afinding%20the%20global%20optima%20by%20exploring%20multiple%20basins%20through%20global%20sampling.%0AWe%20demonstrate%20that%20our%20algorithm%20can%20effectively%20solve%20a%20highly%20constrained%0ATetris%20packing%20problem%20using%20a%20Franka%20arm%20in%20simulation%20and%20deploy%20our%20planner%0Aon%20a%20real%20robot%20arm.%20Website%3A%20https%3A//williamshen-nz.github.io/gpu-tamp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520GPU-Parallelized%2520Task%2520and%2520Motion%2520Planning%26entry.906535625%3DWilliam%2520Shen%2520and%2520Caelan%2520Garrett%2520and%2520Ankit%2520Goyal%2520and%2520Tucker%2520Hermans%2520and%2520Fabio%2520Ramos%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520differentiable%2520optimization-based%2520framework%2520for%2520Task%2520and%2520Motion%250APlanning%2520%2528TAMP%2529%2520that%2520is%2520massively%2520parallelizable%2520on%2520GPUs%252C%2520enabling%2520thousands%2520of%250Asampled%2520seeds%2520to%2520be%2520optimized%2520simultaneously.%2520Existing%2520sampling-based%250Aapproaches%2520inherently%2520disconnect%2520the%2520parameters%2520by%2520generating%2520samples%2520for%2520each%250Aindependently%2520and%2520combining%2520them%2520through%2520composition%2520and%2520rejection%252C%2520while%250Aoptimization-based%2520methods%2520struggle%2520with%2520highly%2520non-convex%2520constraints%2520and%250Alocal%2520optima.%2520Our%2520method%2520treats%2520TAMP%2520constraint%2520satisfaction%2520as%2520optimizing%2520a%250Abatch%2520of%2520particles%252C%2520each%2520representing%2520an%2520assignment%2520to%2520a%2520plan%2520skeleton%2527s%250Acontinuous%2520parameters.%2520We%2520represent%2520the%2520plan%2520skeleton%2527s%2520constraints%2520using%250Adifferentiable%2520cost%2520functions%252C%2520enabling%2520us%2520to%2520compute%2520the%2520gradient%2520of%2520each%250Aparticle%2520and%2520update%2520it%2520toward%2520satisfying%2520solutions.%2520Our%2520use%2520of%2520GPU%2520parallelism%250Abetter%2520covers%2520the%2520parameter%2520space%2520through%2520scale%252C%2520increasing%2520the%2520likelihood%2520of%250Afinding%2520the%2520global%2520optima%2520by%2520exploring%2520multiple%2520basins%2520through%2520global%2520sampling.%250AWe%2520demonstrate%2520that%2520our%2520algorithm%2520can%2520effectively%2520solve%2520a%2520highly%2520constrained%250ATetris%2520packing%2520problem%2520using%2520a%2520Franka%2520arm%2520in%2520simulation%2520and%2520deploy%2520our%2520planner%250Aon%2520a%2520real%2520robot%2520arm.%2520Website%253A%2520https%253A//williamshen-nz.github.io/gpu-tamp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning&entry.906535625=William%20Shen%20and%20Caelan%20Garrett%20and%20Ankit%20Goyal%20and%20Tucker%20Hermans%20and%20Fabio%20Ramos&entry.1292438233=%20%20We%20present%20a%20differentiable%20optimization-based%20framework%20for%20Task%20and%20Motion%0APlanning%20%28TAMP%29%20that%20is%20massively%20parallelizable%20on%20GPUs%2C%20enabling%20thousands%20of%0Asampled%20seeds%20to%20be%20optimized%20simultaneously.%20Existing%20sampling-based%0Aapproaches%20inherently%20disconnect%20the%20parameters%20by%20generating%20samples%20for%20each%0Aindependently%20and%20combining%20them%20through%20composition%20and%20rejection%2C%20while%0Aoptimization-based%20methods%20struggle%20with%20highly%20non-convex%20constraints%20and%0Alocal%20optima.%20Our%20method%20treats%20TAMP%20constraint%20satisfaction%20as%20optimizing%20a%0Abatch%20of%20particles%2C%20each%20representing%20an%20assignment%20to%20a%20plan%20skeleton%27s%0Acontinuous%20parameters.%20We%20represent%20the%20plan%20skeleton%27s%20constraints%20using%0Adifferentiable%20cost%20functions%2C%20enabling%20us%20to%20compute%20the%20gradient%20of%20each%0Aparticle%20and%20update%20it%20toward%20satisfying%20solutions.%20Our%20use%20of%20GPU%20parallelism%0Abetter%20covers%20the%20parameter%20space%20through%20scale%2C%20increasing%20the%20likelihood%20of%0Afinding%20the%20global%20optima%20by%20exploring%20multiple%20basins%20through%20global%20sampling.%0AWe%20demonstrate%20that%20our%20algorithm%20can%20effectively%20solve%20a%20highly%20constrained%0ATetris%20packing%20problem%20using%20a%20Franka%20arm%20in%20simulation%20and%20deploy%20our%20planner%0Aon%20a%20real%20robot%20arm.%20Website%3A%20https%3A//williamshen-nz.github.io/gpu-tamp%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11833v1&entry.124074799=Read"},
{"title": "SignEye: Traffic Sign Interpretation from Vehicle First-Person View", "author": "Chuang Yang and Xu Han and Tao Han and Yuejiao SU and Junyu Gao and Hongyuan Zhang and Yi Wang and Lap-Pui Chau", "abstract": "  Traffic signs play a key role in assisting autonomous driving systems (ADS)\nby enabling the assessment of vehicle behavior in compliance with traffic\nregulations and providing navigation instructions. However, current works are\nlimited to basic sign understanding without considering the egocentric\nvehicle's spatial position, which fails to support further regulation\nassessment and direction navigation. Following the above issues, we introduce a\nnew task: traffic sign interpretation from the vehicle's first-person view,\nreferred to as TSI-FPV. Meanwhile, we develop a traffic guidance assistant\n(TGA) scenario application to re-explore the role of traffic signs in ADS as a\ncomplement to popular autonomous technologies (such as obstacle perception).\nNotably, TGA is not a replacement for electronic map navigation; rather, TGA\ncan be an automatic tool for updating it and complementing it in situations\nsuch as offline conditions or temporary sign adjustments. Lastly, a spatial and\nsemantic logic-aware stepwise reasoning pipeline (SignEye) is constructed to\nachieve the TSI-FPV and TGA, and an application-specific dataset (Traffic-CN)\nis built. Experiments show that TSI-FPV and TGA are achievable via our SignEye\ntrained on Traffic-CN. The results also demonstrate that the TGA can provide\ncomplementary information to ADS beyond existing popular autonomous\ntechnologies.\n", "link": "http://arxiv.org/abs/2411.11507v1", "date": "2024-11-18", "relevancy": 2.5615, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5284}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignEye%3A%20Traffic%20Sign%20Interpretation%20from%20Vehicle%20First-Person%20View&body=Title%3A%20SignEye%3A%20Traffic%20Sign%20Interpretation%20from%20Vehicle%20First-Person%20View%0AAuthor%3A%20Chuang%20Yang%20and%20Xu%20Han%20and%20Tao%20Han%20and%20Yuejiao%20SU%20and%20Junyu%20Gao%20and%20Hongyuan%20Zhang%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau%0AAbstract%3A%20%20%20Traffic%20signs%20play%20a%20key%20role%20in%20assisting%20autonomous%20driving%20systems%20%28ADS%29%0Aby%20enabling%20the%20assessment%20of%20vehicle%20behavior%20in%20compliance%20with%20traffic%0Aregulations%20and%20providing%20navigation%20instructions.%20However%2C%20current%20works%20are%0Alimited%20to%20basic%20sign%20understanding%20without%20considering%20the%20egocentric%0Avehicle%27s%20spatial%20position%2C%20which%20fails%20to%20support%20further%20regulation%0Aassessment%20and%20direction%20navigation.%20Following%20the%20above%20issues%2C%20we%20introduce%20a%0Anew%20task%3A%20traffic%20sign%20interpretation%20from%20the%20vehicle%27s%20first-person%20view%2C%0Areferred%20to%20as%20TSI-FPV.%20Meanwhile%2C%20we%20develop%20a%20traffic%20guidance%20assistant%0A%28TGA%29%20scenario%20application%20to%20re-explore%20the%20role%20of%20traffic%20signs%20in%20ADS%20as%20a%0Acomplement%20to%20popular%20autonomous%20technologies%20%28such%20as%20obstacle%20perception%29.%0ANotably%2C%20TGA%20is%20not%20a%20replacement%20for%20electronic%20map%20navigation%3B%20rather%2C%20TGA%0Acan%20be%20an%20automatic%20tool%20for%20updating%20it%20and%20complementing%20it%20in%20situations%0Asuch%20as%20offline%20conditions%20or%20temporary%20sign%20adjustments.%20Lastly%2C%20a%20spatial%20and%0Asemantic%20logic-aware%20stepwise%20reasoning%20pipeline%20%28SignEye%29%20is%20constructed%20to%0Aachieve%20the%20TSI-FPV%20and%20TGA%2C%20and%20an%20application-specific%20dataset%20%28Traffic-CN%29%0Ais%20built.%20Experiments%20show%20that%20TSI-FPV%20and%20TGA%20are%20achievable%20via%20our%20SignEye%0Atrained%20on%20Traffic-CN.%20The%20results%20also%20demonstrate%20that%20the%20TGA%20can%20provide%0Acomplementary%20information%20to%20ADS%20beyond%20existing%20popular%20autonomous%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignEye%253A%2520Traffic%2520Sign%2520Interpretation%2520from%2520Vehicle%2520First-Person%2520View%26entry.906535625%3DChuang%2520Yang%2520and%2520Xu%2520Han%2520and%2520Tao%2520Han%2520and%2520Yuejiao%2520SU%2520and%2520Junyu%2520Gao%2520and%2520Hongyuan%2520Zhang%2520and%2520Yi%2520Wang%2520and%2520Lap-Pui%2520Chau%26entry.1292438233%3D%2520%2520Traffic%2520signs%2520play%2520a%2520key%2520role%2520in%2520assisting%2520autonomous%2520driving%2520systems%2520%2528ADS%2529%250Aby%2520enabling%2520the%2520assessment%2520of%2520vehicle%2520behavior%2520in%2520compliance%2520with%2520traffic%250Aregulations%2520and%2520providing%2520navigation%2520instructions.%2520However%252C%2520current%2520works%2520are%250Alimited%2520to%2520basic%2520sign%2520understanding%2520without%2520considering%2520the%2520egocentric%250Avehicle%2527s%2520spatial%2520position%252C%2520which%2520fails%2520to%2520support%2520further%2520regulation%250Aassessment%2520and%2520direction%2520navigation.%2520Following%2520the%2520above%2520issues%252C%2520we%2520introduce%2520a%250Anew%2520task%253A%2520traffic%2520sign%2520interpretation%2520from%2520the%2520vehicle%2527s%2520first-person%2520view%252C%250Areferred%2520to%2520as%2520TSI-FPV.%2520Meanwhile%252C%2520we%2520develop%2520a%2520traffic%2520guidance%2520assistant%250A%2528TGA%2529%2520scenario%2520application%2520to%2520re-explore%2520the%2520role%2520of%2520traffic%2520signs%2520in%2520ADS%2520as%2520a%250Acomplement%2520to%2520popular%2520autonomous%2520technologies%2520%2528such%2520as%2520obstacle%2520perception%2529.%250ANotably%252C%2520TGA%2520is%2520not%2520a%2520replacement%2520for%2520electronic%2520map%2520navigation%253B%2520rather%252C%2520TGA%250Acan%2520be%2520an%2520automatic%2520tool%2520for%2520updating%2520it%2520and%2520complementing%2520it%2520in%2520situations%250Asuch%2520as%2520offline%2520conditions%2520or%2520temporary%2520sign%2520adjustments.%2520Lastly%252C%2520a%2520spatial%2520and%250Asemantic%2520logic-aware%2520stepwise%2520reasoning%2520pipeline%2520%2528SignEye%2529%2520is%2520constructed%2520to%250Aachieve%2520the%2520TSI-FPV%2520and%2520TGA%252C%2520and%2520an%2520application-specific%2520dataset%2520%2528Traffic-CN%2529%250Ais%2520built.%2520Experiments%2520show%2520that%2520TSI-FPV%2520and%2520TGA%2520are%2520achievable%2520via%2520our%2520SignEye%250Atrained%2520on%2520Traffic-CN.%2520The%2520results%2520also%2520demonstrate%2520that%2520the%2520TGA%2520can%2520provide%250Acomplementary%2520information%2520to%2520ADS%2520beyond%2520existing%2520popular%2520autonomous%250Atechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignEye%3A%20Traffic%20Sign%20Interpretation%20from%20Vehicle%20First-Person%20View&entry.906535625=Chuang%20Yang%20and%20Xu%20Han%20and%20Tao%20Han%20and%20Yuejiao%20SU%20and%20Junyu%20Gao%20and%20Hongyuan%20Zhang%20and%20Yi%20Wang%20and%20Lap-Pui%20Chau&entry.1292438233=%20%20Traffic%20signs%20play%20a%20key%20role%20in%20assisting%20autonomous%20driving%20systems%20%28ADS%29%0Aby%20enabling%20the%20assessment%20of%20vehicle%20behavior%20in%20compliance%20with%20traffic%0Aregulations%20and%20providing%20navigation%20instructions.%20However%2C%20current%20works%20are%0Alimited%20to%20basic%20sign%20understanding%20without%20considering%20the%20egocentric%0Avehicle%27s%20spatial%20position%2C%20which%20fails%20to%20support%20further%20regulation%0Aassessment%20and%20direction%20navigation.%20Following%20the%20above%20issues%2C%20we%20introduce%20a%0Anew%20task%3A%20traffic%20sign%20interpretation%20from%20the%20vehicle%27s%20first-person%20view%2C%0Areferred%20to%20as%20TSI-FPV.%20Meanwhile%2C%20we%20develop%20a%20traffic%20guidance%20assistant%0A%28TGA%29%20scenario%20application%20to%20re-explore%20the%20role%20of%20traffic%20signs%20in%20ADS%20as%20a%0Acomplement%20to%20popular%20autonomous%20technologies%20%28such%20as%20obstacle%20perception%29.%0ANotably%2C%20TGA%20is%20not%20a%20replacement%20for%20electronic%20map%20navigation%3B%20rather%2C%20TGA%0Acan%20be%20an%20automatic%20tool%20for%20updating%20it%20and%20complementing%20it%20in%20situations%0Asuch%20as%20offline%20conditions%20or%20temporary%20sign%20adjustments.%20Lastly%2C%20a%20spatial%20and%0Asemantic%20logic-aware%20stepwise%20reasoning%20pipeline%20%28SignEye%29%20is%20constructed%20to%0Aachieve%20the%20TSI-FPV%20and%20TGA%2C%20and%20an%20application-specific%20dataset%20%28Traffic-CN%29%0Ais%20built.%20Experiments%20show%20that%20TSI-FPV%20and%20TGA%20are%20achievable%20via%20our%20SignEye%0Atrained%20on%20Traffic-CN.%20The%20results%20also%20demonstrate%20that%20the%20TGA%20can%20provide%0Acomplementary%20information%20to%20ADS%20beyond%20existing%20popular%20autonomous%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11507v1&entry.124074799=Read"},
{"title": "HistoEncoder: a digital pathology foundation model for prostate cancer", "author": "Joona Pohjonen and Abderrahim-Oussama Batouche and Antti Rannikko and Kevin Sandeman and Andrew Erickson and Esa Pitkanen and Tuomas Mirtti", "abstract": "  Foundation models are trained on massive amounts of data to distinguish\ncomplex patterns and can be adapted to a wide range of downstream tasks with\nminimal computational resources. Here, we develop a foundation model for\nprostate cancer digital pathology called HistoEncoder by pre-training on 48\nmillion prostate tissue tile images. We demonstrate that HistoEncoder features\nextracted from tile images with similar histological patterns map closely\ntogether in the feature space. HistoEncoder outperforms models pre-trained with\nnatural images, even without fine-tuning or with 1000 times less training data.\nWe describe two use cases that leverage the capabilities of HistoEncoder by\nfine-tuning the model with a limited amount of data and computational\nresources. First, we show how HistoEncoder can be used to automatically\nannotate large-scale datasets with high accuracy. Second, we combine histomics\nwith commonly used clinical nomograms, significantly improving prostate\ncancer-specific death survival models. Foundation models such as HistoEncoder\ncan allow organizations with limited resources to build effective clinical\nsoftware tools without needing extensive datasets or significant amounts of\ncomputing.\n", "link": "http://arxiv.org/abs/2411.11458v1", "date": "2024-11-18", "relevancy": 2.5612, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer&body=Title%3A%20HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer%0AAuthor%3A%20Joona%20Pohjonen%20and%20Abderrahim-Oussama%20Batouche%20and%20Antti%20Rannikko%20and%20Kevin%20Sandeman%20and%20Andrew%20Erickson%20and%20Esa%20Pitkanen%20and%20Tuomas%20Mirtti%0AAbstract%3A%20%20%20Foundation%20models%20are%20trained%20on%20massive%20amounts%20of%20data%20to%20distinguish%0Acomplex%20patterns%20and%20can%20be%20adapted%20to%20a%20wide%20range%20of%20downstream%20tasks%20with%0Aminimal%20computational%20resources.%20Here%2C%20we%20develop%20a%20foundation%20model%20for%0Aprostate%20cancer%20digital%20pathology%20called%20HistoEncoder%20by%20pre-training%20on%2048%0Amillion%20prostate%20tissue%20tile%20images.%20We%20demonstrate%20that%20HistoEncoder%20features%0Aextracted%20from%20tile%20images%20with%20similar%20histological%20patterns%20map%20closely%0Atogether%20in%20the%20feature%20space.%20HistoEncoder%20outperforms%20models%20pre-trained%20with%0Anatural%20images%2C%20even%20without%20fine-tuning%20or%20with%201000%20times%20less%20training%20data.%0AWe%20describe%20two%20use%20cases%20that%20leverage%20the%20capabilities%20of%20HistoEncoder%20by%0Afine-tuning%20the%20model%20with%20a%20limited%20amount%20of%20data%20and%20computational%0Aresources.%20First%2C%20we%20show%20how%20HistoEncoder%20can%20be%20used%20to%20automatically%0Aannotate%20large-scale%20datasets%20with%20high%20accuracy.%20Second%2C%20we%20combine%20histomics%0Awith%20commonly%20used%20clinical%20nomograms%2C%20significantly%20improving%20prostate%0Acancer-specific%20death%20survival%20models.%20Foundation%20models%20such%20as%20HistoEncoder%0Acan%20allow%20organizations%20with%20limited%20resources%20to%20build%20effective%20clinical%0Asoftware%20tools%20without%20needing%20extensive%20datasets%20or%20significant%20amounts%20of%0Acomputing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistoEncoder%253A%2520a%2520digital%2520pathology%2520foundation%2520model%2520for%2520prostate%2520cancer%26entry.906535625%3DJoona%2520Pohjonen%2520and%2520Abderrahim-Oussama%2520Batouche%2520and%2520Antti%2520Rannikko%2520and%2520Kevin%2520Sandeman%2520and%2520Andrew%2520Erickson%2520and%2520Esa%2520Pitkanen%2520and%2520Tuomas%2520Mirtti%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520trained%2520on%2520massive%2520amounts%2520of%2520data%2520to%2520distinguish%250Acomplex%2520patterns%2520and%2520can%2520be%2520adapted%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%2520with%250Aminimal%2520computational%2520resources.%2520Here%252C%2520we%2520develop%2520a%2520foundation%2520model%2520for%250Aprostate%2520cancer%2520digital%2520pathology%2520called%2520HistoEncoder%2520by%2520pre-training%2520on%252048%250Amillion%2520prostate%2520tissue%2520tile%2520images.%2520We%2520demonstrate%2520that%2520HistoEncoder%2520features%250Aextracted%2520from%2520tile%2520images%2520with%2520similar%2520histological%2520patterns%2520map%2520closely%250Atogether%2520in%2520the%2520feature%2520space.%2520HistoEncoder%2520outperforms%2520models%2520pre-trained%2520with%250Anatural%2520images%252C%2520even%2520without%2520fine-tuning%2520or%2520with%25201000%2520times%2520less%2520training%2520data.%250AWe%2520describe%2520two%2520use%2520cases%2520that%2520leverage%2520the%2520capabilities%2520of%2520HistoEncoder%2520by%250Afine-tuning%2520the%2520model%2520with%2520a%2520limited%2520amount%2520of%2520data%2520and%2520computational%250Aresources.%2520First%252C%2520we%2520show%2520how%2520HistoEncoder%2520can%2520be%2520used%2520to%2520automatically%250Aannotate%2520large-scale%2520datasets%2520with%2520high%2520accuracy.%2520Second%252C%2520we%2520combine%2520histomics%250Awith%2520commonly%2520used%2520clinical%2520nomograms%252C%2520significantly%2520improving%2520prostate%250Acancer-specific%2520death%2520survival%2520models.%2520Foundation%2520models%2520such%2520as%2520HistoEncoder%250Acan%2520allow%2520organizations%2520with%2520limited%2520resources%2520to%2520build%2520effective%2520clinical%250Asoftware%2520tools%2520without%2520needing%2520extensive%2520datasets%2520or%2520significant%2520amounts%2520of%250Acomputing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer&entry.906535625=Joona%20Pohjonen%20and%20Abderrahim-Oussama%20Batouche%20and%20Antti%20Rannikko%20and%20Kevin%20Sandeman%20and%20Andrew%20Erickson%20and%20Esa%20Pitkanen%20and%20Tuomas%20Mirtti&entry.1292438233=%20%20Foundation%20models%20are%20trained%20on%20massive%20amounts%20of%20data%20to%20distinguish%0Acomplex%20patterns%20and%20can%20be%20adapted%20to%20a%20wide%20range%20of%20downstream%20tasks%20with%0Aminimal%20computational%20resources.%20Here%2C%20we%20develop%20a%20foundation%20model%20for%0Aprostate%20cancer%20digital%20pathology%20called%20HistoEncoder%20by%20pre-training%20on%2048%0Amillion%20prostate%20tissue%20tile%20images.%20We%20demonstrate%20that%20HistoEncoder%20features%0Aextracted%20from%20tile%20images%20with%20similar%20histological%20patterns%20map%20closely%0Atogether%20in%20the%20feature%20space.%20HistoEncoder%20outperforms%20models%20pre-trained%20with%0Anatural%20images%2C%20even%20without%20fine-tuning%20or%20with%201000%20times%20less%20training%20data.%0AWe%20describe%20two%20use%20cases%20that%20leverage%20the%20capabilities%20of%20HistoEncoder%20by%0Afine-tuning%20the%20model%20with%20a%20limited%20amount%20of%20data%20and%20computational%0Aresources.%20First%2C%20we%20show%20how%20HistoEncoder%20can%20be%20used%20to%20automatically%0Aannotate%20large-scale%20datasets%20with%20high%20accuracy.%20Second%2C%20we%20combine%20histomics%0Awith%20commonly%20used%20clinical%20nomograms%2C%20significantly%20improving%20prostate%0Acancer-specific%20death%20survival%20models.%20Foundation%20models%20such%20as%20HistoEncoder%0Acan%20allow%20organizations%20with%20limited%20resources%20to%20build%20effective%20clinical%0Asoftware%20tools%20without%20needing%20extensive%20datasets%20or%20significant%20amounts%20of%0Acomputing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11458v1&entry.124074799=Read"},
{"title": "Feature-wise and Sample-wise Adaptive Transfer Learning for\n  High-dimensional Linear Regression", "author": "Zelin He and Ying Sun and Jingyuan Liu and Runze Li", "abstract": "  We consider the transfer learning problem in the high dimensional linear\nregression setting, where the feature dimension is larger than the sample size.\nTo learn transferable information, which may vary across features or the source\nsamples, we propose an adaptive transfer learning method that can detect and\naggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans)\ntransferable structures. We achieve this by employing a fused-penalty, coupled\nwith weights that can adapt according to the transferable structure. To choose\nthe weight, we propose a theoretically informed, data-driven procedure,\nenabling F-AdaTrans to selectively fuse the transferable signals with the\ntarget while filtering out non-transferable signals, and S-AdaTrans to obtain\nthe optimal combination of information transferred from each source sample. We\nshow that, with appropriately chosen weights, F-AdaTrans achieves a convergence\nrate close to that of an oracle estimator with a known transferable structure,\nand S-AdaTrans recovers existing near-minimax optimal rates as a special case.\nThe effectiveness of the proposed method is validated using both simulation and\nreal data, demonstrating favorable performance compared to the existing\nmethods.\n", "link": "http://arxiv.org/abs/2403.13565v2", "date": "2024-11-18", "relevancy": 2.5398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Linear%20Regression&body=Title%3A%20Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Linear%20Regression%0AAuthor%3A%20Zelin%20He%20and%20Ying%20Sun%20and%20Jingyuan%20Liu%20and%20Runze%20Li%0AAbstract%3A%20%20%20We%20consider%20the%20transfer%20learning%20problem%20in%20the%20high%20dimensional%20linear%0Aregression%20setting%2C%20where%20the%20feature%20dimension%20is%20larger%20than%20the%20sample%20size.%0ATo%20learn%20transferable%20information%2C%20which%20may%20vary%20across%20features%20or%20the%20source%0Asamples%2C%20we%20propose%20an%20adaptive%20transfer%20learning%20method%20that%20can%20detect%20and%0Aaggregate%20the%20feature-wise%20%28F-AdaTrans%29%20or%20sample-wise%20%28S-AdaTrans%29%0Atransferable%20structures.%20We%20achieve%20this%20by%20employing%20a%20fused-penalty%2C%20coupled%0Awith%20weights%20that%20can%20adapt%20according%20to%20the%20transferable%20structure.%20To%20choose%0Athe%20weight%2C%20we%20propose%20a%20theoretically%20informed%2C%20data-driven%20procedure%2C%0Aenabling%20F-AdaTrans%20to%20selectively%20fuse%20the%20transferable%20signals%20with%20the%0Atarget%20while%20filtering%20out%20non-transferable%20signals%2C%20and%20S-AdaTrans%20to%20obtain%0Athe%20optimal%20combination%20of%20information%20transferred%20from%20each%20source%20sample.%20We%0Ashow%20that%2C%20with%20appropriately%20chosen%20weights%2C%20F-AdaTrans%20achieves%20a%20convergence%0Arate%20close%20to%20that%20of%20an%20oracle%20estimator%20with%20a%20known%20transferable%20structure%2C%0Aand%20S-AdaTrans%20recovers%20existing%20near-minimax%20optimal%20rates%20as%20a%20special%20case.%0AThe%20effectiveness%20of%20the%20proposed%20method%20is%20validated%20using%20both%20simulation%20and%0Areal%20data%2C%20demonstrating%20favorable%20performance%20compared%20to%20the%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-wise%2520and%2520Sample-wise%2520Adaptive%2520Transfer%2520Learning%2520for%250A%2520%2520High-dimensional%2520Linear%2520Regression%26entry.906535625%3DZelin%2520He%2520and%2520Ying%2520Sun%2520and%2520Jingyuan%2520Liu%2520and%2520Runze%2520Li%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520transfer%2520learning%2520problem%2520in%2520the%2520high%2520dimensional%2520linear%250Aregression%2520setting%252C%2520where%2520the%2520feature%2520dimension%2520is%2520larger%2520than%2520the%2520sample%2520size.%250ATo%2520learn%2520transferable%2520information%252C%2520which%2520may%2520vary%2520across%2520features%2520or%2520the%2520source%250Asamples%252C%2520we%2520propose%2520an%2520adaptive%2520transfer%2520learning%2520method%2520that%2520can%2520detect%2520and%250Aaggregate%2520the%2520feature-wise%2520%2528F-AdaTrans%2529%2520or%2520sample-wise%2520%2528S-AdaTrans%2529%250Atransferable%2520structures.%2520We%2520achieve%2520this%2520by%2520employing%2520a%2520fused-penalty%252C%2520coupled%250Awith%2520weights%2520that%2520can%2520adapt%2520according%2520to%2520the%2520transferable%2520structure.%2520To%2520choose%250Athe%2520weight%252C%2520we%2520propose%2520a%2520theoretically%2520informed%252C%2520data-driven%2520procedure%252C%250Aenabling%2520F-AdaTrans%2520to%2520selectively%2520fuse%2520the%2520transferable%2520signals%2520with%2520the%250Atarget%2520while%2520filtering%2520out%2520non-transferable%2520signals%252C%2520and%2520S-AdaTrans%2520to%2520obtain%250Athe%2520optimal%2520combination%2520of%2520information%2520transferred%2520from%2520each%2520source%2520sample.%2520We%250Ashow%2520that%252C%2520with%2520appropriately%2520chosen%2520weights%252C%2520F-AdaTrans%2520achieves%2520a%2520convergence%250Arate%2520close%2520to%2520that%2520of%2520an%2520oracle%2520estimator%2520with%2520a%2520known%2520transferable%2520structure%252C%250Aand%2520S-AdaTrans%2520recovers%2520existing%2520near-minimax%2520optimal%2520rates%2520as%2520a%2520special%2520case.%250AThe%2520effectiveness%2520of%2520the%2520proposed%2520method%2520is%2520validated%2520using%2520both%2520simulation%2520and%250Areal%2520data%252C%2520demonstrating%2520favorable%2520performance%2520compared%2520to%2520the%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-wise%20and%20Sample-wise%20Adaptive%20Transfer%20Learning%20for%0A%20%20High-dimensional%20Linear%20Regression&entry.906535625=Zelin%20He%20and%20Ying%20Sun%20and%20Jingyuan%20Liu%20and%20Runze%20Li&entry.1292438233=%20%20We%20consider%20the%20transfer%20learning%20problem%20in%20the%20high%20dimensional%20linear%0Aregression%20setting%2C%20where%20the%20feature%20dimension%20is%20larger%20than%20the%20sample%20size.%0ATo%20learn%20transferable%20information%2C%20which%20may%20vary%20across%20features%20or%20the%20source%0Asamples%2C%20we%20propose%20an%20adaptive%20transfer%20learning%20method%20that%20can%20detect%20and%0Aaggregate%20the%20feature-wise%20%28F-AdaTrans%29%20or%20sample-wise%20%28S-AdaTrans%29%0Atransferable%20structures.%20We%20achieve%20this%20by%20employing%20a%20fused-penalty%2C%20coupled%0Awith%20weights%20that%20can%20adapt%20according%20to%20the%20transferable%20structure.%20To%20choose%0Athe%20weight%2C%20we%20propose%20a%20theoretically%20informed%2C%20data-driven%20procedure%2C%0Aenabling%20F-AdaTrans%20to%20selectively%20fuse%20the%20transferable%20signals%20with%20the%0Atarget%20while%20filtering%20out%20non-transferable%20signals%2C%20and%20S-AdaTrans%20to%20obtain%0Athe%20optimal%20combination%20of%20information%20transferred%20from%20each%20source%20sample.%20We%0Ashow%20that%2C%20with%20appropriately%20chosen%20weights%2C%20F-AdaTrans%20achieves%20a%20convergence%0Arate%20close%20to%20that%20of%20an%20oracle%20estimator%20with%20a%20known%20transferable%20structure%2C%0Aand%20S-AdaTrans%20recovers%20existing%20near-minimax%20optimal%20rates%20as%20a%20special%20case.%0AThe%20effectiveness%20of%20the%20proposed%20method%20is%20validated%20using%20both%20simulation%20and%0Areal%20data%2C%20demonstrating%20favorable%20performance%20compared%20to%20the%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13565v2&entry.124074799=Read"},
{"title": "RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator", "author": "Xinhai Li and Jialin Li and Ziheng Zhang and Rui Zhang and Fan Jia and Tiancai Wang and Haoqiang Fan and Kuo-Kun Tseng and Ruiping Wang", "abstract": "  Efficient acquisition of real-world embodied data has been increasingly\ncritical. However, large-scale demonstrations captured by remote operation tend\nto take extremely high costs and fail to scale up the data size in an efficient\nmanner. Sampling the episodes under a simulated environment is a promising way\nfor large-scale collection while existing simulators fail to high-fidelity\nmodeling on texture and physics. To address these limitations, we introduce the\nRoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting\nand the physics engine. RoboGSim mainly includes four parts: Gaussian\nReconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine.\nIt can synthesize the simulated data with novel views, objects, trajectories,\nand scenes. RoboGSim also provides an online, reproducible, and safe evaluation\nfor different manipulation policies. The real2sim and sim2real transfer\nexperiments show a high consistency in the texture and physics. Moreover, the\neffectiveness of synthetic data is validated under the real-world manipulated\ntasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison\non policy learning. More information can be found on our project page\nhttps://robogsim.github.io/ .\n", "link": "http://arxiv.org/abs/2411.11839v1", "date": "2024-11-18", "relevancy": 2.5392, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6574}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6203}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboGSim%3A%20A%20Real2Sim2Real%20Robotic%20Gaussian%20Splatting%20Simulator&body=Title%3A%20RoboGSim%3A%20A%20Real2Sim2Real%20Robotic%20Gaussian%20Splatting%20Simulator%0AAuthor%3A%20Xinhai%20Li%20and%20Jialin%20Li%20and%20Ziheng%20Zhang%20and%20Rui%20Zhang%20and%20Fan%20Jia%20and%20Tiancai%20Wang%20and%20Haoqiang%20Fan%20and%20Kuo-Kun%20Tseng%20and%20Ruiping%20Wang%0AAbstract%3A%20%20%20Efficient%20acquisition%20of%20real-world%20embodied%20data%20has%20been%20increasingly%0Acritical.%20However%2C%20large-scale%20demonstrations%20captured%20by%20remote%20operation%20tend%0Ato%20take%20extremely%20high%20costs%20and%20fail%20to%20scale%20up%20the%20data%20size%20in%20an%20efficient%0Amanner.%20Sampling%20the%20episodes%20under%20a%20simulated%20environment%20is%20a%20promising%20way%0Afor%20large-scale%20collection%20while%20existing%20simulators%20fail%20to%20high-fidelity%0Amodeling%20on%20texture%20and%20physics.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%0ARoboGSim%2C%20a%20real2sim2real%20robotic%20simulator%2C%20powered%20by%203D%20Gaussian%20Splatting%0Aand%20the%20physics%20engine.%20RoboGSim%20mainly%20includes%20four%20parts%3A%20Gaussian%0AReconstructor%2C%20Digital%20Twins%20Builder%2C%20Scene%20Composer%2C%20and%20Interactive%20Engine.%0AIt%20can%20synthesize%20the%20simulated%20data%20with%20novel%20views%2C%20objects%2C%20trajectories%2C%0Aand%20scenes.%20RoboGSim%20also%20provides%20an%20online%2C%20reproducible%2C%20and%20safe%20evaluation%0Afor%20different%20manipulation%20policies.%20The%20real2sim%20and%20sim2real%20transfer%0Aexperiments%20show%20a%20high%20consistency%20in%20the%20texture%20and%20physics.%20Moreover%2C%20the%0Aeffectiveness%20of%20synthetic%20data%20is%20validated%20under%20the%20real-world%20manipulated%0Atasks.%20We%20hope%20RoboGSim%20serves%20as%20a%20closed-loop%20simulator%20for%20fair%20comparison%0Aon%20policy%20learning.%20More%20information%20can%20be%20found%20on%20our%20project%20page%0Ahttps%3A//robogsim.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboGSim%253A%2520A%2520Real2Sim2Real%2520Robotic%2520Gaussian%2520Splatting%2520Simulator%26entry.906535625%3DXinhai%2520Li%2520and%2520Jialin%2520Li%2520and%2520Ziheng%2520Zhang%2520and%2520Rui%2520Zhang%2520and%2520Fan%2520Jia%2520and%2520Tiancai%2520Wang%2520and%2520Haoqiang%2520Fan%2520and%2520Kuo-Kun%2520Tseng%2520and%2520Ruiping%2520Wang%26entry.1292438233%3D%2520%2520Efficient%2520acquisition%2520of%2520real-world%2520embodied%2520data%2520has%2520been%2520increasingly%250Acritical.%2520However%252C%2520large-scale%2520demonstrations%2520captured%2520by%2520remote%2520operation%2520tend%250Ato%2520take%2520extremely%2520high%2520costs%2520and%2520fail%2520to%2520scale%2520up%2520the%2520data%2520size%2520in%2520an%2520efficient%250Amanner.%2520Sampling%2520the%2520episodes%2520under%2520a%2520simulated%2520environment%2520is%2520a%2520promising%2520way%250Afor%2520large-scale%2520collection%2520while%2520existing%2520simulators%2520fail%2520to%2520high-fidelity%250Amodeling%2520on%2520texture%2520and%2520physics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%250ARoboGSim%252C%2520a%2520real2sim2real%2520robotic%2520simulator%252C%2520powered%2520by%25203D%2520Gaussian%2520Splatting%250Aand%2520the%2520physics%2520engine.%2520RoboGSim%2520mainly%2520includes%2520four%2520parts%253A%2520Gaussian%250AReconstructor%252C%2520Digital%2520Twins%2520Builder%252C%2520Scene%2520Composer%252C%2520and%2520Interactive%2520Engine.%250AIt%2520can%2520synthesize%2520the%2520simulated%2520data%2520with%2520novel%2520views%252C%2520objects%252C%2520trajectories%252C%250Aand%2520scenes.%2520RoboGSim%2520also%2520provides%2520an%2520online%252C%2520reproducible%252C%2520and%2520safe%2520evaluation%250Afor%2520different%2520manipulation%2520policies.%2520The%2520real2sim%2520and%2520sim2real%2520transfer%250Aexperiments%2520show%2520a%2520high%2520consistency%2520in%2520the%2520texture%2520and%2520physics.%2520Moreover%252C%2520the%250Aeffectiveness%2520of%2520synthetic%2520data%2520is%2520validated%2520under%2520the%2520real-world%2520manipulated%250Atasks.%2520We%2520hope%2520RoboGSim%2520serves%2520as%2520a%2520closed-loop%2520simulator%2520for%2520fair%2520comparison%250Aon%2520policy%2520learning.%2520More%2520information%2520can%2520be%2520found%2520on%2520our%2520project%2520page%250Ahttps%253A//robogsim.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboGSim%3A%20A%20Real2Sim2Real%20Robotic%20Gaussian%20Splatting%20Simulator&entry.906535625=Xinhai%20Li%20and%20Jialin%20Li%20and%20Ziheng%20Zhang%20and%20Rui%20Zhang%20and%20Fan%20Jia%20and%20Tiancai%20Wang%20and%20Haoqiang%20Fan%20and%20Kuo-Kun%20Tseng%20and%20Ruiping%20Wang&entry.1292438233=%20%20Efficient%20acquisition%20of%20real-world%20embodied%20data%20has%20been%20increasingly%0Acritical.%20However%2C%20large-scale%20demonstrations%20captured%20by%20remote%20operation%20tend%0Ato%20take%20extremely%20high%20costs%20and%20fail%20to%20scale%20up%20the%20data%20size%20in%20an%20efficient%0Amanner.%20Sampling%20the%20episodes%20under%20a%20simulated%20environment%20is%20a%20promising%20way%0Afor%20large-scale%20collection%20while%20existing%20simulators%20fail%20to%20high-fidelity%0Amodeling%20on%20texture%20and%20physics.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%0ARoboGSim%2C%20a%20real2sim2real%20robotic%20simulator%2C%20powered%20by%203D%20Gaussian%20Splatting%0Aand%20the%20physics%20engine.%20RoboGSim%20mainly%20includes%20four%20parts%3A%20Gaussian%0AReconstructor%2C%20Digital%20Twins%20Builder%2C%20Scene%20Composer%2C%20and%20Interactive%20Engine.%0AIt%20can%20synthesize%20the%20simulated%20data%20with%20novel%20views%2C%20objects%2C%20trajectories%2C%0Aand%20scenes.%20RoboGSim%20also%20provides%20an%20online%2C%20reproducible%2C%20and%20safe%20evaluation%0Afor%20different%20manipulation%20policies.%20The%20real2sim%20and%20sim2real%20transfer%0Aexperiments%20show%20a%20high%20consistency%20in%20the%20texture%20and%20physics.%20Moreover%2C%20the%0Aeffectiveness%20of%20synthetic%20data%20is%20validated%20under%20the%20real-world%20manipulated%0Atasks.%20We%20hope%20RoboGSim%20serves%20as%20a%20closed-loop%20simulator%20for%20fair%20comparison%0Aon%20policy%20learning.%20More%20information%20can%20be%20found%20on%20our%20project%20page%0Ahttps%3A//robogsim.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11839v1&entry.124074799=Read"},
{"title": "AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping", "author": "Nicholas Chow and Bo Fu", "abstract": "  This paper showcases AdaptLIL, a real-time adaptive link-indented list\nontology mapping visualization that uses eye gaze as the primary input source.\nThrough a multimodal combination of real-time systems, deep learning, and web\ndevelopment applications, this system uniquely curtails graphical overlays\n(adaptations) to pairwise mappings of link-indented list ontology\nvisualizations for individual users based solely on their eye gaze.\n", "link": "http://arxiv.org/abs/2411.11768v1", "date": "2024-11-18", "relevancy": 2.5215, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5027}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptLIL%3A%20A%20Gaze-Adaptive%20Visualization%20for%20Ontology%20Mapping&body=Title%3A%20AdaptLIL%3A%20A%20Gaze-Adaptive%20Visualization%20for%20Ontology%20Mapping%0AAuthor%3A%20Nicholas%20Chow%20and%20Bo%20Fu%0AAbstract%3A%20%20%20This%20paper%20showcases%20AdaptLIL%2C%20a%20real-time%20adaptive%20link-indented%20list%0Aontology%20mapping%20visualization%20that%20uses%20eye%20gaze%20as%20the%20primary%20input%20source.%0AThrough%20a%20multimodal%20combination%20of%20real-time%20systems%2C%20deep%20learning%2C%20and%20web%0Adevelopment%20applications%2C%20this%20system%20uniquely%20curtails%20graphical%20overlays%0A%28adaptations%29%20to%20pairwise%20mappings%20of%20link-indented%20list%20ontology%0Avisualizations%20for%20individual%20users%20based%20solely%20on%20their%20eye%20gaze.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptLIL%253A%2520A%2520Gaze-Adaptive%2520Visualization%2520for%2520Ontology%2520Mapping%26entry.906535625%3DNicholas%2520Chow%2520and%2520Bo%2520Fu%26entry.1292438233%3D%2520%2520This%2520paper%2520showcases%2520AdaptLIL%252C%2520a%2520real-time%2520adaptive%2520link-indented%2520list%250Aontology%2520mapping%2520visualization%2520that%2520uses%2520eye%2520gaze%2520as%2520the%2520primary%2520input%2520source.%250AThrough%2520a%2520multimodal%2520combination%2520of%2520real-time%2520systems%252C%2520deep%2520learning%252C%2520and%2520web%250Adevelopment%2520applications%252C%2520this%2520system%2520uniquely%2520curtails%2520graphical%2520overlays%250A%2528adaptations%2529%2520to%2520pairwise%2520mappings%2520of%2520link-indented%2520list%2520ontology%250Avisualizations%2520for%2520individual%2520users%2520based%2520solely%2520on%2520their%2520eye%2520gaze.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptLIL%3A%20A%20Gaze-Adaptive%20Visualization%20for%20Ontology%20Mapping&entry.906535625=Nicholas%20Chow%20and%20Bo%20Fu&entry.1292438233=%20%20This%20paper%20showcases%20AdaptLIL%2C%20a%20real-time%20adaptive%20link-indented%20list%0Aontology%20mapping%20visualization%20that%20uses%20eye%20gaze%20as%20the%20primary%20input%20source.%0AThrough%20a%20multimodal%20combination%20of%20real-time%20systems%2C%20deep%20learning%2C%20and%20web%0Adevelopment%20applications%2C%20this%20system%20uniquely%20curtails%20graphical%20overlays%0A%28adaptations%29%20to%20pairwise%20mappings%20of%20link-indented%20list%20ontology%0Avisualizations%20for%20individual%20users%20based%20solely%20on%20their%20eye%20gaze.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11768v1&entry.124074799=Read"},
{"title": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?", "author": "Katie Kang and Amrith Setlur and Dibya Ghosh and Jacob Steinhardt and Claire Tomlin and Sergey Levine and Aviral Kumar", "abstract": "  Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques.\n", "link": "http://arxiv.org/abs/2411.07681v2", "date": "2024-11-18", "relevancy": 2.5199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F&body=Title%3A%20What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F%0AAuthor%3A%20Katie%20Kang%20and%20Amrith%20Setlur%20and%20Dibya%20Ghosh%20and%20Jacob%20Steinhardt%20and%20Claire%20Tomlin%20and%20Sergey%20Levine%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%0Athe%20mechanisms%20behind%20their%20problem-solving%20abilities%20remain%20elusive.%20In%20this%0Awork%2C%20we%20aim%20to%20better%20understand%20how%20the%20learning%20dynamics%20of%20LLM%20finetuning%0Ashapes%20downstream%20generalization.%20Our%20analysis%20focuses%20on%20reasoning%20tasks%2C%0Awhose%20problem%20structure%20allows%20us%20to%20distinguish%20between%20memorization%20%28the%0Aexact%20replication%20of%20reasoning%20steps%20from%20the%20training%20data%29%20and%20performance%0A%28the%20correctness%20of%20the%20final%20solution%29.%20We%20find%20that%20a%20model%27s%20generalization%0Abehavior%20can%20be%20effectively%20characterized%20by%20a%20training%20metric%20we%20call%0Apre-memorization%20train%20accuracy%3A%20the%20accuracy%20of%20model%20samples%20on%20training%0Aqueries%20before%20they%20begin%20to%20copy%20the%20exact%20reasoning%20steps%20from%20the%20training%0Aset.%20On%20the%20dataset%20level%2C%20this%20metric%20is%20able%20to%20reliably%20predict%20test%0Aaccuracy%2C%20achieving%20%24R%5E2%24%20of%20around%20or%20exceeding%200.9%20across%20various%20models%0A%28Llama3%208%2C%20Gemma2%209B%29%2C%20datasets%20%28GSM8k%2C%20MATH%29%2C%20and%20training%20configurations.%20On%0Aa%20per-example%20level%2C%20this%20metric%20is%20also%20indicative%20of%20whether%20individual%20model%0Apredictions%20are%20robust%20to%20perturbations%20in%20the%20training%20query.%20By%20connecting%20a%0Amodel%27s%20learning%20behavior%20to%20its%20generalization%2C%20pre-memorization%20train%0Aaccuracy%20can%20guide%20targeted%20improvements%20to%20training%20strategies.%20We%20focus%20on%0Adata%20curation%20as%20an%20example%2C%20and%20show%20that%20prioritizing%20examples%20with%20low%0Apre-memorization%20accuracy%20leads%20to%201.5-2x%20improvements%20in%20data%20efficiency%0Acompared%20to%20i.i.d.%20data%20scaling%2C%20and%20outperforms%20other%20standard%20data%20curation%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520Learning%2520Dynamics%2520Reveal%2520About%2520Generalization%2520in%2520LLM%2520Reasoning%253F%26entry.906535625%3DKatie%2520Kang%2520and%2520Amrith%2520Setlur%2520and%2520Dibya%2520Ghosh%2520and%2520Jacob%2520Steinhardt%2520and%2520Claire%2520Tomlin%2520and%2520Sergey%2520Levine%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520capabilities%2520of%2520modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Athe%2520mechanisms%2520behind%2520their%2520problem-solving%2520abilities%2520remain%2520elusive.%2520In%2520this%250Awork%252C%2520we%2520aim%2520to%2520better%2520understand%2520how%2520the%2520learning%2520dynamics%2520of%2520LLM%2520finetuning%250Ashapes%2520downstream%2520generalization.%2520Our%2520analysis%2520focuses%2520on%2520reasoning%2520tasks%252C%250Awhose%2520problem%2520structure%2520allows%2520us%2520to%2520distinguish%2520between%2520memorization%2520%2528the%250Aexact%2520replication%2520of%2520reasoning%2520steps%2520from%2520the%2520training%2520data%2529%2520and%2520performance%250A%2528the%2520correctness%2520of%2520the%2520final%2520solution%2529.%2520We%2520find%2520that%2520a%2520model%2527s%2520generalization%250Abehavior%2520can%2520be%2520effectively%2520characterized%2520by%2520a%2520training%2520metric%2520we%2520call%250Apre-memorization%2520train%2520accuracy%253A%2520the%2520accuracy%2520of%2520model%2520samples%2520on%2520training%250Aqueries%2520before%2520they%2520begin%2520to%2520copy%2520the%2520exact%2520reasoning%2520steps%2520from%2520the%2520training%250Aset.%2520On%2520the%2520dataset%2520level%252C%2520this%2520metric%2520is%2520able%2520to%2520reliably%2520predict%2520test%250Aaccuracy%252C%2520achieving%2520%2524R%255E2%2524%2520of%2520around%2520or%2520exceeding%25200.9%2520across%2520various%2520models%250A%2528Llama3%25208%252C%2520Gemma2%25209B%2529%252C%2520datasets%2520%2528GSM8k%252C%2520MATH%2529%252C%2520and%2520training%2520configurations.%2520On%250Aa%2520per-example%2520level%252C%2520this%2520metric%2520is%2520also%2520indicative%2520of%2520whether%2520individual%2520model%250Apredictions%2520are%2520robust%2520to%2520perturbations%2520in%2520the%2520training%2520query.%2520By%2520connecting%2520a%250Amodel%2527s%2520learning%2520behavior%2520to%2520its%2520generalization%252C%2520pre-memorization%2520train%250Aaccuracy%2520can%2520guide%2520targeted%2520improvements%2520to%2520training%2520strategies.%2520We%2520focus%2520on%250Adata%2520curation%2520as%2520an%2520example%252C%2520and%2520show%2520that%2520prioritizing%2520examples%2520with%2520low%250Apre-memorization%2520accuracy%2520leads%2520to%25201.5-2x%2520improvements%2520in%2520data%2520efficiency%250Acompared%2520to%2520i.i.d.%2520data%2520scaling%252C%2520and%2520outperforms%2520other%2520standard%2520data%2520curation%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F&entry.906535625=Katie%20Kang%20and%20Amrith%20Setlur%20and%20Dibya%20Ghosh%20and%20Jacob%20Steinhardt%20and%20Claire%20Tomlin%20and%20Sergey%20Levine%20and%20Aviral%20Kumar&entry.1292438233=%20%20Despite%20the%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%0Athe%20mechanisms%20behind%20their%20problem-solving%20abilities%20remain%20elusive.%20In%20this%0Awork%2C%20we%20aim%20to%20better%20understand%20how%20the%20learning%20dynamics%20of%20LLM%20finetuning%0Ashapes%20downstream%20generalization.%20Our%20analysis%20focuses%20on%20reasoning%20tasks%2C%0Awhose%20problem%20structure%20allows%20us%20to%20distinguish%20between%20memorization%20%28the%0Aexact%20replication%20of%20reasoning%20steps%20from%20the%20training%20data%29%20and%20performance%0A%28the%20correctness%20of%20the%20final%20solution%29.%20We%20find%20that%20a%20model%27s%20generalization%0Abehavior%20can%20be%20effectively%20characterized%20by%20a%20training%20metric%20we%20call%0Apre-memorization%20train%20accuracy%3A%20the%20accuracy%20of%20model%20samples%20on%20training%0Aqueries%20before%20they%20begin%20to%20copy%20the%20exact%20reasoning%20steps%20from%20the%20training%0Aset.%20On%20the%20dataset%20level%2C%20this%20metric%20is%20able%20to%20reliably%20predict%20test%0Aaccuracy%2C%20achieving%20%24R%5E2%24%20of%20around%20or%20exceeding%200.9%20across%20various%20models%0A%28Llama3%208%2C%20Gemma2%209B%29%2C%20datasets%20%28GSM8k%2C%20MATH%29%2C%20and%20training%20configurations.%20On%0Aa%20per-example%20level%2C%20this%20metric%20is%20also%20indicative%20of%20whether%20individual%20model%0Apredictions%20are%20robust%20to%20perturbations%20in%20the%20training%20query.%20By%20connecting%20a%0Amodel%27s%20learning%20behavior%20to%20its%20generalization%2C%20pre-memorization%20train%0Aaccuracy%20can%20guide%20targeted%20improvements%20to%20training%20strategies.%20We%20focus%20on%0Adata%20curation%20as%20an%20example%2C%20and%20show%20that%20prioritizing%20examples%20with%20low%0Apre-memorization%20accuracy%20leads%20to%201.5-2x%20improvements%20in%20data%20efficiency%0Acompared%20to%20i.i.d.%20data%20scaling%2C%20and%20outperforms%20other%20standard%20data%20curation%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07681v2&entry.124074799=Read"},
{"title": "BrightDreamer: Generic 3D Gaussian Generative Framework for Fast\n  Text-to-3D Synthesis", "author": "Lutao Jiang and Xu Zheng and Yuanhuiyi Lyu and Jiazhou Zhou and Lin Wang", "abstract": "  Text-to-3D synthesis has recently seen intriguing advances by combining the\ntext-to-image priors with 3D representation methods, e.g., 3D Gaussian\nSplatting (3D GS), via Score Distillation Sampling (SDS). However, a hurdle of\nexisting methods is the low efficiency, per-prompt optimization for a single 3D\nobject. Therefore, it is imperative for a paradigm shift from per-prompt\noptimization to feed-forward generation for any unseen text prompts, which yet\nremains challenging. An obstacle is how to directly generate a set of millions\nof 3D Gaussians to represent a 3D object. This paper presents BrightDreamer, an\nend-to-end feed-forward approach that can achieve generalizable and fast (77\nms) text-to-3D generation. Our key idea is to formulate the generation process\nas estimating the 3D deformation from an anchor shape with predefined\npositions. For this, we first propose a Text-guided Shape Deformation (TSD)\nnetwork to predict the deformed shape and its new positions, used as the\ncenters (one attribute) of 3D Gaussians. To estimate the other four attributes\n(i.e., scaling, rotation, opacity, and SH), we then design a novel Text-guided\nTriplane Generator (TTG) to generate a triplane representation for a 3D object.\nThe center of each Gaussian enables us to transform the spatial feature into\nthe four attributes. The generated 3D Gaussians can be finally rendered at 705\nframes per second. Extensive experiments demonstrate the superiority of our\nmethod over existing methods. Also, BrightDreamer possesses a strong semantic\nunderstanding capability even for complex text prompts. The code is available\nin the project page.\n", "link": "http://arxiv.org/abs/2403.11273v2", "date": "2024-11-18", "relevancy": 2.5166, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6351}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6255}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrightDreamer%3A%20Generic%203D%20Gaussian%20Generative%20Framework%20for%20Fast%0A%20%20Text-to-3D%20Synthesis&body=Title%3A%20BrightDreamer%3A%20Generic%203D%20Gaussian%20Generative%20Framework%20for%20Fast%0A%20%20Text-to-3D%20Synthesis%0AAuthor%3A%20Lutao%20Jiang%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Jiazhou%20Zhou%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Text-to-3D%20synthesis%20has%20recently%20seen%20intriguing%20advances%20by%20combining%20the%0Atext-to-image%20priors%20with%203D%20representation%20methods%2C%20e.g.%2C%203D%20Gaussian%0ASplatting%20%283D%20GS%29%2C%20via%20Score%20Distillation%20Sampling%20%28SDS%29.%20However%2C%20a%20hurdle%20of%0Aexisting%20methods%20is%20the%20low%20efficiency%2C%20per-prompt%20optimization%20for%20a%20single%203D%0Aobject.%20Therefore%2C%20it%20is%20imperative%20for%20a%20paradigm%20shift%20from%20per-prompt%0Aoptimization%20to%20feed-forward%20generation%20for%20any%20unseen%20text%20prompts%2C%20which%20yet%0Aremains%20challenging.%20An%20obstacle%20is%20how%20to%20directly%20generate%20a%20set%20of%20millions%0Aof%203D%20Gaussians%20to%20represent%20a%203D%20object.%20This%20paper%20presents%20BrightDreamer%2C%20an%0Aend-to-end%20feed-forward%20approach%20that%20can%20achieve%20generalizable%20and%20fast%20%2877%0Ams%29%20text-to-3D%20generation.%20Our%20key%20idea%20is%20to%20formulate%20the%20generation%20process%0Aas%20estimating%20the%203D%20deformation%20from%20an%20anchor%20shape%20with%20predefined%0Apositions.%20For%20this%2C%20we%20first%20propose%20a%20Text-guided%20Shape%20Deformation%20%28TSD%29%0Anetwork%20to%20predict%20the%20deformed%20shape%20and%20its%20new%20positions%2C%20used%20as%20the%0Acenters%20%28one%20attribute%29%20of%203D%20Gaussians.%20To%20estimate%20the%20other%20four%20attributes%0A%28i.e.%2C%20scaling%2C%20rotation%2C%20opacity%2C%20and%20SH%29%2C%20we%20then%20design%20a%20novel%20Text-guided%0ATriplane%20Generator%20%28TTG%29%20to%20generate%20a%20triplane%20representation%20for%20a%203D%20object.%0AThe%20center%20of%20each%20Gaussian%20enables%20us%20to%20transform%20the%20spatial%20feature%20into%0Athe%20four%20attributes.%20The%20generated%203D%20Gaussians%20can%20be%20finally%20rendered%20at%20705%0Aframes%20per%20second.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%0Amethod%20over%20existing%20methods.%20Also%2C%20BrightDreamer%20possesses%20a%20strong%20semantic%0Aunderstanding%20capability%20even%20for%20complex%20text%20prompts.%20The%20code%20is%20available%0Ain%20the%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrightDreamer%253A%2520Generic%25203D%2520Gaussian%2520Generative%2520Framework%2520for%2520Fast%250A%2520%2520Text-to-3D%2520Synthesis%26entry.906535625%3DLutao%2520Jiang%2520and%2520Xu%2520Zheng%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Jiazhou%2520Zhou%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Text-to-3D%2520synthesis%2520has%2520recently%2520seen%2520intriguing%2520advances%2520by%2520combining%2520the%250Atext-to-image%2520priors%2520with%25203D%2520representation%2520methods%252C%2520e.g.%252C%25203D%2520Gaussian%250ASplatting%2520%25283D%2520GS%2529%252C%2520via%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529.%2520However%252C%2520a%2520hurdle%2520of%250Aexisting%2520methods%2520is%2520the%2520low%2520efficiency%252C%2520per-prompt%2520optimization%2520for%2520a%2520single%25203D%250Aobject.%2520Therefore%252C%2520it%2520is%2520imperative%2520for%2520a%2520paradigm%2520shift%2520from%2520per-prompt%250Aoptimization%2520to%2520feed-forward%2520generation%2520for%2520any%2520unseen%2520text%2520prompts%252C%2520which%2520yet%250Aremains%2520challenging.%2520An%2520obstacle%2520is%2520how%2520to%2520directly%2520generate%2520a%2520set%2520of%2520millions%250Aof%25203D%2520Gaussians%2520to%2520represent%2520a%25203D%2520object.%2520This%2520paper%2520presents%2520BrightDreamer%252C%2520an%250Aend-to-end%2520feed-forward%2520approach%2520that%2520can%2520achieve%2520generalizable%2520and%2520fast%2520%252877%250Ams%2529%2520text-to-3D%2520generation.%2520Our%2520key%2520idea%2520is%2520to%2520formulate%2520the%2520generation%2520process%250Aas%2520estimating%2520the%25203D%2520deformation%2520from%2520an%2520anchor%2520shape%2520with%2520predefined%250Apositions.%2520For%2520this%252C%2520we%2520first%2520propose%2520a%2520Text-guided%2520Shape%2520Deformation%2520%2528TSD%2529%250Anetwork%2520to%2520predict%2520the%2520deformed%2520shape%2520and%2520its%2520new%2520positions%252C%2520used%2520as%2520the%250Acenters%2520%2528one%2520attribute%2529%2520of%25203D%2520Gaussians.%2520To%2520estimate%2520the%2520other%2520four%2520attributes%250A%2528i.e.%252C%2520scaling%252C%2520rotation%252C%2520opacity%252C%2520and%2520SH%2529%252C%2520we%2520then%2520design%2520a%2520novel%2520Text-guided%250ATriplane%2520Generator%2520%2528TTG%2529%2520to%2520generate%2520a%2520triplane%2520representation%2520for%2520a%25203D%2520object.%250AThe%2520center%2520of%2520each%2520Gaussian%2520enables%2520us%2520to%2520transform%2520the%2520spatial%2520feature%2520into%250Athe%2520four%2520attributes.%2520The%2520generated%25203D%2520Gaussians%2520can%2520be%2520finally%2520rendered%2520at%2520705%250Aframes%2520per%2520second.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%250Amethod%2520over%2520existing%2520methods.%2520Also%252C%2520BrightDreamer%2520possesses%2520a%2520strong%2520semantic%250Aunderstanding%2520capability%2520even%2520for%2520complex%2520text%2520prompts.%2520The%2520code%2520is%2520available%250Ain%2520the%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrightDreamer%3A%20Generic%203D%20Gaussian%20Generative%20Framework%20for%20Fast%0A%20%20Text-to-3D%20Synthesis&entry.906535625=Lutao%20Jiang%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Jiazhou%20Zhou%20and%20Lin%20Wang&entry.1292438233=%20%20Text-to-3D%20synthesis%20has%20recently%20seen%20intriguing%20advances%20by%20combining%20the%0Atext-to-image%20priors%20with%203D%20representation%20methods%2C%20e.g.%2C%203D%20Gaussian%0ASplatting%20%283D%20GS%29%2C%20via%20Score%20Distillation%20Sampling%20%28SDS%29.%20However%2C%20a%20hurdle%20of%0Aexisting%20methods%20is%20the%20low%20efficiency%2C%20per-prompt%20optimization%20for%20a%20single%203D%0Aobject.%20Therefore%2C%20it%20is%20imperative%20for%20a%20paradigm%20shift%20from%20per-prompt%0Aoptimization%20to%20feed-forward%20generation%20for%20any%20unseen%20text%20prompts%2C%20which%20yet%0Aremains%20challenging.%20An%20obstacle%20is%20how%20to%20directly%20generate%20a%20set%20of%20millions%0Aof%203D%20Gaussians%20to%20represent%20a%203D%20object.%20This%20paper%20presents%20BrightDreamer%2C%20an%0Aend-to-end%20feed-forward%20approach%20that%20can%20achieve%20generalizable%20and%20fast%20%2877%0Ams%29%20text-to-3D%20generation.%20Our%20key%20idea%20is%20to%20formulate%20the%20generation%20process%0Aas%20estimating%20the%203D%20deformation%20from%20an%20anchor%20shape%20with%20predefined%0Apositions.%20For%20this%2C%20we%20first%20propose%20a%20Text-guided%20Shape%20Deformation%20%28TSD%29%0Anetwork%20to%20predict%20the%20deformed%20shape%20and%20its%20new%20positions%2C%20used%20as%20the%0Acenters%20%28one%20attribute%29%20of%203D%20Gaussians.%20To%20estimate%20the%20other%20four%20attributes%0A%28i.e.%2C%20scaling%2C%20rotation%2C%20opacity%2C%20and%20SH%29%2C%20we%20then%20design%20a%20novel%20Text-guided%0ATriplane%20Generator%20%28TTG%29%20to%20generate%20a%20triplane%20representation%20for%20a%203D%20object.%0AThe%20center%20of%20each%20Gaussian%20enables%20us%20to%20transform%20the%20spatial%20feature%20into%0Athe%20four%20attributes.%20The%20generated%203D%20Gaussians%20can%20be%20finally%20rendered%20at%20705%0Aframes%20per%20second.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%0Amethod%20over%20existing%20methods.%20Also%2C%20BrightDreamer%20possesses%20a%20strong%20semantic%0Aunderstanding%20capability%20even%20for%20complex%20text%20prompts.%20The%20code%20is%20available%0Ain%20the%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11273v2&entry.124074799=Read"},
{"title": "PyGim: An Efficient Graph Neural Network Library for Real\n  Processing-In-Memory Architectures", "author": "Christina Giannoula and Peiming Yang and Ivan Fernandez and Jiacheng Yang and Sankeerth Durvasula and Yu Xin Li and Mohammad Sadrosadati and Juan Gomez Luna and Onur Mutlu and Gennady Pekhimenko", "abstract": "  Graph Neural Networks (GNNs) are emerging ML models to analyze\ngraph-structure data. Graph Neural Network (GNN) execution involves both\ncompute-intensive and memory-intensive kernels, the latter dominates the total\ntime, being significantly bottlenecked by data movement between memory and\nprocessors. Processing-In-Memory (PIM) systems can alleviate this data movement\nbottleneck by placing simple processors near or inside to memory arrays. In\nthis work, we introduce PyGim, an efficient ML library that accelerates GNNs on\nreal PIM systems. We propose intelligent parallelization techniques for\nmemory-intensive kernels of GNNs tailored for real PIM systems, and develop\nhandy Python API for them. We provide hybrid GNN execution, in which the\ncompute-intensive and memory-intensive kernels are executed in\nprocessor-centric and memory-centric computing systems, respectively. We\nextensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using\nemerging GNN models, and demonstrate that it outperforms its state-of-the-art\nCPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource\nutilization than CPU and GPU systems. Our work provides useful recommendations\nfor software, system and hardware designers. PyGim is publicly available at\nhttps://github.com/CMU-SAFARI/PyGim.\n", "link": "http://arxiv.org/abs/2402.16731v6", "date": "2024-11-18", "relevancy": 2.5131, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5104}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5093}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyGim%3A%20An%20Efficient%20Graph%20Neural%20Network%20Library%20for%20Real%0A%20%20Processing-In-Memory%20Architectures&body=Title%3A%20PyGim%3A%20An%20Efficient%20Graph%20Neural%20Network%20Library%20for%20Real%0A%20%20Processing-In-Memory%20Architectures%0AAuthor%3A%20Christina%20Giannoula%20and%20Peiming%20Yang%20and%20Ivan%20Fernandez%20and%20Jiacheng%20Yang%20and%20Sankeerth%20Durvasula%20and%20Yu%20Xin%20Li%20and%20Mohammad%20Sadrosadati%20and%20Juan%20Gomez%20Luna%20and%20Onur%20Mutlu%20and%20Gennady%20Pekhimenko%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20ML%20models%20to%20analyze%0Agraph-structure%20data.%20Graph%20Neural%20Network%20%28GNN%29%20execution%20involves%20both%0Acompute-intensive%20and%20memory-intensive%20kernels%2C%20the%20latter%20dominates%20the%20total%0Atime%2C%20being%20significantly%20bottlenecked%20by%20data%20movement%20between%20memory%20and%0Aprocessors.%20Processing-In-Memory%20%28PIM%29%20systems%20can%20alleviate%20this%20data%20movement%0Abottleneck%20by%20placing%20simple%20processors%20near%20or%20inside%20to%20memory%20arrays.%20In%0Athis%20work%2C%20we%20introduce%20PyGim%2C%20an%20efficient%20ML%20library%20that%20accelerates%20GNNs%20on%0Areal%20PIM%20systems.%20We%20propose%20intelligent%20parallelization%20techniques%20for%0Amemory-intensive%20kernels%20of%20GNNs%20tailored%20for%20real%20PIM%20systems%2C%20and%20develop%0Ahandy%20Python%20API%20for%20them.%20We%20provide%20hybrid%20GNN%20execution%2C%20in%20which%20the%0Acompute-intensive%20and%20memory-intensive%20kernels%20are%20executed%20in%0Aprocessor-centric%20and%20memory-centric%20computing%20systems%2C%20respectively.%20We%0Aextensively%20evaluate%20PyGim%20on%20a%20real-world%20PIM%20system%20with%201992%20PIM%20cores%20using%0Aemerging%20GNN%20models%2C%20and%20demonstrate%20that%20it%20outperforms%20its%20state-of-the-art%0ACPU%20counterpart%20on%20Intel%20Xeon%20by%20on%20average%203.04x%2C%20and%20achieves%20higher%20resource%0Autilization%20than%20CPU%20and%20GPU%20systems.%20Our%20work%20provides%20useful%20recommendations%0Afor%20software%2C%20system%20and%20hardware%20designers.%20PyGim%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CMU-SAFARI/PyGim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16731v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyGim%253A%2520An%2520Efficient%2520Graph%2520Neural%2520Network%2520Library%2520for%2520Real%250A%2520%2520Processing-In-Memory%2520Architectures%26entry.906535625%3DChristina%2520Giannoula%2520and%2520Peiming%2520Yang%2520and%2520Ivan%2520Fernandez%2520and%2520Jiacheng%2520Yang%2520and%2520Sankeerth%2520Durvasula%2520and%2520Yu%2520Xin%2520Li%2520and%2520Mohammad%2520Sadrosadati%2520and%2520Juan%2520Gomez%2520Luna%2520and%2520Onur%2520Mutlu%2520and%2520Gennady%2520Pekhimenko%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520emerging%2520ML%2520models%2520to%2520analyze%250Agraph-structure%2520data.%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520execution%2520involves%2520both%250Acompute-intensive%2520and%2520memory-intensive%2520kernels%252C%2520the%2520latter%2520dominates%2520the%2520total%250Atime%252C%2520being%2520significantly%2520bottlenecked%2520by%2520data%2520movement%2520between%2520memory%2520and%250Aprocessors.%2520Processing-In-Memory%2520%2528PIM%2529%2520systems%2520can%2520alleviate%2520this%2520data%2520movement%250Abottleneck%2520by%2520placing%2520simple%2520processors%2520near%2520or%2520inside%2520to%2520memory%2520arrays.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520PyGim%252C%2520an%2520efficient%2520ML%2520library%2520that%2520accelerates%2520GNNs%2520on%250Areal%2520PIM%2520systems.%2520We%2520propose%2520intelligent%2520parallelization%2520techniques%2520for%250Amemory-intensive%2520kernels%2520of%2520GNNs%2520tailored%2520for%2520real%2520PIM%2520systems%252C%2520and%2520develop%250Ahandy%2520Python%2520API%2520for%2520them.%2520We%2520provide%2520hybrid%2520GNN%2520execution%252C%2520in%2520which%2520the%250Acompute-intensive%2520and%2520memory-intensive%2520kernels%2520are%2520executed%2520in%250Aprocessor-centric%2520and%2520memory-centric%2520computing%2520systems%252C%2520respectively.%2520We%250Aextensively%2520evaluate%2520PyGim%2520on%2520a%2520real-world%2520PIM%2520system%2520with%25201992%2520PIM%2520cores%2520using%250Aemerging%2520GNN%2520models%252C%2520and%2520demonstrate%2520that%2520it%2520outperforms%2520its%2520state-of-the-art%250ACPU%2520counterpart%2520on%2520Intel%2520Xeon%2520by%2520on%2520average%25203.04x%252C%2520and%2520achieves%2520higher%2520resource%250Autilization%2520than%2520CPU%2520and%2520GPU%2520systems.%2520Our%2520work%2520provides%2520useful%2520recommendations%250Afor%2520software%252C%2520system%2520and%2520hardware%2520designers.%2520PyGim%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/CMU-SAFARI/PyGim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16731v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyGim%3A%20An%20Efficient%20Graph%20Neural%20Network%20Library%20for%20Real%0A%20%20Processing-In-Memory%20Architectures&entry.906535625=Christina%20Giannoula%20and%20Peiming%20Yang%20and%20Ivan%20Fernandez%20and%20Jiacheng%20Yang%20and%20Sankeerth%20Durvasula%20and%20Yu%20Xin%20Li%20and%20Mohammad%20Sadrosadati%20and%20Juan%20Gomez%20Luna%20and%20Onur%20Mutlu%20and%20Gennady%20Pekhimenko&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20emerging%20ML%20models%20to%20analyze%0Agraph-structure%20data.%20Graph%20Neural%20Network%20%28GNN%29%20execution%20involves%20both%0Acompute-intensive%20and%20memory-intensive%20kernels%2C%20the%20latter%20dominates%20the%20total%0Atime%2C%20being%20significantly%20bottlenecked%20by%20data%20movement%20between%20memory%20and%0Aprocessors.%20Processing-In-Memory%20%28PIM%29%20systems%20can%20alleviate%20this%20data%20movement%0Abottleneck%20by%20placing%20simple%20processors%20near%20or%20inside%20to%20memory%20arrays.%20In%0Athis%20work%2C%20we%20introduce%20PyGim%2C%20an%20efficient%20ML%20library%20that%20accelerates%20GNNs%20on%0Areal%20PIM%20systems.%20We%20propose%20intelligent%20parallelization%20techniques%20for%0Amemory-intensive%20kernels%20of%20GNNs%20tailored%20for%20real%20PIM%20systems%2C%20and%20develop%0Ahandy%20Python%20API%20for%20them.%20We%20provide%20hybrid%20GNN%20execution%2C%20in%20which%20the%0Acompute-intensive%20and%20memory-intensive%20kernels%20are%20executed%20in%0Aprocessor-centric%20and%20memory-centric%20computing%20systems%2C%20respectively.%20We%0Aextensively%20evaluate%20PyGim%20on%20a%20real-world%20PIM%20system%20with%201992%20PIM%20cores%20using%0Aemerging%20GNN%20models%2C%20and%20demonstrate%20that%20it%20outperforms%20its%20state-of-the-art%0ACPU%20counterpart%20on%20Intel%20Xeon%20by%20on%20average%203.04x%2C%20and%20achieves%20higher%20resource%0Autilization%20than%20CPU%20and%20GPU%20systems.%20Our%20work%20provides%20useful%20recommendations%0Afor%20software%2C%20system%20and%20hardware%20designers.%20PyGim%20is%20publicly%20available%20at%0Ahttps%3A//github.com/CMU-SAFARI/PyGim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16731v6&entry.124074799=Read"},
{"title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and\n  new methods", "author": "Egor Kovalev and Georgii Bychkov and Khaled Abud and Aleksandr Gushchin and Anna Chistyakova and Sergey Lavrushkin and Dmitriy Vatolin and Anastasia Antsiferova", "abstract": "  Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI - the first standard for\nend-to-end neural image compression (NIC) methods - the question of its\nrobustness has become critically significant. JPEG AI is among the first\ninternational, real-world applications of neural-network-based models to be\nembedded in consumer devices. However, research on NIC robustness has been\nlimited to open-source codecs and a narrow range of attacks. This paper\nproposes a new methodology for measuring NIC robustness to adversarial attacks.\nWe present the first large-scale evaluation of JPEG AI's robustness, comparing\nit with other NIC models. Our evaluation results and code are publicly\navailable online (link is hidden for a blind review).\n", "link": "http://arxiv.org/abs/2411.11795v1", "date": "2024-11-18", "relevancy": 2.5093, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20adversarial%20robustness%20of%20JPEG%20AI%3A%20methodology%2C%20comparison%20and%0A%20%20new%20methods&body=Title%3A%20Exploring%20adversarial%20robustness%20of%20JPEG%20AI%3A%20methodology%2C%20comparison%20and%0A%20%20new%20methods%0AAuthor%3A%20Egor%20Kovalev%20and%20Georgii%20Bychkov%20and%20Khaled%20Abud%20and%20Aleksandr%20Gushchin%20and%20Anna%20Chistyakova%20and%20Sergey%20Lavrushkin%20and%20Dmitriy%20Vatolin%20and%20Anastasia%20Antsiferova%0AAbstract%3A%20%20%20Adversarial%20robustness%20of%20neural%20networks%20is%20an%20increasingly%20important%20area%0Aof%20research%2C%20combining%20studies%20on%20computer%20vision%20models%2C%20large%20language%20models%0A%28LLMs%29%2C%20and%20others.%20With%20the%20release%20of%20JPEG%20AI%20-%20the%20first%20standard%20for%0Aend-to-end%20neural%20image%20compression%20%28NIC%29%20methods%20-%20the%20question%20of%20its%0Arobustness%20has%20become%20critically%20significant.%20JPEG%20AI%20is%20among%20the%20first%0Ainternational%2C%20real-world%20applications%20of%20neural-network-based%20models%20to%20be%0Aembedded%20in%20consumer%20devices.%20However%2C%20research%20on%20NIC%20robustness%20has%20been%0Alimited%20to%20open-source%20codecs%20and%20a%20narrow%20range%20of%20attacks.%20This%20paper%0Aproposes%20a%20new%20methodology%20for%20measuring%20NIC%20robustness%20to%20adversarial%20attacks.%0AWe%20present%20the%20first%20large-scale%20evaluation%20of%20JPEG%20AI%27s%20robustness%2C%20comparing%0Ait%20with%20other%20NIC%20models.%20Our%20evaluation%20results%20and%20code%20are%20publicly%0Aavailable%20online%20%28link%20is%20hidden%20for%20a%20blind%20review%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520adversarial%2520robustness%2520of%2520JPEG%2520AI%253A%2520methodology%252C%2520comparison%2520and%250A%2520%2520new%2520methods%26entry.906535625%3DEgor%2520Kovalev%2520and%2520Georgii%2520Bychkov%2520and%2520Khaled%2520Abud%2520and%2520Aleksandr%2520Gushchin%2520and%2520Anna%2520Chistyakova%2520and%2520Sergey%2520Lavrushkin%2520and%2520Dmitriy%2520Vatolin%2520and%2520Anastasia%2520Antsiferova%26entry.1292438233%3D%2520%2520Adversarial%2520robustness%2520of%2520neural%2520networks%2520is%2520an%2520increasingly%2520important%2520area%250Aof%2520research%252C%2520combining%2520studies%2520on%2520computer%2520vision%2520models%252C%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520and%2520others.%2520With%2520the%2520release%2520of%2520JPEG%2520AI%2520-%2520the%2520first%2520standard%2520for%250Aend-to-end%2520neural%2520image%2520compression%2520%2528NIC%2529%2520methods%2520-%2520the%2520question%2520of%2520its%250Arobustness%2520has%2520become%2520critically%2520significant.%2520JPEG%2520AI%2520is%2520among%2520the%2520first%250Ainternational%252C%2520real-world%2520applications%2520of%2520neural-network-based%2520models%2520to%2520be%250Aembedded%2520in%2520consumer%2520devices.%2520However%252C%2520research%2520on%2520NIC%2520robustness%2520has%2520been%250Alimited%2520to%2520open-source%2520codecs%2520and%2520a%2520narrow%2520range%2520of%2520attacks.%2520This%2520paper%250Aproposes%2520a%2520new%2520methodology%2520for%2520measuring%2520NIC%2520robustness%2520to%2520adversarial%2520attacks.%250AWe%2520present%2520the%2520first%2520large-scale%2520evaluation%2520of%2520JPEG%2520AI%2527s%2520robustness%252C%2520comparing%250Ait%2520with%2520other%2520NIC%2520models.%2520Our%2520evaluation%2520results%2520and%2520code%2520are%2520publicly%250Aavailable%2520online%2520%2528link%2520is%2520hidden%2520for%2520a%2520blind%2520review%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20adversarial%20robustness%20of%20JPEG%20AI%3A%20methodology%2C%20comparison%20and%0A%20%20new%20methods&entry.906535625=Egor%20Kovalev%20and%20Georgii%20Bychkov%20and%20Khaled%20Abud%20and%20Aleksandr%20Gushchin%20and%20Anna%20Chistyakova%20and%20Sergey%20Lavrushkin%20and%20Dmitriy%20Vatolin%20and%20Anastasia%20Antsiferova&entry.1292438233=%20%20Adversarial%20robustness%20of%20neural%20networks%20is%20an%20increasingly%20important%20area%0Aof%20research%2C%20combining%20studies%20on%20computer%20vision%20models%2C%20large%20language%20models%0A%28LLMs%29%2C%20and%20others.%20With%20the%20release%20of%20JPEG%20AI%20-%20the%20first%20standard%20for%0Aend-to-end%20neural%20image%20compression%20%28NIC%29%20methods%20-%20the%20question%20of%20its%0Arobustness%20has%20become%20critically%20significant.%20JPEG%20AI%20is%20among%20the%20first%0Ainternational%2C%20real-world%20applications%20of%20neural-network-based%20models%20to%20be%0Aembedded%20in%20consumer%20devices.%20However%2C%20research%20on%20NIC%20robustness%20has%20been%0Alimited%20to%20open-source%20codecs%20and%20a%20narrow%20range%20of%20attacks.%20This%20paper%0Aproposes%20a%20new%20methodology%20for%20measuring%20NIC%20robustness%20to%20adversarial%20attacks.%0AWe%20present%20the%20first%20large-scale%20evaluation%20of%20JPEG%20AI%27s%20robustness%2C%20comparing%0Ait%20with%20other%20NIC%20models.%20Our%20evaluation%20results%20and%20code%20are%20publicly%0Aavailable%20online%20%28link%20is%20hidden%20for%20a%20blind%20review%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11795v1&entry.124074799=Read"},
{"title": "Cascaded Diffusion Models for 2D and 3D Microscopy Image Synthesis to\n  Enhance Cell Segmentation", "author": "R\u00fcveyda Yilmaz and Kaan Keven and Yuli Wu and Johannes Stegmaier", "abstract": "  Automated cell segmentation in microscopy images is essential for biomedical\nresearch, yet conventional methods are labor-intensive and prone to error.\nWhile deep learning-based approaches have proven effective, they often require\nlarge annotated datasets, which are scarce due to the challenges of manual\nannotation. To overcome this, we propose a novel framework for synthesizing\ndensely annotated 2D and 3D cell microscopy images using cascaded diffusion\nmodels. Our method synthesizes 2D and 3D cell masks from sparse 2D annotations\nusing multi-level diffusion models and NeuS, a 3D surface reconstruction\napproach. Following that, a pretrained 2D Stable Diffusion model is finetuned\nto generate realistic cell textures and the final outputs are combined to form\ncell populations. We show that training a segmentation model with a combination\nof our synthetic data and real data improves cell segmentation performance by\nup to 9\\% across multiple datasets. Additionally, the FID scores indicate that\nthe synthetic data closely resembles real data. The code for our proposed\napproach will be available at\nhttps://github.com/ruveydayilmaz0/cascaded\\_diffusion.\n", "link": "http://arxiv.org/abs/2411.11515v1", "date": "2024-11-18", "relevancy": 2.5085, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6288}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6288}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascaded%20Diffusion%20Models%20for%202D%20and%203D%20Microscopy%20Image%20Synthesis%20to%0A%20%20Enhance%20Cell%20Segmentation&body=Title%3A%20Cascaded%20Diffusion%20Models%20for%202D%20and%203D%20Microscopy%20Image%20Synthesis%20to%0A%20%20Enhance%20Cell%20Segmentation%0AAuthor%3A%20R%C3%BCveyda%20Yilmaz%20and%20Kaan%20Keven%20and%20Yuli%20Wu%20and%20Johannes%20Stegmaier%0AAbstract%3A%20%20%20Automated%20cell%20segmentation%20in%20microscopy%20images%20is%20essential%20for%20biomedical%0Aresearch%2C%20yet%20conventional%20methods%20are%20labor-intensive%20and%20prone%20to%20error.%0AWhile%20deep%20learning-based%20approaches%20have%20proven%20effective%2C%20they%20often%20require%0Alarge%20annotated%20datasets%2C%20which%20are%20scarce%20due%20to%20the%20challenges%20of%20manual%0Aannotation.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20framework%20for%20synthesizing%0Adensely%20annotated%202D%20and%203D%20cell%20microscopy%20images%20using%20cascaded%20diffusion%0Amodels.%20Our%20method%20synthesizes%202D%20and%203D%20cell%20masks%20from%20sparse%202D%20annotations%0Ausing%20multi-level%20diffusion%20models%20and%20NeuS%2C%20a%203D%20surface%20reconstruction%0Aapproach.%20Following%20that%2C%20a%20pretrained%202D%20Stable%20Diffusion%20model%20is%20finetuned%0Ato%20generate%20realistic%20cell%20textures%20and%20the%20final%20outputs%20are%20combined%20to%20form%0Acell%20populations.%20We%20show%20that%20training%20a%20segmentation%20model%20with%20a%20combination%0Aof%20our%20synthetic%20data%20and%20real%20data%20improves%20cell%20segmentation%20performance%20by%0Aup%20to%209%5C%25%20across%20multiple%20datasets.%20Additionally%2C%20the%20FID%20scores%20indicate%20that%0Athe%20synthetic%20data%20closely%20resembles%20real%20data.%20The%20code%20for%20our%20proposed%0Aapproach%20will%20be%20available%20at%0Ahttps%3A//github.com/ruveydayilmaz0/cascaded%5C_diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascaded%2520Diffusion%2520Models%2520for%25202D%2520and%25203D%2520Microscopy%2520Image%2520Synthesis%2520to%250A%2520%2520Enhance%2520Cell%2520Segmentation%26entry.906535625%3DR%25C3%25BCveyda%2520Yilmaz%2520and%2520Kaan%2520Keven%2520and%2520Yuli%2520Wu%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3D%2520%2520Automated%2520cell%2520segmentation%2520in%2520microscopy%2520images%2520is%2520essential%2520for%2520biomedical%250Aresearch%252C%2520yet%2520conventional%2520methods%2520are%2520labor-intensive%2520and%2520prone%2520to%2520error.%250AWhile%2520deep%2520learning-based%2520approaches%2520have%2520proven%2520effective%252C%2520they%2520often%2520require%250Alarge%2520annotated%2520datasets%252C%2520which%2520are%2520scarce%2520due%2520to%2520the%2520challenges%2520of%2520manual%250Aannotation.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520synthesizing%250Adensely%2520annotated%25202D%2520and%25203D%2520cell%2520microscopy%2520images%2520using%2520cascaded%2520diffusion%250Amodels.%2520Our%2520method%2520synthesizes%25202D%2520and%25203D%2520cell%2520masks%2520from%2520sparse%25202D%2520annotations%250Ausing%2520multi-level%2520diffusion%2520models%2520and%2520NeuS%252C%2520a%25203D%2520surface%2520reconstruction%250Aapproach.%2520Following%2520that%252C%2520a%2520pretrained%25202D%2520Stable%2520Diffusion%2520model%2520is%2520finetuned%250Ato%2520generate%2520realistic%2520cell%2520textures%2520and%2520the%2520final%2520outputs%2520are%2520combined%2520to%2520form%250Acell%2520populations.%2520We%2520show%2520that%2520training%2520a%2520segmentation%2520model%2520with%2520a%2520combination%250Aof%2520our%2520synthetic%2520data%2520and%2520real%2520data%2520improves%2520cell%2520segmentation%2520performance%2520by%250Aup%2520to%25209%255C%2525%2520across%2520multiple%2520datasets.%2520Additionally%252C%2520the%2520FID%2520scores%2520indicate%2520that%250Athe%2520synthetic%2520data%2520closely%2520resembles%2520real%2520data.%2520The%2520code%2520for%2520our%2520proposed%250Aapproach%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/ruveydayilmaz0/cascaded%255C_diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascaded%20Diffusion%20Models%20for%202D%20and%203D%20Microscopy%20Image%20Synthesis%20to%0A%20%20Enhance%20Cell%20Segmentation&entry.906535625=R%C3%BCveyda%20Yilmaz%20and%20Kaan%20Keven%20and%20Yuli%20Wu%20and%20Johannes%20Stegmaier&entry.1292438233=%20%20Automated%20cell%20segmentation%20in%20microscopy%20images%20is%20essential%20for%20biomedical%0Aresearch%2C%20yet%20conventional%20methods%20are%20labor-intensive%20and%20prone%20to%20error.%0AWhile%20deep%20learning-based%20approaches%20have%20proven%20effective%2C%20they%20often%20require%0Alarge%20annotated%20datasets%2C%20which%20are%20scarce%20due%20to%20the%20challenges%20of%20manual%0Aannotation.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20framework%20for%20synthesizing%0Adensely%20annotated%202D%20and%203D%20cell%20microscopy%20images%20using%20cascaded%20diffusion%0Amodels.%20Our%20method%20synthesizes%202D%20and%203D%20cell%20masks%20from%20sparse%202D%20annotations%0Ausing%20multi-level%20diffusion%20models%20and%20NeuS%2C%20a%203D%20surface%20reconstruction%0Aapproach.%20Following%20that%2C%20a%20pretrained%202D%20Stable%20Diffusion%20model%20is%20finetuned%0Ato%20generate%20realistic%20cell%20textures%20and%20the%20final%20outputs%20are%20combined%20to%20form%0Acell%20populations.%20We%20show%20that%20training%20a%20segmentation%20model%20with%20a%20combination%0Aof%20our%20synthetic%20data%20and%20real%20data%20improves%20cell%20segmentation%20performance%20by%0Aup%20to%209%5C%25%20across%20multiple%20datasets.%20Additionally%2C%20the%20FID%20scores%20indicate%20that%0Athe%20synthetic%20data%20closely%20resembles%20real%20data.%20The%20code%20for%20our%20proposed%0Aapproach%20will%20be%20available%20at%0Ahttps%3A//github.com/ruveydayilmaz0/cascaded%5C_diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11515v1&entry.124074799=Read"},
{"title": "VLN-Game: Vision-Language Equilibrium Search for Zero-Shot Semantic\n  Navigation", "author": "Bangguo Yu and Yuzhen Liu and Lei Han and Hamidreza Kasaei and Tingguang Li and Ming Cao", "abstract": "  Following human instructions to explore and search for a specified target in\nan unfamiliar environment is a crucial skill for mobile service robots. Most of\nthe previous works on object goal navigation have typically focused on a single\ninput modality as the target, which may lead to limited consideration of\nlanguage descriptions containing detailed attributes and spatial relationships.\nTo address this limitation, we propose VLN-Game, a novel zero-shot framework\nfor visual target navigation that can process object names and descriptive\nlanguage targets effectively. To be more precise, our approach constructs a 3D\nobject-centric spatial map by integrating pre-trained visual-language features\nwith a 3D reconstruction of the physical environment. Then, the framework\nidentifies the most promising areas to explore in search of potential target\ncandidates. A game-theoretic vision language model is employed to determine\nwhich target best matches the given language description. Experiments conducted\non the Habitat-Matterport 3D (HM3D) dataset demonstrate that the proposed\nframework achieves state-of-the-art performance in both object goal navigation\nand language-based navigation tasks. Moreover, we show that VLN-Game can be\neasily deployed on real-world robots. The success of VLN-Game highlights the\npromising potential of using game-theoretic methods with compact\nvision-language models to advance decision-making capabilities in robotic\nsystems. The supplementary video and code can be accessed via the following\nlink: https://sites.google.com/view/vln-game.\n", "link": "http://arxiv.org/abs/2411.11609v1", "date": "2024-11-18", "relevancy": 2.4953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6319}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLN-Game%3A%20Vision-Language%20Equilibrium%20Search%20for%20Zero-Shot%20Semantic%0A%20%20Navigation&body=Title%3A%20VLN-Game%3A%20Vision-Language%20Equilibrium%20Search%20for%20Zero-Shot%20Semantic%0A%20%20Navigation%0AAuthor%3A%20Bangguo%20Yu%20and%20Yuzhen%20Liu%20and%20Lei%20Han%20and%20Hamidreza%20Kasaei%20and%20Tingguang%20Li%20and%20Ming%20Cao%0AAbstract%3A%20%20%20Following%20human%20instructions%20to%20explore%20and%20search%20for%20a%20specified%20target%20in%0Aan%20unfamiliar%20environment%20is%20a%20crucial%20skill%20for%20mobile%20service%20robots.%20Most%20of%0Athe%20previous%20works%20on%20object%20goal%20navigation%20have%20typically%20focused%20on%20a%20single%0Ainput%20modality%20as%20the%20target%2C%20which%20may%20lead%20to%20limited%20consideration%20of%0Alanguage%20descriptions%20containing%20detailed%20attributes%20and%20spatial%20relationships.%0ATo%20address%20this%20limitation%2C%20we%20propose%20VLN-Game%2C%20a%20novel%20zero-shot%20framework%0Afor%20visual%20target%20navigation%20that%20can%20process%20object%20names%20and%20descriptive%0Alanguage%20targets%20effectively.%20To%20be%20more%20precise%2C%20our%20approach%20constructs%20a%203D%0Aobject-centric%20spatial%20map%20by%20integrating%20pre-trained%20visual-language%20features%0Awith%20a%203D%20reconstruction%20of%20the%20physical%20environment.%20Then%2C%20the%20framework%0Aidentifies%20the%20most%20promising%20areas%20to%20explore%20in%20search%20of%20potential%20target%0Acandidates.%20A%20game-theoretic%20vision%20language%20model%20is%20employed%20to%20determine%0Awhich%20target%20best%20matches%20the%20given%20language%20description.%20Experiments%20conducted%0Aon%20the%20Habitat-Matterport%203D%20%28HM3D%29%20dataset%20demonstrate%20that%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20both%20object%20goal%20navigation%0Aand%20language-based%20navigation%20tasks.%20Moreover%2C%20we%20show%20that%20VLN-Game%20can%20be%0Aeasily%20deployed%20on%20real-world%20robots.%20The%20success%20of%20VLN-Game%20highlights%20the%0Apromising%20potential%20of%20using%20game-theoretic%20methods%20with%20compact%0Avision-language%20models%20to%20advance%20decision-making%20capabilities%20in%20robotic%0Asystems.%20The%20supplementary%20video%20and%20code%20can%20be%20accessed%20via%20the%20following%0Alink%3A%20https%3A//sites.google.com/view/vln-game.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLN-Game%253A%2520Vision-Language%2520Equilibrium%2520Search%2520for%2520Zero-Shot%2520Semantic%250A%2520%2520Navigation%26entry.906535625%3DBangguo%2520Yu%2520and%2520Yuzhen%2520Liu%2520and%2520Lei%2520Han%2520and%2520Hamidreza%2520Kasaei%2520and%2520Tingguang%2520Li%2520and%2520Ming%2520Cao%26entry.1292438233%3D%2520%2520Following%2520human%2520instructions%2520to%2520explore%2520and%2520search%2520for%2520a%2520specified%2520target%2520in%250Aan%2520unfamiliar%2520environment%2520is%2520a%2520crucial%2520skill%2520for%2520mobile%2520service%2520robots.%2520Most%2520of%250Athe%2520previous%2520works%2520on%2520object%2520goal%2520navigation%2520have%2520typically%2520focused%2520on%2520a%2520single%250Ainput%2520modality%2520as%2520the%2520target%252C%2520which%2520may%2520lead%2520to%2520limited%2520consideration%2520of%250Alanguage%2520descriptions%2520containing%2520detailed%2520attributes%2520and%2520spatial%2520relationships.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520propose%2520VLN-Game%252C%2520a%2520novel%2520zero-shot%2520framework%250Afor%2520visual%2520target%2520navigation%2520that%2520can%2520process%2520object%2520names%2520and%2520descriptive%250Alanguage%2520targets%2520effectively.%2520To%2520be%2520more%2520precise%252C%2520our%2520approach%2520constructs%2520a%25203D%250Aobject-centric%2520spatial%2520map%2520by%2520integrating%2520pre-trained%2520visual-language%2520features%250Awith%2520a%25203D%2520reconstruction%2520of%2520the%2520physical%2520environment.%2520Then%252C%2520the%2520framework%250Aidentifies%2520the%2520most%2520promising%2520areas%2520to%2520explore%2520in%2520search%2520of%2520potential%2520target%250Acandidates.%2520A%2520game-theoretic%2520vision%2520language%2520model%2520is%2520employed%2520to%2520determine%250Awhich%2520target%2520best%2520matches%2520the%2520given%2520language%2520description.%2520Experiments%2520conducted%250Aon%2520the%2520Habitat-Matterport%25203D%2520%2528HM3D%2529%2520dataset%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520object%2520goal%2520navigation%250Aand%2520language-based%2520navigation%2520tasks.%2520Moreover%252C%2520we%2520show%2520that%2520VLN-Game%2520can%2520be%250Aeasily%2520deployed%2520on%2520real-world%2520robots.%2520The%2520success%2520of%2520VLN-Game%2520highlights%2520the%250Apromising%2520potential%2520of%2520using%2520game-theoretic%2520methods%2520with%2520compact%250Avision-language%2520models%2520to%2520advance%2520decision-making%2520capabilities%2520in%2520robotic%250Asystems.%2520The%2520supplementary%2520video%2520and%2520code%2520can%2520be%2520accessed%2520via%2520the%2520following%250Alink%253A%2520https%253A//sites.google.com/view/vln-game.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLN-Game%3A%20Vision-Language%20Equilibrium%20Search%20for%20Zero-Shot%20Semantic%0A%20%20Navigation&entry.906535625=Bangguo%20Yu%20and%20Yuzhen%20Liu%20and%20Lei%20Han%20and%20Hamidreza%20Kasaei%20and%20Tingguang%20Li%20and%20Ming%20Cao&entry.1292438233=%20%20Following%20human%20instructions%20to%20explore%20and%20search%20for%20a%20specified%20target%20in%0Aan%20unfamiliar%20environment%20is%20a%20crucial%20skill%20for%20mobile%20service%20robots.%20Most%20of%0Athe%20previous%20works%20on%20object%20goal%20navigation%20have%20typically%20focused%20on%20a%20single%0Ainput%20modality%20as%20the%20target%2C%20which%20may%20lead%20to%20limited%20consideration%20of%0Alanguage%20descriptions%20containing%20detailed%20attributes%20and%20spatial%20relationships.%0ATo%20address%20this%20limitation%2C%20we%20propose%20VLN-Game%2C%20a%20novel%20zero-shot%20framework%0Afor%20visual%20target%20navigation%20that%20can%20process%20object%20names%20and%20descriptive%0Alanguage%20targets%20effectively.%20To%20be%20more%20precise%2C%20our%20approach%20constructs%20a%203D%0Aobject-centric%20spatial%20map%20by%20integrating%20pre-trained%20visual-language%20features%0Awith%20a%203D%20reconstruction%20of%20the%20physical%20environment.%20Then%2C%20the%20framework%0Aidentifies%20the%20most%20promising%20areas%20to%20explore%20in%20search%20of%20potential%20target%0Acandidates.%20A%20game-theoretic%20vision%20language%20model%20is%20employed%20to%20determine%0Awhich%20target%20best%20matches%20the%20given%20language%20description.%20Experiments%20conducted%0Aon%20the%20Habitat-Matterport%203D%20%28HM3D%29%20dataset%20demonstrate%20that%20the%20proposed%0Aframework%20achieves%20state-of-the-art%20performance%20in%20both%20object%20goal%20navigation%0Aand%20language-based%20navigation%20tasks.%20Moreover%2C%20we%20show%20that%20VLN-Game%20can%20be%0Aeasily%20deployed%20on%20real-world%20robots.%20The%20success%20of%20VLN-Game%20highlights%20the%0Apromising%20potential%20of%20using%20game-theoretic%20methods%20with%20compact%0Avision-language%20models%20to%20advance%20decision-making%20capabilities%20in%20robotic%0Asystems.%20The%20supplementary%20video%20and%20code%20can%20be%20accessed%20via%20the%20following%0Alink%3A%20https%3A//sites.google.com/view/vln-game.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11609v1&entry.124074799=Read"},
{"title": "PALMS: Parallel Adaptive Lasso with Multi-directional Signals for Latent\n  Networks Reconstruction", "author": "Zhaoyu Xing and Wei Zhong", "abstract": "  Large-scale networks exist in many field and play an important role in\nreal-world dynamics. However, the networks are usually latent and expensive to\ndetect, which becomes the main challenging for many applications and empirical\nanalysis. Several statistical methods were proposed to infer the edges, but the\ncomplexity of algorithms make them hard to be applied for large-scale networks.\nIn this paper, we proposed a general distributed and parallel computing\nframework for network reconstruction methods via compressive sensing technical,\nto make them feasible for inferring the super large networks in practice.\nCombining with the CALMS, we proposed for those estimators enjoy additional\ntheoretical properties, such as the consistency and asymptotic normality, we\nprove that the approximate estimation utilizing the distributed algorithm can\nkeep the theoretical results.\n", "link": "http://arxiv.org/abs/2411.11464v1", "date": "2024-11-18", "relevancy": 2.4947, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4922}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PALMS%3A%20Parallel%20Adaptive%20Lasso%20with%20Multi-directional%20Signals%20for%20Latent%0A%20%20Networks%20Reconstruction&body=Title%3A%20PALMS%3A%20Parallel%20Adaptive%20Lasso%20with%20Multi-directional%20Signals%20for%20Latent%0A%20%20Networks%20Reconstruction%0AAuthor%3A%20Zhaoyu%20Xing%20and%20Wei%20Zhong%0AAbstract%3A%20%20%20Large-scale%20networks%20exist%20in%20many%20field%20and%20play%20an%20important%20role%20in%0Areal-world%20dynamics.%20However%2C%20the%20networks%20are%20usually%20latent%20and%20expensive%20to%0Adetect%2C%20which%20becomes%20the%20main%20challenging%20for%20many%20applications%20and%20empirical%0Aanalysis.%20Several%20statistical%20methods%20were%20proposed%20to%20infer%20the%20edges%2C%20but%20the%0Acomplexity%20of%20algorithms%20make%20them%20hard%20to%20be%20applied%20for%20large-scale%20networks.%0AIn%20this%20paper%2C%20we%20proposed%20a%20general%20distributed%20and%20parallel%20computing%0Aframework%20for%20network%20reconstruction%20methods%20via%20compressive%20sensing%20technical%2C%0Ato%20make%20them%20feasible%20for%20inferring%20the%20super%20large%20networks%20in%20practice.%0ACombining%20with%20the%20CALMS%2C%20we%20proposed%20for%20those%20estimators%20enjoy%20additional%0Atheoretical%20properties%2C%20such%20as%20the%20consistency%20and%20asymptotic%20normality%2C%20we%0Aprove%20that%20the%20approximate%20estimation%20utilizing%20the%20distributed%20algorithm%20can%0Akeep%20the%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPALMS%253A%2520Parallel%2520Adaptive%2520Lasso%2520with%2520Multi-directional%2520Signals%2520for%2520Latent%250A%2520%2520Networks%2520Reconstruction%26entry.906535625%3DZhaoyu%2520Xing%2520and%2520Wei%2520Zhong%26entry.1292438233%3D%2520%2520Large-scale%2520networks%2520exist%2520in%2520many%2520field%2520and%2520play%2520an%2520important%2520role%2520in%250Areal-world%2520dynamics.%2520However%252C%2520the%2520networks%2520are%2520usually%2520latent%2520and%2520expensive%2520to%250Adetect%252C%2520which%2520becomes%2520the%2520main%2520challenging%2520for%2520many%2520applications%2520and%2520empirical%250Aanalysis.%2520Several%2520statistical%2520methods%2520were%2520proposed%2520to%2520infer%2520the%2520edges%252C%2520but%2520the%250Acomplexity%2520of%2520algorithms%2520make%2520them%2520hard%2520to%2520be%2520applied%2520for%2520large-scale%2520networks.%250AIn%2520this%2520paper%252C%2520we%2520proposed%2520a%2520general%2520distributed%2520and%2520parallel%2520computing%250Aframework%2520for%2520network%2520reconstruction%2520methods%2520via%2520compressive%2520sensing%2520technical%252C%250Ato%2520make%2520them%2520feasible%2520for%2520inferring%2520the%2520super%2520large%2520networks%2520in%2520practice.%250ACombining%2520with%2520the%2520CALMS%252C%2520we%2520proposed%2520for%2520those%2520estimators%2520enjoy%2520additional%250Atheoretical%2520properties%252C%2520such%2520as%2520the%2520consistency%2520and%2520asymptotic%2520normality%252C%2520we%250Aprove%2520that%2520the%2520approximate%2520estimation%2520utilizing%2520the%2520distributed%2520algorithm%2520can%250Akeep%2520the%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PALMS%3A%20Parallel%20Adaptive%20Lasso%20with%20Multi-directional%20Signals%20for%20Latent%0A%20%20Networks%20Reconstruction&entry.906535625=Zhaoyu%20Xing%20and%20Wei%20Zhong&entry.1292438233=%20%20Large-scale%20networks%20exist%20in%20many%20field%20and%20play%20an%20important%20role%20in%0Areal-world%20dynamics.%20However%2C%20the%20networks%20are%20usually%20latent%20and%20expensive%20to%0Adetect%2C%20which%20becomes%20the%20main%20challenging%20for%20many%20applications%20and%20empirical%0Aanalysis.%20Several%20statistical%20methods%20were%20proposed%20to%20infer%20the%20edges%2C%20but%20the%0Acomplexity%20of%20algorithms%20make%20them%20hard%20to%20be%20applied%20for%20large-scale%20networks.%0AIn%20this%20paper%2C%20we%20proposed%20a%20general%20distributed%20and%20parallel%20computing%0Aframework%20for%20network%20reconstruction%20methods%20via%20compressive%20sensing%20technical%2C%0Ato%20make%20them%20feasible%20for%20inferring%20the%20super%20large%20networks%20in%20practice.%0ACombining%20with%20the%20CALMS%2C%20we%20proposed%20for%20those%20estimators%20enjoy%20additional%0Atheoretical%20properties%2C%20such%20as%20the%20consistency%20and%20asymptotic%20normality%2C%20we%0Aprove%20that%20the%20approximate%20estimation%20utilizing%20the%20distributed%20algorithm%20can%0Akeep%20the%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11464v1&entry.124074799=Read"},
{"title": "MagicStick: Controllable Video Editing via Control Handle\n  Transformations", "author": "Yue Ma and Xiaodong Cun and Sen Liang and Jinbo Xing and Yingqing He and Chenyang Qi and Siran Chen and Qifeng Chen", "abstract": "  Text-based video editing has recently attracted considerable interest in\nchanging the style or replacing the objects with a similar structure. Beyond\nthis, we demonstrate that properties such as shape, size, location, motion,\netc., can also be edited in videos. Our key insight is that the keyframe\ntransformations of the specific internal feature (e.g., edge maps of objects or\nhuman pose), can easily propagate to other frames to provide generation\nguidance. We thus propose MagicStick, a controllable video editing method that\nedits the video properties by utilizing the transformation on the extracted\ninternal control signals. In detail, to keep the appearance, we inflate both\nthe pretrained image diffusion model and ControlNet to the temporal dimension\nand train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in\nediting, we perform an inversion and editing framework. Differently, finetuned\nControlNet is introduced in both inversion and generation for attention\nguidance with the proposed attention remix between the spatial attention maps\nof inversion and editing. Yet succinct, our method is the first method to show\nthe ability of video property editing from the pre-trained text-to-image model.\nWe present experiments on numerous examples within our unified framework. We\nalso compare with shape-aware text-based editing and handcrafted motion video\ngeneration, demonstrating our superior temporal consistency and editing\ncapability than previous works. The code and models are available on\nhttps://github.com/mayuelala/MagicStick.\n", "link": "http://arxiv.org/abs/2312.03047v2", "date": "2024-11-18", "relevancy": 2.4777, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.675}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6734}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicStick%3A%20Controllable%20Video%20Editing%20via%20Control%20Handle%0A%20%20Transformations&body=Title%3A%20MagicStick%3A%20Controllable%20Video%20Editing%20via%20Control%20Handle%0A%20%20Transformations%0AAuthor%3A%20Yue%20Ma%20and%20Xiaodong%20Cun%20and%20Sen%20Liang%20and%20Jinbo%20Xing%20and%20Yingqing%20He%20and%20Chenyang%20Qi%20and%20Siran%20Chen%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Text-based%20video%20editing%20has%20recently%20attracted%20considerable%20interest%20in%0Achanging%20the%20style%20or%20replacing%20the%20objects%20with%20a%20similar%20structure.%20Beyond%0Athis%2C%20we%20demonstrate%20that%20properties%20such%20as%20shape%2C%20size%2C%20location%2C%20motion%2C%0Aetc.%2C%20can%20also%20be%20edited%20in%20videos.%20Our%20key%20insight%20is%20that%20the%20keyframe%0Atransformations%20of%20the%20specific%20internal%20feature%20%28e.g.%2C%20edge%20maps%20of%20objects%20or%0Ahuman%20pose%29%2C%20can%20easily%20propagate%20to%20other%20frames%20to%20provide%20generation%0Aguidance.%20We%20thus%20propose%20MagicStick%2C%20a%20controllable%20video%20editing%20method%20that%0Aedits%20the%20video%20properties%20by%20utilizing%20the%20transformation%20on%20the%20extracted%0Ainternal%20control%20signals.%20In%20detail%2C%20to%20keep%20the%20appearance%2C%20we%20inflate%20both%0Athe%20pretrained%20image%20diffusion%20model%20and%20ControlNet%20to%20the%20temporal%20dimension%0Aand%20train%20low-rank%20adaptions%20%28LORA%29%20layers%20to%20fit%20the%20specific%20scenes.%20Then%2C%20in%0Aediting%2C%20we%20perform%20an%20inversion%20and%20editing%20framework.%20Differently%2C%20finetuned%0AControlNet%20is%20introduced%20in%20both%20inversion%20and%20generation%20for%20attention%0Aguidance%20with%20the%20proposed%20attention%20remix%20between%20the%20spatial%20attention%20maps%0Aof%20inversion%20and%20editing.%20Yet%20succinct%2C%20our%20method%20is%20the%20first%20method%20to%20show%0Athe%20ability%20of%20video%20property%20editing%20from%20the%20pre-trained%20text-to-image%20model.%0AWe%20present%20experiments%20on%20numerous%20examples%20within%20our%20unified%20framework.%20We%0Aalso%20compare%20with%20shape-aware%20text-based%20editing%20and%20handcrafted%20motion%20video%0Ageneration%2C%20demonstrating%20our%20superior%20temporal%20consistency%20and%20editing%0Acapability%20than%20previous%20works.%20The%20code%20and%20models%20are%20available%20on%0Ahttps%3A//github.com/mayuelala/MagicStick.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicStick%253A%2520Controllable%2520Video%2520Editing%2520via%2520Control%2520Handle%250A%2520%2520Transformations%26entry.906535625%3DYue%2520Ma%2520and%2520Xiaodong%2520Cun%2520and%2520Sen%2520Liang%2520and%2520Jinbo%2520Xing%2520and%2520Yingqing%2520He%2520and%2520Chenyang%2520Qi%2520and%2520Siran%2520Chen%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Text-based%2520video%2520editing%2520has%2520recently%2520attracted%2520considerable%2520interest%2520in%250Achanging%2520the%2520style%2520or%2520replacing%2520the%2520objects%2520with%2520a%2520similar%2520structure.%2520Beyond%250Athis%252C%2520we%2520demonstrate%2520that%2520properties%2520such%2520as%2520shape%252C%2520size%252C%2520location%252C%2520motion%252C%250Aetc.%252C%2520can%2520also%2520be%2520edited%2520in%2520videos.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520keyframe%250Atransformations%2520of%2520the%2520specific%2520internal%2520feature%2520%2528e.g.%252C%2520edge%2520maps%2520of%2520objects%2520or%250Ahuman%2520pose%2529%252C%2520can%2520easily%2520propagate%2520to%2520other%2520frames%2520to%2520provide%2520generation%250Aguidance.%2520We%2520thus%2520propose%2520MagicStick%252C%2520a%2520controllable%2520video%2520editing%2520method%2520that%250Aedits%2520the%2520video%2520properties%2520by%2520utilizing%2520the%2520transformation%2520on%2520the%2520extracted%250Ainternal%2520control%2520signals.%2520In%2520detail%252C%2520to%2520keep%2520the%2520appearance%252C%2520we%2520inflate%2520both%250Athe%2520pretrained%2520image%2520diffusion%2520model%2520and%2520ControlNet%2520to%2520the%2520temporal%2520dimension%250Aand%2520train%2520low-rank%2520adaptions%2520%2528LORA%2529%2520layers%2520to%2520fit%2520the%2520specific%2520scenes.%2520Then%252C%2520in%250Aediting%252C%2520we%2520perform%2520an%2520inversion%2520and%2520editing%2520framework.%2520Differently%252C%2520finetuned%250AControlNet%2520is%2520introduced%2520in%2520both%2520inversion%2520and%2520generation%2520for%2520attention%250Aguidance%2520with%2520the%2520proposed%2520attention%2520remix%2520between%2520the%2520spatial%2520attention%2520maps%250Aof%2520inversion%2520and%2520editing.%2520Yet%2520succinct%252C%2520our%2520method%2520is%2520the%2520first%2520method%2520to%2520show%250Athe%2520ability%2520of%2520video%2520property%2520editing%2520from%2520the%2520pre-trained%2520text-to-image%2520model.%250AWe%2520present%2520experiments%2520on%2520numerous%2520examples%2520within%2520our%2520unified%2520framework.%2520We%250Aalso%2520compare%2520with%2520shape-aware%2520text-based%2520editing%2520and%2520handcrafted%2520motion%2520video%250Ageneration%252C%2520demonstrating%2520our%2520superior%2520temporal%2520consistency%2520and%2520editing%250Acapability%2520than%2520previous%2520works.%2520The%2520code%2520and%2520models%2520are%2520available%2520on%250Ahttps%253A//github.com/mayuelala/MagicStick.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicStick%3A%20Controllable%20Video%20Editing%20via%20Control%20Handle%0A%20%20Transformations&entry.906535625=Yue%20Ma%20and%20Xiaodong%20Cun%20and%20Sen%20Liang%20and%20Jinbo%20Xing%20and%20Yingqing%20He%20and%20Chenyang%20Qi%20and%20Siran%20Chen%20and%20Qifeng%20Chen&entry.1292438233=%20%20Text-based%20video%20editing%20has%20recently%20attracted%20considerable%20interest%20in%0Achanging%20the%20style%20or%20replacing%20the%20objects%20with%20a%20similar%20structure.%20Beyond%0Athis%2C%20we%20demonstrate%20that%20properties%20such%20as%20shape%2C%20size%2C%20location%2C%20motion%2C%0Aetc.%2C%20can%20also%20be%20edited%20in%20videos.%20Our%20key%20insight%20is%20that%20the%20keyframe%0Atransformations%20of%20the%20specific%20internal%20feature%20%28e.g.%2C%20edge%20maps%20of%20objects%20or%0Ahuman%20pose%29%2C%20can%20easily%20propagate%20to%20other%20frames%20to%20provide%20generation%0Aguidance.%20We%20thus%20propose%20MagicStick%2C%20a%20controllable%20video%20editing%20method%20that%0Aedits%20the%20video%20properties%20by%20utilizing%20the%20transformation%20on%20the%20extracted%0Ainternal%20control%20signals.%20In%20detail%2C%20to%20keep%20the%20appearance%2C%20we%20inflate%20both%0Athe%20pretrained%20image%20diffusion%20model%20and%20ControlNet%20to%20the%20temporal%20dimension%0Aand%20train%20low-rank%20adaptions%20%28LORA%29%20layers%20to%20fit%20the%20specific%20scenes.%20Then%2C%20in%0Aediting%2C%20we%20perform%20an%20inversion%20and%20editing%20framework.%20Differently%2C%20finetuned%0AControlNet%20is%20introduced%20in%20both%20inversion%20and%20generation%20for%20attention%0Aguidance%20with%20the%20proposed%20attention%20remix%20between%20the%20spatial%20attention%20maps%0Aof%20inversion%20and%20editing.%20Yet%20succinct%2C%20our%20method%20is%20the%20first%20method%20to%20show%0Athe%20ability%20of%20video%20property%20editing%20from%20the%20pre-trained%20text-to-image%20model.%0AWe%20present%20experiments%20on%20numerous%20examples%20within%20our%20unified%20framework.%20We%0Aalso%20compare%20with%20shape-aware%20text-based%20editing%20and%20handcrafted%20motion%20video%0Ageneration%2C%20demonstrating%20our%20superior%20temporal%20consistency%20and%20editing%0Acapability%20than%20previous%20works.%20The%20code%20and%20models%20are%20available%20on%0Ahttps%3A//github.com/mayuelala/MagicStick.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03047v2&entry.124074799=Read"},
{"title": "Hierarchical-Graph-Structured Edge Partition Models for Learning\n  Evolving Community Structure", "author": "Xincan Yu and Sikun Yang", "abstract": "  We propose a novel dynamic network model to capture evolving latent\ncommunities within temporal networks. To achieve this, we decompose each\nobserved dynamic edge between vertices using a Poisson-gamma edge partition\nmodel, assigning each vertex to one or more latent communities through\n\\emph{nonnegative} vertex-community memberships. Specifically, hierarchical\ntransition kernels are employed to model the interactions between these latent\ncommunities in the observed temporal network. A hierarchical graph prior is\nplaced on the transition structure of the latent communities, allowing us to\nmodel how they evolve and interact over time. Consequently, our dynamic network\nenables the inferred community structure to merge, split, and interact with one\nanother, providing a comprehensive understanding of complex network dynamics.\nExperiments on various real-world network datasets demonstrate that the\nproposed model not only effectively uncovers interpretable latent structures\nbut also surpasses other state-of-the art dynamic network models in the tasks\nof link prediction and community detection.\n", "link": "http://arxiv.org/abs/2411.11536v1", "date": "2024-11-18", "relevancy": 2.4692, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5154}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4848}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical-Graph-Structured%20Edge%20Partition%20Models%20for%20Learning%0A%20%20Evolving%20Community%20Structure&body=Title%3A%20Hierarchical-Graph-Structured%20Edge%20Partition%20Models%20for%20Learning%0A%20%20Evolving%20Community%20Structure%0AAuthor%3A%20Xincan%20Yu%20and%20Sikun%20Yang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20dynamic%20network%20model%20to%20capture%20evolving%20latent%0Acommunities%20within%20temporal%20networks.%20To%20achieve%20this%2C%20we%20decompose%20each%0Aobserved%20dynamic%20edge%20between%20vertices%20using%20a%20Poisson-gamma%20edge%20partition%0Amodel%2C%20assigning%20each%20vertex%20to%20one%20or%20more%20latent%20communities%20through%0A%5Cemph%7Bnonnegative%7D%20vertex-community%20memberships.%20Specifically%2C%20hierarchical%0Atransition%20kernels%20are%20employed%20to%20model%20the%20interactions%20between%20these%20latent%0Acommunities%20in%20the%20observed%20temporal%20network.%20A%20hierarchical%20graph%20prior%20is%0Aplaced%20on%20the%20transition%20structure%20of%20the%20latent%20communities%2C%20allowing%20us%20to%0Amodel%20how%20they%20evolve%20and%20interact%20over%20time.%20Consequently%2C%20our%20dynamic%20network%0Aenables%20the%20inferred%20community%20structure%20to%20merge%2C%20split%2C%20and%20interact%20with%20one%0Aanother%2C%20providing%20a%20comprehensive%20understanding%20of%20complex%20network%20dynamics.%0AExperiments%20on%20various%20real-world%20network%20datasets%20demonstrate%20that%20the%0Aproposed%20model%20not%20only%20effectively%20uncovers%20interpretable%20latent%20structures%0Abut%20also%20surpasses%20other%20state-of-the%20art%20dynamic%20network%20models%20in%20the%20tasks%0Aof%20link%20prediction%20and%20community%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical-Graph-Structured%2520Edge%2520Partition%2520Models%2520for%2520Learning%250A%2520%2520Evolving%2520Community%2520Structure%26entry.906535625%3DXincan%2520Yu%2520and%2520Sikun%2520Yang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520dynamic%2520network%2520model%2520to%2520capture%2520evolving%2520latent%250Acommunities%2520within%2520temporal%2520networks.%2520To%2520achieve%2520this%252C%2520we%2520decompose%2520each%250Aobserved%2520dynamic%2520edge%2520between%2520vertices%2520using%2520a%2520Poisson-gamma%2520edge%2520partition%250Amodel%252C%2520assigning%2520each%2520vertex%2520to%2520one%2520or%2520more%2520latent%2520communities%2520through%250A%255Cemph%257Bnonnegative%257D%2520vertex-community%2520memberships.%2520Specifically%252C%2520hierarchical%250Atransition%2520kernels%2520are%2520employed%2520to%2520model%2520the%2520interactions%2520between%2520these%2520latent%250Acommunities%2520in%2520the%2520observed%2520temporal%2520network.%2520A%2520hierarchical%2520graph%2520prior%2520is%250Aplaced%2520on%2520the%2520transition%2520structure%2520of%2520the%2520latent%2520communities%252C%2520allowing%2520us%2520to%250Amodel%2520how%2520they%2520evolve%2520and%2520interact%2520over%2520time.%2520Consequently%252C%2520our%2520dynamic%2520network%250Aenables%2520the%2520inferred%2520community%2520structure%2520to%2520merge%252C%2520split%252C%2520and%2520interact%2520with%2520one%250Aanother%252C%2520providing%2520a%2520comprehensive%2520understanding%2520of%2520complex%2520network%2520dynamics.%250AExperiments%2520on%2520various%2520real-world%2520network%2520datasets%2520demonstrate%2520that%2520the%250Aproposed%2520model%2520not%2520only%2520effectively%2520uncovers%2520interpretable%2520latent%2520structures%250Abut%2520also%2520surpasses%2520other%2520state-of-the%2520art%2520dynamic%2520network%2520models%2520in%2520the%2520tasks%250Aof%2520link%2520prediction%2520and%2520community%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical-Graph-Structured%20Edge%20Partition%20Models%20for%20Learning%0A%20%20Evolving%20Community%20Structure&entry.906535625=Xincan%20Yu%20and%20Sikun%20Yang&entry.1292438233=%20%20We%20propose%20a%20novel%20dynamic%20network%20model%20to%20capture%20evolving%20latent%0Acommunities%20within%20temporal%20networks.%20To%20achieve%20this%2C%20we%20decompose%20each%0Aobserved%20dynamic%20edge%20between%20vertices%20using%20a%20Poisson-gamma%20edge%20partition%0Amodel%2C%20assigning%20each%20vertex%20to%20one%20or%20more%20latent%20communities%20through%0A%5Cemph%7Bnonnegative%7D%20vertex-community%20memberships.%20Specifically%2C%20hierarchical%0Atransition%20kernels%20are%20employed%20to%20model%20the%20interactions%20between%20these%20latent%0Acommunities%20in%20the%20observed%20temporal%20network.%20A%20hierarchical%20graph%20prior%20is%0Aplaced%20on%20the%20transition%20structure%20of%20the%20latent%20communities%2C%20allowing%20us%20to%0Amodel%20how%20they%20evolve%20and%20interact%20over%20time.%20Consequently%2C%20our%20dynamic%20network%0Aenables%20the%20inferred%20community%20structure%20to%20merge%2C%20split%2C%20and%20interact%20with%20one%0Aanother%2C%20providing%20a%20comprehensive%20understanding%20of%20complex%20network%20dynamics.%0AExperiments%20on%20various%20real-world%20network%20datasets%20demonstrate%20that%20the%0Aproposed%20model%20not%20only%20effectively%20uncovers%20interpretable%20latent%20structures%0Abut%20also%20surpasses%20other%20state-of-the%20art%20dynamic%20network%20models%20in%20the%20tasks%0Aof%20link%20prediction%20and%20community%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11536v1&entry.124074799=Read"},
{"title": "Variational Graph Autoencoder for Heterogeneous Information Networks\n  with Missing and Inaccurate Attributes", "author": "Yige Zhao and Jianxiang Yu and Yao Cheng and Chengcheng Yu and Yiding Liu and Xiang Li and Shuaiqiang Wang", "abstract": "  Heterogeneous Information Networks (HINs), which consist of various types of\nnodes and edges, have recently demonstrated excellent performance in graph\nmining. However, most existing heterogeneous graph neural networks (HGNNs)\nignore the problems of missing attributes, inaccurate attributes and scarce\nlabels for nodes, which limits their expressiveness. In this paper, we propose\na generative self-supervised model GraMI to address these issues\nsimultaneously. Specifically, GraMI first initializes all the nodes in the\ngraph with a low-dimensional representation matrix. After that, based on the\nvariational graph autoencoder framework, GraMI learns both node-level and\nattribute-level embeddings in the encoder, which can provide fine-grained\nsemantic information to construct node attributes. In the decoder, GraMI\nreconstructs both links and attributes. Instead of directly reconstructing raw\nfeatures for attributed nodes, GraMI generates the initial low-dimensional\nrepresentation matrix for all the nodes, based on which raw features of\nattributed nodes are further reconstructed to leverage accurate attributes. In\nthis way, GraMI can not only complete informative features for non-attributed\nnodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct\nextensive experiments to show the superiority of GraMI in tackling HINs with\nmissing and inaccurate attributes.\n", "link": "http://arxiv.org/abs/2311.07929v2", "date": "2024-11-18", "relevancy": 2.4428, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5012}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4854}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Graph%20Autoencoder%20for%20Heterogeneous%20Information%20Networks%0A%20%20with%20Missing%20and%20Inaccurate%20Attributes&body=Title%3A%20Variational%20Graph%20Autoencoder%20for%20Heterogeneous%20Information%20Networks%0A%20%20with%20Missing%20and%20Inaccurate%20Attributes%0AAuthor%3A%20Yige%20Zhao%20and%20Jianxiang%20Yu%20and%20Yao%20Cheng%20and%20Chengcheng%20Yu%20and%20Yiding%20Liu%20and%20Xiang%20Li%20and%20Shuaiqiang%20Wang%0AAbstract%3A%20%20%20Heterogeneous%20Information%20Networks%20%28HINs%29%2C%20which%20consist%20of%20various%20types%20of%0Anodes%20and%20edges%2C%20have%20recently%20demonstrated%20excellent%20performance%20in%20graph%0Amining.%20However%2C%20most%20existing%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%0Aignore%20the%20problems%20of%20missing%20attributes%2C%20inaccurate%20attributes%20and%20scarce%0Alabels%20for%20nodes%2C%20which%20limits%20their%20expressiveness.%20In%20this%20paper%2C%20we%20propose%0Aa%20generative%20self-supervised%20model%20GraMI%20to%20address%20these%20issues%0Asimultaneously.%20Specifically%2C%20GraMI%20first%20initializes%20all%20the%20nodes%20in%20the%0Agraph%20with%20a%20low-dimensional%20representation%20matrix.%20After%20that%2C%20based%20on%20the%0Avariational%20graph%20autoencoder%20framework%2C%20GraMI%20learns%20both%20node-level%20and%0Aattribute-level%20embeddings%20in%20the%20encoder%2C%20which%20can%20provide%20fine-grained%0Asemantic%20information%20to%20construct%20node%20attributes.%20In%20the%20decoder%2C%20GraMI%0Areconstructs%20both%20links%20and%20attributes.%20Instead%20of%20directly%20reconstructing%20raw%0Afeatures%20for%20attributed%20nodes%2C%20GraMI%20generates%20the%20initial%20low-dimensional%0Arepresentation%20matrix%20for%20all%20the%20nodes%2C%20based%20on%20which%20raw%20features%20of%0Aattributed%20nodes%20are%20further%20reconstructed%20to%20leverage%20accurate%20attributes.%20In%0Athis%20way%2C%20GraMI%20can%20not%20only%20complete%20informative%20features%20for%20non-attributed%0Anodes%2C%20but%20rectify%20inaccurate%20ones%20for%20attributed%20nodes.%20Finally%2C%20we%20conduct%0Aextensive%20experiments%20to%20show%20the%20superiority%20of%20GraMI%20in%20tackling%20HINs%20with%0Amissing%20and%20inaccurate%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Graph%2520Autoencoder%2520for%2520Heterogeneous%2520Information%2520Networks%250A%2520%2520with%2520Missing%2520and%2520Inaccurate%2520Attributes%26entry.906535625%3DYige%2520Zhao%2520and%2520Jianxiang%2520Yu%2520and%2520Yao%2520Cheng%2520and%2520Chengcheng%2520Yu%2520and%2520Yiding%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Shuaiqiang%2520Wang%26entry.1292438233%3D%2520%2520Heterogeneous%2520Information%2520Networks%2520%2528HINs%2529%252C%2520which%2520consist%2520of%2520various%2520types%2520of%250Anodes%2520and%2520edges%252C%2520have%2520recently%2520demonstrated%2520excellent%2520performance%2520in%2520graph%250Amining.%2520However%252C%2520most%2520existing%2520heterogeneous%2520graph%2520neural%2520networks%2520%2528HGNNs%2529%250Aignore%2520the%2520problems%2520of%2520missing%2520attributes%252C%2520inaccurate%2520attributes%2520and%2520scarce%250Alabels%2520for%2520nodes%252C%2520which%2520limits%2520their%2520expressiveness.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520generative%2520self-supervised%2520model%2520GraMI%2520to%2520address%2520these%2520issues%250Asimultaneously.%2520Specifically%252C%2520GraMI%2520first%2520initializes%2520all%2520the%2520nodes%2520in%2520the%250Agraph%2520with%2520a%2520low-dimensional%2520representation%2520matrix.%2520After%2520that%252C%2520based%2520on%2520the%250Avariational%2520graph%2520autoencoder%2520framework%252C%2520GraMI%2520learns%2520both%2520node-level%2520and%250Aattribute-level%2520embeddings%2520in%2520the%2520encoder%252C%2520which%2520can%2520provide%2520fine-grained%250Asemantic%2520information%2520to%2520construct%2520node%2520attributes.%2520In%2520the%2520decoder%252C%2520GraMI%250Areconstructs%2520both%2520links%2520and%2520attributes.%2520Instead%2520of%2520directly%2520reconstructing%2520raw%250Afeatures%2520for%2520attributed%2520nodes%252C%2520GraMI%2520generates%2520the%2520initial%2520low-dimensional%250Arepresentation%2520matrix%2520for%2520all%2520the%2520nodes%252C%2520based%2520on%2520which%2520raw%2520features%2520of%250Aattributed%2520nodes%2520are%2520further%2520reconstructed%2520to%2520leverage%2520accurate%2520attributes.%2520In%250Athis%2520way%252C%2520GraMI%2520can%2520not%2520only%2520complete%2520informative%2520features%2520for%2520non-attributed%250Anodes%252C%2520but%2520rectify%2520inaccurate%2520ones%2520for%2520attributed%2520nodes.%2520Finally%252C%2520we%2520conduct%250Aextensive%2520experiments%2520to%2520show%2520the%2520superiority%2520of%2520GraMI%2520in%2520tackling%2520HINs%2520with%250Amissing%2520and%2520inaccurate%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Graph%20Autoencoder%20for%20Heterogeneous%20Information%20Networks%0A%20%20with%20Missing%20and%20Inaccurate%20Attributes&entry.906535625=Yige%20Zhao%20and%20Jianxiang%20Yu%20and%20Yao%20Cheng%20and%20Chengcheng%20Yu%20and%20Yiding%20Liu%20and%20Xiang%20Li%20and%20Shuaiqiang%20Wang&entry.1292438233=%20%20Heterogeneous%20Information%20Networks%20%28HINs%29%2C%20which%20consist%20of%20various%20types%20of%0Anodes%20and%20edges%2C%20have%20recently%20demonstrated%20excellent%20performance%20in%20graph%0Amining.%20However%2C%20most%20existing%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%0Aignore%20the%20problems%20of%20missing%20attributes%2C%20inaccurate%20attributes%20and%20scarce%0Alabels%20for%20nodes%2C%20which%20limits%20their%20expressiveness.%20In%20this%20paper%2C%20we%20propose%0Aa%20generative%20self-supervised%20model%20GraMI%20to%20address%20these%20issues%0Asimultaneously.%20Specifically%2C%20GraMI%20first%20initializes%20all%20the%20nodes%20in%20the%0Agraph%20with%20a%20low-dimensional%20representation%20matrix.%20After%20that%2C%20based%20on%20the%0Avariational%20graph%20autoencoder%20framework%2C%20GraMI%20learns%20both%20node-level%20and%0Aattribute-level%20embeddings%20in%20the%20encoder%2C%20which%20can%20provide%20fine-grained%0Asemantic%20information%20to%20construct%20node%20attributes.%20In%20the%20decoder%2C%20GraMI%0Areconstructs%20both%20links%20and%20attributes.%20Instead%20of%20directly%20reconstructing%20raw%0Afeatures%20for%20attributed%20nodes%2C%20GraMI%20generates%20the%20initial%20low-dimensional%0Arepresentation%20matrix%20for%20all%20the%20nodes%2C%20based%20on%20which%20raw%20features%20of%0Aattributed%20nodes%20are%20further%20reconstructed%20to%20leverage%20accurate%20attributes.%20In%0Athis%20way%2C%20GraMI%20can%20not%20only%20complete%20informative%20features%20for%20non-attributed%0Anodes%2C%20but%20rectify%20inaccurate%20ones%20for%20attributed%20nodes.%20Finally%2C%20we%20conduct%0Aextensive%20experiments%20to%20show%20the%20superiority%20of%20GraMI%20in%20tackling%20HINs%20with%0Amissing%20and%20inaccurate%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07929v2&entry.124074799=Read"},
{"title": "Revitalizing Electoral Trust: Enhancing Transparency and Efficiency\n  through Automated Voter Counting with Machine Learning", "author": "Mir Faris and Syeda Aynul Karim and Md. Juniadul Islam", "abstract": "  In order to address issues with manual vote counting during election\nprocedures, this study intends to examine the viability of using advanced image\nprocessing techniques for automated voter counting. The study aims to shed\nlight on how automated systems that utilize cutting-edge technologies like\nOpenCV, CVZone, and the MOG2 algorithm could greatly increase the effectiveness\nand openness of electoral operations. The empirical findings demonstrate how\nautomated voter counting can enhance voting processes and rebuild public\nconfidence in election outcomes, particularly in places where trust is low. The\nstudy also emphasizes how rigorous metrics, such as the F1 score, should be\nused to systematically compare the accuracy of automated systems against manual\ncounting methods. This methodology enables a detailed comprehension of the\ndifferences in performance between automated and human counting techniques by\nproviding a nuanced assessment. The incorporation of said measures serves to\nreinforce an extensive assessment structure, guaranteeing the legitimacy and\ndependability of automated voting systems inside the electoral sphere.\n", "link": "http://arxiv.org/abs/2411.11740v1", "date": "2024-11-18", "relevancy": 2.3933, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revitalizing%20Electoral%20Trust%3A%20Enhancing%20Transparency%20and%20Efficiency%0A%20%20through%20Automated%20Voter%20Counting%20with%20Machine%20Learning&body=Title%3A%20Revitalizing%20Electoral%20Trust%3A%20Enhancing%20Transparency%20and%20Efficiency%0A%20%20through%20Automated%20Voter%20Counting%20with%20Machine%20Learning%0AAuthor%3A%20Mir%20Faris%20and%20Syeda%20Aynul%20Karim%20and%20Md.%20Juniadul%20Islam%0AAbstract%3A%20%20%20In%20order%20to%20address%20issues%20with%20manual%20vote%20counting%20during%20election%0Aprocedures%2C%20this%20study%20intends%20to%20examine%20the%20viability%20of%20using%20advanced%20image%0Aprocessing%20techniques%20for%20automated%20voter%20counting.%20The%20study%20aims%20to%20shed%0Alight%20on%20how%20automated%20systems%20that%20utilize%20cutting-edge%20technologies%20like%0AOpenCV%2C%20CVZone%2C%20and%20the%20MOG2%20algorithm%20could%20greatly%20increase%20the%20effectiveness%0Aand%20openness%20of%20electoral%20operations.%20The%20empirical%20findings%20demonstrate%20how%0Aautomated%20voter%20counting%20can%20enhance%20voting%20processes%20and%20rebuild%20public%0Aconfidence%20in%20election%20outcomes%2C%20particularly%20in%20places%20where%20trust%20is%20low.%20The%0Astudy%20also%20emphasizes%20how%20rigorous%20metrics%2C%20such%20as%20the%20F1%20score%2C%20should%20be%0Aused%20to%20systematically%20compare%20the%20accuracy%20of%20automated%20systems%20against%20manual%0Acounting%20methods.%20This%20methodology%20enables%20a%20detailed%20comprehension%20of%20the%0Adifferences%20in%20performance%20between%20automated%20and%20human%20counting%20techniques%20by%0Aproviding%20a%20nuanced%20assessment.%20The%20incorporation%20of%20said%20measures%20serves%20to%0Areinforce%20an%20extensive%20assessment%20structure%2C%20guaranteeing%20the%20legitimacy%20and%0Adependability%20of%20automated%20voting%20systems%20inside%20the%20electoral%20sphere.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevitalizing%2520Electoral%2520Trust%253A%2520Enhancing%2520Transparency%2520and%2520Efficiency%250A%2520%2520through%2520Automated%2520Voter%2520Counting%2520with%2520Machine%2520Learning%26entry.906535625%3DMir%2520Faris%2520and%2520Syeda%2520Aynul%2520Karim%2520and%2520Md.%2520Juniadul%2520Islam%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520address%2520issues%2520with%2520manual%2520vote%2520counting%2520during%2520election%250Aprocedures%252C%2520this%2520study%2520intends%2520to%2520examine%2520the%2520viability%2520of%2520using%2520advanced%2520image%250Aprocessing%2520techniques%2520for%2520automated%2520voter%2520counting.%2520The%2520study%2520aims%2520to%2520shed%250Alight%2520on%2520how%2520automated%2520systems%2520that%2520utilize%2520cutting-edge%2520technologies%2520like%250AOpenCV%252C%2520CVZone%252C%2520and%2520the%2520MOG2%2520algorithm%2520could%2520greatly%2520increase%2520the%2520effectiveness%250Aand%2520openness%2520of%2520electoral%2520operations.%2520The%2520empirical%2520findings%2520demonstrate%2520how%250Aautomated%2520voter%2520counting%2520can%2520enhance%2520voting%2520processes%2520and%2520rebuild%2520public%250Aconfidence%2520in%2520election%2520outcomes%252C%2520particularly%2520in%2520places%2520where%2520trust%2520is%2520low.%2520The%250Astudy%2520also%2520emphasizes%2520how%2520rigorous%2520metrics%252C%2520such%2520as%2520the%2520F1%2520score%252C%2520should%2520be%250Aused%2520to%2520systematically%2520compare%2520the%2520accuracy%2520of%2520automated%2520systems%2520against%2520manual%250Acounting%2520methods.%2520This%2520methodology%2520enables%2520a%2520detailed%2520comprehension%2520of%2520the%250Adifferences%2520in%2520performance%2520between%2520automated%2520and%2520human%2520counting%2520techniques%2520by%250Aproviding%2520a%2520nuanced%2520assessment.%2520The%2520incorporation%2520of%2520said%2520measures%2520serves%2520to%250Areinforce%2520an%2520extensive%2520assessment%2520structure%252C%2520guaranteeing%2520the%2520legitimacy%2520and%250Adependability%2520of%2520automated%2520voting%2520systems%2520inside%2520the%2520electoral%2520sphere.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revitalizing%20Electoral%20Trust%3A%20Enhancing%20Transparency%20and%20Efficiency%0A%20%20through%20Automated%20Voter%20Counting%20with%20Machine%20Learning&entry.906535625=Mir%20Faris%20and%20Syeda%20Aynul%20Karim%20and%20Md.%20Juniadul%20Islam&entry.1292438233=%20%20In%20order%20to%20address%20issues%20with%20manual%20vote%20counting%20during%20election%0Aprocedures%2C%20this%20study%20intends%20to%20examine%20the%20viability%20of%20using%20advanced%20image%0Aprocessing%20techniques%20for%20automated%20voter%20counting.%20The%20study%20aims%20to%20shed%0Alight%20on%20how%20automated%20systems%20that%20utilize%20cutting-edge%20technologies%20like%0AOpenCV%2C%20CVZone%2C%20and%20the%20MOG2%20algorithm%20could%20greatly%20increase%20the%20effectiveness%0Aand%20openness%20of%20electoral%20operations.%20The%20empirical%20findings%20demonstrate%20how%0Aautomated%20voter%20counting%20can%20enhance%20voting%20processes%20and%20rebuild%20public%0Aconfidence%20in%20election%20outcomes%2C%20particularly%20in%20places%20where%20trust%20is%20low.%20The%0Astudy%20also%20emphasizes%20how%20rigorous%20metrics%2C%20such%20as%20the%20F1%20score%2C%20should%20be%0Aused%20to%20systematically%20compare%20the%20accuracy%20of%20automated%20systems%20against%20manual%0Acounting%20methods.%20This%20methodology%20enables%20a%20detailed%20comprehension%20of%20the%0Adifferences%20in%20performance%20between%20automated%20and%20human%20counting%20techniques%20by%0Aproviding%20a%20nuanced%20assessment.%20The%20incorporation%20of%20said%20measures%20serves%20to%0Areinforce%20an%20extensive%20assessment%20structure%2C%20guaranteeing%20the%20legitimacy%20and%0Adependability%20of%20automated%20voting%20systems%20inside%20the%20electoral%20sphere.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11740v1&entry.124074799=Read"},
{"title": "Unpicking Data at the Seams: VAEs, Disentanglement and Independent\n  Components", "author": "Carl Allen", "abstract": "  Disentanglement, or identifying salient statistically independent factors of\nthe data, is of interest in many areas of machine learning and statistics, with\nrelevance to synthetic data generation with controlled properties, robust\nclassification of features, parsimonious encoding, and a greater understanding\nof the generative process underlying the data. Disentanglement arises in\nseveral generative paradigms, including Variational Autoencoders (VAEs),\nGenerative Adversarial Networks and diffusion models. Particular progress has\nrecently been made in understanding disentanglement in VAEs, where the choice\nof diagonal posterior covariance matrices is suggested to promote mutual\northogonality between columns of the decoder's Jacobian. We continue this\nthread to show how this linear independence translates to statistical\nindependence, completing the chain in understanding how the VAE's objective\nidentifies independent components of, or disentangles, the data.\n", "link": "http://arxiv.org/abs/2410.22559v2", "date": "2024-11-18", "relevancy": 2.3837, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5029}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unpicking%20Data%20at%20the%20Seams%3A%20VAEs%2C%20Disentanglement%20and%20Independent%0A%20%20Components&body=Title%3A%20Unpicking%20Data%20at%20the%20Seams%3A%20VAEs%2C%20Disentanglement%20and%20Independent%0A%20%20Components%0AAuthor%3A%20Carl%20Allen%0AAbstract%3A%20%20%20Disentanglement%2C%20or%20identifying%20salient%20statistically%20independent%20factors%20of%0Athe%20data%2C%20is%20of%20interest%20in%20many%20areas%20of%20machine%20learning%20and%20statistics%2C%20with%0Arelevance%20to%20synthetic%20data%20generation%20with%20controlled%20properties%2C%20robust%0Aclassification%20of%20features%2C%20parsimonious%20encoding%2C%20and%20a%20greater%20understanding%0Aof%20the%20generative%20process%20underlying%20the%20data.%20Disentanglement%20arises%20in%0Aseveral%20generative%20paradigms%2C%20including%20Variational%20Autoencoders%20%28VAEs%29%2C%0AGenerative%20Adversarial%20Networks%20and%20diffusion%20models.%20Particular%20progress%20has%0Arecently%20been%20made%20in%20understanding%20disentanglement%20in%20VAEs%2C%20where%20the%20choice%0Aof%20diagonal%20posterior%20covariance%20matrices%20is%20suggested%20to%20promote%20mutual%0Aorthogonality%20between%20columns%20of%20the%20decoder%27s%20Jacobian.%20We%20continue%20this%0Athread%20to%20show%20how%20this%20linear%20independence%20translates%20to%20statistical%0Aindependence%2C%20completing%20the%20chain%20in%20understanding%20how%20the%20VAE%27s%20objective%0Aidentifies%20independent%20components%20of%2C%20or%20disentangles%2C%20the%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22559v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnpicking%2520Data%2520at%2520the%2520Seams%253A%2520VAEs%252C%2520Disentanglement%2520and%2520Independent%250A%2520%2520Components%26entry.906535625%3DCarl%2520Allen%26entry.1292438233%3D%2520%2520Disentanglement%252C%2520or%2520identifying%2520salient%2520statistically%2520independent%2520factors%2520of%250Athe%2520data%252C%2520is%2520of%2520interest%2520in%2520many%2520areas%2520of%2520machine%2520learning%2520and%2520statistics%252C%2520with%250Arelevance%2520to%2520synthetic%2520data%2520generation%2520with%2520controlled%2520properties%252C%2520robust%250Aclassification%2520of%2520features%252C%2520parsimonious%2520encoding%252C%2520and%2520a%2520greater%2520understanding%250Aof%2520the%2520generative%2520process%2520underlying%2520the%2520data.%2520Disentanglement%2520arises%2520in%250Aseveral%2520generative%2520paradigms%252C%2520including%2520Variational%2520Autoencoders%2520%2528VAEs%2529%252C%250AGenerative%2520Adversarial%2520Networks%2520and%2520diffusion%2520models.%2520Particular%2520progress%2520has%250Arecently%2520been%2520made%2520in%2520understanding%2520disentanglement%2520in%2520VAEs%252C%2520where%2520the%2520choice%250Aof%2520diagonal%2520posterior%2520covariance%2520matrices%2520is%2520suggested%2520to%2520promote%2520mutual%250Aorthogonality%2520between%2520columns%2520of%2520the%2520decoder%2527s%2520Jacobian.%2520We%2520continue%2520this%250Athread%2520to%2520show%2520how%2520this%2520linear%2520independence%2520translates%2520to%2520statistical%250Aindependence%252C%2520completing%2520the%2520chain%2520in%2520understanding%2520how%2520the%2520VAE%2527s%2520objective%250Aidentifies%2520independent%2520components%2520of%252C%2520or%2520disentangles%252C%2520the%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22559v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpicking%20Data%20at%20the%20Seams%3A%20VAEs%2C%20Disentanglement%20and%20Independent%0A%20%20Components&entry.906535625=Carl%20Allen&entry.1292438233=%20%20Disentanglement%2C%20or%20identifying%20salient%20statistically%20independent%20factors%20of%0Athe%20data%2C%20is%20of%20interest%20in%20many%20areas%20of%20machine%20learning%20and%20statistics%2C%20with%0Arelevance%20to%20synthetic%20data%20generation%20with%20controlled%20properties%2C%20robust%0Aclassification%20of%20features%2C%20parsimonious%20encoding%2C%20and%20a%20greater%20understanding%0Aof%20the%20generative%20process%20underlying%20the%20data.%20Disentanglement%20arises%20in%0Aseveral%20generative%20paradigms%2C%20including%20Variational%20Autoencoders%20%28VAEs%29%2C%0AGenerative%20Adversarial%20Networks%20and%20diffusion%20models.%20Particular%20progress%20has%0Arecently%20been%20made%20in%20understanding%20disentanglement%20in%20VAEs%2C%20where%20the%20choice%0Aof%20diagonal%20posterior%20covariance%20matrices%20is%20suggested%20to%20promote%20mutual%0Aorthogonality%20between%20columns%20of%20the%20decoder%27s%20Jacobian.%20We%20continue%20this%0Athread%20to%20show%20how%20this%20linear%20independence%20translates%20to%20statistical%0Aindependence%2C%20completing%20the%20chain%20in%20understanding%20how%20the%20VAE%27s%20objective%0Aidentifies%20independent%20components%20of%2C%20or%20disentangles%2C%20the%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22559v2&entry.124074799=Read"},
{"title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset", "author": "Jiazhen Liu and Yuhan Fu and Ruobing Xie and Runquan Xie and Xingwu Sun and Fengzong Lian and Zhanhui Kang and Xirong Li", "abstract": "  Multimodal Large Language Models (MLLMs) hallucinate, resulting in an\nemerging topic of visual hallucination evaluation (VHE). This paper contributes\na ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective\nVHE at a large scale. The essence of VHE is to ask an MLLM questions about\nspecific images to assess its susceptibility to hallucination. Depending on\nwhat to ask (objects, attributes, sentiment, etc.) and how the questions are\nasked, we structure PhD along two dimensions, i.e., task and mode. Five visual\nrecognition tasks, ranging from low-level (object / attribute recognition) to\nmiddle-level (sentiment / position recognition and counting), are considered.\nBesides a normal visual QA mode, which we term PhD-base, PhD also asks\nquestions with inaccurate context (PhD-iac) or with incorrect context\n(PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We\nconstruct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four\npivotal modules: task-specific hallucinatory item (hitem) selection,\nhitem-embedded question generation, inaccurate / incorrect context generation,\nand counter-common-sense (CCS) image generation. With over 14k daily images,\n750 CCS images and 102k VQA triplets in total, PhD reveals considerable\nvariability in MLLMs' performance across various modes and tasks, offering\nvaluable insights into the nature of hallucination. As such, PhD stands as a\npotent tool not only for VHE but may also play a significant role in the\nrefinement of MLLMs.\n", "link": "http://arxiv.org/abs/2403.11116v3", "date": "2024-11-18", "relevancy": 2.3742, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhD%3A%20A%20ChatGPT-Prompted%20Visual%20hallucination%20Evaluation%20Dataset&body=Title%3A%20PhD%3A%20A%20ChatGPT-Prompted%20Visual%20hallucination%20Evaluation%20Dataset%0AAuthor%3A%20Jiazhen%20Liu%20and%20Yuhan%20Fu%20and%20Ruobing%20Xie%20and%20Runquan%20Xie%20and%20Xingwu%20Sun%20and%20Fengzong%20Lian%20and%20Zhanhui%20Kang%20and%20Xirong%20Li%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hallucinate%2C%20resulting%20in%20an%0Aemerging%20topic%20of%20visual%20hallucination%20evaluation%20%28VHE%29.%20This%20paper%20contributes%0Aa%20ChatGPT-Prompted%20visual%20hallucination%20evaluation%20Dataset%20%28PhD%29%20for%20objective%0AVHE%20at%20a%20large%20scale.%20The%20essence%20of%20VHE%20is%20to%20ask%20an%20MLLM%20questions%20about%0Aspecific%20images%20to%20assess%20its%20susceptibility%20to%20hallucination.%20Depending%20on%0Awhat%20to%20ask%20%28objects%2C%20attributes%2C%20sentiment%2C%20etc.%29%20and%20how%20the%20questions%20are%0Aasked%2C%20we%20structure%20PhD%20along%20two%20dimensions%2C%20i.e.%2C%20task%20and%20mode.%20Five%20visual%0Arecognition%20tasks%2C%20ranging%20from%20low-level%20%28object%20/%20attribute%20recognition%29%20to%0Amiddle-level%20%28sentiment%20/%20position%20recognition%20and%20counting%29%2C%20are%20considered.%0ABesides%20a%20normal%20visual%20QA%20mode%2C%20which%20we%20term%20PhD-base%2C%20PhD%20also%20asks%0Aquestions%20with%20inaccurate%20context%20%28PhD-iac%29%20or%20with%20incorrect%20context%0A%28PhD-icc%29%2C%20or%20with%20AI-generated%20counter%20common%20sense%20images%20%28PhD-ccs%29.%20We%0Aconstruct%20PhD%20by%20a%20ChatGPT-assisted%20semi-automated%20pipeline%2C%20encompassing%20four%0Apivotal%20modules%3A%20task-specific%20hallucinatory%20item%20%28hitem%29%20selection%2C%0Ahitem-embedded%20question%20generation%2C%20inaccurate%20/%20incorrect%20context%20generation%2C%0Aand%20counter-common-sense%20%28CCS%29%20image%20generation.%20With%20over%2014k%20daily%20images%2C%0A750%20CCS%20images%20and%20102k%20VQA%20triplets%20in%20total%2C%20PhD%20reveals%20considerable%0Avariability%20in%20MLLMs%27%20performance%20across%20various%20modes%20and%20tasks%2C%20offering%0Avaluable%20insights%20into%20the%20nature%20of%20hallucination.%20As%20such%2C%20PhD%20stands%20as%20a%0Apotent%20tool%20not%20only%20for%20VHE%20but%20may%20also%20play%20a%20significant%20role%20in%20the%0Arefinement%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11116v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhD%253A%2520A%2520ChatGPT-Prompted%2520Visual%2520hallucination%2520Evaluation%2520Dataset%26entry.906535625%3DJiazhen%2520Liu%2520and%2520Yuhan%2520Fu%2520and%2520Ruobing%2520Xie%2520and%2520Runquan%2520Xie%2520and%2520Xingwu%2520Sun%2520and%2520Fengzong%2520Lian%2520and%2520Zhanhui%2520Kang%2520and%2520Xirong%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520hallucinate%252C%2520resulting%2520in%2520an%250Aemerging%2520topic%2520of%2520visual%2520hallucination%2520evaluation%2520%2528VHE%2529.%2520This%2520paper%2520contributes%250Aa%2520ChatGPT-Prompted%2520visual%2520hallucination%2520evaluation%2520Dataset%2520%2528PhD%2529%2520for%2520objective%250AVHE%2520at%2520a%2520large%2520scale.%2520The%2520essence%2520of%2520VHE%2520is%2520to%2520ask%2520an%2520MLLM%2520questions%2520about%250Aspecific%2520images%2520to%2520assess%2520its%2520susceptibility%2520to%2520hallucination.%2520Depending%2520on%250Awhat%2520to%2520ask%2520%2528objects%252C%2520attributes%252C%2520sentiment%252C%2520etc.%2529%2520and%2520how%2520the%2520questions%2520are%250Aasked%252C%2520we%2520structure%2520PhD%2520along%2520two%2520dimensions%252C%2520i.e.%252C%2520task%2520and%2520mode.%2520Five%2520visual%250Arecognition%2520tasks%252C%2520ranging%2520from%2520low-level%2520%2528object%2520/%2520attribute%2520recognition%2529%2520to%250Amiddle-level%2520%2528sentiment%2520/%2520position%2520recognition%2520and%2520counting%2529%252C%2520are%2520considered.%250ABesides%2520a%2520normal%2520visual%2520QA%2520mode%252C%2520which%2520we%2520term%2520PhD-base%252C%2520PhD%2520also%2520asks%250Aquestions%2520with%2520inaccurate%2520context%2520%2528PhD-iac%2529%2520or%2520with%2520incorrect%2520context%250A%2528PhD-icc%2529%252C%2520or%2520with%2520AI-generated%2520counter%2520common%2520sense%2520images%2520%2528PhD-ccs%2529.%2520We%250Aconstruct%2520PhD%2520by%2520a%2520ChatGPT-assisted%2520semi-automated%2520pipeline%252C%2520encompassing%2520four%250Apivotal%2520modules%253A%2520task-specific%2520hallucinatory%2520item%2520%2528hitem%2529%2520selection%252C%250Ahitem-embedded%2520question%2520generation%252C%2520inaccurate%2520/%2520incorrect%2520context%2520generation%252C%250Aand%2520counter-common-sense%2520%2528CCS%2529%2520image%2520generation.%2520With%2520over%252014k%2520daily%2520images%252C%250A750%2520CCS%2520images%2520and%2520102k%2520VQA%2520triplets%2520in%2520total%252C%2520PhD%2520reveals%2520considerable%250Avariability%2520in%2520MLLMs%2527%2520performance%2520across%2520various%2520modes%2520and%2520tasks%252C%2520offering%250Avaluable%2520insights%2520into%2520the%2520nature%2520of%2520hallucination.%2520As%2520such%252C%2520PhD%2520stands%2520as%2520a%250Apotent%2520tool%2520not%2520only%2520for%2520VHE%2520but%2520may%2520also%2520play%2520a%2520significant%2520role%2520in%2520the%250Arefinement%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11116v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhD%3A%20A%20ChatGPT-Prompted%20Visual%20hallucination%20Evaluation%20Dataset&entry.906535625=Jiazhen%20Liu%20and%20Yuhan%20Fu%20and%20Ruobing%20Xie%20and%20Runquan%20Xie%20and%20Xingwu%20Sun%20and%20Fengzong%20Lian%20and%20Zhanhui%20Kang%20and%20Xirong%20Li&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hallucinate%2C%20resulting%20in%20an%0Aemerging%20topic%20of%20visual%20hallucination%20evaluation%20%28VHE%29.%20This%20paper%20contributes%0Aa%20ChatGPT-Prompted%20visual%20hallucination%20evaluation%20Dataset%20%28PhD%29%20for%20objective%0AVHE%20at%20a%20large%20scale.%20The%20essence%20of%20VHE%20is%20to%20ask%20an%20MLLM%20questions%20about%0Aspecific%20images%20to%20assess%20its%20susceptibility%20to%20hallucination.%20Depending%20on%0Awhat%20to%20ask%20%28objects%2C%20attributes%2C%20sentiment%2C%20etc.%29%20and%20how%20the%20questions%20are%0Aasked%2C%20we%20structure%20PhD%20along%20two%20dimensions%2C%20i.e.%2C%20task%20and%20mode.%20Five%20visual%0Arecognition%20tasks%2C%20ranging%20from%20low-level%20%28object%20/%20attribute%20recognition%29%20to%0Amiddle-level%20%28sentiment%20/%20position%20recognition%20and%20counting%29%2C%20are%20considered.%0ABesides%20a%20normal%20visual%20QA%20mode%2C%20which%20we%20term%20PhD-base%2C%20PhD%20also%20asks%0Aquestions%20with%20inaccurate%20context%20%28PhD-iac%29%20or%20with%20incorrect%20context%0A%28PhD-icc%29%2C%20or%20with%20AI-generated%20counter%20common%20sense%20images%20%28PhD-ccs%29.%20We%0Aconstruct%20PhD%20by%20a%20ChatGPT-assisted%20semi-automated%20pipeline%2C%20encompassing%20four%0Apivotal%20modules%3A%20task-specific%20hallucinatory%20item%20%28hitem%29%20selection%2C%0Ahitem-embedded%20question%20generation%2C%20inaccurate%20/%20incorrect%20context%20generation%2C%0Aand%20counter-common-sense%20%28CCS%29%20image%20generation.%20With%20over%2014k%20daily%20images%2C%0A750%20CCS%20images%20and%20102k%20VQA%20triplets%20in%20total%2C%20PhD%20reveals%20considerable%0Avariability%20in%20MLLMs%27%20performance%20across%20various%20modes%20and%20tasks%2C%20offering%0Avaluable%20insights%20into%20the%20nature%20of%20hallucination.%20As%20such%2C%20PhD%20stands%20as%20a%0Apotent%20tool%20not%20only%20for%20VHE%20but%20may%20also%20play%20a%20significant%20role%20in%20the%0Arefinement%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11116v3&entry.124074799=Read"},
{"title": "Reliable Poisoned Sample Detection against Backdoor Attacks Enhanced by\n  Sharpness Aware Minimization", "author": "Mingda Zhang and Mingli Zhu and Zihao Zhu and Baoyuan Wu", "abstract": "  Backdoor attack has been considered as a serious security threat to deep\nneural networks (DNNs). Poisoned sample detection (PSD) that aims at filtering\nout poisoned samples from an untrustworthy training dataset has shown very\npromising performance for defending against data poisoning based backdoor\nattacks. However, we observe that the detection performance of many advanced\nmethods is likely to be unstable when facing weak backdoor attacks, such as low\npoisoning ratio or weak trigger strength. To further verify this observation,\nwe make a statistical investigation among various backdoor attacks and poisoned\nsample detections, showing a positive correlation between backdoor effect and\ndetection performance. It inspires us to strengthen the backdoor effect to\nenhance detection performance. Since we cannot achieve that goal via directly\nmanipulating poisoning ratio or trigger strength, we propose to train one model\nusing the Sharpness-Aware Minimization (SAM) algorithm, rather than the vanilla\ntraining algorithm. We also provide both empirical and theoretical analysis\nabout how SAM training strengthens the backdoor effect. Then, this SAM trained\nmodel can be seamlessly integrated with any off-the-shelf PSD method that\nextracts discriminative features from the trained model for detection, called\nSAM-enhanced PSD. Extensive experiments on several benchmark datasets show the\nreliable detection performance of the proposed method against both weak and\nstrong backdoor attacks, with significant improvements against various attacks\n($+34.38\\%$ TPR on average), over the conventional PSD methods (i.e., without\nSAM enhancement). Overall, this work provides new insights about PSD and\nproposes a novel approach that can complement existing detection methods, which\nmay inspire more in-depth explorations in this field.\n", "link": "http://arxiv.org/abs/2411.11525v1", "date": "2024-11-18", "relevancy": 2.3671, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.47}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Poisoned%20Sample%20Detection%20against%20Backdoor%20Attacks%20Enhanced%20by%0A%20%20Sharpness%20Aware%20Minimization&body=Title%3A%20Reliable%20Poisoned%20Sample%20Detection%20against%20Backdoor%20Attacks%20Enhanced%20by%0A%20%20Sharpness%20Aware%20Minimization%0AAuthor%3A%20Mingda%20Zhang%20and%20Mingli%20Zhu%20and%20Zihao%20Zhu%20and%20Baoyuan%20Wu%0AAbstract%3A%20%20%20Backdoor%20attack%20has%20been%20considered%20as%20a%20serious%20security%20threat%20to%20deep%0Aneural%20networks%20%28DNNs%29.%20Poisoned%20sample%20detection%20%28PSD%29%20that%20aims%20at%20filtering%0Aout%20poisoned%20samples%20from%20an%20untrustworthy%20training%20dataset%20has%20shown%20very%0Apromising%20performance%20for%20defending%20against%20data%20poisoning%20based%20backdoor%0Aattacks.%20However%2C%20we%20observe%20that%20the%20detection%20performance%20of%20many%20advanced%0Amethods%20is%20likely%20to%20be%20unstable%20when%20facing%20weak%20backdoor%20attacks%2C%20such%20as%20low%0Apoisoning%20ratio%20or%20weak%20trigger%20strength.%20To%20further%20verify%20this%20observation%2C%0Awe%20make%20a%20statistical%20investigation%20among%20various%20backdoor%20attacks%20and%20poisoned%0Asample%20detections%2C%20showing%20a%20positive%20correlation%20between%20backdoor%20effect%20and%0Adetection%20performance.%20It%20inspires%20us%20to%20strengthen%20the%20backdoor%20effect%20to%0Aenhance%20detection%20performance.%20Since%20we%20cannot%20achieve%20that%20goal%20via%20directly%0Amanipulating%20poisoning%20ratio%20or%20trigger%20strength%2C%20we%20propose%20to%20train%20one%20model%0Ausing%20the%20Sharpness-Aware%20Minimization%20%28SAM%29%20algorithm%2C%20rather%20than%20the%20vanilla%0Atraining%20algorithm.%20We%20also%20provide%20both%20empirical%20and%20theoretical%20analysis%0Aabout%20how%20SAM%20training%20strengthens%20the%20backdoor%20effect.%20Then%2C%20this%20SAM%20trained%0Amodel%20can%20be%20seamlessly%20integrated%20with%20any%20off-the-shelf%20PSD%20method%20that%0Aextracts%20discriminative%20features%20from%20the%20trained%20model%20for%20detection%2C%20called%0ASAM-enhanced%20PSD.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20show%20the%0Areliable%20detection%20performance%20of%20the%20proposed%20method%20against%20both%20weak%20and%0Astrong%20backdoor%20attacks%2C%20with%20significant%20improvements%20against%20various%20attacks%0A%28%24%2B34.38%5C%25%24%20TPR%20on%20average%29%2C%20over%20the%20conventional%20PSD%20methods%20%28i.e.%2C%20without%0ASAM%20enhancement%29.%20Overall%2C%20this%20work%20provides%20new%20insights%20about%20PSD%20and%0Aproposes%20a%20novel%20approach%20that%20can%20complement%20existing%20detection%20methods%2C%20which%0Amay%20inspire%20more%20in-depth%20explorations%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Poisoned%2520Sample%2520Detection%2520against%2520Backdoor%2520Attacks%2520Enhanced%2520by%250A%2520%2520Sharpness%2520Aware%2520Minimization%26entry.906535625%3DMingda%2520Zhang%2520and%2520Mingli%2520Zhu%2520and%2520Zihao%2520Zhu%2520and%2520Baoyuan%2520Wu%26entry.1292438233%3D%2520%2520Backdoor%2520attack%2520has%2520been%2520considered%2520as%2520a%2520serious%2520security%2520threat%2520to%2520deep%250Aneural%2520networks%2520%2528DNNs%2529.%2520Poisoned%2520sample%2520detection%2520%2528PSD%2529%2520that%2520aims%2520at%2520filtering%250Aout%2520poisoned%2520samples%2520from%2520an%2520untrustworthy%2520training%2520dataset%2520has%2520shown%2520very%250Apromising%2520performance%2520for%2520defending%2520against%2520data%2520poisoning%2520based%2520backdoor%250Aattacks.%2520However%252C%2520we%2520observe%2520that%2520the%2520detection%2520performance%2520of%2520many%2520advanced%250Amethods%2520is%2520likely%2520to%2520be%2520unstable%2520when%2520facing%2520weak%2520backdoor%2520attacks%252C%2520such%2520as%2520low%250Apoisoning%2520ratio%2520or%2520weak%2520trigger%2520strength.%2520To%2520further%2520verify%2520this%2520observation%252C%250Awe%2520make%2520a%2520statistical%2520investigation%2520among%2520various%2520backdoor%2520attacks%2520and%2520poisoned%250Asample%2520detections%252C%2520showing%2520a%2520positive%2520correlation%2520between%2520backdoor%2520effect%2520and%250Adetection%2520performance.%2520It%2520inspires%2520us%2520to%2520strengthen%2520the%2520backdoor%2520effect%2520to%250Aenhance%2520detection%2520performance.%2520Since%2520we%2520cannot%2520achieve%2520that%2520goal%2520via%2520directly%250Amanipulating%2520poisoning%2520ratio%2520or%2520trigger%2520strength%252C%2520we%2520propose%2520to%2520train%2520one%2520model%250Ausing%2520the%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%2520algorithm%252C%2520rather%2520than%2520the%2520vanilla%250Atraining%2520algorithm.%2520We%2520also%2520provide%2520both%2520empirical%2520and%2520theoretical%2520analysis%250Aabout%2520how%2520SAM%2520training%2520strengthens%2520the%2520backdoor%2520effect.%2520Then%252C%2520this%2520SAM%2520trained%250Amodel%2520can%2520be%2520seamlessly%2520integrated%2520with%2520any%2520off-the-shelf%2520PSD%2520method%2520that%250Aextracts%2520discriminative%2520features%2520from%2520the%2520trained%2520model%2520for%2520detection%252C%2520called%250ASAM-enhanced%2520PSD.%2520Extensive%2520experiments%2520on%2520several%2520benchmark%2520datasets%2520show%2520the%250Areliable%2520detection%2520performance%2520of%2520the%2520proposed%2520method%2520against%2520both%2520weak%2520and%250Astrong%2520backdoor%2520attacks%252C%2520with%2520significant%2520improvements%2520against%2520various%2520attacks%250A%2528%2524%252B34.38%255C%2525%2524%2520TPR%2520on%2520average%2529%252C%2520over%2520the%2520conventional%2520PSD%2520methods%2520%2528i.e.%252C%2520without%250ASAM%2520enhancement%2529.%2520Overall%252C%2520this%2520work%2520provides%2520new%2520insights%2520about%2520PSD%2520and%250Aproposes%2520a%2520novel%2520approach%2520that%2520can%2520complement%2520existing%2520detection%2520methods%252C%2520which%250Amay%2520inspire%2520more%2520in-depth%2520explorations%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Poisoned%20Sample%20Detection%20against%20Backdoor%20Attacks%20Enhanced%20by%0A%20%20Sharpness%20Aware%20Minimization&entry.906535625=Mingda%20Zhang%20and%20Mingli%20Zhu%20and%20Zihao%20Zhu%20and%20Baoyuan%20Wu&entry.1292438233=%20%20Backdoor%20attack%20has%20been%20considered%20as%20a%20serious%20security%20threat%20to%20deep%0Aneural%20networks%20%28DNNs%29.%20Poisoned%20sample%20detection%20%28PSD%29%20that%20aims%20at%20filtering%0Aout%20poisoned%20samples%20from%20an%20untrustworthy%20training%20dataset%20has%20shown%20very%0Apromising%20performance%20for%20defending%20against%20data%20poisoning%20based%20backdoor%0Aattacks.%20However%2C%20we%20observe%20that%20the%20detection%20performance%20of%20many%20advanced%0Amethods%20is%20likely%20to%20be%20unstable%20when%20facing%20weak%20backdoor%20attacks%2C%20such%20as%20low%0Apoisoning%20ratio%20or%20weak%20trigger%20strength.%20To%20further%20verify%20this%20observation%2C%0Awe%20make%20a%20statistical%20investigation%20among%20various%20backdoor%20attacks%20and%20poisoned%0Asample%20detections%2C%20showing%20a%20positive%20correlation%20between%20backdoor%20effect%20and%0Adetection%20performance.%20It%20inspires%20us%20to%20strengthen%20the%20backdoor%20effect%20to%0Aenhance%20detection%20performance.%20Since%20we%20cannot%20achieve%20that%20goal%20via%20directly%0Amanipulating%20poisoning%20ratio%20or%20trigger%20strength%2C%20we%20propose%20to%20train%20one%20model%0Ausing%20the%20Sharpness-Aware%20Minimization%20%28SAM%29%20algorithm%2C%20rather%20than%20the%20vanilla%0Atraining%20algorithm.%20We%20also%20provide%20both%20empirical%20and%20theoretical%20analysis%0Aabout%20how%20SAM%20training%20strengthens%20the%20backdoor%20effect.%20Then%2C%20this%20SAM%20trained%0Amodel%20can%20be%20seamlessly%20integrated%20with%20any%20off-the-shelf%20PSD%20method%20that%0Aextracts%20discriminative%20features%20from%20the%20trained%20model%20for%20detection%2C%20called%0ASAM-enhanced%20PSD.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20show%20the%0Areliable%20detection%20performance%20of%20the%20proposed%20method%20against%20both%20weak%20and%0Astrong%20backdoor%20attacks%2C%20with%20significant%20improvements%20against%20various%20attacks%0A%28%24%2B34.38%5C%25%24%20TPR%20on%20average%29%2C%20over%20the%20conventional%20PSD%20methods%20%28i.e.%2C%20without%0ASAM%20enhancement%29.%20Overall%2C%20this%20work%20provides%20new%20insights%20about%20PSD%20and%0Aproposes%20a%20novel%20approach%20that%20can%20complement%20existing%20detection%20methods%2C%20which%0Amay%20inspire%20more%20in-depth%20explorations%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11525v1&entry.124074799=Read"},
{"title": "GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced\n  Aesthetic Text Glyph Layouts", "author": "Junwen He and Yifan Wang and Lijun Wang and Huchuan Lu and Jun-Yan He and Chenyang Li and Hanyuan Chen and Jin-Peng Lan and Bin Luo and Yifeng Geng", "abstract": "  Text logo design heavily relies on the creativity and expertise of\nprofessional designers, in which arranging element layouts is one of the most\nimportant procedures. However, few attention has been paid to this specific\ntask which needs to take precise textural details and user constraints into\nconsideration, but only on the broader tasks such as document/poster layout\ngeneration. In this paper, we propose a VLM-based framework that generates\ncontent-aware text logo layouts by integrating multi-modal inputs with user\nconstraints, supporting a more flexible and stable layout design in real-world\napplications. We introduce two model techniques to reduce the computation for\nprocessing multiple glyph images simultaneously, while does not face\nperformance degradation. To support instruction-tuning of out model, we\nconstruct two extensive text logo datasets, which are 5x more larger than the\nexisting public dataset. Except for the geometric annotations (e.g. text masks\nand character recognition), we also compliment with comprehensive layout\ndescriptions in natural language format, for more effective training to have\nreasoning ability when dealing with complex layouts and custom user\nconstraints. Experimental studies demonstrate the effectiveness of our proposed\nmodel and datasets, when comparing with previous methods in various benchmarks\nto evaluate geometric aesthetics and human preferences. The code and datasets\nwill be publicly available.\n", "link": "http://arxiv.org/abs/2411.11435v1", "date": "2024-11-18", "relevancy": 2.3202, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLDesigner%3A%20Leveraging%20Multi-Modal%20LLMs%20as%20Designer%20for%20Enhanced%0A%20%20Aesthetic%20Text%20Glyph%20Layouts&body=Title%3A%20GLDesigner%3A%20Leveraging%20Multi-Modal%20LLMs%20as%20Designer%20for%20Enhanced%0A%20%20Aesthetic%20Text%20Glyph%20Layouts%0AAuthor%3A%20Junwen%20He%20and%20Yifan%20Wang%20and%20Lijun%20Wang%20and%20Huchuan%20Lu%20and%20Jun-Yan%20He%20and%20Chenyang%20Li%20and%20Hanyuan%20Chen%20and%20Jin-Peng%20Lan%20and%20Bin%20Luo%20and%20Yifeng%20Geng%0AAbstract%3A%20%20%20Text%20logo%20design%20heavily%20relies%20on%20the%20creativity%20and%20expertise%20of%0Aprofessional%20designers%2C%20in%20which%20arranging%20element%20layouts%20is%20one%20of%20the%20most%0Aimportant%20procedures.%20However%2C%20few%20attention%20has%20been%20paid%20to%20this%20specific%0Atask%20which%20needs%20to%20take%20precise%20textural%20details%20and%20user%20constraints%20into%0Aconsideration%2C%20but%20only%20on%20the%20broader%20tasks%20such%20as%20document/poster%20layout%0Ageneration.%20In%20this%20paper%2C%20we%20propose%20a%20VLM-based%20framework%20that%20generates%0Acontent-aware%20text%20logo%20layouts%20by%20integrating%20multi-modal%20inputs%20with%20user%0Aconstraints%2C%20supporting%20a%20more%20flexible%20and%20stable%20layout%20design%20in%20real-world%0Aapplications.%20We%20introduce%20two%20model%20techniques%20to%20reduce%20the%20computation%20for%0Aprocessing%20multiple%20glyph%20images%20simultaneously%2C%20while%20does%20not%20face%0Aperformance%20degradation.%20To%20support%20instruction-tuning%20of%20out%20model%2C%20we%0Aconstruct%20two%20extensive%20text%20logo%20datasets%2C%20which%20are%205x%20more%20larger%20than%20the%0Aexisting%20public%20dataset.%20Except%20for%20the%20geometric%20annotations%20%28e.g.%20text%20masks%0Aand%20character%20recognition%29%2C%20we%20also%20compliment%20with%20comprehensive%20layout%0Adescriptions%20in%20natural%20language%20format%2C%20for%20more%20effective%20training%20to%20have%0Areasoning%20ability%20when%20dealing%20with%20complex%20layouts%20and%20custom%20user%0Aconstraints.%20Experimental%20studies%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amodel%20and%20datasets%2C%20when%20comparing%20with%20previous%20methods%20in%20various%20benchmarks%0Ato%20evaluate%20geometric%20aesthetics%20and%20human%20preferences.%20The%20code%20and%20datasets%0Awill%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLDesigner%253A%2520Leveraging%2520Multi-Modal%2520LLMs%2520as%2520Designer%2520for%2520Enhanced%250A%2520%2520Aesthetic%2520Text%2520Glyph%2520Layouts%26entry.906535625%3DJunwen%2520He%2520and%2520Yifan%2520Wang%2520and%2520Lijun%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520Jun-Yan%2520He%2520and%2520Chenyang%2520Li%2520and%2520Hanyuan%2520Chen%2520and%2520Jin-Peng%2520Lan%2520and%2520Bin%2520Luo%2520and%2520Yifeng%2520Geng%26entry.1292438233%3D%2520%2520Text%2520logo%2520design%2520heavily%2520relies%2520on%2520the%2520creativity%2520and%2520expertise%2520of%250Aprofessional%2520designers%252C%2520in%2520which%2520arranging%2520element%2520layouts%2520is%2520one%2520of%2520the%2520most%250Aimportant%2520procedures.%2520However%252C%2520few%2520attention%2520has%2520been%2520paid%2520to%2520this%2520specific%250Atask%2520which%2520needs%2520to%2520take%2520precise%2520textural%2520details%2520and%2520user%2520constraints%2520into%250Aconsideration%252C%2520but%2520only%2520on%2520the%2520broader%2520tasks%2520such%2520as%2520document/poster%2520layout%250Ageneration.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520VLM-based%2520framework%2520that%2520generates%250Acontent-aware%2520text%2520logo%2520layouts%2520by%2520integrating%2520multi-modal%2520inputs%2520with%2520user%250Aconstraints%252C%2520supporting%2520a%2520more%2520flexible%2520and%2520stable%2520layout%2520design%2520in%2520real-world%250Aapplications.%2520We%2520introduce%2520two%2520model%2520techniques%2520to%2520reduce%2520the%2520computation%2520for%250Aprocessing%2520multiple%2520glyph%2520images%2520simultaneously%252C%2520while%2520does%2520not%2520face%250Aperformance%2520degradation.%2520To%2520support%2520instruction-tuning%2520of%2520out%2520model%252C%2520we%250Aconstruct%2520two%2520extensive%2520text%2520logo%2520datasets%252C%2520which%2520are%25205x%2520more%2520larger%2520than%2520the%250Aexisting%2520public%2520dataset.%2520Except%2520for%2520the%2520geometric%2520annotations%2520%2528e.g.%2520text%2520masks%250Aand%2520character%2520recognition%2529%252C%2520we%2520also%2520compliment%2520with%2520comprehensive%2520layout%250Adescriptions%2520in%2520natural%2520language%2520format%252C%2520for%2520more%2520effective%2520training%2520to%2520have%250Areasoning%2520ability%2520when%2520dealing%2520with%2520complex%2520layouts%2520and%2520custom%2520user%250Aconstraints.%2520Experimental%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amodel%2520and%2520datasets%252C%2520when%2520comparing%2520with%2520previous%2520methods%2520in%2520various%2520benchmarks%250Ato%2520evaluate%2520geometric%2520aesthetics%2520and%2520human%2520preferences.%2520The%2520code%2520and%2520datasets%250Awill%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLDesigner%3A%20Leveraging%20Multi-Modal%20LLMs%20as%20Designer%20for%20Enhanced%0A%20%20Aesthetic%20Text%20Glyph%20Layouts&entry.906535625=Junwen%20He%20and%20Yifan%20Wang%20and%20Lijun%20Wang%20and%20Huchuan%20Lu%20and%20Jun-Yan%20He%20and%20Chenyang%20Li%20and%20Hanyuan%20Chen%20and%20Jin-Peng%20Lan%20and%20Bin%20Luo%20and%20Yifeng%20Geng&entry.1292438233=%20%20Text%20logo%20design%20heavily%20relies%20on%20the%20creativity%20and%20expertise%20of%0Aprofessional%20designers%2C%20in%20which%20arranging%20element%20layouts%20is%20one%20of%20the%20most%0Aimportant%20procedures.%20However%2C%20few%20attention%20has%20been%20paid%20to%20this%20specific%0Atask%20which%20needs%20to%20take%20precise%20textural%20details%20and%20user%20constraints%20into%0Aconsideration%2C%20but%20only%20on%20the%20broader%20tasks%20such%20as%20document/poster%20layout%0Ageneration.%20In%20this%20paper%2C%20we%20propose%20a%20VLM-based%20framework%20that%20generates%0Acontent-aware%20text%20logo%20layouts%20by%20integrating%20multi-modal%20inputs%20with%20user%0Aconstraints%2C%20supporting%20a%20more%20flexible%20and%20stable%20layout%20design%20in%20real-world%0Aapplications.%20We%20introduce%20two%20model%20techniques%20to%20reduce%20the%20computation%20for%0Aprocessing%20multiple%20glyph%20images%20simultaneously%2C%20while%20does%20not%20face%0Aperformance%20degradation.%20To%20support%20instruction-tuning%20of%20out%20model%2C%20we%0Aconstruct%20two%20extensive%20text%20logo%20datasets%2C%20which%20are%205x%20more%20larger%20than%20the%0Aexisting%20public%20dataset.%20Except%20for%20the%20geometric%20annotations%20%28e.g.%20text%20masks%0Aand%20character%20recognition%29%2C%20we%20also%20compliment%20with%20comprehensive%20layout%0Adescriptions%20in%20natural%20language%20format%2C%20for%20more%20effective%20training%20to%20have%0Areasoning%20ability%20when%20dealing%20with%20complex%20layouts%20and%20custom%20user%0Aconstraints.%20Experimental%20studies%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amodel%20and%20datasets%2C%20when%20comparing%20with%20previous%20methods%20in%20various%20benchmarks%0Ato%20evaluate%20geometric%20aesthetics%20and%20human%20preferences.%20The%20code%20and%20datasets%0Awill%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11435v1&entry.124074799=Read"},
{"title": "Efficient Sample-optimal Learning of Gaussian Tree Models via\n  Sample-optimal Testing of Gaussian Mutual Information", "author": "Sutanu Gayen and Sanket Kale and Sayantan Sen", "abstract": "  Learning high-dimensional distributions is a significant challenge in machine\nlearning and statistics. Classical research has mostly concentrated on\nasymptotic analysis of such data under suitable assumptions. While existing\nworks [Bhattacharyya et al.: SICOMP 2023, Daskalakis et al.: STOC 2021, Choo et\nal.: ALT 2024] focus on discrete distributions, the current work addresses the\ntree structure learning problem for Gaussian distributions, providing efficient\nalgorithms with solid theoretical guarantees. This is crucial as real-world\ndistributions are often continuous and differ from the discrete scenarios\nstudied in prior works.\n  In this work, we design a conditional mutual information tester for Gaussian\nrandom variables that can test whether two Gaussian random variables are\nindependent, or their conditional mutual information is at least $\\varepsilon$,\nfor some parameter $\\varepsilon \\in (0,1)$ using\n$\\mathcal{O}(\\varepsilon^{-1})$ samples which we show to be near-optimal. In\ncontrast, an additive estimation would require $\\Omega(\\varepsilon^{-2})$\nsamples. Our upper bound technique uses linear regression on a pair of suitably\ntransformed random variables. Importantly, we show that the chain rule of\nconditional mutual information continues to hold for the estimated\n(conditional) mutual information. As an application of such a mutual\ninformation tester, we give an efficient $\\varepsilon$-approximate\nstructure-learning algorithm for an $n$-variate Gaussian tree model that takes\n$\\widetilde{\\Theta}(n\\varepsilon^{-1})$ samples which we again show to be\nnear-optimal. In contrast, when the underlying Gaussian model is not known to\nbe tree-structured, we show that $\\widetilde{{{\\Theta}}}(n^2\\varepsilon^{-2})$\nsamples are necessary and sufficient to output an $\\varepsilon$-approximate\ntree structure. We perform extensive experiments that corroborate our\ntheoretical convergence bounds.\n", "link": "http://arxiv.org/abs/2411.11516v1", "date": "2024-11-18", "relevancy": 2.2899, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4669}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4562}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Sample-optimal%20Learning%20of%20Gaussian%20Tree%20Models%20via%0A%20%20Sample-optimal%20Testing%20of%20Gaussian%20Mutual%20Information&body=Title%3A%20Efficient%20Sample-optimal%20Learning%20of%20Gaussian%20Tree%20Models%20via%0A%20%20Sample-optimal%20Testing%20of%20Gaussian%20Mutual%20Information%0AAuthor%3A%20Sutanu%20Gayen%20and%20Sanket%20Kale%20and%20Sayantan%20Sen%0AAbstract%3A%20%20%20Learning%20high-dimensional%20distributions%20is%20a%20significant%20challenge%20in%20machine%0Alearning%20and%20statistics.%20Classical%20research%20has%20mostly%20concentrated%20on%0Aasymptotic%20analysis%20of%20such%20data%20under%20suitable%20assumptions.%20While%20existing%0Aworks%20%5BBhattacharyya%20et%20al.%3A%20SICOMP%202023%2C%20Daskalakis%20et%20al.%3A%20STOC%202021%2C%20Choo%20et%0Aal.%3A%20ALT%202024%5D%20focus%20on%20discrete%20distributions%2C%20the%20current%20work%20addresses%20the%0Atree%20structure%20learning%20problem%20for%20Gaussian%20distributions%2C%20providing%20efficient%0Aalgorithms%20with%20solid%20theoretical%20guarantees.%20This%20is%20crucial%20as%20real-world%0Adistributions%20are%20often%20continuous%20and%20differ%20from%20the%20discrete%20scenarios%0Astudied%20in%20prior%20works.%0A%20%20In%20this%20work%2C%20we%20design%20a%20conditional%20mutual%20information%20tester%20for%20Gaussian%0Arandom%20variables%20that%20can%20test%20whether%20two%20Gaussian%20random%20variables%20are%0Aindependent%2C%20or%20their%20conditional%20mutual%20information%20is%20at%20least%20%24%5Cvarepsilon%24%2C%0Afor%20some%20parameter%20%24%5Cvarepsilon%20%5Cin%20%280%2C1%29%24%20using%0A%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20samples%20which%20we%20show%20to%20be%20near-optimal.%20In%0Acontrast%2C%20an%20additive%20estimation%20would%20require%20%24%5COmega%28%5Cvarepsilon%5E%7B-2%7D%29%24%0Asamples.%20Our%20upper%20bound%20technique%20uses%20linear%20regression%20on%20a%20pair%20of%20suitably%0Atransformed%20random%20variables.%20Importantly%2C%20we%20show%20that%20the%20chain%20rule%20of%0Aconditional%20mutual%20information%20continues%20to%20hold%20for%20the%20estimated%0A%28conditional%29%20mutual%20information.%20As%20an%20application%20of%20such%20a%20mutual%0Ainformation%20tester%2C%20we%20give%20an%20efficient%20%24%5Cvarepsilon%24-approximate%0Astructure-learning%20algorithm%20for%20an%20%24n%24-variate%20Gaussian%20tree%20model%20that%20takes%0A%24%5Cwidetilde%7B%5CTheta%7D%28n%5Cvarepsilon%5E%7B-1%7D%29%24%20samples%20which%20we%20again%20show%20to%20be%0Anear-optimal.%20In%20contrast%2C%20when%20the%20underlying%20Gaussian%20model%20is%20not%20known%20to%0Abe%20tree-structured%2C%20we%20show%20that%20%24%5Cwidetilde%7B%7B%7B%5CTheta%7D%7D%7D%28n%5E2%5Cvarepsilon%5E%7B-2%7D%29%24%0Asamples%20are%20necessary%20and%20sufficient%20to%20output%20an%20%24%5Cvarepsilon%24-approximate%0Atree%20structure.%20We%20perform%20extensive%20experiments%20that%20corroborate%20our%0Atheoretical%20convergence%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Sample-optimal%2520Learning%2520of%2520Gaussian%2520Tree%2520Models%2520via%250A%2520%2520Sample-optimal%2520Testing%2520of%2520Gaussian%2520Mutual%2520Information%26entry.906535625%3DSutanu%2520Gayen%2520and%2520Sanket%2520Kale%2520and%2520Sayantan%2520Sen%26entry.1292438233%3D%2520%2520Learning%2520high-dimensional%2520distributions%2520is%2520a%2520significant%2520challenge%2520in%2520machine%250Alearning%2520and%2520statistics.%2520Classical%2520research%2520has%2520mostly%2520concentrated%2520on%250Aasymptotic%2520analysis%2520of%2520such%2520data%2520under%2520suitable%2520assumptions.%2520While%2520existing%250Aworks%2520%255BBhattacharyya%2520et%2520al.%253A%2520SICOMP%25202023%252C%2520Daskalakis%2520et%2520al.%253A%2520STOC%25202021%252C%2520Choo%2520et%250Aal.%253A%2520ALT%25202024%255D%2520focus%2520on%2520discrete%2520distributions%252C%2520the%2520current%2520work%2520addresses%2520the%250Atree%2520structure%2520learning%2520problem%2520for%2520Gaussian%2520distributions%252C%2520providing%2520efficient%250Aalgorithms%2520with%2520solid%2520theoretical%2520guarantees.%2520This%2520is%2520crucial%2520as%2520real-world%250Adistributions%2520are%2520often%2520continuous%2520and%2520differ%2520from%2520the%2520discrete%2520scenarios%250Astudied%2520in%2520prior%2520works.%250A%2520%2520In%2520this%2520work%252C%2520we%2520design%2520a%2520conditional%2520mutual%2520information%2520tester%2520for%2520Gaussian%250Arandom%2520variables%2520that%2520can%2520test%2520whether%2520two%2520Gaussian%2520random%2520variables%2520are%250Aindependent%252C%2520or%2520their%2520conditional%2520mutual%2520information%2520is%2520at%2520least%2520%2524%255Cvarepsilon%2524%252C%250Afor%2520some%2520parameter%2520%2524%255Cvarepsilon%2520%255Cin%2520%25280%252C1%2529%2524%2520using%250A%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-1%257D%2529%2524%2520samples%2520which%2520we%2520show%2520to%2520be%2520near-optimal.%2520In%250Acontrast%252C%2520an%2520additive%2520estimation%2520would%2520require%2520%2524%255COmega%2528%255Cvarepsilon%255E%257B-2%257D%2529%2524%250Asamples.%2520Our%2520upper%2520bound%2520technique%2520uses%2520linear%2520regression%2520on%2520a%2520pair%2520of%2520suitably%250Atransformed%2520random%2520variables.%2520Importantly%252C%2520we%2520show%2520that%2520the%2520chain%2520rule%2520of%250Aconditional%2520mutual%2520information%2520continues%2520to%2520hold%2520for%2520the%2520estimated%250A%2528conditional%2529%2520mutual%2520information.%2520As%2520an%2520application%2520of%2520such%2520a%2520mutual%250Ainformation%2520tester%252C%2520we%2520give%2520an%2520efficient%2520%2524%255Cvarepsilon%2524-approximate%250Astructure-learning%2520algorithm%2520for%2520an%2520%2524n%2524-variate%2520Gaussian%2520tree%2520model%2520that%2520takes%250A%2524%255Cwidetilde%257B%255CTheta%257D%2528n%255Cvarepsilon%255E%257B-1%257D%2529%2524%2520samples%2520which%2520we%2520again%2520show%2520to%2520be%250Anear-optimal.%2520In%2520contrast%252C%2520when%2520the%2520underlying%2520Gaussian%2520model%2520is%2520not%2520known%2520to%250Abe%2520tree-structured%252C%2520we%2520show%2520that%2520%2524%255Cwidetilde%257B%257B%257B%255CTheta%257D%257D%257D%2528n%255E2%255Cvarepsilon%255E%257B-2%257D%2529%2524%250Asamples%2520are%2520necessary%2520and%2520sufficient%2520to%2520output%2520an%2520%2524%255Cvarepsilon%2524-approximate%250Atree%2520structure.%2520We%2520perform%2520extensive%2520experiments%2520that%2520corroborate%2520our%250Atheoretical%2520convergence%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Sample-optimal%20Learning%20of%20Gaussian%20Tree%20Models%20via%0A%20%20Sample-optimal%20Testing%20of%20Gaussian%20Mutual%20Information&entry.906535625=Sutanu%20Gayen%20and%20Sanket%20Kale%20and%20Sayantan%20Sen&entry.1292438233=%20%20Learning%20high-dimensional%20distributions%20is%20a%20significant%20challenge%20in%20machine%0Alearning%20and%20statistics.%20Classical%20research%20has%20mostly%20concentrated%20on%0Aasymptotic%20analysis%20of%20such%20data%20under%20suitable%20assumptions.%20While%20existing%0Aworks%20%5BBhattacharyya%20et%20al.%3A%20SICOMP%202023%2C%20Daskalakis%20et%20al.%3A%20STOC%202021%2C%20Choo%20et%0Aal.%3A%20ALT%202024%5D%20focus%20on%20discrete%20distributions%2C%20the%20current%20work%20addresses%20the%0Atree%20structure%20learning%20problem%20for%20Gaussian%20distributions%2C%20providing%20efficient%0Aalgorithms%20with%20solid%20theoretical%20guarantees.%20This%20is%20crucial%20as%20real-world%0Adistributions%20are%20often%20continuous%20and%20differ%20from%20the%20discrete%20scenarios%0Astudied%20in%20prior%20works.%0A%20%20In%20this%20work%2C%20we%20design%20a%20conditional%20mutual%20information%20tester%20for%20Gaussian%0Arandom%20variables%20that%20can%20test%20whether%20two%20Gaussian%20random%20variables%20are%0Aindependent%2C%20or%20their%20conditional%20mutual%20information%20is%20at%20least%20%24%5Cvarepsilon%24%2C%0Afor%20some%20parameter%20%24%5Cvarepsilon%20%5Cin%20%280%2C1%29%24%20using%0A%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20samples%20which%20we%20show%20to%20be%20near-optimal.%20In%0Acontrast%2C%20an%20additive%20estimation%20would%20require%20%24%5COmega%28%5Cvarepsilon%5E%7B-2%7D%29%24%0Asamples.%20Our%20upper%20bound%20technique%20uses%20linear%20regression%20on%20a%20pair%20of%20suitably%0Atransformed%20random%20variables.%20Importantly%2C%20we%20show%20that%20the%20chain%20rule%20of%0Aconditional%20mutual%20information%20continues%20to%20hold%20for%20the%20estimated%0A%28conditional%29%20mutual%20information.%20As%20an%20application%20of%20such%20a%20mutual%0Ainformation%20tester%2C%20we%20give%20an%20efficient%20%24%5Cvarepsilon%24-approximate%0Astructure-learning%20algorithm%20for%20an%20%24n%24-variate%20Gaussian%20tree%20model%20that%20takes%0A%24%5Cwidetilde%7B%5CTheta%7D%28n%5Cvarepsilon%5E%7B-1%7D%29%24%20samples%20which%20we%20again%20show%20to%20be%0Anear-optimal.%20In%20contrast%2C%20when%20the%20underlying%20Gaussian%20model%20is%20not%20known%20to%0Abe%20tree-structured%2C%20we%20show%20that%20%24%5Cwidetilde%7B%7B%7B%5CTheta%7D%7D%7D%28n%5E2%5Cvarepsilon%5E%7B-2%7D%29%24%0Asamples%20are%20necessary%20and%20sufficient%20to%20output%20an%20%24%5Cvarepsilon%24-approximate%0Atree%20structure.%20We%20perform%20extensive%20experiments%20that%20corroborate%20our%0Atheoretical%20convergence%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11516v1&entry.124074799=Read"},
{"title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large\n  and Small Language Models", "author": "Tao Fan and Yan Kang and Guoqiang Ma and Lixin Fan and Kai Chen and Qiang Yang", "abstract": "  By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.\n", "link": "http://arxiv.org/abs/2411.11707v1", "date": "2024-11-18", "relevancy": 2.2694, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedCoLLM%3A%20A%20Parameter-Efficient%20Federated%20Co-tuning%20Framework%20for%20Large%0A%20%20and%20Small%20Language%20Models&body=Title%3A%20FedCoLLM%3A%20A%20Parameter-Efficient%20Federated%20Co-tuning%20Framework%20for%20Large%0A%20%20and%20Small%20Language%20Models%0AAuthor%3A%20Tao%20Fan%20and%20Yan%20Kang%20and%20Guoqiang%20Ma%20and%20Lixin%20Fan%20and%20Kai%20Chen%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20By%20adapting%20Large%20Language%20Models%20%28LLMs%29%20to%20domain-specific%20tasks%20or%0Aenriching%20them%20with%20domain-specific%20knowledge%2C%20we%20can%20fully%20harness%20the%0Acapabilities%20of%20LLMs.%20Nonetheless%2C%20a%20gap%20persists%20in%20achieving%20simultaneous%0Amutual%20enhancement%20between%20the%20server%27s%20LLM%20and%20the%20downstream%20clients%27%20Small%0ALanguage%20Models%20%28SLMs%29.%20To%20address%20this%2C%20we%20propose%20FedCoLLM%2C%20a%20novel%20and%0Aparameter-efficient%20federated%20framework%20designed%20for%20co-tuning%20LLMs%20and%20SLMs.%0AThis%20approach%20is%20aimed%20at%20adaptively%20transferring%20server-side%20LLMs%20knowledge%20to%0Aclients%27%20SLMs%20while%20simultaneously%20enriching%20the%20LLMs%20with%20domain%20insights%20from%0Athe%20clients.%20To%20accomplish%20this%2C%20FedCoLLM%20utilizes%20lightweight%20adapters%20in%0Aconjunction%20with%20SLMs%2C%20facilitating%20knowledge%20exchange%20between%20server%20and%0Aclients%20in%20a%20manner%20that%20respects%20data%20privacy%20while%20also%20minimizing%0Acomputational%20and%20communication%20overhead.%20Our%20evaluation%20of%20FedCoLLM%2C%20utilizing%0Avarious%20public%20LLMs%20and%20SLMs%20across%20a%20range%20of%20NLP%20text%20generation%20tasks%2C%0Areveals%20that%20the%20performance%20of%20clients%27%20SLMs%20experiences%20notable%20improvements%0Awith%20the%20assistance%20of%20the%20LLMs.%20Simultaneously%2C%20the%20LLMs%20enhanced%20via%20FedCoLLM%0Aachieves%20comparable%20performance%20to%20that%20obtained%20through%20direct%20fine-tuning%20on%0Aclients%27%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedCoLLM%253A%2520A%2520Parameter-Efficient%2520Federated%2520Co-tuning%2520Framework%2520for%2520Large%250A%2520%2520and%2520Small%2520Language%2520Models%26entry.906535625%3DTao%2520Fan%2520and%2520Yan%2520Kang%2520and%2520Guoqiang%2520Ma%2520and%2520Lixin%2520Fan%2520and%2520Kai%2520Chen%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520By%2520adapting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520domain-specific%2520tasks%2520or%250Aenriching%2520them%2520with%2520domain-specific%2520knowledge%252C%2520we%2520can%2520fully%2520harness%2520the%250Acapabilities%2520of%2520LLMs.%2520Nonetheless%252C%2520a%2520gap%2520persists%2520in%2520achieving%2520simultaneous%250Amutual%2520enhancement%2520between%2520the%2520server%2527s%2520LLM%2520and%2520the%2520downstream%2520clients%2527%2520Small%250ALanguage%2520Models%2520%2528SLMs%2529.%2520To%2520address%2520this%252C%2520we%2520propose%2520FedCoLLM%252C%2520a%2520novel%2520and%250Aparameter-efficient%2520federated%2520framework%2520designed%2520for%2520co-tuning%2520LLMs%2520and%2520SLMs.%250AThis%2520approach%2520is%2520aimed%2520at%2520adaptively%2520transferring%2520server-side%2520LLMs%2520knowledge%2520to%250Aclients%2527%2520SLMs%2520while%2520simultaneously%2520enriching%2520the%2520LLMs%2520with%2520domain%2520insights%2520from%250Athe%2520clients.%2520To%2520accomplish%2520this%252C%2520FedCoLLM%2520utilizes%2520lightweight%2520adapters%2520in%250Aconjunction%2520with%2520SLMs%252C%2520facilitating%2520knowledge%2520exchange%2520between%2520server%2520and%250Aclients%2520in%2520a%2520manner%2520that%2520respects%2520data%2520privacy%2520while%2520also%2520minimizing%250Acomputational%2520and%2520communication%2520overhead.%2520Our%2520evaluation%2520of%2520FedCoLLM%252C%2520utilizing%250Avarious%2520public%2520LLMs%2520and%2520SLMs%2520across%2520a%2520range%2520of%2520NLP%2520text%2520generation%2520tasks%252C%250Areveals%2520that%2520the%2520performance%2520of%2520clients%2527%2520SLMs%2520experiences%2520notable%2520improvements%250Awith%2520the%2520assistance%2520of%2520the%2520LLMs.%2520Simultaneously%252C%2520the%2520LLMs%2520enhanced%2520via%2520FedCoLLM%250Aachieves%2520comparable%2520performance%2520to%2520that%2520obtained%2520through%2520direct%2520fine-tuning%2520on%250Aclients%2527%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedCoLLM%3A%20A%20Parameter-Efficient%20Federated%20Co-tuning%20Framework%20for%20Large%0A%20%20and%20Small%20Language%20Models&entry.906535625=Tao%20Fan%20and%20Yan%20Kang%20and%20Guoqiang%20Ma%20and%20Lixin%20Fan%20and%20Kai%20Chen%20and%20Qiang%20Yang&entry.1292438233=%20%20By%20adapting%20Large%20Language%20Models%20%28LLMs%29%20to%20domain-specific%20tasks%20or%0Aenriching%20them%20with%20domain-specific%20knowledge%2C%20we%20can%20fully%20harness%20the%0Acapabilities%20of%20LLMs.%20Nonetheless%2C%20a%20gap%20persists%20in%20achieving%20simultaneous%0Amutual%20enhancement%20between%20the%20server%27s%20LLM%20and%20the%20downstream%20clients%27%20Small%0ALanguage%20Models%20%28SLMs%29.%20To%20address%20this%2C%20we%20propose%20FedCoLLM%2C%20a%20novel%20and%0Aparameter-efficient%20federated%20framework%20designed%20for%20co-tuning%20LLMs%20and%20SLMs.%0AThis%20approach%20is%20aimed%20at%20adaptively%20transferring%20server-side%20LLMs%20knowledge%20to%0Aclients%27%20SLMs%20while%20simultaneously%20enriching%20the%20LLMs%20with%20domain%20insights%20from%0Athe%20clients.%20To%20accomplish%20this%2C%20FedCoLLM%20utilizes%20lightweight%20adapters%20in%0Aconjunction%20with%20SLMs%2C%20facilitating%20knowledge%20exchange%20between%20server%20and%0Aclients%20in%20a%20manner%20that%20respects%20data%20privacy%20while%20also%20minimizing%0Acomputational%20and%20communication%20overhead.%20Our%20evaluation%20of%20FedCoLLM%2C%20utilizing%0Avarious%20public%20LLMs%20and%20SLMs%20across%20a%20range%20of%20NLP%20text%20generation%20tasks%2C%0Areveals%20that%20the%20performance%20of%20clients%27%20SLMs%20experiences%20notable%20improvements%0Awith%20the%20assistance%20of%20the%20LLMs.%20Simultaneously%2C%20the%20LLMs%20enhanced%20via%20FedCoLLM%0Aachieves%20comparable%20performance%20to%20that%20obtained%20through%20direct%20fine-tuning%20on%0Aclients%27%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11707v1&entry.124074799=Read"},
{"title": "Learning a Neural Association Network for Self-supervised Multi-Object\n  Tracking", "author": "Shuai Li and Michael Burke and Subramanian Ramamoorthy and Juergen Gall", "abstract": "  This paper introduces a novel framework to learn data association for\nmulti-object tracking in a self-supervised manner. Fully-supervised learning\nmethods are known to achieve excellent tracking performances, but acquiring\nidentity-level annotations is tedious and time-consuming. Motivated by the fact\nthat in real-world scenarios object motion can be usually represented by a\nMarkov process, we present a novel expectation maximization (EM) algorithm that\ntrains a neural network to associate detections for tracking, without requiring\nprior knowledge of their temporal correspondences. At the core of our method\nlies a neural Kalman filter, with an observation model conditioned on\nassociations of detections parameterized by a neural network. Given a batch of\nframes as input, data associations between detections from adjacent frames are\npredicted by a neural network followed by a Sinkhorn normalization that\ndetermines the assignment probabilities of detections to states. Kalman\nsmoothing is then used to obtain the marginal probability of observations given\nthe inferred states, producing a training objective to maximize this marginal\nprobability using gradient descent. The proposed framework is fully\ndifferentiable, allowing the underlying neural model to be trained end-to-end.\nWe evaluate our approach on the challenging MOT17 and MOT20 datasets and\nachieve state-of-the-art results in comparison to self-supervised trackers\nusing public detections. We furthermore demonstrate the capability of the\nlearned model to generalize across datasets.\n", "link": "http://arxiv.org/abs/2411.11514v1", "date": "2024-11-18", "relevancy": 2.2584, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Neural%20Association%20Network%20for%20Self-supervised%20Multi-Object%0A%20%20Tracking&body=Title%3A%20Learning%20a%20Neural%20Association%20Network%20for%20Self-supervised%20Multi-Object%0A%20%20Tracking%0AAuthor%3A%20Shuai%20Li%20and%20Michael%20Burke%20and%20Subramanian%20Ramamoorthy%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20to%20learn%20data%20association%20for%0Amulti-object%20tracking%20in%20a%20self-supervised%20manner.%20Fully-supervised%20learning%0Amethods%20are%20known%20to%20achieve%20excellent%20tracking%20performances%2C%20but%20acquiring%0Aidentity-level%20annotations%20is%20tedious%20and%20time-consuming.%20Motivated%20by%20the%20fact%0Athat%20in%20real-world%20scenarios%20object%20motion%20can%20be%20usually%20represented%20by%20a%0AMarkov%20process%2C%20we%20present%20a%20novel%20expectation%20maximization%20%28EM%29%20algorithm%20that%0Atrains%20a%20neural%20network%20to%20associate%20detections%20for%20tracking%2C%20without%20requiring%0Aprior%20knowledge%20of%20their%20temporal%20correspondences.%20At%20the%20core%20of%20our%20method%0Alies%20a%20neural%20Kalman%20filter%2C%20with%20an%20observation%20model%20conditioned%20on%0Aassociations%20of%20detections%20parameterized%20by%20a%20neural%20network.%20Given%20a%20batch%20of%0Aframes%20as%20input%2C%20data%20associations%20between%20detections%20from%20adjacent%20frames%20are%0Apredicted%20by%20a%20neural%20network%20followed%20by%20a%20Sinkhorn%20normalization%20that%0Adetermines%20the%20assignment%20probabilities%20of%20detections%20to%20states.%20Kalman%0Asmoothing%20is%20then%20used%20to%20obtain%20the%20marginal%20probability%20of%20observations%20given%0Athe%20inferred%20states%2C%20producing%20a%20training%20objective%20to%20maximize%20this%20marginal%0Aprobability%20using%20gradient%20descent.%20The%20proposed%20framework%20is%20fully%0Adifferentiable%2C%20allowing%20the%20underlying%20neural%20model%20to%20be%20trained%20end-to-end.%0AWe%20evaluate%20our%20approach%20on%20the%20challenging%20MOT17%20and%20MOT20%20datasets%20and%0Aachieve%20state-of-the-art%20results%20in%20comparison%20to%20self-supervised%20trackers%0Ausing%20public%20detections.%20We%20furthermore%20demonstrate%20the%20capability%20of%20the%0Alearned%20model%20to%20generalize%20across%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Neural%2520Association%2520Network%2520for%2520Self-supervised%2520Multi-Object%250A%2520%2520Tracking%26entry.906535625%3DShuai%2520Li%2520and%2520Michael%2520Burke%2520and%2520Subramanian%2520Ramamoorthy%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520to%2520learn%2520data%2520association%2520for%250Amulti-object%2520tracking%2520in%2520a%2520self-supervised%2520manner.%2520Fully-supervised%2520learning%250Amethods%2520are%2520known%2520to%2520achieve%2520excellent%2520tracking%2520performances%252C%2520but%2520acquiring%250Aidentity-level%2520annotations%2520is%2520tedious%2520and%2520time-consuming.%2520Motivated%2520by%2520the%2520fact%250Athat%2520in%2520real-world%2520scenarios%2520object%2520motion%2520can%2520be%2520usually%2520represented%2520by%2520a%250AMarkov%2520process%252C%2520we%2520present%2520a%2520novel%2520expectation%2520maximization%2520%2528EM%2529%2520algorithm%2520that%250Atrains%2520a%2520neural%2520network%2520to%2520associate%2520detections%2520for%2520tracking%252C%2520without%2520requiring%250Aprior%2520knowledge%2520of%2520their%2520temporal%2520correspondences.%2520At%2520the%2520core%2520of%2520our%2520method%250Alies%2520a%2520neural%2520Kalman%2520filter%252C%2520with%2520an%2520observation%2520model%2520conditioned%2520on%250Aassociations%2520of%2520detections%2520parameterized%2520by%2520a%2520neural%2520network.%2520Given%2520a%2520batch%2520of%250Aframes%2520as%2520input%252C%2520data%2520associations%2520between%2520detections%2520from%2520adjacent%2520frames%2520are%250Apredicted%2520by%2520a%2520neural%2520network%2520followed%2520by%2520a%2520Sinkhorn%2520normalization%2520that%250Adetermines%2520the%2520assignment%2520probabilities%2520of%2520detections%2520to%2520states.%2520Kalman%250Asmoothing%2520is%2520then%2520used%2520to%2520obtain%2520the%2520marginal%2520probability%2520of%2520observations%2520given%250Athe%2520inferred%2520states%252C%2520producing%2520a%2520training%2520objective%2520to%2520maximize%2520this%2520marginal%250Aprobability%2520using%2520gradient%2520descent.%2520The%2520proposed%2520framework%2520is%2520fully%250Adifferentiable%252C%2520allowing%2520the%2520underlying%2520neural%2520model%2520to%2520be%2520trained%2520end-to-end.%250AWe%2520evaluate%2520our%2520approach%2520on%2520the%2520challenging%2520MOT17%2520and%2520MOT20%2520datasets%2520and%250Aachieve%2520state-of-the-art%2520results%2520in%2520comparison%2520to%2520self-supervised%2520trackers%250Ausing%2520public%2520detections.%2520We%2520furthermore%2520demonstrate%2520the%2520capability%2520of%2520the%250Alearned%2520model%2520to%2520generalize%2520across%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Neural%20Association%20Network%20for%20Self-supervised%20Multi-Object%0A%20%20Tracking&entry.906535625=Shuai%20Li%20and%20Michael%20Burke%20and%20Subramanian%20Ramamoorthy%20and%20Juergen%20Gall&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20to%20learn%20data%20association%20for%0Amulti-object%20tracking%20in%20a%20self-supervised%20manner.%20Fully-supervised%20learning%0Amethods%20are%20known%20to%20achieve%20excellent%20tracking%20performances%2C%20but%20acquiring%0Aidentity-level%20annotations%20is%20tedious%20and%20time-consuming.%20Motivated%20by%20the%20fact%0Athat%20in%20real-world%20scenarios%20object%20motion%20can%20be%20usually%20represented%20by%20a%0AMarkov%20process%2C%20we%20present%20a%20novel%20expectation%20maximization%20%28EM%29%20algorithm%20that%0Atrains%20a%20neural%20network%20to%20associate%20detections%20for%20tracking%2C%20without%20requiring%0Aprior%20knowledge%20of%20their%20temporal%20correspondences.%20At%20the%20core%20of%20our%20method%0Alies%20a%20neural%20Kalman%20filter%2C%20with%20an%20observation%20model%20conditioned%20on%0Aassociations%20of%20detections%20parameterized%20by%20a%20neural%20network.%20Given%20a%20batch%20of%0Aframes%20as%20input%2C%20data%20associations%20between%20detections%20from%20adjacent%20frames%20are%0Apredicted%20by%20a%20neural%20network%20followed%20by%20a%20Sinkhorn%20normalization%20that%0Adetermines%20the%20assignment%20probabilities%20of%20detections%20to%20states.%20Kalman%0Asmoothing%20is%20then%20used%20to%20obtain%20the%20marginal%20probability%20of%20observations%20given%0Athe%20inferred%20states%2C%20producing%20a%20training%20objective%20to%20maximize%20this%20marginal%0Aprobability%20using%20gradient%20descent.%20The%20proposed%20framework%20is%20fully%0Adifferentiable%2C%20allowing%20the%20underlying%20neural%20model%20to%20be%20trained%20end-to-end.%0AWe%20evaluate%20our%20approach%20on%20the%20challenging%20MOT17%20and%20MOT20%20datasets%20and%0Aachieve%20state-of-the-art%20results%20in%20comparison%20to%20self-supervised%20trackers%0Ausing%20public%20detections.%20We%20furthermore%20demonstrate%20the%20capability%20of%20the%0Alearned%20model%20to%20generalize%20across%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11514v1&entry.124074799=Read"},
{"title": "Feature Selection for Network Intrusion Detection", "author": "Charles Westphal and Stephen Hailes and Mirco Musolesi", "abstract": "  Network Intrusion Detection (NID) remains a key area of research within the\ninformation security community, while also being relevant to Machine Learning\n(ML) practitioners. The latter generally aim to detect attacks using network\nfeatures, which have been extracted from raw network data typically using\ndimensionality reduction methods, such as principal component analysis (PCA).\nHowever, PCA is not able to assess the relevance of features for the task at\nhand. Consequently, the features available are of varying quality, with some\nbeing entirely non-informative. From this, two major drawbacks arise. Firstly,\ntrained and deployed models have to process large amounts of unnecessary data,\ntherefore draining potentially costly resources. Secondly, the noise caused by\nthe presence of irrelevant features can, in some cases, impede a model's\nability to detect an attack. In order to deal with these challenges, we present\nFeature Selection for Network Intrusion Detection (FSNID) a novel\ninformation-theoretic method that facilitates the exclusion of non-informative\nfeatures when detecting network intrusions. The proposed method is based on\nfunction approximation using a neural network, which enables a version of our\napproach that incorporates a recurrent layer. Consequently, this version\nuniquely enables the integration of temporal dependencies. Through an extensive\nset of experiments, we demonstrate that the proposed method selects a\nsignificantly reduced feature set, while maintaining NID performance. Code will\nbe made available upon publication.\n", "link": "http://arxiv.org/abs/2411.11603v1", "date": "2024-11-18", "relevancy": 2.2559, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4518}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Selection%20for%20Network%20Intrusion%20Detection&body=Title%3A%20Feature%20Selection%20for%20Network%20Intrusion%20Detection%0AAuthor%3A%20Charles%20Westphal%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Network%20Intrusion%20Detection%20%28NID%29%20remains%20a%20key%20area%20of%20research%20within%20the%0Ainformation%20security%20community%2C%20while%20also%20being%20relevant%20to%20Machine%20Learning%0A%28ML%29%20practitioners.%20The%20latter%20generally%20aim%20to%20detect%20attacks%20using%20network%0Afeatures%2C%20which%20have%20been%20extracted%20from%20raw%20network%20data%20typically%20using%0Adimensionality%20reduction%20methods%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29.%0AHowever%2C%20PCA%20is%20not%20able%20to%20assess%20the%20relevance%20of%20features%20for%20the%20task%20at%0Ahand.%20Consequently%2C%20the%20features%20available%20are%20of%20varying%20quality%2C%20with%20some%0Abeing%20entirely%20non-informative.%20From%20this%2C%20two%20major%20drawbacks%20arise.%20Firstly%2C%0Atrained%20and%20deployed%20models%20have%20to%20process%20large%20amounts%20of%20unnecessary%20data%2C%0Atherefore%20draining%20potentially%20costly%20resources.%20Secondly%2C%20the%20noise%20caused%20by%0Athe%20presence%20of%20irrelevant%20features%20can%2C%20in%20some%20cases%2C%20impede%20a%20model%27s%0Aability%20to%20detect%20an%20attack.%20In%20order%20to%20deal%20with%20these%20challenges%2C%20we%20present%0AFeature%20Selection%20for%20Network%20Intrusion%20Detection%20%28FSNID%29%20a%20novel%0Ainformation-theoretic%20method%20that%20facilitates%20the%20exclusion%20of%20non-informative%0Afeatures%20when%20detecting%20network%20intrusions.%20The%20proposed%20method%20is%20based%20on%0Afunction%20approximation%20using%20a%20neural%20network%2C%20which%20enables%20a%20version%20of%20our%0Aapproach%20that%20incorporates%20a%20recurrent%20layer.%20Consequently%2C%20this%20version%0Auniquely%20enables%20the%20integration%20of%20temporal%20dependencies.%20Through%20an%20extensive%0Aset%20of%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%20method%20selects%20a%0Asignificantly%20reduced%20feature%20set%2C%20while%20maintaining%20NID%20performance.%20Code%20will%0Abe%20made%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Selection%2520for%2520Network%2520Intrusion%2520Detection%26entry.906535625%3DCharles%2520Westphal%2520and%2520Stephen%2520Hailes%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Network%2520Intrusion%2520Detection%2520%2528NID%2529%2520remains%2520a%2520key%2520area%2520of%2520research%2520within%2520the%250Ainformation%2520security%2520community%252C%2520while%2520also%2520being%2520relevant%2520to%2520Machine%2520Learning%250A%2528ML%2529%2520practitioners.%2520The%2520latter%2520generally%2520aim%2520to%2520detect%2520attacks%2520using%2520network%250Afeatures%252C%2520which%2520have%2520been%2520extracted%2520from%2520raw%2520network%2520data%2520typically%2520using%250Adimensionality%2520reduction%2520methods%252C%2520such%2520as%2520principal%2520component%2520analysis%2520%2528PCA%2529.%250AHowever%252C%2520PCA%2520is%2520not%2520able%2520to%2520assess%2520the%2520relevance%2520of%2520features%2520for%2520the%2520task%2520at%250Ahand.%2520Consequently%252C%2520the%2520features%2520available%2520are%2520of%2520varying%2520quality%252C%2520with%2520some%250Abeing%2520entirely%2520non-informative.%2520From%2520this%252C%2520two%2520major%2520drawbacks%2520arise.%2520Firstly%252C%250Atrained%2520and%2520deployed%2520models%2520have%2520to%2520process%2520large%2520amounts%2520of%2520unnecessary%2520data%252C%250Atherefore%2520draining%2520potentially%2520costly%2520resources.%2520Secondly%252C%2520the%2520noise%2520caused%2520by%250Athe%2520presence%2520of%2520irrelevant%2520features%2520can%252C%2520in%2520some%2520cases%252C%2520impede%2520a%2520model%2527s%250Aability%2520to%2520detect%2520an%2520attack.%2520In%2520order%2520to%2520deal%2520with%2520these%2520challenges%252C%2520we%2520present%250AFeature%2520Selection%2520for%2520Network%2520Intrusion%2520Detection%2520%2528FSNID%2529%2520a%2520novel%250Ainformation-theoretic%2520method%2520that%2520facilitates%2520the%2520exclusion%2520of%2520non-informative%250Afeatures%2520when%2520detecting%2520network%2520intrusions.%2520The%2520proposed%2520method%2520is%2520based%2520on%250Afunction%2520approximation%2520using%2520a%2520neural%2520network%252C%2520which%2520enables%2520a%2520version%2520of%2520our%250Aapproach%2520that%2520incorporates%2520a%2520recurrent%2520layer.%2520Consequently%252C%2520this%2520version%250Auniquely%2520enables%2520the%2520integration%2520of%2520temporal%2520dependencies.%2520Through%2520an%2520extensive%250Aset%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%2520selects%2520a%250Asignificantly%2520reduced%2520feature%2520set%252C%2520while%2520maintaining%2520NID%2520performance.%2520Code%2520will%250Abe%2520made%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Selection%20for%20Network%20Intrusion%20Detection&entry.906535625=Charles%20Westphal%20and%20Stephen%20Hailes%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Network%20Intrusion%20Detection%20%28NID%29%20remains%20a%20key%20area%20of%20research%20within%20the%0Ainformation%20security%20community%2C%20while%20also%20being%20relevant%20to%20Machine%20Learning%0A%28ML%29%20practitioners.%20The%20latter%20generally%20aim%20to%20detect%20attacks%20using%20network%0Afeatures%2C%20which%20have%20been%20extracted%20from%20raw%20network%20data%20typically%20using%0Adimensionality%20reduction%20methods%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29.%0AHowever%2C%20PCA%20is%20not%20able%20to%20assess%20the%20relevance%20of%20features%20for%20the%20task%20at%0Ahand.%20Consequently%2C%20the%20features%20available%20are%20of%20varying%20quality%2C%20with%20some%0Abeing%20entirely%20non-informative.%20From%20this%2C%20two%20major%20drawbacks%20arise.%20Firstly%2C%0Atrained%20and%20deployed%20models%20have%20to%20process%20large%20amounts%20of%20unnecessary%20data%2C%0Atherefore%20draining%20potentially%20costly%20resources.%20Secondly%2C%20the%20noise%20caused%20by%0Athe%20presence%20of%20irrelevant%20features%20can%2C%20in%20some%20cases%2C%20impede%20a%20model%27s%0Aability%20to%20detect%20an%20attack.%20In%20order%20to%20deal%20with%20these%20challenges%2C%20we%20present%0AFeature%20Selection%20for%20Network%20Intrusion%20Detection%20%28FSNID%29%20a%20novel%0Ainformation-theoretic%20method%20that%20facilitates%20the%20exclusion%20of%20non-informative%0Afeatures%20when%20detecting%20network%20intrusions.%20The%20proposed%20method%20is%20based%20on%0Afunction%20approximation%20using%20a%20neural%20network%2C%20which%20enables%20a%20version%20of%20our%0Aapproach%20that%20incorporates%20a%20recurrent%20layer.%20Consequently%2C%20this%20version%0Auniquely%20enables%20the%20integration%20of%20temporal%20dependencies.%20Through%20an%20extensive%0Aset%20of%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%20method%20selects%20a%0Asignificantly%20reduced%20feature%20set%2C%20while%20maintaining%20NID%20performance.%20Code%20will%0Abe%20made%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11603v1&entry.124074799=Read"},
{"title": "Parallelly Tempered Generative Adversarial Networks", "author": "Jinwon Sohn and Qifan Song", "abstract": "  A generative adversarial network (GAN) has been a representative backbone\nmodel in generative artificial intelligence (AI) because of its powerful\nperformance in capturing intricate data-generating processes. However, the GAN\ntraining is well-known for its notorious training instability, usually\ncharacterized by the occurrence of mode collapse. Through the lens of\ngradients' variance, this work particularly analyzes the training instability\nand inefficiency in the presence of mode collapse by linking it to\nmultimodality in the target distribution. To ease the raised training issues\nfrom severe multimodality, we introduce a novel GAN training framework that\nleverages a series of tempered distributions produced via convex interpolation.\nWith our newly developed GAN objective function, the generator can learn all\nthe tempered distributions simultaneously, conceptually resonating with the\nparallel tempering in Statistics. Our simulation studies demonstrate the\nsuperiority of our approach over existing popular training strategies in both\nimage and tabular data synthesis. We theoretically analyze that such\nsignificant improvement can arise from reducing the variance of gradient\nestimates by using the tempered distributions. Finally, we further develop a\nvariant of the proposed framework aimed at generating fair synthetic data which\nis one of the growing interests in the field of trustworthy AI.\n", "link": "http://arxiv.org/abs/2411.11786v1", "date": "2024-11-18", "relevancy": 2.2478, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5837}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5681}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallelly%20Tempered%20Generative%20Adversarial%20Networks&body=Title%3A%20Parallelly%20Tempered%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Jinwon%20Sohn%20and%20Qifan%20Song%0AAbstract%3A%20%20%20A%20generative%20adversarial%20network%20%28GAN%29%20has%20been%20a%20representative%20backbone%0Amodel%20in%20generative%20artificial%20intelligence%20%28AI%29%20because%20of%20its%20powerful%0Aperformance%20in%20capturing%20intricate%20data-generating%20processes.%20However%2C%20the%20GAN%0Atraining%20is%20well-known%20for%20its%20notorious%20training%20instability%2C%20usually%0Acharacterized%20by%20the%20occurrence%20of%20mode%20collapse.%20Through%20the%20lens%20of%0Agradients%27%20variance%2C%20this%20work%20particularly%20analyzes%20the%20training%20instability%0Aand%20inefficiency%20in%20the%20presence%20of%20mode%20collapse%20by%20linking%20it%20to%0Amultimodality%20in%20the%20target%20distribution.%20To%20ease%20the%20raised%20training%20issues%0Afrom%20severe%20multimodality%2C%20we%20introduce%20a%20novel%20GAN%20training%20framework%20that%0Aleverages%20a%20series%20of%20tempered%20distributions%20produced%20via%20convex%20interpolation.%0AWith%20our%20newly%20developed%20GAN%20objective%20function%2C%20the%20generator%20can%20learn%20all%0Athe%20tempered%20distributions%20simultaneously%2C%20conceptually%20resonating%20with%20the%0Aparallel%20tempering%20in%20Statistics.%20Our%20simulation%20studies%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20over%20existing%20popular%20training%20strategies%20in%20both%0Aimage%20and%20tabular%20data%20synthesis.%20We%20theoretically%20analyze%20that%20such%0Asignificant%20improvement%20can%20arise%20from%20reducing%20the%20variance%20of%20gradient%0Aestimates%20by%20using%20the%20tempered%20distributions.%20Finally%2C%20we%20further%20develop%20a%0Avariant%20of%20the%20proposed%20framework%20aimed%20at%20generating%20fair%20synthetic%20data%20which%0Ais%20one%20of%20the%20growing%20interests%20in%20the%20field%20of%20trustworthy%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelly%2520Tempered%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DJinwon%2520Sohn%2520and%2520Qifan%2520Song%26entry.1292438233%3D%2520%2520A%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520has%2520been%2520a%2520representative%2520backbone%250Amodel%2520in%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520because%2520of%2520its%2520powerful%250Aperformance%2520in%2520capturing%2520intricate%2520data-generating%2520processes.%2520However%252C%2520the%2520GAN%250Atraining%2520is%2520well-known%2520for%2520its%2520notorious%2520training%2520instability%252C%2520usually%250Acharacterized%2520by%2520the%2520occurrence%2520of%2520mode%2520collapse.%2520Through%2520the%2520lens%2520of%250Agradients%2527%2520variance%252C%2520this%2520work%2520particularly%2520analyzes%2520the%2520training%2520instability%250Aand%2520inefficiency%2520in%2520the%2520presence%2520of%2520mode%2520collapse%2520by%2520linking%2520it%2520to%250Amultimodality%2520in%2520the%2520target%2520distribution.%2520To%2520ease%2520the%2520raised%2520training%2520issues%250Afrom%2520severe%2520multimodality%252C%2520we%2520introduce%2520a%2520novel%2520GAN%2520training%2520framework%2520that%250Aleverages%2520a%2520series%2520of%2520tempered%2520distributions%2520produced%2520via%2520convex%2520interpolation.%250AWith%2520our%2520newly%2520developed%2520GAN%2520objective%2520function%252C%2520the%2520generator%2520can%2520learn%2520all%250Athe%2520tempered%2520distributions%2520simultaneously%252C%2520conceptually%2520resonating%2520with%2520the%250Aparallel%2520tempering%2520in%2520Statistics.%2520Our%2520simulation%2520studies%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach%2520over%2520existing%2520popular%2520training%2520strategies%2520in%2520both%250Aimage%2520and%2520tabular%2520data%2520synthesis.%2520We%2520theoretically%2520analyze%2520that%2520such%250Asignificant%2520improvement%2520can%2520arise%2520from%2520reducing%2520the%2520variance%2520of%2520gradient%250Aestimates%2520by%2520using%2520the%2520tempered%2520distributions.%2520Finally%252C%2520we%2520further%2520develop%2520a%250Avariant%2520of%2520the%2520proposed%2520framework%2520aimed%2520at%2520generating%2520fair%2520synthetic%2520data%2520which%250Ais%2520one%2520of%2520the%2520growing%2520interests%2520in%2520the%2520field%2520of%2520trustworthy%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallelly%20Tempered%20Generative%20Adversarial%20Networks&entry.906535625=Jinwon%20Sohn%20and%20Qifan%20Song&entry.1292438233=%20%20A%20generative%20adversarial%20network%20%28GAN%29%20has%20been%20a%20representative%20backbone%0Amodel%20in%20generative%20artificial%20intelligence%20%28AI%29%20because%20of%20its%20powerful%0Aperformance%20in%20capturing%20intricate%20data-generating%20processes.%20However%2C%20the%20GAN%0Atraining%20is%20well-known%20for%20its%20notorious%20training%20instability%2C%20usually%0Acharacterized%20by%20the%20occurrence%20of%20mode%20collapse.%20Through%20the%20lens%20of%0Agradients%27%20variance%2C%20this%20work%20particularly%20analyzes%20the%20training%20instability%0Aand%20inefficiency%20in%20the%20presence%20of%20mode%20collapse%20by%20linking%20it%20to%0Amultimodality%20in%20the%20target%20distribution.%20To%20ease%20the%20raised%20training%20issues%0Afrom%20severe%20multimodality%2C%20we%20introduce%20a%20novel%20GAN%20training%20framework%20that%0Aleverages%20a%20series%20of%20tempered%20distributions%20produced%20via%20convex%20interpolation.%0AWith%20our%20newly%20developed%20GAN%20objective%20function%2C%20the%20generator%20can%20learn%20all%0Athe%20tempered%20distributions%20simultaneously%2C%20conceptually%20resonating%20with%20the%0Aparallel%20tempering%20in%20Statistics.%20Our%20simulation%20studies%20demonstrate%20the%0Asuperiority%20of%20our%20approach%20over%20existing%20popular%20training%20strategies%20in%20both%0Aimage%20and%20tabular%20data%20synthesis.%20We%20theoretically%20analyze%20that%20such%0Asignificant%20improvement%20can%20arise%20from%20reducing%20the%20variance%20of%20gradient%0Aestimates%20by%20using%20the%20tempered%20distributions.%20Finally%2C%20we%20further%20develop%20a%0Avariant%20of%20the%20proposed%20framework%20aimed%20at%20generating%20fair%20synthetic%20data%20which%0Ais%20one%20of%20the%20growing%20interests%20in%20the%20field%20of%20trustworthy%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11786v1&entry.124074799=Read"},
{"title": "Artificial Scientific Discovery", "author": "Antonio Norelli", "abstract": "  Rooted in the explosion of deep learning over the past decade, this thesis\nspans from AlphaGo to ChatGPT to empirically examine the fundamental concepts\nneeded to realize the vision of an artificial scientist: a machine with the\ncapacity to autonomously generate original research and contribute to the\nexpansion of human knowledge. The investigation begins with {\\sc Olivaw}, an\nAlphaGo Zero-like agent that discovers Othello knowledge from scratch but is\nunable to communicate it. This realization leads to the development of the\nExplanatory Learning (EL) framework, a formalization of the problem faced by a\nscientist when trying to explain a new phenomenon to their peers. The effective\nEL prescriptions allow us to crack Zendo, a board game simulating the\nscientific endeavor. This success comes with a fundamental insight: an\nartificial scientist must develop its own interpretation of the language used\nto explain its findings. This perspective then leads us to see modern\nmultimodal models as interpreters, and to devise a new way to build\ninterpretable and cost-effective CLIP-like models: by coupling two unimodal\nmodels using little multimodal data and no further training. Finally, we\ndiscuss what ChatGPT and its siblings are still missing to become artificial\nscientists, and introduce Odeen, a benchmark about interpreting explanations\nthat sees LLMs going no further than random chance while being instead fully\nsolved by humans.\n", "link": "http://arxiv.org/abs/2411.11672v1", "date": "2024-11-18", "relevancy": 2.2416, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5736}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Scientific%20Discovery&body=Title%3A%20Artificial%20Scientific%20Discovery%0AAuthor%3A%20Antonio%20Norelli%0AAbstract%3A%20%20%20Rooted%20in%20the%20explosion%20of%20deep%20learning%20over%20the%20past%20decade%2C%20this%20thesis%0Aspans%20from%20AlphaGo%20to%20ChatGPT%20to%20empirically%20examine%20the%20fundamental%20concepts%0Aneeded%20to%20realize%20the%20vision%20of%20an%20artificial%20scientist%3A%20a%20machine%20with%20the%0Acapacity%20to%20autonomously%20generate%20original%20research%20and%20contribute%20to%20the%0Aexpansion%20of%20human%20knowledge.%20The%20investigation%20begins%20with%20%7B%5Csc%20Olivaw%7D%2C%20an%0AAlphaGo%20Zero-like%20agent%20that%20discovers%20Othello%20knowledge%20from%20scratch%20but%20is%0Aunable%20to%20communicate%20it.%20This%20realization%20leads%20to%20the%20development%20of%20the%0AExplanatory%20Learning%20%28EL%29%20framework%2C%20a%20formalization%20of%20the%20problem%20faced%20by%20a%0Ascientist%20when%20trying%20to%20explain%20a%20new%20phenomenon%20to%20their%20peers.%20The%20effective%0AEL%20prescriptions%20allow%20us%20to%20crack%20Zendo%2C%20a%20board%20game%20simulating%20the%0Ascientific%20endeavor.%20This%20success%20comes%20with%20a%20fundamental%20insight%3A%20an%0Aartificial%20scientist%20must%20develop%20its%20own%20interpretation%20of%20the%20language%20used%0Ato%20explain%20its%20findings.%20This%20perspective%20then%20leads%20us%20to%20see%20modern%0Amultimodal%20models%20as%20interpreters%2C%20and%20to%20devise%20a%20new%20way%20to%20build%0Ainterpretable%20and%20cost-effective%20CLIP-like%20models%3A%20by%20coupling%20two%20unimodal%0Amodels%20using%20little%20multimodal%20data%20and%20no%20further%20training.%20Finally%2C%20we%0Adiscuss%20what%20ChatGPT%20and%20its%20siblings%20are%20still%20missing%20to%20become%20artificial%0Ascientists%2C%20and%20introduce%20Odeen%2C%20a%20benchmark%20about%20interpreting%20explanations%0Athat%20sees%20LLMs%20going%20no%20further%20than%20random%20chance%20while%20being%20instead%20fully%0Asolved%20by%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Scientific%2520Discovery%26entry.906535625%3DAntonio%2520Norelli%26entry.1292438233%3D%2520%2520Rooted%2520in%2520the%2520explosion%2520of%2520deep%2520learning%2520over%2520the%2520past%2520decade%252C%2520this%2520thesis%250Aspans%2520from%2520AlphaGo%2520to%2520ChatGPT%2520to%2520empirically%2520examine%2520the%2520fundamental%2520concepts%250Aneeded%2520to%2520realize%2520the%2520vision%2520of%2520an%2520artificial%2520scientist%253A%2520a%2520machine%2520with%2520the%250Acapacity%2520to%2520autonomously%2520generate%2520original%2520research%2520and%2520contribute%2520to%2520the%250Aexpansion%2520of%2520human%2520knowledge.%2520The%2520investigation%2520begins%2520with%2520%257B%255Csc%2520Olivaw%257D%252C%2520an%250AAlphaGo%2520Zero-like%2520agent%2520that%2520discovers%2520Othello%2520knowledge%2520from%2520scratch%2520but%2520is%250Aunable%2520to%2520communicate%2520it.%2520This%2520realization%2520leads%2520to%2520the%2520development%2520of%2520the%250AExplanatory%2520Learning%2520%2528EL%2529%2520framework%252C%2520a%2520formalization%2520of%2520the%2520problem%2520faced%2520by%2520a%250Ascientist%2520when%2520trying%2520to%2520explain%2520a%2520new%2520phenomenon%2520to%2520their%2520peers.%2520The%2520effective%250AEL%2520prescriptions%2520allow%2520us%2520to%2520crack%2520Zendo%252C%2520a%2520board%2520game%2520simulating%2520the%250Ascientific%2520endeavor.%2520This%2520success%2520comes%2520with%2520a%2520fundamental%2520insight%253A%2520an%250Aartificial%2520scientist%2520must%2520develop%2520its%2520own%2520interpretation%2520of%2520the%2520language%2520used%250Ato%2520explain%2520its%2520findings.%2520This%2520perspective%2520then%2520leads%2520us%2520to%2520see%2520modern%250Amultimodal%2520models%2520as%2520interpreters%252C%2520and%2520to%2520devise%2520a%2520new%2520way%2520to%2520build%250Ainterpretable%2520and%2520cost-effective%2520CLIP-like%2520models%253A%2520by%2520coupling%2520two%2520unimodal%250Amodels%2520using%2520little%2520multimodal%2520data%2520and%2520no%2520further%2520training.%2520Finally%252C%2520we%250Adiscuss%2520what%2520ChatGPT%2520and%2520its%2520siblings%2520are%2520still%2520missing%2520to%2520become%2520artificial%250Ascientists%252C%2520and%2520introduce%2520Odeen%252C%2520a%2520benchmark%2520about%2520interpreting%2520explanations%250Athat%2520sees%2520LLMs%2520going%2520no%2520further%2520than%2520random%2520chance%2520while%2520being%2520instead%2520fully%250Asolved%2520by%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Scientific%20Discovery&entry.906535625=Antonio%20Norelli&entry.1292438233=%20%20Rooted%20in%20the%20explosion%20of%20deep%20learning%20over%20the%20past%20decade%2C%20this%20thesis%0Aspans%20from%20AlphaGo%20to%20ChatGPT%20to%20empirically%20examine%20the%20fundamental%20concepts%0Aneeded%20to%20realize%20the%20vision%20of%20an%20artificial%20scientist%3A%20a%20machine%20with%20the%0Acapacity%20to%20autonomously%20generate%20original%20research%20and%20contribute%20to%20the%0Aexpansion%20of%20human%20knowledge.%20The%20investigation%20begins%20with%20%7B%5Csc%20Olivaw%7D%2C%20an%0AAlphaGo%20Zero-like%20agent%20that%20discovers%20Othello%20knowledge%20from%20scratch%20but%20is%0Aunable%20to%20communicate%20it.%20This%20realization%20leads%20to%20the%20development%20of%20the%0AExplanatory%20Learning%20%28EL%29%20framework%2C%20a%20formalization%20of%20the%20problem%20faced%20by%20a%0Ascientist%20when%20trying%20to%20explain%20a%20new%20phenomenon%20to%20their%20peers.%20The%20effective%0AEL%20prescriptions%20allow%20us%20to%20crack%20Zendo%2C%20a%20board%20game%20simulating%20the%0Ascientific%20endeavor.%20This%20success%20comes%20with%20a%20fundamental%20insight%3A%20an%0Aartificial%20scientist%20must%20develop%20its%20own%20interpretation%20of%20the%20language%20used%0Ato%20explain%20its%20findings.%20This%20perspective%20then%20leads%20us%20to%20see%20modern%0Amultimodal%20models%20as%20interpreters%2C%20and%20to%20devise%20a%20new%20way%20to%20build%0Ainterpretable%20and%20cost-effective%20CLIP-like%20models%3A%20by%20coupling%20two%20unimodal%0Amodels%20using%20little%20multimodal%20data%20and%20no%20further%20training.%20Finally%2C%20we%0Adiscuss%20what%20ChatGPT%20and%20its%20siblings%20are%20still%20missing%20to%20become%20artificial%0Ascientists%2C%20and%20introduce%20Odeen%2C%20a%20benchmark%20about%20interpreting%20explanations%0Athat%20sees%20LLMs%20going%20no%20further%20than%20random%20chance%20while%20being%20instead%20fully%0Asolved%20by%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11672v1&entry.124074799=Read"},
{"title": "Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical\n  Image Fusion", "author": "Meng Zhou and Yuxuan Zhang and Xiaolan Xu and Jiayi Wang and Farzad Khalvati", "abstract": "  Multimodal medical image fusion is a crucial task that combines complementary\ninformation from different imaging modalities into a unified representation,\nthereby enhancing diagnostic accuracy and treatment planning. While deep\nlearning methods, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly advanced fusion performance, some of the\nexisting CNN-based methods fall short in capturing fine-grained multiscale and\nedge features, leading to suboptimal feature integration. Transformer-based\nmodels, on the other hand, are computationally intensive in both the training\nand fusion stages, making them impractical for real-time clinical use.\nMoreover, the clinical application of fused images remains unexplored. In this\npaper, we propose a novel CNN-based architecture that addresses these\nlimitations by introducing a Dilated Residual Attention Network Module for\neffective multiscale feature extraction, coupled with a gradient operator to\nenhance edge detail learning. To ensure fast and efficient fusion, we present a\nparameter-free fusion strategy based on the weighted nuclear norm of softmax,\nwhich requires no additional computations during training or inference.\nExtensive experiments, including a downstream brain tumor classification task,\ndemonstrate that our approach outperforms various baseline methods in terms of\nvisual quality, texture preservation, and fusion speed, making it a possible\npractical solution for real-world clinical applications. The code will be\nreleased at https://github.com/simonZhou86/en_dran.\n", "link": "http://arxiv.org/abs/2411.11799v1", "date": "2024-11-18", "relevancy": 2.2397, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5616}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5612}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Enhanced%20Dilated%20Residual%20Attention%20Network%20for%20Multimodal%20Medical%0A%20%20Image%20Fusion&body=Title%3A%20Edge-Enhanced%20Dilated%20Residual%20Attention%20Network%20for%20Multimodal%20Medical%0A%20%20Image%20Fusion%0AAuthor%3A%20Meng%20Zhou%20and%20Yuxuan%20Zhang%20and%20Xiaolan%20Xu%20and%20Jiayi%20Wang%20and%20Farzad%20Khalvati%0AAbstract%3A%20%20%20Multimodal%20medical%20image%20fusion%20is%20a%20crucial%20task%20that%20combines%20complementary%0Ainformation%20from%20different%20imaging%20modalities%20into%20a%20unified%20representation%2C%0Athereby%20enhancing%20diagnostic%20accuracy%20and%20treatment%20planning.%20While%20deep%0Alearning%20methods%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0ATransformers%2C%20have%20significantly%20advanced%20fusion%20performance%2C%20some%20of%20the%0Aexisting%20CNN-based%20methods%20fall%20short%20in%20capturing%20fine-grained%20multiscale%20and%0Aedge%20features%2C%20leading%20to%20suboptimal%20feature%20integration.%20Transformer-based%0Amodels%2C%20on%20the%20other%20hand%2C%20are%20computationally%20intensive%20in%20both%20the%20training%0Aand%20fusion%20stages%2C%20making%20them%20impractical%20for%20real-time%20clinical%20use.%0AMoreover%2C%20the%20clinical%20application%20of%20fused%20images%20remains%20unexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20CNN-based%20architecture%20that%20addresses%20these%0Alimitations%20by%20introducing%20a%20Dilated%20Residual%20Attention%20Network%20Module%20for%0Aeffective%20multiscale%20feature%20extraction%2C%20coupled%20with%20a%20gradient%20operator%20to%0Aenhance%20edge%20detail%20learning.%20To%20ensure%20fast%20and%20efficient%20fusion%2C%20we%20present%20a%0Aparameter-free%20fusion%20strategy%20based%20on%20the%20weighted%20nuclear%20norm%20of%20softmax%2C%0Awhich%20requires%20no%20additional%20computations%20during%20training%20or%20inference.%0AExtensive%20experiments%2C%20including%20a%20downstream%20brain%20tumor%20classification%20task%2C%0Ademonstrate%20that%20our%20approach%20outperforms%20various%20baseline%20methods%20in%20terms%20of%0Avisual%20quality%2C%20texture%20preservation%2C%20and%20fusion%20speed%2C%20making%20it%20a%20possible%0Apractical%20solution%20for%20real-world%20clinical%20applications.%20The%20code%20will%20be%0Areleased%20at%20https%3A//github.com/simonZhou86/en_dran.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Enhanced%2520Dilated%2520Residual%2520Attention%2520Network%2520for%2520Multimodal%2520Medical%250A%2520%2520Image%2520Fusion%26entry.906535625%3DMeng%2520Zhou%2520and%2520Yuxuan%2520Zhang%2520and%2520Xiaolan%2520Xu%2520and%2520Jiayi%2520Wang%2520and%2520Farzad%2520Khalvati%26entry.1292438233%3D%2520%2520Multimodal%2520medical%2520image%2520fusion%2520is%2520a%2520crucial%2520task%2520that%2520combines%2520complementary%250Ainformation%2520from%2520different%2520imaging%2520modalities%2520into%2520a%2520unified%2520representation%252C%250Athereby%2520enhancing%2520diagnostic%2520accuracy%2520and%2520treatment%2520planning.%2520While%2520deep%250Alearning%2520methods%252C%2520particularly%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%250ATransformers%252C%2520have%2520significantly%2520advanced%2520fusion%2520performance%252C%2520some%2520of%2520the%250Aexisting%2520CNN-based%2520methods%2520fall%2520short%2520in%2520capturing%2520fine-grained%2520multiscale%2520and%250Aedge%2520features%252C%2520leading%2520to%2520suboptimal%2520feature%2520integration.%2520Transformer-based%250Amodels%252C%2520on%2520the%2520other%2520hand%252C%2520are%2520computationally%2520intensive%2520in%2520both%2520the%2520training%250Aand%2520fusion%2520stages%252C%2520making%2520them%2520impractical%2520for%2520real-time%2520clinical%2520use.%250AMoreover%252C%2520the%2520clinical%2520application%2520of%2520fused%2520images%2520remains%2520unexplored.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520CNN-based%2520architecture%2520that%2520addresses%2520these%250Alimitations%2520by%2520introducing%2520a%2520Dilated%2520Residual%2520Attention%2520Network%2520Module%2520for%250Aeffective%2520multiscale%2520feature%2520extraction%252C%2520coupled%2520with%2520a%2520gradient%2520operator%2520to%250Aenhance%2520edge%2520detail%2520learning.%2520To%2520ensure%2520fast%2520and%2520efficient%2520fusion%252C%2520we%2520present%2520a%250Aparameter-free%2520fusion%2520strategy%2520based%2520on%2520the%2520weighted%2520nuclear%2520norm%2520of%2520softmax%252C%250Awhich%2520requires%2520no%2520additional%2520computations%2520during%2520training%2520or%2520inference.%250AExtensive%2520experiments%252C%2520including%2520a%2520downstream%2520brain%2520tumor%2520classification%2520task%252C%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520various%2520baseline%2520methods%2520in%2520terms%2520of%250Avisual%2520quality%252C%2520texture%2520preservation%252C%2520and%2520fusion%2520speed%252C%2520making%2520it%2520a%2520possible%250Apractical%2520solution%2520for%2520real-world%2520clinical%2520applications.%2520The%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/simonZhou86/en_dran.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Enhanced%20Dilated%20Residual%20Attention%20Network%20for%20Multimodal%20Medical%0A%20%20Image%20Fusion&entry.906535625=Meng%20Zhou%20and%20Yuxuan%20Zhang%20and%20Xiaolan%20Xu%20and%20Jiayi%20Wang%20and%20Farzad%20Khalvati&entry.1292438233=%20%20Multimodal%20medical%20image%20fusion%20is%20a%20crucial%20task%20that%20combines%20complementary%0Ainformation%20from%20different%20imaging%20modalities%20into%20a%20unified%20representation%2C%0Athereby%20enhancing%20diagnostic%20accuracy%20and%20treatment%20planning.%20While%20deep%0Alearning%20methods%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%0ATransformers%2C%20have%20significantly%20advanced%20fusion%20performance%2C%20some%20of%20the%0Aexisting%20CNN-based%20methods%20fall%20short%20in%20capturing%20fine-grained%20multiscale%20and%0Aedge%20features%2C%20leading%20to%20suboptimal%20feature%20integration.%20Transformer-based%0Amodels%2C%20on%20the%20other%20hand%2C%20are%20computationally%20intensive%20in%20both%20the%20training%0Aand%20fusion%20stages%2C%20making%20them%20impractical%20for%20real-time%20clinical%20use.%0AMoreover%2C%20the%20clinical%20application%20of%20fused%20images%20remains%20unexplored.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20CNN-based%20architecture%20that%20addresses%20these%0Alimitations%20by%20introducing%20a%20Dilated%20Residual%20Attention%20Network%20Module%20for%0Aeffective%20multiscale%20feature%20extraction%2C%20coupled%20with%20a%20gradient%20operator%20to%0Aenhance%20edge%20detail%20learning.%20To%20ensure%20fast%20and%20efficient%20fusion%2C%20we%20present%20a%0Aparameter-free%20fusion%20strategy%20based%20on%20the%20weighted%20nuclear%20norm%20of%20softmax%2C%0Awhich%20requires%20no%20additional%20computations%20during%20training%20or%20inference.%0AExtensive%20experiments%2C%20including%20a%20downstream%20brain%20tumor%20classification%20task%2C%0Ademonstrate%20that%20our%20approach%20outperforms%20various%20baseline%20methods%20in%20terms%20of%0Avisual%20quality%2C%20texture%20preservation%2C%20and%20fusion%20speed%2C%20making%20it%20a%20possible%0Apractical%20solution%20for%20real-world%20clinical%20applications.%20The%20code%20will%20be%0Areleased%20at%20https%3A//github.com/simonZhou86/en_dran.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11799v1&entry.124074799=Read"},
{"title": "Eidos: Efficient, Imperceptible Adversarial 3D Point Clouds", "author": "Hanwei Zhang and Luo Cheng and Qisong He and Wei Huang and Renjue Li and Ronan Sicre and Xiaowei Huang and Holger Hermanns and Lijun Zhang", "abstract": "  Classification of 3D point clouds is a challenging machine learning (ML) task\nwith important real-world applications in a spectrum from autonomous driving\nand robot-assisted surgery to earth observation from low orbit. As with other\nML tasks, classification models are notoriously brittle in the presence of\nadversarial attacks. These are rooted in imperceptible changes to inputs with\nthe effect that a seemingly well-trained model ends up misclassifying the\ninput. This paper adds to the understanding of adversarial attacks by\npresenting Eidos, a framework providing Efficient Imperceptible aDversarial\nattacks on 3D pOint cloudS. Eidos supports a diverse set of imperceptibility\nmetrics. It employs an iterative, two-step procedure to identify optimal\nadversarial examples, thereby enabling a runtime-imperceptibility trade-off. We\nprovide empirical evidence relative to several popular 3D point cloud\nclassification models and several established 3D attack methods, showing Eidos'\nsuperiority with respect to efficiency as well as imperceptibility.\n", "link": "http://arxiv.org/abs/2405.14210v2", "date": "2024-11-18", "relevancy": 2.2276, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5875}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5659}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eidos%3A%20Efficient%2C%20Imperceptible%20Adversarial%203D%20Point%20Clouds&body=Title%3A%20Eidos%3A%20Efficient%2C%20Imperceptible%20Adversarial%203D%20Point%20Clouds%0AAuthor%3A%20Hanwei%20Zhang%20and%20Luo%20Cheng%20and%20Qisong%20He%20and%20Wei%20Huang%20and%20Renjue%20Li%20and%20Ronan%20Sicre%20and%20Xiaowei%20Huang%20and%20Holger%20Hermanns%20and%20Lijun%20Zhang%0AAbstract%3A%20%20%20Classification%20of%203D%20point%20clouds%20is%20a%20challenging%20machine%20learning%20%28ML%29%20task%0Awith%20important%20real-world%20applications%20in%20a%20spectrum%20from%20autonomous%20driving%0Aand%20robot-assisted%20surgery%20to%20earth%20observation%20from%20low%20orbit.%20As%20with%20other%0AML%20tasks%2C%20classification%20models%20are%20notoriously%20brittle%20in%20the%20presence%20of%0Aadversarial%20attacks.%20These%20are%20rooted%20in%20imperceptible%20changes%20to%20inputs%20with%0Athe%20effect%20that%20a%20seemingly%20well-trained%20model%20ends%20up%20misclassifying%20the%0Ainput.%20This%20paper%20adds%20to%20the%20understanding%20of%20adversarial%20attacks%20by%0Apresenting%20Eidos%2C%20a%20framework%20providing%20Efficient%20Imperceptible%20aDversarial%0Aattacks%20on%203D%20pOint%20cloudS.%20Eidos%20supports%20a%20diverse%20set%20of%20imperceptibility%0Ametrics.%20It%20employs%20an%20iterative%2C%20two-step%20procedure%20to%20identify%20optimal%0Aadversarial%20examples%2C%20thereby%20enabling%20a%20runtime-imperceptibility%20trade-off.%20We%0Aprovide%20empirical%20evidence%20relative%20to%20several%20popular%203D%20point%20cloud%0Aclassification%20models%20and%20several%20established%203D%20attack%20methods%2C%20showing%20Eidos%27%0Asuperiority%20with%20respect%20to%20efficiency%20as%20well%20as%20imperceptibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEidos%253A%2520Efficient%252C%2520Imperceptible%2520Adversarial%25203D%2520Point%2520Clouds%26entry.906535625%3DHanwei%2520Zhang%2520and%2520Luo%2520Cheng%2520and%2520Qisong%2520He%2520and%2520Wei%2520Huang%2520and%2520Renjue%2520Li%2520and%2520Ronan%2520Sicre%2520and%2520Xiaowei%2520Huang%2520and%2520Holger%2520Hermanns%2520and%2520Lijun%2520Zhang%26entry.1292438233%3D%2520%2520Classification%2520of%25203D%2520point%2520clouds%2520is%2520a%2520challenging%2520machine%2520learning%2520%2528ML%2529%2520task%250Awith%2520important%2520real-world%2520applications%2520in%2520a%2520spectrum%2520from%2520autonomous%2520driving%250Aand%2520robot-assisted%2520surgery%2520to%2520earth%2520observation%2520from%2520low%2520orbit.%2520As%2520with%2520other%250AML%2520tasks%252C%2520classification%2520models%2520are%2520notoriously%2520brittle%2520in%2520the%2520presence%2520of%250Aadversarial%2520attacks.%2520These%2520are%2520rooted%2520in%2520imperceptible%2520changes%2520to%2520inputs%2520with%250Athe%2520effect%2520that%2520a%2520seemingly%2520well-trained%2520model%2520ends%2520up%2520misclassifying%2520the%250Ainput.%2520This%2520paper%2520adds%2520to%2520the%2520understanding%2520of%2520adversarial%2520attacks%2520by%250Apresenting%2520Eidos%252C%2520a%2520framework%2520providing%2520Efficient%2520Imperceptible%2520aDversarial%250Aattacks%2520on%25203D%2520pOint%2520cloudS.%2520Eidos%2520supports%2520a%2520diverse%2520set%2520of%2520imperceptibility%250Ametrics.%2520It%2520employs%2520an%2520iterative%252C%2520two-step%2520procedure%2520to%2520identify%2520optimal%250Aadversarial%2520examples%252C%2520thereby%2520enabling%2520a%2520runtime-imperceptibility%2520trade-off.%2520We%250Aprovide%2520empirical%2520evidence%2520relative%2520to%2520several%2520popular%25203D%2520point%2520cloud%250Aclassification%2520models%2520and%2520several%2520established%25203D%2520attack%2520methods%252C%2520showing%2520Eidos%2527%250Asuperiority%2520with%2520respect%2520to%2520efficiency%2520as%2520well%2520as%2520imperceptibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eidos%3A%20Efficient%2C%20Imperceptible%20Adversarial%203D%20Point%20Clouds&entry.906535625=Hanwei%20Zhang%20and%20Luo%20Cheng%20and%20Qisong%20He%20and%20Wei%20Huang%20and%20Renjue%20Li%20and%20Ronan%20Sicre%20and%20Xiaowei%20Huang%20and%20Holger%20Hermanns%20and%20Lijun%20Zhang&entry.1292438233=%20%20Classification%20of%203D%20point%20clouds%20is%20a%20challenging%20machine%20learning%20%28ML%29%20task%0Awith%20important%20real-world%20applications%20in%20a%20spectrum%20from%20autonomous%20driving%0Aand%20robot-assisted%20surgery%20to%20earth%20observation%20from%20low%20orbit.%20As%20with%20other%0AML%20tasks%2C%20classification%20models%20are%20notoriously%20brittle%20in%20the%20presence%20of%0Aadversarial%20attacks.%20These%20are%20rooted%20in%20imperceptible%20changes%20to%20inputs%20with%0Athe%20effect%20that%20a%20seemingly%20well-trained%20model%20ends%20up%20misclassifying%20the%0Ainput.%20This%20paper%20adds%20to%20the%20understanding%20of%20adversarial%20attacks%20by%0Apresenting%20Eidos%2C%20a%20framework%20providing%20Efficient%20Imperceptible%20aDversarial%0Aattacks%20on%203D%20pOint%20cloudS.%20Eidos%20supports%20a%20diverse%20set%20of%20imperceptibility%0Ametrics.%20It%20employs%20an%20iterative%2C%20two-step%20procedure%20to%20identify%20optimal%0Aadversarial%20examples%2C%20thereby%20enabling%20a%20runtime-imperceptibility%20trade-off.%20We%0Aprovide%20empirical%20evidence%20relative%20to%20several%20popular%203D%20point%20cloud%0Aclassification%20models%20and%20several%20established%203D%20attack%20methods%2C%20showing%20Eidos%27%0Asuperiority%20with%20respect%20to%20efficiency%20as%20well%20as%20imperceptibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14210v2&entry.124074799=Read"},
{"title": "The Power of Many: Multi-Agent Multimodal Models for Cultural Image\n  Captioning", "author": "Longju Bai and Angana Borah and Oana Ignat and Rada Mihalcea", "abstract": "  Large Multimodal Models (LMMs) exhibit impressive performance across various\nmultimodal tasks. However, their effectiveness in cross-cultural contexts\nremains limited due to the predominantly Western-centric nature of most data\nand models. Conversely, multi-agent models have shown significant capability in\nsolving complex tasks. Our study evaluates the collective performance of LMMs\nin a multi-agent interaction setting for the novel task of cultural image\ncaptioning. Our contributions are as follows: (1) We introduce MosAIC, a\nMulti-Agent framework to enhance cross-cultural Image Captioning using LMMs\nwith distinct cultural personas; (2) We provide a dataset of culturally\nenriched image captions in English for images from China, India, and Romania\nacross three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable\nmetric for evaluating cultural information within image captions; and (4) We\nshow that the multi-agent interaction outperforms single-agent models across\ndifferent metrics, and offer valuable insights for future research. Our dataset\nand models can be accessed at https://github.com/MichiganNLP/MosAIC.\n", "link": "http://arxiv.org/abs/2411.11758v1", "date": "2024-11-18", "relevancy": 2.2212, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5717}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Power%20of%20Many%3A%20Multi-Agent%20Multimodal%20Models%20for%20Cultural%20Image%0A%20%20Captioning&body=Title%3A%20The%20Power%20of%20Many%3A%20Multi-Agent%20Multimodal%20Models%20for%20Cultural%20Image%0A%20%20Captioning%0AAuthor%3A%20Longju%20Bai%20and%20Angana%20Borah%20and%20Oana%20Ignat%20and%20Rada%20Mihalcea%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20exhibit%20impressive%20performance%20across%20various%0Amultimodal%20tasks.%20However%2C%20their%20effectiveness%20in%20cross-cultural%20contexts%0Aremains%20limited%20due%20to%20the%20predominantly%20Western-centric%20nature%20of%20most%20data%0Aand%20models.%20Conversely%2C%20multi-agent%20models%20have%20shown%20significant%20capability%20in%0Asolving%20complex%20tasks.%20Our%20study%20evaluates%20the%20collective%20performance%20of%20LMMs%0Ain%20a%20multi-agent%20interaction%20setting%20for%20the%20novel%20task%20of%20cultural%20image%0Acaptioning.%20Our%20contributions%20are%20as%20follows%3A%20%281%29%20We%20introduce%20MosAIC%2C%20a%0AMulti-Agent%20framework%20to%20enhance%20cross-cultural%20Image%20Captioning%20using%20LMMs%0Awith%20distinct%20cultural%20personas%3B%20%282%29%20We%20provide%20a%20dataset%20of%20culturally%0Aenriched%20image%20captions%20in%20English%20for%20images%20from%20China%2C%20India%2C%20and%20Romania%0Aacross%20three%20datasets%3A%20GeoDE%2C%20GD-VCR%2C%20CVQA%3B%20%283%29%20We%20propose%20a%20culture-adaptable%0Ametric%20for%20evaluating%20cultural%20information%20within%20image%20captions%3B%20and%20%284%29%20We%0Ashow%20that%20the%20multi-agent%20interaction%20outperforms%20single-agent%20models%20across%0Adifferent%20metrics%2C%20and%20offer%20valuable%20insights%20for%20future%20research.%20Our%20dataset%0Aand%20models%20can%20be%20accessed%20at%20https%3A//github.com/MichiganNLP/MosAIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Power%2520of%2520Many%253A%2520Multi-Agent%2520Multimodal%2520Models%2520for%2520Cultural%2520Image%250A%2520%2520Captioning%26entry.906535625%3DLongju%2520Bai%2520and%2520Angana%2520Borah%2520and%2520Oana%2520Ignat%2520and%2520Rada%2520Mihalcea%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520exhibit%2520impressive%2520performance%2520across%2520various%250Amultimodal%2520tasks.%2520However%252C%2520their%2520effectiveness%2520in%2520cross-cultural%2520contexts%250Aremains%2520limited%2520due%2520to%2520the%2520predominantly%2520Western-centric%2520nature%2520of%2520most%2520data%250Aand%2520models.%2520Conversely%252C%2520multi-agent%2520models%2520have%2520shown%2520significant%2520capability%2520in%250Asolving%2520complex%2520tasks.%2520Our%2520study%2520evaluates%2520the%2520collective%2520performance%2520of%2520LMMs%250Ain%2520a%2520multi-agent%2520interaction%2520setting%2520for%2520the%2520novel%2520task%2520of%2520cultural%2520image%250Acaptioning.%2520Our%2520contributions%2520are%2520as%2520follows%253A%2520%25281%2529%2520We%2520introduce%2520MosAIC%252C%2520a%250AMulti-Agent%2520framework%2520to%2520enhance%2520cross-cultural%2520Image%2520Captioning%2520using%2520LMMs%250Awith%2520distinct%2520cultural%2520personas%253B%2520%25282%2529%2520We%2520provide%2520a%2520dataset%2520of%2520culturally%250Aenriched%2520image%2520captions%2520in%2520English%2520for%2520images%2520from%2520China%252C%2520India%252C%2520and%2520Romania%250Aacross%2520three%2520datasets%253A%2520GeoDE%252C%2520GD-VCR%252C%2520CVQA%253B%2520%25283%2529%2520We%2520propose%2520a%2520culture-adaptable%250Ametric%2520for%2520evaluating%2520cultural%2520information%2520within%2520image%2520captions%253B%2520and%2520%25284%2529%2520We%250Ashow%2520that%2520the%2520multi-agent%2520interaction%2520outperforms%2520single-agent%2520models%2520across%250Adifferent%2520metrics%252C%2520and%2520offer%2520valuable%2520insights%2520for%2520future%2520research.%2520Our%2520dataset%250Aand%2520models%2520can%2520be%2520accessed%2520at%2520https%253A//github.com/MichiganNLP/MosAIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Power%20of%20Many%3A%20Multi-Agent%20Multimodal%20Models%20for%20Cultural%20Image%0A%20%20Captioning&entry.906535625=Longju%20Bai%20and%20Angana%20Borah%20and%20Oana%20Ignat%20and%20Rada%20Mihalcea&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20exhibit%20impressive%20performance%20across%20various%0Amultimodal%20tasks.%20However%2C%20their%20effectiveness%20in%20cross-cultural%20contexts%0Aremains%20limited%20due%20to%20the%20predominantly%20Western-centric%20nature%20of%20most%20data%0Aand%20models.%20Conversely%2C%20multi-agent%20models%20have%20shown%20significant%20capability%20in%0Asolving%20complex%20tasks.%20Our%20study%20evaluates%20the%20collective%20performance%20of%20LMMs%0Ain%20a%20multi-agent%20interaction%20setting%20for%20the%20novel%20task%20of%20cultural%20image%0Acaptioning.%20Our%20contributions%20are%20as%20follows%3A%20%281%29%20We%20introduce%20MosAIC%2C%20a%0AMulti-Agent%20framework%20to%20enhance%20cross-cultural%20Image%20Captioning%20using%20LMMs%0Awith%20distinct%20cultural%20personas%3B%20%282%29%20We%20provide%20a%20dataset%20of%20culturally%0Aenriched%20image%20captions%20in%20English%20for%20images%20from%20China%2C%20India%2C%20and%20Romania%0Aacross%20three%20datasets%3A%20GeoDE%2C%20GD-VCR%2C%20CVQA%3B%20%283%29%20We%20propose%20a%20culture-adaptable%0Ametric%20for%20evaluating%20cultural%20information%20within%20image%20captions%3B%20and%20%284%29%20We%0Ashow%20that%20the%20multi-agent%20interaction%20outperforms%20single-agent%20models%20across%0Adifferent%20metrics%2C%20and%20offer%20valuable%20insights%20for%20future%20research.%20Our%20dataset%0Aand%20models%20can%20be%20accessed%20at%20https%3A//github.com/MichiganNLP/MosAIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11758v1&entry.124074799=Read"},
{"title": "Alien Recombination: Exploring Concept Blends Beyond Human Cognitive\n  Availability in Visual Art", "author": "Alejandro Hernandez and Levin Brinkmann and Ignacio Serna and Nasim Rahaman and Hassan Abu Alhaija and Hiromu Yakura and Mar Canet Sola and Bernhard Sch\u00f6lkopf and Iyad Rahwan", "abstract": "  While AI models have demonstrated remarkable capabilities in constrained\ndomains like game strategy, their potential for genuine creativity in\nopen-ended domains like art remains debated. We explore this question by\nexamining how AI can transcend human cognitive limitations in visual art\ncreation. Our research hypothesizes that visual art contains a vast unexplored\nspace of conceptual combinations, constrained not by inherent incompatibility,\nbut by cognitive limitations imposed by artists' cultural, temporal,\ngeographical and social contexts.\n  To test this hypothesis, we present the Alien Recombination method, a novel\napproach utilizing fine-tuned large language models to identify and generate\nconcept combinations that lie beyond human cognitive availability. The system\nmodels and deliberately counteracts human availability bias, the tendency to\nrely on immediately accessible examples, to discover novel artistic\ncombinations.\n  This system not only produces combinations that have never been attempted\nbefore within our dataset but also identifies and generates combinations that\nare cognitively unavailable to all artists in the domain. Furthermore, we\ntranslate these combinations into visual representations, enabling the\nexploration of subjective perceptions of novelty. Our findings suggest that\ncognitive unavailability is a promising metric for optimizing artistic novelty,\noutperforming merely temperature scaling without additional evaluation\ncriteria. This approach uses generative models to connect previously\nunconnected ideas, providing new insight into the potential of framing\nAI-driven creativity as a combinatorial problem.\n", "link": "http://arxiv.org/abs/2411.11494v1", "date": "2024-11-18", "relevancy": 2.2199, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6324}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alien%20Recombination%3A%20Exploring%20Concept%20Blends%20Beyond%20Human%20Cognitive%0A%20%20Availability%20in%20Visual%20Art&body=Title%3A%20Alien%20Recombination%3A%20Exploring%20Concept%20Blends%20Beyond%20Human%20Cognitive%0A%20%20Availability%20in%20Visual%20Art%0AAuthor%3A%20Alejandro%20Hernandez%20and%20Levin%20Brinkmann%20and%20Ignacio%20Serna%20and%20Nasim%20Rahaman%20and%20Hassan%20Abu%20Alhaija%20and%20Hiromu%20Yakura%20and%20Mar%20Canet%20Sola%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Iyad%20Rahwan%0AAbstract%3A%20%20%20While%20AI%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20constrained%0Adomains%20like%20game%20strategy%2C%20their%20potential%20for%20genuine%20creativity%20in%0Aopen-ended%20domains%20like%20art%20remains%20debated.%20We%20explore%20this%20question%20by%0Aexamining%20how%20AI%20can%20transcend%20human%20cognitive%20limitations%20in%20visual%20art%0Acreation.%20Our%20research%20hypothesizes%20that%20visual%20art%20contains%20a%20vast%20unexplored%0Aspace%20of%20conceptual%20combinations%2C%20constrained%20not%20by%20inherent%20incompatibility%2C%0Abut%20by%20cognitive%20limitations%20imposed%20by%20artists%27%20cultural%2C%20temporal%2C%0Ageographical%20and%20social%20contexts.%0A%20%20To%20test%20this%20hypothesis%2C%20we%20present%20the%20Alien%20Recombination%20method%2C%20a%20novel%0Aapproach%20utilizing%20fine-tuned%20large%20language%20models%20to%20identify%20and%20generate%0Aconcept%20combinations%20that%20lie%20beyond%20human%20cognitive%20availability.%20The%20system%0Amodels%20and%20deliberately%20counteracts%20human%20availability%20bias%2C%20the%20tendency%20to%0Arely%20on%20immediately%20accessible%20examples%2C%20to%20discover%20novel%20artistic%0Acombinations.%0A%20%20This%20system%20not%20only%20produces%20combinations%20that%20have%20never%20been%20attempted%0Abefore%20within%20our%20dataset%20but%20also%20identifies%20and%20generates%20combinations%20that%0Aare%20cognitively%20unavailable%20to%20all%20artists%20in%20the%20domain.%20Furthermore%2C%20we%0Atranslate%20these%20combinations%20into%20visual%20representations%2C%20enabling%20the%0Aexploration%20of%20subjective%20perceptions%20of%20novelty.%20Our%20findings%20suggest%20that%0Acognitive%20unavailability%20is%20a%20promising%20metric%20for%20optimizing%20artistic%20novelty%2C%0Aoutperforming%20merely%20temperature%20scaling%20without%20additional%20evaluation%0Acriteria.%20This%20approach%20uses%20generative%20models%20to%20connect%20previously%0Aunconnected%20ideas%2C%20providing%20new%20insight%20into%20the%20potential%20of%20framing%0AAI-driven%20creativity%20as%20a%20combinatorial%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlien%2520Recombination%253A%2520Exploring%2520Concept%2520Blends%2520Beyond%2520Human%2520Cognitive%250A%2520%2520Availability%2520in%2520Visual%2520Art%26entry.906535625%3DAlejandro%2520Hernandez%2520and%2520Levin%2520Brinkmann%2520and%2520Ignacio%2520Serna%2520and%2520Nasim%2520Rahaman%2520and%2520Hassan%2520Abu%2520Alhaija%2520and%2520Hiromu%2520Yakura%2520and%2520Mar%2520Canet%2520Sola%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Iyad%2520Rahwan%26entry.1292438233%3D%2520%2520While%2520AI%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520constrained%250Adomains%2520like%2520game%2520strategy%252C%2520their%2520potential%2520for%2520genuine%2520creativity%2520in%250Aopen-ended%2520domains%2520like%2520art%2520remains%2520debated.%2520We%2520explore%2520this%2520question%2520by%250Aexamining%2520how%2520AI%2520can%2520transcend%2520human%2520cognitive%2520limitations%2520in%2520visual%2520art%250Acreation.%2520Our%2520research%2520hypothesizes%2520that%2520visual%2520art%2520contains%2520a%2520vast%2520unexplored%250Aspace%2520of%2520conceptual%2520combinations%252C%2520constrained%2520not%2520by%2520inherent%2520incompatibility%252C%250Abut%2520by%2520cognitive%2520limitations%2520imposed%2520by%2520artists%2527%2520cultural%252C%2520temporal%252C%250Ageographical%2520and%2520social%2520contexts.%250A%2520%2520To%2520test%2520this%2520hypothesis%252C%2520we%2520present%2520the%2520Alien%2520Recombination%2520method%252C%2520a%2520novel%250Aapproach%2520utilizing%2520fine-tuned%2520large%2520language%2520models%2520to%2520identify%2520and%2520generate%250Aconcept%2520combinations%2520that%2520lie%2520beyond%2520human%2520cognitive%2520availability.%2520The%2520system%250Amodels%2520and%2520deliberately%2520counteracts%2520human%2520availability%2520bias%252C%2520the%2520tendency%2520to%250Arely%2520on%2520immediately%2520accessible%2520examples%252C%2520to%2520discover%2520novel%2520artistic%250Acombinations.%250A%2520%2520This%2520system%2520not%2520only%2520produces%2520combinations%2520that%2520have%2520never%2520been%2520attempted%250Abefore%2520within%2520our%2520dataset%2520but%2520also%2520identifies%2520and%2520generates%2520combinations%2520that%250Aare%2520cognitively%2520unavailable%2520to%2520all%2520artists%2520in%2520the%2520domain.%2520Furthermore%252C%2520we%250Atranslate%2520these%2520combinations%2520into%2520visual%2520representations%252C%2520enabling%2520the%250Aexploration%2520of%2520subjective%2520perceptions%2520of%2520novelty.%2520Our%2520findings%2520suggest%2520that%250Acognitive%2520unavailability%2520is%2520a%2520promising%2520metric%2520for%2520optimizing%2520artistic%2520novelty%252C%250Aoutperforming%2520merely%2520temperature%2520scaling%2520without%2520additional%2520evaluation%250Acriteria.%2520This%2520approach%2520uses%2520generative%2520models%2520to%2520connect%2520previously%250Aunconnected%2520ideas%252C%2520providing%2520new%2520insight%2520into%2520the%2520potential%2520of%2520framing%250AAI-driven%2520creativity%2520as%2520a%2520combinatorial%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alien%20Recombination%3A%20Exploring%20Concept%20Blends%20Beyond%20Human%20Cognitive%0A%20%20Availability%20in%20Visual%20Art&entry.906535625=Alejandro%20Hernandez%20and%20Levin%20Brinkmann%20and%20Ignacio%20Serna%20and%20Nasim%20Rahaman%20and%20Hassan%20Abu%20Alhaija%20and%20Hiromu%20Yakura%20and%20Mar%20Canet%20Sola%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Iyad%20Rahwan&entry.1292438233=%20%20While%20AI%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20constrained%0Adomains%20like%20game%20strategy%2C%20their%20potential%20for%20genuine%20creativity%20in%0Aopen-ended%20domains%20like%20art%20remains%20debated.%20We%20explore%20this%20question%20by%0Aexamining%20how%20AI%20can%20transcend%20human%20cognitive%20limitations%20in%20visual%20art%0Acreation.%20Our%20research%20hypothesizes%20that%20visual%20art%20contains%20a%20vast%20unexplored%0Aspace%20of%20conceptual%20combinations%2C%20constrained%20not%20by%20inherent%20incompatibility%2C%0Abut%20by%20cognitive%20limitations%20imposed%20by%20artists%27%20cultural%2C%20temporal%2C%0Ageographical%20and%20social%20contexts.%0A%20%20To%20test%20this%20hypothesis%2C%20we%20present%20the%20Alien%20Recombination%20method%2C%20a%20novel%0Aapproach%20utilizing%20fine-tuned%20large%20language%20models%20to%20identify%20and%20generate%0Aconcept%20combinations%20that%20lie%20beyond%20human%20cognitive%20availability.%20The%20system%0Amodels%20and%20deliberately%20counteracts%20human%20availability%20bias%2C%20the%20tendency%20to%0Arely%20on%20immediately%20accessible%20examples%2C%20to%20discover%20novel%20artistic%0Acombinations.%0A%20%20This%20system%20not%20only%20produces%20combinations%20that%20have%20never%20been%20attempted%0Abefore%20within%20our%20dataset%20but%20also%20identifies%20and%20generates%20combinations%20that%0Aare%20cognitively%20unavailable%20to%20all%20artists%20in%20the%20domain.%20Furthermore%2C%20we%0Atranslate%20these%20combinations%20into%20visual%20representations%2C%20enabling%20the%0Aexploration%20of%20subjective%20perceptions%20of%20novelty.%20Our%20findings%20suggest%20that%0Acognitive%20unavailability%20is%20a%20promising%20metric%20for%20optimizing%20artistic%20novelty%2C%0Aoutperforming%20merely%20temperature%20scaling%20without%20additional%20evaluation%0Acriteria.%20This%20approach%20uses%20generative%20models%20to%20connect%20previously%0Aunconnected%20ideas%2C%20providing%20new%20insight%20into%20the%20potential%20of%20framing%0AAI-driven%20creativity%20as%20a%20combinatorial%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11494v1&entry.124074799=Read"},
{"title": "MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation\n  Framework", "author": "Adrian Celaya and Evan Lim and Rachel Glenn and Brayden Mi and Alex Balsells and Dawid Schellingerhout and Tucker Netherton and Caroline Chung and Beatrice Riviere and David Fuentes", "abstract": "  Medical imaging segmentation is a highly active area of research, with deep\nlearning-based methods achieving state-of-the-art results in several\nbenchmarks. However, the lack of standardized tools for training, testing, and\nevaluating new methods makes the comparison of methods difficult. To address\nthis, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,\nmodular, and end-to-end medical imaging segmentation framework designed to\nfacilitate consistent training, testing, and evaluation of deep learning-based\nmedical imaging segmentation methods. MIST standardizes data analysis,\npreprocessing, and evaluation pipelines, accommodating multiple architectures\nand loss functions. This standardization ensures reproducible and fair\ncomparisons across different methods. We detail MIST's data format\nrequirements, pipelines, and auxiliary features and demonstrate its efficacy\nusing the BraTS Adult Glioma Post-Treatment Challenge dataset. Our results\nhighlight MIST's ability to produce accurate segmentation masks and its\nscalability across multiple GPUs, showcasing its potential as a powerful tool\nfor future medical imaging research and development.\n", "link": "http://arxiv.org/abs/2407.21343v2", "date": "2024-11-18", "relevancy": 2.2145, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIST%3A%20A%20Simple%20and%20Scalable%20End-To-End%203D%20Medical%20Imaging%20Segmentation%0A%20%20Framework&body=Title%3A%20MIST%3A%20A%20Simple%20and%20Scalable%20End-To-End%203D%20Medical%20Imaging%20Segmentation%0A%20%20Framework%0AAuthor%3A%20Adrian%20Celaya%20and%20Evan%20Lim%20and%20Rachel%20Glenn%20and%20Brayden%20Mi%20and%20Alex%20Balsells%20and%20Dawid%20Schellingerhout%20and%20Tucker%20Netherton%20and%20Caroline%20Chung%20and%20Beatrice%20Riviere%20and%20David%20Fuentes%0AAbstract%3A%20%20%20Medical%20imaging%20segmentation%20is%20a%20highly%20active%20area%20of%20research%2C%20with%20deep%0Alearning-based%20methods%20achieving%20state-of-the-art%20results%20in%20several%0Abenchmarks.%20However%2C%20the%20lack%20of%20standardized%20tools%20for%20training%2C%20testing%2C%20and%0Aevaluating%20new%20methods%20makes%20the%20comparison%20of%20methods%20difficult.%20To%20address%0Athis%2C%20we%20introduce%20the%20Medical%20Imaging%20Segmentation%20Toolkit%20%28MIST%29%2C%20a%20simple%2C%0Amodular%2C%20and%20end-to-end%20medical%20imaging%20segmentation%20framework%20designed%20to%0Afacilitate%20consistent%20training%2C%20testing%2C%20and%20evaluation%20of%20deep%20learning-based%0Amedical%20imaging%20segmentation%20methods.%20MIST%20standardizes%20data%20analysis%2C%0Apreprocessing%2C%20and%20evaluation%20pipelines%2C%20accommodating%20multiple%20architectures%0Aand%20loss%20functions.%20This%20standardization%20ensures%20reproducible%20and%20fair%0Acomparisons%20across%20different%20methods.%20We%20detail%20MIST%27s%20data%20format%0Arequirements%2C%20pipelines%2C%20and%20auxiliary%20features%20and%20demonstrate%20its%20efficacy%0Ausing%20the%20BraTS%20Adult%20Glioma%20Post-Treatment%20Challenge%20dataset.%20Our%20results%0Ahighlight%20MIST%27s%20ability%20to%20produce%20accurate%20segmentation%20masks%20and%20its%0Ascalability%20across%20multiple%20GPUs%2C%20showcasing%20its%20potential%20as%20a%20powerful%20tool%0Afor%20future%20medical%20imaging%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIST%253A%2520A%2520Simple%2520and%2520Scalable%2520End-To-End%25203D%2520Medical%2520Imaging%2520Segmentation%250A%2520%2520Framework%26entry.906535625%3DAdrian%2520Celaya%2520and%2520Evan%2520Lim%2520and%2520Rachel%2520Glenn%2520and%2520Brayden%2520Mi%2520and%2520Alex%2520Balsells%2520and%2520Dawid%2520Schellingerhout%2520and%2520Tucker%2520Netherton%2520and%2520Caroline%2520Chung%2520and%2520Beatrice%2520Riviere%2520and%2520David%2520Fuentes%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520segmentation%2520is%2520a%2520highly%2520active%2520area%2520of%2520research%252C%2520with%2520deep%250Alearning-based%2520methods%2520achieving%2520state-of-the-art%2520results%2520in%2520several%250Abenchmarks.%2520However%252C%2520the%2520lack%2520of%2520standardized%2520tools%2520for%2520training%252C%2520testing%252C%2520and%250Aevaluating%2520new%2520methods%2520makes%2520the%2520comparison%2520of%2520methods%2520difficult.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520the%2520Medical%2520Imaging%2520Segmentation%2520Toolkit%2520%2528MIST%2529%252C%2520a%2520simple%252C%250Amodular%252C%2520and%2520end-to-end%2520medical%2520imaging%2520segmentation%2520framework%2520designed%2520to%250Afacilitate%2520consistent%2520training%252C%2520testing%252C%2520and%2520evaluation%2520of%2520deep%2520learning-based%250Amedical%2520imaging%2520segmentation%2520methods.%2520MIST%2520standardizes%2520data%2520analysis%252C%250Apreprocessing%252C%2520and%2520evaluation%2520pipelines%252C%2520accommodating%2520multiple%2520architectures%250Aand%2520loss%2520functions.%2520This%2520standardization%2520ensures%2520reproducible%2520and%2520fair%250Acomparisons%2520across%2520different%2520methods.%2520We%2520detail%2520MIST%2527s%2520data%2520format%250Arequirements%252C%2520pipelines%252C%2520and%2520auxiliary%2520features%2520and%2520demonstrate%2520its%2520efficacy%250Ausing%2520the%2520BraTS%2520Adult%2520Glioma%2520Post-Treatment%2520Challenge%2520dataset.%2520Our%2520results%250Ahighlight%2520MIST%2527s%2520ability%2520to%2520produce%2520accurate%2520segmentation%2520masks%2520and%2520its%250Ascalability%2520across%2520multiple%2520GPUs%252C%2520showcasing%2520its%2520potential%2520as%2520a%2520powerful%2520tool%250Afor%2520future%2520medical%2520imaging%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIST%3A%20A%20Simple%20and%20Scalable%20End-To-End%203D%20Medical%20Imaging%20Segmentation%0A%20%20Framework&entry.906535625=Adrian%20Celaya%20and%20Evan%20Lim%20and%20Rachel%20Glenn%20and%20Brayden%20Mi%20and%20Alex%20Balsells%20and%20Dawid%20Schellingerhout%20and%20Tucker%20Netherton%20and%20Caroline%20Chung%20and%20Beatrice%20Riviere%20and%20David%20Fuentes&entry.1292438233=%20%20Medical%20imaging%20segmentation%20is%20a%20highly%20active%20area%20of%20research%2C%20with%20deep%0Alearning-based%20methods%20achieving%20state-of-the-art%20results%20in%20several%0Abenchmarks.%20However%2C%20the%20lack%20of%20standardized%20tools%20for%20training%2C%20testing%2C%20and%0Aevaluating%20new%20methods%20makes%20the%20comparison%20of%20methods%20difficult.%20To%20address%0Athis%2C%20we%20introduce%20the%20Medical%20Imaging%20Segmentation%20Toolkit%20%28MIST%29%2C%20a%20simple%2C%0Amodular%2C%20and%20end-to-end%20medical%20imaging%20segmentation%20framework%20designed%20to%0Afacilitate%20consistent%20training%2C%20testing%2C%20and%20evaluation%20of%20deep%20learning-based%0Amedical%20imaging%20segmentation%20methods.%20MIST%20standardizes%20data%20analysis%2C%0Apreprocessing%2C%20and%20evaluation%20pipelines%2C%20accommodating%20multiple%20architectures%0Aand%20loss%20functions.%20This%20standardization%20ensures%20reproducible%20and%20fair%0Acomparisons%20across%20different%20methods.%20We%20detail%20MIST%27s%20data%20format%0Arequirements%2C%20pipelines%2C%20and%20auxiliary%20features%20and%20demonstrate%20its%20efficacy%0Ausing%20the%20BraTS%20Adult%20Glioma%20Post-Treatment%20Challenge%20dataset.%20Our%20results%0Ahighlight%20MIST%27s%20ability%20to%20produce%20accurate%20segmentation%20masks%20and%20its%0Ascalability%20across%20multiple%20GPUs%2C%20showcasing%20its%20potential%20as%20a%20powerful%20tool%0Afor%20future%20medical%20imaging%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21343v2&entry.124074799=Read"},
{"title": "SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images\n  via Vision-Language Model", "author": "Bin Cao and Jianhao Yuan and Yexin Liu and Jian Li and Shuyang Sun and Jing Liu and Bo Zhao", "abstract": "  In the rapidly evolving area of image synthesis, a serious challenge is the\npresence of complex artifacts that compromise perceptual realism of synthetic\nimages. To alleviate artifacts and improve quality of synthetic images, we\nfine-tune Vision-Language Model (VLM) as artifact classifier to automatically\nidentify and classify a wide range of artifacts and provide supervision for\nfurther optimizing generative models. Specifically, we develop a comprehensive\nartifact taxonomy and construct a dataset of synthetic images with artifact\nannotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM\nexhibits superior ability of identifying artifacts and outperforms the baseline\nby 25.66%. To our knowledge, this is the first time such end-to-end artifact\nclassification task and solution have been proposed. Finally, we leverage the\noutput of VLM as feedback to refine the generative model for alleviating\nartifacts. Visualization results and user study demonstrate that the quality of\nimages synthesized by the refined diffusion model has been obviously improved.\n", "link": "http://arxiv.org/abs/2402.18068v3", "date": "2024-11-18", "relevancy": 2.2088, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynArtifact%3A%20Classifying%20and%20Alleviating%20Artifacts%20in%20Synthetic%20Images%0A%20%20via%20Vision-Language%20Model&body=Title%3A%20SynArtifact%3A%20Classifying%20and%20Alleviating%20Artifacts%20in%20Synthetic%20Images%0A%20%20via%20Vision-Language%20Model%0AAuthor%3A%20Bin%20Cao%20and%20Jianhao%20Yuan%20and%20Yexin%20Liu%20and%20Jian%20Li%20and%20Shuyang%20Sun%20and%20Jing%20Liu%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20area%20of%20image%20synthesis%2C%20a%20serious%20challenge%20is%20the%0Apresence%20of%20complex%20artifacts%20that%20compromise%20perceptual%20realism%20of%20synthetic%0Aimages.%20To%20alleviate%20artifacts%20and%20improve%20quality%20of%20synthetic%20images%2C%20we%0Afine-tune%20Vision-Language%20Model%20%28VLM%29%20as%20artifact%20classifier%20to%20automatically%0Aidentify%20and%20classify%20a%20wide%20range%20of%20artifacts%20and%20provide%20supervision%20for%0Afurther%20optimizing%20generative%20models.%20Specifically%2C%20we%20develop%20a%20comprehensive%0Aartifact%20taxonomy%20and%20construct%20a%20dataset%20of%20synthetic%20images%20with%20artifact%0Aannotations%20for%20fine-tuning%20VLM%2C%20named%20SynArtifact-1K.%20The%20fine-tuned%20VLM%0Aexhibits%20superior%20ability%20of%20identifying%20artifacts%20and%20outperforms%20the%20baseline%0Aby%2025.66%25.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20time%20such%20end-to-end%20artifact%0Aclassification%20task%20and%20solution%20have%20been%20proposed.%20Finally%2C%20we%20leverage%20the%0Aoutput%20of%20VLM%20as%20feedback%20to%20refine%20the%20generative%20model%20for%20alleviating%0Aartifacts.%20Visualization%20results%20and%20user%20study%20demonstrate%20that%20the%20quality%20of%0Aimages%20synthesized%20by%20the%20refined%20diffusion%20model%20has%20been%20obviously%20improved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18068v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynArtifact%253A%2520Classifying%2520and%2520Alleviating%2520Artifacts%2520in%2520Synthetic%2520Images%250A%2520%2520via%2520Vision-Language%2520Model%26entry.906535625%3DBin%2520Cao%2520and%2520Jianhao%2520Yuan%2520and%2520Yexin%2520Liu%2520and%2520Jian%2520Li%2520and%2520Shuyang%2520Sun%2520and%2520Jing%2520Liu%2520and%2520Bo%2520Zhao%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520area%2520of%2520image%2520synthesis%252C%2520a%2520serious%2520challenge%2520is%2520the%250Apresence%2520of%2520complex%2520artifacts%2520that%2520compromise%2520perceptual%2520realism%2520of%2520synthetic%250Aimages.%2520To%2520alleviate%2520artifacts%2520and%2520improve%2520quality%2520of%2520synthetic%2520images%252C%2520we%250Afine-tune%2520Vision-Language%2520Model%2520%2528VLM%2529%2520as%2520artifact%2520classifier%2520to%2520automatically%250Aidentify%2520and%2520classify%2520a%2520wide%2520range%2520of%2520artifacts%2520and%2520provide%2520supervision%2520for%250Afurther%2520optimizing%2520generative%2520models.%2520Specifically%252C%2520we%2520develop%2520a%2520comprehensive%250Aartifact%2520taxonomy%2520and%2520construct%2520a%2520dataset%2520of%2520synthetic%2520images%2520with%2520artifact%250Aannotations%2520for%2520fine-tuning%2520VLM%252C%2520named%2520SynArtifact-1K.%2520The%2520fine-tuned%2520VLM%250Aexhibits%2520superior%2520ability%2520of%2520identifying%2520artifacts%2520and%2520outperforms%2520the%2520baseline%250Aby%252025.66%2525.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520time%2520such%2520end-to-end%2520artifact%250Aclassification%2520task%2520and%2520solution%2520have%2520been%2520proposed.%2520Finally%252C%2520we%2520leverage%2520the%250Aoutput%2520of%2520VLM%2520as%2520feedback%2520to%2520refine%2520the%2520generative%2520model%2520for%2520alleviating%250Aartifacts.%2520Visualization%2520results%2520and%2520user%2520study%2520demonstrate%2520that%2520the%2520quality%2520of%250Aimages%2520synthesized%2520by%2520the%2520refined%2520diffusion%2520model%2520has%2520been%2520obviously%2520improved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18068v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynArtifact%3A%20Classifying%20and%20Alleviating%20Artifacts%20in%20Synthetic%20Images%0A%20%20via%20Vision-Language%20Model&entry.906535625=Bin%20Cao%20and%20Jianhao%20Yuan%20and%20Yexin%20Liu%20and%20Jian%20Li%20and%20Shuyang%20Sun%20and%20Jing%20Liu%20and%20Bo%20Zhao&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20area%20of%20image%20synthesis%2C%20a%20serious%20challenge%20is%20the%0Apresence%20of%20complex%20artifacts%20that%20compromise%20perceptual%20realism%20of%20synthetic%0Aimages.%20To%20alleviate%20artifacts%20and%20improve%20quality%20of%20synthetic%20images%2C%20we%0Afine-tune%20Vision-Language%20Model%20%28VLM%29%20as%20artifact%20classifier%20to%20automatically%0Aidentify%20and%20classify%20a%20wide%20range%20of%20artifacts%20and%20provide%20supervision%20for%0Afurther%20optimizing%20generative%20models.%20Specifically%2C%20we%20develop%20a%20comprehensive%0Aartifact%20taxonomy%20and%20construct%20a%20dataset%20of%20synthetic%20images%20with%20artifact%0Aannotations%20for%20fine-tuning%20VLM%2C%20named%20SynArtifact-1K.%20The%20fine-tuned%20VLM%0Aexhibits%20superior%20ability%20of%20identifying%20artifacts%20and%20outperforms%20the%20baseline%0Aby%2025.66%25.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20time%20such%20end-to-end%20artifact%0Aclassification%20task%20and%20solution%20have%20been%20proposed.%20Finally%2C%20we%20leverage%20the%0Aoutput%20of%20VLM%20as%20feedback%20to%20refine%20the%20generative%20model%20for%20alleviating%0Aartifacts.%20Visualization%20results%20and%20user%20study%20demonstrate%20that%20the%20quality%20of%0Aimages%20synthesized%20by%20the%20refined%20diffusion%20model%20has%20been%20obviously%20improved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18068v3&entry.124074799=Read"},
{"title": "Structural-Based Uncertainty in Deep Learning Across Anatomical Scales:\n  Analysis in White Matter Lesion Segmentation", "author": "Nataliia Molchanova and Vatsal Raina and Andrey Malinin and Francesco La Rosa and Adrien Depeursinge and Mark Gales and Cristina Granziera and Henning Muller and Mara Graziani and Meritxell Bach Cuadra", "abstract": "  This paper explores uncertainty quantification (UQ) as an indicator of the\ntrustworthiness of automated deep-learning (DL) tools in the context of white\nmatter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of\nmultiple sclerosis (MS) patients. Our study focuses on two principal aspects of\nuncertainty in structured output segmentation tasks. First, we postulate that a\nreliable uncertainty measure should indicate predictions likely to be incorrect\nwith high uncertainty values. Second, we investigate the merit of quantifying\nuncertainty at different anatomical scales (voxel, lesion, or patient). We\nhypothesize that uncertainty at each scale is related to specific types of\nerrors. Our study aims to confirm this relationship by conducting separate\nanalyses for in-domain and out-of-domain settings. Our primary methodological\ncontributions are (i) the development of novel measures for quantifying\nuncertainty at lesion and patient scales, derived from structural prediction\ndiscrepancies, and (ii) the extension of an error retention curve analysis\nframework to facilitate the evaluation of UQ performance at both lesion and\npatient scales. The results from a multi-centric MRI dataset of 444 patients\ndemonstrate that our proposed measures more effectively capture model errors at\nthe lesion and patient scales compared to measures that average voxel-scale\nuncertainty values. We provide the UQ protocols code at\nhttps://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.\n", "link": "http://arxiv.org/abs/2311.08931v3", "date": "2024-11-18", "relevancy": 2.2011, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6402}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation&body=Title%3A%20Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation%0AAuthor%3A%20Nataliia%20Molchanova%20and%20Vatsal%20Raina%20and%20Andrey%20Malinin%20and%20Francesco%20La%20Rosa%20and%20Adrien%20Depeursinge%20and%20Mark%20Gales%20and%20Cristina%20Granziera%20and%20Henning%20Muller%20and%20Mara%20Graziani%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20as%20an%20indicator%20of%20the%0Atrustworthiness%20of%20automated%20deep-learning%20%28DL%29%20tools%20in%20the%20context%20of%20white%0Amatter%20lesion%20%28WML%29%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20scans%20of%0Amultiple%20sclerosis%20%28MS%29%20patients.%20Our%20study%20focuses%20on%20two%20principal%20aspects%20of%0Auncertainty%20in%20structured%20output%20segmentation%20tasks.%20First%2C%20we%20postulate%20that%20a%0Areliable%20uncertainty%20measure%20should%20indicate%20predictions%20likely%20to%20be%20incorrect%0Awith%20high%20uncertainty%20values.%20Second%2C%20we%20investigate%20the%20merit%20of%20quantifying%0Auncertainty%20at%20different%20anatomical%20scales%20%28voxel%2C%20lesion%2C%20or%20patient%29.%20We%0Ahypothesize%20that%20uncertainty%20at%20each%20scale%20is%20related%20to%20specific%20types%20of%0Aerrors.%20Our%20study%20aims%20to%20confirm%20this%20relationship%20by%20conducting%20separate%0Aanalyses%20for%20in-domain%20and%20out-of-domain%20settings.%20Our%20primary%20methodological%0Acontributions%20are%20%28i%29%20the%20development%20of%20novel%20measures%20for%20quantifying%0Auncertainty%20at%20lesion%20and%20patient%20scales%2C%20derived%20from%20structural%20prediction%0Adiscrepancies%2C%20and%20%28ii%29%20the%20extension%20of%20an%20error%20retention%20curve%20analysis%0Aframework%20to%20facilitate%20the%20evaluation%20of%20UQ%20performance%20at%20both%20lesion%20and%0Apatient%20scales.%20The%20results%20from%20a%20multi-centric%20MRI%20dataset%20of%20444%20patients%0Ademonstrate%20that%20our%20proposed%20measures%20more%20effectively%20capture%20model%20errors%20at%0Athe%20lesion%20and%20patient%20scales%20compared%20to%20measures%20that%20average%20voxel-scale%0Auncertainty%20values.%20We%20provide%20the%20UQ%20protocols%20code%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08931v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural-Based%2520Uncertainty%2520in%2520Deep%2520Learning%2520Across%2520Anatomical%2520Scales%253A%250A%2520%2520Analysis%2520in%2520White%2520Matter%2520Lesion%2520Segmentation%26entry.906535625%3DNataliia%2520Molchanova%2520and%2520Vatsal%2520Raina%2520and%2520Andrey%2520Malinin%2520and%2520Francesco%2520La%2520Rosa%2520and%2520Adrien%2520Depeursinge%2520and%2520Mark%2520Gales%2520and%2520Cristina%2520Granziera%2520and%2520Henning%2520Muller%2520and%2520Mara%2520Graziani%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520uncertainty%2520quantification%2520%2528UQ%2529%2520as%2520an%2520indicator%2520of%2520the%250Atrustworthiness%2520of%2520automated%2520deep-learning%2520%2528DL%2529%2520tools%2520in%2520the%2520context%2520of%2520white%250Amatter%2520lesion%2520%2528WML%2529%2520segmentation%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520scans%2520of%250Amultiple%2520sclerosis%2520%2528MS%2529%2520patients.%2520Our%2520study%2520focuses%2520on%2520two%2520principal%2520aspects%2520of%250Auncertainty%2520in%2520structured%2520output%2520segmentation%2520tasks.%2520First%252C%2520we%2520postulate%2520that%2520a%250Areliable%2520uncertainty%2520measure%2520should%2520indicate%2520predictions%2520likely%2520to%2520be%2520incorrect%250Awith%2520high%2520uncertainty%2520values.%2520Second%252C%2520we%2520investigate%2520the%2520merit%2520of%2520quantifying%250Auncertainty%2520at%2520different%2520anatomical%2520scales%2520%2528voxel%252C%2520lesion%252C%2520or%2520patient%2529.%2520We%250Ahypothesize%2520that%2520uncertainty%2520at%2520each%2520scale%2520is%2520related%2520to%2520specific%2520types%2520of%250Aerrors.%2520Our%2520study%2520aims%2520to%2520confirm%2520this%2520relationship%2520by%2520conducting%2520separate%250Aanalyses%2520for%2520in-domain%2520and%2520out-of-domain%2520settings.%2520Our%2520primary%2520methodological%250Acontributions%2520are%2520%2528i%2529%2520the%2520development%2520of%2520novel%2520measures%2520for%2520quantifying%250Auncertainty%2520at%2520lesion%2520and%2520patient%2520scales%252C%2520derived%2520from%2520structural%2520prediction%250Adiscrepancies%252C%2520and%2520%2528ii%2529%2520the%2520extension%2520of%2520an%2520error%2520retention%2520curve%2520analysis%250Aframework%2520to%2520facilitate%2520the%2520evaluation%2520of%2520UQ%2520performance%2520at%2520both%2520lesion%2520and%250Apatient%2520scales.%2520The%2520results%2520from%2520a%2520multi-centric%2520MRI%2520dataset%2520of%2520444%2520patients%250Ademonstrate%2520that%2520our%2520proposed%2520measures%2520more%2520effectively%2520capture%2520model%2520errors%2520at%250Athe%2520lesion%2520and%2520patient%2520scales%2520compared%2520to%2520measures%2520that%2520average%2520voxel-scale%250Auncertainty%2520values.%2520We%2520provide%2520the%2520UQ%2520protocols%2520code%2520at%250Ahttps%253A//github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08931v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural-Based%20Uncertainty%20in%20Deep%20Learning%20Across%20Anatomical%20Scales%3A%0A%20%20Analysis%20in%20White%20Matter%20Lesion%20Segmentation&entry.906535625=Nataliia%20Molchanova%20and%20Vatsal%20Raina%20and%20Andrey%20Malinin%20and%20Francesco%20La%20Rosa%20and%20Adrien%20Depeursinge%20and%20Mark%20Gales%20and%20Cristina%20Granziera%20and%20Henning%20Muller%20and%20Mara%20Graziani%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20as%20an%20indicator%20of%20the%0Atrustworthiness%20of%20automated%20deep-learning%20%28DL%29%20tools%20in%20the%20context%20of%20white%0Amatter%20lesion%20%28WML%29%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20scans%20of%0Amultiple%20sclerosis%20%28MS%29%20patients.%20Our%20study%20focuses%20on%20two%20principal%20aspects%20of%0Auncertainty%20in%20structured%20output%20segmentation%20tasks.%20First%2C%20we%20postulate%20that%20a%0Areliable%20uncertainty%20measure%20should%20indicate%20predictions%20likely%20to%20be%20incorrect%0Awith%20high%20uncertainty%20values.%20Second%2C%20we%20investigate%20the%20merit%20of%20quantifying%0Auncertainty%20at%20different%20anatomical%20scales%20%28voxel%2C%20lesion%2C%20or%20patient%29.%20We%0Ahypothesize%20that%20uncertainty%20at%20each%20scale%20is%20related%20to%20specific%20types%20of%0Aerrors.%20Our%20study%20aims%20to%20confirm%20this%20relationship%20by%20conducting%20separate%0Aanalyses%20for%20in-domain%20and%20out-of-domain%20settings.%20Our%20primary%20methodological%0Acontributions%20are%20%28i%29%20the%20development%20of%20novel%20measures%20for%20quantifying%0Auncertainty%20at%20lesion%20and%20patient%20scales%2C%20derived%20from%20structural%20prediction%0Adiscrepancies%2C%20and%20%28ii%29%20the%20extension%20of%20an%20error%20retention%20curve%20analysis%0Aframework%20to%20facilitate%20the%20evaluation%20of%20UQ%20performance%20at%20both%20lesion%20and%0Apatient%20scales.%20The%20results%20from%20a%20multi-centric%20MRI%20dataset%20of%20444%20patients%0Ademonstrate%20that%20our%20proposed%20measures%20more%20effectively%20capture%20model%20errors%20at%0Athe%20lesion%20and%20patient%20scales%20compared%20to%20measures%20that%20average%20voxel-scale%0Auncertainty%20values.%20We%20provide%20the%20UQ%20protocols%20code%20at%0Ahttps%3A//github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08931v3&entry.124074799=Read"},
{"title": "Real-Time Fitness Exercise Classification and Counting from Video Frames", "author": "Riccardo Riccio", "abstract": "  This paper introduces a novel method for real-time exercise classification\nusing a Bidirectional Long Short-Term Memory (BiLSTM) neural network. Existing\nexercise recognition approaches often rely on synthetic datasets, raw\ncoordinate inputs sensitive to user and camera variations, and fail to fully\nexploit the temporal dependencies in exercise movements. These issues limit\ntheir generalizability and robustness in real-world conditions, where lighting,\ncamera angles, and user body types vary.\n  To address these challenges, we propose a BiLSTM-based model that leverages\ninvariant features, such as joint angles, alongside raw coordinates. By using\nboth angles and (x, y, z) coordinates, the model adapts to changes in\nperspective, user positioning, and body differences, improving generalization.\nTraining on 30-frame sequences enables the BiLSTM to capture the temporal\ncontext of exercises and recognize patterns evolving over time.\n  We compiled a dataset combining synthetic data from the InfiniteRep dataset\nand real-world videos from Kaggle and other sources. This dataset includes four\ncommon exercises: squat, push-up, shoulder press, and bicep curl. The model was\ntrained and validated on these diverse datasets, achieving an accuracy of over\n99% on the test set. To assess generalizability, the model was tested on 2\nseparate test sets representative of typical usage conditions. Comparisons with\nthe previous approach from the literature are present in the result section\nshowing that the proposed model is the best-performing one.\n  The classifier is integrated into a web application providing real-time\nexercise classification and repetition counting without manual exercise\nselection.\n  Demo and datasets are available at the following GitHub Repository:\nhttps://github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting.\n", "link": "http://arxiv.org/abs/2411.11548v1", "date": "2024-11-18", "relevancy": 2.1874, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.531}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Fitness%20Exercise%20Classification%20and%20Counting%20from%20Video%20Frames&body=Title%3A%20Real-Time%20Fitness%20Exercise%20Classification%20and%20Counting%20from%20Video%20Frames%0AAuthor%3A%20Riccardo%20Riccio%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20method%20for%20real-time%20exercise%20classification%0Ausing%20a%20Bidirectional%20Long%20Short-Term%20Memory%20%28BiLSTM%29%20neural%20network.%20Existing%0Aexercise%20recognition%20approaches%20often%20rely%20on%20synthetic%20datasets%2C%20raw%0Acoordinate%20inputs%20sensitive%20to%20user%20and%20camera%20variations%2C%20and%20fail%20to%20fully%0Aexploit%20the%20temporal%20dependencies%20in%20exercise%20movements.%20These%20issues%20limit%0Atheir%20generalizability%20and%20robustness%20in%20real-world%20conditions%2C%20where%20lighting%2C%0Acamera%20angles%2C%20and%20user%20body%20types%20vary.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20a%20BiLSTM-based%20model%20that%20leverages%0Ainvariant%20features%2C%20such%20as%20joint%20angles%2C%20alongside%20raw%20coordinates.%20By%20using%0Aboth%20angles%20and%20%28x%2C%20y%2C%20z%29%20coordinates%2C%20the%20model%20adapts%20to%20changes%20in%0Aperspective%2C%20user%20positioning%2C%20and%20body%20differences%2C%20improving%20generalization.%0ATraining%20on%2030-frame%20sequences%20enables%20the%20BiLSTM%20to%20capture%20the%20temporal%0Acontext%20of%20exercises%20and%20recognize%20patterns%20evolving%20over%20time.%0A%20%20We%20compiled%20a%20dataset%20combining%20synthetic%20data%20from%20the%20InfiniteRep%20dataset%0Aand%20real-world%20videos%20from%20Kaggle%20and%20other%20sources.%20This%20dataset%20includes%20four%0Acommon%20exercises%3A%20squat%2C%20push-up%2C%20shoulder%20press%2C%20and%20bicep%20curl.%20The%20model%20was%0Atrained%20and%20validated%20on%20these%20diverse%20datasets%2C%20achieving%20an%20accuracy%20of%20over%0A99%25%20on%20the%20test%20set.%20To%20assess%20generalizability%2C%20the%20model%20was%20tested%20on%202%0Aseparate%20test%20sets%20representative%20of%20typical%20usage%20conditions.%20Comparisons%20with%0Athe%20previous%20approach%20from%20the%20literature%20are%20present%20in%20the%20result%20section%0Ashowing%20that%20the%20proposed%20model%20is%20the%20best-performing%20one.%0A%20%20The%20classifier%20is%20integrated%20into%20a%20web%20application%20providing%20real-time%0Aexercise%20classification%20and%20repetition%20counting%20without%20manual%20exercise%0Aselection.%0A%20%20Demo%20and%20datasets%20are%20available%20at%20the%20following%20GitHub%20Repository%3A%0Ahttps%3A//github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Fitness%2520Exercise%2520Classification%2520and%2520Counting%2520from%2520Video%2520Frames%26entry.906535625%3DRiccardo%2520Riccio%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520method%2520for%2520real-time%2520exercise%2520classification%250Ausing%2520a%2520Bidirectional%2520Long%2520Short-Term%2520Memory%2520%2528BiLSTM%2529%2520neural%2520network.%2520Existing%250Aexercise%2520recognition%2520approaches%2520often%2520rely%2520on%2520synthetic%2520datasets%252C%2520raw%250Acoordinate%2520inputs%2520sensitive%2520to%2520user%2520and%2520camera%2520variations%252C%2520and%2520fail%2520to%2520fully%250Aexploit%2520the%2520temporal%2520dependencies%2520in%2520exercise%2520movements.%2520These%2520issues%2520limit%250Atheir%2520generalizability%2520and%2520robustness%2520in%2520real-world%2520conditions%252C%2520where%2520lighting%252C%250Acamera%2520angles%252C%2520and%2520user%2520body%2520types%2520vary.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520BiLSTM-based%2520model%2520that%2520leverages%250Ainvariant%2520features%252C%2520such%2520as%2520joint%2520angles%252C%2520alongside%2520raw%2520coordinates.%2520By%2520using%250Aboth%2520angles%2520and%2520%2528x%252C%2520y%252C%2520z%2529%2520coordinates%252C%2520the%2520model%2520adapts%2520to%2520changes%2520in%250Aperspective%252C%2520user%2520positioning%252C%2520and%2520body%2520differences%252C%2520improving%2520generalization.%250ATraining%2520on%252030-frame%2520sequences%2520enables%2520the%2520BiLSTM%2520to%2520capture%2520the%2520temporal%250Acontext%2520of%2520exercises%2520and%2520recognize%2520patterns%2520evolving%2520over%2520time.%250A%2520%2520We%2520compiled%2520a%2520dataset%2520combining%2520synthetic%2520data%2520from%2520the%2520InfiniteRep%2520dataset%250Aand%2520real-world%2520videos%2520from%2520Kaggle%2520and%2520other%2520sources.%2520This%2520dataset%2520includes%2520four%250Acommon%2520exercises%253A%2520squat%252C%2520push-up%252C%2520shoulder%2520press%252C%2520and%2520bicep%2520curl.%2520The%2520model%2520was%250Atrained%2520and%2520validated%2520on%2520these%2520diverse%2520datasets%252C%2520achieving%2520an%2520accuracy%2520of%2520over%250A99%2525%2520on%2520the%2520test%2520set.%2520To%2520assess%2520generalizability%252C%2520the%2520model%2520was%2520tested%2520on%25202%250Aseparate%2520test%2520sets%2520representative%2520of%2520typical%2520usage%2520conditions.%2520Comparisons%2520with%250Athe%2520previous%2520approach%2520from%2520the%2520literature%2520are%2520present%2520in%2520the%2520result%2520section%250Ashowing%2520that%2520the%2520proposed%2520model%2520is%2520the%2520best-performing%2520one.%250A%2520%2520The%2520classifier%2520is%2520integrated%2520into%2520a%2520web%2520application%2520providing%2520real-time%250Aexercise%2520classification%2520and%2520repetition%2520counting%2520without%2520manual%2520exercise%250Aselection.%250A%2520%2520Demo%2520and%2520datasets%2520are%2520available%2520at%2520the%2520following%2520GitHub%2520Repository%253A%250Ahttps%253A//github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Fitness%20Exercise%20Classification%20and%20Counting%20from%20Video%20Frames&entry.906535625=Riccardo%20Riccio&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20method%20for%20real-time%20exercise%20classification%0Ausing%20a%20Bidirectional%20Long%20Short-Term%20Memory%20%28BiLSTM%29%20neural%20network.%20Existing%0Aexercise%20recognition%20approaches%20often%20rely%20on%20synthetic%20datasets%2C%20raw%0Acoordinate%20inputs%20sensitive%20to%20user%20and%20camera%20variations%2C%20and%20fail%20to%20fully%0Aexploit%20the%20temporal%20dependencies%20in%20exercise%20movements.%20These%20issues%20limit%0Atheir%20generalizability%20and%20robustness%20in%20real-world%20conditions%2C%20where%20lighting%2C%0Acamera%20angles%2C%20and%20user%20body%20types%20vary.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20a%20BiLSTM-based%20model%20that%20leverages%0Ainvariant%20features%2C%20such%20as%20joint%20angles%2C%20alongside%20raw%20coordinates.%20By%20using%0Aboth%20angles%20and%20%28x%2C%20y%2C%20z%29%20coordinates%2C%20the%20model%20adapts%20to%20changes%20in%0Aperspective%2C%20user%20positioning%2C%20and%20body%20differences%2C%20improving%20generalization.%0ATraining%20on%2030-frame%20sequences%20enables%20the%20BiLSTM%20to%20capture%20the%20temporal%0Acontext%20of%20exercises%20and%20recognize%20patterns%20evolving%20over%20time.%0A%20%20We%20compiled%20a%20dataset%20combining%20synthetic%20data%20from%20the%20InfiniteRep%20dataset%0Aand%20real-world%20videos%20from%20Kaggle%20and%20other%20sources.%20This%20dataset%20includes%20four%0Acommon%20exercises%3A%20squat%2C%20push-up%2C%20shoulder%20press%2C%20and%20bicep%20curl.%20The%20model%20was%0Atrained%20and%20validated%20on%20these%20diverse%20datasets%2C%20achieving%20an%20accuracy%20of%20over%0A99%25%20on%20the%20test%20set.%20To%20assess%20generalizability%2C%20the%20model%20was%20tested%20on%202%0Aseparate%20test%20sets%20representative%20of%20typical%20usage%20conditions.%20Comparisons%20with%0Athe%20previous%20approach%20from%20the%20literature%20are%20present%20in%20the%20result%20section%0Ashowing%20that%20the%20proposed%20model%20is%20the%20best-performing%20one.%0A%20%20The%20classifier%20is%20integrated%20into%20a%20web%20application%20providing%20real-time%0Aexercise%20classification%20and%20repetition%20counting%20without%20manual%20exercise%0Aselection.%0A%20%20Demo%20and%20datasets%20are%20available%20at%20the%20following%20GitHub%20Repository%3A%0Ahttps%3A//github.com/RiccardoRiccio/Fitness-AI-Trainer-With-Automatic-Exercise-Recognition-and-Counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11548v1&entry.124074799=Read"},
{"title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment", "author": "Chenhang Cui and An Zhang and Yiyang Zhou and Zhaorun Chen and Gelei Deng and Huaxiu Yao and Tat-Seng Chua", "abstract": "  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n", "link": "http://arxiv.org/abs/2410.14148v2", "date": "2024-11-18", "relevancy": 2.1716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Verifiers%3A%20Preference%20Modeling%20as%20Next-token%20Prediction%20in%0A%20%20Vision-Language%20Alignment&body=Title%3A%20Fine-Grained%20Verifiers%3A%20Preference%20Modeling%20as%20Next-token%20Prediction%20in%0A%20%20Vision-Language%20Alignment%0AAuthor%3A%20Chenhang%20Cui%20and%20An%20Zhang%20and%20Yiyang%20Zhou%20and%20Zhaorun%20Chen%20and%20Gelei%20Deng%20and%20Huaxiu%20Yao%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20and%20pre-trained%0Avision%20models%20have%20accelerated%20the%20development%20of%20vision-language%20large%20models%0A%28VLLMs%29%2C%20enhancing%20the%20interaction%20between%20visual%20and%20linguistic%20modalities.%0ADespite%20their%20notable%20success%20across%20various%20domains%2C%20VLLMs%20face%20challenges%20in%0Amodality%20alignment%2C%20which%20can%20lead%20to%20issues%20like%20hallucinations%20and%20unsafe%0Acontent%20generation.%20Current%20alignment%20techniques%20often%20rely%20on%20coarse%20feedback%0Aand%20external%20datasets%2C%20limiting%20scalability%20and%20performance.%20In%20this%20paper%2C%20we%0Apropose%20FiSAO%20%28Fine-Grained%20Self-Alignment%20Optimization%29%2C%20a%20novel%0Aself-alignment%20method%20that%20utilizes%20the%20model%27s%20own%20visual%20encoder%20as%20a%0Afine-grained%20verifier%20to%20improve%20vision-language%20alignment%20without%20the%20need%20for%0Aadditional%20data.%20By%20leveraging%20token-level%20feedback%20from%20the%20vision%20encoder%2C%0AFiSAO%20significantly%20improves%20vision-language%20alignment%2C%20even%20surpassing%0Atraditional%20preference%20tuning%20methods%20that%20require%20additional%20data.%20Through%0Aboth%20theoretical%20analysis%20and%20experimental%20validation%2C%20we%20demonstrate%20that%0AFiSAO%20effectively%20addresses%20the%20misalignment%20problem%20in%20VLLMs%2C%20marking%20the%0Afirst%20instance%20of%20token-level%20rewards%20being%20applied%20to%20such%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14148v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Verifiers%253A%2520Preference%2520Modeling%2520as%2520Next-token%2520Prediction%2520in%250A%2520%2520Vision-Language%2520Alignment%26entry.906535625%3DChenhang%2520Cui%2520and%2520An%2520Zhang%2520and%2520Yiyang%2520Zhou%2520and%2520Zhaorun%2520Chen%2520and%2520Gelei%2520Deng%2520and%2520Huaxiu%2520Yao%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520pre-trained%250Avision%2520models%2520have%2520accelerated%2520the%2520development%2520of%2520vision-language%2520large%2520models%250A%2528VLLMs%2529%252C%2520enhancing%2520the%2520interaction%2520between%2520visual%2520and%2520linguistic%2520modalities.%250ADespite%2520their%2520notable%2520success%2520across%2520various%2520domains%252C%2520VLLMs%2520face%2520challenges%2520in%250Amodality%2520alignment%252C%2520which%2520can%2520lead%2520to%2520issues%2520like%2520hallucinations%2520and%2520unsafe%250Acontent%2520generation.%2520Current%2520alignment%2520techniques%2520often%2520rely%2520on%2520coarse%2520feedback%250Aand%2520external%2520datasets%252C%2520limiting%2520scalability%2520and%2520performance.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520FiSAO%2520%2528Fine-Grained%2520Self-Alignment%2520Optimization%2529%252C%2520a%2520novel%250Aself-alignment%2520method%2520that%2520utilizes%2520the%2520model%2527s%2520own%2520visual%2520encoder%2520as%2520a%250Afine-grained%2520verifier%2520to%2520improve%2520vision-language%2520alignment%2520without%2520the%2520need%2520for%250Aadditional%2520data.%2520By%2520leveraging%2520token-level%2520feedback%2520from%2520the%2520vision%2520encoder%252C%250AFiSAO%2520significantly%2520improves%2520vision-language%2520alignment%252C%2520even%2520surpassing%250Atraditional%2520preference%2520tuning%2520methods%2520that%2520require%2520additional%2520data.%2520Through%250Aboth%2520theoretical%2520analysis%2520and%2520experimental%2520validation%252C%2520we%2520demonstrate%2520that%250AFiSAO%2520effectively%2520addresses%2520the%2520misalignment%2520problem%2520in%2520VLLMs%252C%2520marking%2520the%250Afirst%2520instance%2520of%2520token-level%2520rewards%2520being%2520applied%2520to%2520such%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14148v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Verifiers%3A%20Preference%20Modeling%20as%20Next-token%20Prediction%20in%0A%20%20Vision-Language%20Alignment&entry.906535625=Chenhang%20Cui%20and%20An%20Zhang%20and%20Yiyang%20Zhou%20and%20Zhaorun%20Chen%20and%20Gelei%20Deng%20and%20Huaxiu%20Yao%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20and%20pre-trained%0Avision%20models%20have%20accelerated%20the%20development%20of%20vision-language%20large%20models%0A%28VLLMs%29%2C%20enhancing%20the%20interaction%20between%20visual%20and%20linguistic%20modalities.%0ADespite%20their%20notable%20success%20across%20various%20domains%2C%20VLLMs%20face%20challenges%20in%0Amodality%20alignment%2C%20which%20can%20lead%20to%20issues%20like%20hallucinations%20and%20unsafe%0Acontent%20generation.%20Current%20alignment%20techniques%20often%20rely%20on%20coarse%20feedback%0Aand%20external%20datasets%2C%20limiting%20scalability%20and%20performance.%20In%20this%20paper%2C%20we%0Apropose%20FiSAO%20%28Fine-Grained%20Self-Alignment%20Optimization%29%2C%20a%20novel%0Aself-alignment%20method%20that%20utilizes%20the%20model%27s%20own%20visual%20encoder%20as%20a%0Afine-grained%20verifier%20to%20improve%20vision-language%20alignment%20without%20the%20need%20for%0Aadditional%20data.%20By%20leveraging%20token-level%20feedback%20from%20the%20vision%20encoder%2C%0AFiSAO%20significantly%20improves%20vision-language%20alignment%2C%20even%20surpassing%0Atraditional%20preference%20tuning%20methods%20that%20require%20additional%20data.%20Through%0Aboth%20theoretical%20analysis%20and%20experimental%20validation%2C%20we%20demonstrate%20that%0AFiSAO%20effectively%20addresses%20the%20misalignment%20problem%20in%20VLLMs%2C%20marking%20the%0Afirst%20instance%20of%20token-level%20rewards%20being%20applied%20to%20such%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14148v2&entry.124074799=Read"},
{"title": "PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement\n  Learning", "author": "Chengyang Ying and Zhongkai Hao and Xinning Zhou and Xuezhou Xu and Hang Su and Xingxing Zhang and Jun Zhu", "abstract": "  Designing generalizable agents capable of adapting to diverse embodiments has\nachieved significant attention in Reinforcement Learning (RL), which is\ncritical for deploying RL agents in various real-world applications. Previous\nCross-Embodiment RL approaches have focused on transferring knowledge across\nembodiments within specific tasks. These methods often result in knowledge\ntightly coupled with those tasks and fail to adequately capture the distinct\ncharacteristics of different embodiments. To address this limitation, we\nintroduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which\nleverages unsupervised learning to enable agents to acquire embodiment-aware\nand task-agnostic knowledge through online interactions within reward-free\nenvironments. We formulate CEURL as a novel Controlled Embodiment Markov\nDecision Process (CE-MDP) and systematically analyze CEURL's pre-training\nobjectives under CE-MDP. Based on these analyses, we develop a novel algorithm\nPre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating\nan intrinsic reward function specifically designed for cross-embodiment\npre-training. PEAC not only provides an intuitive optimization strategy for\ncross-embodiment pre-training but also can integrate flexibly with existing\nunsupervised RL methods, facilitating cross-embodiment exploration and skill\ndiscovery. Extensive experiments in both simulated (e.g., DMC and Robosuite)\nand real-world environments (e.g., legged locomotion) demonstrate that PEAC\nsignificantly improves adaptation performance and cross-embodiment\ngeneralization, demonstrating its effectiveness in overcoming the unique\nchallenges of CEURL. The project page and code are in\nhttps://yingchengyang.github.io/ceurl.\n", "link": "http://arxiv.org/abs/2405.14073v2", "date": "2024-11-18", "relevancy": 2.1611, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEAC%3A%20Unsupervised%20Pre-training%20for%20Cross-Embodiment%20Reinforcement%0A%20%20Learning&body=Title%3A%20PEAC%3A%20Unsupervised%20Pre-training%20for%20Cross-Embodiment%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Chengyang%20Ying%20and%20Zhongkai%20Hao%20and%20Xinning%20Zhou%20and%20Xuezhou%20Xu%20and%20Hang%20Su%20and%20Xingxing%20Zhang%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20Designing%20generalizable%20agents%20capable%20of%20adapting%20to%20diverse%20embodiments%20has%0Aachieved%20significant%20attention%20in%20Reinforcement%20Learning%20%28RL%29%2C%20which%20is%0Acritical%20for%20deploying%20RL%20agents%20in%20various%20real-world%20applications.%20Previous%0ACross-Embodiment%20RL%20approaches%20have%20focused%20on%20transferring%20knowledge%20across%0Aembodiments%20within%20specific%20tasks.%20These%20methods%20often%20result%20in%20knowledge%0Atightly%20coupled%20with%20those%20tasks%20and%20fail%20to%20adequately%20capture%20the%20distinct%0Acharacteristics%20of%20different%20embodiments.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20notion%20of%20Cross-Embodiment%20Unsupervised%20RL%20%28CEURL%29%2C%20which%0Aleverages%20unsupervised%20learning%20to%20enable%20agents%20to%20acquire%20embodiment-aware%0Aand%20task-agnostic%20knowledge%20through%20online%20interactions%20within%20reward-free%0Aenvironments.%20We%20formulate%20CEURL%20as%20a%20novel%20Controlled%20Embodiment%20Markov%0ADecision%20Process%20%28CE-MDP%29%20and%20systematically%20analyze%20CEURL%27s%20pre-training%0Aobjectives%20under%20CE-MDP.%20Based%20on%20these%20analyses%2C%20we%20develop%20a%20novel%20algorithm%0APre-trained%20Embodiment-Aware%20Control%20%28PEAC%29%20for%20handling%20CEURL%2C%20incorporating%0Aan%20intrinsic%20reward%20function%20specifically%20designed%20for%20cross-embodiment%0Apre-training.%20PEAC%20not%20only%20provides%20an%20intuitive%20optimization%20strategy%20for%0Across-embodiment%20pre-training%20but%20also%20can%20integrate%20flexibly%20with%20existing%0Aunsupervised%20RL%20methods%2C%20facilitating%20cross-embodiment%20exploration%20and%20skill%0Adiscovery.%20Extensive%20experiments%20in%20both%20simulated%20%28e.g.%2C%20DMC%20and%20Robosuite%29%0Aand%20real-world%20environments%20%28e.g.%2C%20legged%20locomotion%29%20demonstrate%20that%20PEAC%0Asignificantly%20improves%20adaptation%20performance%20and%20cross-embodiment%0Ageneralization%2C%20demonstrating%20its%20effectiveness%20in%20overcoming%20the%20unique%0Achallenges%20of%20CEURL.%20The%20project%20page%20and%20code%20are%20in%0Ahttps%3A//yingchengyang.github.io/ceurl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEAC%253A%2520Unsupervised%2520Pre-training%2520for%2520Cross-Embodiment%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DChengyang%2520Ying%2520and%2520Zhongkai%2520Hao%2520and%2520Xinning%2520Zhou%2520and%2520Xuezhou%2520Xu%2520and%2520Hang%2520Su%2520and%2520Xingxing%2520Zhang%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520Designing%2520generalizable%2520agents%2520capable%2520of%2520adapting%2520to%2520diverse%2520embodiments%2520has%250Aachieved%2520significant%2520attention%2520in%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520which%2520is%250Acritical%2520for%2520deploying%2520RL%2520agents%2520in%2520various%2520real-world%2520applications.%2520Previous%250ACross-Embodiment%2520RL%2520approaches%2520have%2520focused%2520on%2520transferring%2520knowledge%2520across%250Aembodiments%2520within%2520specific%2520tasks.%2520These%2520methods%2520often%2520result%2520in%2520knowledge%250Atightly%2520coupled%2520with%2520those%2520tasks%2520and%2520fail%2520to%2520adequately%2520capture%2520the%2520distinct%250Acharacteristics%2520of%2520different%2520embodiments.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520the%2520notion%2520of%2520Cross-Embodiment%2520Unsupervised%2520RL%2520%2528CEURL%2529%252C%2520which%250Aleverages%2520unsupervised%2520learning%2520to%2520enable%2520agents%2520to%2520acquire%2520embodiment-aware%250Aand%2520task-agnostic%2520knowledge%2520through%2520online%2520interactions%2520within%2520reward-free%250Aenvironments.%2520We%2520formulate%2520CEURL%2520as%2520a%2520novel%2520Controlled%2520Embodiment%2520Markov%250ADecision%2520Process%2520%2528CE-MDP%2529%2520and%2520systematically%2520analyze%2520CEURL%2527s%2520pre-training%250Aobjectives%2520under%2520CE-MDP.%2520Based%2520on%2520these%2520analyses%252C%2520we%2520develop%2520a%2520novel%2520algorithm%250APre-trained%2520Embodiment-Aware%2520Control%2520%2528PEAC%2529%2520for%2520handling%2520CEURL%252C%2520incorporating%250Aan%2520intrinsic%2520reward%2520function%2520specifically%2520designed%2520for%2520cross-embodiment%250Apre-training.%2520PEAC%2520not%2520only%2520provides%2520an%2520intuitive%2520optimization%2520strategy%2520for%250Across-embodiment%2520pre-training%2520but%2520also%2520can%2520integrate%2520flexibly%2520with%2520existing%250Aunsupervised%2520RL%2520methods%252C%2520facilitating%2520cross-embodiment%2520exploration%2520and%2520skill%250Adiscovery.%2520Extensive%2520experiments%2520in%2520both%2520simulated%2520%2528e.g.%252C%2520DMC%2520and%2520Robosuite%2529%250Aand%2520real-world%2520environments%2520%2528e.g.%252C%2520legged%2520locomotion%2529%2520demonstrate%2520that%2520PEAC%250Asignificantly%2520improves%2520adaptation%2520performance%2520and%2520cross-embodiment%250Ageneralization%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520overcoming%2520the%2520unique%250Achallenges%2520of%2520CEURL.%2520The%2520project%2520page%2520and%2520code%2520are%2520in%250Ahttps%253A//yingchengyang.github.io/ceurl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEAC%3A%20Unsupervised%20Pre-training%20for%20Cross-Embodiment%20Reinforcement%0A%20%20Learning&entry.906535625=Chengyang%20Ying%20and%20Zhongkai%20Hao%20and%20Xinning%20Zhou%20and%20Xuezhou%20Xu%20and%20Hang%20Su%20and%20Xingxing%20Zhang%20and%20Jun%20Zhu&entry.1292438233=%20%20Designing%20generalizable%20agents%20capable%20of%20adapting%20to%20diverse%20embodiments%20has%0Aachieved%20significant%20attention%20in%20Reinforcement%20Learning%20%28RL%29%2C%20which%20is%0Acritical%20for%20deploying%20RL%20agents%20in%20various%20real-world%20applications.%20Previous%0ACross-Embodiment%20RL%20approaches%20have%20focused%20on%20transferring%20knowledge%20across%0Aembodiments%20within%20specific%20tasks.%20These%20methods%20often%20result%20in%20knowledge%0Atightly%20coupled%20with%20those%20tasks%20and%20fail%20to%20adequately%20capture%20the%20distinct%0Acharacteristics%20of%20different%20embodiments.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20notion%20of%20Cross-Embodiment%20Unsupervised%20RL%20%28CEURL%29%2C%20which%0Aleverages%20unsupervised%20learning%20to%20enable%20agents%20to%20acquire%20embodiment-aware%0Aand%20task-agnostic%20knowledge%20through%20online%20interactions%20within%20reward-free%0Aenvironments.%20We%20formulate%20CEURL%20as%20a%20novel%20Controlled%20Embodiment%20Markov%0ADecision%20Process%20%28CE-MDP%29%20and%20systematically%20analyze%20CEURL%27s%20pre-training%0Aobjectives%20under%20CE-MDP.%20Based%20on%20these%20analyses%2C%20we%20develop%20a%20novel%20algorithm%0APre-trained%20Embodiment-Aware%20Control%20%28PEAC%29%20for%20handling%20CEURL%2C%20incorporating%0Aan%20intrinsic%20reward%20function%20specifically%20designed%20for%20cross-embodiment%0Apre-training.%20PEAC%20not%20only%20provides%20an%20intuitive%20optimization%20strategy%20for%0Across-embodiment%20pre-training%20but%20also%20can%20integrate%20flexibly%20with%20existing%0Aunsupervised%20RL%20methods%2C%20facilitating%20cross-embodiment%20exploration%20and%20skill%0Adiscovery.%20Extensive%20experiments%20in%20both%20simulated%20%28e.g.%2C%20DMC%20and%20Robosuite%29%0Aand%20real-world%20environments%20%28e.g.%2C%20legged%20locomotion%29%20demonstrate%20that%20PEAC%0Asignificantly%20improves%20adaptation%20performance%20and%20cross-embodiment%0Ageneralization%2C%20demonstrating%20its%20effectiveness%20in%20overcoming%20the%20unique%0Achallenges%20of%20CEURL.%20The%20project%20page%20and%20code%20are%20in%0Ahttps%3A//yingchengyang.github.io/ceurl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14073v2&entry.124074799=Read"},
{"title": "Physics meets Topology: Physics-informed topological neural networks for\n  learning rigid body dynamics", "author": "Amaury Wei and Olga Fink", "abstract": "  Rigid body interactions are fundamental to numerous scientific disciplines,\nbut remain challenging to simulate due to their abrupt nonlinear nature and\nsensitivity to complex, often unknown environmental factors. These challenges\ncall for adaptable learning-based methods capable of capturing complex\ninteractions beyond explicit physical models and simulations. While graph\nneural networks can handle simple scenarios, they struggle with complex scenes\nand long-term predictions. We introduce a novel framework for modeling rigid\nbody dynamics and learning collision interactions, addressing key limitations\nof existing graph-based methods. Our approach extends the traditional\nrepresentation of meshes by incorporating higher-order topology complexes,\noffering a physically consistent representation. Additionally, we propose a\nphysics-informed message-passing neural architecture, embedding physical laws\ndirectly in the model. Our method demonstrates superior accuracy, even during\nlong rollouts, and exhibits strong generalization to unseen scenarios.\nImportantly, this work addresses the challenge of multi-entity dynamic\ninteractions, with applications spanning diverse scientific and engineering\ndomains.\n", "link": "http://arxiv.org/abs/2411.11467v1", "date": "2024-11-18", "relevancy": 2.1479, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5633}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5581}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics%20meets%20Topology%3A%20Physics-informed%20topological%20neural%20networks%20for%0A%20%20learning%20rigid%20body%20dynamics&body=Title%3A%20Physics%20meets%20Topology%3A%20Physics-informed%20topological%20neural%20networks%20for%0A%20%20learning%20rigid%20body%20dynamics%0AAuthor%3A%20Amaury%20Wei%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics%2520meets%2520Topology%253A%2520Physics-informed%2520topological%2520neural%2520networks%2520for%250A%2520%2520learning%2520rigid%2520body%2520dynamics%26entry.906535625%3DAmaury%2520Wei%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Rigid%2520body%2520interactions%2520are%2520fundamental%2520to%2520numerous%2520scientific%2520disciplines%252C%250Abut%2520remain%2520challenging%2520to%2520simulate%2520due%2520to%2520their%2520abrupt%2520nonlinear%2520nature%2520and%250Asensitivity%2520to%2520complex%252C%2520often%2520unknown%2520environmental%2520factors.%2520These%2520challenges%250Acall%2520for%2520adaptable%2520learning-based%2520methods%2520capable%2520of%2520capturing%2520complex%250Ainteractions%2520beyond%2520explicit%2520physical%2520models%2520and%2520simulations.%2520While%2520graph%250Aneural%2520networks%2520can%2520handle%2520simple%2520scenarios%252C%2520they%2520struggle%2520with%2520complex%2520scenes%250Aand%2520long-term%2520predictions.%2520We%2520introduce%2520a%2520novel%2520framework%2520for%2520modeling%2520rigid%250Abody%2520dynamics%2520and%2520learning%2520collision%2520interactions%252C%2520addressing%2520key%2520limitations%250Aof%2520existing%2520graph-based%2520methods.%2520Our%2520approach%2520extends%2520the%2520traditional%250Arepresentation%2520of%2520meshes%2520by%2520incorporating%2520higher-order%2520topology%2520complexes%252C%250Aoffering%2520a%2520physically%2520consistent%2520representation.%2520Additionally%252C%2520we%2520propose%2520a%250Aphysics-informed%2520message-passing%2520neural%2520architecture%252C%2520embedding%2520physical%2520laws%250Adirectly%2520in%2520the%2520model.%2520Our%2520method%2520demonstrates%2520superior%2520accuracy%252C%2520even%2520during%250Along%2520rollouts%252C%2520and%2520exhibits%2520strong%2520generalization%2520to%2520unseen%2520scenarios.%250AImportantly%252C%2520this%2520work%2520addresses%2520the%2520challenge%2520of%2520multi-entity%2520dynamic%250Ainteractions%252C%2520with%2520applications%2520spanning%2520diverse%2520scientific%2520and%2520engineering%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics%20meets%20Topology%3A%20Physics-informed%20topological%20neural%20networks%20for%0A%20%20learning%20rigid%20body%20dynamics&entry.906535625=Amaury%20Wei%20and%20Olga%20Fink&entry.1292438233=%20%20Rigid%20body%20interactions%20are%20fundamental%20to%20numerous%20scientific%20disciplines%2C%0Abut%20remain%20challenging%20to%20simulate%20due%20to%20their%20abrupt%20nonlinear%20nature%20and%0Asensitivity%20to%20complex%2C%20often%20unknown%20environmental%20factors.%20These%20challenges%0Acall%20for%20adaptable%20learning-based%20methods%20capable%20of%20capturing%20complex%0Ainteractions%20beyond%20explicit%20physical%20models%20and%20simulations.%20While%20graph%0Aneural%20networks%20can%20handle%20simple%20scenarios%2C%20they%20struggle%20with%20complex%20scenes%0Aand%20long-term%20predictions.%20We%20introduce%20a%20novel%20framework%20for%20modeling%20rigid%0Abody%20dynamics%20and%20learning%20collision%20interactions%2C%20addressing%20key%20limitations%0Aof%20existing%20graph-based%20methods.%20Our%20approach%20extends%20the%20traditional%0Arepresentation%20of%20meshes%20by%20incorporating%20higher-order%20topology%20complexes%2C%0Aoffering%20a%20physically%20consistent%20representation.%20Additionally%2C%20we%20propose%20a%0Aphysics-informed%20message-passing%20neural%20architecture%2C%20embedding%20physical%20laws%0Adirectly%20in%20the%20model.%20Our%20method%20demonstrates%20superior%20accuracy%2C%20even%20during%0Along%20rollouts%2C%20and%20exhibits%20strong%20generalization%20to%20unseen%20scenarios.%0AImportantly%2C%20this%20work%20addresses%20the%20challenge%20of%20multi-entity%20dynamic%0Ainteractions%2C%20with%20applications%20spanning%20diverse%20scientific%20and%20engineering%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11467v1&entry.124074799=Read"},
{"title": "Sequential Gaussian Variational Inference for Nonlinear State Estimation\n  and Its Application in Robot Navigation", "author": "Min-Won Seo and Solmaz S. Kia", "abstract": "  Probabilistic state estimation is essential for robots navigating uncertain\nenvironments. Accurately and efficiently managing uncertainty in estimated\nstates is key to robust robotic operation. However, nonlinearities in robotic\nplatforms pose significant challenges that require advanced estimation\ntechniques. Gaussian variational inference (GVI) offers an optimization\nperspective on the estimation problem, providing analytically tractable\nsolutions and efficiencies derived from the geometry of Gaussian space. We\npropose a Sequential Gaussian Variational Inference (S-GVI) method to address\nnonlinearity and provide efficient sequential inference processes. Our approach\nintegrates sequential Bayesian principles into the GVI framework, which are\naddressed using statistical approximations and gradient updates on the\ninformation geometry. Validations through simulations and real-world\nexperiments demonstrate significant improvements in state estimation over the\nMaximum A Posteriori (MAP) estimation method.\n", "link": "http://arxiv.org/abs/2407.05478v5", "date": "2024-11-18", "relevancy": 2.1451, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5256}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Gaussian%20Variational%20Inference%20for%20Nonlinear%20State%20Estimation%0A%20%20and%20Its%20Application%20in%20Robot%20Navigation&body=Title%3A%20Sequential%20Gaussian%20Variational%20Inference%20for%20Nonlinear%20State%20Estimation%0A%20%20and%20Its%20Application%20in%20Robot%20Navigation%0AAuthor%3A%20Min-Won%20Seo%20and%20Solmaz%20S.%20Kia%0AAbstract%3A%20%20%20Probabilistic%20state%20estimation%20is%20essential%20for%20robots%20navigating%20uncertain%0Aenvironments.%20Accurately%20and%20efficiently%20managing%20uncertainty%20in%20estimated%0Astates%20is%20key%20to%20robust%20robotic%20operation.%20However%2C%20nonlinearities%20in%20robotic%0Aplatforms%20pose%20significant%20challenges%20that%20require%20advanced%20estimation%0Atechniques.%20Gaussian%20variational%20inference%20%28GVI%29%20offers%20an%20optimization%0Aperspective%20on%20the%20estimation%20problem%2C%20providing%20analytically%20tractable%0Asolutions%20and%20efficiencies%20derived%20from%20the%20geometry%20of%20Gaussian%20space.%20We%0Apropose%20a%20Sequential%20Gaussian%20Variational%20Inference%20%28S-GVI%29%20method%20to%20address%0Anonlinearity%20and%20provide%20efficient%20sequential%20inference%20processes.%20Our%20approach%0Aintegrates%20sequential%20Bayesian%20principles%20into%20the%20GVI%20framework%2C%20which%20are%0Aaddressed%20using%20statistical%20approximations%20and%20gradient%20updates%20on%20the%0Ainformation%20geometry.%20Validations%20through%20simulations%20and%20real-world%0Aexperiments%20demonstrate%20significant%20improvements%20in%20state%20estimation%20over%20the%0AMaximum%20A%20Posteriori%20%28MAP%29%20estimation%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05478v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Gaussian%2520Variational%2520Inference%2520for%2520Nonlinear%2520State%2520Estimation%250A%2520%2520and%2520Its%2520Application%2520in%2520Robot%2520Navigation%26entry.906535625%3DMin-Won%2520Seo%2520and%2520Solmaz%2520S.%2520Kia%26entry.1292438233%3D%2520%2520Probabilistic%2520state%2520estimation%2520is%2520essential%2520for%2520robots%2520navigating%2520uncertain%250Aenvironments.%2520Accurately%2520and%2520efficiently%2520managing%2520uncertainty%2520in%2520estimated%250Astates%2520is%2520key%2520to%2520robust%2520robotic%2520operation.%2520However%252C%2520nonlinearities%2520in%2520robotic%250Aplatforms%2520pose%2520significant%2520challenges%2520that%2520require%2520advanced%2520estimation%250Atechniques.%2520Gaussian%2520variational%2520inference%2520%2528GVI%2529%2520offers%2520an%2520optimization%250Aperspective%2520on%2520the%2520estimation%2520problem%252C%2520providing%2520analytically%2520tractable%250Asolutions%2520and%2520efficiencies%2520derived%2520from%2520the%2520geometry%2520of%2520Gaussian%2520space.%2520We%250Apropose%2520a%2520Sequential%2520Gaussian%2520Variational%2520Inference%2520%2528S-GVI%2529%2520method%2520to%2520address%250Anonlinearity%2520and%2520provide%2520efficient%2520sequential%2520inference%2520processes.%2520Our%2520approach%250Aintegrates%2520sequential%2520Bayesian%2520principles%2520into%2520the%2520GVI%2520framework%252C%2520which%2520are%250Aaddressed%2520using%2520statistical%2520approximations%2520and%2520gradient%2520updates%2520on%2520the%250Ainformation%2520geometry.%2520Validations%2520through%2520simulations%2520and%2520real-world%250Aexperiments%2520demonstrate%2520significant%2520improvements%2520in%2520state%2520estimation%2520over%2520the%250AMaximum%2520A%2520Posteriori%2520%2528MAP%2529%2520estimation%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05478v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Gaussian%20Variational%20Inference%20for%20Nonlinear%20State%20Estimation%0A%20%20and%20Its%20Application%20in%20Robot%20Navigation&entry.906535625=Min-Won%20Seo%20and%20Solmaz%20S.%20Kia&entry.1292438233=%20%20Probabilistic%20state%20estimation%20is%20essential%20for%20robots%20navigating%20uncertain%0Aenvironments.%20Accurately%20and%20efficiently%20managing%20uncertainty%20in%20estimated%0Astates%20is%20key%20to%20robust%20robotic%20operation.%20However%2C%20nonlinearities%20in%20robotic%0Aplatforms%20pose%20significant%20challenges%20that%20require%20advanced%20estimation%0Atechniques.%20Gaussian%20variational%20inference%20%28GVI%29%20offers%20an%20optimization%0Aperspective%20on%20the%20estimation%20problem%2C%20providing%20analytically%20tractable%0Asolutions%20and%20efficiencies%20derived%20from%20the%20geometry%20of%20Gaussian%20space.%20We%0Apropose%20a%20Sequential%20Gaussian%20Variational%20Inference%20%28S-GVI%29%20method%20to%20address%0Anonlinearity%20and%20provide%20efficient%20sequential%20inference%20processes.%20Our%20approach%0Aintegrates%20sequential%20Bayesian%20principles%20into%20the%20GVI%20framework%2C%20which%20are%0Aaddressed%20using%20statistical%20approximations%20and%20gradient%20updates%20on%20the%0Ainformation%20geometry.%20Validations%20through%20simulations%20and%20real-world%0Aexperiments%20demonstrate%20significant%20improvements%20in%20state%20estimation%20over%20the%0AMaximum%20A%20Posteriori%20%28MAP%29%20estimation%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05478v5&entry.124074799=Read"},
{"title": "V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with\n  Denoising Diffusion", "author": "Xun Huang and Jinlong Wang and Qiming Xia and Siheng Chen and Bisheng Yang and Xin Li and Cheng Wang and Chenglu Wen", "abstract": "  Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D\nobject detection using LiDAR and camera data. However, these methods suffer\nfrom performance degradation in adverse weather conditions. The weatherrobust\n4D radar provides Doppler and additional geometric information, raising the\npossibility of addressing this challenge. To this end, we present V2X-R, the\nfirst simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R\ncontains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point\nclouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes.\nSubsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for\n3D object detection and implement it with various fusion strategies. To achieve\nweather-robust detection, we additionally propose a Multi-modal Denoising\nDiffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D\nradar feature as a condition to prompt the diffusion model to denoise noisy\nLiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline\ndemonstrates superior performance in the V2X-R dataset. Over and above this,\nour MDD module further improved the performance of basic fusion model by up to\n5.73%/6.70% in foggy/snowy conditions with barely disrupting normal\nperformance. The dataset and code will be publicly available at:\nhttps://github.com/ylwhxht/V2X-R.\n", "link": "http://arxiv.org/abs/2411.08402v2", "date": "2024-11-18", "relevancy": 2.1425, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20for%203D%20Object%20Detection%20with%0A%20%20Denoising%20Diffusion&body=Title%3A%20V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20for%203D%20Object%20Detection%20with%0A%20%20Denoising%20Diffusion%0AAuthor%3A%20Xun%20Huang%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Siheng%20Chen%20and%20Bisheng%20Yang%20and%20Xin%20Li%20and%20Cheng%20Wang%20and%20Chenglu%20Wen%0AAbstract%3A%20%20%20Current%20Vehicle-to-Everything%20%28V2X%29%20systems%20have%20significantly%20enhanced%203D%0Aobject%20detection%20using%20LiDAR%20and%20camera%20data.%20However%2C%20these%20methods%20suffer%0Afrom%20performance%20degradation%20in%20adverse%20weather%20conditions.%20The%20weatherrobust%0A4D%20radar%20provides%20Doppler%20and%20additional%20geometric%20information%2C%20raising%20the%0Apossibility%20of%20addressing%20this%20challenge.%20To%20this%20end%2C%20we%20present%20V2X-R%2C%20the%0Afirst%20simulated%20V2X%20dataset%20incorporating%20LiDAR%2C%20camera%2C%20and%204D%20radar.%20V2X-R%0Acontains%2012%2C079%20scenarios%20with%2037%2C727%20frames%20of%20LiDAR%20and%204D%20radar%20point%0Aclouds%2C%20150%2C908%20images%2C%20and%20170%2C859%20annotated%203D%20vehicle%20bounding%20boxes.%0ASubsequently%2C%20we%20propose%20a%20novel%20cooperative%20LiDAR-4D%20radar%20fusion%20pipeline%20for%0A3D%20object%20detection%20and%20implement%20it%20with%20various%20fusion%20strategies.%20To%20achieve%0Aweather-robust%20detection%2C%20we%20additionally%20propose%20a%20Multi-modal%20Denoising%0ADiffusion%20%28MDD%29%20module%20in%20our%20fusion%20pipeline.%20MDD%20utilizes%20weather-robust%204D%0Aradar%20feature%20as%20a%20condition%20to%20prompt%20the%20diffusion%20model%20to%20denoise%20noisy%0ALiDAR%20features.%20Experiments%20show%20that%20our%20LiDAR-4D%20radar%20fusion%20pipeline%0Ademonstrates%20superior%20performance%20in%20the%20V2X-R%20dataset.%20Over%20and%20above%20this%2C%0Aour%20MDD%20module%20further%20improved%20the%20performance%20of%20basic%20fusion%20model%20by%20up%20to%0A5.73%25/6.70%25%20in%20foggy/snowy%20conditions%20with%20barely%20disrupting%20normal%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ylwhxht/V2X-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2X-R%253A%2520Cooperative%2520LiDAR-4D%2520Radar%2520Fusion%2520for%25203D%2520Object%2520Detection%2520with%250A%2520%2520Denoising%2520Diffusion%26entry.906535625%3DXun%2520Huang%2520and%2520Jinlong%2520Wang%2520and%2520Qiming%2520Xia%2520and%2520Siheng%2520Chen%2520and%2520Bisheng%2520Yang%2520and%2520Xin%2520Li%2520and%2520Cheng%2520Wang%2520and%2520Chenglu%2520Wen%26entry.1292438233%3D%2520%2520Current%2520Vehicle-to-Everything%2520%2528V2X%2529%2520systems%2520have%2520significantly%2520enhanced%25203D%250Aobject%2520detection%2520using%2520LiDAR%2520and%2520camera%2520data.%2520However%252C%2520these%2520methods%2520suffer%250Afrom%2520performance%2520degradation%2520in%2520adverse%2520weather%2520conditions.%2520The%2520weatherrobust%250A4D%2520radar%2520provides%2520Doppler%2520and%2520additional%2520geometric%2520information%252C%2520raising%2520the%250Apossibility%2520of%2520addressing%2520this%2520challenge.%2520To%2520this%2520end%252C%2520we%2520present%2520V2X-R%252C%2520the%250Afirst%2520simulated%2520V2X%2520dataset%2520incorporating%2520LiDAR%252C%2520camera%252C%2520and%25204D%2520radar.%2520V2X-R%250Acontains%252012%252C079%2520scenarios%2520with%252037%252C727%2520frames%2520of%2520LiDAR%2520and%25204D%2520radar%2520point%250Aclouds%252C%2520150%252C908%2520images%252C%2520and%2520170%252C859%2520annotated%25203D%2520vehicle%2520bounding%2520boxes.%250ASubsequently%252C%2520we%2520propose%2520a%2520novel%2520cooperative%2520LiDAR-4D%2520radar%2520fusion%2520pipeline%2520for%250A3D%2520object%2520detection%2520and%2520implement%2520it%2520with%2520various%2520fusion%2520strategies.%2520To%2520achieve%250Aweather-robust%2520detection%252C%2520we%2520additionally%2520propose%2520a%2520Multi-modal%2520Denoising%250ADiffusion%2520%2528MDD%2529%2520module%2520in%2520our%2520fusion%2520pipeline.%2520MDD%2520utilizes%2520weather-robust%25204D%250Aradar%2520feature%2520as%2520a%2520condition%2520to%2520prompt%2520the%2520diffusion%2520model%2520to%2520denoise%2520noisy%250ALiDAR%2520features.%2520Experiments%2520show%2520that%2520our%2520LiDAR-4D%2520radar%2520fusion%2520pipeline%250Ademonstrates%2520superior%2520performance%2520in%2520the%2520V2X-R%2520dataset.%2520Over%2520and%2520above%2520this%252C%250Aour%2520MDD%2520module%2520further%2520improved%2520the%2520performance%2520of%2520basic%2520fusion%2520model%2520by%2520up%2520to%250A5.73%2525/6.70%2525%2520in%2520foggy/snowy%2520conditions%2520with%2520barely%2520disrupting%2520normal%250Aperformance.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/ylwhxht/V2X-R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X-R%3A%20Cooperative%20LiDAR-4D%20Radar%20Fusion%20for%203D%20Object%20Detection%20with%0A%20%20Denoising%20Diffusion&entry.906535625=Xun%20Huang%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Siheng%20Chen%20and%20Bisheng%20Yang%20and%20Xin%20Li%20and%20Cheng%20Wang%20and%20Chenglu%20Wen&entry.1292438233=%20%20Current%20Vehicle-to-Everything%20%28V2X%29%20systems%20have%20significantly%20enhanced%203D%0Aobject%20detection%20using%20LiDAR%20and%20camera%20data.%20However%2C%20these%20methods%20suffer%0Afrom%20performance%20degradation%20in%20adverse%20weather%20conditions.%20The%20weatherrobust%0A4D%20radar%20provides%20Doppler%20and%20additional%20geometric%20information%2C%20raising%20the%0Apossibility%20of%20addressing%20this%20challenge.%20To%20this%20end%2C%20we%20present%20V2X-R%2C%20the%0Afirst%20simulated%20V2X%20dataset%20incorporating%20LiDAR%2C%20camera%2C%20and%204D%20radar.%20V2X-R%0Acontains%2012%2C079%20scenarios%20with%2037%2C727%20frames%20of%20LiDAR%20and%204D%20radar%20point%0Aclouds%2C%20150%2C908%20images%2C%20and%20170%2C859%20annotated%203D%20vehicle%20bounding%20boxes.%0ASubsequently%2C%20we%20propose%20a%20novel%20cooperative%20LiDAR-4D%20radar%20fusion%20pipeline%20for%0A3D%20object%20detection%20and%20implement%20it%20with%20various%20fusion%20strategies.%20To%20achieve%0Aweather-robust%20detection%2C%20we%20additionally%20propose%20a%20Multi-modal%20Denoising%0ADiffusion%20%28MDD%29%20module%20in%20our%20fusion%20pipeline.%20MDD%20utilizes%20weather-robust%204D%0Aradar%20feature%20as%20a%20condition%20to%20prompt%20the%20diffusion%20model%20to%20denoise%20noisy%0ALiDAR%20features.%20Experiments%20show%20that%20our%20LiDAR-4D%20radar%20fusion%20pipeline%0Ademonstrates%20superior%20performance%20in%20the%20V2X-R%20dataset.%20Over%20and%20above%20this%2C%0Aour%20MDD%20module%20further%20improved%20the%20performance%20of%20basic%20fusion%20model%20by%20up%20to%0A5.73%25/6.70%25%20in%20foggy/snowy%20conditions%20with%20barely%20disrupting%20normal%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ylwhxht/V2X-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08402v2&entry.124074799=Read"},
{"title": "A Pre-Trained Graph-Based Model for Adaptive Sequencing of Educational\n  Documents", "author": "Jean Vassoyan and Anan Sch\u00fctt and Jill-J\u00eann Vie and Arun-Balajiee Lekshmi-Narayanan and Elisabeth Andr\u00e9 and Nicolas Vayatis", "abstract": "  Massive Open Online Courses (MOOCs) have greatly contributed to making\neducation more accessible.However, many MOOCs maintain a rigid,\none-size-fits-all structure that fails to address the diverse needs and\nbackgrounds of individual learners.Learning path personalization aims to\naddress this limitation, by tailoring sequences of educational content to\noptimize individual student learning outcomes.Existing approaches, however,\noften require either massive student interaction data or extensive expert\nannotation, limiting their broad application.In this study, we introduce a\nnovel data-efficient framework for learning path personalization that operates\nwithout expert annotation.Our method employs a flexible recommender system\npre-trained with reinforcement learning on a dataset of raw course\nmaterials.Through experiments on semi-synthetic data, we show that this\npre-training stage substantially improves data-efficiency in a range of\nadaptive learning scenarios featuring new educational materials.This opens up\nnew perspectives for the design of foundation models for adaptive learning.\n", "link": "http://arxiv.org/abs/2411.11520v1", "date": "2024-11-18", "relevancy": 2.1149, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5401}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pre-Trained%20Graph-Based%20Model%20for%20Adaptive%20Sequencing%20of%20Educational%0A%20%20Documents&body=Title%3A%20A%20Pre-Trained%20Graph-Based%20Model%20for%20Adaptive%20Sequencing%20of%20Educational%0A%20%20Documents%0AAuthor%3A%20Jean%20Vassoyan%20and%20Anan%20Sch%C3%BCtt%20and%20Jill-J%C3%AAnn%20Vie%20and%20Arun-Balajiee%20Lekshmi-Narayanan%20and%20Elisabeth%20Andr%C3%A9%20and%20Nicolas%20Vayatis%0AAbstract%3A%20%20%20Massive%20Open%20Online%20Courses%20%28MOOCs%29%20have%20greatly%20contributed%20to%20making%0Aeducation%20more%20accessible.However%2C%20many%20MOOCs%20maintain%20a%20rigid%2C%0Aone-size-fits-all%20structure%20that%20fails%20to%20address%20the%20diverse%20needs%20and%0Abackgrounds%20of%20individual%20learners.Learning%20path%20personalization%20aims%20to%0Aaddress%20this%20limitation%2C%20by%20tailoring%20sequences%20of%20educational%20content%20to%0Aoptimize%20individual%20student%20learning%20outcomes.Existing%20approaches%2C%20however%2C%0Aoften%20require%20either%20massive%20student%20interaction%20data%20or%20extensive%20expert%0Aannotation%2C%20limiting%20their%20broad%20application.In%20this%20study%2C%20we%20introduce%20a%0Anovel%20data-efficient%20framework%20for%20learning%20path%20personalization%20that%20operates%0Awithout%20expert%20annotation.Our%20method%20employs%20a%20flexible%20recommender%20system%0Apre-trained%20with%20reinforcement%20learning%20on%20a%20dataset%20of%20raw%20course%0Amaterials.Through%20experiments%20on%20semi-synthetic%20data%2C%20we%20show%20that%20this%0Apre-training%20stage%20substantially%20improves%20data-efficiency%20in%20a%20range%20of%0Aadaptive%20learning%20scenarios%20featuring%20new%20educational%20materials.This%20opens%20up%0Anew%20perspectives%20for%20the%20design%20of%20foundation%20models%20for%20adaptive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pre-Trained%2520Graph-Based%2520Model%2520for%2520Adaptive%2520Sequencing%2520of%2520Educational%250A%2520%2520Documents%26entry.906535625%3DJean%2520Vassoyan%2520and%2520Anan%2520Sch%25C3%25BCtt%2520and%2520Jill-J%25C3%25AAnn%2520Vie%2520and%2520Arun-Balajiee%2520Lekshmi-Narayanan%2520and%2520Elisabeth%2520Andr%25C3%25A9%2520and%2520Nicolas%2520Vayatis%26entry.1292438233%3D%2520%2520Massive%2520Open%2520Online%2520Courses%2520%2528MOOCs%2529%2520have%2520greatly%2520contributed%2520to%2520making%250Aeducation%2520more%2520accessible.However%252C%2520many%2520MOOCs%2520maintain%2520a%2520rigid%252C%250Aone-size-fits-all%2520structure%2520that%2520fails%2520to%2520address%2520the%2520diverse%2520needs%2520and%250Abackgrounds%2520of%2520individual%2520learners.Learning%2520path%2520personalization%2520aims%2520to%250Aaddress%2520this%2520limitation%252C%2520by%2520tailoring%2520sequences%2520of%2520educational%2520content%2520to%250Aoptimize%2520individual%2520student%2520learning%2520outcomes.Existing%2520approaches%252C%2520however%252C%250Aoften%2520require%2520either%2520massive%2520student%2520interaction%2520data%2520or%2520extensive%2520expert%250Aannotation%252C%2520limiting%2520their%2520broad%2520application.In%2520this%2520study%252C%2520we%2520introduce%2520a%250Anovel%2520data-efficient%2520framework%2520for%2520learning%2520path%2520personalization%2520that%2520operates%250Awithout%2520expert%2520annotation.Our%2520method%2520employs%2520a%2520flexible%2520recommender%2520system%250Apre-trained%2520with%2520reinforcement%2520learning%2520on%2520a%2520dataset%2520of%2520raw%2520course%250Amaterials.Through%2520experiments%2520on%2520semi-synthetic%2520data%252C%2520we%2520show%2520that%2520this%250Apre-training%2520stage%2520substantially%2520improves%2520data-efficiency%2520in%2520a%2520range%2520of%250Aadaptive%2520learning%2520scenarios%2520featuring%2520new%2520educational%2520materials.This%2520opens%2520up%250Anew%2520perspectives%2520for%2520the%2520design%2520of%2520foundation%2520models%2520for%2520adaptive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pre-Trained%20Graph-Based%20Model%20for%20Adaptive%20Sequencing%20of%20Educational%0A%20%20Documents&entry.906535625=Jean%20Vassoyan%20and%20Anan%20Sch%C3%BCtt%20and%20Jill-J%C3%AAnn%20Vie%20and%20Arun-Balajiee%20Lekshmi-Narayanan%20and%20Elisabeth%20Andr%C3%A9%20and%20Nicolas%20Vayatis&entry.1292438233=%20%20Massive%20Open%20Online%20Courses%20%28MOOCs%29%20have%20greatly%20contributed%20to%20making%0Aeducation%20more%20accessible.However%2C%20many%20MOOCs%20maintain%20a%20rigid%2C%0Aone-size-fits-all%20structure%20that%20fails%20to%20address%20the%20diverse%20needs%20and%0Abackgrounds%20of%20individual%20learners.Learning%20path%20personalization%20aims%20to%0Aaddress%20this%20limitation%2C%20by%20tailoring%20sequences%20of%20educational%20content%20to%0Aoptimize%20individual%20student%20learning%20outcomes.Existing%20approaches%2C%20however%2C%0Aoften%20require%20either%20massive%20student%20interaction%20data%20or%20extensive%20expert%0Aannotation%2C%20limiting%20their%20broad%20application.In%20this%20study%2C%20we%20introduce%20a%0Anovel%20data-efficient%20framework%20for%20learning%20path%20personalization%20that%20operates%0Awithout%20expert%20annotation.Our%20method%20employs%20a%20flexible%20recommender%20system%0Apre-trained%20with%20reinforcement%20learning%20on%20a%20dataset%20of%20raw%20course%0Amaterials.Through%20experiments%20on%20semi-synthetic%20data%2C%20we%20show%20that%20this%0Apre-training%20stage%20substantially%20improves%20data-efficiency%20in%20a%20range%20of%0Aadaptive%20learning%20scenarios%20featuring%20new%20educational%20materials.This%20opens%20up%0Anew%20perspectives%20for%20the%20design%20of%20foundation%20models%20for%20adaptive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11520v1&entry.124074799=Read"},
{"title": "Unveiling the Inflexibility of Adaptive Embedding in Traffic Forecasting", "author": "Hongjun Wang and Jiyuan Chen and Lingyu Zhang and Renhe Jiang and Xuan Song", "abstract": "  Spatiotemporal Graph Neural Networks (ST-GNNs) and Transformers have shown\nsignificant promise in traffic forecasting by effectively modeling temporal and\nspatial correlations. However, rapid urbanization in recent years has led to\ndynamic shifts in traffic patterns and travel demand, posing major challenges\nfor accurate long-term traffic prediction. The generalization capability of\nST-GNNs in extended temporal scenarios and cross-city applications remains\nlargely unexplored. In this study, we evaluate state-of-the-art models on an\nextended traffic benchmark and observe substantial performance degradation in\nexisting ST-GNNs over time, which we attribute to their limited inductive\ncapabilities. Our analysis reveals that this degradation stems from an\ninability to adapt to evolving spatial relationships within urban environments.\nTo address this limitation, we reconsider the design of adaptive embeddings and\npropose a Principal Component Analysis (PCA) embedding approach that enables\nmodels to adapt to new scenarios without retraining. We incorporate PCA\nembeddings into existing ST-GNN and Transformer architectures, achieving marked\nimprovements in performance. Notably, PCA embeddings allow for flexibility in\ngraph structures between training and testing, enabling models trained on one\ncity to perform zero-shot predictions on other cities. This adaptability\ndemonstrates the potential of PCA embeddings in enhancing the robustness and\ngeneralization of spatiotemporal models.\n", "link": "http://arxiv.org/abs/2411.11448v1", "date": "2024-11-18", "relevancy": 2.0826, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Inflexibility%20of%20Adaptive%20Embedding%20in%20Traffic%20Forecasting&body=Title%3A%20Unveiling%20the%20Inflexibility%20of%20Adaptive%20Embedding%20in%20Traffic%20Forecasting%0AAuthor%3A%20Hongjun%20Wang%20and%20Jiyuan%20Chen%20and%20Lingyu%20Zhang%20and%20Renhe%20Jiang%20and%20Xuan%20Song%0AAbstract%3A%20%20%20Spatiotemporal%20Graph%20Neural%20Networks%20%28ST-GNNs%29%20and%20Transformers%20have%20shown%0Asignificant%20promise%20in%20traffic%20forecasting%20by%20effectively%20modeling%20temporal%20and%0Aspatial%20correlations.%20However%2C%20rapid%20urbanization%20in%20recent%20years%20has%20led%20to%0Adynamic%20shifts%20in%20traffic%20patterns%20and%20travel%20demand%2C%20posing%20major%20challenges%0Afor%20accurate%20long-term%20traffic%20prediction.%20The%20generalization%20capability%20of%0AST-GNNs%20in%20extended%20temporal%20scenarios%20and%20cross-city%20applications%20remains%0Alargely%20unexplored.%20In%20this%20study%2C%20we%20evaluate%20state-of-the-art%20models%20on%20an%0Aextended%20traffic%20benchmark%20and%20observe%20substantial%20performance%20degradation%20in%0Aexisting%20ST-GNNs%20over%20time%2C%20which%20we%20attribute%20to%20their%20limited%20inductive%0Acapabilities.%20Our%20analysis%20reveals%20that%20this%20degradation%20stems%20from%20an%0Ainability%20to%20adapt%20to%20evolving%20spatial%20relationships%20within%20urban%20environments.%0ATo%20address%20this%20limitation%2C%20we%20reconsider%20the%20design%20of%20adaptive%20embeddings%20and%0Apropose%20a%20Principal%20Component%20Analysis%20%28PCA%29%20embedding%20approach%20that%20enables%0Amodels%20to%20adapt%20to%20new%20scenarios%20without%20retraining.%20We%20incorporate%20PCA%0Aembeddings%20into%20existing%20ST-GNN%20and%20Transformer%20architectures%2C%20achieving%20marked%0Aimprovements%20in%20performance.%20Notably%2C%20PCA%20embeddings%20allow%20for%20flexibility%20in%0Agraph%20structures%20between%20training%20and%20testing%2C%20enabling%20models%20trained%20on%20one%0Acity%20to%20perform%20zero-shot%20predictions%20on%20other%20cities.%20This%20adaptability%0Ademonstrates%20the%20potential%20of%20PCA%20embeddings%20in%20enhancing%20the%20robustness%20and%0Ageneralization%20of%20spatiotemporal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Inflexibility%2520of%2520Adaptive%2520Embedding%2520in%2520Traffic%2520Forecasting%26entry.906535625%3DHongjun%2520Wang%2520and%2520Jiyuan%2520Chen%2520and%2520Lingyu%2520Zhang%2520and%2520Renhe%2520Jiang%2520and%2520Xuan%2520Song%26entry.1292438233%3D%2520%2520Spatiotemporal%2520Graph%2520Neural%2520Networks%2520%2528ST-GNNs%2529%2520and%2520Transformers%2520have%2520shown%250Asignificant%2520promise%2520in%2520traffic%2520forecasting%2520by%2520effectively%2520modeling%2520temporal%2520and%250Aspatial%2520correlations.%2520However%252C%2520rapid%2520urbanization%2520in%2520recent%2520years%2520has%2520led%2520to%250Adynamic%2520shifts%2520in%2520traffic%2520patterns%2520and%2520travel%2520demand%252C%2520posing%2520major%2520challenges%250Afor%2520accurate%2520long-term%2520traffic%2520prediction.%2520The%2520generalization%2520capability%2520of%250AST-GNNs%2520in%2520extended%2520temporal%2520scenarios%2520and%2520cross-city%2520applications%2520remains%250Alargely%2520unexplored.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520state-of-the-art%2520models%2520on%2520an%250Aextended%2520traffic%2520benchmark%2520and%2520observe%2520substantial%2520performance%2520degradation%2520in%250Aexisting%2520ST-GNNs%2520over%2520time%252C%2520which%2520we%2520attribute%2520to%2520their%2520limited%2520inductive%250Acapabilities.%2520Our%2520analysis%2520reveals%2520that%2520this%2520degradation%2520stems%2520from%2520an%250Ainability%2520to%2520adapt%2520to%2520evolving%2520spatial%2520relationships%2520within%2520urban%2520environments.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520reconsider%2520the%2520design%2520of%2520adaptive%2520embeddings%2520and%250Apropose%2520a%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520embedding%2520approach%2520that%2520enables%250Amodels%2520to%2520adapt%2520to%2520new%2520scenarios%2520without%2520retraining.%2520We%2520incorporate%2520PCA%250Aembeddings%2520into%2520existing%2520ST-GNN%2520and%2520Transformer%2520architectures%252C%2520achieving%2520marked%250Aimprovements%2520in%2520performance.%2520Notably%252C%2520PCA%2520embeddings%2520allow%2520for%2520flexibility%2520in%250Agraph%2520structures%2520between%2520training%2520and%2520testing%252C%2520enabling%2520models%2520trained%2520on%2520one%250Acity%2520to%2520perform%2520zero-shot%2520predictions%2520on%2520other%2520cities.%2520This%2520adaptability%250Ademonstrates%2520the%2520potential%2520of%2520PCA%2520embeddings%2520in%2520enhancing%2520the%2520robustness%2520and%250Ageneralization%2520of%2520spatiotemporal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Inflexibility%20of%20Adaptive%20Embedding%20in%20Traffic%20Forecasting&entry.906535625=Hongjun%20Wang%20and%20Jiyuan%20Chen%20and%20Lingyu%20Zhang%20and%20Renhe%20Jiang%20and%20Xuan%20Song&entry.1292438233=%20%20Spatiotemporal%20Graph%20Neural%20Networks%20%28ST-GNNs%29%20and%20Transformers%20have%20shown%0Asignificant%20promise%20in%20traffic%20forecasting%20by%20effectively%20modeling%20temporal%20and%0Aspatial%20correlations.%20However%2C%20rapid%20urbanization%20in%20recent%20years%20has%20led%20to%0Adynamic%20shifts%20in%20traffic%20patterns%20and%20travel%20demand%2C%20posing%20major%20challenges%0Afor%20accurate%20long-term%20traffic%20prediction.%20The%20generalization%20capability%20of%0AST-GNNs%20in%20extended%20temporal%20scenarios%20and%20cross-city%20applications%20remains%0Alargely%20unexplored.%20In%20this%20study%2C%20we%20evaluate%20state-of-the-art%20models%20on%20an%0Aextended%20traffic%20benchmark%20and%20observe%20substantial%20performance%20degradation%20in%0Aexisting%20ST-GNNs%20over%20time%2C%20which%20we%20attribute%20to%20their%20limited%20inductive%0Acapabilities.%20Our%20analysis%20reveals%20that%20this%20degradation%20stems%20from%20an%0Ainability%20to%20adapt%20to%20evolving%20spatial%20relationships%20within%20urban%20environments.%0ATo%20address%20this%20limitation%2C%20we%20reconsider%20the%20design%20of%20adaptive%20embeddings%20and%0Apropose%20a%20Principal%20Component%20Analysis%20%28PCA%29%20embedding%20approach%20that%20enables%0Amodels%20to%20adapt%20to%20new%20scenarios%20without%20retraining.%20We%20incorporate%20PCA%0Aembeddings%20into%20existing%20ST-GNN%20and%20Transformer%20architectures%2C%20achieving%20marked%0Aimprovements%20in%20performance.%20Notably%2C%20PCA%20embeddings%20allow%20for%20flexibility%20in%0Agraph%20structures%20between%20training%20and%20testing%2C%20enabling%20models%20trained%20on%20one%0Acity%20to%20perform%20zero-shot%20predictions%20on%20other%20cities.%20This%20adaptability%0Ademonstrates%20the%20potential%20of%20PCA%20embeddings%20in%20enhancing%20the%20robustness%20and%0Ageneralization%20of%20spatiotemporal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11448v1&entry.124074799=Read"},
{"title": "High-Speed Cornering Control and Real-Vehicle Deployment for Autonomous\n  Electric Vehicles", "author": "Shiyue Zhao and Junzhi Zhang and Neda Masoud and Yuhong Jiang and Heye Huang and Tao Liu", "abstract": "  Executing drift maneuvers during high-speed cornering presents significant\nchallenges for autonomous vehicles, yet offers the potential to minimize\nturning time and enhance driving dynamics. While reinforcement learning (RL)\nhas shown promising results in simulated environments, discrepancies between\nsimulations and real-world conditions have limited its practical deployment.\nThis study introduces an innovative control framework that integrates\ntrajectory optimization with drift maneuvers, aiming to improve the algorithm's\nadaptability for real-vehicle implementation. We leveraged Bezier-based\npre-trajectory optimization to enhance rewards and optimize the controller\nthrough Twin Delayed Deep Deterministic Policy Gradient (TD3) in a simulated\nenvironment. For real-world deployment, we implement a hybrid RL-MPC fusion\nmechanism, , where TD3-derived maneuvers serve as primary inputs for a Model\nPredictive Controller (MPC). This integration enables precise real-time\ntracking of the optimal trajectory, with MPC providing corrective inputs to\nbridge the gap between simulation and reality. The efficacy of this method is\nvalidated through real-vehicle tests on consumer-grade electric vehicles,\nfocusing on drift U-turns and drift right-angle turns. The control outcomes of\nthese real-vehicle tests are thoroughly documented in the paper, supported by\nsupplementary video evidence (https://youtu.be/5wp67FcpfL8). Notably, this\nstudy is the first to deploy and apply an RL-based transient drift cornering\nalgorithm on consumer-grade electric vehicles.\n", "link": "http://arxiv.org/abs/2411.11762v1", "date": "2024-11-18", "relevancy": 2.0778, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5368}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Speed%20Cornering%20Control%20and%20Real-Vehicle%20Deployment%20for%20Autonomous%0A%20%20Electric%20Vehicles&body=Title%3A%20High-Speed%20Cornering%20Control%20and%20Real-Vehicle%20Deployment%20for%20Autonomous%0A%20%20Electric%20Vehicles%0AAuthor%3A%20Shiyue%20Zhao%20and%20Junzhi%20Zhang%20and%20Neda%20Masoud%20and%20Yuhong%20Jiang%20and%20Heye%20Huang%20and%20Tao%20Liu%0AAbstract%3A%20%20%20Executing%20drift%20maneuvers%20during%20high-speed%20cornering%20presents%20significant%0Achallenges%20for%20autonomous%20vehicles%2C%20yet%20offers%20the%20potential%20to%20minimize%0Aturning%20time%20and%20enhance%20driving%20dynamics.%20While%20reinforcement%20learning%20%28RL%29%0Ahas%20shown%20promising%20results%20in%20simulated%20environments%2C%20discrepancies%20between%0Asimulations%20and%20real-world%20conditions%20have%20limited%20its%20practical%20deployment.%0AThis%20study%20introduces%20an%20innovative%20control%20framework%20that%20integrates%0Atrajectory%20optimization%20with%20drift%20maneuvers%2C%20aiming%20to%20improve%20the%20algorithm%27s%0Aadaptability%20for%20real-vehicle%20implementation.%20We%20leveraged%20Bezier-based%0Apre-trajectory%20optimization%20to%20enhance%20rewards%20and%20optimize%20the%20controller%0Athrough%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20in%20a%20simulated%0Aenvironment.%20For%20real-world%20deployment%2C%20we%20implement%20a%20hybrid%20RL-MPC%20fusion%0Amechanism%2C%20%2C%20where%20TD3-derived%20maneuvers%20serve%20as%20primary%20inputs%20for%20a%20Model%0APredictive%20Controller%20%28MPC%29.%20This%20integration%20enables%20precise%20real-time%0Atracking%20of%20the%20optimal%20trajectory%2C%20with%20MPC%20providing%20corrective%20inputs%20to%0Abridge%20the%20gap%20between%20simulation%20and%20reality.%20The%20efficacy%20of%20this%20method%20is%0Avalidated%20through%20real-vehicle%20tests%20on%20consumer-grade%20electric%20vehicles%2C%0Afocusing%20on%20drift%20U-turns%20and%20drift%20right-angle%20turns.%20The%20control%20outcomes%20of%0Athese%20real-vehicle%20tests%20are%20thoroughly%20documented%20in%20the%20paper%2C%20supported%20by%0Asupplementary%20video%20evidence%20%28https%3A//youtu.be/5wp67FcpfL8%29.%20Notably%2C%20this%0Astudy%20is%20the%20first%20to%20deploy%20and%20apply%20an%20RL-based%20transient%20drift%20cornering%0Aalgorithm%20on%20consumer-grade%20electric%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Speed%2520Cornering%2520Control%2520and%2520Real-Vehicle%2520Deployment%2520for%2520Autonomous%250A%2520%2520Electric%2520Vehicles%26entry.906535625%3DShiyue%2520Zhao%2520and%2520Junzhi%2520Zhang%2520and%2520Neda%2520Masoud%2520and%2520Yuhong%2520Jiang%2520and%2520Heye%2520Huang%2520and%2520Tao%2520Liu%26entry.1292438233%3D%2520%2520Executing%2520drift%2520maneuvers%2520during%2520high-speed%2520cornering%2520presents%2520significant%250Achallenges%2520for%2520autonomous%2520vehicles%252C%2520yet%2520offers%2520the%2520potential%2520to%2520minimize%250Aturning%2520time%2520and%2520enhance%2520driving%2520dynamics.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%250Ahas%2520shown%2520promising%2520results%2520in%2520simulated%2520environments%252C%2520discrepancies%2520between%250Asimulations%2520and%2520real-world%2520conditions%2520have%2520limited%2520its%2520practical%2520deployment.%250AThis%2520study%2520introduces%2520an%2520innovative%2520control%2520framework%2520that%2520integrates%250Atrajectory%2520optimization%2520with%2520drift%2520maneuvers%252C%2520aiming%2520to%2520improve%2520the%2520algorithm%2527s%250Aadaptability%2520for%2520real-vehicle%2520implementation.%2520We%2520leveraged%2520Bezier-based%250Apre-trajectory%2520optimization%2520to%2520enhance%2520rewards%2520and%2520optimize%2520the%2520controller%250Athrough%2520Twin%2520Delayed%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528TD3%2529%2520in%2520a%2520simulated%250Aenvironment.%2520For%2520real-world%2520deployment%252C%2520we%2520implement%2520a%2520hybrid%2520RL-MPC%2520fusion%250Amechanism%252C%2520%252C%2520where%2520TD3-derived%2520maneuvers%2520serve%2520as%2520primary%2520inputs%2520for%2520a%2520Model%250APredictive%2520Controller%2520%2528MPC%2529.%2520This%2520integration%2520enables%2520precise%2520real-time%250Atracking%2520of%2520the%2520optimal%2520trajectory%252C%2520with%2520MPC%2520providing%2520corrective%2520inputs%2520to%250Abridge%2520the%2520gap%2520between%2520simulation%2520and%2520reality.%2520The%2520efficacy%2520of%2520this%2520method%2520is%250Avalidated%2520through%2520real-vehicle%2520tests%2520on%2520consumer-grade%2520electric%2520vehicles%252C%250Afocusing%2520on%2520drift%2520U-turns%2520and%2520drift%2520right-angle%2520turns.%2520The%2520control%2520outcomes%2520of%250Athese%2520real-vehicle%2520tests%2520are%2520thoroughly%2520documented%2520in%2520the%2520paper%252C%2520supported%2520by%250Asupplementary%2520video%2520evidence%2520%2528https%253A//youtu.be/5wp67FcpfL8%2529.%2520Notably%252C%2520this%250Astudy%2520is%2520the%2520first%2520to%2520deploy%2520and%2520apply%2520an%2520RL-based%2520transient%2520drift%2520cornering%250Aalgorithm%2520on%2520consumer-grade%2520electric%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Speed%20Cornering%20Control%20and%20Real-Vehicle%20Deployment%20for%20Autonomous%0A%20%20Electric%20Vehicles&entry.906535625=Shiyue%20Zhao%20and%20Junzhi%20Zhang%20and%20Neda%20Masoud%20and%20Yuhong%20Jiang%20and%20Heye%20Huang%20and%20Tao%20Liu&entry.1292438233=%20%20Executing%20drift%20maneuvers%20during%20high-speed%20cornering%20presents%20significant%0Achallenges%20for%20autonomous%20vehicles%2C%20yet%20offers%20the%20potential%20to%20minimize%0Aturning%20time%20and%20enhance%20driving%20dynamics.%20While%20reinforcement%20learning%20%28RL%29%0Ahas%20shown%20promising%20results%20in%20simulated%20environments%2C%20discrepancies%20between%0Asimulations%20and%20real-world%20conditions%20have%20limited%20its%20practical%20deployment.%0AThis%20study%20introduces%20an%20innovative%20control%20framework%20that%20integrates%0Atrajectory%20optimization%20with%20drift%20maneuvers%2C%20aiming%20to%20improve%20the%20algorithm%27s%0Aadaptability%20for%20real-vehicle%20implementation.%20We%20leveraged%20Bezier-based%0Apre-trajectory%20optimization%20to%20enhance%20rewards%20and%20optimize%20the%20controller%0Athrough%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20%28TD3%29%20in%20a%20simulated%0Aenvironment.%20For%20real-world%20deployment%2C%20we%20implement%20a%20hybrid%20RL-MPC%20fusion%0Amechanism%2C%20%2C%20where%20TD3-derived%20maneuvers%20serve%20as%20primary%20inputs%20for%20a%20Model%0APredictive%20Controller%20%28MPC%29.%20This%20integration%20enables%20precise%20real-time%0Atracking%20of%20the%20optimal%20trajectory%2C%20with%20MPC%20providing%20corrective%20inputs%20to%0Abridge%20the%20gap%20between%20simulation%20and%20reality.%20The%20efficacy%20of%20this%20method%20is%0Avalidated%20through%20real-vehicle%20tests%20on%20consumer-grade%20electric%20vehicles%2C%0Afocusing%20on%20drift%20U-turns%20and%20drift%20right-angle%20turns.%20The%20control%20outcomes%20of%0Athese%20real-vehicle%20tests%20are%20thoroughly%20documented%20in%20the%20paper%2C%20supported%20by%0Asupplementary%20video%20evidence%20%28https%3A//youtu.be/5wp67FcpfL8%29.%20Notably%2C%20this%0Astudy%20is%20the%20first%20to%20deploy%20and%20apply%20an%20RL-based%20transient%20drift%20cornering%0Aalgorithm%20on%20consumer-grade%20electric%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11762v1&entry.124074799=Read"},
{"title": "Learning to mask: Towards generalized face forgery detection", "author": "Jianwei Fei and Yunshu Dai and Huaming Wang and Zhihua Xia", "abstract": "  Generalizability to unseen forgery types is crucial for face forgery\ndetectors. Recent works have made significant progress in terms of\ngeneralization by synthetic forgery data augmentation. In this work, we explore\nanother path for improving the generalization. Our goal is to reduce the\nfeatures that are easy to learn in the training phase, so as to reduce the risk\nof overfitting on specific forgery types. Specifically, in our method, a\nteacher network takes as input the face images and generates an attention map\nof the deep features by a diverse multihead attention ViT. The attention map is\nused to guide a student network to focus on the low-attended features by\nreducing the highly-attended deep features. A deep feature mixup strategy is\nalso proposed to synthesize forgeries in the feature domain. Experiments\ndemonstrate that, without data augmentation, our method is able to achieve\npromising performances on unseen forgeries and highly compressed data.\n", "link": "http://arxiv.org/abs/2212.14309v2", "date": "2024-11-18", "relevancy": 2.0722, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5218}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20mask%3A%20Towards%20generalized%20face%20forgery%20detection&body=Title%3A%20Learning%20to%20mask%3A%20Towards%20generalized%20face%20forgery%20detection%0AAuthor%3A%20Jianwei%20Fei%20and%20Yunshu%20Dai%20and%20Huaming%20Wang%20and%20Zhihua%20Xia%0AAbstract%3A%20%20%20Generalizability%20to%20unseen%20forgery%20types%20is%20crucial%20for%20face%20forgery%0Adetectors.%20Recent%20works%20have%20made%20significant%20progress%20in%20terms%20of%0Ageneralization%20by%20synthetic%20forgery%20data%20augmentation.%20In%20this%20work%2C%20we%20explore%0Aanother%20path%20for%20improving%20the%20generalization.%20Our%20goal%20is%20to%20reduce%20the%0Afeatures%20that%20are%20easy%20to%20learn%20in%20the%20training%20phase%2C%20so%20as%20to%20reduce%20the%20risk%0Aof%20overfitting%20on%20specific%20forgery%20types.%20Specifically%2C%20in%20our%20method%2C%20a%0Ateacher%20network%20takes%20as%20input%20the%20face%20images%20and%20generates%20an%20attention%20map%0Aof%20the%20deep%20features%20by%20a%20diverse%20multihead%20attention%20ViT.%20The%20attention%20map%20is%0Aused%20to%20guide%20a%20student%20network%20to%20focus%20on%20the%20low-attended%20features%20by%0Areducing%20the%20highly-attended%20deep%20features.%20A%20deep%20feature%20mixup%20strategy%20is%0Aalso%20proposed%20to%20synthesize%20forgeries%20in%20the%20feature%20domain.%20Experiments%0Ademonstrate%20that%2C%20without%20data%20augmentation%2C%20our%20method%20is%20able%20to%20achieve%0Apromising%20performances%20on%20unseen%20forgeries%20and%20highly%20compressed%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.14309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520mask%253A%2520Towards%2520generalized%2520face%2520forgery%2520detection%26entry.906535625%3DJianwei%2520Fei%2520and%2520Yunshu%2520Dai%2520and%2520Huaming%2520Wang%2520and%2520Zhihua%2520Xia%26entry.1292438233%3D%2520%2520Generalizability%2520to%2520unseen%2520forgery%2520types%2520is%2520crucial%2520for%2520face%2520forgery%250Adetectors.%2520Recent%2520works%2520have%2520made%2520significant%2520progress%2520in%2520terms%2520of%250Ageneralization%2520by%2520synthetic%2520forgery%2520data%2520augmentation.%2520In%2520this%2520work%252C%2520we%2520explore%250Aanother%2520path%2520for%2520improving%2520the%2520generalization.%2520Our%2520goal%2520is%2520to%2520reduce%2520the%250Afeatures%2520that%2520are%2520easy%2520to%2520learn%2520in%2520the%2520training%2520phase%252C%2520so%2520as%2520to%2520reduce%2520the%2520risk%250Aof%2520overfitting%2520on%2520specific%2520forgery%2520types.%2520Specifically%252C%2520in%2520our%2520method%252C%2520a%250Ateacher%2520network%2520takes%2520as%2520input%2520the%2520face%2520images%2520and%2520generates%2520an%2520attention%2520map%250Aof%2520the%2520deep%2520features%2520by%2520a%2520diverse%2520multihead%2520attention%2520ViT.%2520The%2520attention%2520map%2520is%250Aused%2520to%2520guide%2520a%2520student%2520network%2520to%2520focus%2520on%2520the%2520low-attended%2520features%2520by%250Areducing%2520the%2520highly-attended%2520deep%2520features.%2520A%2520deep%2520feature%2520mixup%2520strategy%2520is%250Aalso%2520proposed%2520to%2520synthesize%2520forgeries%2520in%2520the%2520feature%2520domain.%2520Experiments%250Ademonstrate%2520that%252C%2520without%2520data%2520augmentation%252C%2520our%2520method%2520is%2520able%2520to%2520achieve%250Apromising%2520performances%2520on%2520unseen%2520forgeries%2520and%2520highly%2520compressed%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.14309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20mask%3A%20Towards%20generalized%20face%20forgery%20detection&entry.906535625=Jianwei%20Fei%20and%20Yunshu%20Dai%20and%20Huaming%20Wang%20and%20Zhihua%20Xia&entry.1292438233=%20%20Generalizability%20to%20unseen%20forgery%20types%20is%20crucial%20for%20face%20forgery%0Adetectors.%20Recent%20works%20have%20made%20significant%20progress%20in%20terms%20of%0Ageneralization%20by%20synthetic%20forgery%20data%20augmentation.%20In%20this%20work%2C%20we%20explore%0Aanother%20path%20for%20improving%20the%20generalization.%20Our%20goal%20is%20to%20reduce%20the%0Afeatures%20that%20are%20easy%20to%20learn%20in%20the%20training%20phase%2C%20so%20as%20to%20reduce%20the%20risk%0Aof%20overfitting%20on%20specific%20forgery%20types.%20Specifically%2C%20in%20our%20method%2C%20a%0Ateacher%20network%20takes%20as%20input%20the%20face%20images%20and%20generates%20an%20attention%20map%0Aof%20the%20deep%20features%20by%20a%20diverse%20multihead%20attention%20ViT.%20The%20attention%20map%20is%0Aused%20to%20guide%20a%20student%20network%20to%20focus%20on%20the%20low-attended%20features%20by%0Areducing%20the%20highly-attended%20deep%20features.%20A%20deep%20feature%20mixup%20strategy%20is%0Aalso%20proposed%20to%20synthesize%20forgeries%20in%20the%20feature%20domain.%20Experiments%0Ademonstrate%20that%2C%20without%20data%20augmentation%2C%20our%20method%20is%20able%20to%20achieve%0Apromising%20performances%20on%20unseen%20forgeries%20and%20highly%20compressed%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.14309v2&entry.124074799=Read"},
{"title": "Generative World Explorer", "author": "Taiming Lu and Tianmin Shu and Alan Yuille and Daniel Khashabi and Jieneng Chen", "abstract": "  Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate.In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.\n", "link": "http://arxiv.org/abs/2411.11844v1", "date": "2024-11-18", "relevancy": 2.0672, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.7414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6705}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20World%20Explorer&body=Title%3A%20Generative%20World%20Explorer%0AAuthor%3A%20Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Alan%20Yuille%20and%20Daniel%20Khashabi%20and%20Jieneng%20Chen%0AAbstract%3A%20%20%20Planning%20with%20partial%20observation%20is%20a%20central%20challenge%20in%20embodied%20AI.%20A%0Amajority%20of%20prior%20works%20have%20tackled%20this%20challenge%20by%20developing%20agents%20that%0Aphysically%20explore%20their%20environment%20to%20update%20their%20beliefs%20about%20the%20world%0Astate.In%20contrast%2C%20humans%20can%20%24%5Ctextit%7Bimagine%7D%24%20unseen%20parts%20of%20the%20world%0Athrough%20a%20mental%20exploration%20and%20%24%5Ctextit%7Brevise%7D%24%20their%20beliefs%20with%20imagined%0Aobservations.%20Such%20updated%20beliefs%20can%20allow%20them%20to%20make%20more%20informed%0Adecisions%2C%20without%20necessitating%20the%20physical%20exploration%20of%20the%20world%20at%20all%0Atimes.%20To%20achieve%20this%20human-like%20ability%2C%20we%20introduce%20the%20%24%5Ctextit%7BGenerative%0AWorld%20Explorer%20%28Genex%29%7D%24%2C%20an%20egocentric%20world%20exploration%20framework%20that%20allows%0Aan%20agent%20to%20mentally%20explore%20a%20large-scale%203D%20world%20%28e.g.%2C%20urban%20scenes%29%20and%0Aacquire%20imagined%20observations%20to%20update%20its%20belief.%20This%20updated%20belief%20will%0Athen%20help%20the%20agent%20to%20make%20a%20more%20informed%20decision%20at%20the%20current%20step.%20To%0Atrain%20%24%5Ctextit%7BGenex%7D%24%2C%20we%20create%20a%20synthetic%20urban%20scene%20dataset%2C%20Genex-DB.%0AOur%20experimental%20results%20demonstrate%20that%20%281%29%20%24%5Ctextit%7BGenex%7D%24%20can%20generate%0Ahigh-quality%20and%20consistent%20observations%20during%20long-horizon%20exploration%20of%20a%0Alarge%20virtual%20physical%20world%20and%20%282%29%20the%20beliefs%20updated%20with%20the%20generated%0Aobservations%20can%20inform%20an%20existing%20decision-making%20model%20%28e.g.%2C%20an%20LLM%20agent%29%0Ato%20make%20better%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520World%2520Explorer%26entry.906535625%3DTaiming%2520Lu%2520and%2520Tianmin%2520Shu%2520and%2520Alan%2520Yuille%2520and%2520Daniel%2520Khashabi%2520and%2520Jieneng%2520Chen%26entry.1292438233%3D%2520%2520Planning%2520with%2520partial%2520observation%2520is%2520a%2520central%2520challenge%2520in%2520embodied%2520AI.%2520A%250Amajority%2520of%2520prior%2520works%2520have%2520tackled%2520this%2520challenge%2520by%2520developing%2520agents%2520that%250Aphysically%2520explore%2520their%2520environment%2520to%2520update%2520their%2520beliefs%2520about%2520the%2520world%250Astate.In%2520contrast%252C%2520humans%2520can%2520%2524%255Ctextit%257Bimagine%257D%2524%2520unseen%2520parts%2520of%2520the%2520world%250Athrough%2520a%2520mental%2520exploration%2520and%2520%2524%255Ctextit%257Brevise%257D%2524%2520their%2520beliefs%2520with%2520imagined%250Aobservations.%2520Such%2520updated%2520beliefs%2520can%2520allow%2520them%2520to%2520make%2520more%2520informed%250Adecisions%252C%2520without%2520necessitating%2520the%2520physical%2520exploration%2520of%2520the%2520world%2520at%2520all%250Atimes.%2520To%2520achieve%2520this%2520human-like%2520ability%252C%2520we%2520introduce%2520the%2520%2524%255Ctextit%257BGenerative%250AWorld%2520Explorer%2520%2528Genex%2529%257D%2524%252C%2520an%2520egocentric%2520world%2520exploration%2520framework%2520that%2520allows%250Aan%2520agent%2520to%2520mentally%2520explore%2520a%2520large-scale%25203D%2520world%2520%2528e.g.%252C%2520urban%2520scenes%2529%2520and%250Aacquire%2520imagined%2520observations%2520to%2520update%2520its%2520belief.%2520This%2520updated%2520belief%2520will%250Athen%2520help%2520the%2520agent%2520to%2520make%2520a%2520more%2520informed%2520decision%2520at%2520the%2520current%2520step.%2520To%250Atrain%2520%2524%255Ctextit%257BGenex%257D%2524%252C%2520we%2520create%2520a%2520synthetic%2520urban%2520scene%2520dataset%252C%2520Genex-DB.%250AOur%2520experimental%2520results%2520demonstrate%2520that%2520%25281%2529%2520%2524%255Ctextit%257BGenex%257D%2524%2520can%2520generate%250Ahigh-quality%2520and%2520consistent%2520observations%2520during%2520long-horizon%2520exploration%2520of%2520a%250Alarge%2520virtual%2520physical%2520world%2520and%2520%25282%2529%2520the%2520beliefs%2520updated%2520with%2520the%2520generated%250Aobservations%2520can%2520inform%2520an%2520existing%2520decision-making%2520model%2520%2528e.g.%252C%2520an%2520LLM%2520agent%2529%250Ato%2520make%2520better%2520plans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20World%20Explorer&entry.906535625=Taiming%20Lu%20and%20Tianmin%20Shu%20and%20Alan%20Yuille%20and%20Daniel%20Khashabi%20and%20Jieneng%20Chen&entry.1292438233=%20%20Planning%20with%20partial%20observation%20is%20a%20central%20challenge%20in%20embodied%20AI.%20A%0Amajority%20of%20prior%20works%20have%20tackled%20this%20challenge%20by%20developing%20agents%20that%0Aphysically%20explore%20their%20environment%20to%20update%20their%20beliefs%20about%20the%20world%0Astate.In%20contrast%2C%20humans%20can%20%24%5Ctextit%7Bimagine%7D%24%20unseen%20parts%20of%20the%20world%0Athrough%20a%20mental%20exploration%20and%20%24%5Ctextit%7Brevise%7D%24%20their%20beliefs%20with%20imagined%0Aobservations.%20Such%20updated%20beliefs%20can%20allow%20them%20to%20make%20more%20informed%0Adecisions%2C%20without%20necessitating%20the%20physical%20exploration%20of%20the%20world%20at%20all%0Atimes.%20To%20achieve%20this%20human-like%20ability%2C%20we%20introduce%20the%20%24%5Ctextit%7BGenerative%0AWorld%20Explorer%20%28Genex%29%7D%24%2C%20an%20egocentric%20world%20exploration%20framework%20that%20allows%0Aan%20agent%20to%20mentally%20explore%20a%20large-scale%203D%20world%20%28e.g.%2C%20urban%20scenes%29%20and%0Aacquire%20imagined%20observations%20to%20update%20its%20belief.%20This%20updated%20belief%20will%0Athen%20help%20the%20agent%20to%20make%20a%20more%20informed%20decision%20at%20the%20current%20step.%20To%0Atrain%20%24%5Ctextit%7BGenex%7D%24%2C%20we%20create%20a%20synthetic%20urban%20scene%20dataset%2C%20Genex-DB.%0AOur%20experimental%20results%20demonstrate%20that%20%281%29%20%24%5Ctextit%7BGenex%7D%24%20can%20generate%0Ahigh-quality%20and%20consistent%20observations%20during%20long-horizon%20exploration%20of%20a%0Alarge%20virtual%20physical%20world%20and%20%282%29%20the%20beliefs%20updated%20with%20the%20generated%0Aobservations%20can%20inform%20an%20existing%20decision-making%20model%20%28e.g.%2C%20an%20LLM%20agent%29%0Ato%20make%20better%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11844v1&entry.124074799=Read"},
{"title": "Equivariant spatio-hemispherical networks for diffusion MRI\n  deconvolution", "author": "Axel Elaldi and Guido Gerig and Neel Dey", "abstract": "  Each voxel in a diffusion MRI (dMRI) image contains a spherical signal\ncorresponding to the direction and strength of water diffusion in the brain.\nThis paper advances the analysis of such spatio-spherical data by developing\nconvolutional network layers that are equivariant to the $\\mathbf{E(3) \\times\nSO(3)}$ group and account for the physical symmetries of dMRI including\nrotations, translations, and reflections of space alongside voxel-wise\nrotations. Further, neuronal fibers are typically antipodally symmetric, a fact\nwe leverage to construct highly efficient spatio-hemispherical graph\nconvolutions to accelerate the analysis of high-dimensional dMRI data. In the\ncontext of sparse spherical fiber deconvolution to recover white matter\nmicrostructure, our proposed equivariant network layers yield substantial\nperformance and efficiency gains, leading to better and more practical\nresolution of crossing neuronal fibers and fiber tractography. These gains are\nexperimentally consistent across both simulation and in vivo human datasets.\n", "link": "http://arxiv.org/abs/2411.11819v1", "date": "2024-11-18", "relevancy": 2.0669, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5267}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5147}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20spatio-hemispherical%20networks%20for%20diffusion%20MRI%0A%20%20deconvolution&body=Title%3A%20Equivariant%20spatio-hemispherical%20networks%20for%20diffusion%20MRI%0A%20%20deconvolution%0AAuthor%3A%20Axel%20Elaldi%20and%20Guido%20Gerig%20and%20Neel%20Dey%0AAbstract%3A%20%20%20Each%20voxel%20in%20a%20diffusion%20MRI%20%28dMRI%29%20image%20contains%20a%20spherical%20signal%0Acorresponding%20to%20the%20direction%20and%20strength%20of%20water%20diffusion%20in%20the%20brain.%0AThis%20paper%20advances%20the%20analysis%20of%20such%20spatio-spherical%20data%20by%20developing%0Aconvolutional%20network%20layers%20that%20are%20equivariant%20to%20the%20%24%5Cmathbf%7BE%283%29%20%5Ctimes%0ASO%283%29%7D%24%20group%20and%20account%20for%20the%20physical%20symmetries%20of%20dMRI%20including%0Arotations%2C%20translations%2C%20and%20reflections%20of%20space%20alongside%20voxel-wise%0Arotations.%20Further%2C%20neuronal%20fibers%20are%20typically%20antipodally%20symmetric%2C%20a%20fact%0Awe%20leverage%20to%20construct%20highly%20efficient%20spatio-hemispherical%20graph%0Aconvolutions%20to%20accelerate%20the%20analysis%20of%20high-dimensional%20dMRI%20data.%20In%20the%0Acontext%20of%20sparse%20spherical%20fiber%20deconvolution%20to%20recover%20white%20matter%0Amicrostructure%2C%20our%20proposed%20equivariant%20network%20layers%20yield%20substantial%0Aperformance%20and%20efficiency%20gains%2C%20leading%20to%20better%20and%20more%20practical%0Aresolution%20of%20crossing%20neuronal%20fibers%20and%20fiber%20tractography.%20These%20gains%20are%0Aexperimentally%20consistent%20across%20both%20simulation%20and%20in%20vivo%20human%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520spatio-hemispherical%2520networks%2520for%2520diffusion%2520MRI%250A%2520%2520deconvolution%26entry.906535625%3DAxel%2520Elaldi%2520and%2520Guido%2520Gerig%2520and%2520Neel%2520Dey%26entry.1292438233%3D%2520%2520Each%2520voxel%2520in%2520a%2520diffusion%2520MRI%2520%2528dMRI%2529%2520image%2520contains%2520a%2520spherical%2520signal%250Acorresponding%2520to%2520the%2520direction%2520and%2520strength%2520of%2520water%2520diffusion%2520in%2520the%2520brain.%250AThis%2520paper%2520advances%2520the%2520analysis%2520of%2520such%2520spatio-spherical%2520data%2520by%2520developing%250Aconvolutional%2520network%2520layers%2520that%2520are%2520equivariant%2520to%2520the%2520%2524%255Cmathbf%257BE%25283%2529%2520%255Ctimes%250ASO%25283%2529%257D%2524%2520group%2520and%2520account%2520for%2520the%2520physical%2520symmetries%2520of%2520dMRI%2520including%250Arotations%252C%2520translations%252C%2520and%2520reflections%2520of%2520space%2520alongside%2520voxel-wise%250Arotations.%2520Further%252C%2520neuronal%2520fibers%2520are%2520typically%2520antipodally%2520symmetric%252C%2520a%2520fact%250Awe%2520leverage%2520to%2520construct%2520highly%2520efficient%2520spatio-hemispherical%2520graph%250Aconvolutions%2520to%2520accelerate%2520the%2520analysis%2520of%2520high-dimensional%2520dMRI%2520data.%2520In%2520the%250Acontext%2520of%2520sparse%2520spherical%2520fiber%2520deconvolution%2520to%2520recover%2520white%2520matter%250Amicrostructure%252C%2520our%2520proposed%2520equivariant%2520network%2520layers%2520yield%2520substantial%250Aperformance%2520and%2520efficiency%2520gains%252C%2520leading%2520to%2520better%2520and%2520more%2520practical%250Aresolution%2520of%2520crossing%2520neuronal%2520fibers%2520and%2520fiber%2520tractography.%2520These%2520gains%2520are%250Aexperimentally%2520consistent%2520across%2520both%2520simulation%2520and%2520in%2520vivo%2520human%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20spatio-hemispherical%20networks%20for%20diffusion%20MRI%0A%20%20deconvolution&entry.906535625=Axel%20Elaldi%20and%20Guido%20Gerig%20and%20Neel%20Dey&entry.1292438233=%20%20Each%20voxel%20in%20a%20diffusion%20MRI%20%28dMRI%29%20image%20contains%20a%20spherical%20signal%0Acorresponding%20to%20the%20direction%20and%20strength%20of%20water%20diffusion%20in%20the%20brain.%0AThis%20paper%20advances%20the%20analysis%20of%20such%20spatio-spherical%20data%20by%20developing%0Aconvolutional%20network%20layers%20that%20are%20equivariant%20to%20the%20%24%5Cmathbf%7BE%283%29%20%5Ctimes%0ASO%283%29%7D%24%20group%20and%20account%20for%20the%20physical%20symmetries%20of%20dMRI%20including%0Arotations%2C%20translations%2C%20and%20reflections%20of%20space%20alongside%20voxel-wise%0Arotations.%20Further%2C%20neuronal%20fibers%20are%20typically%20antipodally%20symmetric%2C%20a%20fact%0Awe%20leverage%20to%20construct%20highly%20efficient%20spatio-hemispherical%20graph%0Aconvolutions%20to%20accelerate%20the%20analysis%20of%20high-dimensional%20dMRI%20data.%20In%20the%0Acontext%20of%20sparse%20spherical%20fiber%20deconvolution%20to%20recover%20white%20matter%0Amicrostructure%2C%20our%20proposed%20equivariant%20network%20layers%20yield%20substantial%0Aperformance%20and%20efficiency%20gains%2C%20leading%20to%20better%20and%20more%20practical%0Aresolution%20of%20crossing%20neuronal%20fibers%20and%20fiber%20tractography.%20These%20gains%20are%0Aexperimentally%20consistent%20across%20both%20simulation%20and%20in%20vivo%20human%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11819v1&entry.124074799=Read"},
{"title": "SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly\n  semi-supervised medical image segmentation", "author": "Shiman Li and Jiayue Zhao and Shaolei Liu and Xiaokun Dai and Chenxi Zhang and Zhijian Song", "abstract": "  Deep learning-based medical image segmentation helps assist diagnosis and\naccelerate the treatment process while the model training usually requires\nlarge-scale dense annotation datasets. Weakly semi-supervised medical image\nsegmentation is an essential application because it only requires a small\namount of scribbles and a large number of unlabeled data to train the model,\nwhich greatly reduces the clinician's effort to fully annotate images. To\nhandle the inadequate supervisory information challenge in weakly\nsemi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label\n(SP${}^3$) learning method is proposed, using the structural information\ncontained in superpixel for supplemental information. Specifically, the\nannotation of scribbles is propagated to superpixels and thus obtains a dense\nannotation for supervised training. Since the quality of pseudo-labels is\nlimited by the low-quality annotation, the beneficial superpixels selected by\ndynamic thresholding are used to refine pseudo-labels. Furthermore, aiming to\nalleviate the negative impact of noise in pseudo-label, superpixel-level\nuncertainty is incorporated to guide the pseudo-label supervision for stable\nlearning. Our method achieves state-of-the-art performance on both tumor and\norgan segmentation datasets under the WSSS setting, using only 3\\% of the\nannotation workload compared to fully supervised methods and attaining\napproximately 80\\% Dice score. Additionally, our method outperforms eight\nweakly and semi-supervised methods under both weakly supervised and\nsemi-supervised settings. Results of extensive experiments validate the\neffectiveness and annotation efficiency of our weakly semi-supervised\nsegmentation, which can assist clinicians in achieving automated segmentation\nfor organs or tumors quickly and ultimately benefit patients.\n", "link": "http://arxiv.org/abs/2411.11636v1", "date": "2024-11-18", "relevancy": 2.0616, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5445}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SP%24%7B%20%7D%5E3%24%20%3A%20Superpixel-propagated%20pseudo-label%20learning%20for%20weakly%0A%20%20semi-supervised%20medical%20image%20segmentation&body=Title%3A%20SP%24%7B%20%7D%5E3%24%20%3A%20Superpixel-propagated%20pseudo-label%20learning%20for%20weakly%0A%20%20semi-supervised%20medical%20image%20segmentation%0AAuthor%3A%20Shiman%20Li%20and%20Jiayue%20Zhao%20and%20Shaolei%20Liu%20and%20Xiaokun%20Dai%20and%20Chenxi%20Zhang%20and%20Zhijian%20Song%0AAbstract%3A%20%20%20Deep%20learning-based%20medical%20image%20segmentation%20helps%20assist%20diagnosis%20and%0Aaccelerate%20the%20treatment%20process%20while%20the%20model%20training%20usually%20requires%0Alarge-scale%20dense%20annotation%20datasets.%20Weakly%20semi-supervised%20medical%20image%0Asegmentation%20is%20an%20essential%20application%20because%20it%20only%20requires%20a%20small%0Aamount%20of%20scribbles%20and%20a%20large%20number%20of%20unlabeled%20data%20to%20train%20the%20model%2C%0Awhich%20greatly%20reduces%20the%20clinician%27s%20effort%20to%20fully%20annotate%20images.%20To%0Ahandle%20the%20inadequate%20supervisory%20information%20challenge%20in%20weakly%0Asemi-supervised%20segmentation%20%28WSSS%29%2C%20a%20SuperPixel-Propagated%20Pseudo-label%0A%28SP%24%7B%7D%5E3%24%29%20learning%20method%20is%20proposed%2C%20using%20the%20structural%20information%0Acontained%20in%20superpixel%20for%20supplemental%20information.%20Specifically%2C%20the%0Aannotation%20of%20scribbles%20is%20propagated%20to%20superpixels%20and%20thus%20obtains%20a%20dense%0Aannotation%20for%20supervised%20training.%20Since%20the%20quality%20of%20pseudo-labels%20is%0Alimited%20by%20the%20low-quality%20annotation%2C%20the%20beneficial%20superpixels%20selected%20by%0Adynamic%20thresholding%20are%20used%20to%20refine%20pseudo-labels.%20Furthermore%2C%20aiming%20to%0Aalleviate%20the%20negative%20impact%20of%20noise%20in%20pseudo-label%2C%20superpixel-level%0Auncertainty%20is%20incorporated%20to%20guide%20the%20pseudo-label%20supervision%20for%20stable%0Alearning.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20tumor%20and%0Aorgan%20segmentation%20datasets%20under%20the%20WSSS%20setting%2C%20using%20only%203%5C%25%20of%20the%0Aannotation%20workload%20compared%20to%20fully%20supervised%20methods%20and%20attaining%0Aapproximately%2080%5C%25%20Dice%20score.%20Additionally%2C%20our%20method%20outperforms%20eight%0Aweakly%20and%20semi-supervised%20methods%20under%20both%20weakly%20supervised%20and%0Asemi-supervised%20settings.%20Results%20of%20extensive%20experiments%20validate%20the%0Aeffectiveness%20and%20annotation%20efficiency%20of%20our%20weakly%20semi-supervised%0Asegmentation%2C%20which%20can%20assist%20clinicians%20in%20achieving%20automated%20segmentation%0Afor%20organs%20or%20tumors%20quickly%20and%20ultimately%20benefit%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSP%2524%257B%2520%257D%255E3%2524%2520%253A%2520Superpixel-propagated%2520pseudo-label%2520learning%2520for%2520weakly%250A%2520%2520semi-supervised%2520medical%2520image%2520segmentation%26entry.906535625%3DShiman%2520Li%2520and%2520Jiayue%2520Zhao%2520and%2520Shaolei%2520Liu%2520and%2520Xiaokun%2520Dai%2520and%2520Chenxi%2520Zhang%2520and%2520Zhijian%2520Song%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520medical%2520image%2520segmentation%2520helps%2520assist%2520diagnosis%2520and%250Aaccelerate%2520the%2520treatment%2520process%2520while%2520the%2520model%2520training%2520usually%2520requires%250Alarge-scale%2520dense%2520annotation%2520datasets.%2520Weakly%2520semi-supervised%2520medical%2520image%250Asegmentation%2520is%2520an%2520essential%2520application%2520because%2520it%2520only%2520requires%2520a%2520small%250Aamount%2520of%2520scribbles%2520and%2520a%2520large%2520number%2520of%2520unlabeled%2520data%2520to%2520train%2520the%2520model%252C%250Awhich%2520greatly%2520reduces%2520the%2520clinician%2527s%2520effort%2520to%2520fully%2520annotate%2520images.%2520To%250Ahandle%2520the%2520inadequate%2520supervisory%2520information%2520challenge%2520in%2520weakly%250Asemi-supervised%2520segmentation%2520%2528WSSS%2529%252C%2520a%2520SuperPixel-Propagated%2520Pseudo-label%250A%2528SP%2524%257B%257D%255E3%2524%2529%2520learning%2520method%2520is%2520proposed%252C%2520using%2520the%2520structural%2520information%250Acontained%2520in%2520superpixel%2520for%2520supplemental%2520information.%2520Specifically%252C%2520the%250Aannotation%2520of%2520scribbles%2520is%2520propagated%2520to%2520superpixels%2520and%2520thus%2520obtains%2520a%2520dense%250Aannotation%2520for%2520supervised%2520training.%2520Since%2520the%2520quality%2520of%2520pseudo-labels%2520is%250Alimited%2520by%2520the%2520low-quality%2520annotation%252C%2520the%2520beneficial%2520superpixels%2520selected%2520by%250Adynamic%2520thresholding%2520are%2520used%2520to%2520refine%2520pseudo-labels.%2520Furthermore%252C%2520aiming%2520to%250Aalleviate%2520the%2520negative%2520impact%2520of%2520noise%2520in%2520pseudo-label%252C%2520superpixel-level%250Auncertainty%2520is%2520incorporated%2520to%2520guide%2520the%2520pseudo-label%2520supervision%2520for%2520stable%250Alearning.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520both%2520tumor%2520and%250Aorgan%2520segmentation%2520datasets%2520under%2520the%2520WSSS%2520setting%252C%2520using%2520only%25203%255C%2525%2520of%2520the%250Aannotation%2520workload%2520compared%2520to%2520fully%2520supervised%2520methods%2520and%2520attaining%250Aapproximately%252080%255C%2525%2520Dice%2520score.%2520Additionally%252C%2520our%2520method%2520outperforms%2520eight%250Aweakly%2520and%2520semi-supervised%2520methods%2520under%2520both%2520weakly%2520supervised%2520and%250Asemi-supervised%2520settings.%2520Results%2520of%2520extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520and%2520annotation%2520efficiency%2520of%2520our%2520weakly%2520semi-supervised%250Asegmentation%252C%2520which%2520can%2520assist%2520clinicians%2520in%2520achieving%2520automated%2520segmentation%250Afor%2520organs%2520or%2520tumors%2520quickly%2520and%2520ultimately%2520benefit%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SP%24%7B%20%7D%5E3%24%20%3A%20Superpixel-propagated%20pseudo-label%20learning%20for%20weakly%0A%20%20semi-supervised%20medical%20image%20segmentation&entry.906535625=Shiman%20Li%20and%20Jiayue%20Zhao%20and%20Shaolei%20Liu%20and%20Xiaokun%20Dai%20and%20Chenxi%20Zhang%20and%20Zhijian%20Song&entry.1292438233=%20%20Deep%20learning-based%20medical%20image%20segmentation%20helps%20assist%20diagnosis%20and%0Aaccelerate%20the%20treatment%20process%20while%20the%20model%20training%20usually%20requires%0Alarge-scale%20dense%20annotation%20datasets.%20Weakly%20semi-supervised%20medical%20image%0Asegmentation%20is%20an%20essential%20application%20because%20it%20only%20requires%20a%20small%0Aamount%20of%20scribbles%20and%20a%20large%20number%20of%20unlabeled%20data%20to%20train%20the%20model%2C%0Awhich%20greatly%20reduces%20the%20clinician%27s%20effort%20to%20fully%20annotate%20images.%20To%0Ahandle%20the%20inadequate%20supervisory%20information%20challenge%20in%20weakly%0Asemi-supervised%20segmentation%20%28WSSS%29%2C%20a%20SuperPixel-Propagated%20Pseudo-label%0A%28SP%24%7B%7D%5E3%24%29%20learning%20method%20is%20proposed%2C%20using%20the%20structural%20information%0Acontained%20in%20superpixel%20for%20supplemental%20information.%20Specifically%2C%20the%0Aannotation%20of%20scribbles%20is%20propagated%20to%20superpixels%20and%20thus%20obtains%20a%20dense%0Aannotation%20for%20supervised%20training.%20Since%20the%20quality%20of%20pseudo-labels%20is%0Alimited%20by%20the%20low-quality%20annotation%2C%20the%20beneficial%20superpixels%20selected%20by%0Adynamic%20thresholding%20are%20used%20to%20refine%20pseudo-labels.%20Furthermore%2C%20aiming%20to%0Aalleviate%20the%20negative%20impact%20of%20noise%20in%20pseudo-label%2C%20superpixel-level%0Auncertainty%20is%20incorporated%20to%20guide%20the%20pseudo-label%20supervision%20for%20stable%0Alearning.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20both%20tumor%20and%0Aorgan%20segmentation%20datasets%20under%20the%20WSSS%20setting%2C%20using%20only%203%5C%25%20of%20the%0Aannotation%20workload%20compared%20to%20fully%20supervised%20methods%20and%20attaining%0Aapproximately%2080%5C%25%20Dice%20score.%20Additionally%2C%20our%20method%20outperforms%20eight%0Aweakly%20and%20semi-supervised%20methods%20under%20both%20weakly%20supervised%20and%0Asemi-supervised%20settings.%20Results%20of%20extensive%20experiments%20validate%20the%0Aeffectiveness%20and%20annotation%20efficiency%20of%20our%20weakly%20semi-supervised%0Asegmentation%2C%20which%20can%20assist%20clinicians%20in%20achieving%20automated%20segmentation%0Afor%20organs%20or%20tumors%20quickly%20and%20ultimately%20benefit%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11636v1&entry.124074799=Read"},
{"title": "A Multimodal Adaptive Graph-based Intelligent Classification Model for\n  Fake News", "author": " Jun-hao and  Xu", "abstract": "  Numerous studies have been proposed to detect fake news focusing on\nmulti-modalities based on machine and/or deep learning. However, studies\nfocusing on graph-based structures using geometric deep learning are lacking.\nTo address this challenge, we introduce the Multimodal Adaptive Graph-based\nIntelligent Classification (aptly referred to as MAGIC) for fake news\ndetection. Specifically, the Encoder Representations from Transformers was used\nfor text vectorization whilst ResNet50 was used for images. A comprehensive\ninformation interaction graph was built using the adaptive Graph Attention\nNetwork before classifying the multimodal input through the Softmax function.\nMAGIC was trained and tested on two fake news datasets, that is, Fakeddit\n(English) and Multimodal Fake News Detection (Chinese), with the model\nachieving an accuracy of 98.8\\% and 86.3\\%, respectively. Ablation experiments\nalso revealed MAGIC to yield superior performance across both the datasets.\nFindings show that a graph-based deep learning adaptive model is effective in\ndetecting multimodal fake news, surpassing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.06097v2", "date": "2024-11-18", "relevancy": 2.0258, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5256}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5046}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Adaptive%20Graph-based%20Intelligent%20Classification%20Model%20for%0A%20%20Fake%20News&body=Title%3A%20A%20Multimodal%20Adaptive%20Graph-based%20Intelligent%20Classification%20Model%20for%0A%20%20Fake%20News%0AAuthor%3A%20%20Jun-hao%20and%20%20Xu%0AAbstract%3A%20%20%20Numerous%20studies%20have%20been%20proposed%20to%20detect%20fake%20news%20focusing%20on%0Amulti-modalities%20based%20on%20machine%20and/or%20deep%20learning.%20However%2C%20studies%0Afocusing%20on%20graph-based%20structures%20using%20geometric%20deep%20learning%20are%20lacking.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20the%20Multimodal%20Adaptive%20Graph-based%0AIntelligent%20Classification%20%28aptly%20referred%20to%20as%20MAGIC%29%20for%20fake%20news%0Adetection.%20Specifically%2C%20the%20Encoder%20Representations%20from%20Transformers%20was%20used%0Afor%20text%20vectorization%20whilst%20ResNet50%20was%20used%20for%20images.%20A%20comprehensive%0Ainformation%20interaction%20graph%20was%20built%20using%20the%20adaptive%20Graph%20Attention%0ANetwork%20before%20classifying%20the%20multimodal%20input%20through%20the%20Softmax%20function.%0AMAGIC%20was%20trained%20and%20tested%20on%20two%20fake%20news%20datasets%2C%20that%20is%2C%20Fakeddit%0A%28English%29%20and%20Multimodal%20Fake%20News%20Detection%20%28Chinese%29%2C%20with%20the%20model%0Aachieving%20an%20accuracy%20of%2098.8%5C%25%20and%2086.3%5C%25%2C%20respectively.%20Ablation%20experiments%0Aalso%20revealed%20MAGIC%20to%20yield%20superior%20performance%20across%20both%20the%20datasets.%0AFindings%20show%20that%20a%20graph-based%20deep%20learning%20adaptive%20model%20is%20effective%20in%0Adetecting%20multimodal%20fake%20news%2C%20surpassing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Adaptive%2520Graph-based%2520Intelligent%2520Classification%2520Model%2520for%250A%2520%2520Fake%2520News%26entry.906535625%3D%2520Jun-hao%2520and%2520%2520Xu%26entry.1292438233%3D%2520%2520Numerous%2520studies%2520have%2520been%2520proposed%2520to%2520detect%2520fake%2520news%2520focusing%2520on%250Amulti-modalities%2520based%2520on%2520machine%2520and/or%2520deep%2520learning.%2520However%252C%2520studies%250Afocusing%2520on%2520graph-based%2520structures%2520using%2520geometric%2520deep%2520learning%2520are%2520lacking.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520the%2520Multimodal%2520Adaptive%2520Graph-based%250AIntelligent%2520Classification%2520%2528aptly%2520referred%2520to%2520as%2520MAGIC%2529%2520for%2520fake%2520news%250Adetection.%2520Specifically%252C%2520the%2520Encoder%2520Representations%2520from%2520Transformers%2520was%2520used%250Afor%2520text%2520vectorization%2520whilst%2520ResNet50%2520was%2520used%2520for%2520images.%2520A%2520comprehensive%250Ainformation%2520interaction%2520graph%2520was%2520built%2520using%2520the%2520adaptive%2520Graph%2520Attention%250ANetwork%2520before%2520classifying%2520the%2520multimodal%2520input%2520through%2520the%2520Softmax%2520function.%250AMAGIC%2520was%2520trained%2520and%2520tested%2520on%2520two%2520fake%2520news%2520datasets%252C%2520that%2520is%252C%2520Fakeddit%250A%2528English%2529%2520and%2520Multimodal%2520Fake%2520News%2520Detection%2520%2528Chinese%2529%252C%2520with%2520the%2520model%250Aachieving%2520an%2520accuracy%2520of%252098.8%255C%2525%2520and%252086.3%255C%2525%252C%2520respectively.%2520Ablation%2520experiments%250Aalso%2520revealed%2520MAGIC%2520to%2520yield%2520superior%2520performance%2520across%2520both%2520the%2520datasets.%250AFindings%2520show%2520that%2520a%2520graph-based%2520deep%2520learning%2520adaptive%2520model%2520is%2520effective%2520in%250Adetecting%2520multimodal%2520fake%2520news%252C%2520surpassing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Adaptive%20Graph-based%20Intelligent%20Classification%20Model%20for%0A%20%20Fake%20News&entry.906535625=%20Jun-hao%20and%20%20Xu&entry.1292438233=%20%20Numerous%20studies%20have%20been%20proposed%20to%20detect%20fake%20news%20focusing%20on%0Amulti-modalities%20based%20on%20machine%20and/or%20deep%20learning.%20However%2C%20studies%0Afocusing%20on%20graph-based%20structures%20using%20geometric%20deep%20learning%20are%20lacking.%0ATo%20address%20this%20challenge%2C%20we%20introduce%20the%20Multimodal%20Adaptive%20Graph-based%0AIntelligent%20Classification%20%28aptly%20referred%20to%20as%20MAGIC%29%20for%20fake%20news%0Adetection.%20Specifically%2C%20the%20Encoder%20Representations%20from%20Transformers%20was%20used%0Afor%20text%20vectorization%20whilst%20ResNet50%20was%20used%20for%20images.%20A%20comprehensive%0Ainformation%20interaction%20graph%20was%20built%20using%20the%20adaptive%20Graph%20Attention%0ANetwork%20before%20classifying%20the%20multimodal%20input%20through%20the%20Softmax%20function.%0AMAGIC%20was%20trained%20and%20tested%20on%20two%20fake%20news%20datasets%2C%20that%20is%2C%20Fakeddit%0A%28English%29%20and%20Multimodal%20Fake%20News%20Detection%20%28Chinese%29%2C%20with%20the%20model%0Aachieving%20an%20accuracy%20of%2098.8%5C%25%20and%2086.3%5C%25%2C%20respectively.%20Ablation%20experiments%0Aalso%20revealed%20MAGIC%20to%20yield%20superior%20performance%20across%20both%20the%20datasets.%0AFindings%20show%20that%20a%20graph-based%20deep%20learning%20adaptive%20model%20is%20effective%20in%0Adetecting%20multimodal%20fake%20news%2C%20surpassing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06097v2&entry.124074799=Read"},
{"title": "Learning Differentiable Surrogate Losses for Structured Prediction", "author": "Junjie Yang and Matthieu Labeau and Florence d'Alch\u00e9-Buc", "abstract": "  Structured prediction involves learning to predict complex structures rather\nthan simple scalar values. The main challenge arises from the non-Euclidean\nnature of the output space, which generally requires relaxing the problem\nformulation. Surrogate methods build on kernel-induced losses or more\ngenerally, loss functions admitting an Implicit Loss Embedding, and convert the\noriginal problem into a regression task followed by a decoding step. However,\ndesigning effective losses for objects with complex structures presents\nsignificant challenges and often requires domain-specific expertise. In this\nwork, we introduce a novel framework in which a structured loss function,\nparameterized by neural networks, is learned directly from output training data\nthrough Contrastive Learning, prior to addressing the supervised surrogate\nregression problem. As a result, the differentiable loss not only enables the\nlearning of neural networks due to the finite dimension of the surrogate space\nbut also allows for the prediction of new structures of the output data via a\ndecoding strategy based on gradient descent. Numerical experiments on\nsupervised graph prediction problems show that our approach achieves similar or\neven better performance than methods based on a pre-defined kernel.\n", "link": "http://arxiv.org/abs/2411.11682v1", "date": "2024-11-18", "relevancy": 2.0254, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4805}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Differentiable%20Surrogate%20Losses%20for%20Structured%20Prediction&body=Title%3A%20Learning%20Differentiable%20Surrogate%20Losses%20for%20Structured%20Prediction%0AAuthor%3A%20Junjie%20Yang%20and%20Matthieu%20Labeau%20and%20Florence%20d%27Alch%C3%A9-Buc%0AAbstract%3A%20%20%20Structured%20prediction%20involves%20learning%20to%20predict%20complex%20structures%20rather%0Athan%20simple%20scalar%20values.%20The%20main%20challenge%20arises%20from%20the%20non-Euclidean%0Anature%20of%20the%20output%20space%2C%20which%20generally%20requires%20relaxing%20the%20problem%0Aformulation.%20Surrogate%20methods%20build%20on%20kernel-induced%20losses%20or%20more%0Agenerally%2C%20loss%20functions%20admitting%20an%20Implicit%20Loss%20Embedding%2C%20and%20convert%20the%0Aoriginal%20problem%20into%20a%20regression%20task%20followed%20by%20a%20decoding%20step.%20However%2C%0Adesigning%20effective%20losses%20for%20objects%20with%20complex%20structures%20presents%0Asignificant%20challenges%20and%20often%20requires%20domain-specific%20expertise.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20in%20which%20a%20structured%20loss%20function%2C%0Aparameterized%20by%20neural%20networks%2C%20is%20learned%20directly%20from%20output%20training%20data%0Athrough%20Contrastive%20Learning%2C%20prior%20to%20addressing%20the%20supervised%20surrogate%0Aregression%20problem.%20As%20a%20result%2C%20the%20differentiable%20loss%20not%20only%20enables%20the%0Alearning%20of%20neural%20networks%20due%20to%20the%20finite%20dimension%20of%20the%20surrogate%20space%0Abut%20also%20allows%20for%20the%20prediction%20of%20new%20structures%20of%20the%20output%20data%20via%20a%0Adecoding%20strategy%20based%20on%20gradient%20descent.%20Numerical%20experiments%20on%0Asupervised%20graph%20prediction%20problems%20show%20that%20our%20approach%20achieves%20similar%20or%0Aeven%20better%20performance%20than%20methods%20based%20on%20a%20pre-defined%20kernel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Differentiable%2520Surrogate%2520Losses%2520for%2520Structured%2520Prediction%26entry.906535625%3DJunjie%2520Yang%2520and%2520Matthieu%2520Labeau%2520and%2520Florence%2520d%2527Alch%25C3%25A9-Buc%26entry.1292438233%3D%2520%2520Structured%2520prediction%2520involves%2520learning%2520to%2520predict%2520complex%2520structures%2520rather%250Athan%2520simple%2520scalar%2520values.%2520The%2520main%2520challenge%2520arises%2520from%2520the%2520non-Euclidean%250Anature%2520of%2520the%2520output%2520space%252C%2520which%2520generally%2520requires%2520relaxing%2520the%2520problem%250Aformulation.%2520Surrogate%2520methods%2520build%2520on%2520kernel-induced%2520losses%2520or%2520more%250Agenerally%252C%2520loss%2520functions%2520admitting%2520an%2520Implicit%2520Loss%2520Embedding%252C%2520and%2520convert%2520the%250Aoriginal%2520problem%2520into%2520a%2520regression%2520task%2520followed%2520by%2520a%2520decoding%2520step.%2520However%252C%250Adesigning%2520effective%2520losses%2520for%2520objects%2520with%2520complex%2520structures%2520presents%250Asignificant%2520challenges%2520and%2520often%2520requires%2520domain-specific%2520expertise.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520in%2520which%2520a%2520structured%2520loss%2520function%252C%250Aparameterized%2520by%2520neural%2520networks%252C%2520is%2520learned%2520directly%2520from%2520output%2520training%2520data%250Athrough%2520Contrastive%2520Learning%252C%2520prior%2520to%2520addressing%2520the%2520supervised%2520surrogate%250Aregression%2520problem.%2520As%2520a%2520result%252C%2520the%2520differentiable%2520loss%2520not%2520only%2520enables%2520the%250Alearning%2520of%2520neural%2520networks%2520due%2520to%2520the%2520finite%2520dimension%2520of%2520the%2520surrogate%2520space%250Abut%2520also%2520allows%2520for%2520the%2520prediction%2520of%2520new%2520structures%2520of%2520the%2520output%2520data%2520via%2520a%250Adecoding%2520strategy%2520based%2520on%2520gradient%2520descent.%2520Numerical%2520experiments%2520on%250Asupervised%2520graph%2520prediction%2520problems%2520show%2520that%2520our%2520approach%2520achieves%2520similar%2520or%250Aeven%2520better%2520performance%2520than%2520methods%2520based%2520on%2520a%2520pre-defined%2520kernel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Differentiable%20Surrogate%20Losses%20for%20Structured%20Prediction&entry.906535625=Junjie%20Yang%20and%20Matthieu%20Labeau%20and%20Florence%20d%27Alch%C3%A9-Buc&entry.1292438233=%20%20Structured%20prediction%20involves%20learning%20to%20predict%20complex%20structures%20rather%0Athan%20simple%20scalar%20values.%20The%20main%20challenge%20arises%20from%20the%20non-Euclidean%0Anature%20of%20the%20output%20space%2C%20which%20generally%20requires%20relaxing%20the%20problem%0Aformulation.%20Surrogate%20methods%20build%20on%20kernel-induced%20losses%20or%20more%0Agenerally%2C%20loss%20functions%20admitting%20an%20Implicit%20Loss%20Embedding%2C%20and%20convert%20the%0Aoriginal%20problem%20into%20a%20regression%20task%20followed%20by%20a%20decoding%20step.%20However%2C%0Adesigning%20effective%20losses%20for%20objects%20with%20complex%20structures%20presents%0Asignificant%20challenges%20and%20often%20requires%20domain-specific%20expertise.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20framework%20in%20which%20a%20structured%20loss%20function%2C%0Aparameterized%20by%20neural%20networks%2C%20is%20learned%20directly%20from%20output%20training%20data%0Athrough%20Contrastive%20Learning%2C%20prior%20to%20addressing%20the%20supervised%20surrogate%0Aregression%20problem.%20As%20a%20result%2C%20the%20differentiable%20loss%20not%20only%20enables%20the%0Alearning%20of%20neural%20networks%20due%20to%20the%20finite%20dimension%20of%20the%20surrogate%20space%0Abut%20also%20allows%20for%20the%20prediction%20of%20new%20structures%20of%20the%20output%20data%20via%20a%0Adecoding%20strategy%20based%20on%20gradient%20descent.%20Numerical%20experiments%20on%0Asupervised%20graph%20prediction%20problems%20show%20that%20our%20approach%20achieves%20similar%20or%0Aeven%20better%20performance%20than%20methods%20based%20on%20a%20pre-defined%20kernel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11682v1&entry.124074799=Read"},
{"title": "Relevance-guided Audio Visual Fusion for Video Saliency Prediction", "author": "Li Yu and Xuanzhe Sun and Pan Gao and Moncef Gabbouj", "abstract": "  Audio data, often synchronized with video frames, plays a crucial role in\nguiding the audience's visual attention. Incorporating audio information into\nvideo saliency prediction tasks can enhance the prediction of human visual\nbehavior. However, existing audio-visual saliency prediction methods often\ndirectly fuse audio and visual features, which ignore the possibility of\ninconsistency between the two modalities, such as when the audio serves as\nbackground music. To address this issue, we propose a novel relevance-guided\naudio-visual saliency prediction network dubbed AVRSP. Specifically, the\nRelevance-guided Audio-Visual feature Fusion module (RAVF) dynamically adjusts\nthe retention of audio features based on the semantic relevance between audio\nand visual elements, thereby refining the integration process with visual\nfeatures. Furthermore, the Multi-scale feature Synergy (MS) module integrates\nvisual features from different encoding stages, enhancing the network's ability\nto represent objects at various scales. The Multi-scale Regulator Gate (MRG)\ncould transfer crucial fusion information to visual features, thus optimizing\nthe utilization of multi-scale visual features. Extensive experiments on six\naudio-visual eye movement datasets have demonstrated that our AVRSP network\nachieves competitive performance in audio-visual saliency prediction.\n", "link": "http://arxiv.org/abs/2411.11454v1", "date": "2024-11-18", "relevancy": 2.0238, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relevance-guided%20Audio%20Visual%20Fusion%20for%20Video%20Saliency%20Prediction&body=Title%3A%20Relevance-guided%20Audio%20Visual%20Fusion%20for%20Video%20Saliency%20Prediction%0AAuthor%3A%20Li%20Yu%20and%20Xuanzhe%20Sun%20and%20Pan%20Gao%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20Audio%20data%2C%20often%20synchronized%20with%20video%20frames%2C%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20audience%27s%20visual%20attention.%20Incorporating%20audio%20information%20into%0Avideo%20saliency%20prediction%20tasks%20can%20enhance%20the%20prediction%20of%20human%20visual%0Abehavior.%20However%2C%20existing%20audio-visual%20saliency%20prediction%20methods%20often%0Adirectly%20fuse%20audio%20and%20visual%20features%2C%20which%20ignore%20the%20possibility%20of%0Ainconsistency%20between%20the%20two%20modalities%2C%20such%20as%20when%20the%20audio%20serves%20as%0Abackground%20music.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20relevance-guided%0Aaudio-visual%20saliency%20prediction%20network%20dubbed%20AVRSP.%20Specifically%2C%20the%0ARelevance-guided%20Audio-Visual%20feature%20Fusion%20module%20%28RAVF%29%20dynamically%20adjusts%0Athe%20retention%20of%20audio%20features%20based%20on%20the%20semantic%20relevance%20between%20audio%0Aand%20visual%20elements%2C%20thereby%20refining%20the%20integration%20process%20with%20visual%0Afeatures.%20Furthermore%2C%20the%20Multi-scale%20feature%20Synergy%20%28MS%29%20module%20integrates%0Avisual%20features%20from%20different%20encoding%20stages%2C%20enhancing%20the%20network%27s%20ability%0Ato%20represent%20objects%20at%20various%20scales.%20The%20Multi-scale%20Regulator%20Gate%20%28MRG%29%0Acould%20transfer%20crucial%20fusion%20information%20to%20visual%20features%2C%20thus%20optimizing%0Athe%20utilization%20of%20multi-scale%20visual%20features.%20Extensive%20experiments%20on%20six%0Aaudio-visual%20eye%20movement%20datasets%20have%20demonstrated%20that%20our%20AVRSP%20network%0Aachieves%20competitive%20performance%20in%20audio-visual%20saliency%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelevance-guided%2520Audio%2520Visual%2520Fusion%2520for%2520Video%2520Saliency%2520Prediction%26entry.906535625%3DLi%2520Yu%2520and%2520Xuanzhe%2520Sun%2520and%2520Pan%2520Gao%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520Audio%2520data%252C%2520often%2520synchronized%2520with%2520video%2520frames%252C%2520plays%2520a%2520crucial%2520role%2520in%250Aguiding%2520the%2520audience%2527s%2520visual%2520attention.%2520Incorporating%2520audio%2520information%2520into%250Avideo%2520saliency%2520prediction%2520tasks%2520can%2520enhance%2520the%2520prediction%2520of%2520human%2520visual%250Abehavior.%2520However%252C%2520existing%2520audio-visual%2520saliency%2520prediction%2520methods%2520often%250Adirectly%2520fuse%2520audio%2520and%2520visual%2520features%252C%2520which%2520ignore%2520the%2520possibility%2520of%250Ainconsistency%2520between%2520the%2520two%2520modalities%252C%2520such%2520as%2520when%2520the%2520audio%2520serves%2520as%250Abackground%2520music.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520relevance-guided%250Aaudio-visual%2520saliency%2520prediction%2520network%2520dubbed%2520AVRSP.%2520Specifically%252C%2520the%250ARelevance-guided%2520Audio-Visual%2520feature%2520Fusion%2520module%2520%2528RAVF%2529%2520dynamically%2520adjusts%250Athe%2520retention%2520of%2520audio%2520features%2520based%2520on%2520the%2520semantic%2520relevance%2520between%2520audio%250Aand%2520visual%2520elements%252C%2520thereby%2520refining%2520the%2520integration%2520process%2520with%2520visual%250Afeatures.%2520Furthermore%252C%2520the%2520Multi-scale%2520feature%2520Synergy%2520%2528MS%2529%2520module%2520integrates%250Avisual%2520features%2520from%2520different%2520encoding%2520stages%252C%2520enhancing%2520the%2520network%2527s%2520ability%250Ato%2520represent%2520objects%2520at%2520various%2520scales.%2520The%2520Multi-scale%2520Regulator%2520Gate%2520%2528MRG%2529%250Acould%2520transfer%2520crucial%2520fusion%2520information%2520to%2520visual%2520features%252C%2520thus%2520optimizing%250Athe%2520utilization%2520of%2520multi-scale%2520visual%2520features.%2520Extensive%2520experiments%2520on%2520six%250Aaudio-visual%2520eye%2520movement%2520datasets%2520have%2520demonstrated%2520that%2520our%2520AVRSP%2520network%250Aachieves%2520competitive%2520performance%2520in%2520audio-visual%2520saliency%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relevance-guided%20Audio%20Visual%20Fusion%20for%20Video%20Saliency%20Prediction&entry.906535625=Li%20Yu%20and%20Xuanzhe%20Sun%20and%20Pan%20Gao%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20Audio%20data%2C%20often%20synchronized%20with%20video%20frames%2C%20plays%20a%20crucial%20role%20in%0Aguiding%20the%20audience%27s%20visual%20attention.%20Incorporating%20audio%20information%20into%0Avideo%20saliency%20prediction%20tasks%20can%20enhance%20the%20prediction%20of%20human%20visual%0Abehavior.%20However%2C%20existing%20audio-visual%20saliency%20prediction%20methods%20often%0Adirectly%20fuse%20audio%20and%20visual%20features%2C%20which%20ignore%20the%20possibility%20of%0Ainconsistency%20between%20the%20two%20modalities%2C%20such%20as%20when%20the%20audio%20serves%20as%0Abackground%20music.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20relevance-guided%0Aaudio-visual%20saliency%20prediction%20network%20dubbed%20AVRSP.%20Specifically%2C%20the%0ARelevance-guided%20Audio-Visual%20feature%20Fusion%20module%20%28RAVF%29%20dynamically%20adjusts%0Athe%20retention%20of%20audio%20features%20based%20on%20the%20semantic%20relevance%20between%20audio%0Aand%20visual%20elements%2C%20thereby%20refining%20the%20integration%20process%20with%20visual%0Afeatures.%20Furthermore%2C%20the%20Multi-scale%20feature%20Synergy%20%28MS%29%20module%20integrates%0Avisual%20features%20from%20different%20encoding%20stages%2C%20enhancing%20the%20network%27s%20ability%0Ato%20represent%20objects%20at%20various%20scales.%20The%20Multi-scale%20Regulator%20Gate%20%28MRG%29%0Acould%20transfer%20crucial%20fusion%20information%20to%20visual%20features%2C%20thus%20optimizing%0Athe%20utilization%20of%20multi-scale%20visual%20features.%20Extensive%20experiments%20on%20six%0Aaudio-visual%20eye%20movement%20datasets%20have%20demonstrated%20that%20our%20AVRSP%20network%0Aachieves%20competitive%20performance%20in%20audio-visual%20saliency%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11454v1&entry.124074799=Read"},
{"title": "Robust State Estimation for Legged Robots with Dual Beta Kalman Filter", "author": "Tianyi Zhang and Wenhan Cao and Chang Liu and Tao Zhang and Jiangtao Li and Shengbo Eben Li", "abstract": "  Existing state estimation algorithms for legged robots that rely on\nproprioceptive sensors often overlook foot slippage and leg deformation in the\nphysical world, leading to large estimation errors. To address this limitation,\nwe propose a comprehensive measurement model that accounts for both foot\nslippage and variable leg length by analyzing the relative motion between foot\ncontact points and the robot's body center. We show that leg length is an\nobservable quantity, meaning that its value can be explicitly inferred by\ndesigning an auxiliary filter. To this end, we introduce a dual estimation\nframework that iteratively employs a parameter filter to estimate the leg\nlength parameters and a state filter to estimate the robot's state. To prevent\nerror accumulation in this iterative framework, we construct a partial\nmeasurement model for the parameter filter using the leg static equation. This\napproach ensures that leg length estimation relies solely on joint torques and\nfoot contact forces, avoiding the influence of state estimation errors on the\nparameter estimation. Unlike leg length which can be directly estimated, foot\nslippage cannot be measured directly with the current sensor configuration.\nHowever, since foot slippage occurs at a low frequency, it can be treated as\noutliers in the measurement data. To mitigate the impact of these outliers, we\npropose the beta Kalman filter (beta KF), which redefines the estimation loss\nin canonical Kalman filtering using beta divergence. This divergence can assign\nlow weights to outliers in an adaptive manner, thereby enhancing the robustness\nof the estimation algorithm. These techniques together form the dual\nbeta-Kalman filter (Dual beta KF), a novel algorithm for robust state\nestimation in legged robots. Experimental results on the Unitree GO2 robot\ndemonstrate that the Dual beta KF significantly outperforms state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2411.11483v1", "date": "2024-11-18", "relevancy": 2.0233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6445}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20State%20Estimation%20for%20Legged%20Robots%20with%20Dual%20Beta%20Kalman%20Filter&body=Title%3A%20Robust%20State%20Estimation%20for%20Legged%20Robots%20with%20Dual%20Beta%20Kalman%20Filter%0AAuthor%3A%20Tianyi%20Zhang%20and%20Wenhan%20Cao%20and%20Chang%20Liu%20and%20Tao%20Zhang%20and%20Jiangtao%20Li%20and%20Shengbo%20Eben%20Li%0AAbstract%3A%20%20%20Existing%20state%20estimation%20algorithms%20for%20legged%20robots%20that%20rely%20on%0Aproprioceptive%20sensors%20often%20overlook%20foot%20slippage%20and%20leg%20deformation%20in%20the%0Aphysical%20world%2C%20leading%20to%20large%20estimation%20errors.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20comprehensive%20measurement%20model%20that%20accounts%20for%20both%20foot%0Aslippage%20and%20variable%20leg%20length%20by%20analyzing%20the%20relative%20motion%20between%20foot%0Acontact%20points%20and%20the%20robot%27s%20body%20center.%20We%20show%20that%20leg%20length%20is%20an%0Aobservable%20quantity%2C%20meaning%20that%20its%20value%20can%20be%20explicitly%20inferred%20by%0Adesigning%20an%20auxiliary%20filter.%20To%20this%20end%2C%20we%20introduce%20a%20dual%20estimation%0Aframework%20that%20iteratively%20employs%20a%20parameter%20filter%20to%20estimate%20the%20leg%0Alength%20parameters%20and%20a%20state%20filter%20to%20estimate%20the%20robot%27s%20state.%20To%20prevent%0Aerror%20accumulation%20in%20this%20iterative%20framework%2C%20we%20construct%20a%20partial%0Ameasurement%20model%20for%20the%20parameter%20filter%20using%20the%20leg%20static%20equation.%20This%0Aapproach%20ensures%20that%20leg%20length%20estimation%20relies%20solely%20on%20joint%20torques%20and%0Afoot%20contact%20forces%2C%20avoiding%20the%20influence%20of%20state%20estimation%20errors%20on%20the%0Aparameter%20estimation.%20Unlike%20leg%20length%20which%20can%20be%20directly%20estimated%2C%20foot%0Aslippage%20cannot%20be%20measured%20directly%20with%20the%20current%20sensor%20configuration.%0AHowever%2C%20since%20foot%20slippage%20occurs%20at%20a%20low%20frequency%2C%20it%20can%20be%20treated%20as%0Aoutliers%20in%20the%20measurement%20data.%20To%20mitigate%20the%20impact%20of%20these%20outliers%2C%20we%0Apropose%20the%20beta%20Kalman%20filter%20%28beta%20KF%29%2C%20which%20redefines%20the%20estimation%20loss%0Ain%20canonical%20Kalman%20filtering%20using%20beta%20divergence.%20This%20divergence%20can%20assign%0Alow%20weights%20to%20outliers%20in%20an%20adaptive%20manner%2C%20thereby%20enhancing%20the%20robustness%0Aof%20the%20estimation%20algorithm.%20These%20techniques%20together%20form%20the%20dual%0Abeta-Kalman%20filter%20%28Dual%20beta%20KF%29%2C%20a%20novel%20algorithm%20for%20robust%20state%0Aestimation%20in%20legged%20robots.%20Experimental%20results%20on%20the%20Unitree%20GO2%20robot%0Ademonstrate%20that%20the%20Dual%20beta%20KF%20significantly%20outperforms%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520State%2520Estimation%2520for%2520Legged%2520Robots%2520with%2520Dual%2520Beta%2520Kalman%2520Filter%26entry.906535625%3DTianyi%2520Zhang%2520and%2520Wenhan%2520Cao%2520and%2520Chang%2520Liu%2520and%2520Tao%2520Zhang%2520and%2520Jiangtao%2520Li%2520and%2520Shengbo%2520Eben%2520Li%26entry.1292438233%3D%2520%2520Existing%2520state%2520estimation%2520algorithms%2520for%2520legged%2520robots%2520that%2520rely%2520on%250Aproprioceptive%2520sensors%2520often%2520overlook%2520foot%2520slippage%2520and%2520leg%2520deformation%2520in%2520the%250Aphysical%2520world%252C%2520leading%2520to%2520large%2520estimation%2520errors.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520comprehensive%2520measurement%2520model%2520that%2520accounts%2520for%2520both%2520foot%250Aslippage%2520and%2520variable%2520leg%2520length%2520by%2520analyzing%2520the%2520relative%2520motion%2520between%2520foot%250Acontact%2520points%2520and%2520the%2520robot%2527s%2520body%2520center.%2520We%2520show%2520that%2520leg%2520length%2520is%2520an%250Aobservable%2520quantity%252C%2520meaning%2520that%2520its%2520value%2520can%2520be%2520explicitly%2520inferred%2520by%250Adesigning%2520an%2520auxiliary%2520filter.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520dual%2520estimation%250Aframework%2520that%2520iteratively%2520employs%2520a%2520parameter%2520filter%2520to%2520estimate%2520the%2520leg%250Alength%2520parameters%2520and%2520a%2520state%2520filter%2520to%2520estimate%2520the%2520robot%2527s%2520state.%2520To%2520prevent%250Aerror%2520accumulation%2520in%2520this%2520iterative%2520framework%252C%2520we%2520construct%2520a%2520partial%250Ameasurement%2520model%2520for%2520the%2520parameter%2520filter%2520using%2520the%2520leg%2520static%2520equation.%2520This%250Aapproach%2520ensures%2520that%2520leg%2520length%2520estimation%2520relies%2520solely%2520on%2520joint%2520torques%2520and%250Afoot%2520contact%2520forces%252C%2520avoiding%2520the%2520influence%2520of%2520state%2520estimation%2520errors%2520on%2520the%250Aparameter%2520estimation.%2520Unlike%2520leg%2520length%2520which%2520can%2520be%2520directly%2520estimated%252C%2520foot%250Aslippage%2520cannot%2520be%2520measured%2520directly%2520with%2520the%2520current%2520sensor%2520configuration.%250AHowever%252C%2520since%2520foot%2520slippage%2520occurs%2520at%2520a%2520low%2520frequency%252C%2520it%2520can%2520be%2520treated%2520as%250Aoutliers%2520in%2520the%2520measurement%2520data.%2520To%2520mitigate%2520the%2520impact%2520of%2520these%2520outliers%252C%2520we%250Apropose%2520the%2520beta%2520Kalman%2520filter%2520%2528beta%2520KF%2529%252C%2520which%2520redefines%2520the%2520estimation%2520loss%250Ain%2520canonical%2520Kalman%2520filtering%2520using%2520beta%2520divergence.%2520This%2520divergence%2520can%2520assign%250Alow%2520weights%2520to%2520outliers%2520in%2520an%2520adaptive%2520manner%252C%2520thereby%2520enhancing%2520the%2520robustness%250Aof%2520the%2520estimation%2520algorithm.%2520These%2520techniques%2520together%2520form%2520the%2520dual%250Abeta-Kalman%2520filter%2520%2528Dual%2520beta%2520KF%2529%252C%2520a%2520novel%2520algorithm%2520for%2520robust%2520state%250Aestimation%2520in%2520legged%2520robots.%2520Experimental%2520results%2520on%2520the%2520Unitree%2520GO2%2520robot%250Ademonstrate%2520that%2520the%2520Dual%2520beta%2520KF%2520significantly%2520outperforms%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20State%20Estimation%20for%20Legged%20Robots%20with%20Dual%20Beta%20Kalman%20Filter&entry.906535625=Tianyi%20Zhang%20and%20Wenhan%20Cao%20and%20Chang%20Liu%20and%20Tao%20Zhang%20and%20Jiangtao%20Li%20and%20Shengbo%20Eben%20Li&entry.1292438233=%20%20Existing%20state%20estimation%20algorithms%20for%20legged%20robots%20that%20rely%20on%0Aproprioceptive%20sensors%20often%20overlook%20foot%20slippage%20and%20leg%20deformation%20in%20the%0Aphysical%20world%2C%20leading%20to%20large%20estimation%20errors.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20comprehensive%20measurement%20model%20that%20accounts%20for%20both%20foot%0Aslippage%20and%20variable%20leg%20length%20by%20analyzing%20the%20relative%20motion%20between%20foot%0Acontact%20points%20and%20the%20robot%27s%20body%20center.%20We%20show%20that%20leg%20length%20is%20an%0Aobservable%20quantity%2C%20meaning%20that%20its%20value%20can%20be%20explicitly%20inferred%20by%0Adesigning%20an%20auxiliary%20filter.%20To%20this%20end%2C%20we%20introduce%20a%20dual%20estimation%0Aframework%20that%20iteratively%20employs%20a%20parameter%20filter%20to%20estimate%20the%20leg%0Alength%20parameters%20and%20a%20state%20filter%20to%20estimate%20the%20robot%27s%20state.%20To%20prevent%0Aerror%20accumulation%20in%20this%20iterative%20framework%2C%20we%20construct%20a%20partial%0Ameasurement%20model%20for%20the%20parameter%20filter%20using%20the%20leg%20static%20equation.%20This%0Aapproach%20ensures%20that%20leg%20length%20estimation%20relies%20solely%20on%20joint%20torques%20and%0Afoot%20contact%20forces%2C%20avoiding%20the%20influence%20of%20state%20estimation%20errors%20on%20the%0Aparameter%20estimation.%20Unlike%20leg%20length%20which%20can%20be%20directly%20estimated%2C%20foot%0Aslippage%20cannot%20be%20measured%20directly%20with%20the%20current%20sensor%20configuration.%0AHowever%2C%20since%20foot%20slippage%20occurs%20at%20a%20low%20frequency%2C%20it%20can%20be%20treated%20as%0Aoutliers%20in%20the%20measurement%20data.%20To%20mitigate%20the%20impact%20of%20these%20outliers%2C%20we%0Apropose%20the%20beta%20Kalman%20filter%20%28beta%20KF%29%2C%20which%20redefines%20the%20estimation%20loss%0Ain%20canonical%20Kalman%20filtering%20using%20beta%20divergence.%20This%20divergence%20can%20assign%0Alow%20weights%20to%20outliers%20in%20an%20adaptive%20manner%2C%20thereby%20enhancing%20the%20robustness%0Aof%20the%20estimation%20algorithm.%20These%20techniques%20together%20form%20the%20dual%0Abeta-Kalman%20filter%20%28Dual%20beta%20KF%29%2C%20a%20novel%20algorithm%20for%20robust%20state%0Aestimation%20in%20legged%20robots.%20Experimental%20results%20on%20the%20Unitree%20GO2%20robot%0Ademonstrate%20that%20the%20Dual%20beta%20KF%20significantly%20outperforms%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11483v1&entry.124074799=Read"},
{"title": "SL-YOLO: A Stronger and Lighter Drone Target Detection Model", "author": "Defan Chen and Luchan Zhang", "abstract": "  Detecting small objects in complex scenes, such as those captured by drones,\nis a daunting challenge due to the difficulty in capturing the complex features\nof small targets. While the YOLO family has achieved great success in large\ntarget detection, its performance is less than satisfactory when faced with\nsmall targets. Because of this, this paper proposes a revolutionary model\nSL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small\ntarget detection. We propose the Hierarchical Extended Path Aggregation Network\n(HEPAN), a pioneering cross-scale feature fusion method that can ensure\nunparalleled detection accuracy even in the most challenging environments. At\nthe same time, without sacrificing detection capabilities, we design the C2fDCB\nlightweight module and add the SCDown downsampling module to greatly reduce the\nmodel's parameters and computational complexity. Our experimental results on\nthe VisDrone2019 dataset reveal a significant improvement in performance, with\nmAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to\n28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M,\nand the FPS can reach 132, making it an ideal solution for real-time small\nobject detection in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2411.11477v1", "date": "2024-11-18", "relevancy": 2.0213, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SL-YOLO%3A%20A%20Stronger%20and%20Lighter%20Drone%20Target%20Detection%20Model&body=Title%3A%20SL-YOLO%3A%20A%20Stronger%20and%20Lighter%20Drone%20Target%20Detection%20Model%0AAuthor%3A%20Defan%20Chen%20and%20Luchan%20Zhang%0AAbstract%3A%20%20%20Detecting%20small%20objects%20in%20complex%20scenes%2C%20such%20as%20those%20captured%20by%20drones%2C%0Ais%20a%20daunting%20challenge%20due%20to%20the%20difficulty%20in%20capturing%20the%20complex%20features%0Aof%20small%20targets.%20While%20the%20YOLO%20family%20has%20achieved%20great%20success%20in%20large%0Atarget%20detection%2C%20its%20performance%20is%20less%20than%20satisfactory%20when%20faced%20with%0Asmall%20targets.%20Because%20of%20this%2C%20this%20paper%20proposes%20a%20revolutionary%20model%0ASL-YOLO%20%28Stronger%20and%20Lighter%20YOLO%29%20that%20aims%20to%20break%20the%20bottleneck%20of%20small%0Atarget%20detection.%20We%20propose%20the%20Hierarchical%20Extended%20Path%20Aggregation%20Network%0A%28HEPAN%29%2C%20a%20pioneering%20cross-scale%20feature%20fusion%20method%20that%20can%20ensure%0Aunparalleled%20detection%20accuracy%20even%20in%20the%20most%20challenging%20environments.%20At%0Athe%20same%20time%2C%20without%20sacrificing%20detection%20capabilities%2C%20we%20design%20the%20C2fDCB%0Alightweight%20module%20and%20add%20the%20SCDown%20downsampling%20module%20to%20greatly%20reduce%20the%0Amodel%27s%20parameters%20and%20computational%20complexity.%20Our%20experimental%20results%20on%0Athe%20VisDrone2019%20dataset%20reveal%20a%20significant%20improvement%20in%20performance%2C%20with%0AmAP%400.5%20jumping%20from%2043.0%25%20to%2046.9%25%20and%20mAP%400.5%3A0.95%20increasing%20from%2026.0%25%20to%0A28.9%25.%20At%20the%20same%20time%2C%20the%20model%20parameters%20are%20reduced%20from%2011.1M%20to%209.6M%2C%0Aand%20the%20FPS%20can%20reach%20132%2C%20making%20it%20an%20ideal%20solution%20for%20real-time%20small%0Aobject%20detection%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSL-YOLO%253A%2520A%2520Stronger%2520and%2520Lighter%2520Drone%2520Target%2520Detection%2520Model%26entry.906535625%3DDefan%2520Chen%2520and%2520Luchan%2520Zhang%26entry.1292438233%3D%2520%2520Detecting%2520small%2520objects%2520in%2520complex%2520scenes%252C%2520such%2520as%2520those%2520captured%2520by%2520drones%252C%250Ais%2520a%2520daunting%2520challenge%2520due%2520to%2520the%2520difficulty%2520in%2520capturing%2520the%2520complex%2520features%250Aof%2520small%2520targets.%2520While%2520the%2520YOLO%2520family%2520has%2520achieved%2520great%2520success%2520in%2520large%250Atarget%2520detection%252C%2520its%2520performance%2520is%2520less%2520than%2520satisfactory%2520when%2520faced%2520with%250Asmall%2520targets.%2520Because%2520of%2520this%252C%2520this%2520paper%2520proposes%2520a%2520revolutionary%2520model%250ASL-YOLO%2520%2528Stronger%2520and%2520Lighter%2520YOLO%2529%2520that%2520aims%2520to%2520break%2520the%2520bottleneck%2520of%2520small%250Atarget%2520detection.%2520We%2520propose%2520the%2520Hierarchical%2520Extended%2520Path%2520Aggregation%2520Network%250A%2528HEPAN%2529%252C%2520a%2520pioneering%2520cross-scale%2520feature%2520fusion%2520method%2520that%2520can%2520ensure%250Aunparalleled%2520detection%2520accuracy%2520even%2520in%2520the%2520most%2520challenging%2520environments.%2520At%250Athe%2520same%2520time%252C%2520without%2520sacrificing%2520detection%2520capabilities%252C%2520we%2520design%2520the%2520C2fDCB%250Alightweight%2520module%2520and%2520add%2520the%2520SCDown%2520downsampling%2520module%2520to%2520greatly%2520reduce%2520the%250Amodel%2527s%2520parameters%2520and%2520computational%2520complexity.%2520Our%2520experimental%2520results%2520on%250Athe%2520VisDrone2019%2520dataset%2520reveal%2520a%2520significant%2520improvement%2520in%2520performance%252C%2520with%250AmAP%25400.5%2520jumping%2520from%252043.0%2525%2520to%252046.9%2525%2520and%2520mAP%25400.5%253A0.95%2520increasing%2520from%252026.0%2525%2520to%250A28.9%2525.%2520At%2520the%2520same%2520time%252C%2520the%2520model%2520parameters%2520are%2520reduced%2520from%252011.1M%2520to%25209.6M%252C%250Aand%2520the%2520FPS%2520can%2520reach%2520132%252C%2520making%2520it%2520an%2520ideal%2520solution%2520for%2520real-time%2520small%250Aobject%2520detection%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SL-YOLO%3A%20A%20Stronger%20and%20Lighter%20Drone%20Target%20Detection%20Model&entry.906535625=Defan%20Chen%20and%20Luchan%20Zhang&entry.1292438233=%20%20Detecting%20small%20objects%20in%20complex%20scenes%2C%20such%20as%20those%20captured%20by%20drones%2C%0Ais%20a%20daunting%20challenge%20due%20to%20the%20difficulty%20in%20capturing%20the%20complex%20features%0Aof%20small%20targets.%20While%20the%20YOLO%20family%20has%20achieved%20great%20success%20in%20large%0Atarget%20detection%2C%20its%20performance%20is%20less%20than%20satisfactory%20when%20faced%20with%0Asmall%20targets.%20Because%20of%20this%2C%20this%20paper%20proposes%20a%20revolutionary%20model%0ASL-YOLO%20%28Stronger%20and%20Lighter%20YOLO%29%20that%20aims%20to%20break%20the%20bottleneck%20of%20small%0Atarget%20detection.%20We%20propose%20the%20Hierarchical%20Extended%20Path%20Aggregation%20Network%0A%28HEPAN%29%2C%20a%20pioneering%20cross-scale%20feature%20fusion%20method%20that%20can%20ensure%0Aunparalleled%20detection%20accuracy%20even%20in%20the%20most%20challenging%20environments.%20At%0Athe%20same%20time%2C%20without%20sacrificing%20detection%20capabilities%2C%20we%20design%20the%20C2fDCB%0Alightweight%20module%20and%20add%20the%20SCDown%20downsampling%20module%20to%20greatly%20reduce%20the%0Amodel%27s%20parameters%20and%20computational%20complexity.%20Our%20experimental%20results%20on%0Athe%20VisDrone2019%20dataset%20reveal%20a%20significant%20improvement%20in%20performance%2C%20with%0AmAP%400.5%20jumping%20from%2043.0%25%20to%2046.9%25%20and%20mAP%400.5%3A0.95%20increasing%20from%2026.0%25%20to%0A28.9%25.%20At%20the%20same%20time%2C%20the%20model%20parameters%20are%20reduced%20from%2011.1M%20to%209.6M%2C%0Aand%20the%20FPS%20can%20reach%20132%2C%20making%20it%20an%20ideal%20solution%20for%20real-time%20small%0Aobject%20detection%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11477v1&entry.124074799=Read"},
{"title": "BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration", "author": "Yuzong Chen and Ahmed F. AbouElhamayed and Xilai Dai and Yang Wang and Marta Andronic and George A. Constantinides and Mohamed S. Abdelfattah", "abstract": "  Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks. Yet the substantial memory footprint of LLMs\nsignificantly hinders their deployment. In this paper, we improve the\naccessibility of LLMs through BitMoD, an algorithm-hardware co-design solution\nthat enables efficient LLM acceleration at low weight precision. On the\nalgorithm side, BitMoD introduces fine-grained data type adaptation that uses a\ndifferent numerical data type to quantize a group of (e.g., 128) weights.\nThrough the careful design of these new data types, BitMoD is able to quantize\nLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining\nhigh accuracy. On the hardware side, BitMoD employs a bit-serial processing\nelement to easily support multiple numerical precisions and data types; our\nhardware design includes two key innovations: First, it employs a unified\nrepresentation to process different weight data types, thus reducing the\nhardware cost. Second, it adopts a bit-serial dequantization unit to rescale\nthe per-group partial sum with minimal hardware overhead. Our evaluation on six\nrepresentative LLMs demonstrates that BitMoD significantly outperforms\nstate-of-the-art LLM quantization and acceleration methods. For discriminative\ntasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss\non average. For generative tasks, BitMoD is able to quantize LLM weights to\n3-bit while achieving better perplexity than prior LLM quantization scheme.\nCombining the superior model performance with an efficient accelerator design,\nBitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared\nto prior LLM accelerators ANT and OliVe, respectively.\n", "link": "http://arxiv.org/abs/2411.11745v1", "date": "2024-11-18", "relevancy": 2.0185, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BitMoD%3A%20Bit-serial%20Mixture-of-Datatype%20LLM%20Acceleration&body=Title%3A%20BitMoD%3A%20Bit-serial%20Mixture-of-Datatype%20LLM%20Acceleration%0AAuthor%3A%20Yuzong%20Chen%20and%20Ahmed%20F.%20AbouElhamayed%20and%20Xilai%20Dai%20and%20Yang%20Wang%20and%20Marta%20Andronic%20and%20George%20A.%20Constantinides%20and%20Mohamed%20S.%20Abdelfattah%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20machine%20learning%20tasks.%20Yet%20the%20substantial%20memory%20footprint%20of%20LLMs%0Asignificantly%20hinders%20their%20deployment.%20In%20this%20paper%2C%20we%20improve%20the%0Aaccessibility%20of%20LLMs%20through%20BitMoD%2C%20an%20algorithm-hardware%20co-design%20solution%0Athat%20enables%20efficient%20LLM%20acceleration%20at%20low%20weight%20precision.%20On%20the%0Aalgorithm%20side%2C%20BitMoD%20introduces%20fine-grained%20data%20type%20adaptation%20that%20uses%20a%0Adifferent%20numerical%20data%20type%20to%20quantize%20a%20group%20of%20%28e.g.%2C%20128%29%20weights.%0AThrough%20the%20careful%20design%20of%20these%20new%20data%20types%2C%20BitMoD%20is%20able%20to%20quantize%0ALLM%20weights%20to%20very%20low%20precision%20%28e.g.%2C%204%20bits%20and%203%20bits%29%20while%20maintaining%0Ahigh%20accuracy.%20On%20the%20hardware%20side%2C%20BitMoD%20employs%20a%20bit-serial%20processing%0Aelement%20to%20easily%20support%20multiple%20numerical%20precisions%20and%20data%20types%3B%20our%0Ahardware%20design%20includes%20two%20key%20innovations%3A%20First%2C%20it%20employs%20a%20unified%0Arepresentation%20to%20process%20different%20weight%20data%20types%2C%20thus%20reducing%20the%0Ahardware%20cost.%20Second%2C%20it%20adopts%20a%20bit-serial%20dequantization%20unit%20to%20rescale%0Athe%20per-group%20partial%20sum%20with%20minimal%20hardware%20overhead.%20Our%20evaluation%20on%20six%0Arepresentative%20LLMs%20demonstrates%20that%20BitMoD%20significantly%20outperforms%0Astate-of-the-art%20LLM%20quantization%20and%20acceleration%20methods.%20For%20discriminative%0Atasks%2C%20BitMoD%20can%20quantize%20LLM%20weights%20to%204-bit%20with%20%24%3C%5C%210.5%5C%25%24%20accuracy%20loss%0Aon%20average.%20For%20generative%20tasks%2C%20BitMoD%20is%20able%20to%20quantize%20LLM%20weights%20to%0A3-bit%20while%20achieving%20better%20perplexity%20than%20prior%20LLM%20quantization%20scheme.%0ACombining%20the%20superior%20model%20performance%20with%20an%20efficient%20accelerator%20design%2C%0ABitMoD%20achieves%20an%20average%20of%20%241.69%5Ctimes%24%20and%20%241.48%5Ctimes%24%20speedups%20compared%0Ato%20prior%20LLM%20accelerators%20ANT%20and%20OliVe%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitMoD%253A%2520Bit-serial%2520Mixture-of-Datatype%2520LLM%2520Acceleration%26entry.906535625%3DYuzong%2520Chen%2520and%2520Ahmed%2520F.%2520AbouElhamayed%2520and%2520Xilai%2520Dai%2520and%2520Yang%2520Wang%2520and%2520Marta%2520Andronic%2520and%2520George%2520A.%2520Constantinides%2520and%2520Mohamed%2520S.%2520Abdelfattah%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%250Avarious%2520machine%2520learning%2520tasks.%2520Yet%2520the%2520substantial%2520memory%2520footprint%2520of%2520LLMs%250Asignificantly%2520hinders%2520their%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520improve%2520the%250Aaccessibility%2520of%2520LLMs%2520through%2520BitMoD%252C%2520an%2520algorithm-hardware%2520co-design%2520solution%250Athat%2520enables%2520efficient%2520LLM%2520acceleration%2520at%2520low%2520weight%2520precision.%2520On%2520the%250Aalgorithm%2520side%252C%2520BitMoD%2520introduces%2520fine-grained%2520data%2520type%2520adaptation%2520that%2520uses%2520a%250Adifferent%2520numerical%2520data%2520type%2520to%2520quantize%2520a%2520group%2520of%2520%2528e.g.%252C%2520128%2529%2520weights.%250AThrough%2520the%2520careful%2520design%2520of%2520these%2520new%2520data%2520types%252C%2520BitMoD%2520is%2520able%2520to%2520quantize%250ALLM%2520weights%2520to%2520very%2520low%2520precision%2520%2528e.g.%252C%25204%2520bits%2520and%25203%2520bits%2529%2520while%2520maintaining%250Ahigh%2520accuracy.%2520On%2520the%2520hardware%2520side%252C%2520BitMoD%2520employs%2520a%2520bit-serial%2520processing%250Aelement%2520to%2520easily%2520support%2520multiple%2520numerical%2520precisions%2520and%2520data%2520types%253B%2520our%250Ahardware%2520design%2520includes%2520two%2520key%2520innovations%253A%2520First%252C%2520it%2520employs%2520a%2520unified%250Arepresentation%2520to%2520process%2520different%2520weight%2520data%2520types%252C%2520thus%2520reducing%2520the%250Ahardware%2520cost.%2520Second%252C%2520it%2520adopts%2520a%2520bit-serial%2520dequantization%2520unit%2520to%2520rescale%250Athe%2520per-group%2520partial%2520sum%2520with%2520minimal%2520hardware%2520overhead.%2520Our%2520evaluation%2520on%2520six%250Arepresentative%2520LLMs%2520demonstrates%2520that%2520BitMoD%2520significantly%2520outperforms%250Astate-of-the-art%2520LLM%2520quantization%2520and%2520acceleration%2520methods.%2520For%2520discriminative%250Atasks%252C%2520BitMoD%2520can%2520quantize%2520LLM%2520weights%2520to%25204-bit%2520with%2520%2524%253C%255C%25210.5%255C%2525%2524%2520accuracy%2520loss%250Aon%2520average.%2520For%2520generative%2520tasks%252C%2520BitMoD%2520is%2520able%2520to%2520quantize%2520LLM%2520weights%2520to%250A3-bit%2520while%2520achieving%2520better%2520perplexity%2520than%2520prior%2520LLM%2520quantization%2520scheme.%250ACombining%2520the%2520superior%2520model%2520performance%2520with%2520an%2520efficient%2520accelerator%2520design%252C%250ABitMoD%2520achieves%2520an%2520average%2520of%2520%25241.69%255Ctimes%2524%2520and%2520%25241.48%255Ctimes%2524%2520speedups%2520compared%250Ato%2520prior%2520LLM%2520accelerators%2520ANT%2520and%2520OliVe%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BitMoD%3A%20Bit-serial%20Mixture-of-Datatype%20LLM%20Acceleration&entry.906535625=Yuzong%20Chen%20and%20Ahmed%20F.%20AbouElhamayed%20and%20Xilai%20Dai%20and%20Yang%20Wang%20and%20Marta%20Andronic%20and%20George%20A.%20Constantinides%20and%20Mohamed%20S.%20Abdelfattah&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20across%0Avarious%20machine%20learning%20tasks.%20Yet%20the%20substantial%20memory%20footprint%20of%20LLMs%0Asignificantly%20hinders%20their%20deployment.%20In%20this%20paper%2C%20we%20improve%20the%0Aaccessibility%20of%20LLMs%20through%20BitMoD%2C%20an%20algorithm-hardware%20co-design%20solution%0Athat%20enables%20efficient%20LLM%20acceleration%20at%20low%20weight%20precision.%20On%20the%0Aalgorithm%20side%2C%20BitMoD%20introduces%20fine-grained%20data%20type%20adaptation%20that%20uses%20a%0Adifferent%20numerical%20data%20type%20to%20quantize%20a%20group%20of%20%28e.g.%2C%20128%29%20weights.%0AThrough%20the%20careful%20design%20of%20these%20new%20data%20types%2C%20BitMoD%20is%20able%20to%20quantize%0ALLM%20weights%20to%20very%20low%20precision%20%28e.g.%2C%204%20bits%20and%203%20bits%29%20while%20maintaining%0Ahigh%20accuracy.%20On%20the%20hardware%20side%2C%20BitMoD%20employs%20a%20bit-serial%20processing%0Aelement%20to%20easily%20support%20multiple%20numerical%20precisions%20and%20data%20types%3B%20our%0Ahardware%20design%20includes%20two%20key%20innovations%3A%20First%2C%20it%20employs%20a%20unified%0Arepresentation%20to%20process%20different%20weight%20data%20types%2C%20thus%20reducing%20the%0Ahardware%20cost.%20Second%2C%20it%20adopts%20a%20bit-serial%20dequantization%20unit%20to%20rescale%0Athe%20per-group%20partial%20sum%20with%20minimal%20hardware%20overhead.%20Our%20evaluation%20on%20six%0Arepresentative%20LLMs%20demonstrates%20that%20BitMoD%20significantly%20outperforms%0Astate-of-the-art%20LLM%20quantization%20and%20acceleration%20methods.%20For%20discriminative%0Atasks%2C%20BitMoD%20can%20quantize%20LLM%20weights%20to%204-bit%20with%20%24%3C%5C%210.5%5C%25%24%20accuracy%20loss%0Aon%20average.%20For%20generative%20tasks%2C%20BitMoD%20is%20able%20to%20quantize%20LLM%20weights%20to%0A3-bit%20while%20achieving%20better%20perplexity%20than%20prior%20LLM%20quantization%20scheme.%0ACombining%20the%20superior%20model%20performance%20with%20an%20efficient%20accelerator%20design%2C%0ABitMoD%20achieves%20an%20average%20of%20%241.69%5Ctimes%24%20and%20%241.48%5Ctimes%24%20speedups%20compared%0Ato%20prior%20LLM%20accelerators%20ANT%20and%20OliVe%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11745v1&entry.124074799=Read"},
{"title": "Structure learning with Temporal Gaussian Mixture for model-based\n  Reinforcement Learning", "author": "Th\u00e9ophile Champion and Marek Grze\u015b and Howard Bowman", "abstract": "  Model-based reinforcement learning refers to a set of approaches capable of\nsample-efficient decision making, which create an explicit model of the\nenvironment. This model can subsequently be used for learning optimal policies.\nIn this paper, we propose a temporal Gaussian Mixture Model composed of a\nperception model and a transition model. The perception model extracts discrete\n(latent) states from continuous observations using a variational Gaussian\nmixture likelihood. Importantly, our model constantly monitors the collected\ndata searching for new Gaussian components, i.e., the perception model performs\na form of structure learning (Smith et al., 2020; Friston et al., 2018; Neacsu\net al., 2022) as it learns the number of Gaussian components in the mixture.\nAdditionally, the transition model learns the temporal transition between\nconsecutive time steps by taking advantage of the Dirichlet-categorical\nconjugacy. Both the perception and transition models are able to forget part of\nthe data points, while integrating the information they provide within the\nprior, which ensure fast variational inference. Finally, decision making is\nperformed with a variant of Q-learning which is able to learn Q-values from\nbeliefs over states. Empirically, we have demonstrated the model's ability to\nlearn the structure of several mazes: the model discovered the number of states\nand the transition probabilities between these states. Moreover, using its\nlearned Q-values, the agent was able to successfully navigate from the starting\nposition to the maze's exit.\n", "link": "http://arxiv.org/abs/2411.11511v1", "date": "2024-11-18", "relevancy": 2.0145, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20learning%20with%20Temporal%20Gaussian%20Mixture%20for%20model-based%0A%20%20Reinforcement%20Learning&body=Title%3A%20Structure%20learning%20with%20Temporal%20Gaussian%20Mixture%20for%20model-based%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Th%C3%A9ophile%20Champion%20and%20Marek%20Grze%C5%9B%20and%20Howard%20Bowman%0AAbstract%3A%20%20%20Model-based%20reinforcement%20learning%20refers%20to%20a%20set%20of%20approaches%20capable%20of%0Asample-efficient%20decision%20making%2C%20which%20create%20an%20explicit%20model%20of%20the%0Aenvironment.%20This%20model%20can%20subsequently%20be%20used%20for%20learning%20optimal%20policies.%0AIn%20this%20paper%2C%20we%20propose%20a%20temporal%20Gaussian%20Mixture%20Model%20composed%20of%20a%0Aperception%20model%20and%20a%20transition%20model.%20The%20perception%20model%20extracts%20discrete%0A%28latent%29%20states%20from%20continuous%20observations%20using%20a%20variational%20Gaussian%0Amixture%20likelihood.%20Importantly%2C%20our%20model%20constantly%20monitors%20the%20collected%0Adata%20searching%20for%20new%20Gaussian%20components%2C%20i.e.%2C%20the%20perception%20model%20performs%0Aa%20form%20of%20structure%20learning%20%28Smith%20et%20al.%2C%202020%3B%20Friston%20et%20al.%2C%202018%3B%20Neacsu%0Aet%20al.%2C%202022%29%20as%20it%20learns%20the%20number%20of%20Gaussian%20components%20in%20the%20mixture.%0AAdditionally%2C%20the%20transition%20model%20learns%20the%20temporal%20transition%20between%0Aconsecutive%20time%20steps%20by%20taking%20advantage%20of%20the%20Dirichlet-categorical%0Aconjugacy.%20Both%20the%20perception%20and%20transition%20models%20are%20able%20to%20forget%20part%20of%0Athe%20data%20points%2C%20while%20integrating%20the%20information%20they%20provide%20within%20the%0Aprior%2C%20which%20ensure%20fast%20variational%20inference.%20Finally%2C%20decision%20making%20is%0Aperformed%20with%20a%20variant%20of%20Q-learning%20which%20is%20able%20to%20learn%20Q-values%20from%0Abeliefs%20over%20states.%20Empirically%2C%20we%20have%20demonstrated%20the%20model%27s%20ability%20to%0Alearn%20the%20structure%20of%20several%20mazes%3A%20the%20model%20discovered%20the%20number%20of%20states%0Aand%20the%20transition%20probabilities%20between%20these%20states.%20Moreover%2C%20using%20its%0Alearned%20Q-values%2C%20the%20agent%20was%20able%20to%20successfully%20navigate%20from%20the%20starting%0Aposition%20to%20the%20maze%27s%20exit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520learning%2520with%2520Temporal%2520Gaussian%2520Mixture%2520for%2520model-based%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DTh%25C3%25A9ophile%2520Champion%2520and%2520Marek%2520Grze%25C5%259B%2520and%2520Howard%2520Bowman%26entry.1292438233%3D%2520%2520Model-based%2520reinforcement%2520learning%2520refers%2520to%2520a%2520set%2520of%2520approaches%2520capable%2520of%250Asample-efficient%2520decision%2520making%252C%2520which%2520create%2520an%2520explicit%2520model%2520of%2520the%250Aenvironment.%2520This%2520model%2520can%2520subsequently%2520be%2520used%2520for%2520learning%2520optimal%2520policies.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520temporal%2520Gaussian%2520Mixture%2520Model%2520composed%2520of%2520a%250Aperception%2520model%2520and%2520a%2520transition%2520model.%2520The%2520perception%2520model%2520extracts%2520discrete%250A%2528latent%2529%2520states%2520from%2520continuous%2520observations%2520using%2520a%2520variational%2520Gaussian%250Amixture%2520likelihood.%2520Importantly%252C%2520our%2520model%2520constantly%2520monitors%2520the%2520collected%250Adata%2520searching%2520for%2520new%2520Gaussian%2520components%252C%2520i.e.%252C%2520the%2520perception%2520model%2520performs%250Aa%2520form%2520of%2520structure%2520learning%2520%2528Smith%2520et%2520al.%252C%25202020%253B%2520Friston%2520et%2520al.%252C%25202018%253B%2520Neacsu%250Aet%2520al.%252C%25202022%2529%2520as%2520it%2520learns%2520the%2520number%2520of%2520Gaussian%2520components%2520in%2520the%2520mixture.%250AAdditionally%252C%2520the%2520transition%2520model%2520learns%2520the%2520temporal%2520transition%2520between%250Aconsecutive%2520time%2520steps%2520by%2520taking%2520advantage%2520of%2520the%2520Dirichlet-categorical%250Aconjugacy.%2520Both%2520the%2520perception%2520and%2520transition%2520models%2520are%2520able%2520to%2520forget%2520part%2520of%250Athe%2520data%2520points%252C%2520while%2520integrating%2520the%2520information%2520they%2520provide%2520within%2520the%250Aprior%252C%2520which%2520ensure%2520fast%2520variational%2520inference.%2520Finally%252C%2520decision%2520making%2520is%250Aperformed%2520with%2520a%2520variant%2520of%2520Q-learning%2520which%2520is%2520able%2520to%2520learn%2520Q-values%2520from%250Abeliefs%2520over%2520states.%2520Empirically%252C%2520we%2520have%2520demonstrated%2520the%2520model%2527s%2520ability%2520to%250Alearn%2520the%2520structure%2520of%2520several%2520mazes%253A%2520the%2520model%2520discovered%2520the%2520number%2520of%2520states%250Aand%2520the%2520transition%2520probabilities%2520between%2520these%2520states.%2520Moreover%252C%2520using%2520its%250Alearned%2520Q-values%252C%2520the%2520agent%2520was%2520able%2520to%2520successfully%2520navigate%2520from%2520the%2520starting%250Aposition%2520to%2520the%2520maze%2527s%2520exit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20learning%20with%20Temporal%20Gaussian%20Mixture%20for%20model-based%0A%20%20Reinforcement%20Learning&entry.906535625=Th%C3%A9ophile%20Champion%20and%20Marek%20Grze%C5%9B%20and%20Howard%20Bowman&entry.1292438233=%20%20Model-based%20reinforcement%20learning%20refers%20to%20a%20set%20of%20approaches%20capable%20of%0Asample-efficient%20decision%20making%2C%20which%20create%20an%20explicit%20model%20of%20the%0Aenvironment.%20This%20model%20can%20subsequently%20be%20used%20for%20learning%20optimal%20policies.%0AIn%20this%20paper%2C%20we%20propose%20a%20temporal%20Gaussian%20Mixture%20Model%20composed%20of%20a%0Aperception%20model%20and%20a%20transition%20model.%20The%20perception%20model%20extracts%20discrete%0A%28latent%29%20states%20from%20continuous%20observations%20using%20a%20variational%20Gaussian%0Amixture%20likelihood.%20Importantly%2C%20our%20model%20constantly%20monitors%20the%20collected%0Adata%20searching%20for%20new%20Gaussian%20components%2C%20i.e.%2C%20the%20perception%20model%20performs%0Aa%20form%20of%20structure%20learning%20%28Smith%20et%20al.%2C%202020%3B%20Friston%20et%20al.%2C%202018%3B%20Neacsu%0Aet%20al.%2C%202022%29%20as%20it%20learns%20the%20number%20of%20Gaussian%20components%20in%20the%20mixture.%0AAdditionally%2C%20the%20transition%20model%20learns%20the%20temporal%20transition%20between%0Aconsecutive%20time%20steps%20by%20taking%20advantage%20of%20the%20Dirichlet-categorical%0Aconjugacy.%20Both%20the%20perception%20and%20transition%20models%20are%20able%20to%20forget%20part%20of%0Athe%20data%20points%2C%20while%20integrating%20the%20information%20they%20provide%20within%20the%0Aprior%2C%20which%20ensure%20fast%20variational%20inference.%20Finally%2C%20decision%20making%20is%0Aperformed%20with%20a%20variant%20of%20Q-learning%20which%20is%20able%20to%20learn%20Q-values%20from%0Abeliefs%20over%20states.%20Empirically%2C%20we%20have%20demonstrated%20the%20model%27s%20ability%20to%0Alearn%20the%20structure%20of%20several%20mazes%3A%20the%20model%20discovered%20the%20number%20of%20states%0Aand%20the%20transition%20probabilities%20between%20these%20states.%20Moreover%2C%20using%20its%0Alearned%20Q-values%2C%20the%20agent%20was%20able%20to%20successfully%20navigate%20from%20the%20starting%0Aposition%20to%20the%20maze%27s%20exit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11511v1&entry.124074799=Read"},
{"title": "Scalable spectral representations for multi-agent reinforcement learning\n  in network MDPs", "author": "Zhaolin Ren and Runyu Zhang and Bo Dai and Na Li", "abstract": "  Network Markov Decision Processes (MDPs), a popular model for multi-agent\ncontrol, pose a significant challenge to efficient learning due to the\nexponential growth of the global state-action space with the number of agents.\nIn this work, utilizing the exponential decay property of network dynamics, we\nfirst derive scalable spectral local representations for network MDPs, which\ninduces a network linear subspace for the local $Q$-function of each agent.\nBuilding on these local spectral representations, we design a scalable\nalgorithmic framework for continuous state-action network MDPs, and provide\nend-to-end guarantees for the convergence of our algorithm. Empirically, we\nvalidate the effectiveness of our scalable representation-based approach on two\nbenchmark problems, and demonstrate the advantages of our approach over generic\nfunction approximation approaches to representing the local $Q$-functions.\n", "link": "http://arxiv.org/abs/2410.17221v2", "date": "2024-11-18", "relevancy": 2.009, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.508}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20spectral%20representations%20for%20multi-agent%20reinforcement%20learning%0A%20%20in%20network%20MDPs&body=Title%3A%20Scalable%20spectral%20representations%20for%20multi-agent%20reinforcement%20learning%0A%20%20in%20network%20MDPs%0AAuthor%3A%20Zhaolin%20Ren%20and%20Runyu%20Zhang%20and%20Bo%20Dai%20and%20Na%20Li%0AAbstract%3A%20%20%20Network%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20a%20popular%20model%20for%20multi-agent%0Acontrol%2C%20pose%20a%20significant%20challenge%20to%20efficient%20learning%20due%20to%20the%0Aexponential%20growth%20of%20the%20global%20state-action%20space%20with%20the%20number%20of%20agents.%0AIn%20this%20work%2C%20utilizing%20the%20exponential%20decay%20property%20of%20network%20dynamics%2C%20we%0Afirst%20derive%20scalable%20spectral%20local%20representations%20for%20network%20MDPs%2C%20which%0Ainduces%20a%20network%20linear%20subspace%20for%20the%20local%20%24Q%24-function%20of%20each%20agent.%0ABuilding%20on%20these%20local%20spectral%20representations%2C%20we%20design%20a%20scalable%0Aalgorithmic%20framework%20for%20continuous%20state-action%20network%20MDPs%2C%20and%20provide%0Aend-to-end%20guarantees%20for%20the%20convergence%20of%20our%20algorithm.%20Empirically%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20scalable%20representation-based%20approach%20on%20two%0Abenchmark%20problems%2C%20and%20demonstrate%20the%20advantages%20of%20our%20approach%20over%20generic%0Afunction%20approximation%20approaches%20to%20representing%20the%20local%20%24Q%24-functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520spectral%2520representations%2520for%2520multi-agent%2520reinforcement%2520learning%250A%2520%2520in%2520network%2520MDPs%26entry.906535625%3DZhaolin%2520Ren%2520and%2520Runyu%2520Zhang%2520and%2520Bo%2520Dai%2520and%2520Na%2520Li%26entry.1292438233%3D%2520%2520Network%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529%252C%2520a%2520popular%2520model%2520for%2520multi-agent%250Acontrol%252C%2520pose%2520a%2520significant%2520challenge%2520to%2520efficient%2520learning%2520due%2520to%2520the%250Aexponential%2520growth%2520of%2520the%2520global%2520state-action%2520space%2520with%2520the%2520number%2520of%2520agents.%250AIn%2520this%2520work%252C%2520utilizing%2520the%2520exponential%2520decay%2520property%2520of%2520network%2520dynamics%252C%2520we%250Afirst%2520derive%2520scalable%2520spectral%2520local%2520representations%2520for%2520network%2520MDPs%252C%2520which%250Ainduces%2520a%2520network%2520linear%2520subspace%2520for%2520the%2520local%2520%2524Q%2524-function%2520of%2520each%2520agent.%250ABuilding%2520on%2520these%2520local%2520spectral%2520representations%252C%2520we%2520design%2520a%2520scalable%250Aalgorithmic%2520framework%2520for%2520continuous%2520state-action%2520network%2520MDPs%252C%2520and%2520provide%250Aend-to-end%2520guarantees%2520for%2520the%2520convergence%2520of%2520our%2520algorithm.%2520Empirically%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520scalable%2520representation-based%2520approach%2520on%2520two%250Abenchmark%2520problems%252C%2520and%2520demonstrate%2520the%2520advantages%2520of%2520our%2520approach%2520over%2520generic%250Afunction%2520approximation%2520approaches%2520to%2520representing%2520the%2520local%2520%2524Q%2524-functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20spectral%20representations%20for%20multi-agent%20reinforcement%20learning%0A%20%20in%20network%20MDPs&entry.906535625=Zhaolin%20Ren%20and%20Runyu%20Zhang%20and%20Bo%20Dai%20and%20Na%20Li&entry.1292438233=%20%20Network%20Markov%20Decision%20Processes%20%28MDPs%29%2C%20a%20popular%20model%20for%20multi-agent%0Acontrol%2C%20pose%20a%20significant%20challenge%20to%20efficient%20learning%20due%20to%20the%0Aexponential%20growth%20of%20the%20global%20state-action%20space%20with%20the%20number%20of%20agents.%0AIn%20this%20work%2C%20utilizing%20the%20exponential%20decay%20property%20of%20network%20dynamics%2C%20we%0Afirst%20derive%20scalable%20spectral%20local%20representations%20for%20network%20MDPs%2C%20which%0Ainduces%20a%20network%20linear%20subspace%20for%20the%20local%20%24Q%24-function%20of%20each%20agent.%0ABuilding%20on%20these%20local%20spectral%20representations%2C%20we%20design%20a%20scalable%0Aalgorithmic%20framework%20for%20continuous%20state-action%20network%20MDPs%2C%20and%20provide%0Aend-to-end%20guarantees%20for%20the%20convergence%20of%20our%20algorithm.%20Empirically%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20scalable%20representation-based%20approach%20on%20two%0Abenchmark%20problems%2C%20and%20demonstrate%20the%20advantages%20of%20our%20approach%20over%20generic%0Afunction%20approximation%20approaches%20to%20representing%20the%20local%20%24Q%24-functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17221v2&entry.124074799=Read"},
{"title": "Re-examining learning linear functions in context", "author": "Omar Naim and Guilhem Fouilh\u00e9 and Nicholas Asher", "abstract": "  In context learning (ICL) is an attractive method of solving a wide range of\nproblems. Inspired by Garg et al. (2022), we look closely at ICL in a variety\nof train and test settings for several transformer models of different sizes\ntrained from scratch. Our study complements prior work by pointing out several\nsystematic failures of these models to generalize to data not in the training\ndistribution, thereby showing some limitations of ICL. We find that models\nadopt a strategy for this task that is very different from standard solutions.\n", "link": "http://arxiv.org/abs/2411.11465v1", "date": "2024-11-18", "relevancy": 1.9993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-examining%20learning%20linear%20functions%20in%20context&body=Title%3A%20Re-examining%20learning%20linear%20functions%20in%20context%0AAuthor%3A%20Omar%20Naim%20and%20Guilhem%20Fouilh%C3%A9%20and%20Nicholas%20Asher%0AAbstract%3A%20%20%20In%20context%20learning%20%28ICL%29%20is%20an%20attractive%20method%20of%20solving%20a%20wide%20range%20of%0Aproblems.%20Inspired%20by%20Garg%20et%20al.%20%282022%29%2C%20we%20look%20closely%20at%20ICL%20in%20a%20variety%0Aof%20train%20and%20test%20settings%20for%20several%20transformer%20models%20of%20different%20sizes%0Atrained%20from%20scratch.%20Our%20study%20complements%20prior%20work%20by%20pointing%20out%20several%0Asystematic%20failures%20of%20these%20models%20to%20generalize%20to%20data%20not%20in%20the%20training%0Adistribution%2C%20thereby%20showing%20some%20limitations%20of%20ICL.%20We%20find%20that%20models%0Aadopt%20a%20strategy%20for%20this%20task%20that%20is%20very%20different%20from%20standard%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-examining%2520learning%2520linear%2520functions%2520in%2520context%26entry.906535625%3DOmar%2520Naim%2520and%2520Guilhem%2520Fouilh%25C3%25A9%2520and%2520Nicholas%2520Asher%26entry.1292438233%3D%2520%2520In%2520context%2520learning%2520%2528ICL%2529%2520is%2520an%2520attractive%2520method%2520of%2520solving%2520a%2520wide%2520range%2520of%250Aproblems.%2520Inspired%2520by%2520Garg%2520et%2520al.%2520%25282022%2529%252C%2520we%2520look%2520closely%2520at%2520ICL%2520in%2520a%2520variety%250Aof%2520train%2520and%2520test%2520settings%2520for%2520several%2520transformer%2520models%2520of%2520different%2520sizes%250Atrained%2520from%2520scratch.%2520Our%2520study%2520complements%2520prior%2520work%2520by%2520pointing%2520out%2520several%250Asystematic%2520failures%2520of%2520these%2520models%2520to%2520generalize%2520to%2520data%2520not%2520in%2520the%2520training%250Adistribution%252C%2520thereby%2520showing%2520some%2520limitations%2520of%2520ICL.%2520We%2520find%2520that%2520models%250Aadopt%2520a%2520strategy%2520for%2520this%2520task%2520that%2520is%2520very%2520different%2520from%2520standard%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-examining%20learning%20linear%20functions%20in%20context&entry.906535625=Omar%20Naim%20and%20Guilhem%20Fouilh%C3%A9%20and%20Nicholas%20Asher&entry.1292438233=%20%20In%20context%20learning%20%28ICL%29%20is%20an%20attractive%20method%20of%20solving%20a%20wide%20range%20of%0Aproblems.%20Inspired%20by%20Garg%20et%20al.%20%282022%29%2C%20we%20look%20closely%20at%20ICL%20in%20a%20variety%0Aof%20train%20and%20test%20settings%20for%20several%20transformer%20models%20of%20different%20sizes%0Atrained%20from%20scratch.%20Our%20study%20complements%20prior%20work%20by%20pointing%20out%20several%0Asystematic%20failures%20of%20these%20models%20to%20generalize%20to%20data%20not%20in%20the%20training%0Adistribution%2C%20thereby%20showing%20some%20limitations%20of%20ICL.%20We%20find%20that%20models%0Aadopt%20a%20strategy%20for%20this%20task%20that%20is%20very%20different%20from%20standard%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11465v1&entry.124074799=Read"},
{"title": "BONE: a unifying framework for Bayesian online learning in\n  non-stationary environments", "author": "Gerardo Duran-Martin and Leandro S\u00e1nchez-Betancourt and Alexander Y. Shestopaloff and Kevin Murphy", "abstract": "  We propose a unifying framework for methods that perform Bayesian online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for (B)ayesian (O)nline learning in (N)on-stationary (E)nvironments.\nBONE provides a common structure to tackle a variety of problems, including\nonline continual learning, prequential forecasting, and contextual bandits. The\nframework requires specifying three modelling choices: (i) a model for\nmeasurements (e.g., a neural network), (ii) an auxiliary process to model\nnon-stationarity (e.g., the time since the last changepoint), and (iii) a\nconditional prior over model parameters (e.g., a multivariate Gaussian). The\nframework also requires two algorithmic choices, which we use to carry out\napproximate inference under this framework: (i) an algorithm to estimate\nbeliefs (posterior distribution) about the model parameters given the auxiliary\nvariable, and (ii) an algorithm to estimate beliefs about the auxiliary\nvariable. We show how this modularity allows us to write many different\nexisting methods as instances of BONE; we also use this framework to propose a\nnew method. We then experimentally compare existing methods with our proposed\nnew method on several datasets; we provide insights into the situations that\nmake one method more suitable than another for a given task.\n", "link": "http://arxiv.org/abs/2411.10153v2", "date": "2024-11-18", "relevancy": 1.9987, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BONE%3A%20a%20unifying%20framework%20for%20Bayesian%20online%20learning%20in%0A%20%20non-stationary%20environments&body=Title%3A%20BONE%3A%20a%20unifying%20framework%20for%20Bayesian%20online%20learning%20in%0A%20%20non-stationary%20environments%0AAuthor%3A%20Gerardo%20Duran-Martin%20and%20Leandro%20S%C3%A1nchez-Betancourt%20and%20Alexander%20Y.%20Shestopaloff%20and%20Kevin%20Murphy%0AAbstract%3A%20%20%20We%20propose%20a%20unifying%20framework%20for%20methods%20that%20perform%20Bayesian%20online%0Alearning%20in%20non-stationary%20environments.%20We%20call%20the%20framework%20BONE%2C%20which%0Astands%20for%20%28B%29ayesian%20%28O%29nline%20learning%20in%20%28N%29on-stationary%20%28E%29nvironments.%0ABONE%20provides%20a%20common%20structure%20to%20tackle%20a%20variety%20of%20problems%2C%20including%0Aonline%20continual%20learning%2C%20prequential%20forecasting%2C%20and%20contextual%20bandits.%20The%0Aframework%20requires%20specifying%20three%20modelling%20choices%3A%20%28i%29%20a%20model%20for%0Ameasurements%20%28e.g.%2C%20a%20neural%20network%29%2C%20%28ii%29%20an%20auxiliary%20process%20to%20model%0Anon-stationarity%20%28e.g.%2C%20the%20time%20since%20the%20last%20changepoint%29%2C%20and%20%28iii%29%20a%0Aconditional%20prior%20over%20model%20parameters%20%28e.g.%2C%20a%20multivariate%20Gaussian%29.%20The%0Aframework%20also%20requires%20two%20algorithmic%20choices%2C%20which%20we%20use%20to%20carry%20out%0Aapproximate%20inference%20under%20this%20framework%3A%20%28i%29%20an%20algorithm%20to%20estimate%0Abeliefs%20%28posterior%20distribution%29%20about%20the%20model%20parameters%20given%20the%20auxiliary%0Avariable%2C%20and%20%28ii%29%20an%20algorithm%20to%20estimate%20beliefs%20about%20the%20auxiliary%0Avariable.%20We%20show%20how%20this%20modularity%20allows%20us%20to%20write%20many%20different%0Aexisting%20methods%20as%20instances%20of%20BONE%3B%20we%20also%20use%20this%20framework%20to%20propose%20a%0Anew%20method.%20We%20then%20experimentally%20compare%20existing%20methods%20with%20our%20proposed%0Anew%20method%20on%20several%20datasets%3B%20we%20provide%20insights%20into%20the%20situations%20that%0Amake%20one%20method%20more%20suitable%20than%20another%20for%20a%20given%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBONE%253A%2520a%2520unifying%2520framework%2520for%2520Bayesian%2520online%2520learning%2520in%250A%2520%2520non-stationary%2520environments%26entry.906535625%3DGerardo%2520Duran-Martin%2520and%2520Leandro%2520S%25C3%25A1nchez-Betancourt%2520and%2520Alexander%2520Y.%2520Shestopaloff%2520and%2520Kevin%2520Murphy%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520unifying%2520framework%2520for%2520methods%2520that%2520perform%2520Bayesian%2520online%250Alearning%2520in%2520non-stationary%2520environments.%2520We%2520call%2520the%2520framework%2520BONE%252C%2520which%250Astands%2520for%2520%2528B%2529ayesian%2520%2528O%2529nline%2520learning%2520in%2520%2528N%2529on-stationary%2520%2528E%2529nvironments.%250ABONE%2520provides%2520a%2520common%2520structure%2520to%2520tackle%2520a%2520variety%2520of%2520problems%252C%2520including%250Aonline%2520continual%2520learning%252C%2520prequential%2520forecasting%252C%2520and%2520contextual%2520bandits.%2520The%250Aframework%2520requires%2520specifying%2520three%2520modelling%2520choices%253A%2520%2528i%2529%2520a%2520model%2520for%250Ameasurements%2520%2528e.g.%252C%2520a%2520neural%2520network%2529%252C%2520%2528ii%2529%2520an%2520auxiliary%2520process%2520to%2520model%250Anon-stationarity%2520%2528e.g.%252C%2520the%2520time%2520since%2520the%2520last%2520changepoint%2529%252C%2520and%2520%2528iii%2529%2520a%250Aconditional%2520prior%2520over%2520model%2520parameters%2520%2528e.g.%252C%2520a%2520multivariate%2520Gaussian%2529.%2520The%250Aframework%2520also%2520requires%2520two%2520algorithmic%2520choices%252C%2520which%2520we%2520use%2520to%2520carry%2520out%250Aapproximate%2520inference%2520under%2520this%2520framework%253A%2520%2528i%2529%2520an%2520algorithm%2520to%2520estimate%250Abeliefs%2520%2528posterior%2520distribution%2529%2520about%2520the%2520model%2520parameters%2520given%2520the%2520auxiliary%250Avariable%252C%2520and%2520%2528ii%2529%2520an%2520algorithm%2520to%2520estimate%2520beliefs%2520about%2520the%2520auxiliary%250Avariable.%2520We%2520show%2520how%2520this%2520modularity%2520allows%2520us%2520to%2520write%2520many%2520different%250Aexisting%2520methods%2520as%2520instances%2520of%2520BONE%253B%2520we%2520also%2520use%2520this%2520framework%2520to%2520propose%2520a%250Anew%2520method.%2520We%2520then%2520experimentally%2520compare%2520existing%2520methods%2520with%2520our%2520proposed%250Anew%2520method%2520on%2520several%2520datasets%253B%2520we%2520provide%2520insights%2520into%2520the%2520situations%2520that%250Amake%2520one%2520method%2520more%2520suitable%2520than%2520another%2520for%2520a%2520given%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BONE%3A%20a%20unifying%20framework%20for%20Bayesian%20online%20learning%20in%0A%20%20non-stationary%20environments&entry.906535625=Gerardo%20Duran-Martin%20and%20Leandro%20S%C3%A1nchez-Betancourt%20and%20Alexander%20Y.%20Shestopaloff%20and%20Kevin%20Murphy&entry.1292438233=%20%20We%20propose%20a%20unifying%20framework%20for%20methods%20that%20perform%20Bayesian%20online%0Alearning%20in%20non-stationary%20environments.%20We%20call%20the%20framework%20BONE%2C%20which%0Astands%20for%20%28B%29ayesian%20%28O%29nline%20learning%20in%20%28N%29on-stationary%20%28E%29nvironments.%0ABONE%20provides%20a%20common%20structure%20to%20tackle%20a%20variety%20of%20problems%2C%20including%0Aonline%20continual%20learning%2C%20prequential%20forecasting%2C%20and%20contextual%20bandits.%20The%0Aframework%20requires%20specifying%20three%20modelling%20choices%3A%20%28i%29%20a%20model%20for%0Ameasurements%20%28e.g.%2C%20a%20neural%20network%29%2C%20%28ii%29%20an%20auxiliary%20process%20to%20model%0Anon-stationarity%20%28e.g.%2C%20the%20time%20since%20the%20last%20changepoint%29%2C%20and%20%28iii%29%20a%0Aconditional%20prior%20over%20model%20parameters%20%28e.g.%2C%20a%20multivariate%20Gaussian%29.%20The%0Aframework%20also%20requires%20two%20algorithmic%20choices%2C%20which%20we%20use%20to%20carry%20out%0Aapproximate%20inference%20under%20this%20framework%3A%20%28i%29%20an%20algorithm%20to%20estimate%0Abeliefs%20%28posterior%20distribution%29%20about%20the%20model%20parameters%20given%20the%20auxiliary%0Avariable%2C%20and%20%28ii%29%20an%20algorithm%20to%20estimate%20beliefs%20about%20the%20auxiliary%0Avariable.%20We%20show%20how%20this%20modularity%20allows%20us%20to%20write%20many%20different%0Aexisting%20methods%20as%20instances%20of%20BONE%3B%20we%20also%20use%20this%20framework%20to%20propose%20a%0Anew%20method.%20We%20then%20experimentally%20compare%20existing%20methods%20with%20our%20proposed%0Anew%20method%20on%20several%20datasets%3B%20we%20provide%20insights%20into%20the%20situations%20that%0Amake%20one%20method%20more%20suitable%20than%20another%20for%20a%20given%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10153v2&entry.124074799=Read"},
{"title": "Freezing of Gait Detection Using Gramian Angular Fields and Federated\n  Learning from Wearable Sensors", "author": "Shovito Barua Soumma and S M Raihanul Alam and Rudmila Rahman and Umme Niraj Mahi and Sayyed Mostafa Mostafavi and Hassan Ghasemzadeh", "abstract": "  Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease (PD)\nthat impairs mobility and safety. Traditional detection methods face challenges\ndue to intra and inter-patient variability, and most systems are tested in\ncontrolled settings, limiting their real-world applicability. Addressing these\ngaps, we present FOGSense, a novel FOG detection system designed for\nuncontrolled, free-living conditions. It uses Gramian Angular Field (GAF)\ntransformations and federated deep learning to capture temporal and spatial\ngait patterns missed by traditional methods. We evaluated our FOGSense system\nusing a public PD dataset, 'tdcsfog'. FOGSense improves accuracy by 10.4% over\na single-axis accelerometer, reduces failure points compared to multi-sensor\nsystems, and demonstrates robustness to missing values. The federated\narchitecture allows personalized model adaptation and efficient smartphone\nsynchronization during off-peak hours, making it effective for long-term\nmonitoring as symptoms evolve. Overall, FOGSense achieves a 22.2% improvement\nin F1-score compared to state-of-the-art methods, along with enhanced\nsensitivity for FOG episode detection. Code is available:\nhttps://github.com/shovito66/FOGSense.\n", "link": "http://arxiv.org/abs/2411.11764v1", "date": "2024-11-18", "relevancy": 1.9984, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4926}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freezing%20of%20Gait%20Detection%20Using%20Gramian%20Angular%20Fields%20and%20Federated%0A%20%20Learning%20from%20Wearable%20Sensors&body=Title%3A%20Freezing%20of%20Gait%20Detection%20Using%20Gramian%20Angular%20Fields%20and%20Federated%0A%20%20Learning%20from%20Wearable%20Sensors%0AAuthor%3A%20Shovito%20Barua%20Soumma%20and%20S%20M%20Raihanul%20Alam%20and%20Rudmila%20Rahman%20and%20Umme%20Niraj%20Mahi%20and%20Sayyed%20Mostafa%20Mostafavi%20and%20Hassan%20Ghasemzadeh%0AAbstract%3A%20%20%20Freezing%20of%20gait%20%28FOG%29%20is%20a%20debilitating%20symptom%20of%20Parkinson%27s%20disease%20%28PD%29%0Athat%20impairs%20mobility%20and%20safety.%20Traditional%20detection%20methods%20face%20challenges%0Adue%20to%20intra%20and%20inter-patient%20variability%2C%20and%20most%20systems%20are%20tested%20in%0Acontrolled%20settings%2C%20limiting%20their%20real-world%20applicability.%20Addressing%20these%0Agaps%2C%20we%20present%20FOGSense%2C%20a%20novel%20FOG%20detection%20system%20designed%20for%0Auncontrolled%2C%20free-living%20conditions.%20It%20uses%20Gramian%20Angular%20Field%20%28GAF%29%0Atransformations%20and%20federated%20deep%20learning%20to%20capture%20temporal%20and%20spatial%0Agait%20patterns%20missed%20by%20traditional%20methods.%20We%20evaluated%20our%20FOGSense%20system%0Ausing%20a%20public%20PD%20dataset%2C%20%27tdcsfog%27.%20FOGSense%20improves%20accuracy%20by%2010.4%25%20over%0Aa%20single-axis%20accelerometer%2C%20reduces%20failure%20points%20compared%20to%20multi-sensor%0Asystems%2C%20and%20demonstrates%20robustness%20to%20missing%20values.%20The%20federated%0Aarchitecture%20allows%20personalized%20model%20adaptation%20and%20efficient%20smartphone%0Asynchronization%20during%20off-peak%20hours%2C%20making%20it%20effective%20for%20long-term%0Amonitoring%20as%20symptoms%20evolve.%20Overall%2C%20FOGSense%20achieves%20a%2022.2%25%20improvement%0Ain%20F1-score%20compared%20to%20state-of-the-art%20methods%2C%20along%20with%20enhanced%0Asensitivity%20for%20FOG%20episode%20detection.%20Code%20is%20available%3A%0Ahttps%3A//github.com/shovito66/FOGSense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreezing%2520of%2520Gait%2520Detection%2520Using%2520Gramian%2520Angular%2520Fields%2520and%2520Federated%250A%2520%2520Learning%2520from%2520Wearable%2520Sensors%26entry.906535625%3DShovito%2520Barua%2520Soumma%2520and%2520S%2520M%2520Raihanul%2520Alam%2520and%2520Rudmila%2520Rahman%2520and%2520Umme%2520Niraj%2520Mahi%2520and%2520Sayyed%2520Mostafa%2520Mostafavi%2520and%2520Hassan%2520Ghasemzadeh%26entry.1292438233%3D%2520%2520Freezing%2520of%2520gait%2520%2528FOG%2529%2520is%2520a%2520debilitating%2520symptom%2520of%2520Parkinson%2527s%2520disease%2520%2528PD%2529%250Athat%2520impairs%2520mobility%2520and%2520safety.%2520Traditional%2520detection%2520methods%2520face%2520challenges%250Adue%2520to%2520intra%2520and%2520inter-patient%2520variability%252C%2520and%2520most%2520systems%2520are%2520tested%2520in%250Acontrolled%2520settings%252C%2520limiting%2520their%2520real-world%2520applicability.%2520Addressing%2520these%250Agaps%252C%2520we%2520present%2520FOGSense%252C%2520a%2520novel%2520FOG%2520detection%2520system%2520designed%2520for%250Auncontrolled%252C%2520free-living%2520conditions.%2520It%2520uses%2520Gramian%2520Angular%2520Field%2520%2528GAF%2529%250Atransformations%2520and%2520federated%2520deep%2520learning%2520to%2520capture%2520temporal%2520and%2520spatial%250Agait%2520patterns%2520missed%2520by%2520traditional%2520methods.%2520We%2520evaluated%2520our%2520FOGSense%2520system%250Ausing%2520a%2520public%2520PD%2520dataset%252C%2520%2527tdcsfog%2527.%2520FOGSense%2520improves%2520accuracy%2520by%252010.4%2525%2520over%250Aa%2520single-axis%2520accelerometer%252C%2520reduces%2520failure%2520points%2520compared%2520to%2520multi-sensor%250Asystems%252C%2520and%2520demonstrates%2520robustness%2520to%2520missing%2520values.%2520The%2520federated%250Aarchitecture%2520allows%2520personalized%2520model%2520adaptation%2520and%2520efficient%2520smartphone%250Asynchronization%2520during%2520off-peak%2520hours%252C%2520making%2520it%2520effective%2520for%2520long-term%250Amonitoring%2520as%2520symptoms%2520evolve.%2520Overall%252C%2520FOGSense%2520achieves%2520a%252022.2%2525%2520improvement%250Ain%2520F1-score%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520along%2520with%2520enhanced%250Asensitivity%2520for%2520FOG%2520episode%2520detection.%2520Code%2520is%2520available%253A%250Ahttps%253A//github.com/shovito66/FOGSense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freezing%20of%20Gait%20Detection%20Using%20Gramian%20Angular%20Fields%20and%20Federated%0A%20%20Learning%20from%20Wearable%20Sensors&entry.906535625=Shovito%20Barua%20Soumma%20and%20S%20M%20Raihanul%20Alam%20and%20Rudmila%20Rahman%20and%20Umme%20Niraj%20Mahi%20and%20Sayyed%20Mostafa%20Mostafavi%20and%20Hassan%20Ghasemzadeh&entry.1292438233=%20%20Freezing%20of%20gait%20%28FOG%29%20is%20a%20debilitating%20symptom%20of%20Parkinson%27s%20disease%20%28PD%29%0Athat%20impairs%20mobility%20and%20safety.%20Traditional%20detection%20methods%20face%20challenges%0Adue%20to%20intra%20and%20inter-patient%20variability%2C%20and%20most%20systems%20are%20tested%20in%0Acontrolled%20settings%2C%20limiting%20their%20real-world%20applicability.%20Addressing%20these%0Agaps%2C%20we%20present%20FOGSense%2C%20a%20novel%20FOG%20detection%20system%20designed%20for%0Auncontrolled%2C%20free-living%20conditions.%20It%20uses%20Gramian%20Angular%20Field%20%28GAF%29%0Atransformations%20and%20federated%20deep%20learning%20to%20capture%20temporal%20and%20spatial%0Agait%20patterns%20missed%20by%20traditional%20methods.%20We%20evaluated%20our%20FOGSense%20system%0Ausing%20a%20public%20PD%20dataset%2C%20%27tdcsfog%27.%20FOGSense%20improves%20accuracy%20by%2010.4%25%20over%0Aa%20single-axis%20accelerometer%2C%20reduces%20failure%20points%20compared%20to%20multi-sensor%0Asystems%2C%20and%20demonstrates%20robustness%20to%20missing%20values.%20The%20federated%0Aarchitecture%20allows%20personalized%20model%20adaptation%20and%20efficient%20smartphone%0Asynchronization%20during%20off-peak%20hours%2C%20making%20it%20effective%20for%20long-term%0Amonitoring%20as%20symptoms%20evolve.%20Overall%2C%20FOGSense%20achieves%20a%2022.2%25%20improvement%0Ain%20F1-score%20compared%20to%20state-of-the-art%20methods%2C%20along%20with%20enhanced%0Asensitivity%20for%20FOG%20episode%20detection.%20Code%20is%20available%3A%0Ahttps%3A//github.com/shovito66/FOGSense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11764v1&entry.124074799=Read"},
{"title": "TSINR: Capturing Temporal Continuity via Implicit Neural Representations\n  for Time Series Anomaly Detection", "author": "Mengxuan Li and Ke Liu and Hongyang Chen and Jiajun Bu and Hongwei Wang and Haishuai Wang", "abstract": "  Time series anomaly detection aims to identify unusual patterns in data or\ndeviations from systems' expected behavior. The reconstruction-based methods\nare the mainstream in this task, which learn point-wise representation via\nunsupervised learning. However, the unlabeled anomaly points in training data\nmay cause these reconstruction-based methods to learn and reconstruct anomalous\ndata, resulting in the challenge of capturing normal patterns. In this paper,\nwe propose a time series anomaly detection method based on implicit neural\nrepresentation (INR) reconstruction, named TSINR, to address this challenge.\nDue to the property of spectral bias, TSINR enables prioritizing low-frequency\nsignals and exhibiting poorer performance on high-frequency abnormal data.\nSpecifically, we adopt INR to parameterize time series data as a continuous\nfunction and employ a transformer-based architecture to predict the INR of\ngiven data. As a result, the proposed TSINR method achieves the advantage of\ncapturing the temporal continuity and thus is more sensitive to discontinuous\nanomaly data. In addition, we further design a novel form of INR continuous\nfunction to learn inter- and intra-channel information, and leverage a\npre-trained large language model to amplify the intense fluctuations in\nanomalies. Extensive experiments demonstrate that TSINR achieves superior\noverall performance on both univariate and multivariate time series anomaly\ndetection benchmarks compared to other state-of-the-art reconstruction-based\nmethods. Our codes are available.\n", "link": "http://arxiv.org/abs/2411.11641v1", "date": "2024-11-18", "relevancy": 1.9924, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5301}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4795}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSINR%3A%20Capturing%20Temporal%20Continuity%20via%20Implicit%20Neural%20Representations%0A%20%20for%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20TSINR%3A%20Capturing%20Temporal%20Continuity%20via%20Implicit%20Neural%20Representations%0A%20%20for%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Mengxuan%20Li%20and%20Ke%20Liu%20and%20Hongyang%20Chen%20and%20Jiajun%20Bu%20and%20Hongwei%20Wang%20and%20Haishuai%20Wang%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20aims%20to%20identify%20unusual%20patterns%20in%20data%20or%0Adeviations%20from%20systems%27%20expected%20behavior.%20The%20reconstruction-based%20methods%0Aare%20the%20mainstream%20in%20this%20task%2C%20which%20learn%20point-wise%20representation%20via%0Aunsupervised%20learning.%20However%2C%20the%20unlabeled%20anomaly%20points%20in%20training%20data%0Amay%20cause%20these%20reconstruction-based%20methods%20to%20learn%20and%20reconstruct%20anomalous%0Adata%2C%20resulting%20in%20the%20challenge%20of%20capturing%20normal%20patterns.%20In%20this%20paper%2C%0Awe%20propose%20a%20time%20series%20anomaly%20detection%20method%20based%20on%20implicit%20neural%0Arepresentation%20%28INR%29%20reconstruction%2C%20named%20TSINR%2C%20to%20address%20this%20challenge.%0ADue%20to%20the%20property%20of%20spectral%20bias%2C%20TSINR%20enables%20prioritizing%20low-frequency%0Asignals%20and%20exhibiting%20poorer%20performance%20on%20high-frequency%20abnormal%20data.%0ASpecifically%2C%20we%20adopt%20INR%20to%20parameterize%20time%20series%20data%20as%20a%20continuous%0Afunction%20and%20employ%20a%20transformer-based%20architecture%20to%20predict%20the%20INR%20of%0Agiven%20data.%20As%20a%20result%2C%20the%20proposed%20TSINR%20method%20achieves%20the%20advantage%20of%0Acapturing%20the%20temporal%20continuity%20and%20thus%20is%20more%20sensitive%20to%20discontinuous%0Aanomaly%20data.%20In%20addition%2C%20we%20further%20design%20a%20novel%20form%20of%20INR%20continuous%0Afunction%20to%20learn%20inter-%20and%20intra-channel%20information%2C%20and%20leverage%20a%0Apre-trained%20large%20language%20model%20to%20amplify%20the%20intense%20fluctuations%20in%0Aanomalies.%20Extensive%20experiments%20demonstrate%20that%20TSINR%20achieves%20superior%0Aoverall%20performance%20on%20both%20univariate%20and%20multivariate%20time%20series%20anomaly%0Adetection%20benchmarks%20compared%20to%20other%20state-of-the-art%20reconstruction-based%0Amethods.%20Our%20codes%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSINR%253A%2520Capturing%2520Temporal%2520Continuity%2520via%2520Implicit%2520Neural%2520Representations%250A%2520%2520for%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DMengxuan%2520Li%2520and%2520Ke%2520Liu%2520and%2520Hongyang%2520Chen%2520and%2520Jiajun%2520Bu%2520and%2520Hongwei%2520Wang%2520and%2520Haishuai%2520Wang%26entry.1292438233%3D%2520%2520Time%2520series%2520anomaly%2520detection%2520aims%2520to%2520identify%2520unusual%2520patterns%2520in%2520data%2520or%250Adeviations%2520from%2520systems%2527%2520expected%2520behavior.%2520The%2520reconstruction-based%2520methods%250Aare%2520the%2520mainstream%2520in%2520this%2520task%252C%2520which%2520learn%2520point-wise%2520representation%2520via%250Aunsupervised%2520learning.%2520However%252C%2520the%2520unlabeled%2520anomaly%2520points%2520in%2520training%2520data%250Amay%2520cause%2520these%2520reconstruction-based%2520methods%2520to%2520learn%2520and%2520reconstruct%2520anomalous%250Adata%252C%2520resulting%2520in%2520the%2520challenge%2520of%2520capturing%2520normal%2520patterns.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520time%2520series%2520anomaly%2520detection%2520method%2520based%2520on%2520implicit%2520neural%250Arepresentation%2520%2528INR%2529%2520reconstruction%252C%2520named%2520TSINR%252C%2520to%2520address%2520this%2520challenge.%250ADue%2520to%2520the%2520property%2520of%2520spectral%2520bias%252C%2520TSINR%2520enables%2520prioritizing%2520low-frequency%250Asignals%2520and%2520exhibiting%2520poorer%2520performance%2520on%2520high-frequency%2520abnormal%2520data.%250ASpecifically%252C%2520we%2520adopt%2520INR%2520to%2520parameterize%2520time%2520series%2520data%2520as%2520a%2520continuous%250Afunction%2520and%2520employ%2520a%2520transformer-based%2520architecture%2520to%2520predict%2520the%2520INR%2520of%250Agiven%2520data.%2520As%2520a%2520result%252C%2520the%2520proposed%2520TSINR%2520method%2520achieves%2520the%2520advantage%2520of%250Acapturing%2520the%2520temporal%2520continuity%2520and%2520thus%2520is%2520more%2520sensitive%2520to%2520discontinuous%250Aanomaly%2520data.%2520In%2520addition%252C%2520we%2520further%2520design%2520a%2520novel%2520form%2520of%2520INR%2520continuous%250Afunction%2520to%2520learn%2520inter-%2520and%2520intra-channel%2520information%252C%2520and%2520leverage%2520a%250Apre-trained%2520large%2520language%2520model%2520to%2520amplify%2520the%2520intense%2520fluctuations%2520in%250Aanomalies.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TSINR%2520achieves%2520superior%250Aoverall%2520performance%2520on%2520both%2520univariate%2520and%2520multivariate%2520time%2520series%2520anomaly%250Adetection%2520benchmarks%2520compared%2520to%2520other%2520state-of-the-art%2520reconstruction-based%250Amethods.%2520Our%2520codes%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSINR%3A%20Capturing%20Temporal%20Continuity%20via%20Implicit%20Neural%20Representations%0A%20%20for%20Time%20Series%20Anomaly%20Detection&entry.906535625=Mengxuan%20Li%20and%20Ke%20Liu%20and%20Hongyang%20Chen%20and%20Jiajun%20Bu%20and%20Hongwei%20Wang%20and%20Haishuai%20Wang&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20aims%20to%20identify%20unusual%20patterns%20in%20data%20or%0Adeviations%20from%20systems%27%20expected%20behavior.%20The%20reconstruction-based%20methods%0Aare%20the%20mainstream%20in%20this%20task%2C%20which%20learn%20point-wise%20representation%20via%0Aunsupervised%20learning.%20However%2C%20the%20unlabeled%20anomaly%20points%20in%20training%20data%0Amay%20cause%20these%20reconstruction-based%20methods%20to%20learn%20and%20reconstruct%20anomalous%0Adata%2C%20resulting%20in%20the%20challenge%20of%20capturing%20normal%20patterns.%20In%20this%20paper%2C%0Awe%20propose%20a%20time%20series%20anomaly%20detection%20method%20based%20on%20implicit%20neural%0Arepresentation%20%28INR%29%20reconstruction%2C%20named%20TSINR%2C%20to%20address%20this%20challenge.%0ADue%20to%20the%20property%20of%20spectral%20bias%2C%20TSINR%20enables%20prioritizing%20low-frequency%0Asignals%20and%20exhibiting%20poorer%20performance%20on%20high-frequency%20abnormal%20data.%0ASpecifically%2C%20we%20adopt%20INR%20to%20parameterize%20time%20series%20data%20as%20a%20continuous%0Afunction%20and%20employ%20a%20transformer-based%20architecture%20to%20predict%20the%20INR%20of%0Agiven%20data.%20As%20a%20result%2C%20the%20proposed%20TSINR%20method%20achieves%20the%20advantage%20of%0Acapturing%20the%20temporal%20continuity%20and%20thus%20is%20more%20sensitive%20to%20discontinuous%0Aanomaly%20data.%20In%20addition%2C%20we%20further%20design%20a%20novel%20form%20of%20INR%20continuous%0Afunction%20to%20learn%20inter-%20and%20intra-channel%20information%2C%20and%20leverage%20a%0Apre-trained%20large%20language%20model%20to%20amplify%20the%20intense%20fluctuations%20in%0Aanomalies.%20Extensive%20experiments%20demonstrate%20that%20TSINR%20achieves%20superior%0Aoverall%20performance%20on%20both%20univariate%20and%20multivariate%20time%20series%20anomaly%0Adetection%20benchmarks%20compared%20to%20other%20state-of-the-art%20reconstruction-based%0Amethods.%20Our%20codes%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11641v1&entry.124074799=Read"},
{"title": "Upside-Down Reinforcement Learning for More Interpretable Optimal\n  Control", "author": "Juan Cardenas-Cartagena and Massimiliano Falzari and Marco Zullich and Matthia Sabatelli", "abstract": "  Model-Free Reinforcement Learning (RL) algorithms either learn how to map\nstates to expected rewards or search for policies that can maximize a certain\nperformance function. Model-Based algorithms instead, aim to learn an\napproximation of the underlying model of the RL environment and then use it in\ncombination with planning algorithms. Upside-Down Reinforcement Learning (UDRL)\nis a novel learning paradigm that aims to learn how to predict actions from\nstates and desired commands. This task is formulated as a Supervised Learning\nproblem and has successfully been tackled by Neural Networks (NNs). In this\npaper, we investigate whether function approximation algorithms other than NNs\ncan also be used within a UDRL framework. Our experiments, performed over\nseveral popular optimal control benchmarks, show that tree-based methods like\nRandom Forests and Extremely Randomized Trees can perform just as well as NNs\nwith the significant benefit of resulting in policies that are inherently more\ninterpretable than NNs, therefore paving the way for more transparent, safe,\nand robust RL.\n", "link": "http://arxiv.org/abs/2411.11457v1", "date": "2024-11-18", "relevancy": 1.991, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upside-Down%20Reinforcement%20Learning%20for%20More%20Interpretable%20Optimal%0A%20%20Control&body=Title%3A%20Upside-Down%20Reinforcement%20Learning%20for%20More%20Interpretable%20Optimal%0A%20%20Control%0AAuthor%3A%20Juan%20Cardenas-Cartagena%20and%20Massimiliano%20Falzari%20and%20Marco%20Zullich%20and%20Matthia%20Sabatelli%0AAbstract%3A%20%20%20Model-Free%20Reinforcement%20Learning%20%28RL%29%20algorithms%20either%20learn%20how%20to%20map%0Astates%20to%20expected%20rewards%20or%20search%20for%20policies%20that%20can%20maximize%20a%20certain%0Aperformance%20function.%20Model-Based%20algorithms%20instead%2C%20aim%20to%20learn%20an%0Aapproximation%20of%20the%20underlying%20model%20of%20the%20RL%20environment%20and%20then%20use%20it%20in%0Acombination%20with%20planning%20algorithms.%20Upside-Down%20Reinforcement%20Learning%20%28UDRL%29%0Ais%20a%20novel%20learning%20paradigm%20that%20aims%20to%20learn%20how%20to%20predict%20actions%20from%0Astates%20and%20desired%20commands.%20This%20task%20is%20formulated%20as%20a%20Supervised%20Learning%0Aproblem%20and%20has%20successfully%20been%20tackled%20by%20Neural%20Networks%20%28NNs%29.%20In%20this%0Apaper%2C%20we%20investigate%20whether%20function%20approximation%20algorithms%20other%20than%20NNs%0Acan%20also%20be%20used%20within%20a%20UDRL%20framework.%20Our%20experiments%2C%20performed%20over%0Aseveral%20popular%20optimal%20control%20benchmarks%2C%20show%20that%20tree-based%20methods%20like%0ARandom%20Forests%20and%20Extremely%20Randomized%20Trees%20can%20perform%20just%20as%20well%20as%20NNs%0Awith%20the%20significant%20benefit%20of%20resulting%20in%20policies%20that%20are%20inherently%20more%0Ainterpretable%20than%20NNs%2C%20therefore%20paving%20the%20way%20for%20more%20transparent%2C%20safe%2C%0Aand%20robust%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpside-Down%2520Reinforcement%2520Learning%2520for%2520More%2520Interpretable%2520Optimal%250A%2520%2520Control%26entry.906535625%3DJuan%2520Cardenas-Cartagena%2520and%2520Massimiliano%2520Falzari%2520and%2520Marco%2520Zullich%2520and%2520Matthia%2520Sabatelli%26entry.1292438233%3D%2520%2520Model-Free%2520Reinforcement%2520Learning%2520%2528RL%2529%2520algorithms%2520either%2520learn%2520how%2520to%2520map%250Astates%2520to%2520expected%2520rewards%2520or%2520search%2520for%2520policies%2520that%2520can%2520maximize%2520a%2520certain%250Aperformance%2520function.%2520Model-Based%2520algorithms%2520instead%252C%2520aim%2520to%2520learn%2520an%250Aapproximation%2520of%2520the%2520underlying%2520model%2520of%2520the%2520RL%2520environment%2520and%2520then%2520use%2520it%2520in%250Acombination%2520with%2520planning%2520algorithms.%2520Upside-Down%2520Reinforcement%2520Learning%2520%2528UDRL%2529%250Ais%2520a%2520novel%2520learning%2520paradigm%2520that%2520aims%2520to%2520learn%2520how%2520to%2520predict%2520actions%2520from%250Astates%2520and%2520desired%2520commands.%2520This%2520task%2520is%2520formulated%2520as%2520a%2520Supervised%2520Learning%250Aproblem%2520and%2520has%2520successfully%2520been%2520tackled%2520by%2520Neural%2520Networks%2520%2528NNs%2529.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520whether%2520function%2520approximation%2520algorithms%2520other%2520than%2520NNs%250Acan%2520also%2520be%2520used%2520within%2520a%2520UDRL%2520framework.%2520Our%2520experiments%252C%2520performed%2520over%250Aseveral%2520popular%2520optimal%2520control%2520benchmarks%252C%2520show%2520that%2520tree-based%2520methods%2520like%250ARandom%2520Forests%2520and%2520Extremely%2520Randomized%2520Trees%2520can%2520perform%2520just%2520as%2520well%2520as%2520NNs%250Awith%2520the%2520significant%2520benefit%2520of%2520resulting%2520in%2520policies%2520that%2520are%2520inherently%2520more%250Ainterpretable%2520than%2520NNs%252C%2520therefore%2520paving%2520the%2520way%2520for%2520more%2520transparent%252C%2520safe%252C%250Aand%2520robust%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upside-Down%20Reinforcement%20Learning%20for%20More%20Interpretable%20Optimal%0A%20%20Control&entry.906535625=Juan%20Cardenas-Cartagena%20and%20Massimiliano%20Falzari%20and%20Marco%20Zullich%20and%20Matthia%20Sabatelli&entry.1292438233=%20%20Model-Free%20Reinforcement%20Learning%20%28RL%29%20algorithms%20either%20learn%20how%20to%20map%0Astates%20to%20expected%20rewards%20or%20search%20for%20policies%20that%20can%20maximize%20a%20certain%0Aperformance%20function.%20Model-Based%20algorithms%20instead%2C%20aim%20to%20learn%20an%0Aapproximation%20of%20the%20underlying%20model%20of%20the%20RL%20environment%20and%20then%20use%20it%20in%0Acombination%20with%20planning%20algorithms.%20Upside-Down%20Reinforcement%20Learning%20%28UDRL%29%0Ais%20a%20novel%20learning%20paradigm%20that%20aims%20to%20learn%20how%20to%20predict%20actions%20from%0Astates%20and%20desired%20commands.%20This%20task%20is%20formulated%20as%20a%20Supervised%20Learning%0Aproblem%20and%20has%20successfully%20been%20tackled%20by%20Neural%20Networks%20%28NNs%29.%20In%20this%0Apaper%2C%20we%20investigate%20whether%20function%20approximation%20algorithms%20other%20than%20NNs%0Acan%20also%20be%20used%20within%20a%20UDRL%20framework.%20Our%20experiments%2C%20performed%20over%0Aseveral%20popular%20optimal%20control%20benchmarks%2C%20show%20that%20tree-based%20methods%20like%0ARandom%20Forests%20and%20Extremely%20Randomized%20Trees%20can%20perform%20just%20as%20well%20as%20NNs%0Awith%20the%20significant%20benefit%20of%20resulting%20in%20policies%20that%20are%20inherently%20more%0Ainterpretable%20than%20NNs%2C%20therefore%20paving%20the%20way%20for%20more%20transparent%2C%20safe%2C%0Aand%20robust%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11457v1&entry.124074799=Read"},
{"title": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding", "author": "Ukyo Honda and Tatsushi Oka and Peinan Zhang and Masato Mita", "abstract": "  Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption.\n", "link": "http://arxiv.org/abs/2406.12060v3", "date": "2024-11-18", "relevancy": 1.9872, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.51}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4982}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20Eliminate%20but%20Aggregate%3A%20Post-Hoc%20Control%20over%20Mixture-of-Experts%20to%0A%20%20Address%20Shortcut%20Shifts%20in%20Natural%20Language%20Understanding&body=Title%3A%20Not%20Eliminate%20but%20Aggregate%3A%20Post-Hoc%20Control%20over%20Mixture-of-Experts%20to%0A%20%20Address%20Shortcut%20Shifts%20in%20Natural%20Language%20Understanding%0AAuthor%3A%20Ukyo%20Honda%20and%20Tatsushi%20Oka%20and%20Peinan%20Zhang%20and%20Masato%20Mita%0AAbstract%3A%20%20%20Recent%20models%20for%20natural%20language%20understanding%20are%20inclined%20to%20exploit%0Asimple%20patterns%20in%20datasets%2C%20commonly%20known%20as%20shortcuts.%20These%20shortcuts%20hinge%0Aon%20spurious%20correlations%20between%20labels%20and%20latent%20features%20existing%20in%20the%0Atraining%20data.%20At%20inference%20time%2C%20shortcut-dependent%20models%20are%20likely%20to%0Agenerate%20erroneous%20predictions%20under%20distribution%20shifts%2C%20particularly%20when%0Asome%20latent%20features%20are%20no%20longer%20correlated%20with%20the%20labels.%20To%20avoid%20this%2C%0Aprevious%20studies%20have%20trained%20models%20to%20eliminate%20the%20reliance%20on%20shortcuts.%20In%0Athis%20study%2C%20we%20explore%20a%20different%20direction%3A%20pessimistically%20aggregating%20the%0Apredictions%20of%20a%20mixture-of-experts%2C%20assuming%20each%20expert%20captures%20relatively%0Adifferent%20latent%20features.%20The%20experimental%20results%20demonstrate%20that%20our%0Apost-hoc%20control%20over%20the%20experts%20significantly%20enhances%20the%20model%27s%20robustness%0Ato%20the%20distribution%20shift%20in%20shortcuts.%20Besides%2C%20we%20show%20that%20our%20approach%20has%0Asome%20practical%20advantages.%20We%20also%20analyze%20our%20model%20and%20provide%20results%20to%0Asupport%20the%20assumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12060v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520Eliminate%2520but%2520Aggregate%253A%2520Post-Hoc%2520Control%2520over%2520Mixture-of-Experts%2520to%250A%2520%2520Address%2520Shortcut%2520Shifts%2520in%2520Natural%2520Language%2520Understanding%26entry.906535625%3DUkyo%2520Honda%2520and%2520Tatsushi%2520Oka%2520and%2520Peinan%2520Zhang%2520and%2520Masato%2520Mita%26entry.1292438233%3D%2520%2520Recent%2520models%2520for%2520natural%2520language%2520understanding%2520are%2520inclined%2520to%2520exploit%250Asimple%2520patterns%2520in%2520datasets%252C%2520commonly%2520known%2520as%2520shortcuts.%2520These%2520shortcuts%2520hinge%250Aon%2520spurious%2520correlations%2520between%2520labels%2520and%2520latent%2520features%2520existing%2520in%2520the%250Atraining%2520data.%2520At%2520inference%2520time%252C%2520shortcut-dependent%2520models%2520are%2520likely%2520to%250Agenerate%2520erroneous%2520predictions%2520under%2520distribution%2520shifts%252C%2520particularly%2520when%250Asome%2520latent%2520features%2520are%2520no%2520longer%2520correlated%2520with%2520the%2520labels.%2520To%2520avoid%2520this%252C%250Aprevious%2520studies%2520have%2520trained%2520models%2520to%2520eliminate%2520the%2520reliance%2520on%2520shortcuts.%2520In%250Athis%2520study%252C%2520we%2520explore%2520a%2520different%2520direction%253A%2520pessimistically%2520aggregating%2520the%250Apredictions%2520of%2520a%2520mixture-of-experts%252C%2520assuming%2520each%2520expert%2520captures%2520relatively%250Adifferent%2520latent%2520features.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Apost-hoc%2520control%2520over%2520the%2520experts%2520significantly%2520enhances%2520the%2520model%2527s%2520robustness%250Ato%2520the%2520distribution%2520shift%2520in%2520shortcuts.%2520Besides%252C%2520we%2520show%2520that%2520our%2520approach%2520has%250Asome%2520practical%2520advantages.%2520We%2520also%2520analyze%2520our%2520model%2520and%2520provide%2520results%2520to%250Asupport%2520the%2520assumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12060v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20Eliminate%20but%20Aggregate%3A%20Post-Hoc%20Control%20over%20Mixture-of-Experts%20to%0A%20%20Address%20Shortcut%20Shifts%20in%20Natural%20Language%20Understanding&entry.906535625=Ukyo%20Honda%20and%20Tatsushi%20Oka%20and%20Peinan%20Zhang%20and%20Masato%20Mita&entry.1292438233=%20%20Recent%20models%20for%20natural%20language%20understanding%20are%20inclined%20to%20exploit%0Asimple%20patterns%20in%20datasets%2C%20commonly%20known%20as%20shortcuts.%20These%20shortcuts%20hinge%0Aon%20spurious%20correlations%20between%20labels%20and%20latent%20features%20existing%20in%20the%0Atraining%20data.%20At%20inference%20time%2C%20shortcut-dependent%20models%20are%20likely%20to%0Agenerate%20erroneous%20predictions%20under%20distribution%20shifts%2C%20particularly%20when%0Asome%20latent%20features%20are%20no%20longer%20correlated%20with%20the%20labels.%20To%20avoid%20this%2C%0Aprevious%20studies%20have%20trained%20models%20to%20eliminate%20the%20reliance%20on%20shortcuts.%20In%0Athis%20study%2C%20we%20explore%20a%20different%20direction%3A%20pessimistically%20aggregating%20the%0Apredictions%20of%20a%20mixture-of-experts%2C%20assuming%20each%20expert%20captures%20relatively%0Adifferent%20latent%20features.%20The%20experimental%20results%20demonstrate%20that%20our%0Apost-hoc%20control%20over%20the%20experts%20significantly%20enhances%20the%20model%27s%20robustness%0Ato%20the%20distribution%20shift%20in%20shortcuts.%20Besides%2C%20we%20show%20that%20our%20approach%20has%0Asome%20practical%20advantages.%20We%20also%20analyze%20our%20model%20and%20provide%20results%20to%0Asupport%20the%20assumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12060v3&entry.124074799=Read"},
{"title": "Efficient and Robust Continual Graph Learning for Graph Classification\n  in Biology", "author": "Ding Zhang and Jane Downer and Can Chen and Ren Wang", "abstract": "  Graph classification is essential for understanding complex biological\nsystems, where molecular structures and interactions are naturally represented\nas graphs. Traditional graph neural networks (GNNs) perform well on static\ntasks but struggle in dynamic settings due to catastrophic forgetting. We\npresent Perturbed and Sparsified Continual Graph Learning (PSCGL), a robust and\nefficient continual graph learning framework for graph data classification,\nspecifically targeting biological datasets. We introduce a perturbed sampling\nstrategy to identify critical data points that contribute to model learning and\na motif-based graph sparsification technique to reduce storage needs while\nmaintaining performance. Additionally, our PSCGL framework inherently defends\nagainst graph backdoor attacks, which is crucial for applications in sensitive\nbiological contexts. Extensive experiments on biological datasets demonstrate\nthat PSCGL not only retains knowledge across tasks but also enhances the\nefficiency and robustness of graph classification models in biology.\n", "link": "http://arxiv.org/abs/2411.11668v1", "date": "2024-11-18", "relevancy": 1.981, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5193}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4781}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Robust%20Continual%20Graph%20Learning%20for%20Graph%20Classification%0A%20%20in%20Biology&body=Title%3A%20Efficient%20and%20Robust%20Continual%20Graph%20Learning%20for%20Graph%20Classification%0A%20%20in%20Biology%0AAuthor%3A%20Ding%20Zhang%20and%20Jane%20Downer%20and%20Can%20Chen%20and%20Ren%20Wang%0AAbstract%3A%20%20%20Graph%20classification%20is%20essential%20for%20understanding%20complex%20biological%0Asystems%2C%20where%20molecular%20structures%20and%20interactions%20are%20naturally%20represented%0Aas%20graphs.%20Traditional%20graph%20neural%20networks%20%28GNNs%29%20perform%20well%20on%20static%0Atasks%20but%20struggle%20in%20dynamic%20settings%20due%20to%20catastrophic%20forgetting.%20We%0Apresent%20Perturbed%20and%20Sparsified%20Continual%20Graph%20Learning%20%28PSCGL%29%2C%20a%20robust%20and%0Aefficient%20continual%20graph%20learning%20framework%20for%20graph%20data%20classification%2C%0Aspecifically%20targeting%20biological%20datasets.%20We%20introduce%20a%20perturbed%20sampling%0Astrategy%20to%20identify%20critical%20data%20points%20that%20contribute%20to%20model%20learning%20and%0Aa%20motif-based%20graph%20sparsification%20technique%20to%20reduce%20storage%20needs%20while%0Amaintaining%20performance.%20Additionally%2C%20our%20PSCGL%20framework%20inherently%20defends%0Aagainst%20graph%20backdoor%20attacks%2C%20which%20is%20crucial%20for%20applications%20in%20sensitive%0Abiological%20contexts.%20Extensive%20experiments%20on%20biological%20datasets%20demonstrate%0Athat%20PSCGL%20not%20only%20retains%20knowledge%20across%20tasks%20but%20also%20enhances%20the%0Aefficiency%20and%20robustness%20of%20graph%20classification%20models%20in%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Robust%2520Continual%2520Graph%2520Learning%2520for%2520Graph%2520Classification%250A%2520%2520in%2520Biology%26entry.906535625%3DDing%2520Zhang%2520and%2520Jane%2520Downer%2520and%2520Can%2520Chen%2520and%2520Ren%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520classification%2520is%2520essential%2520for%2520understanding%2520complex%2520biological%250Asystems%252C%2520where%2520molecular%2520structures%2520and%2520interactions%2520are%2520naturally%2520represented%250Aas%2520graphs.%2520Traditional%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520perform%2520well%2520on%2520static%250Atasks%2520but%2520struggle%2520in%2520dynamic%2520settings%2520due%2520to%2520catastrophic%2520forgetting.%2520We%250Apresent%2520Perturbed%2520and%2520Sparsified%2520Continual%2520Graph%2520Learning%2520%2528PSCGL%2529%252C%2520a%2520robust%2520and%250Aefficient%2520continual%2520graph%2520learning%2520framework%2520for%2520graph%2520data%2520classification%252C%250Aspecifically%2520targeting%2520biological%2520datasets.%2520We%2520introduce%2520a%2520perturbed%2520sampling%250Astrategy%2520to%2520identify%2520critical%2520data%2520points%2520that%2520contribute%2520to%2520model%2520learning%2520and%250Aa%2520motif-based%2520graph%2520sparsification%2520technique%2520to%2520reduce%2520storage%2520needs%2520while%250Amaintaining%2520performance.%2520Additionally%252C%2520our%2520PSCGL%2520framework%2520inherently%2520defends%250Aagainst%2520graph%2520backdoor%2520attacks%252C%2520which%2520is%2520crucial%2520for%2520applications%2520in%2520sensitive%250Abiological%2520contexts.%2520Extensive%2520experiments%2520on%2520biological%2520datasets%2520demonstrate%250Athat%2520PSCGL%2520not%2520only%2520retains%2520knowledge%2520across%2520tasks%2520but%2520also%2520enhances%2520the%250Aefficiency%2520and%2520robustness%2520of%2520graph%2520classification%2520models%2520in%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Robust%20Continual%20Graph%20Learning%20for%20Graph%20Classification%0A%20%20in%20Biology&entry.906535625=Ding%20Zhang%20and%20Jane%20Downer%20and%20Can%20Chen%20and%20Ren%20Wang&entry.1292438233=%20%20Graph%20classification%20is%20essential%20for%20understanding%20complex%20biological%0Asystems%2C%20where%20molecular%20structures%20and%20interactions%20are%20naturally%20represented%0Aas%20graphs.%20Traditional%20graph%20neural%20networks%20%28GNNs%29%20perform%20well%20on%20static%0Atasks%20but%20struggle%20in%20dynamic%20settings%20due%20to%20catastrophic%20forgetting.%20We%0Apresent%20Perturbed%20and%20Sparsified%20Continual%20Graph%20Learning%20%28PSCGL%29%2C%20a%20robust%20and%0Aefficient%20continual%20graph%20learning%20framework%20for%20graph%20data%20classification%2C%0Aspecifically%20targeting%20biological%20datasets.%20We%20introduce%20a%20perturbed%20sampling%0Astrategy%20to%20identify%20critical%20data%20points%20that%20contribute%20to%20model%20learning%20and%0Aa%20motif-based%20graph%20sparsification%20technique%20to%20reduce%20storage%20needs%20while%0Amaintaining%20performance.%20Additionally%2C%20our%20PSCGL%20framework%20inherently%20defends%0Aagainst%20graph%20backdoor%20attacks%2C%20which%20is%20crucial%20for%20applications%20in%20sensitive%0Abiological%20contexts.%20Extensive%20experiments%20on%20biological%20datasets%20demonstrate%0Athat%20PSCGL%20not%20only%20retains%20knowledge%20across%20tasks%20but%20also%20enhances%20the%0Aefficiency%20and%20robustness%20of%20graph%20classification%20models%20in%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11668v1&entry.124074799=Read"},
{"title": "Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search", "author": "Jinhao Jiang and Zhipeng Chen and Yingqian Min and Jie Chen and Xiaoxue Cheng and Jiapeng Wang and Yiru Tang and Haoxiang Sun and Jia Deng and Wayne Xin Zhao and Zheng Liu and Dong Yan and Jian Xie and Zhongyuan Wang and Ji-Rong Wen", "abstract": "  Recently, test-time scaling has garnered significant attention from the\nresearch community, largely due to the substantial advancements of the o1 model\nreleased by OpenAI. By allocating more computational resources during the\ninference phase, large language models~(LLMs) can extensively explore the\nsolution space by generating more thought tokens or diverse solutions, thereby\nproducing more accurate responses. However, developing an o1-like reasoning\napproach is challenging, and researchers have been making various attempts to\nadvance this open area of research. In this paper, we present a preliminary\nexploration into enhancing the reasoning abilities of LLMs through\nreward-guided tree search algorithms. This framework is implemented by\nintegrating the policy model, reward model, and search algorithm. It is\nprimarily constructed around a tree search algorithm, where the policy model\nnavigates a dynamically expanding tree guided by a specially trained reward\nmodel. We thoroughly explore various design considerations necessary for\nimplementing this framework and provide a detailed report of the technical\naspects. To assess the effectiveness of our approach, we focus on mathematical\nreasoning tasks and conduct extensive evaluations on four challenging datasets,\nsignificantly enhancing the reasoning abilities of LLMs.\n", "link": "http://arxiv.org/abs/2411.11694v1", "date": "2024-11-18", "relevancy": 1.9669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%3A%20Enhancing%20LLM%20Reasoning%20with%20Reward-guided%20Tree%20Search&body=Title%3A%20Technical%20Report%3A%20Enhancing%20LLM%20Reasoning%20with%20Reward-guided%20Tree%20Search%0AAuthor%3A%20Jinhao%20Jiang%20and%20Zhipeng%20Chen%20and%20Yingqian%20Min%20and%20Jie%20Chen%20and%20Xiaoxue%20Cheng%20and%20Jiapeng%20Wang%20and%20Yiru%20Tang%20and%20Haoxiang%20Sun%20and%20Jia%20Deng%20and%20Wayne%20Xin%20Zhao%20and%20Zheng%20Liu%20and%20Dong%20Yan%20and%20Jian%20Xie%20and%20Zhongyuan%20Wang%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Recently%2C%20test-time%20scaling%20has%20garnered%20significant%20attention%20from%20the%0Aresearch%20community%2C%20largely%20due%20to%20the%20substantial%20advancements%20of%20the%20o1%20model%0Areleased%20by%20OpenAI.%20By%20allocating%20more%20computational%20resources%20during%20the%0Ainference%20phase%2C%20large%20language%20models~%28LLMs%29%20can%20extensively%20explore%20the%0Asolution%20space%20by%20generating%20more%20thought%20tokens%20or%20diverse%20solutions%2C%20thereby%0Aproducing%20more%20accurate%20responses.%20However%2C%20developing%20an%20o1-like%20reasoning%0Aapproach%20is%20challenging%2C%20and%20researchers%20have%20been%20making%20various%20attempts%20to%0Aadvance%20this%20open%20area%20of%20research.%20In%20this%20paper%2C%20we%20present%20a%20preliminary%0Aexploration%20into%20enhancing%20the%20reasoning%20abilities%20of%20LLMs%20through%0Areward-guided%20tree%20search%20algorithms.%20This%20framework%20is%20implemented%20by%0Aintegrating%20the%20policy%20model%2C%20reward%20model%2C%20and%20search%20algorithm.%20It%20is%0Aprimarily%20constructed%20around%20a%20tree%20search%20algorithm%2C%20where%20the%20policy%20model%0Anavigates%20a%20dynamically%20expanding%20tree%20guided%20by%20a%20specially%20trained%20reward%0Amodel.%20We%20thoroughly%20explore%20various%20design%20considerations%20necessary%20for%0Aimplementing%20this%20framework%20and%20provide%20a%20detailed%20report%20of%20the%20technical%0Aaspects.%20To%20assess%20the%20effectiveness%20of%20our%20approach%2C%20we%20focus%20on%20mathematical%0Areasoning%20tasks%20and%20conduct%20extensive%20evaluations%20on%20four%20challenging%20datasets%2C%0Asignificantly%20enhancing%20the%20reasoning%20abilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%253A%2520Enhancing%2520LLM%2520Reasoning%2520with%2520Reward-guided%2520Tree%2520Search%26entry.906535625%3DJinhao%2520Jiang%2520and%2520Zhipeng%2520Chen%2520and%2520Yingqian%2520Min%2520and%2520Jie%2520Chen%2520and%2520Xiaoxue%2520Cheng%2520and%2520Jiapeng%2520Wang%2520and%2520Yiru%2520Tang%2520and%2520Haoxiang%2520Sun%2520and%2520Jia%2520Deng%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Zheng%2520Liu%2520and%2520Dong%2520Yan%2520and%2520Jian%2520Xie%2520and%2520Zhongyuan%2520Wang%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Recently%252C%2520test-time%2520scaling%2520has%2520garnered%2520significant%2520attention%2520from%2520the%250Aresearch%2520community%252C%2520largely%2520due%2520to%2520the%2520substantial%2520advancements%2520of%2520the%2520o1%2520model%250Areleased%2520by%2520OpenAI.%2520By%2520allocating%2520more%2520computational%2520resources%2520during%2520the%250Ainference%2520phase%252C%2520large%2520language%2520models~%2528LLMs%2529%2520can%2520extensively%2520explore%2520the%250Asolution%2520space%2520by%2520generating%2520more%2520thought%2520tokens%2520or%2520diverse%2520solutions%252C%2520thereby%250Aproducing%2520more%2520accurate%2520responses.%2520However%252C%2520developing%2520an%2520o1-like%2520reasoning%250Aapproach%2520is%2520challenging%252C%2520and%2520researchers%2520have%2520been%2520making%2520various%2520attempts%2520to%250Aadvance%2520this%2520open%2520area%2520of%2520research.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520preliminary%250Aexploration%2520into%2520enhancing%2520the%2520reasoning%2520abilities%2520of%2520LLMs%2520through%250Areward-guided%2520tree%2520search%2520algorithms.%2520This%2520framework%2520is%2520implemented%2520by%250Aintegrating%2520the%2520policy%2520model%252C%2520reward%2520model%252C%2520and%2520search%2520algorithm.%2520It%2520is%250Aprimarily%2520constructed%2520around%2520a%2520tree%2520search%2520algorithm%252C%2520where%2520the%2520policy%2520model%250Anavigates%2520a%2520dynamically%2520expanding%2520tree%2520guided%2520by%2520a%2520specially%2520trained%2520reward%250Amodel.%2520We%2520thoroughly%2520explore%2520various%2520design%2520considerations%2520necessary%2520for%250Aimplementing%2520this%2520framework%2520and%2520provide%2520a%2520detailed%2520report%2520of%2520the%2520technical%250Aaspects.%2520To%2520assess%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%2520focus%2520on%2520mathematical%250Areasoning%2520tasks%2520and%2520conduct%2520extensive%2520evaluations%2520on%2520four%2520challenging%2520datasets%252C%250Asignificantly%2520enhancing%2520the%2520reasoning%2520abilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%3A%20Enhancing%20LLM%20Reasoning%20with%20Reward-guided%20Tree%20Search&entry.906535625=Jinhao%20Jiang%20and%20Zhipeng%20Chen%20and%20Yingqian%20Min%20and%20Jie%20Chen%20and%20Xiaoxue%20Cheng%20and%20Jiapeng%20Wang%20and%20Yiru%20Tang%20and%20Haoxiang%20Sun%20and%20Jia%20Deng%20and%20Wayne%20Xin%20Zhao%20and%20Zheng%20Liu%20and%20Dong%20Yan%20and%20Jian%20Xie%20and%20Zhongyuan%20Wang%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Recently%2C%20test-time%20scaling%20has%20garnered%20significant%20attention%20from%20the%0Aresearch%20community%2C%20largely%20due%20to%20the%20substantial%20advancements%20of%20the%20o1%20model%0Areleased%20by%20OpenAI.%20By%20allocating%20more%20computational%20resources%20during%20the%0Ainference%20phase%2C%20large%20language%20models~%28LLMs%29%20can%20extensively%20explore%20the%0Asolution%20space%20by%20generating%20more%20thought%20tokens%20or%20diverse%20solutions%2C%20thereby%0Aproducing%20more%20accurate%20responses.%20However%2C%20developing%20an%20o1-like%20reasoning%0Aapproach%20is%20challenging%2C%20and%20researchers%20have%20been%20making%20various%20attempts%20to%0Aadvance%20this%20open%20area%20of%20research.%20In%20this%20paper%2C%20we%20present%20a%20preliminary%0Aexploration%20into%20enhancing%20the%20reasoning%20abilities%20of%20LLMs%20through%0Areward-guided%20tree%20search%20algorithms.%20This%20framework%20is%20implemented%20by%0Aintegrating%20the%20policy%20model%2C%20reward%20model%2C%20and%20search%20algorithm.%20It%20is%0Aprimarily%20constructed%20around%20a%20tree%20search%20algorithm%2C%20where%20the%20policy%20model%0Anavigates%20a%20dynamically%20expanding%20tree%20guided%20by%20a%20specially%20trained%20reward%0Amodel.%20We%20thoroughly%20explore%20various%20design%20considerations%20necessary%20for%0Aimplementing%20this%20framework%20and%20provide%20a%20detailed%20report%20of%20the%20technical%0Aaspects.%20To%20assess%20the%20effectiveness%20of%20our%20approach%2C%20we%20focus%20on%20mathematical%0Areasoning%20tasks%20and%20conduct%20extensive%20evaluations%20on%20four%20challenging%20datasets%2C%0Asignificantly%20enhancing%20the%20reasoning%20abilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11694v1&entry.124074799=Read"},
{"title": "Characterizing stable regions in the residual stream of LLMs", "author": "Jett Janiak and Jacek Karwowski and Chatrik Singh Mangat and Giorgi Giglemiani and Nora Petrova and Stefan Heimersheim", "abstract": "  We identify stable regions in the residual stream of Transformers, where the\nmodel's output remains insensitive to small activation changes, but exhibits\nhigh sensitivity at region boundaries. These regions emerge during training and\nbecome more defined as training progresses or model size increases. The regions\nappear to be much larger than previously studied polytopes. Our analysis\nsuggests that these stable regions align with semantic distinctions, where\nsimilar prompts cluster within regions, and activations from the same region\nlead to similar next token predictions. This work provides a promising research\ndirection for understanding the complexity of neural networks, shedding light\non training dynamics, and advancing interpretability.\n", "link": "http://arxiv.org/abs/2409.17113v4", "date": "2024-11-18", "relevancy": 1.9652, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20stable%20regions%20in%20the%20residual%20stream%20of%20LLMs&body=Title%3A%20Characterizing%20stable%20regions%20in%20the%20residual%20stream%20of%20LLMs%0AAuthor%3A%20Jett%20Janiak%20and%20Jacek%20Karwowski%20and%20Chatrik%20Singh%20Mangat%20and%20Giorgi%20Giglemiani%20and%20Nora%20Petrova%20and%20Stefan%20Heimersheim%0AAbstract%3A%20%20%20We%20identify%20stable%20regions%20in%20the%20residual%20stream%20of%20Transformers%2C%20where%20the%0Amodel%27s%20output%20remains%20insensitive%20to%20small%20activation%20changes%2C%20but%20exhibits%0Ahigh%20sensitivity%20at%20region%20boundaries.%20These%20regions%20emerge%20during%20training%20and%0Abecome%20more%20defined%20as%20training%20progresses%20or%20model%20size%20increases.%20The%20regions%0Aappear%20to%20be%20much%20larger%20than%20previously%20studied%20polytopes.%20Our%20analysis%0Asuggests%20that%20these%20stable%20regions%20align%20with%20semantic%20distinctions%2C%20where%0Asimilar%20prompts%20cluster%20within%20regions%2C%20and%20activations%20from%20the%20same%20region%0Alead%20to%20similar%20next%20token%20predictions.%20This%20work%20provides%20a%20promising%20research%0Adirection%20for%20understanding%20the%20complexity%20of%20neural%20networks%2C%20shedding%20light%0Aon%20training%20dynamics%2C%20and%20advancing%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17113v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520stable%2520regions%2520in%2520the%2520residual%2520stream%2520of%2520LLMs%26entry.906535625%3DJett%2520Janiak%2520and%2520Jacek%2520Karwowski%2520and%2520Chatrik%2520Singh%2520Mangat%2520and%2520Giorgi%2520Giglemiani%2520and%2520Nora%2520Petrova%2520and%2520Stefan%2520Heimersheim%26entry.1292438233%3D%2520%2520We%2520identify%2520stable%2520regions%2520in%2520the%2520residual%2520stream%2520of%2520Transformers%252C%2520where%2520the%250Amodel%2527s%2520output%2520remains%2520insensitive%2520to%2520small%2520activation%2520changes%252C%2520but%2520exhibits%250Ahigh%2520sensitivity%2520at%2520region%2520boundaries.%2520These%2520regions%2520emerge%2520during%2520training%2520and%250Abecome%2520more%2520defined%2520as%2520training%2520progresses%2520or%2520model%2520size%2520increases.%2520The%2520regions%250Aappear%2520to%2520be%2520much%2520larger%2520than%2520previously%2520studied%2520polytopes.%2520Our%2520analysis%250Asuggests%2520that%2520these%2520stable%2520regions%2520align%2520with%2520semantic%2520distinctions%252C%2520where%250Asimilar%2520prompts%2520cluster%2520within%2520regions%252C%2520and%2520activations%2520from%2520the%2520same%2520region%250Alead%2520to%2520similar%2520next%2520token%2520predictions.%2520This%2520work%2520provides%2520a%2520promising%2520research%250Adirection%2520for%2520understanding%2520the%2520complexity%2520of%2520neural%2520networks%252C%2520shedding%2520light%250Aon%2520training%2520dynamics%252C%2520and%2520advancing%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17113v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20stable%20regions%20in%20the%20residual%20stream%20of%20LLMs&entry.906535625=Jett%20Janiak%20and%20Jacek%20Karwowski%20and%20Chatrik%20Singh%20Mangat%20and%20Giorgi%20Giglemiani%20and%20Nora%20Petrova%20and%20Stefan%20Heimersheim&entry.1292438233=%20%20We%20identify%20stable%20regions%20in%20the%20residual%20stream%20of%20Transformers%2C%20where%20the%0Amodel%27s%20output%20remains%20insensitive%20to%20small%20activation%20changes%2C%20but%20exhibits%0Ahigh%20sensitivity%20at%20region%20boundaries.%20These%20regions%20emerge%20during%20training%20and%0Abecome%20more%20defined%20as%20training%20progresses%20or%20model%20size%20increases.%20The%20regions%0Aappear%20to%20be%20much%20larger%20than%20previously%20studied%20polytopes.%20Our%20analysis%0Asuggests%20that%20these%20stable%20regions%20align%20with%20semantic%20distinctions%2C%20where%0Asimilar%20prompts%20cluster%20within%20regions%2C%20and%20activations%20from%20the%20same%20region%0Alead%20to%20similar%20next%20token%20predictions.%20This%20work%20provides%20a%20promising%20research%0Adirection%20for%20understanding%20the%20complexity%20of%20neural%20networks%2C%20shedding%20light%0Aon%20training%20dynamics%2C%20and%20advancing%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17113v4&entry.124074799=Read"},
{"title": "DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the\n  Generalised $h$-transform", "author": "Alexander Denker and Francisco Vargas and Shreyas Padhy and Kieran Didi and Simon Mathis and Vincent Dutordoir and Riccardo Barbano and Emile Mathieu and Urszula Julia Komorowska and Pietro Lio", "abstract": "  Generative modelling paradigms based on denoising diffusion processes have\nemerged as a leading candidate for conditional sampling in inverse problems. In\nmany real-world applications, we often have access to large, expensively\ntrained unconditional diffusion models, which we aim to exploit for improving\nconditional sampling. Most recent approaches are motivated heuristically and\nlack a unifying framework, obscuring connections between them. Further, they\noften suffer from issues such as being very sensitive to hyperparameters, being\nexpensive to train or needing access to weights hidden behind a closed API. In\nthis work, we unify conditional training and sampling using the mathematically\nwell-understood Doob's h-transform. This new perspective allows us to unify\nmany existing methods under a common umbrella. Under this framework, we propose\nDEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional\ngeneration that simply fine-tunes a very small network to quickly learn the\nconditional $h$-transform, while keeping the larger unconditional network\nunchanged. DEFT is much faster than existing baselines while achieving\nstate-of-the-art performance across a variety of linear and non-linear\nbenchmarks. On image reconstruction tasks, we achieve speedups of up to\n1.6$\\times$, while having the best perceptual quality on natural images and\nreconstruction performance on medical images. Further, we also provide initial\nexperiments on protein motif scaffolding and outperform reconstruction guidance\nmethods.\n", "link": "http://arxiv.org/abs/2406.01781v3", "date": "2024-11-18", "relevancy": 1.9573, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6536}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEFT%3A%20Efficient%20Fine-Tuning%20of%20Diffusion%20Models%20by%20Learning%20the%0A%20%20Generalised%20%24h%24-transform&body=Title%3A%20DEFT%3A%20Efficient%20Fine-Tuning%20of%20Diffusion%20Models%20by%20Learning%20the%0A%20%20Generalised%20%24h%24-transform%0AAuthor%3A%20Alexander%20Denker%20and%20Francisco%20Vargas%20and%20Shreyas%20Padhy%20and%20Kieran%20Didi%20and%20Simon%20Mathis%20and%20Vincent%20Dutordoir%20and%20Riccardo%20Barbano%20and%20Emile%20Mathieu%20and%20Urszula%20Julia%20Komorowska%20and%20Pietro%20Lio%0AAbstract%3A%20%20%20Generative%20modelling%20paradigms%20based%20on%20denoising%20diffusion%20processes%20have%0Aemerged%20as%20a%20leading%20candidate%20for%20conditional%20sampling%20in%20inverse%20problems.%20In%0Amany%20real-world%20applications%2C%20we%20often%20have%20access%20to%20large%2C%20expensively%0Atrained%20unconditional%20diffusion%20models%2C%20which%20we%20aim%20to%20exploit%20for%20improving%0Aconditional%20sampling.%20Most%20recent%20approaches%20are%20motivated%20heuristically%20and%0Alack%20a%20unifying%20framework%2C%20obscuring%20connections%20between%20them.%20Further%2C%20they%0Aoften%20suffer%20from%20issues%20such%20as%20being%20very%20sensitive%20to%20hyperparameters%2C%20being%0Aexpensive%20to%20train%20or%20needing%20access%20to%20weights%20hidden%20behind%20a%20closed%20API.%20In%0Athis%20work%2C%20we%20unify%20conditional%20training%20and%20sampling%20using%20the%20mathematically%0Awell-understood%20Doob%27s%20h-transform.%20This%20new%20perspective%20allows%20us%20to%20unify%0Amany%20existing%20methods%20under%20a%20common%20umbrella.%20Under%20this%20framework%2C%20we%20propose%0ADEFT%20%28Doob%27s%20h-transform%20Efficient%20FineTuning%29%2C%20a%20new%20approach%20for%20conditional%0Ageneration%20that%20simply%20fine-tunes%20a%20very%20small%20network%20to%20quickly%20learn%20the%0Aconditional%20%24h%24-transform%2C%20while%20keeping%20the%20larger%20unconditional%20network%0Aunchanged.%20DEFT%20is%20much%20faster%20than%20existing%20baselines%20while%20achieving%0Astate-of-the-art%20performance%20across%20a%20variety%20of%20linear%20and%20non-linear%0Abenchmarks.%20On%20image%20reconstruction%20tasks%2C%20we%20achieve%20speedups%20of%20up%20to%0A1.6%24%5Ctimes%24%2C%20while%20having%20the%20best%20perceptual%20quality%20on%20natural%20images%20and%0Areconstruction%20performance%20on%20medical%20images.%20Further%2C%20we%20also%20provide%20initial%0Aexperiments%20on%20protein%20motif%20scaffolding%20and%20outperform%20reconstruction%20guidance%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEFT%253A%2520Efficient%2520Fine-Tuning%2520of%2520Diffusion%2520Models%2520by%2520Learning%2520the%250A%2520%2520Generalised%2520%2524h%2524-transform%26entry.906535625%3DAlexander%2520Denker%2520and%2520Francisco%2520Vargas%2520and%2520Shreyas%2520Padhy%2520and%2520Kieran%2520Didi%2520and%2520Simon%2520Mathis%2520and%2520Vincent%2520Dutordoir%2520and%2520Riccardo%2520Barbano%2520and%2520Emile%2520Mathieu%2520and%2520Urszula%2520Julia%2520Komorowska%2520and%2520Pietro%2520Lio%26entry.1292438233%3D%2520%2520Generative%2520modelling%2520paradigms%2520based%2520on%2520denoising%2520diffusion%2520processes%2520have%250Aemerged%2520as%2520a%2520leading%2520candidate%2520for%2520conditional%2520sampling%2520in%2520inverse%2520problems.%2520In%250Amany%2520real-world%2520applications%252C%2520we%2520often%2520have%2520access%2520to%2520large%252C%2520expensively%250Atrained%2520unconditional%2520diffusion%2520models%252C%2520which%2520we%2520aim%2520to%2520exploit%2520for%2520improving%250Aconditional%2520sampling.%2520Most%2520recent%2520approaches%2520are%2520motivated%2520heuristically%2520and%250Alack%2520a%2520unifying%2520framework%252C%2520obscuring%2520connections%2520between%2520them.%2520Further%252C%2520they%250Aoften%2520suffer%2520from%2520issues%2520such%2520as%2520being%2520very%2520sensitive%2520to%2520hyperparameters%252C%2520being%250Aexpensive%2520to%2520train%2520or%2520needing%2520access%2520to%2520weights%2520hidden%2520behind%2520a%2520closed%2520API.%2520In%250Athis%2520work%252C%2520we%2520unify%2520conditional%2520training%2520and%2520sampling%2520using%2520the%2520mathematically%250Awell-understood%2520Doob%2527s%2520h-transform.%2520This%2520new%2520perspective%2520allows%2520us%2520to%2520unify%250Amany%2520existing%2520methods%2520under%2520a%2520common%2520umbrella.%2520Under%2520this%2520framework%252C%2520we%2520propose%250ADEFT%2520%2528Doob%2527s%2520h-transform%2520Efficient%2520FineTuning%2529%252C%2520a%2520new%2520approach%2520for%2520conditional%250Ageneration%2520that%2520simply%2520fine-tunes%2520a%2520very%2520small%2520network%2520to%2520quickly%2520learn%2520the%250Aconditional%2520%2524h%2524-transform%252C%2520while%2520keeping%2520the%2520larger%2520unconditional%2520network%250Aunchanged.%2520DEFT%2520is%2520much%2520faster%2520than%2520existing%2520baselines%2520while%2520achieving%250Astate-of-the-art%2520performance%2520across%2520a%2520variety%2520of%2520linear%2520and%2520non-linear%250Abenchmarks.%2520On%2520image%2520reconstruction%2520tasks%252C%2520we%2520achieve%2520speedups%2520of%2520up%2520to%250A1.6%2524%255Ctimes%2524%252C%2520while%2520having%2520the%2520best%2520perceptual%2520quality%2520on%2520natural%2520images%2520and%250Areconstruction%2520performance%2520on%2520medical%2520images.%2520Further%252C%2520we%2520also%2520provide%2520initial%250Aexperiments%2520on%2520protein%2520motif%2520scaffolding%2520and%2520outperform%2520reconstruction%2520guidance%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEFT%3A%20Efficient%20Fine-Tuning%20of%20Diffusion%20Models%20by%20Learning%20the%0A%20%20Generalised%20%24h%24-transform&entry.906535625=Alexander%20Denker%20and%20Francisco%20Vargas%20and%20Shreyas%20Padhy%20and%20Kieran%20Didi%20and%20Simon%20Mathis%20and%20Vincent%20Dutordoir%20and%20Riccardo%20Barbano%20and%20Emile%20Mathieu%20and%20Urszula%20Julia%20Komorowska%20and%20Pietro%20Lio&entry.1292438233=%20%20Generative%20modelling%20paradigms%20based%20on%20denoising%20diffusion%20processes%20have%0Aemerged%20as%20a%20leading%20candidate%20for%20conditional%20sampling%20in%20inverse%20problems.%20In%0Amany%20real-world%20applications%2C%20we%20often%20have%20access%20to%20large%2C%20expensively%0Atrained%20unconditional%20diffusion%20models%2C%20which%20we%20aim%20to%20exploit%20for%20improving%0Aconditional%20sampling.%20Most%20recent%20approaches%20are%20motivated%20heuristically%20and%0Alack%20a%20unifying%20framework%2C%20obscuring%20connections%20between%20them.%20Further%2C%20they%0Aoften%20suffer%20from%20issues%20such%20as%20being%20very%20sensitive%20to%20hyperparameters%2C%20being%0Aexpensive%20to%20train%20or%20needing%20access%20to%20weights%20hidden%20behind%20a%20closed%20API.%20In%0Athis%20work%2C%20we%20unify%20conditional%20training%20and%20sampling%20using%20the%20mathematically%0Awell-understood%20Doob%27s%20h-transform.%20This%20new%20perspective%20allows%20us%20to%20unify%0Amany%20existing%20methods%20under%20a%20common%20umbrella.%20Under%20this%20framework%2C%20we%20propose%0ADEFT%20%28Doob%27s%20h-transform%20Efficient%20FineTuning%29%2C%20a%20new%20approach%20for%20conditional%0Ageneration%20that%20simply%20fine-tunes%20a%20very%20small%20network%20to%20quickly%20learn%20the%0Aconditional%20%24h%24-transform%2C%20while%20keeping%20the%20larger%20unconditional%20network%0Aunchanged.%20DEFT%20is%20much%20faster%20than%20existing%20baselines%20while%20achieving%0Astate-of-the-art%20performance%20across%20a%20variety%20of%20linear%20and%20non-linear%0Abenchmarks.%20On%20image%20reconstruction%20tasks%2C%20we%20achieve%20speedups%20of%20up%20to%0A1.6%24%5Ctimes%24%2C%20while%20having%20the%20best%20perceptual%20quality%20on%20natural%20images%20and%0Areconstruction%20performance%20on%20medical%20images.%20Further%2C%20we%20also%20provide%20initial%0Aexperiments%20on%20protein%20motif%20scaffolding%20and%20outperform%20reconstruction%20guidance%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01781v3&entry.124074799=Read"},
{"title": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for molecule optimization", "author": "Khiem Le and Nitesh V. Chawla", "abstract": "  Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing them shows limited performance. In this work, we facilitate utilizing\nLLMs in an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to guide the LLM to refine the modified molecule. We\nconduct experiments across both single- and multi-property objectives with 2\nthresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly,\nfor 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95%\nand 20.76% under loose (\\texttt{l}) and strict (\\texttt{s}) thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.\n", "link": "http://arxiv.org/abs/2410.13147v6", "date": "2024-11-18", "relevancy": 1.9547, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Large%20Language%20Models%20in%20an%20iterative%20paradigm%20with%20domain%0A%20%20feedback%20for%20molecule%20optimization&body=Title%3A%20Utilizing%20Large%20Language%20Models%20in%20an%20iterative%20paradigm%20with%20domain%0A%20%20feedback%20for%20molecule%20optimization%0AAuthor%3A%20Khiem%20Le%20and%20Nitesh%20V.%20Chawla%0AAbstract%3A%20%20%20Molecule%20optimization%20is%20a%20critical%20task%20in%20drug%20discovery%20to%20optimize%0Adesired%20properties%20of%20a%20given%20molecule%20through%20chemical%20modification.%20Despite%0ALarge%20Language%20Models%20%28LLMs%29%20holding%20the%20potential%20to%20efficiently%20simulate%20this%0Atask%20by%20using%20natural%20language%20to%20direct%20the%20optimization%2C%20straightforwardly%0Autilizing%20them%20shows%20limited%20performance.%20In%20this%20work%2C%20we%20facilitate%20utilizing%0ALLMs%20in%20an%20iterative%20paradigm%20by%20proposing%20a%20simple%20yet%20highly%20effective%20domain%0Afeedback%20provider%2C%20namely%20%24%5Ctext%7BRe%7D%5E3%24DF.%20In%20detail%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20harnesses%0Aan%20external%20toolkit%2C%20RDKit%2C%20to%20handle%20the%20molecule%20hallucination%2C%20if%20the%0Amodified%20molecule%20is%20chemically%20invalid.%20Otherwise%2C%20its%20desired%20properties%20are%0Acomputed%20and%20compared%20to%20the%20original%20one%2C%20establishing%20reliable%20domain%0Afeedback%20with%20correct%20direction%20and%20distance%20towards%20the%20objective%2C%20followed%20by%0Aa%20retrieved%20example%2C%20to%20guide%20the%20LLM%20to%20refine%20the%20modified%20molecule.%20We%0Aconduct%20experiments%20across%20both%20single-%20and%20multi-property%20objectives%20with%202%0Athresholds%2C%20where%20%24%5Ctext%7BRe%7D%5E3%24DF%20shows%20significant%20improvements.%20Particularly%2C%0Afor%2020%20single-property%20objectives%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20enhances%20Hit%20ratio%20by%2016.95%25%0Aand%2020.76%25%20under%20loose%20%28%5Ctexttt%7Bl%7D%29%20and%20strict%20%28%5Ctexttt%7Bs%7D%29%20thresholds%2C%0Arespectively.%20For%2032%20multi-property%20objectives%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20enhances%20Hit%0Aratio%20by%206.04%25%20and%205.25%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13147v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Large%2520Language%2520Models%2520in%2520an%2520iterative%2520paradigm%2520with%2520domain%250A%2520%2520feedback%2520for%2520molecule%2520optimization%26entry.906535625%3DKhiem%2520Le%2520and%2520Nitesh%2520V.%2520Chawla%26entry.1292438233%3D%2520%2520Molecule%2520optimization%2520is%2520a%2520critical%2520task%2520in%2520drug%2520discovery%2520to%2520optimize%250Adesired%2520properties%2520of%2520a%2520given%2520molecule%2520through%2520chemical%2520modification.%2520Despite%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520holding%2520the%2520potential%2520to%2520efficiently%2520simulate%2520this%250Atask%2520by%2520using%2520natural%2520language%2520to%2520direct%2520the%2520optimization%252C%2520straightforwardly%250Autilizing%2520them%2520shows%2520limited%2520performance.%2520In%2520this%2520work%252C%2520we%2520facilitate%2520utilizing%250ALLMs%2520in%2520an%2520iterative%2520paradigm%2520by%2520proposing%2520a%2520simple%2520yet%2520highly%2520effective%2520domain%250Afeedback%2520provider%252C%2520namely%2520%2524%255Ctext%257BRe%257D%255E3%2524DF.%2520In%2520detail%252C%2520%2524%255Ctext%257BRe%257D%255E3%2524DF%2520harnesses%250Aan%2520external%2520toolkit%252C%2520RDKit%252C%2520to%2520handle%2520the%2520molecule%2520hallucination%252C%2520if%2520the%250Amodified%2520molecule%2520is%2520chemically%2520invalid.%2520Otherwise%252C%2520its%2520desired%2520properties%2520are%250Acomputed%2520and%2520compared%2520to%2520the%2520original%2520one%252C%2520establishing%2520reliable%2520domain%250Afeedback%2520with%2520correct%2520direction%2520and%2520distance%2520towards%2520the%2520objective%252C%2520followed%2520by%250Aa%2520retrieved%2520example%252C%2520to%2520guide%2520the%2520LLM%2520to%2520refine%2520the%2520modified%2520molecule.%2520We%250Aconduct%2520experiments%2520across%2520both%2520single-%2520and%2520multi-property%2520objectives%2520with%25202%250Athresholds%252C%2520where%2520%2524%255Ctext%257BRe%257D%255E3%2524DF%2520shows%2520significant%2520improvements.%2520Particularly%252C%250Afor%252020%2520single-property%2520objectives%252C%2520%2524%255Ctext%257BRe%257D%255E3%2524DF%2520enhances%2520Hit%2520ratio%2520by%252016.95%2525%250Aand%252020.76%2525%2520under%2520loose%2520%2528%255Ctexttt%257Bl%257D%2529%2520and%2520strict%2520%2528%255Ctexttt%257Bs%257D%2529%2520thresholds%252C%250Arespectively.%2520For%252032%2520multi-property%2520objectives%252C%2520%2524%255Ctext%257BRe%257D%255E3%2524DF%2520enhances%2520Hit%250Aratio%2520by%25206.04%2525%2520and%25205.25%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13147v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Large%20Language%20Models%20in%20an%20iterative%20paradigm%20with%20domain%0A%20%20feedback%20for%20molecule%20optimization&entry.906535625=Khiem%20Le%20and%20Nitesh%20V.%20Chawla&entry.1292438233=%20%20Molecule%20optimization%20is%20a%20critical%20task%20in%20drug%20discovery%20to%20optimize%0Adesired%20properties%20of%20a%20given%20molecule%20through%20chemical%20modification.%20Despite%0ALarge%20Language%20Models%20%28LLMs%29%20holding%20the%20potential%20to%20efficiently%20simulate%20this%0Atask%20by%20using%20natural%20language%20to%20direct%20the%20optimization%2C%20straightforwardly%0Autilizing%20them%20shows%20limited%20performance.%20In%20this%20work%2C%20we%20facilitate%20utilizing%0ALLMs%20in%20an%20iterative%20paradigm%20by%20proposing%20a%20simple%20yet%20highly%20effective%20domain%0Afeedback%20provider%2C%20namely%20%24%5Ctext%7BRe%7D%5E3%24DF.%20In%20detail%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20harnesses%0Aan%20external%20toolkit%2C%20RDKit%2C%20to%20handle%20the%20molecule%20hallucination%2C%20if%20the%0Amodified%20molecule%20is%20chemically%20invalid.%20Otherwise%2C%20its%20desired%20properties%20are%0Acomputed%20and%20compared%20to%20the%20original%20one%2C%20establishing%20reliable%20domain%0Afeedback%20with%20correct%20direction%20and%20distance%20towards%20the%20objective%2C%20followed%20by%0Aa%20retrieved%20example%2C%20to%20guide%20the%20LLM%20to%20refine%20the%20modified%20molecule.%20We%0Aconduct%20experiments%20across%20both%20single-%20and%20multi-property%20objectives%20with%202%0Athresholds%2C%20where%20%24%5Ctext%7BRe%7D%5E3%24DF%20shows%20significant%20improvements.%20Particularly%2C%0Afor%2020%20single-property%20objectives%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20enhances%20Hit%20ratio%20by%2016.95%25%0Aand%2020.76%25%20under%20loose%20%28%5Ctexttt%7Bl%7D%29%20and%20strict%20%28%5Ctexttt%7Bs%7D%29%20thresholds%2C%0Arespectively.%20For%2032%20multi-property%20objectives%2C%20%24%5Ctext%7BRe%7D%5E3%24DF%20enhances%20Hit%0Aratio%20by%206.04%25%20and%205.25%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13147v6&entry.124074799=Read"},
{"title": "Investigating Sensitive Directions in GPT-2: An Improved Baseline and\n  Comparative Analysis of SAEs", "author": "Daniel J. Lee and Stefan Heimersheim", "abstract": "  Sensitive directions experiments attempt to understand the computational\nfeatures of Language Models (LMs) by measuring how much the next token\nprediction probabilities change by perturbing activations along specific\ndirections. We extend the sensitive directions work by introducing an improved\nbaseline for perturbation directions. We demonstrate that KL divergence for\nSparse Autoencoder (SAE) reconstruction errors are no longer pathologically\nhigh compared to the improved baseline. We also show that feature directions\nuncovered by SAEs have varying impacts on model outputs depending on the SAE's\nsparsity, with lower L0 SAE feature directions exerting a greater influence.\nAdditionally, we find that end-to-end SAE features do not exhibit stronger\neffects on model outputs compared to traditional SAEs.\n", "link": "http://arxiv.org/abs/2410.12555v2", "date": "2024-11-18", "relevancy": 1.9531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Sensitive%20Directions%20in%20GPT-2%3A%20An%20Improved%20Baseline%20and%0A%20%20Comparative%20Analysis%20of%20SAEs&body=Title%3A%20Investigating%20Sensitive%20Directions%20in%20GPT-2%3A%20An%20Improved%20Baseline%20and%0A%20%20Comparative%20Analysis%20of%20SAEs%0AAuthor%3A%20Daniel%20J.%20Lee%20and%20Stefan%20Heimersheim%0AAbstract%3A%20%20%20Sensitive%20directions%20experiments%20attempt%20to%20understand%20the%20computational%0Afeatures%20of%20Language%20Models%20%28LMs%29%20by%20measuring%20how%20much%20the%20next%20token%0Aprediction%20probabilities%20change%20by%20perturbing%20activations%20along%20specific%0Adirections.%20We%20extend%20the%20sensitive%20directions%20work%20by%20introducing%20an%20improved%0Abaseline%20for%20perturbation%20directions.%20We%20demonstrate%20that%20KL%20divergence%20for%0ASparse%20Autoencoder%20%28SAE%29%20reconstruction%20errors%20are%20no%20longer%20pathologically%0Ahigh%20compared%20to%20the%20improved%20baseline.%20We%20also%20show%20that%20feature%20directions%0Auncovered%20by%20SAEs%20have%20varying%20impacts%20on%20model%20outputs%20depending%20on%20the%20SAE%27s%0Asparsity%2C%20with%20lower%20L0%20SAE%20feature%20directions%20exerting%20a%20greater%20influence.%0AAdditionally%2C%20we%20find%20that%20end-to-end%20SAE%20features%20do%20not%20exhibit%20stronger%0Aeffects%20on%20model%20outputs%20compared%20to%20traditional%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Sensitive%2520Directions%2520in%2520GPT-2%253A%2520An%2520Improved%2520Baseline%2520and%250A%2520%2520Comparative%2520Analysis%2520of%2520SAEs%26entry.906535625%3DDaniel%2520J.%2520Lee%2520and%2520Stefan%2520Heimersheim%26entry.1292438233%3D%2520%2520Sensitive%2520directions%2520experiments%2520attempt%2520to%2520understand%2520the%2520computational%250Afeatures%2520of%2520Language%2520Models%2520%2528LMs%2529%2520by%2520measuring%2520how%2520much%2520the%2520next%2520token%250Aprediction%2520probabilities%2520change%2520by%2520perturbing%2520activations%2520along%2520specific%250Adirections.%2520We%2520extend%2520the%2520sensitive%2520directions%2520work%2520by%2520introducing%2520an%2520improved%250Abaseline%2520for%2520perturbation%2520directions.%2520We%2520demonstrate%2520that%2520KL%2520divergence%2520for%250ASparse%2520Autoencoder%2520%2528SAE%2529%2520reconstruction%2520errors%2520are%2520no%2520longer%2520pathologically%250Ahigh%2520compared%2520to%2520the%2520improved%2520baseline.%2520We%2520also%2520show%2520that%2520feature%2520directions%250Auncovered%2520by%2520SAEs%2520have%2520varying%2520impacts%2520on%2520model%2520outputs%2520depending%2520on%2520the%2520SAE%2527s%250Asparsity%252C%2520with%2520lower%2520L0%2520SAE%2520feature%2520directions%2520exerting%2520a%2520greater%2520influence.%250AAdditionally%252C%2520we%2520find%2520that%2520end-to-end%2520SAE%2520features%2520do%2520not%2520exhibit%2520stronger%250Aeffects%2520on%2520model%2520outputs%2520compared%2520to%2520traditional%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Sensitive%20Directions%20in%20GPT-2%3A%20An%20Improved%20Baseline%20and%0A%20%20Comparative%20Analysis%20of%20SAEs&entry.906535625=Daniel%20J.%20Lee%20and%20Stefan%20Heimersheim&entry.1292438233=%20%20Sensitive%20directions%20experiments%20attempt%20to%20understand%20the%20computational%0Afeatures%20of%20Language%20Models%20%28LMs%29%20by%20measuring%20how%20much%20the%20next%20token%0Aprediction%20probabilities%20change%20by%20perturbing%20activations%20along%20specific%0Adirections.%20We%20extend%20the%20sensitive%20directions%20work%20by%20introducing%20an%20improved%0Abaseline%20for%20perturbation%20directions.%20We%20demonstrate%20that%20KL%20divergence%20for%0ASparse%20Autoencoder%20%28SAE%29%20reconstruction%20errors%20are%20no%20longer%20pathologically%0Ahigh%20compared%20to%20the%20improved%20baseline.%20We%20also%20show%20that%20feature%20directions%0Auncovered%20by%20SAEs%20have%20varying%20impacts%20on%20model%20outputs%20depending%20on%20the%20SAE%27s%0Asparsity%2C%20with%20lower%20L0%20SAE%20feature%20directions%20exerting%20a%20greater%20influence.%0AAdditionally%2C%20we%20find%20that%20end-to-end%20SAE%20features%20do%20not%20exhibit%20stronger%0Aeffects%20on%20model%20outputs%20compared%20to%20traditional%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12555v2&entry.124074799=Read"},
{"title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality", "author": "Viktoriia Chekalina and Anton Razzigaev and Elizaveta Goncharova and Andrey Kuznetsov", "abstract": "  In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.\n", "link": "http://arxiv.org/abs/2411.11531v1", "date": "2024-11-18", "relevancy": 1.9396, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Hallucinations%20in%20Language%20Models%20with%20Knowledge%20Graph%0A%20%20Embeddings%20as%20an%20Additional%20Modality&body=Title%3A%20Addressing%20Hallucinations%20in%20Language%20Models%20with%20Knowledge%20Graph%0A%20%20Embeddings%20as%20an%20Additional%20Modality%0AAuthor%3A%20Viktoriia%20Chekalina%20and%20Anton%20Razzigaev%20and%20Elizaveta%20Goncharova%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20an%20approach%20to%20reduce%20hallucinations%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20incorporating%20Knowledge%20Graphs%20%28KGs%29%20as%20an%20additional%0Amodality.%20Our%20method%20involves%20transforming%20input%20text%20into%20a%20set%20of%20KG%0Aembeddings%20and%20using%20an%20adapter%20to%20integrate%20these%20embeddings%20into%20the%20language%0Amodel%20space%2C%20without%20relying%20on%20external%20retrieval%20processes.%0A%20%20To%20facilitate%20this%2C%20we%20created%20WikiEntities%2C%20a%20dataset%20containing%20over%203%0Amillion%20Wikipedia%20texts%20annotated%20with%20entities%20from%20Wikidata%20and%20their%0Acorresponding%20embeddings%20from%20PyTorch-BigGraph.%20This%20dataset%20serves%20as%20a%0Avaluable%20resource%20for%20training%20Entity%20Linking%20models%20and%20adapting%20the%20described%0Amethod%20to%20various%20LLMs%20using%20specialized%20adapters.%0A%20%20Our%20method%20does%20not%20require%20fine-tuning%20of%20the%20language%20models%20themselves%3B%0Ainstead%2C%20we%20only%20train%20the%20adapter.%20This%20ensures%20that%20the%20model%27s%20performance%0Aon%20other%20tasks%20is%20not%20affected.%20We%20trained%20an%20adapter%20for%20the%20Mistral%207B%2C%20LLaMA%0A2-7B%20%28chat%29%2C%20and%20LLaMA%203-8B%20%28instruct%29%20models%20using%20this%20dataset%20and%0Ademonstrated%20that%20our%20approach%20improves%20performance%20on%20the%20HaluEval%2C%20True-False%0Abenchmarks%20and%20FEVER%20dataset.%20The%20results%20indicate%20that%20incorporating%20KGs%20as%20a%0Anew%20modality%20can%20effectively%20reduce%20hallucinations%20and%20improve%20the%20factual%0Aaccuracy%20of%20language%20models%2C%20all%20without%20the%20need%20for%20external%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Hallucinations%2520in%2520Language%2520Models%2520with%2520Knowledge%2520Graph%250A%2520%2520Embeddings%2520as%2520an%2520Additional%2520Modality%26entry.906535625%3DViktoriia%2520Chekalina%2520and%2520Anton%2520Razzigaev%2520and%2520Elizaveta%2520Goncharova%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520an%2520approach%2520to%2520reduce%2520hallucinations%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520by%2520incorporating%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520as%2520an%2520additional%250Amodality.%2520Our%2520method%2520involves%2520transforming%2520input%2520text%2520into%2520a%2520set%2520of%2520KG%250Aembeddings%2520and%2520using%2520an%2520adapter%2520to%2520integrate%2520these%2520embeddings%2520into%2520the%2520language%250Amodel%2520space%252C%2520without%2520relying%2520on%2520external%2520retrieval%2520processes.%250A%2520%2520To%2520facilitate%2520this%252C%2520we%2520created%2520WikiEntities%252C%2520a%2520dataset%2520containing%2520over%25203%250Amillion%2520Wikipedia%2520texts%2520annotated%2520with%2520entities%2520from%2520Wikidata%2520and%2520their%250Acorresponding%2520embeddings%2520from%2520PyTorch-BigGraph.%2520This%2520dataset%2520serves%2520as%2520a%250Avaluable%2520resource%2520for%2520training%2520Entity%2520Linking%2520models%2520and%2520adapting%2520the%2520described%250Amethod%2520to%2520various%2520LLMs%2520using%2520specialized%2520adapters.%250A%2520%2520Our%2520method%2520does%2520not%2520require%2520fine-tuning%2520of%2520the%2520language%2520models%2520themselves%253B%250Ainstead%252C%2520we%2520only%2520train%2520the%2520adapter.%2520This%2520ensures%2520that%2520the%2520model%2527s%2520performance%250Aon%2520other%2520tasks%2520is%2520not%2520affected.%2520We%2520trained%2520an%2520adapter%2520for%2520the%2520Mistral%25207B%252C%2520LLaMA%250A2-7B%2520%2528chat%2529%252C%2520and%2520LLaMA%25203-8B%2520%2528instruct%2529%2520models%2520using%2520this%2520dataset%2520and%250Ademonstrated%2520that%2520our%2520approach%2520improves%2520performance%2520on%2520the%2520HaluEval%252C%2520True-False%250Abenchmarks%2520and%2520FEVER%2520dataset.%2520The%2520results%2520indicate%2520that%2520incorporating%2520KGs%2520as%2520a%250Anew%2520modality%2520can%2520effectively%2520reduce%2520hallucinations%2520and%2520improve%2520the%2520factual%250Aaccuracy%2520of%2520language%2520models%252C%2520all%2520without%2520the%2520need%2520for%2520external%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Hallucinations%20in%20Language%20Models%20with%20Knowledge%20Graph%0A%20%20Embeddings%20as%20an%20Additional%20Modality&entry.906535625=Viktoriia%20Chekalina%20and%20Anton%20Razzigaev%20and%20Elizaveta%20Goncharova%20and%20Andrey%20Kuznetsov&entry.1292438233=%20%20In%20this%20paper%20we%20present%20an%20approach%20to%20reduce%20hallucinations%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20by%20incorporating%20Knowledge%20Graphs%20%28KGs%29%20as%20an%20additional%0Amodality.%20Our%20method%20involves%20transforming%20input%20text%20into%20a%20set%20of%20KG%0Aembeddings%20and%20using%20an%20adapter%20to%20integrate%20these%20embeddings%20into%20the%20language%0Amodel%20space%2C%20without%20relying%20on%20external%20retrieval%20processes.%0A%20%20To%20facilitate%20this%2C%20we%20created%20WikiEntities%2C%20a%20dataset%20containing%20over%203%0Amillion%20Wikipedia%20texts%20annotated%20with%20entities%20from%20Wikidata%20and%20their%0Acorresponding%20embeddings%20from%20PyTorch-BigGraph.%20This%20dataset%20serves%20as%20a%0Avaluable%20resource%20for%20training%20Entity%20Linking%20models%20and%20adapting%20the%20described%0Amethod%20to%20various%20LLMs%20using%20specialized%20adapters.%0A%20%20Our%20method%20does%20not%20require%20fine-tuning%20of%20the%20language%20models%20themselves%3B%0Ainstead%2C%20we%20only%20train%20the%20adapter.%20This%20ensures%20that%20the%20model%27s%20performance%0Aon%20other%20tasks%20is%20not%20affected.%20We%20trained%20an%20adapter%20for%20the%20Mistral%207B%2C%20LLaMA%0A2-7B%20%28chat%29%2C%20and%20LLaMA%203-8B%20%28instruct%29%20models%20using%20this%20dataset%20and%0Ademonstrated%20that%20our%20approach%20improves%20performance%20on%20the%20HaluEval%2C%20True-False%0Abenchmarks%20and%20FEVER%20dataset.%20The%20results%20indicate%20that%20incorporating%20KGs%20as%20a%0Anew%20modality%20can%20effectively%20reduce%20hallucinations%20and%20improve%20the%20factual%0Aaccuracy%20of%20language%20models%2C%20all%20without%20the%20need%20for%20external%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11531v1&entry.124074799=Read"},
{"title": "BertaQA: How Much Do Language Models Know About Local Culture?", "author": "Julen Etxaniz and Gorka Azkune and Aitor Soroa and Oier Lopez de Lacalle and Mikel Artetxe", "abstract": "  Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.\n", "link": "http://arxiv.org/abs/2406.07302v2", "date": "2024-11-18", "relevancy": 1.9387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BertaQA%3A%20How%20Much%20Do%20Language%20Models%20Know%20About%20Local%20Culture%3F&body=Title%3A%20BertaQA%3A%20How%20Much%20Do%20Language%20Models%20Know%20About%20Local%20Culture%3F%0AAuthor%3A%20Julen%20Etxaniz%20and%20Gorka%20Azkune%20and%20Aitor%20Soroa%20and%20Oier%20Lopez%20de%20Lacalle%20and%20Mikel%20Artetxe%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20extensive%20knowledge%20about%20the%20world%2C%20but%0Amost%20evaluations%20have%20been%20limited%20to%20global%20or%20anglocentric%20subjects.%20This%0Araises%20the%20question%20of%20how%20well%20these%20models%20perform%20on%20topics%20relevant%20to%0Aother%20cultures%2C%20whose%20presence%20on%20the%20web%20is%20not%20that%20prominent.%20To%20address%0Athis%20gap%2C%20we%20introduce%20BertaQA%2C%20a%20multiple-choice%20trivia%20dataset%20that%20is%0Aparallel%20in%20English%20and%20Basque.%20The%20dataset%20consists%20of%20a%20local%20subset%20with%0Aquestions%20pertinent%20to%20the%20Basque%20culture%2C%20and%20a%20global%20subset%20with%20questions%0Aof%20broader%20interest.%20We%20find%20that%20state-of-the-art%20LLMs%20struggle%20with%20local%0Acultural%20knowledge%2C%20even%20as%20they%20excel%20on%20global%20topics.%20However%2C%20we%20show%20that%0Acontinued%20pre-training%20in%20Basque%20significantly%20improves%20the%20models%27%20performance%0Aon%20Basque%20culture%2C%20even%20when%20queried%20in%20English.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20solid%20evidence%20of%20knowledge%20transfer%20from%20a%20low-resource%20to%20a%0Ahigh-resource%20language.%20Our%20analysis%20sheds%20light%20on%20the%20complex%20interplay%0Abetween%20language%20and%20knowledge%2C%20and%20reveals%20that%20some%20prior%20findings%20do%20not%0Afully%20hold%20when%20reassessed%20on%20local%20topics.%20Our%20dataset%20and%20evaluation%20code%20are%0Aavailable%20under%20open%20licenses%20at%20https%3A//github.com/juletx/BertaQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBertaQA%253A%2520How%2520Much%2520Do%2520Language%2520Models%2520Know%2520About%2520Local%2520Culture%253F%26entry.906535625%3DJulen%2520Etxaniz%2520and%2520Gorka%2520Azkune%2520and%2520Aitor%2520Soroa%2520and%2520Oier%2520Lopez%2520de%2520Lacalle%2520and%2520Mikel%2520Artetxe%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520extensive%2520knowledge%2520about%2520the%2520world%252C%2520but%250Amost%2520evaluations%2520have%2520been%2520limited%2520to%2520global%2520or%2520anglocentric%2520subjects.%2520This%250Araises%2520the%2520question%2520of%2520how%2520well%2520these%2520models%2520perform%2520on%2520topics%2520relevant%2520to%250Aother%2520cultures%252C%2520whose%2520presence%2520on%2520the%2520web%2520is%2520not%2520that%2520prominent.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520BertaQA%252C%2520a%2520multiple-choice%2520trivia%2520dataset%2520that%2520is%250Aparallel%2520in%2520English%2520and%2520Basque.%2520The%2520dataset%2520consists%2520of%2520a%2520local%2520subset%2520with%250Aquestions%2520pertinent%2520to%2520the%2520Basque%2520culture%252C%2520and%2520a%2520global%2520subset%2520with%2520questions%250Aof%2520broader%2520interest.%2520We%2520find%2520that%2520state-of-the-art%2520LLMs%2520struggle%2520with%2520local%250Acultural%2520knowledge%252C%2520even%2520as%2520they%2520excel%2520on%2520global%2520topics.%2520However%252C%2520we%2520show%2520that%250Acontinued%2520pre-training%2520in%2520Basque%2520significantly%2520improves%2520the%2520models%2527%2520performance%250Aon%2520Basque%2520culture%252C%2520even%2520when%2520queried%2520in%2520English.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520solid%2520evidence%2520of%2520knowledge%2520transfer%2520from%2520a%2520low-resource%2520to%2520a%250Ahigh-resource%2520language.%2520Our%2520analysis%2520sheds%2520light%2520on%2520the%2520complex%2520interplay%250Abetween%2520language%2520and%2520knowledge%252C%2520and%2520reveals%2520that%2520some%2520prior%2520findings%2520do%2520not%250Afully%2520hold%2520when%2520reassessed%2520on%2520local%2520topics.%2520Our%2520dataset%2520and%2520evaluation%2520code%2520are%250Aavailable%2520under%2520open%2520licenses%2520at%2520https%253A//github.com/juletx/BertaQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BertaQA%3A%20How%20Much%20Do%20Language%20Models%20Know%20About%20Local%20Culture%3F&entry.906535625=Julen%20Etxaniz%20and%20Gorka%20Azkune%20and%20Aitor%20Soroa%20and%20Oier%20Lopez%20de%20Lacalle%20and%20Mikel%20Artetxe&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20extensive%20knowledge%20about%20the%20world%2C%20but%0Amost%20evaluations%20have%20been%20limited%20to%20global%20or%20anglocentric%20subjects.%20This%0Araises%20the%20question%20of%20how%20well%20these%20models%20perform%20on%20topics%20relevant%20to%0Aother%20cultures%2C%20whose%20presence%20on%20the%20web%20is%20not%20that%20prominent.%20To%20address%0Athis%20gap%2C%20we%20introduce%20BertaQA%2C%20a%20multiple-choice%20trivia%20dataset%20that%20is%0Aparallel%20in%20English%20and%20Basque.%20The%20dataset%20consists%20of%20a%20local%20subset%20with%0Aquestions%20pertinent%20to%20the%20Basque%20culture%2C%20and%20a%20global%20subset%20with%20questions%0Aof%20broader%20interest.%20We%20find%20that%20state-of-the-art%20LLMs%20struggle%20with%20local%0Acultural%20knowledge%2C%20even%20as%20they%20excel%20on%20global%20topics.%20However%2C%20we%20show%20that%0Acontinued%20pre-training%20in%20Basque%20significantly%20improves%20the%20models%27%20performance%0Aon%20Basque%20culture%2C%20even%20when%20queried%20in%20English.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20solid%20evidence%20of%20knowledge%20transfer%20from%20a%20low-resource%20to%20a%0Ahigh-resource%20language.%20Our%20analysis%20sheds%20light%20on%20the%20complex%20interplay%0Abetween%20language%20and%20knowledge%2C%20and%20reveals%20that%20some%20prior%20findings%20do%20not%0Afully%20hold%20when%20reassessed%20on%20local%20topics.%20Our%20dataset%20and%20evaluation%20code%20are%0Aavailable%20under%20open%20licenses%20at%20https%3A//github.com/juletx/BertaQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07302v2&entry.124074799=Read"},
{"title": "A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges", "author": "Zifeng Wang and Hanyin Wang and Benjamin Danek and Ying Li and Christina Mack and Hoifung Poon and Yajuan Wang and Pranav Rajpurkar and Jimeng Sun", "abstract": "  The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.\n", "link": "http://arxiv.org/abs/2411.00024v2", "date": "2024-11-18", "relevancy": 1.9371, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Perspective%20for%20Adapting%20Generalist%20AI%20to%20Specialized%20Medical%20AI%0A%20%20Applications%20and%20Their%20Challenges&body=Title%3A%20A%20Perspective%20for%20Adapting%20Generalist%20AI%20to%20Specialized%20Medical%20AI%0A%20%20Applications%20and%20Their%20Challenges%0AAuthor%3A%20Zifeng%20Wang%20and%20Hanyin%20Wang%20and%20Benjamin%20Danek%20and%20Ying%20Li%20and%20Christina%20Mack%20and%20Hoifung%20Poon%20and%20Yajuan%20Wang%20and%20Pranav%20Rajpurkar%20and%20Jimeng%20Sun%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20medical%20applications%20has%0Asparked%20widespread%20interest%20across%20the%20healthcare%20industry%2C%20from%20drug%20discovery%0Aand%20development%20to%20clinical%20decision%20support%2C%20assisting%20telemedicine%2C%20medical%0Adevices%2C%20and%20healthcare%20insurance%20applications.%20This%20perspective%20paper%20aims%20to%0Adiscuss%20the%20inner%20workings%20of%20building%20LLM-powered%20medical%20AI%20applications%20and%0Aintroduces%20a%20comprehensive%20framework%20for%20their%20development.%20We%20review%20existing%0Aliterature%20and%20outline%20the%20unique%20challenges%20of%20applying%20LLMs%20in%20specialized%0Amedical%20contexts.%20Additionally%2C%20we%20introduce%20a%20three-step%20framework%20to%20organize%0Amedical%20LLM%20research%20activities%3A%201%29%20Modeling%3A%20breaking%20down%20complex%20medical%0Aworkflows%20into%20manageable%20steps%20for%20developing%20medical-specific%20models%3B%202%29%0AOptimization%3A%20optimizing%20the%20model%20performance%20with%20crafted%20prompts%20and%0Aintegrating%20external%20knowledge%20and%20tools%2C%20and%203%29%20System%20engineering%3A%0Adecomposing%20complex%20tasks%20into%20subtasks%20and%20leveraging%20human%20expertise%20for%0Abuilding%20medical%20AI%20applications.%20Furthermore%2C%20we%20offer%20a%20detailed%20use%20case%0Aplaybook%20that%20describes%20various%20LLM-powered%20medical%20AI%20applications%2C%20such%20as%0Aoptimizing%20clinical%20trial%20design%2C%20enhancing%20clinical%20decision%20support%2C%20and%0Aadvancing%20medical%20imaging%20analysis.%20Finally%2C%20we%20discuss%20various%20challenges%20and%0Aconsiderations%20for%20building%20medical%20AI%20applications%20with%20LLMs%2C%20such%20as%20handling%0Ahallucination%20issues%2C%20data%20ownership%20and%20compliance%2C%20privacy%2C%20intellectual%0Aproperty%20considerations%2C%20compute%20cost%2C%20sustainability%20issues%2C%20and%20responsible%0AAI%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Perspective%2520for%2520Adapting%2520Generalist%2520AI%2520to%2520Specialized%2520Medical%2520AI%250A%2520%2520Applications%2520and%2520Their%2520Challenges%26entry.906535625%3DZifeng%2520Wang%2520and%2520Hanyin%2520Wang%2520and%2520Benjamin%2520Danek%2520and%2520Ying%2520Li%2520and%2520Christina%2520Mack%2520and%2520Hoifung%2520Poon%2520and%2520Yajuan%2520Wang%2520and%2520Pranav%2520Rajpurkar%2520and%2520Jimeng%2520Sun%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520medical%2520applications%2520has%250Asparked%2520widespread%2520interest%2520across%2520the%2520healthcare%2520industry%252C%2520from%2520drug%2520discovery%250Aand%2520development%2520to%2520clinical%2520decision%2520support%252C%2520assisting%2520telemedicine%252C%2520medical%250Adevices%252C%2520and%2520healthcare%2520insurance%2520applications.%2520This%2520perspective%2520paper%2520aims%2520to%250Adiscuss%2520the%2520inner%2520workings%2520of%2520building%2520LLM-powered%2520medical%2520AI%2520applications%2520and%250Aintroduces%2520a%2520comprehensive%2520framework%2520for%2520their%2520development.%2520We%2520review%2520existing%250Aliterature%2520and%2520outline%2520the%2520unique%2520challenges%2520of%2520applying%2520LLMs%2520in%2520specialized%250Amedical%2520contexts.%2520Additionally%252C%2520we%2520introduce%2520a%2520three-step%2520framework%2520to%2520organize%250Amedical%2520LLM%2520research%2520activities%253A%25201%2529%2520Modeling%253A%2520breaking%2520down%2520complex%2520medical%250Aworkflows%2520into%2520manageable%2520steps%2520for%2520developing%2520medical-specific%2520models%253B%25202%2529%250AOptimization%253A%2520optimizing%2520the%2520model%2520performance%2520with%2520crafted%2520prompts%2520and%250Aintegrating%2520external%2520knowledge%2520and%2520tools%252C%2520and%25203%2529%2520System%2520engineering%253A%250Adecomposing%2520complex%2520tasks%2520into%2520subtasks%2520and%2520leveraging%2520human%2520expertise%2520for%250Abuilding%2520medical%2520AI%2520applications.%2520Furthermore%252C%2520we%2520offer%2520a%2520detailed%2520use%2520case%250Aplaybook%2520that%2520describes%2520various%2520LLM-powered%2520medical%2520AI%2520applications%252C%2520such%2520as%250Aoptimizing%2520clinical%2520trial%2520design%252C%2520enhancing%2520clinical%2520decision%2520support%252C%2520and%250Aadvancing%2520medical%2520imaging%2520analysis.%2520Finally%252C%2520we%2520discuss%2520various%2520challenges%2520and%250Aconsiderations%2520for%2520building%2520medical%2520AI%2520applications%2520with%2520LLMs%252C%2520such%2520as%2520handling%250Ahallucination%2520issues%252C%2520data%2520ownership%2520and%2520compliance%252C%2520privacy%252C%2520intellectual%250Aproperty%2520considerations%252C%2520compute%2520cost%252C%2520sustainability%2520issues%252C%2520and%2520responsible%250AAI%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Perspective%20for%20Adapting%20Generalist%20AI%20to%20Specialized%20Medical%20AI%0A%20%20Applications%20and%20Their%20Challenges&entry.906535625=Zifeng%20Wang%20and%20Hanyin%20Wang%20and%20Benjamin%20Danek%20and%20Ying%20Li%20and%20Christina%20Mack%20and%20Hoifung%20Poon%20and%20Yajuan%20Wang%20and%20Pranav%20Rajpurkar%20and%20Jimeng%20Sun&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20medical%20applications%20has%0Asparked%20widespread%20interest%20across%20the%20healthcare%20industry%2C%20from%20drug%20discovery%0Aand%20development%20to%20clinical%20decision%20support%2C%20assisting%20telemedicine%2C%20medical%0Adevices%2C%20and%20healthcare%20insurance%20applications.%20This%20perspective%20paper%20aims%20to%0Adiscuss%20the%20inner%20workings%20of%20building%20LLM-powered%20medical%20AI%20applications%20and%0Aintroduces%20a%20comprehensive%20framework%20for%20their%20development.%20We%20review%20existing%0Aliterature%20and%20outline%20the%20unique%20challenges%20of%20applying%20LLMs%20in%20specialized%0Amedical%20contexts.%20Additionally%2C%20we%20introduce%20a%20three-step%20framework%20to%20organize%0Amedical%20LLM%20research%20activities%3A%201%29%20Modeling%3A%20breaking%20down%20complex%20medical%0Aworkflows%20into%20manageable%20steps%20for%20developing%20medical-specific%20models%3B%202%29%0AOptimization%3A%20optimizing%20the%20model%20performance%20with%20crafted%20prompts%20and%0Aintegrating%20external%20knowledge%20and%20tools%2C%20and%203%29%20System%20engineering%3A%0Adecomposing%20complex%20tasks%20into%20subtasks%20and%20leveraging%20human%20expertise%20for%0Abuilding%20medical%20AI%20applications.%20Furthermore%2C%20we%20offer%20a%20detailed%20use%20case%0Aplaybook%20that%20describes%20various%20LLM-powered%20medical%20AI%20applications%2C%20such%20as%0Aoptimizing%20clinical%20trial%20design%2C%20enhancing%20clinical%20decision%20support%2C%20and%0Aadvancing%20medical%20imaging%20analysis.%20Finally%2C%20we%20discuss%20various%20challenges%20and%0Aconsiderations%20for%20building%20medical%20AI%20applications%20with%20LLMs%2C%20such%20as%20handling%0Ahallucination%20issues%2C%20data%20ownership%20and%20compliance%2C%20privacy%2C%20intellectual%0Aproperty%20considerations%2C%20compute%20cost%2C%20sustainability%20issues%2C%20and%20responsible%0AAI%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00024v2&entry.124074799=Read"},
{"title": "Chapter 7 Review of Data-Driven Generative AI Models for Knowledge\n  Extraction from Scientific Literature in Healthcare", "author": "Leon Kopitar and Primoz Kocbek and Lucija Gosak and Gregor Stiglic", "abstract": "  This review examines the development of abstractive NLP-based text\nsummarization approaches and compares them to existing techniques for\nextractive summarization. A brief history of text summarization from the 1950s\nto the introduction of pre-trained language models such as Bidirectional\nEncoder Representations from Transformer (BERT) and Generative Pre-training\nTransformers (GPT) are presented. In total, 60 studies were identified in\nPubMed and Web of Science, of which 29 were excluded and 24 were read and\nevaluated for eligibility, resulting in the use of seven studies for further\nanalysis. This chapter also includes a section with examples including an\nexample of a comparison between GPT-3 and state-of-the-art GPT-4 solutions in\nscientific text summarisation. Natural language processing has not yet reached\nits full potential in the generation of brief textual summaries. As there are\nacknowledged concerns that must be addressed, we can expect gradual\nintroduction of such models in practise.\n", "link": "http://arxiv.org/abs/2411.11635v1", "date": "2024-11-18", "relevancy": 1.7878, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4531}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4445}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chapter%207%20Review%20of%20Data-Driven%20Generative%20AI%20Models%20for%20Knowledge%0A%20%20Extraction%20from%20Scientific%20Literature%20in%20Healthcare&body=Title%3A%20Chapter%207%20Review%20of%20Data-Driven%20Generative%20AI%20Models%20for%20Knowledge%0A%20%20Extraction%20from%20Scientific%20Literature%20in%20Healthcare%0AAuthor%3A%20Leon%20Kopitar%20and%20Primoz%20Kocbek%20and%20Lucija%20Gosak%20and%20Gregor%20Stiglic%0AAbstract%3A%20%20%20This%20review%20examines%20the%20development%20of%20abstractive%20NLP-based%20text%0Asummarization%20approaches%20and%20compares%20them%20to%20existing%20techniques%20for%0Aextractive%20summarization.%20A%20brief%20history%20of%20text%20summarization%20from%20the%201950s%0Ato%20the%20introduction%20of%20pre-trained%20language%20models%20such%20as%20Bidirectional%0AEncoder%20Representations%20from%20Transformer%20%28BERT%29%20and%20Generative%20Pre-training%0ATransformers%20%28GPT%29%20are%20presented.%20In%20total%2C%2060%20studies%20were%20identified%20in%0APubMed%20and%20Web%20of%20Science%2C%20of%20which%2029%20were%20excluded%20and%2024%20were%20read%20and%0Aevaluated%20for%20eligibility%2C%20resulting%20in%20the%20use%20of%20seven%20studies%20for%20further%0Aanalysis.%20This%20chapter%20also%20includes%20a%20section%20with%20examples%20including%20an%0Aexample%20of%20a%20comparison%20between%20GPT-3%20and%20state-of-the-art%20GPT-4%20solutions%20in%0Ascientific%20text%20summarisation.%20Natural%20language%20processing%20has%20not%20yet%20reached%0Aits%20full%20potential%20in%20the%20generation%20of%20brief%20textual%20summaries.%20As%20there%20are%0Aacknowledged%20concerns%20that%20must%20be%20addressed%2C%20we%20can%20expect%20gradual%0Aintroduction%20of%20such%20models%20in%20practise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChapter%25207%2520Review%2520of%2520Data-Driven%2520Generative%2520AI%2520Models%2520for%2520Knowledge%250A%2520%2520Extraction%2520from%2520Scientific%2520Literature%2520in%2520Healthcare%26entry.906535625%3DLeon%2520Kopitar%2520and%2520Primoz%2520Kocbek%2520and%2520Lucija%2520Gosak%2520and%2520Gregor%2520Stiglic%26entry.1292438233%3D%2520%2520This%2520review%2520examines%2520the%2520development%2520of%2520abstractive%2520NLP-based%2520text%250Asummarization%2520approaches%2520and%2520compares%2520them%2520to%2520existing%2520techniques%2520for%250Aextractive%2520summarization.%2520A%2520brief%2520history%2520of%2520text%2520summarization%2520from%2520the%25201950s%250Ato%2520the%2520introduction%2520of%2520pre-trained%2520language%2520models%2520such%2520as%2520Bidirectional%250AEncoder%2520Representations%2520from%2520Transformer%2520%2528BERT%2529%2520and%2520Generative%2520Pre-training%250ATransformers%2520%2528GPT%2529%2520are%2520presented.%2520In%2520total%252C%252060%2520studies%2520were%2520identified%2520in%250APubMed%2520and%2520Web%2520of%2520Science%252C%2520of%2520which%252029%2520were%2520excluded%2520and%252024%2520were%2520read%2520and%250Aevaluated%2520for%2520eligibility%252C%2520resulting%2520in%2520the%2520use%2520of%2520seven%2520studies%2520for%2520further%250Aanalysis.%2520This%2520chapter%2520also%2520includes%2520a%2520section%2520with%2520examples%2520including%2520an%250Aexample%2520of%2520a%2520comparison%2520between%2520GPT-3%2520and%2520state-of-the-art%2520GPT-4%2520solutions%2520in%250Ascientific%2520text%2520summarisation.%2520Natural%2520language%2520processing%2520has%2520not%2520yet%2520reached%250Aits%2520full%2520potential%2520in%2520the%2520generation%2520of%2520brief%2520textual%2520summaries.%2520As%2520there%2520are%250Aacknowledged%2520concerns%2520that%2520must%2520be%2520addressed%252C%2520we%2520can%2520expect%2520gradual%250Aintroduction%2520of%2520such%2520models%2520in%2520practise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chapter%207%20Review%20of%20Data-Driven%20Generative%20AI%20Models%20for%20Knowledge%0A%20%20Extraction%20from%20Scientific%20Literature%20in%20Healthcare&entry.906535625=Leon%20Kopitar%20and%20Primoz%20Kocbek%20and%20Lucija%20Gosak%20and%20Gregor%20Stiglic&entry.1292438233=%20%20This%20review%20examines%20the%20development%20of%20abstractive%20NLP-based%20text%0Asummarization%20approaches%20and%20compares%20them%20to%20existing%20techniques%20for%0Aextractive%20summarization.%20A%20brief%20history%20of%20text%20summarization%20from%20the%201950s%0Ato%20the%20introduction%20of%20pre-trained%20language%20models%20such%20as%20Bidirectional%0AEncoder%20Representations%20from%20Transformer%20%28BERT%29%20and%20Generative%20Pre-training%0ATransformers%20%28GPT%29%20are%20presented.%20In%20total%2C%2060%20studies%20were%20identified%20in%0APubMed%20and%20Web%20of%20Science%2C%20of%20which%2029%20were%20excluded%20and%2024%20were%20read%20and%0Aevaluated%20for%20eligibility%2C%20resulting%20in%20the%20use%20of%20seven%20studies%20for%20further%0Aanalysis.%20This%20chapter%20also%20includes%20a%20section%20with%20examples%20including%20an%0Aexample%20of%20a%20comparison%20between%20GPT-3%20and%20state-of-the-art%20GPT-4%20solutions%20in%0Ascientific%20text%20summarisation.%20Natural%20language%20processing%20has%20not%20yet%20reached%0Aits%20full%20potential%20in%20the%20generation%20of%20brief%20textual%20summaries.%20As%20there%20are%0Aacknowledged%20concerns%20that%20must%20be%20addressed%2C%20we%20can%20expect%20gradual%0Aintroduction%20of%20such%20models%20in%20practise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11635v1&entry.124074799=Read"},
{"title": "Tackling prediction tasks in relational databases with LLMs", "author": "Marek Wydmuch and \u0141ukasz Borchmann and Filip Grali\u0144ski", "abstract": "  Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.\n", "link": "http://arxiv.org/abs/2411.11829v1", "date": "2024-11-18", "relevancy": 1.8129, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20prediction%20tasks%20in%20relational%20databases%20with%20LLMs&body=Title%3A%20Tackling%20prediction%20tasks%20in%20relational%20databases%20with%20LLMs%0AAuthor%3A%20Marek%20Wydmuch%20and%20%C5%81ukasz%20Borchmann%20and%20Filip%20Grali%C5%84ski%0AAbstract%3A%20%20%20Though%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20performance%0Aacross%20numerous%20problems%2C%20their%20application%20to%20predictive%20tasks%20in%20relational%0Adatabases%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20address%20the%20notion%20that%0ALLMs%20cannot%20yield%20satisfactory%20results%20on%20relational%20databases%20due%20to%20their%0Ainterconnected%20tables%2C%20complex%20relationships%2C%20and%20heterogeneous%20data%20types.%0AUsing%20the%20recently%20introduced%20RelBench%20benchmark%2C%20we%20demonstrate%20that%20even%20a%0Astraightforward%20application%20of%20LLMs%20achieves%20competitive%20performance%20on%20these%0Atasks.%20These%20findings%20establish%20LLMs%20as%20a%20promising%20new%20baseline%20for%20ML%20on%0Arelational%20databases%20and%20encourage%20further%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520prediction%2520tasks%2520in%2520relational%2520databases%2520with%2520LLMs%26entry.906535625%3DMarek%2520Wydmuch%2520and%2520%25C5%2581ukasz%2520Borchmann%2520and%2520Filip%2520Grali%25C5%2584ski%26entry.1292438233%3D%2520%2520Though%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%2520performance%250Aacross%2520numerous%2520problems%252C%2520their%2520application%2520to%2520predictive%2520tasks%2520in%2520relational%250Adatabases%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520notion%2520that%250ALLMs%2520cannot%2520yield%2520satisfactory%2520results%2520on%2520relational%2520databases%2520due%2520to%2520their%250Ainterconnected%2520tables%252C%2520complex%2520relationships%252C%2520and%2520heterogeneous%2520data%2520types.%250AUsing%2520the%2520recently%2520introduced%2520RelBench%2520benchmark%252C%2520we%2520demonstrate%2520that%2520even%2520a%250Astraightforward%2520application%2520of%2520LLMs%2520achieves%2520competitive%2520performance%2520on%2520these%250Atasks.%2520These%2520findings%2520establish%2520LLMs%2520as%2520a%2520promising%2520new%2520baseline%2520for%2520ML%2520on%250Arelational%2520databases%2520and%2520encourage%2520further%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20prediction%20tasks%20in%20relational%20databases%20with%20LLMs&entry.906535625=Marek%20Wydmuch%20and%20%C5%81ukasz%20Borchmann%20and%20Filip%20Grali%C5%84ski&entry.1292438233=%20%20Though%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20exceptional%20performance%0Aacross%20numerous%20problems%2C%20their%20application%20to%20predictive%20tasks%20in%20relational%0Adatabases%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20address%20the%20notion%20that%0ALLMs%20cannot%20yield%20satisfactory%20results%20on%20relational%20databases%20due%20to%20their%0Ainterconnected%20tables%2C%20complex%20relationships%2C%20and%20heterogeneous%20data%20types.%0AUsing%20the%20recently%20introduced%20RelBench%20benchmark%2C%20we%20demonstrate%20that%20even%20a%0Astraightforward%20application%20of%20LLMs%20achieves%20competitive%20performance%20on%20these%0Atasks.%20These%20findings%20establish%20LLMs%20as%20a%20promising%20new%20baseline%20for%20ML%20on%0Arelational%20databases%20and%20encourage%20further%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11829v1&entry.124074799=Read"},
{"title": "A Review of Digital Pixel Sensors", "author": "Md Rahatul Islam Udoy and Shamiul Alam and Md Mazharul Islam and Akhilesh Jaiswal and Ahmedullah Aziz", "abstract": "  Digital pixel sensor (DPS) has evolved as a pivotal component in modern\nimaging systems and has the potential to revolutionize various fields such as\nmedical imaging, astronomy, surveillance, IoT devices, etc. Compared to analog\npixel sensors, the DPS offers high speed and good image quality. However, the\nintroduced intrinsic complexity within each pixel, primarily attributed to the\naccommodation of the ADC circuit, engenders a substantial increase in the pixel\npitch. Unfortunately, such a pronounced escalation in pixel pitch drastically\nundermines the feasibility of achieving high-density integration, which is an\nobstacle that significantly narrows down the field of potential applications.\nNonetheless, designing compact conversion circuits along with strategic\nintegration of 3D architectural paradigms can be a potential remedy to the\nprevailing situation. This review article presents a comprehensive overview of\nthe vast area of DPS technology. The operating principles, advantages, and\nchallenges of different types of DPS circuits have been analyzed. We categorize\nthe schemes into several categories based on ADC operation. A comparative study\nbased on different performance metrics has also been showcased for a\nwell-rounded understanding.\n", "link": "http://arxiv.org/abs/2402.04507v2", "date": "2024-11-18", "relevancy": 1.7215, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4528}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4268}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Digital%20Pixel%20Sensors&body=Title%3A%20A%20Review%20of%20Digital%20Pixel%20Sensors%0AAuthor%3A%20Md%20Rahatul%20Islam%20Udoy%20and%20Shamiul%20Alam%20and%20Md%20Mazharul%20Islam%20and%20Akhilesh%20Jaiswal%20and%20Ahmedullah%20Aziz%0AAbstract%3A%20%20%20Digital%20pixel%20sensor%20%28DPS%29%20has%20evolved%20as%20a%20pivotal%20component%20in%20modern%0Aimaging%20systems%20and%20has%20the%20potential%20to%20revolutionize%20various%20fields%20such%20as%0Amedical%20imaging%2C%20astronomy%2C%20surveillance%2C%20IoT%20devices%2C%20etc.%20Compared%20to%20analog%0Apixel%20sensors%2C%20the%20DPS%20offers%20high%20speed%20and%20good%20image%20quality.%20However%2C%20the%0Aintroduced%20intrinsic%20complexity%20within%20each%20pixel%2C%20primarily%20attributed%20to%20the%0Aaccommodation%20of%20the%20ADC%20circuit%2C%20engenders%20a%20substantial%20increase%20in%20the%20pixel%0Apitch.%20Unfortunately%2C%20such%20a%20pronounced%20escalation%20in%20pixel%20pitch%20drastically%0Aundermines%20the%20feasibility%20of%20achieving%20high-density%20integration%2C%20which%20is%20an%0Aobstacle%20that%20significantly%20narrows%20down%20the%20field%20of%20potential%20applications.%0ANonetheless%2C%20designing%20compact%20conversion%20circuits%20along%20with%20strategic%0Aintegration%20of%203D%20architectural%20paradigms%20can%20be%20a%20potential%20remedy%20to%20the%0Aprevailing%20situation.%20This%20review%20article%20presents%20a%20comprehensive%20overview%20of%0Athe%20vast%20area%20of%20DPS%20technology.%20The%20operating%20principles%2C%20advantages%2C%20and%0Achallenges%20of%20different%20types%20of%20DPS%20circuits%20have%20been%20analyzed.%20We%20categorize%0Athe%20schemes%20into%20several%20categories%20based%20on%20ADC%20operation.%20A%20comparative%20study%0Abased%20on%20different%20performance%20metrics%20has%20also%20been%20showcased%20for%20a%0Awell-rounded%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Digital%2520Pixel%2520Sensors%26entry.906535625%3DMd%2520Rahatul%2520Islam%2520Udoy%2520and%2520Shamiul%2520Alam%2520and%2520Md%2520Mazharul%2520Islam%2520and%2520Akhilesh%2520Jaiswal%2520and%2520Ahmedullah%2520Aziz%26entry.1292438233%3D%2520%2520Digital%2520pixel%2520sensor%2520%2528DPS%2529%2520has%2520evolved%2520as%2520a%2520pivotal%2520component%2520in%2520modern%250Aimaging%2520systems%2520and%2520has%2520the%2520potential%2520to%2520revolutionize%2520various%2520fields%2520such%2520as%250Amedical%2520imaging%252C%2520astronomy%252C%2520surveillance%252C%2520IoT%2520devices%252C%2520etc.%2520Compared%2520to%2520analog%250Apixel%2520sensors%252C%2520the%2520DPS%2520offers%2520high%2520speed%2520and%2520good%2520image%2520quality.%2520However%252C%2520the%250Aintroduced%2520intrinsic%2520complexity%2520within%2520each%2520pixel%252C%2520primarily%2520attributed%2520to%2520the%250Aaccommodation%2520of%2520the%2520ADC%2520circuit%252C%2520engenders%2520a%2520substantial%2520increase%2520in%2520the%2520pixel%250Apitch.%2520Unfortunately%252C%2520such%2520a%2520pronounced%2520escalation%2520in%2520pixel%2520pitch%2520drastically%250Aundermines%2520the%2520feasibility%2520of%2520achieving%2520high-density%2520integration%252C%2520which%2520is%2520an%250Aobstacle%2520that%2520significantly%2520narrows%2520down%2520the%2520field%2520of%2520potential%2520applications.%250ANonetheless%252C%2520designing%2520compact%2520conversion%2520circuits%2520along%2520with%2520strategic%250Aintegration%2520of%25203D%2520architectural%2520paradigms%2520can%2520be%2520a%2520potential%2520remedy%2520to%2520the%250Aprevailing%2520situation.%2520This%2520review%2520article%2520presents%2520a%2520comprehensive%2520overview%2520of%250Athe%2520vast%2520area%2520of%2520DPS%2520technology.%2520The%2520operating%2520principles%252C%2520advantages%252C%2520and%250Achallenges%2520of%2520different%2520types%2520of%2520DPS%2520circuits%2520have%2520been%2520analyzed.%2520We%2520categorize%250Athe%2520schemes%2520into%2520several%2520categories%2520based%2520on%2520ADC%2520operation.%2520A%2520comparative%2520study%250Abased%2520on%2520different%2520performance%2520metrics%2520has%2520also%2520been%2520showcased%2520for%2520a%250Awell-rounded%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Digital%20Pixel%20Sensors&entry.906535625=Md%20Rahatul%20Islam%20Udoy%20and%20Shamiul%20Alam%20and%20Md%20Mazharul%20Islam%20and%20Akhilesh%20Jaiswal%20and%20Ahmedullah%20Aziz&entry.1292438233=%20%20Digital%20pixel%20sensor%20%28DPS%29%20has%20evolved%20as%20a%20pivotal%20component%20in%20modern%0Aimaging%20systems%20and%20has%20the%20potential%20to%20revolutionize%20various%20fields%20such%20as%0Amedical%20imaging%2C%20astronomy%2C%20surveillance%2C%20IoT%20devices%2C%20etc.%20Compared%20to%20analog%0Apixel%20sensors%2C%20the%20DPS%20offers%20high%20speed%20and%20good%20image%20quality.%20However%2C%20the%0Aintroduced%20intrinsic%20complexity%20within%20each%20pixel%2C%20primarily%20attributed%20to%20the%0Aaccommodation%20of%20the%20ADC%20circuit%2C%20engenders%20a%20substantial%20increase%20in%20the%20pixel%0Apitch.%20Unfortunately%2C%20such%20a%20pronounced%20escalation%20in%20pixel%20pitch%20drastically%0Aundermines%20the%20feasibility%20of%20achieving%20high-density%20integration%2C%20which%20is%20an%0Aobstacle%20that%20significantly%20narrows%20down%20the%20field%20of%20potential%20applications.%0ANonetheless%2C%20designing%20compact%20conversion%20circuits%20along%20with%20strategic%0Aintegration%20of%203D%20architectural%20paradigms%20can%20be%20a%20potential%20remedy%20to%20the%0Aprevailing%20situation.%20This%20review%20article%20presents%20a%20comprehensive%20overview%20of%0Athe%20vast%20area%20of%20DPS%20technology.%20The%20operating%20principles%2C%20advantages%2C%20and%0Achallenges%20of%20different%20types%20of%20DPS%20circuits%20have%20been%20analyzed.%20We%20categorize%0Athe%20schemes%20into%20several%20categories%20based%20on%20ADC%20operation.%20A%20comparative%20study%0Abased%20on%20different%20performance%20metrics%20has%20also%20been%20showcased%20for%20a%0Awell-rounded%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04507v2&entry.124074799=Read"},
{"title": "A Potential Game Perspective in Federated Learning", "author": "Kang Liu and Ziqi Wang and Enrique Zuazua", "abstract": "  Federated learning (FL) is an emerging paradigm for training machine learning\nmodels across distributed clients. Traditionally, in FL settings, a central\nserver assigns training efforts (or strategies) to clients. However, from a\nmarket-oriented perspective, clients may independently choose their training\nefforts based on rational self-interest. To explore this, we propose a\npotential game framework where each client's payoff is determined by their\nindividual efforts and the rewards provided by the server. The rewards are\ninfluenced by the collective efforts of all clients and can be modulated\nthrough a reward factor. Our study begins by establishing the existence of Nash\nequilibria (NEs), followed by an investigation of uniqueness in homogeneous\nsettings. We demonstrate a significant improvement in clients' training efforts\nat a critical reward factor, identifying it as the optimal choice for the\nserver. Furthermore, we prove the convergence of the best-response algorithm to\ncompute NEs for our FL game. Finally, we apply the training efforts derived\nfrom specific NEs to a real-world FL scenario, validating the effectiveness of\nthe identified optimal reward factor.\n", "link": "http://arxiv.org/abs/2411.11793v1", "date": "2024-11-18", "relevancy": 1.8031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4638}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Potential%20Game%20Perspective%20in%20Federated%20Learning&body=Title%3A%20A%20Potential%20Game%20Perspective%20in%20Federated%20Learning%0AAuthor%3A%20Kang%20Liu%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20an%20emerging%20paradigm%20for%20training%20machine%20learning%0Amodels%20across%20distributed%20clients.%20Traditionally%2C%20in%20FL%20settings%2C%20a%20central%0Aserver%20assigns%20training%20efforts%20%28or%20strategies%29%20to%20clients.%20However%2C%20from%20a%0Amarket-oriented%20perspective%2C%20clients%20may%20independently%20choose%20their%20training%0Aefforts%20based%20on%20rational%20self-interest.%20To%20explore%20this%2C%20we%20propose%20a%0Apotential%20game%20framework%20where%20each%20client%27s%20payoff%20is%20determined%20by%20their%0Aindividual%20efforts%20and%20the%20rewards%20provided%20by%20the%20server.%20The%20rewards%20are%0Ainfluenced%20by%20the%20collective%20efforts%20of%20all%20clients%20and%20can%20be%20modulated%0Athrough%20a%20reward%20factor.%20Our%20study%20begins%20by%20establishing%20the%20existence%20of%20Nash%0Aequilibria%20%28NEs%29%2C%20followed%20by%20an%20investigation%20of%20uniqueness%20in%20homogeneous%0Asettings.%20We%20demonstrate%20a%20significant%20improvement%20in%20clients%27%20training%20efforts%0Aat%20a%20critical%20reward%20factor%2C%20identifying%20it%20as%20the%20optimal%20choice%20for%20the%0Aserver.%20Furthermore%2C%20we%20prove%20the%20convergence%20of%20the%20best-response%20algorithm%20to%0Acompute%20NEs%20for%20our%20FL%20game.%20Finally%2C%20we%20apply%20the%20training%20efforts%20derived%0Afrom%20specific%20NEs%20to%20a%20real-world%20FL%20scenario%2C%20validating%20the%20effectiveness%20of%0Athe%20identified%20optimal%20reward%20factor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Potential%2520Game%2520Perspective%2520in%2520Federated%2520Learning%26entry.906535625%3DKang%2520Liu%2520and%2520Ziqi%2520Wang%2520and%2520Enrique%2520Zuazua%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520an%2520emerging%2520paradigm%2520for%2520training%2520machine%2520learning%250Amodels%2520across%2520distributed%2520clients.%2520Traditionally%252C%2520in%2520FL%2520settings%252C%2520a%2520central%250Aserver%2520assigns%2520training%2520efforts%2520%2528or%2520strategies%2529%2520to%2520clients.%2520However%252C%2520from%2520a%250Amarket-oriented%2520perspective%252C%2520clients%2520may%2520independently%2520choose%2520their%2520training%250Aefforts%2520based%2520on%2520rational%2520self-interest.%2520To%2520explore%2520this%252C%2520we%2520propose%2520a%250Apotential%2520game%2520framework%2520where%2520each%2520client%2527s%2520payoff%2520is%2520determined%2520by%2520their%250Aindividual%2520efforts%2520and%2520the%2520rewards%2520provided%2520by%2520the%2520server.%2520The%2520rewards%2520are%250Ainfluenced%2520by%2520the%2520collective%2520efforts%2520of%2520all%2520clients%2520and%2520can%2520be%2520modulated%250Athrough%2520a%2520reward%2520factor.%2520Our%2520study%2520begins%2520by%2520establishing%2520the%2520existence%2520of%2520Nash%250Aequilibria%2520%2528NEs%2529%252C%2520followed%2520by%2520an%2520investigation%2520of%2520uniqueness%2520in%2520homogeneous%250Asettings.%2520We%2520demonstrate%2520a%2520significant%2520improvement%2520in%2520clients%2527%2520training%2520efforts%250Aat%2520a%2520critical%2520reward%2520factor%252C%2520identifying%2520it%2520as%2520the%2520optimal%2520choice%2520for%2520the%250Aserver.%2520Furthermore%252C%2520we%2520prove%2520the%2520convergence%2520of%2520the%2520best-response%2520algorithm%2520to%250Acompute%2520NEs%2520for%2520our%2520FL%2520game.%2520Finally%252C%2520we%2520apply%2520the%2520training%2520efforts%2520derived%250Afrom%2520specific%2520NEs%2520to%2520a%2520real-world%2520FL%2520scenario%252C%2520validating%2520the%2520effectiveness%2520of%250Athe%2520identified%2520optimal%2520reward%2520factor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Potential%20Game%20Perspective%20in%20Federated%20Learning&entry.906535625=Kang%20Liu%20and%20Ziqi%20Wang%20and%20Enrique%20Zuazua&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20an%20emerging%20paradigm%20for%20training%20machine%20learning%0Amodels%20across%20distributed%20clients.%20Traditionally%2C%20in%20FL%20settings%2C%20a%20central%0Aserver%20assigns%20training%20efforts%20%28or%20strategies%29%20to%20clients.%20However%2C%20from%20a%0Amarket-oriented%20perspective%2C%20clients%20may%20independently%20choose%20their%20training%0Aefforts%20based%20on%20rational%20self-interest.%20To%20explore%20this%2C%20we%20propose%20a%0Apotential%20game%20framework%20where%20each%20client%27s%20payoff%20is%20determined%20by%20their%0Aindividual%20efforts%20and%20the%20rewards%20provided%20by%20the%20server.%20The%20rewards%20are%0Ainfluenced%20by%20the%20collective%20efforts%20of%20all%20clients%20and%20can%20be%20modulated%0Athrough%20a%20reward%20factor.%20Our%20study%20begins%20by%20establishing%20the%20existence%20of%20Nash%0Aequilibria%20%28NEs%29%2C%20followed%20by%20an%20investigation%20of%20uniqueness%20in%20homogeneous%0Asettings.%20We%20demonstrate%20a%20significant%20improvement%20in%20clients%27%20training%20efforts%0Aat%20a%20critical%20reward%20factor%2C%20identifying%20it%20as%20the%20optimal%20choice%20for%20the%0Aserver.%20Furthermore%2C%20we%20prove%20the%20convergence%20of%20the%20best-response%20algorithm%20to%0Acompute%20NEs%20for%20our%20FL%20game.%20Finally%2C%20we%20apply%20the%20training%20efforts%20derived%0Afrom%20specific%20NEs%20to%20a%20real-world%20FL%20scenario%2C%20validating%20the%20effectiveness%20of%0Athe%20identified%20optimal%20reward%20factor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11793v1&entry.124074799=Read"},
{"title": "Pursuing Overall Welfare in Federated Learning through Sequential\n  Decision Making", "author": "Seok-Ju Hahn and Gi-Soo Kim and Junghye Lee", "abstract": "  In traditional federated learning, a single global model cannot perform\nequally well for all clients. Therefore, the need to achieve the client-level\nfairness in federated system has been emphasized, which can be realized by\nmodifying the static aggregation scheme for updating the global model to an\nadaptive one, in response to the local signals of the participating clients.\nOur work reveals that existing fairness-aware aggregation strategies can be\nunified into an online convex optimization framework, in other words, a central\nserver's sequential decision making process. To enhance the decision making\ncapability, we propose simple and intuitive improvements for suboptimal designs\nwithin existing methods, presenting AAggFF. Considering practical requirements,\nwe further subdivide our method tailored for the cross-device and the\ncross-silo settings, respectively. Theoretical analyses guarantee sublinear\nregret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the\ncross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting,\nwith $K$ clients and $T$ federation rounds. Extensive experiments demonstrate\nthat the federated system equipped with AAggFF achieves better degree of\nclient-level fairness than existing methods in both practical settings. Code is\navailable at https://github.com/vaseline555/AAggFF\n", "link": "http://arxiv.org/abs/2405.20821v2", "date": "2024-11-18", "relevancy": 1.8186, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4641}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pursuing%20Overall%20Welfare%20in%20Federated%20Learning%20through%20Sequential%0A%20%20Decision%20Making&body=Title%3A%20Pursuing%20Overall%20Welfare%20in%20Federated%20Learning%20through%20Sequential%0A%20%20Decision%20Making%0AAuthor%3A%20Seok-Ju%20Hahn%20and%20Gi-Soo%20Kim%20and%20Junghye%20Lee%0AAbstract%3A%20%20%20In%20traditional%20federated%20learning%2C%20a%20single%20global%20model%20cannot%20perform%0Aequally%20well%20for%20all%20clients.%20Therefore%2C%20the%20need%20to%20achieve%20the%20client-level%0Afairness%20in%20federated%20system%20has%20been%20emphasized%2C%20which%20can%20be%20realized%20by%0Amodifying%20the%20static%20aggregation%20scheme%20for%20updating%20the%20global%20model%20to%20an%0Aadaptive%20one%2C%20in%20response%20to%20the%20local%20signals%20of%20the%20participating%20clients.%0AOur%20work%20reveals%20that%20existing%20fairness-aware%20aggregation%20strategies%20can%20be%0Aunified%20into%20an%20online%20convex%20optimization%20framework%2C%20in%20other%20words%2C%20a%20central%0Aserver%27s%20sequential%20decision%20making%20process.%20To%20enhance%20the%20decision%20making%0Acapability%2C%20we%20propose%20simple%20and%20intuitive%20improvements%20for%20suboptimal%20designs%0Awithin%20existing%20methods%2C%20presenting%20AAggFF.%20Considering%20practical%20requirements%2C%0Awe%20further%20subdivide%20our%20method%20tailored%20for%20the%20cross-device%20and%20the%0Across-silo%20settings%2C%20respectively.%20Theoretical%20analyses%20guarantee%20sublinear%0Aregret%20upper%20bounds%20for%20both%20settings%3A%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%20%5Clog%7BK%7D%7D%29%24%20for%20the%0Across-device%20setting%2C%20and%20%24%5Cmathcal%7BO%7D%28K%20%5Clog%7BT%7D%29%24%20for%20the%20cross-silo%20setting%2C%0Awith%20%24K%24%20clients%20and%20%24T%24%20federation%20rounds.%20Extensive%20experiments%20demonstrate%0Athat%20the%20federated%20system%20equipped%20with%20AAggFF%20achieves%20better%20degree%20of%0Aclient-level%20fairness%20than%20existing%20methods%20in%20both%20practical%20settings.%20Code%20is%0Aavailable%20at%20https%3A//github.com/vaseline555/AAggFF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20821v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPursuing%2520Overall%2520Welfare%2520in%2520Federated%2520Learning%2520through%2520Sequential%250A%2520%2520Decision%2520Making%26entry.906535625%3DSeok-Ju%2520Hahn%2520and%2520Gi-Soo%2520Kim%2520and%2520Junghye%2520Lee%26entry.1292438233%3D%2520%2520In%2520traditional%2520federated%2520learning%252C%2520a%2520single%2520global%2520model%2520cannot%2520perform%250Aequally%2520well%2520for%2520all%2520clients.%2520Therefore%252C%2520the%2520need%2520to%2520achieve%2520the%2520client-level%250Afairness%2520in%2520federated%2520system%2520has%2520been%2520emphasized%252C%2520which%2520can%2520be%2520realized%2520by%250Amodifying%2520the%2520static%2520aggregation%2520scheme%2520for%2520updating%2520the%2520global%2520model%2520to%2520an%250Aadaptive%2520one%252C%2520in%2520response%2520to%2520the%2520local%2520signals%2520of%2520the%2520participating%2520clients.%250AOur%2520work%2520reveals%2520that%2520existing%2520fairness-aware%2520aggregation%2520strategies%2520can%2520be%250Aunified%2520into%2520an%2520online%2520convex%2520optimization%2520framework%252C%2520in%2520other%2520words%252C%2520a%2520central%250Aserver%2527s%2520sequential%2520decision%2520making%2520process.%2520To%2520enhance%2520the%2520decision%2520making%250Acapability%252C%2520we%2520propose%2520simple%2520and%2520intuitive%2520improvements%2520for%2520suboptimal%2520designs%250Awithin%2520existing%2520methods%252C%2520presenting%2520AAggFF.%2520Considering%2520practical%2520requirements%252C%250Awe%2520further%2520subdivide%2520our%2520method%2520tailored%2520for%2520the%2520cross-device%2520and%2520the%250Across-silo%2520settings%252C%2520respectively.%2520Theoretical%2520analyses%2520guarantee%2520sublinear%250Aregret%2520upper%2520bounds%2520for%2520both%2520settings%253A%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257BT%2520%255Clog%257BK%257D%257D%2529%2524%2520for%2520the%250Across-device%2520setting%252C%2520and%2520%2524%255Cmathcal%257BO%257D%2528K%2520%255Clog%257BT%257D%2529%2524%2520for%2520the%2520cross-silo%2520setting%252C%250Awith%2520%2524K%2524%2520clients%2520and%2520%2524T%2524%2520federation%2520rounds.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520the%2520federated%2520system%2520equipped%2520with%2520AAggFF%2520achieves%2520better%2520degree%2520of%250Aclient-level%2520fairness%2520than%2520existing%2520methods%2520in%2520both%2520practical%2520settings.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/vaseline555/AAggFF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20821v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pursuing%20Overall%20Welfare%20in%20Federated%20Learning%20through%20Sequential%0A%20%20Decision%20Making&entry.906535625=Seok-Ju%20Hahn%20and%20Gi-Soo%20Kim%20and%20Junghye%20Lee&entry.1292438233=%20%20In%20traditional%20federated%20learning%2C%20a%20single%20global%20model%20cannot%20perform%0Aequally%20well%20for%20all%20clients.%20Therefore%2C%20the%20need%20to%20achieve%20the%20client-level%0Afairness%20in%20federated%20system%20has%20been%20emphasized%2C%20which%20can%20be%20realized%20by%0Amodifying%20the%20static%20aggregation%20scheme%20for%20updating%20the%20global%20model%20to%20an%0Aadaptive%20one%2C%20in%20response%20to%20the%20local%20signals%20of%20the%20participating%20clients.%0AOur%20work%20reveals%20that%20existing%20fairness-aware%20aggregation%20strategies%20can%20be%0Aunified%20into%20an%20online%20convex%20optimization%20framework%2C%20in%20other%20words%2C%20a%20central%0Aserver%27s%20sequential%20decision%20making%20process.%20To%20enhance%20the%20decision%20making%0Acapability%2C%20we%20propose%20simple%20and%20intuitive%20improvements%20for%20suboptimal%20designs%0Awithin%20existing%20methods%2C%20presenting%20AAggFF.%20Considering%20practical%20requirements%2C%0Awe%20further%20subdivide%20our%20method%20tailored%20for%20the%20cross-device%20and%20the%0Across-silo%20settings%2C%20respectively.%20Theoretical%20analyses%20guarantee%20sublinear%0Aregret%20upper%20bounds%20for%20both%20settings%3A%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%20%5Clog%7BK%7D%7D%29%24%20for%20the%0Across-device%20setting%2C%20and%20%24%5Cmathcal%7BO%7D%28K%20%5Clog%7BT%7D%29%24%20for%20the%20cross-silo%20setting%2C%0Awith%20%24K%24%20clients%20and%20%24T%24%20federation%20rounds.%20Extensive%20experiments%20demonstrate%0Athat%20the%20federated%20system%20equipped%20with%20AAggFF%20achieves%20better%20degree%20of%0Aclient-level%20fairness%20than%20existing%20methods%20in%20both%20practical%20settings.%20Code%20is%0Aavailable%20at%20https%3A//github.com/vaseline555/AAggFF%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20821v2&entry.124074799=Read"},
{"title": "Joint-Space Control of a Structurally Elastic Humanoid Robot", "author": "Connor W. Herron and Christian Runyon and Isaac Pressgrove and Benjamin C. Beiter and Bhaben Kalita and Alexander Leonessa", "abstract": "  In this work, the joint-control strategy is presented for the humanoid robot,\nPANDORA, whose structural components are designed to be compliant. As opposed\nto contemporary approaches which design the elasticity internal to the actuator\nhousing, PANDORA's structural components are designed to be compliant under\nload or, in other words, structurally elastic. To maintain the rapid design\nbenefit of additive manufacturing, this joint control strategy employs a\ndisturbance observer (DOB) modeled from an ideal elastic actuator. This robust\ncontroller treats the model variation from the structurally elastic components\nas a disturbance and eliminates the need for system identification of the 3D\nprinted parts. This enables mechanical design engineers to iterate on the 3D\nprinted linkages without requiring consistent tuning from the joint controller.\nTwo sets of hardware results are presented for validating the controller. The\nfirst set of results are conducted on an ideal elastic actuator testbed that\ndrives an unmodeled, 1 DoF weighted pendulum with a 10 kg mass. The results\nsupport the claim that the DOB can handle significant model variation. The\nsecond set of results is from a robust balancing experiment conducted on the 12\nDoF lower body of PANDORA. The robot maintains balance while an operator\napplies 50 N pushes to the pelvis, where the actuator tracking results are\npresented for the left leg.\n", "link": "http://arxiv.org/abs/2411.11734v1", "date": "2024-11-18", "relevancy": 1.4882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5017}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint-Space%20Control%20of%20a%20Structurally%20Elastic%20Humanoid%20Robot&body=Title%3A%20Joint-Space%20Control%20of%20a%20Structurally%20Elastic%20Humanoid%20Robot%0AAuthor%3A%20Connor%20W.%20Herron%20and%20Christian%20Runyon%20and%20Isaac%20Pressgrove%20and%20Benjamin%20C.%20Beiter%20and%20Bhaben%20Kalita%20and%20Alexander%20Leonessa%0AAbstract%3A%20%20%20In%20this%20work%2C%20the%20joint-control%20strategy%20is%20presented%20for%20the%20humanoid%20robot%2C%0APANDORA%2C%20whose%20structural%20components%20are%20designed%20to%20be%20compliant.%20As%20opposed%0Ato%20contemporary%20approaches%20which%20design%20the%20elasticity%20internal%20to%20the%20actuator%0Ahousing%2C%20PANDORA%27s%20structural%20components%20are%20designed%20to%20be%20compliant%20under%0Aload%20or%2C%20in%20other%20words%2C%20structurally%20elastic.%20To%20maintain%20the%20rapid%20design%0Abenefit%20of%20additive%20manufacturing%2C%20this%20joint%20control%20strategy%20employs%20a%0Adisturbance%20observer%20%28DOB%29%20modeled%20from%20an%20ideal%20elastic%20actuator.%20This%20robust%0Acontroller%20treats%20the%20model%20variation%20from%20the%20structurally%20elastic%20components%0Aas%20a%20disturbance%20and%20eliminates%20the%20need%20for%20system%20identification%20of%20the%203D%0Aprinted%20parts.%20This%20enables%20mechanical%20design%20engineers%20to%20iterate%20on%20the%203D%0Aprinted%20linkages%20without%20requiring%20consistent%20tuning%20from%20the%20joint%20controller.%0ATwo%20sets%20of%20hardware%20results%20are%20presented%20for%20validating%20the%20controller.%20The%0Afirst%20set%20of%20results%20are%20conducted%20on%20an%20ideal%20elastic%20actuator%20testbed%20that%0Adrives%20an%20unmodeled%2C%201%20DoF%20weighted%20pendulum%20with%20a%2010%20kg%20mass.%20The%20results%0Asupport%20the%20claim%20that%20the%20DOB%20can%20handle%20significant%20model%20variation.%20The%0Asecond%20set%20of%20results%20is%20from%20a%20robust%20balancing%20experiment%20conducted%20on%20the%2012%0ADoF%20lower%20body%20of%20PANDORA.%20The%20robot%20maintains%20balance%20while%20an%20operator%0Aapplies%2050%20N%20pushes%20to%20the%20pelvis%2C%20where%20the%20actuator%20tracking%20results%20are%0Apresented%20for%20the%20left%20leg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint-Space%2520Control%2520of%2520a%2520Structurally%2520Elastic%2520Humanoid%2520Robot%26entry.906535625%3DConnor%2520W.%2520Herron%2520and%2520Christian%2520Runyon%2520and%2520Isaac%2520Pressgrove%2520and%2520Benjamin%2520C.%2520Beiter%2520and%2520Bhaben%2520Kalita%2520and%2520Alexander%2520Leonessa%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520the%2520joint-control%2520strategy%2520is%2520presented%2520for%2520the%2520humanoid%2520robot%252C%250APANDORA%252C%2520whose%2520structural%2520components%2520are%2520designed%2520to%2520be%2520compliant.%2520As%2520opposed%250Ato%2520contemporary%2520approaches%2520which%2520design%2520the%2520elasticity%2520internal%2520to%2520the%2520actuator%250Ahousing%252C%2520PANDORA%2527s%2520structural%2520components%2520are%2520designed%2520to%2520be%2520compliant%2520under%250Aload%2520or%252C%2520in%2520other%2520words%252C%2520structurally%2520elastic.%2520To%2520maintain%2520the%2520rapid%2520design%250Abenefit%2520of%2520additive%2520manufacturing%252C%2520this%2520joint%2520control%2520strategy%2520employs%2520a%250Adisturbance%2520observer%2520%2528DOB%2529%2520modeled%2520from%2520an%2520ideal%2520elastic%2520actuator.%2520This%2520robust%250Acontroller%2520treats%2520the%2520model%2520variation%2520from%2520the%2520structurally%2520elastic%2520components%250Aas%2520a%2520disturbance%2520and%2520eliminates%2520the%2520need%2520for%2520system%2520identification%2520of%2520the%25203D%250Aprinted%2520parts.%2520This%2520enables%2520mechanical%2520design%2520engineers%2520to%2520iterate%2520on%2520the%25203D%250Aprinted%2520linkages%2520without%2520requiring%2520consistent%2520tuning%2520from%2520the%2520joint%2520controller.%250ATwo%2520sets%2520of%2520hardware%2520results%2520are%2520presented%2520for%2520validating%2520the%2520controller.%2520The%250Afirst%2520set%2520of%2520results%2520are%2520conducted%2520on%2520an%2520ideal%2520elastic%2520actuator%2520testbed%2520that%250Adrives%2520an%2520unmodeled%252C%25201%2520DoF%2520weighted%2520pendulum%2520with%2520a%252010%2520kg%2520mass.%2520The%2520results%250Asupport%2520the%2520claim%2520that%2520the%2520DOB%2520can%2520handle%2520significant%2520model%2520variation.%2520The%250Asecond%2520set%2520of%2520results%2520is%2520from%2520a%2520robust%2520balancing%2520experiment%2520conducted%2520on%2520the%252012%250ADoF%2520lower%2520body%2520of%2520PANDORA.%2520The%2520robot%2520maintains%2520balance%2520while%2520an%2520operator%250Aapplies%252050%2520N%2520pushes%2520to%2520the%2520pelvis%252C%2520where%2520the%2520actuator%2520tracking%2520results%2520are%250Apresented%2520for%2520the%2520left%2520leg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint-Space%20Control%20of%20a%20Structurally%20Elastic%20Humanoid%20Robot&entry.906535625=Connor%20W.%20Herron%20and%20Christian%20Runyon%20and%20Isaac%20Pressgrove%20and%20Benjamin%20C.%20Beiter%20and%20Bhaben%20Kalita%20and%20Alexander%20Leonessa&entry.1292438233=%20%20In%20this%20work%2C%20the%20joint-control%20strategy%20is%20presented%20for%20the%20humanoid%20robot%2C%0APANDORA%2C%20whose%20structural%20components%20are%20designed%20to%20be%20compliant.%20As%20opposed%0Ato%20contemporary%20approaches%20which%20design%20the%20elasticity%20internal%20to%20the%20actuator%0Ahousing%2C%20PANDORA%27s%20structural%20components%20are%20designed%20to%20be%20compliant%20under%0Aload%20or%2C%20in%20other%20words%2C%20structurally%20elastic.%20To%20maintain%20the%20rapid%20design%0Abenefit%20of%20additive%20manufacturing%2C%20this%20joint%20control%20strategy%20employs%20a%0Adisturbance%20observer%20%28DOB%29%20modeled%20from%20an%20ideal%20elastic%20actuator.%20This%20robust%0Acontroller%20treats%20the%20model%20variation%20from%20the%20structurally%20elastic%20components%0Aas%20a%20disturbance%20and%20eliminates%20the%20need%20for%20system%20identification%20of%20the%203D%0Aprinted%20parts.%20This%20enables%20mechanical%20design%20engineers%20to%20iterate%20on%20the%203D%0Aprinted%20linkages%20without%20requiring%20consistent%20tuning%20from%20the%20joint%20controller.%0ATwo%20sets%20of%20hardware%20results%20are%20presented%20for%20validating%20the%20controller.%20The%0Afirst%20set%20of%20results%20are%20conducted%20on%20an%20ideal%20elastic%20actuator%20testbed%20that%0Adrives%20an%20unmodeled%2C%201%20DoF%20weighted%20pendulum%20with%20a%2010%20kg%20mass.%20The%20results%0Asupport%20the%20claim%20that%20the%20DOB%20can%20handle%20significant%20model%20variation.%20The%0Asecond%20set%20of%20results%20is%20from%20a%20robust%20balancing%20experiment%20conducted%20on%20the%2012%0ADoF%20lower%20body%20of%20PANDORA.%20The%20robot%20maintains%20balance%20while%20an%20operator%0Aapplies%2050%20N%20pushes%20to%20the%20pelvis%2C%20where%20the%20actuator%20tracking%20results%20are%0Apresented%20for%20the%20left%20leg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11734v1&entry.124074799=Read"},
{"title": "ST-Tree with Interpretability for Multivariate Time Series\n  Classification", "author": "Mingsen Du and Yanxuan Wei and Yingxia Tang and Xiangwei Zheng and Shoushui Wei and Cun Ji", "abstract": "  Multivariate time series classification is of great importance in practical\napplications and is a challenging task. However, deep neural network models\nsuch as Transformers exhibit high accuracy in multivariate time series\nclassification but lack interpretability and fail to provide insights into the\ndecision-making process. On the other hand, traditional approaches based on\ndecision tree classifiers offer clear decision processes but relatively lower\naccuracy. Swin Transformer (ST) addresses these issues by leveraging\nself-attention mechanisms to capture both fine-grained local patterns and\nglobal patterns. It can also model multi-scale feature representation learning,\nthereby providing a more comprehensive representation of time series features.\nTo tackle the aforementioned challenges, we propose ST-Tree with\ninterpretability for multivariate time series classification. Specifically, the\nST-Tree model combines ST as the backbone network with an additional neural\ntree model. This integration allows us to fully leverage the advantages of ST\nin learning time series context while providing interpretable decision\nprocesses through the neural tree. This enables researchers to gain clear\ninsights into the model's decision-making process and extract meaningful\ninterpretations. Through experimental evaluations on 10 UEA datasets, we\ndemonstrate that the ST-Tree model improves accuracy in multivariate time\nseries classification tasks and provides interpretability through visualizing\nthe decision-making process across different datasets.\n", "link": "http://arxiv.org/abs/2411.11620v1", "date": "2024-11-18", "relevancy": 1.4413, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ST-Tree%20with%20Interpretability%20for%20Multivariate%20Time%20Series%0A%20%20Classification&body=Title%3A%20ST-Tree%20with%20Interpretability%20for%20Multivariate%20Time%20Series%0A%20%20Classification%0AAuthor%3A%20Mingsen%20Du%20and%20Yanxuan%20Wei%20and%20Yingxia%20Tang%20and%20Xiangwei%20Zheng%20and%20Shoushui%20Wei%20and%20Cun%20Ji%0AAbstract%3A%20%20%20Multivariate%20time%20series%20classification%20is%20of%20great%20importance%20in%20practical%0Aapplications%20and%20is%20a%20challenging%20task.%20However%2C%20deep%20neural%20network%20models%0Asuch%20as%20Transformers%20exhibit%20high%20accuracy%20in%20multivariate%20time%20series%0Aclassification%20but%20lack%20interpretability%20and%20fail%20to%20provide%20insights%20into%20the%0Adecision-making%20process.%20On%20the%20other%20hand%2C%20traditional%20approaches%20based%20on%0Adecision%20tree%20classifiers%20offer%20clear%20decision%20processes%20but%20relatively%20lower%0Aaccuracy.%20Swin%20Transformer%20%28ST%29%20addresses%20these%20issues%20by%20leveraging%0Aself-attention%20mechanisms%20to%20capture%20both%20fine-grained%20local%20patterns%20and%0Aglobal%20patterns.%20It%20can%20also%20model%20multi-scale%20feature%20representation%20learning%2C%0Athereby%20providing%20a%20more%20comprehensive%20representation%20of%20time%20series%20features.%0ATo%20tackle%20the%20aforementioned%20challenges%2C%20we%20propose%20ST-Tree%20with%0Ainterpretability%20for%20multivariate%20time%20series%20classification.%20Specifically%2C%20the%0AST-Tree%20model%20combines%20ST%20as%20the%20backbone%20network%20with%20an%20additional%20neural%0Atree%20model.%20This%20integration%20allows%20us%20to%20fully%20leverage%20the%20advantages%20of%20ST%0Ain%20learning%20time%20series%20context%20while%20providing%20interpretable%20decision%0Aprocesses%20through%20the%20neural%20tree.%20This%20enables%20researchers%20to%20gain%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%20and%20extract%20meaningful%0Ainterpretations.%20Through%20experimental%20evaluations%20on%2010%20UEA%20datasets%2C%20we%0Ademonstrate%20that%20the%20ST-Tree%20model%20improves%20accuracy%20in%20multivariate%20time%0Aseries%20classification%20tasks%20and%20provides%20interpretability%20through%20visualizing%0Athe%20decision-making%20process%20across%20different%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DST-Tree%2520with%2520Interpretability%2520for%2520Multivariate%2520Time%2520Series%250A%2520%2520Classification%26entry.906535625%3DMingsen%2520Du%2520and%2520Yanxuan%2520Wei%2520and%2520Yingxia%2520Tang%2520and%2520Xiangwei%2520Zheng%2520and%2520Shoushui%2520Wei%2520and%2520Cun%2520Ji%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520classification%2520is%2520of%2520great%2520importance%2520in%2520practical%250Aapplications%2520and%2520is%2520a%2520challenging%2520task.%2520However%252C%2520deep%2520neural%2520network%2520models%250Asuch%2520as%2520Transformers%2520exhibit%2520high%2520accuracy%2520in%2520multivariate%2520time%2520series%250Aclassification%2520but%2520lack%2520interpretability%2520and%2520fail%2520to%2520provide%2520insights%2520into%2520the%250Adecision-making%2520process.%2520On%2520the%2520other%2520hand%252C%2520traditional%2520approaches%2520based%2520on%250Adecision%2520tree%2520classifiers%2520offer%2520clear%2520decision%2520processes%2520but%2520relatively%2520lower%250Aaccuracy.%2520Swin%2520Transformer%2520%2528ST%2529%2520addresses%2520these%2520issues%2520by%2520leveraging%250Aself-attention%2520mechanisms%2520to%2520capture%2520both%2520fine-grained%2520local%2520patterns%2520and%250Aglobal%2520patterns.%2520It%2520can%2520also%2520model%2520multi-scale%2520feature%2520representation%2520learning%252C%250Athereby%2520providing%2520a%2520more%2520comprehensive%2520representation%2520of%2520time%2520series%2520features.%250ATo%2520tackle%2520the%2520aforementioned%2520challenges%252C%2520we%2520propose%2520ST-Tree%2520with%250Ainterpretability%2520for%2520multivariate%2520time%2520series%2520classification.%2520Specifically%252C%2520the%250AST-Tree%2520model%2520combines%2520ST%2520as%2520the%2520backbone%2520network%2520with%2520an%2520additional%2520neural%250Atree%2520model.%2520This%2520integration%2520allows%2520us%2520to%2520fully%2520leverage%2520the%2520advantages%2520of%2520ST%250Ain%2520learning%2520time%2520series%2520context%2520while%2520providing%2520interpretable%2520decision%250Aprocesses%2520through%2520the%2520neural%2520tree.%2520This%2520enables%2520researchers%2520to%2520gain%2520clear%250Ainsights%2520into%2520the%2520model%2527s%2520decision-making%2520process%2520and%2520extract%2520meaningful%250Ainterpretations.%2520Through%2520experimental%2520evaluations%2520on%252010%2520UEA%2520datasets%252C%2520we%250Ademonstrate%2520that%2520the%2520ST-Tree%2520model%2520improves%2520accuracy%2520in%2520multivariate%2520time%250Aseries%2520classification%2520tasks%2520and%2520provides%2520interpretability%2520through%2520visualizing%250Athe%2520decision-making%2520process%2520across%2520different%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ST-Tree%20with%20Interpretability%20for%20Multivariate%20Time%20Series%0A%20%20Classification&entry.906535625=Mingsen%20Du%20and%20Yanxuan%20Wei%20and%20Yingxia%20Tang%20and%20Xiangwei%20Zheng%20and%20Shoushui%20Wei%20and%20Cun%20Ji&entry.1292438233=%20%20Multivariate%20time%20series%20classification%20is%20of%20great%20importance%20in%20practical%0Aapplications%20and%20is%20a%20challenging%20task.%20However%2C%20deep%20neural%20network%20models%0Asuch%20as%20Transformers%20exhibit%20high%20accuracy%20in%20multivariate%20time%20series%0Aclassification%20but%20lack%20interpretability%20and%20fail%20to%20provide%20insights%20into%20the%0Adecision-making%20process.%20On%20the%20other%20hand%2C%20traditional%20approaches%20based%20on%0Adecision%20tree%20classifiers%20offer%20clear%20decision%20processes%20but%20relatively%20lower%0Aaccuracy.%20Swin%20Transformer%20%28ST%29%20addresses%20these%20issues%20by%20leveraging%0Aself-attention%20mechanisms%20to%20capture%20both%20fine-grained%20local%20patterns%20and%0Aglobal%20patterns.%20It%20can%20also%20model%20multi-scale%20feature%20representation%20learning%2C%0Athereby%20providing%20a%20more%20comprehensive%20representation%20of%20time%20series%20features.%0ATo%20tackle%20the%20aforementioned%20challenges%2C%20we%20propose%20ST-Tree%20with%0Ainterpretability%20for%20multivariate%20time%20series%20classification.%20Specifically%2C%20the%0AST-Tree%20model%20combines%20ST%20as%20the%20backbone%20network%20with%20an%20additional%20neural%0Atree%20model.%20This%20integration%20allows%20us%20to%20fully%20leverage%20the%20advantages%20of%20ST%0Ain%20learning%20time%20series%20context%20while%20providing%20interpretable%20decision%0Aprocesses%20through%20the%20neural%20tree.%20This%20enables%20researchers%20to%20gain%20clear%0Ainsights%20into%20the%20model%27s%20decision-making%20process%20and%20extract%20meaningful%0Ainterpretations.%20Through%20experimental%20evaluations%20on%2010%20UEA%20datasets%2C%20we%0Ademonstrate%20that%20the%20ST-Tree%20model%20improves%20accuracy%20in%20multivariate%20time%0Aseries%20classification%20tasks%20and%20provides%20interpretability%20through%20visualizing%0Athe%20decision-making%20process%20across%20different%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11620v1&entry.124074799=Read"},
{"title": "Investigating OCR-Sensitive Neurons to Improve Entity Recognition in\n  Historical Documents", "author": "Emanuela Boros and Maud Ehrmann", "abstract": "  This paper investigates the presence of OCR-sensitive neurons within the\nTransformer architecture and their influence on named entity recognition (NER)\nperformance on historical documents. By analysing neuron activation patterns in\nresponse to clean and noisy text inputs, we identify and then neutralise\nOCR-sensitive neurons to improve model performance. Based on two open access\nlarge language models (Llama2 and Mistral), experiments demonstrate the\nexistence of OCR-sensitive regions and show improvements in NER performance on\nhistorical newspapers and classical commentaries, highlighting the potential of\ntargeted neuron modulation to improve models' performance on noisy text.\n", "link": "http://arxiv.org/abs/2409.16934v3", "date": "2024-11-18", "relevancy": 1.9222, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5093}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&body=Title%3A%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents%0AAuthor%3A%20Emanuela%20Boros%20and%20Maud%20Ehrmann%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16934v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520OCR-Sensitive%2520Neurons%2520to%2520Improve%2520Entity%2520Recognition%2520in%250A%2520%2520Historical%2520Documents%26entry.906535625%3DEmanuela%2520Boros%2520and%2520Maud%2520Ehrmann%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520presence%2520of%2520OCR-sensitive%2520neurons%2520within%2520the%250ATransformer%2520architecture%2520and%2520their%2520influence%2520on%2520named%2520entity%2520recognition%2520%2528NER%2529%250Aperformance%2520on%2520historical%2520documents.%2520By%2520analysing%2520neuron%2520activation%2520patterns%2520in%250Aresponse%2520to%2520clean%2520and%2520noisy%2520text%2520inputs%252C%2520we%2520identify%2520and%2520then%2520neutralise%250AOCR-sensitive%2520neurons%2520to%2520improve%2520model%2520performance.%2520Based%2520on%2520two%2520open%2520access%250Alarge%2520language%2520models%2520%2528Llama2%2520and%2520Mistral%2529%252C%2520experiments%2520demonstrate%2520the%250Aexistence%2520of%2520OCR-sensitive%2520regions%2520and%2520show%2520improvements%2520in%2520NER%2520performance%2520on%250Ahistorical%2520newspapers%2520and%2520classical%2520commentaries%252C%2520highlighting%2520the%2520potential%2520of%250Atargeted%2520neuron%2520modulation%2520to%2520improve%2520models%2527%2520performance%2520on%2520noisy%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16934v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&entry.906535625=Emanuela%20Boros%20and%20Maud%20Ehrmann&entry.1292438233=%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16934v3&entry.124074799=Read"},
{"title": "A Complete Survey on LLM-based AI Chatbots", "author": "Sumit Kumar Dam and Choong Seon Hong and Yu Qiao and Chaoning Zhang", "abstract": "  The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.\n", "link": "http://arxiv.org/abs/2406.16937v2", "date": "2024-11-18", "relevancy": 1.3729, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4635}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Complete%20Survey%20on%20LLM-based%20AI%20Chatbots&body=Title%3A%20A%20Complete%20Survey%20on%20LLM-based%20AI%20Chatbots%0AAuthor%3A%20Sumit%20Kumar%20Dam%20and%20Choong%20Seon%20Hong%20and%20Yu%20Qiao%20and%20Chaoning%20Zhang%0AAbstract%3A%20%20%20The%20past%20few%20decades%20have%20witnessed%20an%20upsurge%20in%20data%2C%20forming%20the%0Afoundation%20for%20data-hungry%2C%20learning-based%20AI%20technology.%20Conversational%0Aagents%2C%20often%20referred%20to%20as%20AI%20chatbots%2C%20rely%20heavily%20on%20such%20data%20to%20train%0Alarge%20language%20models%20%28LLMs%29%20and%20generate%20new%20content%20%28knowledge%29%20in%20response%0Ato%20user%20prompts.%20With%20the%20advent%20of%20OpenAI%27s%20ChatGPT%2C%20LLM-based%20chatbots%20have%0Aset%20new%20standards%20in%20the%20AI%20community.%20This%20paper%20presents%20a%20complete%20survey%20of%0Athe%20evolution%20and%20deployment%20of%20LLM-based%20chatbots%20in%20various%20sectors.%20We%20first%0Asummarize%20the%20development%20of%20foundational%20chatbots%2C%20followed%20by%20the%20evolution%0Aof%20LLMs%2C%20and%20then%20provide%20an%20overview%20of%20LLM-based%20chatbots%20currently%20in%20use%0Aand%20those%20in%20the%20development%20phase.%20Recognizing%20AI%20chatbots%20as%20tools%20for%0Agenerating%20new%20knowledge%2C%20we%20explore%20their%20diverse%20applications%20across%20various%0Aindustries.%20We%20then%20discuss%20the%20open%20challenges%2C%20considering%20how%20the%20data%20used%0Ato%20train%20the%20LLMs%20and%20the%20misuse%20of%20the%20generated%20knowledge%20can%20cause%20several%0Aissues.%20Finally%2C%20we%20explore%20the%20future%20outlook%20to%20augment%20their%20efficiency%20and%0Areliability%20in%20numerous%20applications.%20By%20addressing%20key%20milestones%20and%20the%0Apresent-day%20context%20of%20LLM-based%20chatbots%2C%20our%20survey%20invites%20readers%20to%20delve%0Adeeper%20into%20this%20realm%2C%20reflecting%20on%20how%20their%20next%20generation%20will%20reshape%0Aconversational%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Complete%2520Survey%2520on%2520LLM-based%2520AI%2520Chatbots%26entry.906535625%3DSumit%2520Kumar%2520Dam%2520and%2520Choong%2520Seon%2520Hong%2520and%2520Yu%2520Qiao%2520and%2520Chaoning%2520Zhang%26entry.1292438233%3D%2520%2520The%2520past%2520few%2520decades%2520have%2520witnessed%2520an%2520upsurge%2520in%2520data%252C%2520forming%2520the%250Afoundation%2520for%2520data-hungry%252C%2520learning-based%2520AI%2520technology.%2520Conversational%250Aagents%252C%2520often%2520referred%2520to%2520as%2520AI%2520chatbots%252C%2520rely%2520heavily%2520on%2520such%2520data%2520to%2520train%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520generate%2520new%2520content%2520%2528knowledge%2529%2520in%2520response%250Ato%2520user%2520prompts.%2520With%2520the%2520advent%2520of%2520OpenAI%2527s%2520ChatGPT%252C%2520LLM-based%2520chatbots%2520have%250Aset%2520new%2520standards%2520in%2520the%2520AI%2520community.%2520This%2520paper%2520presents%2520a%2520complete%2520survey%2520of%250Athe%2520evolution%2520and%2520deployment%2520of%2520LLM-based%2520chatbots%2520in%2520various%2520sectors.%2520We%2520first%250Asummarize%2520the%2520development%2520of%2520foundational%2520chatbots%252C%2520followed%2520by%2520the%2520evolution%250Aof%2520LLMs%252C%2520and%2520then%2520provide%2520an%2520overview%2520of%2520LLM-based%2520chatbots%2520currently%2520in%2520use%250Aand%2520those%2520in%2520the%2520development%2520phase.%2520Recognizing%2520AI%2520chatbots%2520as%2520tools%2520for%250Agenerating%2520new%2520knowledge%252C%2520we%2520explore%2520their%2520diverse%2520applications%2520across%2520various%250Aindustries.%2520We%2520then%2520discuss%2520the%2520open%2520challenges%252C%2520considering%2520how%2520the%2520data%2520used%250Ato%2520train%2520the%2520LLMs%2520and%2520the%2520misuse%2520of%2520the%2520generated%2520knowledge%2520can%2520cause%2520several%250Aissues.%2520Finally%252C%2520we%2520explore%2520the%2520future%2520outlook%2520to%2520augment%2520their%2520efficiency%2520and%250Areliability%2520in%2520numerous%2520applications.%2520By%2520addressing%2520key%2520milestones%2520and%2520the%250Apresent-day%2520context%2520of%2520LLM-based%2520chatbots%252C%2520our%2520survey%2520invites%2520readers%2520to%2520delve%250Adeeper%2520into%2520this%2520realm%252C%2520reflecting%2520on%2520how%2520their%2520next%2520generation%2520will%2520reshape%250Aconversational%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Complete%20Survey%20on%20LLM-based%20AI%20Chatbots&entry.906535625=Sumit%20Kumar%20Dam%20and%20Choong%20Seon%20Hong%20and%20Yu%20Qiao%20and%20Chaoning%20Zhang&entry.1292438233=%20%20The%20past%20few%20decades%20have%20witnessed%20an%20upsurge%20in%20data%2C%20forming%20the%0Afoundation%20for%20data-hungry%2C%20learning-based%20AI%20technology.%20Conversational%0Aagents%2C%20often%20referred%20to%20as%20AI%20chatbots%2C%20rely%20heavily%20on%20such%20data%20to%20train%0Alarge%20language%20models%20%28LLMs%29%20and%20generate%20new%20content%20%28knowledge%29%20in%20response%0Ato%20user%20prompts.%20With%20the%20advent%20of%20OpenAI%27s%20ChatGPT%2C%20LLM-based%20chatbots%20have%0Aset%20new%20standards%20in%20the%20AI%20community.%20This%20paper%20presents%20a%20complete%20survey%20of%0Athe%20evolution%20and%20deployment%20of%20LLM-based%20chatbots%20in%20various%20sectors.%20We%20first%0Asummarize%20the%20development%20of%20foundational%20chatbots%2C%20followed%20by%20the%20evolution%0Aof%20LLMs%2C%20and%20then%20provide%20an%20overview%20of%20LLM-based%20chatbots%20currently%20in%20use%0Aand%20those%20in%20the%20development%20phase.%20Recognizing%20AI%20chatbots%20as%20tools%20for%0Agenerating%20new%20knowledge%2C%20we%20explore%20their%20diverse%20applications%20across%20various%0Aindustries.%20We%20then%20discuss%20the%20open%20challenges%2C%20considering%20how%20the%20data%20used%0Ato%20train%20the%20LLMs%20and%20the%20misuse%20of%20the%20generated%20knowledge%20can%20cause%20several%0Aissues.%20Finally%2C%20we%20explore%20the%20future%20outlook%20to%20augment%20their%20efficiency%20and%0Areliability%20in%20numerous%20applications.%20By%20addressing%20key%20milestones%20and%20the%0Apresent-day%20context%20of%20LLM-based%20chatbots%2C%20our%20survey%20invites%20readers%20to%20delve%0Adeeper%20into%20this%20realm%2C%20reflecting%20on%20how%20their%20next%20generation%20will%20reshape%0Aconversational%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16937v2&entry.124074799=Read"},
{"title": "Conceptwm: A Diffusion Model Watermark for Concept Protection", "author": "Liangqi Lei and Keke Gai and Jing Yu and Liehuang Zhu and Qi Wu", "abstract": "  The personalization techniques of diffusion models succeed in generating\nspecific concepts but also pose threats to copyright protection and illegal\nuse. Model Watermarking is an effective method to prevent the unauthorized use\nof subject-driven or style-driven image generation, safeguarding concept\ncopyrights. However, under the goal of concept-oriented protection, current\nwatermarking schemes typically add watermarks to all images rather than\napplying them in a refined manner targeted at specific concepts. Additionally,\nthe personalization techniques of diffusion models can easily remove\nwatermarks. Existing watermarking methods struggle to achieve fine-grained\nwatermark embedding with a few images of specific concept and prevent removal\nof watermarks through personalized fine-tuning. Therefore, we introduce a novel\nconcept-oriented watermarking framework that seamlessly embeds imperceptible\nwatermarks into the concept of diffusion models. We conduct extensive\nexperiments and ablation studies to verify our framework. Our code is available\nat https://anonymous.4open.science/r/Conceptwm-4EB3/.\n", "link": "http://arxiv.org/abs/2411.11688v1", "date": "2024-11-18", "relevancy": 1.5904, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5349}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5268}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conceptwm%3A%20A%20Diffusion%20Model%20Watermark%20for%20Concept%20Protection&body=Title%3A%20Conceptwm%3A%20A%20Diffusion%20Model%20Watermark%20for%20Concept%20Protection%0AAuthor%3A%20Liangqi%20Lei%20and%20Keke%20Gai%20and%20Jing%20Yu%20and%20Liehuang%20Zhu%20and%20Qi%20Wu%0AAbstract%3A%20%20%20The%20personalization%20techniques%20of%20diffusion%20models%20succeed%20in%20generating%0Aspecific%20concepts%20but%20also%20pose%20threats%20to%20copyright%20protection%20and%20illegal%0Ause.%20Model%20Watermarking%20is%20an%20effective%20method%20to%20prevent%20the%20unauthorized%20use%0Aof%20subject-driven%20or%20style-driven%20image%20generation%2C%20safeguarding%20concept%0Acopyrights.%20However%2C%20under%20the%20goal%20of%20concept-oriented%20protection%2C%20current%0Awatermarking%20schemes%20typically%20add%20watermarks%20to%20all%20images%20rather%20than%0Aapplying%20them%20in%20a%20refined%20manner%20targeted%20at%20specific%20concepts.%20Additionally%2C%0Athe%20personalization%20techniques%20of%20diffusion%20models%20can%20easily%20remove%0Awatermarks.%20Existing%20watermarking%20methods%20struggle%20to%20achieve%20fine-grained%0Awatermark%20embedding%20with%20a%20few%20images%20of%20specific%20concept%20and%20prevent%20removal%0Aof%20watermarks%20through%20personalized%20fine-tuning.%20Therefore%2C%20we%20introduce%20a%20novel%0Aconcept-oriented%20watermarking%20framework%20that%20seamlessly%20embeds%20imperceptible%0Awatermarks%20into%20the%20concept%20of%20diffusion%20models.%20We%20conduct%20extensive%0Aexperiments%20and%20ablation%20studies%20to%20verify%20our%20framework.%20Our%20code%20is%20available%0Aat%20https%3A//anonymous.4open.science/r/Conceptwm-4EB3/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptwm%253A%2520A%2520Diffusion%2520Model%2520Watermark%2520for%2520Concept%2520Protection%26entry.906535625%3DLiangqi%2520Lei%2520and%2520Keke%2520Gai%2520and%2520Jing%2520Yu%2520and%2520Liehuang%2520Zhu%2520and%2520Qi%2520Wu%26entry.1292438233%3D%2520%2520The%2520personalization%2520techniques%2520of%2520diffusion%2520models%2520succeed%2520in%2520generating%250Aspecific%2520concepts%2520but%2520also%2520pose%2520threats%2520to%2520copyright%2520protection%2520and%2520illegal%250Ause.%2520Model%2520Watermarking%2520is%2520an%2520effective%2520method%2520to%2520prevent%2520the%2520unauthorized%2520use%250Aof%2520subject-driven%2520or%2520style-driven%2520image%2520generation%252C%2520safeguarding%2520concept%250Acopyrights.%2520However%252C%2520under%2520the%2520goal%2520of%2520concept-oriented%2520protection%252C%2520current%250Awatermarking%2520schemes%2520typically%2520add%2520watermarks%2520to%2520all%2520images%2520rather%2520than%250Aapplying%2520them%2520in%2520a%2520refined%2520manner%2520targeted%2520at%2520specific%2520concepts.%2520Additionally%252C%250Athe%2520personalization%2520techniques%2520of%2520diffusion%2520models%2520can%2520easily%2520remove%250Awatermarks.%2520Existing%2520watermarking%2520methods%2520struggle%2520to%2520achieve%2520fine-grained%250Awatermark%2520embedding%2520with%2520a%2520few%2520images%2520of%2520specific%2520concept%2520and%2520prevent%2520removal%250Aof%2520watermarks%2520through%2520personalized%2520fine-tuning.%2520Therefore%252C%2520we%2520introduce%2520a%2520novel%250Aconcept-oriented%2520watermarking%2520framework%2520that%2520seamlessly%2520embeds%2520imperceptible%250Awatermarks%2520into%2520the%2520concept%2520of%2520diffusion%2520models.%2520We%2520conduct%2520extensive%250Aexperiments%2520and%2520ablation%2520studies%2520to%2520verify%2520our%2520framework.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//anonymous.4open.science/r/Conceptwm-4EB3/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conceptwm%3A%20A%20Diffusion%20Model%20Watermark%20for%20Concept%20Protection&entry.906535625=Liangqi%20Lei%20and%20Keke%20Gai%20and%20Jing%20Yu%20and%20Liehuang%20Zhu%20and%20Qi%20Wu&entry.1292438233=%20%20The%20personalization%20techniques%20of%20diffusion%20models%20succeed%20in%20generating%0Aspecific%20concepts%20but%20also%20pose%20threats%20to%20copyright%20protection%20and%20illegal%0Ause.%20Model%20Watermarking%20is%20an%20effective%20method%20to%20prevent%20the%20unauthorized%20use%0Aof%20subject-driven%20or%20style-driven%20image%20generation%2C%20safeguarding%20concept%0Acopyrights.%20However%2C%20under%20the%20goal%20of%20concept-oriented%20protection%2C%20current%0Awatermarking%20schemes%20typically%20add%20watermarks%20to%20all%20images%20rather%20than%0Aapplying%20them%20in%20a%20refined%20manner%20targeted%20at%20specific%20concepts.%20Additionally%2C%0Athe%20personalization%20techniques%20of%20diffusion%20models%20can%20easily%20remove%0Awatermarks.%20Existing%20watermarking%20methods%20struggle%20to%20achieve%20fine-grained%0Awatermark%20embedding%20with%20a%20few%20images%20of%20specific%20concept%20and%20prevent%20removal%0Aof%20watermarks%20through%20personalized%20fine-tuning.%20Therefore%2C%20we%20introduce%20a%20novel%0Aconcept-oriented%20watermarking%20framework%20that%20seamlessly%20embeds%20imperceptible%0Awatermarks%20into%20the%20concept%20of%20diffusion%20models.%20We%20conduct%20extensive%0Aexperiments%20and%20ablation%20studies%20to%20verify%20our%20framework.%20Our%20code%20is%20available%0Aat%20https%3A//anonymous.4open.science/r/Conceptwm-4EB3/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11688v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


