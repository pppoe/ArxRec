<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250106.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SCRREAM : SCan, Register, REnder And Map:A Framework for Annotating\n  Accurate and Dense 3D Indoor Scenes with a Benchmark", "author": "HyunJun Jung and Weihang Li and Shun-Cheng Wu and William Bittner and Nikolas Brasch and Jifei Song and Eduardo P\u00e9rez-Pellitero and Zhensong Zhang and Arthur Moreau and Nassir Navab and Benjamin Busam", "abstract": "  Traditionally, 3d indoor datasets have generally prioritized scale over\nground-truth accuracy in order to obtain improved generalization. However,\nusing these datasets to evaluate dense geometry tasks, such as depth rendering,\ncan be problematic as the meshes of the dataset are often incomplete and may\nproduce wrong ground truth to evaluate the details. In this paper, we propose\nSCRREAM, a dataset annotation framework that allows annotation of fully dense\nmeshes of objects in the scene and registers camera poses on the real image\nsequence, which can produce accurate ground truth for both sparse 3D as well as\ndense 3D tasks. We show the details of the dataset annotation pipeline and\nshowcase four possible variants of datasets that can be obtained from our\nframework with example scenes, such as indoor reconstruction and SLAM, scene\nediting & object removal, human reconstruction and 6d pose estimation. Recent\npipelines for indoor reconstruction and SLAM serve as new benchmarks. In\ncontrast to previous indoor dataset, our design allows to evaluate dense\ngeometry tasks on eleven sample scenes against accurately rendered ground truth\ndepth maps.\n", "link": "http://arxiv.org/abs/2410.22715v2", "date": "2025-01-06", "relevancy": 3.0612, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCRREAM%20%3A%20SCan%2C%20Register%2C%20REnder%20And%20Map%3AA%20Framework%20for%20Annotating%0A%20%20Accurate%20and%20Dense%203D%20Indoor%20Scenes%20with%20a%20Benchmark&body=Title%3A%20SCRREAM%20%3A%20SCan%2C%20Register%2C%20REnder%20And%20Map%3AA%20Framework%20for%20Annotating%0A%20%20Accurate%20and%20Dense%203D%20Indoor%20Scenes%20with%20a%20Benchmark%0AAuthor%3A%20HyunJun%20Jung%20and%20Weihang%20Li%20and%20Shun-Cheng%20Wu%20and%20William%20Bittner%20and%20Nikolas%20Brasch%20and%20Jifei%20Song%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Zhensong%20Zhang%20and%20Arthur%20Moreau%20and%20Nassir%20Navab%20and%20Benjamin%20Busam%0AAbstract%3A%20%20%20Traditionally%2C%203d%20indoor%20datasets%20have%20generally%20prioritized%20scale%20over%0Aground-truth%20accuracy%20in%20order%20to%20obtain%20improved%20generalization.%20However%2C%0Ausing%20these%20datasets%20to%20evaluate%20dense%20geometry%20tasks%2C%20such%20as%20depth%20rendering%2C%0Acan%20be%20problematic%20as%20the%20meshes%20of%20the%20dataset%20are%20often%20incomplete%20and%20may%0Aproduce%20wrong%20ground%20truth%20to%20evaluate%20the%20details.%20In%20this%20paper%2C%20we%20propose%0ASCRREAM%2C%20a%20dataset%20annotation%20framework%20that%20allows%20annotation%20of%20fully%20dense%0Ameshes%20of%20objects%20in%20the%20scene%20and%20registers%20camera%20poses%20on%20the%20real%20image%0Asequence%2C%20which%20can%20produce%20accurate%20ground%20truth%20for%20both%20sparse%203D%20as%20well%20as%0Adense%203D%20tasks.%20We%20show%20the%20details%20of%20the%20dataset%20annotation%20pipeline%20and%0Ashowcase%20four%20possible%20variants%20of%20datasets%20that%20can%20be%20obtained%20from%20our%0Aframework%20with%20example%20scenes%2C%20such%20as%20indoor%20reconstruction%20and%20SLAM%2C%20scene%0Aediting%20%26%20object%20removal%2C%20human%20reconstruction%20and%206d%20pose%20estimation.%20Recent%0Apipelines%20for%20indoor%20reconstruction%20and%20SLAM%20serve%20as%20new%20benchmarks.%20In%0Acontrast%20to%20previous%20indoor%20dataset%2C%20our%20design%20allows%20to%20evaluate%20dense%0Ageometry%20tasks%20on%20eleven%20sample%20scenes%20against%20accurately%20rendered%20ground%20truth%0Adepth%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCRREAM%2520%253A%2520SCan%252C%2520Register%252C%2520REnder%2520And%2520Map%253AA%2520Framework%2520for%2520Annotating%250A%2520%2520Accurate%2520and%2520Dense%25203D%2520Indoor%2520Scenes%2520with%2520a%2520Benchmark%26entry.906535625%3DHyunJun%2520Jung%2520and%2520Weihang%2520Li%2520and%2520Shun-Cheng%2520Wu%2520and%2520William%2520Bittner%2520and%2520Nikolas%2520Brasch%2520and%2520Jifei%2520Song%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%2520and%2520Zhensong%2520Zhang%2520and%2520Arthur%2520Moreau%2520and%2520Nassir%2520Navab%2520and%2520Benjamin%2520Busam%26entry.1292438233%3D%2520%2520Traditionally%252C%25203d%2520indoor%2520datasets%2520have%2520generally%2520prioritized%2520scale%2520over%250Aground-truth%2520accuracy%2520in%2520order%2520to%2520obtain%2520improved%2520generalization.%2520However%252C%250Ausing%2520these%2520datasets%2520to%2520evaluate%2520dense%2520geometry%2520tasks%252C%2520such%2520as%2520depth%2520rendering%252C%250Acan%2520be%2520problematic%2520as%2520the%2520meshes%2520of%2520the%2520dataset%2520are%2520often%2520incomplete%2520and%2520may%250Aproduce%2520wrong%2520ground%2520truth%2520to%2520evaluate%2520the%2520details.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASCRREAM%252C%2520a%2520dataset%2520annotation%2520framework%2520that%2520allows%2520annotation%2520of%2520fully%2520dense%250Ameshes%2520of%2520objects%2520in%2520the%2520scene%2520and%2520registers%2520camera%2520poses%2520on%2520the%2520real%2520image%250Asequence%252C%2520which%2520can%2520produce%2520accurate%2520ground%2520truth%2520for%2520both%2520sparse%25203D%2520as%2520well%2520as%250Adense%25203D%2520tasks.%2520We%2520show%2520the%2520details%2520of%2520the%2520dataset%2520annotation%2520pipeline%2520and%250Ashowcase%2520four%2520possible%2520variants%2520of%2520datasets%2520that%2520can%2520be%2520obtained%2520from%2520our%250Aframework%2520with%2520example%2520scenes%252C%2520such%2520as%2520indoor%2520reconstruction%2520and%2520SLAM%252C%2520scene%250Aediting%2520%2526%2520object%2520removal%252C%2520human%2520reconstruction%2520and%25206d%2520pose%2520estimation.%2520Recent%250Apipelines%2520for%2520indoor%2520reconstruction%2520and%2520SLAM%2520serve%2520as%2520new%2520benchmarks.%2520In%250Acontrast%2520to%2520previous%2520indoor%2520dataset%252C%2520our%2520design%2520allows%2520to%2520evaluate%2520dense%250Ageometry%2520tasks%2520on%2520eleven%2520sample%2520scenes%2520against%2520accurately%2520rendered%2520ground%2520truth%250Adepth%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCRREAM%20%3A%20SCan%2C%20Register%2C%20REnder%20And%20Map%3AA%20Framework%20for%20Annotating%0A%20%20Accurate%20and%20Dense%203D%20Indoor%20Scenes%20with%20a%20Benchmark&entry.906535625=HyunJun%20Jung%20and%20Weihang%20Li%20and%20Shun-Cheng%20Wu%20and%20William%20Bittner%20and%20Nikolas%20Brasch%20and%20Jifei%20Song%20and%20Eduardo%20P%C3%A9rez-Pellitero%20and%20Zhensong%20Zhang%20and%20Arthur%20Moreau%20and%20Nassir%20Navab%20and%20Benjamin%20Busam&entry.1292438233=%20%20Traditionally%2C%203d%20indoor%20datasets%20have%20generally%20prioritized%20scale%20over%0Aground-truth%20accuracy%20in%20order%20to%20obtain%20improved%20generalization.%20However%2C%0Ausing%20these%20datasets%20to%20evaluate%20dense%20geometry%20tasks%2C%20such%20as%20depth%20rendering%2C%0Acan%20be%20problematic%20as%20the%20meshes%20of%20the%20dataset%20are%20often%20incomplete%20and%20may%0Aproduce%20wrong%20ground%20truth%20to%20evaluate%20the%20details.%20In%20this%20paper%2C%20we%20propose%0ASCRREAM%2C%20a%20dataset%20annotation%20framework%20that%20allows%20annotation%20of%20fully%20dense%0Ameshes%20of%20objects%20in%20the%20scene%20and%20registers%20camera%20poses%20on%20the%20real%20image%0Asequence%2C%20which%20can%20produce%20accurate%20ground%20truth%20for%20both%20sparse%203D%20as%20well%20as%0Adense%203D%20tasks.%20We%20show%20the%20details%20of%20the%20dataset%20annotation%20pipeline%20and%0Ashowcase%20four%20possible%20variants%20of%20datasets%20that%20can%20be%20obtained%20from%20our%0Aframework%20with%20example%20scenes%2C%20such%20as%20indoor%20reconstruction%20and%20SLAM%2C%20scene%0Aediting%20%26%20object%20removal%2C%20human%20reconstruction%20and%206d%20pose%20estimation.%20Recent%0Apipelines%20for%20indoor%20reconstruction%20and%20SLAM%20serve%20as%20new%20benchmarks.%20In%0Acontrast%20to%20previous%20indoor%20dataset%2C%20our%20design%20allows%20to%20evaluate%20dense%0Ageometry%20tasks%20on%20eleven%20sample%20scenes%20against%20accurately%20rendered%20ground%20truth%0Adepth%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22715v2&entry.124074799=Read"},
{"title": "Gaussian Masked Autoencoders", "author": "Jathushan Rajasegaran and Xinlei Chen and Rulilong Li and Christoph Feichtenhofer and Jitendra Malik and Shiry Ginosar", "abstract": "  This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While\nreconstructive self-supervised learning frameworks such as MAE learns good\nsemantic abstractions, it is not trained for explicit spatial awareness. Our\napproach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic\nabstractions and spatial understanding jointly. Like MAE, it reconstructs the\nimage end-to-end in the pixel space, but beyond MAE, it also introduces an\nintermediate, 3D Gaussian-based representation and renders images via\nsplatting. We show that GMAE can enable various zero-shot learning capabilities\nof spatial understanding (e.g., figure-ground segmentation, image layering,\nedge detection, etc.) while preserving the high-level semantics of\nself-supervised representation quality from MAE. To our knowledge, we are the\nfirst to employ Gaussian primitives in an image representation learning\nframework beyond optimization-based single-scene reconstructions. We believe\nGMAE will inspire further research in this direction and contribute to\ndeveloping next-generation techniques for modeling high-fidelity visual data.\nMore details at https://brjathu.github.io/gmae\n", "link": "http://arxiv.org/abs/2501.03229v1", "date": "2025-01-06", "relevancy": 3.058, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6252}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6149}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Masked%20Autoencoders&body=Title%3A%20Gaussian%20Masked%20Autoencoders%0AAuthor%3A%20Jathushan%20Rajasegaran%20and%20Xinlei%20Chen%20and%20Rulilong%20Li%20and%20Christoph%20Feichtenhofer%20and%20Jitendra%20Malik%20and%20Shiry%20Ginosar%0AAbstract%3A%20%20%20This%20paper%20explores%20Masked%20Autoencoders%20%28MAE%29%20with%20Gaussian%20Splatting.%20While%0Areconstructive%20self-supervised%20learning%20frameworks%20such%20as%20MAE%20learns%20good%0Asemantic%20abstractions%2C%20it%20is%20not%20trained%20for%20explicit%20spatial%20awareness.%20Our%0Aapproach%2C%20named%20Gaussian%20Masked%20Autoencoder%2C%20or%20GMAE%2C%20aims%20to%20learn%20semantic%0Aabstractions%20and%20spatial%20understanding%20jointly.%20Like%20MAE%2C%20it%20reconstructs%20the%0Aimage%20end-to-end%20in%20the%20pixel%20space%2C%20but%20beyond%20MAE%2C%20it%20also%20introduces%20an%0Aintermediate%2C%203D%20Gaussian-based%20representation%20and%20renders%20images%20via%0Asplatting.%20We%20show%20that%20GMAE%20can%20enable%20various%20zero-shot%20learning%20capabilities%0Aof%20spatial%20understanding%20%28e.g.%2C%20figure-ground%20segmentation%2C%20image%20layering%2C%0Aedge%20detection%2C%20etc.%29%20while%20preserving%20the%20high-level%20semantics%20of%0Aself-supervised%20representation%20quality%20from%20MAE.%20To%20our%20knowledge%2C%20we%20are%20the%0Afirst%20to%20employ%20Gaussian%20primitives%20in%20an%20image%20representation%20learning%0Aframework%20beyond%20optimization-based%20single-scene%20reconstructions.%20We%20believe%0AGMAE%20will%20inspire%20further%20research%20in%20this%20direction%20and%20contribute%20to%0Adeveloping%20next-generation%20techniques%20for%20modeling%20high-fidelity%20visual%20data.%0AMore%20details%20at%20https%3A//brjathu.github.io/gmae%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Masked%2520Autoencoders%26entry.906535625%3DJathushan%2520Rajasegaran%2520and%2520Xinlei%2520Chen%2520and%2520Rulilong%2520Li%2520and%2520Christoph%2520Feichtenhofer%2520and%2520Jitendra%2520Malik%2520and%2520Shiry%2520Ginosar%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520Masked%2520Autoencoders%2520%2528MAE%2529%2520with%2520Gaussian%2520Splatting.%2520While%250Areconstructive%2520self-supervised%2520learning%2520frameworks%2520such%2520as%2520MAE%2520learns%2520good%250Asemantic%2520abstractions%252C%2520it%2520is%2520not%2520trained%2520for%2520explicit%2520spatial%2520awareness.%2520Our%250Aapproach%252C%2520named%2520Gaussian%2520Masked%2520Autoencoder%252C%2520or%2520GMAE%252C%2520aims%2520to%2520learn%2520semantic%250Aabstractions%2520and%2520spatial%2520understanding%2520jointly.%2520Like%2520MAE%252C%2520it%2520reconstructs%2520the%250Aimage%2520end-to-end%2520in%2520the%2520pixel%2520space%252C%2520but%2520beyond%2520MAE%252C%2520it%2520also%2520introduces%2520an%250Aintermediate%252C%25203D%2520Gaussian-based%2520representation%2520and%2520renders%2520images%2520via%250Asplatting.%2520We%2520show%2520that%2520GMAE%2520can%2520enable%2520various%2520zero-shot%2520learning%2520capabilities%250Aof%2520spatial%2520understanding%2520%2528e.g.%252C%2520figure-ground%2520segmentation%252C%2520image%2520layering%252C%250Aedge%2520detection%252C%2520etc.%2529%2520while%2520preserving%2520the%2520high-level%2520semantics%2520of%250Aself-supervised%2520representation%2520quality%2520from%2520MAE.%2520To%2520our%2520knowledge%252C%2520we%2520are%2520the%250Afirst%2520to%2520employ%2520Gaussian%2520primitives%2520in%2520an%2520image%2520representation%2520learning%250Aframework%2520beyond%2520optimization-based%2520single-scene%2520reconstructions.%2520We%2520believe%250AGMAE%2520will%2520inspire%2520further%2520research%2520in%2520this%2520direction%2520and%2520contribute%2520to%250Adeveloping%2520next-generation%2520techniques%2520for%2520modeling%2520high-fidelity%2520visual%2520data.%250AMore%2520details%2520at%2520https%253A//brjathu.github.io/gmae%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Masked%20Autoencoders&entry.906535625=Jathushan%20Rajasegaran%20and%20Xinlei%20Chen%20and%20Rulilong%20Li%20and%20Christoph%20Feichtenhofer%20and%20Jitendra%20Malik%20and%20Shiry%20Ginosar&entry.1292438233=%20%20This%20paper%20explores%20Masked%20Autoencoders%20%28MAE%29%20with%20Gaussian%20Splatting.%20While%0Areconstructive%20self-supervised%20learning%20frameworks%20such%20as%20MAE%20learns%20good%0Asemantic%20abstractions%2C%20it%20is%20not%20trained%20for%20explicit%20spatial%20awareness.%20Our%0Aapproach%2C%20named%20Gaussian%20Masked%20Autoencoder%2C%20or%20GMAE%2C%20aims%20to%20learn%20semantic%0Aabstractions%20and%20spatial%20understanding%20jointly.%20Like%20MAE%2C%20it%20reconstructs%20the%0Aimage%20end-to-end%20in%20the%20pixel%20space%2C%20but%20beyond%20MAE%2C%20it%20also%20introduces%20an%0Aintermediate%2C%203D%20Gaussian-based%20representation%20and%20renders%20images%20via%0Asplatting.%20We%20show%20that%20GMAE%20can%20enable%20various%20zero-shot%20learning%20capabilities%0Aof%20spatial%20understanding%20%28e.g.%2C%20figure-ground%20segmentation%2C%20image%20layering%2C%0Aedge%20detection%2C%20etc.%29%20while%20preserving%20the%20high-level%20semantics%20of%0Aself-supervised%20representation%20quality%20from%20MAE.%20To%20our%20knowledge%2C%20we%20are%20the%0Afirst%20to%20employ%20Gaussian%20primitives%20in%20an%20image%20representation%20learning%0Aframework%20beyond%20optimization-based%20single-scene%20reconstructions.%20We%20believe%0AGMAE%20will%20inspire%20further%20research%20in%20this%20direction%20and%20contribute%20to%0Adeveloping%20next-generation%20techniques%20for%20modeling%20high-fidelity%20visual%20data.%0AMore%20details%20at%20https%3A//brjathu.github.io/gmae%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03229v1&entry.124074799=Read"},
{"title": "3D Focusing-and-Matching Network for Multi-Instance Point Cloud\n  Registration", "author": "Liyuan Zhang and Le Hui and Qi Liu and Bo Li and Yuchao Dai", "abstract": "  Multi-instance point cloud registration aims to estimate the pose of all\ninstances of a model point cloud in the whole scene. Existing methods all adopt\nthe strategy of first obtaining the global correspondence and then clustering\nto obtain the pose of each instance. However, due to the cluttered and occluded\nobjects in the scene, it is difficult to obtain an accurate correspondence\nbetween the model point cloud and all instances in the scene. To this end, we\npropose a simple yet powerful 3D focusing-and-matching network for\nmulti-instance point cloud registration by learning the multiple pair-wise\npoint cloud registration. Specifically, we first present a 3D multi-object\nfocusing module to locate the center of each object and generate object\nproposals. By using self-attention and cross-attention to associate the model\npoint cloud with structurally similar objects, we can locate potential matching\ninstances by regressing object centers. Then, we propose a 3D dual masking\ninstance matching module to estimate the pose between the model point cloud and\neach object proposal. It performs instance mask and overlap mask masks to\naccurately predict the pair-wise correspondence. Extensive experiments on two\npublic benchmarks, Scan2CAD and ROBI, show that our method achieves a new\nstate-of-the-art performance on the multi-instance point cloud registration\ntask. Code is available at https://github.com/zlynpu/3DFMNet.\n", "link": "http://arxiv.org/abs/2411.07740v2", "date": "2025-01-06", "relevancy": 2.9842, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration&body=Title%3A%203D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration%0AAuthor%3A%20Liyuan%20Zhang%20and%20Le%20Hui%20and%20Qi%20Liu%20and%20Bo%20Li%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20Multi-instance%20point%20cloud%20registration%20aims%20to%20estimate%20the%20pose%20of%20all%0Ainstances%20of%20a%20model%20point%20cloud%20in%20the%20whole%20scene.%20Existing%20methods%20all%20adopt%0Athe%20strategy%20of%20first%20obtaining%20the%20global%20correspondence%20and%20then%20clustering%0Ato%20obtain%20the%20pose%20of%20each%20instance.%20However%2C%20due%20to%20the%20cluttered%20and%20occluded%0Aobjects%20in%20the%20scene%2C%20it%20is%20difficult%20to%20obtain%20an%20accurate%20correspondence%0Abetween%20the%20model%20point%20cloud%20and%20all%20instances%20in%20the%20scene.%20To%20this%20end%2C%20we%0Apropose%20a%20simple%20yet%20powerful%203D%20focusing-and-matching%20network%20for%0Amulti-instance%20point%20cloud%20registration%20by%20learning%20the%20multiple%20pair-wise%0Apoint%20cloud%20registration.%20Specifically%2C%20we%20first%20present%20a%203D%20multi-object%0Afocusing%20module%20to%20locate%20the%20center%20of%20each%20object%20and%20generate%20object%0Aproposals.%20By%20using%20self-attention%20and%20cross-attention%20to%20associate%20the%20model%0Apoint%20cloud%20with%20structurally%20similar%20objects%2C%20we%20can%20locate%20potential%20matching%0Ainstances%20by%20regressing%20object%20centers.%20Then%2C%20we%20propose%20a%203D%20dual%20masking%0Ainstance%20matching%20module%20to%20estimate%20the%20pose%20between%20the%20model%20point%20cloud%20and%0Aeach%20object%20proposal.%20It%20performs%20instance%20mask%20and%20overlap%20mask%20masks%20to%0Aaccurately%20predict%20the%20pair-wise%20correspondence.%20Extensive%20experiments%20on%20two%0Apublic%20benchmarks%2C%20Scan2CAD%20and%20ROBI%2C%20show%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance%20on%20the%20multi-instance%20point%20cloud%20registration%0Atask.%20Code%20is%20available%20at%20https%3A//github.com/zlynpu/3DFMNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Focusing-and-Matching%2520Network%2520for%2520Multi-Instance%2520Point%2520Cloud%250A%2520%2520Registration%26entry.906535625%3DLiyuan%2520Zhang%2520and%2520Le%2520Hui%2520and%2520Qi%2520Liu%2520and%2520Bo%2520Li%2520and%2520Yuchao%2520Dai%26entry.1292438233%3D%2520%2520Multi-instance%2520point%2520cloud%2520registration%2520aims%2520to%2520estimate%2520the%2520pose%2520of%2520all%250Ainstances%2520of%2520a%2520model%2520point%2520cloud%2520in%2520the%2520whole%2520scene.%2520Existing%2520methods%2520all%2520adopt%250Athe%2520strategy%2520of%2520first%2520obtaining%2520the%2520global%2520correspondence%2520and%2520then%2520clustering%250Ato%2520obtain%2520the%2520pose%2520of%2520each%2520instance.%2520However%252C%2520due%2520to%2520the%2520cluttered%2520and%2520occluded%250Aobjects%2520in%2520the%2520scene%252C%2520it%2520is%2520difficult%2520to%2520obtain%2520an%2520accurate%2520correspondence%250Abetween%2520the%2520model%2520point%2520cloud%2520and%2520all%2520instances%2520in%2520the%2520scene.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520powerful%25203D%2520focusing-and-matching%2520network%2520for%250Amulti-instance%2520point%2520cloud%2520registration%2520by%2520learning%2520the%2520multiple%2520pair-wise%250Apoint%2520cloud%2520registration.%2520Specifically%252C%2520we%2520first%2520present%2520a%25203D%2520multi-object%250Afocusing%2520module%2520to%2520locate%2520the%2520center%2520of%2520each%2520object%2520and%2520generate%2520object%250Aproposals.%2520By%2520using%2520self-attention%2520and%2520cross-attention%2520to%2520associate%2520the%2520model%250Apoint%2520cloud%2520with%2520structurally%2520similar%2520objects%252C%2520we%2520can%2520locate%2520potential%2520matching%250Ainstances%2520by%2520regressing%2520object%2520centers.%2520Then%252C%2520we%2520propose%2520a%25203D%2520dual%2520masking%250Ainstance%2520matching%2520module%2520to%2520estimate%2520the%2520pose%2520between%2520the%2520model%2520point%2520cloud%2520and%250Aeach%2520object%2520proposal.%2520It%2520performs%2520instance%2520mask%2520and%2520overlap%2520mask%2520masks%2520to%250Aaccurately%2520predict%2520the%2520pair-wise%2520correspondence.%2520Extensive%2520experiments%2520on%2520two%250Apublic%2520benchmarks%252C%2520Scan2CAD%2520and%2520ROBI%252C%2520show%2520that%2520our%2520method%2520achieves%2520a%2520new%250Astate-of-the-art%2520performance%2520on%2520the%2520multi-instance%2520point%2520cloud%2520registration%250Atask.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/zlynpu/3DFMNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration&entry.906535625=Liyuan%20Zhang%20and%20Le%20Hui%20and%20Qi%20Liu%20and%20Bo%20Li%20and%20Yuchao%20Dai&entry.1292438233=%20%20Multi-instance%20point%20cloud%20registration%20aims%20to%20estimate%20the%20pose%20of%20all%0Ainstances%20of%20a%20model%20point%20cloud%20in%20the%20whole%20scene.%20Existing%20methods%20all%20adopt%0Athe%20strategy%20of%20first%20obtaining%20the%20global%20correspondence%20and%20then%20clustering%0Ato%20obtain%20the%20pose%20of%20each%20instance.%20However%2C%20due%20to%20the%20cluttered%20and%20occluded%0Aobjects%20in%20the%20scene%2C%20it%20is%20difficult%20to%20obtain%20an%20accurate%20correspondence%0Abetween%20the%20model%20point%20cloud%20and%20all%20instances%20in%20the%20scene.%20To%20this%20end%2C%20we%0Apropose%20a%20simple%20yet%20powerful%203D%20focusing-and-matching%20network%20for%0Amulti-instance%20point%20cloud%20registration%20by%20learning%20the%20multiple%20pair-wise%0Apoint%20cloud%20registration.%20Specifically%2C%20we%20first%20present%20a%203D%20multi-object%0Afocusing%20module%20to%20locate%20the%20center%20of%20each%20object%20and%20generate%20object%0Aproposals.%20By%20using%20self-attention%20and%20cross-attention%20to%20associate%20the%20model%0Apoint%20cloud%20with%20structurally%20similar%20objects%2C%20we%20can%20locate%20potential%20matching%0Ainstances%20by%20regressing%20object%20centers.%20Then%2C%20we%20propose%20a%203D%20dual%20masking%0Ainstance%20matching%20module%20to%20estimate%20the%20pose%20between%20the%20model%20point%20cloud%20and%0Aeach%20object%20proposal.%20It%20performs%20instance%20mask%20and%20overlap%20mask%20masks%20to%0Aaccurately%20predict%20the%20pair-wise%20correspondence.%20Extensive%20experiments%20on%20two%0Apublic%20benchmarks%2C%20Scan2CAD%20and%20ROBI%2C%20show%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance%20on%20the%20multi-instance%20point%20cloud%20registration%0Atask.%20Code%20is%20available%20at%20https%3A//github.com/zlynpu/3DFMNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07740v2&entry.124074799=Read"},
{"title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering\n  alignment", "author": "Pegah Khayatan and Mustafa Shukor and Jayneel Parekh and Matthieu Cord", "abstract": "  Multimodal LLMs have reached remarkable levels of proficiency in\nunderstanding multimodal inputs, driving extensive research to develop\nincreasingly powerful models. However, much less attention has been paid to\nunderstanding and explaining the underlying mechanisms of these models. Most\nexisting explainability research examines these models only in their final\nstates, overlooking the dynamic representational shifts that occur during\ntraining. In this work, we systematically analyze the evolution of hidden state\nrepresentations to reveal how fine-tuning alters the internal structure of a\nmodel to specialize in new multimodal tasks. Using a concept-based approach, we\nmap hidden states to interpretable visual and textual concepts, enabling us to\ntrace changes in encoded concepts across modalities as training progresses. We\nalso demonstrate the use of shift vectors to capture these concepts changes.\nThese shift vectors allow us to recover fine-tuned concepts by shifting those\nin the original model. Finally, we explore the practical impact of our findings\non model steering, showing that we can adjust multimodal LLMs behaviors without\nany training, such as modifying answer types, captions style, or biasing the\nmodel toward specific responses. Our work sheds light on how multimodal\nrepresentations evolve through fine-tuning and offers a new perspective for\ninterpreting model adaptation in multimodal tasks. The code for this project is\npublicly available at https://github.com/mshukor/xl-vlms.\n", "link": "http://arxiv.org/abs/2501.03012v1", "date": "2025-01-06", "relevancy": 2.8724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Fine-tuning%20Representation%20Shift%20for%20Multimodal%20LLMs%20Steering%0A%20%20alignment&body=Title%3A%20Analyzing%20Fine-tuning%20Representation%20Shift%20for%20Multimodal%20LLMs%20Steering%0A%20%20alignment%0AAuthor%3A%20Pegah%20Khayatan%20and%20Mustafa%20Shukor%20and%20Jayneel%20Parekh%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Multimodal%20LLMs%20have%20reached%20remarkable%20levels%20of%20proficiency%20in%0Aunderstanding%20multimodal%20inputs%2C%20driving%20extensive%20research%20to%20develop%0Aincreasingly%20powerful%20models.%20However%2C%20much%20less%20attention%20has%20been%20paid%20to%0Aunderstanding%20and%20explaining%20the%20underlying%20mechanisms%20of%20these%20models.%20Most%0Aexisting%20explainability%20research%20examines%20these%20models%20only%20in%20their%20final%0Astates%2C%20overlooking%20the%20dynamic%20representational%20shifts%20that%20occur%20during%0Atraining.%20In%20this%20work%2C%20we%20systematically%20analyze%20the%20evolution%20of%20hidden%20state%0Arepresentations%20to%20reveal%20how%20fine-tuning%20alters%20the%20internal%20structure%20of%20a%0Amodel%20to%20specialize%20in%20new%20multimodal%20tasks.%20Using%20a%20concept-based%20approach%2C%20we%0Amap%20hidden%20states%20to%20interpretable%20visual%20and%20textual%20concepts%2C%20enabling%20us%20to%0Atrace%20changes%20in%20encoded%20concepts%20across%20modalities%20as%20training%20progresses.%20We%0Aalso%20demonstrate%20the%20use%20of%20shift%20vectors%20to%20capture%20these%20concepts%20changes.%0AThese%20shift%20vectors%20allow%20us%20to%20recover%20fine-tuned%20concepts%20by%20shifting%20those%0Ain%20the%20original%20model.%20Finally%2C%20we%20explore%20the%20practical%20impact%20of%20our%20findings%0Aon%20model%20steering%2C%20showing%20that%20we%20can%20adjust%20multimodal%20LLMs%20behaviors%20without%0Aany%20training%2C%20such%20as%20modifying%20answer%20types%2C%20captions%20style%2C%20or%20biasing%20the%0Amodel%20toward%20specific%20responses.%20Our%20work%20sheds%20light%20on%20how%20multimodal%0Arepresentations%20evolve%20through%20fine-tuning%20and%20offers%20a%20new%20perspective%20for%0Ainterpreting%20model%20adaptation%20in%20multimodal%20tasks.%20The%20code%20for%20this%20project%20is%0Apublicly%20available%20at%20https%3A//github.com/mshukor/xl-vlms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Fine-tuning%2520Representation%2520Shift%2520for%2520Multimodal%2520LLMs%2520Steering%250A%2520%2520alignment%26entry.906535625%3DPegah%2520Khayatan%2520and%2520Mustafa%2520Shukor%2520and%2520Jayneel%2520Parekh%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Multimodal%2520LLMs%2520have%2520reached%2520remarkable%2520levels%2520of%2520proficiency%2520in%250Aunderstanding%2520multimodal%2520inputs%252C%2520driving%2520extensive%2520research%2520to%2520develop%250Aincreasingly%2520powerful%2520models.%2520However%252C%2520much%2520less%2520attention%2520has%2520been%2520paid%2520to%250Aunderstanding%2520and%2520explaining%2520the%2520underlying%2520mechanisms%2520of%2520these%2520models.%2520Most%250Aexisting%2520explainability%2520research%2520examines%2520these%2520models%2520only%2520in%2520their%2520final%250Astates%252C%2520overlooking%2520the%2520dynamic%2520representational%2520shifts%2520that%2520occur%2520during%250Atraining.%2520In%2520this%2520work%252C%2520we%2520systematically%2520analyze%2520the%2520evolution%2520of%2520hidden%2520state%250Arepresentations%2520to%2520reveal%2520how%2520fine-tuning%2520alters%2520the%2520internal%2520structure%2520of%2520a%250Amodel%2520to%2520specialize%2520in%2520new%2520multimodal%2520tasks.%2520Using%2520a%2520concept-based%2520approach%252C%2520we%250Amap%2520hidden%2520states%2520to%2520interpretable%2520visual%2520and%2520textual%2520concepts%252C%2520enabling%2520us%2520to%250Atrace%2520changes%2520in%2520encoded%2520concepts%2520across%2520modalities%2520as%2520training%2520progresses.%2520We%250Aalso%2520demonstrate%2520the%2520use%2520of%2520shift%2520vectors%2520to%2520capture%2520these%2520concepts%2520changes.%250AThese%2520shift%2520vectors%2520allow%2520us%2520to%2520recover%2520fine-tuned%2520concepts%2520by%2520shifting%2520those%250Ain%2520the%2520original%2520model.%2520Finally%252C%2520we%2520explore%2520the%2520practical%2520impact%2520of%2520our%2520findings%250Aon%2520model%2520steering%252C%2520showing%2520that%2520we%2520can%2520adjust%2520multimodal%2520LLMs%2520behaviors%2520without%250Aany%2520training%252C%2520such%2520as%2520modifying%2520answer%2520types%252C%2520captions%2520style%252C%2520or%2520biasing%2520the%250Amodel%2520toward%2520specific%2520responses.%2520Our%2520work%2520sheds%2520light%2520on%2520how%2520multimodal%250Arepresentations%2520evolve%2520through%2520fine-tuning%2520and%2520offers%2520a%2520new%2520perspective%2520for%250Ainterpreting%2520model%2520adaptation%2520in%2520multimodal%2520tasks.%2520The%2520code%2520for%2520this%2520project%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/mshukor/xl-vlms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Fine-tuning%20Representation%20Shift%20for%20Multimodal%20LLMs%20Steering%0A%20%20alignment&entry.906535625=Pegah%20Khayatan%20and%20Mustafa%20Shukor%20and%20Jayneel%20Parekh%20and%20Matthieu%20Cord&entry.1292438233=%20%20Multimodal%20LLMs%20have%20reached%20remarkable%20levels%20of%20proficiency%20in%0Aunderstanding%20multimodal%20inputs%2C%20driving%20extensive%20research%20to%20develop%0Aincreasingly%20powerful%20models.%20However%2C%20much%20less%20attention%20has%20been%20paid%20to%0Aunderstanding%20and%20explaining%20the%20underlying%20mechanisms%20of%20these%20models.%20Most%0Aexisting%20explainability%20research%20examines%20these%20models%20only%20in%20their%20final%0Astates%2C%20overlooking%20the%20dynamic%20representational%20shifts%20that%20occur%20during%0Atraining.%20In%20this%20work%2C%20we%20systematically%20analyze%20the%20evolution%20of%20hidden%20state%0Arepresentations%20to%20reveal%20how%20fine-tuning%20alters%20the%20internal%20structure%20of%20a%0Amodel%20to%20specialize%20in%20new%20multimodal%20tasks.%20Using%20a%20concept-based%20approach%2C%20we%0Amap%20hidden%20states%20to%20interpretable%20visual%20and%20textual%20concepts%2C%20enabling%20us%20to%0Atrace%20changes%20in%20encoded%20concepts%20across%20modalities%20as%20training%20progresses.%20We%0Aalso%20demonstrate%20the%20use%20of%20shift%20vectors%20to%20capture%20these%20concepts%20changes.%0AThese%20shift%20vectors%20allow%20us%20to%20recover%20fine-tuned%20concepts%20by%20shifting%20those%0Ain%20the%20original%20model.%20Finally%2C%20we%20explore%20the%20practical%20impact%20of%20our%20findings%0Aon%20model%20steering%2C%20showing%20that%20we%20can%20adjust%20multimodal%20LLMs%20behaviors%20without%0Aany%20training%2C%20such%20as%20modifying%20answer%20types%2C%20captions%20style%2C%20or%20biasing%20the%0Amodel%20toward%20specific%20responses.%20Our%20work%20sheds%20light%20on%20how%20multimodal%0Arepresentations%20evolve%20through%20fine-tuning%20and%20offers%20a%20new%20perspective%20for%0Ainterpreting%20model%20adaptation%20in%20multimodal%20tasks.%20The%20code%20for%20this%20project%20is%0Apublicly%20available%20at%20https%3A//github.com/mshukor/xl-vlms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03012v1&entry.124074799=Read"},
{"title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction", "author": "Rui Qian and Shuangrui Ding and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https://github.com/Mark12Ding/Dispider}.\n", "link": "http://arxiv.org/abs/2501.03218v1", "date": "2025-01-06", "relevancy": 2.8091, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dispider%3A%20Enabling%20Video%20LLMs%20with%20Active%20Real-Time%20Interaction%20via%0A%20%20Disentangled%20Perception%2C%20Decision%2C%20and%20Reaction&body=Title%3A%20Dispider%3A%20Enabling%20Video%20LLMs%20with%20Active%20Real-Time%20Interaction%20via%0A%20%20Disentangled%20Perception%2C%20Decision%2C%20and%20Reaction%0AAuthor%3A%20Rui%20Qian%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Active%20Real-time%20interaction%20with%20video%20LLMs%20introduces%20a%20new%20paradigm%20for%0Ahuman-computer%20interaction%2C%20where%20the%20model%20not%20only%20understands%20user%20intent%0Abut%20also%20responds%20while%20continuously%20processing%20streaming%20video%20on%20the%20fly.%0AUnlike%20offline%20video%20LLMs%2C%20which%20analyze%20the%20entire%20video%20before%20answering%0Aquestions%2C%20active%20real-time%20interaction%20requires%20three%20capabilities%3A%201%29%0APerception%3A%20real-time%20video%20monitoring%20and%20interaction%20capturing.%202%29%20Decision%3A%0Araising%20proactive%20interaction%20in%20proper%20situations%2C%203%29%20Reaction%3A%20continuous%0Ainteraction%20with%20users.%20However%2C%20inherent%20conflicts%20exist%20among%20the%20desired%0Acapabilities.%20The%20Decision%20and%20Reaction%20require%20a%20contrary%20Perception%20scale%20and%0Agrain%2C%20and%20the%20autoregressive%20decoding%20blocks%20the%20real-time%20Perception%20and%0ADecision%20during%20the%20Reaction.%20To%20unify%20the%20conflicted%20capabilities%20within%20a%0Aharmonious%20system%2C%20we%20present%20Dispider%2C%20a%20system%20that%20disentangles%20Perception%2C%0ADecision%2C%20and%20Reaction.%20Dispider%20features%20a%20lightweight%20proactive%20streaming%0Avideo%20processing%20module%20that%20tracks%20the%20video%20stream%20and%20identifies%20optimal%0Amoments%20for%20interaction.%20Once%20the%20interaction%20is%20triggered%2C%20an%20asynchronous%0Ainteraction%20module%20provides%20detailed%20responses%2C%20while%20the%20processing%20module%0Acontinues%20to%20monitor%20the%20video%20in%20the%20meantime.%20Our%20disentangled%20and%0Aasynchronous%20design%20ensures%20timely%2C%20contextually%20accurate%2C%20and%20computationally%0Aefficient%20responses%2C%20making%20Dispider%20ideal%20for%20active%20real-time%20interaction%20for%0Along-duration%20video%20streams.%20Experiments%20show%20that%20Dispider%20not%20only%20maintains%0Astrong%20performance%20in%20conventional%20video%20QA%20tasks%2C%20but%20also%20significantly%0Asurpasses%20previous%20online%20models%20in%20streaming%20scenario%20responses%2C%20thereby%0Avalidating%20the%20effectiveness%20of%20our%20architecture.%20The%20code%20and%20model%20are%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/Mark12Ding/Dispider%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDispider%253A%2520Enabling%2520Video%2520LLMs%2520with%2520Active%2520Real-Time%2520Interaction%2520via%250A%2520%2520Disentangled%2520Perception%252C%2520Decision%252C%2520and%2520Reaction%26entry.906535625%3DRui%2520Qian%2520and%2520Shuangrui%2520Ding%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Active%2520Real-time%2520interaction%2520with%2520video%2520LLMs%2520introduces%2520a%2520new%2520paradigm%2520for%250Ahuman-computer%2520interaction%252C%2520where%2520the%2520model%2520not%2520only%2520understands%2520user%2520intent%250Abut%2520also%2520responds%2520while%2520continuously%2520processing%2520streaming%2520video%2520on%2520the%2520fly.%250AUnlike%2520offline%2520video%2520LLMs%252C%2520which%2520analyze%2520the%2520entire%2520video%2520before%2520answering%250Aquestions%252C%2520active%2520real-time%2520interaction%2520requires%2520three%2520capabilities%253A%25201%2529%250APerception%253A%2520real-time%2520video%2520monitoring%2520and%2520interaction%2520capturing.%25202%2529%2520Decision%253A%250Araising%2520proactive%2520interaction%2520in%2520proper%2520situations%252C%25203%2529%2520Reaction%253A%2520continuous%250Ainteraction%2520with%2520users.%2520However%252C%2520inherent%2520conflicts%2520exist%2520among%2520the%2520desired%250Acapabilities.%2520The%2520Decision%2520and%2520Reaction%2520require%2520a%2520contrary%2520Perception%2520scale%2520and%250Agrain%252C%2520and%2520the%2520autoregressive%2520decoding%2520blocks%2520the%2520real-time%2520Perception%2520and%250ADecision%2520during%2520the%2520Reaction.%2520To%2520unify%2520the%2520conflicted%2520capabilities%2520within%2520a%250Aharmonious%2520system%252C%2520we%2520present%2520Dispider%252C%2520a%2520system%2520that%2520disentangles%2520Perception%252C%250ADecision%252C%2520and%2520Reaction.%2520Dispider%2520features%2520a%2520lightweight%2520proactive%2520streaming%250Avideo%2520processing%2520module%2520that%2520tracks%2520the%2520video%2520stream%2520and%2520identifies%2520optimal%250Amoments%2520for%2520interaction.%2520Once%2520the%2520interaction%2520is%2520triggered%252C%2520an%2520asynchronous%250Ainteraction%2520module%2520provides%2520detailed%2520responses%252C%2520while%2520the%2520processing%2520module%250Acontinues%2520to%2520monitor%2520the%2520video%2520in%2520the%2520meantime.%2520Our%2520disentangled%2520and%250Aasynchronous%2520design%2520ensures%2520timely%252C%2520contextually%2520accurate%252C%2520and%2520computationally%250Aefficient%2520responses%252C%2520making%2520Dispider%2520ideal%2520for%2520active%2520real-time%2520interaction%2520for%250Along-duration%2520video%2520streams.%2520Experiments%2520show%2520that%2520Dispider%2520not%2520only%2520maintains%250Astrong%2520performance%2520in%2520conventional%2520video%2520QA%2520tasks%252C%2520but%2520also%2520significantly%250Asurpasses%2520previous%2520online%2520models%2520in%2520streaming%2520scenario%2520responses%252C%2520thereby%250Avalidating%2520the%2520effectiveness%2520of%2520our%2520architecture.%2520The%2520code%2520and%2520model%2520are%250Areleased%2520at%2520%255Curl%257Bhttps%253A//github.com/Mark12Ding/Dispider%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dispider%3A%20Enabling%20Video%20LLMs%20with%20Active%20Real-Time%20Interaction%20via%0A%20%20Disentangled%20Perception%2C%20Decision%2C%20and%20Reaction&entry.906535625=Rui%20Qian%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Active%20Real-time%20interaction%20with%20video%20LLMs%20introduces%20a%20new%20paradigm%20for%0Ahuman-computer%20interaction%2C%20where%20the%20model%20not%20only%20understands%20user%20intent%0Abut%20also%20responds%20while%20continuously%20processing%20streaming%20video%20on%20the%20fly.%0AUnlike%20offline%20video%20LLMs%2C%20which%20analyze%20the%20entire%20video%20before%20answering%0Aquestions%2C%20active%20real-time%20interaction%20requires%20three%20capabilities%3A%201%29%0APerception%3A%20real-time%20video%20monitoring%20and%20interaction%20capturing.%202%29%20Decision%3A%0Araising%20proactive%20interaction%20in%20proper%20situations%2C%203%29%20Reaction%3A%20continuous%0Ainteraction%20with%20users.%20However%2C%20inherent%20conflicts%20exist%20among%20the%20desired%0Acapabilities.%20The%20Decision%20and%20Reaction%20require%20a%20contrary%20Perception%20scale%20and%0Agrain%2C%20and%20the%20autoregressive%20decoding%20blocks%20the%20real-time%20Perception%20and%0ADecision%20during%20the%20Reaction.%20To%20unify%20the%20conflicted%20capabilities%20within%20a%0Aharmonious%20system%2C%20we%20present%20Dispider%2C%20a%20system%20that%20disentangles%20Perception%2C%0ADecision%2C%20and%20Reaction.%20Dispider%20features%20a%20lightweight%20proactive%20streaming%0Avideo%20processing%20module%20that%20tracks%20the%20video%20stream%20and%20identifies%20optimal%0Amoments%20for%20interaction.%20Once%20the%20interaction%20is%20triggered%2C%20an%20asynchronous%0Ainteraction%20module%20provides%20detailed%20responses%2C%20while%20the%20processing%20module%0Acontinues%20to%20monitor%20the%20video%20in%20the%20meantime.%20Our%20disentangled%20and%0Aasynchronous%20design%20ensures%20timely%2C%20contextually%20accurate%2C%20and%20computationally%0Aefficient%20responses%2C%20making%20Dispider%20ideal%20for%20active%20real-time%20interaction%20for%0Along-duration%20video%20streams.%20Experiments%20show%20that%20Dispider%20not%20only%20maintains%0Astrong%20performance%20in%20conventional%20video%20QA%20tasks%2C%20but%20also%20significantly%0Asurpasses%20previous%20online%20models%20in%20streaming%20scenario%20responses%2C%20thereby%0Avalidating%20the%20effectiveness%20of%20our%20architecture.%20The%20code%20and%20model%20are%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/Mark12Ding/Dispider%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03218v1&entry.124074799=Read"},
{"title": "CAT: Content-Adaptive Image Tokenization", "author": "Junhong Shen and Kushal Tirumala and Michihiro Yasunaga and Ishan Misra and Luke Zettlemoyer and Lili Yu and Chunting Zhou", "abstract": "  Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%.\n", "link": "http://arxiv.org/abs/2501.03120v1", "date": "2025-01-06", "relevancy": 2.8072, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20Content-Adaptive%20Image%20Tokenization&body=Title%3A%20CAT%3A%20Content-Adaptive%20Image%20Tokenization%0AAuthor%3A%20Junhong%20Shen%20and%20Kushal%20Tirumala%20and%20Michihiro%20Yasunaga%20and%20Ishan%20Misra%20and%20Luke%20Zettlemoyer%20and%20Lili%20Yu%20and%20Chunting%20Zhou%0AAbstract%3A%20%20%20Most%20existing%20image%20tokenizers%20encode%20images%20into%20a%20fixed%20number%20of%20tokens%20or%0Apatches%2C%20overlooking%20the%20inherent%20variability%20in%20image%20complexity.%20To%20address%0Athis%2C%20we%20introduce%20Content-Adaptive%20Tokenizer%20%28CAT%29%2C%20which%20dynamically%20adjusts%0Arepresentation%20capacity%20based%20on%20the%20image%20content%20and%20encodes%20simpler%20images%0Ainto%20fewer%20tokens.%20We%20design%20a%20caption-based%20evaluation%20system%20that%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20predict%20content%20complexity%20and%20determine%20the%0Aoptimal%20compression%20ratio%20for%20a%20given%20image%2C%20taking%20into%20account%20factors%0Acritical%20to%20human%20perception.%20Trained%20on%20images%20with%20diverse%20compression%0Aratios%2C%20CAT%20demonstrates%20robust%20performance%20in%20image%20reconstruction.%20We%20also%0Autilize%20its%20variable-length%20latent%20representations%20to%20train%20Diffusion%0ATransformers%20%28DiTs%29%20for%20ImageNet%20generation.%20By%20optimizing%20token%20allocation%2C%0ACAT%20improves%20the%20FID%20score%20over%20fixed-ratio%20baselines%20trained%20with%20the%20same%0Aflops%20and%20boosts%20the%20inference%20throughput%20by%2018.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520Content-Adaptive%2520Image%2520Tokenization%26entry.906535625%3DJunhong%2520Shen%2520and%2520Kushal%2520Tirumala%2520and%2520Michihiro%2520Yasunaga%2520and%2520Ishan%2520Misra%2520and%2520Luke%2520Zettlemoyer%2520and%2520Lili%2520Yu%2520and%2520Chunting%2520Zhou%26entry.1292438233%3D%2520%2520Most%2520existing%2520image%2520tokenizers%2520encode%2520images%2520into%2520a%2520fixed%2520number%2520of%2520tokens%2520or%250Apatches%252C%2520overlooking%2520the%2520inherent%2520variability%2520in%2520image%2520complexity.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520Content-Adaptive%2520Tokenizer%2520%2528CAT%2529%252C%2520which%2520dynamically%2520adjusts%250Arepresentation%2520capacity%2520based%2520on%2520the%2520image%2520content%2520and%2520encodes%2520simpler%2520images%250Ainto%2520fewer%2520tokens.%2520We%2520design%2520a%2520caption-based%2520evaluation%2520system%2520that%2520leverages%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520predict%2520content%2520complexity%2520and%2520determine%2520the%250Aoptimal%2520compression%2520ratio%2520for%2520a%2520given%2520image%252C%2520taking%2520into%2520account%2520factors%250Acritical%2520to%2520human%2520perception.%2520Trained%2520on%2520images%2520with%2520diverse%2520compression%250Aratios%252C%2520CAT%2520demonstrates%2520robust%2520performance%2520in%2520image%2520reconstruction.%2520We%2520also%250Autilize%2520its%2520variable-length%2520latent%2520representations%2520to%2520train%2520Diffusion%250ATransformers%2520%2528DiTs%2529%2520for%2520ImageNet%2520generation.%2520By%2520optimizing%2520token%2520allocation%252C%250ACAT%2520improves%2520the%2520FID%2520score%2520over%2520fixed-ratio%2520baselines%2520trained%2520with%2520the%2520same%250Aflops%2520and%2520boosts%2520the%2520inference%2520throughput%2520by%252018.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20Content-Adaptive%20Image%20Tokenization&entry.906535625=Junhong%20Shen%20and%20Kushal%20Tirumala%20and%20Michihiro%20Yasunaga%20and%20Ishan%20Misra%20and%20Luke%20Zettlemoyer%20and%20Lili%20Yu%20and%20Chunting%20Zhou&entry.1292438233=%20%20Most%20existing%20image%20tokenizers%20encode%20images%20into%20a%20fixed%20number%20of%20tokens%20or%0Apatches%2C%20overlooking%20the%20inherent%20variability%20in%20image%20complexity.%20To%20address%0Athis%2C%20we%20introduce%20Content-Adaptive%20Tokenizer%20%28CAT%29%2C%20which%20dynamically%20adjusts%0Arepresentation%20capacity%20based%20on%20the%20image%20content%20and%20encodes%20simpler%20images%0Ainto%20fewer%20tokens.%20We%20design%20a%20caption-based%20evaluation%20system%20that%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20predict%20content%20complexity%20and%20determine%20the%0Aoptimal%20compression%20ratio%20for%20a%20given%20image%2C%20taking%20into%20account%20factors%0Acritical%20to%20human%20perception.%20Trained%20on%20images%20with%20diverse%20compression%0Aratios%2C%20CAT%20demonstrates%20robust%20performance%20in%20image%20reconstruction.%20We%20also%0Autilize%20its%20variable-length%20latent%20representations%20to%20train%20Diffusion%0ATransformers%20%28DiTs%29%20for%20ImageNet%20generation.%20By%20optimizing%20token%20allocation%2C%0ACAT%20improves%20the%20FID%20score%20over%20fixed-ratio%20baselines%20trained%20with%20the%20same%0Aflops%20and%20boosts%20the%20inference%20throughput%20by%2018.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03120v1&entry.124074799=Read"},
{"title": "HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos", "author": "Jinglei Zhang and Jiankang Deng and Chao Ma and Rolandos Alexandros Potamias", "abstract": "  Despite the advent in 3D hand pose estimation, current methods predominantly\nfocus on single-image 3D hand reconstruction in the camera frame, overlooking\nthe world-space motion of the hands. Such limitation prohibits their direct use\nin egocentric video settings, where hands and camera are continuously in\nmotion. In this work, we propose HaWoR, a high-fidelity method for hand motion\nreconstruction in world coordinates from egocentric videos. We propose to\ndecouple the task by reconstructing the hand motion in the camera space and\nestimating the camera trajectory in the world coordinate system. To achieve\nprecise camera trajectory estimation, we propose an adaptive egocentric SLAM\nframework that addresses the shortcomings of traditional SLAM methods,\nproviding robust performance under challenging camera dynamics. To ensure\nrobust hand motion trajectories, even when the hands move out of view frustum,\nwe devise a novel motion infiller network that effectively completes the\nmissing frames of the sequence. Through extensive quantitative and qualitative\nevaluations, we demonstrate that HaWoR achieves state-of-the-art performance on\nboth hand motion reconstruction and world-frame camera trajectory estimation\nunder different egocentric benchmark datasets. Code and models are available on\nhttps://hawor-project.github.io/ .\n", "link": "http://arxiv.org/abs/2501.02973v1", "date": "2025-01-06", "relevancy": 2.8057, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5842}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5624}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HaWoR%3A%20World-Space%20Hand%20Motion%20Reconstruction%20from%20Egocentric%20Videos&body=Title%3A%20HaWoR%3A%20World-Space%20Hand%20Motion%20Reconstruction%20from%20Egocentric%20Videos%0AAuthor%3A%20Jinglei%20Zhang%20and%20Jiankang%20Deng%20and%20Chao%20Ma%20and%20Rolandos%20Alexandros%20Potamias%0AAbstract%3A%20%20%20Despite%20the%20advent%20in%203D%20hand%20pose%20estimation%2C%20current%20methods%20predominantly%0Afocus%20on%20single-image%203D%20hand%20reconstruction%20in%20the%20camera%20frame%2C%20overlooking%0Athe%20world-space%20motion%20of%20the%20hands.%20Such%20limitation%20prohibits%20their%20direct%20use%0Ain%20egocentric%20video%20settings%2C%20where%20hands%20and%20camera%20are%20continuously%20in%0Amotion.%20In%20this%20work%2C%20we%20propose%20HaWoR%2C%20a%20high-fidelity%20method%20for%20hand%20motion%0Areconstruction%20in%20world%20coordinates%20from%20egocentric%20videos.%20We%20propose%20to%0Adecouple%20the%20task%20by%20reconstructing%20the%20hand%20motion%20in%20the%20camera%20space%20and%0Aestimating%20the%20camera%20trajectory%20in%20the%20world%20coordinate%20system.%20To%20achieve%0Aprecise%20camera%20trajectory%20estimation%2C%20we%20propose%20an%20adaptive%20egocentric%20SLAM%0Aframework%20that%20addresses%20the%20shortcomings%20of%20traditional%20SLAM%20methods%2C%0Aproviding%20robust%20performance%20under%20challenging%20camera%20dynamics.%20To%20ensure%0Arobust%20hand%20motion%20trajectories%2C%20even%20when%20the%20hands%20move%20out%20of%20view%20frustum%2C%0Awe%20devise%20a%20novel%20motion%20infiller%20network%20that%20effectively%20completes%20the%0Amissing%20frames%20of%20the%20sequence.%20Through%20extensive%20quantitative%20and%20qualitative%0Aevaluations%2C%20we%20demonstrate%20that%20HaWoR%20achieves%20state-of-the-art%20performance%20on%0Aboth%20hand%20motion%20reconstruction%20and%20world-frame%20camera%20trajectory%20estimation%0Aunder%20different%20egocentric%20benchmark%20datasets.%20Code%20and%20models%20are%20available%20on%0Ahttps%3A//hawor-project.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaWoR%253A%2520World-Space%2520Hand%2520Motion%2520Reconstruction%2520from%2520Egocentric%2520Videos%26entry.906535625%3DJinglei%2520Zhang%2520and%2520Jiankang%2520Deng%2520and%2520Chao%2520Ma%2520and%2520Rolandos%2520Alexandros%2520Potamias%26entry.1292438233%3D%2520%2520Despite%2520the%2520advent%2520in%25203D%2520hand%2520pose%2520estimation%252C%2520current%2520methods%2520predominantly%250Afocus%2520on%2520single-image%25203D%2520hand%2520reconstruction%2520in%2520the%2520camera%2520frame%252C%2520overlooking%250Athe%2520world-space%2520motion%2520of%2520the%2520hands.%2520Such%2520limitation%2520prohibits%2520their%2520direct%2520use%250Ain%2520egocentric%2520video%2520settings%252C%2520where%2520hands%2520and%2520camera%2520are%2520continuously%2520in%250Amotion.%2520In%2520this%2520work%252C%2520we%2520propose%2520HaWoR%252C%2520a%2520high-fidelity%2520method%2520for%2520hand%2520motion%250Areconstruction%2520in%2520world%2520coordinates%2520from%2520egocentric%2520videos.%2520We%2520propose%2520to%250Adecouple%2520the%2520task%2520by%2520reconstructing%2520the%2520hand%2520motion%2520in%2520the%2520camera%2520space%2520and%250Aestimating%2520the%2520camera%2520trajectory%2520in%2520the%2520world%2520coordinate%2520system.%2520To%2520achieve%250Aprecise%2520camera%2520trajectory%2520estimation%252C%2520we%2520propose%2520an%2520adaptive%2520egocentric%2520SLAM%250Aframework%2520that%2520addresses%2520the%2520shortcomings%2520of%2520traditional%2520SLAM%2520methods%252C%250Aproviding%2520robust%2520performance%2520under%2520challenging%2520camera%2520dynamics.%2520To%2520ensure%250Arobust%2520hand%2520motion%2520trajectories%252C%2520even%2520when%2520the%2520hands%2520move%2520out%2520of%2520view%2520frustum%252C%250Awe%2520devise%2520a%2520novel%2520motion%2520infiller%2520network%2520that%2520effectively%2520completes%2520the%250Amissing%2520frames%2520of%2520the%2520sequence.%2520Through%2520extensive%2520quantitative%2520and%2520qualitative%250Aevaluations%252C%2520we%2520demonstrate%2520that%2520HaWoR%2520achieves%2520state-of-the-art%2520performance%2520on%250Aboth%2520hand%2520motion%2520reconstruction%2520and%2520world-frame%2520camera%2520trajectory%2520estimation%250Aunder%2520different%2520egocentric%2520benchmark%2520datasets.%2520Code%2520and%2520models%2520are%2520available%2520on%250Ahttps%253A//hawor-project.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HaWoR%3A%20World-Space%20Hand%20Motion%20Reconstruction%20from%20Egocentric%20Videos&entry.906535625=Jinglei%20Zhang%20and%20Jiankang%20Deng%20and%20Chao%20Ma%20and%20Rolandos%20Alexandros%20Potamias&entry.1292438233=%20%20Despite%20the%20advent%20in%203D%20hand%20pose%20estimation%2C%20current%20methods%20predominantly%0Afocus%20on%20single-image%203D%20hand%20reconstruction%20in%20the%20camera%20frame%2C%20overlooking%0Athe%20world-space%20motion%20of%20the%20hands.%20Such%20limitation%20prohibits%20their%20direct%20use%0Ain%20egocentric%20video%20settings%2C%20where%20hands%20and%20camera%20are%20continuously%20in%0Amotion.%20In%20this%20work%2C%20we%20propose%20HaWoR%2C%20a%20high-fidelity%20method%20for%20hand%20motion%0Areconstruction%20in%20world%20coordinates%20from%20egocentric%20videos.%20We%20propose%20to%0Adecouple%20the%20task%20by%20reconstructing%20the%20hand%20motion%20in%20the%20camera%20space%20and%0Aestimating%20the%20camera%20trajectory%20in%20the%20world%20coordinate%20system.%20To%20achieve%0Aprecise%20camera%20trajectory%20estimation%2C%20we%20propose%20an%20adaptive%20egocentric%20SLAM%0Aframework%20that%20addresses%20the%20shortcomings%20of%20traditional%20SLAM%20methods%2C%0Aproviding%20robust%20performance%20under%20challenging%20camera%20dynamics.%20To%20ensure%0Arobust%20hand%20motion%20trajectories%2C%20even%20when%20the%20hands%20move%20out%20of%20view%20frustum%2C%0Awe%20devise%20a%20novel%20motion%20infiller%20network%20that%20effectively%20completes%20the%0Amissing%20frames%20of%20the%20sequence.%20Through%20extensive%20quantitative%20and%20qualitative%0Aevaluations%2C%20we%20demonstrate%20that%20HaWoR%20achieves%20state-of-the-art%20performance%20on%0Aboth%20hand%20motion%20reconstruction%20and%20world-frame%20camera%20trajectory%20estimation%0Aunder%20different%20egocentric%20benchmark%20datasets.%20Code%20and%20models%20are%20available%20on%0Ahttps%3A//hawor-project.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02973v1&entry.124074799=Read"},
{"title": "RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet\n  Transform Projection-based Network", "author": "Haosheng Zhang and Hao Huang", "abstract": "  In the domain of 3D object classification, a fundamental challenge lies in\naddressing the scarcity of labeled data, which limits the applicability of\ntraditional data-intensive learning paradigms. This challenge is particularly\npronounced in few-shot learning scenarios, where the objective is to achieve\nrobust generalization from minimal annotated samples. To overcome these\nlimitations, it is crucial to identify and leverage the most salient and\ndiscriminative features of 3D objects, thereby enhancing learning efficiency\nand reducing dependency on large-scale labeled datasets. This work introduces\nRW-Net, a novel framework designed to address the challenges above by\nintegrating Rate-Distortion Explanation (RDE) and wavelet transform into a\nstate-of-the-art projection-based 3D object classification architecture. The\nproposed method capitalizes on RDE to extract critical features by identifying\nand preserving the most informative data components while reducing redundancy.\nThis process ensures the retention of essential information for effective\ndecision-making, optimizing the model's ability to learn from limited data.\nComplementing RDE, incorporating the wavelet transform further enhances the\nframework's capability to generalize in low-data regimes. By emphasizing\nlow-frequency components of the input data, the wavelet transform captures\nfundamental geometric and structural attributes of 3D objects. These attributes\nare instrumental in mitigating overfitting and improving the robustness of the\nlearned representations across diverse tasks and domains. To validate the\neffectiveness of our RW-Net, we conduct extensive experiments on three\ndatasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object\nclassification. The results demonstrate that our approach achieves\nstate-of-the-art performance and exhibits superior generalization and\nrobustness in few-shot learning scenarios.\n", "link": "http://arxiv.org/abs/2501.03221v1", "date": "2025-01-06", "relevancy": 2.7946, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RW-Net%3A%20Enhancing%20Few-Shot%20Point%20Cloud%20Classification%20with%20a%20Wavelet%0A%20%20Transform%20Projection-based%20Network&body=Title%3A%20RW-Net%3A%20Enhancing%20Few-Shot%20Point%20Cloud%20Classification%20with%20a%20Wavelet%0A%20%20Transform%20Projection-based%20Network%0AAuthor%3A%20Haosheng%20Zhang%20and%20Hao%20Huang%0AAbstract%3A%20%20%20In%20the%20domain%20of%203D%20object%20classification%2C%20a%20fundamental%20challenge%20lies%20in%0Aaddressing%20the%20scarcity%20of%20labeled%20data%2C%20which%20limits%20the%20applicability%20of%0Atraditional%20data-intensive%20learning%20paradigms.%20This%20challenge%20is%20particularly%0Apronounced%20in%20few-shot%20learning%20scenarios%2C%20where%20the%20objective%20is%20to%20achieve%0Arobust%20generalization%20from%20minimal%20annotated%20samples.%20To%20overcome%20these%0Alimitations%2C%20it%20is%20crucial%20to%20identify%20and%20leverage%20the%20most%20salient%20and%0Adiscriminative%20features%20of%203D%20objects%2C%20thereby%20enhancing%20learning%20efficiency%0Aand%20reducing%20dependency%20on%20large-scale%20labeled%20datasets.%20This%20work%20introduces%0ARW-Net%2C%20a%20novel%20framework%20designed%20to%20address%20the%20challenges%20above%20by%0Aintegrating%20Rate-Distortion%20Explanation%20%28RDE%29%20and%20wavelet%20transform%20into%20a%0Astate-of-the-art%20projection-based%203D%20object%20classification%20architecture.%20The%0Aproposed%20method%20capitalizes%20on%20RDE%20to%20extract%20critical%20features%20by%20identifying%0Aand%20preserving%20the%20most%20informative%20data%20components%20while%20reducing%20redundancy.%0AThis%20process%20ensures%20the%20retention%20of%20essential%20information%20for%20effective%0Adecision-making%2C%20optimizing%20the%20model%27s%20ability%20to%20learn%20from%20limited%20data.%0AComplementing%20RDE%2C%20incorporating%20the%20wavelet%20transform%20further%20enhances%20the%0Aframework%27s%20capability%20to%20generalize%20in%20low-data%20regimes.%20By%20emphasizing%0Alow-frequency%20components%20of%20the%20input%20data%2C%20the%20wavelet%20transform%20captures%0Afundamental%20geometric%20and%20structural%20attributes%20of%203D%20objects.%20These%20attributes%0Aare%20instrumental%20in%20mitigating%20overfitting%20and%20improving%20the%20robustness%20of%20the%0Alearned%20representations%20across%20diverse%20tasks%20and%20domains.%20To%20validate%20the%0Aeffectiveness%20of%20our%20RW-Net%2C%20we%20conduct%20extensive%20experiments%20on%20three%0Adatasets%3A%20ModelNet40%2C%20ModelNet40-C%2C%20and%20ScanObjectNN%20for%20few-shot%203D%20object%0Aclassification.%20The%20results%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20and%20exhibits%20superior%20generalization%20and%0Arobustness%20in%20few-shot%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRW-Net%253A%2520Enhancing%2520Few-Shot%2520Point%2520Cloud%2520Classification%2520with%2520a%2520Wavelet%250A%2520%2520Transform%2520Projection-based%2520Network%26entry.906535625%3DHaosheng%2520Zhang%2520and%2520Hao%2520Huang%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%25203D%2520object%2520classification%252C%2520a%2520fundamental%2520challenge%2520lies%2520in%250Aaddressing%2520the%2520scarcity%2520of%2520labeled%2520data%252C%2520which%2520limits%2520the%2520applicability%2520of%250Atraditional%2520data-intensive%2520learning%2520paradigms.%2520This%2520challenge%2520is%2520particularly%250Apronounced%2520in%2520few-shot%2520learning%2520scenarios%252C%2520where%2520the%2520objective%2520is%2520to%2520achieve%250Arobust%2520generalization%2520from%2520minimal%2520annotated%2520samples.%2520To%2520overcome%2520these%250Alimitations%252C%2520it%2520is%2520crucial%2520to%2520identify%2520and%2520leverage%2520the%2520most%2520salient%2520and%250Adiscriminative%2520features%2520of%25203D%2520objects%252C%2520thereby%2520enhancing%2520learning%2520efficiency%250Aand%2520reducing%2520dependency%2520on%2520large-scale%2520labeled%2520datasets.%2520This%2520work%2520introduces%250ARW-Net%252C%2520a%2520novel%2520framework%2520designed%2520to%2520address%2520the%2520challenges%2520above%2520by%250Aintegrating%2520Rate-Distortion%2520Explanation%2520%2528RDE%2529%2520and%2520wavelet%2520transform%2520into%2520a%250Astate-of-the-art%2520projection-based%25203D%2520object%2520classification%2520architecture.%2520The%250Aproposed%2520method%2520capitalizes%2520on%2520RDE%2520to%2520extract%2520critical%2520features%2520by%2520identifying%250Aand%2520preserving%2520the%2520most%2520informative%2520data%2520components%2520while%2520reducing%2520redundancy.%250AThis%2520process%2520ensures%2520the%2520retention%2520of%2520essential%2520information%2520for%2520effective%250Adecision-making%252C%2520optimizing%2520the%2520model%2527s%2520ability%2520to%2520learn%2520from%2520limited%2520data.%250AComplementing%2520RDE%252C%2520incorporating%2520the%2520wavelet%2520transform%2520further%2520enhances%2520the%250Aframework%2527s%2520capability%2520to%2520generalize%2520in%2520low-data%2520regimes.%2520By%2520emphasizing%250Alow-frequency%2520components%2520of%2520the%2520input%2520data%252C%2520the%2520wavelet%2520transform%2520captures%250Afundamental%2520geometric%2520and%2520structural%2520attributes%2520of%25203D%2520objects.%2520These%2520attributes%250Aare%2520instrumental%2520in%2520mitigating%2520overfitting%2520and%2520improving%2520the%2520robustness%2520of%2520the%250Alearned%2520representations%2520across%2520diverse%2520tasks%2520and%2520domains.%2520To%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520RW-Net%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520three%250Adatasets%253A%2520ModelNet40%252C%2520ModelNet40-C%252C%2520and%2520ScanObjectNN%2520for%2520few-shot%25203D%2520object%250Aclassification.%2520The%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520and%2520exhibits%2520superior%2520generalization%2520and%250Arobustness%2520in%2520few-shot%2520learning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RW-Net%3A%20Enhancing%20Few-Shot%20Point%20Cloud%20Classification%20with%20a%20Wavelet%0A%20%20Transform%20Projection-based%20Network&entry.906535625=Haosheng%20Zhang%20and%20Hao%20Huang&entry.1292438233=%20%20In%20the%20domain%20of%203D%20object%20classification%2C%20a%20fundamental%20challenge%20lies%20in%0Aaddressing%20the%20scarcity%20of%20labeled%20data%2C%20which%20limits%20the%20applicability%20of%0Atraditional%20data-intensive%20learning%20paradigms.%20This%20challenge%20is%20particularly%0Apronounced%20in%20few-shot%20learning%20scenarios%2C%20where%20the%20objective%20is%20to%20achieve%0Arobust%20generalization%20from%20minimal%20annotated%20samples.%20To%20overcome%20these%0Alimitations%2C%20it%20is%20crucial%20to%20identify%20and%20leverage%20the%20most%20salient%20and%0Adiscriminative%20features%20of%203D%20objects%2C%20thereby%20enhancing%20learning%20efficiency%0Aand%20reducing%20dependency%20on%20large-scale%20labeled%20datasets.%20This%20work%20introduces%0ARW-Net%2C%20a%20novel%20framework%20designed%20to%20address%20the%20challenges%20above%20by%0Aintegrating%20Rate-Distortion%20Explanation%20%28RDE%29%20and%20wavelet%20transform%20into%20a%0Astate-of-the-art%20projection-based%203D%20object%20classification%20architecture.%20The%0Aproposed%20method%20capitalizes%20on%20RDE%20to%20extract%20critical%20features%20by%20identifying%0Aand%20preserving%20the%20most%20informative%20data%20components%20while%20reducing%20redundancy.%0AThis%20process%20ensures%20the%20retention%20of%20essential%20information%20for%20effective%0Adecision-making%2C%20optimizing%20the%20model%27s%20ability%20to%20learn%20from%20limited%20data.%0AComplementing%20RDE%2C%20incorporating%20the%20wavelet%20transform%20further%20enhances%20the%0Aframework%27s%20capability%20to%20generalize%20in%20low-data%20regimes.%20By%20emphasizing%0Alow-frequency%20components%20of%20the%20input%20data%2C%20the%20wavelet%20transform%20captures%0Afundamental%20geometric%20and%20structural%20attributes%20of%203D%20objects.%20These%20attributes%0Aare%20instrumental%20in%20mitigating%20overfitting%20and%20improving%20the%20robustness%20of%20the%0Alearned%20representations%20across%20diverse%20tasks%20and%20domains.%20To%20validate%20the%0Aeffectiveness%20of%20our%20RW-Net%2C%20we%20conduct%20extensive%20experiments%20on%20three%0Adatasets%3A%20ModelNet40%2C%20ModelNet40-C%2C%20and%20ScanObjectNN%20for%20few-shot%203D%20object%0Aclassification.%20The%20results%20demonstrate%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20and%20exhibits%20superior%20generalization%20and%0Arobustness%20in%20few-shot%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03221v1&entry.124074799=Read"},
{"title": "Human Gaze Boosts Object-Centered Representation Learning", "author": "Timothy Schauml\u00f6ffel and Arthur Aubret and Gemma Roig and Jochen Triesch", "abstract": "  Recent self-supervised learning (SSL) models trained on human-like egocentric\nvisual inputs substantially underperform on image recognition tasks compared to\nhumans. These models train on raw, uniform visual inputs collected from\nhead-mounted cameras. This is different from humans, as the anatomical\nstructure of the retina and visual cortex relatively amplifies the central\nvisual information, i.e. around humans' gaze location. This selective\namplification in humans likely aids in forming object-centered visual\nrepresentations. Here, we investigate whether focusing on central visual\ninformation boosts egocentric visual object learning. We simulate 5-months of\negocentric visual experience using the large-scale Ego4D dataset and generate\ngaze locations with a human gaze prediction model. To account for the\nimportance of central vision in humans, we crop the visual area around the gaze\nlocation. Finally, we train a time-based SSL model on these modified inputs.\nOur experiments demonstrate that focusing on central vision leads to better\nobject-centered representations. Our analysis shows that the SSL model\nleverages the temporal dynamics of the gaze movements to build stronger visual\nrepresentations. Overall, our work marks a significant step toward bio-inspired\nlearning of visual representations.\n", "link": "http://arxiv.org/abs/2501.02966v1", "date": "2025-01-06", "relevancy": 2.7778, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5633}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Gaze%20Boosts%20Object-Centered%20Representation%20Learning&body=Title%3A%20Human%20Gaze%20Boosts%20Object-Centered%20Representation%20Learning%0AAuthor%3A%20Timothy%20Schauml%C3%B6ffel%20and%20Arthur%20Aubret%20and%20Gemma%20Roig%20and%20Jochen%20Triesch%0AAbstract%3A%20%20%20Recent%20self-supervised%20learning%20%28SSL%29%20models%20trained%20on%20human-like%20egocentric%0Avisual%20inputs%20substantially%20underperform%20on%20image%20recognition%20tasks%20compared%20to%0Ahumans.%20These%20models%20train%20on%20raw%2C%20uniform%20visual%20inputs%20collected%20from%0Ahead-mounted%20cameras.%20This%20is%20different%20from%20humans%2C%20as%20the%20anatomical%0Astructure%20of%20the%20retina%20and%20visual%20cortex%20relatively%20amplifies%20the%20central%0Avisual%20information%2C%20i.e.%20around%20humans%27%20gaze%20location.%20This%20selective%0Aamplification%20in%20humans%20likely%20aids%20in%20forming%20object-centered%20visual%0Arepresentations.%20Here%2C%20we%20investigate%20whether%20focusing%20on%20central%20visual%0Ainformation%20boosts%20egocentric%20visual%20object%20learning.%20We%20simulate%205-months%20of%0Aegocentric%20visual%20experience%20using%20the%20large-scale%20Ego4D%20dataset%20and%20generate%0Agaze%20locations%20with%20a%20human%20gaze%20prediction%20model.%20To%20account%20for%20the%0Aimportance%20of%20central%20vision%20in%20humans%2C%20we%20crop%20the%20visual%20area%20around%20the%20gaze%0Alocation.%20Finally%2C%20we%20train%20a%20time-based%20SSL%20model%20on%20these%20modified%20inputs.%0AOur%20experiments%20demonstrate%20that%20focusing%20on%20central%20vision%20leads%20to%20better%0Aobject-centered%20representations.%20Our%20analysis%20shows%20that%20the%20SSL%20model%0Aleverages%20the%20temporal%20dynamics%20of%20the%20gaze%20movements%20to%20build%20stronger%20visual%0Arepresentations.%20Overall%2C%20our%20work%20marks%20a%20significant%20step%20toward%20bio-inspired%0Alearning%20of%20visual%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Gaze%2520Boosts%2520Object-Centered%2520Representation%2520Learning%26entry.906535625%3DTimothy%2520Schauml%25C3%25B6ffel%2520and%2520Arthur%2520Aubret%2520and%2520Gemma%2520Roig%2520and%2520Jochen%2520Triesch%26entry.1292438233%3D%2520%2520Recent%2520self-supervised%2520learning%2520%2528SSL%2529%2520models%2520trained%2520on%2520human-like%2520egocentric%250Avisual%2520inputs%2520substantially%2520underperform%2520on%2520image%2520recognition%2520tasks%2520compared%2520to%250Ahumans.%2520These%2520models%2520train%2520on%2520raw%252C%2520uniform%2520visual%2520inputs%2520collected%2520from%250Ahead-mounted%2520cameras.%2520This%2520is%2520different%2520from%2520humans%252C%2520as%2520the%2520anatomical%250Astructure%2520of%2520the%2520retina%2520and%2520visual%2520cortex%2520relatively%2520amplifies%2520the%2520central%250Avisual%2520information%252C%2520i.e.%2520around%2520humans%2527%2520gaze%2520location.%2520This%2520selective%250Aamplification%2520in%2520humans%2520likely%2520aids%2520in%2520forming%2520object-centered%2520visual%250Arepresentations.%2520Here%252C%2520we%2520investigate%2520whether%2520focusing%2520on%2520central%2520visual%250Ainformation%2520boosts%2520egocentric%2520visual%2520object%2520learning.%2520We%2520simulate%25205-months%2520of%250Aegocentric%2520visual%2520experience%2520using%2520the%2520large-scale%2520Ego4D%2520dataset%2520and%2520generate%250Agaze%2520locations%2520with%2520a%2520human%2520gaze%2520prediction%2520model.%2520To%2520account%2520for%2520the%250Aimportance%2520of%2520central%2520vision%2520in%2520humans%252C%2520we%2520crop%2520the%2520visual%2520area%2520around%2520the%2520gaze%250Alocation.%2520Finally%252C%2520we%2520train%2520a%2520time-based%2520SSL%2520model%2520on%2520these%2520modified%2520inputs.%250AOur%2520experiments%2520demonstrate%2520that%2520focusing%2520on%2520central%2520vision%2520leads%2520to%2520better%250Aobject-centered%2520representations.%2520Our%2520analysis%2520shows%2520that%2520the%2520SSL%2520model%250Aleverages%2520the%2520temporal%2520dynamics%2520of%2520the%2520gaze%2520movements%2520to%2520build%2520stronger%2520visual%250Arepresentations.%2520Overall%252C%2520our%2520work%2520marks%2520a%2520significant%2520step%2520toward%2520bio-inspired%250Alearning%2520of%2520visual%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Gaze%20Boosts%20Object-Centered%20Representation%20Learning&entry.906535625=Timothy%20Schauml%C3%B6ffel%20and%20Arthur%20Aubret%20and%20Gemma%20Roig%20and%20Jochen%20Triesch&entry.1292438233=%20%20Recent%20self-supervised%20learning%20%28SSL%29%20models%20trained%20on%20human-like%20egocentric%0Avisual%20inputs%20substantially%20underperform%20on%20image%20recognition%20tasks%20compared%20to%0Ahumans.%20These%20models%20train%20on%20raw%2C%20uniform%20visual%20inputs%20collected%20from%0Ahead-mounted%20cameras.%20This%20is%20different%20from%20humans%2C%20as%20the%20anatomical%0Astructure%20of%20the%20retina%20and%20visual%20cortex%20relatively%20amplifies%20the%20central%0Avisual%20information%2C%20i.e.%20around%20humans%27%20gaze%20location.%20This%20selective%0Aamplification%20in%20humans%20likely%20aids%20in%20forming%20object-centered%20visual%0Arepresentations.%20Here%2C%20we%20investigate%20whether%20focusing%20on%20central%20visual%0Ainformation%20boosts%20egocentric%20visual%20object%20learning.%20We%20simulate%205-months%20of%0Aegocentric%20visual%20experience%20using%20the%20large-scale%20Ego4D%20dataset%20and%20generate%0Agaze%20locations%20with%20a%20human%20gaze%20prediction%20model.%20To%20account%20for%20the%0Aimportance%20of%20central%20vision%20in%20humans%2C%20we%20crop%20the%20visual%20area%20around%20the%20gaze%0Alocation.%20Finally%2C%20we%20train%20a%20time-based%20SSL%20model%20on%20these%20modified%20inputs.%0AOur%20experiments%20demonstrate%20that%20focusing%20on%20central%20vision%20leads%20to%20better%0Aobject-centered%20representations.%20Our%20analysis%20shows%20that%20the%20SSL%20model%0Aleverages%20the%20temporal%20dynamics%20of%20the%20gaze%20movements%20to%20build%20stronger%20visual%0Arepresentations.%20Overall%2C%20our%20work%20marks%20a%20significant%20step%20toward%20bio-inspired%0Alearning%20of%20visual%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02966v1&entry.124074799=Read"},
{"title": "A Trust-Guided Approach to MR Image Reconstruction with Side Information", "author": "Arda Atal\u0131k and Sumit Chopra and Daniel K. Sodickson", "abstract": "  Reducing MRI scan times can improve patient care and lower healthcare costs.\nMany acceleration methods are designed to reconstruct diagnostic-quality images\nfrom limited sets of acquired $\\textit{k}$-space data. This task can be framed\nas a linear inverse problem (LIP), where, as a result of undersampling, the\nforward operator may become rank-deficient or exhibit small singular values.\nThis results in ambiguities in reconstruction, in which multiple generally\nincorrect or non-diagnostic images can map to the same acquired data. To\naddress such ambiguities, it is crucial to incorporate prior knowledge, for\nexample in the form of regularization. Another form of prior knowledge less\ncommonly used in medical imaging is contextual side information garnered from\nother sources than the current acquisition. Here, we propose the\n$\\textbf{T}$rust-$\\textbf{G}$uided $\\textbf{V}$ariational $\\textbf{N}$etwork\n$\\textbf{(TGVN)}$, a novel end-to-end deep learning framework that effectively\nintegrates side information into LIPs. TGVN eliminates undesirable solutions\nfrom the ambiguous space of the forward operator while remaining faithful to\nthe acquired data. We demonstrate its effectiveness in multi-coil,\nmulti-contrast MR image reconstruction, where incomplete or low-quality\nmeasurements from one contrast are used as side information to reconstruct\nhigh-quality images of another contrast from heavily under-sampled data. Our\nmethod is robust across different contrasts, anatomies, and field strengths.\nCompared to baselines that also utilize side information, TGVN achieves\nsuperior image quality at challenging under-sampling levels, drastically\nspeeding up acquisition while minimizing hallucinations. Our approach is also\nversatile enough to incorporate many different types of side information\n(including previous scans or even text) into any LIP.\n", "link": "http://arxiv.org/abs/2501.03021v1", "date": "2025-01-06", "relevancy": 2.7617, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5728}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5514}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Trust-Guided%20Approach%20to%20MR%20Image%20Reconstruction%20with%20Side%20Information&body=Title%3A%20A%20Trust-Guided%20Approach%20to%20MR%20Image%20Reconstruction%20with%20Side%20Information%0AAuthor%3A%20Arda%20Atal%C4%B1k%20and%20Sumit%20Chopra%20and%20Daniel%20K.%20Sodickson%0AAbstract%3A%20%20%20Reducing%20MRI%20scan%20times%20can%20improve%20patient%20care%20and%20lower%20healthcare%20costs.%0AMany%20acceleration%20methods%20are%20designed%20to%20reconstruct%20diagnostic-quality%20images%0Afrom%20limited%20sets%20of%20acquired%20%24%5Ctextit%7Bk%7D%24-space%20data.%20This%20task%20can%20be%20framed%0Aas%20a%20linear%20inverse%20problem%20%28LIP%29%2C%20where%2C%20as%20a%20result%20of%20undersampling%2C%20the%0Aforward%20operator%20may%20become%20rank-deficient%20or%20exhibit%20small%20singular%20values.%0AThis%20results%20in%20ambiguities%20in%20reconstruction%2C%20in%20which%20multiple%20generally%0Aincorrect%20or%20non-diagnostic%20images%20can%20map%20to%20the%20same%20acquired%20data.%20To%0Aaddress%20such%20ambiguities%2C%20it%20is%20crucial%20to%20incorporate%20prior%20knowledge%2C%20for%0Aexample%20in%20the%20form%20of%20regularization.%20Another%20form%20of%20prior%20knowledge%20less%0Acommonly%20used%20in%20medical%20imaging%20is%20contextual%20side%20information%20garnered%20from%0Aother%20sources%20than%20the%20current%20acquisition.%20Here%2C%20we%20propose%20the%0A%24%5Ctextbf%7BT%7D%24rust-%24%5Ctextbf%7BG%7D%24uided%20%24%5Ctextbf%7BV%7D%24ariational%20%24%5Ctextbf%7BN%7D%24etwork%0A%24%5Ctextbf%7B%28TGVN%29%7D%24%2C%20a%20novel%20end-to-end%20deep%20learning%20framework%20that%20effectively%0Aintegrates%20side%20information%20into%20LIPs.%20TGVN%20eliminates%20undesirable%20solutions%0Afrom%20the%20ambiguous%20space%20of%20the%20forward%20operator%20while%20remaining%20faithful%20to%0Athe%20acquired%20data.%20We%20demonstrate%20its%20effectiveness%20in%20multi-coil%2C%0Amulti-contrast%20MR%20image%20reconstruction%2C%20where%20incomplete%20or%20low-quality%0Ameasurements%20from%20one%20contrast%20are%20used%20as%20side%20information%20to%20reconstruct%0Ahigh-quality%20images%20of%20another%20contrast%20from%20heavily%20under-sampled%20data.%20Our%0Amethod%20is%20robust%20across%20different%20contrasts%2C%20anatomies%2C%20and%20field%20strengths.%0ACompared%20to%20baselines%20that%20also%20utilize%20side%20information%2C%20TGVN%20achieves%0Asuperior%20image%20quality%20at%20challenging%20under-sampling%20levels%2C%20drastically%0Aspeeding%20up%20acquisition%20while%20minimizing%20hallucinations.%20Our%20approach%20is%20also%0Aversatile%20enough%20to%20incorporate%20many%20different%20types%20of%20side%20information%0A%28including%20previous%20scans%20or%20even%20text%29%20into%20any%20LIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Trust-Guided%2520Approach%2520to%2520MR%2520Image%2520Reconstruction%2520with%2520Side%2520Information%26entry.906535625%3DArda%2520Atal%25C4%25B1k%2520and%2520Sumit%2520Chopra%2520and%2520Daniel%2520K.%2520Sodickson%26entry.1292438233%3D%2520%2520Reducing%2520MRI%2520scan%2520times%2520can%2520improve%2520patient%2520care%2520and%2520lower%2520healthcare%2520costs.%250AMany%2520acceleration%2520methods%2520are%2520designed%2520to%2520reconstruct%2520diagnostic-quality%2520images%250Afrom%2520limited%2520sets%2520of%2520acquired%2520%2524%255Ctextit%257Bk%257D%2524-space%2520data.%2520This%2520task%2520can%2520be%2520framed%250Aas%2520a%2520linear%2520inverse%2520problem%2520%2528LIP%2529%252C%2520where%252C%2520as%2520a%2520result%2520of%2520undersampling%252C%2520the%250Aforward%2520operator%2520may%2520become%2520rank-deficient%2520or%2520exhibit%2520small%2520singular%2520values.%250AThis%2520results%2520in%2520ambiguities%2520in%2520reconstruction%252C%2520in%2520which%2520multiple%2520generally%250Aincorrect%2520or%2520non-diagnostic%2520images%2520can%2520map%2520to%2520the%2520same%2520acquired%2520data.%2520To%250Aaddress%2520such%2520ambiguities%252C%2520it%2520is%2520crucial%2520to%2520incorporate%2520prior%2520knowledge%252C%2520for%250Aexample%2520in%2520the%2520form%2520of%2520regularization.%2520Another%2520form%2520of%2520prior%2520knowledge%2520less%250Acommonly%2520used%2520in%2520medical%2520imaging%2520is%2520contextual%2520side%2520information%2520garnered%2520from%250Aother%2520sources%2520than%2520the%2520current%2520acquisition.%2520Here%252C%2520we%2520propose%2520the%250A%2524%255Ctextbf%257BT%257D%2524rust-%2524%255Ctextbf%257BG%257D%2524uided%2520%2524%255Ctextbf%257BV%257D%2524ariational%2520%2524%255Ctextbf%257BN%257D%2524etwork%250A%2524%255Ctextbf%257B%2528TGVN%2529%257D%2524%252C%2520a%2520novel%2520end-to-end%2520deep%2520learning%2520framework%2520that%2520effectively%250Aintegrates%2520side%2520information%2520into%2520LIPs.%2520TGVN%2520eliminates%2520undesirable%2520solutions%250Afrom%2520the%2520ambiguous%2520space%2520of%2520the%2520forward%2520operator%2520while%2520remaining%2520faithful%2520to%250Athe%2520acquired%2520data.%2520We%2520demonstrate%2520its%2520effectiveness%2520in%2520multi-coil%252C%250Amulti-contrast%2520MR%2520image%2520reconstruction%252C%2520where%2520incomplete%2520or%2520low-quality%250Ameasurements%2520from%2520one%2520contrast%2520are%2520used%2520as%2520side%2520information%2520to%2520reconstruct%250Ahigh-quality%2520images%2520of%2520another%2520contrast%2520from%2520heavily%2520under-sampled%2520data.%2520Our%250Amethod%2520is%2520robust%2520across%2520different%2520contrasts%252C%2520anatomies%252C%2520and%2520field%2520strengths.%250ACompared%2520to%2520baselines%2520that%2520also%2520utilize%2520side%2520information%252C%2520TGVN%2520achieves%250Asuperior%2520image%2520quality%2520at%2520challenging%2520under-sampling%2520levels%252C%2520drastically%250Aspeeding%2520up%2520acquisition%2520while%2520minimizing%2520hallucinations.%2520Our%2520approach%2520is%2520also%250Aversatile%2520enough%2520to%2520incorporate%2520many%2520different%2520types%2520of%2520side%2520information%250A%2528including%2520previous%2520scans%2520or%2520even%2520text%2529%2520into%2520any%2520LIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Trust-Guided%20Approach%20to%20MR%20Image%20Reconstruction%20with%20Side%20Information&entry.906535625=Arda%20Atal%C4%B1k%20and%20Sumit%20Chopra%20and%20Daniel%20K.%20Sodickson&entry.1292438233=%20%20Reducing%20MRI%20scan%20times%20can%20improve%20patient%20care%20and%20lower%20healthcare%20costs.%0AMany%20acceleration%20methods%20are%20designed%20to%20reconstruct%20diagnostic-quality%20images%0Afrom%20limited%20sets%20of%20acquired%20%24%5Ctextit%7Bk%7D%24-space%20data.%20This%20task%20can%20be%20framed%0Aas%20a%20linear%20inverse%20problem%20%28LIP%29%2C%20where%2C%20as%20a%20result%20of%20undersampling%2C%20the%0Aforward%20operator%20may%20become%20rank-deficient%20or%20exhibit%20small%20singular%20values.%0AThis%20results%20in%20ambiguities%20in%20reconstruction%2C%20in%20which%20multiple%20generally%0Aincorrect%20or%20non-diagnostic%20images%20can%20map%20to%20the%20same%20acquired%20data.%20To%0Aaddress%20such%20ambiguities%2C%20it%20is%20crucial%20to%20incorporate%20prior%20knowledge%2C%20for%0Aexample%20in%20the%20form%20of%20regularization.%20Another%20form%20of%20prior%20knowledge%20less%0Acommonly%20used%20in%20medical%20imaging%20is%20contextual%20side%20information%20garnered%20from%0Aother%20sources%20than%20the%20current%20acquisition.%20Here%2C%20we%20propose%20the%0A%24%5Ctextbf%7BT%7D%24rust-%24%5Ctextbf%7BG%7D%24uided%20%24%5Ctextbf%7BV%7D%24ariational%20%24%5Ctextbf%7BN%7D%24etwork%0A%24%5Ctextbf%7B%28TGVN%29%7D%24%2C%20a%20novel%20end-to-end%20deep%20learning%20framework%20that%20effectively%0Aintegrates%20side%20information%20into%20LIPs.%20TGVN%20eliminates%20undesirable%20solutions%0Afrom%20the%20ambiguous%20space%20of%20the%20forward%20operator%20while%20remaining%20faithful%20to%0Athe%20acquired%20data.%20We%20demonstrate%20its%20effectiveness%20in%20multi-coil%2C%0Amulti-contrast%20MR%20image%20reconstruction%2C%20where%20incomplete%20or%20low-quality%0Ameasurements%20from%20one%20contrast%20are%20used%20as%20side%20information%20to%20reconstruct%0Ahigh-quality%20images%20of%20another%20contrast%20from%20heavily%20under-sampled%20data.%20Our%0Amethod%20is%20robust%20across%20different%20contrasts%2C%20anatomies%2C%20and%20field%20strengths.%0ACompared%20to%20baselines%20that%20also%20utilize%20side%20information%2C%20TGVN%20achieves%0Asuperior%20image%20quality%20at%20challenging%20under-sampling%20levels%2C%20drastically%0Aspeeding%20up%20acquisition%20while%20minimizing%20hallucinations.%20Our%20approach%20is%20also%0Aversatile%20enough%20to%20incorporate%20many%20different%20types%20of%20side%20information%0A%28including%20previous%20scans%20or%20even%20text%29%20into%20any%20LIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03021v1&entry.124074799=Read"},
{"title": "Noise-Robust Target-Speaker Voice Activity Detection Through\n  Self-Supervised Pretraining", "author": "Holger Severin Bovbjerg and Jan \u00d8stergaard and Jesper Jensen and Zheng-Hua Tan", "abstract": "  Target-Speaker Voice Activity Detection (TS-VAD) is the task of detecting the\npresence of speech from a known target-speaker in an audio frame. Recently,\ndeep neural network-based models have shown good performance in this task.\nHowever, training these models requires extensive labelled data, which is\ncostly and time-consuming to obtain, particularly if generalization to unseen\nenvironments is crucial. To mitigate this, we propose a causal, Self-Supervised\nLearning (SSL) pretraining framework, called Denoising Autoregressive\nPredictive Coding (DN-APC), to enhance TS-VAD performance in noisy conditions.\nWe also explore various speaker conditioning methods and evaluate their\nperformance under different noisy conditions. Our experiments show that DN-APC\nimproves performance in noisy conditions, with a general improvement of approx.\n2% in both seen and unseen noise. Additionally, we find that FiLM conditioning\nprovides the best overall performance. Representation analysis via tSNE plots\nreveals robust initial representations of speech and non-speech from\npretraining. This underscores the effectiveness of SSL pretraining in improving\nthe robustness and performance of TS-VAD models in noisy environments.\n", "link": "http://arxiv.org/abs/2501.03184v1", "date": "2025-01-06", "relevancy": 2.7525, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6145}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise-Robust%20Target-Speaker%20Voice%20Activity%20Detection%20Through%0A%20%20Self-Supervised%20Pretraining&body=Title%3A%20Noise-Robust%20Target-Speaker%20Voice%20Activity%20Detection%20Through%0A%20%20Self-Supervised%20Pretraining%0AAuthor%3A%20Holger%20Severin%20Bovbjerg%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20%20%20Target-Speaker%20Voice%20Activity%20Detection%20%28TS-VAD%29%20is%20the%20task%20of%20detecting%20the%0Apresence%20of%20speech%20from%20a%20known%20target-speaker%20in%20an%20audio%20frame.%20Recently%2C%0Adeep%20neural%20network-based%20models%20have%20shown%20good%20performance%20in%20this%20task.%0AHowever%2C%20training%20these%20models%20requires%20extensive%20labelled%20data%2C%20which%20is%0Acostly%20and%20time-consuming%20to%20obtain%2C%20particularly%20if%20generalization%20to%20unseen%0Aenvironments%20is%20crucial.%20To%20mitigate%20this%2C%20we%20propose%20a%20causal%2C%20Self-Supervised%0ALearning%20%28SSL%29%20pretraining%20framework%2C%20called%20Denoising%20Autoregressive%0APredictive%20Coding%20%28DN-APC%29%2C%20to%20enhance%20TS-VAD%20performance%20in%20noisy%20conditions.%0AWe%20also%20explore%20various%20speaker%20conditioning%20methods%20and%20evaluate%20their%0Aperformance%20under%20different%20noisy%20conditions.%20Our%20experiments%20show%20that%20DN-APC%0Aimproves%20performance%20in%20noisy%20conditions%2C%20with%20a%20general%20improvement%20of%20approx.%0A2%25%20in%20both%20seen%20and%20unseen%20noise.%20Additionally%2C%20we%20find%20that%20FiLM%20conditioning%0Aprovides%20the%20best%20overall%20performance.%20Representation%20analysis%20via%20tSNE%20plots%0Areveals%20robust%20initial%20representations%20of%20speech%20and%20non-speech%20from%0Apretraining.%20This%20underscores%20the%20effectiveness%20of%20SSL%20pretraining%20in%20improving%0Athe%20robustness%20and%20performance%20of%20TS-VAD%20models%20in%20noisy%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise-Robust%2520Target-Speaker%2520Voice%2520Activity%2520Detection%2520Through%250A%2520%2520Self-Supervised%2520Pretraining%26entry.906535625%3DHolger%2520Severin%2520Bovbjerg%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Jesper%2520Jensen%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3D%2520%2520Target-Speaker%2520Voice%2520Activity%2520Detection%2520%2528TS-VAD%2529%2520is%2520the%2520task%2520of%2520detecting%2520the%250Apresence%2520of%2520speech%2520from%2520a%2520known%2520target-speaker%2520in%2520an%2520audio%2520frame.%2520Recently%252C%250Adeep%2520neural%2520network-based%2520models%2520have%2520shown%2520good%2520performance%2520in%2520this%2520task.%250AHowever%252C%2520training%2520these%2520models%2520requires%2520extensive%2520labelled%2520data%252C%2520which%2520is%250Acostly%2520and%2520time-consuming%2520to%2520obtain%252C%2520particularly%2520if%2520generalization%2520to%2520unseen%250Aenvironments%2520is%2520crucial.%2520To%2520mitigate%2520this%252C%2520we%2520propose%2520a%2520causal%252C%2520Self-Supervised%250ALearning%2520%2528SSL%2529%2520pretraining%2520framework%252C%2520called%2520Denoising%2520Autoregressive%250APredictive%2520Coding%2520%2528DN-APC%2529%252C%2520to%2520enhance%2520TS-VAD%2520performance%2520in%2520noisy%2520conditions.%250AWe%2520also%2520explore%2520various%2520speaker%2520conditioning%2520methods%2520and%2520evaluate%2520their%250Aperformance%2520under%2520different%2520noisy%2520conditions.%2520Our%2520experiments%2520show%2520that%2520DN-APC%250Aimproves%2520performance%2520in%2520noisy%2520conditions%252C%2520with%2520a%2520general%2520improvement%2520of%2520approx.%250A2%2525%2520in%2520both%2520seen%2520and%2520unseen%2520noise.%2520Additionally%252C%2520we%2520find%2520that%2520FiLM%2520conditioning%250Aprovides%2520the%2520best%2520overall%2520performance.%2520Representation%2520analysis%2520via%2520tSNE%2520plots%250Areveals%2520robust%2520initial%2520representations%2520of%2520speech%2520and%2520non-speech%2520from%250Apretraining.%2520This%2520underscores%2520the%2520effectiveness%2520of%2520SSL%2520pretraining%2520in%2520improving%250Athe%2520robustness%2520and%2520performance%2520of%2520TS-VAD%2520models%2520in%2520noisy%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise-Robust%20Target-Speaker%20Voice%20Activity%20Detection%20Through%0A%20%20Self-Supervised%20Pretraining&entry.906535625=Holger%20Severin%20Bovbjerg%20and%20Jan%20%C3%98stergaard%20and%20Jesper%20Jensen%20and%20Zheng-Hua%20Tan&entry.1292438233=%20%20Target-Speaker%20Voice%20Activity%20Detection%20%28TS-VAD%29%20is%20the%20task%20of%20detecting%20the%0Apresence%20of%20speech%20from%20a%20known%20target-speaker%20in%20an%20audio%20frame.%20Recently%2C%0Adeep%20neural%20network-based%20models%20have%20shown%20good%20performance%20in%20this%20task.%0AHowever%2C%20training%20these%20models%20requires%20extensive%20labelled%20data%2C%20which%20is%0Acostly%20and%20time-consuming%20to%20obtain%2C%20particularly%20if%20generalization%20to%20unseen%0Aenvironments%20is%20crucial.%20To%20mitigate%20this%2C%20we%20propose%20a%20causal%2C%20Self-Supervised%0ALearning%20%28SSL%29%20pretraining%20framework%2C%20called%20Denoising%20Autoregressive%0APredictive%20Coding%20%28DN-APC%29%2C%20to%20enhance%20TS-VAD%20performance%20in%20noisy%20conditions.%0AWe%20also%20explore%20various%20speaker%20conditioning%20methods%20and%20evaluate%20their%0Aperformance%20under%20different%20noisy%20conditions.%20Our%20experiments%20show%20that%20DN-APC%0Aimproves%20performance%20in%20noisy%20conditions%2C%20with%20a%20general%20improvement%20of%20approx.%0A2%25%20in%20both%20seen%20and%20unseen%20noise.%20Additionally%2C%20we%20find%20that%20FiLM%20conditioning%0Aprovides%20the%20best%20overall%20performance.%20Representation%20analysis%20via%20tSNE%20plots%0Areveals%20robust%20initial%20representations%20of%20speech%20and%20non-speech%20from%0Apretraining.%20This%20underscores%20the%20effectiveness%20of%20SSL%20pretraining%20in%20improving%0Athe%20robustness%20and%20performance%20of%20TS-VAD%20models%20in%20noisy%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03184v1&entry.124074799=Read"},
{"title": "ETO:Efficient Transformer-based Local Feature Matching by Organizing\n  Multiple Homography Hypotheses", "author": "Junjie Ni and Guofeng Zhang and Guanglin Li and Yijin Li and Xinyang Liu and Zhaoyang Huang and Hujun Bao", "abstract": "  We tackle the efficiency problem of learning local feature matching. Recent\nadvancements have given rise to purely CNN-based and transformer-based\napproaches, each augmented with deep learning techniques. While CNN-based\nmethods often excel in matching speed, transformer-based methods tend to\nprovide more accurate matches. We propose an efficient transformer-based\nnetwork architecture for local feature matching. This technique is built on\nconstructing multiple homography hypotheses to approximate the continuous\ncorrespondence in the real world and uni-directional cross-attention to\naccelerate the refinement. On the YFCC100M dataset, our matching accuracy is\ncompetitive with LoFTR, a state-of-the-art transformer-based architecture,\nwhile the inference speed is boosted to 4 times, even outperforming the\nCNN-based methods. Comprehensive evaluations on other open datasets such as\nMegadepth, ScanNet, and HPatches demonstrate our method's efficacy,\nhighlighting its potential to significantly enhance a wide array of downstream\napplications.\n", "link": "http://arxiv.org/abs/2410.22733v3", "date": "2025-01-06", "relevancy": 2.745, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ETO%3AEfficient%20Transformer-based%20Local%20Feature%20Matching%20by%20Organizing%0A%20%20Multiple%20Homography%20Hypotheses&body=Title%3A%20ETO%3AEfficient%20Transformer-based%20Local%20Feature%20Matching%20by%20Organizing%0A%20%20Multiple%20Homography%20Hypotheses%0AAuthor%3A%20Junjie%20Ni%20and%20Guofeng%20Zhang%20and%20Guanglin%20Li%20and%20Yijin%20Li%20and%20Xinyang%20Liu%20and%20Zhaoyang%20Huang%20and%20Hujun%20Bao%0AAbstract%3A%20%20%20We%20tackle%20the%20efficiency%20problem%20of%20learning%20local%20feature%20matching.%20Recent%0Aadvancements%20have%20given%20rise%20to%20purely%20CNN-based%20and%20transformer-based%0Aapproaches%2C%20each%20augmented%20with%20deep%20learning%20techniques.%20While%20CNN-based%0Amethods%20often%20excel%20in%20matching%20speed%2C%20transformer-based%20methods%20tend%20to%0Aprovide%20more%20accurate%20matches.%20We%20propose%20an%20efficient%20transformer-based%0Anetwork%20architecture%20for%20local%20feature%20matching.%20This%20technique%20is%20built%20on%0Aconstructing%20multiple%20homography%20hypotheses%20to%20approximate%20the%20continuous%0Acorrespondence%20in%20the%20real%20world%20and%20uni-directional%20cross-attention%20to%0Aaccelerate%20the%20refinement.%20On%20the%20YFCC100M%20dataset%2C%20our%20matching%20accuracy%20is%0Acompetitive%20with%20LoFTR%2C%20a%20state-of-the-art%20transformer-based%20architecture%2C%0Awhile%20the%20inference%20speed%20is%20boosted%20to%204%20times%2C%20even%20outperforming%20the%0ACNN-based%20methods.%20Comprehensive%20evaluations%20on%20other%20open%20datasets%20such%20as%0AMegadepth%2C%20ScanNet%2C%20and%20HPatches%20demonstrate%20our%20method%27s%20efficacy%2C%0Ahighlighting%20its%20potential%20to%20significantly%20enhance%20a%20wide%20array%20of%20downstream%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22733v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DETO%253AEfficient%2520Transformer-based%2520Local%2520Feature%2520Matching%2520by%2520Organizing%250A%2520%2520Multiple%2520Homography%2520Hypotheses%26entry.906535625%3DJunjie%2520Ni%2520and%2520Guofeng%2520Zhang%2520and%2520Guanglin%2520Li%2520and%2520Yijin%2520Li%2520and%2520Xinyang%2520Liu%2520and%2520Zhaoyang%2520Huang%2520and%2520Hujun%2520Bao%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520efficiency%2520problem%2520of%2520learning%2520local%2520feature%2520matching.%2520Recent%250Aadvancements%2520have%2520given%2520rise%2520to%2520purely%2520CNN-based%2520and%2520transformer-based%250Aapproaches%252C%2520each%2520augmented%2520with%2520deep%2520learning%2520techniques.%2520While%2520CNN-based%250Amethods%2520often%2520excel%2520in%2520matching%2520speed%252C%2520transformer-based%2520methods%2520tend%2520to%250Aprovide%2520more%2520accurate%2520matches.%2520We%2520propose%2520an%2520efficient%2520transformer-based%250Anetwork%2520architecture%2520for%2520local%2520feature%2520matching.%2520This%2520technique%2520is%2520built%2520on%250Aconstructing%2520multiple%2520homography%2520hypotheses%2520to%2520approximate%2520the%2520continuous%250Acorrespondence%2520in%2520the%2520real%2520world%2520and%2520uni-directional%2520cross-attention%2520to%250Aaccelerate%2520the%2520refinement.%2520On%2520the%2520YFCC100M%2520dataset%252C%2520our%2520matching%2520accuracy%2520is%250Acompetitive%2520with%2520LoFTR%252C%2520a%2520state-of-the-art%2520transformer-based%2520architecture%252C%250Awhile%2520the%2520inference%2520speed%2520is%2520boosted%2520to%25204%2520times%252C%2520even%2520outperforming%2520the%250ACNN-based%2520methods.%2520Comprehensive%2520evaluations%2520on%2520other%2520open%2520datasets%2520such%2520as%250AMegadepth%252C%2520ScanNet%252C%2520and%2520HPatches%2520demonstrate%2520our%2520method%2527s%2520efficacy%252C%250Ahighlighting%2520its%2520potential%2520to%2520significantly%2520enhance%2520a%2520wide%2520array%2520of%2520downstream%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22733v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ETO%3AEfficient%20Transformer-based%20Local%20Feature%20Matching%20by%20Organizing%0A%20%20Multiple%20Homography%20Hypotheses&entry.906535625=Junjie%20Ni%20and%20Guofeng%20Zhang%20and%20Guanglin%20Li%20and%20Yijin%20Li%20and%20Xinyang%20Liu%20and%20Zhaoyang%20Huang%20and%20Hujun%20Bao&entry.1292438233=%20%20We%20tackle%20the%20efficiency%20problem%20of%20learning%20local%20feature%20matching.%20Recent%0Aadvancements%20have%20given%20rise%20to%20purely%20CNN-based%20and%20transformer-based%0Aapproaches%2C%20each%20augmented%20with%20deep%20learning%20techniques.%20While%20CNN-based%0Amethods%20often%20excel%20in%20matching%20speed%2C%20transformer-based%20methods%20tend%20to%0Aprovide%20more%20accurate%20matches.%20We%20propose%20an%20efficient%20transformer-based%0Anetwork%20architecture%20for%20local%20feature%20matching.%20This%20technique%20is%20built%20on%0Aconstructing%20multiple%20homography%20hypotheses%20to%20approximate%20the%20continuous%0Acorrespondence%20in%20the%20real%20world%20and%20uni-directional%20cross-attention%20to%0Aaccelerate%20the%20refinement.%20On%20the%20YFCC100M%20dataset%2C%20our%20matching%20accuracy%20is%0Acompetitive%20with%20LoFTR%2C%20a%20state-of-the-art%20transformer-based%20architecture%2C%0Awhile%20the%20inference%20speed%20is%20boosted%20to%204%20times%2C%20even%20outperforming%20the%0ACNN-based%20methods.%20Comprehensive%20evaluations%20on%20other%20open%20datasets%20such%20as%0AMegadepth%2C%20ScanNet%2C%20and%20HPatches%20demonstrate%20our%20method%27s%20efficacy%2C%0Ahighlighting%20its%20potential%20to%20significantly%20enhance%20a%20wide%20array%20of%20downstream%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22733v3&entry.124074799=Read"},
{"title": "MvKeTR: Chest CT Report Generation with Multi-View Perception and\n  Knowledge Enhancement", "author": "Xiwei Deng and Xianchun He and Jiangfeng Bao and Yudan Zhou and Shuhui Cai and Congbo Cai and Zhong Chen", "abstract": "  CT report generation (CTRG) aims to automatically generate diagnostic reports\nfor 3D volumes, relieving clinicians' workload and improving patient care.\nDespite clinical value, existing works fail to effectively incorporate\ndiagnostic information from multiple anatomical views and lack related clinical\nexpertise essential for accurate and reliable diagnosis. To resolve these\nlimitations, we propose a novel Multi-view perception Knowledge-enhanced\nTransformer (MvKeTR) to mimic the diagnostic workflow of clinicians. Just as\nradiologists first examine CT scans from multiple planes, a Multi-View\nPerception Aggregator (MVPA) with view-aware attention effectively synthesizes\ndiagnostic information from multiple anatomical views. Then, inspired by how\nradiologists further refer to relevant clinical records to guide diagnostic\ndecision-making, a Cross-Modal Knowledge Enhancer (CMKE) retrieves the most\nsimilar reports based on the query volume to incorporate domain knowledge into\nthe diagnosis procedure. Furthermore, instead of traditional MLPs, we employ\nKolmogorov-Arnold Networks (KANs) with learnable nonlinear activation functions\nas the fundamental building blocks of both modules to better capture intricate\ndiagnostic patterns in CT interpretation. Extensive experiments on the public\nCTRG-Chest-548K dataset demonstrate that our method outpaces prior\nstate-of-the-art (SOTA) models across almost all metrics. The code will be made\npublicly available.\n", "link": "http://arxiv.org/abs/2411.18309v2", "date": "2025-01-06", "relevancy": 2.7245, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement&body=Title%3A%20MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement%0AAuthor%3A%20Xiwei%20Deng%20and%20Xianchun%20He%20and%20Jiangfeng%20Bao%20and%20Yudan%20Zhou%20and%20Shuhui%20Cai%20and%20Congbo%20Cai%20and%20Zhong%20Chen%0AAbstract%3A%20%20%20CT%20report%20generation%20%28CTRG%29%20aims%20to%20automatically%20generate%20diagnostic%20reports%0Afor%203D%20volumes%2C%20relieving%20clinicians%27%20workload%20and%20improving%20patient%20care.%0ADespite%20clinical%20value%2C%20existing%20works%20fail%20to%20effectively%20incorporate%0Adiagnostic%20information%20from%20multiple%20anatomical%20views%20and%20lack%20related%20clinical%0Aexpertise%20essential%20for%20accurate%20and%20reliable%20diagnosis.%20To%20resolve%20these%0Alimitations%2C%20we%20propose%20a%20novel%20Multi-view%20perception%20Knowledge-enhanced%0ATransformer%20%28MvKeTR%29%20to%20mimic%20the%20diagnostic%20workflow%20of%20clinicians.%20Just%20as%0Aradiologists%20first%20examine%20CT%20scans%20from%20multiple%20planes%2C%20a%20Multi-View%0APerception%20Aggregator%20%28MVPA%29%20with%20view-aware%20attention%20effectively%20synthesizes%0Adiagnostic%20information%20from%20multiple%20anatomical%20views.%20Then%2C%20inspired%20by%20how%0Aradiologists%20further%20refer%20to%20relevant%20clinical%20records%20to%20guide%20diagnostic%0Adecision-making%2C%20a%20Cross-Modal%20Knowledge%20Enhancer%20%28CMKE%29%20retrieves%20the%20most%0Asimilar%20reports%20based%20on%20the%20query%20volume%20to%20incorporate%20domain%20knowledge%20into%0Athe%20diagnosis%20procedure.%20Furthermore%2C%20instead%20of%20traditional%20MLPs%2C%20we%20employ%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20with%20learnable%20nonlinear%20activation%20functions%0Aas%20the%20fundamental%20building%20blocks%20of%20both%20modules%20to%20better%20capture%20intricate%0Adiagnostic%20patterns%20in%20CT%20interpretation.%20Extensive%20experiments%20on%20the%20public%0ACTRG-Chest-548K%20dataset%20demonstrate%20that%20our%20method%20outpaces%20prior%0Astate-of-the-art%20%28SOTA%29%20models%20across%20almost%20all%20metrics.%20The%20code%20will%20be%20made%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMvKeTR%253A%2520Chest%2520CT%2520Report%2520Generation%2520with%2520Multi-View%2520Perception%2520and%250A%2520%2520Knowledge%2520Enhancement%26entry.906535625%3DXiwei%2520Deng%2520and%2520Xianchun%2520He%2520and%2520Jiangfeng%2520Bao%2520and%2520Yudan%2520Zhou%2520and%2520Shuhui%2520Cai%2520and%2520Congbo%2520Cai%2520and%2520Zhong%2520Chen%26entry.1292438233%3D%2520%2520CT%2520report%2520generation%2520%2528CTRG%2529%2520aims%2520to%2520automatically%2520generate%2520diagnostic%2520reports%250Afor%25203D%2520volumes%252C%2520relieving%2520clinicians%2527%2520workload%2520and%2520improving%2520patient%2520care.%250ADespite%2520clinical%2520value%252C%2520existing%2520works%2520fail%2520to%2520effectively%2520incorporate%250Adiagnostic%2520information%2520from%2520multiple%2520anatomical%2520views%2520and%2520lack%2520related%2520clinical%250Aexpertise%2520essential%2520for%2520accurate%2520and%2520reliable%2520diagnosis.%2520To%2520resolve%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520novel%2520Multi-view%2520perception%2520Knowledge-enhanced%250ATransformer%2520%2528MvKeTR%2529%2520to%2520mimic%2520the%2520diagnostic%2520workflow%2520of%2520clinicians.%2520Just%2520as%250Aradiologists%2520first%2520examine%2520CT%2520scans%2520from%2520multiple%2520planes%252C%2520a%2520Multi-View%250APerception%2520Aggregator%2520%2528MVPA%2529%2520with%2520view-aware%2520attention%2520effectively%2520synthesizes%250Adiagnostic%2520information%2520from%2520multiple%2520anatomical%2520views.%2520Then%252C%2520inspired%2520by%2520how%250Aradiologists%2520further%2520refer%2520to%2520relevant%2520clinical%2520records%2520to%2520guide%2520diagnostic%250Adecision-making%252C%2520a%2520Cross-Modal%2520Knowledge%2520Enhancer%2520%2528CMKE%2529%2520retrieves%2520the%2520most%250Asimilar%2520reports%2520based%2520on%2520the%2520query%2520volume%2520to%2520incorporate%2520domain%2520knowledge%2520into%250Athe%2520diagnosis%2520procedure.%2520Furthermore%252C%2520instead%2520of%2520traditional%2520MLPs%252C%2520we%2520employ%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520with%2520learnable%2520nonlinear%2520activation%2520functions%250Aas%2520the%2520fundamental%2520building%2520blocks%2520of%2520both%2520modules%2520to%2520better%2520capture%2520intricate%250Adiagnostic%2520patterns%2520in%2520CT%2520interpretation.%2520Extensive%2520experiments%2520on%2520the%2520public%250ACTRG-Chest-548K%2520dataset%2520demonstrate%2520that%2520our%2520method%2520outpaces%2520prior%250Astate-of-the-art%2520%2528SOTA%2529%2520models%2520across%2520almost%2520all%2520metrics.%2520The%2520code%2520will%2520be%2520made%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MvKeTR%3A%20Chest%20CT%20Report%20Generation%20with%20Multi-View%20Perception%20and%0A%20%20Knowledge%20Enhancement&entry.906535625=Xiwei%20Deng%20and%20Xianchun%20He%20and%20Jiangfeng%20Bao%20and%20Yudan%20Zhou%20and%20Shuhui%20Cai%20and%20Congbo%20Cai%20and%20Zhong%20Chen&entry.1292438233=%20%20CT%20report%20generation%20%28CTRG%29%20aims%20to%20automatically%20generate%20diagnostic%20reports%0Afor%203D%20volumes%2C%20relieving%20clinicians%27%20workload%20and%20improving%20patient%20care.%0ADespite%20clinical%20value%2C%20existing%20works%20fail%20to%20effectively%20incorporate%0Adiagnostic%20information%20from%20multiple%20anatomical%20views%20and%20lack%20related%20clinical%0Aexpertise%20essential%20for%20accurate%20and%20reliable%20diagnosis.%20To%20resolve%20these%0Alimitations%2C%20we%20propose%20a%20novel%20Multi-view%20perception%20Knowledge-enhanced%0ATransformer%20%28MvKeTR%29%20to%20mimic%20the%20diagnostic%20workflow%20of%20clinicians.%20Just%20as%0Aradiologists%20first%20examine%20CT%20scans%20from%20multiple%20planes%2C%20a%20Multi-View%0APerception%20Aggregator%20%28MVPA%29%20with%20view-aware%20attention%20effectively%20synthesizes%0Adiagnostic%20information%20from%20multiple%20anatomical%20views.%20Then%2C%20inspired%20by%20how%0Aradiologists%20further%20refer%20to%20relevant%20clinical%20records%20to%20guide%20diagnostic%0Adecision-making%2C%20a%20Cross-Modal%20Knowledge%20Enhancer%20%28CMKE%29%20retrieves%20the%20most%0Asimilar%20reports%20based%20on%20the%20query%20volume%20to%20incorporate%20domain%20knowledge%20into%0Athe%20diagnosis%20procedure.%20Furthermore%2C%20instead%20of%20traditional%20MLPs%2C%20we%20employ%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20with%20learnable%20nonlinear%20activation%20functions%0Aas%20the%20fundamental%20building%20blocks%20of%20both%20modules%20to%20better%20capture%20intricate%0Adiagnostic%20patterns%20in%20CT%20interpretation.%20Extensive%20experiments%20on%20the%20public%0ACTRG-Chest-548K%20dataset%20demonstrate%20that%20our%20method%20outpaces%20prior%0Astate-of-the-art%20%28SOTA%29%20models%20across%20almost%20all%20metrics.%20The%20code%20will%20be%20made%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18309v2&entry.124074799=Read"},
{"title": "MDP3: A Training-free Approach for List-wise Frame Selection in\n  Video-LLMs", "author": "Hui Sun and Shiyin Lu and Huanyu Wang and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Ming Li", "abstract": "  Video large language models (Video-LLMs) have made significant progress in\nunderstanding videos. However, processing multiple frames leads to lengthy\nvisual token sequences, presenting challenges such as the limited context\nlength cannot accommodate the entire video, and the inclusion of irrelevant\nframes hinders visual perception. Hence, effective frame selection is crucial.\nThis paper emphasizes that frame selection should follow three key principles:\nquery relevance, list-wise diversity, and sequentiality. Existing methods, such\nas uniform frame sampling and query-frame matching, do not capture all of these\nprinciples. Thus, we propose Markov decision determinantal point process with\ndynamic programming (MDP3) for frame selection, a training-free and\nmodel-agnostic method that can be seamlessly integrated into existing\nVideo-LLMs. Our method first estimates frame similarities conditioned on the\nquery using a conditional Gaussian kernel within the reproducing kernel Hilbert\nspace~(RKHS). We then apply the determinantal point process~(DPP) to the\nsimilarity matrix to capture both query relevance and list-wise diversity. To\nincorporate sequentiality, we segment the video and apply DPP within each\nsegment, conditioned on the preceding segment selection, modeled as a Markov\ndecision process~(MDP) for allocating selection sizes across segments.\nTheoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the\nNP-hard list-wise frame selection problem with pseudo-polynomial time\ncomplexity, demonstrating its efficiency. Empirically, MDP3 significantly\noutperforms existing methods, verifying its effectiveness and robustness.\n", "link": "http://arxiv.org/abs/2501.02885v1", "date": "2025-01-06", "relevancy": 2.7205, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDP3%3A%20A%20Training-free%20Approach%20for%20List-wise%20Frame%20Selection%20in%0A%20%20Video-LLMs&body=Title%3A%20MDP3%3A%20A%20Training-free%20Approach%20for%20List-wise%20Frame%20Selection%20in%0A%20%20Video-LLMs%0AAuthor%3A%20Hui%20Sun%20and%20Shiyin%20Lu%20and%20Huanyu%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Ming%20Li%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28Video-LLMs%29%20have%20made%20significant%20progress%20in%0Aunderstanding%20videos.%20However%2C%20processing%20multiple%20frames%20leads%20to%20lengthy%0Avisual%20token%20sequences%2C%20presenting%20challenges%20such%20as%20the%20limited%20context%0Alength%20cannot%20accommodate%20the%20entire%20video%2C%20and%20the%20inclusion%20of%20irrelevant%0Aframes%20hinders%20visual%20perception.%20Hence%2C%20effective%20frame%20selection%20is%20crucial.%0AThis%20paper%20emphasizes%20that%20frame%20selection%20should%20follow%20three%20key%20principles%3A%0Aquery%20relevance%2C%20list-wise%20diversity%2C%20and%20sequentiality.%20Existing%20methods%2C%20such%0Aas%20uniform%20frame%20sampling%20and%20query-frame%20matching%2C%20do%20not%20capture%20all%20of%20these%0Aprinciples.%20Thus%2C%20we%20propose%20Markov%20decision%20determinantal%20point%20process%20with%0Adynamic%20programming%20%28MDP3%29%20for%20frame%20selection%2C%20a%20training-free%20and%0Amodel-agnostic%20method%20that%20can%20be%20seamlessly%20integrated%20into%20existing%0AVideo-LLMs.%20Our%20method%20first%20estimates%20frame%20similarities%20conditioned%20on%20the%0Aquery%20using%20a%20conditional%20Gaussian%20kernel%20within%20the%20reproducing%20kernel%20Hilbert%0Aspace~%28RKHS%29.%20We%20then%20apply%20the%20determinantal%20point%20process~%28DPP%29%20to%20the%0Asimilarity%20matrix%20to%20capture%20both%20query%20relevance%20and%20list-wise%20diversity.%20To%0Aincorporate%20sequentiality%2C%20we%20segment%20the%20video%20and%20apply%20DPP%20within%20each%0Asegment%2C%20conditioned%20on%20the%20preceding%20segment%20selection%2C%20modeled%20as%20a%20Markov%0Adecision%20process~%28MDP%29%20for%20allocating%20selection%20sizes%20across%20segments.%0ATheoretically%2C%20MDP3%20provides%20a%20%5C%28%281%20-%201/e%29%5C%29-approximate%20solution%20to%20the%0ANP-hard%20list-wise%20frame%20selection%20problem%20with%20pseudo-polynomial%20time%0Acomplexity%2C%20demonstrating%20its%20efficiency.%20Empirically%2C%20MDP3%20significantly%0Aoutperforms%20existing%20methods%2C%20verifying%20its%20effectiveness%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDP3%253A%2520A%2520Training-free%2520Approach%2520for%2520List-wise%2520Frame%2520Selection%2520in%250A%2520%2520Video-LLMs%26entry.906535625%3DHui%2520Sun%2520and%2520Shiyin%2520Lu%2520and%2520Huanyu%2520Wang%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528Video-LLMs%2529%2520have%2520made%2520significant%2520progress%2520in%250Aunderstanding%2520videos.%2520However%252C%2520processing%2520multiple%2520frames%2520leads%2520to%2520lengthy%250Avisual%2520token%2520sequences%252C%2520presenting%2520challenges%2520such%2520as%2520the%2520limited%2520context%250Alength%2520cannot%2520accommodate%2520the%2520entire%2520video%252C%2520and%2520the%2520inclusion%2520of%2520irrelevant%250Aframes%2520hinders%2520visual%2520perception.%2520Hence%252C%2520effective%2520frame%2520selection%2520is%2520crucial.%250AThis%2520paper%2520emphasizes%2520that%2520frame%2520selection%2520should%2520follow%2520three%2520key%2520principles%253A%250Aquery%2520relevance%252C%2520list-wise%2520diversity%252C%2520and%2520sequentiality.%2520Existing%2520methods%252C%2520such%250Aas%2520uniform%2520frame%2520sampling%2520and%2520query-frame%2520matching%252C%2520do%2520not%2520capture%2520all%2520of%2520these%250Aprinciples.%2520Thus%252C%2520we%2520propose%2520Markov%2520decision%2520determinantal%2520point%2520process%2520with%250Adynamic%2520programming%2520%2528MDP3%2529%2520for%2520frame%2520selection%252C%2520a%2520training-free%2520and%250Amodel-agnostic%2520method%2520that%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%250AVideo-LLMs.%2520Our%2520method%2520first%2520estimates%2520frame%2520similarities%2520conditioned%2520on%2520the%250Aquery%2520using%2520a%2520conditional%2520Gaussian%2520kernel%2520within%2520the%2520reproducing%2520kernel%2520Hilbert%250Aspace~%2528RKHS%2529.%2520We%2520then%2520apply%2520the%2520determinantal%2520point%2520process~%2528DPP%2529%2520to%2520the%250Asimilarity%2520matrix%2520to%2520capture%2520both%2520query%2520relevance%2520and%2520list-wise%2520diversity.%2520To%250Aincorporate%2520sequentiality%252C%2520we%2520segment%2520the%2520video%2520and%2520apply%2520DPP%2520within%2520each%250Asegment%252C%2520conditioned%2520on%2520the%2520preceding%2520segment%2520selection%252C%2520modeled%2520as%2520a%2520Markov%250Adecision%2520process~%2528MDP%2529%2520for%2520allocating%2520selection%2520sizes%2520across%2520segments.%250ATheoretically%252C%2520MDP3%2520provides%2520a%2520%255C%2528%25281%2520-%25201/e%2529%255C%2529-approximate%2520solution%2520to%2520the%250ANP-hard%2520list-wise%2520frame%2520selection%2520problem%2520with%2520pseudo-polynomial%2520time%250Acomplexity%252C%2520demonstrating%2520its%2520efficiency.%2520Empirically%252C%2520MDP3%2520significantly%250Aoutperforms%2520existing%2520methods%252C%2520verifying%2520its%2520effectiveness%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDP3%3A%20A%20Training-free%20Approach%20for%20List-wise%20Frame%20Selection%20in%0A%20%20Video-LLMs&entry.906535625=Hui%20Sun%20and%20Shiyin%20Lu%20and%20Huanyu%20Wang%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Ming%20Li&entry.1292438233=%20%20Video%20large%20language%20models%20%28Video-LLMs%29%20have%20made%20significant%20progress%20in%0Aunderstanding%20videos.%20However%2C%20processing%20multiple%20frames%20leads%20to%20lengthy%0Avisual%20token%20sequences%2C%20presenting%20challenges%20such%20as%20the%20limited%20context%0Alength%20cannot%20accommodate%20the%20entire%20video%2C%20and%20the%20inclusion%20of%20irrelevant%0Aframes%20hinders%20visual%20perception.%20Hence%2C%20effective%20frame%20selection%20is%20crucial.%0AThis%20paper%20emphasizes%20that%20frame%20selection%20should%20follow%20three%20key%20principles%3A%0Aquery%20relevance%2C%20list-wise%20diversity%2C%20and%20sequentiality.%20Existing%20methods%2C%20such%0Aas%20uniform%20frame%20sampling%20and%20query-frame%20matching%2C%20do%20not%20capture%20all%20of%20these%0Aprinciples.%20Thus%2C%20we%20propose%20Markov%20decision%20determinantal%20point%20process%20with%0Adynamic%20programming%20%28MDP3%29%20for%20frame%20selection%2C%20a%20training-free%20and%0Amodel-agnostic%20method%20that%20can%20be%20seamlessly%20integrated%20into%20existing%0AVideo-LLMs.%20Our%20method%20first%20estimates%20frame%20similarities%20conditioned%20on%20the%0Aquery%20using%20a%20conditional%20Gaussian%20kernel%20within%20the%20reproducing%20kernel%20Hilbert%0Aspace~%28RKHS%29.%20We%20then%20apply%20the%20determinantal%20point%20process~%28DPP%29%20to%20the%0Asimilarity%20matrix%20to%20capture%20both%20query%20relevance%20and%20list-wise%20diversity.%20To%0Aincorporate%20sequentiality%2C%20we%20segment%20the%20video%20and%20apply%20DPP%20within%20each%0Asegment%2C%20conditioned%20on%20the%20preceding%20segment%20selection%2C%20modeled%20as%20a%20Markov%0Adecision%20process~%28MDP%29%20for%20allocating%20selection%20sizes%20across%20segments.%0ATheoretically%2C%20MDP3%20provides%20a%20%5C%28%281%20-%201/e%29%5C%29-approximate%20solution%20to%20the%0ANP-hard%20list-wise%20frame%20selection%20problem%20with%20pseudo-polynomial%20time%0Acomplexity%2C%20demonstrating%20its%20efficiency.%20Empirically%2C%20MDP3%20significantly%0Aoutperforms%20existing%20methods%2C%20verifying%20its%20effectiveness%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02885v1&entry.124074799=Read"},
{"title": "SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized\n  Zero-Shot Learning", "author": "William Heyden and Habib Ullah and M. Salman Siddiqui and Fadi Al Machot", "abstract": "  Zero-Shot Learning (ZSL) presents the challenge of identifying categories not\nseen during training. This task is crucial in domains where it is costly,\nprohibited, or simply not feasible to collect training data. ZSL depends on a\nmapping between the visual space and available semantic information. Prior\nworks learn a mapping between spaces that can be exploited during inference. We\ncontend, however, that the disparity between meticulously curated semantic\nspaces and the inherently noisy nature of real-world data remains a substantial\nand unresolved challenge. In this paper, we address this by introducing a\nSemantic Encoder-Enhanced Representations for Zero-Shot Learning (SEER-ZSL). We\npropose a hybrid strategy to address the generalization gap. First, we aim to\ndistill meaningful semantic information using a probabilistic encoder,\nenhancing the semantic consistency and robustness. Second, we distill the\nvisual space by exploiting the learned data distribution through an\nadversarially trained generator. Finally, we align the distilled information,\nenabling a mapping of unseen categories onto the true data manifold. We\ndemonstrate empirically that this approach yields a model that outperforms the\nstate-of-the-art benchmarks in terms of both generalization and benchmarks\nacross diverse settings with small, medium, and large datasets. The complete\ncode is available on GitHub.\n", "link": "http://arxiv.org/abs/2312.13100v2", "date": "2025-01-06", "relevancy": 2.7154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEER-ZSL%3A%20Semantic%20Encoder-Enhanced%20Representations%20for%20Generalized%0A%20%20Zero-Shot%20Learning&body=Title%3A%20SEER-ZSL%3A%20Semantic%20Encoder-Enhanced%20Representations%20for%20Generalized%0A%20%20Zero-Shot%20Learning%0AAuthor%3A%20William%20Heyden%20and%20Habib%20Ullah%20and%20M.%20Salman%20Siddiqui%20and%20Fadi%20Al%20Machot%0AAbstract%3A%20%20%20Zero-Shot%20Learning%20%28ZSL%29%20presents%20the%20challenge%20of%20identifying%20categories%20not%0Aseen%20during%20training.%20This%20task%20is%20crucial%20in%20domains%20where%20it%20is%20costly%2C%0Aprohibited%2C%20or%20simply%20not%20feasible%20to%20collect%20training%20data.%20ZSL%20depends%20on%20a%0Amapping%20between%20the%20visual%20space%20and%20available%20semantic%20information.%20Prior%0Aworks%20learn%20a%20mapping%20between%20spaces%20that%20can%20be%20exploited%20during%20inference.%20We%0Acontend%2C%20however%2C%20that%20the%20disparity%20between%20meticulously%20curated%20semantic%0Aspaces%20and%20the%20inherently%20noisy%20nature%20of%20real-world%20data%20remains%20a%20substantial%0Aand%20unresolved%20challenge.%20In%20this%20paper%2C%20we%20address%20this%20by%20introducing%20a%0ASemantic%20Encoder-Enhanced%20Representations%20for%20Zero-Shot%20Learning%20%28SEER-ZSL%29.%20We%0Apropose%20a%20hybrid%20strategy%20to%20address%20the%20generalization%20gap.%20First%2C%20we%20aim%20to%0Adistill%20meaningful%20semantic%20information%20using%20a%20probabilistic%20encoder%2C%0Aenhancing%20the%20semantic%20consistency%20and%20robustness.%20Second%2C%20we%20distill%20the%0Avisual%20space%20by%20exploiting%20the%20learned%20data%20distribution%20through%20an%0Aadversarially%20trained%20generator.%20Finally%2C%20we%20align%20the%20distilled%20information%2C%0Aenabling%20a%20mapping%20of%20unseen%20categories%20onto%20the%20true%20data%20manifold.%20We%0Ademonstrate%20empirically%20that%20this%20approach%20yields%20a%20model%20that%20outperforms%20the%0Astate-of-the-art%20benchmarks%20in%20terms%20of%20both%20generalization%20and%20benchmarks%0Aacross%20diverse%20settings%20with%20small%2C%20medium%2C%20and%20large%20datasets.%20The%20complete%0Acode%20is%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEER-ZSL%253A%2520Semantic%2520Encoder-Enhanced%2520Representations%2520for%2520Generalized%250A%2520%2520Zero-Shot%2520Learning%26entry.906535625%3DWilliam%2520Heyden%2520and%2520Habib%2520Ullah%2520and%2520M.%2520Salman%2520Siddiqui%2520and%2520Fadi%2520Al%2520Machot%26entry.1292438233%3D%2520%2520Zero-Shot%2520Learning%2520%2528ZSL%2529%2520presents%2520the%2520challenge%2520of%2520identifying%2520categories%2520not%250Aseen%2520during%2520training.%2520This%2520task%2520is%2520crucial%2520in%2520domains%2520where%2520it%2520is%2520costly%252C%250Aprohibited%252C%2520or%2520simply%2520not%2520feasible%2520to%2520collect%2520training%2520data.%2520ZSL%2520depends%2520on%2520a%250Amapping%2520between%2520the%2520visual%2520space%2520and%2520available%2520semantic%2520information.%2520Prior%250Aworks%2520learn%2520a%2520mapping%2520between%2520spaces%2520that%2520can%2520be%2520exploited%2520during%2520inference.%2520We%250Acontend%252C%2520however%252C%2520that%2520the%2520disparity%2520between%2520meticulously%2520curated%2520semantic%250Aspaces%2520and%2520the%2520inherently%2520noisy%2520nature%2520of%2520real-world%2520data%2520remains%2520a%2520substantial%250Aand%2520unresolved%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520by%2520introducing%2520a%250ASemantic%2520Encoder-Enhanced%2520Representations%2520for%2520Zero-Shot%2520Learning%2520%2528SEER-ZSL%2529.%2520We%250Apropose%2520a%2520hybrid%2520strategy%2520to%2520address%2520the%2520generalization%2520gap.%2520First%252C%2520we%2520aim%2520to%250Adistill%2520meaningful%2520semantic%2520information%2520using%2520a%2520probabilistic%2520encoder%252C%250Aenhancing%2520the%2520semantic%2520consistency%2520and%2520robustness.%2520Second%252C%2520we%2520distill%2520the%250Avisual%2520space%2520by%2520exploiting%2520the%2520learned%2520data%2520distribution%2520through%2520an%250Aadversarially%2520trained%2520generator.%2520Finally%252C%2520we%2520align%2520the%2520distilled%2520information%252C%250Aenabling%2520a%2520mapping%2520of%2520unseen%2520categories%2520onto%2520the%2520true%2520data%2520manifold.%2520We%250Ademonstrate%2520empirically%2520that%2520this%2520approach%2520yields%2520a%2520model%2520that%2520outperforms%2520the%250Astate-of-the-art%2520benchmarks%2520in%2520terms%2520of%2520both%2520generalization%2520and%2520benchmarks%250Aacross%2520diverse%2520settings%2520with%2520small%252C%2520medium%252C%2520and%2520large%2520datasets.%2520The%2520complete%250Acode%2520is%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEER-ZSL%3A%20Semantic%20Encoder-Enhanced%20Representations%20for%20Generalized%0A%20%20Zero-Shot%20Learning&entry.906535625=William%20Heyden%20and%20Habib%20Ullah%20and%20M.%20Salman%20Siddiqui%20and%20Fadi%20Al%20Machot&entry.1292438233=%20%20Zero-Shot%20Learning%20%28ZSL%29%20presents%20the%20challenge%20of%20identifying%20categories%20not%0Aseen%20during%20training.%20This%20task%20is%20crucial%20in%20domains%20where%20it%20is%20costly%2C%0Aprohibited%2C%20or%20simply%20not%20feasible%20to%20collect%20training%20data.%20ZSL%20depends%20on%20a%0Amapping%20between%20the%20visual%20space%20and%20available%20semantic%20information.%20Prior%0Aworks%20learn%20a%20mapping%20between%20spaces%20that%20can%20be%20exploited%20during%20inference.%20We%0Acontend%2C%20however%2C%20that%20the%20disparity%20between%20meticulously%20curated%20semantic%0Aspaces%20and%20the%20inherently%20noisy%20nature%20of%20real-world%20data%20remains%20a%20substantial%0Aand%20unresolved%20challenge.%20In%20this%20paper%2C%20we%20address%20this%20by%20introducing%20a%0ASemantic%20Encoder-Enhanced%20Representations%20for%20Zero-Shot%20Learning%20%28SEER-ZSL%29.%20We%0Apropose%20a%20hybrid%20strategy%20to%20address%20the%20generalization%20gap.%20First%2C%20we%20aim%20to%0Adistill%20meaningful%20semantic%20information%20using%20a%20probabilistic%20encoder%2C%0Aenhancing%20the%20semantic%20consistency%20and%20robustness.%20Second%2C%20we%20distill%20the%0Avisual%20space%20by%20exploiting%20the%20learned%20data%20distribution%20through%20an%0Aadversarially%20trained%20generator.%20Finally%2C%20we%20align%20the%20distilled%20information%2C%0Aenabling%20a%20mapping%20of%20unseen%20categories%20onto%20the%20true%20data%20manifold.%20We%0Ademonstrate%20empirically%20that%20this%20approach%20yields%20a%20model%20that%20outperforms%20the%0Astate-of-the-art%20benchmarks%20in%20terms%20of%20both%20generalization%20and%20benchmarks%0Aacross%20diverse%20settings%20with%20small%2C%20medium%2C%20and%20large%20datasets.%20The%20complete%0Acode%20is%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13100v2&entry.124074799=Read"},
{"title": "Large language models for artificial general intelligence (AGI): A\n  survey of foundational principles and approaches", "author": "Alhassan Mumuni and Fuseini Mumuni", "abstract": "  Generative artificial intelligence (AI) systems based on large-scale\npretrained foundation models (PFMs) such as vision-language models, large\nlanguage models (LLMs), diffusion models and vision-language-action (VLA)\nmodels have demonstrated the ability to solve complex and truly non-trivial AI\nproblems in a wide variety of domains and contexts. Multimodal large language\nmodels (MLLMs), in particular, learn from vast and diverse data sources,\nallowing rich and nuanced representations of the world and, thereby, providing\nextensive capabilities, including the ability to reason, engage in meaningful\ndialog; collaborate with humans and other agents to jointly solve complex\nproblems; and understand social and emotional aspects of humans. Despite this\nimpressive feat, the cognitive abilities of state-of-the-art LLMs trained on\nlarge-scale datasets are still superficial and brittle. Consequently, generic\nLLMs are severely limited in their generalist capabilities. A number of\nfoundational problems -- embodiment, symbol grounding, causality and memory --\nare required to be addressed for LLMs to attain human-level general\nintelligence. These concepts are more aligned with human cognition and provide\nLLMs with inherent human-like cognitive properties that support the realization\nof physically-plausible, semantically meaningful, flexible and more\ngeneralizable knowledge and intelligence. In this work, we discuss the\naforementioned foundational issues and survey state-of-the art approaches for\nimplementing these concepts in LLMs. Specifically, we discuss how the\nprinciples of embodiment, symbol grounding, causality and memory can be\nleveraged toward the attainment of artificial general intelligence (AGI) in an\norganic manner.\n", "link": "http://arxiv.org/abs/2501.03151v1", "date": "2025-01-06", "relevancy": 2.7048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20for%20artificial%20general%20intelligence%20%28AGI%29%3A%20A%0A%20%20survey%20of%20foundational%20principles%20and%20approaches&body=Title%3A%20Large%20language%20models%20for%20artificial%20general%20intelligence%20%28AGI%29%3A%20A%0A%20%20survey%20of%20foundational%20principles%20and%20approaches%0AAuthor%3A%20Alhassan%20Mumuni%20and%20Fuseini%20Mumuni%0AAbstract%3A%20%20%20Generative%20artificial%20intelligence%20%28AI%29%20systems%20based%20on%20large-scale%0Apretrained%20foundation%20models%20%28PFMs%29%20such%20as%20vision-language%20models%2C%20large%0Alanguage%20models%20%28LLMs%29%2C%20diffusion%20models%20and%20vision-language-action%20%28VLA%29%0Amodels%20have%20demonstrated%20the%20ability%20to%20solve%20complex%20and%20truly%20non-trivial%20AI%0Aproblems%20in%20a%20wide%20variety%20of%20domains%20and%20contexts.%20Multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20in%20particular%2C%20learn%20from%20vast%20and%20diverse%20data%20sources%2C%0Aallowing%20rich%20and%20nuanced%20representations%20of%20the%20world%20and%2C%20thereby%2C%20providing%0Aextensive%20capabilities%2C%20including%20the%20ability%20to%20reason%2C%20engage%20in%20meaningful%0Adialog%3B%20collaborate%20with%20humans%20and%20other%20agents%20to%20jointly%20solve%20complex%0Aproblems%3B%20and%20understand%20social%20and%20emotional%20aspects%20of%20humans.%20Despite%20this%0Aimpressive%20feat%2C%20the%20cognitive%20abilities%20of%20state-of-the-art%20LLMs%20trained%20on%0Alarge-scale%20datasets%20are%20still%20superficial%20and%20brittle.%20Consequently%2C%20generic%0ALLMs%20are%20severely%20limited%20in%20their%20generalist%20capabilities.%20A%20number%20of%0Afoundational%20problems%20--%20embodiment%2C%20symbol%20grounding%2C%20causality%20and%20memory%20--%0Aare%20required%20to%20be%20addressed%20for%20LLMs%20to%20attain%20human-level%20general%0Aintelligence.%20These%20concepts%20are%20more%20aligned%20with%20human%20cognition%20and%20provide%0ALLMs%20with%20inherent%20human-like%20cognitive%20properties%20that%20support%20the%20realization%0Aof%20physically-plausible%2C%20semantically%20meaningful%2C%20flexible%20and%20more%0Ageneralizable%20knowledge%20and%20intelligence.%20In%20this%20work%2C%20we%20discuss%20the%0Aaforementioned%20foundational%20issues%20and%20survey%20state-of-the%20art%20approaches%20for%0Aimplementing%20these%20concepts%20in%20LLMs.%20Specifically%2C%20we%20discuss%20how%20the%0Aprinciples%20of%20embodiment%2C%20symbol%20grounding%2C%20causality%20and%20memory%20can%20be%0Aleveraged%20toward%20the%20attainment%20of%20artificial%20general%20intelligence%20%28AGI%29%20in%20an%0Aorganic%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520for%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%253A%2520A%250A%2520%2520survey%2520of%2520foundational%2520principles%2520and%2520approaches%26entry.906535625%3DAlhassan%2520Mumuni%2520and%2520Fuseini%2520Mumuni%26entry.1292438233%3D%2520%2520Generative%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520based%2520on%2520large-scale%250Apretrained%2520foundation%2520models%2520%2528PFMs%2529%2520such%2520as%2520vision-language%2520models%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520diffusion%2520models%2520and%2520vision-language-action%2520%2528VLA%2529%250Amodels%2520have%2520demonstrated%2520the%2520ability%2520to%2520solve%2520complex%2520and%2520truly%2520non-trivial%2520AI%250Aproblems%2520in%2520a%2520wide%2520variety%2520of%2520domains%2520and%2520contexts.%2520Multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520in%2520particular%252C%2520learn%2520from%2520vast%2520and%2520diverse%2520data%2520sources%252C%250Aallowing%2520rich%2520and%2520nuanced%2520representations%2520of%2520the%2520world%2520and%252C%2520thereby%252C%2520providing%250Aextensive%2520capabilities%252C%2520including%2520the%2520ability%2520to%2520reason%252C%2520engage%2520in%2520meaningful%250Adialog%253B%2520collaborate%2520with%2520humans%2520and%2520other%2520agents%2520to%2520jointly%2520solve%2520complex%250Aproblems%253B%2520and%2520understand%2520social%2520and%2520emotional%2520aspects%2520of%2520humans.%2520Despite%2520this%250Aimpressive%2520feat%252C%2520the%2520cognitive%2520abilities%2520of%2520state-of-the-art%2520LLMs%2520trained%2520on%250Alarge-scale%2520datasets%2520are%2520still%2520superficial%2520and%2520brittle.%2520Consequently%252C%2520generic%250ALLMs%2520are%2520severely%2520limited%2520in%2520their%2520generalist%2520capabilities.%2520A%2520number%2520of%250Afoundational%2520problems%2520--%2520embodiment%252C%2520symbol%2520grounding%252C%2520causality%2520and%2520memory%2520--%250Aare%2520required%2520to%2520be%2520addressed%2520for%2520LLMs%2520to%2520attain%2520human-level%2520general%250Aintelligence.%2520These%2520concepts%2520are%2520more%2520aligned%2520with%2520human%2520cognition%2520and%2520provide%250ALLMs%2520with%2520inherent%2520human-like%2520cognitive%2520properties%2520that%2520support%2520the%2520realization%250Aof%2520physically-plausible%252C%2520semantically%2520meaningful%252C%2520flexible%2520and%2520more%250Ageneralizable%2520knowledge%2520and%2520intelligence.%2520In%2520this%2520work%252C%2520we%2520discuss%2520the%250Aaforementioned%2520foundational%2520issues%2520and%2520survey%2520state-of-the%2520art%2520approaches%2520for%250Aimplementing%2520these%2520concepts%2520in%2520LLMs.%2520Specifically%252C%2520we%2520discuss%2520how%2520the%250Aprinciples%2520of%2520embodiment%252C%2520symbol%2520grounding%252C%2520causality%2520and%2520memory%2520can%2520be%250Aleveraged%2520toward%2520the%2520attainment%2520of%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%2520in%2520an%250Aorganic%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20for%20artificial%20general%20intelligence%20%28AGI%29%3A%20A%0A%20%20survey%20of%20foundational%20principles%20and%20approaches&entry.906535625=Alhassan%20Mumuni%20and%20Fuseini%20Mumuni&entry.1292438233=%20%20Generative%20artificial%20intelligence%20%28AI%29%20systems%20based%20on%20large-scale%0Apretrained%20foundation%20models%20%28PFMs%29%20such%20as%20vision-language%20models%2C%20large%0Alanguage%20models%20%28LLMs%29%2C%20diffusion%20models%20and%20vision-language-action%20%28VLA%29%0Amodels%20have%20demonstrated%20the%20ability%20to%20solve%20complex%20and%20truly%20non-trivial%20AI%0Aproblems%20in%20a%20wide%20variety%20of%20domains%20and%20contexts.%20Multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20in%20particular%2C%20learn%20from%20vast%20and%20diverse%20data%20sources%2C%0Aallowing%20rich%20and%20nuanced%20representations%20of%20the%20world%20and%2C%20thereby%2C%20providing%0Aextensive%20capabilities%2C%20including%20the%20ability%20to%20reason%2C%20engage%20in%20meaningful%0Adialog%3B%20collaborate%20with%20humans%20and%20other%20agents%20to%20jointly%20solve%20complex%0Aproblems%3B%20and%20understand%20social%20and%20emotional%20aspects%20of%20humans.%20Despite%20this%0Aimpressive%20feat%2C%20the%20cognitive%20abilities%20of%20state-of-the-art%20LLMs%20trained%20on%0Alarge-scale%20datasets%20are%20still%20superficial%20and%20brittle.%20Consequently%2C%20generic%0ALLMs%20are%20severely%20limited%20in%20their%20generalist%20capabilities.%20A%20number%20of%0Afoundational%20problems%20--%20embodiment%2C%20symbol%20grounding%2C%20causality%20and%20memory%20--%0Aare%20required%20to%20be%20addressed%20for%20LLMs%20to%20attain%20human-level%20general%0Aintelligence.%20These%20concepts%20are%20more%20aligned%20with%20human%20cognition%20and%20provide%0ALLMs%20with%20inherent%20human-like%20cognitive%20properties%20that%20support%20the%20realization%0Aof%20physically-plausible%2C%20semantically%20meaningful%2C%20flexible%20and%20more%0Ageneralizable%20knowledge%20and%20intelligence.%20In%20this%20work%2C%20we%20discuss%20the%0Aaforementioned%20foundational%20issues%20and%20survey%20state-of-the%20art%20approaches%20for%0Aimplementing%20these%20concepts%20in%20LLMs.%20Specifically%2C%20we%20discuss%20how%20the%0Aprinciples%20of%20embodiment%2C%20symbol%20grounding%2C%20causality%20and%20memory%20can%20be%0Aleveraged%20toward%20the%20attainment%20of%20artificial%20general%20intelligence%20%28AGI%29%20in%20an%0Aorganic%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03151v1&entry.124074799=Read"},
{"title": "Geometry Restoration and Dewarping of Camera-Captured Document Images", "author": "Valery Istomin and Oleg Pereziabov and Ilya Afanasyev", "abstract": "  This research focuses on developing a method for restoring the topology of\ndigital images of paper documents captured by a camera, using algorithms for\ndetection, segmentation, geometry restoration, and dewarping. Our methodology\nemploys deep learning (DL) for document outline detection, followed by computer\nvision (CV) to create a topological 2D grid using cubic polynomial\ninterpolation and correct nonlinear distortions by remapping the image. Using\nclassical CV methods makes the document topology restoration process more\nefficient and faster, as it requires significantly fewer computational\nresources and memory. We developed a new pipeline for automatic document\ndewarping and reconstruction, along with a framework and annotated dataset to\ndemonstrate its efficiency. Our experiments confirm the promise of our\nmethodology and its superiority over existing benchmarks (including mobile apps\nand popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both\nvisually and in terms of document readability via Optical Character Recognition\n(OCR) and geometry restoration metrics. This paves the way for creating\nhigh-quality digital copies of paper documents and enhancing the efficiency of\nOCR systems. Project page: https://github.com/HorizonParadox/DRCCBI\n", "link": "http://arxiv.org/abs/2501.03145v1", "date": "2025-01-06", "relevancy": 2.7019, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images&body=Title%3A%20Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images%0AAuthor%3A%20Valery%20Istomin%20and%20Oleg%20Pereziabov%20and%20Ilya%20Afanasyev%0AAbstract%3A%20%20%20This%20research%20focuses%20on%20developing%20a%20method%20for%20restoring%20the%20topology%20of%0Adigital%20images%20of%20paper%20documents%20captured%20by%20a%20camera%2C%20using%20algorithms%20for%0Adetection%2C%20segmentation%2C%20geometry%20restoration%2C%20and%20dewarping.%20Our%20methodology%0Aemploys%20deep%20learning%20%28DL%29%20for%20document%20outline%20detection%2C%20followed%20by%20computer%0Avision%20%28CV%29%20to%20create%20a%20topological%202D%20grid%20using%20cubic%20polynomial%0Ainterpolation%20and%20correct%20nonlinear%20distortions%20by%20remapping%20the%20image.%20Using%0Aclassical%20CV%20methods%20makes%20the%20document%20topology%20restoration%20process%20more%0Aefficient%20and%20faster%2C%20as%20it%20requires%20significantly%20fewer%20computational%0Aresources%20and%20memory.%20We%20developed%20a%20new%20pipeline%20for%20automatic%20document%0Adewarping%20and%20reconstruction%2C%20along%20with%20a%20framework%20and%20annotated%20dataset%20to%0Ademonstrate%20its%20efficiency.%20Our%20experiments%20confirm%20the%20promise%20of%20our%0Amethodology%20and%20its%20superiority%20over%20existing%20benchmarks%20%28including%20mobile%20apps%0Aand%20popular%20DL%20solutions%2C%20such%20as%20RectiNet%2C%20DocGeoNet%2C%20and%20DocTr%2B%2B%29%20both%0Avisually%20and%20in%20terms%20of%20document%20readability%20via%20Optical%20Character%20Recognition%0A%28OCR%29%20and%20geometry%20restoration%20metrics.%20This%20paves%20the%20way%20for%20creating%0Ahigh-quality%20digital%20copies%20of%20paper%20documents%20and%20enhancing%20the%20efficiency%20of%0AOCR%20systems.%20Project%20page%3A%20https%3A//github.com/HorizonParadox/DRCCBI%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Restoration%2520and%2520Dewarping%2520of%2520Camera-Captured%2520Document%2520Images%26entry.906535625%3DValery%2520Istomin%2520and%2520Oleg%2520Pereziabov%2520and%2520Ilya%2520Afanasyev%26entry.1292438233%3D%2520%2520This%2520research%2520focuses%2520on%2520developing%2520a%2520method%2520for%2520restoring%2520the%2520topology%2520of%250Adigital%2520images%2520of%2520paper%2520documents%2520captured%2520by%2520a%2520camera%252C%2520using%2520algorithms%2520for%250Adetection%252C%2520segmentation%252C%2520geometry%2520restoration%252C%2520and%2520dewarping.%2520Our%2520methodology%250Aemploys%2520deep%2520learning%2520%2528DL%2529%2520for%2520document%2520outline%2520detection%252C%2520followed%2520by%2520computer%250Avision%2520%2528CV%2529%2520to%2520create%2520a%2520topological%25202D%2520grid%2520using%2520cubic%2520polynomial%250Ainterpolation%2520and%2520correct%2520nonlinear%2520distortions%2520by%2520remapping%2520the%2520image.%2520Using%250Aclassical%2520CV%2520methods%2520makes%2520the%2520document%2520topology%2520restoration%2520process%2520more%250Aefficient%2520and%2520faster%252C%2520as%2520it%2520requires%2520significantly%2520fewer%2520computational%250Aresources%2520and%2520memory.%2520We%2520developed%2520a%2520new%2520pipeline%2520for%2520automatic%2520document%250Adewarping%2520and%2520reconstruction%252C%2520along%2520with%2520a%2520framework%2520and%2520annotated%2520dataset%2520to%250Ademonstrate%2520its%2520efficiency.%2520Our%2520experiments%2520confirm%2520the%2520promise%2520of%2520our%250Amethodology%2520and%2520its%2520superiority%2520over%2520existing%2520benchmarks%2520%2528including%2520mobile%2520apps%250Aand%2520popular%2520DL%2520solutions%252C%2520such%2520as%2520RectiNet%252C%2520DocGeoNet%252C%2520and%2520DocTr%252B%252B%2529%2520both%250Avisually%2520and%2520in%2520terms%2520of%2520document%2520readability%2520via%2520Optical%2520Character%2520Recognition%250A%2528OCR%2529%2520and%2520geometry%2520restoration%2520metrics.%2520This%2520paves%2520the%2520way%2520for%2520creating%250Ahigh-quality%2520digital%2520copies%2520of%2520paper%2520documents%2520and%2520enhancing%2520the%2520efficiency%2520of%250AOCR%2520systems.%2520Project%2520page%253A%2520https%253A//github.com/HorizonParadox/DRCCBI%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Restoration%20and%20Dewarping%20of%20Camera-Captured%20Document%20Images&entry.906535625=Valery%20Istomin%20and%20Oleg%20Pereziabov%20and%20Ilya%20Afanasyev&entry.1292438233=%20%20This%20research%20focuses%20on%20developing%20a%20method%20for%20restoring%20the%20topology%20of%0Adigital%20images%20of%20paper%20documents%20captured%20by%20a%20camera%2C%20using%20algorithms%20for%0Adetection%2C%20segmentation%2C%20geometry%20restoration%2C%20and%20dewarping.%20Our%20methodology%0Aemploys%20deep%20learning%20%28DL%29%20for%20document%20outline%20detection%2C%20followed%20by%20computer%0Avision%20%28CV%29%20to%20create%20a%20topological%202D%20grid%20using%20cubic%20polynomial%0Ainterpolation%20and%20correct%20nonlinear%20distortions%20by%20remapping%20the%20image.%20Using%0Aclassical%20CV%20methods%20makes%20the%20document%20topology%20restoration%20process%20more%0Aefficient%20and%20faster%2C%20as%20it%20requires%20significantly%20fewer%20computational%0Aresources%20and%20memory.%20We%20developed%20a%20new%20pipeline%20for%20automatic%20document%0Adewarping%20and%20reconstruction%2C%20along%20with%20a%20framework%20and%20annotated%20dataset%20to%0Ademonstrate%20its%20efficiency.%20Our%20experiments%20confirm%20the%20promise%20of%20our%0Amethodology%20and%20its%20superiority%20over%20existing%20benchmarks%20%28including%20mobile%20apps%0Aand%20popular%20DL%20solutions%2C%20such%20as%20RectiNet%2C%20DocGeoNet%2C%20and%20DocTr%2B%2B%29%20both%0Avisually%20and%20in%20terms%20of%20document%20readability%20via%20Optical%20Character%20Recognition%0A%28OCR%29%20and%20geometry%20restoration%20metrics.%20This%20paves%20the%20way%20for%20creating%0Ahigh-quality%20digital%20copies%20of%20paper%20documents%20and%20enhancing%20the%20efficiency%20of%0AOCR%20systems.%20Project%20page%3A%20https%3A//github.com/HorizonParadox/DRCCBI%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03145v1&entry.124074799=Read"},
{"title": "GLFC: Unified Global-Local Feature and Contrast Learning with\n  Mamba-Enhanced UNet for Synthetic CT Generation from CBCT", "author": "Xianhao Zhou and Jianghao Wu and Huangxuan Zhao and Lei Chen and Shaoting Zhang and Guotai Wang and Guotai Wang", "abstract": "  Generating synthetic Computed Tomography (CT) images from Cone Beam Computed\nTomography (CBCT) is desirable for improving the image quality of CBCT.\nExisting synthetic CT (sCT) generation methods using Convolutional Neural\nNetworks (CNN) and Transformers often face difficulties in effectively\ncapturing both global and local features and contrasts for high-quality sCT\ngeneration. In this work, we propose a Global-Local Feature and Contrast\nlearning (GLFC) framework for sCT generation. First, a Mamba-Enhanced UNet\n(MEUNet) is introduced by integrating Mamba blocks into the skip connections of\na high-resolution UNet for effective global and local feature learning. Second,\nwe propose a Multiple Contrast Loss (MCL) that calculates synthetic loss at\ndifferent intensity windows to improve quality for both soft tissues and bone\nregions. Experiments on the SynthRAD2023 dataset demonstrate that GLFC improved\nthe SSIM of sCT from 77.91% to 91.50% compared with the original CBCT, and\nsignificantly outperformed several existing methods for sCT generation. The\ncode is available at https://github.com/intelland/GLFC\n", "link": "http://arxiv.org/abs/2501.02992v1", "date": "2025-01-06", "relevancy": 2.6774, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5355}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLFC%3A%20Unified%20Global-Local%20Feature%20and%20Contrast%20Learning%20with%0A%20%20Mamba-Enhanced%20UNet%20for%20Synthetic%20CT%20Generation%20from%20CBCT&body=Title%3A%20GLFC%3A%20Unified%20Global-Local%20Feature%20and%20Contrast%20Learning%20with%0A%20%20Mamba-Enhanced%20UNet%20for%20Synthetic%20CT%20Generation%20from%20CBCT%0AAuthor%3A%20Xianhao%20Zhou%20and%20Jianghao%20Wu%20and%20Huangxuan%20Zhao%20and%20Lei%20Chen%20and%20Shaoting%20Zhang%20and%20Guotai%20Wang%20and%20Guotai%20Wang%0AAbstract%3A%20%20%20Generating%20synthetic%20Computed%20Tomography%20%28CT%29%20images%20from%20Cone%20Beam%20Computed%0ATomography%20%28CBCT%29%20is%20desirable%20for%20improving%20the%20image%20quality%20of%20CBCT.%0AExisting%20synthetic%20CT%20%28sCT%29%20generation%20methods%20using%20Convolutional%20Neural%0ANetworks%20%28CNN%29%20and%20Transformers%20often%20face%20difficulties%20in%20effectively%0Acapturing%20both%20global%20and%20local%20features%20and%20contrasts%20for%20high-quality%20sCT%0Ageneration.%20In%20this%20work%2C%20we%20propose%20a%20Global-Local%20Feature%20and%20Contrast%0Alearning%20%28GLFC%29%20framework%20for%20sCT%20generation.%20First%2C%20a%20Mamba-Enhanced%20UNet%0A%28MEUNet%29%20is%20introduced%20by%20integrating%20Mamba%20blocks%20into%20the%20skip%20connections%20of%0Aa%20high-resolution%20UNet%20for%20effective%20global%20and%20local%20feature%20learning.%20Second%2C%0Awe%20propose%20a%20Multiple%20Contrast%20Loss%20%28MCL%29%20that%20calculates%20synthetic%20loss%20at%0Adifferent%20intensity%20windows%20to%20improve%20quality%20for%20both%20soft%20tissues%20and%20bone%0Aregions.%20Experiments%20on%20the%20SynthRAD2023%20dataset%20demonstrate%20that%20GLFC%20improved%0Athe%20SSIM%20of%20sCT%20from%2077.91%25%20to%2091.50%25%20compared%20with%20the%20original%20CBCT%2C%20and%0Asignificantly%20outperformed%20several%20existing%20methods%20for%20sCT%20generation.%20The%0Acode%20is%20available%20at%20https%3A//github.com/intelland/GLFC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLFC%253A%2520Unified%2520Global-Local%2520Feature%2520and%2520Contrast%2520Learning%2520with%250A%2520%2520Mamba-Enhanced%2520UNet%2520for%2520Synthetic%2520CT%2520Generation%2520from%2520CBCT%26entry.906535625%3DXianhao%2520Zhou%2520and%2520Jianghao%2520Wu%2520and%2520Huangxuan%2520Zhao%2520and%2520Lei%2520Chen%2520and%2520Shaoting%2520Zhang%2520and%2520Guotai%2520Wang%2520and%2520Guotai%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520synthetic%2520Computed%2520Tomography%2520%2528CT%2529%2520images%2520from%2520Cone%2520Beam%2520Computed%250ATomography%2520%2528CBCT%2529%2520is%2520desirable%2520for%2520improving%2520the%2520image%2520quality%2520of%2520CBCT.%250AExisting%2520synthetic%2520CT%2520%2528sCT%2529%2520generation%2520methods%2520using%2520Convolutional%2520Neural%250ANetworks%2520%2528CNN%2529%2520and%2520Transformers%2520often%2520face%2520difficulties%2520in%2520effectively%250Acapturing%2520both%2520global%2520and%2520local%2520features%2520and%2520contrasts%2520for%2520high-quality%2520sCT%250Ageneration.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Global-Local%2520Feature%2520and%2520Contrast%250Alearning%2520%2528GLFC%2529%2520framework%2520for%2520sCT%2520generation.%2520First%252C%2520a%2520Mamba-Enhanced%2520UNet%250A%2528MEUNet%2529%2520is%2520introduced%2520by%2520integrating%2520Mamba%2520blocks%2520into%2520the%2520skip%2520connections%2520of%250Aa%2520high-resolution%2520UNet%2520for%2520effective%2520global%2520and%2520local%2520feature%2520learning.%2520Second%252C%250Awe%2520propose%2520a%2520Multiple%2520Contrast%2520Loss%2520%2528MCL%2529%2520that%2520calculates%2520synthetic%2520loss%2520at%250Adifferent%2520intensity%2520windows%2520to%2520improve%2520quality%2520for%2520both%2520soft%2520tissues%2520and%2520bone%250Aregions.%2520Experiments%2520on%2520the%2520SynthRAD2023%2520dataset%2520demonstrate%2520that%2520GLFC%2520improved%250Athe%2520SSIM%2520of%2520sCT%2520from%252077.91%2525%2520to%252091.50%2525%2520compared%2520with%2520the%2520original%2520CBCT%252C%2520and%250Asignificantly%2520outperformed%2520several%2520existing%2520methods%2520for%2520sCT%2520generation.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/intelland/GLFC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLFC%3A%20Unified%20Global-Local%20Feature%20and%20Contrast%20Learning%20with%0A%20%20Mamba-Enhanced%20UNet%20for%20Synthetic%20CT%20Generation%20from%20CBCT&entry.906535625=Xianhao%20Zhou%20and%20Jianghao%20Wu%20and%20Huangxuan%20Zhao%20and%20Lei%20Chen%20and%20Shaoting%20Zhang%20and%20Guotai%20Wang%20and%20Guotai%20Wang&entry.1292438233=%20%20Generating%20synthetic%20Computed%20Tomography%20%28CT%29%20images%20from%20Cone%20Beam%20Computed%0ATomography%20%28CBCT%29%20is%20desirable%20for%20improving%20the%20image%20quality%20of%20CBCT.%0AExisting%20synthetic%20CT%20%28sCT%29%20generation%20methods%20using%20Convolutional%20Neural%0ANetworks%20%28CNN%29%20and%20Transformers%20often%20face%20difficulties%20in%20effectively%0Acapturing%20both%20global%20and%20local%20features%20and%20contrasts%20for%20high-quality%20sCT%0Ageneration.%20In%20this%20work%2C%20we%20propose%20a%20Global-Local%20Feature%20and%20Contrast%0Alearning%20%28GLFC%29%20framework%20for%20sCT%20generation.%20First%2C%20a%20Mamba-Enhanced%20UNet%0A%28MEUNet%29%20is%20introduced%20by%20integrating%20Mamba%20blocks%20into%20the%20skip%20connections%20of%0Aa%20high-resolution%20UNet%20for%20effective%20global%20and%20local%20feature%20learning.%20Second%2C%0Awe%20propose%20a%20Multiple%20Contrast%20Loss%20%28MCL%29%20that%20calculates%20synthetic%20loss%20at%0Adifferent%20intensity%20windows%20to%20improve%20quality%20for%20both%20soft%20tissues%20and%20bone%0Aregions.%20Experiments%20on%20the%20SynthRAD2023%20dataset%20demonstrate%20that%20GLFC%20improved%0Athe%20SSIM%20of%20sCT%20from%2077.91%25%20to%2091.50%25%20compared%20with%20the%20original%20CBCT%2C%20and%0Asignificantly%20outperformed%20several%20existing%20methods%20for%20sCT%20generation.%20The%0Acode%20is%20available%20at%20https%3A//github.com/intelland/GLFC%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02992v1&entry.124074799=Read"},
{"title": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for\n  Zero-shot Audio Captioning", "author": "Xiquan Li and Wenxi Chen and Ziyang Ma and Xuenan Xu and Yuzhe Liang and Zhisheng Zheng and Qiuqiang Kong and Xie Chen", "abstract": "  While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios.\n", "link": "http://arxiv.org/abs/2410.09472v2", "date": "2025-01-06", "relevancy": 2.6557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRCap%3A%20Decoding%20CLAP%20Latents%20with%20Retrieval-Augmented%20Generation%20for%0A%20%20Zero-shot%20Audio%20Captioning&body=Title%3A%20DRCap%3A%20Decoding%20CLAP%20Latents%20with%20Retrieval-Augmented%20Generation%20for%0A%20%20Zero-shot%20Audio%20Captioning%0AAuthor%3A%20Xiquan%20Li%20and%20Wenxi%20Chen%20and%20Ziyang%20Ma%20and%20Xuenan%20Xu%20and%20Yuzhe%20Liang%20and%20Zhisheng%20Zheng%20and%20Qiuqiang%20Kong%20and%20Xie%20Chen%0AAbstract%3A%20%20%20While%20automated%20audio%20captioning%20%28AAC%29%20has%20made%20notable%20progress%2C%20traditional%0Afully%20supervised%20AAC%20models%20still%20face%20two%20critical%20challenges%3A%20the%20need%20for%0Aexpensive%20audio-text%20pair%20data%20for%20training%20and%20performance%20degradation%20when%0Atransferring%20across%20domains.%20To%20overcome%20these%20limitations%2C%20we%20present%20DRCap%2C%20a%0Adata-efficient%20and%20flexible%20zero-shot%20audio%20captioning%20system%20that%20requires%0Atext-only%20data%20for%20training%20and%20can%20quickly%20adapt%20to%20new%20domains%20without%0Aadditional%20fine-tuning.%20DRCap%20integrates%20a%20contrastive%20language-audio%0Apre-training%20%28CLAP%29%20model%20and%20a%20large-language%20model%20%28LLM%29%20as%20its%20backbone.%0ADuring%20training%2C%20the%20model%20predicts%20the%20ground-truth%20caption%20with%20a%20fixed%20text%0Aencoder%20from%20CLAP%2C%20whereas%2C%20during%20inference%2C%20the%20text%20encoder%20is%20replaced%20with%0Athe%20audio%20encoder%20to%20generate%20captions%20for%20audio%20clips%20in%20a%20zero-shot%20manner.%0ATo%20mitigate%20the%20modality%20gap%20of%20the%20CLAP%20model%2C%20we%20use%20both%20the%20projection%0Astrategy%20from%20the%20encoder%20side%20and%20the%20retrieval-augmented%20generation%20strategy%0Afrom%20the%20decoder%20side.%20Specifically%2C%20audio%20embeddings%20are%20first%20projected%20onto%0Aa%20text%20embedding%20support%20to%20absorb%20extensive%20semantic%20information%20within%20the%0Ajoint%20multi-modal%20space%20of%20CLAP.%20At%20the%20same%20time%2C%20similar%20captions%20retrieved%0Afrom%20a%20datastore%20are%20fed%20as%20prompts%20to%20instruct%20the%20LLM%2C%20incorporating%20external%0Aknowledge%20to%20take%20full%20advantage%20of%20its%20strong%20generative%20capability.%0AConditioned%20on%20both%20the%20projected%20CLAP%20embedding%20and%20the%20retrieved%20similar%0Acaptions%2C%20the%20model%20is%20able%20to%20produce%20a%20more%20accurate%20and%20semantically%20rich%0Atextual%20description.%20By%20tailoring%20the%20text%20embedding%20support%20and%20the%20caption%0Adatastore%20to%20the%20target%20domain%2C%20DRCap%20acquires%20a%20robust%20ability%20to%20adapt%20to%20new%0Adomains%20in%20a%20training-free%20manner.%20Experimental%20results%20demonstrate%20that%20DRCap%0Aoutperforms%20all%20other%20zero-shot%20models%20in%20in-domain%20scenarios%20and%20achieves%0Astate-of-the-art%20performance%20in%20cross-domain%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRCap%253A%2520Decoding%2520CLAP%2520Latents%2520with%2520Retrieval-Augmented%2520Generation%2520for%250A%2520%2520Zero-shot%2520Audio%2520Captioning%26entry.906535625%3DXiquan%2520Li%2520and%2520Wenxi%2520Chen%2520and%2520Ziyang%2520Ma%2520and%2520Xuenan%2520Xu%2520and%2520Yuzhe%2520Liang%2520and%2520Zhisheng%2520Zheng%2520and%2520Qiuqiang%2520Kong%2520and%2520Xie%2520Chen%26entry.1292438233%3D%2520%2520While%2520automated%2520audio%2520captioning%2520%2528AAC%2529%2520has%2520made%2520notable%2520progress%252C%2520traditional%250Afully%2520supervised%2520AAC%2520models%2520still%2520face%2520two%2520critical%2520challenges%253A%2520the%2520need%2520for%250Aexpensive%2520audio-text%2520pair%2520data%2520for%2520training%2520and%2520performance%2520degradation%2520when%250Atransferring%2520across%2520domains.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%2520DRCap%252C%2520a%250Adata-efficient%2520and%2520flexible%2520zero-shot%2520audio%2520captioning%2520system%2520that%2520requires%250Atext-only%2520data%2520for%2520training%2520and%2520can%2520quickly%2520adapt%2520to%2520new%2520domains%2520without%250Aadditional%2520fine-tuning.%2520DRCap%2520integrates%2520a%2520contrastive%2520language-audio%250Apre-training%2520%2528CLAP%2529%2520model%2520and%2520a%2520large-language%2520model%2520%2528LLM%2529%2520as%2520its%2520backbone.%250ADuring%2520training%252C%2520the%2520model%2520predicts%2520the%2520ground-truth%2520caption%2520with%2520a%2520fixed%2520text%250Aencoder%2520from%2520CLAP%252C%2520whereas%252C%2520during%2520inference%252C%2520the%2520text%2520encoder%2520is%2520replaced%2520with%250Athe%2520audio%2520encoder%2520to%2520generate%2520captions%2520for%2520audio%2520clips%2520in%2520a%2520zero-shot%2520manner.%250ATo%2520mitigate%2520the%2520modality%2520gap%2520of%2520the%2520CLAP%2520model%252C%2520we%2520use%2520both%2520the%2520projection%250Astrategy%2520from%2520the%2520encoder%2520side%2520and%2520the%2520retrieval-augmented%2520generation%2520strategy%250Afrom%2520the%2520decoder%2520side.%2520Specifically%252C%2520audio%2520embeddings%2520are%2520first%2520projected%2520onto%250Aa%2520text%2520embedding%2520support%2520to%2520absorb%2520extensive%2520semantic%2520information%2520within%2520the%250Ajoint%2520multi-modal%2520space%2520of%2520CLAP.%2520At%2520the%2520same%2520time%252C%2520similar%2520captions%2520retrieved%250Afrom%2520a%2520datastore%2520are%2520fed%2520as%2520prompts%2520to%2520instruct%2520the%2520LLM%252C%2520incorporating%2520external%250Aknowledge%2520to%2520take%2520full%2520advantage%2520of%2520its%2520strong%2520generative%2520capability.%250AConditioned%2520on%2520both%2520the%2520projected%2520CLAP%2520embedding%2520and%2520the%2520retrieved%2520similar%250Acaptions%252C%2520the%2520model%2520is%2520able%2520to%2520produce%2520a%2520more%2520accurate%2520and%2520semantically%2520rich%250Atextual%2520description.%2520By%2520tailoring%2520the%2520text%2520embedding%2520support%2520and%2520the%2520caption%250Adatastore%2520to%2520the%2520target%2520domain%252C%2520DRCap%2520acquires%2520a%2520robust%2520ability%2520to%2520adapt%2520to%2520new%250Adomains%2520in%2520a%2520training-free%2520manner.%2520Experimental%2520results%2520demonstrate%2520that%2520DRCap%250Aoutperforms%2520all%2520other%2520zero-shot%2520models%2520in%2520in-domain%2520scenarios%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520cross-domain%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRCap%3A%20Decoding%20CLAP%20Latents%20with%20Retrieval-Augmented%20Generation%20for%0A%20%20Zero-shot%20Audio%20Captioning&entry.906535625=Xiquan%20Li%20and%20Wenxi%20Chen%20and%20Ziyang%20Ma%20and%20Xuenan%20Xu%20and%20Yuzhe%20Liang%20and%20Zhisheng%20Zheng%20and%20Qiuqiang%20Kong%20and%20Xie%20Chen&entry.1292438233=%20%20While%20automated%20audio%20captioning%20%28AAC%29%20has%20made%20notable%20progress%2C%20traditional%0Afully%20supervised%20AAC%20models%20still%20face%20two%20critical%20challenges%3A%20the%20need%20for%0Aexpensive%20audio-text%20pair%20data%20for%20training%20and%20performance%20degradation%20when%0Atransferring%20across%20domains.%20To%20overcome%20these%20limitations%2C%20we%20present%20DRCap%2C%20a%0Adata-efficient%20and%20flexible%20zero-shot%20audio%20captioning%20system%20that%20requires%0Atext-only%20data%20for%20training%20and%20can%20quickly%20adapt%20to%20new%20domains%20without%0Aadditional%20fine-tuning.%20DRCap%20integrates%20a%20contrastive%20language-audio%0Apre-training%20%28CLAP%29%20model%20and%20a%20large-language%20model%20%28LLM%29%20as%20its%20backbone.%0ADuring%20training%2C%20the%20model%20predicts%20the%20ground-truth%20caption%20with%20a%20fixed%20text%0Aencoder%20from%20CLAP%2C%20whereas%2C%20during%20inference%2C%20the%20text%20encoder%20is%20replaced%20with%0Athe%20audio%20encoder%20to%20generate%20captions%20for%20audio%20clips%20in%20a%20zero-shot%20manner.%0ATo%20mitigate%20the%20modality%20gap%20of%20the%20CLAP%20model%2C%20we%20use%20both%20the%20projection%0Astrategy%20from%20the%20encoder%20side%20and%20the%20retrieval-augmented%20generation%20strategy%0Afrom%20the%20decoder%20side.%20Specifically%2C%20audio%20embeddings%20are%20first%20projected%20onto%0Aa%20text%20embedding%20support%20to%20absorb%20extensive%20semantic%20information%20within%20the%0Ajoint%20multi-modal%20space%20of%20CLAP.%20At%20the%20same%20time%2C%20similar%20captions%20retrieved%0Afrom%20a%20datastore%20are%20fed%20as%20prompts%20to%20instruct%20the%20LLM%2C%20incorporating%20external%0Aknowledge%20to%20take%20full%20advantage%20of%20its%20strong%20generative%20capability.%0AConditioned%20on%20both%20the%20projected%20CLAP%20embedding%20and%20the%20retrieved%20similar%0Acaptions%2C%20the%20model%20is%20able%20to%20produce%20a%20more%20accurate%20and%20semantically%20rich%0Atextual%20description.%20By%20tailoring%20the%20text%20embedding%20support%20and%20the%20caption%0Adatastore%20to%20the%20target%20domain%2C%20DRCap%20acquires%20a%20robust%20ability%20to%20adapt%20to%20new%0Adomains%20in%20a%20training-free%20manner.%20Experimental%20results%20demonstrate%20that%20DRCap%0Aoutperforms%20all%20other%20zero-shot%20models%20in%20in-domain%20scenarios%20and%20achieves%0Astate-of-the-art%20performance%20in%20cross-domain%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09472v2&entry.124074799=Read"},
{"title": "Normalizing Batch Normalization for Long-Tailed Recognition", "author": "Yuxiang Bao and Guoliang Kang and Linlin Yang and Xiaoyue Duan and Bo Zhao and Baochang Zhang", "abstract": "  In real-world scenarios, the number of training samples across classes\nusually subjects to a long-tailed distribution. The conventionally trained\nnetwork may achieve unexpected inferior performance on the rare class compared\nto the frequent class. Most previous works attempt to rectify the network bias\nfrom the data-level or from the classifier-level. Differently, in this paper,\nwe identify that the bias towards the frequent class may be encoded into\nfeatures, i.e., the rare-specific features which play a key role in\ndiscriminating the rare class are much weaker than the frequent-specific\nfeatures. Based on such an observation, we introduce a simple yet effective\napproach, normalizing the parameters of Batch Normalization (BN) layer to\nexplicitly rectify the feature bias. To achieve this end, we represent the\nWeight/Bias parameters of a BN layer as a vector, normalize it into a unit one\nand multiply the unit vector by a scalar learnable parameter. Through\ndecoupling the direction and magnitude of parameters in BN layer to learn, the\nWeight/Bias exhibits a more balanced distribution and thus the strength of\nfeatures becomes more even. Extensive experiments on various long-tailed\nrecognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist\n2018) show that our method outperforms previous state-of-the-arts remarkably.\nThe code and checkpoints are available at https://github.com/yuxiangbao/NBN.\n", "link": "http://arxiv.org/abs/2501.03122v1", "date": "2025-01-06", "relevancy": 2.6515, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5204}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalizing%20Batch%20Normalization%20for%20Long-Tailed%20Recognition&body=Title%3A%20Normalizing%20Batch%20Normalization%20for%20Long-Tailed%20Recognition%0AAuthor%3A%20Yuxiang%20Bao%20and%20Guoliang%20Kang%20and%20Linlin%20Yang%20and%20Xiaoyue%20Duan%20and%20Bo%20Zhao%20and%20Baochang%20Zhang%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20the%20number%20of%20training%20samples%20across%20classes%0Ausually%20subjects%20to%20a%20long-tailed%20distribution.%20The%20conventionally%20trained%0Anetwork%20may%20achieve%20unexpected%20inferior%20performance%20on%20the%20rare%20class%20compared%0Ato%20the%20frequent%20class.%20Most%20previous%20works%20attempt%20to%20rectify%20the%20network%20bias%0Afrom%20the%20data-level%20or%20from%20the%20classifier-level.%20Differently%2C%20in%20this%20paper%2C%0Awe%20identify%20that%20the%20bias%20towards%20the%20frequent%20class%20may%20be%20encoded%20into%0Afeatures%2C%20i.e.%2C%20the%20rare-specific%20features%20which%20play%20a%20key%20role%20in%0Adiscriminating%20the%20rare%20class%20are%20much%20weaker%20than%20the%20frequent-specific%0Afeatures.%20Based%20on%20such%20an%20observation%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aapproach%2C%20normalizing%20the%20parameters%20of%20Batch%20Normalization%20%28BN%29%20layer%20to%0Aexplicitly%20rectify%20the%20feature%20bias.%20To%20achieve%20this%20end%2C%20we%20represent%20the%0AWeight/Bias%20parameters%20of%20a%20BN%20layer%20as%20a%20vector%2C%20normalize%20it%20into%20a%20unit%20one%0Aand%20multiply%20the%20unit%20vector%20by%20a%20scalar%20learnable%20parameter.%20Through%0Adecoupling%20the%20direction%20and%20magnitude%20of%20parameters%20in%20BN%20layer%20to%20learn%2C%20the%0AWeight/Bias%20exhibits%20a%20more%20balanced%20distribution%20and%20thus%20the%20strength%20of%0Afeatures%20becomes%20more%20even.%20Extensive%20experiments%20on%20various%20long-tailed%0Arecognition%20benchmarks%20%28i.e.%2C%20CIFAR-10/100-LT%2C%20ImageNet-LT%20and%20iNaturalist%0A2018%29%20show%20that%20our%20method%20outperforms%20previous%20state-of-the-arts%20remarkably.%0AThe%20code%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/yuxiangbao/NBN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalizing%2520Batch%2520Normalization%2520for%2520Long-Tailed%2520Recognition%26entry.906535625%3DYuxiang%2520Bao%2520and%2520Guoliang%2520Kang%2520and%2520Linlin%2520Yang%2520and%2520Xiaoyue%2520Duan%2520and%2520Bo%2520Zhao%2520and%2520Baochang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520the%2520number%2520of%2520training%2520samples%2520across%2520classes%250Ausually%2520subjects%2520to%2520a%2520long-tailed%2520distribution.%2520The%2520conventionally%2520trained%250Anetwork%2520may%2520achieve%2520unexpected%2520inferior%2520performance%2520on%2520the%2520rare%2520class%2520compared%250Ato%2520the%2520frequent%2520class.%2520Most%2520previous%2520works%2520attempt%2520to%2520rectify%2520the%2520network%2520bias%250Afrom%2520the%2520data-level%2520or%2520from%2520the%2520classifier-level.%2520Differently%252C%2520in%2520this%2520paper%252C%250Awe%2520identify%2520that%2520the%2520bias%2520towards%2520the%2520frequent%2520class%2520may%2520be%2520encoded%2520into%250Afeatures%252C%2520i.e.%252C%2520the%2520rare-specific%2520features%2520which%2520play%2520a%2520key%2520role%2520in%250Adiscriminating%2520the%2520rare%2520class%2520are%2520much%2520weaker%2520than%2520the%2520frequent-specific%250Afeatures.%2520Based%2520on%2520such%2520an%2520observation%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%250Aapproach%252C%2520normalizing%2520the%2520parameters%2520of%2520Batch%2520Normalization%2520%2528BN%2529%2520layer%2520to%250Aexplicitly%2520rectify%2520the%2520feature%2520bias.%2520To%2520achieve%2520this%2520end%252C%2520we%2520represent%2520the%250AWeight/Bias%2520parameters%2520of%2520a%2520BN%2520layer%2520as%2520a%2520vector%252C%2520normalize%2520it%2520into%2520a%2520unit%2520one%250Aand%2520multiply%2520the%2520unit%2520vector%2520by%2520a%2520scalar%2520learnable%2520parameter.%2520Through%250Adecoupling%2520the%2520direction%2520and%2520magnitude%2520of%2520parameters%2520in%2520BN%2520layer%2520to%2520learn%252C%2520the%250AWeight/Bias%2520exhibits%2520a%2520more%2520balanced%2520distribution%2520and%2520thus%2520the%2520strength%2520of%250Afeatures%2520becomes%2520more%2520even.%2520Extensive%2520experiments%2520on%2520various%2520long-tailed%250Arecognition%2520benchmarks%2520%2528i.e.%252C%2520CIFAR-10/100-LT%252C%2520ImageNet-LT%2520and%2520iNaturalist%250A2018%2529%2520show%2520that%2520our%2520method%2520outperforms%2520previous%2520state-of-the-arts%2520remarkably.%250AThe%2520code%2520and%2520checkpoints%2520are%2520available%2520at%2520https%253A//github.com/yuxiangbao/NBN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalizing%20Batch%20Normalization%20for%20Long-Tailed%20Recognition&entry.906535625=Yuxiang%20Bao%20and%20Guoliang%20Kang%20and%20Linlin%20Yang%20and%20Xiaoyue%20Duan%20and%20Bo%20Zhao%20and%20Baochang%20Zhang&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20the%20number%20of%20training%20samples%20across%20classes%0Ausually%20subjects%20to%20a%20long-tailed%20distribution.%20The%20conventionally%20trained%0Anetwork%20may%20achieve%20unexpected%20inferior%20performance%20on%20the%20rare%20class%20compared%0Ato%20the%20frequent%20class.%20Most%20previous%20works%20attempt%20to%20rectify%20the%20network%20bias%0Afrom%20the%20data-level%20or%20from%20the%20classifier-level.%20Differently%2C%20in%20this%20paper%2C%0Awe%20identify%20that%20the%20bias%20towards%20the%20frequent%20class%20may%20be%20encoded%20into%0Afeatures%2C%20i.e.%2C%20the%20rare-specific%20features%20which%20play%20a%20key%20role%20in%0Adiscriminating%20the%20rare%20class%20are%20much%20weaker%20than%20the%20frequent-specific%0Afeatures.%20Based%20on%20such%20an%20observation%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aapproach%2C%20normalizing%20the%20parameters%20of%20Batch%20Normalization%20%28BN%29%20layer%20to%0Aexplicitly%20rectify%20the%20feature%20bias.%20To%20achieve%20this%20end%2C%20we%20represent%20the%0AWeight/Bias%20parameters%20of%20a%20BN%20layer%20as%20a%20vector%2C%20normalize%20it%20into%20a%20unit%20one%0Aand%20multiply%20the%20unit%20vector%20by%20a%20scalar%20learnable%20parameter.%20Through%0Adecoupling%20the%20direction%20and%20magnitude%20of%20parameters%20in%20BN%20layer%20to%20learn%2C%20the%0AWeight/Bias%20exhibits%20a%20more%20balanced%20distribution%20and%20thus%20the%20strength%20of%0Afeatures%20becomes%20more%20even.%20Extensive%20experiments%20on%20various%20long-tailed%0Arecognition%20benchmarks%20%28i.e.%2C%20CIFAR-10/100-LT%2C%20ImageNet-LT%20and%20iNaturalist%0A2018%29%20show%20that%20our%20method%20outperforms%20previous%20state-of-the-arts%20remarkably.%0AThe%20code%20and%20checkpoints%20are%20available%20at%20https%3A//github.com/yuxiangbao/NBN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03122v1&entry.124074799=Read"},
{"title": "Automated Generation of Challenging Multiple-Choice Questions for Vision\n  Language Model Evaluation", "author": "Yuhui Zhang and Yuchang Su and Yiming Liu and Xiaohan Wang and James Burgess and Elaine Sui and Chenyu Wang and Josiah Aklilu and Alejandro Lozano and Anjiang Wei and Ludwig Schmidt and Serena Yeung-Levy", "abstract": "  The rapid development of vision language models (VLMs) demands rigorous and\nreliable evaluation. However, current visual question answering (VQA)\nbenchmarks often depend on open-ended questions, making accurate evaluation\ndifficult due to the variability in natural language responses. To address\nthis, we introduce AutoConverter, an agentic framework that automatically\nconverts these open-ended questions into multiple-choice format, enabling\nobjective evaluation while reducing the costly question creation process. Our\nexperiments demonstrate that AutoConverter can generate correct and challenging\nmultiple-choice questions, with VLMs demonstrating consistently similar or\nlower accuracy on these questions compared to human-created ones. Using\nAutoConverter, we construct VMCBench, a benchmark created by transforming 20\nexisting VQA datasets into a unified multiple-choice format, totaling 9,018\nquestions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench,\nsetting a new standard for scalable, consistent, and reproducible VLM\nevaluation.\n", "link": "http://arxiv.org/abs/2501.03225v1", "date": "2025-01-06", "relevancy": 2.6423, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.545}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Generation%20of%20Challenging%20Multiple-Choice%20Questions%20for%20Vision%0A%20%20Language%20Model%20Evaluation&body=Title%3A%20Automated%20Generation%20of%20Challenging%20Multiple-Choice%20Questions%20for%20Vision%0A%20%20Language%20Model%20Evaluation%0AAuthor%3A%20Yuhui%20Zhang%20and%20Yuchang%20Su%20and%20Yiming%20Liu%20and%20Xiaohan%20Wang%20and%20James%20Burgess%20and%20Elaine%20Sui%20and%20Chenyu%20Wang%20and%20Josiah%20Aklilu%20and%20Alejandro%20Lozano%20and%20Anjiang%20Wei%20and%20Ludwig%20Schmidt%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20vision%20language%20models%20%28VLMs%29%20demands%20rigorous%20and%0Areliable%20evaluation.%20However%2C%20current%20visual%20question%20answering%20%28VQA%29%0Abenchmarks%20often%20depend%20on%20open-ended%20questions%2C%20making%20accurate%20evaluation%0Adifficult%20due%20to%20the%20variability%20in%20natural%20language%20responses.%20To%20address%0Athis%2C%20we%20introduce%20AutoConverter%2C%20an%20agentic%20framework%20that%20automatically%0Aconverts%20these%20open-ended%20questions%20into%20multiple-choice%20format%2C%20enabling%0Aobjective%20evaluation%20while%20reducing%20the%20costly%20question%20creation%20process.%20Our%0Aexperiments%20demonstrate%20that%20AutoConverter%20can%20generate%20correct%20and%20challenging%0Amultiple-choice%20questions%2C%20with%20VLMs%20demonstrating%20consistently%20similar%20or%0Alower%20accuracy%20on%20these%20questions%20compared%20to%20human-created%20ones.%20Using%0AAutoConverter%2C%20we%20construct%20VMCBench%2C%20a%20benchmark%20created%20by%20transforming%2020%0Aexisting%20VQA%20datasets%20into%20a%20unified%20multiple-choice%20format%2C%20totaling%209%2C018%0Aquestions.%20We%20comprehensively%20evaluate%2033%20state-of-the-art%20VLMs%20on%20VMCBench%2C%0Asetting%20a%20new%20standard%20for%20scalable%2C%20consistent%2C%20and%20reproducible%20VLM%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Generation%2520of%2520Challenging%2520Multiple-Choice%2520Questions%2520for%2520Vision%250A%2520%2520Language%2520Model%2520Evaluation%26entry.906535625%3DYuhui%2520Zhang%2520and%2520Yuchang%2520Su%2520and%2520Yiming%2520Liu%2520and%2520Xiaohan%2520Wang%2520and%2520James%2520Burgess%2520and%2520Elaine%2520Sui%2520and%2520Chenyu%2520Wang%2520and%2520Josiah%2520Aklilu%2520and%2520Alejandro%2520Lozano%2520and%2520Anjiang%2520Wei%2520and%2520Ludwig%2520Schmidt%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520vision%2520language%2520models%2520%2528VLMs%2529%2520demands%2520rigorous%2520and%250Areliable%2520evaluation.%2520However%252C%2520current%2520visual%2520question%2520answering%2520%2528VQA%2529%250Abenchmarks%2520often%2520depend%2520on%2520open-ended%2520questions%252C%2520making%2520accurate%2520evaluation%250Adifficult%2520due%2520to%2520the%2520variability%2520in%2520natural%2520language%2520responses.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520AutoConverter%252C%2520an%2520agentic%2520framework%2520that%2520automatically%250Aconverts%2520these%2520open-ended%2520questions%2520into%2520multiple-choice%2520format%252C%2520enabling%250Aobjective%2520evaluation%2520while%2520reducing%2520the%2520costly%2520question%2520creation%2520process.%2520Our%250Aexperiments%2520demonstrate%2520that%2520AutoConverter%2520can%2520generate%2520correct%2520and%2520challenging%250Amultiple-choice%2520questions%252C%2520with%2520VLMs%2520demonstrating%2520consistently%2520similar%2520or%250Alower%2520accuracy%2520on%2520these%2520questions%2520compared%2520to%2520human-created%2520ones.%2520Using%250AAutoConverter%252C%2520we%2520construct%2520VMCBench%252C%2520a%2520benchmark%2520created%2520by%2520transforming%252020%250Aexisting%2520VQA%2520datasets%2520into%2520a%2520unified%2520multiple-choice%2520format%252C%2520totaling%25209%252C018%250Aquestions.%2520We%2520comprehensively%2520evaluate%252033%2520state-of-the-art%2520VLMs%2520on%2520VMCBench%252C%250Asetting%2520a%2520new%2520standard%2520for%2520scalable%252C%2520consistent%252C%2520and%2520reproducible%2520VLM%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Generation%20of%20Challenging%20Multiple-Choice%20Questions%20for%20Vision%0A%20%20Language%20Model%20Evaluation&entry.906535625=Yuhui%20Zhang%20and%20Yuchang%20Su%20and%20Yiming%20Liu%20and%20Xiaohan%20Wang%20and%20James%20Burgess%20and%20Elaine%20Sui%20and%20Chenyu%20Wang%20and%20Josiah%20Aklilu%20and%20Alejandro%20Lozano%20and%20Anjiang%20Wei%20and%20Ludwig%20Schmidt%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20The%20rapid%20development%20of%20vision%20language%20models%20%28VLMs%29%20demands%20rigorous%20and%0Areliable%20evaluation.%20However%2C%20current%20visual%20question%20answering%20%28VQA%29%0Abenchmarks%20often%20depend%20on%20open-ended%20questions%2C%20making%20accurate%20evaluation%0Adifficult%20due%20to%20the%20variability%20in%20natural%20language%20responses.%20To%20address%0Athis%2C%20we%20introduce%20AutoConverter%2C%20an%20agentic%20framework%20that%20automatically%0Aconverts%20these%20open-ended%20questions%20into%20multiple-choice%20format%2C%20enabling%0Aobjective%20evaluation%20while%20reducing%20the%20costly%20question%20creation%20process.%20Our%0Aexperiments%20demonstrate%20that%20AutoConverter%20can%20generate%20correct%20and%20challenging%0Amultiple-choice%20questions%2C%20with%20VLMs%20demonstrating%20consistently%20similar%20or%0Alower%20accuracy%20on%20these%20questions%20compared%20to%20human-created%20ones.%20Using%0AAutoConverter%2C%20we%20construct%20VMCBench%2C%20a%20benchmark%20created%20by%20transforming%2020%0Aexisting%20VQA%20datasets%20into%20a%20unified%20multiple-choice%20format%2C%20totaling%209%2C018%0Aquestions.%20We%20comprehensively%20evaluate%2033%20state-of-the-art%20VLMs%20on%20VMCBench%2C%0Asetting%20a%20new%20standard%20for%20scalable%2C%20consistent%2C%20and%20reproducible%20VLM%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03225v1&entry.124074799=Read"},
{"title": "Conservation-informed Graph Learning for Spatiotemporal Dynamics\n  Prediction", "author": "Yuan Mi and Pu Ren and Hongteng Xu and Hongsheng Liu and Zidong Wang and Yike Guo and Ji-Rong Wen and Hao Sun and Yang Liu", "abstract": "  Data-centric methods have shown great potential in understanding and\npredicting spatiotemporal dynamics, enabling better design and control of the\nobject system. However, deep learning models often lack interpretability, fail\nto obey intrinsic physics, and struggle to cope with the various domains. While\ngeometry-based methods, e.g., graph neural networks (GNNs), have been proposed\nto further tackle these challenges, they still need to find the implicit\nphysical laws from large datasets and rely excessively on rich labeled data. In\nthis paper, we herein introduce the conservation-informed GNN (CiGNN), an\nend-to-end explainable learning framework, to learn spatiotemporal dynamics\nbased on limited training data. The network is designed to conform to the\ngeneral conservation law via symmetry, where conservative and non-conservative\ninformation passes over a multiscale space enhanced by a latent temporal\nmarching strategy. The efficacy of our model has been verified in various\nspatiotemporal systems based on synthetic and real-world datasets, showing\nsuperiority over baseline models. Results demonstrate that CiGNN exhibits\nremarkable accuracy and generalizability, and is readily applicable to learning\nfor prediction of various spatiotemporal dynamics in a spatial domain with\ncomplex geometry.\n", "link": "http://arxiv.org/abs/2412.20962v3", "date": "2025-01-06", "relevancy": 2.6323, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5314}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction&body=Title%3A%20Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction%0AAuthor%3A%20Yuan%20Mi%20and%20Pu%20Ren%20and%20Hongteng%20Xu%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Hao%20Sun%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Data-centric%20methods%20have%20shown%20great%20potential%20in%20understanding%20and%0Apredicting%20spatiotemporal%20dynamics%2C%20enabling%20better%20design%20and%20control%20of%20the%0Aobject%20system.%20However%2C%20deep%20learning%20models%20often%20lack%20interpretability%2C%20fail%0Ato%20obey%20intrinsic%20physics%2C%20and%20struggle%20to%20cope%20with%20the%20various%20domains.%20While%0Ageometry-based%20methods%2C%20e.g.%2C%20graph%20neural%20networks%20%28GNNs%29%2C%20have%20been%20proposed%0Ato%20further%20tackle%20these%20challenges%2C%20they%20still%20need%20to%20find%20the%20implicit%0Aphysical%20laws%20from%20large%20datasets%20and%20rely%20excessively%20on%20rich%20labeled%20data.%20In%0Athis%20paper%2C%20we%20herein%20introduce%20the%20conservation-informed%20GNN%20%28CiGNN%29%2C%20an%0Aend-to-end%20explainable%20learning%20framework%2C%20to%20learn%20spatiotemporal%20dynamics%0Abased%20on%20limited%20training%20data.%20The%20network%20is%20designed%20to%20conform%20to%20the%0Ageneral%20conservation%20law%20via%20symmetry%2C%20where%20conservative%20and%20non-conservative%0Ainformation%20passes%20over%20a%20multiscale%20space%20enhanced%20by%20a%20latent%20temporal%0Amarching%20strategy.%20The%20efficacy%20of%20our%20model%20has%20been%20verified%20in%20various%0Aspatiotemporal%20systems%20based%20on%20synthetic%20and%20real-world%20datasets%2C%20showing%0Asuperiority%20over%20baseline%20models.%20Results%20demonstrate%20that%20CiGNN%20exhibits%0Aremarkable%20accuracy%20and%20generalizability%2C%20and%20is%20readily%20applicable%20to%20learning%0Afor%20prediction%20of%20various%20spatiotemporal%20dynamics%20in%20a%20spatial%20domain%20with%0Acomplex%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20962v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConservation-informed%2520Graph%2520Learning%2520for%2520Spatiotemporal%2520Dynamics%250A%2520%2520Prediction%26entry.906535625%3DYuan%2520Mi%2520and%2520Pu%2520Ren%2520and%2520Hongteng%2520Xu%2520and%2520Hongsheng%2520Liu%2520and%2520Zidong%2520Wang%2520and%2520Yike%2520Guo%2520and%2520Ji-Rong%2520Wen%2520and%2520Hao%2520Sun%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Data-centric%2520methods%2520have%2520shown%2520great%2520potential%2520in%2520understanding%2520and%250Apredicting%2520spatiotemporal%2520dynamics%252C%2520enabling%2520better%2520design%2520and%2520control%2520of%2520the%250Aobject%2520system.%2520However%252C%2520deep%2520learning%2520models%2520often%2520lack%2520interpretability%252C%2520fail%250Ato%2520obey%2520intrinsic%2520physics%252C%2520and%2520struggle%2520to%2520cope%2520with%2520the%2520various%2520domains.%2520While%250Ageometry-based%2520methods%252C%2520e.g.%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520have%2520been%2520proposed%250Ato%2520further%2520tackle%2520these%2520challenges%252C%2520they%2520still%2520need%2520to%2520find%2520the%2520implicit%250Aphysical%2520laws%2520from%2520large%2520datasets%2520and%2520rely%2520excessively%2520on%2520rich%2520labeled%2520data.%2520In%250Athis%2520paper%252C%2520we%2520herein%2520introduce%2520the%2520conservation-informed%2520GNN%2520%2528CiGNN%2529%252C%2520an%250Aend-to-end%2520explainable%2520learning%2520framework%252C%2520to%2520learn%2520spatiotemporal%2520dynamics%250Abased%2520on%2520limited%2520training%2520data.%2520The%2520network%2520is%2520designed%2520to%2520conform%2520to%2520the%250Ageneral%2520conservation%2520law%2520via%2520symmetry%252C%2520where%2520conservative%2520and%2520non-conservative%250Ainformation%2520passes%2520over%2520a%2520multiscale%2520space%2520enhanced%2520by%2520a%2520latent%2520temporal%250Amarching%2520strategy.%2520The%2520efficacy%2520of%2520our%2520model%2520has%2520been%2520verified%2520in%2520various%250Aspatiotemporal%2520systems%2520based%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520showing%250Asuperiority%2520over%2520baseline%2520models.%2520Results%2520demonstrate%2520that%2520CiGNN%2520exhibits%250Aremarkable%2520accuracy%2520and%2520generalizability%252C%2520and%2520is%2520readily%2520applicable%2520to%2520learning%250Afor%2520prediction%2520of%2520various%2520spatiotemporal%2520dynamics%2520in%2520a%2520spatial%2520domain%2520with%250Acomplex%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20962v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction&entry.906535625=Yuan%20Mi%20and%20Pu%20Ren%20and%20Hongteng%20Xu%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Hao%20Sun%20and%20Yang%20Liu&entry.1292438233=%20%20Data-centric%20methods%20have%20shown%20great%20potential%20in%20understanding%20and%0Apredicting%20spatiotemporal%20dynamics%2C%20enabling%20better%20design%20and%20control%20of%20the%0Aobject%20system.%20However%2C%20deep%20learning%20models%20often%20lack%20interpretability%2C%20fail%0Ato%20obey%20intrinsic%20physics%2C%20and%20struggle%20to%20cope%20with%20the%20various%20domains.%20While%0Ageometry-based%20methods%2C%20e.g.%2C%20graph%20neural%20networks%20%28GNNs%29%2C%20have%20been%20proposed%0Ato%20further%20tackle%20these%20challenges%2C%20they%20still%20need%20to%20find%20the%20implicit%0Aphysical%20laws%20from%20large%20datasets%20and%20rely%20excessively%20on%20rich%20labeled%20data.%20In%0Athis%20paper%2C%20we%20herein%20introduce%20the%20conservation-informed%20GNN%20%28CiGNN%29%2C%20an%0Aend-to-end%20explainable%20learning%20framework%2C%20to%20learn%20spatiotemporal%20dynamics%0Abased%20on%20limited%20training%20data.%20The%20network%20is%20designed%20to%20conform%20to%20the%0Ageneral%20conservation%20law%20via%20symmetry%2C%20where%20conservative%20and%20non-conservative%0Ainformation%20passes%20over%20a%20multiscale%20space%20enhanced%20by%20a%20latent%20temporal%0Amarching%20strategy.%20The%20efficacy%20of%20our%20model%20has%20been%20verified%20in%20various%0Aspatiotemporal%20systems%20based%20on%20synthetic%20and%20real-world%20datasets%2C%20showing%0Asuperiority%20over%20baseline%20models.%20Results%20demonstrate%20that%20CiGNN%20exhibits%0Aremarkable%20accuracy%20and%20generalizability%2C%20and%20is%20readily%20applicable%20to%20learning%0Afor%20prediction%20of%20various%20spatiotemporal%20dynamics%20in%20a%20spatial%20domain%20with%0Acomplex%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20962v3&entry.124074799=Read"},
{"title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for\n  Real-World Video Super-Resolution", "author": "Rui Xie and Yinhong Liu and Penghao Zhou and Chen Zhao and Jun Zhou and Kai Zhang and Zhenyu Zhang and Jian Yang and Zhenheng Yang and Ying Tai", "abstract": "  Image diffusion models have been adapted for real-world video\nsuper-resolution to tackle over-smoothing issues in GAN-based methods. However,\nthese models struggle to maintain temporal consistency, as they are trained on\nstatic images, limiting their ability to capture temporal dynamics effectively.\nIntegrating text-to-video (T2V) models into video super-resolution for improved\ntemporal modeling is straightforward. However, two key challenges remain:\nartifacts introduced by complex degradations in real-world scenarios, and\ncompromised fidelity due to the strong generative capacity of powerful T2V\nmodels (\\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of\nrestored videos, we introduce\\textbf{~\\name}\n(\\textbf{S}patial-\\textbf{T}emporal \\textbf{A}ugmentation with T2V models for\n\\textbf{R}eal-world video super-resolution), a novel approach that leverages\nT2V models for real-world video super-resolution, achieving realistic spatial\ndetails and robust temporal consistency. Specifically, we introduce a Local\nInformation Enhancement Module (LIEM) before the global attention block to\nenrich local details and mitigate degradation artifacts. Moreover, we propose a\nDynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus\non different frequency components across diffusion steps. Extensive experiments\ndemonstrate\\textbf{~\\name}~outperforms state-of-the-art methods on both\nsynthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2501.02976v1", "date": "2025-01-06", "relevancy": 2.5994, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7027}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6458}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR%3A%20Spatial-Temporal%20Augmentation%20with%20Text-to-Video%20Models%20for%0A%20%20Real-World%20Video%20Super-Resolution&body=Title%3A%20STAR%3A%20Spatial-Temporal%20Augmentation%20with%20Text-to-Video%20Models%20for%0A%20%20Real-World%20Video%20Super-Resolution%0AAuthor%3A%20Rui%20Xie%20and%20Yinhong%20Liu%20and%20Penghao%20Zhou%20and%20Chen%20Zhao%20and%20Jun%20Zhou%20and%20Kai%20Zhang%20and%20Zhenyu%20Zhang%20and%20Jian%20Yang%20and%20Zhenheng%20Yang%20and%20Ying%20Tai%0AAbstract%3A%20%20%20Image%20diffusion%20models%20have%20been%20adapted%20for%20real-world%20video%0Asuper-resolution%20to%20tackle%20over-smoothing%20issues%20in%20GAN-based%20methods.%20However%2C%0Athese%20models%20struggle%20to%20maintain%20temporal%20consistency%2C%20as%20they%20are%20trained%20on%0Astatic%20images%2C%20limiting%20their%20ability%20to%20capture%20temporal%20dynamics%20effectively.%0AIntegrating%20text-to-video%20%28T2V%29%20models%20into%20video%20super-resolution%20for%20improved%0Atemporal%20modeling%20is%20straightforward.%20However%2C%20two%20key%20challenges%20remain%3A%0Aartifacts%20introduced%20by%20complex%20degradations%20in%20real-world%20scenarios%2C%20and%0Acompromised%20fidelity%20due%20to%20the%20strong%20generative%20capacity%20of%20powerful%20T2V%0Amodels%20%28%5Ctextit%7Be.g.%7D%2C%20CogVideoX-5B%29.%20To%20enhance%20the%20spatio-temporal%20quality%20of%0Arestored%20videos%2C%20we%20introduce%5Ctextbf%7B~%5Cname%7D%0A%28%5Ctextbf%7BS%7Dpatial-%5Ctextbf%7BT%7Demporal%20%5Ctextbf%7BA%7Dugmentation%20with%20T2V%20models%20for%0A%5Ctextbf%7BR%7Deal-world%20video%20super-resolution%29%2C%20a%20novel%20approach%20that%20leverages%0AT2V%20models%20for%20real-world%20video%20super-resolution%2C%20achieving%20realistic%20spatial%0Adetails%20and%20robust%20temporal%20consistency.%20Specifically%2C%20we%20introduce%20a%20Local%0AInformation%20Enhancement%20Module%20%28LIEM%29%20before%20the%20global%20attention%20block%20to%0Aenrich%20local%20details%20and%20mitigate%20degradation%20artifacts.%20Moreover%2C%20we%20propose%20a%0ADynamic%20Frequency%20%28DF%29%20Loss%20to%20reinforce%20fidelity%2C%20guiding%20the%20model%20to%20focus%0Aon%20different%20frequency%20components%20across%20diffusion%20steps.%20Extensive%20experiments%0Ademonstrate%5Ctextbf%7B~%5Cname%7D~outperforms%20state-of-the-art%20methods%20on%20both%0Asynthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR%253A%2520Spatial-Temporal%2520Augmentation%2520with%2520Text-to-Video%2520Models%2520for%250A%2520%2520Real-World%2520Video%2520Super-Resolution%26entry.906535625%3DRui%2520Xie%2520and%2520Yinhong%2520Liu%2520and%2520Penghao%2520Zhou%2520and%2520Chen%2520Zhao%2520and%2520Jun%2520Zhou%2520and%2520Kai%2520Zhang%2520and%2520Zhenyu%2520Zhang%2520and%2520Jian%2520Yang%2520and%2520Zhenheng%2520Yang%2520and%2520Ying%2520Tai%26entry.1292438233%3D%2520%2520Image%2520diffusion%2520models%2520have%2520been%2520adapted%2520for%2520real-world%2520video%250Asuper-resolution%2520to%2520tackle%2520over-smoothing%2520issues%2520in%2520GAN-based%2520methods.%2520However%252C%250Athese%2520models%2520struggle%2520to%2520maintain%2520temporal%2520consistency%252C%2520as%2520they%2520are%2520trained%2520on%250Astatic%2520images%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520temporal%2520dynamics%2520effectively.%250AIntegrating%2520text-to-video%2520%2528T2V%2529%2520models%2520into%2520video%2520super-resolution%2520for%2520improved%250Atemporal%2520modeling%2520is%2520straightforward.%2520However%252C%2520two%2520key%2520challenges%2520remain%253A%250Aartifacts%2520introduced%2520by%2520complex%2520degradations%2520in%2520real-world%2520scenarios%252C%2520and%250Acompromised%2520fidelity%2520due%2520to%2520the%2520strong%2520generative%2520capacity%2520of%2520powerful%2520T2V%250Amodels%2520%2528%255Ctextit%257Be.g.%257D%252C%2520CogVideoX-5B%2529.%2520To%2520enhance%2520the%2520spatio-temporal%2520quality%2520of%250Arestored%2520videos%252C%2520we%2520introduce%255Ctextbf%257B~%255Cname%257D%250A%2528%255Ctextbf%257BS%257Dpatial-%255Ctextbf%257BT%257Demporal%2520%255Ctextbf%257BA%257Dugmentation%2520with%2520T2V%2520models%2520for%250A%255Ctextbf%257BR%257Deal-world%2520video%2520super-resolution%2529%252C%2520a%2520novel%2520approach%2520that%2520leverages%250AT2V%2520models%2520for%2520real-world%2520video%2520super-resolution%252C%2520achieving%2520realistic%2520spatial%250Adetails%2520and%2520robust%2520temporal%2520consistency.%2520Specifically%252C%2520we%2520introduce%2520a%2520Local%250AInformation%2520Enhancement%2520Module%2520%2528LIEM%2529%2520before%2520the%2520global%2520attention%2520block%2520to%250Aenrich%2520local%2520details%2520and%2520mitigate%2520degradation%2520artifacts.%2520Moreover%252C%2520we%2520propose%2520a%250ADynamic%2520Frequency%2520%2528DF%2529%2520Loss%2520to%2520reinforce%2520fidelity%252C%2520guiding%2520the%2520model%2520to%2520focus%250Aon%2520different%2520frequency%2520components%2520across%2520diffusion%2520steps.%2520Extensive%2520experiments%250Ademonstrate%255Ctextbf%257B~%255Cname%257D~outperforms%2520state-of-the-art%2520methods%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR%3A%20Spatial-Temporal%20Augmentation%20with%20Text-to-Video%20Models%20for%0A%20%20Real-World%20Video%20Super-Resolution&entry.906535625=Rui%20Xie%20and%20Yinhong%20Liu%20and%20Penghao%20Zhou%20and%20Chen%20Zhao%20and%20Jun%20Zhou%20and%20Kai%20Zhang%20and%20Zhenyu%20Zhang%20and%20Jian%20Yang%20and%20Zhenheng%20Yang%20and%20Ying%20Tai&entry.1292438233=%20%20Image%20diffusion%20models%20have%20been%20adapted%20for%20real-world%20video%0Asuper-resolution%20to%20tackle%20over-smoothing%20issues%20in%20GAN-based%20methods.%20However%2C%0Athese%20models%20struggle%20to%20maintain%20temporal%20consistency%2C%20as%20they%20are%20trained%20on%0Astatic%20images%2C%20limiting%20their%20ability%20to%20capture%20temporal%20dynamics%20effectively.%0AIntegrating%20text-to-video%20%28T2V%29%20models%20into%20video%20super-resolution%20for%20improved%0Atemporal%20modeling%20is%20straightforward.%20However%2C%20two%20key%20challenges%20remain%3A%0Aartifacts%20introduced%20by%20complex%20degradations%20in%20real-world%20scenarios%2C%20and%0Acompromised%20fidelity%20due%20to%20the%20strong%20generative%20capacity%20of%20powerful%20T2V%0Amodels%20%28%5Ctextit%7Be.g.%7D%2C%20CogVideoX-5B%29.%20To%20enhance%20the%20spatio-temporal%20quality%20of%0Arestored%20videos%2C%20we%20introduce%5Ctextbf%7B~%5Cname%7D%0A%28%5Ctextbf%7BS%7Dpatial-%5Ctextbf%7BT%7Demporal%20%5Ctextbf%7BA%7Dugmentation%20with%20T2V%20models%20for%0A%5Ctextbf%7BR%7Deal-world%20video%20super-resolution%29%2C%20a%20novel%20approach%20that%20leverages%0AT2V%20models%20for%20real-world%20video%20super-resolution%2C%20achieving%20realistic%20spatial%0Adetails%20and%20robust%20temporal%20consistency.%20Specifically%2C%20we%20introduce%20a%20Local%0AInformation%20Enhancement%20Module%20%28LIEM%29%20before%20the%20global%20attention%20block%20to%0Aenrich%20local%20details%20and%20mitigate%20degradation%20artifacts.%20Moreover%2C%20we%20propose%20a%0ADynamic%20Frequency%20%28DF%29%20Loss%20to%20reinforce%20fidelity%2C%20guiding%20the%20model%20to%20focus%0Aon%20different%20frequency%20components%20across%20diffusion%20steps.%20Extensive%20experiments%0Ademonstrate%5Ctextbf%7B~%5Cname%7D~outperforms%20state-of-the-art%20methods%20on%20both%0Asynthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02976v1&entry.124074799=Read"},
{"title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video\n  Generation", "author": "Guy Yariv and Yuval Kirstain and Amit Zohar and Shelly Sheynin and Yaniv Taigman and Yossi Adi and Sagie Benaim and Adam Polyak", "abstract": "  We consider the task of Image-to-Video (I2V) generation, which involves\ntransforming static images into realistic video sequences based on a textual\ndescription. While recent advancements produce photorealistic outputs, they\nfrequently struggle to create videos with accurate and consistent object\nmotion, especially in multi-object scenarios. To address these limitations, we\npropose a two-stage compositional framework that decomposes I2V generation\ninto: (i) An explicit intermediate representation generation stage, followed by\n(ii) A video generation stage that is conditioned on this representation. Our\nkey innovation is the introduction of a mask-based motion trajectory as an\nintermediate representation, that captures both semantic object information and\nmotion, enabling an expressive but compact representation of motion and\nsemantics. To incorporate the learned representation in the second stage, we\nutilize object-level attention objectives. Specifically, we consider a spatial,\nper-object, masked-cross attention objective, integrating object-specific\nprompts into corresponding latent space regions and a masked spatio-temporal\nself-attention objective, ensuring frame-to-frame consistency for each object.\nWe evaluate our method on challenging benchmarks with multi-object and\nhigh-motion scenarios and empirically demonstrate that the proposed method\nachieves state-of-the-art results in temporal coherence, motion realism, and\ntext-prompt faithfulness. Additionally, we introduce \\benchmark, a new\nchallenging benchmark for single-object and multi-object I2V generation, and\ndemonstrate our method's superiority on this benchmark. Project page is\navailable at https://guyyariv.github.io/TTM/.\n", "link": "http://arxiv.org/abs/2501.03059v1", "date": "2025-01-06", "relevancy": 2.5958, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6564}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through-The-Mask%3A%20Mask-based%20Motion%20Trajectories%20for%20Image-to-Video%0A%20%20Generation&body=Title%3A%20Through-The-Mask%3A%20Mask-based%20Motion%20Trajectories%20for%20Image-to-Video%0A%20%20Generation%0AAuthor%3A%20Guy%20Yariv%20and%20Yuval%20Kirstain%20and%20Amit%20Zohar%20and%20Shelly%20Sheynin%20and%20Yaniv%20Taigman%20and%20Yossi%20Adi%20and%20Sagie%20Benaim%20and%20Adam%20Polyak%0AAbstract%3A%20%20%20We%20consider%20the%20task%20of%20Image-to-Video%20%28I2V%29%20generation%2C%20which%20involves%0Atransforming%20static%20images%20into%20realistic%20video%20sequences%20based%20on%20a%20textual%0Adescription.%20While%20recent%20advancements%20produce%20photorealistic%20outputs%2C%20they%0Afrequently%20struggle%20to%20create%20videos%20with%20accurate%20and%20consistent%20object%0Amotion%2C%20especially%20in%20multi-object%20scenarios.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20two-stage%20compositional%20framework%20that%20decomposes%20I2V%20generation%0Ainto%3A%20%28i%29%20An%20explicit%20intermediate%20representation%20generation%20stage%2C%20followed%20by%0A%28ii%29%20A%20video%20generation%20stage%20that%20is%20conditioned%20on%20this%20representation.%20Our%0Akey%20innovation%20is%20the%20introduction%20of%20a%20mask-based%20motion%20trajectory%20as%20an%0Aintermediate%20representation%2C%20that%20captures%20both%20semantic%20object%20information%20and%0Amotion%2C%20enabling%20an%20expressive%20but%20compact%20representation%20of%20motion%20and%0Asemantics.%20To%20incorporate%20the%20learned%20representation%20in%20the%20second%20stage%2C%20we%0Autilize%20object-level%20attention%20objectives.%20Specifically%2C%20we%20consider%20a%20spatial%2C%0Aper-object%2C%20masked-cross%20attention%20objective%2C%20integrating%20object-specific%0Aprompts%20into%20corresponding%20latent%20space%20regions%20and%20a%20masked%20spatio-temporal%0Aself-attention%20objective%2C%20ensuring%20frame-to-frame%20consistency%20for%20each%20object.%0AWe%20evaluate%20our%20method%20on%20challenging%20benchmarks%20with%20multi-object%20and%0Ahigh-motion%20scenarios%20and%20empirically%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20results%20in%20temporal%20coherence%2C%20motion%20realism%2C%20and%0Atext-prompt%20faithfulness.%20Additionally%2C%20we%20introduce%20%5Cbenchmark%2C%20a%20new%0Achallenging%20benchmark%20for%20single-object%20and%20multi-object%20I2V%20generation%2C%20and%0Ademonstrate%20our%20method%27s%20superiority%20on%20this%20benchmark.%20Project%20page%20is%0Aavailable%20at%20https%3A//guyyariv.github.io/TTM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough-The-Mask%253A%2520Mask-based%2520Motion%2520Trajectories%2520for%2520Image-to-Video%250A%2520%2520Generation%26entry.906535625%3DGuy%2520Yariv%2520and%2520Yuval%2520Kirstain%2520and%2520Amit%2520Zohar%2520and%2520Shelly%2520Sheynin%2520and%2520Yaniv%2520Taigman%2520and%2520Yossi%2520Adi%2520and%2520Sagie%2520Benaim%2520and%2520Adam%2520Polyak%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520task%2520of%2520Image-to-Video%2520%2528I2V%2529%2520generation%252C%2520which%2520involves%250Atransforming%2520static%2520images%2520into%2520realistic%2520video%2520sequences%2520based%2520on%2520a%2520textual%250Adescription.%2520While%2520recent%2520advancements%2520produce%2520photorealistic%2520outputs%252C%2520they%250Afrequently%2520struggle%2520to%2520create%2520videos%2520with%2520accurate%2520and%2520consistent%2520object%250Amotion%252C%2520especially%2520in%2520multi-object%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520two-stage%2520compositional%2520framework%2520that%2520decomposes%2520I2V%2520generation%250Ainto%253A%2520%2528i%2529%2520An%2520explicit%2520intermediate%2520representation%2520generation%2520stage%252C%2520followed%2520by%250A%2528ii%2529%2520A%2520video%2520generation%2520stage%2520that%2520is%2520conditioned%2520on%2520this%2520representation.%2520Our%250Akey%2520innovation%2520is%2520the%2520introduction%2520of%2520a%2520mask-based%2520motion%2520trajectory%2520as%2520an%250Aintermediate%2520representation%252C%2520that%2520captures%2520both%2520semantic%2520object%2520information%2520and%250Amotion%252C%2520enabling%2520an%2520expressive%2520but%2520compact%2520representation%2520of%2520motion%2520and%250Asemantics.%2520To%2520incorporate%2520the%2520learned%2520representation%2520in%2520the%2520second%2520stage%252C%2520we%250Autilize%2520object-level%2520attention%2520objectives.%2520Specifically%252C%2520we%2520consider%2520a%2520spatial%252C%250Aper-object%252C%2520masked-cross%2520attention%2520objective%252C%2520integrating%2520object-specific%250Aprompts%2520into%2520corresponding%2520latent%2520space%2520regions%2520and%2520a%2520masked%2520spatio-temporal%250Aself-attention%2520objective%252C%2520ensuring%2520frame-to-frame%2520consistency%2520for%2520each%2520object.%250AWe%2520evaluate%2520our%2520method%2520on%2520challenging%2520benchmarks%2520with%2520multi-object%2520and%250Ahigh-motion%2520scenarios%2520and%2520empirically%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aachieves%2520state-of-the-art%2520results%2520in%2520temporal%2520coherence%252C%2520motion%2520realism%252C%2520and%250Atext-prompt%2520faithfulness.%2520Additionally%252C%2520we%2520introduce%2520%255Cbenchmark%252C%2520a%2520new%250Achallenging%2520benchmark%2520for%2520single-object%2520and%2520multi-object%2520I2V%2520generation%252C%2520and%250Ademonstrate%2520our%2520method%2527s%2520superiority%2520on%2520this%2520benchmark.%2520Project%2520page%2520is%250Aavailable%2520at%2520https%253A//guyyariv.github.io/TTM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through-The-Mask%3A%20Mask-based%20Motion%20Trajectories%20for%20Image-to-Video%0A%20%20Generation&entry.906535625=Guy%20Yariv%20and%20Yuval%20Kirstain%20and%20Amit%20Zohar%20and%20Shelly%20Sheynin%20and%20Yaniv%20Taigman%20and%20Yossi%20Adi%20and%20Sagie%20Benaim%20and%20Adam%20Polyak&entry.1292438233=%20%20We%20consider%20the%20task%20of%20Image-to-Video%20%28I2V%29%20generation%2C%20which%20involves%0Atransforming%20static%20images%20into%20realistic%20video%20sequences%20based%20on%20a%20textual%0Adescription.%20While%20recent%20advancements%20produce%20photorealistic%20outputs%2C%20they%0Afrequently%20struggle%20to%20create%20videos%20with%20accurate%20and%20consistent%20object%0Amotion%2C%20especially%20in%20multi-object%20scenarios.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20two-stage%20compositional%20framework%20that%20decomposes%20I2V%20generation%0Ainto%3A%20%28i%29%20An%20explicit%20intermediate%20representation%20generation%20stage%2C%20followed%20by%0A%28ii%29%20A%20video%20generation%20stage%20that%20is%20conditioned%20on%20this%20representation.%20Our%0Akey%20innovation%20is%20the%20introduction%20of%20a%20mask-based%20motion%20trajectory%20as%20an%0Aintermediate%20representation%2C%20that%20captures%20both%20semantic%20object%20information%20and%0Amotion%2C%20enabling%20an%20expressive%20but%20compact%20representation%20of%20motion%20and%0Asemantics.%20To%20incorporate%20the%20learned%20representation%20in%20the%20second%20stage%2C%20we%0Autilize%20object-level%20attention%20objectives.%20Specifically%2C%20we%20consider%20a%20spatial%2C%0Aper-object%2C%20masked-cross%20attention%20objective%2C%20integrating%20object-specific%0Aprompts%20into%20corresponding%20latent%20space%20regions%20and%20a%20masked%20spatio-temporal%0Aself-attention%20objective%2C%20ensuring%20frame-to-frame%20consistency%20for%20each%20object.%0AWe%20evaluate%20our%20method%20on%20challenging%20benchmarks%20with%20multi-object%20and%0Ahigh-motion%20scenarios%20and%20empirically%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20results%20in%20temporal%20coherence%2C%20motion%20realism%2C%20and%0Atext-prompt%20faithfulness.%20Additionally%2C%20we%20introduce%20%5Cbenchmark%2C%20a%20new%0Achallenging%20benchmark%20for%20single-object%20and%20multi-object%20I2V%20generation%2C%20and%0Ademonstrate%20our%20method%27s%20superiority%20on%20this%20benchmark.%20Project%20page%20is%0Aavailable%20at%20https%3A//guyyariv.github.io/TTM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03059v1&entry.124074799=Read"},
{"title": "ChronoSense: Exploring Temporal Understanding in Large Language Models\n  with Time Intervals of Events", "author": "Duygu Sezen Islakoglu and Jan-Christoph Kalo", "abstract": "  Large Language Models (LLMs) have achieved remarkable success in various NLP\ntasks, yet they still face significant challenges in reasoning and arithmetic.\nTemporal reasoning, a critical component of natural language understanding, has\nraised increasing research attention. However, comprehensive testing of Allen's\ninterval relations (e.g., before, after, during) -- a fundamental framework for\ntemporal relationships -- remains underexplored. To fill this gap, we present\nChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It\nincludes 16 tasks, focusing on identifying the Allen relation between two\ntemporal events and temporal arithmetic, using both abstract events and\nreal-world data from Wikidata. We assess the performance of seven recent LLMs\nusing this benchmark and the results indicate that models handle Allen\nrelations, even symmetrical ones, quite differently. Moreover, the findings\nsuggest that the models may rely on memorization to answer time-related\nquestions. Overall, the models' low performance highlights the need for\nimproved temporal understanding in LLMs and ChronoSense offers a robust\nframework for future research in this area. Our dataset and the source code are\navailable at https://github.com/duyguislakoglu/chronosense.\n", "link": "http://arxiv.org/abs/2501.03040v1", "date": "2025-01-06", "relevancy": 2.5568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChronoSense%3A%20Exploring%20Temporal%20Understanding%20in%20Large%20Language%20Models%0A%20%20with%20Time%20Intervals%20of%20Events&body=Title%3A%20ChronoSense%3A%20Exploring%20Temporal%20Understanding%20in%20Large%20Language%20Models%0A%20%20with%20Time%20Intervals%20of%20Events%0AAuthor%3A%20Duygu%20Sezen%20Islakoglu%20and%20Jan-Christoph%20Kalo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20various%20NLP%0Atasks%2C%20yet%20they%20still%20face%20significant%20challenges%20in%20reasoning%20and%20arithmetic.%0ATemporal%20reasoning%2C%20a%20critical%20component%20of%20natural%20language%20understanding%2C%20has%0Araised%20increasing%20research%20attention.%20However%2C%20comprehensive%20testing%20of%20Allen%27s%0Ainterval%20relations%20%28e.g.%2C%20before%2C%20after%2C%20during%29%20--%20a%20fundamental%20framework%20for%0Atemporal%20relationships%20--%20remains%20underexplored.%20To%20fill%20this%20gap%2C%20we%20present%0AChronoSense%2C%20a%20new%20benchmark%20for%20evaluating%20LLMs%27%20temporal%20understanding.%20It%0Aincludes%2016%20tasks%2C%20focusing%20on%20identifying%20the%20Allen%20relation%20between%20two%0Atemporal%20events%20and%20temporal%20arithmetic%2C%20using%20both%20abstract%20events%20and%0Areal-world%20data%20from%20Wikidata.%20We%20assess%20the%20performance%20of%20seven%20recent%20LLMs%0Ausing%20this%20benchmark%20and%20the%20results%20indicate%20that%20models%20handle%20Allen%0Arelations%2C%20even%20symmetrical%20ones%2C%20quite%20differently.%20Moreover%2C%20the%20findings%0Asuggest%20that%20the%20models%20may%20rely%20on%20memorization%20to%20answer%20time-related%0Aquestions.%20Overall%2C%20the%20models%27%20low%20performance%20highlights%20the%20need%20for%0Aimproved%20temporal%20understanding%20in%20LLMs%20and%20ChronoSense%20offers%20a%20robust%0Aframework%20for%20future%20research%20in%20this%20area.%20Our%20dataset%20and%20the%20source%20code%20are%0Aavailable%20at%20https%3A//github.com/duyguislakoglu/chronosense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronoSense%253A%2520Exploring%2520Temporal%2520Understanding%2520in%2520Large%2520Language%2520Models%250A%2520%2520with%2520Time%2520Intervals%2520of%2520Events%26entry.906535625%3DDuygu%2520Sezen%2520Islakoglu%2520and%2520Jan-Christoph%2520Kalo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520various%2520NLP%250Atasks%252C%2520yet%2520they%2520still%2520face%2520significant%2520challenges%2520in%2520reasoning%2520and%2520arithmetic.%250ATemporal%2520reasoning%252C%2520a%2520critical%2520component%2520of%2520natural%2520language%2520understanding%252C%2520has%250Araised%2520increasing%2520research%2520attention.%2520However%252C%2520comprehensive%2520testing%2520of%2520Allen%2527s%250Ainterval%2520relations%2520%2528e.g.%252C%2520before%252C%2520after%252C%2520during%2529%2520--%2520a%2520fundamental%2520framework%2520for%250Atemporal%2520relationships%2520--%2520remains%2520underexplored.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%250AChronoSense%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520LLMs%2527%2520temporal%2520understanding.%2520It%250Aincludes%252016%2520tasks%252C%2520focusing%2520on%2520identifying%2520the%2520Allen%2520relation%2520between%2520two%250Atemporal%2520events%2520and%2520temporal%2520arithmetic%252C%2520using%2520both%2520abstract%2520events%2520and%250Areal-world%2520data%2520from%2520Wikidata.%2520We%2520assess%2520the%2520performance%2520of%2520seven%2520recent%2520LLMs%250Ausing%2520this%2520benchmark%2520and%2520the%2520results%2520indicate%2520that%2520models%2520handle%2520Allen%250Arelations%252C%2520even%2520symmetrical%2520ones%252C%2520quite%2520differently.%2520Moreover%252C%2520the%2520findings%250Asuggest%2520that%2520the%2520models%2520may%2520rely%2520on%2520memorization%2520to%2520answer%2520time-related%250Aquestions.%2520Overall%252C%2520the%2520models%2527%2520low%2520performance%2520highlights%2520the%2520need%2520for%250Aimproved%2520temporal%2520understanding%2520in%2520LLMs%2520and%2520ChronoSense%2520offers%2520a%2520robust%250Aframework%2520for%2520future%2520research%2520in%2520this%2520area.%2520Our%2520dataset%2520and%2520the%2520source%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/duyguislakoglu/chronosense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChronoSense%3A%20Exploring%20Temporal%20Understanding%20in%20Large%20Language%20Models%0A%20%20with%20Time%20Intervals%20of%20Events&entry.906535625=Duygu%20Sezen%20Islakoglu%20and%20Jan-Christoph%20Kalo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20in%20various%20NLP%0Atasks%2C%20yet%20they%20still%20face%20significant%20challenges%20in%20reasoning%20and%20arithmetic.%0ATemporal%20reasoning%2C%20a%20critical%20component%20of%20natural%20language%20understanding%2C%20has%0Araised%20increasing%20research%20attention.%20However%2C%20comprehensive%20testing%20of%20Allen%27s%0Ainterval%20relations%20%28e.g.%2C%20before%2C%20after%2C%20during%29%20--%20a%20fundamental%20framework%20for%0Atemporal%20relationships%20--%20remains%20underexplored.%20To%20fill%20this%20gap%2C%20we%20present%0AChronoSense%2C%20a%20new%20benchmark%20for%20evaluating%20LLMs%27%20temporal%20understanding.%20It%0Aincludes%2016%20tasks%2C%20focusing%20on%20identifying%20the%20Allen%20relation%20between%20two%0Atemporal%20events%20and%20temporal%20arithmetic%2C%20using%20both%20abstract%20events%20and%0Areal-world%20data%20from%20Wikidata.%20We%20assess%20the%20performance%20of%20seven%20recent%20LLMs%0Ausing%20this%20benchmark%20and%20the%20results%20indicate%20that%20models%20handle%20Allen%0Arelations%2C%20even%20symmetrical%20ones%2C%20quite%20differently.%20Moreover%2C%20the%20findings%0Asuggest%20that%20the%20models%20may%20rely%20on%20memorization%20to%20answer%20time-related%0Aquestions.%20Overall%2C%20the%20models%27%20low%20performance%20highlights%20the%20need%20for%0Aimproved%20temporal%20understanding%20in%20LLMs%20and%20ChronoSense%20offers%20a%20robust%0Aframework%20for%20future%20research%20in%20this%20area.%20Our%20dataset%20and%20the%20source%20code%20are%0Aavailable%20at%20https%3A//github.com/duyguislakoglu/chronosense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03040v1&entry.124074799=Read"},
{"title": "Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac\n  MRI Segmentation", "author": "Xiaoxiao He and Haizhou Shi and Ligong Han and Chaowei Tan and Bo Liu and Zihao Xu and Meng Ye and Leon Axel and Kang Li and Dimitris Metaxas", "abstract": "  Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health\nproblems in the United States. Precise cardiac image segmentation is crucial\nfor extracting quantitative measures that help categorize cardiac dyssynchrony.\nHowever, achieving high accuracy often depends on centralizing large datasets\nfrom different hospitals, which can be challenging due to privacy concerns. To\nsolve this problem, Federated Learning (FL) is proposed to enable decentralized\nmodel training on such data without exchanging sensitive information. However,\nbandwidth limitations and data heterogeneity remain as significant challenges\nin conventional FL algorithms. In this paper, we propose a novel efficient and\nadaptive federate learning method for cardiac segmentation that improves model\nperformance while reducing the bandwidth requirement. Our method leverages the\nlow-rank adaptation (LoRA) to regularize model weight update and reduce\ncommunication overhead. We also propose a \\mymethod{} aggregation technique to\naddress data heterogeneity among clients. This technique adaptively penalizes\nthe aggregated weights from different clients by comparing the validation\naccuracy in each client, allowing better generalization performance and fast\nlocal adaptation. In-client and cross-client evaluations on public cardiac MR\ndatasets demonstrate the superiority of our method over other LoRA-based\nfederate learning approaches.\n", "link": "http://arxiv.org/abs/2501.03223v1", "date": "2025-01-06", "relevancy": 2.5556, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rate-My-LoRA%3A%20Efficient%20and%20Adaptive%20Federated%20Model%20Tuning%20for%20Cardiac%0A%20%20MRI%20Segmentation&body=Title%3A%20Rate-My-LoRA%3A%20Efficient%20and%20Adaptive%20Federated%20Model%20Tuning%20for%20Cardiac%0A%20%20MRI%20Segmentation%0AAuthor%3A%20Xiaoxiao%20He%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Chaowei%20Tan%20and%20Bo%20Liu%20and%20Zihao%20Xu%20and%20Meng%20Ye%20and%20Leon%20Axel%20and%20Kang%20Li%20and%20Dimitris%20Metaxas%0AAbstract%3A%20%20%20Cardiovascular%20disease%20%28CVD%29%20and%20cardiac%20dyssynchrony%20are%20major%20public%20health%0Aproblems%20in%20the%20United%20States.%20Precise%20cardiac%20image%20segmentation%20is%20crucial%0Afor%20extracting%20quantitative%20measures%20that%20help%20categorize%20cardiac%20dyssynchrony.%0AHowever%2C%20achieving%20high%20accuracy%20often%20depends%20on%20centralizing%20large%20datasets%0Afrom%20different%20hospitals%2C%20which%20can%20be%20challenging%20due%20to%20privacy%20concerns.%20To%0Asolve%20this%20problem%2C%20Federated%20Learning%20%28FL%29%20is%20proposed%20to%20enable%20decentralized%0Amodel%20training%20on%20such%20data%20without%20exchanging%20sensitive%20information.%20However%2C%0Abandwidth%20limitations%20and%20data%20heterogeneity%20remain%20as%20significant%20challenges%0Ain%20conventional%20FL%20algorithms.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20efficient%20and%0Aadaptive%20federate%20learning%20method%20for%20cardiac%20segmentation%20that%20improves%20model%0Aperformance%20while%20reducing%20the%20bandwidth%20requirement.%20Our%20method%20leverages%20the%0Alow-rank%20adaptation%20%28LoRA%29%20to%20regularize%20model%20weight%20update%20and%20reduce%0Acommunication%20overhead.%20We%20also%20propose%20a%20%5Cmymethod%7B%7D%20aggregation%20technique%20to%0Aaddress%20data%20heterogeneity%20among%20clients.%20This%20technique%20adaptively%20penalizes%0Athe%20aggregated%20weights%20from%20different%20clients%20by%20comparing%20the%20validation%0Aaccuracy%20in%20each%20client%2C%20allowing%20better%20generalization%20performance%20and%20fast%0Alocal%20adaptation.%20In-client%20and%20cross-client%20evaluations%20on%20public%20cardiac%20MR%0Adatasets%20demonstrate%20the%20superiority%20of%20our%20method%20over%20other%20LoRA-based%0Afederate%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRate-My-LoRA%253A%2520Efficient%2520and%2520Adaptive%2520Federated%2520Model%2520Tuning%2520for%2520Cardiac%250A%2520%2520MRI%2520Segmentation%26entry.906535625%3DXiaoxiao%2520He%2520and%2520Haizhou%2520Shi%2520and%2520Ligong%2520Han%2520and%2520Chaowei%2520Tan%2520and%2520Bo%2520Liu%2520and%2520Zihao%2520Xu%2520and%2520Meng%2520Ye%2520and%2520Leon%2520Axel%2520and%2520Kang%2520Li%2520and%2520Dimitris%2520Metaxas%26entry.1292438233%3D%2520%2520Cardiovascular%2520disease%2520%2528CVD%2529%2520and%2520cardiac%2520dyssynchrony%2520are%2520major%2520public%2520health%250Aproblems%2520in%2520the%2520United%2520States.%2520Precise%2520cardiac%2520image%2520segmentation%2520is%2520crucial%250Afor%2520extracting%2520quantitative%2520measures%2520that%2520help%2520categorize%2520cardiac%2520dyssynchrony.%250AHowever%252C%2520achieving%2520high%2520accuracy%2520often%2520depends%2520on%2520centralizing%2520large%2520datasets%250Afrom%2520different%2520hospitals%252C%2520which%2520can%2520be%2520challenging%2520due%2520to%2520privacy%2520concerns.%2520To%250Asolve%2520this%2520problem%252C%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520proposed%2520to%2520enable%2520decentralized%250Amodel%2520training%2520on%2520such%2520data%2520without%2520exchanging%2520sensitive%2520information.%2520However%252C%250Abandwidth%2520limitations%2520and%2520data%2520heterogeneity%2520remain%2520as%2520significant%2520challenges%250Ain%2520conventional%2520FL%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520efficient%2520and%250Aadaptive%2520federate%2520learning%2520method%2520for%2520cardiac%2520segmentation%2520that%2520improves%2520model%250Aperformance%2520while%2520reducing%2520the%2520bandwidth%2520requirement.%2520Our%2520method%2520leverages%2520the%250Alow-rank%2520adaptation%2520%2528LoRA%2529%2520to%2520regularize%2520model%2520weight%2520update%2520and%2520reduce%250Acommunication%2520overhead.%2520We%2520also%2520propose%2520a%2520%255Cmymethod%257B%257D%2520aggregation%2520technique%2520to%250Aaddress%2520data%2520heterogeneity%2520among%2520clients.%2520This%2520technique%2520adaptively%2520penalizes%250Athe%2520aggregated%2520weights%2520from%2520different%2520clients%2520by%2520comparing%2520the%2520validation%250Aaccuracy%2520in%2520each%2520client%252C%2520allowing%2520better%2520generalization%2520performance%2520and%2520fast%250Alocal%2520adaptation.%2520In-client%2520and%2520cross-client%2520evaluations%2520on%2520public%2520cardiac%2520MR%250Adatasets%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520other%2520LoRA-based%250Afederate%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rate-My-LoRA%3A%20Efficient%20and%20Adaptive%20Federated%20Model%20Tuning%20for%20Cardiac%0A%20%20MRI%20Segmentation&entry.906535625=Xiaoxiao%20He%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Chaowei%20Tan%20and%20Bo%20Liu%20and%20Zihao%20Xu%20and%20Meng%20Ye%20and%20Leon%20Axel%20and%20Kang%20Li%20and%20Dimitris%20Metaxas&entry.1292438233=%20%20Cardiovascular%20disease%20%28CVD%29%20and%20cardiac%20dyssynchrony%20are%20major%20public%20health%0Aproblems%20in%20the%20United%20States.%20Precise%20cardiac%20image%20segmentation%20is%20crucial%0Afor%20extracting%20quantitative%20measures%20that%20help%20categorize%20cardiac%20dyssynchrony.%0AHowever%2C%20achieving%20high%20accuracy%20often%20depends%20on%20centralizing%20large%20datasets%0Afrom%20different%20hospitals%2C%20which%20can%20be%20challenging%20due%20to%20privacy%20concerns.%20To%0Asolve%20this%20problem%2C%20Federated%20Learning%20%28FL%29%20is%20proposed%20to%20enable%20decentralized%0Amodel%20training%20on%20such%20data%20without%20exchanging%20sensitive%20information.%20However%2C%0Abandwidth%20limitations%20and%20data%20heterogeneity%20remain%20as%20significant%20challenges%0Ain%20conventional%20FL%20algorithms.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20efficient%20and%0Aadaptive%20federate%20learning%20method%20for%20cardiac%20segmentation%20that%20improves%20model%0Aperformance%20while%20reducing%20the%20bandwidth%20requirement.%20Our%20method%20leverages%20the%0Alow-rank%20adaptation%20%28LoRA%29%20to%20regularize%20model%20weight%20update%20and%20reduce%0Acommunication%20overhead.%20We%20also%20propose%20a%20%5Cmymethod%7B%7D%20aggregation%20technique%20to%0Aaddress%20data%20heterogeneity%20among%20clients.%20This%20technique%20adaptively%20penalizes%0Athe%20aggregated%20weights%20from%20different%20clients%20by%20comparing%20the%20validation%0Aaccuracy%20in%20each%20client%2C%20allowing%20better%20generalization%20performance%20and%20fast%0Alocal%20adaptation.%20In-client%20and%20cross-client%20evaluations%20on%20public%20cardiac%20MR%0Adatasets%20demonstrate%20the%20superiority%20of%20our%20method%20over%20other%20LoRA-based%0Afederate%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03223v1&entry.124074799=Read"},
{"title": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot\n  In-Context Learning for SQL2Text", "author": "Ali Al-Lawati and Jason Lucas and Prasenjit Mitra", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious NLP tasks, including semantic parsing, which trans lates natural\nlanguage into formal code representations. However, the reverse process,\ntranslating code into natural language, termed semantic captioning, has\nreceived less attention. This task is becoming increasingly important as LLMs\nare integrated into platforms for code generation, security analysis, and\neducational purposes. In this paper, we focus on the captioning of SQL query\n(SQL2Text) to address the critical need for understanding and explaining SQL\nqueries in an era where LLM-generated code poses potential security risks. We\nrepurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt\nusing GPT-4o to generate multiple additional utterances, which enhances the\nrobustness of the datasets for the reverse task. We conduct our experiments\nusing in-context learning (ICL) based on different sample selection methods,\nemphasizing smaller, more computationally efficient LLMs. Our findings\ndemonstrate that leveraging the inherent graph properties of SQL for ICL sample\nselection significantly outperforms random selection by up to 39% on BLEU score\nand provides better results than alternative methods. Dataset and codes are\npublished: \\url{https://github.com/aliwister/ast-icl}.\n", "link": "http://arxiv.org/abs/2501.03166v1", "date": "2025-01-06", "relevancy": 2.5504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Captioning%3A%20Benchmark%20Dataset%20and%20Graph-Aware%20Few-Shot%0A%20%20In-Context%20Learning%20for%20SQL2Text&body=Title%3A%20Semantic%20Captioning%3A%20Benchmark%20Dataset%20and%20Graph-Aware%20Few-Shot%0A%20%20In-Context%20Learning%20for%20SQL2Text%0AAuthor%3A%20Ali%20Al-Lawati%20and%20Jason%20Lucas%20and%20Prasenjit%20Mitra%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20in%0Avarious%20NLP%20tasks%2C%20including%20semantic%20parsing%2C%20which%20trans%20lates%20natural%0Alanguage%20into%20formal%20code%20representations.%20However%2C%20the%20reverse%20process%2C%0Atranslating%20code%20into%20natural%20language%2C%20termed%20semantic%20captioning%2C%20has%0Areceived%20less%20attention.%20This%20task%20is%20becoming%20increasingly%20important%20as%20LLMs%0Aare%20integrated%20into%20platforms%20for%20code%20generation%2C%20security%20analysis%2C%20and%0Aeducational%20purposes.%20In%20this%20paper%2C%20we%20focus%20on%20the%20captioning%20of%20SQL%20query%0A%28SQL2Text%29%20to%20address%20the%20critical%20need%20for%20understanding%20and%20explaining%20SQL%0Aqueries%20in%20an%20era%20where%20LLM-generated%20code%20poses%20potential%20security%20risks.%20We%0Arepurpose%20Text2SQL%20datasets%20for%20SQL2Text%20by%20introducing%20an%20iterative%20ICL%20prompt%0Ausing%20GPT-4o%20to%20generate%20multiple%20additional%20utterances%2C%20which%20enhances%20the%0Arobustness%20of%20the%20datasets%20for%20the%20reverse%20task.%20We%20conduct%20our%20experiments%0Ausing%20in-context%20learning%20%28ICL%29%20based%20on%20different%20sample%20selection%20methods%2C%0Aemphasizing%20smaller%2C%20more%20computationally%20efficient%20LLMs.%20Our%20findings%0Ademonstrate%20that%20leveraging%20the%20inherent%20graph%20properties%20of%20SQL%20for%20ICL%20sample%0Aselection%20significantly%20outperforms%20random%20selection%20by%20up%20to%2039%25%20on%20BLEU%20score%0Aand%20provides%20better%20results%20than%20alternative%20methods.%20Dataset%20and%20codes%20are%0Apublished%3A%20%5Curl%7Bhttps%3A//github.com/aliwister/ast-icl%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Captioning%253A%2520Benchmark%2520Dataset%2520and%2520Graph-Aware%2520Few-Shot%250A%2520%2520In-Context%2520Learning%2520for%2520SQL2Text%26entry.906535625%3DAli%2520Al-Lawati%2520and%2520Jason%2520Lucas%2520and%2520Prasenjit%2520Mitra%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%250Avarious%2520NLP%2520tasks%252C%2520including%2520semantic%2520parsing%252C%2520which%2520trans%2520lates%2520natural%250Alanguage%2520into%2520formal%2520code%2520representations.%2520However%252C%2520the%2520reverse%2520process%252C%250Atranslating%2520code%2520into%2520natural%2520language%252C%2520termed%2520semantic%2520captioning%252C%2520has%250Areceived%2520less%2520attention.%2520This%2520task%2520is%2520becoming%2520increasingly%2520important%2520as%2520LLMs%250Aare%2520integrated%2520into%2520platforms%2520for%2520code%2520generation%252C%2520security%2520analysis%252C%2520and%250Aeducational%2520purposes.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520captioning%2520of%2520SQL%2520query%250A%2528SQL2Text%2529%2520to%2520address%2520the%2520critical%2520need%2520for%2520understanding%2520and%2520explaining%2520SQL%250Aqueries%2520in%2520an%2520era%2520where%2520LLM-generated%2520code%2520poses%2520potential%2520security%2520risks.%2520We%250Arepurpose%2520Text2SQL%2520datasets%2520for%2520SQL2Text%2520by%2520introducing%2520an%2520iterative%2520ICL%2520prompt%250Ausing%2520GPT-4o%2520to%2520generate%2520multiple%2520additional%2520utterances%252C%2520which%2520enhances%2520the%250Arobustness%2520of%2520the%2520datasets%2520for%2520the%2520reverse%2520task.%2520We%2520conduct%2520our%2520experiments%250Ausing%2520in-context%2520learning%2520%2528ICL%2529%2520based%2520on%2520different%2520sample%2520selection%2520methods%252C%250Aemphasizing%2520smaller%252C%2520more%2520computationally%2520efficient%2520LLMs.%2520Our%2520findings%250Ademonstrate%2520that%2520leveraging%2520the%2520inherent%2520graph%2520properties%2520of%2520SQL%2520for%2520ICL%2520sample%250Aselection%2520significantly%2520outperforms%2520random%2520selection%2520by%2520up%2520to%252039%2525%2520on%2520BLEU%2520score%250Aand%2520provides%2520better%2520results%2520than%2520alternative%2520methods.%2520Dataset%2520and%2520codes%2520are%250Apublished%253A%2520%255Curl%257Bhttps%253A//github.com/aliwister/ast-icl%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Captioning%3A%20Benchmark%20Dataset%20and%20Graph-Aware%20Few-Shot%0A%20%20In-Context%20Learning%20for%20SQL2Text&entry.906535625=Ali%20Al-Lawati%20and%20Jason%20Lucas%20and%20Prasenjit%20Mitra&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20in%0Avarious%20NLP%20tasks%2C%20including%20semantic%20parsing%2C%20which%20trans%20lates%20natural%0Alanguage%20into%20formal%20code%20representations.%20However%2C%20the%20reverse%20process%2C%0Atranslating%20code%20into%20natural%20language%2C%20termed%20semantic%20captioning%2C%20has%0Areceived%20less%20attention.%20This%20task%20is%20becoming%20increasingly%20important%20as%20LLMs%0Aare%20integrated%20into%20platforms%20for%20code%20generation%2C%20security%20analysis%2C%20and%0Aeducational%20purposes.%20In%20this%20paper%2C%20we%20focus%20on%20the%20captioning%20of%20SQL%20query%0A%28SQL2Text%29%20to%20address%20the%20critical%20need%20for%20understanding%20and%20explaining%20SQL%0Aqueries%20in%20an%20era%20where%20LLM-generated%20code%20poses%20potential%20security%20risks.%20We%0Arepurpose%20Text2SQL%20datasets%20for%20SQL2Text%20by%20introducing%20an%20iterative%20ICL%20prompt%0Ausing%20GPT-4o%20to%20generate%20multiple%20additional%20utterances%2C%20which%20enhances%20the%0Arobustness%20of%20the%20datasets%20for%20the%20reverse%20task.%20We%20conduct%20our%20experiments%0Ausing%20in-context%20learning%20%28ICL%29%20based%20on%20different%20sample%20selection%20methods%2C%0Aemphasizing%20smaller%2C%20more%20computationally%20efficient%20LLMs.%20Our%20findings%0Ademonstrate%20that%20leveraging%20the%20inherent%20graph%20properties%20of%20SQL%20for%20ICL%20sample%0Aselection%20significantly%20outperforms%20random%20selection%20by%20up%20to%2039%25%20on%20BLEU%20score%0Aand%20provides%20better%20results%20than%20alternative%20methods.%20Dataset%20and%20codes%20are%0Apublished%3A%20%5Curl%7Bhttps%3A//github.com/aliwister/ast-icl%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03166v1&entry.124074799=Read"},
{"title": "Driving by the Rules: A Benchmark for Integrating Traffic Sign\n  Regulations into Vectorized HD Map", "author": "Xinyuan Chang and Maixuan Xue and Xinran Liu and Zheng Pan and Xing Wei", "abstract": "  Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current online mapping solutions often\nprioritize the construction of the geometric and connectivity layers of HD\nmaps, overlooking the construction of the traffic regulation layer within HD\nmaps. Addressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over $10,000$ annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. Built upon this benchmark and the newly defined task of\nintegrating traffic regulations into online HD maps, we provide modular and\nend-to-end solutions: VLE-MEE and RuleVLM, offering a strong baseline for\nadvancing autonomous driving technology. It fills a critical gap in the\nintegration of traffic sign rules, contributing to the development of reliable\nautonomous driving systems.\n", "link": "http://arxiv.org/abs/2410.23780v2", "date": "2025-01-06", "relevancy": 2.5446, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map&body=Title%3A%20Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map%0AAuthor%3A%20Xinyuan%20Chang%20and%20Maixuan%20Xue%20and%20Xinran%20Liu%20and%20Zheng%20Pan%20and%20Xing%20Wei%0AAbstract%3A%20%20%20Ensuring%20adherence%20to%20traffic%20sign%20regulations%20is%20essential%20for%20both%20human%0Aand%20autonomous%20vehicle%20navigation.%20While%20current%20online%20mapping%20solutions%20often%0Aprioritize%20the%20construction%20of%20the%20geometric%20and%20connectivity%20layers%20of%20HD%0Amaps%2C%20overlooking%20the%20construction%20of%20the%20traffic%20regulation%20layer%20within%20HD%0Amaps.%20Addressing%20this%20gap%2C%20we%20introduce%20MapDR%2C%20a%20novel%20dataset%20designed%20for%20the%0Aextraction%20of%20Driving%20Rules%20from%20traffic%20signs%20and%20their%20association%20with%0Avectorized%2C%20locally%20perceived%20HD%20Maps.%20MapDR%20features%20over%20%2410%2C000%24%20annotated%0Avideo%20clips%20that%20capture%20the%20intricate%20correlation%20between%20traffic%20sign%0Aregulations%20and%20lanes.%20Built%20upon%20this%20benchmark%20and%20the%20newly%20defined%20task%20of%0Aintegrating%20traffic%20regulations%20into%20online%20HD%20maps%2C%20we%20provide%20modular%20and%0Aend-to-end%20solutions%3A%20VLE-MEE%20and%20RuleVLM%2C%20offering%20a%20strong%20baseline%20for%0Aadvancing%20autonomous%20driving%20technology.%20It%20fills%20a%20critical%20gap%20in%20the%0Aintegration%20of%20traffic%20sign%20rules%2C%20contributing%20to%20the%20development%20of%20reliable%0Aautonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520by%2520the%2520Rules%253A%2520A%2520Benchmark%2520for%2520Integrating%2520Traffic%2520Sign%250A%2520%2520Regulations%2520into%2520Vectorized%2520HD%2520Map%26entry.906535625%3DXinyuan%2520Chang%2520and%2520Maixuan%2520Xue%2520and%2520Xinran%2520Liu%2520and%2520Zheng%2520Pan%2520and%2520Xing%2520Wei%26entry.1292438233%3D%2520%2520Ensuring%2520adherence%2520to%2520traffic%2520sign%2520regulations%2520is%2520essential%2520for%2520both%2520human%250Aand%2520autonomous%2520vehicle%2520navigation.%2520While%2520current%2520online%2520mapping%2520solutions%2520often%250Aprioritize%2520the%2520construction%2520of%2520the%2520geometric%2520and%2520connectivity%2520layers%2520of%2520HD%250Amaps%252C%2520overlooking%2520the%2520construction%2520of%2520the%2520traffic%2520regulation%2520layer%2520within%2520HD%250Amaps.%2520Addressing%2520this%2520gap%252C%2520we%2520introduce%2520MapDR%252C%2520a%2520novel%2520dataset%2520designed%2520for%2520the%250Aextraction%2520of%2520Driving%2520Rules%2520from%2520traffic%2520signs%2520and%2520their%2520association%2520with%250Avectorized%252C%2520locally%2520perceived%2520HD%2520Maps.%2520MapDR%2520features%2520over%2520%252410%252C000%2524%2520annotated%250Avideo%2520clips%2520that%2520capture%2520the%2520intricate%2520correlation%2520between%2520traffic%2520sign%250Aregulations%2520and%2520lanes.%2520Built%2520upon%2520this%2520benchmark%2520and%2520the%2520newly%2520defined%2520task%2520of%250Aintegrating%2520traffic%2520regulations%2520into%2520online%2520HD%2520maps%252C%2520we%2520provide%2520modular%2520and%250Aend-to-end%2520solutions%253A%2520VLE-MEE%2520and%2520RuleVLM%252C%2520offering%2520a%2520strong%2520baseline%2520for%250Aadvancing%2520autonomous%2520driving%2520technology.%2520It%2520fills%2520a%2520critical%2520gap%2520in%2520the%250Aintegration%2520of%2520traffic%2520sign%2520rules%252C%2520contributing%2520to%2520the%2520development%2520of%2520reliable%250Aautonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map&entry.906535625=Xinyuan%20Chang%20and%20Maixuan%20Xue%20and%20Xinran%20Liu%20and%20Zheng%20Pan%20and%20Xing%20Wei&entry.1292438233=%20%20Ensuring%20adherence%20to%20traffic%20sign%20regulations%20is%20essential%20for%20both%20human%0Aand%20autonomous%20vehicle%20navigation.%20While%20current%20online%20mapping%20solutions%20often%0Aprioritize%20the%20construction%20of%20the%20geometric%20and%20connectivity%20layers%20of%20HD%0Amaps%2C%20overlooking%20the%20construction%20of%20the%20traffic%20regulation%20layer%20within%20HD%0Amaps.%20Addressing%20this%20gap%2C%20we%20introduce%20MapDR%2C%20a%20novel%20dataset%20designed%20for%20the%0Aextraction%20of%20Driving%20Rules%20from%20traffic%20signs%20and%20their%20association%20with%0Avectorized%2C%20locally%20perceived%20HD%20Maps.%20MapDR%20features%20over%20%2410%2C000%24%20annotated%0Avideo%20clips%20that%20capture%20the%20intricate%20correlation%20between%20traffic%20sign%0Aregulations%20and%20lanes.%20Built%20upon%20this%20benchmark%20and%20the%20newly%20defined%20task%20of%0Aintegrating%20traffic%20regulations%20into%20online%20HD%20maps%2C%20we%20provide%20modular%20and%0Aend-to-end%20solutions%3A%20VLE-MEE%20and%20RuleVLM%2C%20offering%20a%20strong%20baseline%20for%0Aadvancing%20autonomous%20driving%20technology.%20It%20fills%20a%20critical%20gap%20in%20the%0Aintegration%20of%20traffic%20sign%20rules%2C%20contributing%20to%20the%20development%20of%20reliable%0Aautonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23780v2&entry.124074799=Read"},
{"title": "VCEval: Rethinking What is a Good Educational Video and How to\n  Automatically Evaluate It", "author": "Xiaoxuan Zhu and Zhouhong Gu and Sihang Jiang and Zhixu Li and Hongwei Feng and Yanghua Xiao", "abstract": "  Online courses have significantly lowered the barrier to accessing education,\nyet the varying content quality of these videos poses challenges. In this work,\nwe focus on the task of automatically evaluating the quality of video course\ncontent. We have constructed a dataset with a substantial collection of video\ncourses and teaching materials. We propose three evaluation principles and\ndesign a new evaluation framework, \\textit{VCEval}, based on these principles.\nThe task is modeled as a multiple-choice question-answering task, with a\nlanguage model serving as the evaluator. Our method effectively distinguishes\nvideo courses of different content quality and produces a range of\ninterpretable results.\n", "link": "http://arxiv.org/abs/2407.12005v2", "date": "2025-01-06", "relevancy": 2.5363, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCEval%3A%20Rethinking%20What%20is%20a%20Good%20Educational%20Video%20and%20How%20to%0A%20%20Automatically%20Evaluate%20It&body=Title%3A%20VCEval%3A%20Rethinking%20What%20is%20a%20Good%20Educational%20Video%20and%20How%20to%0A%20%20Automatically%20Evaluate%20It%0AAuthor%3A%20Xiaoxuan%20Zhu%20and%20Zhouhong%20Gu%20and%20Sihang%20Jiang%20and%20Zhixu%20Li%20and%20Hongwei%20Feng%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Online%20courses%20have%20significantly%20lowered%20the%20barrier%20to%20accessing%20education%2C%0Ayet%20the%20varying%20content%20quality%20of%20these%20videos%20poses%20challenges.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20task%20of%20automatically%20evaluating%20the%20quality%20of%20video%20course%0Acontent.%20We%20have%20constructed%20a%20dataset%20with%20a%20substantial%20collection%20of%20video%0Acourses%20and%20teaching%20materials.%20We%20propose%20three%20evaluation%20principles%20and%0Adesign%20a%20new%20evaluation%20framework%2C%20%5Ctextit%7BVCEval%7D%2C%20based%20on%20these%20principles.%0AThe%20task%20is%20modeled%20as%20a%20multiple-choice%20question-answering%20task%2C%20with%20a%0Alanguage%20model%20serving%20as%20the%20evaluator.%20Our%20method%20effectively%20distinguishes%0Avideo%20courses%20of%20different%20content%20quality%20and%20produces%20a%20range%20of%0Ainterpretable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCEval%253A%2520Rethinking%2520What%2520is%2520a%2520Good%2520Educational%2520Video%2520and%2520How%2520to%250A%2520%2520Automatically%2520Evaluate%2520It%26entry.906535625%3DXiaoxuan%2520Zhu%2520and%2520Zhouhong%2520Gu%2520and%2520Sihang%2520Jiang%2520and%2520Zhixu%2520Li%2520and%2520Hongwei%2520Feng%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3D%2520%2520Online%2520courses%2520have%2520significantly%2520lowered%2520the%2520barrier%2520to%2520accessing%2520education%252C%250Ayet%2520the%2520varying%2520content%2520quality%2520of%2520these%2520videos%2520poses%2520challenges.%2520In%2520this%2520work%252C%250Awe%2520focus%2520on%2520the%2520task%2520of%2520automatically%2520evaluating%2520the%2520quality%2520of%2520video%2520course%250Acontent.%2520We%2520have%2520constructed%2520a%2520dataset%2520with%2520a%2520substantial%2520collection%2520of%2520video%250Acourses%2520and%2520teaching%2520materials.%2520We%2520propose%2520three%2520evaluation%2520principles%2520and%250Adesign%2520a%2520new%2520evaluation%2520framework%252C%2520%255Ctextit%257BVCEval%257D%252C%2520based%2520on%2520these%2520principles.%250AThe%2520task%2520is%2520modeled%2520as%2520a%2520multiple-choice%2520question-answering%2520task%252C%2520with%2520a%250Alanguage%2520model%2520serving%2520as%2520the%2520evaluator.%2520Our%2520method%2520effectively%2520distinguishes%250Avideo%2520courses%2520of%2520different%2520content%2520quality%2520and%2520produces%2520a%2520range%2520of%250Ainterpretable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCEval%3A%20Rethinking%20What%20is%20a%20Good%20Educational%20Video%20and%20How%20to%0A%20%20Automatically%20Evaluate%20It&entry.906535625=Xiaoxuan%20Zhu%20and%20Zhouhong%20Gu%20and%20Sihang%20Jiang%20and%20Zhixu%20Li%20and%20Hongwei%20Feng%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Online%20courses%20have%20significantly%20lowered%20the%20barrier%20to%20accessing%20education%2C%0Ayet%20the%20varying%20content%20quality%20of%20these%20videos%20poses%20challenges.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20task%20of%20automatically%20evaluating%20the%20quality%20of%20video%20course%0Acontent.%20We%20have%20constructed%20a%20dataset%20with%20a%20substantial%20collection%20of%20video%0Acourses%20and%20teaching%20materials.%20We%20propose%20three%20evaluation%20principles%20and%0Adesign%20a%20new%20evaluation%20framework%2C%20%5Ctextit%7BVCEval%7D%2C%20based%20on%20these%20principles.%0AThe%20task%20is%20modeled%20as%20a%20multiple-choice%20question-answering%20task%2C%20with%20a%0Alanguage%20model%20serving%20as%20the%20evaluator.%20Our%20method%20effectively%20distinguishes%0Avideo%20courses%20of%20different%20content%20quality%20and%20produces%20a%20range%20of%0Ainterpretable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12005v2&entry.124074799=Read"},
{"title": "LOHA: Direct Graph Spectral Contrastive Learning Between Low-pass and\n  High-pass Views", "author": "Ziyun Zou and Yinghui Jiang and Lian Shen and Juan Liu and Xiangrong Liu", "abstract": "  Spectral Graph Neural Networks effectively handle graphs with different\nhomophily levels, with low-pass filter mining feature smoothness and high-pass\nfilter capturing differences. When these distinct filters could naturally form\ntwo opposite views for self-supervised learning, the commonalities between the\ncounterparts for the same node remain unexplored, leading to suboptimal\nperformance. In this paper, a simple yet effective self-supervised contrastive\nframework, LOHA, is proposed to address this gap. LOHA optimally leverages\nlow-pass and high-pass views by embracing \"harmony in diversity\". Rather than\nsolely maximizing the difference between these distinct views, which may lead\nto feature separation, LOHA harmonizes the diversity by treating the\npropagation of graph signals from both views as a composite feature.\nSpecifically, a novel high-dimensional feature named spectral signal trend is\nproposed to serve as the basis for the composite feature, which remains\nrelatively unaffected by changing filters and focuses solely on original\nfeature differences. LOHA achieves an average performance improvement of 2.8%\nover runner-up models on 9 real-world datasets with varying homophily levels.\nNotably, LOHA even surpasses fully-supervised models on several datasets, which\nunderscores the potential of LOHA in advancing the efficacy of spectral GNNs\nfor diverse graph structures.\n", "link": "http://arxiv.org/abs/2501.02969v1", "date": "2025-01-06", "relevancy": 2.5348, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5412}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOHA%3A%20Direct%20Graph%20Spectral%20Contrastive%20Learning%20Between%20Low-pass%20and%0A%20%20High-pass%20Views&body=Title%3A%20LOHA%3A%20Direct%20Graph%20Spectral%20Contrastive%20Learning%20Between%20Low-pass%20and%0A%20%20High-pass%20Views%0AAuthor%3A%20Ziyun%20Zou%20and%20Yinghui%20Jiang%20and%20Lian%20Shen%20and%20Juan%20Liu%20and%20Xiangrong%20Liu%0AAbstract%3A%20%20%20Spectral%20Graph%20Neural%20Networks%20effectively%20handle%20graphs%20with%20different%0Ahomophily%20levels%2C%20with%20low-pass%20filter%20mining%20feature%20smoothness%20and%20high-pass%0Afilter%20capturing%20differences.%20When%20these%20distinct%20filters%20could%20naturally%20form%0Atwo%20opposite%20views%20for%20self-supervised%20learning%2C%20the%20commonalities%20between%20the%0Acounterparts%20for%20the%20same%20node%20remain%20unexplored%2C%20leading%20to%20suboptimal%0Aperformance.%20In%20this%20paper%2C%20a%20simple%20yet%20effective%20self-supervised%20contrastive%0Aframework%2C%20LOHA%2C%20is%20proposed%20to%20address%20this%20gap.%20LOHA%20optimally%20leverages%0Alow-pass%20and%20high-pass%20views%20by%20embracing%20%22harmony%20in%20diversity%22.%20Rather%20than%0Asolely%20maximizing%20the%20difference%20between%20these%20distinct%20views%2C%20which%20may%20lead%0Ato%20feature%20separation%2C%20LOHA%20harmonizes%20the%20diversity%20by%20treating%20the%0Apropagation%20of%20graph%20signals%20from%20both%20views%20as%20a%20composite%20feature.%0ASpecifically%2C%20a%20novel%20high-dimensional%20feature%20named%20spectral%20signal%20trend%20is%0Aproposed%20to%20serve%20as%20the%20basis%20for%20the%20composite%20feature%2C%20which%20remains%0Arelatively%20unaffected%20by%20changing%20filters%20and%20focuses%20solely%20on%20original%0Afeature%20differences.%20LOHA%20achieves%20an%20average%20performance%20improvement%20of%202.8%25%0Aover%20runner-up%20models%20on%209%20real-world%20datasets%20with%20varying%20homophily%20levels.%0ANotably%2C%20LOHA%20even%20surpasses%20fully-supervised%20models%20on%20several%20datasets%2C%20which%0Aunderscores%20the%20potential%20of%20LOHA%20in%20advancing%20the%20efficacy%20of%20spectral%20GNNs%0Afor%20diverse%20graph%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOHA%253A%2520Direct%2520Graph%2520Spectral%2520Contrastive%2520Learning%2520Between%2520Low-pass%2520and%250A%2520%2520High-pass%2520Views%26entry.906535625%3DZiyun%2520Zou%2520and%2520Yinghui%2520Jiang%2520and%2520Lian%2520Shen%2520and%2520Juan%2520Liu%2520and%2520Xiangrong%2520Liu%26entry.1292438233%3D%2520%2520Spectral%2520Graph%2520Neural%2520Networks%2520effectively%2520handle%2520graphs%2520with%2520different%250Ahomophily%2520levels%252C%2520with%2520low-pass%2520filter%2520mining%2520feature%2520smoothness%2520and%2520high-pass%250Afilter%2520capturing%2520differences.%2520When%2520these%2520distinct%2520filters%2520could%2520naturally%2520form%250Atwo%2520opposite%2520views%2520for%2520self-supervised%2520learning%252C%2520the%2520commonalities%2520between%2520the%250Acounterparts%2520for%2520the%2520same%2520node%2520remain%2520unexplored%252C%2520leading%2520to%2520suboptimal%250Aperformance.%2520In%2520this%2520paper%252C%2520a%2520simple%2520yet%2520effective%2520self-supervised%2520contrastive%250Aframework%252C%2520LOHA%252C%2520is%2520proposed%2520to%2520address%2520this%2520gap.%2520LOHA%2520optimally%2520leverages%250Alow-pass%2520and%2520high-pass%2520views%2520by%2520embracing%2520%2522harmony%2520in%2520diversity%2522.%2520Rather%2520than%250Asolely%2520maximizing%2520the%2520difference%2520between%2520these%2520distinct%2520views%252C%2520which%2520may%2520lead%250Ato%2520feature%2520separation%252C%2520LOHA%2520harmonizes%2520the%2520diversity%2520by%2520treating%2520the%250Apropagation%2520of%2520graph%2520signals%2520from%2520both%2520views%2520as%2520a%2520composite%2520feature.%250ASpecifically%252C%2520a%2520novel%2520high-dimensional%2520feature%2520named%2520spectral%2520signal%2520trend%2520is%250Aproposed%2520to%2520serve%2520as%2520the%2520basis%2520for%2520the%2520composite%2520feature%252C%2520which%2520remains%250Arelatively%2520unaffected%2520by%2520changing%2520filters%2520and%2520focuses%2520solely%2520on%2520original%250Afeature%2520differences.%2520LOHA%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%25202.8%2525%250Aover%2520runner-up%2520models%2520on%25209%2520real-world%2520datasets%2520with%2520varying%2520homophily%2520levels.%250ANotably%252C%2520LOHA%2520even%2520surpasses%2520fully-supervised%2520models%2520on%2520several%2520datasets%252C%2520which%250Aunderscores%2520the%2520potential%2520of%2520LOHA%2520in%2520advancing%2520the%2520efficacy%2520of%2520spectral%2520GNNs%250Afor%2520diverse%2520graph%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOHA%3A%20Direct%20Graph%20Spectral%20Contrastive%20Learning%20Between%20Low-pass%20and%0A%20%20High-pass%20Views&entry.906535625=Ziyun%20Zou%20and%20Yinghui%20Jiang%20and%20Lian%20Shen%20and%20Juan%20Liu%20and%20Xiangrong%20Liu&entry.1292438233=%20%20Spectral%20Graph%20Neural%20Networks%20effectively%20handle%20graphs%20with%20different%0Ahomophily%20levels%2C%20with%20low-pass%20filter%20mining%20feature%20smoothness%20and%20high-pass%0Afilter%20capturing%20differences.%20When%20these%20distinct%20filters%20could%20naturally%20form%0Atwo%20opposite%20views%20for%20self-supervised%20learning%2C%20the%20commonalities%20between%20the%0Acounterparts%20for%20the%20same%20node%20remain%20unexplored%2C%20leading%20to%20suboptimal%0Aperformance.%20In%20this%20paper%2C%20a%20simple%20yet%20effective%20self-supervised%20contrastive%0Aframework%2C%20LOHA%2C%20is%20proposed%20to%20address%20this%20gap.%20LOHA%20optimally%20leverages%0Alow-pass%20and%20high-pass%20views%20by%20embracing%20%22harmony%20in%20diversity%22.%20Rather%20than%0Asolely%20maximizing%20the%20difference%20between%20these%20distinct%20views%2C%20which%20may%20lead%0Ato%20feature%20separation%2C%20LOHA%20harmonizes%20the%20diversity%20by%20treating%20the%0Apropagation%20of%20graph%20signals%20from%20both%20views%20as%20a%20composite%20feature.%0ASpecifically%2C%20a%20novel%20high-dimensional%20feature%20named%20spectral%20signal%20trend%20is%0Aproposed%20to%20serve%20as%20the%20basis%20for%20the%20composite%20feature%2C%20which%20remains%0Arelatively%20unaffected%20by%20changing%20filters%20and%20focuses%20solely%20on%20original%0Afeature%20differences.%20LOHA%20achieves%20an%20average%20performance%20improvement%20of%202.8%25%0Aover%20runner-up%20models%20on%209%20real-world%20datasets%20with%20varying%20homophily%20levels.%0ANotably%2C%20LOHA%20even%20surpasses%20fully-supervised%20models%20on%20several%20datasets%2C%20which%0Aunderscores%20the%20potential%20of%20LOHA%20in%20advancing%20the%20efficacy%20of%20spectral%20GNNs%0Afor%20diverse%20graph%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02969v1&entry.124074799=Read"},
{"title": "TreeLearn: A deep learning method for segmenting individual trees from\n  ground-based LiDAR forest point clouds", "author": "Jonathan Henrich and Jan van Delden and Dominik Seidel and Thomas Kneib and Alexander Ecker", "abstract": "  Laser-scanned point clouds of forests make it possible to extract valuable\ninformation for forest management. To consider single trees, a forest point\ncloud needs to be segmented into individual tree point clouds. Existing\nsegmentation methods are usually based on hand-crafted algorithms, such as\nidentifying trunks and growing trees from them, and face difficulties in dense\nforests with overlapping tree crowns. In this study, we propose TreeLearn, a\ndeep learning-based approach for tree instance segmentation of forest point\nclouds. TreeLearn is trained on already segmented point clouds in a data-driven\nmanner, making it less reliant on predefined features and algorithms.\nFurthermore, TreeLearn is implemented as a fully automatic pipeline and does\nnot rely on extensive hyperparameter tuning, which makes it easy to use.\nAdditionally, we introduce a new manually segmented benchmark forest dataset\ncontaining 156 full trees. The data is generated by mobile laser scanning and\ncontributes to create a larger and more diverse data basis for model\ndevelopment and fine-grained instance segmentation evaluation. We trained\nTreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360\nsoftware. An evaluation on the benchmark dataset shows that TreeLearn performs\nas well as the algorithm used to generate its training data. Furthermore, the\nperformance can be vastly improved by fine-tuning the model using manually\nannotated datasets. We evaluate TreeLearn on our benchmark dataset and the\nWytham Woods dataset, outperforming the recent SegmentAnyTree, ForAINet and\nTLS2Trees methods. The TreeLearn code and all datasets that were created in the\ncourse of this work are made publicly available.\n", "link": "http://arxiv.org/abs/2309.08471v3", "date": "2025-01-06", "relevancy": 2.5215, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TreeLearn%3A%20A%20deep%20learning%20method%20for%20segmenting%20individual%20trees%20from%0A%20%20ground-based%20LiDAR%20forest%20point%20clouds&body=Title%3A%20TreeLearn%3A%20A%20deep%20learning%20method%20for%20segmenting%20individual%20trees%20from%0A%20%20ground-based%20LiDAR%20forest%20point%20clouds%0AAuthor%3A%20Jonathan%20Henrich%20and%20Jan%20van%20Delden%20and%20Dominik%20Seidel%20and%20Thomas%20Kneib%20and%20Alexander%20Ecker%0AAbstract%3A%20%20%20Laser-scanned%20point%20clouds%20of%20forests%20make%20it%20possible%20to%20extract%20valuable%0Ainformation%20for%20forest%20management.%20To%20consider%20single%20trees%2C%20a%20forest%20point%0Acloud%20needs%20to%20be%20segmented%20into%20individual%20tree%20point%20clouds.%20Existing%0Asegmentation%20methods%20are%20usually%20based%20on%20hand-crafted%20algorithms%2C%20such%20as%0Aidentifying%20trunks%20and%20growing%20trees%20from%20them%2C%20and%20face%20difficulties%20in%20dense%0Aforests%20with%20overlapping%20tree%20crowns.%20In%20this%20study%2C%20we%20propose%20TreeLearn%2C%20a%0Adeep%20learning-based%20approach%20for%20tree%20instance%20segmentation%20of%20forest%20point%0Aclouds.%20TreeLearn%20is%20trained%20on%20already%20segmented%20point%20clouds%20in%20a%20data-driven%0Amanner%2C%20making%20it%20less%20reliant%20on%20predefined%20features%20and%20algorithms.%0AFurthermore%2C%20TreeLearn%20is%20implemented%20as%20a%20fully%20automatic%20pipeline%20and%20does%0Anot%20rely%20on%20extensive%20hyperparameter%20tuning%2C%20which%20makes%20it%20easy%20to%20use.%0AAdditionally%2C%20we%20introduce%20a%20new%20manually%20segmented%20benchmark%20forest%20dataset%0Acontaining%20156%20full%20trees.%20The%20data%20is%20generated%20by%20mobile%20laser%20scanning%20and%0Acontributes%20to%20create%20a%20larger%20and%20more%20diverse%20data%20basis%20for%20model%0Adevelopment%20and%20fine-grained%20instance%20segmentation%20evaluation.%20We%20trained%0ATreeLearn%20on%20forest%20point%20clouds%20of%206665%20trees%2C%20labeled%20using%20the%20Lidar360%0Asoftware.%20An%20evaluation%20on%20the%20benchmark%20dataset%20shows%20that%20TreeLearn%20performs%0Aas%20well%20as%20the%20algorithm%20used%20to%20generate%20its%20training%20data.%20Furthermore%2C%20the%0Aperformance%20can%20be%20vastly%20improved%20by%20fine-tuning%20the%20model%20using%20manually%0Aannotated%20datasets.%20We%20evaluate%20TreeLearn%20on%20our%20benchmark%20dataset%20and%20the%0AWytham%20Woods%20dataset%2C%20outperforming%20the%20recent%20SegmentAnyTree%2C%20ForAINet%20and%0ATLS2Trees%20methods.%20The%20TreeLearn%20code%20and%20all%20datasets%20that%20were%20created%20in%20the%0Acourse%20of%20this%20work%20are%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08471v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreeLearn%253A%2520A%2520deep%2520learning%2520method%2520for%2520segmenting%2520individual%2520trees%2520from%250A%2520%2520ground-based%2520LiDAR%2520forest%2520point%2520clouds%26entry.906535625%3DJonathan%2520Henrich%2520and%2520Jan%2520van%2520Delden%2520and%2520Dominik%2520Seidel%2520and%2520Thomas%2520Kneib%2520and%2520Alexander%2520Ecker%26entry.1292438233%3D%2520%2520Laser-scanned%2520point%2520clouds%2520of%2520forests%2520make%2520it%2520possible%2520to%2520extract%2520valuable%250Ainformation%2520for%2520forest%2520management.%2520To%2520consider%2520single%2520trees%252C%2520a%2520forest%2520point%250Acloud%2520needs%2520to%2520be%2520segmented%2520into%2520individual%2520tree%2520point%2520clouds.%2520Existing%250Asegmentation%2520methods%2520are%2520usually%2520based%2520on%2520hand-crafted%2520algorithms%252C%2520such%2520as%250Aidentifying%2520trunks%2520and%2520growing%2520trees%2520from%2520them%252C%2520and%2520face%2520difficulties%2520in%2520dense%250Aforests%2520with%2520overlapping%2520tree%2520crowns.%2520In%2520this%2520study%252C%2520we%2520propose%2520TreeLearn%252C%2520a%250Adeep%2520learning-based%2520approach%2520for%2520tree%2520instance%2520segmentation%2520of%2520forest%2520point%250Aclouds.%2520TreeLearn%2520is%2520trained%2520on%2520already%2520segmented%2520point%2520clouds%2520in%2520a%2520data-driven%250Amanner%252C%2520making%2520it%2520less%2520reliant%2520on%2520predefined%2520features%2520and%2520algorithms.%250AFurthermore%252C%2520TreeLearn%2520is%2520implemented%2520as%2520a%2520fully%2520automatic%2520pipeline%2520and%2520does%250Anot%2520rely%2520on%2520extensive%2520hyperparameter%2520tuning%252C%2520which%2520makes%2520it%2520easy%2520to%2520use.%250AAdditionally%252C%2520we%2520introduce%2520a%2520new%2520manually%2520segmented%2520benchmark%2520forest%2520dataset%250Acontaining%2520156%2520full%2520trees.%2520The%2520data%2520is%2520generated%2520by%2520mobile%2520laser%2520scanning%2520and%250Acontributes%2520to%2520create%2520a%2520larger%2520and%2520more%2520diverse%2520data%2520basis%2520for%2520model%250Adevelopment%2520and%2520fine-grained%2520instance%2520segmentation%2520evaluation.%2520We%2520trained%250ATreeLearn%2520on%2520forest%2520point%2520clouds%2520of%25206665%2520trees%252C%2520labeled%2520using%2520the%2520Lidar360%250Asoftware.%2520An%2520evaluation%2520on%2520the%2520benchmark%2520dataset%2520shows%2520that%2520TreeLearn%2520performs%250Aas%2520well%2520as%2520the%2520algorithm%2520used%2520to%2520generate%2520its%2520training%2520data.%2520Furthermore%252C%2520the%250Aperformance%2520can%2520be%2520vastly%2520improved%2520by%2520fine-tuning%2520the%2520model%2520using%2520manually%250Aannotated%2520datasets.%2520We%2520evaluate%2520TreeLearn%2520on%2520our%2520benchmark%2520dataset%2520and%2520the%250AWytham%2520Woods%2520dataset%252C%2520outperforming%2520the%2520recent%2520SegmentAnyTree%252C%2520ForAINet%2520and%250ATLS2Trees%2520methods.%2520The%2520TreeLearn%2520code%2520and%2520all%2520datasets%2520that%2520were%2520created%2520in%2520the%250Acourse%2520of%2520this%2520work%2520are%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08471v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TreeLearn%3A%20A%20deep%20learning%20method%20for%20segmenting%20individual%20trees%20from%0A%20%20ground-based%20LiDAR%20forest%20point%20clouds&entry.906535625=Jonathan%20Henrich%20and%20Jan%20van%20Delden%20and%20Dominik%20Seidel%20and%20Thomas%20Kneib%20and%20Alexander%20Ecker&entry.1292438233=%20%20Laser-scanned%20point%20clouds%20of%20forests%20make%20it%20possible%20to%20extract%20valuable%0Ainformation%20for%20forest%20management.%20To%20consider%20single%20trees%2C%20a%20forest%20point%0Acloud%20needs%20to%20be%20segmented%20into%20individual%20tree%20point%20clouds.%20Existing%0Asegmentation%20methods%20are%20usually%20based%20on%20hand-crafted%20algorithms%2C%20such%20as%0Aidentifying%20trunks%20and%20growing%20trees%20from%20them%2C%20and%20face%20difficulties%20in%20dense%0Aforests%20with%20overlapping%20tree%20crowns.%20In%20this%20study%2C%20we%20propose%20TreeLearn%2C%20a%0Adeep%20learning-based%20approach%20for%20tree%20instance%20segmentation%20of%20forest%20point%0Aclouds.%20TreeLearn%20is%20trained%20on%20already%20segmented%20point%20clouds%20in%20a%20data-driven%0Amanner%2C%20making%20it%20less%20reliant%20on%20predefined%20features%20and%20algorithms.%0AFurthermore%2C%20TreeLearn%20is%20implemented%20as%20a%20fully%20automatic%20pipeline%20and%20does%0Anot%20rely%20on%20extensive%20hyperparameter%20tuning%2C%20which%20makes%20it%20easy%20to%20use.%0AAdditionally%2C%20we%20introduce%20a%20new%20manually%20segmented%20benchmark%20forest%20dataset%0Acontaining%20156%20full%20trees.%20The%20data%20is%20generated%20by%20mobile%20laser%20scanning%20and%0Acontributes%20to%20create%20a%20larger%20and%20more%20diverse%20data%20basis%20for%20model%0Adevelopment%20and%20fine-grained%20instance%20segmentation%20evaluation.%20We%20trained%0ATreeLearn%20on%20forest%20point%20clouds%20of%206665%20trees%2C%20labeled%20using%20the%20Lidar360%0Asoftware.%20An%20evaluation%20on%20the%20benchmark%20dataset%20shows%20that%20TreeLearn%20performs%0Aas%20well%20as%20the%20algorithm%20used%20to%20generate%20its%20training%20data.%20Furthermore%2C%20the%0Aperformance%20can%20be%20vastly%20improved%20by%20fine-tuning%20the%20model%20using%20manually%0Aannotated%20datasets.%20We%20evaluate%20TreeLearn%20on%20our%20benchmark%20dataset%20and%20the%0AWytham%20Woods%20dataset%2C%20outperforming%20the%20recent%20SegmentAnyTree%2C%20ForAINet%20and%0ATLS2Trees%20methods.%20The%20TreeLearn%20code%20and%20all%20datasets%20that%20were%20created%20in%20the%0Acourse%20of%20this%20work%20are%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08471v3&entry.124074799=Read"},
{"title": "LOLA -- An Open-Source Massively Multilingual Large Language Model", "author": "Nikit Srivastava and Denis Kuchelev and Tatiana Moteu Ngoli and Kshitij Shetty and Michael R\u00f6der and Hamada Zahera and Diego Moussallem and Axel-Cyrille Ngonga Ngomo", "abstract": "  This paper presents LOLA, a massively multilingual large language model\ntrained on more than 160 languages using a sparse Mixture-of-Experts\nTransformer architecture. Our architectural and implementation choices address\nthe challenge of harnessing linguistic diversity while maintaining efficiency\nand avoiding the common pitfalls of multilinguality. Our analysis of the\nevaluation results shows competitive performance in natural language generation\nand understanding tasks. Additionally, we demonstrate how the learned\nexpert-routing mechanism exploits implicit phylogenetic linguistic patterns to\npotentially alleviate the curse of multilinguality. We provide an in-depth look\nat the training process, an analysis of the datasets, and a balanced\nexploration of the model's strengths and limitations. As an open-source model,\nLOLA promotes reproducibility and serves as a robust foundation for future\nresearch. Our findings enable the development of compute-efficient multilingual\nmodels with strong, scalable performance across languages.\n", "link": "http://arxiv.org/abs/2409.11272v6", "date": "2025-01-06", "relevancy": 2.5153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&body=Title%3A%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model%0AAuthor%3A%20Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20Ngoli%20and%20Kshitij%20Shetty%20and%20Michael%20R%C3%B6der%20and%20Hamada%20Zahera%20and%20Diego%20Moussallem%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11272v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOLA%2520--%2520An%2520Open-Source%2520Massively%2520Multilingual%2520Large%2520Language%2520Model%26entry.906535625%3DNikit%2520Srivastava%2520and%2520Denis%2520Kuchelev%2520and%2520Tatiana%2520Moteu%2520Ngoli%2520and%2520Kshitij%2520Shetty%2520and%2520Michael%2520R%25C3%25B6der%2520and%2520Hamada%2520Zahera%2520and%2520Diego%2520Moussallem%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LOLA%252C%2520a%2520massively%2520multilingual%2520large%2520language%2520model%250Atrained%2520on%2520more%2520than%2520160%2520languages%2520using%2520a%2520sparse%2520Mixture-of-Experts%250ATransformer%2520architecture.%2520Our%2520architectural%2520and%2520implementation%2520choices%2520address%250Athe%2520challenge%2520of%2520harnessing%2520linguistic%2520diversity%2520while%2520maintaining%2520efficiency%250Aand%2520avoiding%2520the%2520common%2520pitfalls%2520of%2520multilinguality.%2520Our%2520analysis%2520of%2520the%250Aevaluation%2520results%2520shows%2520competitive%2520performance%2520in%2520natural%2520language%2520generation%250Aand%2520understanding%2520tasks.%2520Additionally%252C%2520we%2520demonstrate%2520how%2520the%2520learned%250Aexpert-routing%2520mechanism%2520exploits%2520implicit%2520phylogenetic%2520linguistic%2520patterns%2520to%250Apotentially%2520alleviate%2520the%2520curse%2520of%2520multilinguality.%2520We%2520provide%2520an%2520in-depth%2520look%250Aat%2520the%2520training%2520process%252C%2520an%2520analysis%2520of%2520the%2520datasets%252C%2520and%2520a%2520balanced%250Aexploration%2520of%2520the%2520model%2527s%2520strengths%2520and%2520limitations.%2520As%2520an%2520open-source%2520model%252C%250ALOLA%2520promotes%2520reproducibility%2520and%2520serves%2520as%2520a%2520robust%2520foundation%2520for%2520future%250Aresearch.%2520Our%2520findings%2520enable%2520the%2520development%2520of%2520compute-efficient%2520multilingual%250Amodels%2520with%2520strong%252C%2520scalable%2520performance%2520across%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11272v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&entry.906535625=Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20Ngoli%20and%20Kshitij%20Shetty%20and%20Michael%20R%C3%B6der%20and%20Hamada%20Zahera%20and%20Diego%20Moussallem%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11272v6&entry.124074799=Read"},
{"title": "Qinco2: Vector Compression and Search with Improved Implicit Neural\n  Codebooks", "author": "Th\u00e9ophane Vallaeys and Matthew Muckley and Jakob Verbeek and Matthijs Douze", "abstract": "  Vector quantization is a fundamental technique for compression and\nlarge-scale nearest neighbor search. For high-accuracy operating points,\nmulti-codebook quantization associates data vectors with one element from each\nof multiple codebooks. An example is residual quantization (RQ), which\niteratively quantizes the residual error of previous steps. Dependencies\nbetween the different parts of the code are, however, ignored in RQ, which\nleads to suboptimal rate-distortion performance. QINCo recently addressed this\ninefficiency by using a neural network to determine the quantization codebook\nin RQ based on the vector reconstruction from previous steps. In this paper we\nintroduce QINCo2 which extends and improves QINCo with (i) improved vector\nencoding using codeword pre-selection and beam-search, (ii) a fast approximate\ndecoder leveraging codeword pairs to establish accurate short-lists for search,\nand (iii) an optimized training procedure and network architecture. We conduct\nexperiments on four datasets to evaluate QINCo2 for vector compression and\nbillion-scale nearest neighbor search. We obtain outstanding results in both\nsettings, improving the state-of-the-art reconstruction MSE by 34% for 16-byte\nvector compression on BigANN, and search accuracy by 24% with 8-byte encodings\non Deep1M.\n", "link": "http://arxiv.org/abs/2501.03078v1", "date": "2025-01-06", "relevancy": 2.4998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qinco2%3A%20Vector%20Compression%20and%20Search%20with%20Improved%20Implicit%20Neural%0A%20%20Codebooks&body=Title%3A%20Qinco2%3A%20Vector%20Compression%20and%20Search%20with%20Improved%20Implicit%20Neural%0A%20%20Codebooks%0AAuthor%3A%20Th%C3%A9ophane%20Vallaeys%20and%20Matthew%20Muckley%20and%20Jakob%20Verbeek%20and%20Matthijs%20Douze%0AAbstract%3A%20%20%20Vector%20quantization%20is%20a%20fundamental%20technique%20for%20compression%20and%0Alarge-scale%20nearest%20neighbor%20search.%20For%20high-accuracy%20operating%20points%2C%0Amulti-codebook%20quantization%20associates%20data%20vectors%20with%20one%20element%20from%20each%0Aof%20multiple%20codebooks.%20An%20example%20is%20residual%20quantization%20%28RQ%29%2C%20which%0Aiteratively%20quantizes%20the%20residual%20error%20of%20previous%20steps.%20Dependencies%0Abetween%20the%20different%20parts%20of%20the%20code%20are%2C%20however%2C%20ignored%20in%20RQ%2C%20which%0Aleads%20to%20suboptimal%20rate-distortion%20performance.%20QINCo%20recently%20addressed%20this%0Ainefficiency%20by%20using%20a%20neural%20network%20to%20determine%20the%20quantization%20codebook%0Ain%20RQ%20based%20on%20the%20vector%20reconstruction%20from%20previous%20steps.%20In%20this%20paper%20we%0Aintroduce%20QINCo2%20which%20extends%20and%20improves%20QINCo%20with%20%28i%29%20improved%20vector%0Aencoding%20using%20codeword%20pre-selection%20and%20beam-search%2C%20%28ii%29%20a%20fast%20approximate%0Adecoder%20leveraging%20codeword%20pairs%20to%20establish%20accurate%20short-lists%20for%20search%2C%0Aand%20%28iii%29%20an%20optimized%20training%20procedure%20and%20network%20architecture.%20We%20conduct%0Aexperiments%20on%20four%20datasets%20to%20evaluate%20QINCo2%20for%20vector%20compression%20and%0Abillion-scale%20nearest%20neighbor%20search.%20We%20obtain%20outstanding%20results%20in%20both%0Asettings%2C%20improving%20the%20state-of-the-art%20reconstruction%20MSE%20by%2034%25%20for%2016-byte%0Avector%20compression%20on%20BigANN%2C%20and%20search%20accuracy%20by%2024%25%20with%208-byte%20encodings%0Aon%20Deep1M.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQinco2%253A%2520Vector%2520Compression%2520and%2520Search%2520with%2520Improved%2520Implicit%2520Neural%250A%2520%2520Codebooks%26entry.906535625%3DTh%25C3%25A9ophane%2520Vallaeys%2520and%2520Matthew%2520Muckley%2520and%2520Jakob%2520Verbeek%2520and%2520Matthijs%2520Douze%26entry.1292438233%3D%2520%2520Vector%2520quantization%2520is%2520a%2520fundamental%2520technique%2520for%2520compression%2520and%250Alarge-scale%2520nearest%2520neighbor%2520search.%2520For%2520high-accuracy%2520operating%2520points%252C%250Amulti-codebook%2520quantization%2520associates%2520data%2520vectors%2520with%2520one%2520element%2520from%2520each%250Aof%2520multiple%2520codebooks.%2520An%2520example%2520is%2520residual%2520quantization%2520%2528RQ%2529%252C%2520which%250Aiteratively%2520quantizes%2520the%2520residual%2520error%2520of%2520previous%2520steps.%2520Dependencies%250Abetween%2520the%2520different%2520parts%2520of%2520the%2520code%2520are%252C%2520however%252C%2520ignored%2520in%2520RQ%252C%2520which%250Aleads%2520to%2520suboptimal%2520rate-distortion%2520performance.%2520QINCo%2520recently%2520addressed%2520this%250Ainefficiency%2520by%2520using%2520a%2520neural%2520network%2520to%2520determine%2520the%2520quantization%2520codebook%250Ain%2520RQ%2520based%2520on%2520the%2520vector%2520reconstruction%2520from%2520previous%2520steps.%2520In%2520this%2520paper%2520we%250Aintroduce%2520QINCo2%2520which%2520extends%2520and%2520improves%2520QINCo%2520with%2520%2528i%2529%2520improved%2520vector%250Aencoding%2520using%2520codeword%2520pre-selection%2520and%2520beam-search%252C%2520%2528ii%2529%2520a%2520fast%2520approximate%250Adecoder%2520leveraging%2520codeword%2520pairs%2520to%2520establish%2520accurate%2520short-lists%2520for%2520search%252C%250Aand%2520%2528iii%2529%2520an%2520optimized%2520training%2520procedure%2520and%2520network%2520architecture.%2520We%2520conduct%250Aexperiments%2520on%2520four%2520datasets%2520to%2520evaluate%2520QINCo2%2520for%2520vector%2520compression%2520and%250Abillion-scale%2520nearest%2520neighbor%2520search.%2520We%2520obtain%2520outstanding%2520results%2520in%2520both%250Asettings%252C%2520improving%2520the%2520state-of-the-art%2520reconstruction%2520MSE%2520by%252034%2525%2520for%252016-byte%250Avector%2520compression%2520on%2520BigANN%252C%2520and%2520search%2520accuracy%2520by%252024%2525%2520with%25208-byte%2520encodings%250Aon%2520Deep1M.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qinco2%3A%20Vector%20Compression%20and%20Search%20with%20Improved%20Implicit%20Neural%0A%20%20Codebooks&entry.906535625=Th%C3%A9ophane%20Vallaeys%20and%20Matthew%20Muckley%20and%20Jakob%20Verbeek%20and%20Matthijs%20Douze&entry.1292438233=%20%20Vector%20quantization%20is%20a%20fundamental%20technique%20for%20compression%20and%0Alarge-scale%20nearest%20neighbor%20search.%20For%20high-accuracy%20operating%20points%2C%0Amulti-codebook%20quantization%20associates%20data%20vectors%20with%20one%20element%20from%20each%0Aof%20multiple%20codebooks.%20An%20example%20is%20residual%20quantization%20%28RQ%29%2C%20which%0Aiteratively%20quantizes%20the%20residual%20error%20of%20previous%20steps.%20Dependencies%0Abetween%20the%20different%20parts%20of%20the%20code%20are%2C%20however%2C%20ignored%20in%20RQ%2C%20which%0Aleads%20to%20suboptimal%20rate-distortion%20performance.%20QINCo%20recently%20addressed%20this%0Ainefficiency%20by%20using%20a%20neural%20network%20to%20determine%20the%20quantization%20codebook%0Ain%20RQ%20based%20on%20the%20vector%20reconstruction%20from%20previous%20steps.%20In%20this%20paper%20we%0Aintroduce%20QINCo2%20which%20extends%20and%20improves%20QINCo%20with%20%28i%29%20improved%20vector%0Aencoding%20using%20codeword%20pre-selection%20and%20beam-search%2C%20%28ii%29%20a%20fast%20approximate%0Adecoder%20leveraging%20codeword%20pairs%20to%20establish%20accurate%20short-lists%20for%20search%2C%0Aand%20%28iii%29%20an%20optimized%20training%20procedure%20and%20network%20architecture.%20We%20conduct%0Aexperiments%20on%20four%20datasets%20to%20evaluate%20QINCo2%20for%20vector%20compression%20and%0Abillion-scale%20nearest%20neighbor%20search.%20We%20obtain%20outstanding%20results%20in%20both%0Asettings%2C%20improving%20the%20state-of-the-art%20reconstruction%20MSE%20by%2034%25%20for%2016-byte%0Avector%20compression%20on%20BigANN%2C%20and%20search%20accuracy%20by%2024%25%20with%208-byte%20encodings%0Aon%20Deep1M.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03078v1&entry.124074799=Read"},
{"title": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for\n  Time Series Representation Learning", "author": "En Fu and Yanyan Hu", "abstract": "  Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.\n", "link": "http://arxiv.org/abs/2412.20790v2", "date": "2025-01-06", "relevancy": 2.4987, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5338}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4871}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Masked%20Embedding%20Inference%3A%20A%20Non-Contrastive%20Approach%20for%0A%20%20Time%20Series%20Representation%20Learning&body=Title%3A%20Frequency-Masked%20Embedding%20Inference%3A%20A%20Non-Contrastive%20Approach%20for%0A%20%20Time%20Series%20Representation%20Learning%0AAuthor%3A%20En%20Fu%20and%20Yanyan%20Hu%0AAbstract%3A%20%20%20Contrastive%20learning%20underpins%20most%20current%20self-supervised%20time%20series%0Arepresentation%20methods.%20The%20strategy%20for%20constructing%20positive%20and%20negative%0Asample%20pairs%20significantly%20affects%20the%20final%20representation%20quality.%20However%2C%0Adue%20to%20the%20continuous%20nature%20of%20time%20series%20semantics%2C%20the%20modeling%20approach%20of%0Acontrastive%20learning%20struggles%20to%20accommodate%20the%20characteristics%20of%20time%0Aseries%20data.%20This%20results%20in%20issues%20such%20as%20difficulties%20in%20constructing%20hard%0Anegative%20samples%20and%20the%20potential%20introduction%20of%20inappropriate%20biases%20during%0Apositive%20sample%20construction.%20Although%20some%20recent%20works%20have%20developed%20several%0Ascientific%20strategies%20for%20constructing%20positive%20and%20negative%20sample%20pairs%20with%0Aimproved%20effectiveness%2C%20they%20remain%20constrained%20by%20the%20contrastive%20learning%0Aframework.%20To%20fundamentally%20overcome%20the%20limitations%20of%20contrastive%20learning%2C%0Athis%20paper%20introduces%20Frequency-masked%20Embedding%20Inference%20%28FEI%29%2C%20a%20novel%0Anon-contrastive%20method%20that%20completely%20eliminates%20the%20need%20for%20positive%20and%0Anegative%20samples.%20The%20proposed%20FEI%20constructs%202%20inference%20branches%20based%20on%20a%0Aprompting%20strategy%3A%201%29%20Using%20frequency%20masking%20as%20prompts%20to%20infer%20the%0Aembedding%20representation%20of%20the%20target%20series%20with%20missing%20frequency%20bands%20in%0Athe%20embedding%20space%2C%20and%202%29%20Using%20the%20target%20series%20as%20prompts%20to%20infer%20its%0Afrequency%20masking%20embedding.%20In%20this%20way%2C%20FEI%20enables%20continuous%20semantic%0Arelationship%20modeling%20for%20time%20series.%20Experiments%20on%208%20widely%20used%20time%20series%0Adatasets%20for%20classification%20and%20regression%20tasks%2C%20using%20linear%20evaluation%20and%0Aend-to-end%20fine-tuning%2C%20show%20that%20FEI%20significantly%20outperforms%20existing%0Acontrastive-based%20methods%20in%20terms%20of%20generalization.%20This%20study%20provides%20new%0Ainsights%20into%20self-supervised%20representation%20learning%20for%20time%20series.%20The%20code%0Ais%20available%20at%0Ahttps%3A//github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Masked%2520Embedding%2520Inference%253A%2520A%2520Non-Contrastive%2520Approach%2520for%250A%2520%2520Time%2520Series%2520Representation%2520Learning%26entry.906535625%3DEn%2520Fu%2520and%2520Yanyan%2520Hu%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520underpins%2520most%2520current%2520self-supervised%2520time%2520series%250Arepresentation%2520methods.%2520The%2520strategy%2520for%2520constructing%2520positive%2520and%2520negative%250Asample%2520pairs%2520significantly%2520affects%2520the%2520final%2520representation%2520quality.%2520However%252C%250Adue%2520to%2520the%2520continuous%2520nature%2520of%2520time%2520series%2520semantics%252C%2520the%2520modeling%2520approach%2520of%250Acontrastive%2520learning%2520struggles%2520to%2520accommodate%2520the%2520characteristics%2520of%2520time%250Aseries%2520data.%2520This%2520results%2520in%2520issues%2520such%2520as%2520difficulties%2520in%2520constructing%2520hard%250Anegative%2520samples%2520and%2520the%2520potential%2520introduction%2520of%2520inappropriate%2520biases%2520during%250Apositive%2520sample%2520construction.%2520Although%2520some%2520recent%2520works%2520have%2520developed%2520several%250Ascientific%2520strategies%2520for%2520constructing%2520positive%2520and%2520negative%2520sample%2520pairs%2520with%250Aimproved%2520effectiveness%252C%2520they%2520remain%2520constrained%2520by%2520the%2520contrastive%2520learning%250Aframework.%2520To%2520fundamentally%2520overcome%2520the%2520limitations%2520of%2520contrastive%2520learning%252C%250Athis%2520paper%2520introduces%2520Frequency-masked%2520Embedding%2520Inference%2520%2528FEI%2529%252C%2520a%2520novel%250Anon-contrastive%2520method%2520that%2520completely%2520eliminates%2520the%2520need%2520for%2520positive%2520and%250Anegative%2520samples.%2520The%2520proposed%2520FEI%2520constructs%25202%2520inference%2520branches%2520based%2520on%2520a%250Aprompting%2520strategy%253A%25201%2529%2520Using%2520frequency%2520masking%2520as%2520prompts%2520to%2520infer%2520the%250Aembedding%2520representation%2520of%2520the%2520target%2520series%2520with%2520missing%2520frequency%2520bands%2520in%250Athe%2520embedding%2520space%252C%2520and%25202%2529%2520Using%2520the%2520target%2520series%2520as%2520prompts%2520to%2520infer%2520its%250Afrequency%2520masking%2520embedding.%2520In%2520this%2520way%252C%2520FEI%2520enables%2520continuous%2520semantic%250Arelationship%2520modeling%2520for%2520time%2520series.%2520Experiments%2520on%25208%2520widely%2520used%2520time%2520series%250Adatasets%2520for%2520classification%2520and%2520regression%2520tasks%252C%2520using%2520linear%2520evaluation%2520and%250Aend-to-end%2520fine-tuning%252C%2520show%2520that%2520FEI%2520significantly%2520outperforms%2520existing%250Acontrastive-based%2520methods%2520in%2520terms%2520of%2520generalization.%2520This%2520study%2520provides%2520new%250Ainsights%2520into%2520self-supervised%2520representation%2520learning%2520for%2520time%2520series.%2520The%2520code%250Ais%2520available%2520at%250Ahttps%253A//github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Masked%20Embedding%20Inference%3A%20A%20Non-Contrastive%20Approach%20for%0A%20%20Time%20Series%20Representation%20Learning&entry.906535625=En%20Fu%20and%20Yanyan%20Hu&entry.1292438233=%20%20Contrastive%20learning%20underpins%20most%20current%20self-supervised%20time%20series%0Arepresentation%20methods.%20The%20strategy%20for%20constructing%20positive%20and%20negative%0Asample%20pairs%20significantly%20affects%20the%20final%20representation%20quality.%20However%2C%0Adue%20to%20the%20continuous%20nature%20of%20time%20series%20semantics%2C%20the%20modeling%20approach%20of%0Acontrastive%20learning%20struggles%20to%20accommodate%20the%20characteristics%20of%20time%0Aseries%20data.%20This%20results%20in%20issues%20such%20as%20difficulties%20in%20constructing%20hard%0Anegative%20samples%20and%20the%20potential%20introduction%20of%20inappropriate%20biases%20during%0Apositive%20sample%20construction.%20Although%20some%20recent%20works%20have%20developed%20several%0Ascientific%20strategies%20for%20constructing%20positive%20and%20negative%20sample%20pairs%20with%0Aimproved%20effectiveness%2C%20they%20remain%20constrained%20by%20the%20contrastive%20learning%0Aframework.%20To%20fundamentally%20overcome%20the%20limitations%20of%20contrastive%20learning%2C%0Athis%20paper%20introduces%20Frequency-masked%20Embedding%20Inference%20%28FEI%29%2C%20a%20novel%0Anon-contrastive%20method%20that%20completely%20eliminates%20the%20need%20for%20positive%20and%0Anegative%20samples.%20The%20proposed%20FEI%20constructs%202%20inference%20branches%20based%20on%20a%0Aprompting%20strategy%3A%201%29%20Using%20frequency%20masking%20as%20prompts%20to%20infer%20the%0Aembedding%20representation%20of%20the%20target%20series%20with%20missing%20frequency%20bands%20in%0Athe%20embedding%20space%2C%20and%202%29%20Using%20the%20target%20series%20as%20prompts%20to%20infer%20its%0Afrequency%20masking%20embedding.%20In%20this%20way%2C%20FEI%20enables%20continuous%20semantic%0Arelationship%20modeling%20for%20time%20series.%20Experiments%20on%208%20widely%20used%20time%20series%0Adatasets%20for%20classification%20and%20regression%20tasks%2C%20using%20linear%20evaluation%20and%0Aend-to-end%20fine-tuning%2C%20show%20that%20FEI%20significantly%20outperforms%20existing%0Acontrastive-based%20methods%20in%20terms%20of%20generalization.%20This%20study%20provides%20new%0Ainsights%20into%20self-supervised%20representation%20learning%20for%20time%20series.%20The%20code%0Ais%20available%20at%0Ahttps%3A//github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20790v2&entry.124074799=Read"},
{"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "author": "Xindi Wu and Mengzhou Xia and Rulin Shao and Zhiwei Deng and Pang Wei Koh and Olga Russakovsky", "abstract": "  Visual Instruction Tuning typically requires a large amount of\nvision-language training data. This data often containing redundant information\nthat increases computational costs without proportional performance gains. In\nthis work, we introduce ICONS, a gradient-driven Influence CONsensus approach\nfor vision-language data Selection that selects a compact training dataset for\nefficient multi-task training. The key element of our approach is cross-task\ninfluence consensus, which uses majority voting across task-specific influence\nmatrices to identify samples that are consistently valuable across multiple\ntasks, allowing us to effectively prioritize data that optimizes for overall\nperformance. Experiments show that models trained on our selected data (20% of\nLLaVA-665K) achieve 98.6% of the relative performance obtained using the full\ndataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet\nhighly informative subset of LLaVA-665K visual instruction tuning data,\npreserving high impact training data for efficient vision-language model\ndevelopment.\n", "link": "http://arxiv.org/abs/2501.00654v2", "date": "2025-01-06", "relevancy": 2.4898, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICONS%3A%20Influence%20Consensus%20for%20Vision-Language%20Data%20Selection&body=Title%3A%20ICONS%3A%20Influence%20Consensus%20for%20Vision-Language%20Data%20Selection%0AAuthor%3A%20Xindi%20Wu%20and%20Mengzhou%20Xia%20and%20Rulin%20Shao%20and%20Zhiwei%20Deng%20and%20Pang%20Wei%20Koh%20and%20Olga%20Russakovsky%0AAbstract%3A%20%20%20Visual%20Instruction%20Tuning%20typically%20requires%20a%20large%20amount%20of%0Avision-language%20training%20data.%20This%20data%20often%20containing%20redundant%20information%0Athat%20increases%20computational%20costs%20without%20proportional%20performance%20gains.%20In%0Athis%20work%2C%20we%20introduce%20ICONS%2C%20a%20gradient-driven%20Influence%20CONsensus%20approach%0Afor%20vision-language%20data%20Selection%20that%20selects%20a%20compact%20training%20dataset%20for%0Aefficient%20multi-task%20training.%20The%20key%20element%20of%20our%20approach%20is%20cross-task%0Ainfluence%20consensus%2C%20which%20uses%20majority%20voting%20across%20task-specific%20influence%0Amatrices%20to%20identify%20samples%20that%20are%20consistently%20valuable%20across%20multiple%0Atasks%2C%20allowing%20us%20to%20effectively%20prioritize%20data%20that%20optimizes%20for%20overall%0Aperformance.%20Experiments%20show%20that%20models%20trained%20on%20our%20selected%20data%20%2820%25%20of%0ALLaVA-665K%29%20achieve%2098.6%25%20of%20the%20relative%20performance%20obtained%20using%20the%20full%0Adataset.%20Additionally%2C%20we%20release%20this%20subset%2C%20LLaVA-ICONS-133K%2C%20a%20compact%20yet%0Ahighly%20informative%20subset%20of%20LLaVA-665K%20visual%20instruction%20tuning%20data%2C%0Apreserving%20high%20impact%20training%20data%20for%20efficient%20vision-language%20model%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICONS%253A%2520Influence%2520Consensus%2520for%2520Vision-Language%2520Data%2520Selection%26entry.906535625%3DXindi%2520Wu%2520and%2520Mengzhou%2520Xia%2520and%2520Rulin%2520Shao%2520and%2520Zhiwei%2520Deng%2520and%2520Pang%2520Wei%2520Koh%2520and%2520Olga%2520Russakovsky%26entry.1292438233%3D%2520%2520Visual%2520Instruction%2520Tuning%2520typically%2520requires%2520a%2520large%2520amount%2520of%250Avision-language%2520training%2520data.%2520This%2520data%2520often%2520containing%2520redundant%2520information%250Athat%2520increases%2520computational%2520costs%2520without%2520proportional%2520performance%2520gains.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520ICONS%252C%2520a%2520gradient-driven%2520Influence%2520CONsensus%2520approach%250Afor%2520vision-language%2520data%2520Selection%2520that%2520selects%2520a%2520compact%2520training%2520dataset%2520for%250Aefficient%2520multi-task%2520training.%2520The%2520key%2520element%2520of%2520our%2520approach%2520is%2520cross-task%250Ainfluence%2520consensus%252C%2520which%2520uses%2520majority%2520voting%2520across%2520task-specific%2520influence%250Amatrices%2520to%2520identify%2520samples%2520that%2520are%2520consistently%2520valuable%2520across%2520multiple%250Atasks%252C%2520allowing%2520us%2520to%2520effectively%2520prioritize%2520data%2520that%2520optimizes%2520for%2520overall%250Aperformance.%2520Experiments%2520show%2520that%2520models%2520trained%2520on%2520our%2520selected%2520data%2520%252820%2525%2520of%250ALLaVA-665K%2529%2520achieve%252098.6%2525%2520of%2520the%2520relative%2520performance%2520obtained%2520using%2520the%2520full%250Adataset.%2520Additionally%252C%2520we%2520release%2520this%2520subset%252C%2520LLaVA-ICONS-133K%252C%2520a%2520compact%2520yet%250Ahighly%2520informative%2520subset%2520of%2520LLaVA-665K%2520visual%2520instruction%2520tuning%2520data%252C%250Apreserving%2520high%2520impact%2520training%2520data%2520for%2520efficient%2520vision-language%2520model%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICONS%3A%20Influence%20Consensus%20for%20Vision-Language%20Data%20Selection&entry.906535625=Xindi%20Wu%20and%20Mengzhou%20Xia%20and%20Rulin%20Shao%20and%20Zhiwei%20Deng%20and%20Pang%20Wei%20Koh%20and%20Olga%20Russakovsky&entry.1292438233=%20%20Visual%20Instruction%20Tuning%20typically%20requires%20a%20large%20amount%20of%0Avision-language%20training%20data.%20This%20data%20often%20containing%20redundant%20information%0Athat%20increases%20computational%20costs%20without%20proportional%20performance%20gains.%20In%0Athis%20work%2C%20we%20introduce%20ICONS%2C%20a%20gradient-driven%20Influence%20CONsensus%20approach%0Afor%20vision-language%20data%20Selection%20that%20selects%20a%20compact%20training%20dataset%20for%0Aefficient%20multi-task%20training.%20The%20key%20element%20of%20our%20approach%20is%20cross-task%0Ainfluence%20consensus%2C%20which%20uses%20majority%20voting%20across%20task-specific%20influence%0Amatrices%20to%20identify%20samples%20that%20are%20consistently%20valuable%20across%20multiple%0Atasks%2C%20allowing%20us%20to%20effectively%20prioritize%20data%20that%20optimizes%20for%20overall%0Aperformance.%20Experiments%20show%20that%20models%20trained%20on%20our%20selected%20data%20%2820%25%20of%0ALLaVA-665K%29%20achieve%2098.6%25%20of%20the%20relative%20performance%20obtained%20using%20the%20full%0Adataset.%20Additionally%2C%20we%20release%20this%20subset%2C%20LLaVA-ICONS-133K%2C%20a%20compact%20yet%0Ahighly%20informative%20subset%20of%20LLaVA-665K%20visual%20instruction%20tuning%20data%2C%0Apreserving%20high%20impact%20training%20data%20for%20efficient%20vision-language%20model%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00654v2&entry.124074799=Read"},
{"title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\n  Control", "author": "Yuanpeng Tu and Hao Luo and Xi Chen and Sihui Ji and Xiang Bai and Hengshuang Zhao", "abstract": "  Despite significant advancements in video generation, inserting a given\nobject into videos remains a challenging task. The difficulty lies in\npreserving the appearance details of the reference object and accurately\nmodeling coherent motions at the same time. In this paper, we propose\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\ndetail preservation and precise motion control. Starting from a text-to-video\nmodel, we utilize an ID extractor to inject the global identity and leverage a\nbox sequence to control the overall motion. To preserve the detailed appearance\nand meanwhile support fine-grained motion control, we design a pixel warper. It\ntakes the reference image with arbitrary key-points and the corresponding\nkey-point trajectories as inputs. It warps the pixel details according to the\ntrajectories and fuses the warped features with the diffusion U-Net, thus\nimproving detail preservation and supporting users in manipulating the motion\ntrajectories. In addition, we propose a training strategy involving both videos\nand static images with a weighted loss to enhance insertion quality.\nVideoAnydoor demonstrates significant superiority over existing methods and\nnaturally supports various downstream applications (e.g., talking head\ngeneration, video virtual try-on, multi-region editing) without task-specific\nfine-tuning.\n", "link": "http://arxiv.org/abs/2501.01427v2", "date": "2025-01-06", "relevancy": 2.485, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6557}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6241}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAnydoor%3A%20High-fidelity%20Video%20Object%20Insertion%20with%20Precise%20Motion%0A%20%20Control&body=Title%3A%20VideoAnydoor%3A%20High-fidelity%20Video%20Object%20Insertion%20with%20Precise%20Motion%0A%20%20Control%0AAuthor%3A%20Yuanpeng%20Tu%20and%20Hao%20Luo%20and%20Xi%20Chen%20and%20Sihui%20Ji%20and%20Xiang%20Bai%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20video%20generation%2C%20inserting%20a%20given%0Aobject%20into%20videos%20remains%20a%20challenging%20task.%20The%20difficulty%20lies%20in%0Apreserving%20the%20appearance%20details%20of%20the%20reference%20object%20and%20accurately%0Amodeling%20coherent%20motions%20at%20the%20same%20time.%20In%20this%20paper%2C%20we%20propose%0AVideoAnydoor%2C%20a%20zero-shot%20video%20object%20insertion%20framework%20with%20high-fidelity%0Adetail%20preservation%20and%20precise%20motion%20control.%20Starting%20from%20a%20text-to-video%0Amodel%2C%20we%20utilize%20an%20ID%20extractor%20to%20inject%20the%20global%20identity%20and%20leverage%20a%0Abox%20sequence%20to%20control%20the%20overall%20motion.%20To%20preserve%20the%20detailed%20appearance%0Aand%20meanwhile%20support%20fine-grained%20motion%20control%2C%20we%20design%20a%20pixel%20warper.%20It%0Atakes%20the%20reference%20image%20with%20arbitrary%20key-points%20and%20the%20corresponding%0Akey-point%20trajectories%20as%20inputs.%20It%20warps%20the%20pixel%20details%20according%20to%20the%0Atrajectories%20and%20fuses%20the%20warped%20features%20with%20the%20diffusion%20U-Net%2C%20thus%0Aimproving%20detail%20preservation%20and%20supporting%20users%20in%20manipulating%20the%20motion%0Atrajectories.%20In%20addition%2C%20we%20propose%20a%20training%20strategy%20involving%20both%20videos%0Aand%20static%20images%20with%20a%20weighted%20loss%20to%20enhance%20insertion%20quality.%0AVideoAnydoor%20demonstrates%20significant%20superiority%20over%20existing%20methods%20and%0Anaturally%20supports%20various%20downstream%20applications%20%28e.g.%2C%20talking%20head%0Ageneration%2C%20video%20virtual%20try-on%2C%20multi-region%20editing%29%20without%20task-specific%0Afine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAnydoor%253A%2520High-fidelity%2520Video%2520Object%2520Insertion%2520with%2520Precise%2520Motion%250A%2520%2520Control%26entry.906535625%3DYuanpeng%2520Tu%2520and%2520Hao%2520Luo%2520and%2520Xi%2520Chen%2520and%2520Sihui%2520Ji%2520and%2520Xiang%2520Bai%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520video%2520generation%252C%2520inserting%2520a%2520given%250Aobject%2520into%2520videos%2520remains%2520a%2520challenging%2520task.%2520The%2520difficulty%2520lies%2520in%250Apreserving%2520the%2520appearance%2520details%2520of%2520the%2520reference%2520object%2520and%2520accurately%250Amodeling%2520coherent%2520motions%2520at%2520the%2520same%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%250AVideoAnydoor%252C%2520a%2520zero-shot%2520video%2520object%2520insertion%2520framework%2520with%2520high-fidelity%250Adetail%2520preservation%2520and%2520precise%2520motion%2520control.%2520Starting%2520from%2520a%2520text-to-video%250Amodel%252C%2520we%2520utilize%2520an%2520ID%2520extractor%2520to%2520inject%2520the%2520global%2520identity%2520and%2520leverage%2520a%250Abox%2520sequence%2520to%2520control%2520the%2520overall%2520motion.%2520To%2520preserve%2520the%2520detailed%2520appearance%250Aand%2520meanwhile%2520support%2520fine-grained%2520motion%2520control%252C%2520we%2520design%2520a%2520pixel%2520warper.%2520It%250Atakes%2520the%2520reference%2520image%2520with%2520arbitrary%2520key-points%2520and%2520the%2520corresponding%250Akey-point%2520trajectories%2520as%2520inputs.%2520It%2520warps%2520the%2520pixel%2520details%2520according%2520to%2520the%250Atrajectories%2520and%2520fuses%2520the%2520warped%2520features%2520with%2520the%2520diffusion%2520U-Net%252C%2520thus%250Aimproving%2520detail%2520preservation%2520and%2520supporting%2520users%2520in%2520manipulating%2520the%2520motion%250Atrajectories.%2520In%2520addition%252C%2520we%2520propose%2520a%2520training%2520strategy%2520involving%2520both%2520videos%250Aand%2520static%2520images%2520with%2520a%2520weighted%2520loss%2520to%2520enhance%2520insertion%2520quality.%250AVideoAnydoor%2520demonstrates%2520significant%2520superiority%2520over%2520existing%2520methods%2520and%250Anaturally%2520supports%2520various%2520downstream%2520applications%2520%2528e.g.%252C%2520talking%2520head%250Ageneration%252C%2520video%2520virtual%2520try-on%252C%2520multi-region%2520editing%2529%2520without%2520task-specific%250Afine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAnydoor%3A%20High-fidelity%20Video%20Object%20Insertion%20with%20Precise%20Motion%0A%20%20Control&entry.906535625=Yuanpeng%20Tu%20and%20Hao%20Luo%20and%20Xi%20Chen%20and%20Sihui%20Ji%20and%20Xiang%20Bai%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20video%20generation%2C%20inserting%20a%20given%0Aobject%20into%20videos%20remains%20a%20challenging%20task.%20The%20difficulty%20lies%20in%0Apreserving%20the%20appearance%20details%20of%20the%20reference%20object%20and%20accurately%0Amodeling%20coherent%20motions%20at%20the%20same%20time.%20In%20this%20paper%2C%20we%20propose%0AVideoAnydoor%2C%20a%20zero-shot%20video%20object%20insertion%20framework%20with%20high-fidelity%0Adetail%20preservation%20and%20precise%20motion%20control.%20Starting%20from%20a%20text-to-video%0Amodel%2C%20we%20utilize%20an%20ID%20extractor%20to%20inject%20the%20global%20identity%20and%20leverage%20a%0Abox%20sequence%20to%20control%20the%20overall%20motion.%20To%20preserve%20the%20detailed%20appearance%0Aand%20meanwhile%20support%20fine-grained%20motion%20control%2C%20we%20design%20a%20pixel%20warper.%20It%0Atakes%20the%20reference%20image%20with%20arbitrary%20key-points%20and%20the%20corresponding%0Akey-point%20trajectories%20as%20inputs.%20It%20warps%20the%20pixel%20details%20according%20to%20the%0Atrajectories%20and%20fuses%20the%20warped%20features%20with%20the%20diffusion%20U-Net%2C%20thus%0Aimproving%20detail%20preservation%20and%20supporting%20users%20in%20manipulating%20the%20motion%0Atrajectories.%20In%20addition%2C%20we%20propose%20a%20training%20strategy%20involving%20both%20videos%0Aand%20static%20images%20with%20a%20weighted%20loss%20to%20enhance%20insertion%20quality.%0AVideoAnydoor%20demonstrates%20significant%20superiority%20over%20existing%20methods%20and%0Anaturally%20supports%20various%20downstream%20applications%20%28e.g.%2C%20talking%20head%0Ageneration%2C%20video%20virtual%20try-on%2C%20multi-region%20editing%29%20without%20task-specific%0Afine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01427v2&entry.124074799=Read"},
{"title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning", "author": "Zhen Li and Yupeng Su and Runming Yang and Zhongwei Xie and Ngai Wong and Hongxia Yang", "abstract": "  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. We introduce a multidimensional evaluation\nframework that qualitatively assesses specific capability dimensions and\nconduct quantitative analyses on the step-by-step outputs of various\nquantization methods. Our results demonstrate that quantization differentially\naffects numerical computation and reasoning planning abilities, identifying key\nareas where quantized models experience performance degradation.\n", "link": "http://arxiv.org/abs/2501.03035v1", "date": "2025-01-06", "relevancy": 2.4783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&body=Title%3A%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning%0AAuthor%3A%20Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20We%20introduce%20a%20multidimensional%20evaluation%0Aframework%20that%20qualitatively%20assesses%20specific%20capability%20dimensions%20and%0Aconduct%20quantitative%20analyses%20on%20the%20step-by-step%20outputs%20of%20various%0Aquantization%20methods.%20Our%20results%20demonstrate%20that%20quantization%20differentially%0Aaffects%20numerical%20computation%20and%20reasoning%20planning%20abilities%2C%20identifying%20key%0Aareas%20where%20quantized%20models%20experience%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization%2520Meets%2520Reasoning%253A%2520Exploring%2520LLM%2520Low-Bit%2520Quantization%250A%2520%2520Degradation%2520for%2520Mathematical%2520Reasoning%26entry.906535625%3DZhen%2520Li%2520and%2520Yupeng%2520Su%2520and%2520Runming%2520Yang%2520and%2520Zhongwei%2520Xie%2520and%2520Ngai%2520Wong%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520significant%2520advancements%2520in%2520complex%250Amathematical%2520reasoning%2520benchmarks%252C%2520such%2520as%2520MATH.%2520However%252C%2520their%2520substantial%250Acomputational%2520requirements%2520present%2520challenges%2520for%2520practical%2520deployment.%2520Model%250Aquantization%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520reduce%2520memory%2520usage%2520and%250Acomputational%2520costs%2520by%2520employing%2520lower%2520precision%2520and%2520bit-width%2520representations.%250AIn%2520this%2520study%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%2520quantization%2520on%250Amathematical%2520reasoning%2520tasks.%2520We%2520introduce%2520a%2520multidimensional%2520evaluation%250Aframework%2520that%2520qualitatively%2520assesses%2520specific%2520capability%2520dimensions%2520and%250Aconduct%2520quantitative%2520analyses%2520on%2520the%2520step-by-step%2520outputs%2520of%2520various%250Aquantization%2520methods.%2520Our%2520results%2520demonstrate%2520that%2520quantization%2520differentially%250Aaffects%2520numerical%2520computation%2520and%2520reasoning%2520planning%2520abilities%252C%2520identifying%2520key%250Aareas%2520where%2520quantized%2520models%2520experience%2520performance%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&entry.906535625=Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20We%20introduce%20a%20multidimensional%20evaluation%0Aframework%20that%20qualitatively%20assesses%20specific%20capability%20dimensions%20and%0Aconduct%20quantitative%20analyses%20on%20the%20step-by-step%20outputs%20of%20various%0Aquantization%20methods.%20Our%20results%20demonstrate%20that%20quantization%20differentially%0Aaffects%20numerical%20computation%20and%20reasoning%20planning%20abilities%2C%20identifying%20key%0Aareas%20where%20quantized%20models%20experience%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03035v1&entry.124074799=Read"},
{"title": "LightGNN: Simple Graph Neural Network for Recommendation", "author": "Guoxuan Chen and Lianghao Xia and Chao Huang", "abstract": "  Graph neural networks (GNNs) have demonstrated superior performance in\ncollaborative recommendation through their ability to conduct high-order\nrepresentation smoothing, effectively capturing structural information within\nusers' interaction patterns. However, existing GNN paradigms face significant\nchallenges in scalability and robustness when handling large-scale, noisy, and\nreal-world datasets. To address these challenges, we present LightGNN, a\nlightweight and distillation-based GNN pruning framework designed to\nsubstantially reduce model complexity while preserving essential collaboration\nmodeling capabilities. Our LightGNN framework introduces a computationally\nefficient pruning module that adaptively identifies and removes redundant edges\nand embedding entries for model compression. The framework is guided by a\nresource-friendly hierarchical knowledge distillation objective, whose\nintermediate layer augments the observed graph to maintain performance,\nparticularly in high-rate compression scenarios. Extensive experiments on\npublic datasets demonstrate LightGNN's effectiveness, significantly improving\nboth computational efficiency and recommendation accuracy. Notably, LightGNN\nachieves an 80% reduction in edge count and 90% reduction in embedding entries\nwhile maintaining performance comparable to more complex state-of-the-art\nbaselines. The implementation of our LightGNN framework is available at the\ngithub repository: https://github.com/HKUDS/LightGNN.\n", "link": "http://arxiv.org/abs/2501.03228v1", "date": "2025-01-06", "relevancy": 2.4719, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5049}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightGNN%3A%20Simple%20Graph%20Neural%20Network%20for%20Recommendation&body=Title%3A%20LightGNN%3A%20Simple%20Graph%20Neural%20Network%20for%20Recommendation%0AAuthor%3A%20Guoxuan%20Chen%20and%20Lianghao%20Xia%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20superior%20performance%20in%0Acollaborative%20recommendation%20through%20their%20ability%20to%20conduct%20high-order%0Arepresentation%20smoothing%2C%20effectively%20capturing%20structural%20information%20within%0Ausers%27%20interaction%20patterns.%20However%2C%20existing%20GNN%20paradigms%20face%20significant%0Achallenges%20in%20scalability%20and%20robustness%20when%20handling%20large-scale%2C%20noisy%2C%20and%0Areal-world%20datasets.%20To%20address%20these%20challenges%2C%20we%20present%20LightGNN%2C%20a%0Alightweight%20and%20distillation-based%20GNN%20pruning%20framework%20designed%20to%0Asubstantially%20reduce%20model%20complexity%20while%20preserving%20essential%20collaboration%0Amodeling%20capabilities.%20Our%20LightGNN%20framework%20introduces%20a%20computationally%0Aefficient%20pruning%20module%20that%20adaptively%20identifies%20and%20removes%20redundant%20edges%0Aand%20embedding%20entries%20for%20model%20compression.%20The%20framework%20is%20guided%20by%20a%0Aresource-friendly%20hierarchical%20knowledge%20distillation%20objective%2C%20whose%0Aintermediate%20layer%20augments%20the%20observed%20graph%20to%20maintain%20performance%2C%0Aparticularly%20in%20high-rate%20compression%20scenarios.%20Extensive%20experiments%20on%0Apublic%20datasets%20demonstrate%20LightGNN%27s%20effectiveness%2C%20significantly%20improving%0Aboth%20computational%20efficiency%20and%20recommendation%20accuracy.%20Notably%2C%20LightGNN%0Aachieves%20an%2080%25%20reduction%20in%20edge%20count%20and%2090%25%20reduction%20in%20embedding%20entries%0Awhile%20maintaining%20performance%20comparable%20to%20more%20complex%20state-of-the-art%0Abaselines.%20The%20implementation%20of%20our%20LightGNN%20framework%20is%20available%20at%20the%0Agithub%20repository%3A%20https%3A//github.com/HKUDS/LightGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightGNN%253A%2520Simple%2520Graph%2520Neural%2520Network%2520for%2520Recommendation%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Lianghao%2520Xia%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520superior%2520performance%2520in%250Acollaborative%2520recommendation%2520through%2520their%2520ability%2520to%2520conduct%2520high-order%250Arepresentation%2520smoothing%252C%2520effectively%2520capturing%2520structural%2520information%2520within%250Ausers%2527%2520interaction%2520patterns.%2520However%252C%2520existing%2520GNN%2520paradigms%2520face%2520significant%250Achallenges%2520in%2520scalability%2520and%2520robustness%2520when%2520handling%2520large-scale%252C%2520noisy%252C%2520and%250Areal-world%2520datasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520LightGNN%252C%2520a%250Alightweight%2520and%2520distillation-based%2520GNN%2520pruning%2520framework%2520designed%2520to%250Asubstantially%2520reduce%2520model%2520complexity%2520while%2520preserving%2520essential%2520collaboration%250Amodeling%2520capabilities.%2520Our%2520LightGNN%2520framework%2520introduces%2520a%2520computationally%250Aefficient%2520pruning%2520module%2520that%2520adaptively%2520identifies%2520and%2520removes%2520redundant%2520edges%250Aand%2520embedding%2520entries%2520for%2520model%2520compression.%2520The%2520framework%2520is%2520guided%2520by%2520a%250Aresource-friendly%2520hierarchical%2520knowledge%2520distillation%2520objective%252C%2520whose%250Aintermediate%2520layer%2520augments%2520the%2520observed%2520graph%2520to%2520maintain%2520performance%252C%250Aparticularly%2520in%2520high-rate%2520compression%2520scenarios.%2520Extensive%2520experiments%2520on%250Apublic%2520datasets%2520demonstrate%2520LightGNN%2527s%2520effectiveness%252C%2520significantly%2520improving%250Aboth%2520computational%2520efficiency%2520and%2520recommendation%2520accuracy.%2520Notably%252C%2520LightGNN%250Aachieves%2520an%252080%2525%2520reduction%2520in%2520edge%2520count%2520and%252090%2525%2520reduction%2520in%2520embedding%2520entries%250Awhile%2520maintaining%2520performance%2520comparable%2520to%2520more%2520complex%2520state-of-the-art%250Abaselines.%2520The%2520implementation%2520of%2520our%2520LightGNN%2520framework%2520is%2520available%2520at%2520the%250Agithub%2520repository%253A%2520https%253A//github.com/HKUDS/LightGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightGNN%3A%20Simple%20Graph%20Neural%20Network%20for%20Recommendation&entry.906535625=Guoxuan%20Chen%20and%20Lianghao%20Xia%20and%20Chao%20Huang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20superior%20performance%20in%0Acollaborative%20recommendation%20through%20their%20ability%20to%20conduct%20high-order%0Arepresentation%20smoothing%2C%20effectively%20capturing%20structural%20information%20within%0Ausers%27%20interaction%20patterns.%20However%2C%20existing%20GNN%20paradigms%20face%20significant%0Achallenges%20in%20scalability%20and%20robustness%20when%20handling%20large-scale%2C%20noisy%2C%20and%0Areal-world%20datasets.%20To%20address%20these%20challenges%2C%20we%20present%20LightGNN%2C%20a%0Alightweight%20and%20distillation-based%20GNN%20pruning%20framework%20designed%20to%0Asubstantially%20reduce%20model%20complexity%20while%20preserving%20essential%20collaboration%0Amodeling%20capabilities.%20Our%20LightGNN%20framework%20introduces%20a%20computationally%0Aefficient%20pruning%20module%20that%20adaptively%20identifies%20and%20removes%20redundant%20edges%0Aand%20embedding%20entries%20for%20model%20compression.%20The%20framework%20is%20guided%20by%20a%0Aresource-friendly%20hierarchical%20knowledge%20distillation%20objective%2C%20whose%0Aintermediate%20layer%20augments%20the%20observed%20graph%20to%20maintain%20performance%2C%0Aparticularly%20in%20high-rate%20compression%20scenarios.%20Extensive%20experiments%20on%0Apublic%20datasets%20demonstrate%20LightGNN%27s%20effectiveness%2C%20significantly%20improving%0Aboth%20computational%20efficiency%20and%20recommendation%20accuracy.%20Notably%2C%20LightGNN%0Aachieves%20an%2080%25%20reduction%20in%20edge%20count%20and%2090%25%20reduction%20in%20embedding%20entries%0Awhile%20maintaining%20performance%20comparable%20to%20more%20complex%20state-of-the-art%0Abaselines.%20The%20implementation%20of%20our%20LightGNN%20framework%20is%20available%20at%20the%0Agithub%20repository%3A%20https%3A//github.com/HKUDS/LightGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03228v1&entry.124074799=Read"},
{"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion\n  Understanding for Vision Language Models", "author": "Wenyi Hong and Yean Cheng and Zhuoyi Yang and Weihan Wang and Lefan Wang and Xiaotao Gu and Shiyu Huang and Yuxiao Dong and Jie Tang", "abstract": "  In recent years, vision language models (VLMs) have made significant\nadvancements in video understanding. However, a crucial capability -\nfine-grained motion comprehension - remains under-explored in current\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\nevaluation benchmark designed to assess the fine-grained motion comprehension\nof video understanding models. MotionBench evaluates models' motion-level\nperception through six primary categories of motion-oriented question types and\nincludes data collected from diverse sources, ensuring a broad representation\nof real-world video content. Experimental results reveal that existing VLMs\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\nto perceive fine-grained motion within a limited sequence length of LLM, we\nconduct extensive experiments reviewing VLM architectures optimized for video\nfeature compression and propose a novel and efficient Through-Encoder (TE)\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\nyield improvements in motion understanding, yet there is still substantial room\nfor enhancement. Our benchmark aims to guide and motivate the development of\nmore capable video understanding models, emphasizing the importance of\nfine-grained motion comprehension. Project page: https://motion-bench.github.io .\n", "link": "http://arxiv.org/abs/2501.02955v1", "date": "2025-01-06", "relevancy": 2.459, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6159}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionBench%3A%20Benchmarking%20and%20Improving%20Fine-grained%20Video%20Motion%0A%20%20Understanding%20for%20Vision%20Language%20Models&body=Title%3A%20MotionBench%3A%20Benchmarking%20and%20Improving%20Fine-grained%20Video%20Motion%0A%20%20Understanding%20for%20Vision%20Language%20Models%0AAuthor%3A%20Wenyi%20Hong%20and%20Yean%20Cheng%20and%20Zhuoyi%20Yang%20and%20Weihan%20Wang%20and%20Lefan%20Wang%20and%20Xiaotao%20Gu%20and%20Shiyu%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20vision%20language%20models%20%28VLMs%29%20have%20made%20significant%0Aadvancements%20in%20video%20understanding.%20However%2C%20a%20crucial%20capability%20-%0Afine-grained%20motion%20comprehension%20-%20remains%20under-explored%20in%20current%0Abenchmarks.%20To%20address%20this%20gap%2C%20we%20propose%20MotionBench%2C%20a%20comprehensive%0Aevaluation%20benchmark%20designed%20to%20assess%20the%20fine-grained%20motion%20comprehension%0Aof%20video%20understanding%20models.%20MotionBench%20evaluates%20models%27%20motion-level%0Aperception%20through%20six%20primary%20categories%20of%20motion-oriented%20question%20types%20and%0Aincludes%20data%20collected%20from%20diverse%20sources%2C%20ensuring%20a%20broad%20representation%0Aof%20real-world%20video%20content.%20Experimental%20results%20reveal%20that%20existing%20VLMs%0Aperform%20poorly%20in%20understanding%20fine-grained%20motions.%20To%20enhance%20VLM%27s%20ability%0Ato%20perceive%20fine-grained%20motion%20within%20a%20limited%20sequence%20length%20of%20LLM%2C%20we%0Aconduct%20extensive%20experiments%20reviewing%20VLM%20architectures%20optimized%20for%20video%0Afeature%20compression%20and%20propose%20a%20novel%20and%20efficient%20Through-Encoder%20%28TE%29%0AFusion%20method.%20Experiments%20show%20that%20higher%20frame%20rate%20inputs%20and%20TE%20Fusion%0Ayield%20improvements%20in%20motion%20understanding%2C%20yet%20there%20is%20still%20substantial%20room%0Afor%20enhancement.%20Our%20benchmark%20aims%20to%20guide%20and%20motivate%20the%20development%20of%0Amore%20capable%20video%20understanding%20models%2C%20emphasizing%20the%20importance%20of%0Afine-grained%20motion%20comprehension.%20Project%20page%3A%20https%3A//motion-bench.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionBench%253A%2520Benchmarking%2520and%2520Improving%2520Fine-grained%2520Video%2520Motion%250A%2520%2520Understanding%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DWenyi%2520Hong%2520and%2520Yean%2520Cheng%2520and%2520Zhuoyi%2520Yang%2520and%2520Weihan%2520Wang%2520and%2520Lefan%2520Wang%2520and%2520Xiaotao%2520Gu%2520and%2520Shiyu%2520Huang%2520and%2520Yuxiao%2520Dong%2520and%2520Jie%2520Tang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%250Aadvancements%2520in%2520video%2520understanding.%2520However%252C%2520a%2520crucial%2520capability%2520-%250Afine-grained%2520motion%2520comprehension%2520-%2520remains%2520under-explored%2520in%2520current%250Abenchmarks.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520MotionBench%252C%2520a%2520comprehensive%250Aevaluation%2520benchmark%2520designed%2520to%2520assess%2520the%2520fine-grained%2520motion%2520comprehension%250Aof%2520video%2520understanding%2520models.%2520MotionBench%2520evaluates%2520models%2527%2520motion-level%250Aperception%2520through%2520six%2520primary%2520categories%2520of%2520motion-oriented%2520question%2520types%2520and%250Aincludes%2520data%2520collected%2520from%2520diverse%2520sources%252C%2520ensuring%2520a%2520broad%2520representation%250Aof%2520real-world%2520video%2520content.%2520Experimental%2520results%2520reveal%2520that%2520existing%2520VLMs%250Aperform%2520poorly%2520in%2520understanding%2520fine-grained%2520motions.%2520To%2520enhance%2520VLM%2527s%2520ability%250Ato%2520perceive%2520fine-grained%2520motion%2520within%2520a%2520limited%2520sequence%2520length%2520of%2520LLM%252C%2520we%250Aconduct%2520extensive%2520experiments%2520reviewing%2520VLM%2520architectures%2520optimized%2520for%2520video%250Afeature%2520compression%2520and%2520propose%2520a%2520novel%2520and%2520efficient%2520Through-Encoder%2520%2528TE%2529%250AFusion%2520method.%2520Experiments%2520show%2520that%2520higher%2520frame%2520rate%2520inputs%2520and%2520TE%2520Fusion%250Ayield%2520improvements%2520in%2520motion%2520understanding%252C%2520yet%2520there%2520is%2520still%2520substantial%2520room%250Afor%2520enhancement.%2520Our%2520benchmark%2520aims%2520to%2520guide%2520and%2520motivate%2520the%2520development%2520of%250Amore%2520capable%2520video%2520understanding%2520models%252C%2520emphasizing%2520the%2520importance%2520of%250Afine-grained%2520motion%2520comprehension.%2520Project%2520page%253A%2520https%253A//motion-bench.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionBench%3A%20Benchmarking%20and%20Improving%20Fine-grained%20Video%20Motion%0A%20%20Understanding%20for%20Vision%20Language%20Models&entry.906535625=Wenyi%20Hong%20and%20Yean%20Cheng%20and%20Zhuoyi%20Yang%20and%20Weihan%20Wang%20and%20Lefan%20Wang%20and%20Xiaotao%20Gu%20and%20Shiyu%20Huang%20and%20Yuxiao%20Dong%20and%20Jie%20Tang&entry.1292438233=%20%20In%20recent%20years%2C%20vision%20language%20models%20%28VLMs%29%20have%20made%20significant%0Aadvancements%20in%20video%20understanding.%20However%2C%20a%20crucial%20capability%20-%0Afine-grained%20motion%20comprehension%20-%20remains%20under-explored%20in%20current%0Abenchmarks.%20To%20address%20this%20gap%2C%20we%20propose%20MotionBench%2C%20a%20comprehensive%0Aevaluation%20benchmark%20designed%20to%20assess%20the%20fine-grained%20motion%20comprehension%0Aof%20video%20understanding%20models.%20MotionBench%20evaluates%20models%27%20motion-level%0Aperception%20through%20six%20primary%20categories%20of%20motion-oriented%20question%20types%20and%0Aincludes%20data%20collected%20from%20diverse%20sources%2C%20ensuring%20a%20broad%20representation%0Aof%20real-world%20video%20content.%20Experimental%20results%20reveal%20that%20existing%20VLMs%0Aperform%20poorly%20in%20understanding%20fine-grained%20motions.%20To%20enhance%20VLM%27s%20ability%0Ato%20perceive%20fine-grained%20motion%20within%20a%20limited%20sequence%20length%20of%20LLM%2C%20we%0Aconduct%20extensive%20experiments%20reviewing%20VLM%20architectures%20optimized%20for%20video%0Afeature%20compression%20and%20propose%20a%20novel%20and%20efficient%20Through-Encoder%20%28TE%29%0AFusion%20method.%20Experiments%20show%20that%20higher%20frame%20rate%20inputs%20and%20TE%20Fusion%0Ayield%20improvements%20in%20motion%20understanding%2C%20yet%20there%20is%20still%20substantial%20room%0Afor%20enhancement.%20Our%20benchmark%20aims%20to%20guide%20and%20motivate%20the%20development%20of%0Amore%20capable%20video%20understanding%20models%2C%20emphasizing%20the%20importance%20of%0Afine-grained%20motion%20comprehension.%20Project%20page%3A%20https%3A//motion-bench.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02955v1&entry.124074799=Read"},
{"title": "Reviewing Intelligent Cinematography: AI research for camera-based video\n  production", "author": "Adrian Azzarelli and Nantheera Anantrasirichai and David R Bull", "abstract": "  This paper offers the first comprehensive review of artificial intelligence\n(AI) research in the context of real camera content acquisition for\nentertainment purposes and is aimed at both researchers and cinematographers.\nAddressing the lack of review papers in the field of intelligent\ncinematography} (IC) and the breadth of related computer vision research, we\npresent a holistic view of the IC landscape while providing technical insight,\nimportant for experts across disciplines. We provide technical background on\ngenerative AI, object detection, automated camera calibration and 3-D content\nacquisition, with references to assist non-technical readers. The application\nsections categorize work in terms of four production types: General Production,\nVirtual Production, Live Production and Aerial Production. Within each\napplication section, we (1) sub-classify work according to research topic and\n(2) describe the trends and challenges relevant to each type of production. In\nthe final chapter, we address the greater scope of IC research and summarize\nthe significant potential of this area to influence the creative industries\nsector. We suggest that work relating to virtual production has the greatest\npotential to impact other mediums of production, driven by the growing interest\nin LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D\ncapture for virtual modeling of real world scenes and actors. We also address\nethical and legal concerns regarding the use of creative AI that impact on\nartists, actors, technologists and the general public.\n", "link": "http://arxiv.org/abs/2405.05039v3", "date": "2025-01-06", "relevancy": 2.4514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&body=Title%3A%20Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production%0AAuthor%3A%20Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull%0AAbstract%3A%20%20%20This%20paper%20offers%20the%20first%20comprehensive%20review%20of%20artificial%20intelligence%0A%28AI%29%20research%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%0Aentertainment%20purposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%0AAddressing%20the%20lack%20of%20review%20papers%20in%20the%20field%20of%20intelligent%0Acinematography%7D%20%28IC%29%20and%20the%20breadth%20of%20related%20computer%20vision%20research%2C%20we%0Apresent%20a%20holistic%20view%20of%20the%20IC%20landscape%20while%20providing%20technical%20insight%2C%0Aimportant%20for%20experts%20across%20disciplines.%20We%20provide%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20with%20references%20to%20assist%20non-technical%20readers.%20The%20application%0Asections%20categorize%20work%20in%20terms%20of%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Within%20each%0Aapplication%20section%2C%20we%20%281%29%20sub-classify%20work%20according%20to%20research%20topic%20and%0A%282%29%20describe%20the%20trends%20and%20challenges%20relevant%20to%20each%20type%20of%20production.%20In%0Athe%20final%20chapter%2C%20we%20address%20the%20greater%20scope%20of%20IC%20research%20and%20summarize%0Athe%20significant%20potential%20of%20this%20area%20to%20influence%20the%20creative%20industries%0Asector.%20We%20suggest%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%0Apotential%20to%20impact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%0Ain%20LED%20volumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%0Acapture%20for%20virtual%20modeling%20of%20real%20world%20scenes%20and%20actors.%20We%20also%20address%0Aethical%20and%20legal%20concerns%20regarding%20the%20use%20of%20creative%20AI%20that%20impact%20on%0Aartists%2C%20actors%2C%20technologists%20and%20the%20general%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReviewing%2520Intelligent%2520Cinematography%253A%2520AI%2520research%2520for%2520camera-based%2520video%250A%2520%2520production%26entry.906535625%3DAdrian%2520Azzarelli%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520David%2520R%2520Bull%26entry.1292438233%3D%2520%2520This%2520paper%2520offers%2520the%2520first%2520comprehensive%2520review%2520of%2520artificial%2520intelligence%250A%2528AI%2529%2520research%2520in%2520the%2520context%2520of%2520real%2520camera%2520content%2520acquisition%2520for%250Aentertainment%2520purposes%2520and%2520is%2520aimed%2520at%2520both%2520researchers%2520and%2520cinematographers.%250AAddressing%2520the%2520lack%2520of%2520review%2520papers%2520in%2520the%2520field%2520of%2520intelligent%250Acinematography%257D%2520%2528IC%2529%2520and%2520the%2520breadth%2520of%2520related%2520computer%2520vision%2520research%252C%2520we%250Apresent%2520a%2520holistic%2520view%2520of%2520the%2520IC%2520landscape%2520while%2520providing%2520technical%2520insight%252C%250Aimportant%2520for%2520experts%2520across%2520disciplines.%2520We%2520provide%2520technical%2520background%2520on%250Agenerative%2520AI%252C%2520object%2520detection%252C%2520automated%2520camera%2520calibration%2520and%25203-D%2520content%250Aacquisition%252C%2520with%2520references%2520to%2520assist%2520non-technical%2520readers.%2520The%2520application%250Asections%2520categorize%2520work%2520in%2520terms%2520of%2520four%2520production%2520types%253A%2520General%2520Production%252C%250AVirtual%2520Production%252C%2520Live%2520Production%2520and%2520Aerial%2520Production.%2520Within%2520each%250Aapplication%2520section%252C%2520we%2520%25281%2529%2520sub-classify%2520work%2520according%2520to%2520research%2520topic%2520and%250A%25282%2529%2520describe%2520the%2520trends%2520and%2520challenges%2520relevant%2520to%2520each%2520type%2520of%2520production.%2520In%250Athe%2520final%2520chapter%252C%2520we%2520address%2520the%2520greater%2520scope%2520of%2520IC%2520research%2520and%2520summarize%250Athe%2520significant%2520potential%2520of%2520this%2520area%2520to%2520influence%2520the%2520creative%2520industries%250Asector.%2520We%2520suggest%2520that%2520work%2520relating%2520to%2520virtual%2520production%2520has%2520the%2520greatest%250Apotential%2520to%2520impact%2520other%2520mediums%2520of%2520production%252C%2520driven%2520by%2520the%2520growing%2520interest%250Ain%2520LED%2520volumes/stages%2520for%2520in-camera%2520virtual%2520effects%2520%2528ICVFX%2529%2520and%2520automated%25203-D%250Acapture%2520for%2520virtual%2520modeling%2520of%2520real%2520world%2520scenes%2520and%2520actors.%2520We%2520also%2520address%250Aethical%2520and%2520legal%2520concerns%2520regarding%2520the%2520use%2520of%2520creative%2520AI%2520that%2520impact%2520on%250Aartists%252C%2520actors%252C%2520technologists%2520and%2520the%2520general%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reviewing%20Intelligent%20Cinematography%3A%20AI%20research%20for%20camera-based%20video%0A%20%20production&entry.906535625=Adrian%20Azzarelli%20and%20Nantheera%20Anantrasirichai%20and%20David%20R%20Bull&entry.1292438233=%20%20This%20paper%20offers%20the%20first%20comprehensive%20review%20of%20artificial%20intelligence%0A%28AI%29%20research%20in%20the%20context%20of%20real%20camera%20content%20acquisition%20for%0Aentertainment%20purposes%20and%20is%20aimed%20at%20both%20researchers%20and%20cinematographers.%0AAddressing%20the%20lack%20of%20review%20papers%20in%20the%20field%20of%20intelligent%0Acinematography%7D%20%28IC%29%20and%20the%20breadth%20of%20related%20computer%20vision%20research%2C%20we%0Apresent%20a%20holistic%20view%20of%20the%20IC%20landscape%20while%20providing%20technical%20insight%2C%0Aimportant%20for%20experts%20across%20disciplines.%20We%20provide%20technical%20background%20on%0Agenerative%20AI%2C%20object%20detection%2C%20automated%20camera%20calibration%20and%203-D%20content%0Aacquisition%2C%20with%20references%20to%20assist%20non-technical%20readers.%20The%20application%0Asections%20categorize%20work%20in%20terms%20of%20four%20production%20types%3A%20General%20Production%2C%0AVirtual%20Production%2C%20Live%20Production%20and%20Aerial%20Production.%20Within%20each%0Aapplication%20section%2C%20we%20%281%29%20sub-classify%20work%20according%20to%20research%20topic%20and%0A%282%29%20describe%20the%20trends%20and%20challenges%20relevant%20to%20each%20type%20of%20production.%20In%0Athe%20final%20chapter%2C%20we%20address%20the%20greater%20scope%20of%20IC%20research%20and%20summarize%0Athe%20significant%20potential%20of%20this%20area%20to%20influence%20the%20creative%20industries%0Asector.%20We%20suggest%20that%20work%20relating%20to%20virtual%20production%20has%20the%20greatest%0Apotential%20to%20impact%20other%20mediums%20of%20production%2C%20driven%20by%20the%20growing%20interest%0Ain%20LED%20volumes/stages%20for%20in-camera%20virtual%20effects%20%28ICVFX%29%20and%20automated%203-D%0Acapture%20for%20virtual%20modeling%20of%20real%20world%20scenes%20and%20actors.%20We%20also%20address%0Aethical%20and%20legal%20concerns%20regarding%20the%20use%20of%20creative%20AI%20that%20impact%20on%0Aartists%2C%20actors%2C%20technologists%20and%20the%20general%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05039v3&entry.124074799=Read"},
{"title": "BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning", "author": "Beichen Zhang and Yuhong Liu and Xiaoyi Dong and Yuhang Zang and Pan Zhang and Haodong Duan and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS.\n", "link": "http://arxiv.org/abs/2501.03226v1", "date": "2025-01-06", "relevancy": 2.4237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoostStep%3A%20Boosting%20mathematical%20capability%20of%20Large%20Language%20Models%20via%0A%20%20improved%20single-step%20reasoning&body=Title%3A%20BoostStep%3A%20Boosting%20mathematical%20capability%20of%20Large%20Language%20Models%20via%0A%20%20improved%20single-step%20reasoning%0AAuthor%3A%20Beichen%20Zhang%20and%20Yuhong%20Liu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Pan%20Zhang%20and%20Haodong%20Duan%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Cutting-edge%20large%20language%20models%20%28LLMs%29%20demonstrate%20promising%20performance%0Ain%20solving%20complex%20math%20problems%20with%20a%20divide-and-conquer%20pipeline%20and%20the%0Aassistance%20of%20in-context%20learning%20%28ICL%29%20examples.%20However%2C%20their%20potential%20for%0Aimprovement%20is%20limited%20by%20two%20critical%20problems%20within%20their%20ICL%20examples%3A%0Agranularity-mismatch%20and%20the%20ensuing%20negative-effect%20noise%20problem.%0ASpecifically%2C%20the%20LLMs%20are%20capable%20of%20the%20dividing%20process%20yet%20mostly%20failed%20by%0Ainaccurate%20reasoning%20within%20a%20few%20conquer%20steps%2C%20while%20the%20ICL%20examples%0Aretrieved%20in%20question-grained%20sometimes%20lack%20relevant%20steps%20for%20a%20specific%0Achallenging%20reasoning%20step.%20Further%2C%20this%20disconnect%20may%20hinder%20the%20correct%0Areasoning%20due%20to%20its%20irrelevance.%20To%20this%20end%2C%20we%20focus%20on%20improving%20the%0Areasoning%20quality%20within%20each%20step%20and%20present%20BoostStep.%20BoostStep%20aligns%20the%0Agranularity%20between%20the%20retrieving%20and%20reasoning%20on%20step%20grained%2C%20and%20provides%0Ahighly%20related%20ICL%20examples%20for%20each%20reasoning%20step%20with%20a%20novel%20%60first-try%27%0Astrategy.%20BoostStep%20provides%20more%20relevant%20examples%20than%20the%20coarse%0Aquestion-grained%20strategy%2C%20enhancing%20the%20model%20reasoning%20quality%20within%20each%0Astep%20steadily.%20BoostStep%20is%20a%20general%20and%20robust%20reasoning-enhancing%20method%0Athat%20not%20only%20improves%20standalone%20reasoning%20performance%20but%20also%20integrates%0Aseamlessly%20with%20Monte%20Carlo%20Tree%20Search%20methods%20%28MCTS%29%20to%20refine%20both%20candidate%0Ageneration%20and%20decision-making.%20Quantitatively%2C%20it%20improves%20GPT-4o%20and%0AQwen2.5-Math-72B%20by%203.6%5C%25%20and%202.0%5C%25%20respectively%20on%20various%20mathematical%0Abenchmarks%2C%20and%207.5%5C%25%20gain%20combined%20with%20MCTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoostStep%253A%2520Boosting%2520mathematical%2520capability%2520of%2520Large%2520Language%2520Models%2520via%250A%2520%2520improved%2520single-step%2520reasoning%26entry.906535625%3DBeichen%2520Zhang%2520and%2520Yuhong%2520Liu%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Pan%2520Zhang%2520and%2520Haodong%2520Duan%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Cutting-edge%2520large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520promising%2520performance%250Ain%2520solving%2520complex%2520math%2520problems%2520with%2520a%2520divide-and-conquer%2520pipeline%2520and%2520the%250Aassistance%2520of%2520in-context%2520learning%2520%2528ICL%2529%2520examples.%2520However%252C%2520their%2520potential%2520for%250Aimprovement%2520is%2520limited%2520by%2520two%2520critical%2520problems%2520within%2520their%2520ICL%2520examples%253A%250Agranularity-mismatch%2520and%2520the%2520ensuing%2520negative-effect%2520noise%2520problem.%250ASpecifically%252C%2520the%2520LLMs%2520are%2520capable%2520of%2520the%2520dividing%2520process%2520yet%2520mostly%2520failed%2520by%250Ainaccurate%2520reasoning%2520within%2520a%2520few%2520conquer%2520steps%252C%2520while%2520the%2520ICL%2520examples%250Aretrieved%2520in%2520question-grained%2520sometimes%2520lack%2520relevant%2520steps%2520for%2520a%2520specific%250Achallenging%2520reasoning%2520step.%2520Further%252C%2520this%2520disconnect%2520may%2520hinder%2520the%2520correct%250Areasoning%2520due%2520to%2520its%2520irrelevance.%2520To%2520this%2520end%252C%2520we%2520focus%2520on%2520improving%2520the%250Areasoning%2520quality%2520within%2520each%2520step%2520and%2520present%2520BoostStep.%2520BoostStep%2520aligns%2520the%250Agranularity%2520between%2520the%2520retrieving%2520and%2520reasoning%2520on%2520step%2520grained%252C%2520and%2520provides%250Ahighly%2520related%2520ICL%2520examples%2520for%2520each%2520reasoning%2520step%2520with%2520a%2520novel%2520%2560first-try%2527%250Astrategy.%2520BoostStep%2520provides%2520more%2520relevant%2520examples%2520than%2520the%2520coarse%250Aquestion-grained%2520strategy%252C%2520enhancing%2520the%2520model%2520reasoning%2520quality%2520within%2520each%250Astep%2520steadily.%2520BoostStep%2520is%2520a%2520general%2520and%2520robust%2520reasoning-enhancing%2520method%250Athat%2520not%2520only%2520improves%2520standalone%2520reasoning%2520performance%2520but%2520also%2520integrates%250Aseamlessly%2520with%2520Monte%2520Carlo%2520Tree%2520Search%2520methods%2520%2528MCTS%2529%2520to%2520refine%2520both%2520candidate%250Ageneration%2520and%2520decision-making.%2520Quantitatively%252C%2520it%2520improves%2520GPT-4o%2520and%250AQwen2.5-Math-72B%2520by%25203.6%255C%2525%2520and%25202.0%255C%2525%2520respectively%2520on%2520various%2520mathematical%250Abenchmarks%252C%2520and%25207.5%255C%2525%2520gain%2520combined%2520with%2520MCTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoostStep%3A%20Boosting%20mathematical%20capability%20of%20Large%20Language%20Models%20via%0A%20%20improved%20single-step%20reasoning&entry.906535625=Beichen%20Zhang%20and%20Yuhong%20Liu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Pan%20Zhang%20and%20Haodong%20Duan%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Cutting-edge%20large%20language%20models%20%28LLMs%29%20demonstrate%20promising%20performance%0Ain%20solving%20complex%20math%20problems%20with%20a%20divide-and-conquer%20pipeline%20and%20the%0Aassistance%20of%20in-context%20learning%20%28ICL%29%20examples.%20However%2C%20their%20potential%20for%0Aimprovement%20is%20limited%20by%20two%20critical%20problems%20within%20their%20ICL%20examples%3A%0Agranularity-mismatch%20and%20the%20ensuing%20negative-effect%20noise%20problem.%0ASpecifically%2C%20the%20LLMs%20are%20capable%20of%20the%20dividing%20process%20yet%20mostly%20failed%20by%0Ainaccurate%20reasoning%20within%20a%20few%20conquer%20steps%2C%20while%20the%20ICL%20examples%0Aretrieved%20in%20question-grained%20sometimes%20lack%20relevant%20steps%20for%20a%20specific%0Achallenging%20reasoning%20step.%20Further%2C%20this%20disconnect%20may%20hinder%20the%20correct%0Areasoning%20due%20to%20its%20irrelevance.%20To%20this%20end%2C%20we%20focus%20on%20improving%20the%0Areasoning%20quality%20within%20each%20step%20and%20present%20BoostStep.%20BoostStep%20aligns%20the%0Agranularity%20between%20the%20retrieving%20and%20reasoning%20on%20step%20grained%2C%20and%20provides%0Ahighly%20related%20ICL%20examples%20for%20each%20reasoning%20step%20with%20a%20novel%20%60first-try%27%0Astrategy.%20BoostStep%20provides%20more%20relevant%20examples%20than%20the%20coarse%0Aquestion-grained%20strategy%2C%20enhancing%20the%20model%20reasoning%20quality%20within%20each%0Astep%20steadily.%20BoostStep%20is%20a%20general%20and%20robust%20reasoning-enhancing%20method%0Athat%20not%20only%20improves%20standalone%20reasoning%20performance%20but%20also%20integrates%0Aseamlessly%20with%20Monte%20Carlo%20Tree%20Search%20methods%20%28MCTS%29%20to%20refine%20both%20candidate%0Ageneration%20and%20decision-making.%20Quantitatively%2C%20it%20improves%20GPT-4o%20and%0AQwen2.5-Math-72B%20by%203.6%5C%25%20and%202.0%5C%25%20respectively%20on%20various%20mathematical%0Abenchmarks%2C%20and%207.5%5C%25%20gain%20combined%20with%20MCTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03226v1&entry.124074799=Read"},
{"title": "CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural\n  Networks", "author": "Atmane Ayoub Mansour Bahara and Kamel Soa\u00efd Ferrahia and Mohamed-Lamine Messai and Hamida Seba and Karima Amrouche", "abstract": "  Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their sophisticated and stealthy nature. Traditional\nIntrusion Detection Systems (IDS) often fall short in detecting these\nmulti-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed\nto enhance IDS capabilities by analyzing the complex relationships within\nnetworked data. However, existing GNN-based solutions are hampered by high\nfalse positive rates and substantial resource consumption. In this paper, we\npresent a novel IDS designed to detect APTs using a Spatio-Temporal Graph\nNeural Network Autoencoder. Our approach leverages spatial information to\nunderstand the interactions between entities within a graph and temporal\ninformation to capture the evolution of the graph over time. This dual\nperspective is crucial for identifying the sequential stages of APTs.\nFurthermore, to address privacy and scalability concerns, we deploy our\narchitecture in a federated learning environment. This setup ensures that local\ndata remains on-premise while encrypted model-weights are shared and aggregated\nusing homomorphic encryption, maintaining data privacy and security. Our\nevaluation shows that this system effectively detects APTs with lower false\npositive rates and optimized resource usage compared to existing methods,\nhighlighting the potential of spatio-temporal analysis and federated learning\nin enhancing cybersecurity defenses.\n", "link": "http://arxiv.org/abs/2501.02981v1", "date": "2025-01-06", "relevancy": 2.4069, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4888}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CONTINUUM%3A%20Detecting%20APT%20Attacks%20through%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20CONTINUUM%3A%20Detecting%20APT%20Attacks%20through%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Atmane%20Ayoub%20Mansour%20Bahara%20and%20Kamel%20Soa%C3%AFd%20Ferrahia%20and%20Mohamed-Lamine%20Messai%20and%20Hamida%20Seba%20and%20Karima%20Amrouche%0AAbstract%3A%20%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20represent%20a%20significant%20challenge%20in%0Acybersecurity%20due%20to%20their%20sophisticated%20and%20stealthy%20nature.%20Traditional%0AIntrusion%20Detection%20Systems%20%28IDS%29%20often%20fall%20short%20in%20detecting%20these%0Amulti-stage%20attacks.%20Recently%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20been%20employed%0Ato%20enhance%20IDS%20capabilities%20by%20analyzing%20the%20complex%20relationships%20within%0Anetworked%20data.%20However%2C%20existing%20GNN-based%20solutions%20are%20hampered%20by%20high%0Afalse%20positive%20rates%20and%20substantial%20resource%20consumption.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20IDS%20designed%20to%20detect%20APTs%20using%20a%20Spatio-Temporal%20Graph%0ANeural%20Network%20Autoencoder.%20Our%20approach%20leverages%20spatial%20information%20to%0Aunderstand%20the%20interactions%20between%20entities%20within%20a%20graph%20and%20temporal%0Ainformation%20to%20capture%20the%20evolution%20of%20the%20graph%20over%20time.%20This%20dual%0Aperspective%20is%20crucial%20for%20identifying%20the%20sequential%20stages%20of%20APTs.%0AFurthermore%2C%20to%20address%20privacy%20and%20scalability%20concerns%2C%20we%20deploy%20our%0Aarchitecture%20in%20a%20federated%20learning%20environment.%20This%20setup%20ensures%20that%20local%0Adata%20remains%20on-premise%20while%20encrypted%20model-weights%20are%20shared%20and%20aggregated%0Ausing%20homomorphic%20encryption%2C%20maintaining%20data%20privacy%20and%20security.%20Our%0Aevaluation%20shows%20that%20this%20system%20effectively%20detects%20APTs%20with%20lower%20false%0Apositive%20rates%20and%20optimized%20resource%20usage%20compared%20to%20existing%20methods%2C%0Ahighlighting%20the%20potential%20of%20spatio-temporal%20analysis%20and%20federated%20learning%0Ain%20enhancing%20cybersecurity%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCONTINUUM%253A%2520Detecting%2520APT%2520Attacks%2520through%2520Spatial-Temporal%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DAtmane%2520Ayoub%2520Mansour%2520Bahara%2520and%2520Kamel%2520Soa%25C3%25AFd%2520Ferrahia%2520and%2520Mohamed-Lamine%2520Messai%2520and%2520Hamida%2520Seba%2520and%2520Karima%2520Amrouche%26entry.1292438233%3D%2520%2520Advanced%2520Persistent%2520Threats%2520%2528APTs%2529%2520represent%2520a%2520significant%2520challenge%2520in%250Acybersecurity%2520due%2520to%2520their%2520sophisticated%2520and%2520stealthy%2520nature.%2520Traditional%250AIntrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520often%2520fall%2520short%2520in%2520detecting%2520these%250Amulti-stage%2520attacks.%2520Recently%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520been%2520employed%250Ato%2520enhance%2520IDS%2520capabilities%2520by%2520analyzing%2520the%2520complex%2520relationships%2520within%250Anetworked%2520data.%2520However%252C%2520existing%2520GNN-based%2520solutions%2520are%2520hampered%2520by%2520high%250Afalse%2520positive%2520rates%2520and%2520substantial%2520resource%2520consumption.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520IDS%2520designed%2520to%2520detect%2520APTs%2520using%2520a%2520Spatio-Temporal%2520Graph%250ANeural%2520Network%2520Autoencoder.%2520Our%2520approach%2520leverages%2520spatial%2520information%2520to%250Aunderstand%2520the%2520interactions%2520between%2520entities%2520within%2520a%2520graph%2520and%2520temporal%250Ainformation%2520to%2520capture%2520the%2520evolution%2520of%2520the%2520graph%2520over%2520time.%2520This%2520dual%250Aperspective%2520is%2520crucial%2520for%2520identifying%2520the%2520sequential%2520stages%2520of%2520APTs.%250AFurthermore%252C%2520to%2520address%2520privacy%2520and%2520scalability%2520concerns%252C%2520we%2520deploy%2520our%250Aarchitecture%2520in%2520a%2520federated%2520learning%2520environment.%2520This%2520setup%2520ensures%2520that%2520local%250Adata%2520remains%2520on-premise%2520while%2520encrypted%2520model-weights%2520are%2520shared%2520and%2520aggregated%250Ausing%2520homomorphic%2520encryption%252C%2520maintaining%2520data%2520privacy%2520and%2520security.%2520Our%250Aevaluation%2520shows%2520that%2520this%2520system%2520effectively%2520detects%2520APTs%2520with%2520lower%2520false%250Apositive%2520rates%2520and%2520optimized%2520resource%2520usage%2520compared%2520to%2520existing%2520methods%252C%250Ahighlighting%2520the%2520potential%2520of%2520spatio-temporal%2520analysis%2520and%2520federated%2520learning%250Ain%2520enhancing%2520cybersecurity%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CONTINUUM%3A%20Detecting%20APT%20Attacks%20through%20Spatial-Temporal%20Graph%20Neural%0A%20%20Networks&entry.906535625=Atmane%20Ayoub%20Mansour%20Bahara%20and%20Kamel%20Soa%C3%AFd%20Ferrahia%20and%20Mohamed-Lamine%20Messai%20and%20Hamida%20Seba%20and%20Karima%20Amrouche&entry.1292438233=%20%20Advanced%20Persistent%20Threats%20%28APTs%29%20represent%20a%20significant%20challenge%20in%0Acybersecurity%20due%20to%20their%20sophisticated%20and%20stealthy%20nature.%20Traditional%0AIntrusion%20Detection%20Systems%20%28IDS%29%20often%20fall%20short%20in%20detecting%20these%0Amulti-stage%20attacks.%20Recently%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20been%20employed%0Ato%20enhance%20IDS%20capabilities%20by%20analyzing%20the%20complex%20relationships%20within%0Anetworked%20data.%20However%2C%20existing%20GNN-based%20solutions%20are%20hampered%20by%20high%0Afalse%20positive%20rates%20and%20substantial%20resource%20consumption.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20IDS%20designed%20to%20detect%20APTs%20using%20a%20Spatio-Temporal%20Graph%0ANeural%20Network%20Autoencoder.%20Our%20approach%20leverages%20spatial%20information%20to%0Aunderstand%20the%20interactions%20between%20entities%20within%20a%20graph%20and%20temporal%0Ainformation%20to%20capture%20the%20evolution%20of%20the%20graph%20over%20time.%20This%20dual%0Aperspective%20is%20crucial%20for%20identifying%20the%20sequential%20stages%20of%20APTs.%0AFurthermore%2C%20to%20address%20privacy%20and%20scalability%20concerns%2C%20we%20deploy%20our%0Aarchitecture%20in%20a%20federated%20learning%20environment.%20This%20setup%20ensures%20that%20local%0Adata%20remains%20on-premise%20while%20encrypted%20model-weights%20are%20shared%20and%20aggregated%0Ausing%20homomorphic%20encryption%2C%20maintaining%20data%20privacy%20and%20security.%20Our%0Aevaluation%20shows%20that%20this%20system%20effectively%20detects%20APTs%20with%20lower%20false%0Apositive%20rates%20and%20optimized%20resource%20usage%20compared%20to%20existing%20methods%2C%0Ahighlighting%20the%20potential%20of%20spatio-temporal%20analysis%20and%20federated%20learning%0Ain%20enhancing%20cybersecurity%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02981v1&entry.124074799=Read"},
{"title": "GLiREL -- Generalist Model for Zero-Shot Relation Extraction", "author": "Jack Boylan and Chris Hokamp and Demian Gholipour Ghalandari", "abstract": "  We introduce GLiREL (Generalist Lightweight model for zero-shot Relation\nExtraction), an efficient architecture and training paradigm for zero-shot\nrelation classification. Inspired by recent advancements in zero-shot named\nentity recognition, this work presents an approach to efficiently and\naccurately predict zero-shot relationship labels between multiple entities in a\nsingle forward pass. Experiments using the FewRel and WikiZSL benchmarks\ndemonstrate that our approach achieves state-of-the-art results on the\nzero-shot relation classification task. In addition, we contribute a protocol\nfor synthetically-generating datasets with diverse relation labels.\n", "link": "http://arxiv.org/abs/2501.03172v1", "date": "2025-01-06", "relevancy": 2.3989, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5167}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLiREL%20--%20Generalist%20Model%20for%20Zero-Shot%20Relation%20Extraction&body=Title%3A%20GLiREL%20--%20Generalist%20Model%20for%20Zero-Shot%20Relation%20Extraction%0AAuthor%3A%20Jack%20Boylan%20and%20Chris%20Hokamp%20and%20Demian%20Gholipour%20Ghalandari%0AAbstract%3A%20%20%20We%20introduce%20GLiREL%20%28Generalist%20Lightweight%20model%20for%20zero-shot%20Relation%0AExtraction%29%2C%20an%20efficient%20architecture%20and%20training%20paradigm%20for%20zero-shot%0Arelation%20classification.%20Inspired%20by%20recent%20advancements%20in%20zero-shot%20named%0Aentity%20recognition%2C%20this%20work%20presents%20an%20approach%20to%20efficiently%20and%0Aaccurately%20predict%20zero-shot%20relationship%20labels%20between%20multiple%20entities%20in%20a%0Asingle%20forward%20pass.%20Experiments%20using%20the%20FewRel%20and%20WikiZSL%20benchmarks%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20the%0Azero-shot%20relation%20classification%20task.%20In%20addition%2C%20we%20contribute%20a%20protocol%0Afor%20synthetically-generating%20datasets%20with%20diverse%20relation%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLiREL%2520--%2520Generalist%2520Model%2520for%2520Zero-Shot%2520Relation%2520Extraction%26entry.906535625%3DJack%2520Boylan%2520and%2520Chris%2520Hokamp%2520and%2520Demian%2520Gholipour%2520Ghalandari%26entry.1292438233%3D%2520%2520We%2520introduce%2520GLiREL%2520%2528Generalist%2520Lightweight%2520model%2520for%2520zero-shot%2520Relation%250AExtraction%2529%252C%2520an%2520efficient%2520architecture%2520and%2520training%2520paradigm%2520for%2520zero-shot%250Arelation%2520classification.%2520Inspired%2520by%2520recent%2520advancements%2520in%2520zero-shot%2520named%250Aentity%2520recognition%252C%2520this%2520work%2520presents%2520an%2520approach%2520to%2520efficiently%2520and%250Aaccurately%2520predict%2520zero-shot%2520relationship%2520labels%2520between%2520multiple%2520entities%2520in%2520a%250Asingle%2520forward%2520pass.%2520Experiments%2520using%2520the%2520FewRel%2520and%2520WikiZSL%2520benchmarks%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520the%250Azero-shot%2520relation%2520classification%2520task.%2520In%2520addition%252C%2520we%2520contribute%2520a%2520protocol%250Afor%2520synthetically-generating%2520datasets%2520with%2520diverse%2520relation%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLiREL%20--%20Generalist%20Model%20for%20Zero-Shot%20Relation%20Extraction&entry.906535625=Jack%20Boylan%20and%20Chris%20Hokamp%20and%20Demian%20Gholipour%20Ghalandari&entry.1292438233=%20%20We%20introduce%20GLiREL%20%28Generalist%20Lightweight%20model%20for%20zero-shot%20Relation%0AExtraction%29%2C%20an%20efficient%20architecture%20and%20training%20paradigm%20for%20zero-shot%0Arelation%20classification.%20Inspired%20by%20recent%20advancements%20in%20zero-shot%20named%0Aentity%20recognition%2C%20this%20work%20presents%20an%20approach%20to%20efficiently%20and%0Aaccurately%20predict%20zero-shot%20relationship%20labels%20between%20multiple%20entities%20in%20a%0Asingle%20forward%20pass.%20Experiments%20using%20the%20FewRel%20and%20WikiZSL%20benchmarks%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20the%0Azero-shot%20relation%20classification%20task.%20In%20addition%2C%20we%20contribute%20a%20protocol%0Afor%20synthetically-generating%20datasets%20with%20diverse%20relation%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03172v1&entry.124074799=Read"},
{"title": "Segment Anything Model for Zero-shot Single Particle Tracking in Liquid\n  Phase Transmission Electron Microscopy", "author": "Risha Goel and Zain Shabeeb and Isabel Panicker and Vida Jamali", "abstract": "  Liquid phase transmission electron microscopy (LPTEM) offers an unparalleled\ncombination of spatial and temporal resolution, making it a promising tool for\nsingle particle tracking at the nanoscale. However, the absence of a\nstandardized framework for identifying and tracking nanoparticles in noisy\nLPTEM videos has impeded progress in the field to develop this technique as a\nsingle particle tracking tool. To address this, we leveraged Segment Anything\nModel 2 (SAM 2), released by Meta, which is a foundation model developed for\nsegmenting videos and images. Here, we demonstrate that SAM 2 can successfully\nsegment LPTEM videos in a zero-shot manner and without requiring fine-tuning.\nBuilding on this capability, we introduce SAM4EM, a comprehensive framework\nthat integrates promptable video segmentation with particle tracking and\nstatistical analysis, providing an end-to-end LPTEM analysis framework for\nsingle particle tracking. SAM4EM achieves nearly 50-fold higher accuracy in\nsegmenting and analyzing LPTEM videos compared to state-of-the-art methods,\npaving the way for broader applications of LPTEM in nanoscale imaging.\n", "link": "http://arxiv.org/abs/2501.03153v1", "date": "2025-01-06", "relevancy": 2.3983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4777}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Anything%20Model%20for%20Zero-shot%20Single%20Particle%20Tracking%20in%20Liquid%0A%20%20Phase%20Transmission%20Electron%20Microscopy&body=Title%3A%20Segment%20Anything%20Model%20for%20Zero-shot%20Single%20Particle%20Tracking%20in%20Liquid%0A%20%20Phase%20Transmission%20Electron%20Microscopy%0AAuthor%3A%20Risha%20Goel%20and%20Zain%20Shabeeb%20and%20Isabel%20Panicker%20and%20Vida%20Jamali%0AAbstract%3A%20%20%20Liquid%20phase%20transmission%20electron%20microscopy%20%28LPTEM%29%20offers%20an%20unparalleled%0Acombination%20of%20spatial%20and%20temporal%20resolution%2C%20making%20it%20a%20promising%20tool%20for%0Asingle%20particle%20tracking%20at%20the%20nanoscale.%20However%2C%20the%20absence%20of%20a%0Astandardized%20framework%20for%20identifying%20and%20tracking%20nanoparticles%20in%20noisy%0ALPTEM%20videos%20has%20impeded%20progress%20in%20the%20field%20to%20develop%20this%20technique%20as%20a%0Asingle%20particle%20tracking%20tool.%20To%20address%20this%2C%20we%20leveraged%20Segment%20Anything%0AModel%202%20%28SAM%202%29%2C%20released%20by%20Meta%2C%20which%20is%20a%20foundation%20model%20developed%20for%0Asegmenting%20videos%20and%20images.%20Here%2C%20we%20demonstrate%20that%20SAM%202%20can%20successfully%0Asegment%20LPTEM%20videos%20in%20a%20zero-shot%20manner%20and%20without%20requiring%20fine-tuning.%0ABuilding%20on%20this%20capability%2C%20we%20introduce%20SAM4EM%2C%20a%20comprehensive%20framework%0Athat%20integrates%20promptable%20video%20segmentation%20with%20particle%20tracking%20and%0Astatistical%20analysis%2C%20providing%20an%20end-to-end%20LPTEM%20analysis%20framework%20for%0Asingle%20particle%20tracking.%20SAM4EM%20achieves%20nearly%2050-fold%20higher%20accuracy%20in%0Asegmenting%20and%20analyzing%20LPTEM%20videos%20compared%20to%20state-of-the-art%20methods%2C%0Apaving%20the%20way%20for%20broader%20applications%20of%20LPTEM%20in%20nanoscale%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Anything%2520Model%2520for%2520Zero-shot%2520Single%2520Particle%2520Tracking%2520in%2520Liquid%250A%2520%2520Phase%2520Transmission%2520Electron%2520Microscopy%26entry.906535625%3DRisha%2520Goel%2520and%2520Zain%2520Shabeeb%2520and%2520Isabel%2520Panicker%2520and%2520Vida%2520Jamali%26entry.1292438233%3D%2520%2520Liquid%2520phase%2520transmission%2520electron%2520microscopy%2520%2528LPTEM%2529%2520offers%2520an%2520unparalleled%250Acombination%2520of%2520spatial%2520and%2520temporal%2520resolution%252C%2520making%2520it%2520a%2520promising%2520tool%2520for%250Asingle%2520particle%2520tracking%2520at%2520the%2520nanoscale.%2520However%252C%2520the%2520absence%2520of%2520a%250Astandardized%2520framework%2520for%2520identifying%2520and%2520tracking%2520nanoparticles%2520in%2520noisy%250ALPTEM%2520videos%2520has%2520impeded%2520progress%2520in%2520the%2520field%2520to%2520develop%2520this%2520technique%2520as%2520a%250Asingle%2520particle%2520tracking%2520tool.%2520To%2520address%2520this%252C%2520we%2520leveraged%2520Segment%2520Anything%250AModel%25202%2520%2528SAM%25202%2529%252C%2520released%2520by%2520Meta%252C%2520which%2520is%2520a%2520foundation%2520model%2520developed%2520for%250Asegmenting%2520videos%2520and%2520images.%2520Here%252C%2520we%2520demonstrate%2520that%2520SAM%25202%2520can%2520successfully%250Asegment%2520LPTEM%2520videos%2520in%2520a%2520zero-shot%2520manner%2520and%2520without%2520requiring%2520fine-tuning.%250ABuilding%2520on%2520this%2520capability%252C%2520we%2520introduce%2520SAM4EM%252C%2520a%2520comprehensive%2520framework%250Athat%2520integrates%2520promptable%2520video%2520segmentation%2520with%2520particle%2520tracking%2520and%250Astatistical%2520analysis%252C%2520providing%2520an%2520end-to-end%2520LPTEM%2520analysis%2520framework%2520for%250Asingle%2520particle%2520tracking.%2520SAM4EM%2520achieves%2520nearly%252050-fold%2520higher%2520accuracy%2520in%250Asegmenting%2520and%2520analyzing%2520LPTEM%2520videos%2520compared%2520to%2520state-of-the-art%2520methods%252C%250Apaving%2520the%2520way%2520for%2520broader%2520applications%2520of%2520LPTEM%2520in%2520nanoscale%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Anything%20Model%20for%20Zero-shot%20Single%20Particle%20Tracking%20in%20Liquid%0A%20%20Phase%20Transmission%20Electron%20Microscopy&entry.906535625=Risha%20Goel%20and%20Zain%20Shabeeb%20and%20Isabel%20Panicker%20and%20Vida%20Jamali&entry.1292438233=%20%20Liquid%20phase%20transmission%20electron%20microscopy%20%28LPTEM%29%20offers%20an%20unparalleled%0Acombination%20of%20spatial%20and%20temporal%20resolution%2C%20making%20it%20a%20promising%20tool%20for%0Asingle%20particle%20tracking%20at%20the%20nanoscale.%20However%2C%20the%20absence%20of%20a%0Astandardized%20framework%20for%20identifying%20and%20tracking%20nanoparticles%20in%20noisy%0ALPTEM%20videos%20has%20impeded%20progress%20in%20the%20field%20to%20develop%20this%20technique%20as%20a%0Asingle%20particle%20tracking%20tool.%20To%20address%20this%2C%20we%20leveraged%20Segment%20Anything%0AModel%202%20%28SAM%202%29%2C%20released%20by%20Meta%2C%20which%20is%20a%20foundation%20model%20developed%20for%0Asegmenting%20videos%20and%20images.%20Here%2C%20we%20demonstrate%20that%20SAM%202%20can%20successfully%0Asegment%20LPTEM%20videos%20in%20a%20zero-shot%20manner%20and%20without%20requiring%20fine-tuning.%0ABuilding%20on%20this%20capability%2C%20we%20introduce%20SAM4EM%2C%20a%20comprehensive%20framework%0Athat%20integrates%20promptable%20video%20segmentation%20with%20particle%20tracking%20and%0Astatistical%20analysis%2C%20providing%20an%20end-to-end%20LPTEM%20analysis%20framework%20for%0Asingle%20particle%20tracking.%20SAM4EM%20achieves%20nearly%2050-fold%20higher%20accuracy%20in%0Asegmenting%20and%20analyzing%20LPTEM%20videos%20compared%20to%20state-of-the-art%20methods%2C%0Apaving%20the%20way%20for%20broader%20applications%20of%20LPTEM%20in%20nanoscale%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03153v1&entry.124074799=Read"},
{"title": "SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild", "author": "Jiawei Liu and Yuanzhi Zhu and Feiyu Gao and Zhibo Yang and Peng Wang and Junyang Lin and Xinggang Wang and Wenyu Liu", "abstract": "  Generating visual text in natural scene images is a challenging task with\nmany unsolved problems. Different from generating text on artificially designed\nimages (such as posters, covers, cartoons, etc.), the text in natural scene\nimages needs to meet the following four key criteria: (1) Fidelity: the\ngenerated text should appear as realistic as a photograph and be completely\naccurate, with no errors in any of the strokes. (2) Reasonability: the text\nshould be generated on reasonable carrier areas (such as boards, signs, walls,\netc.), and the generated text content should also be relevant to the scene. (3)\nUtility: the generated text can facilitate to the training of natural scene OCR\n(Optical Character Recognition) tasks. (4) Controllability: The attribute of\nthe text (such as font and color) should be controllable as needed.In this\npaper, we propose a two stage method, SceneVTG++, which simultaneously\nsatisfies the four aspects mentioned above. SceneVTG++ consists of a Text\nLayout and Content Generator (TLCG) and a Controllable Local Text Diffusion\n(CLTD). The former utilizes the world knowledge of multi modal large language\nmodels to find reasonable text areas and recommend text content according to\nthe nature scene background images, while the latter generates controllable\nmultilingual text based on the diffusion model. Through extensive experiments,\nwe respectively verified the effectiveness of TLCG and CLTD, and demonstrated\nthe state-of-the-art text generation performance of SceneVTG++. In addition,\nthe generated images have superior utility in OCR tasks like text detection and\ntext recognition. Codes and datasets will be available.\n", "link": "http://arxiv.org/abs/2501.02962v1", "date": "2025-01-06", "relevancy": 2.3912, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6234}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5825}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneVTG%2B%2B%3A%20Controllable%20Multilingual%20Visual%20Text%20Generation%20in%20the%20Wild&body=Title%3A%20SceneVTG%2B%2B%3A%20Controllable%20Multilingual%20Visual%20Text%20Generation%20in%20the%20Wild%0AAuthor%3A%20Jiawei%20Liu%20and%20Yuanzhi%20Zhu%20and%20Feiyu%20Gao%20and%20Zhibo%20Yang%20and%20Peng%20Wang%20and%20Junyang%20Lin%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu%0AAbstract%3A%20%20%20Generating%20visual%20text%20in%20natural%20scene%20images%20is%20a%20challenging%20task%20with%0Amany%20unsolved%20problems.%20Different%20from%20generating%20text%20on%20artificially%20designed%0Aimages%20%28such%20as%20posters%2C%20covers%2C%20cartoons%2C%20etc.%29%2C%20the%20text%20in%20natural%20scene%0Aimages%20needs%20to%20meet%20the%20following%20four%20key%20criteria%3A%20%281%29%20Fidelity%3A%20the%0Agenerated%20text%20should%20appear%20as%20realistic%20as%20a%20photograph%20and%20be%20completely%0Aaccurate%2C%20with%20no%20errors%20in%20any%20of%20the%20strokes.%20%282%29%20Reasonability%3A%20the%20text%0Ashould%20be%20generated%20on%20reasonable%20carrier%20areas%20%28such%20as%20boards%2C%20signs%2C%20walls%2C%0Aetc.%29%2C%20and%20the%20generated%20text%20content%20should%20also%20be%20relevant%20to%20the%20scene.%20%283%29%0AUtility%3A%20the%20generated%20text%20can%20facilitate%20to%20the%20training%20of%20natural%20scene%20OCR%0A%28Optical%20Character%20Recognition%29%20tasks.%20%284%29%20Controllability%3A%20The%20attribute%20of%0Athe%20text%20%28such%20as%20font%20and%20color%29%20should%20be%20controllable%20as%20needed.In%20this%0Apaper%2C%20we%20propose%20a%20two%20stage%20method%2C%20SceneVTG%2B%2B%2C%20which%20simultaneously%0Asatisfies%20the%20four%20aspects%20mentioned%20above.%20SceneVTG%2B%2B%20consists%20of%20a%20Text%0ALayout%20and%20Content%20Generator%20%28TLCG%29%20and%20a%20Controllable%20Local%20Text%20Diffusion%0A%28CLTD%29.%20The%20former%20utilizes%20the%20world%20knowledge%20of%20multi%20modal%20large%20language%0Amodels%20to%20find%20reasonable%20text%20areas%20and%20recommend%20text%20content%20according%20to%0Athe%20nature%20scene%20background%20images%2C%20while%20the%20latter%20generates%20controllable%0Amultilingual%20text%20based%20on%20the%20diffusion%20model.%20Through%20extensive%20experiments%2C%0Awe%20respectively%20verified%20the%20effectiveness%20of%20TLCG%20and%20CLTD%2C%20and%20demonstrated%0Athe%20state-of-the-art%20text%20generation%20performance%20of%20SceneVTG%2B%2B.%20In%20addition%2C%0Athe%20generated%20images%20have%20superior%20utility%20in%20OCR%20tasks%20like%20text%20detection%20and%0Atext%20recognition.%20Codes%20and%20datasets%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneVTG%252B%252B%253A%2520Controllable%2520Multilingual%2520Visual%2520Text%2520Generation%2520in%2520the%2520Wild%26entry.906535625%3DJiawei%2520Liu%2520and%2520Yuanzhi%2520Zhu%2520and%2520Feiyu%2520Gao%2520and%2520Zhibo%2520Yang%2520and%2520Peng%2520Wang%2520and%2520Junyang%2520Lin%2520and%2520Xinggang%2520Wang%2520and%2520Wenyu%2520Liu%26entry.1292438233%3D%2520%2520Generating%2520visual%2520text%2520in%2520natural%2520scene%2520images%2520is%2520a%2520challenging%2520task%2520with%250Amany%2520unsolved%2520problems.%2520Different%2520from%2520generating%2520text%2520on%2520artificially%2520designed%250Aimages%2520%2528such%2520as%2520posters%252C%2520covers%252C%2520cartoons%252C%2520etc.%2529%252C%2520the%2520text%2520in%2520natural%2520scene%250Aimages%2520needs%2520to%2520meet%2520the%2520following%2520four%2520key%2520criteria%253A%2520%25281%2529%2520Fidelity%253A%2520the%250Agenerated%2520text%2520should%2520appear%2520as%2520realistic%2520as%2520a%2520photograph%2520and%2520be%2520completely%250Aaccurate%252C%2520with%2520no%2520errors%2520in%2520any%2520of%2520the%2520strokes.%2520%25282%2529%2520Reasonability%253A%2520the%2520text%250Ashould%2520be%2520generated%2520on%2520reasonable%2520carrier%2520areas%2520%2528such%2520as%2520boards%252C%2520signs%252C%2520walls%252C%250Aetc.%2529%252C%2520and%2520the%2520generated%2520text%2520content%2520should%2520also%2520be%2520relevant%2520to%2520the%2520scene.%2520%25283%2529%250AUtility%253A%2520the%2520generated%2520text%2520can%2520facilitate%2520to%2520the%2520training%2520of%2520natural%2520scene%2520OCR%250A%2528Optical%2520Character%2520Recognition%2529%2520tasks.%2520%25284%2529%2520Controllability%253A%2520The%2520attribute%2520of%250Athe%2520text%2520%2528such%2520as%2520font%2520and%2520color%2529%2520should%2520be%2520controllable%2520as%2520needed.In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520two%2520stage%2520method%252C%2520SceneVTG%252B%252B%252C%2520which%2520simultaneously%250Asatisfies%2520the%2520four%2520aspects%2520mentioned%2520above.%2520SceneVTG%252B%252B%2520consists%2520of%2520a%2520Text%250ALayout%2520and%2520Content%2520Generator%2520%2528TLCG%2529%2520and%2520a%2520Controllable%2520Local%2520Text%2520Diffusion%250A%2528CLTD%2529.%2520The%2520former%2520utilizes%2520the%2520world%2520knowledge%2520of%2520multi%2520modal%2520large%2520language%250Amodels%2520to%2520find%2520reasonable%2520text%2520areas%2520and%2520recommend%2520text%2520content%2520according%2520to%250Athe%2520nature%2520scene%2520background%2520images%252C%2520while%2520the%2520latter%2520generates%2520controllable%250Amultilingual%2520text%2520based%2520on%2520the%2520diffusion%2520model.%2520Through%2520extensive%2520experiments%252C%250Awe%2520respectively%2520verified%2520the%2520effectiveness%2520of%2520TLCG%2520and%2520CLTD%252C%2520and%2520demonstrated%250Athe%2520state-of-the-art%2520text%2520generation%2520performance%2520of%2520SceneVTG%252B%252B.%2520In%2520addition%252C%250Athe%2520generated%2520images%2520have%2520superior%2520utility%2520in%2520OCR%2520tasks%2520like%2520text%2520detection%2520and%250Atext%2520recognition.%2520Codes%2520and%2520datasets%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneVTG%2B%2B%3A%20Controllable%20Multilingual%20Visual%20Text%20Generation%20in%20the%20Wild&entry.906535625=Jiawei%20Liu%20and%20Yuanzhi%20Zhu%20and%20Feiyu%20Gao%20and%20Zhibo%20Yang%20and%20Peng%20Wang%20and%20Junyang%20Lin%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu&entry.1292438233=%20%20Generating%20visual%20text%20in%20natural%20scene%20images%20is%20a%20challenging%20task%20with%0Amany%20unsolved%20problems.%20Different%20from%20generating%20text%20on%20artificially%20designed%0Aimages%20%28such%20as%20posters%2C%20covers%2C%20cartoons%2C%20etc.%29%2C%20the%20text%20in%20natural%20scene%0Aimages%20needs%20to%20meet%20the%20following%20four%20key%20criteria%3A%20%281%29%20Fidelity%3A%20the%0Agenerated%20text%20should%20appear%20as%20realistic%20as%20a%20photograph%20and%20be%20completely%0Aaccurate%2C%20with%20no%20errors%20in%20any%20of%20the%20strokes.%20%282%29%20Reasonability%3A%20the%20text%0Ashould%20be%20generated%20on%20reasonable%20carrier%20areas%20%28such%20as%20boards%2C%20signs%2C%20walls%2C%0Aetc.%29%2C%20and%20the%20generated%20text%20content%20should%20also%20be%20relevant%20to%20the%20scene.%20%283%29%0AUtility%3A%20the%20generated%20text%20can%20facilitate%20to%20the%20training%20of%20natural%20scene%20OCR%0A%28Optical%20Character%20Recognition%29%20tasks.%20%284%29%20Controllability%3A%20The%20attribute%20of%0Athe%20text%20%28such%20as%20font%20and%20color%29%20should%20be%20controllable%20as%20needed.In%20this%0Apaper%2C%20we%20propose%20a%20two%20stage%20method%2C%20SceneVTG%2B%2B%2C%20which%20simultaneously%0Asatisfies%20the%20four%20aspects%20mentioned%20above.%20SceneVTG%2B%2B%20consists%20of%20a%20Text%0ALayout%20and%20Content%20Generator%20%28TLCG%29%20and%20a%20Controllable%20Local%20Text%20Diffusion%0A%28CLTD%29.%20The%20former%20utilizes%20the%20world%20knowledge%20of%20multi%20modal%20large%20language%0Amodels%20to%20find%20reasonable%20text%20areas%20and%20recommend%20text%20content%20according%20to%0Athe%20nature%20scene%20background%20images%2C%20while%20the%20latter%20generates%20controllable%0Amultilingual%20text%20based%20on%20the%20diffusion%20model.%20Through%20extensive%20experiments%2C%0Awe%20respectively%20verified%20the%20effectiveness%20of%20TLCG%20and%20CLTD%2C%20and%20demonstrated%0Athe%20state-of-the-art%20text%20generation%20performance%20of%20SceneVTG%2B%2B.%20In%20addition%2C%0Athe%20generated%20images%20have%20superior%20utility%20in%20OCR%20tasks%20like%20text%20detection%20and%0Atext%20recognition.%20Codes%20and%20datasets%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02962v1&entry.124074799=Read"},
{"title": "Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis", "author": "Thang-Anh-Quan Nguyen and Nathan Piasco and Luis Rold\u00e3o and Moussab Bennehar and Dzmitry Tsishkou and Laurent Caraffa and Jean-Philippe Tarel and Roland Br\u00e9mond", "abstract": "  In this paper, we present PointmapDiffusion, a novel framework for\nsingle-image novel view synthesis (NVS) that utilizes pre-trained 2D diffusion\nmodels. Our method is the first to leverage pointmaps (i.e. rasterized 3D scene\ncoordinates) as a conditioning signal, capturing geometric prior from the\nreference images to guide the diffusion process. By embedding reference\nattention blocks and a ControlNet for pointmap features, our model balances\nbetween generative capability and geometric consistency, enabling accurate view\nsynthesis across varying viewpoints. Extensive experiments on diverse\nreal-world datasets demonstrate that PointmapDiffusion achieves high-quality,\nmulti-view consistent results with significantly fewer trainable parameters\ncompared to other baselines for single-image NVS tasks.\n", "link": "http://arxiv.org/abs/2501.02913v1", "date": "2025-01-06", "relevancy": 2.3871, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6338}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5894}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointmap-Conditioned%20Diffusion%20for%20Consistent%20Novel%20View%20Synthesis&body=Title%3A%20Pointmap-Conditioned%20Diffusion%20for%20Consistent%20Novel%20View%20Synthesis%0AAuthor%3A%20Thang-Anh-Quan%20Nguyen%20and%20Nathan%20Piasco%20and%20Luis%20Rold%C3%A3o%20and%20Moussab%20Bennehar%20and%20Dzmitry%20Tsishkou%20and%20Laurent%20Caraffa%20and%20Jean-Philippe%20Tarel%20and%20Roland%20Br%C3%A9mond%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20PointmapDiffusion%2C%20a%20novel%20framework%20for%0Asingle-image%20novel%20view%20synthesis%20%28NVS%29%20that%20utilizes%20pre-trained%202D%20diffusion%0Amodels.%20Our%20method%20is%20the%20first%20to%20leverage%20pointmaps%20%28i.e.%20rasterized%203D%20scene%0Acoordinates%29%20as%20a%20conditioning%20signal%2C%20capturing%20geometric%20prior%20from%20the%0Areference%20images%20to%20guide%20the%20diffusion%20process.%20By%20embedding%20reference%0Aattention%20blocks%20and%20a%20ControlNet%20for%20pointmap%20features%2C%20our%20model%20balances%0Abetween%20generative%20capability%20and%20geometric%20consistency%2C%20enabling%20accurate%20view%0Asynthesis%20across%20varying%20viewpoints.%20Extensive%20experiments%20on%20diverse%0Areal-world%20datasets%20demonstrate%20that%20PointmapDiffusion%20achieves%20high-quality%2C%0Amulti-view%20consistent%20results%20with%20significantly%20fewer%20trainable%20parameters%0Acompared%20to%20other%20baselines%20for%20single-image%20NVS%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointmap-Conditioned%2520Diffusion%2520for%2520Consistent%2520Novel%2520View%2520Synthesis%26entry.906535625%3DThang-Anh-Quan%2520Nguyen%2520and%2520Nathan%2520Piasco%2520and%2520Luis%2520Rold%25C3%25A3o%2520and%2520Moussab%2520Bennehar%2520and%2520Dzmitry%2520Tsishkou%2520and%2520Laurent%2520Caraffa%2520and%2520Jean-Philippe%2520Tarel%2520and%2520Roland%2520Br%25C3%25A9mond%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520PointmapDiffusion%252C%2520a%2520novel%2520framework%2520for%250Asingle-image%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520that%2520utilizes%2520pre-trained%25202D%2520diffusion%250Amodels.%2520Our%2520method%2520is%2520the%2520first%2520to%2520leverage%2520pointmaps%2520%2528i.e.%2520rasterized%25203D%2520scene%250Acoordinates%2529%2520as%2520a%2520conditioning%2520signal%252C%2520capturing%2520geometric%2520prior%2520from%2520the%250Areference%2520images%2520to%2520guide%2520the%2520diffusion%2520process.%2520By%2520embedding%2520reference%250Aattention%2520blocks%2520and%2520a%2520ControlNet%2520for%2520pointmap%2520features%252C%2520our%2520model%2520balances%250Abetween%2520generative%2520capability%2520and%2520geometric%2520consistency%252C%2520enabling%2520accurate%2520view%250Asynthesis%2520across%2520varying%2520viewpoints.%2520Extensive%2520experiments%2520on%2520diverse%250Areal-world%2520datasets%2520demonstrate%2520that%2520PointmapDiffusion%2520achieves%2520high-quality%252C%250Amulti-view%2520consistent%2520results%2520with%2520significantly%2520fewer%2520trainable%2520parameters%250Acompared%2520to%2520other%2520baselines%2520for%2520single-image%2520NVS%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointmap-Conditioned%20Diffusion%20for%20Consistent%20Novel%20View%20Synthesis&entry.906535625=Thang-Anh-Quan%20Nguyen%20and%20Nathan%20Piasco%20and%20Luis%20Rold%C3%A3o%20and%20Moussab%20Bennehar%20and%20Dzmitry%20Tsishkou%20and%20Laurent%20Caraffa%20and%20Jean-Philippe%20Tarel%20and%20Roland%20Br%C3%A9mond&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20PointmapDiffusion%2C%20a%20novel%20framework%20for%0Asingle-image%20novel%20view%20synthesis%20%28NVS%29%20that%20utilizes%20pre-trained%202D%20diffusion%0Amodels.%20Our%20method%20is%20the%20first%20to%20leverage%20pointmaps%20%28i.e.%20rasterized%203D%20scene%0Acoordinates%29%20as%20a%20conditioning%20signal%2C%20capturing%20geometric%20prior%20from%20the%0Areference%20images%20to%20guide%20the%20diffusion%20process.%20By%20embedding%20reference%0Aattention%20blocks%20and%20a%20ControlNet%20for%20pointmap%20features%2C%20our%20model%20balances%0Abetween%20generative%20capability%20and%20geometric%20consistency%2C%20enabling%20accurate%20view%0Asynthesis%20across%20varying%20viewpoints.%20Extensive%20experiments%20on%20diverse%0Areal-world%20datasets%20demonstrate%20that%20PointmapDiffusion%20achieves%20high-quality%2C%0Amulti-view%20consistent%20results%20with%20significantly%20fewer%20trainable%20parameters%0Acompared%20to%20other%20baselines%20for%20single-image%20NVS%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02913v1&entry.124074799=Read"},
{"title": "FoundPAD: Foundation Models Reloaded for Face Presentation Attack\n  Detection", "author": "Guray Ozgur and Eduarda Caldeira and Tahar Chettaoui and Fadi Boutros and Raghavendra Ramachandra and Naser Damer", "abstract": "  Although face recognition systems have seen a massive performance enhancement\nin recent years, they are still targeted by threats such as presentation\nattacks, leading to the need for generalizable presentation attack detection\n(PAD) algorithms. Current PAD solutions suffer from two main problems: low\ngeneralization to unknown cenarios and large training data requirements.\nFoundation models (FM) are pre-trained on extensive datasets, achieving\nremarkable results when generalizing to unseen domains and allowing for\nefficient task-specific adaption even when little training data are available.\nIn this work, we recognize the potential of FMs to address common PAD problems\nand tackle the PAD task with an adapted FM for the first time. The FM under\nconsideration is adapted with LoRA weights while simultaneously training a\nclassification header. The resultant architecture, FoundPAD, is highly\ngeneralizable to unseen domains, achieving competitive results in several\nsettings under different data availability scenarios and even when using\nsynthetic training data. To encourage reproducibility and facilitate further\nresearch in PAD, we publicly release the implementation of FoundPAD at\nhttps://github.com/gurayozgur/FoundPAD .\n", "link": "http://arxiv.org/abs/2501.02892v1", "date": "2025-01-06", "relevancy": 2.382, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4693}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoundPAD%3A%20Foundation%20Models%20Reloaded%20for%20Face%20Presentation%20Attack%0A%20%20Detection&body=Title%3A%20FoundPAD%3A%20Foundation%20Models%20Reloaded%20for%20Face%20Presentation%20Attack%0A%20%20Detection%0AAuthor%3A%20Guray%20Ozgur%20and%20Eduarda%20Caldeira%20and%20Tahar%20Chettaoui%20and%20Fadi%20Boutros%20and%20Raghavendra%20Ramachandra%20and%20Naser%20Damer%0AAbstract%3A%20%20%20Although%20face%20recognition%20systems%20have%20seen%20a%20massive%20performance%20enhancement%0Ain%20recent%20years%2C%20they%20are%20still%20targeted%20by%20threats%20such%20as%20presentation%0Aattacks%2C%20leading%20to%20the%20need%20for%20generalizable%20presentation%20attack%20detection%0A%28PAD%29%20algorithms.%20Current%20PAD%20solutions%20suffer%20from%20two%20main%20problems%3A%20low%0Ageneralization%20to%20unknown%20cenarios%20and%20large%20training%20data%20requirements.%0AFoundation%20models%20%28FM%29%20are%20pre-trained%20on%20extensive%20datasets%2C%20achieving%0Aremarkable%20results%20when%20generalizing%20to%20unseen%20domains%20and%20allowing%20for%0Aefficient%20task-specific%20adaption%20even%20when%20little%20training%20data%20are%20available.%0AIn%20this%20work%2C%20we%20recognize%20the%20potential%20of%20FMs%20to%20address%20common%20PAD%20problems%0Aand%20tackle%20the%20PAD%20task%20with%20an%20adapted%20FM%20for%20the%20first%20time.%20The%20FM%20under%0Aconsideration%20is%20adapted%20with%20LoRA%20weights%20while%20simultaneously%20training%20a%0Aclassification%20header.%20The%20resultant%20architecture%2C%20FoundPAD%2C%20is%20highly%0Ageneralizable%20to%20unseen%20domains%2C%20achieving%20competitive%20results%20in%20several%0Asettings%20under%20different%20data%20availability%20scenarios%20and%20even%20when%20using%0Asynthetic%20training%20data.%20To%20encourage%20reproducibility%20and%20facilitate%20further%0Aresearch%20in%20PAD%2C%20we%20publicly%20release%20the%20implementation%20of%20FoundPAD%20at%0Ahttps%3A//github.com/gurayozgur/FoundPAD%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundPAD%253A%2520Foundation%2520Models%2520Reloaded%2520for%2520Face%2520Presentation%2520Attack%250A%2520%2520Detection%26entry.906535625%3DGuray%2520Ozgur%2520and%2520Eduarda%2520Caldeira%2520and%2520Tahar%2520Chettaoui%2520and%2520Fadi%2520Boutros%2520and%2520Raghavendra%2520Ramachandra%2520and%2520Naser%2520Damer%26entry.1292438233%3D%2520%2520Although%2520face%2520recognition%2520systems%2520have%2520seen%2520a%2520massive%2520performance%2520enhancement%250Ain%2520recent%2520years%252C%2520they%2520are%2520still%2520targeted%2520by%2520threats%2520such%2520as%2520presentation%250Aattacks%252C%2520leading%2520to%2520the%2520need%2520for%2520generalizable%2520presentation%2520attack%2520detection%250A%2528PAD%2529%2520algorithms.%2520Current%2520PAD%2520solutions%2520suffer%2520from%2520two%2520main%2520problems%253A%2520low%250Ageneralization%2520to%2520unknown%2520cenarios%2520and%2520large%2520training%2520data%2520requirements.%250AFoundation%2520models%2520%2528FM%2529%2520are%2520pre-trained%2520on%2520extensive%2520datasets%252C%2520achieving%250Aremarkable%2520results%2520when%2520generalizing%2520to%2520unseen%2520domains%2520and%2520allowing%2520for%250Aefficient%2520task-specific%2520adaption%2520even%2520when%2520little%2520training%2520data%2520are%2520available.%250AIn%2520this%2520work%252C%2520we%2520recognize%2520the%2520potential%2520of%2520FMs%2520to%2520address%2520common%2520PAD%2520problems%250Aand%2520tackle%2520the%2520PAD%2520task%2520with%2520an%2520adapted%2520FM%2520for%2520the%2520first%2520time.%2520The%2520FM%2520under%250Aconsideration%2520is%2520adapted%2520with%2520LoRA%2520weights%2520while%2520simultaneously%2520training%2520a%250Aclassification%2520header.%2520The%2520resultant%2520architecture%252C%2520FoundPAD%252C%2520is%2520highly%250Ageneralizable%2520to%2520unseen%2520domains%252C%2520achieving%2520competitive%2520results%2520in%2520several%250Asettings%2520under%2520different%2520data%2520availability%2520scenarios%2520and%2520even%2520when%2520using%250Asynthetic%2520training%2520data.%2520To%2520encourage%2520reproducibility%2520and%2520facilitate%2520further%250Aresearch%2520in%2520PAD%252C%2520we%2520publicly%2520release%2520the%2520implementation%2520of%2520FoundPAD%2520at%250Ahttps%253A//github.com/gurayozgur/FoundPAD%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoundPAD%3A%20Foundation%20Models%20Reloaded%20for%20Face%20Presentation%20Attack%0A%20%20Detection&entry.906535625=Guray%20Ozgur%20and%20Eduarda%20Caldeira%20and%20Tahar%20Chettaoui%20and%20Fadi%20Boutros%20and%20Raghavendra%20Ramachandra%20and%20Naser%20Damer&entry.1292438233=%20%20Although%20face%20recognition%20systems%20have%20seen%20a%20massive%20performance%20enhancement%0Ain%20recent%20years%2C%20they%20are%20still%20targeted%20by%20threats%20such%20as%20presentation%0Aattacks%2C%20leading%20to%20the%20need%20for%20generalizable%20presentation%20attack%20detection%0A%28PAD%29%20algorithms.%20Current%20PAD%20solutions%20suffer%20from%20two%20main%20problems%3A%20low%0Ageneralization%20to%20unknown%20cenarios%20and%20large%20training%20data%20requirements.%0AFoundation%20models%20%28FM%29%20are%20pre-trained%20on%20extensive%20datasets%2C%20achieving%0Aremarkable%20results%20when%20generalizing%20to%20unseen%20domains%20and%20allowing%20for%0Aefficient%20task-specific%20adaption%20even%20when%20little%20training%20data%20are%20available.%0AIn%20this%20work%2C%20we%20recognize%20the%20potential%20of%20FMs%20to%20address%20common%20PAD%20problems%0Aand%20tackle%20the%20PAD%20task%20with%20an%20adapted%20FM%20for%20the%20first%20time.%20The%20FM%20under%0Aconsideration%20is%20adapted%20with%20LoRA%20weights%20while%20simultaneously%20training%20a%0Aclassification%20header.%20The%20resultant%20architecture%2C%20FoundPAD%2C%20is%20highly%0Ageneralizable%20to%20unseen%20domains%2C%20achieving%20competitive%20results%20in%20several%0Asettings%20under%20different%20data%20availability%20scenarios%20and%20even%20when%20using%0Asynthetic%20training%20data.%20To%20encourage%20reproducibility%20and%20facilitate%20further%0Aresearch%20in%20PAD%2C%20we%20publicly%20release%20the%20implementation%20of%20FoundPAD%20at%0Ahttps%3A//github.com/gurayozgur/FoundPAD%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02892v1&entry.124074799=Read"},
{"title": "Comprehensive Pathological Image Segmentation via Teacher Aggregation\n  for Tumor Microenvironment Analysis", "author": "Daisuke Komura and Maki Takao and Mieko Ochi and Takumi Onoyama and Hiroto Katoh and Hiroyuki Abe and Hiroyuki Sano and Teppei Konishi and Toshio Kumasaka and Tomoyuki Yokose and Yohei Miyagi and Tetsuo Ushiku and Shumpei Ishikawa", "abstract": "  The tumor microenvironment (TME) plays a crucial role in cancer progression\nand treatment response, yet current methods for its comprehensive analysis in\nH&E-stained tissue slides face significant limitations in the diversity of\ntissue cell types and accuracy. Here, we present PAGET (Pathological image\nsegmentation via AGgrEgated Teachers), a new knowledge distillation approach\nthat integrates multiple segmentation models while considering the hierarchical\nnature of cell types in the TME. By leveraging a unique dataset created through\nimmunohistochemical restaining techniques and existing segmentation models,\nPAGET enables simultaneous identification and classification of 14 key TME\ncomponents. We demonstrate PAGET's ability to perform rapid, comprehensive TME\nsegmentation across various tissue types and medical institutions, advancing\nthe quantitative analysis of tumor microenvironments. This method represents a\nsignificant step forward in enhancing our understanding of cancer biology and\nsupporting precise clinical decision-making from large-scale histopathology\nimages.\n", "link": "http://arxiv.org/abs/2501.02909v1", "date": "2025-01-06", "relevancy": 2.3815, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4652}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Pathological%20Image%20Segmentation%20via%20Teacher%20Aggregation%0A%20%20for%20Tumor%20Microenvironment%20Analysis&body=Title%3A%20Comprehensive%20Pathological%20Image%20Segmentation%20via%20Teacher%20Aggregation%0A%20%20for%20Tumor%20Microenvironment%20Analysis%0AAuthor%3A%20Daisuke%20Komura%20and%20Maki%20Takao%20and%20Mieko%20Ochi%20and%20Takumi%20Onoyama%20and%20Hiroto%20Katoh%20and%20Hiroyuki%20Abe%20and%20Hiroyuki%20Sano%20and%20Teppei%20Konishi%20and%20Toshio%20Kumasaka%20and%20Tomoyuki%20Yokose%20and%20Yohei%20Miyagi%20and%20Tetsuo%20Ushiku%20and%20Shumpei%20Ishikawa%0AAbstract%3A%20%20%20The%20tumor%20microenvironment%20%28TME%29%20plays%20a%20crucial%20role%20in%20cancer%20progression%0Aand%20treatment%20response%2C%20yet%20current%20methods%20for%20its%20comprehensive%20analysis%20in%0AH%26E-stained%20tissue%20slides%20face%20significant%20limitations%20in%20the%20diversity%20of%0Atissue%20cell%20types%20and%20accuracy.%20Here%2C%20we%20present%20PAGET%20%28Pathological%20image%0Asegmentation%20via%20AGgrEgated%20Teachers%29%2C%20a%20new%20knowledge%20distillation%20approach%0Athat%20integrates%20multiple%20segmentation%20models%20while%20considering%20the%20hierarchical%0Anature%20of%20cell%20types%20in%20the%20TME.%20By%20leveraging%20a%20unique%20dataset%20created%20through%0Aimmunohistochemical%20restaining%20techniques%20and%20existing%20segmentation%20models%2C%0APAGET%20enables%20simultaneous%20identification%20and%20classification%20of%2014%20key%20TME%0Acomponents.%20We%20demonstrate%20PAGET%27s%20ability%20to%20perform%20rapid%2C%20comprehensive%20TME%0Asegmentation%20across%20various%20tissue%20types%20and%20medical%20institutions%2C%20advancing%0Athe%20quantitative%20analysis%20of%20tumor%20microenvironments.%20This%20method%20represents%20a%0Asignificant%20step%20forward%20in%20enhancing%20our%20understanding%20of%20cancer%20biology%20and%0Asupporting%20precise%20clinical%20decision-making%20from%20large-scale%20histopathology%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Pathological%2520Image%2520Segmentation%2520via%2520Teacher%2520Aggregation%250A%2520%2520for%2520Tumor%2520Microenvironment%2520Analysis%26entry.906535625%3DDaisuke%2520Komura%2520and%2520Maki%2520Takao%2520and%2520Mieko%2520Ochi%2520and%2520Takumi%2520Onoyama%2520and%2520Hiroto%2520Katoh%2520and%2520Hiroyuki%2520Abe%2520and%2520Hiroyuki%2520Sano%2520and%2520Teppei%2520Konishi%2520and%2520Toshio%2520Kumasaka%2520and%2520Tomoyuki%2520Yokose%2520and%2520Yohei%2520Miyagi%2520and%2520Tetsuo%2520Ushiku%2520and%2520Shumpei%2520Ishikawa%26entry.1292438233%3D%2520%2520The%2520tumor%2520microenvironment%2520%2528TME%2529%2520plays%2520a%2520crucial%2520role%2520in%2520cancer%2520progression%250Aand%2520treatment%2520response%252C%2520yet%2520current%2520methods%2520for%2520its%2520comprehensive%2520analysis%2520in%250AH%2526E-stained%2520tissue%2520slides%2520face%2520significant%2520limitations%2520in%2520the%2520diversity%2520of%250Atissue%2520cell%2520types%2520and%2520accuracy.%2520Here%252C%2520we%2520present%2520PAGET%2520%2528Pathological%2520image%250Asegmentation%2520via%2520AGgrEgated%2520Teachers%2529%252C%2520a%2520new%2520knowledge%2520distillation%2520approach%250Athat%2520integrates%2520multiple%2520segmentation%2520models%2520while%2520considering%2520the%2520hierarchical%250Anature%2520of%2520cell%2520types%2520in%2520the%2520TME.%2520By%2520leveraging%2520a%2520unique%2520dataset%2520created%2520through%250Aimmunohistochemical%2520restaining%2520techniques%2520and%2520existing%2520segmentation%2520models%252C%250APAGET%2520enables%2520simultaneous%2520identification%2520and%2520classification%2520of%252014%2520key%2520TME%250Acomponents.%2520We%2520demonstrate%2520PAGET%2527s%2520ability%2520to%2520perform%2520rapid%252C%2520comprehensive%2520TME%250Asegmentation%2520across%2520various%2520tissue%2520types%2520and%2520medical%2520institutions%252C%2520advancing%250Athe%2520quantitative%2520analysis%2520of%2520tumor%2520microenvironments.%2520This%2520method%2520represents%2520a%250Asignificant%2520step%2520forward%2520in%2520enhancing%2520our%2520understanding%2520of%2520cancer%2520biology%2520and%250Asupporting%2520precise%2520clinical%2520decision-making%2520from%2520large-scale%2520histopathology%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Pathological%20Image%20Segmentation%20via%20Teacher%20Aggregation%0A%20%20for%20Tumor%20Microenvironment%20Analysis&entry.906535625=Daisuke%20Komura%20and%20Maki%20Takao%20and%20Mieko%20Ochi%20and%20Takumi%20Onoyama%20and%20Hiroto%20Katoh%20and%20Hiroyuki%20Abe%20and%20Hiroyuki%20Sano%20and%20Teppei%20Konishi%20and%20Toshio%20Kumasaka%20and%20Tomoyuki%20Yokose%20and%20Yohei%20Miyagi%20and%20Tetsuo%20Ushiku%20and%20Shumpei%20Ishikawa&entry.1292438233=%20%20The%20tumor%20microenvironment%20%28TME%29%20plays%20a%20crucial%20role%20in%20cancer%20progression%0Aand%20treatment%20response%2C%20yet%20current%20methods%20for%20its%20comprehensive%20analysis%20in%0AH%26E-stained%20tissue%20slides%20face%20significant%20limitations%20in%20the%20diversity%20of%0Atissue%20cell%20types%20and%20accuracy.%20Here%2C%20we%20present%20PAGET%20%28Pathological%20image%0Asegmentation%20via%20AGgrEgated%20Teachers%29%2C%20a%20new%20knowledge%20distillation%20approach%0Athat%20integrates%20multiple%20segmentation%20models%20while%20considering%20the%20hierarchical%0Anature%20of%20cell%20types%20in%20the%20TME.%20By%20leveraging%20a%20unique%20dataset%20created%20through%0Aimmunohistochemical%20restaining%20techniques%20and%20existing%20segmentation%20models%2C%0APAGET%20enables%20simultaneous%20identification%20and%20classification%20of%2014%20key%20TME%0Acomponents.%20We%20demonstrate%20PAGET%27s%20ability%20to%20perform%20rapid%2C%20comprehensive%20TME%0Asegmentation%20across%20various%20tissue%20types%20and%20medical%20institutions%2C%20advancing%0Athe%20quantitative%20analysis%20of%20tumor%20microenvironments.%20This%20method%20represents%20a%0Asignificant%20step%20forward%20in%20enhancing%20our%20understanding%20of%20cancer%20biology%20and%0Asupporting%20precise%20clinical%20decision-making%20from%20large-scale%20histopathology%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02909v1&entry.124074799=Read"},
{"title": "Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging\n  and Variational Autoencoders", "author": "Mahmoud Abdulsalam and Usman Zahidi and Bradley Hurst and Simon Pearson and Grzegorz Cielniak and James Brown", "abstract": "  Tomato anomalies/damages pose a significant challenge in greenhouse farming.\nWhile this method of cultivation benefits from efficient resource utilization,\nanomalies can significantly degrade the quality of farm produce. A common\nanomaly associated with tomatoes is splitting, characterized by the development\nof cracks on the tomato skin, which degrades its quality. Detecting this type\nof anomaly is challenging due to dynamic variations in appearance and sizes,\ncompounded by dataset scarcity. We address this problem in an unsupervised\nmanner by utilizing a tailored variational autoencoder (VAE) with hyperspectral\ninput. Preliminary analysis of the dataset enabled us to select the optimal\nrange of wavelengths for detecting this anomaly. Our findings indicate that the\n530nm - 550nm range is suitable for identifying tomato dry splits. The analysis\non reconstruction loss allow us to not only detect the anomalies but also to\nsome degree estimate the anomalous regions.\n", "link": "http://arxiv.org/abs/2501.02921v1", "date": "2025-01-06", "relevancy": 2.3754, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4816}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4784}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders&body=Title%3A%20Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders%0AAuthor%3A%20Mahmoud%20Abdulsalam%20and%20Usman%20Zahidi%20and%20Bradley%20Hurst%20and%20Simon%20Pearson%20and%20Grzegorz%20Cielniak%20and%20James%20Brown%0AAbstract%3A%20%20%20Tomato%20anomalies/damages%20pose%20a%20significant%20challenge%20in%20greenhouse%20farming.%0AWhile%20this%20method%20of%20cultivation%20benefits%20from%20efficient%20resource%20utilization%2C%0Aanomalies%20can%20significantly%20degrade%20the%20quality%20of%20farm%20produce.%20A%20common%0Aanomaly%20associated%20with%20tomatoes%20is%20splitting%2C%20characterized%20by%20the%20development%0Aof%20cracks%20on%20the%20tomato%20skin%2C%20which%20degrades%20its%20quality.%20Detecting%20this%20type%0Aof%20anomaly%20is%20challenging%20due%20to%20dynamic%20variations%20in%20appearance%20and%20sizes%2C%0Acompounded%20by%20dataset%20scarcity.%20We%20address%20this%20problem%20in%20an%20unsupervised%0Amanner%20by%20utilizing%20a%20tailored%20variational%20autoencoder%20%28VAE%29%20with%20hyperspectral%0Ainput.%20Preliminary%20analysis%20of%20the%20dataset%20enabled%20us%20to%20select%20the%20optimal%0Arange%20of%20wavelengths%20for%20detecting%20this%20anomaly.%20Our%20findings%20indicate%20that%20the%0A530nm%20-%20550nm%20range%20is%20suitable%20for%20identifying%20tomato%20dry%20splits.%20The%20analysis%0Aon%20reconstruction%20loss%20allow%20us%20to%20not%20only%20detect%20the%20anomalies%20but%20also%20to%0Asome%20degree%20estimate%20the%20anomalous%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Tomato%2520Split%2520Anomaly%2520Detection%2520using%2520Hyperspectral%2520Imaging%250A%2520%2520and%2520Variational%2520Autoencoders%26entry.906535625%3DMahmoud%2520Abdulsalam%2520and%2520Usman%2520Zahidi%2520and%2520Bradley%2520Hurst%2520and%2520Simon%2520Pearson%2520and%2520Grzegorz%2520Cielniak%2520and%2520James%2520Brown%26entry.1292438233%3D%2520%2520Tomato%2520anomalies/damages%2520pose%2520a%2520significant%2520challenge%2520in%2520greenhouse%2520farming.%250AWhile%2520this%2520method%2520of%2520cultivation%2520benefits%2520from%2520efficient%2520resource%2520utilization%252C%250Aanomalies%2520can%2520significantly%2520degrade%2520the%2520quality%2520of%2520farm%2520produce.%2520A%2520common%250Aanomaly%2520associated%2520with%2520tomatoes%2520is%2520splitting%252C%2520characterized%2520by%2520the%2520development%250Aof%2520cracks%2520on%2520the%2520tomato%2520skin%252C%2520which%2520degrades%2520its%2520quality.%2520Detecting%2520this%2520type%250Aof%2520anomaly%2520is%2520challenging%2520due%2520to%2520dynamic%2520variations%2520in%2520appearance%2520and%2520sizes%252C%250Acompounded%2520by%2520dataset%2520scarcity.%2520We%2520address%2520this%2520problem%2520in%2520an%2520unsupervised%250Amanner%2520by%2520utilizing%2520a%2520tailored%2520variational%2520autoencoder%2520%2528VAE%2529%2520with%2520hyperspectral%250Ainput.%2520Preliminary%2520analysis%2520of%2520the%2520dataset%2520enabled%2520us%2520to%2520select%2520the%2520optimal%250Arange%2520of%2520wavelengths%2520for%2520detecting%2520this%2520anomaly.%2520Our%2520findings%2520indicate%2520that%2520the%250A530nm%2520-%2520550nm%2520range%2520is%2520suitable%2520for%2520identifying%2520tomato%2520dry%2520splits.%2520The%2520analysis%250Aon%2520reconstruction%2520loss%2520allow%2520us%2520to%2520not%2520only%2520detect%2520the%2520anomalies%2520but%2520also%2520to%250Asome%2520degree%2520estimate%2520the%2520anomalous%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Tomato%20Split%20Anomaly%20Detection%20using%20Hyperspectral%20Imaging%0A%20%20and%20Variational%20Autoencoders&entry.906535625=Mahmoud%20Abdulsalam%20and%20Usman%20Zahidi%20and%20Bradley%20Hurst%20and%20Simon%20Pearson%20and%20Grzegorz%20Cielniak%20and%20James%20Brown&entry.1292438233=%20%20Tomato%20anomalies/damages%20pose%20a%20significant%20challenge%20in%20greenhouse%20farming.%0AWhile%20this%20method%20of%20cultivation%20benefits%20from%20efficient%20resource%20utilization%2C%0Aanomalies%20can%20significantly%20degrade%20the%20quality%20of%20farm%20produce.%20A%20common%0Aanomaly%20associated%20with%20tomatoes%20is%20splitting%2C%20characterized%20by%20the%20development%0Aof%20cracks%20on%20the%20tomato%20skin%2C%20which%20degrades%20its%20quality.%20Detecting%20this%20type%0Aof%20anomaly%20is%20challenging%20due%20to%20dynamic%20variations%20in%20appearance%20and%20sizes%2C%0Acompounded%20by%20dataset%20scarcity.%20We%20address%20this%20problem%20in%20an%20unsupervised%0Amanner%20by%20utilizing%20a%20tailored%20variational%20autoencoder%20%28VAE%29%20with%20hyperspectral%0Ainput.%20Preliminary%20analysis%20of%20the%20dataset%20enabled%20us%20to%20select%20the%20optimal%0Arange%20of%20wavelengths%20for%20detecting%20this%20anomaly.%20Our%20findings%20indicate%20that%20the%0A530nm%20-%20550nm%20range%20is%20suitable%20for%20identifying%20tomato%20dry%20splits.%20The%20analysis%0Aon%20reconstruction%20loss%20allow%20us%20to%20not%20only%20detect%20the%20anomalies%20but%20also%20to%0Asome%20degree%20estimate%20the%20anomalous%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02921v1&entry.124074799=Read"},
{"title": "Personalized Fashion Recommendation with Image Attributes and Aesthetics\n  Assessment", "author": "Chongxian Chen and Fan Mo and Xin Fan and Hayato Yamana", "abstract": "  Personalized fashion recommendation is a difficult task because 1) the\ndecisions are highly correlated with users' aesthetic appetite, which previous\nwork frequently overlooks, and 2) many new items are constantly rolling out\nthat cause strict cold-start problems in the popular identity (ID)-based\nrecommendation methods. These new items are critical to recommend because of\ntrend-driven consumerism. In this work, we aim to provide more accurate\npersonalized fashion recommendations and solve the cold-start problem by\nconverting available information, especially images, into two attribute graphs\nfocusing on optimized image utilization and noise-reducing user modeling.\nCompared with previous methods that separate image and text as two components,\nthe proposed method combines image and text information to create a richer\nattributes graph. Capitalizing on the advancement of large language and vision\nmodels, we experiment with extracting fine-grained attributes efficiently and\nas desired using two different prompts. Preliminary experiments on the IQON3000\ndataset have shown that the proposed method achieves competitive accuracy\ncompared with baselines.\n", "link": "http://arxiv.org/abs/2501.03085v1", "date": "2025-01-06", "relevancy": 2.3539, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6499}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5913}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Fashion%20Recommendation%20with%20Image%20Attributes%20and%20Aesthetics%0A%20%20Assessment&body=Title%3A%20Personalized%20Fashion%20Recommendation%20with%20Image%20Attributes%20and%20Aesthetics%0A%20%20Assessment%0AAuthor%3A%20Chongxian%20Chen%20and%20Fan%20Mo%20and%20Xin%20Fan%20and%20Hayato%20Yamana%0AAbstract%3A%20%20%20Personalized%20fashion%20recommendation%20is%20a%20difficult%20task%20because%201%29%20the%0Adecisions%20are%20highly%20correlated%20with%20users%27%20aesthetic%20appetite%2C%20which%20previous%0Awork%20frequently%20overlooks%2C%20and%202%29%20many%20new%20items%20are%20constantly%20rolling%20out%0Athat%20cause%20strict%20cold-start%20problems%20in%20the%20popular%20identity%20%28ID%29-based%0Arecommendation%20methods.%20These%20new%20items%20are%20critical%20to%20recommend%20because%20of%0Atrend-driven%20consumerism.%20In%20this%20work%2C%20we%20aim%20to%20provide%20more%20accurate%0Apersonalized%20fashion%20recommendations%20and%20solve%20the%20cold-start%20problem%20by%0Aconverting%20available%20information%2C%20especially%20images%2C%20into%20two%20attribute%20graphs%0Afocusing%20on%20optimized%20image%20utilization%20and%20noise-reducing%20user%20modeling.%0ACompared%20with%20previous%20methods%20that%20separate%20image%20and%20text%20as%20two%20components%2C%0Athe%20proposed%20method%20combines%20image%20and%20text%20information%20to%20create%20a%20richer%0Aattributes%20graph.%20Capitalizing%20on%20the%20advancement%20of%20large%20language%20and%20vision%0Amodels%2C%20we%20experiment%20with%20extracting%20fine-grained%20attributes%20efficiently%20and%0Aas%20desired%20using%20two%20different%20prompts.%20Preliminary%20experiments%20on%20the%20IQON3000%0Adataset%20have%20shown%20that%20the%20proposed%20method%20achieves%20competitive%20accuracy%0Acompared%20with%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Fashion%2520Recommendation%2520with%2520Image%2520Attributes%2520and%2520Aesthetics%250A%2520%2520Assessment%26entry.906535625%3DChongxian%2520Chen%2520and%2520Fan%2520Mo%2520and%2520Xin%2520Fan%2520and%2520Hayato%2520Yamana%26entry.1292438233%3D%2520%2520Personalized%2520fashion%2520recommendation%2520is%2520a%2520difficult%2520task%2520because%25201%2529%2520the%250Adecisions%2520are%2520highly%2520correlated%2520with%2520users%2527%2520aesthetic%2520appetite%252C%2520which%2520previous%250Awork%2520frequently%2520overlooks%252C%2520and%25202%2529%2520many%2520new%2520items%2520are%2520constantly%2520rolling%2520out%250Athat%2520cause%2520strict%2520cold-start%2520problems%2520in%2520the%2520popular%2520identity%2520%2528ID%2529-based%250Arecommendation%2520methods.%2520These%2520new%2520items%2520are%2520critical%2520to%2520recommend%2520because%2520of%250Atrend-driven%2520consumerism.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520provide%2520more%2520accurate%250Apersonalized%2520fashion%2520recommendations%2520and%2520solve%2520the%2520cold-start%2520problem%2520by%250Aconverting%2520available%2520information%252C%2520especially%2520images%252C%2520into%2520two%2520attribute%2520graphs%250Afocusing%2520on%2520optimized%2520image%2520utilization%2520and%2520noise-reducing%2520user%2520modeling.%250ACompared%2520with%2520previous%2520methods%2520that%2520separate%2520image%2520and%2520text%2520as%2520two%2520components%252C%250Athe%2520proposed%2520method%2520combines%2520image%2520and%2520text%2520information%2520to%2520create%2520a%2520richer%250Aattributes%2520graph.%2520Capitalizing%2520on%2520the%2520advancement%2520of%2520large%2520language%2520and%2520vision%250Amodels%252C%2520we%2520experiment%2520with%2520extracting%2520fine-grained%2520attributes%2520efficiently%2520and%250Aas%2520desired%2520using%2520two%2520different%2520prompts.%2520Preliminary%2520experiments%2520on%2520the%2520IQON3000%250Adataset%2520have%2520shown%2520that%2520the%2520proposed%2520method%2520achieves%2520competitive%2520accuracy%250Acompared%2520with%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Fashion%20Recommendation%20with%20Image%20Attributes%20and%20Aesthetics%0A%20%20Assessment&entry.906535625=Chongxian%20Chen%20and%20Fan%20Mo%20and%20Xin%20Fan%20and%20Hayato%20Yamana&entry.1292438233=%20%20Personalized%20fashion%20recommendation%20is%20a%20difficult%20task%20because%201%29%20the%0Adecisions%20are%20highly%20correlated%20with%20users%27%20aesthetic%20appetite%2C%20which%20previous%0Awork%20frequently%20overlooks%2C%20and%202%29%20many%20new%20items%20are%20constantly%20rolling%20out%0Athat%20cause%20strict%20cold-start%20problems%20in%20the%20popular%20identity%20%28ID%29-based%0Arecommendation%20methods.%20These%20new%20items%20are%20critical%20to%20recommend%20because%20of%0Atrend-driven%20consumerism.%20In%20this%20work%2C%20we%20aim%20to%20provide%20more%20accurate%0Apersonalized%20fashion%20recommendations%20and%20solve%20the%20cold-start%20problem%20by%0Aconverting%20available%20information%2C%20especially%20images%2C%20into%20two%20attribute%20graphs%0Afocusing%20on%20optimized%20image%20utilization%20and%20noise-reducing%20user%20modeling.%0ACompared%20with%20previous%20methods%20that%20separate%20image%20and%20text%20as%20two%20components%2C%0Athe%20proposed%20method%20combines%20image%20and%20text%20information%20to%20create%20a%20richer%0Aattributes%20graph.%20Capitalizing%20on%20the%20advancement%20of%20large%20language%20and%20vision%0Amodels%2C%20we%20experiment%20with%20extracting%20fine-grained%20attributes%20efficiently%20and%0Aas%20desired%20using%20two%20different%20prompts.%20Preliminary%20experiments%20on%20the%20IQON3000%0Adataset%20have%20shown%20that%20the%20proposed%20method%20achieves%20competitive%20accuracy%0Acompared%20with%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03085v1&entry.124074799=Read"},
{"title": "Autoregressive Image Diffusion: Generation of Image Sequence and\n  Application in MRI", "author": "Guanxiong Luo and Shoujin Huang and Martin Uecker", "abstract": "  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging\nmodality. However, a persistent challenge lies in balancing image quality with\nimaging speed. This trade-off is primarily constrained by k-space measurements,\nwhich traverse specific trajectories in the spatial Fourier domain (k-space).\nThese measurements are often undersampled to shorten acquisition times,\nresulting in image artifacts and compromised quality. Generative models learn\nimage distributions and can be used to reconstruct high-quality images from\nundersampled k-space data. In this work, we present the autoregressive image\ndiffusion (AID) model for image sequences and use it to sample the posterior\nfor accelerated MRI reconstruction. The algorithm incorporates both\nundersampled k-space and pre-existing information. Models trained with fastMRI\ndataset are evaluated comprehensively. The results show that the AID model can\nrobustly generate sequentially coherent image sequences. In MRI applications,\nthe AID can outperform the standard diffusion model and reduce hallucinations,\ndue to the learned inter-image dependencies. The project code is available at\nhttps://github.com/mrirecon/aid.\n", "link": "http://arxiv.org/abs/2405.14327v5", "date": "2025-01-06", "relevancy": 2.3433, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5838}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&body=Title%3A%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI%0AAuthor%3A%20Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%20MRI%20applications%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%20The%20project%20code%20is%20available%20at%0Ahttps%3A//github.com/mrirecon/aid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14327v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Image%2520Diffusion%253A%2520Generation%2520of%2520Image%2520Sequence%2520and%250A%2520%2520Application%2520in%2520MRI%26entry.906535625%3DGuanxiong%2520Luo%2520and%2520Shoujin%2520Huang%2520and%2520Martin%2520Uecker%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520imaging%250Amodality.%2520However%252C%2520a%2520persistent%2520challenge%2520lies%2520in%2520balancing%2520image%2520quality%2520with%250Aimaging%2520speed.%2520This%2520trade-off%2520is%2520primarily%2520constrained%2520by%2520k-space%2520measurements%252C%250Awhich%2520traverse%2520specific%2520trajectories%2520in%2520the%2520spatial%2520Fourier%2520domain%2520%2528k-space%2529.%250AThese%2520measurements%2520are%2520often%2520undersampled%2520to%2520shorten%2520acquisition%2520times%252C%250Aresulting%2520in%2520image%2520artifacts%2520and%2520compromised%2520quality.%2520Generative%2520models%2520learn%250Aimage%2520distributions%2520and%2520can%2520be%2520used%2520to%2520reconstruct%2520high-quality%2520images%2520from%250Aundersampled%2520k-space%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520autoregressive%2520image%250Adiffusion%2520%2528AID%2529%2520model%2520for%2520image%2520sequences%2520and%2520use%2520it%2520to%2520sample%2520the%2520posterior%250Afor%2520accelerated%2520MRI%2520reconstruction.%2520The%2520algorithm%2520incorporates%2520both%250Aundersampled%2520k-space%2520and%2520pre-existing%2520information.%2520Models%2520trained%2520with%2520fastMRI%250Adataset%2520are%2520evaluated%2520comprehensively.%2520The%2520results%2520show%2520that%2520the%2520AID%2520model%2520can%250Arobustly%2520generate%2520sequentially%2520coherent%2520image%2520sequences.%2520In%2520MRI%2520applications%252C%250Athe%2520AID%2520can%2520outperform%2520the%2520standard%2520diffusion%2520model%2520and%2520reduce%2520hallucinations%252C%250Adue%2520to%2520the%2520learned%2520inter-image%2520dependencies.%2520The%2520project%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mrirecon/aid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14327v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&entry.906535625=Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%20MRI%20applications%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%20The%20project%20code%20is%20available%20at%0Ahttps%3A//github.com/mrirecon/aid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14327v5&entry.124074799=Read"},
{"title": "4D-CS: Exploiting Cluster Prior for 4D Spatio-Temporal LiDAR Semantic\n  Segmentation", "author": "Jiexi Zhong and Zhiheng Li and Yubo Cui and Zheng Fang", "abstract": "  Semantic segmentation of LiDAR points has significant value for autonomous\ndriving and mobile robot systems. Most approaches explore spatio-temporal\ninformation of multi-scan to identify the semantic classes and motion states\nfor each point. However, these methods often overlook the segmentation\nconsistency in space and time, which may result in point clouds within the same\nobject being predicted as different categories. To handle this issue, our core\nidea is to generate cluster labels across multiple frames that can reflect the\ncomplete spatial structure and temporal information of objects. These labels\nserve as explicit guidance for our dual-branch network, 4D-CS, which integrates\npoint-based and cluster-based branches to enable more consistent segmentation.\nSpecifically, in the point-based branch, we leverage historical knowledge to\nenrich the current feature through temporal fusion on multiple views. In the\ncluster-based branch, we propose a new strategy to produce cluster labels of\nforeground objects and apply them to gather point-wise information to derive\ncluster features. We then merge neighboring clusters across multiple scans to\nrestore missing features due to occlusion. Finally, in the point-cluster fusion\nstage, we adaptively fuse the information from the two branches to optimize\nsegmentation results. Extensive experiments confirm the effectiveness of the\nproposed method, and we achieve state-of-the-art results on the multi-scan\nsemantic and moving object segmentation on SemanticKITTI and nuScenes datasets.\nThe code will be available at https://github.com/NEU-REAL/4D-CS.git.\n", "link": "http://arxiv.org/abs/2501.02937v1", "date": "2025-01-06", "relevancy": 2.3178, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5809}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D-CS%3A%20Exploiting%20Cluster%20Prior%20for%204D%20Spatio-Temporal%20LiDAR%20Semantic%0A%20%20Segmentation&body=Title%3A%204D-CS%3A%20Exploiting%20Cluster%20Prior%20for%204D%20Spatio-Temporal%20LiDAR%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jiexi%20Zhong%20and%20Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Zheng%20Fang%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20LiDAR%20points%20has%20significant%20value%20for%20autonomous%0Adriving%20and%20mobile%20robot%20systems.%20Most%20approaches%20explore%20spatio-temporal%0Ainformation%20of%20multi-scan%20to%20identify%20the%20semantic%20classes%20and%20motion%20states%0Afor%20each%20point.%20However%2C%20these%20methods%20often%20overlook%20the%20segmentation%0Aconsistency%20in%20space%20and%20time%2C%20which%20may%20result%20in%20point%20clouds%20within%20the%20same%0Aobject%20being%20predicted%20as%20different%20categories.%20To%20handle%20this%20issue%2C%20our%20core%0Aidea%20is%20to%20generate%20cluster%20labels%20across%20multiple%20frames%20that%20can%20reflect%20the%0Acomplete%20spatial%20structure%20and%20temporal%20information%20of%20objects.%20These%20labels%0Aserve%20as%20explicit%20guidance%20for%20our%20dual-branch%20network%2C%204D-CS%2C%20which%20integrates%0Apoint-based%20and%20cluster-based%20branches%20to%20enable%20more%20consistent%20segmentation.%0ASpecifically%2C%20in%20the%20point-based%20branch%2C%20we%20leverage%20historical%20knowledge%20to%0Aenrich%20the%20current%20feature%20through%20temporal%20fusion%20on%20multiple%20views.%20In%20the%0Acluster-based%20branch%2C%20we%20propose%20a%20new%20strategy%20to%20produce%20cluster%20labels%20of%0Aforeground%20objects%20and%20apply%20them%20to%20gather%20point-wise%20information%20to%20derive%0Acluster%20features.%20We%20then%20merge%20neighboring%20clusters%20across%20multiple%20scans%20to%0Arestore%20missing%20features%20due%20to%20occlusion.%20Finally%2C%20in%20the%20point-cluster%20fusion%0Astage%2C%20we%20adaptively%20fuse%20the%20information%20from%20the%20two%20branches%20to%20optimize%0Asegmentation%20results.%20Extensive%20experiments%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%2C%20and%20we%20achieve%20state-of-the-art%20results%20on%20the%20multi-scan%0Asemantic%20and%20moving%20object%20segmentation%20on%20SemanticKITTI%20and%20nuScenes%20datasets.%0AThe%20code%20will%20be%20available%20at%20https%3A//github.com/NEU-REAL/4D-CS.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D-CS%253A%2520Exploiting%2520Cluster%2520Prior%2520for%25204D%2520Spatio-Temporal%2520LiDAR%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJiexi%2520Zhong%2520and%2520Zhiheng%2520Li%2520and%2520Yubo%2520Cui%2520and%2520Zheng%2520Fang%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520LiDAR%2520points%2520has%2520significant%2520value%2520for%2520autonomous%250Adriving%2520and%2520mobile%2520robot%2520systems.%2520Most%2520approaches%2520explore%2520spatio-temporal%250Ainformation%2520of%2520multi-scan%2520to%2520identify%2520the%2520semantic%2520classes%2520and%2520motion%2520states%250Afor%2520each%2520point.%2520However%252C%2520these%2520methods%2520often%2520overlook%2520the%2520segmentation%250Aconsistency%2520in%2520space%2520and%2520time%252C%2520which%2520may%2520result%2520in%2520point%2520clouds%2520within%2520the%2520same%250Aobject%2520being%2520predicted%2520as%2520different%2520categories.%2520To%2520handle%2520this%2520issue%252C%2520our%2520core%250Aidea%2520is%2520to%2520generate%2520cluster%2520labels%2520across%2520multiple%2520frames%2520that%2520can%2520reflect%2520the%250Acomplete%2520spatial%2520structure%2520and%2520temporal%2520information%2520of%2520objects.%2520These%2520labels%250Aserve%2520as%2520explicit%2520guidance%2520for%2520our%2520dual-branch%2520network%252C%25204D-CS%252C%2520which%2520integrates%250Apoint-based%2520and%2520cluster-based%2520branches%2520to%2520enable%2520more%2520consistent%2520segmentation.%250ASpecifically%252C%2520in%2520the%2520point-based%2520branch%252C%2520we%2520leverage%2520historical%2520knowledge%2520to%250Aenrich%2520the%2520current%2520feature%2520through%2520temporal%2520fusion%2520on%2520multiple%2520views.%2520In%2520the%250Acluster-based%2520branch%252C%2520we%2520propose%2520a%2520new%2520strategy%2520to%2520produce%2520cluster%2520labels%2520of%250Aforeground%2520objects%2520and%2520apply%2520them%2520to%2520gather%2520point-wise%2520information%2520to%2520derive%250Acluster%2520features.%2520We%2520then%2520merge%2520neighboring%2520clusters%2520across%2520multiple%2520scans%2520to%250Arestore%2520missing%2520features%2520due%2520to%2520occlusion.%2520Finally%252C%2520in%2520the%2520point-cluster%2520fusion%250Astage%252C%2520we%2520adaptively%2520fuse%2520the%2520information%2520from%2520the%2520two%2520branches%2520to%2520optimize%250Asegmentation%2520results.%2520Extensive%2520experiments%2520confirm%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%252C%2520and%2520we%2520achieve%2520state-of-the-art%2520results%2520on%2520the%2520multi-scan%250Asemantic%2520and%2520moving%2520object%2520segmentation%2520on%2520SemanticKITTI%2520and%2520nuScenes%2520datasets.%250AThe%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/NEU-REAL/4D-CS.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D-CS%3A%20Exploiting%20Cluster%20Prior%20for%204D%20Spatio-Temporal%20LiDAR%20Semantic%0A%20%20Segmentation&entry.906535625=Jiexi%20Zhong%20and%20Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Zheng%20Fang&entry.1292438233=%20%20Semantic%20segmentation%20of%20LiDAR%20points%20has%20significant%20value%20for%20autonomous%0Adriving%20and%20mobile%20robot%20systems.%20Most%20approaches%20explore%20spatio-temporal%0Ainformation%20of%20multi-scan%20to%20identify%20the%20semantic%20classes%20and%20motion%20states%0Afor%20each%20point.%20However%2C%20these%20methods%20often%20overlook%20the%20segmentation%0Aconsistency%20in%20space%20and%20time%2C%20which%20may%20result%20in%20point%20clouds%20within%20the%20same%0Aobject%20being%20predicted%20as%20different%20categories.%20To%20handle%20this%20issue%2C%20our%20core%0Aidea%20is%20to%20generate%20cluster%20labels%20across%20multiple%20frames%20that%20can%20reflect%20the%0Acomplete%20spatial%20structure%20and%20temporal%20information%20of%20objects.%20These%20labels%0Aserve%20as%20explicit%20guidance%20for%20our%20dual-branch%20network%2C%204D-CS%2C%20which%20integrates%0Apoint-based%20and%20cluster-based%20branches%20to%20enable%20more%20consistent%20segmentation.%0ASpecifically%2C%20in%20the%20point-based%20branch%2C%20we%20leverage%20historical%20knowledge%20to%0Aenrich%20the%20current%20feature%20through%20temporal%20fusion%20on%20multiple%20views.%20In%20the%0Acluster-based%20branch%2C%20we%20propose%20a%20new%20strategy%20to%20produce%20cluster%20labels%20of%0Aforeground%20objects%20and%20apply%20them%20to%20gather%20point-wise%20information%20to%20derive%0Acluster%20features.%20We%20then%20merge%20neighboring%20clusters%20across%20multiple%20scans%20to%0Arestore%20missing%20features%20due%20to%20occlusion.%20Finally%2C%20in%20the%20point-cluster%20fusion%0Astage%2C%20we%20adaptively%20fuse%20the%20information%20from%20the%20two%20branches%20to%20optimize%0Asegmentation%20results.%20Extensive%20experiments%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%2C%20and%20we%20achieve%20state-of-the-art%20results%20on%20the%20multi-scan%0Asemantic%20and%20moving%20object%20segmentation%20on%20SemanticKITTI%20and%20nuScenes%20datasets.%0AThe%20code%20will%20be%20available%20at%20https%3A//github.com/NEU-REAL/4D-CS.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02937v1&entry.124074799=Read"},
{"title": "TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via\n  Adaptive KAN Mechanisms and Intelligent Feature Scaling", "author": "Ruiquan Ge and Xiao Yu and Yifei Chen and Guanyu Zhou and Fan Jia and Shenghao Zhu and Junhao Jia and Chenyan Zhang and Yifei Sun and Dong Zeng and Changmiao Wang and Qiegen Liu and Shanzhou Niu", "abstract": "  Magnetic Resonance Imaging (MRI) has become essential in clinical diagnosis\ndue to its high resolution and multiple contrast mechanisms. However, the\nrelatively long acquisition time limits its broader application. To address\nthis issue, this study presents an innovative conditional guided diffusion\nmodel, named as TC-KANRecon, which incorporates the Multi-Free U-KAN (MF-UKAN)\nmodule and a dynamic clipping strategy. TC-KANRecon model aims to accelerate\nthe MRI reconstruction process through deep learning methods while maintaining\nthe quality of the reconstructed images. The MF-UKAN module can effectively\nbalance the tradeoff between image denoising and structure preservation.\nSpecifically, it presents the multi-head attention mechanisms and scalar\nmodulation factors, which significantly enhances the model's robustness and\nstructure preservation capabilities in complex noise environments. Moreover,\nthe dynamic clipping strategy in TC-KANRecon adjusts the cropping interval\naccording to the sampling steps, thereby mitigating image detail loss\ntypicalching the visual features of the images. Furthermore, the MC-Model\nincorporates full-sampling k-space information, realizing efficient fusion of\nconditional information, enhancing the model's ability to process complex data,\nand improving the realism and detail richness of reconstructed images.\nExperimental results demonstrate that the proposed method outperforms other MRI\nreconstruction methods in both qualitative and quantitative evaluations.\nNotably, TC-KANRecon method exhibits excellent reconstruction results when\nprocessing high-noise, low-sampling-rate MRI data. Our source code is available\nat https://github.com/lcbkmm/TC-KANRecon.\n", "link": "http://arxiv.org/abs/2408.05705v2", "date": "2025-01-06", "relevancy": 2.3051, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-KANRecon%3A%20High-Quality%20and%20Accelerated%20MRI%20Reconstruction%20via%0A%20%20Adaptive%20KAN%20Mechanisms%20and%20Intelligent%20Feature%20Scaling&body=Title%3A%20TC-KANRecon%3A%20High-Quality%20and%20Accelerated%20MRI%20Reconstruction%20via%0A%20%20Adaptive%20KAN%20Mechanisms%20and%20Intelligent%20Feature%20Scaling%0AAuthor%3A%20Ruiquan%20Ge%20and%20Xiao%20Yu%20and%20Yifei%20Chen%20and%20Guanyu%20Zhou%20and%20Fan%20Jia%20and%20Shenghao%20Zhu%20and%20Junhao%20Jia%20and%20Chenyan%20Zhang%20and%20Yifei%20Sun%20and%20Dong%20Zeng%20and%20Changmiao%20Wang%20and%20Qiegen%20Liu%20and%20Shanzhou%20Niu%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20has%20become%20essential%20in%20clinical%20diagnosis%0Adue%20to%20its%20high%20resolution%20and%20multiple%20contrast%20mechanisms.%20However%2C%20the%0Arelatively%20long%20acquisition%20time%20limits%20its%20broader%20application.%20To%20address%0Athis%20issue%2C%20this%20study%20presents%20an%20innovative%20conditional%20guided%20diffusion%0Amodel%2C%20named%20as%20TC-KANRecon%2C%20which%20incorporates%20the%20Multi-Free%20U-KAN%20%28MF-UKAN%29%0Amodule%20and%20a%20dynamic%20clipping%20strategy.%20TC-KANRecon%20model%20aims%20to%20accelerate%0Athe%20MRI%20reconstruction%20process%20through%20deep%20learning%20methods%20while%20maintaining%0Athe%20quality%20of%20the%20reconstructed%20images.%20The%20MF-UKAN%20module%20can%20effectively%0Abalance%20the%20tradeoff%20between%20image%20denoising%20and%20structure%20preservation.%0ASpecifically%2C%20it%20presents%20the%20multi-head%20attention%20mechanisms%20and%20scalar%0Amodulation%20factors%2C%20which%20significantly%20enhances%20the%20model%27s%20robustness%20and%0Astructure%20preservation%20capabilities%20in%20complex%20noise%20environments.%20Moreover%2C%0Athe%20dynamic%20clipping%20strategy%20in%20TC-KANRecon%20adjusts%20the%20cropping%20interval%0Aaccording%20to%20the%20sampling%20steps%2C%20thereby%20mitigating%20image%20detail%20loss%0Atypicalching%20the%20visual%20features%20of%20the%20images.%20Furthermore%2C%20the%20MC-Model%0Aincorporates%20full-sampling%20k-space%20information%2C%20realizing%20efficient%20fusion%20of%0Aconditional%20information%2C%20enhancing%20the%20model%27s%20ability%20to%20process%20complex%20data%2C%0Aand%20improving%20the%20realism%20and%20detail%20richness%20of%20reconstructed%20images.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20method%20outperforms%20other%20MRI%0Areconstruction%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluations.%0ANotably%2C%20TC-KANRecon%20method%20exhibits%20excellent%20reconstruction%20results%20when%0Aprocessing%20high-noise%2C%20low-sampling-rate%20MRI%20data.%20Our%20source%20code%20is%20available%0Aat%20https%3A//github.com/lcbkmm/TC-KANRecon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-KANRecon%253A%2520High-Quality%2520and%2520Accelerated%2520MRI%2520Reconstruction%2520via%250A%2520%2520Adaptive%2520KAN%2520Mechanisms%2520and%2520Intelligent%2520Feature%2520Scaling%26entry.906535625%3DRuiquan%2520Ge%2520and%2520Xiao%2520Yu%2520and%2520Yifei%2520Chen%2520and%2520Guanyu%2520Zhou%2520and%2520Fan%2520Jia%2520and%2520Shenghao%2520Zhu%2520and%2520Junhao%2520Jia%2520and%2520Chenyan%2520Zhang%2520and%2520Yifei%2520Sun%2520and%2520Dong%2520Zeng%2520and%2520Changmiao%2520Wang%2520and%2520Qiegen%2520Liu%2520and%2520Shanzhou%2520Niu%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520has%2520become%2520essential%2520in%2520clinical%2520diagnosis%250Adue%2520to%2520its%2520high%2520resolution%2520and%2520multiple%2520contrast%2520mechanisms.%2520However%252C%2520the%250Arelatively%2520long%2520acquisition%2520time%2520limits%2520its%2520broader%2520application.%2520To%2520address%250Athis%2520issue%252C%2520this%2520study%2520presents%2520an%2520innovative%2520conditional%2520guided%2520diffusion%250Amodel%252C%2520named%2520as%2520TC-KANRecon%252C%2520which%2520incorporates%2520the%2520Multi-Free%2520U-KAN%2520%2528MF-UKAN%2529%250Amodule%2520and%2520a%2520dynamic%2520clipping%2520strategy.%2520TC-KANRecon%2520model%2520aims%2520to%2520accelerate%250Athe%2520MRI%2520reconstruction%2520process%2520through%2520deep%2520learning%2520methods%2520while%2520maintaining%250Athe%2520quality%2520of%2520the%2520reconstructed%2520images.%2520The%2520MF-UKAN%2520module%2520can%2520effectively%250Abalance%2520the%2520tradeoff%2520between%2520image%2520denoising%2520and%2520structure%2520preservation.%250ASpecifically%252C%2520it%2520presents%2520the%2520multi-head%2520attention%2520mechanisms%2520and%2520scalar%250Amodulation%2520factors%252C%2520which%2520significantly%2520enhances%2520the%2520model%2527s%2520robustness%2520and%250Astructure%2520preservation%2520capabilities%2520in%2520complex%2520noise%2520environments.%2520Moreover%252C%250Athe%2520dynamic%2520clipping%2520strategy%2520in%2520TC-KANRecon%2520adjusts%2520the%2520cropping%2520interval%250Aaccording%2520to%2520the%2520sampling%2520steps%252C%2520thereby%2520mitigating%2520image%2520detail%2520loss%250Atypicalching%2520the%2520visual%2520features%2520of%2520the%2520images.%2520Furthermore%252C%2520the%2520MC-Model%250Aincorporates%2520full-sampling%2520k-space%2520information%252C%2520realizing%2520efficient%2520fusion%2520of%250Aconditional%2520information%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520process%2520complex%2520data%252C%250Aand%2520improving%2520the%2520realism%2520and%2520detail%2520richness%2520of%2520reconstructed%2520images.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520other%2520MRI%250Areconstruction%2520methods%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations.%250ANotably%252C%2520TC-KANRecon%2520method%2520exhibits%2520excellent%2520reconstruction%2520results%2520when%250Aprocessing%2520high-noise%252C%2520low-sampling-rate%2520MRI%2520data.%2520Our%2520source%2520code%2520is%2520available%250Aat%2520https%253A//github.com/lcbkmm/TC-KANRecon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-KANRecon%3A%20High-Quality%20and%20Accelerated%20MRI%20Reconstruction%20via%0A%20%20Adaptive%20KAN%20Mechanisms%20and%20Intelligent%20Feature%20Scaling&entry.906535625=Ruiquan%20Ge%20and%20Xiao%20Yu%20and%20Yifei%20Chen%20and%20Guanyu%20Zhou%20and%20Fan%20Jia%20and%20Shenghao%20Zhu%20and%20Junhao%20Jia%20and%20Chenyan%20Zhang%20and%20Yifei%20Sun%20and%20Dong%20Zeng%20and%20Changmiao%20Wang%20and%20Qiegen%20Liu%20and%20Shanzhou%20Niu&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20has%20become%20essential%20in%20clinical%20diagnosis%0Adue%20to%20its%20high%20resolution%20and%20multiple%20contrast%20mechanisms.%20However%2C%20the%0Arelatively%20long%20acquisition%20time%20limits%20its%20broader%20application.%20To%20address%0Athis%20issue%2C%20this%20study%20presents%20an%20innovative%20conditional%20guided%20diffusion%0Amodel%2C%20named%20as%20TC-KANRecon%2C%20which%20incorporates%20the%20Multi-Free%20U-KAN%20%28MF-UKAN%29%0Amodule%20and%20a%20dynamic%20clipping%20strategy.%20TC-KANRecon%20model%20aims%20to%20accelerate%0Athe%20MRI%20reconstruction%20process%20through%20deep%20learning%20methods%20while%20maintaining%0Athe%20quality%20of%20the%20reconstructed%20images.%20The%20MF-UKAN%20module%20can%20effectively%0Abalance%20the%20tradeoff%20between%20image%20denoising%20and%20structure%20preservation.%0ASpecifically%2C%20it%20presents%20the%20multi-head%20attention%20mechanisms%20and%20scalar%0Amodulation%20factors%2C%20which%20significantly%20enhances%20the%20model%27s%20robustness%20and%0Astructure%20preservation%20capabilities%20in%20complex%20noise%20environments.%20Moreover%2C%0Athe%20dynamic%20clipping%20strategy%20in%20TC-KANRecon%20adjusts%20the%20cropping%20interval%0Aaccording%20to%20the%20sampling%20steps%2C%20thereby%20mitigating%20image%20detail%20loss%0Atypicalching%20the%20visual%20features%20of%20the%20images.%20Furthermore%2C%20the%20MC-Model%0Aincorporates%20full-sampling%20k-space%20information%2C%20realizing%20efficient%20fusion%20of%0Aconditional%20information%2C%20enhancing%20the%20model%27s%20ability%20to%20process%20complex%20data%2C%0Aand%20improving%20the%20realism%20and%20detail%20richness%20of%20reconstructed%20images.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20method%20outperforms%20other%20MRI%0Areconstruction%20methods%20in%20both%20qualitative%20and%20quantitative%20evaluations.%0ANotably%2C%20TC-KANRecon%20method%20exhibits%20excellent%20reconstruction%20results%20when%0Aprocessing%20high-noise%2C%20low-sampling-rate%20MRI%20data.%20Our%20source%20code%20is%20available%0Aat%20https%3A//github.com/lcbkmm/TC-KANRecon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05705v2&entry.124074799=Read"},
{"title": "ProTracker: Probabilistic Integration for Robust and Accurate Point\n  Tracking", "author": "Tingyang Zhang and Chen Wang and Zhiyang Dou and Qingzhe Gao and Jiahui Lei and Baoquan Chen and Lingjie Liu", "abstract": "  In this paper, we propose ProTracker, a novel framework for robust and\naccurate long-term dense tracking of arbitrary points in videos. The key idea\nof our method is incorporating probabilistic integration to refine multiple\npredictions from both optical flow and semantic features for robust short-term\nand long-term tracking. Specifically, we integrate optical flow estimations in\na probabilistic manner, producing smooth and accurate trajectories by\nmaximizing the likelihood of each prediction. To effectively re-localize\nchallenging points that disappear and reappear due to occlusion, we further\nincorporate long-term feature correspondence into our flow predictions for\ncontinuous trajectory generation. Extensive experiments show that ProTracker\nachieves the state-of-the-art performance among unsupervised and\nself-supervised approaches, and even outperforms supervised methods on several\nbenchmarks. Our code and model will be publicly available upon publication.\n", "link": "http://arxiv.org/abs/2501.03220v1", "date": "2025-01-06", "relevancy": 2.2904, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5762}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProTracker%3A%20Probabilistic%20Integration%20for%20Robust%20and%20Accurate%20Point%0A%20%20Tracking&body=Title%3A%20ProTracker%3A%20Probabilistic%20Integration%20for%20Robust%20and%20Accurate%20Point%0A%20%20Tracking%0AAuthor%3A%20Tingyang%20Zhang%20and%20Chen%20Wang%20and%20Zhiyang%20Dou%20and%20Qingzhe%20Gao%20and%20Jiahui%20Lei%20and%20Baoquan%20Chen%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20ProTracker%2C%20a%20novel%20framework%20for%20robust%20and%0Aaccurate%20long-term%20dense%20tracking%20of%20arbitrary%20points%20in%20videos.%20The%20key%20idea%0Aof%20our%20method%20is%20incorporating%20probabilistic%20integration%20to%20refine%20multiple%0Apredictions%20from%20both%20optical%20flow%20and%20semantic%20features%20for%20robust%20short-term%0Aand%20long-term%20tracking.%20Specifically%2C%20we%20integrate%20optical%20flow%20estimations%20in%0Aa%20probabilistic%20manner%2C%20producing%20smooth%20and%20accurate%20trajectories%20by%0Amaximizing%20the%20likelihood%20of%20each%20prediction.%20To%20effectively%20re-localize%0Achallenging%20points%20that%20disappear%20and%20reappear%20due%20to%20occlusion%2C%20we%20further%0Aincorporate%20long-term%20feature%20correspondence%20into%20our%20flow%20predictions%20for%0Acontinuous%20trajectory%20generation.%20Extensive%20experiments%20show%20that%20ProTracker%0Aachieves%20the%20state-of-the-art%20performance%20among%20unsupervised%20and%0Aself-supervised%20approaches%2C%20and%20even%20outperforms%20supervised%20methods%20on%20several%0Abenchmarks.%20Our%20code%20and%20model%20will%20be%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProTracker%253A%2520Probabilistic%2520Integration%2520for%2520Robust%2520and%2520Accurate%2520Point%250A%2520%2520Tracking%26entry.906535625%3DTingyang%2520Zhang%2520and%2520Chen%2520Wang%2520and%2520Zhiyang%2520Dou%2520and%2520Qingzhe%2520Gao%2520and%2520Jiahui%2520Lei%2520and%2520Baoquan%2520Chen%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProTracker%252C%2520a%2520novel%2520framework%2520for%2520robust%2520and%250Aaccurate%2520long-term%2520dense%2520tracking%2520of%2520arbitrary%2520points%2520in%2520videos.%2520The%2520key%2520idea%250Aof%2520our%2520method%2520is%2520incorporating%2520probabilistic%2520integration%2520to%2520refine%2520multiple%250Apredictions%2520from%2520both%2520optical%2520flow%2520and%2520semantic%2520features%2520for%2520robust%2520short-term%250Aand%2520long-term%2520tracking.%2520Specifically%252C%2520we%2520integrate%2520optical%2520flow%2520estimations%2520in%250Aa%2520probabilistic%2520manner%252C%2520producing%2520smooth%2520and%2520accurate%2520trajectories%2520by%250Amaximizing%2520the%2520likelihood%2520of%2520each%2520prediction.%2520To%2520effectively%2520re-localize%250Achallenging%2520points%2520that%2520disappear%2520and%2520reappear%2520due%2520to%2520occlusion%252C%2520we%2520further%250Aincorporate%2520long-term%2520feature%2520correspondence%2520into%2520our%2520flow%2520predictions%2520for%250Acontinuous%2520trajectory%2520generation.%2520Extensive%2520experiments%2520show%2520that%2520ProTracker%250Aachieves%2520the%2520state-of-the-art%2520performance%2520among%2520unsupervised%2520and%250Aself-supervised%2520approaches%252C%2520and%2520even%2520outperforms%2520supervised%2520methods%2520on%2520several%250Abenchmarks.%2520Our%2520code%2520and%2520model%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProTracker%3A%20Probabilistic%20Integration%20for%20Robust%20and%20Accurate%20Point%0A%20%20Tracking&entry.906535625=Tingyang%20Zhang%20and%20Chen%20Wang%20and%20Zhiyang%20Dou%20and%20Qingzhe%20Gao%20and%20Jiahui%20Lei%20and%20Baoquan%20Chen%20and%20Lingjie%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20ProTracker%2C%20a%20novel%20framework%20for%20robust%20and%0Aaccurate%20long-term%20dense%20tracking%20of%20arbitrary%20points%20in%20videos.%20The%20key%20idea%0Aof%20our%20method%20is%20incorporating%20probabilistic%20integration%20to%20refine%20multiple%0Apredictions%20from%20both%20optical%20flow%20and%20semantic%20features%20for%20robust%20short-term%0Aand%20long-term%20tracking.%20Specifically%2C%20we%20integrate%20optical%20flow%20estimations%20in%0Aa%20probabilistic%20manner%2C%20producing%20smooth%20and%20accurate%20trajectories%20by%0Amaximizing%20the%20likelihood%20of%20each%20prediction.%20To%20effectively%20re-localize%0Achallenging%20points%20that%20disappear%20and%20reappear%20due%20to%20occlusion%2C%20we%20further%0Aincorporate%20long-term%20feature%20correspondence%20into%20our%20flow%20predictions%20for%0Acontinuous%20trajectory%20generation.%20Extensive%20experiments%20show%20that%20ProTracker%0Aachieves%20the%20state-of-the-art%20performance%20among%20unsupervised%20and%0Aself-supervised%20approaches%2C%20and%20even%20outperforms%20supervised%20methods%20on%20several%0Abenchmarks.%20Our%20code%20and%20model%20will%20be%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03220v1&entry.124074799=Read"},
{"title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the\n  Wild", "author": "Wanpeng Hu and Haodi Liu and Lin Chen and Feng Zhou and Changming Xiao and Qi Yang and Changshui Zhang", "abstract": "  Complex visual reasoning remains a key challenge today. Typically, the\nchallenge is tackled using methodologies such as Chain of Thought (COT) and\nvisual instruction tuning. However, how to organically combine these two\nmethodologies for greater success remains unexplored. Also, issues like\nhallucinations and high training cost still need to be addressed. In this work,\nwe devise an innovative multi-round training and reasoning framework suitable\nfor lightweight Multimodal Large Language Models (MLLMs). Our self-questioning\napproach heuristically guides MLLMs to focus on visual clues relevant to the\ntarget problem, reducing hallucinations and enhancing the model's ability to\ndescribe fine-grained image details. This ultimately enables the model to\nperform well in complex visual reasoning and question-answering tasks. We have\nnamed this framework Socratic Questioning(SQ). To facilitate future research,\nwe create a multimodal mini-dataset named CapQA, which includes 1k images of\nfine-grained activities, for visual instruction tuning and evaluation, our\nproposed SQ method leads to a 31.2% improvement in the hallucination score. Our\nextensive experiments on various benchmarks demonstrate SQ's remarkable\ncapabilities in heuristic self-questioning, zero-shot visual reasoning and\nhallucination mitigation. Our model and code will be publicly available.\n", "link": "http://arxiv.org/abs/2501.02964v1", "date": "2025-01-06", "relevancy": 2.2859, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Socratic%20Questioning%3A%20Learn%20to%20Self-guide%20Multimodal%20Reasoning%20in%20the%0A%20%20Wild&body=Title%3A%20Socratic%20Questioning%3A%20Learn%20to%20Self-guide%20Multimodal%20Reasoning%20in%20the%0A%20%20Wild%0AAuthor%3A%20Wanpeng%20Hu%20and%20Haodi%20Liu%20and%20Lin%20Chen%20and%20Feng%20Zhou%20and%20Changming%20Xiao%20and%20Qi%20Yang%20and%20Changshui%20Zhang%0AAbstract%3A%20%20%20Complex%20visual%20reasoning%20remains%20a%20key%20challenge%20today.%20Typically%2C%20the%0Achallenge%20is%20tackled%20using%20methodologies%20such%20as%20Chain%20of%20Thought%20%28COT%29%20and%0Avisual%20instruction%20tuning.%20However%2C%20how%20to%20organically%20combine%20these%20two%0Amethodologies%20for%20greater%20success%20remains%20unexplored.%20Also%2C%20issues%20like%0Ahallucinations%20and%20high%20training%20cost%20still%20need%20to%20be%20addressed.%20In%20this%20work%2C%0Awe%20devise%20an%20innovative%20multi-round%20training%20and%20reasoning%20framework%20suitable%0Afor%20lightweight%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Our%20self-questioning%0Aapproach%20heuristically%20guides%20MLLMs%20to%20focus%20on%20visual%20clues%20relevant%20to%20the%0Atarget%20problem%2C%20reducing%20hallucinations%20and%20enhancing%20the%20model%27s%20ability%20to%0Adescribe%20fine-grained%20image%20details.%20This%20ultimately%20enables%20the%20model%20to%0Aperform%20well%20in%20complex%20visual%20reasoning%20and%20question-answering%20tasks.%20We%20have%0Anamed%20this%20framework%20Socratic%20Questioning%28SQ%29.%20To%20facilitate%20future%20research%2C%0Awe%20create%20a%20multimodal%20mini-dataset%20named%20CapQA%2C%20which%20includes%201k%20images%20of%0Afine-grained%20activities%2C%20for%20visual%20instruction%20tuning%20and%20evaluation%2C%20our%0Aproposed%20SQ%20method%20leads%20to%20a%2031.2%25%20improvement%20in%20the%20hallucination%20score.%20Our%0Aextensive%20experiments%20on%20various%20benchmarks%20demonstrate%20SQ%27s%20remarkable%0Acapabilities%20in%20heuristic%20self-questioning%2C%20zero-shot%20visual%20reasoning%20and%0Ahallucination%20mitigation.%20Our%20model%20and%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocratic%2520Questioning%253A%2520Learn%2520to%2520Self-guide%2520Multimodal%2520Reasoning%2520in%2520the%250A%2520%2520Wild%26entry.906535625%3DWanpeng%2520Hu%2520and%2520Haodi%2520Liu%2520and%2520Lin%2520Chen%2520and%2520Feng%2520Zhou%2520and%2520Changming%2520Xiao%2520and%2520Qi%2520Yang%2520and%2520Changshui%2520Zhang%26entry.1292438233%3D%2520%2520Complex%2520visual%2520reasoning%2520remains%2520a%2520key%2520challenge%2520today.%2520Typically%252C%2520the%250Achallenge%2520is%2520tackled%2520using%2520methodologies%2520such%2520as%2520Chain%2520of%2520Thought%2520%2528COT%2529%2520and%250Avisual%2520instruction%2520tuning.%2520However%252C%2520how%2520to%2520organically%2520combine%2520these%2520two%250Amethodologies%2520for%2520greater%2520success%2520remains%2520unexplored.%2520Also%252C%2520issues%2520like%250Ahallucinations%2520and%2520high%2520training%2520cost%2520still%2520need%2520to%2520be%2520addressed.%2520In%2520this%2520work%252C%250Awe%2520devise%2520an%2520innovative%2520multi-round%2520training%2520and%2520reasoning%2520framework%2520suitable%250Afor%2520lightweight%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Our%2520self-questioning%250Aapproach%2520heuristically%2520guides%2520MLLMs%2520to%2520focus%2520on%2520visual%2520clues%2520relevant%2520to%2520the%250Atarget%2520problem%252C%2520reducing%2520hallucinations%2520and%2520enhancing%2520the%2520model%2527s%2520ability%2520to%250Adescribe%2520fine-grained%2520image%2520details.%2520This%2520ultimately%2520enables%2520the%2520model%2520to%250Aperform%2520well%2520in%2520complex%2520visual%2520reasoning%2520and%2520question-answering%2520tasks.%2520We%2520have%250Anamed%2520this%2520framework%2520Socratic%2520Questioning%2528SQ%2529.%2520To%2520facilitate%2520future%2520research%252C%250Awe%2520create%2520a%2520multimodal%2520mini-dataset%2520named%2520CapQA%252C%2520which%2520includes%25201k%2520images%2520of%250Afine-grained%2520activities%252C%2520for%2520visual%2520instruction%2520tuning%2520and%2520evaluation%252C%2520our%250Aproposed%2520SQ%2520method%2520leads%2520to%2520a%252031.2%2525%2520improvement%2520in%2520the%2520hallucination%2520score.%2520Our%250Aextensive%2520experiments%2520on%2520various%2520benchmarks%2520demonstrate%2520SQ%2527s%2520remarkable%250Acapabilities%2520in%2520heuristic%2520self-questioning%252C%2520zero-shot%2520visual%2520reasoning%2520and%250Ahallucination%2520mitigation.%2520Our%2520model%2520and%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Socratic%20Questioning%3A%20Learn%20to%20Self-guide%20Multimodal%20Reasoning%20in%20the%0A%20%20Wild&entry.906535625=Wanpeng%20Hu%20and%20Haodi%20Liu%20and%20Lin%20Chen%20and%20Feng%20Zhou%20and%20Changming%20Xiao%20and%20Qi%20Yang%20and%20Changshui%20Zhang&entry.1292438233=%20%20Complex%20visual%20reasoning%20remains%20a%20key%20challenge%20today.%20Typically%2C%20the%0Achallenge%20is%20tackled%20using%20methodologies%20such%20as%20Chain%20of%20Thought%20%28COT%29%20and%0Avisual%20instruction%20tuning.%20However%2C%20how%20to%20organically%20combine%20these%20two%0Amethodologies%20for%20greater%20success%20remains%20unexplored.%20Also%2C%20issues%20like%0Ahallucinations%20and%20high%20training%20cost%20still%20need%20to%20be%20addressed.%20In%20this%20work%2C%0Awe%20devise%20an%20innovative%20multi-round%20training%20and%20reasoning%20framework%20suitable%0Afor%20lightweight%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Our%20self-questioning%0Aapproach%20heuristically%20guides%20MLLMs%20to%20focus%20on%20visual%20clues%20relevant%20to%20the%0Atarget%20problem%2C%20reducing%20hallucinations%20and%20enhancing%20the%20model%27s%20ability%20to%0Adescribe%20fine-grained%20image%20details.%20This%20ultimately%20enables%20the%20model%20to%0Aperform%20well%20in%20complex%20visual%20reasoning%20and%20question-answering%20tasks.%20We%20have%0Anamed%20this%20framework%20Socratic%20Questioning%28SQ%29.%20To%20facilitate%20future%20research%2C%0Awe%20create%20a%20multimodal%20mini-dataset%20named%20CapQA%2C%20which%20includes%201k%20images%20of%0Afine-grained%20activities%2C%20for%20visual%20instruction%20tuning%20and%20evaluation%2C%20our%0Aproposed%20SQ%20method%20leads%20to%20a%2031.2%25%20improvement%20in%20the%20hallucination%20score.%20Our%0Aextensive%20experiments%20on%20various%20benchmarks%20demonstrate%20SQ%27s%20remarkable%0Acapabilities%20in%20heuristic%20self-questioning%2C%20zero-shot%20visual%20reasoning%20and%0Ahallucination%20mitigation.%20Our%20model%20and%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02964v1&entry.124074799=Read"},
{"title": "VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and\n  Proprioception", "author": "Zhaoliang Wan and Yonggen Ling and Senlin Yi and Lu Qi and Wangwei Lee and Minglei Lu and Sicheng Yang and Xiao Teng and Peng Lu and Xu Yang and Ming-Hsuan Yang and Hui Cheng", "abstract": "  This paper addresses the scarcity of large-scale datasets for accurate\nobject-in-hand pose estimation, which is crucial for robotic in-hand\nmanipulation within the ``Perception-Planning-Control\" paradigm. Specifically,\nwe introduce VinT-6D, the first extensive multi-modal dataset integrating\nvision, touch, and proprioception, to enhance robotic manipulation. VinT-6D\ncomprises 2 million VinT-Sim and 0.1 million VinT-Real splits, collected via\nsimulations in MuJoCo and Blender and a custom-designed real-world platform.\nThis dataset is tailored for robotic hands, offering models with whole-hand\ntactile perception and high-quality, well-aligned data. To the best of our\nknowledge, the VinT-Real is the largest considering the collection difficulties\nin the real-world environment so that it can bridge the gap of simulation to\nreal compared to the previous works. Built upon VinT-6D, we present a benchmark\nmethod that shows significant improvements in performance by fusing multi-modal\ninformation. The project is available at https://VinT-6D.github.io/.\n", "link": "http://arxiv.org/abs/2501.00510v2", "date": "2025-01-06", "relevancy": 2.2712, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5775}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5659}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VinT-6D%3A%20A%20Large-Scale%20Object-in-hand%20Dataset%20from%20Vision%2C%20Touch%20and%0A%20%20Proprioception&body=Title%3A%20VinT-6D%3A%20A%20Large-Scale%20Object-in-hand%20Dataset%20from%20Vision%2C%20Touch%20and%0A%20%20Proprioception%0AAuthor%3A%20Zhaoliang%20Wan%20and%20Yonggen%20Ling%20and%20Senlin%20Yi%20and%20Lu%20Qi%20and%20Wangwei%20Lee%20and%20Minglei%20Lu%20and%20Sicheng%20Yang%20and%20Xiao%20Teng%20and%20Peng%20Lu%20and%20Xu%20Yang%20and%20Ming-Hsuan%20Yang%20and%20Hui%20Cheng%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20scarcity%20of%20large-scale%20datasets%20for%20accurate%0Aobject-in-hand%20pose%20estimation%2C%20which%20is%20crucial%20for%20robotic%20in-hand%0Amanipulation%20within%20the%20%60%60Perception-Planning-Control%22%20paradigm.%20Specifically%2C%0Awe%20introduce%20VinT-6D%2C%20the%20first%20extensive%20multi-modal%20dataset%20integrating%0Avision%2C%20touch%2C%20and%20proprioception%2C%20to%20enhance%20robotic%20manipulation.%20VinT-6D%0Acomprises%202%20million%20VinT-Sim%20and%200.1%20million%20VinT-Real%20splits%2C%20collected%20via%0Asimulations%20in%20MuJoCo%20and%20Blender%20and%20a%20custom-designed%20real-world%20platform.%0AThis%20dataset%20is%20tailored%20for%20robotic%20hands%2C%20offering%20models%20with%20whole-hand%0Atactile%20perception%20and%20high-quality%2C%20well-aligned%20data.%20To%20the%20best%20of%20our%0Aknowledge%2C%20the%20VinT-Real%20is%20the%20largest%20considering%20the%20collection%20difficulties%0Ain%20the%20real-world%20environment%20so%20that%20it%20can%20bridge%20the%20gap%20of%20simulation%20to%0Areal%20compared%20to%20the%20previous%20works.%20Built%20upon%20VinT-6D%2C%20we%20present%20a%20benchmark%0Amethod%20that%20shows%20significant%20improvements%20in%20performance%20by%20fusing%20multi-modal%0Ainformation.%20The%20project%20is%20available%20at%20https%3A//VinT-6D.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVinT-6D%253A%2520A%2520Large-Scale%2520Object-in-hand%2520Dataset%2520from%2520Vision%252C%2520Touch%2520and%250A%2520%2520Proprioception%26entry.906535625%3DZhaoliang%2520Wan%2520and%2520Yonggen%2520Ling%2520and%2520Senlin%2520Yi%2520and%2520Lu%2520Qi%2520and%2520Wangwei%2520Lee%2520and%2520Minglei%2520Lu%2520and%2520Sicheng%2520Yang%2520and%2520Xiao%2520Teng%2520and%2520Peng%2520Lu%2520and%2520Xu%2520Yang%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Hui%2520Cheng%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520scarcity%2520of%2520large-scale%2520datasets%2520for%2520accurate%250Aobject-in-hand%2520pose%2520estimation%252C%2520which%2520is%2520crucial%2520for%2520robotic%2520in-hand%250Amanipulation%2520within%2520the%2520%2560%2560Perception-Planning-Control%2522%2520paradigm.%2520Specifically%252C%250Awe%2520introduce%2520VinT-6D%252C%2520the%2520first%2520extensive%2520multi-modal%2520dataset%2520integrating%250Avision%252C%2520touch%252C%2520and%2520proprioception%252C%2520to%2520enhance%2520robotic%2520manipulation.%2520VinT-6D%250Acomprises%25202%2520million%2520VinT-Sim%2520and%25200.1%2520million%2520VinT-Real%2520splits%252C%2520collected%2520via%250Asimulations%2520in%2520MuJoCo%2520and%2520Blender%2520and%2520a%2520custom-designed%2520real-world%2520platform.%250AThis%2520dataset%2520is%2520tailored%2520for%2520robotic%2520hands%252C%2520offering%2520models%2520with%2520whole-hand%250Atactile%2520perception%2520and%2520high-quality%252C%2520well-aligned%2520data.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520VinT-Real%2520is%2520the%2520largest%2520considering%2520the%2520collection%2520difficulties%250Ain%2520the%2520real-world%2520environment%2520so%2520that%2520it%2520can%2520bridge%2520the%2520gap%2520of%2520simulation%2520to%250Areal%2520compared%2520to%2520the%2520previous%2520works.%2520Built%2520upon%2520VinT-6D%252C%2520we%2520present%2520a%2520benchmark%250Amethod%2520that%2520shows%2520significant%2520improvements%2520in%2520performance%2520by%2520fusing%2520multi-modal%250Ainformation.%2520The%2520project%2520is%2520available%2520at%2520https%253A//VinT-6D.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VinT-6D%3A%20A%20Large-Scale%20Object-in-hand%20Dataset%20from%20Vision%2C%20Touch%20and%0A%20%20Proprioception&entry.906535625=Zhaoliang%20Wan%20and%20Yonggen%20Ling%20and%20Senlin%20Yi%20and%20Lu%20Qi%20and%20Wangwei%20Lee%20and%20Minglei%20Lu%20and%20Sicheng%20Yang%20and%20Xiao%20Teng%20and%20Peng%20Lu%20and%20Xu%20Yang%20and%20Ming-Hsuan%20Yang%20and%20Hui%20Cheng&entry.1292438233=%20%20This%20paper%20addresses%20the%20scarcity%20of%20large-scale%20datasets%20for%20accurate%0Aobject-in-hand%20pose%20estimation%2C%20which%20is%20crucial%20for%20robotic%20in-hand%0Amanipulation%20within%20the%20%60%60Perception-Planning-Control%22%20paradigm.%20Specifically%2C%0Awe%20introduce%20VinT-6D%2C%20the%20first%20extensive%20multi-modal%20dataset%20integrating%0Avision%2C%20touch%2C%20and%20proprioception%2C%20to%20enhance%20robotic%20manipulation.%20VinT-6D%0Acomprises%202%20million%20VinT-Sim%20and%200.1%20million%20VinT-Real%20splits%2C%20collected%20via%0Asimulations%20in%20MuJoCo%20and%20Blender%20and%20a%20custom-designed%20real-world%20platform.%0AThis%20dataset%20is%20tailored%20for%20robotic%20hands%2C%20offering%20models%20with%20whole-hand%0Atactile%20perception%20and%20high-quality%2C%20well-aligned%20data.%20To%20the%20best%20of%20our%0Aknowledge%2C%20the%20VinT-Real%20is%20the%20largest%20considering%20the%20collection%20difficulties%0Ain%20the%20real-world%20environment%20so%20that%20it%20can%20bridge%20the%20gap%20of%20simulation%20to%0Areal%20compared%20to%20the%20previous%20works.%20Built%20upon%20VinT-6D%2C%20we%20present%20a%20benchmark%0Amethod%20that%20shows%20significant%20improvements%20in%20performance%20by%20fusing%20multi-modal%0Ainformation.%20The%20project%20is%20available%20at%20https%3A//VinT-6D.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00510v2&entry.124074799=Read"},
{"title": "Region of Interest based Medical Image Compression", "author": "Utkarsh Prakash Srivastava and Toshiaki Fujii", "abstract": "  The vast volume of medical image data necessitates efficient compression\ntechniques to support remote healthcare services. This paper explores Region of\nInterest (ROI) coding to address the balance between compression rate and image\nquality. By leveraging UNET segmentation on the Brats 2020 dataset, we\naccurately identify tumor regions, which are critical for diagnosis. These\nregions are then subjected to High Efficiency Video Coding (HEVC) for\ncompression, enhancing compression rates while preserving essential diagnostic\ninformation. This approach ensures that critical image regions maintain their\nquality, while non-essential areas are compressed more. Our method optimizes\nstorage space and transmission bandwidth, meeting the demands of telemedicine\nand large-scale medical imaging. Through this technique, we provide a robust\nsolution that maintains the integrity of vital data and improves the efficiency\nof medical image handling.\n", "link": "http://arxiv.org/abs/2501.02895v1", "date": "2025-01-06", "relevancy": 2.246, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.44}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region%20of%20Interest%20based%20Medical%20Image%20Compression&body=Title%3A%20Region%20of%20Interest%20based%20Medical%20Image%20Compression%0AAuthor%3A%20Utkarsh%20Prakash%20Srivastava%20and%20Toshiaki%20Fujii%0AAbstract%3A%20%20%20The%20vast%20volume%20of%20medical%20image%20data%20necessitates%20efficient%20compression%0Atechniques%20to%20support%20remote%20healthcare%20services.%20This%20paper%20explores%20Region%20of%0AInterest%20%28ROI%29%20coding%20to%20address%20the%20balance%20between%20compression%20rate%20and%20image%0Aquality.%20By%20leveraging%20UNET%20segmentation%20on%20the%20Brats%202020%20dataset%2C%20we%0Aaccurately%20identify%20tumor%20regions%2C%20which%20are%20critical%20for%20diagnosis.%20These%0Aregions%20are%20then%20subjected%20to%20High%20Efficiency%20Video%20Coding%20%28HEVC%29%20for%0Acompression%2C%20enhancing%20compression%20rates%20while%20preserving%20essential%20diagnostic%0Ainformation.%20This%20approach%20ensures%20that%20critical%20image%20regions%20maintain%20their%0Aquality%2C%20while%20non-essential%20areas%20are%20compressed%20more.%20Our%20method%20optimizes%0Astorage%20space%20and%20transmission%20bandwidth%2C%20meeting%20the%20demands%20of%20telemedicine%0Aand%20large-scale%20medical%20imaging.%20Through%20this%20technique%2C%20we%20provide%20a%20robust%0Asolution%20that%20maintains%20the%20integrity%20of%20vital%20data%20and%20improves%20the%20efficiency%0Aof%20medical%20image%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion%2520of%2520Interest%2520based%2520Medical%2520Image%2520Compression%26entry.906535625%3DUtkarsh%2520Prakash%2520Srivastava%2520and%2520Toshiaki%2520Fujii%26entry.1292438233%3D%2520%2520The%2520vast%2520volume%2520of%2520medical%2520image%2520data%2520necessitates%2520efficient%2520compression%250Atechniques%2520to%2520support%2520remote%2520healthcare%2520services.%2520This%2520paper%2520explores%2520Region%2520of%250AInterest%2520%2528ROI%2529%2520coding%2520to%2520address%2520the%2520balance%2520between%2520compression%2520rate%2520and%2520image%250Aquality.%2520By%2520leveraging%2520UNET%2520segmentation%2520on%2520the%2520Brats%25202020%2520dataset%252C%2520we%250Aaccurately%2520identify%2520tumor%2520regions%252C%2520which%2520are%2520critical%2520for%2520diagnosis.%2520These%250Aregions%2520are%2520then%2520subjected%2520to%2520High%2520Efficiency%2520Video%2520Coding%2520%2528HEVC%2529%2520for%250Acompression%252C%2520enhancing%2520compression%2520rates%2520while%2520preserving%2520essential%2520diagnostic%250Ainformation.%2520This%2520approach%2520ensures%2520that%2520critical%2520image%2520regions%2520maintain%2520their%250Aquality%252C%2520while%2520non-essential%2520areas%2520are%2520compressed%2520more.%2520Our%2520method%2520optimizes%250Astorage%2520space%2520and%2520transmission%2520bandwidth%252C%2520meeting%2520the%2520demands%2520of%2520telemedicine%250Aand%2520large-scale%2520medical%2520imaging.%2520Through%2520this%2520technique%252C%2520we%2520provide%2520a%2520robust%250Asolution%2520that%2520maintains%2520the%2520integrity%2520of%2520vital%2520data%2520and%2520improves%2520the%2520efficiency%250Aof%2520medical%2520image%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region%20of%20Interest%20based%20Medical%20Image%20Compression&entry.906535625=Utkarsh%20Prakash%20Srivastava%20and%20Toshiaki%20Fujii&entry.1292438233=%20%20The%20vast%20volume%20of%20medical%20image%20data%20necessitates%20efficient%20compression%0Atechniques%20to%20support%20remote%20healthcare%20services.%20This%20paper%20explores%20Region%20of%0AInterest%20%28ROI%29%20coding%20to%20address%20the%20balance%20between%20compression%20rate%20and%20image%0Aquality.%20By%20leveraging%20UNET%20segmentation%20on%20the%20Brats%202020%20dataset%2C%20we%0Aaccurately%20identify%20tumor%20regions%2C%20which%20are%20critical%20for%20diagnosis.%20These%0Aregions%20are%20then%20subjected%20to%20High%20Efficiency%20Video%20Coding%20%28HEVC%29%20for%0Acompression%2C%20enhancing%20compression%20rates%20while%20preserving%20essential%20diagnostic%0Ainformation.%20This%20approach%20ensures%20that%20critical%20image%20regions%20maintain%20their%0Aquality%2C%20while%20non-essential%20areas%20are%20compressed%20more.%20Our%20method%20optimizes%0Astorage%20space%20and%20transmission%20bandwidth%2C%20meeting%20the%20demands%20of%20telemedicine%0Aand%20large-scale%20medical%20imaging.%20Through%20this%20technique%2C%20we%20provide%20a%20robust%0Asolution%20that%20maintains%20the%20integrity%20of%20vital%20data%20and%20improves%20the%20efficiency%0Aof%20medical%20image%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02895v1&entry.124074799=Read"},
{"title": "PiLaMIM: Toward Richer Visual Representations by Integrating Pixel and\n  Latent Masked Image Modeling", "author": "Junmyeong Lee and Eui Jun Hwang and Sukmin Cho and Jong C. Park", "abstract": "  In Masked Image Modeling (MIM), two primary methods exist: Pixel MIM and\nLatent MIM, each utilizing different reconstruction targets, raw pixels and\nlatent representations, respectively. Pixel MIM tends to capture low-level\nvisual details such as color and texture, while Latent MIM focuses on\nhigh-level semantics of an object. However, these distinct strengths of each\nmethod can lead to suboptimal performance in tasks that rely on a particular\nlevel of visual features. To address this limitation, we propose PiLaMIM, a\nunified framework that combines Pixel MIM and Latent MIM to integrate their\ncomplementary strengths. Our method uses a single encoder along with two\ndistinct decoders: one for predicting pixel values and another for latent\nrepresentations, ensuring the capture of both high-level and low-level visual\nfeatures. We further integrate the CLS token into the reconstruction process to\naggregate global context, enabling the model to capture more semantic\ninformation. Extensive experiments demonstrate that PiLaMIM outperforms key\nbaselines such as MAE, I-JEPA and BootMAE in most cases, proving its\neffectiveness in extracting richer visual representations.\n", "link": "http://arxiv.org/abs/2501.03005v1", "date": "2025-01-06", "relevancy": 2.2425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5903}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiLaMIM%3A%20Toward%20Richer%20Visual%20Representations%20by%20Integrating%20Pixel%20and%0A%20%20Latent%20Masked%20Image%20Modeling&body=Title%3A%20PiLaMIM%3A%20Toward%20Richer%20Visual%20Representations%20by%20Integrating%20Pixel%20and%0A%20%20Latent%20Masked%20Image%20Modeling%0AAuthor%3A%20Junmyeong%20Lee%20and%20Eui%20Jun%20Hwang%20and%20Sukmin%20Cho%20and%20Jong%20C.%20Park%0AAbstract%3A%20%20%20In%20Masked%20Image%20Modeling%20%28MIM%29%2C%20two%20primary%20methods%20exist%3A%20Pixel%20MIM%20and%0ALatent%20MIM%2C%20each%20utilizing%20different%20reconstruction%20targets%2C%20raw%20pixels%20and%0Alatent%20representations%2C%20respectively.%20Pixel%20MIM%20tends%20to%20capture%20low-level%0Avisual%20details%20such%20as%20color%20and%20texture%2C%20while%20Latent%20MIM%20focuses%20on%0Ahigh-level%20semantics%20of%20an%20object.%20However%2C%20these%20distinct%20strengths%20of%20each%0Amethod%20can%20lead%20to%20suboptimal%20performance%20in%20tasks%20that%20rely%20on%20a%20particular%0Alevel%20of%20visual%20features.%20To%20address%20this%20limitation%2C%20we%20propose%20PiLaMIM%2C%20a%0Aunified%20framework%20that%20combines%20Pixel%20MIM%20and%20Latent%20MIM%20to%20integrate%20their%0Acomplementary%20strengths.%20Our%20method%20uses%20a%20single%20encoder%20along%20with%20two%0Adistinct%20decoders%3A%20one%20for%20predicting%20pixel%20values%20and%20another%20for%20latent%0Arepresentations%2C%20ensuring%20the%20capture%20of%20both%20high-level%20and%20low-level%20visual%0Afeatures.%20We%20further%20integrate%20the%20CLS%20token%20into%20the%20reconstruction%20process%20to%0Aaggregate%20global%20context%2C%20enabling%20the%20model%20to%20capture%20more%20semantic%0Ainformation.%20Extensive%20experiments%20demonstrate%20that%20PiLaMIM%20outperforms%20key%0Abaselines%20such%20as%20MAE%2C%20I-JEPA%20and%20BootMAE%20in%20most%20cases%2C%20proving%20its%0Aeffectiveness%20in%20extracting%20richer%20visual%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiLaMIM%253A%2520Toward%2520Richer%2520Visual%2520Representations%2520by%2520Integrating%2520Pixel%2520and%250A%2520%2520Latent%2520Masked%2520Image%2520Modeling%26entry.906535625%3DJunmyeong%2520Lee%2520and%2520Eui%2520Jun%2520Hwang%2520and%2520Sukmin%2520Cho%2520and%2520Jong%2520C.%2520Park%26entry.1292438233%3D%2520%2520In%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%252C%2520two%2520primary%2520methods%2520exist%253A%2520Pixel%2520MIM%2520and%250ALatent%2520MIM%252C%2520each%2520utilizing%2520different%2520reconstruction%2520targets%252C%2520raw%2520pixels%2520and%250Alatent%2520representations%252C%2520respectively.%2520Pixel%2520MIM%2520tends%2520to%2520capture%2520low-level%250Avisual%2520details%2520such%2520as%2520color%2520and%2520texture%252C%2520while%2520Latent%2520MIM%2520focuses%2520on%250Ahigh-level%2520semantics%2520of%2520an%2520object.%2520However%252C%2520these%2520distinct%2520strengths%2520of%2520each%250Amethod%2520can%2520lead%2520to%2520suboptimal%2520performance%2520in%2520tasks%2520that%2520rely%2520on%2520a%2520particular%250Alevel%2520of%2520visual%2520features.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520PiLaMIM%252C%2520a%250Aunified%2520framework%2520that%2520combines%2520Pixel%2520MIM%2520and%2520Latent%2520MIM%2520to%2520integrate%2520their%250Acomplementary%2520strengths.%2520Our%2520method%2520uses%2520a%2520single%2520encoder%2520along%2520with%2520two%250Adistinct%2520decoders%253A%2520one%2520for%2520predicting%2520pixel%2520values%2520and%2520another%2520for%2520latent%250Arepresentations%252C%2520ensuring%2520the%2520capture%2520of%2520both%2520high-level%2520and%2520low-level%2520visual%250Afeatures.%2520We%2520further%2520integrate%2520the%2520CLS%2520token%2520into%2520the%2520reconstruction%2520process%2520to%250Aaggregate%2520global%2520context%252C%2520enabling%2520the%2520model%2520to%2520capture%2520more%2520semantic%250Ainformation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PiLaMIM%2520outperforms%2520key%250Abaselines%2520such%2520as%2520MAE%252C%2520I-JEPA%2520and%2520BootMAE%2520in%2520most%2520cases%252C%2520proving%2520its%250Aeffectiveness%2520in%2520extracting%2520richer%2520visual%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiLaMIM%3A%20Toward%20Richer%20Visual%20Representations%20by%20Integrating%20Pixel%20and%0A%20%20Latent%20Masked%20Image%20Modeling&entry.906535625=Junmyeong%20Lee%20and%20Eui%20Jun%20Hwang%20and%20Sukmin%20Cho%20and%20Jong%20C.%20Park&entry.1292438233=%20%20In%20Masked%20Image%20Modeling%20%28MIM%29%2C%20two%20primary%20methods%20exist%3A%20Pixel%20MIM%20and%0ALatent%20MIM%2C%20each%20utilizing%20different%20reconstruction%20targets%2C%20raw%20pixels%20and%0Alatent%20representations%2C%20respectively.%20Pixel%20MIM%20tends%20to%20capture%20low-level%0Avisual%20details%20such%20as%20color%20and%20texture%2C%20while%20Latent%20MIM%20focuses%20on%0Ahigh-level%20semantics%20of%20an%20object.%20However%2C%20these%20distinct%20strengths%20of%20each%0Amethod%20can%20lead%20to%20suboptimal%20performance%20in%20tasks%20that%20rely%20on%20a%20particular%0Alevel%20of%20visual%20features.%20To%20address%20this%20limitation%2C%20we%20propose%20PiLaMIM%2C%20a%0Aunified%20framework%20that%20combines%20Pixel%20MIM%20and%20Latent%20MIM%20to%20integrate%20their%0Acomplementary%20strengths.%20Our%20method%20uses%20a%20single%20encoder%20along%20with%20two%0Adistinct%20decoders%3A%20one%20for%20predicting%20pixel%20values%20and%20another%20for%20latent%0Arepresentations%2C%20ensuring%20the%20capture%20of%20both%20high-level%20and%20low-level%20visual%0Afeatures.%20We%20further%20integrate%20the%20CLS%20token%20into%20the%20reconstruction%20process%20to%0Aaggregate%20global%20context%2C%20enabling%20the%20model%20to%20capture%20more%20semantic%0Ainformation.%20Extensive%20experiments%20demonstrate%20that%20PiLaMIM%20outperforms%20key%0Abaselines%20such%20as%20MAE%2C%20I-JEPA%20and%20BootMAE%20in%20most%20cases%2C%20proving%20its%0Aeffectiveness%20in%20extracting%20richer%20visual%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03005v1&entry.124074799=Read"},
{"title": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning", "author": "Chao Feng and Yuanzhe Gao and Alberto Huertas Celdran and Gerome Bovet and Burkhard Stiller", "abstract": "  Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. However, model training inevitably leaves exploitable traces that can\nbe used to infer sensitive information. In Decentralized FL (DFL), the overlay\ntopology significantly influences its models' convergence, robustness, and\nsecurity. This study explores the feasibility of inferring the overlay topology\nof DFL systems based solely on model behavior, introducing a novel Topology\nInference Attack. A taxonomy of topology inference attacks is proposed,\ncategorizing them by the attacker's capabilities and knowledge. Practical\nattack strategies are developed for different scenarios, and quantitative\nexperiments are conducted to identify key factors influencing the attack\neffectiveness. Experimental results demonstrate that analyzing only the public\nmodels of individual nodes can accurately infer the DFL topology, underscoring\nthe risk of sensitive information leakage in DFL systems. This finding offers\nvaluable insights for improving privacy preservation in decentralized learning\nenvironments.\n", "link": "http://arxiv.org/abs/2501.03119v1", "date": "2025-01-06", "relevancy": 2.212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4439}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning&body=Title%3A%20From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning%0AAuthor%3A%20Chao%20Feng%20and%20Yuanzhe%20Gao%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20widely%20recognized%20as%20a%20privacy-preserving%20machine%0Alearning%20paradigm%20due%20to%20its%20model-sharing%20mechanism%20that%20avoids%20direct%20data%0Aexchange.%20However%2C%20model%20training%20inevitably%20leaves%20exploitable%20traces%20that%20can%0Abe%20used%20to%20infer%20sensitive%20information.%20In%20Decentralized%20FL%20%28DFL%29%2C%20the%20overlay%0Atopology%20significantly%20influences%20its%20models%27%20convergence%2C%20robustness%2C%20and%0Asecurity.%20This%20study%20explores%20the%20feasibility%20of%20inferring%20the%20overlay%20topology%0Aof%20DFL%20systems%20based%20solely%20on%20model%20behavior%2C%20introducing%20a%20novel%20Topology%0AInference%20Attack.%20A%20taxonomy%20of%20topology%20inference%20attacks%20is%20proposed%2C%0Acategorizing%20them%20by%20the%20attacker%27s%20capabilities%20and%20knowledge.%20Practical%0Aattack%20strategies%20are%20developed%20for%20different%20scenarios%2C%20and%20quantitative%0Aexperiments%20are%20conducted%20to%20identify%20key%20factors%20influencing%20the%20attack%0Aeffectiveness.%20Experimental%20results%20demonstrate%20that%20analyzing%20only%20the%20public%0Amodels%20of%20individual%20nodes%20can%20accurately%20infer%20the%20DFL%20topology%2C%20underscoring%0Athe%20risk%20of%20sensitive%20information%20leakage%20in%20DFL%20systems.%20This%20finding%20offers%0Avaluable%20insights%20for%20improving%20privacy%20preservation%20in%20decentralized%20learning%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Models%2520to%2520Network%2520Topologies%253A%2520A%2520Topology%2520Inference%2520Attack%2520in%250A%2520%2520Decentralized%2520Federated%2520Learning%26entry.906535625%3DChao%2520Feng%2520and%2520Yuanzhe%2520Gao%2520and%2520Alberto%2520Huertas%2520Celdran%2520and%2520Gerome%2520Bovet%2520and%2520Burkhard%2520Stiller%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520widely%2520recognized%2520as%2520a%2520privacy-preserving%2520machine%250Alearning%2520paradigm%2520due%2520to%2520its%2520model-sharing%2520mechanism%2520that%2520avoids%2520direct%2520data%250Aexchange.%2520However%252C%2520model%2520training%2520inevitably%2520leaves%2520exploitable%2520traces%2520that%2520can%250Abe%2520used%2520to%2520infer%2520sensitive%2520information.%2520In%2520Decentralized%2520FL%2520%2528DFL%2529%252C%2520the%2520overlay%250Atopology%2520significantly%2520influences%2520its%2520models%2527%2520convergence%252C%2520robustness%252C%2520and%250Asecurity.%2520This%2520study%2520explores%2520the%2520feasibility%2520of%2520inferring%2520the%2520overlay%2520topology%250Aof%2520DFL%2520systems%2520based%2520solely%2520on%2520model%2520behavior%252C%2520introducing%2520a%2520novel%2520Topology%250AInference%2520Attack.%2520A%2520taxonomy%2520of%2520topology%2520inference%2520attacks%2520is%2520proposed%252C%250Acategorizing%2520them%2520by%2520the%2520attacker%2527s%2520capabilities%2520and%2520knowledge.%2520Practical%250Aattack%2520strategies%2520are%2520developed%2520for%2520different%2520scenarios%252C%2520and%2520quantitative%250Aexperiments%2520are%2520conducted%2520to%2520identify%2520key%2520factors%2520influencing%2520the%2520attack%250Aeffectiveness.%2520Experimental%2520results%2520demonstrate%2520that%2520analyzing%2520only%2520the%2520public%250Amodels%2520of%2520individual%2520nodes%2520can%2520accurately%2520infer%2520the%2520DFL%2520topology%252C%2520underscoring%250Athe%2520risk%2520of%2520sensitive%2520information%2520leakage%2520in%2520DFL%2520systems.%2520This%2520finding%2520offers%250Avaluable%2520insights%2520for%2520improving%2520privacy%2520preservation%2520in%2520decentralized%2520learning%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Models%20to%20Network%20Topologies%3A%20A%20Topology%20Inference%20Attack%20in%0A%20%20Decentralized%20Federated%20Learning&entry.906535625=Chao%20Feng%20and%20Yuanzhe%20Gao%20and%20Alberto%20Huertas%20Celdran%20and%20Gerome%20Bovet%20and%20Burkhard%20Stiller&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20widely%20recognized%20as%20a%20privacy-preserving%20machine%0Alearning%20paradigm%20due%20to%20its%20model-sharing%20mechanism%20that%20avoids%20direct%20data%0Aexchange.%20However%2C%20model%20training%20inevitably%20leaves%20exploitable%20traces%20that%20can%0Abe%20used%20to%20infer%20sensitive%20information.%20In%20Decentralized%20FL%20%28DFL%29%2C%20the%20overlay%0Atopology%20significantly%20influences%20its%20models%27%20convergence%2C%20robustness%2C%20and%0Asecurity.%20This%20study%20explores%20the%20feasibility%20of%20inferring%20the%20overlay%20topology%0Aof%20DFL%20systems%20based%20solely%20on%20model%20behavior%2C%20introducing%20a%20novel%20Topology%0AInference%20Attack.%20A%20taxonomy%20of%20topology%20inference%20attacks%20is%20proposed%2C%0Acategorizing%20them%20by%20the%20attacker%27s%20capabilities%20and%20knowledge.%20Practical%0Aattack%20strategies%20are%20developed%20for%20different%20scenarios%2C%20and%20quantitative%0Aexperiments%20are%20conducted%20to%20identify%20key%20factors%20influencing%20the%20attack%0Aeffectiveness.%20Experimental%20results%20demonstrate%20that%20analyzing%20only%20the%20public%0Amodels%20of%20individual%20nodes%20can%20accurately%20infer%20the%20DFL%20topology%2C%20underscoring%0Athe%20risk%20of%20sensitive%20information%20leakage%20in%20DFL%20systems.%20This%20finding%20offers%0Avaluable%20insights%20for%20improving%20privacy%20preservation%20in%20decentralized%20learning%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03119v1&entry.124074799=Read"},
{"title": "Wheel-GINS: A GNSS/INS Integrated Navigation System with a Wheel-mounted\n  IMU", "author": "Yibin Wu and Jian Kuang and Xiaoji Niu and Cyrill Stachniss and Lasse Klingbeil and Heiner Kuhlmann", "abstract": "  A long-term accurate and robust localization system is essential for mobile\nrobots to operate efficiently outdoors. Recent studies have shown the\nsignificant advantages of the wheel-mounted inertial measurement unit\n(Wheel-IMU)-based dead reckoning system. However, it still drifts over extended\nperiods because of the absence of external correction signals. To achieve the\ngoal of long-term accurate localization, we propose Wheel-GINS, a Global\nNavigation Satellite System (GNSS)/inertial navigation system (INS) integrated\nnavigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position\nmeasurement with the Wheel-IMU via an extended Kalman filter to limit the\nlong-term error drift and provide continuous state estimation when the GNSS\nsignal is blocked. Considering the specificities of the GNSS/Wheel-IMU\nintegration, we conduct detailed modeling and online estimation of the\nWheel-IMU installation parameters, including the Wheel-IMU leverarm and\nmounting angle and the wheel radius error. Experimental results have shown that\nWheel-GINS outperforms the traditional GNSS/Odometer/INS integrated navigation\nsystem during GNSS outages. At the same time, Wheel-GINS can effectively\nestimate the Wheel-IMU installation parameters online and, consequently,\nimprove the localization accuracy and practicality of the system. The source\ncode of our implementation is publicly available\n(https://github.com/i2Nav-WHU/Wheel-GINS).\n", "link": "http://arxiv.org/abs/2501.03079v1", "date": "2025-01-06", "relevancy": 2.1877, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5809}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5393}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wheel-GINS%3A%20A%20GNSS/INS%20Integrated%20Navigation%20System%20with%20a%20Wheel-mounted%0A%20%20IMU&body=Title%3A%20Wheel-GINS%3A%20A%20GNSS/INS%20Integrated%20Navigation%20System%20with%20a%20Wheel-mounted%0A%20%20IMU%0AAuthor%3A%20Yibin%20Wu%20and%20Jian%20Kuang%20and%20Xiaoji%20Niu%20and%20Cyrill%20Stachniss%20and%20Lasse%20Klingbeil%20and%20Heiner%20Kuhlmann%0AAbstract%3A%20%20%20A%20long-term%20accurate%20and%20robust%20localization%20system%20is%20essential%20for%20mobile%0Arobots%20to%20operate%20efficiently%20outdoors.%20Recent%20studies%20have%20shown%20the%0Asignificant%20advantages%20of%20the%20wheel-mounted%20inertial%20measurement%20unit%0A%28Wheel-IMU%29-based%20dead%20reckoning%20system.%20However%2C%20it%20still%20drifts%20over%20extended%0Aperiods%20because%20of%20the%20absence%20of%20external%20correction%20signals.%20To%20achieve%20the%0Agoal%20of%20long-term%20accurate%20localization%2C%20we%20propose%20Wheel-GINS%2C%20a%20Global%0ANavigation%20Satellite%20System%20%28GNSS%29/inertial%20navigation%20system%20%28INS%29%20integrated%0Anavigation%20system%20using%20a%20Wheel-IMU.%20Wheel-GINS%20fuses%20the%20GNSS%20position%0Ameasurement%20with%20the%20Wheel-IMU%20via%20an%20extended%20Kalman%20filter%20to%20limit%20the%0Along-term%20error%20drift%20and%20provide%20continuous%20state%20estimation%20when%20the%20GNSS%0Asignal%20is%20blocked.%20Considering%20the%20specificities%20of%20the%20GNSS/Wheel-IMU%0Aintegration%2C%20we%20conduct%20detailed%20modeling%20and%20online%20estimation%20of%20the%0AWheel-IMU%20installation%20parameters%2C%20including%20the%20Wheel-IMU%20leverarm%20and%0Amounting%20angle%20and%20the%20wheel%20radius%20error.%20Experimental%20results%20have%20shown%20that%0AWheel-GINS%20outperforms%20the%20traditional%20GNSS/Odometer/INS%20integrated%20navigation%0Asystem%20during%20GNSS%20outages.%20At%20the%20same%20time%2C%20Wheel-GINS%20can%20effectively%0Aestimate%20the%20Wheel-IMU%20installation%20parameters%20online%20and%2C%20consequently%2C%0Aimprove%20the%20localization%20accuracy%20and%20practicality%20of%20the%20system.%20The%20source%0Acode%20of%20our%20implementation%20is%20publicly%20available%0A%28https%3A//github.com/i2Nav-WHU/Wheel-GINS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWheel-GINS%253A%2520A%2520GNSS/INS%2520Integrated%2520Navigation%2520System%2520with%2520a%2520Wheel-mounted%250A%2520%2520IMU%26entry.906535625%3DYibin%2520Wu%2520and%2520Jian%2520Kuang%2520and%2520Xiaoji%2520Niu%2520and%2520Cyrill%2520Stachniss%2520and%2520Lasse%2520Klingbeil%2520and%2520Heiner%2520Kuhlmann%26entry.1292438233%3D%2520%2520A%2520long-term%2520accurate%2520and%2520robust%2520localization%2520system%2520is%2520essential%2520for%2520mobile%250Arobots%2520to%2520operate%2520efficiently%2520outdoors.%2520Recent%2520studies%2520have%2520shown%2520the%250Asignificant%2520advantages%2520of%2520the%2520wheel-mounted%2520inertial%2520measurement%2520unit%250A%2528Wheel-IMU%2529-based%2520dead%2520reckoning%2520system.%2520However%252C%2520it%2520still%2520drifts%2520over%2520extended%250Aperiods%2520because%2520of%2520the%2520absence%2520of%2520external%2520correction%2520signals.%2520To%2520achieve%2520the%250Agoal%2520of%2520long-term%2520accurate%2520localization%252C%2520we%2520propose%2520Wheel-GINS%252C%2520a%2520Global%250ANavigation%2520Satellite%2520System%2520%2528GNSS%2529/inertial%2520navigation%2520system%2520%2528INS%2529%2520integrated%250Anavigation%2520system%2520using%2520a%2520Wheel-IMU.%2520Wheel-GINS%2520fuses%2520the%2520GNSS%2520position%250Ameasurement%2520with%2520the%2520Wheel-IMU%2520via%2520an%2520extended%2520Kalman%2520filter%2520to%2520limit%2520the%250Along-term%2520error%2520drift%2520and%2520provide%2520continuous%2520state%2520estimation%2520when%2520the%2520GNSS%250Asignal%2520is%2520blocked.%2520Considering%2520the%2520specificities%2520of%2520the%2520GNSS/Wheel-IMU%250Aintegration%252C%2520we%2520conduct%2520detailed%2520modeling%2520and%2520online%2520estimation%2520of%2520the%250AWheel-IMU%2520installation%2520parameters%252C%2520including%2520the%2520Wheel-IMU%2520leverarm%2520and%250Amounting%2520angle%2520and%2520the%2520wheel%2520radius%2520error.%2520Experimental%2520results%2520have%2520shown%2520that%250AWheel-GINS%2520outperforms%2520the%2520traditional%2520GNSS/Odometer/INS%2520integrated%2520navigation%250Asystem%2520during%2520GNSS%2520outages.%2520At%2520the%2520same%2520time%252C%2520Wheel-GINS%2520can%2520effectively%250Aestimate%2520the%2520Wheel-IMU%2520installation%2520parameters%2520online%2520and%252C%2520consequently%252C%250Aimprove%2520the%2520localization%2520accuracy%2520and%2520practicality%2520of%2520the%2520system.%2520The%2520source%250Acode%2520of%2520our%2520implementation%2520is%2520publicly%2520available%250A%2528https%253A//github.com/i2Nav-WHU/Wheel-GINS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wheel-GINS%3A%20A%20GNSS/INS%20Integrated%20Navigation%20System%20with%20a%20Wheel-mounted%0A%20%20IMU&entry.906535625=Yibin%20Wu%20and%20Jian%20Kuang%20and%20Xiaoji%20Niu%20and%20Cyrill%20Stachniss%20and%20Lasse%20Klingbeil%20and%20Heiner%20Kuhlmann&entry.1292438233=%20%20A%20long-term%20accurate%20and%20robust%20localization%20system%20is%20essential%20for%20mobile%0Arobots%20to%20operate%20efficiently%20outdoors.%20Recent%20studies%20have%20shown%20the%0Asignificant%20advantages%20of%20the%20wheel-mounted%20inertial%20measurement%20unit%0A%28Wheel-IMU%29-based%20dead%20reckoning%20system.%20However%2C%20it%20still%20drifts%20over%20extended%0Aperiods%20because%20of%20the%20absence%20of%20external%20correction%20signals.%20To%20achieve%20the%0Agoal%20of%20long-term%20accurate%20localization%2C%20we%20propose%20Wheel-GINS%2C%20a%20Global%0ANavigation%20Satellite%20System%20%28GNSS%29/inertial%20navigation%20system%20%28INS%29%20integrated%0Anavigation%20system%20using%20a%20Wheel-IMU.%20Wheel-GINS%20fuses%20the%20GNSS%20position%0Ameasurement%20with%20the%20Wheel-IMU%20via%20an%20extended%20Kalman%20filter%20to%20limit%20the%0Along-term%20error%20drift%20and%20provide%20continuous%20state%20estimation%20when%20the%20GNSS%0Asignal%20is%20blocked.%20Considering%20the%20specificities%20of%20the%20GNSS/Wheel-IMU%0Aintegration%2C%20we%20conduct%20detailed%20modeling%20and%20online%20estimation%20of%20the%0AWheel-IMU%20installation%20parameters%2C%20including%20the%20Wheel-IMU%20leverarm%20and%0Amounting%20angle%20and%20the%20wheel%20radius%20error.%20Experimental%20results%20have%20shown%20that%0AWheel-GINS%20outperforms%20the%20traditional%20GNSS/Odometer/INS%20integrated%20navigation%0Asystem%20during%20GNSS%20outages.%20At%20the%20same%20time%2C%20Wheel-GINS%20can%20effectively%0Aestimate%20the%20Wheel-IMU%20installation%20parameters%20online%20and%2C%20consequently%2C%0Aimprove%20the%20localization%20accuracy%20and%20practicality%20of%20the%20system.%20The%20source%0Acode%20of%20our%20implementation%20is%20publicly%20available%0A%28https%3A//github.com/i2Nav-WHU/Wheel-GINS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03079v1&entry.124074799=Read"},
{"title": "Scale-wise Bidirectional Alignment Network for Referring Remote Sensing\n  Image Segmentation", "author": "Kun Li and George Vosselman and Michael Ying Yang", "abstract": "  The goal of referring remote sensing image segmentation (RRSIS) is to extract\nspecific pixel-level regions within an aerial image via a natural language\nexpression. Recent advancements, particularly Transformer-based fusion designs,\nhave demonstrated remarkable progress in this domain. However, existing methods\nprimarily focus on refining visual features using language-aware guidance\nduring the cross-modal fusion stage, neglecting the complementary\nvision-to-language flow. This limitation often leads to irrelevant or\nsuboptimal representations. In addition, the diverse spatial scales of ground\nobjects in aerial images pose significant challenges to the visual perception\ncapabilities of existing models when conditioned on textual inputs. In this\npaper, we propose an innovative framework called Scale-wise Bidirectional\nAlignment Network (SBANet) to address these challenges for RRSIS. Specifically,\nwe design a Bidirectional Alignment Module (BAM) with learnable query tokens to\nselectively and effectively represent visual and linguistic features,\nemphasizing regions associated with key tokens. BAM is further enhanced with a\ndynamic feature selection block, designed to provide both macro- and\nmicro-level visual features, preserving global context and local details to\nfacilitate more effective cross-modal interaction. Furthermore, SBANet\nincorporates a text-conditioned channel and spatial aggregator to bridge the\ngap between the encoder and decoder, enhancing cross-scale information exchange\nin complex aerial scenarios. Extensive experiments demonstrate that our\nproposed method achieves superior performance in comparison to previous\nstate-of-the-art methods on the RRSIS-D and RefSegRS datasets, both\nquantitatively and qualitatively. The code will be released after publication.\n", "link": "http://arxiv.org/abs/2501.00851v2", "date": "2025-01-06", "relevancy": 2.1866, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5539}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-wise%20Bidirectional%20Alignment%20Network%20for%20Referring%20Remote%20Sensing%0A%20%20Image%20Segmentation&body=Title%3A%20Scale-wise%20Bidirectional%20Alignment%20Network%20for%20Referring%20Remote%20Sensing%0A%20%20Image%20Segmentation%0AAuthor%3A%20Kun%20Li%20and%20George%20Vosselman%20and%20Michael%20Ying%20Yang%0AAbstract%3A%20%20%20The%20goal%20of%20referring%20remote%20sensing%20image%20segmentation%20%28RRSIS%29%20is%20to%20extract%0Aspecific%20pixel-level%20regions%20within%20an%20aerial%20image%20via%20a%20natural%20language%0Aexpression.%20Recent%20advancements%2C%20particularly%20Transformer-based%20fusion%20designs%2C%0Ahave%20demonstrated%20remarkable%20progress%20in%20this%20domain.%20However%2C%20existing%20methods%0Aprimarily%20focus%20on%20refining%20visual%20features%20using%20language-aware%20guidance%0Aduring%20the%20cross-modal%20fusion%20stage%2C%20neglecting%20the%20complementary%0Avision-to-language%20flow.%20This%20limitation%20often%20leads%20to%20irrelevant%20or%0Asuboptimal%20representations.%20In%20addition%2C%20the%20diverse%20spatial%20scales%20of%20ground%0Aobjects%20in%20aerial%20images%20pose%20significant%20challenges%20to%20the%20visual%20perception%0Acapabilities%20of%20existing%20models%20when%20conditioned%20on%20textual%20inputs.%20In%20this%0Apaper%2C%20we%20propose%20an%20innovative%20framework%20called%20Scale-wise%20Bidirectional%0AAlignment%20Network%20%28SBANet%29%20to%20address%20these%20challenges%20for%20RRSIS.%20Specifically%2C%0Awe%20design%20a%20Bidirectional%20Alignment%20Module%20%28BAM%29%20with%20learnable%20query%20tokens%20to%0Aselectively%20and%20effectively%20represent%20visual%20and%20linguistic%20features%2C%0Aemphasizing%20regions%20associated%20with%20key%20tokens.%20BAM%20is%20further%20enhanced%20with%20a%0Adynamic%20feature%20selection%20block%2C%20designed%20to%20provide%20both%20macro-%20and%0Amicro-level%20visual%20features%2C%20preserving%20global%20context%20and%20local%20details%20to%0Afacilitate%20more%20effective%20cross-modal%20interaction.%20Furthermore%2C%20SBANet%0Aincorporates%20a%20text-conditioned%20channel%20and%20spatial%20aggregator%20to%20bridge%20the%0Agap%20between%20the%20encoder%20and%20decoder%2C%20enhancing%20cross-scale%20information%20exchange%0Ain%20complex%20aerial%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20method%20achieves%20superior%20performance%20in%20comparison%20to%20previous%0Astate-of-the-art%20methods%20on%20the%20RRSIS-D%20and%20RefSegRS%20datasets%2C%20both%0Aquantitatively%20and%20qualitatively.%20The%20code%20will%20be%20released%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-wise%2520Bidirectional%2520Alignment%2520Network%2520for%2520Referring%2520Remote%2520Sensing%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DKun%2520Li%2520and%2520George%2520Vosselman%2520and%2520Michael%2520Ying%2520Yang%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520referring%2520remote%2520sensing%2520image%2520segmentation%2520%2528RRSIS%2529%2520is%2520to%2520extract%250Aspecific%2520pixel-level%2520regions%2520within%2520an%2520aerial%2520image%2520via%2520a%2520natural%2520language%250Aexpression.%2520Recent%2520advancements%252C%2520particularly%2520Transformer-based%2520fusion%2520designs%252C%250Ahave%2520demonstrated%2520remarkable%2520progress%2520in%2520this%2520domain.%2520However%252C%2520existing%2520methods%250Aprimarily%2520focus%2520on%2520refining%2520visual%2520features%2520using%2520language-aware%2520guidance%250Aduring%2520the%2520cross-modal%2520fusion%2520stage%252C%2520neglecting%2520the%2520complementary%250Avision-to-language%2520flow.%2520This%2520limitation%2520often%2520leads%2520to%2520irrelevant%2520or%250Asuboptimal%2520representations.%2520In%2520addition%252C%2520the%2520diverse%2520spatial%2520scales%2520of%2520ground%250Aobjects%2520in%2520aerial%2520images%2520pose%2520significant%2520challenges%2520to%2520the%2520visual%2520perception%250Acapabilities%2520of%2520existing%2520models%2520when%2520conditioned%2520on%2520textual%2520inputs.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520innovative%2520framework%2520called%2520Scale-wise%2520Bidirectional%250AAlignment%2520Network%2520%2528SBANet%2529%2520to%2520address%2520these%2520challenges%2520for%2520RRSIS.%2520Specifically%252C%250Awe%2520design%2520a%2520Bidirectional%2520Alignment%2520Module%2520%2528BAM%2529%2520with%2520learnable%2520query%2520tokens%2520to%250Aselectively%2520and%2520effectively%2520represent%2520visual%2520and%2520linguistic%2520features%252C%250Aemphasizing%2520regions%2520associated%2520with%2520key%2520tokens.%2520BAM%2520is%2520further%2520enhanced%2520with%2520a%250Adynamic%2520feature%2520selection%2520block%252C%2520designed%2520to%2520provide%2520both%2520macro-%2520and%250Amicro-level%2520visual%2520features%252C%2520preserving%2520global%2520context%2520and%2520local%2520details%2520to%250Afacilitate%2520more%2520effective%2520cross-modal%2520interaction.%2520Furthermore%252C%2520SBANet%250Aincorporates%2520a%2520text-conditioned%2520channel%2520and%2520spatial%2520aggregator%2520to%2520bridge%2520the%250Agap%2520between%2520the%2520encoder%2520and%2520decoder%252C%2520enhancing%2520cross-scale%2520information%2520exchange%250Ain%2520complex%2520aerial%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aproposed%2520method%2520achieves%2520superior%2520performance%2520in%2520comparison%2520to%2520previous%250Astate-of-the-art%2520methods%2520on%2520the%2520RRSIS-D%2520and%2520RefSegRS%2520datasets%252C%2520both%250Aquantitatively%2520and%2520qualitatively.%2520The%2520code%2520will%2520be%2520released%2520after%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-wise%20Bidirectional%20Alignment%20Network%20for%20Referring%20Remote%20Sensing%0A%20%20Image%20Segmentation&entry.906535625=Kun%20Li%20and%20George%20Vosselman%20and%20Michael%20Ying%20Yang&entry.1292438233=%20%20The%20goal%20of%20referring%20remote%20sensing%20image%20segmentation%20%28RRSIS%29%20is%20to%20extract%0Aspecific%20pixel-level%20regions%20within%20an%20aerial%20image%20via%20a%20natural%20language%0Aexpression.%20Recent%20advancements%2C%20particularly%20Transformer-based%20fusion%20designs%2C%0Ahave%20demonstrated%20remarkable%20progress%20in%20this%20domain.%20However%2C%20existing%20methods%0Aprimarily%20focus%20on%20refining%20visual%20features%20using%20language-aware%20guidance%0Aduring%20the%20cross-modal%20fusion%20stage%2C%20neglecting%20the%20complementary%0Avision-to-language%20flow.%20This%20limitation%20often%20leads%20to%20irrelevant%20or%0Asuboptimal%20representations.%20In%20addition%2C%20the%20diverse%20spatial%20scales%20of%20ground%0Aobjects%20in%20aerial%20images%20pose%20significant%20challenges%20to%20the%20visual%20perception%0Acapabilities%20of%20existing%20models%20when%20conditioned%20on%20textual%20inputs.%20In%20this%0Apaper%2C%20we%20propose%20an%20innovative%20framework%20called%20Scale-wise%20Bidirectional%0AAlignment%20Network%20%28SBANet%29%20to%20address%20these%20challenges%20for%20RRSIS.%20Specifically%2C%0Awe%20design%20a%20Bidirectional%20Alignment%20Module%20%28BAM%29%20with%20learnable%20query%20tokens%20to%0Aselectively%20and%20effectively%20represent%20visual%20and%20linguistic%20features%2C%0Aemphasizing%20regions%20associated%20with%20key%20tokens.%20BAM%20is%20further%20enhanced%20with%20a%0Adynamic%20feature%20selection%20block%2C%20designed%20to%20provide%20both%20macro-%20and%0Amicro-level%20visual%20features%2C%20preserving%20global%20context%20and%20local%20details%20to%0Afacilitate%20more%20effective%20cross-modal%20interaction.%20Furthermore%2C%20SBANet%0Aincorporates%20a%20text-conditioned%20channel%20and%20spatial%20aggregator%20to%20bridge%20the%0Agap%20between%20the%20encoder%20and%20decoder%2C%20enhancing%20cross-scale%20information%20exchange%0Ain%20complex%20aerial%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20our%0Aproposed%20method%20achieves%20superior%20performance%20in%20comparison%20to%20previous%0Astate-of-the-art%20methods%20on%20the%20RRSIS-D%20and%20RefSegRS%20datasets%2C%20both%0Aquantitatively%20and%20qualitatively.%20The%20code%20will%20be%20released%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00851v2&entry.124074799=Read"},
{"title": "Scaling Efficient LLMs", "author": "B. N. Kausik", "abstract": "  Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale roughly eighteen fold (2) for efficient LLMs, the number\nof parameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.24} (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.\n", "link": "http://arxiv.org/abs/2402.14746v2", "date": "2025-01-06", "relevancy": 2.1843, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Efficient%20LLMs&body=Title%3A%20Scaling%20Efficient%20LLMs%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Trained%20LLMs%20are%20typically%20sparse%20in%20that%20most%20of%20the%20parameters%20are%20zero%2C%0Araising%20questions%20on%20efficiency.%20In%20response%2C%20we%20inquire%20into%20efficient%20LLMs%2C%0Ai.e.%20those%20with%20the%20fewest%20parameters%20that%20achieve%20the%20desired%20accuracy%20on%20a%0Atraining%20corpus.%20Specifically%2C%20we%20compare%20theoretical%20and%20empirical%20estimates%0Afor%20training%20loss%20to%20obtain%20upper%20and%20lower%20bounds%20on%20the%20number%20of%20unique%0Asequences%20in%20a%20natural%20training%20corpus%20as%20a%20function%20of%20its%20size.%20Our%20result%0Aimplies%20%281%29%20to%20double%20the%20number%20of%20skills%20represented%20in%20a%20training%20corpus%2C%0Athe%20corpus%20must%20scale%20roughly%20eighteen%20fold%20%282%29%20for%20efficient%20LLMs%2C%20the%20number%0Aof%20parameters%20N%20and%20the%20size%20D%20of%20a%20natural%20training%20corpus%20scale%20as%20%24N%20%5Cpropto%0AD%5E%7B0.24%7D%20%283%29%20if%20the%20number%20of%20parameters%20of%20an%20LLM%20is%20smaller%20than%20the%20number%0Aof%20unique%20sequences%20in%20the%20training%20corpus%2C%20scaling%20up%20can%20uncover%20emergent%0Askills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Efficient%2520LLMs%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Trained%2520LLMs%2520are%2520typically%2520sparse%2520in%2520that%2520most%2520of%2520the%2520parameters%2520are%2520zero%252C%250Araising%2520questions%2520on%2520efficiency.%2520In%2520response%252C%2520we%2520inquire%2520into%2520efficient%2520LLMs%252C%250Ai.e.%2520those%2520with%2520the%2520fewest%2520parameters%2520that%2520achieve%2520the%2520desired%2520accuracy%2520on%2520a%250Atraining%2520corpus.%2520Specifically%252C%2520we%2520compare%2520theoretical%2520and%2520empirical%2520estimates%250Afor%2520training%2520loss%2520to%2520obtain%2520upper%2520and%2520lower%2520bounds%2520on%2520the%2520number%2520of%2520unique%250Asequences%2520in%2520a%2520natural%2520training%2520corpus%2520as%2520a%2520function%2520of%2520its%2520size.%2520Our%2520result%250Aimplies%2520%25281%2529%2520to%2520double%2520the%2520number%2520of%2520skills%2520represented%2520in%2520a%2520training%2520corpus%252C%250Athe%2520corpus%2520must%2520scale%2520roughly%2520eighteen%2520fold%2520%25282%2529%2520for%2520efficient%2520LLMs%252C%2520the%2520number%250Aof%2520parameters%2520N%2520and%2520the%2520size%2520D%2520of%2520a%2520natural%2520training%2520corpus%2520scale%2520as%2520%2524N%2520%255Cpropto%250AD%255E%257B0.24%257D%2520%25283%2529%2520if%2520the%2520number%2520of%2520parameters%2520of%2520an%2520LLM%2520is%2520smaller%2520than%2520the%2520number%250Aof%2520unique%2520sequences%2520in%2520the%2520training%2520corpus%252C%2520scaling%2520up%2520can%2520uncover%2520emergent%250Askills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Efficient%20LLMs&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Trained%20LLMs%20are%20typically%20sparse%20in%20that%20most%20of%20the%20parameters%20are%20zero%2C%0Araising%20questions%20on%20efficiency.%20In%20response%2C%20we%20inquire%20into%20efficient%20LLMs%2C%0Ai.e.%20those%20with%20the%20fewest%20parameters%20that%20achieve%20the%20desired%20accuracy%20on%20a%0Atraining%20corpus.%20Specifically%2C%20we%20compare%20theoretical%20and%20empirical%20estimates%0Afor%20training%20loss%20to%20obtain%20upper%20and%20lower%20bounds%20on%20the%20number%20of%20unique%0Asequences%20in%20a%20natural%20training%20corpus%20as%20a%20function%20of%20its%20size.%20Our%20result%0Aimplies%20%281%29%20to%20double%20the%20number%20of%20skills%20represented%20in%20a%20training%20corpus%2C%0Athe%20corpus%20must%20scale%20roughly%20eighteen%20fold%20%282%29%20for%20efficient%20LLMs%2C%20the%20number%0Aof%20parameters%20N%20and%20the%20size%20D%20of%20a%20natural%20training%20corpus%20scale%20as%20%24N%20%5Cpropto%0AD%5E%7B0.24%7D%20%283%29%20if%20the%20number%20of%20parameters%20of%20an%20LLM%20is%20smaller%20than%20the%20number%0Aof%20unique%20sequences%20in%20the%20training%20corpus%2C%20scaling%20up%20can%20uncover%20emergent%0Askills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14746v2&entry.124074799=Read"},
{"title": "Task-Agnostic Federated Learning", "author": "Zhengtao Yao and Hong Nguyen and Ajitesh Srivastava and Jose Luis Ambite", "abstract": "  In the realm of medical imaging, leveraging large-scale datasets from various\ninstitutions is crucial for developing precise deep learning models, yet\nprivacy concerns frequently impede data sharing. federated learning (FL)\nemerges as a prominent solution for preserving privacy while facilitating\ncollaborative learning. However, its application in real-world scenarios faces\nseveral obstacles, such as task & data heterogeneity, label scarcity,\nnon-identically distributed (non-IID) data, computational vaiation, etc. In\nreal-world, medical institutions may not want to disclose their tasks to FL\nserver and generalization challenge of out-of-network institutions with un-seen\ntask want to join the on-going federated system. This study address\ntask-agnostic and generalization problem on un-seen tasks by adapting\nself-supervised FL framework. Utilizing Vision Transformer (ViT) as consensus\nfeature encoder for self-supervised pre-training, no initial labels required,\nthe framework enabling effective representation learning across diverse\ndatasets and tasks. Our extensive evaluations, using various real-world non-IID\nmedical imaging datasets, validate our approach's efficacy, retaining 90\\% of\nF1 accuracy with only 5\\% of the training data typically required for\ncentralized approaches and exhibiting superior adaptability to\nout-of-distribution task. The result indicate that federated learning\narchitecture can be a potential approach toward multi-task foundation modeling.\n", "link": "http://arxiv.org/abs/2406.17235v2", "date": "2025-01-06", "relevancy": 2.1431, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5642}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Agnostic%20Federated%20Learning&body=Title%3A%20Task-Agnostic%20Federated%20Learning%0AAuthor%3A%20Zhengtao%20Yao%20and%20Hong%20Nguyen%20and%20Ajitesh%20Srivastava%20and%20Jose%20Luis%20Ambite%0AAbstract%3A%20%20%20In%20the%20realm%20of%20medical%20imaging%2C%20leveraging%20large-scale%20datasets%20from%20various%0Ainstitutions%20is%20crucial%20for%20developing%20precise%20deep%20learning%20models%2C%20yet%0Aprivacy%20concerns%20frequently%20impede%20data%20sharing.%20federated%20learning%20%28FL%29%0Aemerges%20as%20a%20prominent%20solution%20for%20preserving%20privacy%20while%20facilitating%0Acollaborative%20learning.%20However%2C%20its%20application%20in%20real-world%20scenarios%20faces%0Aseveral%20obstacles%2C%20such%20as%20task%20%26%20data%20heterogeneity%2C%20label%20scarcity%2C%0Anon-identically%20distributed%20%28non-IID%29%20data%2C%20computational%20vaiation%2C%20etc.%20In%0Areal-world%2C%20medical%20institutions%20may%20not%20want%20to%20disclose%20their%20tasks%20to%20FL%0Aserver%20and%20generalization%20challenge%20of%20out-of-network%20institutions%20with%20un-seen%0Atask%20want%20to%20join%20the%20on-going%20federated%20system.%20This%20study%20address%0Atask-agnostic%20and%20generalization%20problem%20on%20un-seen%20tasks%20by%20adapting%0Aself-supervised%20FL%20framework.%20Utilizing%20Vision%20Transformer%20%28ViT%29%20as%20consensus%0Afeature%20encoder%20for%20self-supervised%20pre-training%2C%20no%20initial%20labels%20required%2C%0Athe%20framework%20enabling%20effective%20representation%20learning%20across%20diverse%0Adatasets%20and%20tasks.%20Our%20extensive%20evaluations%2C%20using%20various%20real-world%20non-IID%0Amedical%20imaging%20datasets%2C%20validate%20our%20approach%27s%20efficacy%2C%20retaining%2090%5C%25%20of%0AF1%20accuracy%20with%20only%205%5C%25%20of%20the%20training%20data%20typically%20required%20for%0Acentralized%20approaches%20and%20exhibiting%20superior%20adaptability%20to%0Aout-of-distribution%20task.%20The%20result%20indicate%20that%20federated%20learning%0Aarchitecture%20can%20be%20a%20potential%20approach%20toward%20multi-task%20foundation%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Agnostic%2520Federated%2520Learning%26entry.906535625%3DZhengtao%2520Yao%2520and%2520Hong%2520Nguyen%2520and%2520Ajitesh%2520Srivastava%2520and%2520Jose%2520Luis%2520Ambite%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520medical%2520imaging%252C%2520leveraging%2520large-scale%2520datasets%2520from%2520various%250Ainstitutions%2520is%2520crucial%2520for%2520developing%2520precise%2520deep%2520learning%2520models%252C%2520yet%250Aprivacy%2520concerns%2520frequently%2520impede%2520data%2520sharing.%2520federated%2520learning%2520%2528FL%2529%250Aemerges%2520as%2520a%2520prominent%2520solution%2520for%2520preserving%2520privacy%2520while%2520facilitating%250Acollaborative%2520learning.%2520However%252C%2520its%2520application%2520in%2520real-world%2520scenarios%2520faces%250Aseveral%2520obstacles%252C%2520such%2520as%2520task%2520%2526%2520data%2520heterogeneity%252C%2520label%2520scarcity%252C%250Anon-identically%2520distributed%2520%2528non-IID%2529%2520data%252C%2520computational%2520vaiation%252C%2520etc.%2520In%250Areal-world%252C%2520medical%2520institutions%2520may%2520not%2520want%2520to%2520disclose%2520their%2520tasks%2520to%2520FL%250Aserver%2520and%2520generalization%2520challenge%2520of%2520out-of-network%2520institutions%2520with%2520un-seen%250Atask%2520want%2520to%2520join%2520the%2520on-going%2520federated%2520system.%2520This%2520study%2520address%250Atask-agnostic%2520and%2520generalization%2520problem%2520on%2520un-seen%2520tasks%2520by%2520adapting%250Aself-supervised%2520FL%2520framework.%2520Utilizing%2520Vision%2520Transformer%2520%2528ViT%2529%2520as%2520consensus%250Afeature%2520encoder%2520for%2520self-supervised%2520pre-training%252C%2520no%2520initial%2520labels%2520required%252C%250Athe%2520framework%2520enabling%2520effective%2520representation%2520learning%2520across%2520diverse%250Adatasets%2520and%2520tasks.%2520Our%2520extensive%2520evaluations%252C%2520using%2520various%2520real-world%2520non-IID%250Amedical%2520imaging%2520datasets%252C%2520validate%2520our%2520approach%2527s%2520efficacy%252C%2520retaining%252090%255C%2525%2520of%250AF1%2520accuracy%2520with%2520only%25205%255C%2525%2520of%2520the%2520training%2520data%2520typically%2520required%2520for%250Acentralized%2520approaches%2520and%2520exhibiting%2520superior%2520adaptability%2520to%250Aout-of-distribution%2520task.%2520The%2520result%2520indicate%2520that%2520federated%2520learning%250Aarchitecture%2520can%2520be%2520a%2520potential%2520approach%2520toward%2520multi-task%2520foundation%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Agnostic%20Federated%20Learning&entry.906535625=Zhengtao%20Yao%20and%20Hong%20Nguyen%20and%20Ajitesh%20Srivastava%20and%20Jose%20Luis%20Ambite&entry.1292438233=%20%20In%20the%20realm%20of%20medical%20imaging%2C%20leveraging%20large-scale%20datasets%20from%20various%0Ainstitutions%20is%20crucial%20for%20developing%20precise%20deep%20learning%20models%2C%20yet%0Aprivacy%20concerns%20frequently%20impede%20data%20sharing.%20federated%20learning%20%28FL%29%0Aemerges%20as%20a%20prominent%20solution%20for%20preserving%20privacy%20while%20facilitating%0Acollaborative%20learning.%20However%2C%20its%20application%20in%20real-world%20scenarios%20faces%0Aseveral%20obstacles%2C%20such%20as%20task%20%26%20data%20heterogeneity%2C%20label%20scarcity%2C%0Anon-identically%20distributed%20%28non-IID%29%20data%2C%20computational%20vaiation%2C%20etc.%20In%0Areal-world%2C%20medical%20institutions%20may%20not%20want%20to%20disclose%20their%20tasks%20to%20FL%0Aserver%20and%20generalization%20challenge%20of%20out-of-network%20institutions%20with%20un-seen%0Atask%20want%20to%20join%20the%20on-going%20federated%20system.%20This%20study%20address%0Atask-agnostic%20and%20generalization%20problem%20on%20un-seen%20tasks%20by%20adapting%0Aself-supervised%20FL%20framework.%20Utilizing%20Vision%20Transformer%20%28ViT%29%20as%20consensus%0Afeature%20encoder%20for%20self-supervised%20pre-training%2C%20no%20initial%20labels%20required%2C%0Athe%20framework%20enabling%20effective%20representation%20learning%20across%20diverse%0Adatasets%20and%20tasks.%20Our%20extensive%20evaluations%2C%20using%20various%20real-world%20non-IID%0Amedical%20imaging%20datasets%2C%20validate%20our%20approach%27s%20efficacy%2C%20retaining%2090%5C%25%20of%0AF1%20accuracy%20with%20only%205%5C%25%20of%20the%20training%20data%20typically%20required%20for%0Acentralized%20approaches%20and%20exhibiting%20superior%20adaptability%20to%0Aout-of-distribution%20task.%20The%20result%20indicate%20that%20federated%20learning%0Aarchitecture%20can%20be%20a%20potential%20approach%20toward%20multi-task%20foundation%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17235v2&entry.124074799=Read"},
{"title": "Spiking monocular event based 6D pose estimation for space application", "author": "Jonathan Courtois and Beno\u00eet Miramond and Alain Pegatoquet", "abstract": "  With the growing interest in on On-orbit servicing (OOS) and Active Debris\nRemoval (ADR) missions, spacecraft poses estimation algorithms are being\ndeveloped using deep learning to improve the precision of this complex task and\nfind the most efficient solution. With the advances of bio-inspired low-power\nsolutions, such a spiking neural networks and event-based processing and\ncameras, and their recent work for space applications, we propose to\ninvestigate the feasibility of a fully event-based solution to improve\nevent-based pose estimation for spacecraft. In this paper, we address the first\nevent-based dataset SEENIC with real event frames captured by an event-based\ncamera on a testbed. We show the methods and results of the first event-based\nsolution for this use case, where our small spiking end-to-end network (S2E2)\nsolution achieves interesting results over 21cm position error and 14degree\nrotation error, which is the first step towards fully event-based processing\nfor embedded spacecraft pose estimation.\n", "link": "http://arxiv.org/abs/2501.02916v1", "date": "2025-01-06", "relevancy": 2.1275, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5293}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20monocular%20event%20based%206D%20pose%20estimation%20for%20space%20application&body=Title%3A%20Spiking%20monocular%20event%20based%206D%20pose%20estimation%20for%20space%20application%0AAuthor%3A%20Jonathan%20Courtois%20and%20Beno%C3%AEt%20Miramond%20and%20Alain%20Pegatoquet%0AAbstract%3A%20%20%20With%20the%20growing%20interest%20in%20on%20On-orbit%20servicing%20%28OOS%29%20and%20Active%20Debris%0ARemoval%20%28ADR%29%20missions%2C%20spacecraft%20poses%20estimation%20algorithms%20are%20being%0Adeveloped%20using%20deep%20learning%20to%20improve%20the%20precision%20of%20this%20complex%20task%20and%0Afind%20the%20most%20efficient%20solution.%20With%20the%20advances%20of%20bio-inspired%20low-power%0Asolutions%2C%20such%20a%20spiking%20neural%20networks%20and%20event-based%20processing%20and%0Acameras%2C%20and%20their%20recent%20work%20for%20space%20applications%2C%20we%20propose%20to%0Ainvestigate%20the%20feasibility%20of%20a%20fully%20event-based%20solution%20to%20improve%0Aevent-based%20pose%20estimation%20for%20spacecraft.%20In%20this%20paper%2C%20we%20address%20the%20first%0Aevent-based%20dataset%20SEENIC%20with%20real%20event%20frames%20captured%20by%20an%20event-based%0Acamera%20on%20a%20testbed.%20We%20show%20the%20methods%20and%20results%20of%20the%20first%20event-based%0Asolution%20for%20this%20use%20case%2C%20where%20our%20small%20spiking%20end-to-end%20network%20%28S2E2%29%0Asolution%20achieves%20interesting%20results%20over%2021cm%20position%20error%20and%2014degree%0Arotation%20error%2C%20which%20is%20the%20first%20step%20towards%20fully%20event-based%20processing%0Afor%20embedded%20spacecraft%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520monocular%2520event%2520based%25206D%2520pose%2520estimation%2520for%2520space%2520application%26entry.906535625%3DJonathan%2520Courtois%2520and%2520Beno%25C3%25AEt%2520Miramond%2520and%2520Alain%2520Pegatoquet%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520interest%2520in%2520on%2520On-orbit%2520servicing%2520%2528OOS%2529%2520and%2520Active%2520Debris%250ARemoval%2520%2528ADR%2529%2520missions%252C%2520spacecraft%2520poses%2520estimation%2520algorithms%2520are%2520being%250Adeveloped%2520using%2520deep%2520learning%2520to%2520improve%2520the%2520precision%2520of%2520this%2520complex%2520task%2520and%250Afind%2520the%2520most%2520efficient%2520solution.%2520With%2520the%2520advances%2520of%2520bio-inspired%2520low-power%250Asolutions%252C%2520such%2520a%2520spiking%2520neural%2520networks%2520and%2520event-based%2520processing%2520and%250Acameras%252C%2520and%2520their%2520recent%2520work%2520for%2520space%2520applications%252C%2520we%2520propose%2520to%250Ainvestigate%2520the%2520feasibility%2520of%2520a%2520fully%2520event-based%2520solution%2520to%2520improve%250Aevent-based%2520pose%2520estimation%2520for%2520spacecraft.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520first%250Aevent-based%2520dataset%2520SEENIC%2520with%2520real%2520event%2520frames%2520captured%2520by%2520an%2520event-based%250Acamera%2520on%2520a%2520testbed.%2520We%2520show%2520the%2520methods%2520and%2520results%2520of%2520the%2520first%2520event-based%250Asolution%2520for%2520this%2520use%2520case%252C%2520where%2520our%2520small%2520spiking%2520end-to-end%2520network%2520%2528S2E2%2529%250Asolution%2520achieves%2520interesting%2520results%2520over%252021cm%2520position%2520error%2520and%252014degree%250Arotation%2520error%252C%2520which%2520is%2520the%2520first%2520step%2520towards%2520fully%2520event-based%2520processing%250Afor%2520embedded%2520spacecraft%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20monocular%20event%20based%206D%20pose%20estimation%20for%20space%20application&entry.906535625=Jonathan%20Courtois%20and%20Beno%C3%AEt%20Miramond%20and%20Alain%20Pegatoquet&entry.1292438233=%20%20With%20the%20growing%20interest%20in%20on%20On-orbit%20servicing%20%28OOS%29%20and%20Active%20Debris%0ARemoval%20%28ADR%29%20missions%2C%20spacecraft%20poses%20estimation%20algorithms%20are%20being%0Adeveloped%20using%20deep%20learning%20to%20improve%20the%20precision%20of%20this%20complex%20task%20and%0Afind%20the%20most%20efficient%20solution.%20With%20the%20advances%20of%20bio-inspired%20low-power%0Asolutions%2C%20such%20a%20spiking%20neural%20networks%20and%20event-based%20processing%20and%0Acameras%2C%20and%20their%20recent%20work%20for%20space%20applications%2C%20we%20propose%20to%0Ainvestigate%20the%20feasibility%20of%20a%20fully%20event-based%20solution%20to%20improve%0Aevent-based%20pose%20estimation%20for%20spacecraft.%20In%20this%20paper%2C%20we%20address%20the%20first%0Aevent-based%20dataset%20SEENIC%20with%20real%20event%20frames%20captured%20by%20an%20event-based%0Acamera%20on%20a%20testbed.%20We%20show%20the%20methods%20and%20results%20of%20the%20first%20event-based%0Asolution%20for%20this%20use%20case%2C%20where%20our%20small%20spiking%20end-to-end%20network%20%28S2E2%29%0Asolution%20achieves%20interesting%20results%20over%2021cm%20position%20error%20and%2014degree%0Arotation%20error%2C%20which%20is%20the%20first%20step%20towards%20fully%20event-based%20processing%0Afor%20embedded%20spacecraft%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02916v1&entry.124074799=Read"},
{"title": "Unsupervised Training of Convex Regularizers using Maximum Likelihood\n  Estimation", "author": "Hong Ye Tan and Ziruo Cai and Marcelo Pereyra and Subhadip Mukherjee and Junqi Tang and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Imaging is a standard example of an inverse problem, where the task of\nreconstructing a ground truth from a noisy measurement is ill-posed. Recent\nstate-of-the-art approaches for imaging use deep learning, spearheaded by\nunrolled and end-to-end models and trained on various image datasets. However,\nmany such methods require the availability of ground truth data, which may be\nunavailable or expensive, leading to a fundamental barrier that can not be\nbypassed by choice of architecture. Unsupervised learning presents an\nalternative paradigm that bypasses this requirement, as they can be learned\ndirectly on noisy data and do not require any ground truths. A principled\nBayesian approach to unsupervised learning is to maximize the marginal\nlikelihood with respect to the given noisy measurements, which is intrinsically\nlinked to classical variational regularization. We propose an unsupervised\napproach using maximum marginal likelihood estimation to train a convex neural\nnetwork-based image regularization term directly on noisy measurements,\nimproving upon previous work in both model expressiveness and dataset size.\nExperiments demonstrate that the proposed method produces priors that are near\ncompetitive when compared to the analogous supervised training method for\nvarious image corruption operators, maintaining significantly better\ngeneralization properties when compared to end-to-end methods. Moreover, we\nprovide a detailed theoretical analysis of the convergence properties of our\nproposed algorithm.\n", "link": "http://arxiv.org/abs/2404.05445v3", "date": "2025-01-06", "relevancy": 2.122, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5049}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&body=Title%3A%20Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation%0AAuthor%3A%20Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Imaging%20is%20a%20standard%20example%20of%20an%20inverse%20problem%2C%20where%20the%20task%20of%0Areconstructing%20a%20ground%20truth%20from%20a%20noisy%20measurement%20is%20ill-posed.%20Recent%0Astate-of-the-art%20approaches%20for%20imaging%20use%20deep%20learning%2C%20spearheaded%20by%0Aunrolled%20and%20end-to-end%20models%20and%20trained%20on%20various%20image%20datasets.%20However%2C%0Amany%20such%20methods%20require%20the%20availability%20of%20ground%20truth%20data%2C%20which%20may%20be%0Aunavailable%20or%20expensive%2C%20leading%20to%20a%20fundamental%20barrier%20that%20can%20not%20be%0Abypassed%20by%20choice%20of%20architecture.%20Unsupervised%20learning%20presents%20an%0Aalternative%20paradigm%20that%20bypasses%20this%20requirement%2C%20as%20they%20can%20be%20learned%0Adirectly%20on%20noisy%20data%20and%20do%20not%20require%20any%20ground%20truths.%20A%20principled%0ABayesian%20approach%20to%20unsupervised%20learning%20is%20to%20maximize%20the%20marginal%0Alikelihood%20with%20respect%20to%20the%20given%20noisy%20measurements%2C%20which%20is%20intrinsically%0Alinked%20to%20classical%20variational%20regularization.%20We%20propose%20an%20unsupervised%0Aapproach%20using%20maximum%20marginal%20likelihood%20estimation%20to%20train%20a%20convex%20neural%0Anetwork-based%20image%20regularization%20term%20directly%20on%20noisy%20measurements%2C%0Aimproving%20upon%20previous%20work%20in%20both%20model%20expressiveness%20and%20dataset%20size.%0AExperiments%20demonstrate%20that%20the%20proposed%20method%20produces%20priors%20that%20are%20near%0Acompetitive%20when%20compared%20to%20the%20analogous%20supervised%20training%20method%20for%0Avarious%20image%20corruption%20operators%2C%20maintaining%20significantly%20better%0Ageneralization%20properties%20when%20compared%20to%20end-to-end%20methods.%20Moreover%2C%20we%0Aprovide%20a%20detailed%20theoretical%20analysis%20of%20the%20convergence%20properties%20of%20our%0Aproposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05445v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Training%2520of%2520Convex%2520Regularizers%2520using%2520Maximum%2520Likelihood%250A%2520%2520Estimation%26entry.906535625%3DHong%2520Ye%2520Tan%2520and%2520Ziruo%2520Cai%2520and%2520Marcelo%2520Pereyra%2520and%2520Subhadip%2520Mukherjee%2520and%2520Junqi%2520Tang%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520Imaging%2520is%2520a%2520standard%2520example%2520of%2520an%2520inverse%2520problem%252C%2520where%2520the%2520task%2520of%250Areconstructing%2520a%2520ground%2520truth%2520from%2520a%2520noisy%2520measurement%2520is%2520ill-posed.%2520Recent%250Astate-of-the-art%2520approaches%2520for%2520imaging%2520use%2520deep%2520learning%252C%2520spearheaded%2520by%250Aunrolled%2520and%2520end-to-end%2520models%2520and%2520trained%2520on%2520various%2520image%2520datasets.%2520However%252C%250Amany%2520such%2520methods%2520require%2520the%2520availability%2520of%2520ground%2520truth%2520data%252C%2520which%2520may%2520be%250Aunavailable%2520or%2520expensive%252C%2520leading%2520to%2520a%2520fundamental%2520barrier%2520that%2520can%2520not%2520be%250Abypassed%2520by%2520choice%2520of%2520architecture.%2520Unsupervised%2520learning%2520presents%2520an%250Aalternative%2520paradigm%2520that%2520bypasses%2520this%2520requirement%252C%2520as%2520they%2520can%2520be%2520learned%250Adirectly%2520on%2520noisy%2520data%2520and%2520do%2520not%2520require%2520any%2520ground%2520truths.%2520A%2520principled%250ABayesian%2520approach%2520to%2520unsupervised%2520learning%2520is%2520to%2520maximize%2520the%2520marginal%250Alikelihood%2520with%2520respect%2520to%2520the%2520given%2520noisy%2520measurements%252C%2520which%2520is%2520intrinsically%250Alinked%2520to%2520classical%2520variational%2520regularization.%2520We%2520propose%2520an%2520unsupervised%250Aapproach%2520using%2520maximum%2520marginal%2520likelihood%2520estimation%2520to%2520train%2520a%2520convex%2520neural%250Anetwork-based%2520image%2520regularization%2520term%2520directly%2520on%2520noisy%2520measurements%252C%250Aimproving%2520upon%2520previous%2520work%2520in%2520both%2520model%2520expressiveness%2520and%2520dataset%2520size.%250AExperiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520produces%2520priors%2520that%2520are%2520near%250Acompetitive%2520when%2520compared%2520to%2520the%2520analogous%2520supervised%2520training%2520method%2520for%250Avarious%2520image%2520corruption%2520operators%252C%2520maintaining%2520significantly%2520better%250Ageneralization%2520properties%2520when%2520compared%2520to%2520end-to-end%2520methods.%2520Moreover%252C%2520we%250Aprovide%2520a%2520detailed%2520theoretical%2520analysis%2520of%2520the%2520convergence%2520properties%2520of%2520our%250Aproposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05445v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Training%20of%20Convex%20Regularizers%20using%20Maximum%20Likelihood%0A%20%20Estimation&entry.906535625=Hong%20Ye%20Tan%20and%20Ziruo%20Cai%20and%20Marcelo%20Pereyra%20and%20Subhadip%20Mukherjee%20and%20Junqi%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Imaging%20is%20a%20standard%20example%20of%20an%20inverse%20problem%2C%20where%20the%20task%20of%0Areconstructing%20a%20ground%20truth%20from%20a%20noisy%20measurement%20is%20ill-posed.%20Recent%0Astate-of-the-art%20approaches%20for%20imaging%20use%20deep%20learning%2C%20spearheaded%20by%0Aunrolled%20and%20end-to-end%20models%20and%20trained%20on%20various%20image%20datasets.%20However%2C%0Amany%20such%20methods%20require%20the%20availability%20of%20ground%20truth%20data%2C%20which%20may%20be%0Aunavailable%20or%20expensive%2C%20leading%20to%20a%20fundamental%20barrier%20that%20can%20not%20be%0Abypassed%20by%20choice%20of%20architecture.%20Unsupervised%20learning%20presents%20an%0Aalternative%20paradigm%20that%20bypasses%20this%20requirement%2C%20as%20they%20can%20be%20learned%0Adirectly%20on%20noisy%20data%20and%20do%20not%20require%20any%20ground%20truths.%20A%20principled%0ABayesian%20approach%20to%20unsupervised%20learning%20is%20to%20maximize%20the%20marginal%0Alikelihood%20with%20respect%20to%20the%20given%20noisy%20measurements%2C%20which%20is%20intrinsically%0Alinked%20to%20classical%20variational%20regularization.%20We%20propose%20an%20unsupervised%0Aapproach%20using%20maximum%20marginal%20likelihood%20estimation%20to%20train%20a%20convex%20neural%0Anetwork-based%20image%20regularization%20term%20directly%20on%20noisy%20measurements%2C%0Aimproving%20upon%20previous%20work%20in%20both%20model%20expressiveness%20and%20dataset%20size.%0AExperiments%20demonstrate%20that%20the%20proposed%20method%20produces%20priors%20that%20are%20near%0Acompetitive%20when%20compared%20to%20the%20analogous%20supervised%20training%20method%20for%0Avarious%20image%20corruption%20operators%2C%20maintaining%20significantly%20better%0Ageneralization%20properties%20when%20compared%20to%20end-to-end%20methods.%20Moreover%2C%20we%0Aprovide%20a%20detailed%20theoretical%20analysis%20of%20the%20convergence%20properties%20of%20our%0Aproposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05445v3&entry.124074799=Read"},
{"title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine\n  Learning and Explainable AI for Academic Integrity", "author": "Ayat A. Najjar and Huthaifa I. Ashqar and Omar A. Darwish and Eman Hammad", "abstract": "  This study seeks to enhance academic integrity by providing tools to detect\nAI-generated content in student work using advanced technologies. The findings\npromote transparency and accountability, helping educators maintain ethical\nstandards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which\nhas 1000 observations, 500 of which are written by humans and the other 500\nproduced by ChatGPT. We evaluate various machine learning (ML) and deep\nlearning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).\nResults demonstrate that traditional ML algorithms, specifically XGBoost and\nRandom Forest, achieve high performance (83% and 81% accuracies respectively).\nResults also show that classifying shorter content seems to be more challenging\nthan classifying longer content. Further, using Explainable Artificial\nIntelligence (XAI) we identify discriminative features influencing the ML\nmodel's predictions, where human-written content tends to use a practical\nlanguage (e.g., use and allow). Meanwhile AI-generated text is characterized by\nmore abstract and formal terms (e.g., realm and employ). Finally, a comparative\nanalysis with GPTZero show that our narrowly focused, simple, and fine-tuned\nmodel can outperform generalized systems like GPTZero. The proposed model\nachieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when\ntasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a\ntendency to classify challenging and small-content cases as either mixed or\nunrecognized while our proposed model showed a more balanced performance across\nthe three classes.\n", "link": "http://arxiv.org/abs/2501.03203v1", "date": "2025-01-06", "relevancy": 2.1133, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5445}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5275}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20AI-Generated%20Text%20in%20Educational%20Content%3A%20Leveraging%20Machine%0A%20%20Learning%20and%20Explainable%20AI%20for%20Academic%20Integrity&body=Title%3A%20Detecting%20AI-Generated%20Text%20in%20Educational%20Content%3A%20Leveraging%20Machine%0A%20%20Learning%20and%20Explainable%20AI%20for%20Academic%20Integrity%0AAuthor%3A%20Ayat%20A.%20Najjar%20and%20Huthaifa%20I.%20Ashqar%20and%20Omar%20A.%20Darwish%20and%20Eman%20Hammad%0AAbstract%3A%20%20%20This%20study%20seeks%20to%20enhance%20academic%20integrity%20by%20providing%20tools%20to%20detect%0AAI-generated%20content%20in%20student%20work%20using%20advanced%20technologies.%20The%20findings%0Apromote%20transparency%20and%20accountability%2C%20helping%20educators%20maintain%20ethical%0Astandards%20and%20supporting%20the%20responsible%20integration%20of%20AI%20in%20education.%20A%20key%0Acontribution%20of%20this%20work%20is%20the%20generation%20of%20the%20CyberHumanAI%20dataset%2C%20which%0Ahas%201000%20observations%2C%20500%20of%20which%20are%20written%20by%20humans%20and%20the%20other%20500%0Aproduced%20by%20ChatGPT.%20We%20evaluate%20various%20machine%20learning%20%28ML%29%20and%20deep%0Alearning%20%28DL%29%20algorithms%20on%20the%20CyberHumanAI%20dataset%20comparing%20human-written%0Aand%20AI-generated%20content%20from%20Large%20Language%20Models%20%28LLMs%29%20%28i.e.%2C%20ChatGPT%29.%0AResults%20demonstrate%20that%20traditional%20ML%20algorithms%2C%20specifically%20XGBoost%20and%0ARandom%20Forest%2C%20achieve%20high%20performance%20%2883%25%20and%2081%25%20accuracies%20respectively%29.%0AResults%20also%20show%20that%20classifying%20shorter%20content%20seems%20to%20be%20more%20challenging%0Athan%20classifying%20longer%20content.%20Further%2C%20using%20Explainable%20Artificial%0AIntelligence%20%28XAI%29%20we%20identify%20discriminative%20features%20influencing%20the%20ML%0Amodel%27s%20predictions%2C%20where%20human-written%20content%20tends%20to%20use%20a%20practical%0Alanguage%20%28e.g.%2C%20use%20and%20allow%29.%20Meanwhile%20AI-generated%20text%20is%20characterized%20by%0Amore%20abstract%20and%20formal%20terms%20%28e.g.%2C%20realm%20and%20employ%29.%20Finally%2C%20a%20comparative%0Aanalysis%20with%20GPTZero%20show%20that%20our%20narrowly%20focused%2C%20simple%2C%20and%20fine-tuned%0Amodel%20can%20outperform%20generalized%20systems%20like%20GPTZero.%20The%20proposed%20model%0Aachieved%20approximately%2077.5%25%20accuracy%20compared%20to%20GPTZero%27s%2048.5%25%20accuracy%20when%0Atasked%20to%20classify%20Pure%20AI%2C%20Pure%20Human%2C%20and%20mixed%20class.%20GPTZero%20showed%20a%0Atendency%20to%20classify%20challenging%20and%20small-content%20cases%20as%20either%20mixed%20or%0Aunrecognized%20while%20our%20proposed%20model%20showed%20a%20more%20balanced%20performance%20across%0Athe%20three%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520AI-Generated%2520Text%2520in%2520Educational%2520Content%253A%2520Leveraging%2520Machine%250A%2520%2520Learning%2520and%2520Explainable%2520AI%2520for%2520Academic%2520Integrity%26entry.906535625%3DAyat%2520A.%2520Najjar%2520and%2520Huthaifa%2520I.%2520Ashqar%2520and%2520Omar%2520A.%2520Darwish%2520and%2520Eman%2520Hammad%26entry.1292438233%3D%2520%2520This%2520study%2520seeks%2520to%2520enhance%2520academic%2520integrity%2520by%2520providing%2520tools%2520to%2520detect%250AAI-generated%2520content%2520in%2520student%2520work%2520using%2520advanced%2520technologies.%2520The%2520findings%250Apromote%2520transparency%2520and%2520accountability%252C%2520helping%2520educators%2520maintain%2520ethical%250Astandards%2520and%2520supporting%2520the%2520responsible%2520integration%2520of%2520AI%2520in%2520education.%2520A%2520key%250Acontribution%2520of%2520this%2520work%2520is%2520the%2520generation%2520of%2520the%2520CyberHumanAI%2520dataset%252C%2520which%250Ahas%25201000%2520observations%252C%2520500%2520of%2520which%2520are%2520written%2520by%2520humans%2520and%2520the%2520other%2520500%250Aproduced%2520by%2520ChatGPT.%2520We%2520evaluate%2520various%2520machine%2520learning%2520%2528ML%2529%2520and%2520deep%250Alearning%2520%2528DL%2529%2520algorithms%2520on%2520the%2520CyberHumanAI%2520dataset%2520comparing%2520human-written%250Aand%2520AI-generated%2520content%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520%2528i.e.%252C%2520ChatGPT%2529.%250AResults%2520demonstrate%2520that%2520traditional%2520ML%2520algorithms%252C%2520specifically%2520XGBoost%2520and%250ARandom%2520Forest%252C%2520achieve%2520high%2520performance%2520%252883%2525%2520and%252081%2525%2520accuracies%2520respectively%2529.%250AResults%2520also%2520show%2520that%2520classifying%2520shorter%2520content%2520seems%2520to%2520be%2520more%2520challenging%250Athan%2520classifying%2520longer%2520content.%2520Further%252C%2520using%2520Explainable%2520Artificial%250AIntelligence%2520%2528XAI%2529%2520we%2520identify%2520discriminative%2520features%2520influencing%2520the%2520ML%250Amodel%2527s%2520predictions%252C%2520where%2520human-written%2520content%2520tends%2520to%2520use%2520a%2520practical%250Alanguage%2520%2528e.g.%252C%2520use%2520and%2520allow%2529.%2520Meanwhile%2520AI-generated%2520text%2520is%2520characterized%2520by%250Amore%2520abstract%2520and%2520formal%2520terms%2520%2528e.g.%252C%2520realm%2520and%2520employ%2529.%2520Finally%252C%2520a%2520comparative%250Aanalysis%2520with%2520GPTZero%2520show%2520that%2520our%2520narrowly%2520focused%252C%2520simple%252C%2520and%2520fine-tuned%250Amodel%2520can%2520outperform%2520generalized%2520systems%2520like%2520GPTZero.%2520The%2520proposed%2520model%250Aachieved%2520approximately%252077.5%2525%2520accuracy%2520compared%2520to%2520GPTZero%2527s%252048.5%2525%2520accuracy%2520when%250Atasked%2520to%2520classify%2520Pure%2520AI%252C%2520Pure%2520Human%252C%2520and%2520mixed%2520class.%2520GPTZero%2520showed%2520a%250Atendency%2520to%2520classify%2520challenging%2520and%2520small-content%2520cases%2520as%2520either%2520mixed%2520or%250Aunrecognized%2520while%2520our%2520proposed%2520model%2520showed%2520a%2520more%2520balanced%2520performance%2520across%250Athe%2520three%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20AI-Generated%20Text%20in%20Educational%20Content%3A%20Leveraging%20Machine%0A%20%20Learning%20and%20Explainable%20AI%20for%20Academic%20Integrity&entry.906535625=Ayat%20A.%20Najjar%20and%20Huthaifa%20I.%20Ashqar%20and%20Omar%20A.%20Darwish%20and%20Eman%20Hammad&entry.1292438233=%20%20This%20study%20seeks%20to%20enhance%20academic%20integrity%20by%20providing%20tools%20to%20detect%0AAI-generated%20content%20in%20student%20work%20using%20advanced%20technologies.%20The%20findings%0Apromote%20transparency%20and%20accountability%2C%20helping%20educators%20maintain%20ethical%0Astandards%20and%20supporting%20the%20responsible%20integration%20of%20AI%20in%20education.%20A%20key%0Acontribution%20of%20this%20work%20is%20the%20generation%20of%20the%20CyberHumanAI%20dataset%2C%20which%0Ahas%201000%20observations%2C%20500%20of%20which%20are%20written%20by%20humans%20and%20the%20other%20500%0Aproduced%20by%20ChatGPT.%20We%20evaluate%20various%20machine%20learning%20%28ML%29%20and%20deep%0Alearning%20%28DL%29%20algorithms%20on%20the%20CyberHumanAI%20dataset%20comparing%20human-written%0Aand%20AI-generated%20content%20from%20Large%20Language%20Models%20%28LLMs%29%20%28i.e.%2C%20ChatGPT%29.%0AResults%20demonstrate%20that%20traditional%20ML%20algorithms%2C%20specifically%20XGBoost%20and%0ARandom%20Forest%2C%20achieve%20high%20performance%20%2883%25%20and%2081%25%20accuracies%20respectively%29.%0AResults%20also%20show%20that%20classifying%20shorter%20content%20seems%20to%20be%20more%20challenging%0Athan%20classifying%20longer%20content.%20Further%2C%20using%20Explainable%20Artificial%0AIntelligence%20%28XAI%29%20we%20identify%20discriminative%20features%20influencing%20the%20ML%0Amodel%27s%20predictions%2C%20where%20human-written%20content%20tends%20to%20use%20a%20practical%0Alanguage%20%28e.g.%2C%20use%20and%20allow%29.%20Meanwhile%20AI-generated%20text%20is%20characterized%20by%0Amore%20abstract%20and%20formal%20terms%20%28e.g.%2C%20realm%20and%20employ%29.%20Finally%2C%20a%20comparative%0Aanalysis%20with%20GPTZero%20show%20that%20our%20narrowly%20focused%2C%20simple%2C%20and%20fine-tuned%0Amodel%20can%20outperform%20generalized%20systems%20like%20GPTZero.%20The%20proposed%20model%0Aachieved%20approximately%2077.5%25%20accuracy%20compared%20to%20GPTZero%27s%2048.5%25%20accuracy%20when%0Atasked%20to%20classify%20Pure%20AI%2C%20Pure%20Human%2C%20and%20mixed%20class.%20GPTZero%20showed%20a%0Atendency%20to%20classify%20challenging%20and%20small-content%20cases%20as%20either%20mixed%20or%0Aunrecognized%20while%20our%20proposed%20model%20showed%20a%20more%20balanced%20performance%20across%0Athe%20three%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03203v1&entry.124074799=Read"},
{"title": "Label-free Concept Based Multiple Instance Learning for Gigapixel\n  Histopathology", "author": "Susu Sun and Leslie Tessier and Fr\u00e9d\u00e9rique Meeuwsen and Cl\u00e9ment Grisi and Dominique van Midden and Geert Litjens and Christian F. Baumgartner", "abstract": "  Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide\nImage (WSI) analysis with only slide-level annotations. Interpretability is\ncrucial for safely deploying such algorithms in high-stakes medical domains.\nTraditional MIL methods offer explanations by highlighting salient regions.\nHowever, such spatial heatmaps provide limited insights for end users. To\naddress this, we propose a novel inherently interpretable WSI-classification\napproach that uses human-understandable pathology concepts to generate\nexplanations. Our proposed Concept MIL model leverages recent advances in\nvision-language models to directly predict pathology concepts based on image\nfeatures. The model's predictions are obtained through a linear combination of\nthe concepts identified on the top-K patches of a WSI, enabling inherent\nexplanations by tracing each concept's influence on the prediction. In contrast\nto traditional concept-based interpretable models, our approach eliminates the\nneed for costly human annotations by leveraging the vision-language model. We\nvalidate our method on two widely used pathology datasets: Camelyon16 and\nPANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9,\nputting it on par with state-of-the-art models. We further find that 87.1\\%\n(Camelyon16) and 85.3\\% (PANDA) of the top 20 patches fall within the tumor\nregion. A user study shows that the concepts identified by our model align with\nthe concepts used by pathologists, making it a promising strategy for\nhuman-interpretable WSI classification.\n", "link": "http://arxiv.org/abs/2501.02922v1", "date": "2025-01-06", "relevancy": 2.11, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-free%20Concept%20Based%20Multiple%20Instance%20Learning%20for%20Gigapixel%0A%20%20Histopathology&body=Title%3A%20Label-free%20Concept%20Based%20Multiple%20Instance%20Learning%20for%20Gigapixel%0A%20%20Histopathology%0AAuthor%3A%20Susu%20Sun%20and%20Leslie%20Tessier%20and%20Fr%C3%A9d%C3%A9rique%20Meeuwsen%20and%20Cl%C3%A9ment%20Grisi%20and%20Dominique%20van%20Midden%20and%20Geert%20Litjens%20and%20Christian%20F.%20Baumgartner%0AAbstract%3A%20%20%20Multiple%20Instance%20Learning%20%28MIL%29%20methods%20allow%20for%20gigapixel%20Whole-Slide%0AImage%20%28WSI%29%20analysis%20with%20only%20slide-level%20annotations.%20Interpretability%20is%0Acrucial%20for%20safely%20deploying%20such%20algorithms%20in%20high-stakes%20medical%20domains.%0ATraditional%20MIL%20methods%20offer%20explanations%20by%20highlighting%20salient%20regions.%0AHowever%2C%20such%20spatial%20heatmaps%20provide%20limited%20insights%20for%20end%20users.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20inherently%20interpretable%20WSI-classification%0Aapproach%20that%20uses%20human-understandable%20pathology%20concepts%20to%20generate%0Aexplanations.%20Our%20proposed%20Concept%20MIL%20model%20leverages%20recent%20advances%20in%0Avision-language%20models%20to%20directly%20predict%20pathology%20concepts%20based%20on%20image%0Afeatures.%20The%20model%27s%20predictions%20are%20obtained%20through%20a%20linear%20combination%20of%0Athe%20concepts%20identified%20on%20the%20top-K%20patches%20of%20a%20WSI%2C%20enabling%20inherent%0Aexplanations%20by%20tracing%20each%20concept%27s%20influence%20on%20the%20prediction.%20In%20contrast%0Ato%20traditional%20concept-based%20interpretable%20models%2C%20our%20approach%20eliminates%20the%0Aneed%20for%20costly%20human%20annotations%20by%20leveraging%20the%20vision-language%20model.%20We%0Avalidate%20our%20method%20on%20two%20widely%20used%20pathology%20datasets%3A%20Camelyon16%20and%0APANDA.%20On%20both%20datasets%2C%20Concept%20MIL%20achieves%20AUC%20and%20accuracy%20scores%20over%200.9%2C%0Aputting%20it%20on%20par%20with%20state-of-the-art%20models.%20We%20further%20find%20that%2087.1%5C%25%0A%28Camelyon16%29%20and%2085.3%5C%25%20%28PANDA%29%20of%20the%20top%2020%20patches%20fall%20within%20the%20tumor%0Aregion.%20A%20user%20study%20shows%20that%20the%20concepts%20identified%20by%20our%20model%20align%20with%0Athe%20concepts%20used%20by%20pathologists%2C%20making%20it%20a%20promising%20strategy%20for%0Ahuman-interpretable%20WSI%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-free%2520Concept%2520Based%2520Multiple%2520Instance%2520Learning%2520for%2520Gigapixel%250A%2520%2520Histopathology%26entry.906535625%3DSusu%2520Sun%2520and%2520Leslie%2520Tessier%2520and%2520Fr%25C3%25A9d%25C3%25A9rique%2520Meeuwsen%2520and%2520Cl%25C3%25A9ment%2520Grisi%2520and%2520Dominique%2520van%2520Midden%2520and%2520Geert%2520Litjens%2520and%2520Christian%2520F.%2520Baumgartner%26entry.1292438233%3D%2520%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520methods%2520allow%2520for%2520gigapixel%2520Whole-Slide%250AImage%2520%2528WSI%2529%2520analysis%2520with%2520only%2520slide-level%2520annotations.%2520Interpretability%2520is%250Acrucial%2520for%2520safely%2520deploying%2520such%2520algorithms%2520in%2520high-stakes%2520medical%2520domains.%250ATraditional%2520MIL%2520methods%2520offer%2520explanations%2520by%2520highlighting%2520salient%2520regions.%250AHowever%252C%2520such%2520spatial%2520heatmaps%2520provide%2520limited%2520insights%2520for%2520end%2520users.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520inherently%2520interpretable%2520WSI-classification%250Aapproach%2520that%2520uses%2520human-understandable%2520pathology%2520concepts%2520to%2520generate%250Aexplanations.%2520Our%2520proposed%2520Concept%2520MIL%2520model%2520leverages%2520recent%2520advances%2520in%250Avision-language%2520models%2520to%2520directly%2520predict%2520pathology%2520concepts%2520based%2520on%2520image%250Afeatures.%2520The%2520model%2527s%2520predictions%2520are%2520obtained%2520through%2520a%2520linear%2520combination%2520of%250Athe%2520concepts%2520identified%2520on%2520the%2520top-K%2520patches%2520of%2520a%2520WSI%252C%2520enabling%2520inherent%250Aexplanations%2520by%2520tracing%2520each%2520concept%2527s%2520influence%2520on%2520the%2520prediction.%2520In%2520contrast%250Ato%2520traditional%2520concept-based%2520interpretable%2520models%252C%2520our%2520approach%2520eliminates%2520the%250Aneed%2520for%2520costly%2520human%2520annotations%2520by%2520leveraging%2520the%2520vision-language%2520model.%2520We%250Avalidate%2520our%2520method%2520on%2520two%2520widely%2520used%2520pathology%2520datasets%253A%2520Camelyon16%2520and%250APANDA.%2520On%2520both%2520datasets%252C%2520Concept%2520MIL%2520achieves%2520AUC%2520and%2520accuracy%2520scores%2520over%25200.9%252C%250Aputting%2520it%2520on%2520par%2520with%2520state-of-the-art%2520models.%2520We%2520further%2520find%2520that%252087.1%255C%2525%250A%2528Camelyon16%2529%2520and%252085.3%255C%2525%2520%2528PANDA%2529%2520of%2520the%2520top%252020%2520patches%2520fall%2520within%2520the%2520tumor%250Aregion.%2520A%2520user%2520study%2520shows%2520that%2520the%2520concepts%2520identified%2520by%2520our%2520model%2520align%2520with%250Athe%2520concepts%2520used%2520by%2520pathologists%252C%2520making%2520it%2520a%2520promising%2520strategy%2520for%250Ahuman-interpretable%2520WSI%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-free%20Concept%20Based%20Multiple%20Instance%20Learning%20for%20Gigapixel%0A%20%20Histopathology&entry.906535625=Susu%20Sun%20and%20Leslie%20Tessier%20and%20Fr%C3%A9d%C3%A9rique%20Meeuwsen%20and%20Cl%C3%A9ment%20Grisi%20and%20Dominique%20van%20Midden%20and%20Geert%20Litjens%20and%20Christian%20F.%20Baumgartner&entry.1292438233=%20%20Multiple%20Instance%20Learning%20%28MIL%29%20methods%20allow%20for%20gigapixel%20Whole-Slide%0AImage%20%28WSI%29%20analysis%20with%20only%20slide-level%20annotations.%20Interpretability%20is%0Acrucial%20for%20safely%20deploying%20such%20algorithms%20in%20high-stakes%20medical%20domains.%0ATraditional%20MIL%20methods%20offer%20explanations%20by%20highlighting%20salient%20regions.%0AHowever%2C%20such%20spatial%20heatmaps%20provide%20limited%20insights%20for%20end%20users.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20inherently%20interpretable%20WSI-classification%0Aapproach%20that%20uses%20human-understandable%20pathology%20concepts%20to%20generate%0Aexplanations.%20Our%20proposed%20Concept%20MIL%20model%20leverages%20recent%20advances%20in%0Avision-language%20models%20to%20directly%20predict%20pathology%20concepts%20based%20on%20image%0Afeatures.%20The%20model%27s%20predictions%20are%20obtained%20through%20a%20linear%20combination%20of%0Athe%20concepts%20identified%20on%20the%20top-K%20patches%20of%20a%20WSI%2C%20enabling%20inherent%0Aexplanations%20by%20tracing%20each%20concept%27s%20influence%20on%20the%20prediction.%20In%20contrast%0Ato%20traditional%20concept-based%20interpretable%20models%2C%20our%20approach%20eliminates%20the%0Aneed%20for%20costly%20human%20annotations%20by%20leveraging%20the%20vision-language%20model.%20We%0Avalidate%20our%20method%20on%20two%20widely%20used%20pathology%20datasets%3A%20Camelyon16%20and%0APANDA.%20On%20both%20datasets%2C%20Concept%20MIL%20achieves%20AUC%20and%20accuracy%20scores%20over%200.9%2C%0Aputting%20it%20on%20par%20with%20state-of-the-art%20models.%20We%20further%20find%20that%2087.1%5C%25%0A%28Camelyon16%29%20and%2085.3%5C%25%20%28PANDA%29%20of%20the%20top%2020%20patches%20fall%20within%20the%20tumor%0Aregion.%20A%20user%20study%20shows%20that%20the%20concepts%20identified%20by%20our%20model%20align%20with%0Athe%20concepts%20used%20by%20pathologists%2C%20making%20it%20a%20promising%20strategy%20for%0Ahuman-interpretable%20WSI%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02922v1&entry.124074799=Read"},
{"title": "A Bio-Inspired Research Paradigm of Collision Perception Neurons\n  Enabling Neuro-Robotic Integration: The LGMD Case", "author": "Ziyan Qin and Jigen Peng and Shigang Yue and Qinbing Fu", "abstract": "  Compared to human vision, insect visual systems excel at rapid and precise\ncollision detection, despite relying on only tens of thousands of neurons\norganized through a few neuropils. This efficiency makes them an attractive\nmodel system for developing artificial collision-detecting systems.\nSpecifically, researchers have identified collision-selective neurons in the\nlocust's optic lobe, called lobula giant movement detectors (LGMDs), which\nrespond specifically to approaching objects. Research upon LGMD neurons began\nin the early 1970s. Initially, due to their large size, these neurons were\nidentified as motion detectors, but their role as looming detectors was\nrecognized over time. Since then, progress in neuroscience, computational\nmodeling of LGMD's visual neural circuits, and LGMD-based robotics has advanced\nin tandem, each field supporting and driving the others. Today, with a deeper\nunderstanding of LGMD neurons, LGMD-based models have significantly improved\ncollision-free navigation in mobile robots including ground and aerial robots.\nThis review highlights recent developments in LGMD research from the\nperspectives of neuroscience, computational modeling, and robotics. It\nemphasizes a biologically plausible research paradigm, where insights from\nneuroscience inform real-world applications, which would in turn validate and\nadvance neuroscience. With strong support from extensive research and growing\napplication demand, this paradigm has reached a mature stage and demonstrates\nversatility across different areas of neuroscience research, thereby enhancing\nour understanding of the interconnections between neuroscience, computational\nmodeling, and robotics. Furthermore, other motion-sensitive neurons have also\nshown promising potential for adopting this research paradigm.\n", "link": "http://arxiv.org/abs/2501.02982v1", "date": "2025-01-06", "relevancy": 2.107, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bio-Inspired%20Research%20Paradigm%20of%20Collision%20Perception%20Neurons%0A%20%20Enabling%20Neuro-Robotic%20Integration%3A%20The%20LGMD%20Case&body=Title%3A%20A%20Bio-Inspired%20Research%20Paradigm%20of%20Collision%20Perception%20Neurons%0A%20%20Enabling%20Neuro-Robotic%20Integration%3A%20The%20LGMD%20Case%0AAuthor%3A%20Ziyan%20Qin%20and%20Jigen%20Peng%20and%20Shigang%20Yue%20and%20Qinbing%20Fu%0AAbstract%3A%20%20%20Compared%20to%20human%20vision%2C%20insect%20visual%20systems%20excel%20at%20rapid%20and%20precise%0Acollision%20detection%2C%20despite%20relying%20on%20only%20tens%20of%20thousands%20of%20neurons%0Aorganized%20through%20a%20few%20neuropils.%20This%20efficiency%20makes%20them%20an%20attractive%0Amodel%20system%20for%20developing%20artificial%20collision-detecting%20systems.%0ASpecifically%2C%20researchers%20have%20identified%20collision-selective%20neurons%20in%20the%0Alocust%27s%20optic%20lobe%2C%20called%20lobula%20giant%20movement%20detectors%20%28LGMDs%29%2C%20which%0Arespond%20specifically%20to%20approaching%20objects.%20Research%20upon%20LGMD%20neurons%20began%0Ain%20the%20early%201970s.%20Initially%2C%20due%20to%20their%20large%20size%2C%20these%20neurons%20were%0Aidentified%20as%20motion%20detectors%2C%20but%20their%20role%20as%20looming%20detectors%20was%0Arecognized%20over%20time.%20Since%20then%2C%20progress%20in%20neuroscience%2C%20computational%0Amodeling%20of%20LGMD%27s%20visual%20neural%20circuits%2C%20and%20LGMD-based%20robotics%20has%20advanced%0Ain%20tandem%2C%20each%20field%20supporting%20and%20driving%20the%20others.%20Today%2C%20with%20a%20deeper%0Aunderstanding%20of%20LGMD%20neurons%2C%20LGMD-based%20models%20have%20significantly%20improved%0Acollision-free%20navigation%20in%20mobile%20robots%20including%20ground%20and%20aerial%20robots.%0AThis%20review%20highlights%20recent%20developments%20in%20LGMD%20research%20from%20the%0Aperspectives%20of%20neuroscience%2C%20computational%20modeling%2C%20and%20robotics.%20It%0Aemphasizes%20a%20biologically%20plausible%20research%20paradigm%2C%20where%20insights%20from%0Aneuroscience%20inform%20real-world%20applications%2C%20which%20would%20in%20turn%20validate%20and%0Aadvance%20neuroscience.%20With%20strong%20support%20from%20extensive%20research%20and%20growing%0Aapplication%20demand%2C%20this%20paradigm%20has%20reached%20a%20mature%20stage%20and%20demonstrates%0Aversatility%20across%20different%20areas%20of%20neuroscience%20research%2C%20thereby%20enhancing%0Aour%20understanding%20of%20the%20interconnections%20between%20neuroscience%2C%20computational%0Amodeling%2C%20and%20robotics.%20Furthermore%2C%20other%20motion-sensitive%20neurons%20have%20also%0Ashown%20promising%20potential%20for%20adopting%20this%20research%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bio-Inspired%2520Research%2520Paradigm%2520of%2520Collision%2520Perception%2520Neurons%250A%2520%2520Enabling%2520Neuro-Robotic%2520Integration%253A%2520The%2520LGMD%2520Case%26entry.906535625%3DZiyan%2520Qin%2520and%2520Jigen%2520Peng%2520and%2520Shigang%2520Yue%2520and%2520Qinbing%2520Fu%26entry.1292438233%3D%2520%2520Compared%2520to%2520human%2520vision%252C%2520insect%2520visual%2520systems%2520excel%2520at%2520rapid%2520and%2520precise%250Acollision%2520detection%252C%2520despite%2520relying%2520on%2520only%2520tens%2520of%2520thousands%2520of%2520neurons%250Aorganized%2520through%2520a%2520few%2520neuropils.%2520This%2520efficiency%2520makes%2520them%2520an%2520attractive%250Amodel%2520system%2520for%2520developing%2520artificial%2520collision-detecting%2520systems.%250ASpecifically%252C%2520researchers%2520have%2520identified%2520collision-selective%2520neurons%2520in%2520the%250Alocust%2527s%2520optic%2520lobe%252C%2520called%2520lobula%2520giant%2520movement%2520detectors%2520%2528LGMDs%2529%252C%2520which%250Arespond%2520specifically%2520to%2520approaching%2520objects.%2520Research%2520upon%2520LGMD%2520neurons%2520began%250Ain%2520the%2520early%25201970s.%2520Initially%252C%2520due%2520to%2520their%2520large%2520size%252C%2520these%2520neurons%2520were%250Aidentified%2520as%2520motion%2520detectors%252C%2520but%2520their%2520role%2520as%2520looming%2520detectors%2520was%250Arecognized%2520over%2520time.%2520Since%2520then%252C%2520progress%2520in%2520neuroscience%252C%2520computational%250Amodeling%2520of%2520LGMD%2527s%2520visual%2520neural%2520circuits%252C%2520and%2520LGMD-based%2520robotics%2520has%2520advanced%250Ain%2520tandem%252C%2520each%2520field%2520supporting%2520and%2520driving%2520the%2520others.%2520Today%252C%2520with%2520a%2520deeper%250Aunderstanding%2520of%2520LGMD%2520neurons%252C%2520LGMD-based%2520models%2520have%2520significantly%2520improved%250Acollision-free%2520navigation%2520in%2520mobile%2520robots%2520including%2520ground%2520and%2520aerial%2520robots.%250AThis%2520review%2520highlights%2520recent%2520developments%2520in%2520LGMD%2520research%2520from%2520the%250Aperspectives%2520of%2520neuroscience%252C%2520computational%2520modeling%252C%2520and%2520robotics.%2520It%250Aemphasizes%2520a%2520biologically%2520plausible%2520research%2520paradigm%252C%2520where%2520insights%2520from%250Aneuroscience%2520inform%2520real-world%2520applications%252C%2520which%2520would%2520in%2520turn%2520validate%2520and%250Aadvance%2520neuroscience.%2520With%2520strong%2520support%2520from%2520extensive%2520research%2520and%2520growing%250Aapplication%2520demand%252C%2520this%2520paradigm%2520has%2520reached%2520a%2520mature%2520stage%2520and%2520demonstrates%250Aversatility%2520across%2520different%2520areas%2520of%2520neuroscience%2520research%252C%2520thereby%2520enhancing%250Aour%2520understanding%2520of%2520the%2520interconnections%2520between%2520neuroscience%252C%2520computational%250Amodeling%252C%2520and%2520robotics.%2520Furthermore%252C%2520other%2520motion-sensitive%2520neurons%2520have%2520also%250Ashown%2520promising%2520potential%2520for%2520adopting%2520this%2520research%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bio-Inspired%20Research%20Paradigm%20of%20Collision%20Perception%20Neurons%0A%20%20Enabling%20Neuro-Robotic%20Integration%3A%20The%20LGMD%20Case&entry.906535625=Ziyan%20Qin%20and%20Jigen%20Peng%20and%20Shigang%20Yue%20and%20Qinbing%20Fu&entry.1292438233=%20%20Compared%20to%20human%20vision%2C%20insect%20visual%20systems%20excel%20at%20rapid%20and%20precise%0Acollision%20detection%2C%20despite%20relying%20on%20only%20tens%20of%20thousands%20of%20neurons%0Aorganized%20through%20a%20few%20neuropils.%20This%20efficiency%20makes%20them%20an%20attractive%0Amodel%20system%20for%20developing%20artificial%20collision-detecting%20systems.%0ASpecifically%2C%20researchers%20have%20identified%20collision-selective%20neurons%20in%20the%0Alocust%27s%20optic%20lobe%2C%20called%20lobula%20giant%20movement%20detectors%20%28LGMDs%29%2C%20which%0Arespond%20specifically%20to%20approaching%20objects.%20Research%20upon%20LGMD%20neurons%20began%0Ain%20the%20early%201970s.%20Initially%2C%20due%20to%20their%20large%20size%2C%20these%20neurons%20were%0Aidentified%20as%20motion%20detectors%2C%20but%20their%20role%20as%20looming%20detectors%20was%0Arecognized%20over%20time.%20Since%20then%2C%20progress%20in%20neuroscience%2C%20computational%0Amodeling%20of%20LGMD%27s%20visual%20neural%20circuits%2C%20and%20LGMD-based%20robotics%20has%20advanced%0Ain%20tandem%2C%20each%20field%20supporting%20and%20driving%20the%20others.%20Today%2C%20with%20a%20deeper%0Aunderstanding%20of%20LGMD%20neurons%2C%20LGMD-based%20models%20have%20significantly%20improved%0Acollision-free%20navigation%20in%20mobile%20robots%20including%20ground%20and%20aerial%20robots.%0AThis%20review%20highlights%20recent%20developments%20in%20LGMD%20research%20from%20the%0Aperspectives%20of%20neuroscience%2C%20computational%20modeling%2C%20and%20robotics.%20It%0Aemphasizes%20a%20biologically%20plausible%20research%20paradigm%2C%20where%20insights%20from%0Aneuroscience%20inform%20real-world%20applications%2C%20which%20would%20in%20turn%20validate%20and%0Aadvance%20neuroscience.%20With%20strong%20support%20from%20extensive%20research%20and%20growing%0Aapplication%20demand%2C%20this%20paradigm%20has%20reached%20a%20mature%20stage%20and%20demonstrates%0Aversatility%20across%20different%20areas%20of%20neuroscience%20research%2C%20thereby%20enhancing%0Aour%20understanding%20of%20the%20interconnections%20between%20neuroscience%2C%20computational%0Amodeling%2C%20and%20robotics.%20Furthermore%2C%20other%20motion-sensitive%20neurons%20have%20also%0Ashown%20promising%20potential%20for%20adopting%20this%20research%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02982v1&entry.124074799=Read"},
{"title": "Risk Controlled Image Retrieval", "author": "Kaiwen Cai and Chris Xiaoxuan Lu and Xingyu Zhao and Xiaowei Huang", "abstract": "  Most image retrieval research prioritizes improving predictive performance,\noften overlooking situations where the reliability of predictions is equally\nimportant. The gap between model performance and reliability requirements\nhighlights the need for a systematic approach to analyze and address the risks\nassociated with image retrieval. Uncertainty quantification technique can be\napplied to mitigate this issue by assessing uncertainty for retrieval sets, but\nit provides only a heuristic estimate of uncertainty rather than a guarantee.\nTo address these limitations, we present Risk Controlled Image Retrieval\n(RCIR), which generates retrieval sets with coverage guarantee, i.e., retrieval\nsets that are guaranteed to contain the true nearest neighbors with a\npredefined probability. RCIR can be easily integrated with existing\nuncertainty-aware image retrieval systems, agnostic to data distribution and\nmodel selection. To the best of our knowledge, this is the first work that\nprovides coverage guarantees to image retrieval. The validity and efficiency of\nRCIR are demonstrated on four real-world datasets: CAR-196, CUB-200,\nPittsburgh, and ChestX-Det.\n", "link": "http://arxiv.org/abs/2307.07336v3", "date": "2025-01-06", "relevancy": 2.0972, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5356}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk%20Controlled%20Image%20Retrieval&body=Title%3A%20Risk%20Controlled%20Image%20Retrieval%0AAuthor%3A%20Kaiwen%20Cai%20and%20Chris%20Xiaoxuan%20Lu%20and%20Xingyu%20Zhao%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Most%20image%20retrieval%20research%20prioritizes%20improving%20predictive%20performance%2C%0Aoften%20overlooking%20situations%20where%20the%20reliability%20of%20predictions%20is%20equally%0Aimportant.%20The%20gap%20between%20model%20performance%20and%20reliability%20requirements%0Ahighlights%20the%20need%20for%20a%20systematic%20approach%20to%20analyze%20and%20address%20the%20risks%0Aassociated%20with%20image%20retrieval.%20Uncertainty%20quantification%20technique%20can%20be%0Aapplied%20to%20mitigate%20this%20issue%20by%20assessing%20uncertainty%20for%20retrieval%20sets%2C%20but%0Ait%20provides%20only%20a%20heuristic%20estimate%20of%20uncertainty%20rather%20than%20a%20guarantee.%0ATo%20address%20these%20limitations%2C%20we%20present%20Risk%20Controlled%20Image%20Retrieval%0A%28RCIR%29%2C%20which%20generates%20retrieval%20sets%20with%20coverage%20guarantee%2C%20i.e.%2C%20retrieval%0Asets%20that%20are%20guaranteed%20to%20contain%20the%20true%20nearest%20neighbors%20with%20a%0Apredefined%20probability.%20RCIR%20can%20be%20easily%20integrated%20with%20existing%0Auncertainty-aware%20image%20retrieval%20systems%2C%20agnostic%20to%20data%20distribution%20and%0Amodel%20selection.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%0Aprovides%20coverage%20guarantees%20to%20image%20retrieval.%20The%20validity%20and%20efficiency%20of%0ARCIR%20are%20demonstrated%20on%20four%20real-world%20datasets%3A%20CAR-196%2C%20CUB-200%2C%0APittsburgh%2C%20and%20ChestX-Det.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.07336v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk%2520Controlled%2520Image%2520Retrieval%26entry.906535625%3DKaiwen%2520Cai%2520and%2520Chris%2520Xiaoxuan%2520Lu%2520and%2520Xingyu%2520Zhao%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Most%2520image%2520retrieval%2520research%2520prioritizes%2520improving%2520predictive%2520performance%252C%250Aoften%2520overlooking%2520situations%2520where%2520the%2520reliability%2520of%2520predictions%2520is%2520equally%250Aimportant.%2520The%2520gap%2520between%2520model%2520performance%2520and%2520reliability%2520requirements%250Ahighlights%2520the%2520need%2520for%2520a%2520systematic%2520approach%2520to%2520analyze%2520and%2520address%2520the%2520risks%250Aassociated%2520with%2520image%2520retrieval.%2520Uncertainty%2520quantification%2520technique%2520can%2520be%250Aapplied%2520to%2520mitigate%2520this%2520issue%2520by%2520assessing%2520uncertainty%2520for%2520retrieval%2520sets%252C%2520but%250Ait%2520provides%2520only%2520a%2520heuristic%2520estimate%2520of%2520uncertainty%2520rather%2520than%2520a%2520guarantee.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520present%2520Risk%2520Controlled%2520Image%2520Retrieval%250A%2528RCIR%2529%252C%2520which%2520generates%2520retrieval%2520sets%2520with%2520coverage%2520guarantee%252C%2520i.e.%252C%2520retrieval%250Asets%2520that%2520are%2520guaranteed%2520to%2520contain%2520the%2520true%2520nearest%2520neighbors%2520with%2520a%250Apredefined%2520probability.%2520RCIR%2520can%2520be%2520easily%2520integrated%2520with%2520existing%250Auncertainty-aware%2520image%2520retrieval%2520systems%252C%2520agnostic%2520to%2520data%2520distribution%2520and%250Amodel%2520selection.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%250Aprovides%2520coverage%2520guarantees%2520to%2520image%2520retrieval.%2520The%2520validity%2520and%2520efficiency%2520of%250ARCIR%2520are%2520demonstrated%2520on%2520four%2520real-world%2520datasets%253A%2520CAR-196%252C%2520CUB-200%252C%250APittsburgh%252C%2520and%2520ChestX-Det.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.07336v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20Controlled%20Image%20Retrieval&entry.906535625=Kaiwen%20Cai%20and%20Chris%20Xiaoxuan%20Lu%20and%20Xingyu%20Zhao%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Most%20image%20retrieval%20research%20prioritizes%20improving%20predictive%20performance%2C%0Aoften%20overlooking%20situations%20where%20the%20reliability%20of%20predictions%20is%20equally%0Aimportant.%20The%20gap%20between%20model%20performance%20and%20reliability%20requirements%0Ahighlights%20the%20need%20for%20a%20systematic%20approach%20to%20analyze%20and%20address%20the%20risks%0Aassociated%20with%20image%20retrieval.%20Uncertainty%20quantification%20technique%20can%20be%0Aapplied%20to%20mitigate%20this%20issue%20by%20assessing%20uncertainty%20for%20retrieval%20sets%2C%20but%0Ait%20provides%20only%20a%20heuristic%20estimate%20of%20uncertainty%20rather%20than%20a%20guarantee.%0ATo%20address%20these%20limitations%2C%20we%20present%20Risk%20Controlled%20Image%20Retrieval%0A%28RCIR%29%2C%20which%20generates%20retrieval%20sets%20with%20coverage%20guarantee%2C%20i.e.%2C%20retrieval%0Asets%20that%20are%20guaranteed%20to%20contain%20the%20true%20nearest%20neighbors%20with%20a%0Apredefined%20probability.%20RCIR%20can%20be%20easily%20integrated%20with%20existing%0Auncertainty-aware%20image%20retrieval%20systems%2C%20agnostic%20to%20data%20distribution%20and%0Amodel%20selection.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%0Aprovides%20coverage%20guarantees%20to%20image%20retrieval.%20The%20validity%20and%20efficiency%20of%0ARCIR%20are%20demonstrated%20on%20four%20real-world%20datasets%3A%20CAR-196%2C%20CUB-200%2C%0APittsburgh%2C%20and%20ChestX-Det.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07336v3&entry.124074799=Read"},
{"title": "A Novel Structure-Agnostic Multi-Objective Approach for Weight-Sharing\n  Compression in Deep Neural Networks", "author": "Rasa Khosrowshahli and Shahryar Rahnamayan and Beatrice Ombuki-Berman", "abstract": "  Deep neural networks suffer from storing millions and billions of weights in\nmemory post-training, making challenging memory-intensive models to deploy on\nembedded devices. The weight-sharing technique is one of the popular\ncompression approaches that use fewer weight values and share across specific\nconnections in the network. In this paper, we propose a multi-objective\nevolutionary algorithm (MOEA) based compression framework independent of neural\nnetwork architecture, dimension, task, and dataset. We use uniformly sized bins\nto quantize network weights into a single codebook (lookup table) for efficient\nweight representation. Using MOEA, we search for Pareto optimal $k$ bins by\noptimizing two objectives. Then, we apply the iterative merge technique to\nnon-dominated Pareto frontier solutions by combining neighboring bins without\ndegrading performance to decrease the number of bins and increase the\ncompression ratio. Our approach is model- and layer-independent, meaning the\nweights are mixed in the clusters from any layer, and the uniform quantization\nmethod used in this work has $O(N)$ complexity instead of non-uniform\nquantization methods such as k-means with $O(Nkt)$ complexity. In addition, we\nuse the center of clusters as the shared weight values instead of retraining\nshared weights, which is computationally expensive. The advantage of using\nevolutionary multi-objective optimization is that it can obtain non-dominated\nPareto frontier solutions with respect to performance and shared weights. The\nexperimental results show that we can reduce the neural network memory by\n$13.72 \\sim14.98 \\times$ on CIFAR-10, $11.61 \\sim 12.99\\times$ on CIFAR-100,\nand $7.44 \\sim 8.58\\times$ on ImageNet showcasing the effectiveness of the\nproposed deep neural network compression framework.\n", "link": "http://arxiv.org/abs/2501.03095v1", "date": "2025-01-06", "relevancy": 2.0811, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5354}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Structure-Agnostic%20Multi-Objective%20Approach%20for%20Weight-Sharing%0A%20%20Compression%20in%20Deep%20Neural%20Networks&body=Title%3A%20A%20Novel%20Structure-Agnostic%20Multi-Objective%20Approach%20for%20Weight-Sharing%0A%20%20Compression%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Rasa%20Khosrowshahli%20and%20Shahryar%20Rahnamayan%20and%20Beatrice%20Ombuki-Berman%0AAbstract%3A%20%20%20Deep%20neural%20networks%20suffer%20from%20storing%20millions%20and%20billions%20of%20weights%20in%0Amemory%20post-training%2C%20making%20challenging%20memory-intensive%20models%20to%20deploy%20on%0Aembedded%20devices.%20The%20weight-sharing%20technique%20is%20one%20of%20the%20popular%0Acompression%20approaches%20that%20use%20fewer%20weight%20values%20and%20share%20across%20specific%0Aconnections%20in%20the%20network.%20In%20this%20paper%2C%20we%20propose%20a%20multi-objective%0Aevolutionary%20algorithm%20%28MOEA%29%20based%20compression%20framework%20independent%20of%20neural%0Anetwork%20architecture%2C%20dimension%2C%20task%2C%20and%20dataset.%20We%20use%20uniformly%20sized%20bins%0Ato%20quantize%20network%20weights%20into%20a%20single%20codebook%20%28lookup%20table%29%20for%20efficient%0Aweight%20representation.%20Using%20MOEA%2C%20we%20search%20for%20Pareto%20optimal%20%24k%24%20bins%20by%0Aoptimizing%20two%20objectives.%20Then%2C%20we%20apply%20the%20iterative%20merge%20technique%20to%0Anon-dominated%20Pareto%20frontier%20solutions%20by%20combining%20neighboring%20bins%20without%0Adegrading%20performance%20to%20decrease%20the%20number%20of%20bins%20and%20increase%20the%0Acompression%20ratio.%20Our%20approach%20is%20model-%20and%20layer-independent%2C%20meaning%20the%0Aweights%20are%20mixed%20in%20the%20clusters%20from%20any%20layer%2C%20and%20the%20uniform%20quantization%0Amethod%20used%20in%20this%20work%20has%20%24O%28N%29%24%20complexity%20instead%20of%20non-uniform%0Aquantization%20methods%20such%20as%20k-means%20with%20%24O%28Nkt%29%24%20complexity.%20In%20addition%2C%20we%0Ause%20the%20center%20of%20clusters%20as%20the%20shared%20weight%20values%20instead%20of%20retraining%0Ashared%20weights%2C%20which%20is%20computationally%20expensive.%20The%20advantage%20of%20using%0Aevolutionary%20multi-objective%20optimization%20is%20that%20it%20can%20obtain%20non-dominated%0APareto%20frontier%20solutions%20with%20respect%20to%20performance%20and%20shared%20weights.%20The%0Aexperimental%20results%20show%20that%20we%20can%20reduce%20the%20neural%20network%20memory%20by%0A%2413.72%20%5Csim14.98%20%5Ctimes%24%20on%20CIFAR-10%2C%20%2411.61%20%5Csim%2012.99%5Ctimes%24%20on%20CIFAR-100%2C%0Aand%20%247.44%20%5Csim%208.58%5Ctimes%24%20on%20ImageNet%20showcasing%20the%20effectiveness%20of%20the%0Aproposed%20deep%20neural%20network%20compression%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Structure-Agnostic%2520Multi-Objective%2520Approach%2520for%2520Weight-Sharing%250A%2520%2520Compression%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DRasa%2520Khosrowshahli%2520and%2520Shahryar%2520Rahnamayan%2520and%2520Beatrice%2520Ombuki-Berman%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520suffer%2520from%2520storing%2520millions%2520and%2520billions%2520of%2520weights%2520in%250Amemory%2520post-training%252C%2520making%2520challenging%2520memory-intensive%2520models%2520to%2520deploy%2520on%250Aembedded%2520devices.%2520The%2520weight-sharing%2520technique%2520is%2520one%2520of%2520the%2520popular%250Acompression%2520approaches%2520that%2520use%2520fewer%2520weight%2520values%2520and%2520share%2520across%2520specific%250Aconnections%2520in%2520the%2520network.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-objective%250Aevolutionary%2520algorithm%2520%2528MOEA%2529%2520based%2520compression%2520framework%2520independent%2520of%2520neural%250Anetwork%2520architecture%252C%2520dimension%252C%2520task%252C%2520and%2520dataset.%2520We%2520use%2520uniformly%2520sized%2520bins%250Ato%2520quantize%2520network%2520weights%2520into%2520a%2520single%2520codebook%2520%2528lookup%2520table%2529%2520for%2520efficient%250Aweight%2520representation.%2520Using%2520MOEA%252C%2520we%2520search%2520for%2520Pareto%2520optimal%2520%2524k%2524%2520bins%2520by%250Aoptimizing%2520two%2520objectives.%2520Then%252C%2520we%2520apply%2520the%2520iterative%2520merge%2520technique%2520to%250Anon-dominated%2520Pareto%2520frontier%2520solutions%2520by%2520combining%2520neighboring%2520bins%2520without%250Adegrading%2520performance%2520to%2520decrease%2520the%2520number%2520of%2520bins%2520and%2520increase%2520the%250Acompression%2520ratio.%2520Our%2520approach%2520is%2520model-%2520and%2520layer-independent%252C%2520meaning%2520the%250Aweights%2520are%2520mixed%2520in%2520the%2520clusters%2520from%2520any%2520layer%252C%2520and%2520the%2520uniform%2520quantization%250Amethod%2520used%2520in%2520this%2520work%2520has%2520%2524O%2528N%2529%2524%2520complexity%2520instead%2520of%2520non-uniform%250Aquantization%2520methods%2520such%2520as%2520k-means%2520with%2520%2524O%2528Nkt%2529%2524%2520complexity.%2520In%2520addition%252C%2520we%250Ause%2520the%2520center%2520of%2520clusters%2520as%2520the%2520shared%2520weight%2520values%2520instead%2520of%2520retraining%250Ashared%2520weights%252C%2520which%2520is%2520computationally%2520expensive.%2520The%2520advantage%2520of%2520using%250Aevolutionary%2520multi-objective%2520optimization%2520is%2520that%2520it%2520can%2520obtain%2520non-dominated%250APareto%2520frontier%2520solutions%2520with%2520respect%2520to%2520performance%2520and%2520shared%2520weights.%2520The%250Aexperimental%2520results%2520show%2520that%2520we%2520can%2520reduce%2520the%2520neural%2520network%2520memory%2520by%250A%252413.72%2520%255Csim14.98%2520%255Ctimes%2524%2520on%2520CIFAR-10%252C%2520%252411.61%2520%255Csim%252012.99%255Ctimes%2524%2520on%2520CIFAR-100%252C%250Aand%2520%25247.44%2520%255Csim%25208.58%255Ctimes%2524%2520on%2520ImageNet%2520showcasing%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520deep%2520neural%2520network%2520compression%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Structure-Agnostic%20Multi-Objective%20Approach%20for%20Weight-Sharing%0A%20%20Compression%20in%20Deep%20Neural%20Networks&entry.906535625=Rasa%20Khosrowshahli%20and%20Shahryar%20Rahnamayan%20and%20Beatrice%20Ombuki-Berman&entry.1292438233=%20%20Deep%20neural%20networks%20suffer%20from%20storing%20millions%20and%20billions%20of%20weights%20in%0Amemory%20post-training%2C%20making%20challenging%20memory-intensive%20models%20to%20deploy%20on%0Aembedded%20devices.%20The%20weight-sharing%20technique%20is%20one%20of%20the%20popular%0Acompression%20approaches%20that%20use%20fewer%20weight%20values%20and%20share%20across%20specific%0Aconnections%20in%20the%20network.%20In%20this%20paper%2C%20we%20propose%20a%20multi-objective%0Aevolutionary%20algorithm%20%28MOEA%29%20based%20compression%20framework%20independent%20of%20neural%0Anetwork%20architecture%2C%20dimension%2C%20task%2C%20and%20dataset.%20We%20use%20uniformly%20sized%20bins%0Ato%20quantize%20network%20weights%20into%20a%20single%20codebook%20%28lookup%20table%29%20for%20efficient%0Aweight%20representation.%20Using%20MOEA%2C%20we%20search%20for%20Pareto%20optimal%20%24k%24%20bins%20by%0Aoptimizing%20two%20objectives.%20Then%2C%20we%20apply%20the%20iterative%20merge%20technique%20to%0Anon-dominated%20Pareto%20frontier%20solutions%20by%20combining%20neighboring%20bins%20without%0Adegrading%20performance%20to%20decrease%20the%20number%20of%20bins%20and%20increase%20the%0Acompression%20ratio.%20Our%20approach%20is%20model-%20and%20layer-independent%2C%20meaning%20the%0Aweights%20are%20mixed%20in%20the%20clusters%20from%20any%20layer%2C%20and%20the%20uniform%20quantization%0Amethod%20used%20in%20this%20work%20has%20%24O%28N%29%24%20complexity%20instead%20of%20non-uniform%0Aquantization%20methods%20such%20as%20k-means%20with%20%24O%28Nkt%29%24%20complexity.%20In%20addition%2C%20we%0Ause%20the%20center%20of%20clusters%20as%20the%20shared%20weight%20values%20instead%20of%20retraining%0Ashared%20weights%2C%20which%20is%20computationally%20expensive.%20The%20advantage%20of%20using%0Aevolutionary%20multi-objective%20optimization%20is%20that%20it%20can%20obtain%20non-dominated%0APareto%20frontier%20solutions%20with%20respect%20to%20performance%20and%20shared%20weights.%20The%0Aexperimental%20results%20show%20that%20we%20can%20reduce%20the%20neural%20network%20memory%20by%0A%2413.72%20%5Csim14.98%20%5Ctimes%24%20on%20CIFAR-10%2C%20%2411.61%20%5Csim%2012.99%5Ctimes%24%20on%20CIFAR-100%2C%0Aand%20%247.44%20%5Csim%208.58%5Ctimes%24%20on%20ImageNet%20showcasing%20the%20effectiveness%20of%20the%0Aproposed%20deep%20neural%20network%20compression%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03095v1&entry.124074799=Read"},
{"title": "CAMP: Collaborative Attention Model with Profiles for Vehicle Routing\n  Problems", "author": "Chuanbo Hua and Federico Berto and Jiwoo Son and Seunghyun Kang and Changhyun Kwon and Jinkyoo Park", "abstract": "  The profiled vehicle routing problem (PVRP) is a generalization of the\nheterogeneous capacitated vehicle routing problem (HCVRP) in which the\nobjective is to optimize the routes of vehicles to serve client demands subject\nto different vehicle profiles, with each having a preference or constraint on a\nper-client basis. While existing learning methods have shown promise for\nsolving the HCVRP in real-time, no learning method exists to solve the more\npractical and challenging PVRP. In this paper, we propose a Collaborative\nAttention Model with Profiles (CAMP), a novel approach that learns efficient\nsolvers for PVRP using multi-agent reinforcement learning. CAMP employs a\nspecialized attention-based encoder architecture to embed profiled client\nembeddings in parallel for each vehicle profile. We design a communication\nlayer between agents for collaborative decision-making across profiled\nembeddings at each decoding step and a batched pointer mechanism to attend to\nthe profiled embeddings to evaluate the likelihood of the next actions. We\nevaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly\ninfluence the reward function, and PVRP with zone constraints with different\nnumbers of agents and clients, demonstrating that our learned solvers achieve\ncompetitive results compared to both classical state-of-the-art neural\nmulti-agent models in terms of solution quality and computational efficiency.\nWe make our code openly available at https://github.com/ai4co/camp.\n", "link": "http://arxiv.org/abs/2501.02977v1", "date": "2025-01-06", "relevancy": 2.0692, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMP%3A%20Collaborative%20Attention%20Model%20with%20Profiles%20for%20Vehicle%20Routing%0A%20%20Problems&body=Title%3A%20CAMP%3A%20Collaborative%20Attention%20Model%20with%20Profiles%20for%20Vehicle%20Routing%0A%20%20Problems%0AAuthor%3A%20Chuanbo%20Hua%20and%20Federico%20Berto%20and%20Jiwoo%20Son%20and%20Seunghyun%20Kang%20and%20Changhyun%20Kwon%20and%20Jinkyoo%20Park%0AAbstract%3A%20%20%20The%20profiled%20vehicle%20routing%20problem%20%28PVRP%29%20is%20a%20generalization%20of%20the%0Aheterogeneous%20capacitated%20vehicle%20routing%20problem%20%28HCVRP%29%20in%20which%20the%0Aobjective%20is%20to%20optimize%20the%20routes%20of%20vehicles%20to%20serve%20client%20demands%20subject%0Ato%20different%20vehicle%20profiles%2C%20with%20each%20having%20a%20preference%20or%20constraint%20on%20a%0Aper-client%20basis.%20While%20existing%20learning%20methods%20have%20shown%20promise%20for%0Asolving%20the%20HCVRP%20in%20real-time%2C%20no%20learning%20method%20exists%20to%20solve%20the%20more%0Apractical%20and%20challenging%20PVRP.%20In%20this%20paper%2C%20we%20propose%20a%20Collaborative%0AAttention%20Model%20with%20Profiles%20%28CAMP%29%2C%20a%20novel%20approach%20that%20learns%20efficient%0Asolvers%20for%20PVRP%20using%20multi-agent%20reinforcement%20learning.%20CAMP%20employs%20a%0Aspecialized%20attention-based%20encoder%20architecture%20to%20embed%20profiled%20client%0Aembeddings%20in%20parallel%20for%20each%20vehicle%20profile.%20We%20design%20a%20communication%0Alayer%20between%20agents%20for%20collaborative%20decision-making%20across%20profiled%0Aembeddings%20at%20each%20decoding%20step%20and%20a%20batched%20pointer%20mechanism%20to%20attend%20to%0Athe%20profiled%20embeddings%20to%20evaluate%20the%20likelihood%20of%20the%20next%20actions.%20We%0Aevaluate%20CAMP%20on%20two%20variants%20of%20PVRPs%3A%20PVRP%20with%20preferences%2C%20which%20explicitly%0Ainfluence%20the%20reward%20function%2C%20and%20PVRP%20with%20zone%20constraints%20with%20different%0Anumbers%20of%20agents%20and%20clients%2C%20demonstrating%20that%20our%20learned%20solvers%20achieve%0Acompetitive%20results%20compared%20to%20both%20classical%20state-of-the-art%20neural%0Amulti-agent%20models%20in%20terms%20of%20solution%20quality%20and%20computational%20efficiency.%0AWe%20make%20our%20code%20openly%20available%20at%20https%3A//github.com/ai4co/camp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMP%253A%2520Collaborative%2520Attention%2520Model%2520with%2520Profiles%2520for%2520Vehicle%2520Routing%250A%2520%2520Problems%26entry.906535625%3DChuanbo%2520Hua%2520and%2520Federico%2520Berto%2520and%2520Jiwoo%2520Son%2520and%2520Seunghyun%2520Kang%2520and%2520Changhyun%2520Kwon%2520and%2520Jinkyoo%2520Park%26entry.1292438233%3D%2520%2520The%2520profiled%2520vehicle%2520routing%2520problem%2520%2528PVRP%2529%2520is%2520a%2520generalization%2520of%2520the%250Aheterogeneous%2520capacitated%2520vehicle%2520routing%2520problem%2520%2528HCVRP%2529%2520in%2520which%2520the%250Aobjective%2520is%2520to%2520optimize%2520the%2520routes%2520of%2520vehicles%2520to%2520serve%2520client%2520demands%2520subject%250Ato%2520different%2520vehicle%2520profiles%252C%2520with%2520each%2520having%2520a%2520preference%2520or%2520constraint%2520on%2520a%250Aper-client%2520basis.%2520While%2520existing%2520learning%2520methods%2520have%2520shown%2520promise%2520for%250Asolving%2520the%2520HCVRP%2520in%2520real-time%252C%2520no%2520learning%2520method%2520exists%2520to%2520solve%2520the%2520more%250Apractical%2520and%2520challenging%2520PVRP.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Collaborative%250AAttention%2520Model%2520with%2520Profiles%2520%2528CAMP%2529%252C%2520a%2520novel%2520approach%2520that%2520learns%2520efficient%250Asolvers%2520for%2520PVRP%2520using%2520multi-agent%2520reinforcement%2520learning.%2520CAMP%2520employs%2520a%250Aspecialized%2520attention-based%2520encoder%2520architecture%2520to%2520embed%2520profiled%2520client%250Aembeddings%2520in%2520parallel%2520for%2520each%2520vehicle%2520profile.%2520We%2520design%2520a%2520communication%250Alayer%2520between%2520agents%2520for%2520collaborative%2520decision-making%2520across%2520profiled%250Aembeddings%2520at%2520each%2520decoding%2520step%2520and%2520a%2520batched%2520pointer%2520mechanism%2520to%2520attend%2520to%250Athe%2520profiled%2520embeddings%2520to%2520evaluate%2520the%2520likelihood%2520of%2520the%2520next%2520actions.%2520We%250Aevaluate%2520CAMP%2520on%2520two%2520variants%2520of%2520PVRPs%253A%2520PVRP%2520with%2520preferences%252C%2520which%2520explicitly%250Ainfluence%2520the%2520reward%2520function%252C%2520and%2520PVRP%2520with%2520zone%2520constraints%2520with%2520different%250Anumbers%2520of%2520agents%2520and%2520clients%252C%2520demonstrating%2520that%2520our%2520learned%2520solvers%2520achieve%250Acompetitive%2520results%2520compared%2520to%2520both%2520classical%2520state-of-the-art%2520neural%250Amulti-agent%2520models%2520in%2520terms%2520of%2520solution%2520quality%2520and%2520computational%2520efficiency.%250AWe%2520make%2520our%2520code%2520openly%2520available%2520at%2520https%253A//github.com/ai4co/camp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMP%3A%20Collaborative%20Attention%20Model%20with%20Profiles%20for%20Vehicle%20Routing%0A%20%20Problems&entry.906535625=Chuanbo%20Hua%20and%20Federico%20Berto%20and%20Jiwoo%20Son%20and%20Seunghyun%20Kang%20and%20Changhyun%20Kwon%20and%20Jinkyoo%20Park&entry.1292438233=%20%20The%20profiled%20vehicle%20routing%20problem%20%28PVRP%29%20is%20a%20generalization%20of%20the%0Aheterogeneous%20capacitated%20vehicle%20routing%20problem%20%28HCVRP%29%20in%20which%20the%0Aobjective%20is%20to%20optimize%20the%20routes%20of%20vehicles%20to%20serve%20client%20demands%20subject%0Ato%20different%20vehicle%20profiles%2C%20with%20each%20having%20a%20preference%20or%20constraint%20on%20a%0Aper-client%20basis.%20While%20existing%20learning%20methods%20have%20shown%20promise%20for%0Asolving%20the%20HCVRP%20in%20real-time%2C%20no%20learning%20method%20exists%20to%20solve%20the%20more%0Apractical%20and%20challenging%20PVRP.%20In%20this%20paper%2C%20we%20propose%20a%20Collaborative%0AAttention%20Model%20with%20Profiles%20%28CAMP%29%2C%20a%20novel%20approach%20that%20learns%20efficient%0Asolvers%20for%20PVRP%20using%20multi-agent%20reinforcement%20learning.%20CAMP%20employs%20a%0Aspecialized%20attention-based%20encoder%20architecture%20to%20embed%20profiled%20client%0Aembeddings%20in%20parallel%20for%20each%20vehicle%20profile.%20We%20design%20a%20communication%0Alayer%20between%20agents%20for%20collaborative%20decision-making%20across%20profiled%0Aembeddings%20at%20each%20decoding%20step%20and%20a%20batched%20pointer%20mechanism%20to%20attend%20to%0Athe%20profiled%20embeddings%20to%20evaluate%20the%20likelihood%20of%20the%20next%20actions.%20We%0Aevaluate%20CAMP%20on%20two%20variants%20of%20PVRPs%3A%20PVRP%20with%20preferences%2C%20which%20explicitly%0Ainfluence%20the%20reward%20function%2C%20and%20PVRP%20with%20zone%20constraints%20with%20different%0Anumbers%20of%20agents%20and%20clients%2C%20demonstrating%20that%20our%20learned%20solvers%20achieve%0Acompetitive%20results%20compared%20to%20both%20classical%20state-of-the-art%20neural%0Amulti-agent%20models%20in%20terms%20of%20solution%20quality%20and%20computational%20efficiency.%0AWe%20make%20our%20code%20openly%20available%20at%20https%3A//github.com/ai4co/camp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02977v1&entry.124074799=Read"},
{"title": "Restore-RWKV: Efficient and Effective Medical Image Restoration with\n  RWKV", "author": "Zhiwen Yang and Jiayin Li and Hui Zhang and Dan Zhao and Bingzheng Wei and Yan Xu", "abstract": "  Transformers have revolutionized medical image restoration, but the quadratic\ncomplexity still poses limitations for their application to high-resolution\nmedical images. The recent advent of the Receptance Weighted Key Value (RWKV)\nmodel in the natural language processing field has attracted much attention due\nto its ability to process long sequences efficiently. To leverage its advanced\ndesign, we propose Restore-RWKV, the first RWKV-based model for medical image\nrestoration. Since the original RWKV model is designed for 1D sequences, we\nmake two necessary modifications for modeling spatial relations in 2D medical\nimages. First, we present a recurrent WKV (Re-WKV) attention mechanism that\ncaptures global dependencies with linear computational complexity. Re-WKV\nincorporates bidirectional attention as basic for a global receptive field and\nrecurrent attention to effectively model 2D dependencies from various scan\ndirections. Second, we develop an omnidirectional token shift (Omni-Shift)\nlayer that enhances local dependencies by shifting tokens from all directions\nand across a wide context range. These adaptations make the proposed\nRestore-RWKV an efficient and effective model for medical image restoration.\nEven a lightweight variant of Restore-RWKV, with only 1.16 million parameters,\nachieves comparable or even superior results compared to existing\nstate-of-the-art (SOTA) methods. Extensive experiments demonstrate that the\nresulting Restore-RWKV achieves SOTA performance across a range of medical\nimage restoration tasks, including PET image synthesis, CT image denoising, MRI\nimage super-resolution, and all-in-one medical image restoration. Code is\navailable at: https://github.com/Yaziwel/Restore-RWKV.\n", "link": "http://arxiv.org/abs/2407.11087v3", "date": "2025-01-06", "relevancy": 2.0511, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5214}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5184}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restore-RWKV%3A%20Efficient%20and%20Effective%20Medical%20Image%20Restoration%20with%0A%20%20RWKV&body=Title%3A%20Restore-RWKV%3A%20Efficient%20and%20Effective%20Medical%20Image%20Restoration%20with%0A%20%20RWKV%0AAuthor%3A%20Zhiwen%20Yang%20and%20Jiayin%20Li%20and%20Hui%20Zhang%20and%20Dan%20Zhao%20and%20Bingzheng%20Wei%20and%20Yan%20Xu%0AAbstract%3A%20%20%20Transformers%20have%20revolutionized%20medical%20image%20restoration%2C%20but%20the%20quadratic%0Acomplexity%20still%20poses%20limitations%20for%20their%20application%20to%20high-resolution%0Amedical%20images.%20The%20recent%20advent%20of%20the%20Receptance%20Weighted%20Key%20Value%20%28RWKV%29%0Amodel%20in%20the%20natural%20language%20processing%20field%20has%20attracted%20much%20attention%20due%0Ato%20its%20ability%20to%20process%20long%20sequences%20efficiently.%20To%20leverage%20its%20advanced%0Adesign%2C%20we%20propose%20Restore-RWKV%2C%20the%20first%20RWKV-based%20model%20for%20medical%20image%0Arestoration.%20Since%20the%20original%20RWKV%20model%20is%20designed%20for%201D%20sequences%2C%20we%0Amake%20two%20necessary%20modifications%20for%20modeling%20spatial%20relations%20in%202D%20medical%0Aimages.%20First%2C%20we%20present%20a%20recurrent%20WKV%20%28Re-WKV%29%20attention%20mechanism%20that%0Acaptures%20global%20dependencies%20with%20linear%20computational%20complexity.%20Re-WKV%0Aincorporates%20bidirectional%20attention%20as%20basic%20for%20a%20global%20receptive%20field%20and%0Arecurrent%20attention%20to%20effectively%20model%202D%20dependencies%20from%20various%20scan%0Adirections.%20Second%2C%20we%20develop%20an%20omnidirectional%20token%20shift%20%28Omni-Shift%29%0Alayer%20that%20enhances%20local%20dependencies%20by%20shifting%20tokens%20from%20all%20directions%0Aand%20across%20a%20wide%20context%20range.%20These%20adaptations%20make%20the%20proposed%0ARestore-RWKV%20an%20efficient%20and%20effective%20model%20for%20medical%20image%20restoration.%0AEven%20a%20lightweight%20variant%20of%20Restore-RWKV%2C%20with%20only%201.16%20million%20parameters%2C%0Aachieves%20comparable%20or%20even%20superior%20results%20compared%20to%20existing%0Astate-of-the-art%20%28SOTA%29%20methods.%20Extensive%20experiments%20demonstrate%20that%20the%0Aresulting%20Restore-RWKV%20achieves%20SOTA%20performance%20across%20a%20range%20of%20medical%0Aimage%20restoration%20tasks%2C%20including%20PET%20image%20synthesis%2C%20CT%20image%20denoising%2C%20MRI%0Aimage%20super-resolution%2C%20and%20all-in-one%20medical%20image%20restoration.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/Yaziwel/Restore-RWKV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11087v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestore-RWKV%253A%2520Efficient%2520and%2520Effective%2520Medical%2520Image%2520Restoration%2520with%250A%2520%2520RWKV%26entry.906535625%3DZhiwen%2520Yang%2520and%2520Jiayin%2520Li%2520and%2520Hui%2520Zhang%2520and%2520Dan%2520Zhao%2520and%2520Bingzheng%2520Wei%2520and%2520Yan%2520Xu%26entry.1292438233%3D%2520%2520Transformers%2520have%2520revolutionized%2520medical%2520image%2520restoration%252C%2520but%2520the%2520quadratic%250Acomplexity%2520still%2520poses%2520limitations%2520for%2520their%2520application%2520to%2520high-resolution%250Amedical%2520images.%2520The%2520recent%2520advent%2520of%2520the%2520Receptance%2520Weighted%2520Key%2520Value%2520%2528RWKV%2529%250Amodel%2520in%2520the%2520natural%2520language%2520processing%2520field%2520has%2520attracted%2520much%2520attention%2520due%250Ato%2520its%2520ability%2520to%2520process%2520long%2520sequences%2520efficiently.%2520To%2520leverage%2520its%2520advanced%250Adesign%252C%2520we%2520propose%2520Restore-RWKV%252C%2520the%2520first%2520RWKV-based%2520model%2520for%2520medical%2520image%250Arestoration.%2520Since%2520the%2520original%2520RWKV%2520model%2520is%2520designed%2520for%25201D%2520sequences%252C%2520we%250Amake%2520two%2520necessary%2520modifications%2520for%2520modeling%2520spatial%2520relations%2520in%25202D%2520medical%250Aimages.%2520First%252C%2520we%2520present%2520a%2520recurrent%2520WKV%2520%2528Re-WKV%2529%2520attention%2520mechanism%2520that%250Acaptures%2520global%2520dependencies%2520with%2520linear%2520computational%2520complexity.%2520Re-WKV%250Aincorporates%2520bidirectional%2520attention%2520as%2520basic%2520for%2520a%2520global%2520receptive%2520field%2520and%250Arecurrent%2520attention%2520to%2520effectively%2520model%25202D%2520dependencies%2520from%2520various%2520scan%250Adirections.%2520Second%252C%2520we%2520develop%2520an%2520omnidirectional%2520token%2520shift%2520%2528Omni-Shift%2529%250Alayer%2520that%2520enhances%2520local%2520dependencies%2520by%2520shifting%2520tokens%2520from%2520all%2520directions%250Aand%2520across%2520a%2520wide%2520context%2520range.%2520These%2520adaptations%2520make%2520the%2520proposed%250ARestore-RWKV%2520an%2520efficient%2520and%2520effective%2520model%2520for%2520medical%2520image%2520restoration.%250AEven%2520a%2520lightweight%2520variant%2520of%2520Restore-RWKV%252C%2520with%2520only%25201.16%2520million%2520parameters%252C%250Aachieves%2520comparable%2520or%2520even%2520superior%2520results%2520compared%2520to%2520existing%250Astate-of-the-art%2520%2528SOTA%2529%2520methods.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%250Aresulting%2520Restore-RWKV%2520achieves%2520SOTA%2520performance%2520across%2520a%2520range%2520of%2520medical%250Aimage%2520restoration%2520tasks%252C%2520including%2520PET%2520image%2520synthesis%252C%2520CT%2520image%2520denoising%252C%2520MRI%250Aimage%2520super-resolution%252C%2520and%2520all-in-one%2520medical%2520image%2520restoration.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/Yaziwel/Restore-RWKV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11087v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restore-RWKV%3A%20Efficient%20and%20Effective%20Medical%20Image%20Restoration%20with%0A%20%20RWKV&entry.906535625=Zhiwen%20Yang%20and%20Jiayin%20Li%20and%20Hui%20Zhang%20and%20Dan%20Zhao%20and%20Bingzheng%20Wei%20and%20Yan%20Xu&entry.1292438233=%20%20Transformers%20have%20revolutionized%20medical%20image%20restoration%2C%20but%20the%20quadratic%0Acomplexity%20still%20poses%20limitations%20for%20their%20application%20to%20high-resolution%0Amedical%20images.%20The%20recent%20advent%20of%20the%20Receptance%20Weighted%20Key%20Value%20%28RWKV%29%0Amodel%20in%20the%20natural%20language%20processing%20field%20has%20attracted%20much%20attention%20due%0Ato%20its%20ability%20to%20process%20long%20sequences%20efficiently.%20To%20leverage%20its%20advanced%0Adesign%2C%20we%20propose%20Restore-RWKV%2C%20the%20first%20RWKV-based%20model%20for%20medical%20image%0Arestoration.%20Since%20the%20original%20RWKV%20model%20is%20designed%20for%201D%20sequences%2C%20we%0Amake%20two%20necessary%20modifications%20for%20modeling%20spatial%20relations%20in%202D%20medical%0Aimages.%20First%2C%20we%20present%20a%20recurrent%20WKV%20%28Re-WKV%29%20attention%20mechanism%20that%0Acaptures%20global%20dependencies%20with%20linear%20computational%20complexity.%20Re-WKV%0Aincorporates%20bidirectional%20attention%20as%20basic%20for%20a%20global%20receptive%20field%20and%0Arecurrent%20attention%20to%20effectively%20model%202D%20dependencies%20from%20various%20scan%0Adirections.%20Second%2C%20we%20develop%20an%20omnidirectional%20token%20shift%20%28Omni-Shift%29%0Alayer%20that%20enhances%20local%20dependencies%20by%20shifting%20tokens%20from%20all%20directions%0Aand%20across%20a%20wide%20context%20range.%20These%20adaptations%20make%20the%20proposed%0ARestore-RWKV%20an%20efficient%20and%20effective%20model%20for%20medical%20image%20restoration.%0AEven%20a%20lightweight%20variant%20of%20Restore-RWKV%2C%20with%20only%201.16%20million%20parameters%2C%0Aachieves%20comparable%20or%20even%20superior%20results%20compared%20to%20existing%0Astate-of-the-art%20%28SOTA%29%20methods.%20Extensive%20experiments%20demonstrate%20that%20the%0Aresulting%20Restore-RWKV%20achieves%20SOTA%20performance%20across%20a%20range%20of%20medical%0Aimage%20restoration%20tasks%2C%20including%20PET%20image%20synthesis%2C%20CT%20image%20denoising%2C%20MRI%0Aimage%20super-resolution%2C%20and%20all-in-one%20medical%20image%20restoration.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/Yaziwel/Restore-RWKV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11087v3&entry.124074799=Read"},
{"title": "AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain\n  Adaptation for Medical Image Segmentation", "author": "Haojin Li and Heng Li and Jianyu Chen and Rihan Zhong and Ke Niu and Huazhu Fu and Jiang Liu", "abstract": "  Decoupling domain-variant information (DVI) from domain-invariant information\n(DII) serves as a prominent strategy for mitigating domain shifts in the\npractical implementation of deep learning algorithms. However, in medical\nsettings, concerns surrounding data collection and privacy often restrict\naccess to both training and test data, hindering the empirical decoupling of\ninformation by existing methods. To tackle this issue, we propose an Autonomous\nInformation Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm,\nwhich leverages a frequency-based learnable information filter to autonomously\ndecouple DVI and DII. Information Bottleneck (IB) and Self-supervision (SS) are\nincorporated to optimize the learnable frequency filter. The IB governs the\ninformation flow within the filter to diminish redundant DVI, while SS\npreserves DII in alignment with the specific task and image modality. Thus, the\nautonomous information filter can overcome domain shifts relying solely on\ntarget data. A series of experiments covering various medical image modalities\nand segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA\nthrough comparisons with leading algorithms and ablation studies. The code is\navailable at https://github.com/JingHuaMan/AIF-SFDA.\n", "link": "http://arxiv.org/abs/2501.03074v1", "date": "2025-01-06", "relevancy": 2.0471, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5312}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4979}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIF-SFDA%3A%20Autonomous%20Information%20Filter-driven%20Source-Free%20Domain%0A%20%20Adaptation%20for%20Medical%20Image%20Segmentation&body=Title%3A%20AIF-SFDA%3A%20Autonomous%20Information%20Filter-driven%20Source-Free%20Domain%0A%20%20Adaptation%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Haojin%20Li%20and%20Heng%20Li%20and%20Jianyu%20Chen%20and%20Rihan%20Zhong%20and%20Ke%20Niu%20and%20Huazhu%20Fu%20and%20Jiang%20Liu%0AAbstract%3A%20%20%20Decoupling%20domain-variant%20information%20%28DVI%29%20from%20domain-invariant%20information%0A%28DII%29%20serves%20as%20a%20prominent%20strategy%20for%20mitigating%20domain%20shifts%20in%20the%0Apractical%20implementation%20of%20deep%20learning%20algorithms.%20However%2C%20in%20medical%0Asettings%2C%20concerns%20surrounding%20data%20collection%20and%20privacy%20often%20restrict%0Aaccess%20to%20both%20training%20and%20test%20data%2C%20hindering%20the%20empirical%20decoupling%20of%0Ainformation%20by%20existing%20methods.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20Autonomous%0AInformation%20Filter-driven%20Source-free%20Domain%20Adaptation%20%28AIF-SFDA%29%20algorithm%2C%0Awhich%20leverages%20a%20frequency-based%20learnable%20information%20filter%20to%20autonomously%0Adecouple%20DVI%20and%20DII.%20Information%20Bottleneck%20%28IB%29%20and%20Self-supervision%20%28SS%29%20are%0Aincorporated%20to%20optimize%20the%20learnable%20frequency%20filter.%20The%20IB%20governs%20the%0Ainformation%20flow%20within%20the%20filter%20to%20diminish%20redundant%20DVI%2C%20while%20SS%0Apreserves%20DII%20in%20alignment%20with%20the%20specific%20task%20and%20image%20modality.%20Thus%2C%20the%0Aautonomous%20information%20filter%20can%20overcome%20domain%20shifts%20relying%20solely%20on%0Atarget%20data.%20A%20series%20of%20experiments%20covering%20various%20medical%20image%20modalities%0Aand%20segmentation%20tasks%20were%20conducted%20to%20demonstrate%20the%20benefits%20of%20AIF-SFDA%0Athrough%20comparisons%20with%20leading%20algorithms%20and%20ablation%20studies.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/JingHuaMan/AIF-SFDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIF-SFDA%253A%2520Autonomous%2520Information%2520Filter-driven%2520Source-Free%2520Domain%250A%2520%2520Adaptation%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DHaojin%2520Li%2520and%2520Heng%2520Li%2520and%2520Jianyu%2520Chen%2520and%2520Rihan%2520Zhong%2520and%2520Ke%2520Niu%2520and%2520Huazhu%2520Fu%2520and%2520Jiang%2520Liu%26entry.1292438233%3D%2520%2520Decoupling%2520domain-variant%2520information%2520%2528DVI%2529%2520from%2520domain-invariant%2520information%250A%2528DII%2529%2520serves%2520as%2520a%2520prominent%2520strategy%2520for%2520mitigating%2520domain%2520shifts%2520in%2520the%250Apractical%2520implementation%2520of%2520deep%2520learning%2520algorithms.%2520However%252C%2520in%2520medical%250Asettings%252C%2520concerns%2520surrounding%2520data%2520collection%2520and%2520privacy%2520often%2520restrict%250Aaccess%2520to%2520both%2520training%2520and%2520test%2520data%252C%2520hindering%2520the%2520empirical%2520decoupling%2520of%250Ainformation%2520by%2520existing%2520methods.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520an%2520Autonomous%250AInformation%2520Filter-driven%2520Source-free%2520Domain%2520Adaptation%2520%2528AIF-SFDA%2529%2520algorithm%252C%250Awhich%2520leverages%2520a%2520frequency-based%2520learnable%2520information%2520filter%2520to%2520autonomously%250Adecouple%2520DVI%2520and%2520DII.%2520Information%2520Bottleneck%2520%2528IB%2529%2520and%2520Self-supervision%2520%2528SS%2529%2520are%250Aincorporated%2520to%2520optimize%2520the%2520learnable%2520frequency%2520filter.%2520The%2520IB%2520governs%2520the%250Ainformation%2520flow%2520within%2520the%2520filter%2520to%2520diminish%2520redundant%2520DVI%252C%2520while%2520SS%250Apreserves%2520DII%2520in%2520alignment%2520with%2520the%2520specific%2520task%2520and%2520image%2520modality.%2520Thus%252C%2520the%250Aautonomous%2520information%2520filter%2520can%2520overcome%2520domain%2520shifts%2520relying%2520solely%2520on%250Atarget%2520data.%2520A%2520series%2520of%2520experiments%2520covering%2520various%2520medical%2520image%2520modalities%250Aand%2520segmentation%2520tasks%2520were%2520conducted%2520to%2520demonstrate%2520the%2520benefits%2520of%2520AIF-SFDA%250Athrough%2520comparisons%2520with%2520leading%2520algorithms%2520and%2520ablation%2520studies.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/JingHuaMan/AIF-SFDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIF-SFDA%3A%20Autonomous%20Information%20Filter-driven%20Source-Free%20Domain%0A%20%20Adaptation%20for%20Medical%20Image%20Segmentation&entry.906535625=Haojin%20Li%20and%20Heng%20Li%20and%20Jianyu%20Chen%20and%20Rihan%20Zhong%20and%20Ke%20Niu%20and%20Huazhu%20Fu%20and%20Jiang%20Liu&entry.1292438233=%20%20Decoupling%20domain-variant%20information%20%28DVI%29%20from%20domain-invariant%20information%0A%28DII%29%20serves%20as%20a%20prominent%20strategy%20for%20mitigating%20domain%20shifts%20in%20the%0Apractical%20implementation%20of%20deep%20learning%20algorithms.%20However%2C%20in%20medical%0Asettings%2C%20concerns%20surrounding%20data%20collection%20and%20privacy%20often%20restrict%0Aaccess%20to%20both%20training%20and%20test%20data%2C%20hindering%20the%20empirical%20decoupling%20of%0Ainformation%20by%20existing%20methods.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20Autonomous%0AInformation%20Filter-driven%20Source-free%20Domain%20Adaptation%20%28AIF-SFDA%29%20algorithm%2C%0Awhich%20leverages%20a%20frequency-based%20learnable%20information%20filter%20to%20autonomously%0Adecouple%20DVI%20and%20DII.%20Information%20Bottleneck%20%28IB%29%20and%20Self-supervision%20%28SS%29%20are%0Aincorporated%20to%20optimize%20the%20learnable%20frequency%20filter.%20The%20IB%20governs%20the%0Ainformation%20flow%20within%20the%20filter%20to%20diminish%20redundant%20DVI%2C%20while%20SS%0Apreserves%20DII%20in%20alignment%20with%20the%20specific%20task%20and%20image%20modality.%20Thus%2C%20the%0Aautonomous%20information%20filter%20can%20overcome%20domain%20shifts%20relying%20solely%20on%0Atarget%20data.%20A%20series%20of%20experiments%20covering%20various%20medical%20image%20modalities%0Aand%20segmentation%20tasks%20were%20conducted%20to%20demonstrate%20the%20benefits%20of%20AIF-SFDA%0Athrough%20comparisons%20with%20leading%20algorithms%20and%20ablation%20studies.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/JingHuaMan/AIF-SFDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03074v1&entry.124074799=Read"},
{"title": "Detecting and Mitigating Hallucination in Large Vision Language Models\n  via Fine-Grained AI Feedback", "author": "Wenyi Xiao and Ziwei Huang and Leilei Gan and Wanggui He and Haoyuan Li and Zhelun Yu and Fangxun Shu and Hao Jiang and Linchao Zhu", "abstract": "  The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.\n", "link": "http://arxiv.org/abs/2404.14233v2", "date": "2025-01-06", "relevancy": 2.0324, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Mitigating%20Hallucination%20in%20Large%20Vision%20Language%20Models%0A%20%20via%20Fine-Grained%20AI%20Feedback&body=Title%3A%20Detecting%20and%20Mitigating%20Hallucination%20in%20Large%20Vision%20Language%20Models%0A%20%20via%20Fine-Grained%20AI%20Feedback%0AAuthor%3A%20Wenyi%20Xiao%20and%20Ziwei%20Huang%20and%20Leilei%20Gan%20and%20Wanggui%20He%20and%20Haoyuan%20Li%20and%20Zhelun%20Yu%20and%20Fangxun%20Shu%20and%20Hao%20Jiang%20and%20Linchao%20Zhu%0AAbstract%3A%20%20%20The%20rapidly%20developing%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20shown%0Anotable%20capabilities%20on%20a%20range%20of%20multi-modal%20tasks%2C%20but%20still%20face%20the%0Ahallucination%20phenomena%20where%20the%20generated%20texts%20do%20not%20align%20with%20the%20given%0Acontexts%2C%20significantly%20restricting%20the%20usages%20of%20LVLMs.%20Most%20previous%20work%0Adetects%20and%20mitigates%20hallucination%20at%20the%20coarse-grained%20level%20or%20requires%0Aexpensive%20annotation%20%28e.g.%2C%20labeling%20by%20proprietary%20models%20or%20human%20experts%29.%0ATo%20address%20these%20issues%2C%20we%20propose%20detecting%20and%20mitigating%20hallucinations%20in%0ALVLMs%20via%20fine-grained%20AI%20feedback.%20The%20basic%20idea%20is%20that%20we%20generate%20a%0Asmall-size%20sentence-level%20hallucination%20annotation%20dataset%20by%20proprietary%0Amodels%2C%20whereby%20we%20train%20a%20hallucination%20detection%20model%20which%20can%20perform%0Asentence-level%20hallucination%20detection%2C%20covering%20primary%20hallucination%20types%0A%28i.e.%2C%20object%2C%20attribute%2C%20and%20relationship%29.%20Then%2C%20we%20propose%20a%0Adetect-then-rewrite%20pipeline%20to%20automatically%20construct%20preference%20dataset%20for%0Atraining%20hallucination%20mitigating%20model.%20Furthermore%2C%20we%20propose%0Adifferentiating%20the%20severity%20of%20hallucinations%2C%20and%20introducing%20a%20Hallucination%0ASeverity-Aware%20Direct%20Preference%20Optimization%20%28HSA-DPO%29%20for%20mitigating%0Ahallucination%20in%20LVLMs%20by%20incorporating%20the%20severity%20of%20hallucinations%20into%0Apreference%20learning.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14233v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Mitigating%2520Hallucination%2520in%2520Large%2520Vision%2520Language%2520Models%250A%2520%2520via%2520Fine-Grained%2520AI%2520Feedback%26entry.906535625%3DWenyi%2520Xiao%2520and%2520Ziwei%2520Huang%2520and%2520Leilei%2520Gan%2520and%2520Wanggui%2520He%2520and%2520Haoyuan%2520Li%2520and%2520Zhelun%2520Yu%2520and%2520Fangxun%2520Shu%2520and%2520Hao%2520Jiang%2520and%2520Linchao%2520Zhu%26entry.1292438233%3D%2520%2520The%2520rapidly%2520developing%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520have%2520shown%250Anotable%2520capabilities%2520on%2520a%2520range%2520of%2520multi-modal%2520tasks%252C%2520but%2520still%2520face%2520the%250Ahallucination%2520phenomena%2520where%2520the%2520generated%2520texts%2520do%2520not%2520align%2520with%2520the%2520given%250Acontexts%252C%2520significantly%2520restricting%2520the%2520usages%2520of%2520LVLMs.%2520Most%2520previous%2520work%250Adetects%2520and%2520mitigates%2520hallucination%2520at%2520the%2520coarse-grained%2520level%2520or%2520requires%250Aexpensive%2520annotation%2520%2528e.g.%252C%2520labeling%2520by%2520proprietary%2520models%2520or%2520human%2520experts%2529.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520detecting%2520and%2520mitigating%2520hallucinations%2520in%250ALVLMs%2520via%2520fine-grained%2520AI%2520feedback.%2520The%2520basic%2520idea%2520is%2520that%2520we%2520generate%2520a%250Asmall-size%2520sentence-level%2520hallucination%2520annotation%2520dataset%2520by%2520proprietary%250Amodels%252C%2520whereby%2520we%2520train%2520a%2520hallucination%2520detection%2520model%2520which%2520can%2520perform%250Asentence-level%2520hallucination%2520detection%252C%2520covering%2520primary%2520hallucination%2520types%250A%2528i.e.%252C%2520object%252C%2520attribute%252C%2520and%2520relationship%2529.%2520Then%252C%2520we%2520propose%2520a%250Adetect-then-rewrite%2520pipeline%2520to%2520automatically%2520construct%2520preference%2520dataset%2520for%250Atraining%2520hallucination%2520mitigating%2520model.%2520Furthermore%252C%2520we%2520propose%250Adifferentiating%2520the%2520severity%2520of%2520hallucinations%252C%2520and%2520introducing%2520a%2520Hallucination%250ASeverity-Aware%2520Direct%2520Preference%2520Optimization%2520%2528HSA-DPO%2529%2520for%2520mitigating%250Ahallucination%2520in%2520LVLMs%2520by%2520incorporating%2520the%2520severity%2520of%2520hallucinations%2520into%250Apreference%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14233v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Mitigating%20Hallucination%20in%20Large%20Vision%20Language%20Models%0A%20%20via%20Fine-Grained%20AI%20Feedback&entry.906535625=Wenyi%20Xiao%20and%20Ziwei%20Huang%20and%20Leilei%20Gan%20and%20Wanggui%20He%20and%20Haoyuan%20Li%20and%20Zhelun%20Yu%20and%20Fangxun%20Shu%20and%20Hao%20Jiang%20and%20Linchao%20Zhu&entry.1292438233=%20%20The%20rapidly%20developing%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20have%20shown%0Anotable%20capabilities%20on%20a%20range%20of%20multi-modal%20tasks%2C%20but%20still%20face%20the%0Ahallucination%20phenomena%20where%20the%20generated%20texts%20do%20not%20align%20with%20the%20given%0Acontexts%2C%20significantly%20restricting%20the%20usages%20of%20LVLMs.%20Most%20previous%20work%0Adetects%20and%20mitigates%20hallucination%20at%20the%20coarse-grained%20level%20or%20requires%0Aexpensive%20annotation%20%28e.g.%2C%20labeling%20by%20proprietary%20models%20or%20human%20experts%29.%0ATo%20address%20these%20issues%2C%20we%20propose%20detecting%20and%20mitigating%20hallucinations%20in%0ALVLMs%20via%20fine-grained%20AI%20feedback.%20The%20basic%20idea%20is%20that%20we%20generate%20a%0Asmall-size%20sentence-level%20hallucination%20annotation%20dataset%20by%20proprietary%0Amodels%2C%20whereby%20we%20train%20a%20hallucination%20detection%20model%20which%20can%20perform%0Asentence-level%20hallucination%20detection%2C%20covering%20primary%20hallucination%20types%0A%28i.e.%2C%20object%2C%20attribute%2C%20and%20relationship%29.%20Then%2C%20we%20propose%20a%0Adetect-then-rewrite%20pipeline%20to%20automatically%20construct%20preference%20dataset%20for%0Atraining%20hallucination%20mitigating%20model.%20Furthermore%2C%20we%20propose%0Adifferentiating%20the%20severity%20of%20hallucinations%2C%20and%20introducing%20a%20Hallucination%0ASeverity-Aware%20Direct%20Preference%20Optimization%20%28HSA-DPO%29%20for%20mitigating%0Ahallucination%20in%20LVLMs%20by%20incorporating%20the%20severity%20of%20hallucinations%20into%0Apreference%20learning.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14233v2&entry.124074799=Read"},
{"title": "Decoupling Knowledge and Reasoning in Transformers: A Modular\n  Architecture with Generalized Cross-Attention", "author": "Zhenyu Guo and Wenguang Chen", "abstract": "  Transformers have achieved remarkable success across diverse domains, but\ntheir monolithic architecture presents challenges in interpretability,\nadaptability, and scalability. This paper introduces a novel modular\nTransformer architecture that explicitly decouples knowledge and reasoning\nthrough a generalized cross-attention mechanism to a globally shared knowledge\nbase with layer-specific transformations, specifically designed for effective\nknowledge retrieval. Critically, we provide a rigorous mathematical derivation\ndemonstrating that the Feed-Forward Network (FFN) in a standard Transformer is\na specialized case (a closure) of this generalized cross-attention, revealing\nits role in implicit knowledge retrieval and validating our design. This\ntheoretical framework provides a new lens for understanding FFNs and lays the\nfoundation for future research exploring enhanced interpretability,\nadaptability, and scalability, enabling richer interplay with external\nknowledge bases and other systems.\n", "link": "http://arxiv.org/abs/2501.00823v2", "date": "2025-01-06", "relevancy": 2.0313, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Knowledge%20and%20Reasoning%20in%20Transformers%3A%20A%20Modular%0A%20%20Architecture%20with%20Generalized%20Cross-Attention&body=Title%3A%20Decoupling%20Knowledge%20and%20Reasoning%20in%20Transformers%3A%20A%20Modular%0A%20%20Architecture%20with%20Generalized%20Cross-Attention%0AAuthor%3A%20Zhenyu%20Guo%20and%20Wenguang%20Chen%0AAbstract%3A%20%20%20Transformers%20have%20achieved%20remarkable%20success%20across%20diverse%20domains%2C%20but%0Atheir%20monolithic%20architecture%20presents%20challenges%20in%20interpretability%2C%0Aadaptability%2C%20and%20scalability.%20This%20paper%20introduces%20a%20novel%20modular%0ATransformer%20architecture%20that%20explicitly%20decouples%20knowledge%20and%20reasoning%0Athrough%20a%20generalized%20cross-attention%20mechanism%20to%20a%20globally%20shared%20knowledge%0Abase%20with%20layer-specific%20transformations%2C%20specifically%20designed%20for%20effective%0Aknowledge%20retrieval.%20Critically%2C%20we%20provide%20a%20rigorous%20mathematical%20derivation%0Ademonstrating%20that%20the%20Feed-Forward%20Network%20%28FFN%29%20in%20a%20standard%20Transformer%20is%0Aa%20specialized%20case%20%28a%20closure%29%20of%20this%20generalized%20cross-attention%2C%20revealing%0Aits%20role%20in%20implicit%20knowledge%20retrieval%20and%20validating%20our%20design.%20This%0Atheoretical%20framework%20provides%20a%20new%20lens%20for%20understanding%20FFNs%20and%20lays%20the%0Afoundation%20for%20future%20research%20exploring%20enhanced%20interpretability%2C%0Aadaptability%2C%20and%20scalability%2C%20enabling%20richer%20interplay%20with%20external%0Aknowledge%20bases%20and%20other%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Knowledge%2520and%2520Reasoning%2520in%2520Transformers%253A%2520A%2520Modular%250A%2520%2520Architecture%2520with%2520Generalized%2520Cross-Attention%26entry.906535625%3DZhenyu%2520Guo%2520and%2520Wenguang%2520Chen%26entry.1292438233%3D%2520%2520Transformers%2520have%2520achieved%2520remarkable%2520success%2520across%2520diverse%2520domains%252C%2520but%250Atheir%2520monolithic%2520architecture%2520presents%2520challenges%2520in%2520interpretability%252C%250Aadaptability%252C%2520and%2520scalability.%2520This%2520paper%2520introduces%2520a%2520novel%2520modular%250ATransformer%2520architecture%2520that%2520explicitly%2520decouples%2520knowledge%2520and%2520reasoning%250Athrough%2520a%2520generalized%2520cross-attention%2520mechanism%2520to%2520a%2520globally%2520shared%2520knowledge%250Abase%2520with%2520layer-specific%2520transformations%252C%2520specifically%2520designed%2520for%2520effective%250Aknowledge%2520retrieval.%2520Critically%252C%2520we%2520provide%2520a%2520rigorous%2520mathematical%2520derivation%250Ademonstrating%2520that%2520the%2520Feed-Forward%2520Network%2520%2528FFN%2529%2520in%2520a%2520standard%2520Transformer%2520is%250Aa%2520specialized%2520case%2520%2528a%2520closure%2529%2520of%2520this%2520generalized%2520cross-attention%252C%2520revealing%250Aits%2520role%2520in%2520implicit%2520knowledge%2520retrieval%2520and%2520validating%2520our%2520design.%2520This%250Atheoretical%2520framework%2520provides%2520a%2520new%2520lens%2520for%2520understanding%2520FFNs%2520and%2520lays%2520the%250Afoundation%2520for%2520future%2520research%2520exploring%2520enhanced%2520interpretability%252C%250Aadaptability%252C%2520and%2520scalability%252C%2520enabling%2520richer%2520interplay%2520with%2520external%250Aknowledge%2520bases%2520and%2520other%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Knowledge%20and%20Reasoning%20in%20Transformers%3A%20A%20Modular%0A%20%20Architecture%20with%20Generalized%20Cross-Attention&entry.906535625=Zhenyu%20Guo%20and%20Wenguang%20Chen&entry.1292438233=%20%20Transformers%20have%20achieved%20remarkable%20success%20across%20diverse%20domains%2C%20but%0Atheir%20monolithic%20architecture%20presents%20challenges%20in%20interpretability%2C%0Aadaptability%2C%20and%20scalability.%20This%20paper%20introduces%20a%20novel%20modular%0ATransformer%20architecture%20that%20explicitly%20decouples%20knowledge%20and%20reasoning%0Athrough%20a%20generalized%20cross-attention%20mechanism%20to%20a%20globally%20shared%20knowledge%0Abase%20with%20layer-specific%20transformations%2C%20specifically%20designed%20for%20effective%0Aknowledge%20retrieval.%20Critically%2C%20we%20provide%20a%20rigorous%20mathematical%20derivation%0Ademonstrating%20that%20the%20Feed-Forward%20Network%20%28FFN%29%20in%20a%20standard%20Transformer%20is%0Aa%20specialized%20case%20%28a%20closure%29%20of%20this%20generalized%20cross-attention%2C%20revealing%0Aits%20role%20in%20implicit%20knowledge%20retrieval%20and%20validating%20our%20design.%20This%0Atheoretical%20framework%20provides%20a%20new%20lens%20for%20understanding%20FFNs%20and%20lays%20the%0Afoundation%20for%20future%20research%20exploring%20enhanced%20interpretability%2C%0Aadaptability%2C%20and%20scalability%2C%20enabling%20richer%20interplay%20with%20external%0Aknowledge%20bases%20and%20other%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00823v2&entry.124074799=Read"},
{"title": "CNMBert: A Model for Hanyu Pinyin Abbreviation to Character Conversion\n  Task", "author": "Zishuo Feng and Feng Cao", "abstract": "  The task of converting hanyu pinyin abbreviations to Chinese characters is a\nsignificant branch within the domain of Chinese Spelling Correction (CSC). It\nplays an important role in many downstream applications like named entity\nrecognition and sentiment analysis. This task is typically one of text-length\nalignment and seems easy to solve; however, due to the limited information\ncontent in pinyin abbreviations, achieving accurate conversion is challenging.\nIn this paper, we treat this as a Fill-Mask task then propose CNMBert, which\nstands for zh-CN Pinyin Multi-mask Bert Model, as a solution to this issue. By\nintroducing a multi-mask strategy and Mixture-of-Experts (MoE) layers, CNMBert\noutperforms fine-tuned GPT models and ChatGPT-4o with a 61.53 MRR score and\n51.86 accuracy on a 10,373-sample test dataset.\n", "link": "http://arxiv.org/abs/2411.11770v3", "date": "2025-01-06", "relevancy": 2.0235, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4012}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNMBert%3A%20A%20Model%20for%20Hanyu%20Pinyin%20Abbreviation%20to%20Character%20Conversion%0A%20%20Task&body=Title%3A%20CNMBert%3A%20A%20Model%20for%20Hanyu%20Pinyin%20Abbreviation%20to%20Character%20Conversion%0A%20%20Task%0AAuthor%3A%20Zishuo%20Feng%20and%20Feng%20Cao%0AAbstract%3A%20%20%20The%20task%20of%20converting%20hanyu%20pinyin%20abbreviations%20to%20Chinese%20characters%20is%20a%0Asignificant%20branch%20within%20the%20domain%20of%20Chinese%20Spelling%20Correction%20%28CSC%29.%20It%0Aplays%20an%20important%20role%20in%20many%20downstream%20applications%20like%20named%20entity%0Arecognition%20and%20sentiment%20analysis.%20This%20task%20is%20typically%20one%20of%20text-length%0Aalignment%20and%20seems%20easy%20to%20solve%3B%20however%2C%20due%20to%20the%20limited%20information%0Acontent%20in%20pinyin%20abbreviations%2C%20achieving%20accurate%20conversion%20is%20challenging.%0AIn%20this%20paper%2C%20we%20treat%20this%20as%20a%20Fill-Mask%20task%20then%20propose%20CNMBert%2C%20which%0Astands%20for%20zh-CN%20Pinyin%20Multi-mask%20Bert%20Model%2C%20as%20a%20solution%20to%20this%20issue.%20By%0Aintroducing%20a%20multi-mask%20strategy%20and%20Mixture-of-Experts%20%28MoE%29%20layers%2C%20CNMBert%0Aoutperforms%20fine-tuned%20GPT%20models%20and%20ChatGPT-4o%20with%20a%2061.53%20MRR%20score%20and%0A51.86%20accuracy%20on%20a%2010%2C373-sample%20test%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11770v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNMBert%253A%2520A%2520Model%2520for%2520Hanyu%2520Pinyin%2520Abbreviation%2520to%2520Character%2520Conversion%250A%2520%2520Task%26entry.906535625%3DZishuo%2520Feng%2520and%2520Feng%2520Cao%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520converting%2520hanyu%2520pinyin%2520abbreviations%2520to%2520Chinese%2520characters%2520is%2520a%250Asignificant%2520branch%2520within%2520the%2520domain%2520of%2520Chinese%2520Spelling%2520Correction%2520%2528CSC%2529.%2520It%250Aplays%2520an%2520important%2520role%2520in%2520many%2520downstream%2520applications%2520like%2520named%2520entity%250Arecognition%2520and%2520sentiment%2520analysis.%2520This%2520task%2520is%2520typically%2520one%2520of%2520text-length%250Aalignment%2520and%2520seems%2520easy%2520to%2520solve%253B%2520however%252C%2520due%2520to%2520the%2520limited%2520information%250Acontent%2520in%2520pinyin%2520abbreviations%252C%2520achieving%2520accurate%2520conversion%2520is%2520challenging.%250AIn%2520this%2520paper%252C%2520we%2520treat%2520this%2520as%2520a%2520Fill-Mask%2520task%2520then%2520propose%2520CNMBert%252C%2520which%250Astands%2520for%2520zh-CN%2520Pinyin%2520Multi-mask%2520Bert%2520Model%252C%2520as%2520a%2520solution%2520to%2520this%2520issue.%2520By%250Aintroducing%2520a%2520multi-mask%2520strategy%2520and%2520Mixture-of-Experts%2520%2528MoE%2529%2520layers%252C%2520CNMBert%250Aoutperforms%2520fine-tuned%2520GPT%2520models%2520and%2520ChatGPT-4o%2520with%2520a%252061.53%2520MRR%2520score%2520and%250A51.86%2520accuracy%2520on%2520a%252010%252C373-sample%2520test%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11770v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNMBert%3A%20A%20Model%20for%20Hanyu%20Pinyin%20Abbreviation%20to%20Character%20Conversion%0A%20%20Task&entry.906535625=Zishuo%20Feng%20and%20Feng%20Cao&entry.1292438233=%20%20The%20task%20of%20converting%20hanyu%20pinyin%20abbreviations%20to%20Chinese%20characters%20is%20a%0Asignificant%20branch%20within%20the%20domain%20of%20Chinese%20Spelling%20Correction%20%28CSC%29.%20It%0Aplays%20an%20important%20role%20in%20many%20downstream%20applications%20like%20named%20entity%0Arecognition%20and%20sentiment%20analysis.%20This%20task%20is%20typically%20one%20of%20text-length%0Aalignment%20and%20seems%20easy%20to%20solve%3B%20however%2C%20due%20to%20the%20limited%20information%0Acontent%20in%20pinyin%20abbreviations%2C%20achieving%20accurate%20conversion%20is%20challenging.%0AIn%20this%20paper%2C%20we%20treat%20this%20as%20a%20Fill-Mask%20task%20then%20propose%20CNMBert%2C%20which%0Astands%20for%20zh-CN%20Pinyin%20Multi-mask%20Bert%20Model%2C%20as%20a%20solution%20to%20this%20issue.%20By%0Aintroducing%20a%20multi-mask%20strategy%20and%20Mixture-of-Experts%20%28MoE%29%20layers%2C%20CNMBert%0Aoutperforms%20fine-tuned%20GPT%20models%20and%20ChatGPT-4o%20with%20a%2061.53%20MRR%20score%20and%0A51.86%20accuracy%20on%20a%2010%2C373-sample%20test%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11770v3&entry.124074799=Read"},
{"title": "Are Your LLMs Capable of Stable Reasoning?", "author": "Junnan Liu and Hongwei Liu and Linchen Xiao and Ziyi Wang and Kuikun Liu and Songyang Gao and Wenwei Zhang and Songyang Zhang and Kai Chen", "abstract": "  The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.\n", "link": "http://arxiv.org/abs/2412.13147v3", "date": "2025-01-06", "relevancy": 2.017, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&body=Title%3A%20Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F%0AAuthor%3A%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20demonstrated%0Aremarkable%20progress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%0Adiscrepancy%20persists%20between%20benchmark%20performances%20and%20real-world%0Aapplications.%20We%20identify%20this%20gap%20as%20primarily%20stemming%20from%20current%0Aevaluation%20protocols%20and%20metrics%2C%20which%20inadequately%20capture%20the%20full%20spectrum%0Aof%20LLM%20capabilities%2C%20particularly%20in%20complex%20reasoning%20tasks%20where%20both%0Aaccuracy%20and%20consistency%20are%20crucial.%20This%20work%20makes%20two%20key%20contributions.%0AFirst%2C%20we%20introduce%20G-Pass%40k%2C%20a%20novel%20evaluation%20metric%20that%20provides%20a%0Acontinuous%20assessment%20of%20model%20performance%20across%20multiple%20sampling%20attempts%2C%0Aquantifying%20both%20the%20model%27s%20peak%20performance%20potential%20and%20its%20stability.%0ASecond%2C%20we%20present%20LiveMathBench%2C%20a%20dynamic%20benchmark%20comprising%20challenging%2C%0Acontemporary%20mathematical%20problems%20designed%20to%20minimize%20data%20leakage%20risks%0Aduring%20evaluation.%20Through%20extensive%20experiments%20using%20G-Pass%40k%20on%0Astate-of-the-art%20LLMs%20with%20LiveMathBench%2C%20we%20provide%20comprehensive%20insights%0Ainto%20both%20their%20maximum%20capabilities%20and%20operational%20consistency.%20Our%20findings%0Areveal%20substantial%20room%20for%20improvement%20in%20LLMs%27%20%22realistic%22%20reasoning%0Acapabilities%2C%20highlighting%20the%20need%20for%20more%20robust%20evaluation%20methods.%20The%0Abenchmark%20and%20detailed%20results%20are%20available%20at%3A%0Ahttps%3A//github.com/open-compass/GPassK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13147v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Your%2520LLMs%2520Capable%2520of%2520Stable%2520Reasoning%253F%26entry.906535625%3DJunnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Linchen%2520Xiao%2520and%2520Ziyi%2520Wang%2520and%2520Kuikun%2520Liu%2520and%2520Songyang%2520Gao%2520and%2520Wenwei%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520demonstrated%250Aremarkable%2520progress%2520in%2520complex%2520reasoning%2520tasks.%2520However%252C%2520a%2520significant%250Adiscrepancy%2520persists%2520between%2520benchmark%2520performances%2520and%2520real-world%250Aapplications.%2520We%2520identify%2520this%2520gap%2520as%2520primarily%2520stemming%2520from%2520current%250Aevaluation%2520protocols%2520and%2520metrics%252C%2520which%2520inadequately%2520capture%2520the%2520full%2520spectrum%250Aof%2520LLM%2520capabilities%252C%2520particularly%2520in%2520complex%2520reasoning%2520tasks%2520where%2520both%250Aaccuracy%2520and%2520consistency%2520are%2520crucial.%2520This%2520work%2520makes%2520two%2520key%2520contributions.%250AFirst%252C%2520we%2520introduce%2520G-Pass%2540k%252C%2520a%2520novel%2520evaluation%2520metric%2520that%2520provides%2520a%250Acontinuous%2520assessment%2520of%2520model%2520performance%2520across%2520multiple%2520sampling%2520attempts%252C%250Aquantifying%2520both%2520the%2520model%2527s%2520peak%2520performance%2520potential%2520and%2520its%2520stability.%250ASecond%252C%2520we%2520present%2520LiveMathBench%252C%2520a%2520dynamic%2520benchmark%2520comprising%2520challenging%252C%250Acontemporary%2520mathematical%2520problems%2520designed%2520to%2520minimize%2520data%2520leakage%2520risks%250Aduring%2520evaluation.%2520Through%2520extensive%2520experiments%2520using%2520G-Pass%2540k%2520on%250Astate-of-the-art%2520LLMs%2520with%2520LiveMathBench%252C%2520we%2520provide%2520comprehensive%2520insights%250Ainto%2520both%2520their%2520maximum%2520capabilities%2520and%2520operational%2520consistency.%2520Our%2520findings%250Areveal%2520substantial%2520room%2520for%2520improvement%2520in%2520LLMs%2527%2520%2522realistic%2522%2520reasoning%250Acapabilities%252C%2520highlighting%2520the%2520need%2520for%2520more%2520robust%2520evaluation%2520methods.%2520The%250Abenchmark%2520and%2520detailed%2520results%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/open-compass/GPassK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13147v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Your%20LLMs%20Capable%20of%20Stable%20Reasoning%3F&entry.906535625=Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Ziyi%20Wang%20and%20Kuikun%20Liu%20and%20Songyang%20Gao%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20demonstrated%0Aremarkable%20progress%20in%20complex%20reasoning%20tasks.%20However%2C%20a%20significant%0Adiscrepancy%20persists%20between%20benchmark%20performances%20and%20real-world%0Aapplications.%20We%20identify%20this%20gap%20as%20primarily%20stemming%20from%20current%0Aevaluation%20protocols%20and%20metrics%2C%20which%20inadequately%20capture%20the%20full%20spectrum%0Aof%20LLM%20capabilities%2C%20particularly%20in%20complex%20reasoning%20tasks%20where%20both%0Aaccuracy%20and%20consistency%20are%20crucial.%20This%20work%20makes%20two%20key%20contributions.%0AFirst%2C%20we%20introduce%20G-Pass%40k%2C%20a%20novel%20evaluation%20metric%20that%20provides%20a%0Acontinuous%20assessment%20of%20model%20performance%20across%20multiple%20sampling%20attempts%2C%0Aquantifying%20both%20the%20model%27s%20peak%20performance%20potential%20and%20its%20stability.%0ASecond%2C%20we%20present%20LiveMathBench%2C%20a%20dynamic%20benchmark%20comprising%20challenging%2C%0Acontemporary%20mathematical%20problems%20designed%20to%20minimize%20data%20leakage%20risks%0Aduring%20evaluation.%20Through%20extensive%20experiments%20using%20G-Pass%40k%20on%0Astate-of-the-art%20LLMs%20with%20LiveMathBench%2C%20we%20provide%20comprehensive%20insights%0Ainto%20both%20their%20maximum%20capabilities%20and%20operational%20consistency.%20Our%20findings%0Areveal%20substantial%20room%20for%20improvement%20in%20LLMs%27%20%22realistic%22%20reasoning%0Acapabilities%2C%20highlighting%20the%20need%20for%20more%20robust%20evaluation%20methods.%20The%0Abenchmark%20and%20detailed%20results%20are%20available%20at%3A%0Ahttps%3A//github.com/open-compass/GPassK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13147v3&entry.124074799=Read"},
{"title": "Single-Channel Distance-Based Source Separation for Mobile GPU in\n  Outdoor and Indoor Environments", "author": "Hanbin Bae and Byungjun Kang and Jiwon Kim and Jaeyong Hwang and Hosang Sung and Hoon-Young Cho", "abstract": "  This study emphasizes the significance of exploring distance-based source\nseparation (DSS) in outdoor environments. Unlike existing studies that\nprimarily focus on indoor settings, the proposed model is designed to capture\nthe unique characteristics of outdoor audio sources. It incorporates advanced\ntechniques, including a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA\nmay not capture physical cues as explicitly as the quadratic RSA, the linear\nRSA enhances the model's context awareness, leading to improved performance on\nthe DSS that requires an understanding of physical cues in outdoor and indoor\nenvironments. The experimental results demonstrated that the proposed model\novercomes the limitations of existing approaches and considerably enhances\nenergy efficiency and real-time inference speed on mobile devices.\n", "link": "http://arxiv.org/abs/2501.03045v1", "date": "2025-01-06", "relevancy": 2.0167, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5587}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Channel%20Distance-Based%20Source%20Separation%20for%20Mobile%20GPU%20in%0A%20%20Outdoor%20and%20Indoor%20Environments&body=Title%3A%20Single-Channel%20Distance-Based%20Source%20Separation%20for%20Mobile%20GPU%20in%0A%20%20Outdoor%20and%20Indoor%20Environments%0AAuthor%3A%20Hanbin%20Bae%20and%20Byungjun%20Kang%20and%20Jiwon%20Kim%20and%20Jaeyong%20Hwang%20and%20Hosang%20Sung%20and%20Hoon-Young%20Cho%0AAbstract%3A%20%20%20This%20study%20emphasizes%20the%20significance%20of%20exploring%20distance-based%20source%0Aseparation%20%28DSS%29%20in%20outdoor%20environments.%20Unlike%20existing%20studies%20that%0Aprimarily%20focus%20on%20indoor%20settings%2C%20the%20proposed%20model%20is%20designed%20to%20capture%0Athe%20unique%20characteristics%20of%20outdoor%20audio%20sources.%20It%20incorporates%20advanced%0Atechniques%2C%20including%20a%20two-stage%20conformer%20block%2C%20a%20linear%20relation-aware%0Aself-attention%20%28RSA%29%2C%20and%20a%20TensorFlow%20Lite%20GPU%20delegate.%20While%20the%20linear%20RSA%0Amay%20not%20capture%20physical%20cues%20as%20explicitly%20as%20the%20quadratic%20RSA%2C%20the%20linear%0ARSA%20enhances%20the%20model%27s%20context%20awareness%2C%20leading%20to%20improved%20performance%20on%0Athe%20DSS%20that%20requires%20an%20understanding%20of%20physical%20cues%20in%20outdoor%20and%20indoor%0Aenvironments.%20The%20experimental%20results%20demonstrated%20that%20the%20proposed%20model%0Aovercomes%20the%20limitations%20of%20existing%20approaches%20and%20considerably%20enhances%0Aenergy%20efficiency%20and%20real-time%20inference%20speed%20on%20mobile%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Channel%2520Distance-Based%2520Source%2520Separation%2520for%2520Mobile%2520GPU%2520in%250A%2520%2520Outdoor%2520and%2520Indoor%2520Environments%26entry.906535625%3DHanbin%2520Bae%2520and%2520Byungjun%2520Kang%2520and%2520Jiwon%2520Kim%2520and%2520Jaeyong%2520Hwang%2520and%2520Hosang%2520Sung%2520and%2520Hoon-Young%2520Cho%26entry.1292438233%3D%2520%2520This%2520study%2520emphasizes%2520the%2520significance%2520of%2520exploring%2520distance-based%2520source%250Aseparation%2520%2528DSS%2529%2520in%2520outdoor%2520environments.%2520Unlike%2520existing%2520studies%2520that%250Aprimarily%2520focus%2520on%2520indoor%2520settings%252C%2520the%2520proposed%2520model%2520is%2520designed%2520to%2520capture%250Athe%2520unique%2520characteristics%2520of%2520outdoor%2520audio%2520sources.%2520It%2520incorporates%2520advanced%250Atechniques%252C%2520including%2520a%2520two-stage%2520conformer%2520block%252C%2520a%2520linear%2520relation-aware%250Aself-attention%2520%2528RSA%2529%252C%2520and%2520a%2520TensorFlow%2520Lite%2520GPU%2520delegate.%2520While%2520the%2520linear%2520RSA%250Amay%2520not%2520capture%2520physical%2520cues%2520as%2520explicitly%2520as%2520the%2520quadratic%2520RSA%252C%2520the%2520linear%250ARSA%2520enhances%2520the%2520model%2527s%2520context%2520awareness%252C%2520leading%2520to%2520improved%2520performance%2520on%250Athe%2520DSS%2520that%2520requires%2520an%2520understanding%2520of%2520physical%2520cues%2520in%2520outdoor%2520and%2520indoor%250Aenvironments.%2520The%2520experimental%2520results%2520demonstrated%2520that%2520the%2520proposed%2520model%250Aovercomes%2520the%2520limitations%2520of%2520existing%2520approaches%2520and%2520considerably%2520enhances%250Aenergy%2520efficiency%2520and%2520real-time%2520inference%2520speed%2520on%2520mobile%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Channel%20Distance-Based%20Source%20Separation%20for%20Mobile%20GPU%20in%0A%20%20Outdoor%20and%20Indoor%20Environments&entry.906535625=Hanbin%20Bae%20and%20Byungjun%20Kang%20and%20Jiwon%20Kim%20and%20Jaeyong%20Hwang%20and%20Hosang%20Sung%20and%20Hoon-Young%20Cho&entry.1292438233=%20%20This%20study%20emphasizes%20the%20significance%20of%20exploring%20distance-based%20source%0Aseparation%20%28DSS%29%20in%20outdoor%20environments.%20Unlike%20existing%20studies%20that%0Aprimarily%20focus%20on%20indoor%20settings%2C%20the%20proposed%20model%20is%20designed%20to%20capture%0Athe%20unique%20characteristics%20of%20outdoor%20audio%20sources.%20It%20incorporates%20advanced%0Atechniques%2C%20including%20a%20two-stage%20conformer%20block%2C%20a%20linear%20relation-aware%0Aself-attention%20%28RSA%29%2C%20and%20a%20TensorFlow%20Lite%20GPU%20delegate.%20While%20the%20linear%20RSA%0Amay%20not%20capture%20physical%20cues%20as%20explicitly%20as%20the%20quadratic%20RSA%2C%20the%20linear%0ARSA%20enhances%20the%20model%27s%20context%20awareness%2C%20leading%20to%20improved%20performance%20on%0Athe%20DSS%20that%20requires%20an%20understanding%20of%20physical%20cues%20in%20outdoor%20and%20indoor%0Aenvironments.%20The%20experimental%20results%20demonstrated%20that%20the%20proposed%20model%0Aovercomes%20the%20limitations%20of%20existing%20approaches%20and%20considerably%20enhances%0Aenergy%20efficiency%20and%20real-time%20inference%20speed%20on%20mobile%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03045v1&entry.124074799=Read"},
{"title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent\n  Diffusion Models", "author": "Jingfeng Yao and Xinggang Wang", "abstract": "  Latent diffusion models with Transformer architectures excel at generating\nhigh-fidelity images. However, recent studies reveal an optimization dilemma in\nthis two-stage design: while increasing the per-token feature dimension in\nvisual tokenizers improves reconstruction quality, it requires substantially\nlarger diffusion models and more training iterations to achieve comparable\ngeneration performance. Consequently, existing systems often settle for\nsub-optimal solutions, either producing visual artifacts due to information\nloss within tokenizers or failing to converge fully due to expensive\ncomputation costs. We argue that this dilemma stems from the inherent\ndifficulty in learning unconstrained high-dimensional latent spaces. To address\nthis, we propose aligning the latent space with pre-trained vision foundation\nmodels when training the visual tokenizers. Our proposed VA-VAE (Vision\nfoundation model Aligned Variational AutoEncoder) significantly expands the\nreconstruction-generation frontier of latent diffusion models, enabling faster\nconvergence of Diffusion Transformers (DiT) in high-dimensional latent spaces.\nTo exploit the full potential of VA-VAE, we build an enhanced DiT baseline with\nimproved training strategies and architecture designs, termed LightningDiT. The\nintegrated system achieves state-of-the-art (SOTA) performance on ImageNet\n256x256 generation with an FID score of 1.35 while demonstrating remarkable\ntraining efficiency by reaching an FID score of 2.11 in just 64\nepochs--representing an over 21 times convergence speedup compared to the\noriginal DiT. Models and codes are available at:\nhttps://github.com/hustvl/LightningDiT.\n", "link": "http://arxiv.org/abs/2501.01423v2", "date": "2025-01-06", "relevancy": 2.0076, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6836}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6713}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20vs.%20Generation%3A%20Taming%20Optimization%20Dilemma%20in%20Latent%0A%20%20Diffusion%20Models&body=Title%3A%20Reconstruction%20vs.%20Generation%3A%20Taming%20Optimization%20Dilemma%20in%20Latent%0A%20%20Diffusion%20Models%0AAuthor%3A%20Jingfeng%20Yao%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Latent%20diffusion%20models%20with%20Transformer%20architectures%20excel%20at%20generating%0Ahigh-fidelity%20images.%20However%2C%20recent%20studies%20reveal%20an%20optimization%20dilemma%20in%0Athis%20two-stage%20design%3A%20while%20increasing%20the%20per-token%20feature%20dimension%20in%0Avisual%20tokenizers%20improves%20reconstruction%20quality%2C%20it%20requires%20substantially%0Alarger%20diffusion%20models%20and%20more%20training%20iterations%20to%20achieve%20comparable%0Ageneration%20performance.%20Consequently%2C%20existing%20systems%20often%20settle%20for%0Asub-optimal%20solutions%2C%20either%20producing%20visual%20artifacts%20due%20to%20information%0Aloss%20within%20tokenizers%20or%20failing%20to%20converge%20fully%20due%20to%20expensive%0Acomputation%20costs.%20We%20argue%20that%20this%20dilemma%20stems%20from%20the%20inherent%0Adifficulty%20in%20learning%20unconstrained%20high-dimensional%20latent%20spaces.%20To%20address%0Athis%2C%20we%20propose%20aligning%20the%20latent%20space%20with%20pre-trained%20vision%20foundation%0Amodels%20when%20training%20the%20visual%20tokenizers.%20Our%20proposed%20VA-VAE%20%28Vision%0Afoundation%20model%20Aligned%20Variational%20AutoEncoder%29%20significantly%20expands%20the%0Areconstruction-generation%20frontier%20of%20latent%20diffusion%20models%2C%20enabling%20faster%0Aconvergence%20of%20Diffusion%20Transformers%20%28DiT%29%20in%20high-dimensional%20latent%20spaces.%0ATo%20exploit%20the%20full%20potential%20of%20VA-VAE%2C%20we%20build%20an%20enhanced%20DiT%20baseline%20with%0Aimproved%20training%20strategies%20and%20architecture%20designs%2C%20termed%20LightningDiT.%20The%0Aintegrated%20system%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20ImageNet%0A256x256%20generation%20with%20an%20FID%20score%20of%201.35%20while%20demonstrating%20remarkable%0Atraining%20efficiency%20by%20reaching%20an%20FID%20score%20of%202.11%20in%20just%2064%0Aepochs--representing%20an%20over%2021%20times%20convergence%20speedup%20compared%20to%20the%0Aoriginal%20DiT.%20Models%20and%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/hustvl/LightningDiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520vs.%2520Generation%253A%2520Taming%2520Optimization%2520Dilemma%2520in%2520Latent%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DJingfeng%2520Yao%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Latent%2520diffusion%2520models%2520with%2520Transformer%2520architectures%2520excel%2520at%2520generating%250Ahigh-fidelity%2520images.%2520However%252C%2520recent%2520studies%2520reveal%2520an%2520optimization%2520dilemma%2520in%250Athis%2520two-stage%2520design%253A%2520while%2520increasing%2520the%2520per-token%2520feature%2520dimension%2520in%250Avisual%2520tokenizers%2520improves%2520reconstruction%2520quality%252C%2520it%2520requires%2520substantially%250Alarger%2520diffusion%2520models%2520and%2520more%2520training%2520iterations%2520to%2520achieve%2520comparable%250Ageneration%2520performance.%2520Consequently%252C%2520existing%2520systems%2520often%2520settle%2520for%250Asub-optimal%2520solutions%252C%2520either%2520producing%2520visual%2520artifacts%2520due%2520to%2520information%250Aloss%2520within%2520tokenizers%2520or%2520failing%2520to%2520converge%2520fully%2520due%2520to%2520expensive%250Acomputation%2520costs.%2520We%2520argue%2520that%2520this%2520dilemma%2520stems%2520from%2520the%2520inherent%250Adifficulty%2520in%2520learning%2520unconstrained%2520high-dimensional%2520latent%2520spaces.%2520To%2520address%250Athis%252C%2520we%2520propose%2520aligning%2520the%2520latent%2520space%2520with%2520pre-trained%2520vision%2520foundation%250Amodels%2520when%2520training%2520the%2520visual%2520tokenizers.%2520Our%2520proposed%2520VA-VAE%2520%2528Vision%250Afoundation%2520model%2520Aligned%2520Variational%2520AutoEncoder%2529%2520significantly%2520expands%2520the%250Areconstruction-generation%2520frontier%2520of%2520latent%2520diffusion%2520models%252C%2520enabling%2520faster%250Aconvergence%2520of%2520Diffusion%2520Transformers%2520%2528DiT%2529%2520in%2520high-dimensional%2520latent%2520spaces.%250ATo%2520exploit%2520the%2520full%2520potential%2520of%2520VA-VAE%252C%2520we%2520build%2520an%2520enhanced%2520DiT%2520baseline%2520with%250Aimproved%2520training%2520strategies%2520and%2520architecture%2520designs%252C%2520termed%2520LightningDiT.%2520The%250Aintegrated%2520system%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520ImageNet%250A256x256%2520generation%2520with%2520an%2520FID%2520score%2520of%25201.35%2520while%2520demonstrating%2520remarkable%250Atraining%2520efficiency%2520by%2520reaching%2520an%2520FID%2520score%2520of%25202.11%2520in%2520just%252064%250Aepochs--representing%2520an%2520over%252021%2520times%2520convergence%2520speedup%2520compared%2520to%2520the%250Aoriginal%2520DiT.%2520Models%2520and%2520codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/hustvl/LightningDiT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20vs.%20Generation%3A%20Taming%20Optimization%20Dilemma%20in%20Latent%0A%20%20Diffusion%20Models&entry.906535625=Jingfeng%20Yao%20and%20Xinggang%20Wang&entry.1292438233=%20%20Latent%20diffusion%20models%20with%20Transformer%20architectures%20excel%20at%20generating%0Ahigh-fidelity%20images.%20However%2C%20recent%20studies%20reveal%20an%20optimization%20dilemma%20in%0Athis%20two-stage%20design%3A%20while%20increasing%20the%20per-token%20feature%20dimension%20in%0Avisual%20tokenizers%20improves%20reconstruction%20quality%2C%20it%20requires%20substantially%0Alarger%20diffusion%20models%20and%20more%20training%20iterations%20to%20achieve%20comparable%0Ageneration%20performance.%20Consequently%2C%20existing%20systems%20often%20settle%20for%0Asub-optimal%20solutions%2C%20either%20producing%20visual%20artifacts%20due%20to%20information%0Aloss%20within%20tokenizers%20or%20failing%20to%20converge%20fully%20due%20to%20expensive%0Acomputation%20costs.%20We%20argue%20that%20this%20dilemma%20stems%20from%20the%20inherent%0Adifficulty%20in%20learning%20unconstrained%20high-dimensional%20latent%20spaces.%20To%20address%0Athis%2C%20we%20propose%20aligning%20the%20latent%20space%20with%20pre-trained%20vision%20foundation%0Amodels%20when%20training%20the%20visual%20tokenizers.%20Our%20proposed%20VA-VAE%20%28Vision%0Afoundation%20model%20Aligned%20Variational%20AutoEncoder%29%20significantly%20expands%20the%0Areconstruction-generation%20frontier%20of%20latent%20diffusion%20models%2C%20enabling%20faster%0Aconvergence%20of%20Diffusion%20Transformers%20%28DiT%29%20in%20high-dimensional%20latent%20spaces.%0ATo%20exploit%20the%20full%20potential%20of%20VA-VAE%2C%20we%20build%20an%20enhanced%20DiT%20baseline%20with%0Aimproved%20training%20strategies%20and%20architecture%20designs%2C%20termed%20LightningDiT.%20The%0Aintegrated%20system%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20ImageNet%0A256x256%20generation%20with%20an%20FID%20score%20of%201.35%20while%20demonstrating%20remarkable%0Atraining%20efficiency%20by%20reaching%20an%20FID%20score%20of%202.11%20in%20just%2064%0Aepochs--representing%20an%20over%2021%20times%20convergence%20speedup%20compared%20to%20the%0Aoriginal%20DiT.%20Models%20and%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/hustvl/LightningDiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01423v2&entry.124074799=Read"},
{"title": "A Novel Automatic Real-time Motion Tracking Method for Magnetic\n  Resonance Imaging-guided Radiotherapy: Leveraging the Enhanced\n  Tracking-Learning-Detection Framework with Automatic Segmentation", "author": "Shengqi Chen and Zilin Wang and Jianrong Dai and Shirui Qin and Ying Cao and Ruiao Zhao and Jiayun Chen and Guohua Wu and Yuan Tang", "abstract": "  Background and Purpose: Accurate motion tracking in MRI-guided Radiotherapy\n(MRIgRT) is essential for effective treatment delivery. This study aimed to\nenhance motion tracking precision in MRIgRT through an automatic real-time\nmarkerless tracking method using an enhanced Tracking-Learning-Detection (ETLD)\nframework with automatic segmentation. Materials and Methods: We developed a\nnovel MRIgRT motion tracking and segmentation method by integrating the ETLD\nframework with an improved Chan-Vese model (ICV), named ETLD+ICV. The ETLD\nframework was upgraded for real-time cine MRI, including advanced image\npreprocessing, no-reference image quality assessment, an enhanced median-flow\ntracker, and a refined detector with dynamic search region adjustments. ICV was\nused for precise target volume coverage, refining the segmented region frame by\nframe using tracking results, with key parameters optimized. The method was\ntested on 3.5D MRI scans from 10 patients with liver metastases. Results:\nEvaluation of 106,000 frames across 77 treatment fractions showed\nsub-millimeter tracking errors of less than 0.8mm, with over 99% precision and\n98% recall for all subjects in the Beam Eye View(BEV)/Beam Path View(BPV)\norientation. The ETLD+ICV method achieved a dice global score of more than 82%\nfor all subjects, demonstrating the method's extensibility and precise target\nvolume coverage. Conclusion: This study successfully developed an automatic\nreal-time markerless motion tracking method for MRIgRT that significantly\noutperforms current methods. The novel method not only delivers exceptional\nprecision in tracking and segmentation but also shows enhanced adaptability to\nclinical demands, making it an indispensable asset in improving the efficacy of\nradiotherapy treatments.\n", "link": "http://arxiv.org/abs/2411.07503v2", "date": "2025-01-06", "relevancy": 2.0074, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5073}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Automatic%20Real-time%20Motion%20Tracking%20Method%20for%20Magnetic%0A%20%20Resonance%20Imaging-guided%20Radiotherapy%3A%20Leveraging%20the%20Enhanced%0A%20%20Tracking-Learning-Detection%20Framework%20with%20Automatic%20Segmentation&body=Title%3A%20A%20Novel%20Automatic%20Real-time%20Motion%20Tracking%20Method%20for%20Magnetic%0A%20%20Resonance%20Imaging-guided%20Radiotherapy%3A%20Leveraging%20the%20Enhanced%0A%20%20Tracking-Learning-Detection%20Framework%20with%20Automatic%20Segmentation%0AAuthor%3A%20Shengqi%20Chen%20and%20Zilin%20Wang%20and%20Jianrong%20Dai%20and%20Shirui%20Qin%20and%20Ying%20Cao%20and%20Ruiao%20Zhao%20and%20Jiayun%20Chen%20and%20Guohua%20Wu%20and%20Yuan%20Tang%0AAbstract%3A%20%20%20Background%20and%20Purpose%3A%20Accurate%20motion%20tracking%20in%20MRI-guided%20Radiotherapy%0A%28MRIgRT%29%20is%20essential%20for%20effective%20treatment%20delivery.%20This%20study%20aimed%20to%0Aenhance%20motion%20tracking%20precision%20in%20MRIgRT%20through%20an%20automatic%20real-time%0Amarkerless%20tracking%20method%20using%20an%20enhanced%20Tracking-Learning-Detection%20%28ETLD%29%0Aframework%20with%20automatic%20segmentation.%20Materials%20and%20Methods%3A%20We%20developed%20a%0Anovel%20MRIgRT%20motion%20tracking%20and%20segmentation%20method%20by%20integrating%20the%20ETLD%0Aframework%20with%20an%20improved%20Chan-Vese%20model%20%28ICV%29%2C%20named%20ETLD%2BICV.%20The%20ETLD%0Aframework%20was%20upgraded%20for%20real-time%20cine%20MRI%2C%20including%20advanced%20image%0Apreprocessing%2C%20no-reference%20image%20quality%20assessment%2C%20an%20enhanced%20median-flow%0Atracker%2C%20and%20a%20refined%20detector%20with%20dynamic%20search%20region%20adjustments.%20ICV%20was%0Aused%20for%20precise%20target%20volume%20coverage%2C%20refining%20the%20segmented%20region%20frame%20by%0Aframe%20using%20tracking%20results%2C%20with%20key%20parameters%20optimized.%20The%20method%20was%0Atested%20on%203.5D%20MRI%20scans%20from%2010%20patients%20with%20liver%20metastases.%20Results%3A%0AEvaluation%20of%20106%2C000%20frames%20across%2077%20treatment%20fractions%20showed%0Asub-millimeter%20tracking%20errors%20of%20less%20than%200.8mm%2C%20with%20over%2099%25%20precision%20and%0A98%25%20recall%20for%20all%20subjects%20in%20the%20Beam%20Eye%20View%28BEV%29/Beam%20Path%20View%28BPV%29%0Aorientation.%20The%20ETLD%2BICV%20method%20achieved%20a%20dice%20global%20score%20of%20more%20than%2082%25%0Afor%20all%20subjects%2C%20demonstrating%20the%20method%27s%20extensibility%20and%20precise%20target%0Avolume%20coverage.%20Conclusion%3A%20This%20study%20successfully%20developed%20an%20automatic%0Areal-time%20markerless%20motion%20tracking%20method%20for%20MRIgRT%20that%20significantly%0Aoutperforms%20current%20methods.%20The%20novel%20method%20not%20only%20delivers%20exceptional%0Aprecision%20in%20tracking%20and%20segmentation%20but%20also%20shows%20enhanced%20adaptability%20to%0Aclinical%20demands%2C%20making%20it%20an%20indispensable%20asset%20in%20improving%20the%20efficacy%20of%0Aradiotherapy%20treatments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Automatic%2520Real-time%2520Motion%2520Tracking%2520Method%2520for%2520Magnetic%250A%2520%2520Resonance%2520Imaging-guided%2520Radiotherapy%253A%2520Leveraging%2520the%2520Enhanced%250A%2520%2520Tracking-Learning-Detection%2520Framework%2520with%2520Automatic%2520Segmentation%26entry.906535625%3DShengqi%2520Chen%2520and%2520Zilin%2520Wang%2520and%2520Jianrong%2520Dai%2520and%2520Shirui%2520Qin%2520and%2520Ying%2520Cao%2520and%2520Ruiao%2520Zhao%2520and%2520Jiayun%2520Chen%2520and%2520Guohua%2520Wu%2520and%2520Yuan%2520Tang%26entry.1292438233%3D%2520%2520Background%2520and%2520Purpose%253A%2520Accurate%2520motion%2520tracking%2520in%2520MRI-guided%2520Radiotherapy%250A%2528MRIgRT%2529%2520is%2520essential%2520for%2520effective%2520treatment%2520delivery.%2520This%2520study%2520aimed%2520to%250Aenhance%2520motion%2520tracking%2520precision%2520in%2520MRIgRT%2520through%2520an%2520automatic%2520real-time%250Amarkerless%2520tracking%2520method%2520using%2520an%2520enhanced%2520Tracking-Learning-Detection%2520%2528ETLD%2529%250Aframework%2520with%2520automatic%2520segmentation.%2520Materials%2520and%2520Methods%253A%2520We%2520developed%2520a%250Anovel%2520MRIgRT%2520motion%2520tracking%2520and%2520segmentation%2520method%2520by%2520integrating%2520the%2520ETLD%250Aframework%2520with%2520an%2520improved%2520Chan-Vese%2520model%2520%2528ICV%2529%252C%2520named%2520ETLD%252BICV.%2520The%2520ETLD%250Aframework%2520was%2520upgraded%2520for%2520real-time%2520cine%2520MRI%252C%2520including%2520advanced%2520image%250Apreprocessing%252C%2520no-reference%2520image%2520quality%2520assessment%252C%2520an%2520enhanced%2520median-flow%250Atracker%252C%2520and%2520a%2520refined%2520detector%2520with%2520dynamic%2520search%2520region%2520adjustments.%2520ICV%2520was%250Aused%2520for%2520precise%2520target%2520volume%2520coverage%252C%2520refining%2520the%2520segmented%2520region%2520frame%2520by%250Aframe%2520using%2520tracking%2520results%252C%2520with%2520key%2520parameters%2520optimized.%2520The%2520method%2520was%250Atested%2520on%25203.5D%2520MRI%2520scans%2520from%252010%2520patients%2520with%2520liver%2520metastases.%2520Results%253A%250AEvaluation%2520of%2520106%252C000%2520frames%2520across%252077%2520treatment%2520fractions%2520showed%250Asub-millimeter%2520tracking%2520errors%2520of%2520less%2520than%25200.8mm%252C%2520with%2520over%252099%2525%2520precision%2520and%250A98%2525%2520recall%2520for%2520all%2520subjects%2520in%2520the%2520Beam%2520Eye%2520View%2528BEV%2529/Beam%2520Path%2520View%2528BPV%2529%250Aorientation.%2520The%2520ETLD%252BICV%2520method%2520achieved%2520a%2520dice%2520global%2520score%2520of%2520more%2520than%252082%2525%250Afor%2520all%2520subjects%252C%2520demonstrating%2520the%2520method%2527s%2520extensibility%2520and%2520precise%2520target%250Avolume%2520coverage.%2520Conclusion%253A%2520This%2520study%2520successfully%2520developed%2520an%2520automatic%250Areal-time%2520markerless%2520motion%2520tracking%2520method%2520for%2520MRIgRT%2520that%2520significantly%250Aoutperforms%2520current%2520methods.%2520The%2520novel%2520method%2520not%2520only%2520delivers%2520exceptional%250Aprecision%2520in%2520tracking%2520and%2520segmentation%2520but%2520also%2520shows%2520enhanced%2520adaptability%2520to%250Aclinical%2520demands%252C%2520making%2520it%2520an%2520indispensable%2520asset%2520in%2520improving%2520the%2520efficacy%2520of%250Aradiotherapy%2520treatments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Automatic%20Real-time%20Motion%20Tracking%20Method%20for%20Magnetic%0A%20%20Resonance%20Imaging-guided%20Radiotherapy%3A%20Leveraging%20the%20Enhanced%0A%20%20Tracking-Learning-Detection%20Framework%20with%20Automatic%20Segmentation&entry.906535625=Shengqi%20Chen%20and%20Zilin%20Wang%20and%20Jianrong%20Dai%20and%20Shirui%20Qin%20and%20Ying%20Cao%20and%20Ruiao%20Zhao%20and%20Jiayun%20Chen%20and%20Guohua%20Wu%20and%20Yuan%20Tang&entry.1292438233=%20%20Background%20and%20Purpose%3A%20Accurate%20motion%20tracking%20in%20MRI-guided%20Radiotherapy%0A%28MRIgRT%29%20is%20essential%20for%20effective%20treatment%20delivery.%20This%20study%20aimed%20to%0Aenhance%20motion%20tracking%20precision%20in%20MRIgRT%20through%20an%20automatic%20real-time%0Amarkerless%20tracking%20method%20using%20an%20enhanced%20Tracking-Learning-Detection%20%28ETLD%29%0Aframework%20with%20automatic%20segmentation.%20Materials%20and%20Methods%3A%20We%20developed%20a%0Anovel%20MRIgRT%20motion%20tracking%20and%20segmentation%20method%20by%20integrating%20the%20ETLD%0Aframework%20with%20an%20improved%20Chan-Vese%20model%20%28ICV%29%2C%20named%20ETLD%2BICV.%20The%20ETLD%0Aframework%20was%20upgraded%20for%20real-time%20cine%20MRI%2C%20including%20advanced%20image%0Apreprocessing%2C%20no-reference%20image%20quality%20assessment%2C%20an%20enhanced%20median-flow%0Atracker%2C%20and%20a%20refined%20detector%20with%20dynamic%20search%20region%20adjustments.%20ICV%20was%0Aused%20for%20precise%20target%20volume%20coverage%2C%20refining%20the%20segmented%20region%20frame%20by%0Aframe%20using%20tracking%20results%2C%20with%20key%20parameters%20optimized.%20The%20method%20was%0Atested%20on%203.5D%20MRI%20scans%20from%2010%20patients%20with%20liver%20metastases.%20Results%3A%0AEvaluation%20of%20106%2C000%20frames%20across%2077%20treatment%20fractions%20showed%0Asub-millimeter%20tracking%20errors%20of%20less%20than%200.8mm%2C%20with%20over%2099%25%20precision%20and%0A98%25%20recall%20for%20all%20subjects%20in%20the%20Beam%20Eye%20View%28BEV%29/Beam%20Path%20View%28BPV%29%0Aorientation.%20The%20ETLD%2BICV%20method%20achieved%20a%20dice%20global%20score%20of%20more%20than%2082%25%0Afor%20all%20subjects%2C%20demonstrating%20the%20method%27s%20extensibility%20and%20precise%20target%0Avolume%20coverage.%20Conclusion%3A%20This%20study%20successfully%20developed%20an%20automatic%0Areal-time%20markerless%20motion%20tracking%20method%20for%20MRIgRT%20that%20significantly%0Aoutperforms%20current%20methods.%20The%20novel%20method%20not%20only%20delivers%20exceptional%0Aprecision%20in%20tracking%20and%20segmentation%20but%20also%20shows%20enhanced%20adaptability%20to%0Aclinical%20demands%2C%20making%20it%20an%20indispensable%20asset%20in%20improving%20the%20efficacy%20of%0Aradiotherapy%20treatments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07503v2&entry.124074799=Read"},
{"title": "QuArch: A Question-Answering Dataset for AI Agents in Computer\n  Architecture", "author": "Shvetank Prakash and Andrew Cheng and Jason Yik and Arya Tschand and Radhika Ghosal and Ikechukwu Uchendu and Jessica Quaye and Jeffrey Ma and Shreyas Grampurohit and Sofia Giannuzzi and Arnav Balyan and Fin Amin and Aadya Pipersenia and Yash Choudhary and Ankita Nayak and Amir Yazdanbakhsh and Vijay Janapa Reddi", "abstract": "  We introduce QuArch, a dataset of 1500 human-validated question-answer pairs\ndesigned to evaluate and enhance language models' understanding of computer\narchitecture. The dataset covers areas including processor design, memory\nsystems, and performance optimization. Our analysis highlights a significant\nperformance gap: the best closed-source model achieves 84% accuracy, while the\ntop small open-source model reaches 72%. We observe notable struggles in memory\nsystems, interconnection networks, and benchmarking. Fine-tuning with QuArch\nimproves small model accuracy by up to 8%, establishing a foundation for\nadvancing AI-driven computer architecture research. The dataset and leaderboard\nare at https://harvard-edge.github.io/QuArch/.\n", "link": "http://arxiv.org/abs/2501.01892v2", "date": "2025-01-06", "relevancy": 1.9996, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture&body=Title%3A%20QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture%0AAuthor%3A%20Shvetank%20Prakash%20and%20Andrew%20Cheng%20and%20Jason%20Yik%20and%20Arya%20Tschand%20and%20Radhika%20Ghosal%20and%20Ikechukwu%20Uchendu%20and%20Jessica%20Quaye%20and%20Jeffrey%20Ma%20and%20Shreyas%20Grampurohit%20and%20Sofia%20Giannuzzi%20and%20Arnav%20Balyan%20and%20Fin%20Amin%20and%20Aadya%20Pipersenia%20and%20Yash%20Choudhary%20and%20Ankita%20Nayak%20and%20Amir%20Yazdanbakhsh%20and%20Vijay%20Janapa%20Reddi%0AAbstract%3A%20%20%20We%20introduce%20QuArch%2C%20a%20dataset%20of%201500%20human-validated%20question-answer%20pairs%0Adesigned%20to%20evaluate%20and%20enhance%20language%20models%27%20understanding%20of%20computer%0Aarchitecture.%20The%20dataset%20covers%20areas%20including%20processor%20design%2C%20memory%0Asystems%2C%20and%20performance%20optimization.%20Our%20analysis%20highlights%20a%20significant%0Aperformance%20gap%3A%20the%20best%20closed-source%20model%20achieves%2084%25%20accuracy%2C%20while%20the%0Atop%20small%20open-source%20model%20reaches%2072%25.%20We%20observe%20notable%20struggles%20in%20memory%0Asystems%2C%20interconnection%20networks%2C%20and%20benchmarking.%20Fine-tuning%20with%20QuArch%0Aimproves%20small%20model%20accuracy%20by%20up%20to%208%25%2C%20establishing%20a%20foundation%20for%0Aadvancing%20AI-driven%20computer%20architecture%20research.%20The%20dataset%20and%20leaderboard%0Aare%20at%20https%3A//harvard-edge.github.io/QuArch/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01892v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuArch%253A%2520A%2520Question-Answering%2520Dataset%2520for%2520AI%2520Agents%2520in%2520Computer%250A%2520%2520Architecture%26entry.906535625%3DShvetank%2520Prakash%2520and%2520Andrew%2520Cheng%2520and%2520Jason%2520Yik%2520and%2520Arya%2520Tschand%2520and%2520Radhika%2520Ghosal%2520and%2520Ikechukwu%2520Uchendu%2520and%2520Jessica%2520Quaye%2520and%2520Jeffrey%2520Ma%2520and%2520Shreyas%2520Grampurohit%2520and%2520Sofia%2520Giannuzzi%2520and%2520Arnav%2520Balyan%2520and%2520Fin%2520Amin%2520and%2520Aadya%2520Pipersenia%2520and%2520Yash%2520Choudhary%2520and%2520Ankita%2520Nayak%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Vijay%2520Janapa%2520Reddi%26entry.1292438233%3D%2520%2520We%2520introduce%2520QuArch%252C%2520a%2520dataset%2520of%25201500%2520human-validated%2520question-answer%2520pairs%250Adesigned%2520to%2520evaluate%2520and%2520enhance%2520language%2520models%2527%2520understanding%2520of%2520computer%250Aarchitecture.%2520The%2520dataset%2520covers%2520areas%2520including%2520processor%2520design%252C%2520memory%250Asystems%252C%2520and%2520performance%2520optimization.%2520Our%2520analysis%2520highlights%2520a%2520significant%250Aperformance%2520gap%253A%2520the%2520best%2520closed-source%2520model%2520achieves%252084%2525%2520accuracy%252C%2520while%2520the%250Atop%2520small%2520open-source%2520model%2520reaches%252072%2525.%2520We%2520observe%2520notable%2520struggles%2520in%2520memory%250Asystems%252C%2520interconnection%2520networks%252C%2520and%2520benchmarking.%2520Fine-tuning%2520with%2520QuArch%250Aimproves%2520small%2520model%2520accuracy%2520by%2520up%2520to%25208%2525%252C%2520establishing%2520a%2520foundation%2520for%250Aadvancing%2520AI-driven%2520computer%2520architecture%2520research.%2520The%2520dataset%2520and%2520leaderboard%250Aare%2520at%2520https%253A//harvard-edge.github.io/QuArch/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01892v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuArch%3A%20A%20Question-Answering%20Dataset%20for%20AI%20Agents%20in%20Computer%0A%20%20Architecture&entry.906535625=Shvetank%20Prakash%20and%20Andrew%20Cheng%20and%20Jason%20Yik%20and%20Arya%20Tschand%20and%20Radhika%20Ghosal%20and%20Ikechukwu%20Uchendu%20and%20Jessica%20Quaye%20and%20Jeffrey%20Ma%20and%20Shreyas%20Grampurohit%20and%20Sofia%20Giannuzzi%20and%20Arnav%20Balyan%20and%20Fin%20Amin%20and%20Aadya%20Pipersenia%20and%20Yash%20Choudhary%20and%20Ankita%20Nayak%20and%20Amir%20Yazdanbakhsh%20and%20Vijay%20Janapa%20Reddi&entry.1292438233=%20%20We%20introduce%20QuArch%2C%20a%20dataset%20of%201500%20human-validated%20question-answer%20pairs%0Adesigned%20to%20evaluate%20and%20enhance%20language%20models%27%20understanding%20of%20computer%0Aarchitecture.%20The%20dataset%20covers%20areas%20including%20processor%20design%2C%20memory%0Asystems%2C%20and%20performance%20optimization.%20Our%20analysis%20highlights%20a%20significant%0Aperformance%20gap%3A%20the%20best%20closed-source%20model%20achieves%2084%25%20accuracy%2C%20while%20the%0Atop%20small%20open-source%20model%20reaches%2072%25.%20We%20observe%20notable%20struggles%20in%20memory%0Asystems%2C%20interconnection%20networks%2C%20and%20benchmarking.%20Fine-tuning%20with%20QuArch%0Aimproves%20small%20model%20accuracy%20by%20up%20to%208%25%2C%20establishing%20a%20foundation%20for%0Aadvancing%20AI-driven%20computer%20architecture%20research.%20The%20dataset%20and%20leaderboard%0Aare%20at%20https%3A//harvard-edge.github.io/QuArch/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01892v2&entry.124074799=Read"},
{"title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level\n  Reward Models", "author": "Mingyang Song and Zhaochen Su and Xiaoye Qu and Jiawei Zhou and Yu Cheng", "abstract": "  Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development.\n", "link": "http://arxiv.org/abs/2501.03124v1", "date": "2025-01-06", "relevancy": 1.9941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models&body=Title%3A%20PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models%0AAuthor%3A%20Mingyang%20Song%20and%20Zhaochen%20Su%20and%20Xiaoye%20Qu%20and%20Jiawei%20Zhou%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Process-level%20Reward%20Models%20%28PRMs%29%20are%20crucial%20for%20complex%20reasoning%20and%0Adecision-making%20tasks%2C%20where%20each%20intermediate%20step%20plays%20an%20important%20role%20in%0Athe%20reasoning%20process.%20Since%20language%20models%20are%20prone%20to%20various%20types%20of%0Aerrors%20during%20the%20reasoning%20process%2C%20PRMs%20are%20required%20to%20possess%20nuanced%0Acapabilities%20for%20detecting%20various%20implicit%20error%20types%20in%20real-world%0Ascenarios.%20However%2C%20current%20benchmarks%20primarily%20focus%20on%20step%20correctness%2C%0Afailing%20to%20evaluate%20PRMs%27%20performance%20systematically.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20PRMBench%2C%20a%20process-level%20benchmark%20specifically%20designed%20to%20assess%0Athe%20fine-grained%20error%20detection%20capabilities%20of%20PRMs.%20PRMBench%20comprises%206%2C216%0Acarefully%20designed%20problems%20and%2083%2C456%20step-level%20labels%2C%20evaluating%20models%0Aacross%20multiple%20dimensions%2C%20including%20simplicity%2C%20soundness%2C%20and%20sensitivity.%0AIn%20our%20experiments%20on%2015%20models%2C%20spanning%20both%20open-source%20PRMs%20and%0Aclosed-source%20large%20language%20models%20prompted%20as%20critic%20models%2C%20we%20uncover%0Asignificant%20weaknesses%20in%20current%20PRMs.%20These%20findings%20underscore%20the%0Achallenges%20inherent%20in%20process-level%20evaluation%20and%20highlight%20key%20directions%0Afor%20future%20research.%20We%20hope%20PRMBench%20can%20be%20a%20robust%20bench%20for%20advancing%0Aresearch%20on%20PRM%20evaluation%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRMBench%253A%2520A%2520Fine-grained%2520and%2520Challenging%2520Benchmark%2520for%2520Process-Level%250A%2520%2520Reward%2520Models%26entry.906535625%3DMingyang%2520Song%2520and%2520Zhaochen%2520Su%2520and%2520Xiaoye%2520Qu%2520and%2520Jiawei%2520Zhou%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Process-level%2520Reward%2520Models%2520%2528PRMs%2529%2520are%2520crucial%2520for%2520complex%2520reasoning%2520and%250Adecision-making%2520tasks%252C%2520where%2520each%2520intermediate%2520step%2520plays%2520an%2520important%2520role%2520in%250Athe%2520reasoning%2520process.%2520Since%2520language%2520models%2520are%2520prone%2520to%2520various%2520types%2520of%250Aerrors%2520during%2520the%2520reasoning%2520process%252C%2520PRMs%2520are%2520required%2520to%2520possess%2520nuanced%250Acapabilities%2520for%2520detecting%2520various%2520implicit%2520error%2520types%2520in%2520real-world%250Ascenarios.%2520However%252C%2520current%2520benchmarks%2520primarily%2520focus%2520on%2520step%2520correctness%252C%250Afailing%2520to%2520evaluate%2520PRMs%2527%2520performance%2520systematically.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520PRMBench%252C%2520a%2520process-level%2520benchmark%2520specifically%2520designed%2520to%2520assess%250Athe%2520fine-grained%2520error%2520detection%2520capabilities%2520of%2520PRMs.%2520PRMBench%2520comprises%25206%252C216%250Acarefully%2520designed%2520problems%2520and%252083%252C456%2520step-level%2520labels%252C%2520evaluating%2520models%250Aacross%2520multiple%2520dimensions%252C%2520including%2520simplicity%252C%2520soundness%252C%2520and%2520sensitivity.%250AIn%2520our%2520experiments%2520on%252015%2520models%252C%2520spanning%2520both%2520open-source%2520PRMs%2520and%250Aclosed-source%2520large%2520language%2520models%2520prompted%2520as%2520critic%2520models%252C%2520we%2520uncover%250Asignificant%2520weaknesses%2520in%2520current%2520PRMs.%2520These%2520findings%2520underscore%2520the%250Achallenges%2520inherent%2520in%2520process-level%2520evaluation%2520and%2520highlight%2520key%2520directions%250Afor%2520future%2520research.%2520We%2520hope%2520PRMBench%2520can%2520be%2520a%2520robust%2520bench%2520for%2520advancing%250Aresearch%2520on%2520PRM%2520evaluation%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models&entry.906535625=Mingyang%20Song%20and%20Zhaochen%20Su%20and%20Xiaoye%20Qu%20and%20Jiawei%20Zhou%20and%20Yu%20Cheng&entry.1292438233=%20%20Process-level%20Reward%20Models%20%28PRMs%29%20are%20crucial%20for%20complex%20reasoning%20and%0Adecision-making%20tasks%2C%20where%20each%20intermediate%20step%20plays%20an%20important%20role%20in%0Athe%20reasoning%20process.%20Since%20language%20models%20are%20prone%20to%20various%20types%20of%0Aerrors%20during%20the%20reasoning%20process%2C%20PRMs%20are%20required%20to%20possess%20nuanced%0Acapabilities%20for%20detecting%20various%20implicit%20error%20types%20in%20real-world%0Ascenarios.%20However%2C%20current%20benchmarks%20primarily%20focus%20on%20step%20correctness%2C%0Afailing%20to%20evaluate%20PRMs%27%20performance%20systematically.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20PRMBench%2C%20a%20process-level%20benchmark%20specifically%20designed%20to%20assess%0Athe%20fine-grained%20error%20detection%20capabilities%20of%20PRMs.%20PRMBench%20comprises%206%2C216%0Acarefully%20designed%20problems%20and%2083%2C456%20step-level%20labels%2C%20evaluating%20models%0Aacross%20multiple%20dimensions%2C%20including%20simplicity%2C%20soundness%2C%20and%20sensitivity.%0AIn%20our%20experiments%20on%2015%20models%2C%20spanning%20both%20open-source%20PRMs%20and%0Aclosed-source%20large%20language%20models%20prompted%20as%20critic%20models%2C%20we%20uncover%0Asignificant%20weaknesses%20in%20current%20PRMs.%20These%20findings%20underscore%20the%0Achallenges%20inherent%20in%20process-level%20evaluation%20and%20highlight%20key%20directions%0Afor%20future%20research.%20We%20hope%20PRMBench%20can%20be%20a%20robust%20bench%20for%20advancing%0Aresearch%20on%20PRM%20evaluation%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03124v1&entry.124074799=Read"},
{"title": "MSA-CNN: A Lightweight Multi-Scale CNN with Attention for Sleep Stage\n  Classification", "author": "Stephan Goerttler and Yucheng Wang and Emadeldeen Eldele and Min Wu and Fei He", "abstract": "  Recent advancements in machine learning-based signal analysis, coupled with\nopen data initiatives, have fuelled efforts in automatic sleep stage\nclassification. Despite the proliferation of classification models, few have\nprioritised reducing model complexity, which is a crucial factor for practical\napplications. In this work, we introduce Multi-Scale and Attention\nConvolutional Neural Network (MSA-CNN), a lightweight architecture featuring as\nfew as ~10,000 parameters. MSA-CNN leverages a novel multi-scale module\nemploying complementary pooling to eliminate redundant filter parameters and\ndense convolutions. Model complexity is further reduced by separating temporal\nand spatial feature extraction and using cost-effective global spatial\nconvolutions. This separation of tasks not only reduces model complexity but\nalso mirrors the approach used by human experts in sleep stage scoring. We\nevaluated both small and large configurations of MSA-CNN against nine\nstate-of-the-art baseline models across three public datasets, treating\nunivariate and multivariate models separately. Our evaluation, based on\nrepeated cross-validation and re-evaluation of all baseline models,\ndemonstrated that the large MSA-CNN outperformed all baseline models on all\nthree datasets in terms of accuracy and Cohen's kappa, despite its\nsignificantly reduced parameter count. Lastly, we explored various model\nvariants and conducted an in-depth analysis of the key modules and techniques,\nproviding deeper insights into the underlying mechanisms. The code for our\nmodels, baselines, and evaluation procedures is available at\nhttps://github.com/sgoerttler/MSA-CNN.\n", "link": "http://arxiv.org/abs/2501.02949v1", "date": "2025-01-06", "relevancy": 1.9752, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4954}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSA-CNN%3A%20A%20Lightweight%20Multi-Scale%20CNN%20with%20Attention%20for%20Sleep%20Stage%0A%20%20Classification&body=Title%3A%20MSA-CNN%3A%20A%20Lightweight%20Multi-Scale%20CNN%20with%20Attention%20for%20Sleep%20Stage%0A%20%20Classification%0AAuthor%3A%20Stephan%20Goerttler%20and%20Yucheng%20Wang%20and%20Emadeldeen%20Eldele%20and%20Min%20Wu%20and%20Fei%20He%0AAbstract%3A%20%20%20Recent%20advancements%20in%20machine%20learning-based%20signal%20analysis%2C%20coupled%20with%0Aopen%20data%20initiatives%2C%20have%20fuelled%20efforts%20in%20automatic%20sleep%20stage%0Aclassification.%20Despite%20the%20proliferation%20of%20classification%20models%2C%20few%20have%0Aprioritised%20reducing%20model%20complexity%2C%20which%20is%20a%20crucial%20factor%20for%20practical%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20Multi-Scale%20and%20Attention%0AConvolutional%20Neural%20Network%20%28MSA-CNN%29%2C%20a%20lightweight%20architecture%20featuring%20as%0Afew%20as%20~10%2C000%20parameters.%20MSA-CNN%20leverages%20a%20novel%20multi-scale%20module%0Aemploying%20complementary%20pooling%20to%20eliminate%20redundant%20filter%20parameters%20and%0Adense%20convolutions.%20Model%20complexity%20is%20further%20reduced%20by%20separating%20temporal%0Aand%20spatial%20feature%20extraction%20and%20using%20cost-effective%20global%20spatial%0Aconvolutions.%20This%20separation%20of%20tasks%20not%20only%20reduces%20model%20complexity%20but%0Aalso%20mirrors%20the%20approach%20used%20by%20human%20experts%20in%20sleep%20stage%20scoring.%20We%0Aevaluated%20both%20small%20and%20large%20configurations%20of%20MSA-CNN%20against%20nine%0Astate-of-the-art%20baseline%20models%20across%20three%20public%20datasets%2C%20treating%0Aunivariate%20and%20multivariate%20models%20separately.%20Our%20evaluation%2C%20based%20on%0Arepeated%20cross-validation%20and%20re-evaluation%20of%20all%20baseline%20models%2C%0Ademonstrated%20that%20the%20large%20MSA-CNN%20outperformed%20all%20baseline%20models%20on%20all%0Athree%20datasets%20in%20terms%20of%20accuracy%20and%20Cohen%27s%20kappa%2C%20despite%20its%0Asignificantly%20reduced%20parameter%20count.%20Lastly%2C%20we%20explored%20various%20model%0Avariants%20and%20conducted%20an%20in-depth%20analysis%20of%20the%20key%20modules%20and%20techniques%2C%0Aproviding%20deeper%20insights%20into%20the%20underlying%20mechanisms.%20The%20code%20for%20our%0Amodels%2C%20baselines%2C%20and%20evaluation%20procedures%20is%20available%20at%0Ahttps%3A//github.com/sgoerttler/MSA-CNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSA-CNN%253A%2520A%2520Lightweight%2520Multi-Scale%2520CNN%2520with%2520Attention%2520for%2520Sleep%2520Stage%250A%2520%2520Classification%26entry.906535625%3DStephan%2520Goerttler%2520and%2520Yucheng%2520Wang%2520and%2520Emadeldeen%2520Eldele%2520and%2520Min%2520Wu%2520and%2520Fei%2520He%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520machine%2520learning-based%2520signal%2520analysis%252C%2520coupled%2520with%250Aopen%2520data%2520initiatives%252C%2520have%2520fuelled%2520efforts%2520in%2520automatic%2520sleep%2520stage%250Aclassification.%2520Despite%2520the%2520proliferation%2520of%2520classification%2520models%252C%2520few%2520have%250Aprioritised%2520reducing%2520model%2520complexity%252C%2520which%2520is%2520a%2520crucial%2520factor%2520for%2520practical%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Multi-Scale%2520and%2520Attention%250AConvolutional%2520Neural%2520Network%2520%2528MSA-CNN%2529%252C%2520a%2520lightweight%2520architecture%2520featuring%2520as%250Afew%2520as%2520~10%252C000%2520parameters.%2520MSA-CNN%2520leverages%2520a%2520novel%2520multi-scale%2520module%250Aemploying%2520complementary%2520pooling%2520to%2520eliminate%2520redundant%2520filter%2520parameters%2520and%250Adense%2520convolutions.%2520Model%2520complexity%2520is%2520further%2520reduced%2520by%2520separating%2520temporal%250Aand%2520spatial%2520feature%2520extraction%2520and%2520using%2520cost-effective%2520global%2520spatial%250Aconvolutions.%2520This%2520separation%2520of%2520tasks%2520not%2520only%2520reduces%2520model%2520complexity%2520but%250Aalso%2520mirrors%2520the%2520approach%2520used%2520by%2520human%2520experts%2520in%2520sleep%2520stage%2520scoring.%2520We%250Aevaluated%2520both%2520small%2520and%2520large%2520configurations%2520of%2520MSA-CNN%2520against%2520nine%250Astate-of-the-art%2520baseline%2520models%2520across%2520three%2520public%2520datasets%252C%2520treating%250Aunivariate%2520and%2520multivariate%2520models%2520separately.%2520Our%2520evaluation%252C%2520based%2520on%250Arepeated%2520cross-validation%2520and%2520re-evaluation%2520of%2520all%2520baseline%2520models%252C%250Ademonstrated%2520that%2520the%2520large%2520MSA-CNN%2520outperformed%2520all%2520baseline%2520models%2520on%2520all%250Athree%2520datasets%2520in%2520terms%2520of%2520accuracy%2520and%2520Cohen%2527s%2520kappa%252C%2520despite%2520its%250Asignificantly%2520reduced%2520parameter%2520count.%2520Lastly%252C%2520we%2520explored%2520various%2520model%250Avariants%2520and%2520conducted%2520an%2520in-depth%2520analysis%2520of%2520the%2520key%2520modules%2520and%2520techniques%252C%250Aproviding%2520deeper%2520insights%2520into%2520the%2520underlying%2520mechanisms.%2520The%2520code%2520for%2520our%250Amodels%252C%2520baselines%252C%2520and%2520evaluation%2520procedures%2520is%2520available%2520at%250Ahttps%253A//github.com/sgoerttler/MSA-CNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSA-CNN%3A%20A%20Lightweight%20Multi-Scale%20CNN%20with%20Attention%20for%20Sleep%20Stage%0A%20%20Classification&entry.906535625=Stephan%20Goerttler%20and%20Yucheng%20Wang%20and%20Emadeldeen%20Eldele%20and%20Min%20Wu%20and%20Fei%20He&entry.1292438233=%20%20Recent%20advancements%20in%20machine%20learning-based%20signal%20analysis%2C%20coupled%20with%0Aopen%20data%20initiatives%2C%20have%20fuelled%20efforts%20in%20automatic%20sleep%20stage%0Aclassification.%20Despite%20the%20proliferation%20of%20classification%20models%2C%20few%20have%0Aprioritised%20reducing%20model%20complexity%2C%20which%20is%20a%20crucial%20factor%20for%20practical%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20Multi-Scale%20and%20Attention%0AConvolutional%20Neural%20Network%20%28MSA-CNN%29%2C%20a%20lightweight%20architecture%20featuring%20as%0Afew%20as%20~10%2C000%20parameters.%20MSA-CNN%20leverages%20a%20novel%20multi-scale%20module%0Aemploying%20complementary%20pooling%20to%20eliminate%20redundant%20filter%20parameters%20and%0Adense%20convolutions.%20Model%20complexity%20is%20further%20reduced%20by%20separating%20temporal%0Aand%20spatial%20feature%20extraction%20and%20using%20cost-effective%20global%20spatial%0Aconvolutions.%20This%20separation%20of%20tasks%20not%20only%20reduces%20model%20complexity%20but%0Aalso%20mirrors%20the%20approach%20used%20by%20human%20experts%20in%20sleep%20stage%20scoring.%20We%0Aevaluated%20both%20small%20and%20large%20configurations%20of%20MSA-CNN%20against%20nine%0Astate-of-the-art%20baseline%20models%20across%20three%20public%20datasets%2C%20treating%0Aunivariate%20and%20multivariate%20models%20separately.%20Our%20evaluation%2C%20based%20on%0Arepeated%20cross-validation%20and%20re-evaluation%20of%20all%20baseline%20models%2C%0Ademonstrated%20that%20the%20large%20MSA-CNN%20outperformed%20all%20baseline%20models%20on%20all%0Athree%20datasets%20in%20terms%20of%20accuracy%20and%20Cohen%27s%20kappa%2C%20despite%20its%0Asignificantly%20reduced%20parameter%20count.%20Lastly%2C%20we%20explored%20various%20model%0Avariants%20and%20conducted%20an%20in-depth%20analysis%20of%20the%20key%20modules%20and%20techniques%2C%0Aproviding%20deeper%20insights%20into%20the%20underlying%20mechanisms.%20The%20code%20for%20our%0Amodels%2C%20baselines%2C%20and%20evaluation%20procedures%20is%20available%20at%0Ahttps%3A//github.com/sgoerttler/MSA-CNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02949v1&entry.124074799=Read"},
{"title": "Context Awareness Gate For Retrieval Augmented Generation", "author": "Mohammad Hassan Heydari and Arshia Hemmat and Erfan Naman and Afsaneh Fatemi", "abstract": "  Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.\n", "link": "http://arxiv.org/abs/2411.16133v2", "date": "2025-01-06", "relevancy": 1.9614, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5421}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4829}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Awareness%20Gate%20For%20Retrieval%20Augmented%20Generation&body=Title%3A%20Context%20Awareness%20Gate%20For%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Mohammad%20Hassan%20Heydari%20and%20Arshia%20Hemmat%20and%20Erfan%20Naman%20and%20Afsaneh%20Fatemi%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%20widely%20adopted%20approach%0Ato%20mitigate%20the%20limitations%20of%20large%20language%20models%20%28LLMs%29%20in%20answering%0Adomain-specific%20questions.%20Previous%20research%20has%20predominantly%20focused%20on%0Aimproving%20the%20accuracy%20and%20quality%20of%20retrieved%20data%20chunks%20to%20enhance%20the%0Aoverall%20performance%20of%20the%20generation%20pipeline.%20However%2C%20despite%20ongoing%0Aadvancements%2C%20the%20critical%20issue%20of%20retrieving%20irrelevant%20information%20--%20which%0Acan%20impair%20the%20ability%20of%20the%20model%20to%20utilize%20its%20internal%20knowledge%0Aeffectively%20--%20has%20received%20minimal%20attention.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20retrieving%20irrelevant%20information%20in%20open-domain%20question%20answering%2C%0Ahighlighting%20its%20significant%20detrimental%20effect%20on%20the%20quality%20of%20LLM%20outputs.%0ATo%20address%20this%20challenge%2C%20we%20propose%20the%20Context%20Awareness%20Gate%20%28CAG%29%0Aarchitecture%2C%20a%20novel%20mechanism%20that%20dynamically%20adjusts%20the%20LLMs%27%20input%20prompt%0Abased%20on%20whether%20the%20user%20query%20necessitates%20external%20context%20retrieval.%0AAdditionally%2C%20we%20introduce%20the%20Vector%20Candidates%20method%2C%20a%20core%20mathematical%0Acomponent%20of%20CAG%20that%20is%20statistical%2C%20LLM-independent%2C%20and%20highly%20scalable.%20We%0Afurther%20examine%20the%20distributions%20of%20relationships%20between%20contexts%20and%0Aquestions%2C%20presenting%20a%20statistical%20analysis%20of%20these%20distributions.%20This%0Aanalysis%20can%20be%20leveraged%20to%20enhance%20the%20context%20retrieval%20process%20in%20Retrieval%0AAugmented%20Generation%20%28RAG%29%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Awareness%2520Gate%2520For%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DMohammad%2520Hassan%2520Heydari%2520and%2520Arshia%2520Hemmat%2520and%2520Erfan%2520Naman%2520and%2520Afsaneh%2520Fatemi%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520has%2520emerged%2520as%2520a%2520widely%2520adopted%2520approach%250Ato%2520mitigate%2520the%2520limitations%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520answering%250Adomain-specific%2520questions.%2520Previous%2520research%2520has%2520predominantly%2520focused%2520on%250Aimproving%2520the%2520accuracy%2520and%2520quality%2520of%2520retrieved%2520data%2520chunks%2520to%2520enhance%2520the%250Aoverall%2520performance%2520of%2520the%2520generation%2520pipeline.%2520However%252C%2520despite%2520ongoing%250Aadvancements%252C%2520the%2520critical%2520issue%2520of%2520retrieving%2520irrelevant%2520information%2520--%2520which%250Acan%2520impair%2520the%2520ability%2520of%2520the%2520model%2520to%2520utilize%2520its%2520internal%2520knowledge%250Aeffectively%2520--%2520has%2520received%2520minimal%2520attention.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520retrieving%2520irrelevant%2520information%2520in%2520open-domain%2520question%2520answering%252C%250Ahighlighting%2520its%2520significant%2520detrimental%2520effect%2520on%2520the%2520quality%2520of%2520LLM%2520outputs.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Context%2520Awareness%2520Gate%2520%2528CAG%2529%250Aarchitecture%252C%2520a%2520novel%2520mechanism%2520that%2520dynamically%2520adjusts%2520the%2520LLMs%2527%2520input%2520prompt%250Abased%2520on%2520whether%2520the%2520user%2520query%2520necessitates%2520external%2520context%2520retrieval.%250AAdditionally%252C%2520we%2520introduce%2520the%2520Vector%2520Candidates%2520method%252C%2520a%2520core%2520mathematical%250Acomponent%2520of%2520CAG%2520that%2520is%2520statistical%252C%2520LLM-independent%252C%2520and%2520highly%2520scalable.%2520We%250Afurther%2520examine%2520the%2520distributions%2520of%2520relationships%2520between%2520contexts%2520and%250Aquestions%252C%2520presenting%2520a%2520statistical%2520analysis%2520of%2520these%2520distributions.%2520This%250Aanalysis%2520can%2520be%2520leveraged%2520to%2520enhance%2520the%2520context%2520retrieval%2520process%2520in%2520Retrieval%250AAugmented%2520Generation%2520%2528RAG%2529%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Awareness%20Gate%20For%20Retrieval%20Augmented%20Generation&entry.906535625=Mohammad%20Hassan%20Heydari%20and%20Arshia%20Hemmat%20and%20Erfan%20Naman%20and%20Afsaneh%20Fatemi&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%20widely%20adopted%20approach%0Ato%20mitigate%20the%20limitations%20of%20large%20language%20models%20%28LLMs%29%20in%20answering%0Adomain-specific%20questions.%20Previous%20research%20has%20predominantly%20focused%20on%0Aimproving%20the%20accuracy%20and%20quality%20of%20retrieved%20data%20chunks%20to%20enhance%20the%0Aoverall%20performance%20of%20the%20generation%20pipeline.%20However%2C%20despite%20ongoing%0Aadvancements%2C%20the%20critical%20issue%20of%20retrieving%20irrelevant%20information%20--%20which%0Acan%20impair%20the%20ability%20of%20the%20model%20to%20utilize%20its%20internal%20knowledge%0Aeffectively%20--%20has%20received%20minimal%20attention.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20retrieving%20irrelevant%20information%20in%20open-domain%20question%20answering%2C%0Ahighlighting%20its%20significant%20detrimental%20effect%20on%20the%20quality%20of%20LLM%20outputs.%0ATo%20address%20this%20challenge%2C%20we%20propose%20the%20Context%20Awareness%20Gate%20%28CAG%29%0Aarchitecture%2C%20a%20novel%20mechanism%20that%20dynamically%20adjusts%20the%20LLMs%27%20input%20prompt%0Abased%20on%20whether%20the%20user%20query%20necessitates%20external%20context%20retrieval.%0AAdditionally%2C%20we%20introduce%20the%20Vector%20Candidates%20method%2C%20a%20core%20mathematical%0Acomponent%20of%20CAG%20that%20is%20statistical%2C%20LLM-independent%2C%20and%20highly%20scalable.%20We%0Afurther%20examine%20the%20distributions%20of%20relationships%20between%20contexts%20and%0Aquestions%2C%20presenting%20a%20statistical%20analysis%20of%20these%20distributions.%20This%0Aanalysis%20can%20be%20leveraged%20to%20enhance%20the%20context%20retrieval%20process%20in%20Retrieval%0AAugmented%20Generation%20%28RAG%29%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16133v2&entry.124074799=Read"},
{"title": "Dr. Tongue: Sign-Oriented Multi-label Detection for Remote Tongue\n  Diagnosis", "author": "Yiliang Chen and Steven SC Ho and Cheng Xu and Yao Jie Xie and Wing-Fai Yeung and Shengfeng He and Jing Qin", "abstract": "  Tongue diagnosis is a vital tool in Western and Traditional Chinese Medicine,\nproviding key insights into a patient's health by analyzing tongue attributes.\nThe COVID-19 pandemic has heightened the need for accurate remote medical\nassessments, emphasizing the importance of precise tongue attribute recognition\nvia telehealth. To address this, we propose a Sign-Oriented multi-label\nAttributes Detection framework. Our approach begins with an adaptive tongue\nfeature extraction module that standardizes tongue images and mitigates\nenvironmental factors. This is followed by a Sign-oriented Network (SignNet)\nthat identifies specific tongue attributes, emulating the diagnostic process of\nexperienced practitioners and enabling comprehensive health evaluations. To\nvalidate our methodology, we developed an extensive tongue image dataset\nspecifically designed for telemedicine. Unlike existing datasets, ours is\ntailored for remote diagnosis, with a comprehensive set of attribute labels.\nThis dataset will be openly available, providing a valuable resource for\nresearch. Initial tests have shown improved accuracy in detecting various\ntongue attributes, highlighting our framework's potential as an essential tool\nfor remote medical assessments.\n", "link": "http://arxiv.org/abs/2501.03053v1", "date": "2025-01-06", "relevancy": 1.958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis&body=Title%3A%20Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis%0AAuthor%3A%20Yiliang%20Chen%20and%20Steven%20SC%20Ho%20and%20Cheng%20Xu%20and%20Yao%20Jie%20Xie%20and%20Wing-Fai%20Yeung%20and%20Shengfeng%20He%20and%20Jing%20Qin%0AAbstract%3A%20%20%20Tongue%20diagnosis%20is%20a%20vital%20tool%20in%20Western%20and%20Traditional%20Chinese%20Medicine%2C%0Aproviding%20key%20insights%20into%20a%20patient%27s%20health%20by%20analyzing%20tongue%20attributes.%0AThe%20COVID-19%20pandemic%20has%20heightened%20the%20need%20for%20accurate%20remote%20medical%0Aassessments%2C%20emphasizing%20the%20importance%20of%20precise%20tongue%20attribute%20recognition%0Avia%20telehealth.%20To%20address%20this%2C%20we%20propose%20a%20Sign-Oriented%20multi-label%0AAttributes%20Detection%20framework.%20Our%20approach%20begins%20with%20an%20adaptive%20tongue%0Afeature%20extraction%20module%20that%20standardizes%20tongue%20images%20and%20mitigates%0Aenvironmental%20factors.%20This%20is%20followed%20by%20a%20Sign-oriented%20Network%20%28SignNet%29%0Athat%20identifies%20specific%20tongue%20attributes%2C%20emulating%20the%20diagnostic%20process%20of%0Aexperienced%20practitioners%20and%20enabling%20comprehensive%20health%20evaluations.%20To%0Avalidate%20our%20methodology%2C%20we%20developed%20an%20extensive%20tongue%20image%20dataset%0Aspecifically%20designed%20for%20telemedicine.%20Unlike%20existing%20datasets%2C%20ours%20is%0Atailored%20for%20remote%20diagnosis%2C%20with%20a%20comprehensive%20set%20of%20attribute%20labels.%0AThis%20dataset%20will%20be%20openly%20available%2C%20providing%20a%20valuable%20resource%20for%0Aresearch.%20Initial%20tests%20have%20shown%20improved%20accuracy%20in%20detecting%20various%0Atongue%20attributes%2C%20highlighting%20our%20framework%27s%20potential%20as%20an%20essential%20tool%0Afor%20remote%20medical%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.%2520Tongue%253A%2520Sign-Oriented%2520Multi-label%2520Detection%2520for%2520Remote%2520Tongue%250A%2520%2520Diagnosis%26entry.906535625%3DYiliang%2520Chen%2520and%2520Steven%2520SC%2520Ho%2520and%2520Cheng%2520Xu%2520and%2520Yao%2520Jie%2520Xie%2520and%2520Wing-Fai%2520Yeung%2520and%2520Shengfeng%2520He%2520and%2520Jing%2520Qin%26entry.1292438233%3D%2520%2520Tongue%2520diagnosis%2520is%2520a%2520vital%2520tool%2520in%2520Western%2520and%2520Traditional%2520Chinese%2520Medicine%252C%250Aproviding%2520key%2520insights%2520into%2520a%2520patient%2527s%2520health%2520by%2520analyzing%2520tongue%2520attributes.%250AThe%2520COVID-19%2520pandemic%2520has%2520heightened%2520the%2520need%2520for%2520accurate%2520remote%2520medical%250Aassessments%252C%2520emphasizing%2520the%2520importance%2520of%2520precise%2520tongue%2520attribute%2520recognition%250Avia%2520telehealth.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Sign-Oriented%2520multi-label%250AAttributes%2520Detection%2520framework.%2520Our%2520approach%2520begins%2520with%2520an%2520adaptive%2520tongue%250Afeature%2520extraction%2520module%2520that%2520standardizes%2520tongue%2520images%2520and%2520mitigates%250Aenvironmental%2520factors.%2520This%2520is%2520followed%2520by%2520a%2520Sign-oriented%2520Network%2520%2528SignNet%2529%250Athat%2520identifies%2520specific%2520tongue%2520attributes%252C%2520emulating%2520the%2520diagnostic%2520process%2520of%250Aexperienced%2520practitioners%2520and%2520enabling%2520comprehensive%2520health%2520evaluations.%2520To%250Avalidate%2520our%2520methodology%252C%2520we%2520developed%2520an%2520extensive%2520tongue%2520image%2520dataset%250Aspecifically%2520designed%2520for%2520telemedicine.%2520Unlike%2520existing%2520datasets%252C%2520ours%2520is%250Atailored%2520for%2520remote%2520diagnosis%252C%2520with%2520a%2520comprehensive%2520set%2520of%2520attribute%2520labels.%250AThis%2520dataset%2520will%2520be%2520openly%2520available%252C%2520providing%2520a%2520valuable%2520resource%2520for%250Aresearch.%2520Initial%2520tests%2520have%2520shown%2520improved%2520accuracy%2520in%2520detecting%2520various%250Atongue%2520attributes%252C%2520highlighting%2520our%2520framework%2527s%2520potential%2520as%2520an%2520essential%2520tool%250Afor%2520remote%2520medical%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.%20Tongue%3A%20Sign-Oriented%20Multi-label%20Detection%20for%20Remote%20Tongue%0A%20%20Diagnosis&entry.906535625=Yiliang%20Chen%20and%20Steven%20SC%20Ho%20and%20Cheng%20Xu%20and%20Yao%20Jie%20Xie%20and%20Wing-Fai%20Yeung%20and%20Shengfeng%20He%20and%20Jing%20Qin&entry.1292438233=%20%20Tongue%20diagnosis%20is%20a%20vital%20tool%20in%20Western%20and%20Traditional%20Chinese%20Medicine%2C%0Aproviding%20key%20insights%20into%20a%20patient%27s%20health%20by%20analyzing%20tongue%20attributes.%0AThe%20COVID-19%20pandemic%20has%20heightened%20the%20need%20for%20accurate%20remote%20medical%0Aassessments%2C%20emphasizing%20the%20importance%20of%20precise%20tongue%20attribute%20recognition%0Avia%20telehealth.%20To%20address%20this%2C%20we%20propose%20a%20Sign-Oriented%20multi-label%0AAttributes%20Detection%20framework.%20Our%20approach%20begins%20with%20an%20adaptive%20tongue%0Afeature%20extraction%20module%20that%20standardizes%20tongue%20images%20and%20mitigates%0Aenvironmental%20factors.%20This%20is%20followed%20by%20a%20Sign-oriented%20Network%20%28SignNet%29%0Athat%20identifies%20specific%20tongue%20attributes%2C%20emulating%20the%20diagnostic%20process%20of%0Aexperienced%20practitioners%20and%20enabling%20comprehensive%20health%20evaluations.%20To%0Avalidate%20our%20methodology%2C%20we%20developed%20an%20extensive%20tongue%20image%20dataset%0Aspecifically%20designed%20for%20telemedicine.%20Unlike%20existing%20datasets%2C%20ours%20is%0Atailored%20for%20remote%20diagnosis%2C%20with%20a%20comprehensive%20set%20of%20attribute%20labels.%0AThis%20dataset%20will%20be%20openly%20available%2C%20providing%20a%20valuable%20resource%20for%0Aresearch.%20Initial%20tests%20have%20shown%20improved%20accuracy%20in%20detecting%20various%0Atongue%20attributes%2C%20highlighting%20our%20framework%27s%20potential%20as%20an%20essential%20tool%0Afor%20remote%20medical%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03053v1&entry.124074799=Read"},
{"title": "Scalable Forward-Forward Algorithm", "author": "Andrii Krutsylo", "abstract": "  We propose a scalable Forward-Forward (FF) algorithm that eliminates the need\nfor backpropagation by training each layer separately. Unlike backpropagation,\nFF avoids backward gradients and can be more modular and memory efficient,\nmaking it appealing for large networks. We extend FF to modern convolutional\narchitectures, such as MobileNetV3 and ResNet18, by introducing a new way to\ncompute losses for convolutional layers. Experiments show that our method\nachieves performance comparable to standard backpropagation. Furthermore, when\nwe divide the network into blocks, such as the residual blocks in ResNet, and\napply backpropagation only within each block, but not across blocks, our hybrid\ndesign tends to outperform backpropagation baselines while maintaining a\nsimilar training speed. Finally, we present experiments on small datasets and\ntransfer learning that confirm the adaptability of our method.\n", "link": "http://arxiv.org/abs/2501.03176v1", "date": "2025-01-06", "relevancy": 1.9419, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5055}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Forward-Forward%20Algorithm&body=Title%3A%20Scalable%20Forward-Forward%20Algorithm%0AAuthor%3A%20Andrii%20Krutsylo%0AAbstract%3A%20%20%20We%20propose%20a%20scalable%20Forward-Forward%20%28FF%29%20algorithm%20that%20eliminates%20the%20need%0Afor%20backpropagation%20by%20training%20each%20layer%20separately.%20Unlike%20backpropagation%2C%0AFF%20avoids%20backward%20gradients%20and%20can%20be%20more%20modular%20and%20memory%20efficient%2C%0Amaking%20it%20appealing%20for%20large%20networks.%20We%20extend%20FF%20to%20modern%20convolutional%0Aarchitectures%2C%20such%20as%20MobileNetV3%20and%20ResNet18%2C%20by%20introducing%20a%20new%20way%20to%0Acompute%20losses%20for%20convolutional%20layers.%20Experiments%20show%20that%20our%20method%0Aachieves%20performance%20comparable%20to%20standard%20backpropagation.%20Furthermore%2C%20when%0Awe%20divide%20the%20network%20into%20blocks%2C%20such%20as%20the%20residual%20blocks%20in%20ResNet%2C%20and%0Aapply%20backpropagation%20only%20within%20each%20block%2C%20but%20not%20across%20blocks%2C%20our%20hybrid%0Adesign%20tends%20to%20outperform%20backpropagation%20baselines%20while%20maintaining%20a%0Asimilar%20training%20speed.%20Finally%2C%20we%20present%20experiments%20on%20small%20datasets%20and%0Atransfer%20learning%20that%20confirm%20the%20adaptability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Forward-Forward%2520Algorithm%26entry.906535625%3DAndrii%2520Krutsylo%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520scalable%2520Forward-Forward%2520%2528FF%2529%2520algorithm%2520that%2520eliminates%2520the%2520need%250Afor%2520backpropagation%2520by%2520training%2520each%2520layer%2520separately.%2520Unlike%2520backpropagation%252C%250AFF%2520avoids%2520backward%2520gradients%2520and%2520can%2520be%2520more%2520modular%2520and%2520memory%2520efficient%252C%250Amaking%2520it%2520appealing%2520for%2520large%2520networks.%2520We%2520extend%2520FF%2520to%2520modern%2520convolutional%250Aarchitectures%252C%2520such%2520as%2520MobileNetV3%2520and%2520ResNet18%252C%2520by%2520introducing%2520a%2520new%2520way%2520to%250Acompute%2520losses%2520for%2520convolutional%2520layers.%2520Experiments%2520show%2520that%2520our%2520method%250Aachieves%2520performance%2520comparable%2520to%2520standard%2520backpropagation.%2520Furthermore%252C%2520when%250Awe%2520divide%2520the%2520network%2520into%2520blocks%252C%2520such%2520as%2520the%2520residual%2520blocks%2520in%2520ResNet%252C%2520and%250Aapply%2520backpropagation%2520only%2520within%2520each%2520block%252C%2520but%2520not%2520across%2520blocks%252C%2520our%2520hybrid%250Adesign%2520tends%2520to%2520outperform%2520backpropagation%2520baselines%2520while%2520maintaining%2520a%250Asimilar%2520training%2520speed.%2520Finally%252C%2520we%2520present%2520experiments%2520on%2520small%2520datasets%2520and%250Atransfer%2520learning%2520that%2520confirm%2520the%2520adaptability%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Forward-Forward%20Algorithm&entry.906535625=Andrii%20Krutsylo&entry.1292438233=%20%20We%20propose%20a%20scalable%20Forward-Forward%20%28FF%29%20algorithm%20that%20eliminates%20the%20need%0Afor%20backpropagation%20by%20training%20each%20layer%20separately.%20Unlike%20backpropagation%2C%0AFF%20avoids%20backward%20gradients%20and%20can%20be%20more%20modular%20and%20memory%20efficient%2C%0Amaking%20it%20appealing%20for%20large%20networks.%20We%20extend%20FF%20to%20modern%20convolutional%0Aarchitectures%2C%20such%20as%20MobileNetV3%20and%20ResNet18%2C%20by%20introducing%20a%20new%20way%20to%0Acompute%20losses%20for%20convolutional%20layers.%20Experiments%20show%20that%20our%20method%0Aachieves%20performance%20comparable%20to%20standard%20backpropagation.%20Furthermore%2C%20when%0Awe%20divide%20the%20network%20into%20blocks%2C%20such%20as%20the%20residual%20blocks%20in%20ResNet%2C%20and%0Aapply%20backpropagation%20only%20within%20each%20block%2C%20but%20not%20across%20blocks%2C%20our%20hybrid%0Adesign%20tends%20to%20outperform%20backpropagation%20baselines%20while%20maintaining%20a%0Asimilar%20training%20speed.%20Finally%2C%20we%20present%20experiments%20on%20small%20datasets%20and%0Atransfer%20learning%20that%20confirm%20the%20adaptability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03176v1&entry.124074799=Read"},
{"title": "CALM: Curiosity-Driven Auditing for Large Language Models", "author": "Xiang Zheng and Longxiang Wang and Yi Liu and Xingjun Ma and Chao Shen and Cong Wang", "abstract": "  Auditing Large Language Models (LLMs) is a crucial and challenging task. In\nthis study, we focus on auditing black-box LLMs without access to their\nparameters, only to the provided service. We treat this type of auditing as a\nblack-box optimization problem where the goal is to automatically uncover\ninput-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe\nbehaviors. For instance, we may seek a non-toxic input that the target LLM\nresponds to with a toxic output or an input that induces the hallucinative\nresponse from the target LLM containing politically sensitive individuals. This\nblack-box optimization is challenging due to the scarcity of feasible points,\nthe discrete nature of the prompt space, and the large search space. To address\nthese challenges, we propose Curiosity-Driven Auditing for Large Language\nModels (CALM), which uses intrinsically motivated reinforcement learning to\nfinetune an LLM as the auditor agent to uncover potential harmful and biased\ninput-output pairs of the target LLM. CALM successfully identifies derogatory\ncompletions involving celebrities and uncovers inputs that elicit specific\nnames under the black-box setting. This work offers a promising direction for\nauditing black-box LLMs. Our code is available at\nhttps://github.com/x-zheng16/CALM.git.\n", "link": "http://arxiv.org/abs/2501.02997v1", "date": "2025-01-06", "relevancy": 1.9379, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5146}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4894}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CALM%3A%20Curiosity-Driven%20Auditing%20for%20Large%20Language%20Models&body=Title%3A%20CALM%3A%20Curiosity-Driven%20Auditing%20for%20Large%20Language%20Models%0AAuthor%3A%20Xiang%20Zheng%20and%20Longxiang%20Wang%20and%20Yi%20Liu%20and%20Xingjun%20Ma%20and%20Chao%20Shen%20and%20Cong%20Wang%0AAbstract%3A%20%20%20Auditing%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20crucial%20and%20challenging%20task.%20In%0Athis%20study%2C%20we%20focus%20on%20auditing%20black-box%20LLMs%20without%20access%20to%20their%0Aparameters%2C%20only%20to%20the%20provided%20service.%20We%20treat%20this%20type%20of%20auditing%20as%20a%0Ablack-box%20optimization%20problem%20where%20the%20goal%20is%20to%20automatically%20uncover%0Ainput-output%20pairs%20of%20the%20target%20LLMs%20that%20exhibit%20illegal%2C%20immoral%2C%20or%20unsafe%0Abehaviors.%20For%20instance%2C%20we%20may%20seek%20a%20non-toxic%20input%20that%20the%20target%20LLM%0Aresponds%20to%20with%20a%20toxic%20output%20or%20an%20input%20that%20induces%20the%20hallucinative%0Aresponse%20from%20the%20target%20LLM%20containing%20politically%20sensitive%20individuals.%20This%0Ablack-box%20optimization%20is%20challenging%20due%20to%20the%20scarcity%20of%20feasible%20points%2C%0Athe%20discrete%20nature%20of%20the%20prompt%20space%2C%20and%20the%20large%20search%20space.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Curiosity-Driven%20Auditing%20for%20Large%20Language%0AModels%20%28CALM%29%2C%20which%20uses%20intrinsically%20motivated%20reinforcement%20learning%20to%0Afinetune%20an%20LLM%20as%20the%20auditor%20agent%20to%20uncover%20potential%20harmful%20and%20biased%0Ainput-output%20pairs%20of%20the%20target%20LLM.%20CALM%20successfully%20identifies%20derogatory%0Acompletions%20involving%20celebrities%20and%20uncovers%20inputs%20that%20elicit%20specific%0Anames%20under%20the%20black-box%20setting.%20This%20work%20offers%20a%20promising%20direction%20for%0Aauditing%20black-box%20LLMs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/x-zheng16/CALM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCALM%253A%2520Curiosity-Driven%2520Auditing%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DXiang%2520Zheng%2520and%2520Longxiang%2520Wang%2520and%2520Yi%2520Liu%2520and%2520Xingjun%2520Ma%2520and%2520Chao%2520Shen%2520and%2520Cong%2520Wang%26entry.1292438233%3D%2520%2520Auditing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520a%2520crucial%2520and%2520challenging%2520task.%2520In%250Athis%2520study%252C%2520we%2520focus%2520on%2520auditing%2520black-box%2520LLMs%2520without%2520access%2520to%2520their%250Aparameters%252C%2520only%2520to%2520the%2520provided%2520service.%2520We%2520treat%2520this%2520type%2520of%2520auditing%2520as%2520a%250Ablack-box%2520optimization%2520problem%2520where%2520the%2520goal%2520is%2520to%2520automatically%2520uncover%250Ainput-output%2520pairs%2520of%2520the%2520target%2520LLMs%2520that%2520exhibit%2520illegal%252C%2520immoral%252C%2520or%2520unsafe%250Abehaviors.%2520For%2520instance%252C%2520we%2520may%2520seek%2520a%2520non-toxic%2520input%2520that%2520the%2520target%2520LLM%250Aresponds%2520to%2520with%2520a%2520toxic%2520output%2520or%2520an%2520input%2520that%2520induces%2520the%2520hallucinative%250Aresponse%2520from%2520the%2520target%2520LLM%2520containing%2520politically%2520sensitive%2520individuals.%2520This%250Ablack-box%2520optimization%2520is%2520challenging%2520due%2520to%2520the%2520scarcity%2520of%2520feasible%2520points%252C%250Athe%2520discrete%2520nature%2520of%2520the%2520prompt%2520space%252C%2520and%2520the%2520large%2520search%2520space.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520Curiosity-Driven%2520Auditing%2520for%2520Large%2520Language%250AModels%2520%2528CALM%2529%252C%2520which%2520uses%2520intrinsically%2520motivated%2520reinforcement%2520learning%2520to%250Afinetune%2520an%2520LLM%2520as%2520the%2520auditor%2520agent%2520to%2520uncover%2520potential%2520harmful%2520and%2520biased%250Ainput-output%2520pairs%2520of%2520the%2520target%2520LLM.%2520CALM%2520successfully%2520identifies%2520derogatory%250Acompletions%2520involving%2520celebrities%2520and%2520uncovers%2520inputs%2520that%2520elicit%2520specific%250Anames%2520under%2520the%2520black-box%2520setting.%2520This%2520work%2520offers%2520a%2520promising%2520direction%2520for%250Aauditing%2520black-box%2520LLMs.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/x-zheng16/CALM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CALM%3A%20Curiosity-Driven%20Auditing%20for%20Large%20Language%20Models&entry.906535625=Xiang%20Zheng%20and%20Longxiang%20Wang%20and%20Yi%20Liu%20and%20Xingjun%20Ma%20and%20Chao%20Shen%20and%20Cong%20Wang&entry.1292438233=%20%20Auditing%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20crucial%20and%20challenging%20task.%20In%0Athis%20study%2C%20we%20focus%20on%20auditing%20black-box%20LLMs%20without%20access%20to%20their%0Aparameters%2C%20only%20to%20the%20provided%20service.%20We%20treat%20this%20type%20of%20auditing%20as%20a%0Ablack-box%20optimization%20problem%20where%20the%20goal%20is%20to%20automatically%20uncover%0Ainput-output%20pairs%20of%20the%20target%20LLMs%20that%20exhibit%20illegal%2C%20immoral%2C%20or%20unsafe%0Abehaviors.%20For%20instance%2C%20we%20may%20seek%20a%20non-toxic%20input%20that%20the%20target%20LLM%0Aresponds%20to%20with%20a%20toxic%20output%20or%20an%20input%20that%20induces%20the%20hallucinative%0Aresponse%20from%20the%20target%20LLM%20containing%20politically%20sensitive%20individuals.%20This%0Ablack-box%20optimization%20is%20challenging%20due%20to%20the%20scarcity%20of%20feasible%20points%2C%0Athe%20discrete%20nature%20of%20the%20prompt%20space%2C%20and%20the%20large%20search%20space.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Curiosity-Driven%20Auditing%20for%20Large%20Language%0AModels%20%28CALM%29%2C%20which%20uses%20intrinsically%20motivated%20reinforcement%20learning%20to%0Afinetune%20an%20LLM%20as%20the%20auditor%20agent%20to%20uncover%20potential%20harmful%20and%20biased%0Ainput-output%20pairs%20of%20the%20target%20LLM.%20CALM%20successfully%20identifies%20derogatory%0Acompletions%20involving%20celebrities%20and%20uncovers%20inputs%20that%20elicit%20specific%0Anames%20under%20the%20black-box%20setting.%20This%20work%20offers%20a%20promising%20direction%20for%0Aauditing%20black-box%20LLMs.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/x-zheng16/CALM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02997v1&entry.124074799=Read"},
{"title": "NeuroPMD: Neural Fields for Density Estimation on Product Manifolds", "author": "William Consagra and Zhiling Gu and Zhengwu Zhang", "abstract": "  We propose a novel deep neural network methodology for density estimation on\nproduct Riemannian manifold domains. In our approach, the network directly\nparameterizes the unknown density function and is trained using a penalized\nmaximum likelihood framework, with a penalty term formed using manifold\ndifferential operators. The network architecture and estimation algorithm are\ncarefully designed to handle the challenges of high-dimensional product\nmanifold domains, effectively mitigating the curse of dimensionality that\nlimits traditional kernel and basis expansion estimators, as well as overcoming\nthe convergence issues encountered by non-specialized neural network methods.\nExtensive simulations and a real-world application to brain structural\nconnectivity data highlight the clear advantages of our method over the\ncompeting alternatives.\n", "link": "http://arxiv.org/abs/2501.02994v1", "date": "2025-01-06", "relevancy": 1.9356, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroPMD%3A%20Neural%20Fields%20for%20Density%20Estimation%20on%20Product%20Manifolds&body=Title%3A%20NeuroPMD%3A%20Neural%20Fields%20for%20Density%20Estimation%20on%20Product%20Manifolds%0AAuthor%3A%20William%20Consagra%20and%20Zhiling%20Gu%20and%20Zhengwu%20Zhang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20deep%20neural%20network%20methodology%20for%20density%20estimation%20on%0Aproduct%20Riemannian%20manifold%20domains.%20In%20our%20approach%2C%20the%20network%20directly%0Aparameterizes%20the%20unknown%20density%20function%20and%20is%20trained%20using%20a%20penalized%0Amaximum%20likelihood%20framework%2C%20with%20a%20penalty%20term%20formed%20using%20manifold%0Adifferential%20operators.%20The%20network%20architecture%20and%20estimation%20algorithm%20are%0Acarefully%20designed%20to%20handle%20the%20challenges%20of%20high-dimensional%20product%0Amanifold%20domains%2C%20effectively%20mitigating%20the%20curse%20of%20dimensionality%20that%0Alimits%20traditional%20kernel%20and%20basis%20expansion%20estimators%2C%20as%20well%20as%20overcoming%0Athe%20convergence%20issues%20encountered%20by%20non-specialized%20neural%20network%20methods.%0AExtensive%20simulations%20and%20a%20real-world%20application%20to%20brain%20structural%0Aconnectivity%20data%20highlight%20the%20clear%20advantages%20of%20our%20method%20over%20the%0Acompeting%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroPMD%253A%2520Neural%2520Fields%2520for%2520Density%2520Estimation%2520on%2520Product%2520Manifolds%26entry.906535625%3DWilliam%2520Consagra%2520and%2520Zhiling%2520Gu%2520and%2520Zhengwu%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520deep%2520neural%2520network%2520methodology%2520for%2520density%2520estimation%2520on%250Aproduct%2520Riemannian%2520manifold%2520domains.%2520In%2520our%2520approach%252C%2520the%2520network%2520directly%250Aparameterizes%2520the%2520unknown%2520density%2520function%2520and%2520is%2520trained%2520using%2520a%2520penalized%250Amaximum%2520likelihood%2520framework%252C%2520with%2520a%2520penalty%2520term%2520formed%2520using%2520manifold%250Adifferential%2520operators.%2520The%2520network%2520architecture%2520and%2520estimation%2520algorithm%2520are%250Acarefully%2520designed%2520to%2520handle%2520the%2520challenges%2520of%2520high-dimensional%2520product%250Amanifold%2520domains%252C%2520effectively%2520mitigating%2520the%2520curse%2520of%2520dimensionality%2520that%250Alimits%2520traditional%2520kernel%2520and%2520basis%2520expansion%2520estimators%252C%2520as%2520well%2520as%2520overcoming%250Athe%2520convergence%2520issues%2520encountered%2520by%2520non-specialized%2520neural%2520network%2520methods.%250AExtensive%2520simulations%2520and%2520a%2520real-world%2520application%2520to%2520brain%2520structural%250Aconnectivity%2520data%2520highlight%2520the%2520clear%2520advantages%2520of%2520our%2520method%2520over%2520the%250Acompeting%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroPMD%3A%20Neural%20Fields%20for%20Density%20Estimation%20on%20Product%20Manifolds&entry.906535625=William%20Consagra%20and%20Zhiling%20Gu%20and%20Zhengwu%20Zhang&entry.1292438233=%20%20We%20propose%20a%20novel%20deep%20neural%20network%20methodology%20for%20density%20estimation%20on%0Aproduct%20Riemannian%20manifold%20domains.%20In%20our%20approach%2C%20the%20network%20directly%0Aparameterizes%20the%20unknown%20density%20function%20and%20is%20trained%20using%20a%20penalized%0Amaximum%20likelihood%20framework%2C%20with%20a%20penalty%20term%20formed%20using%20manifold%0Adifferential%20operators.%20The%20network%20architecture%20and%20estimation%20algorithm%20are%0Acarefully%20designed%20to%20handle%20the%20challenges%20of%20high-dimensional%20product%0Amanifold%20domains%2C%20effectively%20mitigating%20the%20curse%20of%20dimensionality%20that%0Alimits%20traditional%20kernel%20and%20basis%20expansion%20estimators%2C%20as%20well%20as%20overcoming%0Athe%20convergence%20issues%20encountered%20by%20non-specialized%20neural%20network%20methods.%0AExtensive%20simulations%20and%20a%20real-world%20application%20to%20brain%20structural%0Aconnectivity%20data%20highlight%20the%20clear%20advantages%20of%20our%20method%20over%20the%0Acompeting%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02994v1&entry.124074799=Read"},
{"title": "A Bayesian Approach for Discovering Time- Delayed Differential Equation\n  from Data", "author": "Debangshu Chowdhury and Souvik Chakraborty", "abstract": "  Time-delayed differential equations (TDDEs) are widely used to model complex\ndynamic systems where future states depend on past states with a delay.\nHowever, inferring the underlying TDDEs from observed data remains a\nchallenging problem due to the inherent nonlinearity, uncertainty, and noise in\nreal-world systems. Conventional equation discovery methods often exhibit\nlimitations when dealing with large time delays, relying on deterministic\ntechniques or optimization-based approaches that may struggle with scalability\nand robustness. In this paper, we present BayTiDe - Bayesian Approach for\nDiscovering Time-Delayed Differential Equations from Data, that is capable of\nidentifying arbitrarily large values of time delay to an accuracy that is\ndirectly proportional to the resolution of the data input to it. BayTiDe\nleverages Bayesian inference combined with a sparsity-promoting discontinuous\nspike-and-slab prior to accurately identify time-delayed differential\nequations. The approach accommodates arbitrarily large time delays with\naccuracy proportional to the input data resolution, while efficiently narrowing\nthe search space to achieve significant computational savings. We demonstrate\nthe efficiency and robustness of BayTiDe through a range of numerical examples,\nvalidating its ability to recover delayed differential equations from noisy\ndata.\n", "link": "http://arxiv.org/abs/2501.02934v1", "date": "2025-01-06", "relevancy": 1.9291, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4801}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bayesian%20Approach%20for%20Discovering%20Time-%20Delayed%20Differential%20Equation%0A%20%20from%20Data&body=Title%3A%20A%20Bayesian%20Approach%20for%20Discovering%20Time-%20Delayed%20Differential%20Equation%0A%20%20from%20Data%0AAuthor%3A%20Debangshu%20Chowdhury%20and%20Souvik%20Chakraborty%0AAbstract%3A%20%20%20Time-delayed%20differential%20equations%20%28TDDEs%29%20are%20widely%20used%20to%20model%20complex%0Adynamic%20systems%20where%20future%20states%20depend%20on%20past%20states%20with%20a%20delay.%0AHowever%2C%20inferring%20the%20underlying%20TDDEs%20from%20observed%20data%20remains%20a%0Achallenging%20problem%20due%20to%20the%20inherent%20nonlinearity%2C%20uncertainty%2C%20and%20noise%20in%0Areal-world%20systems.%20Conventional%20equation%20discovery%20methods%20often%20exhibit%0Alimitations%20when%20dealing%20with%20large%20time%20delays%2C%20relying%20on%20deterministic%0Atechniques%20or%20optimization-based%20approaches%20that%20may%20struggle%20with%20scalability%0Aand%20robustness.%20In%20this%20paper%2C%20we%20present%20BayTiDe%20-%20Bayesian%20Approach%20for%0ADiscovering%20Time-Delayed%20Differential%20Equations%20from%20Data%2C%20that%20is%20capable%20of%0Aidentifying%20arbitrarily%20large%20values%20of%20time%20delay%20to%20an%20accuracy%20that%20is%0Adirectly%20proportional%20to%20the%20resolution%20of%20the%20data%20input%20to%20it.%20BayTiDe%0Aleverages%20Bayesian%20inference%20combined%20with%20a%20sparsity-promoting%20discontinuous%0Aspike-and-slab%20prior%20to%20accurately%20identify%20time-delayed%20differential%0Aequations.%20The%20approach%20accommodates%20arbitrarily%20large%20time%20delays%20with%0Aaccuracy%20proportional%20to%20the%20input%20data%20resolution%2C%20while%20efficiently%20narrowing%0Athe%20search%20space%20to%20achieve%20significant%20computational%20savings.%20We%20demonstrate%0Athe%20efficiency%20and%20robustness%20of%20BayTiDe%20through%20a%20range%20of%20numerical%20examples%2C%0Avalidating%20its%20ability%20to%20recover%20delayed%20differential%20equations%20from%20noisy%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bayesian%2520Approach%2520for%2520Discovering%2520Time-%2520Delayed%2520Differential%2520Equation%250A%2520%2520from%2520Data%26entry.906535625%3DDebangshu%2520Chowdhury%2520and%2520Souvik%2520Chakraborty%26entry.1292438233%3D%2520%2520Time-delayed%2520differential%2520equations%2520%2528TDDEs%2529%2520are%2520widely%2520used%2520to%2520model%2520complex%250Adynamic%2520systems%2520where%2520future%2520states%2520depend%2520on%2520past%2520states%2520with%2520a%2520delay.%250AHowever%252C%2520inferring%2520the%2520underlying%2520TDDEs%2520from%2520observed%2520data%2520remains%2520a%250Achallenging%2520problem%2520due%2520to%2520the%2520inherent%2520nonlinearity%252C%2520uncertainty%252C%2520and%2520noise%2520in%250Areal-world%2520systems.%2520Conventional%2520equation%2520discovery%2520methods%2520often%2520exhibit%250Alimitations%2520when%2520dealing%2520with%2520large%2520time%2520delays%252C%2520relying%2520on%2520deterministic%250Atechniques%2520or%2520optimization-based%2520approaches%2520that%2520may%2520struggle%2520with%2520scalability%250Aand%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520present%2520BayTiDe%2520-%2520Bayesian%2520Approach%2520for%250ADiscovering%2520Time-Delayed%2520Differential%2520Equations%2520from%2520Data%252C%2520that%2520is%2520capable%2520of%250Aidentifying%2520arbitrarily%2520large%2520values%2520of%2520time%2520delay%2520to%2520an%2520accuracy%2520that%2520is%250Adirectly%2520proportional%2520to%2520the%2520resolution%2520of%2520the%2520data%2520input%2520to%2520it.%2520BayTiDe%250Aleverages%2520Bayesian%2520inference%2520combined%2520with%2520a%2520sparsity-promoting%2520discontinuous%250Aspike-and-slab%2520prior%2520to%2520accurately%2520identify%2520time-delayed%2520differential%250Aequations.%2520The%2520approach%2520accommodates%2520arbitrarily%2520large%2520time%2520delays%2520with%250Aaccuracy%2520proportional%2520to%2520the%2520input%2520data%2520resolution%252C%2520while%2520efficiently%2520narrowing%250Athe%2520search%2520space%2520to%2520achieve%2520significant%2520computational%2520savings.%2520We%2520demonstrate%250Athe%2520efficiency%2520and%2520robustness%2520of%2520BayTiDe%2520through%2520a%2520range%2520of%2520numerical%2520examples%252C%250Avalidating%2520its%2520ability%2520to%2520recover%2520delayed%2520differential%2520equations%2520from%2520noisy%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bayesian%20Approach%20for%20Discovering%20Time-%20Delayed%20Differential%20Equation%0A%20%20from%20Data&entry.906535625=Debangshu%20Chowdhury%20and%20Souvik%20Chakraborty&entry.1292438233=%20%20Time-delayed%20differential%20equations%20%28TDDEs%29%20are%20widely%20used%20to%20model%20complex%0Adynamic%20systems%20where%20future%20states%20depend%20on%20past%20states%20with%20a%20delay.%0AHowever%2C%20inferring%20the%20underlying%20TDDEs%20from%20observed%20data%20remains%20a%0Achallenging%20problem%20due%20to%20the%20inherent%20nonlinearity%2C%20uncertainty%2C%20and%20noise%20in%0Areal-world%20systems.%20Conventional%20equation%20discovery%20methods%20often%20exhibit%0Alimitations%20when%20dealing%20with%20large%20time%20delays%2C%20relying%20on%20deterministic%0Atechniques%20or%20optimization-based%20approaches%20that%20may%20struggle%20with%20scalability%0Aand%20robustness.%20In%20this%20paper%2C%20we%20present%20BayTiDe%20-%20Bayesian%20Approach%20for%0ADiscovering%20Time-Delayed%20Differential%20Equations%20from%20Data%2C%20that%20is%20capable%20of%0Aidentifying%20arbitrarily%20large%20values%20of%20time%20delay%20to%20an%20accuracy%20that%20is%0Adirectly%20proportional%20to%20the%20resolution%20of%20the%20data%20input%20to%20it.%20BayTiDe%0Aleverages%20Bayesian%20inference%20combined%20with%20a%20sparsity-promoting%20discontinuous%0Aspike-and-slab%20prior%20to%20accurately%20identify%20time-delayed%20differential%0Aequations.%20The%20approach%20accommodates%20arbitrarily%20large%20time%20delays%20with%0Aaccuracy%20proportional%20to%20the%20input%20data%20resolution%2C%20while%20efficiently%20narrowing%0Athe%20search%20space%20to%20achieve%20significant%20computational%20savings.%20We%20demonstrate%0Athe%20efficiency%20and%20robustness%20of%20BayTiDe%20through%20a%20range%20of%20numerical%20examples%2C%0Avalidating%20its%20ability%20to%20recover%20delayed%20differential%20equations%20from%20noisy%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02934v1&entry.124074799=Read"},
{"title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning", "author": "Katharina Stein and Daniel Fi\u0161er and J\u00f6rg Hoffmann and Alexander Koller", "abstract": "  Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL.\n", "link": "http://arxiv.org/abs/2311.09830v3", "date": "2025-01-06", "relevancy": 1.9119, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning&body=Title%3A%20Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning%0AAuthor%3A%20Katharina%20Stein%20and%20Daniel%20Fi%C5%A1er%20and%20J%C3%B6rg%20Hoffmann%20and%20Alexander%20Koller%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20a%20large%20variety%20of%20NLP%0Atasks.%20An%20active%20debate%20is%20to%20what%20extent%20they%20can%20do%20reasoning%20and%20planning.%0APrior%20work%20has%20assessed%20the%20latter%20in%20the%20specific%20context%20of%20PDDL%20planning%2C%0Abased%20on%20manually%20converting%20three%20PDDL%20domains%20into%20natural%20language%20%28NL%29%0Aprompts.%20Here%20we%20automate%20this%20conversion%20step%2C%20showing%20how%20to%20leverage%20an%20LLM%0Ato%20automatically%20generate%20NL%20prompts%20from%20PDDL%20input.%20Our%20automatically%0Agenerated%20NL%20prompts%20result%20in%20similar%20LLM-planning%20performance%20as%20the%20previous%0Amanually%20generated%20ones.%20Beyond%20this%2C%20the%20automation%20enables%20us%20to%20run%20much%0Alarger%20experiments%2C%20providing%20for%20the%20first%20time%20a%20broad%20evaluation%20of%20LLM%0Aplanning%20performance%20in%20PDDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09830v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520the%2520Generation%2520of%2520Prompts%2520for%2520LLM-based%2520Action%2520Choice%2520in%2520PDDL%250A%2520%2520Planning%26entry.906535625%3DKatharina%2520Stein%2520and%2520Daniel%2520Fi%25C5%25A1er%2520and%2520J%25C3%25B6rg%2520Hoffmann%2520and%2520Alexander%2520Koller%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520a%2520large%2520variety%2520of%2520NLP%250Atasks.%2520An%2520active%2520debate%2520is%2520to%2520what%2520extent%2520they%2520can%2520do%2520reasoning%2520and%2520planning.%250APrior%2520work%2520has%2520assessed%2520the%2520latter%2520in%2520the%2520specific%2520context%2520of%2520PDDL%2520planning%252C%250Abased%2520on%2520manually%2520converting%2520three%2520PDDL%2520domains%2520into%2520natural%2520language%2520%2528NL%2529%250Aprompts.%2520Here%2520we%2520automate%2520this%2520conversion%2520step%252C%2520showing%2520how%2520to%2520leverage%2520an%2520LLM%250Ato%2520automatically%2520generate%2520NL%2520prompts%2520from%2520PDDL%2520input.%2520Our%2520automatically%250Agenerated%2520NL%2520prompts%2520result%2520in%2520similar%2520LLM-planning%2520performance%2520as%2520the%2520previous%250Amanually%2520generated%2520ones.%2520Beyond%2520this%252C%2520the%2520automation%2520enables%2520us%2520to%2520run%2520much%250Alarger%2520experiments%252C%2520providing%2520for%2520the%2520first%2520time%2520a%2520broad%2520evaluation%2520of%2520LLM%250Aplanning%2520performance%2520in%2520PDDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09830v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20the%20Generation%20of%20Prompts%20for%20LLM-based%20Action%20Choice%20in%20PDDL%0A%20%20Planning&entry.906535625=Katharina%20Stein%20and%20Daniel%20Fi%C5%A1er%20and%20J%C3%B6rg%20Hoffmann%20and%20Alexander%20Koller&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20a%20large%20variety%20of%20NLP%0Atasks.%20An%20active%20debate%20is%20to%20what%20extent%20they%20can%20do%20reasoning%20and%20planning.%0APrior%20work%20has%20assessed%20the%20latter%20in%20the%20specific%20context%20of%20PDDL%20planning%2C%0Abased%20on%20manually%20converting%20three%20PDDL%20domains%20into%20natural%20language%20%28NL%29%0Aprompts.%20Here%20we%20automate%20this%20conversion%20step%2C%20showing%20how%20to%20leverage%20an%20LLM%0Ato%20automatically%20generate%20NL%20prompts%20from%20PDDL%20input.%20Our%20automatically%0Agenerated%20NL%20prompts%20result%20in%20similar%20LLM-planning%20performance%20as%20the%20previous%0Amanually%20generated%20ones.%20Beyond%20this%2C%20the%20automation%20enables%20us%20to%20run%20much%0Alarger%20experiments%2C%20providing%20for%20the%20first%20time%20a%20broad%20evaluation%20of%20LLM%0Aplanning%20performance%20in%20PDDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09830v3&entry.124074799=Read"},
{"title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use", "author": "Mohit Chandra and Siddharth Sriraman and Gaurav Verma and Harneet Singh Khanuja and Jose Suarez Campayo and Zihang Li and Michael L. Birnbaum and Munmun De Choudhury", "abstract": "  Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.\n", "link": "http://arxiv.org/abs/2410.19155v2", "date": "2025-01-06", "relevancy": 1.7744, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4412}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lived%20Experience%20Not%20Found%3A%20LLMs%20Struggle%20to%20Align%20with%20Experts%20on%0A%20%20Addressing%20Adverse%20Drug%20Reactions%20from%20Psychiatric%20Medication%20Use&body=Title%3A%20Lived%20Experience%20Not%20Found%3A%20LLMs%20Struggle%20to%20Align%20with%20Experts%20on%0A%20%20Addressing%20Adverse%20Drug%20Reactions%20from%20Psychiatric%20Medication%20Use%0AAuthor%3A%20Mohit%20Chandra%20and%20Siddharth%20Sriraman%20and%20Gaurav%20Verma%20and%20Harneet%20Singh%20Khanuja%20and%20Jose%20Suarez%20Campayo%20and%20Zihang%20Li%20and%20Michael%20L.%20Birnbaum%20and%20Munmun%20De%20Choudhury%0AAbstract%3A%20%20%20Adverse%20Drug%20Reactions%20%28ADRs%29%20from%20psychiatric%20medications%20are%20the%20leading%0Acause%20of%20hospitalizations%20among%20mental%20health%20patients.%20With%20healthcare%20systems%0Aand%20online%20communities%20facing%20limitations%20in%20resolving%20ADR-related%20issues%2C%0ALarge%20Language%20Models%20%28LLMs%29%20have%20the%20potential%20to%20fill%20this%20gap.%20Despite%20the%0Aincreasing%20capabilities%20of%20LLMs%2C%20past%20research%20has%20not%20explored%20their%0Acapabilities%20in%20detecting%20ADRs%20related%20to%20psychiatric%20medications%20or%20in%0Aproviding%20effective%20harm%20reduction%20strategies.%20To%20address%20this%2C%20we%20introduce%0Athe%20Psych-ADR%20benchmark%20and%20the%20Adverse%20Drug%20Reaction%20Response%20Assessment%0A%28ADRA%29%20framework%20to%20systematically%20evaluate%20LLM%20performance%20in%20detecting%20ADR%0Aexpressions%20and%20delivering%20expert-aligned%20mitigation%20strategies.%20Our%20analyses%0Ashow%20that%20LLMs%20struggle%20with%20understanding%20the%20nuances%20of%20ADRs%20and%0Adifferentiating%20between%20types%20of%20ADRs.%20While%20LLMs%20align%20with%20experts%20in%20terms%0Aof%20expressed%20emotions%20and%20tone%20of%20the%20text%2C%20their%20responses%20are%20more%20complex%2C%0Aharder%20to%20read%2C%20and%20only%2070.86%25%20aligned%20with%20expert%20strategies.%20Furthermore%2C%0Athey%20provide%20less%20actionable%20advice%20by%20a%20margin%20of%2012.32%25%20on%20average.%20Our%20work%0Aprovides%20a%20comprehensive%20benchmark%20and%20evaluation%20framework%20for%20assessing%20LLMs%0Ain%20strategy-driven%20tasks%20within%20high-risk%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLived%2520Experience%2520Not%2520Found%253A%2520LLMs%2520Struggle%2520to%2520Align%2520with%2520Experts%2520on%250A%2520%2520Addressing%2520Adverse%2520Drug%2520Reactions%2520from%2520Psychiatric%2520Medication%2520Use%26entry.906535625%3DMohit%2520Chandra%2520and%2520Siddharth%2520Sriraman%2520and%2520Gaurav%2520Verma%2520and%2520Harneet%2520Singh%2520Khanuja%2520and%2520Jose%2520Suarez%2520Campayo%2520and%2520Zihang%2520Li%2520and%2520Michael%2520L.%2520Birnbaum%2520and%2520Munmun%2520De%2520Choudhury%26entry.1292438233%3D%2520%2520Adverse%2520Drug%2520Reactions%2520%2528ADRs%2529%2520from%2520psychiatric%2520medications%2520are%2520the%2520leading%250Acause%2520of%2520hospitalizations%2520among%2520mental%2520health%2520patients.%2520With%2520healthcare%2520systems%250Aand%2520online%2520communities%2520facing%2520limitations%2520in%2520resolving%2520ADR-related%2520issues%252C%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520the%2520potential%2520to%2520fill%2520this%2520gap.%2520Despite%2520the%250Aincreasing%2520capabilities%2520of%2520LLMs%252C%2520past%2520research%2520has%2520not%2520explored%2520their%250Acapabilities%2520in%2520detecting%2520ADRs%2520related%2520to%2520psychiatric%2520medications%2520or%2520in%250Aproviding%2520effective%2520harm%2520reduction%2520strategies.%2520To%2520address%2520this%252C%2520we%2520introduce%250Athe%2520Psych-ADR%2520benchmark%2520and%2520the%2520Adverse%2520Drug%2520Reaction%2520Response%2520Assessment%250A%2528ADRA%2529%2520framework%2520to%2520systematically%2520evaluate%2520LLM%2520performance%2520in%2520detecting%2520ADR%250Aexpressions%2520and%2520delivering%2520expert-aligned%2520mitigation%2520strategies.%2520Our%2520analyses%250Ashow%2520that%2520LLMs%2520struggle%2520with%2520understanding%2520the%2520nuances%2520of%2520ADRs%2520and%250Adifferentiating%2520between%2520types%2520of%2520ADRs.%2520While%2520LLMs%2520align%2520with%2520experts%2520in%2520terms%250Aof%2520expressed%2520emotions%2520and%2520tone%2520of%2520the%2520text%252C%2520their%2520responses%2520are%2520more%2520complex%252C%250Aharder%2520to%2520read%252C%2520and%2520only%252070.86%2525%2520aligned%2520with%2520expert%2520strategies.%2520Furthermore%252C%250Athey%2520provide%2520less%2520actionable%2520advice%2520by%2520a%2520margin%2520of%252012.32%2525%2520on%2520average.%2520Our%2520work%250Aprovides%2520a%2520comprehensive%2520benchmark%2520and%2520evaluation%2520framework%2520for%2520assessing%2520LLMs%250Ain%2520strategy-driven%2520tasks%2520within%2520high-risk%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lived%20Experience%20Not%20Found%3A%20LLMs%20Struggle%20to%20Align%20with%20Experts%20on%0A%20%20Addressing%20Adverse%20Drug%20Reactions%20from%20Psychiatric%20Medication%20Use&entry.906535625=Mohit%20Chandra%20and%20Siddharth%20Sriraman%20and%20Gaurav%20Verma%20and%20Harneet%20Singh%20Khanuja%20and%20Jose%20Suarez%20Campayo%20and%20Zihang%20Li%20and%20Michael%20L.%20Birnbaum%20and%20Munmun%20De%20Choudhury&entry.1292438233=%20%20Adverse%20Drug%20Reactions%20%28ADRs%29%20from%20psychiatric%20medications%20are%20the%20leading%0Acause%20of%20hospitalizations%20among%20mental%20health%20patients.%20With%20healthcare%20systems%0Aand%20online%20communities%20facing%20limitations%20in%20resolving%20ADR-related%20issues%2C%0ALarge%20Language%20Models%20%28LLMs%29%20have%20the%20potential%20to%20fill%20this%20gap.%20Despite%20the%0Aincreasing%20capabilities%20of%20LLMs%2C%20past%20research%20has%20not%20explored%20their%0Acapabilities%20in%20detecting%20ADRs%20related%20to%20psychiatric%20medications%20or%20in%0Aproviding%20effective%20harm%20reduction%20strategies.%20To%20address%20this%2C%20we%20introduce%0Athe%20Psych-ADR%20benchmark%20and%20the%20Adverse%20Drug%20Reaction%20Response%20Assessment%0A%28ADRA%29%20framework%20to%20systematically%20evaluate%20LLM%20performance%20in%20detecting%20ADR%0Aexpressions%20and%20delivering%20expert-aligned%20mitigation%20strategies.%20Our%20analyses%0Ashow%20that%20LLMs%20struggle%20with%20understanding%20the%20nuances%20of%20ADRs%20and%0Adifferentiating%20between%20types%20of%20ADRs.%20While%20LLMs%20align%20with%20experts%20in%20terms%0Aof%20expressed%20emotions%20and%20tone%20of%20the%20text%2C%20their%20responses%20are%20more%20complex%2C%0Aharder%20to%20read%2C%20and%20only%2070.86%25%20aligned%20with%20expert%20strategies.%20Furthermore%2C%0Athey%20provide%20less%20actionable%20advice%20by%20a%20margin%20of%2012.32%25%20on%20average.%20Our%20work%0Aprovides%20a%20comprehensive%20benchmark%20and%20evaluation%20framework%20for%20assessing%20LLMs%0Ain%20strategy-driven%20tasks%20within%20high-risk%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19155v2&entry.124074799=Read"},
{"title": "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots", "author": "Sahar Salimpour and Jorge Pe\u00f1a-Queralta and Diego Paez-Granados and Jukka Heikkonen and Tomi Westerlund", "abstract": "  Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps://github.com/sahars93/RL-Navigation.\n", "link": "http://arxiv.org/abs/2501.02902v1", "date": "2025-01-06", "relevancy": 1.6512, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5655}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim-to-Real%20Transfer%20for%20Mobile%20Robots%20with%20Reinforcement%20Learning%3A%20from%0A%20%20NVIDIA%20Isaac%20Sim%20to%20Gazebo%20and%20Real%20ROS%202%20Robots&body=Title%3A%20Sim-to-Real%20Transfer%20for%20Mobile%20Robots%20with%20Reinforcement%20Learning%3A%20from%0A%20%20NVIDIA%20Isaac%20Sim%20to%20Gazebo%20and%20Real%20ROS%202%20Robots%0AAuthor%3A%20Sahar%20Salimpour%20and%20Jorge%20Pe%C3%B1a-Queralta%20and%20Diego%20Paez-Granados%20and%20Jukka%20Heikkonen%20and%20Tomi%20Westerlund%0AAbstract%3A%20%20%20Unprecedented%20agility%20and%20dexterous%20manipulation%20have%20been%20demonstrated%20with%0Acontrollers%20based%20on%20deep%20reinforcement%20learning%20%28RL%29%2C%20with%20a%20significant%0Aimpact%20on%20legged%20and%20humanoid%20robots.%20Modern%20tooling%20and%20simulation%20platforms%2C%0Asuch%20as%20NVIDIA%20Isaac%20Sim%2C%20have%20been%20enabling%20such%20advances.%20This%20article%0Afocuses%20on%20demonstrating%20the%20applications%20of%20Isaac%20in%20local%20planning%20and%0Aobstacle%20avoidance%20as%20one%20of%20the%20most%20fundamental%20ways%20in%20which%20a%20mobile%20robot%0Ainteracts%20with%20its%20environments.%20Although%20there%20is%20extensive%20research%20on%0Aproprioception-based%20RL%20policies%2C%20the%20article%20highlights%20less%20standardized%20and%0Areproducible%20approaches%20to%20exteroception.%20At%20the%20same%20time%2C%20the%20article%20aims%20to%0Aprovide%20a%20base%20framework%20for%20end-to-end%20local%20navigation%20policies%20and%20how%20a%0Acustom%20robot%20can%20be%20trained%20in%20such%20simulation%20environment.%20We%20benchmark%0Aend-to-end%20policies%20with%20the%20state-of-the-art%20Nav2%2C%20navigation%20stack%20in%20Robot%0AOperating%20System%20%28ROS%29.%20We%20also%20cover%20the%20sim-to-real%20transfer%20process%20by%0Ademonstrating%20zero-shot%20transferability%20of%20policies%20trained%20in%20the%20Isaac%0Asimulator%20to%20real-world%20robots.%20This%20is%20further%20evidenced%20by%20the%20tests%20with%0Adifferent%20simulated%20robots%2C%20which%20show%20the%20generalization%20of%20the%20learned%0Apolicy.%20Finally%2C%20the%20benchmarks%20demonstrate%20comparable%20performance%20to%20Nav2%2C%0Aopening%20the%20door%20to%20quick%20deployment%20of%20state-of-the-art%20end-to-end%20local%0Aplanners%20for%20custom%20robot%20platforms%2C%20but%20importantly%20furthering%20the%0Apossibilities%20by%20expanding%20the%20state%20and%20action%20spaces%20or%20task%20definitions%20for%0Amore%20complex%20missions.%20Overall%2C%20with%20this%20article%20we%20introduce%20the%20most%0Aimportant%20steps%2C%20and%20aspects%20to%20consider%2C%20in%20deploying%20RL%20policies%20for%20local%0Apath%20planning%20and%20obstacle%20avoidance%20with%20Isaac%20Sim%20training%2C%20Gazebo%20testing%2C%0Aand%20ROS%202%20for%20real-time%20inference%20in%20real%20robots.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sahars93/RL-Navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim-to-Real%2520Transfer%2520for%2520Mobile%2520Robots%2520with%2520Reinforcement%2520Learning%253A%2520from%250A%2520%2520NVIDIA%2520Isaac%2520Sim%2520to%2520Gazebo%2520and%2520Real%2520ROS%25202%2520Robots%26entry.906535625%3DSahar%2520Salimpour%2520and%2520Jorge%2520Pe%25C3%25B1a-Queralta%2520and%2520Diego%2520Paez-Granados%2520and%2520Jukka%2520Heikkonen%2520and%2520Tomi%2520Westerlund%26entry.1292438233%3D%2520%2520Unprecedented%2520agility%2520and%2520dexterous%2520manipulation%2520have%2520been%2520demonstrated%2520with%250Acontrollers%2520based%2520on%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520with%2520a%2520significant%250Aimpact%2520on%2520legged%2520and%2520humanoid%2520robots.%2520Modern%2520tooling%2520and%2520simulation%2520platforms%252C%250Asuch%2520as%2520NVIDIA%2520Isaac%2520Sim%252C%2520have%2520been%2520enabling%2520such%2520advances.%2520This%2520article%250Afocuses%2520on%2520demonstrating%2520the%2520applications%2520of%2520Isaac%2520in%2520local%2520planning%2520and%250Aobstacle%2520avoidance%2520as%2520one%2520of%2520the%2520most%2520fundamental%2520ways%2520in%2520which%2520a%2520mobile%2520robot%250Ainteracts%2520with%2520its%2520environments.%2520Although%2520there%2520is%2520extensive%2520research%2520on%250Aproprioception-based%2520RL%2520policies%252C%2520the%2520article%2520highlights%2520less%2520standardized%2520and%250Areproducible%2520approaches%2520to%2520exteroception.%2520At%2520the%2520same%2520time%252C%2520the%2520article%2520aims%2520to%250Aprovide%2520a%2520base%2520framework%2520for%2520end-to-end%2520local%2520navigation%2520policies%2520and%2520how%2520a%250Acustom%2520robot%2520can%2520be%2520trained%2520in%2520such%2520simulation%2520environment.%2520We%2520benchmark%250Aend-to-end%2520policies%2520with%2520the%2520state-of-the-art%2520Nav2%252C%2520navigation%2520stack%2520in%2520Robot%250AOperating%2520System%2520%2528ROS%2529.%2520We%2520also%2520cover%2520the%2520sim-to-real%2520transfer%2520process%2520by%250Ademonstrating%2520zero-shot%2520transferability%2520of%2520policies%2520trained%2520in%2520the%2520Isaac%250Asimulator%2520to%2520real-world%2520robots.%2520This%2520is%2520further%2520evidenced%2520by%2520the%2520tests%2520with%250Adifferent%2520simulated%2520robots%252C%2520which%2520show%2520the%2520generalization%2520of%2520the%2520learned%250Apolicy.%2520Finally%252C%2520the%2520benchmarks%2520demonstrate%2520comparable%2520performance%2520to%2520Nav2%252C%250Aopening%2520the%2520door%2520to%2520quick%2520deployment%2520of%2520state-of-the-art%2520end-to-end%2520local%250Aplanners%2520for%2520custom%2520robot%2520platforms%252C%2520but%2520importantly%2520furthering%2520the%250Apossibilities%2520by%2520expanding%2520the%2520state%2520and%2520action%2520spaces%2520or%2520task%2520definitions%2520for%250Amore%2520complex%2520missions.%2520Overall%252C%2520with%2520this%2520article%2520we%2520introduce%2520the%2520most%250Aimportant%2520steps%252C%2520and%2520aspects%2520to%2520consider%252C%2520in%2520deploying%2520RL%2520policies%2520for%2520local%250Apath%2520planning%2520and%2520obstacle%2520avoidance%2520with%2520Isaac%2520Sim%2520training%252C%2520Gazebo%2520testing%252C%250Aand%2520ROS%25202%2520for%2520real-time%2520inference%2520in%2520real%2520robots.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sahars93/RL-Navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim-to-Real%20Transfer%20for%20Mobile%20Robots%20with%20Reinforcement%20Learning%3A%20from%0A%20%20NVIDIA%20Isaac%20Sim%20to%20Gazebo%20and%20Real%20ROS%202%20Robots&entry.906535625=Sahar%20Salimpour%20and%20Jorge%20Pe%C3%B1a-Queralta%20and%20Diego%20Paez-Granados%20and%20Jukka%20Heikkonen%20and%20Tomi%20Westerlund&entry.1292438233=%20%20Unprecedented%20agility%20and%20dexterous%20manipulation%20have%20been%20demonstrated%20with%0Acontrollers%20based%20on%20deep%20reinforcement%20learning%20%28RL%29%2C%20with%20a%20significant%0Aimpact%20on%20legged%20and%20humanoid%20robots.%20Modern%20tooling%20and%20simulation%20platforms%2C%0Asuch%20as%20NVIDIA%20Isaac%20Sim%2C%20have%20been%20enabling%20such%20advances.%20This%20article%0Afocuses%20on%20demonstrating%20the%20applications%20of%20Isaac%20in%20local%20planning%20and%0Aobstacle%20avoidance%20as%20one%20of%20the%20most%20fundamental%20ways%20in%20which%20a%20mobile%20robot%0Ainteracts%20with%20its%20environments.%20Although%20there%20is%20extensive%20research%20on%0Aproprioception-based%20RL%20policies%2C%20the%20article%20highlights%20less%20standardized%20and%0Areproducible%20approaches%20to%20exteroception.%20At%20the%20same%20time%2C%20the%20article%20aims%20to%0Aprovide%20a%20base%20framework%20for%20end-to-end%20local%20navigation%20policies%20and%20how%20a%0Acustom%20robot%20can%20be%20trained%20in%20such%20simulation%20environment.%20We%20benchmark%0Aend-to-end%20policies%20with%20the%20state-of-the-art%20Nav2%2C%20navigation%20stack%20in%20Robot%0AOperating%20System%20%28ROS%29.%20We%20also%20cover%20the%20sim-to-real%20transfer%20process%20by%0Ademonstrating%20zero-shot%20transferability%20of%20policies%20trained%20in%20the%20Isaac%0Asimulator%20to%20real-world%20robots.%20This%20is%20further%20evidenced%20by%20the%20tests%20with%0Adifferent%20simulated%20robots%2C%20which%20show%20the%20generalization%20of%20the%20learned%0Apolicy.%20Finally%2C%20the%20benchmarks%20demonstrate%20comparable%20performance%20to%20Nav2%2C%0Aopening%20the%20door%20to%20quick%20deployment%20of%20state-of-the-art%20end-to-end%20local%0Aplanners%20for%20custom%20robot%20platforms%2C%20but%20importantly%20furthering%20the%0Apossibilities%20by%20expanding%20the%20state%20and%20action%20spaces%20or%20task%20definitions%20for%0Amore%20complex%20missions.%20Overall%2C%20with%20this%20article%20we%20introduce%20the%20most%0Aimportant%20steps%2C%20and%20aspects%20to%20consider%2C%20in%20deploying%20RL%20policies%20for%20local%0Apath%20planning%20and%20obstacle%20avoidance%20with%20Isaac%20Sim%20training%2C%20Gazebo%20testing%2C%0Aand%20ROS%202%20for%20real-time%20inference%20in%20real%20robots.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sahars93/RL-Navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02902v1&entry.124074799=Read"},
{"title": "Enhancing Multirotor Drone Efficiency: Exploring Minimum Energy\n  Consumption Rate of Forward Flight under Varying Payload", "author": "Ayush Patnaik and Nicolas Michel and Xinfan Lin", "abstract": "  Multirotor unmanned aerial vehicle is a prevailing type of aircraft with wide\nreal-world applications. Energy efficiency is a critical aspect of its\nperformance, determining the range and duration of the missions that can be\nperformed. In this study, we show both analytically and numerically that the\noptimum of a key energy efficiency index in forward flight, namely energy per\nmeter traveled per unit mass, is a constant under different vehicle mass\n(including payload). Note that this relationship is only true under the optimal\nforward velocity that minimizes the energy consumption (under different mass),\nbut not under arbitrary velocity. The study is based on a previously developed\nmodel capturing the first-principle energy dynamics of the multirotor, and a\nkey step is to prove that the pitch angle under optimal velocity is a constant.\nBy employing both analytical derivation and validation studies, the research\nprovides critical insights into the optimization of multirotor energy\nefficiency, and facilitate the development of flight control strategies to\nextend mission duration and range.\n", "link": "http://arxiv.org/abs/2501.03102v1", "date": "2025-01-06", "relevancy": 1.6038, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multirotor%20Drone%20Efficiency%3A%20Exploring%20Minimum%20Energy%0A%20%20Consumption%20Rate%20of%20Forward%20Flight%20under%20Varying%20Payload&body=Title%3A%20Enhancing%20Multirotor%20Drone%20Efficiency%3A%20Exploring%20Minimum%20Energy%0A%20%20Consumption%20Rate%20of%20Forward%20Flight%20under%20Varying%20Payload%0AAuthor%3A%20Ayush%20Patnaik%20and%20Nicolas%20Michel%20and%20Xinfan%20Lin%0AAbstract%3A%20%20%20Multirotor%20unmanned%20aerial%20vehicle%20is%20a%20prevailing%20type%20of%20aircraft%20with%20wide%0Areal-world%20applications.%20Energy%20efficiency%20is%20a%20critical%20aspect%20of%20its%0Aperformance%2C%20determining%20the%20range%20and%20duration%20of%20the%20missions%20that%20can%20be%0Aperformed.%20In%20this%20study%2C%20we%20show%20both%20analytically%20and%20numerically%20that%20the%0Aoptimum%20of%20a%20key%20energy%20efficiency%20index%20in%20forward%20flight%2C%20namely%20energy%20per%0Ameter%20traveled%20per%20unit%20mass%2C%20is%20a%20constant%20under%20different%20vehicle%20mass%0A%28including%20payload%29.%20Note%20that%20this%20relationship%20is%20only%20true%20under%20the%20optimal%0Aforward%20velocity%20that%20minimizes%20the%20energy%20consumption%20%28under%20different%20mass%29%2C%0Abut%20not%20under%20arbitrary%20velocity.%20The%20study%20is%20based%20on%20a%20previously%20developed%0Amodel%20capturing%20the%20first-principle%20energy%20dynamics%20of%20the%20multirotor%2C%20and%20a%0Akey%20step%20is%20to%20prove%20that%20the%20pitch%20angle%20under%20optimal%20velocity%20is%20a%20constant.%0ABy%20employing%20both%20analytical%20derivation%20and%20validation%20studies%2C%20the%20research%0Aprovides%20critical%20insights%20into%20the%20optimization%20of%20multirotor%20energy%0Aefficiency%2C%20and%20facilitate%20the%20development%20of%20flight%20control%20strategies%20to%0Aextend%20mission%20duration%20and%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multirotor%2520Drone%2520Efficiency%253A%2520Exploring%2520Minimum%2520Energy%250A%2520%2520Consumption%2520Rate%2520of%2520Forward%2520Flight%2520under%2520Varying%2520Payload%26entry.906535625%3DAyush%2520Patnaik%2520and%2520Nicolas%2520Michel%2520and%2520Xinfan%2520Lin%26entry.1292438233%3D%2520%2520Multirotor%2520unmanned%2520aerial%2520vehicle%2520is%2520a%2520prevailing%2520type%2520of%2520aircraft%2520with%2520wide%250Areal-world%2520applications.%2520Energy%2520efficiency%2520is%2520a%2520critical%2520aspect%2520of%2520its%250Aperformance%252C%2520determining%2520the%2520range%2520and%2520duration%2520of%2520the%2520missions%2520that%2520can%2520be%250Aperformed.%2520In%2520this%2520study%252C%2520we%2520show%2520both%2520analytically%2520and%2520numerically%2520that%2520the%250Aoptimum%2520of%2520a%2520key%2520energy%2520efficiency%2520index%2520in%2520forward%2520flight%252C%2520namely%2520energy%2520per%250Ameter%2520traveled%2520per%2520unit%2520mass%252C%2520is%2520a%2520constant%2520under%2520different%2520vehicle%2520mass%250A%2528including%2520payload%2529.%2520Note%2520that%2520this%2520relationship%2520is%2520only%2520true%2520under%2520the%2520optimal%250Aforward%2520velocity%2520that%2520minimizes%2520the%2520energy%2520consumption%2520%2528under%2520different%2520mass%2529%252C%250Abut%2520not%2520under%2520arbitrary%2520velocity.%2520The%2520study%2520is%2520based%2520on%2520a%2520previously%2520developed%250Amodel%2520capturing%2520the%2520first-principle%2520energy%2520dynamics%2520of%2520the%2520multirotor%252C%2520and%2520a%250Akey%2520step%2520is%2520to%2520prove%2520that%2520the%2520pitch%2520angle%2520under%2520optimal%2520velocity%2520is%2520a%2520constant.%250ABy%2520employing%2520both%2520analytical%2520derivation%2520and%2520validation%2520studies%252C%2520the%2520research%250Aprovides%2520critical%2520insights%2520into%2520the%2520optimization%2520of%2520multirotor%2520energy%250Aefficiency%252C%2520and%2520facilitate%2520the%2520development%2520of%2520flight%2520control%2520strategies%2520to%250Aextend%2520mission%2520duration%2520and%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multirotor%20Drone%20Efficiency%3A%20Exploring%20Minimum%20Energy%0A%20%20Consumption%20Rate%20of%20Forward%20Flight%20under%20Varying%20Payload&entry.906535625=Ayush%20Patnaik%20and%20Nicolas%20Michel%20and%20Xinfan%20Lin&entry.1292438233=%20%20Multirotor%20unmanned%20aerial%20vehicle%20is%20a%20prevailing%20type%20of%20aircraft%20with%20wide%0Areal-world%20applications.%20Energy%20efficiency%20is%20a%20critical%20aspect%20of%20its%0Aperformance%2C%20determining%20the%20range%20and%20duration%20of%20the%20missions%20that%20can%20be%0Aperformed.%20In%20this%20study%2C%20we%20show%20both%20analytically%20and%20numerically%20that%20the%0Aoptimum%20of%20a%20key%20energy%20efficiency%20index%20in%20forward%20flight%2C%20namely%20energy%20per%0Ameter%20traveled%20per%20unit%20mass%2C%20is%20a%20constant%20under%20different%20vehicle%20mass%0A%28including%20payload%29.%20Note%20that%20this%20relationship%20is%20only%20true%20under%20the%20optimal%0Aforward%20velocity%20that%20minimizes%20the%20energy%20consumption%20%28under%20different%20mass%29%2C%0Abut%20not%20under%20arbitrary%20velocity.%20The%20study%20is%20based%20on%20a%20previously%20developed%0Amodel%20capturing%20the%20first-principle%20energy%20dynamics%20of%20the%20multirotor%2C%20and%20a%0Akey%20step%20is%20to%20prove%20that%20the%20pitch%20angle%20under%20optimal%20velocity%20is%20a%20constant.%0ABy%20employing%20both%20analytical%20derivation%20and%20validation%20studies%2C%20the%20research%0Aprovides%20critical%20insights%20into%20the%20optimization%20of%20multirotor%20energy%0Aefficiency%2C%20and%20facilitate%20the%20development%20of%20flight%20control%20strategies%20to%0Aextend%20mission%20duration%20and%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03102v1&entry.124074799=Read"},
{"title": "SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose\n  Estimation", "author": "Haozheng Xu and Alistair Weld and Chi Xu and Alfie Roddan and Joao Cartucho and Mert Asim Karaoglu and Alexander Ladikos and Yangke Li and Yiping Li and Daiyun Shen and Shoujie Yang and Geonhee Lee and Seyeon Park and Jongho Shin and Young-Gon Kim and Lucy Fothergill and Dominic Jones and Pietro Valdastri and Duygu Sarikaya and Stamatia Giannarou", "abstract": "  Accurate instrument pose estimation is a crucial step towards the future of\nrobotic surgery, enabling applications such as autonomous surgical task\nexecution. Vision-based methods for surgical instrument pose estimation provide\na practical approach to tool tracking, but they often require markers to be\nattached to the instruments. Recently, more research has focused on the\ndevelopment of marker-less methods based on deep learning. However, acquiring\nrealistic surgical data, with ground truth instrument poses, required for deep\nlearning training, is challenging. To address the issues in surgical instrument\npose estimation, we introduce the Surgical Robot Instrument Pose Estimation\n(SurgRIPE) challenge, hosted at the 26th International Conference on Medical\nImage Computing and Computer-Assisted Intervention (MICCAI) in 2023. The\nobjectives of this challenge are: (1) to provide the surgical vision community\nwith realistic surgical video data paired with ground truth instrument poses,\nand (2) to establish a benchmark for evaluating markerless pose estimation\nmethods. The challenge led to the development of several novel algorithms that\nshowcased improved accuracy and robustness over existing methods. The\nperformance evaluation study on the SurgRIPE dataset highlights the potential\nof these advanced algorithms to be integrated into robotic surgery systems,\npaving the way for more precise and autonomous surgical procedures. The\nSurgRIPE challenge has successfully established a new benchmark for the field,\nencouraging further research and development in surgical robot instrument pose\nestimation.\n", "link": "http://arxiv.org/abs/2501.02990v1", "date": "2025-01-06", "relevancy": 0.9742, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5073}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgRIPE%20challenge%3A%20Benchmark%20of%20Surgical%20Robot%20Instrument%20Pose%0A%20%20Estimation&body=Title%3A%20SurgRIPE%20challenge%3A%20Benchmark%20of%20Surgical%20Robot%20Instrument%20Pose%0A%20%20Estimation%0AAuthor%3A%20Haozheng%20Xu%20and%20Alistair%20Weld%20and%20Chi%20Xu%20and%20Alfie%20Roddan%20and%20Joao%20Cartucho%20and%20Mert%20Asim%20Karaoglu%20and%20Alexander%20Ladikos%20and%20Yangke%20Li%20and%20Yiping%20Li%20and%20Daiyun%20Shen%20and%20Shoujie%20Yang%20and%20Geonhee%20Lee%20and%20Seyeon%20Park%20and%20Jongho%20Shin%20and%20Young-Gon%20Kim%20and%20Lucy%20Fothergill%20and%20Dominic%20Jones%20and%20Pietro%20Valdastri%20and%20Duygu%20Sarikaya%20and%20Stamatia%20Giannarou%0AAbstract%3A%20%20%20Accurate%20instrument%20pose%20estimation%20is%20a%20crucial%20step%20towards%20the%20future%20of%0Arobotic%20surgery%2C%20enabling%20applications%20such%20as%20autonomous%20surgical%20task%0Aexecution.%20Vision-based%20methods%20for%20surgical%20instrument%20pose%20estimation%20provide%0Aa%20practical%20approach%20to%20tool%20tracking%2C%20but%20they%20often%20require%20markers%20to%20be%0Aattached%20to%20the%20instruments.%20Recently%2C%20more%20research%20has%20focused%20on%20the%0Adevelopment%20of%20marker-less%20methods%20based%20on%20deep%20learning.%20However%2C%20acquiring%0Arealistic%20surgical%20data%2C%20with%20ground%20truth%20instrument%20poses%2C%20required%20for%20deep%0Alearning%20training%2C%20is%20challenging.%20To%20address%20the%20issues%20in%20surgical%20instrument%0Apose%20estimation%2C%20we%20introduce%20the%20Surgical%20Robot%20Instrument%20Pose%20Estimation%0A%28SurgRIPE%29%20challenge%2C%20hosted%20at%20the%2026th%20International%20Conference%20on%20Medical%0AImage%20Computing%20and%20Computer-Assisted%20Intervention%20%28MICCAI%29%20in%202023.%20The%0Aobjectives%20of%20this%20challenge%20are%3A%20%281%29%20to%20provide%20the%20surgical%20vision%20community%0Awith%20realistic%20surgical%20video%20data%20paired%20with%20ground%20truth%20instrument%20poses%2C%0Aand%20%282%29%20to%20establish%20a%20benchmark%20for%20evaluating%20markerless%20pose%20estimation%0Amethods.%20The%20challenge%20led%20to%20the%20development%20of%20several%20novel%20algorithms%20that%0Ashowcased%20improved%20accuracy%20and%20robustness%20over%20existing%20methods.%20The%0Aperformance%20evaluation%20study%20on%20the%20SurgRIPE%20dataset%20highlights%20the%20potential%0Aof%20these%20advanced%20algorithms%20to%20be%20integrated%20into%20robotic%20surgery%20systems%2C%0Apaving%20the%20way%20for%20more%20precise%20and%20autonomous%20surgical%20procedures.%20The%0ASurgRIPE%20challenge%20has%20successfully%20established%20a%20new%20benchmark%20for%20the%20field%2C%0Aencouraging%20further%20research%20and%20development%20in%20surgical%20robot%20instrument%20pose%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgRIPE%2520challenge%253A%2520Benchmark%2520of%2520Surgical%2520Robot%2520Instrument%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DHaozheng%2520Xu%2520and%2520Alistair%2520Weld%2520and%2520Chi%2520Xu%2520and%2520Alfie%2520Roddan%2520and%2520Joao%2520Cartucho%2520and%2520Mert%2520Asim%2520Karaoglu%2520and%2520Alexander%2520Ladikos%2520and%2520Yangke%2520Li%2520and%2520Yiping%2520Li%2520and%2520Daiyun%2520Shen%2520and%2520Shoujie%2520Yang%2520and%2520Geonhee%2520Lee%2520and%2520Seyeon%2520Park%2520and%2520Jongho%2520Shin%2520and%2520Young-Gon%2520Kim%2520and%2520Lucy%2520Fothergill%2520and%2520Dominic%2520Jones%2520and%2520Pietro%2520Valdastri%2520and%2520Duygu%2520Sarikaya%2520and%2520Stamatia%2520Giannarou%26entry.1292438233%3D%2520%2520Accurate%2520instrument%2520pose%2520estimation%2520is%2520a%2520crucial%2520step%2520towards%2520the%2520future%2520of%250Arobotic%2520surgery%252C%2520enabling%2520applications%2520such%2520as%2520autonomous%2520surgical%2520task%250Aexecution.%2520Vision-based%2520methods%2520for%2520surgical%2520instrument%2520pose%2520estimation%2520provide%250Aa%2520practical%2520approach%2520to%2520tool%2520tracking%252C%2520but%2520they%2520often%2520require%2520markers%2520to%2520be%250Aattached%2520to%2520the%2520instruments.%2520Recently%252C%2520more%2520research%2520has%2520focused%2520on%2520the%250Adevelopment%2520of%2520marker-less%2520methods%2520based%2520on%2520deep%2520learning.%2520However%252C%2520acquiring%250Arealistic%2520surgical%2520data%252C%2520with%2520ground%2520truth%2520instrument%2520poses%252C%2520required%2520for%2520deep%250Alearning%2520training%252C%2520is%2520challenging.%2520To%2520address%2520the%2520issues%2520in%2520surgical%2520instrument%250Apose%2520estimation%252C%2520we%2520introduce%2520the%2520Surgical%2520Robot%2520Instrument%2520Pose%2520Estimation%250A%2528SurgRIPE%2529%2520challenge%252C%2520hosted%2520at%2520the%252026th%2520International%2520Conference%2520on%2520Medical%250AImage%2520Computing%2520and%2520Computer-Assisted%2520Intervention%2520%2528MICCAI%2529%2520in%25202023.%2520The%250Aobjectives%2520of%2520this%2520challenge%2520are%253A%2520%25281%2529%2520to%2520provide%2520the%2520surgical%2520vision%2520community%250Awith%2520realistic%2520surgical%2520video%2520data%2520paired%2520with%2520ground%2520truth%2520instrument%2520poses%252C%250Aand%2520%25282%2529%2520to%2520establish%2520a%2520benchmark%2520for%2520evaluating%2520markerless%2520pose%2520estimation%250Amethods.%2520The%2520challenge%2520led%2520to%2520the%2520development%2520of%2520several%2520novel%2520algorithms%2520that%250Ashowcased%2520improved%2520accuracy%2520and%2520robustness%2520over%2520existing%2520methods.%2520The%250Aperformance%2520evaluation%2520study%2520on%2520the%2520SurgRIPE%2520dataset%2520highlights%2520the%2520potential%250Aof%2520these%2520advanced%2520algorithms%2520to%2520be%2520integrated%2520into%2520robotic%2520surgery%2520systems%252C%250Apaving%2520the%2520way%2520for%2520more%2520precise%2520and%2520autonomous%2520surgical%2520procedures.%2520The%250ASurgRIPE%2520challenge%2520has%2520successfully%2520established%2520a%2520new%2520benchmark%2520for%2520the%2520field%252C%250Aencouraging%2520further%2520research%2520and%2520development%2520in%2520surgical%2520robot%2520instrument%2520pose%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgRIPE%20challenge%3A%20Benchmark%20of%20Surgical%20Robot%20Instrument%20Pose%0A%20%20Estimation&entry.906535625=Haozheng%20Xu%20and%20Alistair%20Weld%20and%20Chi%20Xu%20and%20Alfie%20Roddan%20and%20Joao%20Cartucho%20and%20Mert%20Asim%20Karaoglu%20and%20Alexander%20Ladikos%20and%20Yangke%20Li%20and%20Yiping%20Li%20and%20Daiyun%20Shen%20and%20Shoujie%20Yang%20and%20Geonhee%20Lee%20and%20Seyeon%20Park%20and%20Jongho%20Shin%20and%20Young-Gon%20Kim%20and%20Lucy%20Fothergill%20and%20Dominic%20Jones%20and%20Pietro%20Valdastri%20and%20Duygu%20Sarikaya%20and%20Stamatia%20Giannarou&entry.1292438233=%20%20Accurate%20instrument%20pose%20estimation%20is%20a%20crucial%20step%20towards%20the%20future%20of%0Arobotic%20surgery%2C%20enabling%20applications%20such%20as%20autonomous%20surgical%20task%0Aexecution.%20Vision-based%20methods%20for%20surgical%20instrument%20pose%20estimation%20provide%0Aa%20practical%20approach%20to%20tool%20tracking%2C%20but%20they%20often%20require%20markers%20to%20be%0Aattached%20to%20the%20instruments.%20Recently%2C%20more%20research%20has%20focused%20on%20the%0Adevelopment%20of%20marker-less%20methods%20based%20on%20deep%20learning.%20However%2C%20acquiring%0Arealistic%20surgical%20data%2C%20with%20ground%20truth%20instrument%20poses%2C%20required%20for%20deep%0Alearning%20training%2C%20is%20challenging.%20To%20address%20the%20issues%20in%20surgical%20instrument%0Apose%20estimation%2C%20we%20introduce%20the%20Surgical%20Robot%20Instrument%20Pose%20Estimation%0A%28SurgRIPE%29%20challenge%2C%20hosted%20at%20the%2026th%20International%20Conference%20on%20Medical%0AImage%20Computing%20and%20Computer-Assisted%20Intervention%20%28MICCAI%29%20in%202023.%20The%0Aobjectives%20of%20this%20challenge%20are%3A%20%281%29%20to%20provide%20the%20surgical%20vision%20community%0Awith%20realistic%20surgical%20video%20data%20paired%20with%20ground%20truth%20instrument%20poses%2C%0Aand%20%282%29%20to%20establish%20a%20benchmark%20for%20evaluating%20markerless%20pose%20estimation%0Amethods.%20The%20challenge%20led%20to%20the%20development%20of%20several%20novel%20algorithms%20that%0Ashowcased%20improved%20accuracy%20and%20robustness%20over%20existing%20methods.%20The%0Aperformance%20evaluation%20study%20on%20the%20SurgRIPE%20dataset%20highlights%20the%20potential%0Aof%20these%20advanced%20algorithms%20to%20be%20integrated%20into%20robotic%20surgery%20systems%2C%0Apaving%20the%20way%20for%20more%20precise%20and%20autonomous%20surgical%20procedures.%20The%0ASurgRIPE%20challenge%20has%20successfully%20established%20a%20new%20benchmark%20for%20the%20field%2C%0Aencouraging%20further%20research%20and%20development%20in%20surgical%20robot%20instrument%20pose%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02990v1&entry.124074799=Read"},
{"title": "Enhancing Sample Efficiency and Exploration in Reinforcement Learning\n  through the Integration of Diffusion Models and Proximal Policy Optimization", "author": "Gao Tianci and Dmitriev D. Dmitry and Konstantin A. Neusypin and Yang Bo and Rao Shengren", "abstract": "  Recent advancements in reinforcement learning (RL) have been fueled by\nlarge-scale data and deep neural networks, particularly for high-dimensional\nand complex tasks. Online RL methods like Proximal Policy Optimization (PPO)\nare effective in dynamic scenarios but require substantial real-time data,\nposing challenges in resource-constrained or slow simulation environments.\nOffline RL addresses this by pre-learning policies from large datasets, though\nits success depends on the quality and diversity of the data. This work\nproposes a framework that enhances PPO algorithms by incorporating a diffusion\nmodel to generate high-quality virtual trajectories for offline datasets. This\napproach improves exploration and sample efficiency, leading to significant\ngains in cumulative rewards, convergence speed, and strategy stability in\ncomplex tasks. Our contributions are threefold: we explore the potential of\ndiffusion models in RL, particularly for offline datasets, extend the\napplication of online RL to offline environments, and experimentally validate\nthe performance improvements of PPO with diffusion models. These findings\nprovide new insights and methods for applying RL to high-dimensional, complex\ntasks. Finally, we open-source our code at https://github.com/TianciGao/DiffPPO\n", "link": "http://arxiv.org/abs/2409.01427v4", "date": "2025-01-06", "relevancy": 1.5498, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sample%20Efficiency%20and%20Exploration%20in%20Reinforcement%20Learning%0A%20%20through%20the%20Integration%20of%20Diffusion%20Models%20and%20Proximal%20Policy%20Optimization&body=Title%3A%20Enhancing%20Sample%20Efficiency%20and%20Exploration%20in%20Reinforcement%20Learning%0A%20%20through%20the%20Integration%20of%20Diffusion%20Models%20and%20Proximal%20Policy%20Optimization%0AAuthor%3A%20Gao%20Tianci%20and%20Dmitriev%20D.%20Dmitry%20and%20Konstantin%20A.%20Neusypin%20and%20Yang%20Bo%20and%20Rao%20Shengren%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reinforcement%20learning%20%28RL%29%20have%20been%20fueled%20by%0Alarge-scale%20data%20and%20deep%20neural%20networks%2C%20particularly%20for%20high-dimensional%0Aand%20complex%20tasks.%20Online%20RL%20methods%20like%20Proximal%20Policy%20Optimization%20%28PPO%29%0Aare%20effective%20in%20dynamic%20scenarios%20but%20require%20substantial%20real-time%20data%2C%0Aposing%20challenges%20in%20resource-constrained%20or%20slow%20simulation%20environments.%0AOffline%20RL%20addresses%20this%20by%20pre-learning%20policies%20from%20large%20datasets%2C%20though%0Aits%20success%20depends%20on%20the%20quality%20and%20diversity%20of%20the%20data.%20This%20work%0Aproposes%20a%20framework%20that%20enhances%20PPO%20algorithms%20by%20incorporating%20a%20diffusion%0Amodel%20to%20generate%20high-quality%20virtual%20trajectories%20for%20offline%20datasets.%20This%0Aapproach%20improves%20exploration%20and%20sample%20efficiency%2C%20leading%20to%20significant%0Agains%20in%20cumulative%20rewards%2C%20convergence%20speed%2C%20and%20strategy%20stability%20in%0Acomplex%20tasks.%20Our%20contributions%20are%20threefold%3A%20we%20explore%20the%20potential%20of%0Adiffusion%20models%20in%20RL%2C%20particularly%20for%20offline%20datasets%2C%20extend%20the%0Aapplication%20of%20online%20RL%20to%20offline%20environments%2C%20and%20experimentally%20validate%0Athe%20performance%20improvements%20of%20PPO%20with%20diffusion%20models.%20These%20findings%0Aprovide%20new%20insights%20and%20methods%20for%20applying%20RL%20to%20high-dimensional%2C%20complex%0Atasks.%20Finally%2C%20we%20open-source%20our%20code%20at%20https%3A//github.com/TianciGao/DiffPPO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01427v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sample%2520Efficiency%2520and%2520Exploration%2520in%2520Reinforcement%2520Learning%250A%2520%2520through%2520the%2520Integration%2520of%2520Diffusion%2520Models%2520and%2520Proximal%2520Policy%2520Optimization%26entry.906535625%3DGao%2520Tianci%2520and%2520Dmitriev%2520D.%2520Dmitry%2520and%2520Konstantin%2520A.%2520Neusypin%2520and%2520Yang%2520Bo%2520and%2520Rao%2520Shengren%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520been%2520fueled%2520by%250Alarge-scale%2520data%2520and%2520deep%2520neural%2520networks%252C%2520particularly%2520for%2520high-dimensional%250Aand%2520complex%2520tasks.%2520Online%2520RL%2520methods%2520like%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%250Aare%2520effective%2520in%2520dynamic%2520scenarios%2520but%2520require%2520substantial%2520real-time%2520data%252C%250Aposing%2520challenges%2520in%2520resource-constrained%2520or%2520slow%2520simulation%2520environments.%250AOffline%2520RL%2520addresses%2520this%2520by%2520pre-learning%2520policies%2520from%2520large%2520datasets%252C%2520though%250Aits%2520success%2520depends%2520on%2520the%2520quality%2520and%2520diversity%2520of%2520the%2520data.%2520This%2520work%250Aproposes%2520a%2520framework%2520that%2520enhances%2520PPO%2520algorithms%2520by%2520incorporating%2520a%2520diffusion%250Amodel%2520to%2520generate%2520high-quality%2520virtual%2520trajectories%2520for%2520offline%2520datasets.%2520This%250Aapproach%2520improves%2520exploration%2520and%2520sample%2520efficiency%252C%2520leading%2520to%2520significant%250Agains%2520in%2520cumulative%2520rewards%252C%2520convergence%2520speed%252C%2520and%2520strategy%2520stability%2520in%250Acomplex%2520tasks.%2520Our%2520contributions%2520are%2520threefold%253A%2520we%2520explore%2520the%2520potential%2520of%250Adiffusion%2520models%2520in%2520RL%252C%2520particularly%2520for%2520offline%2520datasets%252C%2520extend%2520the%250Aapplication%2520of%2520online%2520RL%2520to%2520offline%2520environments%252C%2520and%2520experimentally%2520validate%250Athe%2520performance%2520improvements%2520of%2520PPO%2520with%2520diffusion%2520models.%2520These%2520findings%250Aprovide%2520new%2520insights%2520and%2520methods%2520for%2520applying%2520RL%2520to%2520high-dimensional%252C%2520complex%250Atasks.%2520Finally%252C%2520we%2520open-source%2520our%2520code%2520at%2520https%253A//github.com/TianciGao/DiffPPO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01427v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sample%20Efficiency%20and%20Exploration%20in%20Reinforcement%20Learning%0A%20%20through%20the%20Integration%20of%20Diffusion%20Models%20and%20Proximal%20Policy%20Optimization&entry.906535625=Gao%20Tianci%20and%20Dmitriev%20D.%20Dmitry%20and%20Konstantin%20A.%20Neusypin%20and%20Yang%20Bo%20and%20Rao%20Shengren&entry.1292438233=%20%20Recent%20advancements%20in%20reinforcement%20learning%20%28RL%29%20have%20been%20fueled%20by%0Alarge-scale%20data%20and%20deep%20neural%20networks%2C%20particularly%20for%20high-dimensional%0Aand%20complex%20tasks.%20Online%20RL%20methods%20like%20Proximal%20Policy%20Optimization%20%28PPO%29%0Aare%20effective%20in%20dynamic%20scenarios%20but%20require%20substantial%20real-time%20data%2C%0Aposing%20challenges%20in%20resource-constrained%20or%20slow%20simulation%20environments.%0AOffline%20RL%20addresses%20this%20by%20pre-learning%20policies%20from%20large%20datasets%2C%20though%0Aits%20success%20depends%20on%20the%20quality%20and%20diversity%20of%20the%20data.%20This%20work%0Aproposes%20a%20framework%20that%20enhances%20PPO%20algorithms%20by%20incorporating%20a%20diffusion%0Amodel%20to%20generate%20high-quality%20virtual%20trajectories%20for%20offline%20datasets.%20This%0Aapproach%20improves%20exploration%20and%20sample%20efficiency%2C%20leading%20to%20significant%0Agains%20in%20cumulative%20rewards%2C%20convergence%20speed%2C%20and%20strategy%20stability%20in%0Acomplex%20tasks.%20Our%20contributions%20are%20threefold%3A%20we%20explore%20the%20potential%20of%0Adiffusion%20models%20in%20RL%2C%20particularly%20for%20offline%20datasets%2C%20extend%20the%0Aapplication%20of%20online%20RL%20to%20offline%20environments%2C%20and%20experimentally%20validate%0Athe%20performance%20improvements%20of%20PPO%20with%20diffusion%20models.%20These%20findings%0Aprovide%20new%20insights%20and%20methods%20for%20applying%20RL%20to%20high-dimensional%2C%20complex%0Atasks.%20Finally%2C%20we%20open-source%20our%20code%20at%20https%3A//github.com/TianciGao/DiffPPO%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01427v4&entry.124074799=Read"},
{"title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation", "author": "Benjamin Steenhoek and Michele Tufano and Neel Sundaresan and Alexey Svyatkovskiy", "abstract": "  Software testing is a crucial but time-consuming aspect of software\ndevelopment, and recently, Large Language Models (LLMs) have gained popularity\nfor automated test case generation. However, because LLMs are trained on vast\namounts of open-source code, they often generate test cases that do not adhere\nto best practices and may even contain test smells (anti-patterns). To address\nthis issue, we propose Reinforcement Learning from Static Quality Metrics\n(RLSQM), wherein we utilize Reinforcement Learning to generate high-quality\nunit tests based on static analysis-based quality metrics. First, we analyzed\nLLM-generated tests and show that LLMs frequently do generate undesirable test\nsmells -- up to 37% of the time. Then, we implemented lightweight static\nanalysis-based reward model and trained LLMs using this reward model to\noptimize for five code quality metrics. Our experimental results demonstrate\nthat the RL-optimized Codex model consistently generated higher-quality test\ncases than the base LLM, improving quality metrics by up to 23%, and generated\nnearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all\ncode quality metrics, in spite of training a substantially cheaper Codex model.\nWe provide insights into how reliably utilize RL to improve test generation\nquality and show that RLSQM is a significant step towards enhancing the overall\nefficiency and reliability of automated software testing. Our data are\navailable at https://doi.org/10.6084/m9.figshare.25983166.\n", "link": "http://arxiv.org/abs/2412.14308v2", "date": "2025-01-06", "relevancy": 1.4338, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4904}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4745}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20from%20Automatic%20Feedback%20for%20High-Quality%20Unit%0A%20%20Test%20Generation&body=Title%3A%20Reinforcement%20Learning%20from%20Automatic%20Feedback%20for%20High-Quality%20Unit%0A%20%20Test%20Generation%0AAuthor%3A%20Benjamin%20Steenhoek%20and%20Michele%20Tufano%20and%20Neel%20Sundaresan%20and%20Alexey%20Svyatkovskiy%0AAbstract%3A%20%20%20Software%20testing%20is%20a%20crucial%20but%20time-consuming%20aspect%20of%20software%0Adevelopment%2C%20and%20recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20popularity%0Afor%20automated%20test%20case%20generation.%20However%2C%20because%20LLMs%20are%20trained%20on%20vast%0Aamounts%20of%20open-source%20code%2C%20they%20often%20generate%20test%20cases%20that%20do%20not%20adhere%0Ato%20best%20practices%20and%20may%20even%20contain%20test%20smells%20%28anti-patterns%29.%20To%20address%0Athis%20issue%2C%20we%20propose%20Reinforcement%20Learning%20from%20Static%20Quality%20Metrics%0A%28RLSQM%29%2C%20wherein%20we%20utilize%20Reinforcement%20Learning%20to%20generate%20high-quality%0Aunit%20tests%20based%20on%20static%20analysis-based%20quality%20metrics.%20First%2C%20we%20analyzed%0ALLM-generated%20tests%20and%20show%20that%20LLMs%20frequently%20do%20generate%20undesirable%20test%0Asmells%20--%20up%20to%2037%25%20of%20the%20time.%20Then%2C%20we%20implemented%20lightweight%20static%0Aanalysis-based%20reward%20model%20and%20trained%20LLMs%20using%20this%20reward%20model%20to%0Aoptimize%20for%20five%20code%20quality%20metrics.%20Our%20experimental%20results%20demonstrate%0Athat%20the%20RL-optimized%20Codex%20model%20consistently%20generated%20higher-quality%20test%0Acases%20than%20the%20base%20LLM%2C%20improving%20quality%20metrics%20by%20up%20to%2023%25%2C%20and%20generated%0Anearly%20100%25%20syntactically-correct%20code.%20RLSQM%20also%20outperformed%20GPT-4%20on%20all%0Acode%20quality%20metrics%2C%20in%20spite%20of%20training%20a%20substantially%20cheaper%20Codex%20model.%0AWe%20provide%20insights%20into%20how%20reliably%20utilize%20RL%20to%20improve%20test%20generation%0Aquality%20and%20show%20that%20RLSQM%20is%20a%20significant%20step%20towards%20enhancing%20the%20overall%0Aefficiency%20and%20reliability%20of%20automated%20software%20testing.%20Our%20data%20are%0Aavailable%20at%20https%3A//doi.org/10.6084/m9.figshare.25983166.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520from%2520Automatic%2520Feedback%2520for%2520High-Quality%2520Unit%250A%2520%2520Test%2520Generation%26entry.906535625%3DBenjamin%2520Steenhoek%2520and%2520Michele%2520Tufano%2520and%2520Neel%2520Sundaresan%2520and%2520Alexey%2520Svyatkovskiy%26entry.1292438233%3D%2520%2520Software%2520testing%2520is%2520a%2520crucial%2520but%2520time-consuming%2520aspect%2520of%2520software%250Adevelopment%252C%2520and%2520recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520gained%2520popularity%250Afor%2520automated%2520test%2520case%2520generation.%2520However%252C%2520because%2520LLMs%2520are%2520trained%2520on%2520vast%250Aamounts%2520of%2520open-source%2520code%252C%2520they%2520often%2520generate%2520test%2520cases%2520that%2520do%2520not%2520adhere%250Ato%2520best%2520practices%2520and%2520may%2520even%2520contain%2520test%2520smells%2520%2528anti-patterns%2529.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520Reinforcement%2520Learning%2520from%2520Static%2520Quality%2520Metrics%250A%2528RLSQM%2529%252C%2520wherein%2520we%2520utilize%2520Reinforcement%2520Learning%2520to%2520generate%2520high-quality%250Aunit%2520tests%2520based%2520on%2520static%2520analysis-based%2520quality%2520metrics.%2520First%252C%2520we%2520analyzed%250ALLM-generated%2520tests%2520and%2520show%2520that%2520LLMs%2520frequently%2520do%2520generate%2520undesirable%2520test%250Asmells%2520--%2520up%2520to%252037%2525%2520of%2520the%2520time.%2520Then%252C%2520we%2520implemented%2520lightweight%2520static%250Aanalysis-based%2520reward%2520model%2520and%2520trained%2520LLMs%2520using%2520this%2520reward%2520model%2520to%250Aoptimize%2520for%2520five%2520code%2520quality%2520metrics.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520the%2520RL-optimized%2520Codex%2520model%2520consistently%2520generated%2520higher-quality%2520test%250Acases%2520than%2520the%2520base%2520LLM%252C%2520improving%2520quality%2520metrics%2520by%2520up%2520to%252023%2525%252C%2520and%2520generated%250Anearly%2520100%2525%2520syntactically-correct%2520code.%2520RLSQM%2520also%2520outperformed%2520GPT-4%2520on%2520all%250Acode%2520quality%2520metrics%252C%2520in%2520spite%2520of%2520training%2520a%2520substantially%2520cheaper%2520Codex%2520model.%250AWe%2520provide%2520insights%2520into%2520how%2520reliably%2520utilize%2520RL%2520to%2520improve%2520test%2520generation%250Aquality%2520and%2520show%2520that%2520RLSQM%2520is%2520a%2520significant%2520step%2520towards%2520enhancing%2520the%2520overall%250Aefficiency%2520and%2520reliability%2520of%2520automated%2520software%2520testing.%2520Our%2520data%2520are%250Aavailable%2520at%2520https%253A//doi.org/10.6084/m9.figshare.25983166.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20from%20Automatic%20Feedback%20for%20High-Quality%20Unit%0A%20%20Test%20Generation&entry.906535625=Benjamin%20Steenhoek%20and%20Michele%20Tufano%20and%20Neel%20Sundaresan%20and%20Alexey%20Svyatkovskiy&entry.1292438233=%20%20Software%20testing%20is%20a%20crucial%20but%20time-consuming%20aspect%20of%20software%0Adevelopment%2C%20and%20recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20popularity%0Afor%20automated%20test%20case%20generation.%20However%2C%20because%20LLMs%20are%20trained%20on%20vast%0Aamounts%20of%20open-source%20code%2C%20they%20often%20generate%20test%20cases%20that%20do%20not%20adhere%0Ato%20best%20practices%20and%20may%20even%20contain%20test%20smells%20%28anti-patterns%29.%20To%20address%0Athis%20issue%2C%20we%20propose%20Reinforcement%20Learning%20from%20Static%20Quality%20Metrics%0A%28RLSQM%29%2C%20wherein%20we%20utilize%20Reinforcement%20Learning%20to%20generate%20high-quality%0Aunit%20tests%20based%20on%20static%20analysis-based%20quality%20metrics.%20First%2C%20we%20analyzed%0ALLM-generated%20tests%20and%20show%20that%20LLMs%20frequently%20do%20generate%20undesirable%20test%0Asmells%20--%20up%20to%2037%25%20of%20the%20time.%20Then%2C%20we%20implemented%20lightweight%20static%0Aanalysis-based%20reward%20model%20and%20trained%20LLMs%20using%20this%20reward%20model%20to%0Aoptimize%20for%20five%20code%20quality%20metrics.%20Our%20experimental%20results%20demonstrate%0Athat%20the%20RL-optimized%20Codex%20model%20consistently%20generated%20higher-quality%20test%0Acases%20than%20the%20base%20LLM%2C%20improving%20quality%20metrics%20by%20up%20to%2023%25%2C%20and%20generated%0Anearly%20100%25%20syntactically-correct%20code.%20RLSQM%20also%20outperformed%20GPT-4%20on%20all%0Acode%20quality%20metrics%2C%20in%20spite%20of%20training%20a%20substantially%20cheaper%20Codex%20model.%0AWe%20provide%20insights%20into%20how%20reliably%20utilize%20RL%20to%20improve%20test%20generation%0Aquality%20and%20show%20that%20RLSQM%20is%20a%20significant%20step%20towards%20enhancing%20the%20overall%0Aefficiency%20and%20reliability%20of%20automated%20software%20testing.%20Our%20data%20are%0Aavailable%20at%20https%3A//doi.org/10.6084/m9.figshare.25983166.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14308v2&entry.124074799=Read"},
{"title": "Communication Bounds for the Distributed Experts Problem", "author": "Zhihao Jia and Qi Pang and Trung Tran and David Woodruff and Zhihao Zhang and Wenting Zheng", "abstract": "  In this work, we study the experts problem in the distributed setting where\nan expert's cost needs to be aggregated across multiple servers. Our study\nconsiders various communication models such as the message-passing model and\nthe broadcast model, along with multiple aggregation functions, such as summing\nand taking the $\\ell_p$ norm of an expert's cost across servers. We propose the\nfirst communication-efficient protocols that achieve near-optimal regret in\nthese settings, even against a strong adversary who can choose the inputs\nadaptively. Additionally, we give a conditional lower bound showing that the\ncommunication of our protocols is nearly optimal. Finally, we implement our\nprotocols and demonstrate empirical savings on the HPO-B benchmarks.\n", "link": "http://arxiv.org/abs/2501.03132v1", "date": "2025-01-06", "relevancy": 1.1132, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3771}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3762}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication%20Bounds%20for%20the%20Distributed%20Experts%20Problem&body=Title%3A%20Communication%20Bounds%20for%20the%20Distributed%20Experts%20Problem%0AAuthor%3A%20Zhihao%20Jia%20and%20Qi%20Pang%20and%20Trung%20Tran%20and%20David%20Woodruff%20and%20Zhihao%20Zhang%20and%20Wenting%20Zheng%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20experts%20problem%20in%20the%20distributed%20setting%20where%0Aan%20expert%27s%20cost%20needs%20to%20be%20aggregated%20across%20multiple%20servers.%20Our%20study%0Aconsiders%20various%20communication%20models%20such%20as%20the%20message-passing%20model%20and%0Athe%20broadcast%20model%2C%20along%20with%20multiple%20aggregation%20functions%2C%20such%20as%20summing%0Aand%20taking%20the%20%24%5Cell_p%24%20norm%20of%20an%20expert%27s%20cost%20across%20servers.%20We%20propose%20the%0Afirst%20communication-efficient%20protocols%20that%20achieve%20near-optimal%20regret%20in%0Athese%20settings%2C%20even%20against%20a%20strong%20adversary%20who%20can%20choose%20the%20inputs%0Aadaptively.%20Additionally%2C%20we%20give%20a%20conditional%20lower%20bound%20showing%20that%20the%0Acommunication%20of%20our%20protocols%20is%20nearly%20optimal.%20Finally%2C%20we%20implement%20our%0Aprotocols%20and%20demonstrate%20empirical%20savings%20on%20the%20HPO-B%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication%2520Bounds%2520for%2520the%2520Distributed%2520Experts%2520Problem%26entry.906535625%3DZhihao%2520Jia%2520and%2520Qi%2520Pang%2520and%2520Trung%2520Tran%2520and%2520David%2520Woodruff%2520and%2520Zhihao%2520Zhang%2520and%2520Wenting%2520Zheng%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520experts%2520problem%2520in%2520the%2520distributed%2520setting%2520where%250Aan%2520expert%2527s%2520cost%2520needs%2520to%2520be%2520aggregated%2520across%2520multiple%2520servers.%2520Our%2520study%250Aconsiders%2520various%2520communication%2520models%2520such%2520as%2520the%2520message-passing%2520model%2520and%250Athe%2520broadcast%2520model%252C%2520along%2520with%2520multiple%2520aggregation%2520functions%252C%2520such%2520as%2520summing%250Aand%2520taking%2520the%2520%2524%255Cell_p%2524%2520norm%2520of%2520an%2520expert%2527s%2520cost%2520across%2520servers.%2520We%2520propose%2520the%250Afirst%2520communication-efficient%2520protocols%2520that%2520achieve%2520near-optimal%2520regret%2520in%250Athese%2520settings%252C%2520even%2520against%2520a%2520strong%2520adversary%2520who%2520can%2520choose%2520the%2520inputs%250Aadaptively.%2520Additionally%252C%2520we%2520give%2520a%2520conditional%2520lower%2520bound%2520showing%2520that%2520the%250Acommunication%2520of%2520our%2520protocols%2520is%2520nearly%2520optimal.%2520Finally%252C%2520we%2520implement%2520our%250Aprotocols%2520and%2520demonstrate%2520empirical%2520savings%2520on%2520the%2520HPO-B%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication%20Bounds%20for%20the%20Distributed%20Experts%20Problem&entry.906535625=Zhihao%20Jia%20and%20Qi%20Pang%20and%20Trung%20Tran%20and%20David%20Woodruff%20and%20Zhihao%20Zhang%20and%20Wenting%20Zheng&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20experts%20problem%20in%20the%20distributed%20setting%20where%0Aan%20expert%27s%20cost%20needs%20to%20be%20aggregated%20across%20multiple%20servers.%20Our%20study%0Aconsiders%20various%20communication%20models%20such%20as%20the%20message-passing%20model%20and%0Athe%20broadcast%20model%2C%20along%20with%20multiple%20aggregation%20functions%2C%20such%20as%20summing%0Aand%20taking%20the%20%24%5Cell_p%24%20norm%20of%20an%20expert%27s%20cost%20across%20servers.%20We%20propose%20the%0Afirst%20communication-efficient%20protocols%20that%20achieve%20near-optimal%20regret%20in%0Athese%20settings%2C%20even%20against%20a%20strong%20adversary%20who%20can%20choose%20the%20inputs%0Aadaptively.%20Additionally%2C%20we%20give%20a%20conditional%20lower%20bound%20showing%20that%20the%0Acommunication%20of%20our%20protocols%20is%20nearly%20optimal.%20Finally%2C%20we%20implement%20our%0Aprotocols%20and%20demonstrate%20empirical%20savings%20on%20the%20HPO-B%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03132v1&entry.124074799=Read"},
{"title": "Parametric Matrix Models", "author": "Patrick Cook and Danny Jammooa and Morten Hjorth-Jensen and Daniel D. Lee and Dean Lee", "abstract": "  We present a general class of machine learning algorithms called parametric\nmatrix models. In contrast with most existing machine learning models that\nimitate the biology of neurons, parametric matrix models use matrix equations\nthat emulate physical systems. Similar to how physics problems are usually\nsolved, parametric matrix models learn the governing equations that lead to the\ndesired outputs. Parametric matrix models can be efficiently trained from\nempirical data, and the equations may use algebraic, differential, or integral\nrelations. While originally designed for scientific computing, we prove that\nparametric matrix models are universal function approximators that can be\napplied to general machine learning problems. After introducing the underlying\ntheory, we apply parametric matrix models to a series of different challenges\nthat show their performance for a wide range of problems. For all the\nchallenges tested here, parametric matrix models produce accurate results\nwithin an efficient and interpretable computational framework that allows for\ninput feature extrapolation.\n", "link": "http://arxiv.org/abs/2401.11694v6", "date": "2025-01-06", "relevancy": 1.3461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4389}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parametric%20Matrix%20Models&body=Title%3A%20Parametric%20Matrix%20Models%0AAuthor%3A%20Patrick%20Cook%20and%20Danny%20Jammooa%20and%20Morten%20Hjorth-Jensen%20and%20Daniel%20D.%20Lee%20and%20Dean%20Lee%0AAbstract%3A%20%20%20We%20present%20a%20general%20class%20of%20machine%20learning%20algorithms%20called%20parametric%0Amatrix%20models.%20In%20contrast%20with%20most%20existing%20machine%20learning%20models%20that%0Aimitate%20the%20biology%20of%20neurons%2C%20parametric%20matrix%20models%20use%20matrix%20equations%0Athat%20emulate%20physical%20systems.%20Similar%20to%20how%20physics%20problems%20are%20usually%0Asolved%2C%20parametric%20matrix%20models%20learn%20the%20governing%20equations%20that%20lead%20to%20the%0Adesired%20outputs.%20Parametric%20matrix%20models%20can%20be%20efficiently%20trained%20from%0Aempirical%20data%2C%20and%20the%20equations%20may%20use%20algebraic%2C%20differential%2C%20or%20integral%0Arelations.%20While%20originally%20designed%20for%20scientific%20computing%2C%20we%20prove%20that%0Aparametric%20matrix%20models%20are%20universal%20function%20approximators%20that%20can%20be%0Aapplied%20to%20general%20machine%20learning%20problems.%20After%20introducing%20the%20underlying%0Atheory%2C%20we%20apply%20parametric%20matrix%20models%20to%20a%20series%20of%20different%20challenges%0Athat%20show%20their%20performance%20for%20a%20wide%20range%20of%20problems.%20For%20all%20the%0Achallenges%20tested%20here%2C%20parametric%20matrix%20models%20produce%20accurate%20results%0Awithin%20an%20efficient%20and%20interpretable%20computational%20framework%20that%20allows%20for%0Ainput%20feature%20extrapolation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11694v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParametric%2520Matrix%2520Models%26entry.906535625%3DPatrick%2520Cook%2520and%2520Danny%2520Jammooa%2520and%2520Morten%2520Hjorth-Jensen%2520and%2520Daniel%2520D.%2520Lee%2520and%2520Dean%2520Lee%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520general%2520class%2520of%2520machine%2520learning%2520algorithms%2520called%2520parametric%250Amatrix%2520models.%2520In%2520contrast%2520with%2520most%2520existing%2520machine%2520learning%2520models%2520that%250Aimitate%2520the%2520biology%2520of%2520neurons%252C%2520parametric%2520matrix%2520models%2520use%2520matrix%2520equations%250Athat%2520emulate%2520physical%2520systems.%2520Similar%2520to%2520how%2520physics%2520problems%2520are%2520usually%250Asolved%252C%2520parametric%2520matrix%2520models%2520learn%2520the%2520governing%2520equations%2520that%2520lead%2520to%2520the%250Adesired%2520outputs.%2520Parametric%2520matrix%2520models%2520can%2520be%2520efficiently%2520trained%2520from%250Aempirical%2520data%252C%2520and%2520the%2520equations%2520may%2520use%2520algebraic%252C%2520differential%252C%2520or%2520integral%250Arelations.%2520While%2520originally%2520designed%2520for%2520scientific%2520computing%252C%2520we%2520prove%2520that%250Aparametric%2520matrix%2520models%2520are%2520universal%2520function%2520approximators%2520that%2520can%2520be%250Aapplied%2520to%2520general%2520machine%2520learning%2520problems.%2520After%2520introducing%2520the%2520underlying%250Atheory%252C%2520we%2520apply%2520parametric%2520matrix%2520models%2520to%2520a%2520series%2520of%2520different%2520challenges%250Athat%2520show%2520their%2520performance%2520for%2520a%2520wide%2520range%2520of%2520problems.%2520For%2520all%2520the%250Achallenges%2520tested%2520here%252C%2520parametric%2520matrix%2520models%2520produce%2520accurate%2520results%250Awithin%2520an%2520efficient%2520and%2520interpretable%2520computational%2520framework%2520that%2520allows%2520for%250Ainput%2520feature%2520extrapolation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11694v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric%20Matrix%20Models&entry.906535625=Patrick%20Cook%20and%20Danny%20Jammooa%20and%20Morten%20Hjorth-Jensen%20and%20Daniel%20D.%20Lee%20and%20Dean%20Lee&entry.1292438233=%20%20We%20present%20a%20general%20class%20of%20machine%20learning%20algorithms%20called%20parametric%0Amatrix%20models.%20In%20contrast%20with%20most%20existing%20machine%20learning%20models%20that%0Aimitate%20the%20biology%20of%20neurons%2C%20parametric%20matrix%20models%20use%20matrix%20equations%0Athat%20emulate%20physical%20systems.%20Similar%20to%20how%20physics%20problems%20are%20usually%0Asolved%2C%20parametric%20matrix%20models%20learn%20the%20governing%20equations%20that%20lead%20to%20the%0Adesired%20outputs.%20Parametric%20matrix%20models%20can%20be%20efficiently%20trained%20from%0Aempirical%20data%2C%20and%20the%20equations%20may%20use%20algebraic%2C%20differential%2C%20or%20integral%0Arelations.%20While%20originally%20designed%20for%20scientific%20computing%2C%20we%20prove%20that%0Aparametric%20matrix%20models%20are%20universal%20function%20approximators%20that%20can%20be%0Aapplied%20to%20general%20machine%20learning%20problems.%20After%20introducing%20the%20underlying%0Atheory%2C%20we%20apply%20parametric%20matrix%20models%20to%20a%20series%20of%20different%20challenges%0Athat%20show%20their%20performance%20for%20a%20wide%20range%20of%20problems.%20For%20all%20the%0Achallenges%20tested%20here%2C%20parametric%20matrix%20models%20produce%20accurate%20results%0Awithin%20an%20efficient%20and%20interpretable%20computational%20framework%20that%20allows%20for%0Ainput%20feature%20extrapolation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11694v6&entry.124074799=Read"},
{"title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence", "author": "Huiwen Liu and Feida Zhu and Ling Cheng", "abstract": "  Existing research on federated learning has been focused on the setting where\nlearning is coordinated by a centralized entity. Yet the greatest potential of\nfuture collaborative intelligence would be unleashed in a more open and\ndemocratized setting with no central entity in a dominant role, referred to as\n\"decentralized federated learning\". New challenges arise accordingly in\nachieving both correct model training and fair reward allocation with\ncollective effort among all participating nodes, especially with the threat of\nthe Byzantine node jeopardising both tasks.\n  In this paper, we propose a blockchain-based decentralized Byzantine\nfault-tolerant federated learning framework based on a novel Proof-of-Data\n(PoD) consensus protocol to resolve both the \"trust\" and \"incentive\"\ncomponents. By decoupling model training and contribution accounting, PoD is\nable to enjoy not only the benefit of learning efficiency and system liveliness\nfrom asynchronous societal-scale PoW-style learning but also the finality of\nconsensus and reward allocation from epoch-based BFT-style voting. To mitigate\nfalse reward claims by data forgery from Byzantine attacks, a privacy-aware\ndata verification and contribution-based reward allocation mechanism is\ndesigned to complete the framework. Our evaluation results show that PoD\ndemonstrates performance in model training close to that of the centralized\ncounterpart while achieving trust in consensus and fairness for reward\nallocation with a fault tolerance ratio of 1/3.\n", "link": "http://arxiv.org/abs/2501.02971v1", "date": "2025-01-06", "relevancy": 1.8269, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4601}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4552}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proof-of-Data%3A%20A%20Consensus%20Protocol%20for%20Collaborative%20Intelligence&body=Title%3A%20Proof-of-Data%3A%20A%20Consensus%20Protocol%20for%20Collaborative%20Intelligence%0AAuthor%3A%20Huiwen%20Liu%20and%20Feida%20Zhu%20and%20Ling%20Cheng%0AAbstract%3A%20%20%20Existing%20research%20on%20federated%20learning%20has%20been%20focused%20on%20the%20setting%20where%0Alearning%20is%20coordinated%20by%20a%20centralized%20entity.%20Yet%20the%20greatest%20potential%20of%0Afuture%20collaborative%20intelligence%20would%20be%20unleashed%20in%20a%20more%20open%20and%0Ademocratized%20setting%20with%20no%20central%20entity%20in%20a%20dominant%20role%2C%20referred%20to%20as%0A%22decentralized%20federated%20learning%22.%20New%20challenges%20arise%20accordingly%20in%0Aachieving%20both%20correct%20model%20training%20and%20fair%20reward%20allocation%20with%0Acollective%20effort%20among%20all%20participating%20nodes%2C%20especially%20with%20the%20threat%20of%0Athe%20Byzantine%20node%20jeopardising%20both%20tasks.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%20decentralized%20Byzantine%0Afault-tolerant%20federated%20learning%20framework%20based%20on%20a%20novel%20Proof-of-Data%0A%28PoD%29%20consensus%20protocol%20to%20resolve%20both%20the%20%22trust%22%20and%20%22incentive%22%0Acomponents.%20By%20decoupling%20model%20training%20and%20contribution%20accounting%2C%20PoD%20is%0Aable%20to%20enjoy%20not%20only%20the%20benefit%20of%20learning%20efficiency%20and%20system%20liveliness%0Afrom%20asynchronous%20societal-scale%20PoW-style%20learning%20but%20also%20the%20finality%20of%0Aconsensus%20and%20reward%20allocation%20from%20epoch-based%20BFT-style%20voting.%20To%20mitigate%0Afalse%20reward%20claims%20by%20data%20forgery%20from%20Byzantine%20attacks%2C%20a%20privacy-aware%0Adata%20verification%20and%20contribution-based%20reward%20allocation%20mechanism%20is%0Adesigned%20to%20complete%20the%20framework.%20Our%20evaluation%20results%20show%20that%20PoD%0Ademonstrates%20performance%20in%20model%20training%20close%20to%20that%20of%20the%20centralized%0Acounterpart%20while%20achieving%20trust%20in%20consensus%20and%20fairness%20for%20reward%0Aallocation%20with%20a%20fault%20tolerance%20ratio%20of%201/3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProof-of-Data%253A%2520A%2520Consensus%2520Protocol%2520for%2520Collaborative%2520Intelligence%26entry.906535625%3DHuiwen%2520Liu%2520and%2520Feida%2520Zhu%2520and%2520Ling%2520Cheng%26entry.1292438233%3D%2520%2520Existing%2520research%2520on%2520federated%2520learning%2520has%2520been%2520focused%2520on%2520the%2520setting%2520where%250Alearning%2520is%2520coordinated%2520by%2520a%2520centralized%2520entity.%2520Yet%2520the%2520greatest%2520potential%2520of%250Afuture%2520collaborative%2520intelligence%2520would%2520be%2520unleashed%2520in%2520a%2520more%2520open%2520and%250Ademocratized%2520setting%2520with%2520no%2520central%2520entity%2520in%2520a%2520dominant%2520role%252C%2520referred%2520to%2520as%250A%2522decentralized%2520federated%2520learning%2522.%2520New%2520challenges%2520arise%2520accordingly%2520in%250Aachieving%2520both%2520correct%2520model%2520training%2520and%2520fair%2520reward%2520allocation%2520with%250Acollective%2520effort%2520among%2520all%2520participating%2520nodes%252C%2520especially%2520with%2520the%2520threat%2520of%250Athe%2520Byzantine%2520node%2520jeopardising%2520both%2520tasks.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520blockchain-based%2520decentralized%2520Byzantine%250Afault-tolerant%2520federated%2520learning%2520framework%2520based%2520on%2520a%2520novel%2520Proof-of-Data%250A%2528PoD%2529%2520consensus%2520protocol%2520to%2520resolve%2520both%2520the%2520%2522trust%2522%2520and%2520%2522incentive%2522%250Acomponents.%2520By%2520decoupling%2520model%2520training%2520and%2520contribution%2520accounting%252C%2520PoD%2520is%250Aable%2520to%2520enjoy%2520not%2520only%2520the%2520benefit%2520of%2520learning%2520efficiency%2520and%2520system%2520liveliness%250Afrom%2520asynchronous%2520societal-scale%2520PoW-style%2520learning%2520but%2520also%2520the%2520finality%2520of%250Aconsensus%2520and%2520reward%2520allocation%2520from%2520epoch-based%2520BFT-style%2520voting.%2520To%2520mitigate%250Afalse%2520reward%2520claims%2520by%2520data%2520forgery%2520from%2520Byzantine%2520attacks%252C%2520a%2520privacy-aware%250Adata%2520verification%2520and%2520contribution-based%2520reward%2520allocation%2520mechanism%2520is%250Adesigned%2520to%2520complete%2520the%2520framework.%2520Our%2520evaluation%2520results%2520show%2520that%2520PoD%250Ademonstrates%2520performance%2520in%2520model%2520training%2520close%2520to%2520that%2520of%2520the%2520centralized%250Acounterpart%2520while%2520achieving%2520trust%2520in%2520consensus%2520and%2520fairness%2520for%2520reward%250Aallocation%2520with%2520a%2520fault%2520tolerance%2520ratio%2520of%25201/3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proof-of-Data%3A%20A%20Consensus%20Protocol%20for%20Collaborative%20Intelligence&entry.906535625=Huiwen%20Liu%20and%20Feida%20Zhu%20and%20Ling%20Cheng&entry.1292438233=%20%20Existing%20research%20on%20federated%20learning%20has%20been%20focused%20on%20the%20setting%20where%0Alearning%20is%20coordinated%20by%20a%20centralized%20entity.%20Yet%20the%20greatest%20potential%20of%0Afuture%20collaborative%20intelligence%20would%20be%20unleashed%20in%20a%20more%20open%20and%0Ademocratized%20setting%20with%20no%20central%20entity%20in%20a%20dominant%20role%2C%20referred%20to%20as%0A%22decentralized%20federated%20learning%22.%20New%20challenges%20arise%20accordingly%20in%0Aachieving%20both%20correct%20model%20training%20and%20fair%20reward%20allocation%20with%0Acollective%20effort%20among%20all%20participating%20nodes%2C%20especially%20with%20the%20threat%20of%0Athe%20Byzantine%20node%20jeopardising%20both%20tasks.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%20decentralized%20Byzantine%0Afault-tolerant%20federated%20learning%20framework%20based%20on%20a%20novel%20Proof-of-Data%0A%28PoD%29%20consensus%20protocol%20to%20resolve%20both%20the%20%22trust%22%20and%20%22incentive%22%0Acomponents.%20By%20decoupling%20model%20training%20and%20contribution%20accounting%2C%20PoD%20is%0Aable%20to%20enjoy%20not%20only%20the%20benefit%20of%20learning%20efficiency%20and%20system%20liveliness%0Afrom%20asynchronous%20societal-scale%20PoW-style%20learning%20but%20also%20the%20finality%20of%0Aconsensus%20and%20reward%20allocation%20from%20epoch-based%20BFT-style%20voting.%20To%20mitigate%0Afalse%20reward%20claims%20by%20data%20forgery%20from%20Byzantine%20attacks%2C%20a%20privacy-aware%0Adata%20verification%20and%20contribution-based%20reward%20allocation%20mechanism%20is%0Adesigned%20to%20complete%20the%20framework.%20Our%20evaluation%20results%20show%20that%20PoD%0Ademonstrates%20performance%20in%20model%20training%20close%20to%20that%20of%20the%20centralized%0Acounterpart%20while%20achieving%20trust%20in%20consensus%20and%20fairness%20for%20reward%0Aallocation%20with%20a%20fault%20tolerance%20ratio%20of%201/3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02971v1&entry.124074799=Read"},
{"title": "An Open-source Sim2Real Approach for Sensor-independent Robot Navigation\n  in a Grid", "author": "Murad Mehrab Abrar and Souryadeep Mondal and Michelle Hickner", "abstract": "  This paper presents a Sim2Real (Simulation to Reality) approach to bridge the\ngap between a trained agent in a simulated environment and its real-world\nimplementation in navigating a robot in a similar setting. Specifically, we\nfocus on navigating a quadruped robot in a real-world grid-like environment\ninspired by the Gymnasium Frozen Lake -- a highly user-friendly and free\nApplication Programming Interface (API) to develop and test Reinforcement\nLearning (RL) algorithms. We detail the development of a pipeline to transfer\nmotion policies learned in the Frozen Lake simulation to a physical quadruped\nrobot, thus enabling autonomous navigation and obstacle avoidance in a grid\nwithout relying on expensive localization and mapping sensors. The work\ninvolves training an RL agent in the Frozen Lake environment and utilizing the\nresulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In\naddition to detailing the RL implementation, inverse kinematics-based quadruped\ngaits, and the transfer policy pipeline, we open-source the project on GitHub\nand include a demonstration video of our Sim2Real transfer approach. This work\nprovides an accessible, straightforward, and low-cost framework for\nresearchers, students, and hobbyists to explore and implement RL-based robot\nnavigation in real-world grid environments.\n", "link": "http://arxiv.org/abs/2411.03494v2", "date": "2025-01-06", "relevancy": 1.7469, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5808}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Open-source%20Sim2Real%20Approach%20for%20Sensor-independent%20Robot%20Navigation%0A%20%20in%20a%20Grid&body=Title%3A%20An%20Open-source%20Sim2Real%20Approach%20for%20Sensor-independent%20Robot%20Navigation%0A%20%20in%20a%20Grid%0AAuthor%3A%20Murad%20Mehrab%20Abrar%20and%20Souryadeep%20Mondal%20and%20Michelle%20Hickner%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Sim2Real%20%28Simulation%20to%20Reality%29%20approach%20to%20bridge%20the%0Agap%20between%20a%20trained%20agent%20in%20a%20simulated%20environment%20and%20its%20real-world%0Aimplementation%20in%20navigating%20a%20robot%20in%20a%20similar%20setting.%20Specifically%2C%20we%0Afocus%20on%20navigating%20a%20quadruped%20robot%20in%20a%20real-world%20grid-like%20environment%0Ainspired%20by%20the%20Gymnasium%20Frozen%20Lake%20--%20a%20highly%20user-friendly%20and%20free%0AApplication%20Programming%20Interface%20%28API%29%20to%20develop%20and%20test%20Reinforcement%0ALearning%20%28RL%29%20algorithms.%20We%20detail%20the%20development%20of%20a%20pipeline%20to%20transfer%0Amotion%20policies%20learned%20in%20the%20Frozen%20Lake%20simulation%20to%20a%20physical%20quadruped%0Arobot%2C%20thus%20enabling%20autonomous%20navigation%20and%20obstacle%20avoidance%20in%20a%20grid%0Awithout%20relying%20on%20expensive%20localization%20and%20mapping%20sensors.%20The%20work%0Ainvolves%20training%20an%20RL%20agent%20in%20the%20Frozen%20Lake%20environment%20and%20utilizing%20the%0Aresulting%20Q-table%20to%20control%20a%2012%20Degrees-of-Freedom%20%28DOF%29%20quadruped%20robot.%20In%0Aaddition%20to%20detailing%20the%20RL%20implementation%2C%20inverse%20kinematics-based%20quadruped%0Agaits%2C%20and%20the%20transfer%20policy%20pipeline%2C%20we%20open-source%20the%20project%20on%20GitHub%0Aand%20include%20a%20demonstration%20video%20of%20our%20Sim2Real%20transfer%20approach.%20This%20work%0Aprovides%20an%20accessible%2C%20straightforward%2C%20and%20low-cost%20framework%20for%0Aresearchers%2C%20students%2C%20and%20hobbyists%20to%20explore%20and%20implement%20RL-based%20robot%0Anavigation%20in%20real-world%20grid%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Open-source%2520Sim2Real%2520Approach%2520for%2520Sensor-independent%2520Robot%2520Navigation%250A%2520%2520in%2520a%2520Grid%26entry.906535625%3DMurad%2520Mehrab%2520Abrar%2520and%2520Souryadeep%2520Mondal%2520and%2520Michelle%2520Hickner%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Sim2Real%2520%2528Simulation%2520to%2520Reality%2529%2520approach%2520to%2520bridge%2520the%250Agap%2520between%2520a%2520trained%2520agent%2520in%2520a%2520simulated%2520environment%2520and%2520its%2520real-world%250Aimplementation%2520in%2520navigating%2520a%2520robot%2520in%2520a%2520similar%2520setting.%2520Specifically%252C%2520we%250Afocus%2520on%2520navigating%2520a%2520quadruped%2520robot%2520in%2520a%2520real-world%2520grid-like%2520environment%250Ainspired%2520by%2520the%2520Gymnasium%2520Frozen%2520Lake%2520--%2520a%2520highly%2520user-friendly%2520and%2520free%250AApplication%2520Programming%2520Interface%2520%2528API%2529%2520to%2520develop%2520and%2520test%2520Reinforcement%250ALearning%2520%2528RL%2529%2520algorithms.%2520We%2520detail%2520the%2520development%2520of%2520a%2520pipeline%2520to%2520transfer%250Amotion%2520policies%2520learned%2520in%2520the%2520Frozen%2520Lake%2520simulation%2520to%2520a%2520physical%2520quadruped%250Arobot%252C%2520thus%2520enabling%2520autonomous%2520navigation%2520and%2520obstacle%2520avoidance%2520in%2520a%2520grid%250Awithout%2520relying%2520on%2520expensive%2520localization%2520and%2520mapping%2520sensors.%2520The%2520work%250Ainvolves%2520training%2520an%2520RL%2520agent%2520in%2520the%2520Frozen%2520Lake%2520environment%2520and%2520utilizing%2520the%250Aresulting%2520Q-table%2520to%2520control%2520a%252012%2520Degrees-of-Freedom%2520%2528DOF%2529%2520quadruped%2520robot.%2520In%250Aaddition%2520to%2520detailing%2520the%2520RL%2520implementation%252C%2520inverse%2520kinematics-based%2520quadruped%250Agaits%252C%2520and%2520the%2520transfer%2520policy%2520pipeline%252C%2520we%2520open-source%2520the%2520project%2520on%2520GitHub%250Aand%2520include%2520a%2520demonstration%2520video%2520of%2520our%2520Sim2Real%2520transfer%2520approach.%2520This%2520work%250Aprovides%2520an%2520accessible%252C%2520straightforward%252C%2520and%2520low-cost%2520framework%2520for%250Aresearchers%252C%2520students%252C%2520and%2520hobbyists%2520to%2520explore%2520and%2520implement%2520RL-based%2520robot%250Anavigation%2520in%2520real-world%2520grid%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Open-source%20Sim2Real%20Approach%20for%20Sensor-independent%20Robot%20Navigation%0A%20%20in%20a%20Grid&entry.906535625=Murad%20Mehrab%20Abrar%20and%20Souryadeep%20Mondal%20and%20Michelle%20Hickner&entry.1292438233=%20%20This%20paper%20presents%20a%20Sim2Real%20%28Simulation%20to%20Reality%29%20approach%20to%20bridge%20the%0Agap%20between%20a%20trained%20agent%20in%20a%20simulated%20environment%20and%20its%20real-world%0Aimplementation%20in%20navigating%20a%20robot%20in%20a%20similar%20setting.%20Specifically%2C%20we%0Afocus%20on%20navigating%20a%20quadruped%20robot%20in%20a%20real-world%20grid-like%20environment%0Ainspired%20by%20the%20Gymnasium%20Frozen%20Lake%20--%20a%20highly%20user-friendly%20and%20free%0AApplication%20Programming%20Interface%20%28API%29%20to%20develop%20and%20test%20Reinforcement%0ALearning%20%28RL%29%20algorithms.%20We%20detail%20the%20development%20of%20a%20pipeline%20to%20transfer%0Amotion%20policies%20learned%20in%20the%20Frozen%20Lake%20simulation%20to%20a%20physical%20quadruped%0Arobot%2C%20thus%20enabling%20autonomous%20navigation%20and%20obstacle%20avoidance%20in%20a%20grid%0Awithout%20relying%20on%20expensive%20localization%20and%20mapping%20sensors.%20The%20work%0Ainvolves%20training%20an%20RL%20agent%20in%20the%20Frozen%20Lake%20environment%20and%20utilizing%20the%0Aresulting%20Q-table%20to%20control%20a%2012%20Degrees-of-Freedom%20%28DOF%29%20quadruped%20robot.%20In%0Aaddition%20to%20detailing%20the%20RL%20implementation%2C%20inverse%20kinematics-based%20quadruped%0Agaits%2C%20and%20the%20transfer%20policy%20pipeline%2C%20we%20open-source%20the%20project%20on%20GitHub%0Aand%20include%20a%20demonstration%20video%20of%20our%20Sim2Real%20transfer%20approach.%20This%20work%0Aprovides%20an%20accessible%2C%20straightforward%2C%20and%20low-cost%20framework%20for%0Aresearchers%2C%20students%2C%20and%20hobbyists%20to%20explore%20and%20implement%20RL-based%20robot%0Anavigation%20in%20real-world%20grid%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03494v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


