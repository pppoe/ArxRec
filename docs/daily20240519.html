<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240516.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "author": "Ruiqi Gao and Aleksander Holynski and Philipp Henzler and Arthur Brussee and Ricardo Martin-Brualla and Pratul Srinivasan and Jonathan T. Barron and Ben Poole", "abstract": "  Advances in 3D reconstruction have enabled high-quality 3D capture, but\nrequire a user to collect hundreds to thousands of images to create a 3D scene.\nWe present CAT3D, a method for creating anything in 3D by simulating this\nreal-world capture process with a multi-view diffusion model. Given any number\nof input images and a set of target novel viewpoints, our model generates\nhighly consistent novel views of a scene. These generated views can be used as\ninput to robust 3D reconstruction techniques to produce 3D representations that\ncan be rendered from any viewpoint in real-time. CAT3D can create entire 3D\nscenes in as little as one minute, and outperforms existing methods for single\nimage and few-view 3D scene creation. See our project page for results and\ninteractive demos at https://cat3d.github.io .\n", "link": "http://arxiv.org/abs/2405.10314v1", "date": "2024-05-16", "relevancy": 2.8644, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5773}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT3D%3A%20Create%20Anything%20in%203D%20with%20Multi-View%20Diffusion%20Models&body=Title%3A%20CAT3D%3A%20Create%20Anything%20in%203D%20with%20Multi-View%20Diffusion%20Models%0AAuthor%3A%20Ruiqi%20Gao%20and%20Aleksander%20Holynski%20and%20Philipp%20Henzler%20and%20Arthur%20Brussee%20and%20Ricardo%20Martin-Brualla%20and%20Pratul%20Srinivasan%20and%20Jonathan%20T.%20Barron%20and%20Ben%20Poole%0AAbstract%3A%20%20%20Advances%20in%203D%20reconstruction%20have%20enabled%20high-quality%203D%20capture%2C%20but%0Arequire%20a%20user%20to%20collect%20hundreds%20to%20thousands%20of%20images%20to%20create%20a%203D%20scene.%0AWe%20present%20CAT3D%2C%20a%20method%20for%20creating%20anything%20in%203D%20by%20simulating%20this%0Areal-world%20capture%20process%20with%20a%20multi-view%20diffusion%20model.%20Given%20any%20number%0Aof%20input%20images%20and%20a%20set%20of%20target%20novel%20viewpoints%2C%20our%20model%20generates%0Ahighly%20consistent%20novel%20views%20of%20a%20scene.%20These%20generated%20views%20can%20be%20used%20as%0Ainput%20to%20robust%203D%20reconstruction%20techniques%20to%20produce%203D%20representations%20that%0Acan%20be%20rendered%20from%20any%20viewpoint%20in%20real-time.%20CAT3D%20can%20create%20entire%203D%0Ascenes%20in%20as%20little%20as%20one%20minute%2C%20and%20outperforms%20existing%20methods%20for%20single%0Aimage%20and%20few-view%203D%20scene%20creation.%20See%20our%20project%20page%20for%20results%20and%0Ainteractive%20demos%20at%20https%3A//cat3d.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT3D%253A%2520Create%2520Anything%2520in%25203D%2520with%2520Multi-View%2520Diffusion%2520Models%26entry.906535625%3DRuiqi%2520Gao%2520and%2520Aleksander%2520Holynski%2520and%2520Philipp%2520Henzler%2520and%2520Arthur%2520Brussee%2520and%2520Ricardo%2520Martin-Brualla%2520and%2520Pratul%2520Srinivasan%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Ben%2520Poole%26entry.1292438233%3D%2520%2520Advances%2520in%25203D%2520reconstruction%2520have%2520enabled%2520high-quality%25203D%2520capture%252C%2520but%250Arequire%2520a%2520user%2520to%2520collect%2520hundreds%2520to%2520thousands%2520of%2520images%2520to%2520create%2520a%25203D%2520scene.%250AWe%2520present%2520CAT3D%252C%2520a%2520method%2520for%2520creating%2520anything%2520in%25203D%2520by%2520simulating%2520this%250Areal-world%2520capture%2520process%2520with%2520a%2520multi-view%2520diffusion%2520model.%2520Given%2520any%2520number%250Aof%2520input%2520images%2520and%2520a%2520set%2520of%2520target%2520novel%2520viewpoints%252C%2520our%2520model%2520generates%250Ahighly%2520consistent%2520novel%2520views%2520of%2520a%2520scene.%2520These%2520generated%2520views%2520can%2520be%2520used%2520as%250Ainput%2520to%2520robust%25203D%2520reconstruction%2520techniques%2520to%2520produce%25203D%2520representations%2520that%250Acan%2520be%2520rendered%2520from%2520any%2520viewpoint%2520in%2520real-time.%2520CAT3D%2520can%2520create%2520entire%25203D%250Ascenes%2520in%2520as%2520little%2520as%2520one%2520minute%252C%2520and%2520outperforms%2520existing%2520methods%2520for%2520single%250Aimage%2520and%2520few-view%25203D%2520scene%2520creation.%2520See%2520our%2520project%2520page%2520for%2520results%2520and%250Ainteractive%2520demos%2520at%2520https%253A//cat3d.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT3D%3A%20Create%20Anything%20in%203D%20with%20Multi-View%20Diffusion%20Models&entry.906535625=Ruiqi%20Gao%20and%20Aleksander%20Holynski%20and%20Philipp%20Henzler%20and%20Arthur%20Brussee%20and%20Ricardo%20Martin-Brualla%20and%20Pratul%20Srinivasan%20and%20Jonathan%20T.%20Barron%20and%20Ben%20Poole&entry.1292438233=%20%20Advances%20in%203D%20reconstruction%20have%20enabled%20high-quality%203D%20capture%2C%20but%0Arequire%20a%20user%20to%20collect%20hundreds%20to%20thousands%20of%20images%20to%20create%20a%203D%20scene.%0AWe%20present%20CAT3D%2C%20a%20method%20for%20creating%20anything%20in%203D%20by%20simulating%20this%0Areal-world%20capture%20process%20with%20a%20multi-view%20diffusion%20model.%20Given%20any%20number%0Aof%20input%20images%20and%20a%20set%20of%20target%20novel%20viewpoints%2C%20our%20model%20generates%0Ahighly%20consistent%20novel%20views%20of%20a%20scene.%20These%20generated%20views%20can%20be%20used%20as%0Ainput%20to%20robust%203D%20reconstruction%20techniques%20to%20produce%203D%20representations%20that%0Acan%20be%20rendered%20from%20any%20viewpoint%20in%20real-time.%20CAT3D%20can%20create%20entire%203D%0Ascenes%20in%20as%20little%20as%20one%20minute%2C%20and%20outperforms%20existing%20methods%20for%20single%0Aimage%20and%20few-view%203D%20scene%20creation.%20See%20our%20project%20page%20for%20results%20and%0Ainteractive%20demos%20at%20https%3A//cat3d.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10314v1&entry.124074799=Read"},
{"title": "Mesh Neural Cellular Automata", "author": "Ehsan Pajouheshgar and Yitao Xu and Alexander Mordvintsev and Eyvind Niklasson and Tong Zhang and Sabine S\u00fcsstrunk", "abstract": "  Texture modeling and synthesis are essential for enhancing the realism of\nvirtual environments. Methods that directly synthesize textures in 3D offer\ndistinct advantages to the UV-mapping-based methods as they can create seamless\ntextures and align more closely with the ways textures form in nature. We\npropose Mesh Neural Cellular Automata (MeshNCA), a method that directly\nsynthesizes dynamic textures on 3D meshes without requiring any UV maps.\nMeshNCA is a generalized type of cellular automata that can operate on a set of\ncells arranged on non-grid structures such as the vertices of a 3D mesh.\nMeshNCA accommodates multi-modal supervision and can be trained using different\ntargets such as images, text prompts, and motion vector fields. Only trained on\nan Icosphere mesh, MeshNCA shows remarkable test-time generalization and can\nsynthesize textures on unseen meshes in real time. We conduct qualitative and\nquantitative comparisons to demonstrate that MeshNCA outperforms other 3D\ntexture synthesis methods in terms of generalization and producing high-quality\ntextures. Moreover, we introduce a way of grafting trained MeshNCA instances,\nenabling interpolation between textures. MeshNCA allows several user\ninteractions including texture density/orientation controls,\ngrafting/regenerate brushes, and motion speed/direction controls. Finally, we\nimplement the forward pass of our MeshNCA model using the WebGL shading\nlanguage and showcase our trained models in an online interactive demo, which\nis accessible on personal computers and smartphones and is available at\nhttps://meshnca.github.io.\n", "link": "http://arxiv.org/abs/2311.02820v2", "date": "2024-05-16", "relevancy": 2.744, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6134}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5173}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh%20Neural%20Cellular%20Automata&body=Title%3A%20Mesh%20Neural%20Cellular%20Automata%0AAuthor%3A%20Ehsan%20Pajouheshgar%20and%20Yitao%20Xu%20and%20Alexander%20Mordvintsev%20and%20Eyvind%20Niklasson%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Texture%20modeling%20and%20synthesis%20are%20essential%20for%20enhancing%20the%20realism%20of%0Avirtual%20environments.%20Methods%20that%20directly%20synthesize%20textures%20in%203D%20offer%0Adistinct%20advantages%20to%20the%20UV-mapping-based%20methods%20as%20they%20can%20create%20seamless%0Atextures%20and%20align%20more%20closely%20with%20the%20ways%20textures%20form%20in%20nature.%20We%0Apropose%20Mesh%20Neural%20Cellular%20Automata%20%28MeshNCA%29%2C%20a%20method%20that%20directly%0Asynthesizes%20dynamic%20textures%20on%203D%20meshes%20without%20requiring%20any%20UV%20maps.%0AMeshNCA%20is%20a%20generalized%20type%20of%20cellular%20automata%20that%20can%20operate%20on%20a%20set%20of%0Acells%20arranged%20on%20non-grid%20structures%20such%20as%20the%20vertices%20of%20a%203D%20mesh.%0AMeshNCA%20accommodates%20multi-modal%20supervision%20and%20can%20be%20trained%20using%20different%0Atargets%20such%20as%20images%2C%20text%20prompts%2C%20and%20motion%20vector%20fields.%20Only%20trained%20on%0Aan%20Icosphere%20mesh%2C%20MeshNCA%20shows%20remarkable%20test-time%20generalization%20and%20can%0Asynthesize%20textures%20on%20unseen%20meshes%20in%20real%20time.%20We%20conduct%20qualitative%20and%0Aquantitative%20comparisons%20to%20demonstrate%20that%20MeshNCA%20outperforms%20other%203D%0Atexture%20synthesis%20methods%20in%20terms%20of%20generalization%20and%20producing%20high-quality%0Atextures.%20Moreover%2C%20we%20introduce%20a%20way%20of%20grafting%20trained%20MeshNCA%20instances%2C%0Aenabling%20interpolation%20between%20textures.%20MeshNCA%20allows%20several%20user%0Ainteractions%20including%20texture%20density/orientation%20controls%2C%0Agrafting/regenerate%20brushes%2C%20and%20motion%20speed/direction%20controls.%20Finally%2C%20we%0Aimplement%20the%20forward%20pass%20of%20our%20MeshNCA%20model%20using%20the%20WebGL%20shading%0Alanguage%20and%20showcase%20our%20trained%20models%20in%20an%20online%20interactive%20demo%2C%20which%0Ais%20accessible%20on%20personal%20computers%20and%20smartphones%20and%20is%20available%20at%0Ahttps%3A//meshnca.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh%2520Neural%2520Cellular%2520Automata%26entry.906535625%3DEhsan%2520Pajouheshgar%2520and%2520Yitao%2520Xu%2520and%2520Alexander%2520Mordvintsev%2520and%2520Eyvind%2520Niklasson%2520and%2520Tong%2520Zhang%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Texture%2520modeling%2520and%2520synthesis%2520are%2520essential%2520for%2520enhancing%2520the%2520realism%2520of%250Avirtual%2520environments.%2520Methods%2520that%2520directly%2520synthesize%2520textures%2520in%25203D%2520offer%250Adistinct%2520advantages%2520to%2520the%2520UV-mapping-based%2520methods%2520as%2520they%2520can%2520create%2520seamless%250Atextures%2520and%2520align%2520more%2520closely%2520with%2520the%2520ways%2520textures%2520form%2520in%2520nature.%2520We%250Apropose%2520Mesh%2520Neural%2520Cellular%2520Automata%2520%2528MeshNCA%2529%252C%2520a%2520method%2520that%2520directly%250Asynthesizes%2520dynamic%2520textures%2520on%25203D%2520meshes%2520without%2520requiring%2520any%2520UV%2520maps.%250AMeshNCA%2520is%2520a%2520generalized%2520type%2520of%2520cellular%2520automata%2520that%2520can%2520operate%2520on%2520a%2520set%2520of%250Acells%2520arranged%2520on%2520non-grid%2520structures%2520such%2520as%2520the%2520vertices%2520of%2520a%25203D%2520mesh.%250AMeshNCA%2520accommodates%2520multi-modal%2520supervision%2520and%2520can%2520be%2520trained%2520using%2520different%250Atargets%2520such%2520as%2520images%252C%2520text%2520prompts%252C%2520and%2520motion%2520vector%2520fields.%2520Only%2520trained%2520on%250Aan%2520Icosphere%2520mesh%252C%2520MeshNCA%2520shows%2520remarkable%2520test-time%2520generalization%2520and%2520can%250Asynthesize%2520textures%2520on%2520unseen%2520meshes%2520in%2520real%2520time.%2520We%2520conduct%2520qualitative%2520and%250Aquantitative%2520comparisons%2520to%2520demonstrate%2520that%2520MeshNCA%2520outperforms%2520other%25203D%250Atexture%2520synthesis%2520methods%2520in%2520terms%2520of%2520generalization%2520and%2520producing%2520high-quality%250Atextures.%2520Moreover%252C%2520we%2520introduce%2520a%2520way%2520of%2520grafting%2520trained%2520MeshNCA%2520instances%252C%250Aenabling%2520interpolation%2520between%2520textures.%2520MeshNCA%2520allows%2520several%2520user%250Ainteractions%2520including%2520texture%2520density/orientation%2520controls%252C%250Agrafting/regenerate%2520brushes%252C%2520and%2520motion%2520speed/direction%2520controls.%2520Finally%252C%2520we%250Aimplement%2520the%2520forward%2520pass%2520of%2520our%2520MeshNCA%2520model%2520using%2520the%2520WebGL%2520shading%250Alanguage%2520and%2520showcase%2520our%2520trained%2520models%2520in%2520an%2520online%2520interactive%2520demo%252C%2520which%250Ais%2520accessible%2520on%2520personal%2520computers%2520and%2520smartphones%2520and%2520is%2520available%2520at%250Ahttps%253A//meshnca.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh%20Neural%20Cellular%20Automata&entry.906535625=Ehsan%20Pajouheshgar%20and%20Yitao%20Xu%20and%20Alexander%20Mordvintsev%20and%20Eyvind%20Niklasson%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Texture%20modeling%20and%20synthesis%20are%20essential%20for%20enhancing%20the%20realism%20of%0Avirtual%20environments.%20Methods%20that%20directly%20synthesize%20textures%20in%203D%20offer%0Adistinct%20advantages%20to%20the%20UV-mapping-based%20methods%20as%20they%20can%20create%20seamless%0Atextures%20and%20align%20more%20closely%20with%20the%20ways%20textures%20form%20in%20nature.%20We%0Apropose%20Mesh%20Neural%20Cellular%20Automata%20%28MeshNCA%29%2C%20a%20method%20that%20directly%0Asynthesizes%20dynamic%20textures%20on%203D%20meshes%20without%20requiring%20any%20UV%20maps.%0AMeshNCA%20is%20a%20generalized%20type%20of%20cellular%20automata%20that%20can%20operate%20on%20a%20set%20of%0Acells%20arranged%20on%20non-grid%20structures%20such%20as%20the%20vertices%20of%20a%203D%20mesh.%0AMeshNCA%20accommodates%20multi-modal%20supervision%20and%20can%20be%20trained%20using%20different%0Atargets%20such%20as%20images%2C%20text%20prompts%2C%20and%20motion%20vector%20fields.%20Only%20trained%20on%0Aan%20Icosphere%20mesh%2C%20MeshNCA%20shows%20remarkable%20test-time%20generalization%20and%20can%0Asynthesize%20textures%20on%20unseen%20meshes%20in%20real%20time.%20We%20conduct%20qualitative%20and%0Aquantitative%20comparisons%20to%20demonstrate%20that%20MeshNCA%20outperforms%20other%203D%0Atexture%20synthesis%20methods%20in%20terms%20of%20generalization%20and%20producing%20high-quality%0Atextures.%20Moreover%2C%20we%20introduce%20a%20way%20of%20grafting%20trained%20MeshNCA%20instances%2C%0Aenabling%20interpolation%20between%20textures.%20MeshNCA%20allows%20several%20user%0Ainteractions%20including%20texture%20density/orientation%20controls%2C%0Agrafting/regenerate%20brushes%2C%20and%20motion%20speed/direction%20controls.%20Finally%2C%20we%0Aimplement%20the%20forward%20pass%20of%20our%20MeshNCA%20model%20using%20the%20WebGL%20shading%0Alanguage%20and%20showcase%20our%20trained%20models%20in%20an%20online%20interactive%20demo%2C%20which%0Ais%20accessible%20on%20personal%20computers%20and%20smartphones%20and%20is%20available%20at%0Ahttps%3A//meshnca.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02820v2&entry.124074799=Read"},
{"title": "Toon3D: Seeing Cartoons from a New Perspective", "author": "Ethan Weber and Riley Peterlinz and Rohan Mathur and Frederik Warburg and Alexei A. Efros and Angjoo Kanazawa", "abstract": "  In this work, we recover the underlying 3D structure of non-geometrically\nconsistent scenes. We focus our analysis on hand-drawn images from cartoons and\nanime. Many cartoons are created by artists without a 3D rendering engine,\nwhich means that any new image of a scene is hand-drawn. The hand-drawn images\nare usually faithful representations of the world, but only in a qualitative\nsense, since it is difficult for humans to draw multiple perspectives of an\nobject or scene 3D consistently. Nevertheless, people can easily perceive 3D\nscenes from inconsistent inputs! In this work, we correct for 2D drawing\ninconsistencies to recover a plausible 3D structure such that the newly warped\ndrawings are consistent with each other. Our pipeline consists of a\nuser-friendly annotation tool, camera pose estimation, and image deformation to\nrecover a dense structure. Our method warps images to obey a perspective camera\nmodel, enabling our aligned results to be plugged into novel-view synthesis\nreconstruction methods to experience cartoons from viewpoints never drawn\nbefore. Our project page is https://toon3d.studio/.\n", "link": "http://arxiv.org/abs/2405.10320v1", "date": "2024-05-16", "relevancy": 2.7262, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5391}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toon3D%3A%20Seeing%20Cartoons%20from%20a%20New%20Perspective&body=Title%3A%20Toon3D%3A%20Seeing%20Cartoons%20from%20a%20New%20Perspective%0AAuthor%3A%20Ethan%20Weber%20and%20Riley%20Peterlinz%20and%20Rohan%20Mathur%20and%20Frederik%20Warburg%20and%20Alexei%20A.%20Efros%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20recover%20the%20underlying%203D%20structure%20of%20non-geometrically%0Aconsistent%20scenes.%20We%20focus%20our%20analysis%20on%20hand-drawn%20images%20from%20cartoons%20and%0Aanime.%20Many%20cartoons%20are%20created%20by%20artists%20without%20a%203D%20rendering%20engine%2C%0Awhich%20means%20that%20any%20new%20image%20of%20a%20scene%20is%20hand-drawn.%20The%20hand-drawn%20images%0Aare%20usually%20faithful%20representations%20of%20the%20world%2C%20but%20only%20in%20a%20qualitative%0Asense%2C%20since%20it%20is%20difficult%20for%20humans%20to%20draw%20multiple%20perspectives%20of%20an%0Aobject%20or%20scene%203D%20consistently.%20Nevertheless%2C%20people%20can%20easily%20perceive%203D%0Ascenes%20from%20inconsistent%20inputs%21%20In%20this%20work%2C%20we%20correct%20for%202D%20drawing%0Ainconsistencies%20to%20recover%20a%20plausible%203D%20structure%20such%20that%20the%20newly%20warped%0Adrawings%20are%20consistent%20with%20each%20other.%20Our%20pipeline%20consists%20of%20a%0Auser-friendly%20annotation%20tool%2C%20camera%20pose%20estimation%2C%20and%20image%20deformation%20to%0Arecover%20a%20dense%20structure.%20Our%20method%20warps%20images%20to%20obey%20a%20perspective%20camera%0Amodel%2C%20enabling%20our%20aligned%20results%20to%20be%20plugged%20into%20novel-view%20synthesis%0Areconstruction%20methods%20to%20experience%20cartoons%20from%20viewpoints%20never%20drawn%0Abefore.%20Our%20project%20page%20is%20https%3A//toon3d.studio/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToon3D%253A%2520Seeing%2520Cartoons%2520from%2520a%2520New%2520Perspective%26entry.906535625%3DEthan%2520Weber%2520and%2520Riley%2520Peterlinz%2520and%2520Rohan%2520Mathur%2520and%2520Frederik%2520Warburg%2520and%2520Alexei%2520A.%2520Efros%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520recover%2520the%2520underlying%25203D%2520structure%2520of%2520non-geometrically%250Aconsistent%2520scenes.%2520We%2520focus%2520our%2520analysis%2520on%2520hand-drawn%2520images%2520from%2520cartoons%2520and%250Aanime.%2520Many%2520cartoons%2520are%2520created%2520by%2520artists%2520without%2520a%25203D%2520rendering%2520engine%252C%250Awhich%2520means%2520that%2520any%2520new%2520image%2520of%2520a%2520scene%2520is%2520hand-drawn.%2520The%2520hand-drawn%2520images%250Aare%2520usually%2520faithful%2520representations%2520of%2520the%2520world%252C%2520but%2520only%2520in%2520a%2520qualitative%250Asense%252C%2520since%2520it%2520is%2520difficult%2520for%2520humans%2520to%2520draw%2520multiple%2520perspectives%2520of%2520an%250Aobject%2520or%2520scene%25203D%2520consistently.%2520Nevertheless%252C%2520people%2520can%2520easily%2520perceive%25203D%250Ascenes%2520from%2520inconsistent%2520inputs%2521%2520In%2520this%2520work%252C%2520we%2520correct%2520for%25202D%2520drawing%250Ainconsistencies%2520to%2520recover%2520a%2520plausible%25203D%2520structure%2520such%2520that%2520the%2520newly%2520warped%250Adrawings%2520are%2520consistent%2520with%2520each%2520other.%2520Our%2520pipeline%2520consists%2520of%2520a%250Auser-friendly%2520annotation%2520tool%252C%2520camera%2520pose%2520estimation%252C%2520and%2520image%2520deformation%2520to%250Arecover%2520a%2520dense%2520structure.%2520Our%2520method%2520warps%2520images%2520to%2520obey%2520a%2520perspective%2520camera%250Amodel%252C%2520enabling%2520our%2520aligned%2520results%2520to%2520be%2520plugged%2520into%2520novel-view%2520synthesis%250Areconstruction%2520methods%2520to%2520experience%2520cartoons%2520from%2520viewpoints%2520never%2520drawn%250Abefore.%2520Our%2520project%2520page%2520is%2520https%253A//toon3d.studio/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toon3D%3A%20Seeing%20Cartoons%20from%20a%20New%20Perspective&entry.906535625=Ethan%20Weber%20and%20Riley%20Peterlinz%20and%20Rohan%20Mathur%20and%20Frederik%20Warburg%20and%20Alexei%20A.%20Efros%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20In%20this%20work%2C%20we%20recover%20the%20underlying%203D%20structure%20of%20non-geometrically%0Aconsistent%20scenes.%20We%20focus%20our%20analysis%20on%20hand-drawn%20images%20from%20cartoons%20and%0Aanime.%20Many%20cartoons%20are%20created%20by%20artists%20without%20a%203D%20rendering%20engine%2C%0Awhich%20means%20that%20any%20new%20image%20of%20a%20scene%20is%20hand-drawn.%20The%20hand-drawn%20images%0Aare%20usually%20faithful%20representations%20of%20the%20world%2C%20but%20only%20in%20a%20qualitative%0Asense%2C%20since%20it%20is%20difficult%20for%20humans%20to%20draw%20multiple%20perspectives%20of%20an%0Aobject%20or%20scene%203D%20consistently.%20Nevertheless%2C%20people%20can%20easily%20perceive%203D%0Ascenes%20from%20inconsistent%20inputs%21%20In%20this%20work%2C%20we%20correct%20for%202D%20drawing%0Ainconsistencies%20to%20recover%20a%20plausible%203D%20structure%20such%20that%20the%20newly%20warped%0Adrawings%20are%20consistent%20with%20each%20other.%20Our%20pipeline%20consists%20of%20a%0Auser-friendly%20annotation%20tool%2C%20camera%20pose%20estimation%2C%20and%20image%20deformation%20to%0Arecover%20a%20dense%20structure.%20Our%20method%20warps%20images%20to%20obey%20a%20perspective%20camera%0Amodel%2C%20enabling%20our%20aligned%20results%20to%20be%20plugged%20into%20novel-view%20synthesis%0Areconstruction%20methods%20to%20experience%20cartoons%20from%20viewpoints%20never%20drawn%0Abefore.%20Our%20project%20page%20is%20https%3A//toon3d.studio/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10320v1&entry.124074799=Read"},
{"title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation", "author": "Manh Luong and Khai Nguyen and Nhat Ho and Reza Haf and Dinh Phung and Lizhen Qu", "abstract": "  The Learning-to-match (LTM) framework proves to be an effective inverse\noptimal transport approach for learning the underlying ground metric between\ntwo sources of data, facilitating subsequent matching. However, the\nconventional LTM framework faces scalability challenges, necessitating the use\nof the entire dataset each time the parameters of the ground metric are\nupdated. In adapting LTM to the deep learning context, we introduce the\nmini-batch Learning-to-match (m-LTM) framework for audio-text retrieval\nproblems. This framework leverages mini-batch subsampling and\nMahalanobis-enhanced family of ground metrics. Moreover, to cope with\nmisaligned training data in practice, we propose a variant using partial\noptimal transport to mitigate the harm of misaligned data pairs in training\ndata. We conduct extensive experiments on audio-text matching problems using\nthree datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our\nproposed method is capable of learning rich and expressive joint embedding\nspace, which achieves SOTA performance. Beyond this, the proposed m-LTM\nframework is able to close the modality gap across audio and text embedding,\nwhich surpasses both triplet and contrastive loss in the zero-shot sound event\ndetection task on the ESC-50 dataset. Notably, our strategy of employing\npartial optimal transport with m-LTM demonstrates greater noise tolerance than\ncontrastive loss, especially under varying noise ratios in training data on the\nAudioCaps dataset. Our code is available at\nhttps://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval\n", "link": "http://arxiv.org/abs/2405.10084v1", "date": "2024-05-16", "relevancy": 2.7149, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5342}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Deep%20Audio-Text%20Retrieval%20Through%20the%20Lens%20of%20Transportation&body=Title%3A%20Revisiting%20Deep%20Audio-Text%20Retrieval%20Through%20the%20Lens%20of%20Transportation%0AAuthor%3A%20Manh%20Luong%20and%20Khai%20Nguyen%20and%20Nhat%20Ho%20and%20Reza%20Haf%20and%20Dinh%20Phung%20and%20Lizhen%20Qu%0AAbstract%3A%20%20%20The%20Learning-to-match%20%28LTM%29%20framework%20proves%20to%20be%20an%20effective%20inverse%0Aoptimal%20transport%20approach%20for%20learning%20the%20underlying%20ground%20metric%20between%0Atwo%20sources%20of%20data%2C%20facilitating%20subsequent%20matching.%20However%2C%20the%0Aconventional%20LTM%20framework%20faces%20scalability%20challenges%2C%20necessitating%20the%20use%0Aof%20the%20entire%20dataset%20each%20time%20the%20parameters%20of%20the%20ground%20metric%20are%0Aupdated.%20In%20adapting%20LTM%20to%20the%20deep%20learning%20context%2C%20we%20introduce%20the%0Amini-batch%20Learning-to-match%20%28m-LTM%29%20framework%20for%20audio-text%20retrieval%0Aproblems.%20This%20framework%20leverages%20mini-batch%20subsampling%20and%0AMahalanobis-enhanced%20family%20of%20ground%20metrics.%20Moreover%2C%20to%20cope%20with%0Amisaligned%20training%20data%20in%20practice%2C%20we%20propose%20a%20variant%20using%20partial%0Aoptimal%20transport%20to%20mitigate%20the%20harm%20of%20misaligned%20data%20pairs%20in%20training%0Adata.%20We%20conduct%20extensive%20experiments%20on%20audio-text%20matching%20problems%20using%0Athree%20datasets%3A%20AudioCaps%2C%20Clotho%2C%20and%20ESC-50.%20Results%20demonstrate%20that%20our%0Aproposed%20method%20is%20capable%20of%20learning%20rich%20and%20expressive%20joint%20embedding%0Aspace%2C%20which%20achieves%20SOTA%20performance.%20Beyond%20this%2C%20the%20proposed%20m-LTM%0Aframework%20is%20able%20to%20close%20the%20modality%20gap%20across%20audio%20and%20text%20embedding%2C%0Awhich%20surpasses%20both%20triplet%20and%20contrastive%20loss%20in%20the%20zero-shot%20sound%20event%0Adetection%20task%20on%20the%20ESC-50%20dataset.%20Notably%2C%20our%20strategy%20of%20employing%0Apartial%20optimal%20transport%20with%20m-LTM%20demonstrates%20greater%20noise%20tolerance%20than%0Acontrastive%20loss%2C%20especially%20under%20varying%20noise%20ratios%20in%20training%20data%20on%20the%0AAudioCaps%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Deep%2520Audio-Text%2520Retrieval%2520Through%2520the%2520Lens%2520of%2520Transportation%26entry.906535625%3DManh%2520Luong%2520and%2520Khai%2520Nguyen%2520and%2520Nhat%2520Ho%2520and%2520Reza%2520Haf%2520and%2520Dinh%2520Phung%2520and%2520Lizhen%2520Qu%26entry.1292438233%3D%2520%2520The%2520Learning-to-match%2520%2528LTM%2529%2520framework%2520proves%2520to%2520be%2520an%2520effective%2520inverse%250Aoptimal%2520transport%2520approach%2520for%2520learning%2520the%2520underlying%2520ground%2520metric%2520between%250Atwo%2520sources%2520of%2520data%252C%2520facilitating%2520subsequent%2520matching.%2520However%252C%2520the%250Aconventional%2520LTM%2520framework%2520faces%2520scalability%2520challenges%252C%2520necessitating%2520the%2520use%250Aof%2520the%2520entire%2520dataset%2520each%2520time%2520the%2520parameters%2520of%2520the%2520ground%2520metric%2520are%250Aupdated.%2520In%2520adapting%2520LTM%2520to%2520the%2520deep%2520learning%2520context%252C%2520we%2520introduce%2520the%250Amini-batch%2520Learning-to-match%2520%2528m-LTM%2529%2520framework%2520for%2520audio-text%2520retrieval%250Aproblems.%2520This%2520framework%2520leverages%2520mini-batch%2520subsampling%2520and%250AMahalanobis-enhanced%2520family%2520of%2520ground%2520metrics.%2520Moreover%252C%2520to%2520cope%2520with%250Amisaligned%2520training%2520data%2520in%2520practice%252C%2520we%2520propose%2520a%2520variant%2520using%2520partial%250Aoptimal%2520transport%2520to%2520mitigate%2520the%2520harm%2520of%2520misaligned%2520data%2520pairs%2520in%2520training%250Adata.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520audio-text%2520matching%2520problems%2520using%250Athree%2520datasets%253A%2520AudioCaps%252C%2520Clotho%252C%2520and%2520ESC-50.%2520Results%2520demonstrate%2520that%2520our%250Aproposed%2520method%2520is%2520capable%2520of%2520learning%2520rich%2520and%2520expressive%2520joint%2520embedding%250Aspace%252C%2520which%2520achieves%2520SOTA%2520performance.%2520Beyond%2520this%252C%2520the%2520proposed%2520m-LTM%250Aframework%2520is%2520able%2520to%2520close%2520the%2520modality%2520gap%2520across%2520audio%2520and%2520text%2520embedding%252C%250Awhich%2520surpasses%2520both%2520triplet%2520and%2520contrastive%2520loss%2520in%2520the%2520zero-shot%2520sound%2520event%250Adetection%2520task%2520on%2520the%2520ESC-50%2520dataset.%2520Notably%252C%2520our%2520strategy%2520of%2520employing%250Apartial%2520optimal%2520transport%2520with%2520m-LTM%2520demonstrates%2520greater%2520noise%2520tolerance%2520than%250Acontrastive%2520loss%252C%2520especially%2520under%2520varying%2520noise%2520ratios%2520in%2520training%2520data%2520on%2520the%250AAudioCaps%2520dataset.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Deep%20Audio-Text%20Retrieval%20Through%20the%20Lens%20of%20Transportation&entry.906535625=Manh%20Luong%20and%20Khai%20Nguyen%20and%20Nhat%20Ho%20and%20Reza%20Haf%20and%20Dinh%20Phung%20and%20Lizhen%20Qu&entry.1292438233=%20%20The%20Learning-to-match%20%28LTM%29%20framework%20proves%20to%20be%20an%20effective%20inverse%0Aoptimal%20transport%20approach%20for%20learning%20the%20underlying%20ground%20metric%20between%0Atwo%20sources%20of%20data%2C%20facilitating%20subsequent%20matching.%20However%2C%20the%0Aconventional%20LTM%20framework%20faces%20scalability%20challenges%2C%20necessitating%20the%20use%0Aof%20the%20entire%20dataset%20each%20time%20the%20parameters%20of%20the%20ground%20metric%20are%0Aupdated.%20In%20adapting%20LTM%20to%20the%20deep%20learning%20context%2C%20we%20introduce%20the%0Amini-batch%20Learning-to-match%20%28m-LTM%29%20framework%20for%20audio-text%20retrieval%0Aproblems.%20This%20framework%20leverages%20mini-batch%20subsampling%20and%0AMahalanobis-enhanced%20family%20of%20ground%20metrics.%20Moreover%2C%20to%20cope%20with%0Amisaligned%20training%20data%20in%20practice%2C%20we%20propose%20a%20variant%20using%20partial%0Aoptimal%20transport%20to%20mitigate%20the%20harm%20of%20misaligned%20data%20pairs%20in%20training%0Adata.%20We%20conduct%20extensive%20experiments%20on%20audio-text%20matching%20problems%20using%0Athree%20datasets%3A%20AudioCaps%2C%20Clotho%2C%20and%20ESC-50.%20Results%20demonstrate%20that%20our%0Aproposed%20method%20is%20capable%20of%20learning%20rich%20and%20expressive%20joint%20embedding%0Aspace%2C%20which%20achieves%20SOTA%20performance.%20Beyond%20this%2C%20the%20proposed%20m-LTM%0Aframework%20is%20able%20to%20close%20the%20modality%20gap%20across%20audio%20and%20text%20embedding%2C%0Awhich%20surpasses%20both%20triplet%20and%20contrastive%20loss%20in%20the%20zero-shot%20sound%20event%0Adetection%20task%20on%20the%20ESC-50%20dataset.%20Notably%2C%20our%20strategy%20of%20employing%0Apartial%20optimal%20transport%20with%20m-LTM%20demonstrates%20greater%20noise%20tolerance%20than%0Acontrastive%20loss%2C%20especially%20under%20varying%20noise%20ratios%20in%20training%20data%20on%20the%0AAudioCaps%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10084v1&entry.124074799=Read"},
{"title": "Towards Task-Compatible Compressible Representations", "author": "Anderson de Andrade and Ivan Baji\u0107", "abstract": "  We identify an issue in multi-task learnable compression, in which a\nrepresentation learned for one task does not positively contribute to the\nrate-distortion performance of a different task as much as expected, given the\nestimated amount of information available in it. We interpret this issue using\nthe predictive $\\mathcal{V}$-information framework. In learnable scalable\ncoding, previous work increased the utilization of side-information for input\nreconstruction by also rewarding input reconstruction when learning this shared\nrepresentation. We evaluate the impact of this idea in the context of input\nreconstruction more rigorously and extended it to other computer vision tasks.\nWe perform experiments using representations trained for object detection on\nCOCO 2017 and depth estimation on the Cityscapes dataset, and use them to\nassist in image reconstruction and semantic segmentation tasks. The results\nshow considerable improvements in the rate-distortion performance of the\nassisted tasks. Moreover, using the proposed representations, the performance\nof the base tasks are also improved. Results suggest that the proposed method\ninduces simpler representations that are more compatible with downstream\nprocesses.\n", "link": "http://arxiv.org/abs/2405.10244v1", "date": "2024-05-16", "relevancy": 2.6429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5162}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Task-Compatible%20Compressible%20Representations&body=Title%3A%20Towards%20Task-Compatible%20Compressible%20Representations%0AAuthor%3A%20Anderson%20de%20Andrade%20and%20Ivan%20Baji%C4%87%0AAbstract%3A%20%20%20We%20identify%20an%20issue%20in%20multi-task%20learnable%20compression%2C%20in%20which%20a%0Arepresentation%20learned%20for%20one%20task%20does%20not%20positively%20contribute%20to%20the%0Arate-distortion%20performance%20of%20a%20different%20task%20as%20much%20as%20expected%2C%20given%20the%0Aestimated%20amount%20of%20information%20available%20in%20it.%20We%20interpret%20this%20issue%20using%0Athe%20predictive%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20In%20learnable%20scalable%0Acoding%2C%20previous%20work%20increased%20the%20utilization%20of%20side-information%20for%20input%0Areconstruction%20by%20also%20rewarding%20input%20reconstruction%20when%20learning%20this%20shared%0Arepresentation.%20We%20evaluate%20the%20impact%20of%20this%20idea%20in%20the%20context%20of%20input%0Areconstruction%20more%20rigorously%20and%20extended%20it%20to%20other%20computer%20vision%20tasks.%0AWe%20perform%20experiments%20using%20representations%20trained%20for%20object%20detection%20on%0ACOCO%202017%20and%20depth%20estimation%20on%20the%20Cityscapes%20dataset%2C%20and%20use%20them%20to%0Aassist%20in%20image%20reconstruction%20and%20semantic%20segmentation%20tasks.%20The%20results%0Ashow%20considerable%20improvements%20in%20the%20rate-distortion%20performance%20of%20the%0Aassisted%20tasks.%20Moreover%2C%20using%20the%20proposed%20representations%2C%20the%20performance%0Aof%20the%20base%20tasks%20are%20also%20improved.%20Results%20suggest%20that%20the%20proposed%20method%0Ainduces%20simpler%20representations%20that%20are%20more%20compatible%20with%20downstream%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Task-Compatible%2520Compressible%2520Representations%26entry.906535625%3DAnderson%2520de%2520Andrade%2520and%2520Ivan%2520Baji%25C4%2587%26entry.1292438233%3D%2520%2520We%2520identify%2520an%2520issue%2520in%2520multi-task%2520learnable%2520compression%252C%2520in%2520which%2520a%250Arepresentation%2520learned%2520for%2520one%2520task%2520does%2520not%2520positively%2520contribute%2520to%2520the%250Arate-distortion%2520performance%2520of%2520a%2520different%2520task%2520as%2520much%2520as%2520expected%252C%2520given%2520the%250Aestimated%2520amount%2520of%2520information%2520available%2520in%2520it.%2520We%2520interpret%2520this%2520issue%2520using%250Athe%2520predictive%2520%2524%255Cmathcal%257BV%257D%2524-information%2520framework.%2520In%2520learnable%2520scalable%250Acoding%252C%2520previous%2520work%2520increased%2520the%2520utilization%2520of%2520side-information%2520for%2520input%250Areconstruction%2520by%2520also%2520rewarding%2520input%2520reconstruction%2520when%2520learning%2520this%2520shared%250Arepresentation.%2520We%2520evaluate%2520the%2520impact%2520of%2520this%2520idea%2520in%2520the%2520context%2520of%2520input%250Areconstruction%2520more%2520rigorously%2520and%2520extended%2520it%2520to%2520other%2520computer%2520vision%2520tasks.%250AWe%2520perform%2520experiments%2520using%2520representations%2520trained%2520for%2520object%2520detection%2520on%250ACOCO%25202017%2520and%2520depth%2520estimation%2520on%2520the%2520Cityscapes%2520dataset%252C%2520and%2520use%2520them%2520to%250Aassist%2520in%2520image%2520reconstruction%2520and%2520semantic%2520segmentation%2520tasks.%2520The%2520results%250Ashow%2520considerable%2520improvements%2520in%2520the%2520rate-distortion%2520performance%2520of%2520the%250Aassisted%2520tasks.%2520Moreover%252C%2520using%2520the%2520proposed%2520representations%252C%2520the%2520performance%250Aof%2520the%2520base%2520tasks%2520are%2520also%2520improved.%2520Results%2520suggest%2520that%2520the%2520proposed%2520method%250Ainduces%2520simpler%2520representations%2520that%2520are%2520more%2520compatible%2520with%2520downstream%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Task-Compatible%20Compressible%20Representations&entry.906535625=Anderson%20de%20Andrade%20and%20Ivan%20Baji%C4%87&entry.1292438233=%20%20We%20identify%20an%20issue%20in%20multi-task%20learnable%20compression%2C%20in%20which%20a%0Arepresentation%20learned%20for%20one%20task%20does%20not%20positively%20contribute%20to%20the%0Arate-distortion%20performance%20of%20a%20different%20task%20as%20much%20as%20expected%2C%20given%20the%0Aestimated%20amount%20of%20information%20available%20in%20it.%20We%20interpret%20this%20issue%20using%0Athe%20predictive%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20In%20learnable%20scalable%0Acoding%2C%20previous%20work%20increased%20the%20utilization%20of%20side-information%20for%20input%0Areconstruction%20by%20also%20rewarding%20input%20reconstruction%20when%20learning%20this%20shared%0Arepresentation.%20We%20evaluate%20the%20impact%20of%20this%20idea%20in%20the%20context%20of%20input%0Areconstruction%20more%20rigorously%20and%20extended%20it%20to%20other%20computer%20vision%20tasks.%0AWe%20perform%20experiments%20using%20representations%20trained%20for%20object%20detection%20on%0ACOCO%202017%20and%20depth%20estimation%20on%20the%20Cityscapes%20dataset%2C%20and%20use%20them%20to%0Aassist%20in%20image%20reconstruction%20and%20semantic%20segmentation%20tasks.%20The%20results%0Ashow%20considerable%20improvements%20in%20the%20rate-distortion%20performance%20of%20the%0Aassisted%20tasks.%20Moreover%2C%20using%20the%20proposed%20representations%2C%20the%20performance%0Aof%20the%20base%20tasks%20are%20also%20improved.%20Results%20suggest%20that%20the%20proposed%20method%0Ainduces%20simpler%20representations%20that%20are%20more%20compatible%20with%20downstream%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10244v1&entry.124074799=Read"},
{"title": "Filling Missing Values Matters for Range Image-Based Point Cloud\n  Segmentation", "author": "Bike Chen and Chen Gong and Juha R\u00f6ning", "abstract": "  Point cloud segmentation (PCS) plays an essential role in robot perception\nand navigation tasks. To efficiently understand large-scale outdoor point\nclouds, their range image representation is commonly adopted. This image-like\nrepresentation is compact and structured, making range image-based PCS models\npractical. However, undesirable missing values in the range images damage the\nshapes and patterns of objects. This problem creates difficulty for the models\nin learning coherent and complete geometric information from the objects.\nConsequently, the PCS models only achieve inferior performance. Delving deeply\ninto this issue, we find that the use of unreasonable projection approaches and\ndeskewing scans mainly leads to unwanted missing values in the range images.\nBesides, almost all previous works fail to consider filling in the unexpected\nmissing values in the PCS task. To alleviate this problem, we first propose a\nnew projection method, namely scan unfolding++ (SU++), to avoid massive missing\nvalues in the generated range images. Then, we introduce a simple yet effective\napproach, namely range-dependent $K$-nearest neighbor interpolation ($K$NNI),\nto further fill in missing values. Finally, we introduce the Filling Missing\nValues Network (FMVNet) and Fast FMVNet. Extensive experimental results on\nSemanticKITTI, SemanticPOSS, and nuScenes datasets demonstrate that by\nemploying the proposed SU++ and $K$NNI, existing range image-based PCS models\nconsistently achieve better performance than the baseline models. Besides, both\nFMVNet and Fast FMVNet achieve state-of-the-art performance in terms of the\nspeed-accuracy trade-off. The proposed methods can be applied to other range\nimage-based tasks and practical applications.\n", "link": "http://arxiv.org/abs/2405.10175v1", "date": "2024-05-16", "relevancy": 2.6213, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5564}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Filling%20Missing%20Values%20Matters%20for%20Range%20Image-Based%20Point%20Cloud%0A%20%20Segmentation&body=Title%3A%20Filling%20Missing%20Values%20Matters%20for%20Range%20Image-Based%20Point%20Cloud%0A%20%20Segmentation%0AAuthor%3A%20Bike%20Chen%20and%20Chen%20Gong%20and%20Juha%20R%C3%B6ning%0AAbstract%3A%20%20%20Point%20cloud%20segmentation%20%28PCS%29%20plays%20an%20essential%20role%20in%20robot%20perception%0Aand%20navigation%20tasks.%20To%20efficiently%20understand%20large-scale%20outdoor%20point%0Aclouds%2C%20their%20range%20image%20representation%20is%20commonly%20adopted.%20This%20image-like%0Arepresentation%20is%20compact%20and%20structured%2C%20making%20range%20image-based%20PCS%20models%0Apractical.%20However%2C%20undesirable%20missing%20values%20in%20the%20range%20images%20damage%20the%0Ashapes%20and%20patterns%20of%20objects.%20This%20problem%20creates%20difficulty%20for%20the%20models%0Ain%20learning%20coherent%20and%20complete%20geometric%20information%20from%20the%20objects.%0AConsequently%2C%20the%20PCS%20models%20only%20achieve%20inferior%20performance.%20Delving%20deeply%0Ainto%20this%20issue%2C%20we%20find%20that%20the%20use%20of%20unreasonable%20projection%20approaches%20and%0Adeskewing%20scans%20mainly%20leads%20to%20unwanted%20missing%20values%20in%20the%20range%20images.%0ABesides%2C%20almost%20all%20previous%20works%20fail%20to%20consider%20filling%20in%20the%20unexpected%0Amissing%20values%20in%20the%20PCS%20task.%20To%20alleviate%20this%20problem%2C%20we%20first%20propose%20a%0Anew%20projection%20method%2C%20namely%20scan%20unfolding%2B%2B%20%28SU%2B%2B%29%2C%20to%20avoid%20massive%20missing%0Avalues%20in%20the%20generated%20range%20images.%20Then%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aapproach%2C%20namely%20range-dependent%20%24K%24-nearest%20neighbor%20interpolation%20%28%24K%24NNI%29%2C%0Ato%20further%20fill%20in%20missing%20values.%20Finally%2C%20we%20introduce%20the%20Filling%20Missing%0AValues%20Network%20%28FMVNet%29%20and%20Fast%20FMVNet.%20Extensive%20experimental%20results%20on%0ASemanticKITTI%2C%20SemanticPOSS%2C%20and%20nuScenes%20datasets%20demonstrate%20that%20by%0Aemploying%20the%20proposed%20SU%2B%2B%20and%20%24K%24NNI%2C%20existing%20range%20image-based%20PCS%20models%0Aconsistently%20achieve%20better%20performance%20than%20the%20baseline%20models.%20Besides%2C%20both%0AFMVNet%20and%20Fast%20FMVNet%20achieve%20state-of-the-art%20performance%20in%20terms%20of%20the%0Aspeed-accuracy%20trade-off.%20The%20proposed%20methods%20can%20be%20applied%20to%20other%20range%0Aimage-based%20tasks%20and%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFilling%2520Missing%2520Values%2520Matters%2520for%2520Range%2520Image-Based%2520Point%2520Cloud%250A%2520%2520Segmentation%26entry.906535625%3DBike%2520Chen%2520and%2520Chen%2520Gong%2520and%2520Juha%2520R%25C3%25B6ning%26entry.1292438233%3D%2520%2520Point%2520cloud%2520segmentation%2520%2528PCS%2529%2520plays%2520an%2520essential%2520role%2520in%2520robot%2520perception%250Aand%2520navigation%2520tasks.%2520To%2520efficiently%2520understand%2520large-scale%2520outdoor%2520point%250Aclouds%252C%2520their%2520range%2520image%2520representation%2520is%2520commonly%2520adopted.%2520This%2520image-like%250Arepresentation%2520is%2520compact%2520and%2520structured%252C%2520making%2520range%2520image-based%2520PCS%2520models%250Apractical.%2520However%252C%2520undesirable%2520missing%2520values%2520in%2520the%2520range%2520images%2520damage%2520the%250Ashapes%2520and%2520patterns%2520of%2520objects.%2520This%2520problem%2520creates%2520difficulty%2520for%2520the%2520models%250Ain%2520learning%2520coherent%2520and%2520complete%2520geometric%2520information%2520from%2520the%2520objects.%250AConsequently%252C%2520the%2520PCS%2520models%2520only%2520achieve%2520inferior%2520performance.%2520Delving%2520deeply%250Ainto%2520this%2520issue%252C%2520we%2520find%2520that%2520the%2520use%2520of%2520unreasonable%2520projection%2520approaches%2520and%250Adeskewing%2520scans%2520mainly%2520leads%2520to%2520unwanted%2520missing%2520values%2520in%2520the%2520range%2520images.%250ABesides%252C%2520almost%2520all%2520previous%2520works%2520fail%2520to%2520consider%2520filling%2520in%2520the%2520unexpected%250Amissing%2520values%2520in%2520the%2520PCS%2520task.%2520To%2520alleviate%2520this%2520problem%252C%2520we%2520first%2520propose%2520a%250Anew%2520projection%2520method%252C%2520namely%2520scan%2520unfolding%252B%252B%2520%2528SU%252B%252B%2529%252C%2520to%2520avoid%2520massive%2520missing%250Avalues%2520in%2520the%2520generated%2520range%2520images.%2520Then%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%250Aapproach%252C%2520namely%2520range-dependent%2520%2524K%2524-nearest%2520neighbor%2520interpolation%2520%2528%2524K%2524NNI%2529%252C%250Ato%2520further%2520fill%2520in%2520missing%2520values.%2520Finally%252C%2520we%2520introduce%2520the%2520Filling%2520Missing%250AValues%2520Network%2520%2528FMVNet%2529%2520and%2520Fast%2520FMVNet.%2520Extensive%2520experimental%2520results%2520on%250ASemanticKITTI%252C%2520SemanticPOSS%252C%2520and%2520nuScenes%2520datasets%2520demonstrate%2520that%2520by%250Aemploying%2520the%2520proposed%2520SU%252B%252B%2520and%2520%2524K%2524NNI%252C%2520existing%2520range%2520image-based%2520PCS%2520models%250Aconsistently%2520achieve%2520better%2520performance%2520than%2520the%2520baseline%2520models.%2520Besides%252C%2520both%250AFMVNet%2520and%2520Fast%2520FMVNet%2520achieve%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520the%250Aspeed-accuracy%2520trade-off.%2520The%2520proposed%2520methods%2520can%2520be%2520applied%2520to%2520other%2520range%250Aimage-based%2520tasks%2520and%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filling%20Missing%20Values%20Matters%20for%20Range%20Image-Based%20Point%20Cloud%0A%20%20Segmentation&entry.906535625=Bike%20Chen%20and%20Chen%20Gong%20and%20Juha%20R%C3%B6ning&entry.1292438233=%20%20Point%20cloud%20segmentation%20%28PCS%29%20plays%20an%20essential%20role%20in%20robot%20perception%0Aand%20navigation%20tasks.%20To%20efficiently%20understand%20large-scale%20outdoor%20point%0Aclouds%2C%20their%20range%20image%20representation%20is%20commonly%20adopted.%20This%20image-like%0Arepresentation%20is%20compact%20and%20structured%2C%20making%20range%20image-based%20PCS%20models%0Apractical.%20However%2C%20undesirable%20missing%20values%20in%20the%20range%20images%20damage%20the%0Ashapes%20and%20patterns%20of%20objects.%20This%20problem%20creates%20difficulty%20for%20the%20models%0Ain%20learning%20coherent%20and%20complete%20geometric%20information%20from%20the%20objects.%0AConsequently%2C%20the%20PCS%20models%20only%20achieve%20inferior%20performance.%20Delving%20deeply%0Ainto%20this%20issue%2C%20we%20find%20that%20the%20use%20of%20unreasonable%20projection%20approaches%20and%0Adeskewing%20scans%20mainly%20leads%20to%20unwanted%20missing%20values%20in%20the%20range%20images.%0ABesides%2C%20almost%20all%20previous%20works%20fail%20to%20consider%20filling%20in%20the%20unexpected%0Amissing%20values%20in%20the%20PCS%20task.%20To%20alleviate%20this%20problem%2C%20we%20first%20propose%20a%0Anew%20projection%20method%2C%20namely%20scan%20unfolding%2B%2B%20%28SU%2B%2B%29%2C%20to%20avoid%20massive%20missing%0Avalues%20in%20the%20generated%20range%20images.%20Then%2C%20we%20introduce%20a%20simple%20yet%20effective%0Aapproach%2C%20namely%20range-dependent%20%24K%24-nearest%20neighbor%20interpolation%20%28%24K%24NNI%29%2C%0Ato%20further%20fill%20in%20missing%20values.%20Finally%2C%20we%20introduce%20the%20Filling%20Missing%0AValues%20Network%20%28FMVNet%29%20and%20Fast%20FMVNet.%20Extensive%20experimental%20results%20on%0ASemanticKITTI%2C%20SemanticPOSS%2C%20and%20nuScenes%20datasets%20demonstrate%20that%20by%0Aemploying%20the%20proposed%20SU%2B%2B%20and%20%24K%24NNI%2C%20existing%20range%20image-based%20PCS%20models%0Aconsistently%20achieve%20better%20performance%20than%20the%20baseline%20models.%20Besides%2C%20both%0AFMVNet%20and%20Fast%20FMVNet%20achieve%20state-of-the-art%20performance%20in%20terms%20of%20the%0Aspeed-accuracy%20trade-off.%20The%20proposed%20methods%20can%20be%20applied%20to%20other%20range%0Aimage-based%20tasks%20and%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10175v1&entry.124074799=Read"},
{"title": "GS-Planner: A Gaussian-Splatting-based Planning Framework for Active\n  High-Fidelity Reconstruction", "author": "Rui Jin and Yuman Gao and Haojian Lu and Fei Gao", "abstract": "  Active reconstruction technique enables robots to autonomously collect scene\ndata for full coverage, relieving users from tedious and time-consuming data\ncapturing process. However, designed based on unsuitable scene representations,\nexisting methods show unrealistic reconstruction results or the inability of\nonline quality evaluation. Due to the recent advancements in explicit radiance\nfield technology, online active high-fidelity reconstruction has become\nachievable. In this paper, we propose GS-Planner, a planning framework for\nactive high-fidelity reconstruction using 3D Gaussian Splatting. With\nimprovement on 3DGS to recognize unobserved regions, we evaluate the\nreconstruction quality and completeness of 3DGS map online to guide the robot.\nThen we design a sampling-based active reconstruction strategy to explore the\nunobserved areas and improve the reconstruction geometric and textural quality.\nTo establish a complete robot active reconstruction system, we choose quadrotor\nas the robotic platform for its high agility. Then we devise a safety\nconstraint with 3DGS to generate executable trajectories for quadrotor\nnavigation in the 3DGS map. To validate the effectiveness of our method, we\nconduct extensive experiments and ablation studies in highly realistic\nsimulation scenes.\n", "link": "http://arxiv.org/abs/2405.10142v1", "date": "2024-05-16", "relevancy": 2.6154, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7034}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6289}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Planner%3A%20A%20Gaussian-Splatting-based%20Planning%20Framework%20for%20Active%0A%20%20High-Fidelity%20Reconstruction&body=Title%3A%20GS-Planner%3A%20A%20Gaussian-Splatting-based%20Planning%20Framework%20for%20Active%0A%20%20High-Fidelity%20Reconstruction%0AAuthor%3A%20Rui%20Jin%20and%20Yuman%20Gao%20and%20Haojian%20Lu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Active%20reconstruction%20technique%20enables%20robots%20to%20autonomously%20collect%20scene%0Adata%20for%20full%20coverage%2C%20relieving%20users%20from%20tedious%20and%20time-consuming%20data%0Acapturing%20process.%20However%2C%20designed%20based%20on%20unsuitable%20scene%20representations%2C%0Aexisting%20methods%20show%20unrealistic%20reconstruction%20results%20or%20the%20inability%20of%0Aonline%20quality%20evaluation.%20Due%20to%20the%20recent%20advancements%20in%20explicit%20radiance%0Afield%20technology%2C%20online%20active%20high-fidelity%20reconstruction%20has%20become%0Aachievable.%20In%20this%20paper%2C%20we%20propose%20GS-Planner%2C%20a%20planning%20framework%20for%0Aactive%20high-fidelity%20reconstruction%20using%203D%20Gaussian%20Splatting.%20With%0Aimprovement%20on%203DGS%20to%20recognize%20unobserved%20regions%2C%20we%20evaluate%20the%0Areconstruction%20quality%20and%20completeness%20of%203DGS%20map%20online%20to%20guide%20the%20robot.%0AThen%20we%20design%20a%20sampling-based%20active%20reconstruction%20strategy%20to%20explore%20the%0Aunobserved%20areas%20and%20improve%20the%20reconstruction%20geometric%20and%20textural%20quality.%0ATo%20establish%20a%20complete%20robot%20active%20reconstruction%20system%2C%20we%20choose%20quadrotor%0Aas%20the%20robotic%20platform%20for%20its%20high%20agility.%20Then%20we%20devise%20a%20safety%0Aconstraint%20with%203DGS%20to%20generate%20executable%20trajectories%20for%20quadrotor%0Anavigation%20in%20the%203DGS%20map.%20To%20validate%20the%20effectiveness%20of%20our%20method%2C%20we%0Aconduct%20extensive%20experiments%20and%20ablation%20studies%20in%20highly%20realistic%0Asimulation%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Planner%253A%2520A%2520Gaussian-Splatting-based%2520Planning%2520Framework%2520for%2520Active%250A%2520%2520High-Fidelity%2520Reconstruction%26entry.906535625%3DRui%2520Jin%2520and%2520Yuman%2520Gao%2520and%2520Haojian%2520Lu%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520Active%2520reconstruction%2520technique%2520enables%2520robots%2520to%2520autonomously%2520collect%2520scene%250Adata%2520for%2520full%2520coverage%252C%2520relieving%2520users%2520from%2520tedious%2520and%2520time-consuming%2520data%250Acapturing%2520process.%2520However%252C%2520designed%2520based%2520on%2520unsuitable%2520scene%2520representations%252C%250Aexisting%2520methods%2520show%2520unrealistic%2520reconstruction%2520results%2520or%2520the%2520inability%2520of%250Aonline%2520quality%2520evaluation.%2520Due%2520to%2520the%2520recent%2520advancements%2520in%2520explicit%2520radiance%250Afield%2520technology%252C%2520online%2520active%2520high-fidelity%2520reconstruction%2520has%2520become%250Aachievable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GS-Planner%252C%2520a%2520planning%2520framework%2520for%250Aactive%2520high-fidelity%2520reconstruction%2520using%25203D%2520Gaussian%2520Splatting.%2520With%250Aimprovement%2520on%25203DGS%2520to%2520recognize%2520unobserved%2520regions%252C%2520we%2520evaluate%2520the%250Areconstruction%2520quality%2520and%2520completeness%2520of%25203DGS%2520map%2520online%2520to%2520guide%2520the%2520robot.%250AThen%2520we%2520design%2520a%2520sampling-based%2520active%2520reconstruction%2520strategy%2520to%2520explore%2520the%250Aunobserved%2520areas%2520and%2520improve%2520the%2520reconstruction%2520geometric%2520and%2520textural%2520quality.%250ATo%2520establish%2520a%2520complete%2520robot%2520active%2520reconstruction%2520system%252C%2520we%2520choose%2520quadrotor%250Aas%2520the%2520robotic%2520platform%2520for%2520its%2520high%2520agility.%2520Then%2520we%2520devise%2520a%2520safety%250Aconstraint%2520with%25203DGS%2520to%2520generate%2520executable%2520trajectories%2520for%2520quadrotor%250Anavigation%2520in%2520the%25203DGS%2520map.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%250Aconduct%2520extensive%2520experiments%2520and%2520ablation%2520studies%2520in%2520highly%2520realistic%250Asimulation%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Planner%3A%20A%20Gaussian-Splatting-based%20Planning%20Framework%20for%20Active%0A%20%20High-Fidelity%20Reconstruction&entry.906535625=Rui%20Jin%20and%20Yuman%20Gao%20and%20Haojian%20Lu%20and%20Fei%20Gao&entry.1292438233=%20%20Active%20reconstruction%20technique%20enables%20robots%20to%20autonomously%20collect%20scene%0Adata%20for%20full%20coverage%2C%20relieving%20users%20from%20tedious%20and%20time-consuming%20data%0Acapturing%20process.%20However%2C%20designed%20based%20on%20unsuitable%20scene%20representations%2C%0Aexisting%20methods%20show%20unrealistic%20reconstruction%20results%20or%20the%20inability%20of%0Aonline%20quality%20evaluation.%20Due%20to%20the%20recent%20advancements%20in%20explicit%20radiance%0Afield%20technology%2C%20online%20active%20high-fidelity%20reconstruction%20has%20become%0Aachievable.%20In%20this%20paper%2C%20we%20propose%20GS-Planner%2C%20a%20planning%20framework%20for%0Aactive%20high-fidelity%20reconstruction%20using%203D%20Gaussian%20Splatting.%20With%0Aimprovement%20on%203DGS%20to%20recognize%20unobserved%20regions%2C%20we%20evaluate%20the%0Areconstruction%20quality%20and%20completeness%20of%203DGS%20map%20online%20to%20guide%20the%20robot.%0AThen%20we%20design%20a%20sampling-based%20active%20reconstruction%20strategy%20to%20explore%20the%0Aunobserved%20areas%20and%20improve%20the%20reconstruction%20geometric%20and%20textural%20quality.%0ATo%20establish%20a%20complete%20robot%20active%20reconstruction%20system%2C%20we%20choose%20quadrotor%0Aas%20the%20robotic%20platform%20for%20its%20high%20agility.%20Then%20we%20devise%20a%20safety%0Aconstraint%20with%203DGS%20to%20generate%20executable%20trajectories%20for%20quadrotor%0Anavigation%20in%20the%203DGS%20map.%20To%20validate%20the%20effectiveness%20of%20our%20method%2C%20we%0Aconduct%20extensive%20experiments%20and%20ablation%20studies%20in%20highly%20realistic%0Asimulation%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10142v1&entry.124074799=Read"},
{"title": "SpecDETR: A Transformer-based Hyperspectral Point Object Detection\n  Network", "author": "Zhaoxu Li and Wei An and Gaowei Guo and Longguang Wang and Yingqian Wang and Zaiping Lin", "abstract": "  Hyperspectral target detection (HTD) aims to identify specific materials\nbased on spectral information in hyperspectral imagery and can detect point\ntargets, some of which occupy a smaller than one-pixel area. However, existing\nHTD methods are developed based on per-pixel binary classification, which\nlimits the feature representation capability for point targets. In this paper,\nwe rethink the hyperspectral point target detection from the object detection\nperspective, and focus more on the object-level prediction capability rather\nthan the pixel classification capability. Inspired by the token-based\nprocessing flow of Detection Transformer (DETR), we propose the first\nspecialized network for hyperspectral multi-class point object detection,\nSpecDETR. Without the backbone part of the current object detection framework,\nSpecDETR treats the spectral features of each pixel in hyperspectral images as\na token and utilizes a multi-layer Transformer encoder with local and global\ncoordination attention modules to extract deep spatial-spectral joint features.\nSpecDETR regards point object detection as a one-to-many set prediction\nproblem, thereby achieving a concise and efficient DETR decoder that surpasses\nthe current state-of-the-art DETR decoder in terms of parameters and accuracy\nin point object detection. We develop a simulated hyperSpectral Point Object\nDetection benchmark termed SPOD, and for the first time, evaluate and compare\nthe performance of current object detection networks and HTD methods on\nhyperspectral multi-class point object detection. SpecDETR demonstrates\nsuperior performance as compared to current object detection networks and HTD\nmethods on the SPOD dataset. Additionally, we validate on a public HTD dataset\nthat by using data simulation instead of manual annotation, SpecDETR can detect\nreal-world single-spectral point objects directly.\n", "link": "http://arxiv.org/abs/2405.10148v1", "date": "2024-05-16", "relevancy": 2.5907, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecDETR%3A%20A%20Transformer-based%20Hyperspectral%20Point%20Object%20Detection%0A%20%20Network&body=Title%3A%20SpecDETR%3A%20A%20Transformer-based%20Hyperspectral%20Point%20Object%20Detection%0A%20%20Network%0AAuthor%3A%20Zhaoxu%20Li%20and%20Wei%20An%20and%20Gaowei%20Guo%20and%20Longguang%20Wang%20and%20Yingqian%20Wang%20and%20Zaiping%20Lin%0AAbstract%3A%20%20%20Hyperspectral%20target%20detection%20%28HTD%29%20aims%20to%20identify%20specific%20materials%0Abased%20on%20spectral%20information%20in%20hyperspectral%20imagery%20and%20can%20detect%20point%0Atargets%2C%20some%20of%20which%20occupy%20a%20smaller%20than%20one-pixel%20area.%20However%2C%20existing%0AHTD%20methods%20are%20developed%20based%20on%20per-pixel%20binary%20classification%2C%20which%0Alimits%20the%20feature%20representation%20capability%20for%20point%20targets.%20In%20this%20paper%2C%0Awe%20rethink%20the%20hyperspectral%20point%20target%20detection%20from%20the%20object%20detection%0Aperspective%2C%20and%20focus%20more%20on%20the%20object-level%20prediction%20capability%20rather%0Athan%20the%20pixel%20classification%20capability.%20Inspired%20by%20the%20token-based%0Aprocessing%20flow%20of%20Detection%20Transformer%20%28DETR%29%2C%20we%20propose%20the%20first%0Aspecialized%20network%20for%20hyperspectral%20multi-class%20point%20object%20detection%2C%0ASpecDETR.%20Without%20the%20backbone%20part%20of%20the%20current%20object%20detection%20framework%2C%0ASpecDETR%20treats%20the%20spectral%20features%20of%20each%20pixel%20in%20hyperspectral%20images%20as%0Aa%20token%20and%20utilizes%20a%20multi-layer%20Transformer%20encoder%20with%20local%20and%20global%0Acoordination%20attention%20modules%20to%20extract%20deep%20spatial-spectral%20joint%20features.%0ASpecDETR%20regards%20point%20object%20detection%20as%20a%20one-to-many%20set%20prediction%0Aproblem%2C%20thereby%20achieving%20a%20concise%20and%20efficient%20DETR%20decoder%20that%20surpasses%0Athe%20current%20state-of-the-art%20DETR%20decoder%20in%20terms%20of%20parameters%20and%20accuracy%0Ain%20point%20object%20detection.%20We%20develop%20a%20simulated%20hyperSpectral%20Point%20Object%0ADetection%20benchmark%20termed%20SPOD%2C%20and%20for%20the%20first%20time%2C%20evaluate%20and%20compare%0Athe%20performance%20of%20current%20object%20detection%20networks%20and%20HTD%20methods%20on%0Ahyperspectral%20multi-class%20point%20object%20detection.%20SpecDETR%20demonstrates%0Asuperior%20performance%20as%20compared%20to%20current%20object%20detection%20networks%20and%20HTD%0Amethods%20on%20the%20SPOD%20dataset.%20Additionally%2C%20we%20validate%20on%20a%20public%20HTD%20dataset%0Athat%20by%20using%20data%20simulation%20instead%20of%20manual%20annotation%2C%20SpecDETR%20can%20detect%0Areal-world%20single-spectral%20point%20objects%20directly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecDETR%253A%2520A%2520Transformer-based%2520Hyperspectral%2520Point%2520Object%2520Detection%250A%2520%2520Network%26entry.906535625%3DZhaoxu%2520Li%2520and%2520Wei%2520An%2520and%2520Gaowei%2520Guo%2520and%2520Longguang%2520Wang%2520and%2520Yingqian%2520Wang%2520and%2520Zaiping%2520Lin%26entry.1292438233%3D%2520%2520Hyperspectral%2520target%2520detection%2520%2528HTD%2529%2520aims%2520to%2520identify%2520specific%2520materials%250Abased%2520on%2520spectral%2520information%2520in%2520hyperspectral%2520imagery%2520and%2520can%2520detect%2520point%250Atargets%252C%2520some%2520of%2520which%2520occupy%2520a%2520smaller%2520than%2520one-pixel%2520area.%2520However%252C%2520existing%250AHTD%2520methods%2520are%2520developed%2520based%2520on%2520per-pixel%2520binary%2520classification%252C%2520which%250Alimits%2520the%2520feature%2520representation%2520capability%2520for%2520point%2520targets.%2520In%2520this%2520paper%252C%250Awe%2520rethink%2520the%2520hyperspectral%2520point%2520target%2520detection%2520from%2520the%2520object%2520detection%250Aperspective%252C%2520and%2520focus%2520more%2520on%2520the%2520object-level%2520prediction%2520capability%2520rather%250Athan%2520the%2520pixel%2520classification%2520capability.%2520Inspired%2520by%2520the%2520token-based%250Aprocessing%2520flow%2520of%2520Detection%2520Transformer%2520%2528DETR%2529%252C%2520we%2520propose%2520the%2520first%250Aspecialized%2520network%2520for%2520hyperspectral%2520multi-class%2520point%2520object%2520detection%252C%250ASpecDETR.%2520Without%2520the%2520backbone%2520part%2520of%2520the%2520current%2520object%2520detection%2520framework%252C%250ASpecDETR%2520treats%2520the%2520spectral%2520features%2520of%2520each%2520pixel%2520in%2520hyperspectral%2520images%2520as%250Aa%2520token%2520and%2520utilizes%2520a%2520multi-layer%2520Transformer%2520encoder%2520with%2520local%2520and%2520global%250Acoordination%2520attention%2520modules%2520to%2520extract%2520deep%2520spatial-spectral%2520joint%2520features.%250ASpecDETR%2520regards%2520point%2520object%2520detection%2520as%2520a%2520one-to-many%2520set%2520prediction%250Aproblem%252C%2520thereby%2520achieving%2520a%2520concise%2520and%2520efficient%2520DETR%2520decoder%2520that%2520surpasses%250Athe%2520current%2520state-of-the-art%2520DETR%2520decoder%2520in%2520terms%2520of%2520parameters%2520and%2520accuracy%250Ain%2520point%2520object%2520detection.%2520We%2520develop%2520a%2520simulated%2520hyperSpectral%2520Point%2520Object%250ADetection%2520benchmark%2520termed%2520SPOD%252C%2520and%2520for%2520the%2520first%2520time%252C%2520evaluate%2520and%2520compare%250Athe%2520performance%2520of%2520current%2520object%2520detection%2520networks%2520and%2520HTD%2520methods%2520on%250Ahyperspectral%2520multi-class%2520point%2520object%2520detection.%2520SpecDETR%2520demonstrates%250Asuperior%2520performance%2520as%2520compared%2520to%2520current%2520object%2520detection%2520networks%2520and%2520HTD%250Amethods%2520on%2520the%2520SPOD%2520dataset.%2520Additionally%252C%2520we%2520validate%2520on%2520a%2520public%2520HTD%2520dataset%250Athat%2520by%2520using%2520data%2520simulation%2520instead%2520of%2520manual%2520annotation%252C%2520SpecDETR%2520can%2520detect%250Areal-world%2520single-spectral%2520point%2520objects%2520directly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecDETR%3A%20A%20Transformer-based%20Hyperspectral%20Point%20Object%20Detection%0A%20%20Network&entry.906535625=Zhaoxu%20Li%20and%20Wei%20An%20and%20Gaowei%20Guo%20and%20Longguang%20Wang%20and%20Yingqian%20Wang%20and%20Zaiping%20Lin&entry.1292438233=%20%20Hyperspectral%20target%20detection%20%28HTD%29%20aims%20to%20identify%20specific%20materials%0Abased%20on%20spectral%20information%20in%20hyperspectral%20imagery%20and%20can%20detect%20point%0Atargets%2C%20some%20of%20which%20occupy%20a%20smaller%20than%20one-pixel%20area.%20However%2C%20existing%0AHTD%20methods%20are%20developed%20based%20on%20per-pixel%20binary%20classification%2C%20which%0Alimits%20the%20feature%20representation%20capability%20for%20point%20targets.%20In%20this%20paper%2C%0Awe%20rethink%20the%20hyperspectral%20point%20target%20detection%20from%20the%20object%20detection%0Aperspective%2C%20and%20focus%20more%20on%20the%20object-level%20prediction%20capability%20rather%0Athan%20the%20pixel%20classification%20capability.%20Inspired%20by%20the%20token-based%0Aprocessing%20flow%20of%20Detection%20Transformer%20%28DETR%29%2C%20we%20propose%20the%20first%0Aspecialized%20network%20for%20hyperspectral%20multi-class%20point%20object%20detection%2C%0ASpecDETR.%20Without%20the%20backbone%20part%20of%20the%20current%20object%20detection%20framework%2C%0ASpecDETR%20treats%20the%20spectral%20features%20of%20each%20pixel%20in%20hyperspectral%20images%20as%0Aa%20token%20and%20utilizes%20a%20multi-layer%20Transformer%20encoder%20with%20local%20and%20global%0Acoordination%20attention%20modules%20to%20extract%20deep%20spatial-spectral%20joint%20features.%0ASpecDETR%20regards%20point%20object%20detection%20as%20a%20one-to-many%20set%20prediction%0Aproblem%2C%20thereby%20achieving%20a%20concise%20and%20efficient%20DETR%20decoder%20that%20surpasses%0Athe%20current%20state-of-the-art%20DETR%20decoder%20in%20terms%20of%20parameters%20and%20accuracy%0Ain%20point%20object%20detection.%20We%20develop%20a%20simulated%20hyperSpectral%20Point%20Object%0ADetection%20benchmark%20termed%20SPOD%2C%20and%20for%20the%20first%20time%2C%20evaluate%20and%20compare%0Athe%20performance%20of%20current%20object%20detection%20networks%20and%20HTD%20methods%20on%0Ahyperspectral%20multi-class%20point%20object%20detection.%20SpecDETR%20demonstrates%0Asuperior%20performance%20as%20compared%20to%20current%20object%20detection%20networks%20and%20HTD%0Amethods%20on%20the%20SPOD%20dataset.%20Additionally%2C%20we%20validate%20on%20a%20public%20HTD%20dataset%0Athat%20by%20using%20data%20simulation%20instead%20of%20manual%20annotation%2C%20SpecDETR%20can%20detect%0Areal-world%20single-spectral%20point%20objects%20directly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10148v1&entry.124074799=Read"},
{"title": "Libra: Building Decoupled Vision System on Large Language Models", "author": "Yifan Xu and Xiaoshan Yang and Yaguang Song and Changsheng Xu", "abstract": "  In this work, we introduce Libra, a prototype model with a decoupled vision\nsystem on a large language model (LLM). The decoupled vision system decouples\ninner-modal modeling and cross-modal interaction, yielding unique visual\ninformation modeling and effective cross-modal comprehension. Libra is trained\nthrough discrete auto-regressive modeling on both vision and language inputs.\nSpecifically, we incorporate a routed visual expert with a cross-modal bridge\nmodule into a pretrained LLM to route the vision and language flows during\nattention computing to enable different attention patterns in inner-modal\nmodeling and cross-modal interaction scenarios. Experimental results\ndemonstrate that the dedicated design of Libra achieves a strong MLLM baseline\nthat rivals existing works in the image-to-text scenario with merely 50 million\ntraining data, providing a new perspective for future multimodal foundation\nmodels. Code is available at https://github.com/YifanXu74/Libra.\n", "link": "http://arxiv.org/abs/2405.10140v1", "date": "2024-05-16", "relevancy": 2.5579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.514}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Libra%3A%20Building%20Decoupled%20Vision%20System%20on%20Large%20Language%20Models&body=Title%3A%20Libra%3A%20Building%20Decoupled%20Vision%20System%20on%20Large%20Language%20Models%0AAuthor%3A%20Yifan%20Xu%20and%20Xiaoshan%20Yang%20and%20Yaguang%20Song%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Libra%2C%20a%20prototype%20model%20with%20a%20decoupled%20vision%0Asystem%20on%20a%20large%20language%20model%20%28LLM%29.%20The%20decoupled%20vision%20system%20decouples%0Ainner-modal%20modeling%20and%20cross-modal%20interaction%2C%20yielding%20unique%20visual%0Ainformation%20modeling%20and%20effective%20cross-modal%20comprehension.%20Libra%20is%20trained%0Athrough%20discrete%20auto-regressive%20modeling%20on%20both%20vision%20and%20language%20inputs.%0ASpecifically%2C%20we%20incorporate%20a%20routed%20visual%20expert%20with%20a%20cross-modal%20bridge%0Amodule%20into%20a%20pretrained%20LLM%20to%20route%20the%20vision%20and%20language%20flows%20during%0Aattention%20computing%20to%20enable%20different%20attention%20patterns%20in%20inner-modal%0Amodeling%20and%20cross-modal%20interaction%20scenarios.%20Experimental%20results%0Ademonstrate%20that%20the%20dedicated%20design%20of%20Libra%20achieves%20a%20strong%20MLLM%20baseline%0Athat%20rivals%20existing%20works%20in%20the%20image-to-text%20scenario%20with%20merely%2050%20million%0Atraining%20data%2C%20providing%20a%20new%20perspective%20for%20future%20multimodal%20foundation%0Amodels.%20Code%20is%20available%20at%20https%3A//github.com/YifanXu74/Libra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLibra%253A%2520Building%2520Decoupled%2520Vision%2520System%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DYifan%2520Xu%2520and%2520Xiaoshan%2520Yang%2520and%2520Yaguang%2520Song%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Libra%252C%2520a%2520prototype%2520model%2520with%2520a%2520decoupled%2520vision%250Asystem%2520on%2520a%2520large%2520language%2520model%2520%2528LLM%2529.%2520The%2520decoupled%2520vision%2520system%2520decouples%250Ainner-modal%2520modeling%2520and%2520cross-modal%2520interaction%252C%2520yielding%2520unique%2520visual%250Ainformation%2520modeling%2520and%2520effective%2520cross-modal%2520comprehension.%2520Libra%2520is%2520trained%250Athrough%2520discrete%2520auto-regressive%2520modeling%2520on%2520both%2520vision%2520and%2520language%2520inputs.%250ASpecifically%252C%2520we%2520incorporate%2520a%2520routed%2520visual%2520expert%2520with%2520a%2520cross-modal%2520bridge%250Amodule%2520into%2520a%2520pretrained%2520LLM%2520to%2520route%2520the%2520vision%2520and%2520language%2520flows%2520during%250Aattention%2520computing%2520to%2520enable%2520different%2520attention%2520patterns%2520in%2520inner-modal%250Amodeling%2520and%2520cross-modal%2520interaction%2520scenarios.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520dedicated%2520design%2520of%2520Libra%2520achieves%2520a%2520strong%2520MLLM%2520baseline%250Athat%2520rivals%2520existing%2520works%2520in%2520the%2520image-to-text%2520scenario%2520with%2520merely%252050%2520million%250Atraining%2520data%252C%2520providing%2520a%2520new%2520perspective%2520for%2520future%2520multimodal%2520foundation%250Amodels.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/YifanXu74/Libra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Libra%3A%20Building%20Decoupled%20Vision%20System%20on%20Large%20Language%20Models&entry.906535625=Yifan%20Xu%20and%20Xiaoshan%20Yang%20and%20Yaguang%20Song%20and%20Changsheng%20Xu&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Libra%2C%20a%20prototype%20model%20with%20a%20decoupled%20vision%0Asystem%20on%20a%20large%20language%20model%20%28LLM%29.%20The%20decoupled%20vision%20system%20decouples%0Ainner-modal%20modeling%20and%20cross-modal%20interaction%2C%20yielding%20unique%20visual%0Ainformation%20modeling%20and%20effective%20cross-modal%20comprehension.%20Libra%20is%20trained%0Athrough%20discrete%20auto-regressive%20modeling%20on%20both%20vision%20and%20language%20inputs.%0ASpecifically%2C%20we%20incorporate%20a%20routed%20visual%20expert%20with%20a%20cross-modal%20bridge%0Amodule%20into%20a%20pretrained%20LLM%20to%20route%20the%20vision%20and%20language%20flows%20during%0Aattention%20computing%20to%20enable%20different%20attention%20patterns%20in%20inner-modal%0Amodeling%20and%20cross-modal%20interaction%20scenarios.%20Experimental%20results%0Ademonstrate%20that%20the%20dedicated%20design%20of%20Libra%20achieves%20a%20strong%20MLLM%20baseline%0Athat%20rivals%20existing%20works%20in%20the%20image-to-text%20scenario%20with%20merely%2050%20million%0Atraining%20data%2C%20providing%20a%20new%20perspective%20for%20future%20multimodal%20foundation%0Amodels.%20Code%20is%20available%20at%20https%3A//github.com/YifanXu74/Libra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10140v1&entry.124074799=Read"},
{"title": "ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph\n  Neural Networks", "author": "Zhehan Zhao and Lu Bai and Lixin Cui and Ming Li and Yue Wang and Lixiang Xu and Edwin R. Hancock", "abstract": "  Graph Neural Networks (GNNs) are powerful tools for graph classification. One\nimportant operation for GNNs is the downsampling or pooling that can learn\neffective embeddings from the node representations. In this paper, we propose a\nnew hierarchical pooling operation, namely the Edge-Node Attention-based\nDifferentiable Pooling (ENADPool), for GNNs to learn effective graph\nrepresentations. Unlike the classical hierarchical pooling operation that is\nbased on the unclear node assignment and simply computes the averaged feature\nover the nodes of each cluster, the proposed ENADPool not only employs a hard\nclustering strategy to assign each node into an unique cluster, but also\ncompress the node features as well as their edge connectivity strengths into\nthe resulting hierarchical structure based on the attention mechanism after\neach pooling step. As a result, the proposed ENADPool simultaneously identifies\nthe importance of different nodes within each separated cluster and edges\nbetween corresponding clusters, that significantly addresses the shortcomings\nof the uniform edge-node based structure information aggregation arising in the\nclassical hierarchical pooling operation. Moreover, to mitigate the\nover-smoothing problem arising in existing GNNs, we propose a Multi-distance\nGNN (MD-GNN) model associated with the proposed ENADPool operation, allowing\nthe nodes to actively and directly receive the feature information from\nneighbors at different random walk steps. Experiments demonstrate the\neffectiveness of the MD-GNN associated with the proposed ENADPool.\n", "link": "http://arxiv.org/abs/2405.10218v1", "date": "2024-05-16", "relevancy": 2.5548, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4871}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENADPool%3A%20The%20Edge-Node%20Attention-based%20Differentiable%20Pooling%20for%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20ENADPool%3A%20The%20Edge-Node%20Attention-based%20Differentiable%20Pooling%20for%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Zhehan%20Zhao%20and%20Lu%20Bai%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Lixiang%20Xu%20and%20Edwin%20R.%20Hancock%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20tools%20for%20graph%20classification.%20One%0Aimportant%20operation%20for%20GNNs%20is%20the%20downsampling%20or%20pooling%20that%20can%20learn%0Aeffective%20embeddings%20from%20the%20node%20representations.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20hierarchical%20pooling%20operation%2C%20namely%20the%20Edge-Node%20Attention-based%0ADifferentiable%20Pooling%20%28ENADPool%29%2C%20for%20GNNs%20to%20learn%20effective%20graph%0Arepresentations.%20Unlike%20the%20classical%20hierarchical%20pooling%20operation%20that%20is%0Abased%20on%20the%20unclear%20node%20assignment%20and%20simply%20computes%20the%20averaged%20feature%0Aover%20the%20nodes%20of%20each%20cluster%2C%20the%20proposed%20ENADPool%20not%20only%20employs%20a%20hard%0Aclustering%20strategy%20to%20assign%20each%20node%20into%20an%20unique%20cluster%2C%20but%20also%0Acompress%20the%20node%20features%20as%20well%20as%20their%20edge%20connectivity%20strengths%20into%0Athe%20resulting%20hierarchical%20structure%20based%20on%20the%20attention%20mechanism%20after%0Aeach%20pooling%20step.%20As%20a%20result%2C%20the%20proposed%20ENADPool%20simultaneously%20identifies%0Athe%20importance%20of%20different%20nodes%20within%20each%20separated%20cluster%20and%20edges%0Abetween%20corresponding%20clusters%2C%20that%20significantly%20addresses%20the%20shortcomings%0Aof%20the%20uniform%20edge-node%20based%20structure%20information%20aggregation%20arising%20in%20the%0Aclassical%20hierarchical%20pooling%20operation.%20Moreover%2C%20to%20mitigate%20the%0Aover-smoothing%20problem%20arising%20in%20existing%20GNNs%2C%20we%20propose%20a%20Multi-distance%0AGNN%20%28MD-GNN%29%20model%20associated%20with%20the%20proposed%20ENADPool%20operation%2C%20allowing%0Athe%20nodes%20to%20actively%20and%20directly%20receive%20the%20feature%20information%20from%0Aneighbors%20at%20different%20random%20walk%20steps.%20Experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20MD-GNN%20associated%20with%20the%20proposed%20ENADPool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENADPool%253A%2520The%2520Edge-Node%2520Attention-based%2520Differentiable%2520Pooling%2520for%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DZhehan%2520Zhao%2520and%2520Lu%2520Bai%2520and%2520Lixin%2520Cui%2520and%2520Ming%2520Li%2520and%2520Yue%2520Wang%2520and%2520Lixiang%2520Xu%2520and%2520Edwin%2520R.%2520Hancock%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520powerful%2520tools%2520for%2520graph%2520classification.%2520One%250Aimportant%2520operation%2520for%2520GNNs%2520is%2520the%2520downsampling%2520or%2520pooling%2520that%2520can%2520learn%250Aeffective%2520embeddings%2520from%2520the%2520node%2520representations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anew%2520hierarchical%2520pooling%2520operation%252C%2520namely%2520the%2520Edge-Node%2520Attention-based%250ADifferentiable%2520Pooling%2520%2528ENADPool%2529%252C%2520for%2520GNNs%2520to%2520learn%2520effective%2520graph%250Arepresentations.%2520Unlike%2520the%2520classical%2520hierarchical%2520pooling%2520operation%2520that%2520is%250Abased%2520on%2520the%2520unclear%2520node%2520assignment%2520and%2520simply%2520computes%2520the%2520averaged%2520feature%250Aover%2520the%2520nodes%2520of%2520each%2520cluster%252C%2520the%2520proposed%2520ENADPool%2520not%2520only%2520employs%2520a%2520hard%250Aclustering%2520strategy%2520to%2520assign%2520each%2520node%2520into%2520an%2520unique%2520cluster%252C%2520but%2520also%250Acompress%2520the%2520node%2520features%2520as%2520well%2520as%2520their%2520edge%2520connectivity%2520strengths%2520into%250Athe%2520resulting%2520hierarchical%2520structure%2520based%2520on%2520the%2520attention%2520mechanism%2520after%250Aeach%2520pooling%2520step.%2520As%2520a%2520result%252C%2520the%2520proposed%2520ENADPool%2520simultaneously%2520identifies%250Athe%2520importance%2520of%2520different%2520nodes%2520within%2520each%2520separated%2520cluster%2520and%2520edges%250Abetween%2520corresponding%2520clusters%252C%2520that%2520significantly%2520addresses%2520the%2520shortcomings%250Aof%2520the%2520uniform%2520edge-node%2520based%2520structure%2520information%2520aggregation%2520arising%2520in%2520the%250Aclassical%2520hierarchical%2520pooling%2520operation.%2520Moreover%252C%2520to%2520mitigate%2520the%250Aover-smoothing%2520problem%2520arising%2520in%2520existing%2520GNNs%252C%2520we%2520propose%2520a%2520Multi-distance%250AGNN%2520%2528MD-GNN%2529%2520model%2520associated%2520with%2520the%2520proposed%2520ENADPool%2520operation%252C%2520allowing%250Athe%2520nodes%2520to%2520actively%2520and%2520directly%2520receive%2520the%2520feature%2520information%2520from%250Aneighbors%2520at%2520different%2520random%2520walk%2520steps.%2520Experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520MD-GNN%2520associated%2520with%2520the%2520proposed%2520ENADPool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENADPool%3A%20The%20Edge-Node%20Attention-based%20Differentiable%20Pooling%20for%20Graph%0A%20%20Neural%20Networks&entry.906535625=Zhehan%20Zhao%20and%20Lu%20Bai%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Lixiang%20Xu%20and%20Edwin%20R.%20Hancock&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20tools%20for%20graph%20classification.%20One%0Aimportant%20operation%20for%20GNNs%20is%20the%20downsampling%20or%20pooling%20that%20can%20learn%0Aeffective%20embeddings%20from%20the%20node%20representations.%20In%20this%20paper%2C%20we%20propose%20a%0Anew%20hierarchical%20pooling%20operation%2C%20namely%20the%20Edge-Node%20Attention-based%0ADifferentiable%20Pooling%20%28ENADPool%29%2C%20for%20GNNs%20to%20learn%20effective%20graph%0Arepresentations.%20Unlike%20the%20classical%20hierarchical%20pooling%20operation%20that%20is%0Abased%20on%20the%20unclear%20node%20assignment%20and%20simply%20computes%20the%20averaged%20feature%0Aover%20the%20nodes%20of%20each%20cluster%2C%20the%20proposed%20ENADPool%20not%20only%20employs%20a%20hard%0Aclustering%20strategy%20to%20assign%20each%20node%20into%20an%20unique%20cluster%2C%20but%20also%0Acompress%20the%20node%20features%20as%20well%20as%20their%20edge%20connectivity%20strengths%20into%0Athe%20resulting%20hierarchical%20structure%20based%20on%20the%20attention%20mechanism%20after%0Aeach%20pooling%20step.%20As%20a%20result%2C%20the%20proposed%20ENADPool%20simultaneously%20identifies%0Athe%20importance%20of%20different%20nodes%20within%20each%20separated%20cluster%20and%20edges%0Abetween%20corresponding%20clusters%2C%20that%20significantly%20addresses%20the%20shortcomings%0Aof%20the%20uniform%20edge-node%20based%20structure%20information%20aggregation%20arising%20in%20the%0Aclassical%20hierarchical%20pooling%20operation.%20Moreover%2C%20to%20mitigate%20the%0Aover-smoothing%20problem%20arising%20in%20existing%20GNNs%2C%20we%20propose%20a%20Multi-distance%0AGNN%20%28MD-GNN%29%20model%20associated%20with%20the%20proposed%20ENADPool%20operation%2C%20allowing%0Athe%20nodes%20to%20actively%20and%20directly%20receive%20the%20feature%20information%20from%0Aneighbors%20at%20different%20random%20walk%20steps.%20Experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20MD-GNN%20associated%20with%20the%20proposed%20ENADPool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10218v1&entry.124074799=Read"},
{"title": "NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic\n  environments", "author": "Ziheng Xu and Jianwei Niu and Qingfeng Li and Tao Ren and Chen Chen", "abstract": "  Neural implicit representations have been explored to enhance visual SLAM\nalgorithms, especially in providing high-fidelity dense map. Existing methods\noperate robustly in static scenes but struggle with the disruption caused by\nmoving objects. In this paper we present NID-SLAM, which significantly improves\nthe performance of neural SLAM in dynamic environments. We propose a new\napproach to enhance inaccurate regions in semantic masks, particularly in\nmarginal areas. Utilizing the geometric information present in depth images,\nthis method enables accurate removal of dynamic objects, thereby reducing the\nprobability of camera drift. Additionally, we introduce a keyframe selection\nstrategy for dynamic scenes, which enhances camera tracking robustness against\nlarge-scale objects and improves the efficiency of mapping. Experiments on\npublicly available RGB-D datasets demonstrate that our method outperforms\ncompetitive neural SLAM approaches in tracking accuracy and mapping quality in\ndynamic environments.\n", "link": "http://arxiv.org/abs/2401.01189v2", "date": "2024-05-16", "relevancy": 2.5473, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NID-SLAM%3A%20Neural%20Implicit%20Representation-based%20RGB-D%20SLAM%20in%20dynamic%0A%20%20environments&body=Title%3A%20NID-SLAM%3A%20Neural%20Implicit%20Representation-based%20RGB-D%20SLAM%20in%20dynamic%0A%20%20environments%0AAuthor%3A%20Ziheng%20Xu%20and%20Jianwei%20Niu%20and%20Qingfeng%20Li%20and%20Tao%20Ren%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20have%20been%20explored%20to%20enhance%20visual%20SLAM%0Aalgorithms%2C%20especially%20in%20providing%20high-fidelity%20dense%20map.%20Existing%20methods%0Aoperate%20robustly%20in%20static%20scenes%20but%20struggle%20with%20the%20disruption%20caused%20by%0Amoving%20objects.%20In%20this%20paper%20we%20present%20NID-SLAM%2C%20which%20significantly%20improves%0Athe%20performance%20of%20neural%20SLAM%20in%20dynamic%20environments.%20We%20propose%20a%20new%0Aapproach%20to%20enhance%20inaccurate%20regions%20in%20semantic%20masks%2C%20particularly%20in%0Amarginal%20areas.%20Utilizing%20the%20geometric%20information%20present%20in%20depth%20images%2C%0Athis%20method%20enables%20accurate%20removal%20of%20dynamic%20objects%2C%20thereby%20reducing%20the%0Aprobability%20of%20camera%20drift.%20Additionally%2C%20we%20introduce%20a%20keyframe%20selection%0Astrategy%20for%20dynamic%20scenes%2C%20which%20enhances%20camera%20tracking%20robustness%20against%0Alarge-scale%20objects%20and%20improves%20the%20efficiency%20of%20mapping.%20Experiments%20on%0Apublicly%20available%20RGB-D%20datasets%20demonstrate%20that%20our%20method%20outperforms%0Acompetitive%20neural%20SLAM%20approaches%20in%20tracking%20accuracy%20and%20mapping%20quality%20in%0Adynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNID-SLAM%253A%2520Neural%2520Implicit%2520Representation-based%2520RGB-D%2520SLAM%2520in%2520dynamic%250A%2520%2520environments%26entry.906535625%3DZiheng%2520Xu%2520and%2520Jianwei%2520Niu%2520and%2520Qingfeng%2520Li%2520and%2520Tao%2520Ren%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520have%2520been%2520explored%2520to%2520enhance%2520visual%2520SLAM%250Aalgorithms%252C%2520especially%2520in%2520providing%2520high-fidelity%2520dense%2520map.%2520Existing%2520methods%250Aoperate%2520robustly%2520in%2520static%2520scenes%2520but%2520struggle%2520with%2520the%2520disruption%2520caused%2520by%250Amoving%2520objects.%2520In%2520this%2520paper%2520we%2520present%2520NID-SLAM%252C%2520which%2520significantly%2520improves%250Athe%2520performance%2520of%2520neural%2520SLAM%2520in%2520dynamic%2520environments.%2520We%2520propose%2520a%2520new%250Aapproach%2520to%2520enhance%2520inaccurate%2520regions%2520in%2520semantic%2520masks%252C%2520particularly%2520in%250Amarginal%2520areas.%2520Utilizing%2520the%2520geometric%2520information%2520present%2520in%2520depth%2520images%252C%250Athis%2520method%2520enables%2520accurate%2520removal%2520of%2520dynamic%2520objects%252C%2520thereby%2520reducing%2520the%250Aprobability%2520of%2520camera%2520drift.%2520Additionally%252C%2520we%2520introduce%2520a%2520keyframe%2520selection%250Astrategy%2520for%2520dynamic%2520scenes%252C%2520which%2520enhances%2520camera%2520tracking%2520robustness%2520against%250Alarge-scale%2520objects%2520and%2520improves%2520the%2520efficiency%2520of%2520mapping.%2520Experiments%2520on%250Apublicly%2520available%2520RGB-D%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Acompetitive%2520neural%2520SLAM%2520approaches%2520in%2520tracking%2520accuracy%2520and%2520mapping%2520quality%2520in%250Adynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NID-SLAM%3A%20Neural%20Implicit%20Representation-based%20RGB-D%20SLAM%20in%20dynamic%0A%20%20environments&entry.906535625=Ziheng%20Xu%20and%20Jianwei%20Niu%20and%20Qingfeng%20Li%20and%20Tao%20Ren%20and%20Chen%20Chen&entry.1292438233=%20%20Neural%20implicit%20representations%20have%20been%20explored%20to%20enhance%20visual%20SLAM%0Aalgorithms%2C%20especially%20in%20providing%20high-fidelity%20dense%20map.%20Existing%20methods%0Aoperate%20robustly%20in%20static%20scenes%20but%20struggle%20with%20the%20disruption%20caused%20by%0Amoving%20objects.%20In%20this%20paper%20we%20present%20NID-SLAM%2C%20which%20significantly%20improves%0Athe%20performance%20of%20neural%20SLAM%20in%20dynamic%20environments.%20We%20propose%20a%20new%0Aapproach%20to%20enhance%20inaccurate%20regions%20in%20semantic%20masks%2C%20particularly%20in%0Amarginal%20areas.%20Utilizing%20the%20geometric%20information%20present%20in%20depth%20images%2C%0Athis%20method%20enables%20accurate%20removal%20of%20dynamic%20objects%2C%20thereby%20reducing%20the%0Aprobability%20of%20camera%20drift.%20Additionally%2C%20we%20introduce%20a%20keyframe%20selection%0Astrategy%20for%20dynamic%20scenes%2C%20which%20enhances%20camera%20tracking%20robustness%20against%0Alarge-scale%20objects%20and%20improves%20the%20efficiency%20of%20mapping.%20Experiments%20on%0Apublicly%20available%20RGB-D%20datasets%20demonstrate%20that%20our%20method%20outperforms%0Acompetitive%20neural%20SLAM%20approaches%20in%20tracking%20accuracy%20and%20mapping%20quality%20in%0Adynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01189v2&entry.124074799=Read"},
{"title": "A Tale of Two Languages: Large-Vocabulary Continuous Sign Language\n  Recognition from Spoken Language Supervision", "author": "Charles Raude and K R Prajwal and Liliane Momeni and Hannah Bull and Samuel Albanie and Andrew Zisserman and G\u00fcl Varol", "abstract": "  In this work, our goals are two fold: large-vocabulary continuous sign\nlanguage recognition (CSLR), and sign language retrieval. To this end, we\nintroduce a multi-task Transformer model, CSLR2, that is able to ingest a\nsigning sequence and output in a joint embedding space between signed language\nand spoken language text. To enable CSLR evaluation in the large-vocabulary\nsetting, we introduce new dataset annotations that have been manually\ncollected. These provide continuous sign-level annotations for six hours of\ntest videos, and will be made publicly available. We demonstrate that by a\ncareful choice of loss functions, training the model for both the CSLR and\nretrieval tasks is mutually beneficial in terms of performance -- retrieval\nimproves CSLR performance by providing context, while CSLR improves retrieval\nwith more fine-grained supervision. We further show the benefits of leveraging\nweak and noisy supervision from large-vocabulary datasets such as BOBSL, namely\nsign-level pseudo-labels, and English subtitles. Our model significantly\noutperforms the previous state of the art on both tasks.\n", "link": "http://arxiv.org/abs/2405.10266v1", "date": "2024-05-16", "relevancy": 2.4882, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5255}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4887}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tale%20of%20Two%20Languages%3A%20Large-Vocabulary%20Continuous%20Sign%20Language%0A%20%20Recognition%20from%20Spoken%20Language%20Supervision&body=Title%3A%20A%20Tale%20of%20Two%20Languages%3A%20Large-Vocabulary%20Continuous%20Sign%20Language%0A%20%20Recognition%20from%20Spoken%20Language%20Supervision%0AAuthor%3A%20Charles%20Raude%20and%20K%20R%20Prajwal%20and%20Liliane%20Momeni%20and%20Hannah%20Bull%20and%20Samuel%20Albanie%20and%20Andrew%20Zisserman%20and%20G%C3%BCl%20Varol%0AAbstract%3A%20%20%20In%20this%20work%2C%20our%20goals%20are%20two%20fold%3A%20large-vocabulary%20continuous%20sign%0Alanguage%20recognition%20%28CSLR%29%2C%20and%20sign%20language%20retrieval.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-task%20Transformer%20model%2C%20CSLR2%2C%20that%20is%20able%20to%20ingest%20a%0Asigning%20sequence%20and%20output%20in%20a%20joint%20embedding%20space%20between%20signed%20language%0Aand%20spoken%20language%20text.%20To%20enable%20CSLR%20evaluation%20in%20the%20large-vocabulary%0Asetting%2C%20we%20introduce%20new%20dataset%20annotations%20that%20have%20been%20manually%0Acollected.%20These%20provide%20continuous%20sign-level%20annotations%20for%20six%20hours%20of%0Atest%20videos%2C%20and%20will%20be%20made%20publicly%20available.%20We%20demonstrate%20that%20by%20a%0Acareful%20choice%20of%20loss%20functions%2C%20training%20the%20model%20for%20both%20the%20CSLR%20and%0Aretrieval%20tasks%20is%20mutually%20beneficial%20in%20terms%20of%20performance%20--%20retrieval%0Aimproves%20CSLR%20performance%20by%20providing%20context%2C%20while%20CSLR%20improves%20retrieval%0Awith%20more%20fine-grained%20supervision.%20We%20further%20show%20the%20benefits%20of%20leveraging%0Aweak%20and%20noisy%20supervision%20from%20large-vocabulary%20datasets%20such%20as%20BOBSL%2C%20namely%0Asign-level%20pseudo-labels%2C%20and%20English%20subtitles.%20Our%20model%20significantly%0Aoutperforms%20the%20previous%20state%20of%20the%20art%20on%20both%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tale%2520of%2520Two%2520Languages%253A%2520Large-Vocabulary%2520Continuous%2520Sign%2520Language%250A%2520%2520Recognition%2520from%2520Spoken%2520Language%2520Supervision%26entry.906535625%3DCharles%2520Raude%2520and%2520K%2520R%2520Prajwal%2520and%2520Liliane%2520Momeni%2520and%2520Hannah%2520Bull%2520and%2520Samuel%2520Albanie%2520and%2520Andrew%2520Zisserman%2520and%2520G%25C3%25BCl%2520Varol%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520our%2520goals%2520are%2520two%2520fold%253A%2520large-vocabulary%2520continuous%2520sign%250Alanguage%2520recognition%2520%2528CSLR%2529%252C%2520and%2520sign%2520language%2520retrieval.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520multi-task%2520Transformer%2520model%252C%2520CSLR2%252C%2520that%2520is%2520able%2520to%2520ingest%2520a%250Asigning%2520sequence%2520and%2520output%2520in%2520a%2520joint%2520embedding%2520space%2520between%2520signed%2520language%250Aand%2520spoken%2520language%2520text.%2520To%2520enable%2520CSLR%2520evaluation%2520in%2520the%2520large-vocabulary%250Asetting%252C%2520we%2520introduce%2520new%2520dataset%2520annotations%2520that%2520have%2520been%2520manually%250Acollected.%2520These%2520provide%2520continuous%2520sign-level%2520annotations%2520for%2520six%2520hours%2520of%250Atest%2520videos%252C%2520and%2520will%2520be%2520made%2520publicly%2520available.%2520We%2520demonstrate%2520that%2520by%2520a%250Acareful%2520choice%2520of%2520loss%2520functions%252C%2520training%2520the%2520model%2520for%2520both%2520the%2520CSLR%2520and%250Aretrieval%2520tasks%2520is%2520mutually%2520beneficial%2520in%2520terms%2520of%2520performance%2520--%2520retrieval%250Aimproves%2520CSLR%2520performance%2520by%2520providing%2520context%252C%2520while%2520CSLR%2520improves%2520retrieval%250Awith%2520more%2520fine-grained%2520supervision.%2520We%2520further%2520show%2520the%2520benefits%2520of%2520leveraging%250Aweak%2520and%2520noisy%2520supervision%2520from%2520large-vocabulary%2520datasets%2520such%2520as%2520BOBSL%252C%2520namely%250Asign-level%2520pseudo-labels%252C%2520and%2520English%2520subtitles.%2520Our%2520model%2520significantly%250Aoutperforms%2520the%2520previous%2520state%2520of%2520the%2520art%2520on%2520both%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tale%20of%20Two%20Languages%3A%20Large-Vocabulary%20Continuous%20Sign%20Language%0A%20%20Recognition%20from%20Spoken%20Language%20Supervision&entry.906535625=Charles%20Raude%20and%20K%20R%20Prajwal%20and%20Liliane%20Momeni%20and%20Hannah%20Bull%20and%20Samuel%20Albanie%20and%20Andrew%20Zisserman%20and%20G%C3%BCl%20Varol&entry.1292438233=%20%20In%20this%20work%2C%20our%20goals%20are%20two%20fold%3A%20large-vocabulary%20continuous%20sign%0Alanguage%20recognition%20%28CSLR%29%2C%20and%20sign%20language%20retrieval.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-task%20Transformer%20model%2C%20CSLR2%2C%20that%20is%20able%20to%20ingest%20a%0Asigning%20sequence%20and%20output%20in%20a%20joint%20embedding%20space%20between%20signed%20language%0Aand%20spoken%20language%20text.%20To%20enable%20CSLR%20evaluation%20in%20the%20large-vocabulary%0Asetting%2C%20we%20introduce%20new%20dataset%20annotations%20that%20have%20been%20manually%0Acollected.%20These%20provide%20continuous%20sign-level%20annotations%20for%20six%20hours%20of%0Atest%20videos%2C%20and%20will%20be%20made%20publicly%20available.%20We%20demonstrate%20that%20by%20a%0Acareful%20choice%20of%20loss%20functions%2C%20training%20the%20model%20for%20both%20the%20CSLR%20and%0Aretrieval%20tasks%20is%20mutually%20beneficial%20in%20terms%20of%20performance%20--%20retrieval%0Aimproves%20CSLR%20performance%20by%20providing%20context%2C%20while%20CSLR%20improves%20retrieval%0Awith%20more%20fine-grained%20supervision.%20We%20further%20show%20the%20benefits%20of%20leveraging%0Aweak%20and%20noisy%20supervision%20from%20large-vocabulary%20datasets%20such%20as%20BOBSL%2C%20namely%0Asign-level%20pseudo-labels%2C%20and%20English%20subtitles.%20Our%20model%20significantly%0Aoutperforms%20the%20previous%20state%20of%20the%20art%20on%20both%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10266v1&entry.124074799=Read"},
{"title": "EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in\n  Linear Time", "author": "Shengyao Lu and Bang Liu and Keith G. Mills and Jiao He and Di Niu", "abstract": "  Understanding and explaining the predictions of Graph Neural Networks (GNNs),\nis crucial for enhancing their safety and trustworthiness. Subgraph-level\nexplanations are gaining attention for their intuitive appeal. However, most\nexisting subgraph-level explainers face efficiency challenges in explaining\nGNNs due to complex search processes. The key challenge is to find a balance\nbetween intuitiveness and efficiency while ensuring transparency. Additionally,\nthese explainers usually induce subgraphs by nodes, which may introduce\nless-intuitive disconnected nodes in the subgraph-level explanations or omit\nmany important subgraph structures. In this paper, we reveal that inducing\nsubgraph explanations by edges is more comprehensive than other subgraph\ninducing techniques. We also emphasize the need of determining the subgraph\nexplanation size for each data instance, as different data instances may\ninvolve different important substructures. Building upon these considerations,\nwe introduce a training-free approach, named EiG-Search. We employ an efficient\nlinear-time search algorithm over the edge-induced subgraphs, where the edges\nare ranked by an enhanced gradient-based importance. We conduct extensive\nexperiments on a total of seven datasets, demonstrating its superior\nperformance and efficiency both quantitatively and qualitatively over the\nleading baselines.\n", "link": "http://arxiv.org/abs/2405.01762v2", "date": "2024-05-16", "relevancy": 2.4173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5262}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.469}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EiG-Search%3A%20Generating%20Edge-Induced%20Subgraphs%20for%20GNN%20Explanation%20in%0A%20%20Linear%20Time&body=Title%3A%20EiG-Search%3A%20Generating%20Edge-Induced%20Subgraphs%20for%20GNN%20Explanation%20in%0A%20%20Linear%20Time%0AAuthor%3A%20Shengyao%20Lu%20and%20Bang%20Liu%20and%20Keith%20G.%20Mills%20and%20Jiao%20He%20and%20Di%20Niu%0AAbstract%3A%20%20%20Understanding%20and%20explaining%20the%20predictions%20of%20Graph%20Neural%20Networks%20%28GNNs%29%2C%0Ais%20crucial%20for%20enhancing%20their%20safety%20and%20trustworthiness.%20Subgraph-level%0Aexplanations%20are%20gaining%20attention%20for%20their%20intuitive%20appeal.%20However%2C%20most%0Aexisting%20subgraph-level%20explainers%20face%20efficiency%20challenges%20in%20explaining%0AGNNs%20due%20to%20complex%20search%20processes.%20The%20key%20challenge%20is%20to%20find%20a%20balance%0Abetween%20intuitiveness%20and%20efficiency%20while%20ensuring%20transparency.%20Additionally%2C%0Athese%20explainers%20usually%20induce%20subgraphs%20by%20nodes%2C%20which%20may%20introduce%0Aless-intuitive%20disconnected%20nodes%20in%20the%20subgraph-level%20explanations%20or%20omit%0Amany%20important%20subgraph%20structures.%20In%20this%20paper%2C%20we%20reveal%20that%20inducing%0Asubgraph%20explanations%20by%20edges%20is%20more%20comprehensive%20than%20other%20subgraph%0Ainducing%20techniques.%20We%20also%20emphasize%20the%20need%20of%20determining%20the%20subgraph%0Aexplanation%20size%20for%20each%20data%20instance%2C%20as%20different%20data%20instances%20may%0Ainvolve%20different%20important%20substructures.%20Building%20upon%20these%20considerations%2C%0Awe%20introduce%20a%20training-free%20approach%2C%20named%20EiG-Search.%20We%20employ%20an%20efficient%0Alinear-time%20search%20algorithm%20over%20the%20edge-induced%20subgraphs%2C%20where%20the%20edges%0Aare%20ranked%20by%20an%20enhanced%20gradient-based%20importance.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20total%20of%20seven%20datasets%2C%20demonstrating%20its%20superior%0Aperformance%20and%20efficiency%20both%20quantitatively%20and%20qualitatively%20over%20the%0Aleading%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEiG-Search%253A%2520Generating%2520Edge-Induced%2520Subgraphs%2520for%2520GNN%2520Explanation%2520in%250A%2520%2520Linear%2520Time%26entry.906535625%3DShengyao%2520Lu%2520and%2520Bang%2520Liu%2520and%2520Keith%2520G.%2520Mills%2520and%2520Jiao%2520He%2520and%2520Di%2520Niu%26entry.1292438233%3D%2520%2520Understanding%2520and%2520explaining%2520the%2520predictions%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%250Ais%2520crucial%2520for%2520enhancing%2520their%2520safety%2520and%2520trustworthiness.%2520Subgraph-level%250Aexplanations%2520are%2520gaining%2520attention%2520for%2520their%2520intuitive%2520appeal.%2520However%252C%2520most%250Aexisting%2520subgraph-level%2520explainers%2520face%2520efficiency%2520challenges%2520in%2520explaining%250AGNNs%2520due%2520to%2520complex%2520search%2520processes.%2520The%2520key%2520challenge%2520is%2520to%2520find%2520a%2520balance%250Abetween%2520intuitiveness%2520and%2520efficiency%2520while%2520ensuring%2520transparency.%2520Additionally%252C%250Athese%2520explainers%2520usually%2520induce%2520subgraphs%2520by%2520nodes%252C%2520which%2520may%2520introduce%250Aless-intuitive%2520disconnected%2520nodes%2520in%2520the%2520subgraph-level%2520explanations%2520or%2520omit%250Amany%2520important%2520subgraph%2520structures.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520inducing%250Asubgraph%2520explanations%2520by%2520edges%2520is%2520more%2520comprehensive%2520than%2520other%2520subgraph%250Ainducing%2520techniques.%2520We%2520also%2520emphasize%2520the%2520need%2520of%2520determining%2520the%2520subgraph%250Aexplanation%2520size%2520for%2520each%2520data%2520instance%252C%2520as%2520different%2520data%2520instances%2520may%250Ainvolve%2520different%2520important%2520substructures.%2520Building%2520upon%2520these%2520considerations%252C%250Awe%2520introduce%2520a%2520training-free%2520approach%252C%2520named%2520EiG-Search.%2520We%2520employ%2520an%2520efficient%250Alinear-time%2520search%2520algorithm%2520over%2520the%2520edge-induced%2520subgraphs%252C%2520where%2520the%2520edges%250Aare%2520ranked%2520by%2520an%2520enhanced%2520gradient-based%2520importance.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520a%2520total%2520of%2520seven%2520datasets%252C%2520demonstrating%2520its%2520superior%250Aperformance%2520and%2520efficiency%2520both%2520quantitatively%2520and%2520qualitatively%2520over%2520the%250Aleading%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EiG-Search%3A%20Generating%20Edge-Induced%20Subgraphs%20for%20GNN%20Explanation%20in%0A%20%20Linear%20Time&entry.906535625=Shengyao%20Lu%20and%20Bang%20Liu%20and%20Keith%20G.%20Mills%20and%20Jiao%20He%20and%20Di%20Niu&entry.1292438233=%20%20Understanding%20and%20explaining%20the%20predictions%20of%20Graph%20Neural%20Networks%20%28GNNs%29%2C%0Ais%20crucial%20for%20enhancing%20their%20safety%20and%20trustworthiness.%20Subgraph-level%0Aexplanations%20are%20gaining%20attention%20for%20their%20intuitive%20appeal.%20However%2C%20most%0Aexisting%20subgraph-level%20explainers%20face%20efficiency%20challenges%20in%20explaining%0AGNNs%20due%20to%20complex%20search%20processes.%20The%20key%20challenge%20is%20to%20find%20a%20balance%0Abetween%20intuitiveness%20and%20efficiency%20while%20ensuring%20transparency.%20Additionally%2C%0Athese%20explainers%20usually%20induce%20subgraphs%20by%20nodes%2C%20which%20may%20introduce%0Aless-intuitive%20disconnected%20nodes%20in%20the%20subgraph-level%20explanations%20or%20omit%0Amany%20important%20subgraph%20structures.%20In%20this%20paper%2C%20we%20reveal%20that%20inducing%0Asubgraph%20explanations%20by%20edges%20is%20more%20comprehensive%20than%20other%20subgraph%0Ainducing%20techniques.%20We%20also%20emphasize%20the%20need%20of%20determining%20the%20subgraph%0Aexplanation%20size%20for%20each%20data%20instance%2C%20as%20different%20data%20instances%20may%0Ainvolve%20different%20important%20substructures.%20Building%20upon%20these%20considerations%2C%0Awe%20introduce%20a%20training-free%20approach%2C%20named%20EiG-Search.%20We%20employ%20an%20efficient%0Alinear-time%20search%20algorithm%20over%20the%20edge-induced%20subgraphs%2C%20where%20the%20edges%0Aare%20ranked%20by%20an%20enhanced%20gradient-based%20importance.%20We%20conduct%20extensive%0Aexperiments%20on%20a%20total%20of%20seven%20datasets%2C%20demonstrating%20its%20superior%0Aperformance%20and%20efficiency%20both%20quantitatively%20and%20qualitatively%20over%20the%0Aleading%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01762v2&entry.124074799=Read"},
{"title": "Asynchronous Federated Stochastic Optimization with Exact Averaging for\n  Heterogeneous Local Objectives", "author": "Charikleia Iakovidou and Kibaek Kim", "abstract": "  Federated learning (FL) was recently proposed to securely train models with\ndata held over multiple locations (\"clients\") under the coordination of a\ncentral server. Two major challenges hindering the performance of FL algorithms\nare long training times caused by straggling clients and a decrease in training\naccuracy induced by non-iid local distributions (\"client drift\"). In this work\nwe propose and analyze AREA, a new stochastic (sub)gradient algorithm that is\nrobust to client drift and utilizes asynchronous communication to speed up\nconvergence in the presence of stragglers. Moreover, AREA is, to the best of\nour knowledge, the first method that is both guaranteed to converge under\narbitrarily long delays, and converges to an error neighborhood whose size\ndepends only on the variance of the stochastic (sub)gradients used and thus is\nindependent of both the heterogeneity between the local datasets and the length\nof client delays, without the use of delay-adaptive stepsizes. Our numerical\nresults confirm our theoretical analysis and suggest that AREA outperforms\nstate-of-the-art methods when local data are highly non-iid.\n", "link": "http://arxiv.org/abs/2405.10123v1", "date": "2024-05-16", "relevancy": 2.3985, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4932}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4741}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Federated%20Stochastic%20Optimization%20with%20Exact%20Averaging%20for%0A%20%20Heterogeneous%20Local%20Objectives&body=Title%3A%20Asynchronous%20Federated%20Stochastic%20Optimization%20with%20Exact%20Averaging%20for%0A%20%20Heterogeneous%20Local%20Objectives%0AAuthor%3A%20Charikleia%20Iakovidou%20and%20Kibaek%20Kim%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20was%20recently%20proposed%20to%20securely%20train%20models%20with%0Adata%20held%20over%20multiple%20locations%20%28%22clients%22%29%20under%20the%20coordination%20of%20a%0Acentral%20server.%20Two%20major%20challenges%20hindering%20the%20performance%20of%20FL%20algorithms%0Aare%20long%20training%20times%20caused%20by%20straggling%20clients%20and%20a%20decrease%20in%20training%0Aaccuracy%20induced%20by%20non-iid%20local%20distributions%20%28%22client%20drift%22%29.%20In%20this%20work%0Awe%20propose%20and%20analyze%20AREA%2C%20a%20new%20stochastic%20%28sub%29gradient%20algorithm%20that%20is%0Arobust%20to%20client%20drift%20and%20utilizes%20asynchronous%20communication%20to%20speed%20up%0Aconvergence%20in%20the%20presence%20of%20stragglers.%20Moreover%2C%20AREA%20is%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20method%20that%20is%20both%20guaranteed%20to%20converge%20under%0Aarbitrarily%20long%20delays%2C%20and%20converges%20to%20an%20error%20neighborhood%20whose%20size%0Adepends%20only%20on%20the%20variance%20of%20the%20stochastic%20%28sub%29gradients%20used%20and%20thus%20is%0Aindependent%20of%20both%20the%20heterogeneity%20between%20the%20local%20datasets%20and%20the%20length%0Aof%20client%20delays%2C%20without%20the%20use%20of%20delay-adaptive%20stepsizes.%20Our%20numerical%0Aresults%20confirm%20our%20theoretical%20analysis%20and%20suggest%20that%20AREA%20outperforms%0Astate-of-the-art%20methods%20when%20local%20data%20are%20highly%20non-iid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Federated%2520Stochastic%2520Optimization%2520with%2520Exact%2520Averaging%2520for%250A%2520%2520Heterogeneous%2520Local%2520Objectives%26entry.906535625%3DCharikleia%2520Iakovidou%2520and%2520Kibaek%2520Kim%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520was%2520recently%2520proposed%2520to%2520securely%2520train%2520models%2520with%250Adata%2520held%2520over%2520multiple%2520locations%2520%2528%2522clients%2522%2529%2520under%2520the%2520coordination%2520of%2520a%250Acentral%2520server.%2520Two%2520major%2520challenges%2520hindering%2520the%2520performance%2520of%2520FL%2520algorithms%250Aare%2520long%2520training%2520times%2520caused%2520by%2520straggling%2520clients%2520and%2520a%2520decrease%2520in%2520training%250Aaccuracy%2520induced%2520by%2520non-iid%2520local%2520distributions%2520%2528%2522client%2520drift%2522%2529.%2520In%2520this%2520work%250Awe%2520propose%2520and%2520analyze%2520AREA%252C%2520a%2520new%2520stochastic%2520%2528sub%2529gradient%2520algorithm%2520that%2520is%250Arobust%2520to%2520client%2520drift%2520and%2520utilizes%2520asynchronous%2520communication%2520to%2520speed%2520up%250Aconvergence%2520in%2520the%2520presence%2520of%2520stragglers.%2520Moreover%252C%2520AREA%2520is%252C%2520to%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520the%2520first%2520method%2520that%2520is%2520both%2520guaranteed%2520to%2520converge%2520under%250Aarbitrarily%2520long%2520delays%252C%2520and%2520converges%2520to%2520an%2520error%2520neighborhood%2520whose%2520size%250Adepends%2520only%2520on%2520the%2520variance%2520of%2520the%2520stochastic%2520%2528sub%2529gradients%2520used%2520and%2520thus%2520is%250Aindependent%2520of%2520both%2520the%2520heterogeneity%2520between%2520the%2520local%2520datasets%2520and%2520the%2520length%250Aof%2520client%2520delays%252C%2520without%2520the%2520use%2520of%2520delay-adaptive%2520stepsizes.%2520Our%2520numerical%250Aresults%2520confirm%2520our%2520theoretical%2520analysis%2520and%2520suggest%2520that%2520AREA%2520outperforms%250Astate-of-the-art%2520methods%2520when%2520local%2520data%2520are%2520highly%2520non-iid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Federated%20Stochastic%20Optimization%20with%20Exact%20Averaging%20for%0A%20%20Heterogeneous%20Local%20Objectives&entry.906535625=Charikleia%20Iakovidou%20and%20Kibaek%20Kim&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20was%20recently%20proposed%20to%20securely%20train%20models%20with%0Adata%20held%20over%20multiple%20locations%20%28%22clients%22%29%20under%20the%20coordination%20of%20a%0Acentral%20server.%20Two%20major%20challenges%20hindering%20the%20performance%20of%20FL%20algorithms%0Aare%20long%20training%20times%20caused%20by%20straggling%20clients%20and%20a%20decrease%20in%20training%0Aaccuracy%20induced%20by%20non-iid%20local%20distributions%20%28%22client%20drift%22%29.%20In%20this%20work%0Awe%20propose%20and%20analyze%20AREA%2C%20a%20new%20stochastic%20%28sub%29gradient%20algorithm%20that%20is%0Arobust%20to%20client%20drift%20and%20utilizes%20asynchronous%20communication%20to%20speed%20up%0Aconvergence%20in%20the%20presence%20of%20stragglers.%20Moreover%2C%20AREA%20is%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20method%20that%20is%20both%20guaranteed%20to%20converge%20under%0Aarbitrarily%20long%20delays%2C%20and%20converges%20to%20an%20error%20neighborhood%20whose%20size%0Adepends%20only%20on%20the%20variance%20of%20the%20stochastic%20%28sub%29gradients%20used%20and%20thus%20is%0Aindependent%20of%20both%20the%20heterogeneity%20between%20the%20local%20datasets%20and%20the%20length%0Aof%20client%20delays%2C%20without%20the%20use%20of%20delay-adaptive%20stepsizes.%20Our%20numerical%0Aresults%20confirm%20our%20theoretical%20analysis%20and%20suggest%20that%20AREA%20outperforms%0Astate-of-the-art%20methods%20when%20local%20data%20are%20highly%20non-iid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10123v1&entry.124074799=Read"},
{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "author": "Binghui Chen and Chongyang Zhong and Wangmeng Xiang and Yifeng Geng and Xuansong Xie", "abstract": "  Due to the significant advances in large-scale text-to-image generation by\ndiffusion model (DM), controllable human image generation has been attracting\nmuch attention recently. Existing works, such as Controlnet [36], T2I-adapter\n[20] and HumanSD [10] have demonstrated good abilities in generating human\nimages based on pose conditions, they still fail to meet the requirements of\nreal e-commerce scenarios. These include (1) the interaction between the shown\nproduct and human should be considered, (2) human parts like face/hand/arm/foot\nand the interaction between human model and product should be hyper-realistic,\nand (3) the identity of the product shown in advertising should be exactly\nconsistent with the product itself. To this end, in this paper, we first define\na new human image generation task for e-commerce marketing, i.e.,\nObject-ID-retentive Human-object Interaction image Generation (OHG), and then\npropose a VirtualModel framework to generate human images for product shown,\nwhich supports displays of any categories of products and any types of\nhuman-object interaction. As shown in Figure 1, VirtualModel not only\noutperforms other methods in terms of accurate pose control and image quality\nbut also allows for the display of user-specified product objects by\nmaintaining the product-ID consistency and enhancing the plausibility of\nhuman-object interaction. Codes and data will be released.\n", "link": "http://arxiv.org/abs/2405.09985v1", "date": "2024-05-16", "relevancy": 2.3863, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6301}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5976}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VirtualModel%3A%20Generating%20Object-ID-retentive%20Human-object%20Interaction%0A%20%20Image%20by%20Diffusion%20Model%20for%20E-commerce%20Marketing&body=Title%3A%20VirtualModel%3A%20Generating%20Object-ID-retentive%20Human-object%20Interaction%0A%20%20Image%20by%20Diffusion%20Model%20for%20E-commerce%20Marketing%0AAuthor%3A%20Binghui%20Chen%20and%20Chongyang%20Zhong%20and%20Wangmeng%20Xiang%20and%20Yifeng%20Geng%20and%20Xuansong%20Xie%0AAbstract%3A%20%20%20Due%20to%20the%20significant%20advances%20in%20large-scale%20text-to-image%20generation%20by%0Adiffusion%20model%20%28DM%29%2C%20controllable%20human%20image%20generation%20has%20been%20attracting%0Amuch%20attention%20recently.%20Existing%20works%2C%20such%20as%20Controlnet%20%5B36%5D%2C%20T2I-adapter%0A%5B20%5D%20and%20HumanSD%20%5B10%5D%20have%20demonstrated%20good%20abilities%20in%20generating%20human%0Aimages%20based%20on%20pose%20conditions%2C%20they%20still%20fail%20to%20meet%20the%20requirements%20of%0Areal%20e-commerce%20scenarios.%20These%20include%20%281%29%20the%20interaction%20between%20the%20shown%0Aproduct%20and%20human%20should%20be%20considered%2C%20%282%29%20human%20parts%20like%20face/hand/arm/foot%0Aand%20the%20interaction%20between%20human%20model%20and%20product%20should%20be%20hyper-realistic%2C%0Aand%20%283%29%20the%20identity%20of%20the%20product%20shown%20in%20advertising%20should%20be%20exactly%0Aconsistent%20with%20the%20product%20itself.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20first%20define%0Aa%20new%20human%20image%20generation%20task%20for%20e-commerce%20marketing%2C%20i.e.%2C%0AObject-ID-retentive%20Human-object%20Interaction%20image%20Generation%20%28OHG%29%2C%20and%20then%0Apropose%20a%20VirtualModel%20framework%20to%20generate%20human%20images%20for%20product%20shown%2C%0Awhich%20supports%20displays%20of%20any%20categories%20of%20products%20and%20any%20types%20of%0Ahuman-object%20interaction.%20As%20shown%20in%20Figure%201%2C%20VirtualModel%20not%20only%0Aoutperforms%20other%20methods%20in%20terms%20of%20accurate%20pose%20control%20and%20image%20quality%0Abut%20also%20allows%20for%20the%20display%20of%20user-specified%20product%20objects%20by%0Amaintaining%20the%20product-ID%20consistency%20and%20enhancing%20the%20plausibility%20of%0Ahuman-object%20interaction.%20Codes%20and%20data%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtualModel%253A%2520Generating%2520Object-ID-retentive%2520Human-object%2520Interaction%250A%2520%2520Image%2520by%2520Diffusion%2520Model%2520for%2520E-commerce%2520Marketing%26entry.906535625%3DBinghui%2520Chen%2520and%2520Chongyang%2520Zhong%2520and%2520Wangmeng%2520Xiang%2520and%2520Yifeng%2520Geng%2520and%2520Xuansong%2520Xie%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520significant%2520advances%2520in%2520large-scale%2520text-to-image%2520generation%2520by%250Adiffusion%2520model%2520%2528DM%2529%252C%2520controllable%2520human%2520image%2520generation%2520has%2520been%2520attracting%250Amuch%2520attention%2520recently.%2520Existing%2520works%252C%2520such%2520as%2520Controlnet%2520%255B36%255D%252C%2520T2I-adapter%250A%255B20%255D%2520and%2520HumanSD%2520%255B10%255D%2520have%2520demonstrated%2520good%2520abilities%2520in%2520generating%2520human%250Aimages%2520based%2520on%2520pose%2520conditions%252C%2520they%2520still%2520fail%2520to%2520meet%2520the%2520requirements%2520of%250Areal%2520e-commerce%2520scenarios.%2520These%2520include%2520%25281%2529%2520the%2520interaction%2520between%2520the%2520shown%250Aproduct%2520and%2520human%2520should%2520be%2520considered%252C%2520%25282%2529%2520human%2520parts%2520like%2520face/hand/arm/foot%250Aand%2520the%2520interaction%2520between%2520human%2520model%2520and%2520product%2520should%2520be%2520hyper-realistic%252C%250Aand%2520%25283%2529%2520the%2520identity%2520of%2520the%2520product%2520shown%2520in%2520advertising%2520should%2520be%2520exactly%250Aconsistent%2520with%2520the%2520product%2520itself.%2520To%2520this%2520end%252C%2520in%2520this%2520paper%252C%2520we%2520first%2520define%250Aa%2520new%2520human%2520image%2520generation%2520task%2520for%2520e-commerce%2520marketing%252C%2520i.e.%252C%250AObject-ID-retentive%2520Human-object%2520Interaction%2520image%2520Generation%2520%2528OHG%2529%252C%2520and%2520then%250Apropose%2520a%2520VirtualModel%2520framework%2520to%2520generate%2520human%2520images%2520for%2520product%2520shown%252C%250Awhich%2520supports%2520displays%2520of%2520any%2520categories%2520of%2520products%2520and%2520any%2520types%2520of%250Ahuman-object%2520interaction.%2520As%2520shown%2520in%2520Figure%25201%252C%2520VirtualModel%2520not%2520only%250Aoutperforms%2520other%2520methods%2520in%2520terms%2520of%2520accurate%2520pose%2520control%2520and%2520image%2520quality%250Abut%2520also%2520allows%2520for%2520the%2520display%2520of%2520user-specified%2520product%2520objects%2520by%250Amaintaining%2520the%2520product-ID%2520consistency%2520and%2520enhancing%2520the%2520plausibility%2520of%250Ahuman-object%2520interaction.%2520Codes%2520and%2520data%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VirtualModel%3A%20Generating%20Object-ID-retentive%20Human-object%20Interaction%0A%20%20Image%20by%20Diffusion%20Model%20for%20E-commerce%20Marketing&entry.906535625=Binghui%20Chen%20and%20Chongyang%20Zhong%20and%20Wangmeng%20Xiang%20and%20Yifeng%20Geng%20and%20Xuansong%20Xie&entry.1292438233=%20%20Due%20to%20the%20significant%20advances%20in%20large-scale%20text-to-image%20generation%20by%0Adiffusion%20model%20%28DM%29%2C%20controllable%20human%20image%20generation%20has%20been%20attracting%0Amuch%20attention%20recently.%20Existing%20works%2C%20such%20as%20Controlnet%20%5B36%5D%2C%20T2I-adapter%0A%5B20%5D%20and%20HumanSD%20%5B10%5D%20have%20demonstrated%20good%20abilities%20in%20generating%20human%0Aimages%20based%20on%20pose%20conditions%2C%20they%20still%20fail%20to%20meet%20the%20requirements%20of%0Areal%20e-commerce%20scenarios.%20These%20include%20%281%29%20the%20interaction%20between%20the%20shown%0Aproduct%20and%20human%20should%20be%20considered%2C%20%282%29%20human%20parts%20like%20face/hand/arm/foot%0Aand%20the%20interaction%20between%20human%20model%20and%20product%20should%20be%20hyper-realistic%2C%0Aand%20%283%29%20the%20identity%20of%20the%20product%20shown%20in%20advertising%20should%20be%20exactly%0Aconsistent%20with%20the%20product%20itself.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20first%20define%0Aa%20new%20human%20image%20generation%20task%20for%20e-commerce%20marketing%2C%20i.e.%2C%0AObject-ID-retentive%20Human-object%20Interaction%20image%20Generation%20%28OHG%29%2C%20and%20then%0Apropose%20a%20VirtualModel%20framework%20to%20generate%20human%20images%20for%20product%20shown%2C%0Awhich%20supports%20displays%20of%20any%20categories%20of%20products%20and%20any%20types%20of%0Ahuman-object%20interaction.%20As%20shown%20in%20Figure%201%2C%20VirtualModel%20not%20only%0Aoutperforms%20other%20methods%20in%20terms%20of%20accurate%20pose%20control%20and%20image%20quality%0Abut%20also%20allows%20for%20the%20display%20of%20user-specified%20product%20objects%20by%0Amaintaining%20the%20product-ID%20consistency%20and%20enhancing%20the%20plausibility%20of%0Ahuman-object%20interaction.%20Codes%20and%20data%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09985v1&entry.124074799=Read"},
{"title": "Lookbehind-SAM: k steps back, 1 step forward", "author": "Gon\u00e7alo Mordido and Pranshu Malviya and Aristide Baratin and Sarath Chandar", "abstract": "  Sharpness-aware minimization (SAM) methods have gained increasing popularity\nby formulating the problem of minimizing both loss value and loss sharpness as\na minimax objective. In this work, we increase the efficiency of the\nmaximization and minimization parts of SAM's objective to achieve a better\nloss-sharpness trade-off. By taking inspiration from the Lookahead optimizer,\nwhich uses multiple descent steps ahead, we propose Lookbehind, which performs\nmultiple ascent steps behind to enhance the maximization step of SAM and find a\nworst-case perturbation with higher loss. Then, to mitigate the variance in the\ndescent step arising from the gathered gradients across the multiple ascent\nsteps, we employ linear interpolation to refine the minimization step.\nLookbehind leads to a myriad of benefits across a variety of tasks.\nParticularly, we show increased generalization performance, greater robustness\nagainst noisy weights, as well as improved learning and less catastrophic\nforgetting in lifelong learning settings. Our code is available at\nhttps://github.com/chandar-lab/Lookbehind-SAM.\n", "link": "http://arxiv.org/abs/2307.16704v3", "date": "2024-05-16", "relevancy": 2.3744, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4627}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lookbehind-SAM%3A%20k%20steps%20back%2C%201%20step%20forward&body=Title%3A%20Lookbehind-SAM%3A%20k%20steps%20back%2C%201%20step%20forward%0AAuthor%3A%20Gon%C3%A7alo%20Mordido%20and%20Pranshu%20Malviya%20and%20Aristide%20Baratin%20and%20Sarath%20Chandar%0AAbstract%3A%20%20%20Sharpness-aware%20minimization%20%28SAM%29%20methods%20have%20gained%20increasing%20popularity%0Aby%20formulating%20the%20problem%20of%20minimizing%20both%20loss%20value%20and%20loss%20sharpness%20as%0Aa%20minimax%20objective.%20In%20this%20work%2C%20we%20increase%20the%20efficiency%20of%20the%0Amaximization%20and%20minimization%20parts%20of%20SAM%27s%20objective%20to%20achieve%20a%20better%0Aloss-sharpness%20trade-off.%20By%20taking%20inspiration%20from%20the%20Lookahead%20optimizer%2C%0Awhich%20uses%20multiple%20descent%20steps%20ahead%2C%20we%20propose%20Lookbehind%2C%20which%20performs%0Amultiple%20ascent%20steps%20behind%20to%20enhance%20the%20maximization%20step%20of%20SAM%20and%20find%20a%0Aworst-case%20perturbation%20with%20higher%20loss.%20Then%2C%20to%20mitigate%20the%20variance%20in%20the%0Adescent%20step%20arising%20from%20the%20gathered%20gradients%20across%20the%20multiple%20ascent%0Asteps%2C%20we%20employ%20linear%20interpolation%20to%20refine%20the%20minimization%20step.%0ALookbehind%20leads%20to%20a%20myriad%20of%20benefits%20across%20a%20variety%20of%20tasks.%0AParticularly%2C%20we%20show%20increased%20generalization%20performance%2C%20greater%20robustness%0Aagainst%20noisy%20weights%2C%20as%20well%20as%20improved%20learning%20and%20less%20catastrophic%0Aforgetting%20in%20lifelong%20learning%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/chandar-lab/Lookbehind-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16704v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookbehind-SAM%253A%2520k%2520steps%2520back%252C%25201%2520step%2520forward%26entry.906535625%3DGon%25C3%25A7alo%2520Mordido%2520and%2520Pranshu%2520Malviya%2520and%2520Aristide%2520Baratin%2520and%2520Sarath%2520Chandar%26entry.1292438233%3D%2520%2520Sharpness-aware%2520minimization%2520%2528SAM%2529%2520methods%2520have%2520gained%2520increasing%2520popularity%250Aby%2520formulating%2520the%2520problem%2520of%2520minimizing%2520both%2520loss%2520value%2520and%2520loss%2520sharpness%2520as%250Aa%2520minimax%2520objective.%2520In%2520this%2520work%252C%2520we%2520increase%2520the%2520efficiency%2520of%2520the%250Amaximization%2520and%2520minimization%2520parts%2520of%2520SAM%2527s%2520objective%2520to%2520achieve%2520a%2520better%250Aloss-sharpness%2520trade-off.%2520By%2520taking%2520inspiration%2520from%2520the%2520Lookahead%2520optimizer%252C%250Awhich%2520uses%2520multiple%2520descent%2520steps%2520ahead%252C%2520we%2520propose%2520Lookbehind%252C%2520which%2520performs%250Amultiple%2520ascent%2520steps%2520behind%2520to%2520enhance%2520the%2520maximization%2520step%2520of%2520SAM%2520and%2520find%2520a%250Aworst-case%2520perturbation%2520with%2520higher%2520loss.%2520Then%252C%2520to%2520mitigate%2520the%2520variance%2520in%2520the%250Adescent%2520step%2520arising%2520from%2520the%2520gathered%2520gradients%2520across%2520the%2520multiple%2520ascent%250Asteps%252C%2520we%2520employ%2520linear%2520interpolation%2520to%2520refine%2520the%2520minimization%2520step.%250ALookbehind%2520leads%2520to%2520a%2520myriad%2520of%2520benefits%2520across%2520a%2520variety%2520of%2520tasks.%250AParticularly%252C%2520we%2520show%2520increased%2520generalization%2520performance%252C%2520greater%2520robustness%250Aagainst%2520noisy%2520weights%252C%2520as%2520well%2520as%2520improved%2520learning%2520and%2520less%2520catastrophic%250Aforgetting%2520in%2520lifelong%2520learning%2520settings.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/chandar-lab/Lookbehind-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16704v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lookbehind-SAM%3A%20k%20steps%20back%2C%201%20step%20forward&entry.906535625=Gon%C3%A7alo%20Mordido%20and%20Pranshu%20Malviya%20and%20Aristide%20Baratin%20and%20Sarath%20Chandar&entry.1292438233=%20%20Sharpness-aware%20minimization%20%28SAM%29%20methods%20have%20gained%20increasing%20popularity%0Aby%20formulating%20the%20problem%20of%20minimizing%20both%20loss%20value%20and%20loss%20sharpness%20as%0Aa%20minimax%20objective.%20In%20this%20work%2C%20we%20increase%20the%20efficiency%20of%20the%0Amaximization%20and%20minimization%20parts%20of%20SAM%27s%20objective%20to%20achieve%20a%20better%0Aloss-sharpness%20trade-off.%20By%20taking%20inspiration%20from%20the%20Lookahead%20optimizer%2C%0Awhich%20uses%20multiple%20descent%20steps%20ahead%2C%20we%20propose%20Lookbehind%2C%20which%20performs%0Amultiple%20ascent%20steps%20behind%20to%20enhance%20the%20maximization%20step%20of%20SAM%20and%20find%20a%0Aworst-case%20perturbation%20with%20higher%20loss.%20Then%2C%20to%20mitigate%20the%20variance%20in%20the%0Adescent%20step%20arising%20from%20the%20gathered%20gradients%20across%20the%20multiple%20ascent%0Asteps%2C%20we%20employ%20linear%20interpolation%20to%20refine%20the%20minimization%20step.%0ALookbehind%20leads%20to%20a%20myriad%20of%20benefits%20across%20a%20variety%20of%20tasks.%0AParticularly%2C%20we%20show%20increased%20generalization%20performance%2C%20greater%20robustness%0Aagainst%20noisy%20weights%2C%20as%20well%20as%20improved%20learning%20and%20less%20catastrophic%0Aforgetting%20in%20lifelong%20learning%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/chandar-lab/Lookbehind-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16704v3&entry.124074799=Read"},
{"title": "Bilateral Event Mining and Complementary for Event Stream\n  Super-Resolution", "author": "Zhilin Huang and Quanmin Liang and Yijie Yu and Chujun Qin and Xiawu Zheng and Kai Huang and Zikun Zhou and Wenming Yang", "abstract": "  Event Stream Super-Resolution (ESR) aims to address the challenge of\ninsufficient spatial resolution in event streams, which holds great\nsignificance for the application of event cameras in complex scenarios.\nPrevious works for ESR often process positive and negative events in a mixed\nparadigm. This paradigm limits their ability to effectively model the unique\ncharacteristics of each event and mutually refine each other by considering\ntheir correlations. In this paper, we propose a bilateral event mining and\ncomplementary network (BMCNet) to fully leverage the potential of each event\nand capture the shared information to complement each other simultaneously.\nSpecifically, we resort to a two-stream network to accomplish comprehensive\nmining of each type of events individually. To facilitate the exchange of\ninformation between two streams, we propose a bilateral information exchange\n(BIE) module. This module is layer-wisely embedded between two streams,\nenabling the effective propagation of hierarchical global information while\nalleviating the impact of invalid information brought by inherent\ncharacteristics of events. The experimental results demonstrate that our\napproach outperforms the previous state-of-the-art methods in ESR, achieving\nperformance improvements of over 11\\% on both real and synthetic datasets.\nMoreover, our method significantly enhances the performance of event-based\ndownstream tasks such as object recognition and video reconstruction. Our code\nis available at https://github.com/Lqm26/BMCNet-ESR.\n", "link": "http://arxiv.org/abs/2405.10037v1", "date": "2024-05-16", "relevancy": 2.3625, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4722}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bilateral%20Event%20Mining%20and%20Complementary%20for%20Event%20Stream%0A%20%20Super-Resolution&body=Title%3A%20Bilateral%20Event%20Mining%20and%20Complementary%20for%20Event%20Stream%0A%20%20Super-Resolution%0AAuthor%3A%20Zhilin%20Huang%20and%20Quanmin%20Liang%20and%20Yijie%20Yu%20and%20Chujun%20Qin%20and%20Xiawu%20Zheng%20and%20Kai%20Huang%20and%20Zikun%20Zhou%20and%20Wenming%20Yang%0AAbstract%3A%20%20%20Event%20Stream%20Super-Resolution%20%28ESR%29%20aims%20to%20address%20the%20challenge%20of%0Ainsufficient%20spatial%20resolution%20in%20event%20streams%2C%20which%20holds%20great%0Asignificance%20for%20the%20application%20of%20event%20cameras%20in%20complex%20scenarios.%0APrevious%20works%20for%20ESR%20often%20process%20positive%20and%20negative%20events%20in%20a%20mixed%0Aparadigm.%20This%20paradigm%20limits%20their%20ability%20to%20effectively%20model%20the%20unique%0Acharacteristics%20of%20each%20event%20and%20mutually%20refine%20each%20other%20by%20considering%0Atheir%20correlations.%20In%20this%20paper%2C%20we%20propose%20a%20bilateral%20event%20mining%20and%0Acomplementary%20network%20%28BMCNet%29%20to%20fully%20leverage%20the%20potential%20of%20each%20event%0Aand%20capture%20the%20shared%20information%20to%20complement%20each%20other%20simultaneously.%0ASpecifically%2C%20we%20resort%20to%20a%20two-stream%20network%20to%20accomplish%20comprehensive%0Amining%20of%20each%20type%20of%20events%20individually.%20To%20facilitate%20the%20exchange%20of%0Ainformation%20between%20two%20streams%2C%20we%20propose%20a%20bilateral%20information%20exchange%0A%28BIE%29%20module.%20This%20module%20is%20layer-wisely%20embedded%20between%20two%20streams%2C%0Aenabling%20the%20effective%20propagation%20of%20hierarchical%20global%20information%20while%0Aalleviating%20the%20impact%20of%20invalid%20information%20brought%20by%20inherent%0Acharacteristics%20of%20events.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20previous%20state-of-the-art%20methods%20in%20ESR%2C%20achieving%0Aperformance%20improvements%20of%20over%2011%5C%25%20on%20both%20real%20and%20synthetic%20datasets.%0AMoreover%2C%20our%20method%20significantly%20enhances%20the%20performance%20of%20event-based%0Adownstream%20tasks%20such%20as%20object%20recognition%20and%20video%20reconstruction.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/Lqm26/BMCNet-ESR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBilateral%2520Event%2520Mining%2520and%2520Complementary%2520for%2520Event%2520Stream%250A%2520%2520Super-Resolution%26entry.906535625%3DZhilin%2520Huang%2520and%2520Quanmin%2520Liang%2520and%2520Yijie%2520Yu%2520and%2520Chujun%2520Qin%2520and%2520Xiawu%2520Zheng%2520and%2520Kai%2520Huang%2520and%2520Zikun%2520Zhou%2520and%2520Wenming%2520Yang%26entry.1292438233%3D%2520%2520Event%2520Stream%2520Super-Resolution%2520%2528ESR%2529%2520aims%2520to%2520address%2520the%2520challenge%2520of%250Ainsufficient%2520spatial%2520resolution%2520in%2520event%2520streams%252C%2520which%2520holds%2520great%250Asignificance%2520for%2520the%2520application%2520of%2520event%2520cameras%2520in%2520complex%2520scenarios.%250APrevious%2520works%2520for%2520ESR%2520often%2520process%2520positive%2520and%2520negative%2520events%2520in%2520a%2520mixed%250Aparadigm.%2520This%2520paradigm%2520limits%2520their%2520ability%2520to%2520effectively%2520model%2520the%2520unique%250Acharacteristics%2520of%2520each%2520event%2520and%2520mutually%2520refine%2520each%2520other%2520by%2520considering%250Atheir%2520correlations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520bilateral%2520event%2520mining%2520and%250Acomplementary%2520network%2520%2528BMCNet%2529%2520to%2520fully%2520leverage%2520the%2520potential%2520of%2520each%2520event%250Aand%2520capture%2520the%2520shared%2520information%2520to%2520complement%2520each%2520other%2520simultaneously.%250ASpecifically%252C%2520we%2520resort%2520to%2520a%2520two-stream%2520network%2520to%2520accomplish%2520comprehensive%250Amining%2520of%2520each%2520type%2520of%2520events%2520individually.%2520To%2520facilitate%2520the%2520exchange%2520of%250Ainformation%2520between%2520two%2520streams%252C%2520we%2520propose%2520a%2520bilateral%2520information%2520exchange%250A%2528BIE%2529%2520module.%2520This%2520module%2520is%2520layer-wisely%2520embedded%2520between%2520two%2520streams%252C%250Aenabling%2520the%2520effective%2520propagation%2520of%2520hierarchical%2520global%2520information%2520while%250Aalleviating%2520the%2520impact%2520of%2520invalid%2520information%2520brought%2520by%2520inherent%250Acharacteristics%2520of%2520events.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520the%2520previous%2520state-of-the-art%2520methods%2520in%2520ESR%252C%2520achieving%250Aperformance%2520improvements%2520of%2520over%252011%255C%2525%2520on%2520both%2520real%2520and%2520synthetic%2520datasets.%250AMoreover%252C%2520our%2520method%2520significantly%2520enhances%2520the%2520performance%2520of%2520event-based%250Adownstream%2520tasks%2520such%2520as%2520object%2520recognition%2520and%2520video%2520reconstruction.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/Lqm26/BMCNet-ESR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilateral%20Event%20Mining%20and%20Complementary%20for%20Event%20Stream%0A%20%20Super-Resolution&entry.906535625=Zhilin%20Huang%20and%20Quanmin%20Liang%20and%20Yijie%20Yu%20and%20Chujun%20Qin%20and%20Xiawu%20Zheng%20and%20Kai%20Huang%20and%20Zikun%20Zhou%20and%20Wenming%20Yang&entry.1292438233=%20%20Event%20Stream%20Super-Resolution%20%28ESR%29%20aims%20to%20address%20the%20challenge%20of%0Ainsufficient%20spatial%20resolution%20in%20event%20streams%2C%20which%20holds%20great%0Asignificance%20for%20the%20application%20of%20event%20cameras%20in%20complex%20scenarios.%0APrevious%20works%20for%20ESR%20often%20process%20positive%20and%20negative%20events%20in%20a%20mixed%0Aparadigm.%20This%20paradigm%20limits%20their%20ability%20to%20effectively%20model%20the%20unique%0Acharacteristics%20of%20each%20event%20and%20mutually%20refine%20each%20other%20by%20considering%0Atheir%20correlations.%20In%20this%20paper%2C%20we%20propose%20a%20bilateral%20event%20mining%20and%0Acomplementary%20network%20%28BMCNet%29%20to%20fully%20leverage%20the%20potential%20of%20each%20event%0Aand%20capture%20the%20shared%20information%20to%20complement%20each%20other%20simultaneously.%0ASpecifically%2C%20we%20resort%20to%20a%20two-stream%20network%20to%20accomplish%20comprehensive%0Amining%20of%20each%20type%20of%20events%20individually.%20To%20facilitate%20the%20exchange%20of%0Ainformation%20between%20two%20streams%2C%20we%20propose%20a%20bilateral%20information%20exchange%0A%28BIE%29%20module.%20This%20module%20is%20layer-wisely%20embedded%20between%20two%20streams%2C%0Aenabling%20the%20effective%20propagation%20of%20hierarchical%20global%20information%20while%0Aalleviating%20the%20impact%20of%20invalid%20information%20brought%20by%20inherent%0Acharacteristics%20of%20events.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20outperforms%20the%20previous%20state-of-the-art%20methods%20in%20ESR%2C%20achieving%0Aperformance%20improvements%20of%20over%2011%5C%25%20on%20both%20real%20and%20synthetic%20datasets.%0AMoreover%2C%20our%20method%20significantly%20enhances%20the%20performance%20of%20event-based%0Adownstream%20tasks%20such%20as%20object%20recognition%20and%20video%20reconstruction.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/Lqm26/BMCNet-ESR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10037v1&entry.124074799=Read"},
{"title": "Machine Learning Infused Distributed Optimization for Coordinating\n  Virtual Power Plant Assets", "author": "Meiyi Li and Javad Mohammadi", "abstract": "  Amid the increasing interest in the deployment of Distributed Energy\nResources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool\nfor aggregating diverse DERs and facilitating their participation in wholesale\nenergy markets. These VPP deployments have been fueled by the Federal Energy\nRegulatory Commission's Order 2222, which makes DERs and VPPs competitive\nacross market segments. However, the diversity and decentralized nature of DERs\npresent significant challenges to the scalable coordination of VPP assets. To\naddress efficiency and speed bottlenecks, this paper presents a novel machine\nlearning-assisted distributed optimization to coordinate VPP assets. Our\nmethod, named LOOP-MAC(Learning to Optimize the Optimization Process for\nMulti-agent Coordination), adopts a multi-agent coordination perspective where\neach VPP agent manages multiple DERs and utilizes neural network approximators\nto expedite the solution search. The LOOP-MAC method employs a gauge map to\nguarantee strict compliance with local constraints, effectively reducing the\nneed for additional post-processing steps. Our results highlight the advantages\nof LOOP-MAC, showcasing accelerated solution times per iteration and\nsignificantly reduced convergence times. The LOOP-MAC method outperforms\nconventional centralized and distributed optimization methods in optimization\ntasks that require repetitive and sequential execution.\n", "link": "http://arxiv.org/abs/2310.17882v2", "date": "2024-05-16", "relevancy": 2.3369, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.502}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Infused%20Distributed%20Optimization%20for%20Coordinating%0A%20%20Virtual%20Power%20Plant%20Assets&body=Title%3A%20Machine%20Learning%20Infused%20Distributed%20Optimization%20for%20Coordinating%0A%20%20Virtual%20Power%20Plant%20Assets%0AAuthor%3A%20Meiyi%20Li%20and%20Javad%20Mohammadi%0AAbstract%3A%20%20%20Amid%20the%20increasing%20interest%20in%20the%20deployment%20of%20Distributed%20Energy%0AResources%20%28DERs%29%2C%20the%20Virtual%20Power%20Plant%20%28VPP%29%20has%20emerged%20as%20a%20pivotal%20tool%0Afor%20aggregating%20diverse%20DERs%20and%20facilitating%20their%20participation%20in%20wholesale%0Aenergy%20markets.%20These%20VPP%20deployments%20have%20been%20fueled%20by%20the%20Federal%20Energy%0ARegulatory%20Commission%27s%20Order%202222%2C%20which%20makes%20DERs%20and%20VPPs%20competitive%0Aacross%20market%20segments.%20However%2C%20the%20diversity%20and%20decentralized%20nature%20of%20DERs%0Apresent%20significant%20challenges%20to%20the%20scalable%20coordination%20of%20VPP%20assets.%20To%0Aaddress%20efficiency%20and%20speed%20bottlenecks%2C%20this%20paper%20presents%20a%20novel%20machine%0Alearning-assisted%20distributed%20optimization%20to%20coordinate%20VPP%20assets.%20Our%0Amethod%2C%20named%20LOOP-MAC%28Learning%20to%20Optimize%20the%20Optimization%20Process%20for%0AMulti-agent%20Coordination%29%2C%20adopts%20a%20multi-agent%20coordination%20perspective%20where%0Aeach%20VPP%20agent%20manages%20multiple%20DERs%20and%20utilizes%20neural%20network%20approximators%0Ato%20expedite%20the%20solution%20search.%20The%20LOOP-MAC%20method%20employs%20a%20gauge%20map%20to%0Aguarantee%20strict%20compliance%20with%20local%20constraints%2C%20effectively%20reducing%20the%0Aneed%20for%20additional%20post-processing%20steps.%20Our%20results%20highlight%20the%20advantages%0Aof%20LOOP-MAC%2C%20showcasing%20accelerated%20solution%20times%20per%20iteration%20and%0Asignificantly%20reduced%20convergence%20times.%20The%20LOOP-MAC%20method%20outperforms%0Aconventional%20centralized%20and%20distributed%20optimization%20methods%20in%20optimization%0Atasks%20that%20require%20repetitive%20and%20sequential%20execution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Infused%2520Distributed%2520Optimization%2520for%2520Coordinating%250A%2520%2520Virtual%2520Power%2520Plant%2520Assets%26entry.906535625%3DMeiyi%2520Li%2520and%2520Javad%2520Mohammadi%26entry.1292438233%3D%2520%2520Amid%2520the%2520increasing%2520interest%2520in%2520the%2520deployment%2520of%2520Distributed%2520Energy%250AResources%2520%2528DERs%2529%252C%2520the%2520Virtual%2520Power%2520Plant%2520%2528VPP%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520tool%250Afor%2520aggregating%2520diverse%2520DERs%2520and%2520facilitating%2520their%2520participation%2520in%2520wholesale%250Aenergy%2520markets.%2520These%2520VPP%2520deployments%2520have%2520been%2520fueled%2520by%2520the%2520Federal%2520Energy%250ARegulatory%2520Commission%2527s%2520Order%25202222%252C%2520which%2520makes%2520DERs%2520and%2520VPPs%2520competitive%250Aacross%2520market%2520segments.%2520However%252C%2520the%2520diversity%2520and%2520decentralized%2520nature%2520of%2520DERs%250Apresent%2520significant%2520challenges%2520to%2520the%2520scalable%2520coordination%2520of%2520VPP%2520assets.%2520To%250Aaddress%2520efficiency%2520and%2520speed%2520bottlenecks%252C%2520this%2520paper%2520presents%2520a%2520novel%2520machine%250Alearning-assisted%2520distributed%2520optimization%2520to%2520coordinate%2520VPP%2520assets.%2520Our%250Amethod%252C%2520named%2520LOOP-MAC%2528Learning%2520to%2520Optimize%2520the%2520Optimization%2520Process%2520for%250AMulti-agent%2520Coordination%2529%252C%2520adopts%2520a%2520multi-agent%2520coordination%2520perspective%2520where%250Aeach%2520VPP%2520agent%2520manages%2520multiple%2520DERs%2520and%2520utilizes%2520neural%2520network%2520approximators%250Ato%2520expedite%2520the%2520solution%2520search.%2520The%2520LOOP-MAC%2520method%2520employs%2520a%2520gauge%2520map%2520to%250Aguarantee%2520strict%2520compliance%2520with%2520local%2520constraints%252C%2520effectively%2520reducing%2520the%250Aneed%2520for%2520additional%2520post-processing%2520steps.%2520Our%2520results%2520highlight%2520the%2520advantages%250Aof%2520LOOP-MAC%252C%2520showcasing%2520accelerated%2520solution%2520times%2520per%2520iteration%2520and%250Asignificantly%2520reduced%2520convergence%2520times.%2520The%2520LOOP-MAC%2520method%2520outperforms%250Aconventional%2520centralized%2520and%2520distributed%2520optimization%2520methods%2520in%2520optimization%250Atasks%2520that%2520require%2520repetitive%2520and%2520sequential%2520execution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Infused%20Distributed%20Optimization%20for%20Coordinating%0A%20%20Virtual%20Power%20Plant%20Assets&entry.906535625=Meiyi%20Li%20and%20Javad%20Mohammadi&entry.1292438233=%20%20Amid%20the%20increasing%20interest%20in%20the%20deployment%20of%20Distributed%20Energy%0AResources%20%28DERs%29%2C%20the%20Virtual%20Power%20Plant%20%28VPP%29%20has%20emerged%20as%20a%20pivotal%20tool%0Afor%20aggregating%20diverse%20DERs%20and%20facilitating%20their%20participation%20in%20wholesale%0Aenergy%20markets.%20These%20VPP%20deployments%20have%20been%20fueled%20by%20the%20Federal%20Energy%0ARegulatory%20Commission%27s%20Order%202222%2C%20which%20makes%20DERs%20and%20VPPs%20competitive%0Aacross%20market%20segments.%20However%2C%20the%20diversity%20and%20decentralized%20nature%20of%20DERs%0Apresent%20significant%20challenges%20to%20the%20scalable%20coordination%20of%20VPP%20assets.%20To%0Aaddress%20efficiency%20and%20speed%20bottlenecks%2C%20this%20paper%20presents%20a%20novel%20machine%0Alearning-assisted%20distributed%20optimization%20to%20coordinate%20VPP%20assets.%20Our%0Amethod%2C%20named%20LOOP-MAC%28Learning%20to%20Optimize%20the%20Optimization%20Process%20for%0AMulti-agent%20Coordination%29%2C%20adopts%20a%20multi-agent%20coordination%20perspective%20where%0Aeach%20VPP%20agent%20manages%20multiple%20DERs%20and%20utilizes%20neural%20network%20approximators%0Ato%20expedite%20the%20solution%20search.%20The%20LOOP-MAC%20method%20employs%20a%20gauge%20map%20to%0Aguarantee%20strict%20compliance%20with%20local%20constraints%2C%20effectively%20reducing%20the%0Aneed%20for%20additional%20post-processing%20steps.%20Our%20results%20highlight%20the%20advantages%0Aof%20LOOP-MAC%2C%20showcasing%20accelerated%20solution%20times%20per%20iteration%20and%0Asignificantly%20reduced%20convergence%20times.%20The%20LOOP-MAC%20method%20outperforms%0Aconventional%20centralized%20and%20distributed%20optimization%20methods%20in%20optimization%0Atasks%20that%20require%20repetitive%20and%20sequential%20execution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17882v2&entry.124074799=Read"},
{"title": "GPT Store Mining and Analysis", "author": "Dongxun Su and Yanjie Zhao and Xinyi Hou and Shenao Wang and Haoyu Wang", "abstract": "  As a pivotal extension of the renowned ChatGPT, the GPT Store serves as a\ndynamic marketplace for various Generative Pre-trained Transformer (GPT)\nmodels, shaping the frontier of conversational AI. This paper presents an\nin-depth measurement study of the GPT Store, with a focus on the categorization\nof GPTs by topic, factors influencing GPT popularity, and the potential\nsecurity risks. Our investigation starts with assessing the categorization of\nGPTs in the GPT Store, analyzing how they are organized by topics, and\nevaluating the effectiveness of the classification system. We then examine the\nfactors that affect the popularity of specific GPTs, looking into user\npreferences, algorithmic influences, and market trends. Finally, the study\ndelves into the security risks of the GPT Store, identifying potential threats\nand evaluating the robustness of existing security measures. This study offers\na detailed overview of the GPT Store's current state, shedding light on its\noperational dynamics and user interaction patterns. Our findings aim to enhance\nunderstanding of the GPT ecosystem, providing valuable insights for future\nresearch, development, and policy-making in generative AI.\n", "link": "http://arxiv.org/abs/2405.10210v1", "date": "2024-05-16", "relevancy": 2.3203, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4746}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4735}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20Store%20Mining%20and%20Analysis&body=Title%3A%20GPT%20Store%20Mining%20and%20Analysis%0AAuthor%3A%20Dongxun%20Su%20and%20Yanjie%20Zhao%20and%20Xinyi%20Hou%20and%20Shenao%20Wang%20and%20Haoyu%20Wang%0AAbstract%3A%20%20%20As%20a%20pivotal%20extension%20of%20the%20renowned%20ChatGPT%2C%20the%20GPT%20Store%20serves%20as%20a%0Adynamic%20marketplace%20for%20various%20Generative%20Pre-trained%20Transformer%20%28GPT%29%0Amodels%2C%20shaping%20the%20frontier%20of%20conversational%20AI.%20This%20paper%20presents%20an%0Ain-depth%20measurement%20study%20of%20the%20GPT%20Store%2C%20with%20a%20focus%20on%20the%20categorization%0Aof%20GPTs%20by%20topic%2C%20factors%20influencing%20GPT%20popularity%2C%20and%20the%20potential%0Asecurity%20risks.%20Our%20investigation%20starts%20with%20assessing%20the%20categorization%20of%0AGPTs%20in%20the%20GPT%20Store%2C%20analyzing%20how%20they%20are%20organized%20by%20topics%2C%20and%0Aevaluating%20the%20effectiveness%20of%20the%20classification%20system.%20We%20then%20examine%20the%0Afactors%20that%20affect%20the%20popularity%20of%20specific%20GPTs%2C%20looking%20into%20user%0Apreferences%2C%20algorithmic%20influences%2C%20and%20market%20trends.%20Finally%2C%20the%20study%0Adelves%20into%20the%20security%20risks%20of%20the%20GPT%20Store%2C%20identifying%20potential%20threats%0Aand%20evaluating%20the%20robustness%20of%20existing%20security%20measures.%20This%20study%20offers%0Aa%20detailed%20overview%20of%20the%20GPT%20Store%27s%20current%20state%2C%20shedding%20light%20on%20its%0Aoperational%20dynamics%20and%20user%20interaction%20patterns.%20Our%20findings%20aim%20to%20enhance%0Aunderstanding%20of%20the%20GPT%20ecosystem%2C%20providing%20valuable%20insights%20for%20future%0Aresearch%2C%20development%2C%20and%20policy-making%20in%20generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520Store%2520Mining%2520and%2520Analysis%26entry.906535625%3DDongxun%2520Su%2520and%2520Yanjie%2520Zhao%2520and%2520Xinyi%2520Hou%2520and%2520Shenao%2520Wang%2520and%2520Haoyu%2520Wang%26entry.1292438233%3D%2520%2520As%2520a%2520pivotal%2520extension%2520of%2520the%2520renowned%2520ChatGPT%252C%2520the%2520GPT%2520Store%2520serves%2520as%2520a%250Adynamic%2520marketplace%2520for%2520various%2520Generative%2520Pre-trained%2520Transformer%2520%2528GPT%2529%250Amodels%252C%2520shaping%2520the%2520frontier%2520of%2520conversational%2520AI.%2520This%2520paper%2520presents%2520an%250Ain-depth%2520measurement%2520study%2520of%2520the%2520GPT%2520Store%252C%2520with%2520a%2520focus%2520on%2520the%2520categorization%250Aof%2520GPTs%2520by%2520topic%252C%2520factors%2520influencing%2520GPT%2520popularity%252C%2520and%2520the%2520potential%250Asecurity%2520risks.%2520Our%2520investigation%2520starts%2520with%2520assessing%2520the%2520categorization%2520of%250AGPTs%2520in%2520the%2520GPT%2520Store%252C%2520analyzing%2520how%2520they%2520are%2520organized%2520by%2520topics%252C%2520and%250Aevaluating%2520the%2520effectiveness%2520of%2520the%2520classification%2520system.%2520We%2520then%2520examine%2520the%250Afactors%2520that%2520affect%2520the%2520popularity%2520of%2520specific%2520GPTs%252C%2520looking%2520into%2520user%250Apreferences%252C%2520algorithmic%2520influences%252C%2520and%2520market%2520trends.%2520Finally%252C%2520the%2520study%250Adelves%2520into%2520the%2520security%2520risks%2520of%2520the%2520GPT%2520Store%252C%2520identifying%2520potential%2520threats%250Aand%2520evaluating%2520the%2520robustness%2520of%2520existing%2520security%2520measures.%2520This%2520study%2520offers%250Aa%2520detailed%2520overview%2520of%2520the%2520GPT%2520Store%2527s%2520current%2520state%252C%2520shedding%2520light%2520on%2520its%250Aoperational%2520dynamics%2520and%2520user%2520interaction%2520patterns.%2520Our%2520findings%2520aim%2520to%2520enhance%250Aunderstanding%2520of%2520the%2520GPT%2520ecosystem%252C%2520providing%2520valuable%2520insights%2520for%2520future%250Aresearch%252C%2520development%252C%2520and%2520policy-making%2520in%2520generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20Store%20Mining%20and%20Analysis&entry.906535625=Dongxun%20Su%20and%20Yanjie%20Zhao%20and%20Xinyi%20Hou%20and%20Shenao%20Wang%20and%20Haoyu%20Wang&entry.1292438233=%20%20As%20a%20pivotal%20extension%20of%20the%20renowned%20ChatGPT%2C%20the%20GPT%20Store%20serves%20as%20a%0Adynamic%20marketplace%20for%20various%20Generative%20Pre-trained%20Transformer%20%28GPT%29%0Amodels%2C%20shaping%20the%20frontier%20of%20conversational%20AI.%20This%20paper%20presents%20an%0Ain-depth%20measurement%20study%20of%20the%20GPT%20Store%2C%20with%20a%20focus%20on%20the%20categorization%0Aof%20GPTs%20by%20topic%2C%20factors%20influencing%20GPT%20popularity%2C%20and%20the%20potential%0Asecurity%20risks.%20Our%20investigation%20starts%20with%20assessing%20the%20categorization%20of%0AGPTs%20in%20the%20GPT%20Store%2C%20analyzing%20how%20they%20are%20organized%20by%20topics%2C%20and%0Aevaluating%20the%20effectiveness%20of%20the%20classification%20system.%20We%20then%20examine%20the%0Afactors%20that%20affect%20the%20popularity%20of%20specific%20GPTs%2C%20looking%20into%20user%0Apreferences%2C%20algorithmic%20influences%2C%20and%20market%20trends.%20Finally%2C%20the%20study%0Adelves%20into%20the%20security%20risks%20of%20the%20GPT%20Store%2C%20identifying%20potential%20threats%0Aand%20evaluating%20the%20robustness%20of%20existing%20security%20measures.%20This%20study%20offers%0Aa%20detailed%20overview%20of%20the%20GPT%20Store%27s%20current%20state%2C%20shedding%20light%20on%20its%0Aoperational%20dynamics%20and%20user%20interaction%20patterns.%20Our%20findings%20aim%20to%20enhance%0Aunderstanding%20of%20the%20GPT%20ecosystem%2C%20providing%20valuable%20insights%20for%20future%0Aresearch%2C%20development%2C%20and%20policy-making%20in%20generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10210v1&entry.124074799=Read"},
{"title": "4D Panoptic Scene Graph Generation", "author": "Jingkang Yang and Jun Cen and Wenxuan Peng and Shuai Liu and Fangzhou Hong and Xiangtai Li and Kaiyang Zhou and Qifeng Chen and Ziwei Liu", "abstract": "  We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.\n", "link": "http://arxiv.org/abs/2405.10305v1", "date": "2024-05-16", "relevancy": 2.3172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.585}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Panoptic%20Scene%20Graph%20Generation&body=Title%3A%204D%20Panoptic%20Scene%20Graph%20Generation%0AAuthor%3A%20Jingkang%20Yang%20and%20Jun%20Cen%20and%20Wenxuan%20Peng%20and%20Shuai%20Liu%20and%20Fangzhou%20Hong%20and%20Xiangtai%20Li%20and%20Kaiyang%20Zhou%20and%20Qifeng%20Chen%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20are%20living%20in%20a%20three-dimensional%20space%20while%20moving%20forward%20through%20a%0Afourth%20dimension%3A%20time.%20To%20allow%20artificial%20intelligence%20to%20develop%20a%0Acomprehensive%20understanding%20of%20such%20a%204D%20environment%2C%20we%20introduce%204D%20Panoptic%0AScene%20Graph%20%28PSG-4D%29%2C%20a%20new%20representation%20that%20bridges%20the%20raw%20visual%20data%0Aperceived%20in%20a%20dynamic%204D%20world%20and%20high-level%20visual%20understanding.%0ASpecifically%2C%20PSG-4D%20abstracts%20rich%204D%20sensory%20data%20into%20nodes%2C%20which%20represent%0Aentities%20with%20precise%20location%20and%20status%20information%2C%20and%20edges%2C%20which%20capture%0Athe%20temporal%20relations.%20To%20facilitate%20research%20in%20this%20new%20area%2C%20we%20build%20a%0Arichly%20annotated%20PSG-4D%20dataset%20consisting%20of%203K%20RGB-D%20videos%20with%20a%20total%20of%0A1M%20frames%2C%20each%20of%20which%20is%20labeled%20with%204D%20panoptic%20segmentation%20masks%20as%20well%0Aas%20fine-grained%2C%20dynamic%20scene%20graphs.%20To%20solve%20PSG-4D%2C%20we%20propose%20PSG4DFormer%2C%0Aa%20Transformer-based%20model%20that%20can%20predict%20panoptic%20segmentation%20masks%2C%20track%0Amasks%20along%20the%20time%20axis%2C%20and%20generate%20the%20corresponding%20scene%20graphs%20via%20a%0Arelation%20component.%20Extensive%20experiments%20on%20the%20new%20dataset%20show%20that%20our%0Amethod%20can%20serve%20as%20a%20strong%20baseline%20for%20future%20research%20on%20PSG-4D.%20In%20the%0Aend%2C%20we%20provide%20a%20real-world%20application%20example%20to%20demonstrate%20how%20we%20can%0Aachieve%20dynamic%20scene%20understanding%20by%20integrating%20a%20large%20language%20model%20into%0Aour%20PSG-4D%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Panoptic%2520Scene%2520Graph%2520Generation%26entry.906535625%3DJingkang%2520Yang%2520and%2520Jun%2520Cen%2520and%2520Wenxuan%2520Peng%2520and%2520Shuai%2520Liu%2520and%2520Fangzhou%2520Hong%2520and%2520Xiangtai%2520Li%2520and%2520Kaiyang%2520Zhou%2520and%2520Qifeng%2520Chen%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520are%2520living%2520in%2520a%2520three-dimensional%2520space%2520while%2520moving%2520forward%2520through%2520a%250Afourth%2520dimension%253A%2520time.%2520To%2520allow%2520artificial%2520intelligence%2520to%2520develop%2520a%250Acomprehensive%2520understanding%2520of%2520such%2520a%25204D%2520environment%252C%2520we%2520introduce%25204D%2520Panoptic%250AScene%2520Graph%2520%2528PSG-4D%2529%252C%2520a%2520new%2520representation%2520that%2520bridges%2520the%2520raw%2520visual%2520data%250Aperceived%2520in%2520a%2520dynamic%25204D%2520world%2520and%2520high-level%2520visual%2520understanding.%250ASpecifically%252C%2520PSG-4D%2520abstracts%2520rich%25204D%2520sensory%2520data%2520into%2520nodes%252C%2520which%2520represent%250Aentities%2520with%2520precise%2520location%2520and%2520status%2520information%252C%2520and%2520edges%252C%2520which%2520capture%250Athe%2520temporal%2520relations.%2520To%2520facilitate%2520research%2520in%2520this%2520new%2520area%252C%2520we%2520build%2520a%250Arichly%2520annotated%2520PSG-4D%2520dataset%2520consisting%2520of%25203K%2520RGB-D%2520videos%2520with%2520a%2520total%2520of%250A1M%2520frames%252C%2520each%2520of%2520which%2520is%2520labeled%2520with%25204D%2520panoptic%2520segmentation%2520masks%2520as%2520well%250Aas%2520fine-grained%252C%2520dynamic%2520scene%2520graphs.%2520To%2520solve%2520PSG-4D%252C%2520we%2520propose%2520PSG4DFormer%252C%250Aa%2520Transformer-based%2520model%2520that%2520can%2520predict%2520panoptic%2520segmentation%2520masks%252C%2520track%250Amasks%2520along%2520the%2520time%2520axis%252C%2520and%2520generate%2520the%2520corresponding%2520scene%2520graphs%2520via%2520a%250Arelation%2520component.%2520Extensive%2520experiments%2520on%2520the%2520new%2520dataset%2520show%2520that%2520our%250Amethod%2520can%2520serve%2520as%2520a%2520strong%2520baseline%2520for%2520future%2520research%2520on%2520PSG-4D.%2520In%2520the%250Aend%252C%2520we%2520provide%2520a%2520real-world%2520application%2520example%2520to%2520demonstrate%2520how%2520we%2520can%250Aachieve%2520dynamic%2520scene%2520understanding%2520by%2520integrating%2520a%2520large%2520language%2520model%2520into%250Aour%2520PSG-4D%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Panoptic%20Scene%20Graph%20Generation&entry.906535625=Jingkang%20Yang%20and%20Jun%20Cen%20and%20Wenxuan%20Peng%20and%20Shuai%20Liu%20and%20Fangzhou%20Hong%20and%20Xiangtai%20Li%20and%20Kaiyang%20Zhou%20and%20Qifeng%20Chen%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20are%20living%20in%20a%20three-dimensional%20space%20while%20moving%20forward%20through%20a%0Afourth%20dimension%3A%20time.%20To%20allow%20artificial%20intelligence%20to%20develop%20a%0Acomprehensive%20understanding%20of%20such%20a%204D%20environment%2C%20we%20introduce%204D%20Panoptic%0AScene%20Graph%20%28PSG-4D%29%2C%20a%20new%20representation%20that%20bridges%20the%20raw%20visual%20data%0Aperceived%20in%20a%20dynamic%204D%20world%20and%20high-level%20visual%20understanding.%0ASpecifically%2C%20PSG-4D%20abstracts%20rich%204D%20sensory%20data%20into%20nodes%2C%20which%20represent%0Aentities%20with%20precise%20location%20and%20status%20information%2C%20and%20edges%2C%20which%20capture%0Athe%20temporal%20relations.%20To%20facilitate%20research%20in%20this%20new%20area%2C%20we%20build%20a%0Arichly%20annotated%20PSG-4D%20dataset%20consisting%20of%203K%20RGB-D%20videos%20with%20a%20total%20of%0A1M%20frames%2C%20each%20of%20which%20is%20labeled%20with%204D%20panoptic%20segmentation%20masks%20as%20well%0Aas%20fine-grained%2C%20dynamic%20scene%20graphs.%20To%20solve%20PSG-4D%2C%20we%20propose%20PSG4DFormer%2C%0Aa%20Transformer-based%20model%20that%20can%20predict%20panoptic%20segmentation%20masks%2C%20track%0Amasks%20along%20the%20time%20axis%2C%20and%20generate%20the%20corresponding%20scene%20graphs%20via%20a%0Arelation%20component.%20Extensive%20experiments%20on%20the%20new%20dataset%20show%20that%20our%0Amethod%20can%20serve%20as%20a%20strong%20baseline%20for%20future%20research%20on%20PSG-4D.%20In%20the%0Aend%2C%20we%20provide%20a%20real-world%20application%20example%20to%20demonstrate%20how%20we%20can%0Aachieve%20dynamic%20scene%20understanding%20by%20integrating%20a%20large%20language%20model%20into%0Aour%20PSG-4D%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10305v1&entry.124074799=Read"},
{"title": "ViKi-HyCo: A Hybrid-Control approach for complex car-like maneuvers", "author": "Edison P. Velasco S\u00e1nchez and Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n and Francisco A. Candelas and Santiago T. Puente and Fernando Torres", "abstract": "  While Visual Servoing is deeply studied to perform simple maneuvers, the\nliterature does not commonly address complex cases where the target is far out\nof the camera's field of view (FOV) during the maneuver. For this reason, in\nthis paper, we present ViKi-HyCo (Visual Servoing and Kinematic\nHybrid-Controller). This approach generates the necessary maneuvers for the\ncomplex positioning of a non-holonomic mobile robot in outdoor environments. In\nthis method, we use \\hbox{LiDAR-camera} fusion to estimate objects bounding\nboxes using image and metrics modalities. With the multi-modality nature of our\nrepresentation, we can automatically obtain a target for a visual servoing\ncontroller. At the same time, we also have a metric target, which allows us to\nhybridize with a kinematic controller. Given this hybridization, we can perform\ncomplex maneuvers even when the target is far away from the camera's FOV. The\nproposed approach does not require an object-tracking algorithm and can be\napplied to any robotic positioning task where its kinematic model is known.\nViKi-HyCo has an error of 0.0428 \\pm 0.0467 m in the X-axis and 0.0515 \\pm\n0.0323 m in the Y-axis at the end of a complete positioning task.\n", "link": "http://arxiv.org/abs/2311.07268v3", "date": "2024-05-16", "relevancy": 2.2834, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViKi-HyCo%3A%20A%20Hybrid-Control%20approach%20for%20complex%20car-like%20maneuvers&body=Title%3A%20ViKi-HyCo%3A%20A%20Hybrid-Control%20approach%20for%20complex%20car-like%20maneuvers%0AAuthor%3A%20Edison%20P.%20Velasco%20S%C3%A1nchez%20and%20Miguel%20%C3%81ngel%20Mu%C3%B1oz-Ba%C3%B1%C3%B3n%20and%20Francisco%20A.%20Candelas%20and%20Santiago%20T.%20Puente%20and%20Fernando%20Torres%0AAbstract%3A%20%20%20While%20Visual%20Servoing%20is%20deeply%20studied%20to%20perform%20simple%20maneuvers%2C%20the%0Aliterature%20does%20not%20commonly%20address%20complex%20cases%20where%20the%20target%20is%20far%20out%0Aof%20the%20camera%27s%20field%20of%20view%20%28FOV%29%20during%20the%20maneuver.%20For%20this%20reason%2C%20in%0Athis%20paper%2C%20we%20present%20ViKi-HyCo%20%28Visual%20Servoing%20and%20Kinematic%0AHybrid-Controller%29.%20This%20approach%20generates%20the%20necessary%20maneuvers%20for%20the%0Acomplex%20positioning%20of%20a%20non-holonomic%20mobile%20robot%20in%20outdoor%20environments.%20In%0Athis%20method%2C%20we%20use%20%5Chbox%7BLiDAR-camera%7D%20fusion%20to%20estimate%20objects%20bounding%0Aboxes%20using%20image%20and%20metrics%20modalities.%20With%20the%20multi-modality%20nature%20of%20our%0Arepresentation%2C%20we%20can%20automatically%20obtain%20a%20target%20for%20a%20visual%20servoing%0Acontroller.%20At%20the%20same%20time%2C%20we%20also%20have%20a%20metric%20target%2C%20which%20allows%20us%20to%0Ahybridize%20with%20a%20kinematic%20controller.%20Given%20this%20hybridization%2C%20we%20can%20perform%0Acomplex%20maneuvers%20even%20when%20the%20target%20is%20far%20away%20from%20the%20camera%27s%20FOV.%20The%0Aproposed%20approach%20does%20not%20require%20an%20object-tracking%20algorithm%20and%20can%20be%0Aapplied%20to%20any%20robotic%20positioning%20task%20where%20its%20kinematic%20model%20is%20known.%0AViKi-HyCo%20has%20an%20error%20of%200.0428%20%5Cpm%200.0467%20m%20in%20the%20X-axis%20and%200.0515%20%5Cpm%0A0.0323%20m%20in%20the%20Y-axis%20at%20the%20end%20of%20a%20complete%20positioning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07268v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViKi-HyCo%253A%2520A%2520Hybrid-Control%2520approach%2520for%2520complex%2520car-like%2520maneuvers%26entry.906535625%3DEdison%2520P.%2520Velasco%2520S%25C3%25A1nchez%2520and%2520Miguel%2520%25C3%2581ngel%2520Mu%25C3%25B1oz-Ba%25C3%25B1%25C3%25B3n%2520and%2520Francisco%2520A.%2520Candelas%2520and%2520Santiago%2520T.%2520Puente%2520and%2520Fernando%2520Torres%26entry.1292438233%3D%2520%2520While%2520Visual%2520Servoing%2520is%2520deeply%2520studied%2520to%2520perform%2520simple%2520maneuvers%252C%2520the%250Aliterature%2520does%2520not%2520commonly%2520address%2520complex%2520cases%2520where%2520the%2520target%2520is%2520far%2520out%250Aof%2520the%2520camera%2527s%2520field%2520of%2520view%2520%2528FOV%2529%2520during%2520the%2520maneuver.%2520For%2520this%2520reason%252C%2520in%250Athis%2520paper%252C%2520we%2520present%2520ViKi-HyCo%2520%2528Visual%2520Servoing%2520and%2520Kinematic%250AHybrid-Controller%2529.%2520This%2520approach%2520generates%2520the%2520necessary%2520maneuvers%2520for%2520the%250Acomplex%2520positioning%2520of%2520a%2520non-holonomic%2520mobile%2520robot%2520in%2520outdoor%2520environments.%2520In%250Athis%2520method%252C%2520we%2520use%2520%255Chbox%257BLiDAR-camera%257D%2520fusion%2520to%2520estimate%2520objects%2520bounding%250Aboxes%2520using%2520image%2520and%2520metrics%2520modalities.%2520With%2520the%2520multi-modality%2520nature%2520of%2520our%250Arepresentation%252C%2520we%2520can%2520automatically%2520obtain%2520a%2520target%2520for%2520a%2520visual%2520servoing%250Acontroller.%2520At%2520the%2520same%2520time%252C%2520we%2520also%2520have%2520a%2520metric%2520target%252C%2520which%2520allows%2520us%2520to%250Ahybridize%2520with%2520a%2520kinematic%2520controller.%2520Given%2520this%2520hybridization%252C%2520we%2520can%2520perform%250Acomplex%2520maneuvers%2520even%2520when%2520the%2520target%2520is%2520far%2520away%2520from%2520the%2520camera%2527s%2520FOV.%2520The%250Aproposed%2520approach%2520does%2520not%2520require%2520an%2520object-tracking%2520algorithm%2520and%2520can%2520be%250Aapplied%2520to%2520any%2520robotic%2520positioning%2520task%2520where%2520its%2520kinematic%2520model%2520is%2520known.%250AViKi-HyCo%2520has%2520an%2520error%2520of%25200.0428%2520%255Cpm%25200.0467%2520m%2520in%2520the%2520X-axis%2520and%25200.0515%2520%255Cpm%250A0.0323%2520m%2520in%2520the%2520Y-axis%2520at%2520the%2520end%2520of%2520a%2520complete%2520positioning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07268v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViKi-HyCo%3A%20A%20Hybrid-Control%20approach%20for%20complex%20car-like%20maneuvers&entry.906535625=Edison%20P.%20Velasco%20S%C3%A1nchez%20and%20Miguel%20%C3%81ngel%20Mu%C3%B1oz-Ba%C3%B1%C3%B3n%20and%20Francisco%20A.%20Candelas%20and%20Santiago%20T.%20Puente%20and%20Fernando%20Torres&entry.1292438233=%20%20While%20Visual%20Servoing%20is%20deeply%20studied%20to%20perform%20simple%20maneuvers%2C%20the%0Aliterature%20does%20not%20commonly%20address%20complex%20cases%20where%20the%20target%20is%20far%20out%0Aof%20the%20camera%27s%20field%20of%20view%20%28FOV%29%20during%20the%20maneuver.%20For%20this%20reason%2C%20in%0Athis%20paper%2C%20we%20present%20ViKi-HyCo%20%28Visual%20Servoing%20and%20Kinematic%0AHybrid-Controller%29.%20This%20approach%20generates%20the%20necessary%20maneuvers%20for%20the%0Acomplex%20positioning%20of%20a%20non-holonomic%20mobile%20robot%20in%20outdoor%20environments.%20In%0Athis%20method%2C%20we%20use%20%5Chbox%7BLiDAR-camera%7D%20fusion%20to%20estimate%20objects%20bounding%0Aboxes%20using%20image%20and%20metrics%20modalities.%20With%20the%20multi-modality%20nature%20of%20our%0Arepresentation%2C%20we%20can%20automatically%20obtain%20a%20target%20for%20a%20visual%20servoing%0Acontroller.%20At%20the%20same%20time%2C%20we%20also%20have%20a%20metric%20target%2C%20which%20allows%20us%20to%0Ahybridize%20with%20a%20kinematic%20controller.%20Given%20this%20hybridization%2C%20we%20can%20perform%0Acomplex%20maneuvers%20even%20when%20the%20target%20is%20far%20away%20from%20the%20camera%27s%20FOV.%20The%0Aproposed%20approach%20does%20not%20require%20an%20object-tracking%20algorithm%20and%20can%20be%0Aapplied%20to%20any%20robotic%20positioning%20task%20where%20its%20kinematic%20model%20is%20known.%0AViKi-HyCo%20has%20an%20error%20of%200.0428%20%5Cpm%200.0467%20m%20in%20the%20X-axis%20and%200.0515%20%5Cpm%0A0.0323%20m%20in%20the%20Y-axis%20at%20the%20end%20of%20a%20complete%20positioning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07268v3&entry.124074799=Read"},
{"title": "DiverGen: Improving Instance Segmentation by Learning Wider Data\n  Distribution with More Diverse Generative Data", "author": "Chengxiang Fan and Muzhi Zhu and Hao Chen and Yang Liu and Weijia Wu and Huaqi Zhang and Chunhua Shen", "abstract": "  Instance segmentation is data-hungry, and as model capacity increases, data\nscale becomes crucial for improving the accuracy. Most instance segmentation\ndatasets today require costly manual annotation, limiting their data scale.\nModels trained on such data are prone to overfitting on the training set,\nespecially for those rare categories. While recent works have delved into\nexploiting generative models to create synthetic datasets for data\naugmentation, these approaches do not efficiently harness the full potential of\ngenerative models.\n  To address these issues, we introduce a more efficient strategy to construct\ngenerative datasets for data augmentation, termed DiverGen. Firstly, we provide\nan explanation of the role of generative data from the perspective of\ndistribution discrepancy. We investigate the impact of different data on the\ndistribution learned by the model. We argue that generative data can expand the\ndata distribution that the model can learn, thus mitigating overfitting.\nAdditionally, we find that the diversity of generative data is crucial for\nimproving model performance and enhance it through various strategies,\nincluding category diversity, prompt diversity, and generative model diversity.\nWith these strategies, we can scale the data to millions while maintaining the\ntrend of model performance improvement. On the LVIS dataset, DiverGen\nsignificantly outperforms the strong model X-Paste, achieving +1.1 box AP and\n+1.1 mask AP across all categories, and +1.9 box AP and +2.5 mask AP for rare\ncategories.\n", "link": "http://arxiv.org/abs/2405.10185v1", "date": "2024-05-16", "relevancy": 2.2723, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5927}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5527}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiverGen%3A%20Improving%20Instance%20Segmentation%20by%20Learning%20Wider%20Data%0A%20%20Distribution%20with%20More%20Diverse%20Generative%20Data&body=Title%3A%20DiverGen%3A%20Improving%20Instance%20Segmentation%20by%20Learning%20Wider%20Data%0A%20%20Distribution%20with%20More%20Diverse%20Generative%20Data%0AAuthor%3A%20Chengxiang%20Fan%20and%20Muzhi%20Zhu%20and%20Hao%20Chen%20and%20Yang%20Liu%20and%20Weijia%20Wu%20and%20Huaqi%20Zhang%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Instance%20segmentation%20is%20data-hungry%2C%20and%20as%20model%20capacity%20increases%2C%20data%0Ascale%20becomes%20crucial%20for%20improving%20the%20accuracy.%20Most%20instance%20segmentation%0Adatasets%20today%20require%20costly%20manual%20annotation%2C%20limiting%20their%20data%20scale.%0AModels%20trained%20on%20such%20data%20are%20prone%20to%20overfitting%20on%20the%20training%20set%2C%0Aespecially%20for%20those%20rare%20categories.%20While%20recent%20works%20have%20delved%20into%0Aexploiting%20generative%20models%20to%20create%20synthetic%20datasets%20for%20data%0Aaugmentation%2C%20these%20approaches%20do%20not%20efficiently%20harness%20the%20full%20potential%20of%0Agenerative%20models.%0A%20%20To%20address%20these%20issues%2C%20we%20introduce%20a%20more%20efficient%20strategy%20to%20construct%0Agenerative%20datasets%20for%20data%20augmentation%2C%20termed%20DiverGen.%20Firstly%2C%20we%20provide%0Aan%20explanation%20of%20the%20role%20of%20generative%20data%20from%20the%20perspective%20of%0Adistribution%20discrepancy.%20We%20investigate%20the%20impact%20of%20different%20data%20on%20the%0Adistribution%20learned%20by%20the%20model.%20We%20argue%20that%20generative%20data%20can%20expand%20the%0Adata%20distribution%20that%20the%20model%20can%20learn%2C%20thus%20mitigating%20overfitting.%0AAdditionally%2C%20we%20find%20that%20the%20diversity%20of%20generative%20data%20is%20crucial%20for%0Aimproving%20model%20performance%20and%20enhance%20it%20through%20various%20strategies%2C%0Aincluding%20category%20diversity%2C%20prompt%20diversity%2C%20and%20generative%20model%20diversity.%0AWith%20these%20strategies%2C%20we%20can%20scale%20the%20data%20to%20millions%20while%20maintaining%20the%0Atrend%20of%20model%20performance%20improvement.%20On%20the%20LVIS%20dataset%2C%20DiverGen%0Asignificantly%20outperforms%20the%20strong%20model%20X-Paste%2C%20achieving%20%2B1.1%20box%20AP%20and%0A%2B1.1%20mask%20AP%20across%20all%20categories%2C%20and%20%2B1.9%20box%20AP%20and%20%2B2.5%20mask%20AP%20for%20rare%0Acategories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverGen%253A%2520Improving%2520Instance%2520Segmentation%2520by%2520Learning%2520Wider%2520Data%250A%2520%2520Distribution%2520with%2520More%2520Diverse%2520Generative%2520Data%26entry.906535625%3DChengxiang%2520Fan%2520and%2520Muzhi%2520Zhu%2520and%2520Hao%2520Chen%2520and%2520Yang%2520Liu%2520and%2520Weijia%2520Wu%2520and%2520Huaqi%2520Zhang%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520is%2520data-hungry%252C%2520and%2520as%2520model%2520capacity%2520increases%252C%2520data%250Ascale%2520becomes%2520crucial%2520for%2520improving%2520the%2520accuracy.%2520Most%2520instance%2520segmentation%250Adatasets%2520today%2520require%2520costly%2520manual%2520annotation%252C%2520limiting%2520their%2520data%2520scale.%250AModels%2520trained%2520on%2520such%2520data%2520are%2520prone%2520to%2520overfitting%2520on%2520the%2520training%2520set%252C%250Aespecially%2520for%2520those%2520rare%2520categories.%2520While%2520recent%2520works%2520have%2520delved%2520into%250Aexploiting%2520generative%2520models%2520to%2520create%2520synthetic%2520datasets%2520for%2520data%250Aaugmentation%252C%2520these%2520approaches%2520do%2520not%2520efficiently%2520harness%2520the%2520full%2520potential%2520of%250Agenerative%2520models.%250A%2520%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520more%2520efficient%2520strategy%2520to%2520construct%250Agenerative%2520datasets%2520for%2520data%2520augmentation%252C%2520termed%2520DiverGen.%2520Firstly%252C%2520we%2520provide%250Aan%2520explanation%2520of%2520the%2520role%2520of%2520generative%2520data%2520from%2520the%2520perspective%2520of%250Adistribution%2520discrepancy.%2520We%2520investigate%2520the%2520impact%2520of%2520different%2520data%2520on%2520the%250Adistribution%2520learned%2520by%2520the%2520model.%2520We%2520argue%2520that%2520generative%2520data%2520can%2520expand%2520the%250Adata%2520distribution%2520that%2520the%2520model%2520can%2520learn%252C%2520thus%2520mitigating%2520overfitting.%250AAdditionally%252C%2520we%2520find%2520that%2520the%2520diversity%2520of%2520generative%2520data%2520is%2520crucial%2520for%250Aimproving%2520model%2520performance%2520and%2520enhance%2520it%2520through%2520various%2520strategies%252C%250Aincluding%2520category%2520diversity%252C%2520prompt%2520diversity%252C%2520and%2520generative%2520model%2520diversity.%250AWith%2520these%2520strategies%252C%2520we%2520can%2520scale%2520the%2520data%2520to%2520millions%2520while%2520maintaining%2520the%250Atrend%2520of%2520model%2520performance%2520improvement.%2520On%2520the%2520LVIS%2520dataset%252C%2520DiverGen%250Asignificantly%2520outperforms%2520the%2520strong%2520model%2520X-Paste%252C%2520achieving%2520%252B1.1%2520box%2520AP%2520and%250A%252B1.1%2520mask%2520AP%2520across%2520all%2520categories%252C%2520and%2520%252B1.9%2520box%2520AP%2520and%2520%252B2.5%2520mask%2520AP%2520for%2520rare%250Acategories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiverGen%3A%20Improving%20Instance%20Segmentation%20by%20Learning%20Wider%20Data%0A%20%20Distribution%20with%20More%20Diverse%20Generative%20Data&entry.906535625=Chengxiang%20Fan%20and%20Muzhi%20Zhu%20and%20Hao%20Chen%20and%20Yang%20Liu%20and%20Weijia%20Wu%20and%20Huaqi%20Zhang%20and%20Chunhua%20Shen&entry.1292438233=%20%20Instance%20segmentation%20is%20data-hungry%2C%20and%20as%20model%20capacity%20increases%2C%20data%0Ascale%20becomes%20crucial%20for%20improving%20the%20accuracy.%20Most%20instance%20segmentation%0Adatasets%20today%20require%20costly%20manual%20annotation%2C%20limiting%20their%20data%20scale.%0AModels%20trained%20on%20such%20data%20are%20prone%20to%20overfitting%20on%20the%20training%20set%2C%0Aespecially%20for%20those%20rare%20categories.%20While%20recent%20works%20have%20delved%20into%0Aexploiting%20generative%20models%20to%20create%20synthetic%20datasets%20for%20data%0Aaugmentation%2C%20these%20approaches%20do%20not%20efficiently%20harness%20the%20full%20potential%20of%0Agenerative%20models.%0A%20%20To%20address%20these%20issues%2C%20we%20introduce%20a%20more%20efficient%20strategy%20to%20construct%0Agenerative%20datasets%20for%20data%20augmentation%2C%20termed%20DiverGen.%20Firstly%2C%20we%20provide%0Aan%20explanation%20of%20the%20role%20of%20generative%20data%20from%20the%20perspective%20of%0Adistribution%20discrepancy.%20We%20investigate%20the%20impact%20of%20different%20data%20on%20the%0Adistribution%20learned%20by%20the%20model.%20We%20argue%20that%20generative%20data%20can%20expand%20the%0Adata%20distribution%20that%20the%20model%20can%20learn%2C%20thus%20mitigating%20overfitting.%0AAdditionally%2C%20we%20find%20that%20the%20diversity%20of%20generative%20data%20is%20crucial%20for%0Aimproving%20model%20performance%20and%20enhance%20it%20through%20various%20strategies%2C%0Aincluding%20category%20diversity%2C%20prompt%20diversity%2C%20and%20generative%20model%20diversity.%0AWith%20these%20strategies%2C%20we%20can%20scale%20the%20data%20to%20millions%20while%20maintaining%20the%0Atrend%20of%20model%20performance%20improvement.%20On%20the%20LVIS%20dataset%2C%20DiverGen%0Asignificantly%20outperforms%20the%20strong%20model%20X-Paste%2C%20achieving%20%2B1.1%20box%20AP%20and%0A%2B1.1%20mask%20AP%20across%20all%20categories%2C%20and%20%2B1.9%20box%20AP%20and%20%2B2.5%20mask%20AP%20for%20rare%0Acategories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10185v1&entry.124074799=Read"},
{"title": "Generative Design through Quality-Diversity Data Synthesis and Language\n  Models", "author": "Adam Gaier and James Stoddart and Lorenzo Villaggi and Shyam Sudhakaran", "abstract": "  Two fundamental challenges face generative models in engineering\napplications: the acquisition of high-performing, diverse datasets, and the\nadherence to precise constraints in generated designs. We propose a novel\napproach combining optimization, constraint satisfaction, and language models\nto tackle these challenges in architectural design. Our method uses\nQuality-Diversity (QD) to generate a diverse, high-performing dataset. We then\nfine-tune a language model with this dataset to generate high-level designs.\nThese designs are then refined into detailed, constraint-compliant layouts\nusing the Wave Function Collapse algorithm. Our system demonstrates reliable\nadherence to textual guidance, enabling the generation of layouts with targeted\narchitectural and performance features. Crucially, our results indicate that\ndata synthesized through the evolutionary search of QD not only improves\noverall model performance but is essential for the model's ability to closely\nadhere to textual guidance. This improvement underscores the pivotal role\nevolutionary computation can play in creating the datasets key to training\ngenerative models for design. Web article at https://tilegpt.github.io\n", "link": "http://arxiv.org/abs/2405.09997v1", "date": "2024-05-16", "relevancy": 2.2618, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5738}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5715}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Design%20through%20Quality-Diversity%20Data%20Synthesis%20and%20Language%0A%20%20Models&body=Title%3A%20Generative%20Design%20through%20Quality-Diversity%20Data%20Synthesis%20and%20Language%0A%20%20Models%0AAuthor%3A%20Adam%20Gaier%20and%20James%20Stoddart%20and%20Lorenzo%20Villaggi%20and%20Shyam%20Sudhakaran%0AAbstract%3A%20%20%20Two%20fundamental%20challenges%20face%20generative%20models%20in%20engineering%0Aapplications%3A%20the%20acquisition%20of%20high-performing%2C%20diverse%20datasets%2C%20and%20the%0Aadherence%20to%20precise%20constraints%20in%20generated%20designs.%20We%20propose%20a%20novel%0Aapproach%20combining%20optimization%2C%20constraint%20satisfaction%2C%20and%20language%20models%0Ato%20tackle%20these%20challenges%20in%20architectural%20design.%20Our%20method%20uses%0AQuality-Diversity%20%28QD%29%20to%20generate%20a%20diverse%2C%20high-performing%20dataset.%20We%20then%0Afine-tune%20a%20language%20model%20with%20this%20dataset%20to%20generate%20high-level%20designs.%0AThese%20designs%20are%20then%20refined%20into%20detailed%2C%20constraint-compliant%20layouts%0Ausing%20the%20Wave%20Function%20Collapse%20algorithm.%20Our%20system%20demonstrates%20reliable%0Aadherence%20to%20textual%20guidance%2C%20enabling%20the%20generation%20of%20layouts%20with%20targeted%0Aarchitectural%20and%20performance%20features.%20Crucially%2C%20our%20results%20indicate%20that%0Adata%20synthesized%20through%20the%20evolutionary%20search%20of%20QD%20not%20only%20improves%0Aoverall%20model%20performance%20but%20is%20essential%20for%20the%20model%27s%20ability%20to%20closely%0Aadhere%20to%20textual%20guidance.%20This%20improvement%20underscores%20the%20pivotal%20role%0Aevolutionary%20computation%20can%20play%20in%20creating%20the%20datasets%20key%20to%20training%0Agenerative%20models%20for%20design.%20Web%20article%20at%20https%3A//tilegpt.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Design%2520through%2520Quality-Diversity%2520Data%2520Synthesis%2520and%2520Language%250A%2520%2520Models%26entry.906535625%3DAdam%2520Gaier%2520and%2520James%2520Stoddart%2520and%2520Lorenzo%2520Villaggi%2520and%2520Shyam%2520Sudhakaran%26entry.1292438233%3D%2520%2520Two%2520fundamental%2520challenges%2520face%2520generative%2520models%2520in%2520engineering%250Aapplications%253A%2520the%2520acquisition%2520of%2520high-performing%252C%2520diverse%2520datasets%252C%2520and%2520the%250Aadherence%2520to%2520precise%2520constraints%2520in%2520generated%2520designs.%2520We%2520propose%2520a%2520novel%250Aapproach%2520combining%2520optimization%252C%2520constraint%2520satisfaction%252C%2520and%2520language%2520models%250Ato%2520tackle%2520these%2520challenges%2520in%2520architectural%2520design.%2520Our%2520method%2520uses%250AQuality-Diversity%2520%2528QD%2529%2520to%2520generate%2520a%2520diverse%252C%2520high-performing%2520dataset.%2520We%2520then%250Afine-tune%2520a%2520language%2520model%2520with%2520this%2520dataset%2520to%2520generate%2520high-level%2520designs.%250AThese%2520designs%2520are%2520then%2520refined%2520into%2520detailed%252C%2520constraint-compliant%2520layouts%250Ausing%2520the%2520Wave%2520Function%2520Collapse%2520algorithm.%2520Our%2520system%2520demonstrates%2520reliable%250Aadherence%2520to%2520textual%2520guidance%252C%2520enabling%2520the%2520generation%2520of%2520layouts%2520with%2520targeted%250Aarchitectural%2520and%2520performance%2520features.%2520Crucially%252C%2520our%2520results%2520indicate%2520that%250Adata%2520synthesized%2520through%2520the%2520evolutionary%2520search%2520of%2520QD%2520not%2520only%2520improves%250Aoverall%2520model%2520performance%2520but%2520is%2520essential%2520for%2520the%2520model%2527s%2520ability%2520to%2520closely%250Aadhere%2520to%2520textual%2520guidance.%2520This%2520improvement%2520underscores%2520the%2520pivotal%2520role%250Aevolutionary%2520computation%2520can%2520play%2520in%2520creating%2520the%2520datasets%2520key%2520to%2520training%250Agenerative%2520models%2520for%2520design.%2520Web%2520article%2520at%2520https%253A//tilegpt.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Design%20through%20Quality-Diversity%20Data%20Synthesis%20and%20Language%0A%20%20Models&entry.906535625=Adam%20Gaier%20and%20James%20Stoddart%20and%20Lorenzo%20Villaggi%20and%20Shyam%20Sudhakaran&entry.1292438233=%20%20Two%20fundamental%20challenges%20face%20generative%20models%20in%20engineering%0Aapplications%3A%20the%20acquisition%20of%20high-performing%2C%20diverse%20datasets%2C%20and%20the%0Aadherence%20to%20precise%20constraints%20in%20generated%20designs.%20We%20propose%20a%20novel%0Aapproach%20combining%20optimization%2C%20constraint%20satisfaction%2C%20and%20language%20models%0Ato%20tackle%20these%20challenges%20in%20architectural%20design.%20Our%20method%20uses%0AQuality-Diversity%20%28QD%29%20to%20generate%20a%20diverse%2C%20high-performing%20dataset.%20We%20then%0Afine-tune%20a%20language%20model%20with%20this%20dataset%20to%20generate%20high-level%20designs.%0AThese%20designs%20are%20then%20refined%20into%20detailed%2C%20constraint-compliant%20layouts%0Ausing%20the%20Wave%20Function%20Collapse%20algorithm.%20Our%20system%20demonstrates%20reliable%0Aadherence%20to%20textual%20guidance%2C%20enabling%20the%20generation%20of%20layouts%20with%20targeted%0Aarchitectural%20and%20performance%20features.%20Crucially%2C%20our%20results%20indicate%20that%0Adata%20synthesized%20through%20the%20evolutionary%20search%20of%20QD%20not%20only%20improves%0Aoverall%20model%20performance%20but%20is%20essential%20for%20the%20model%27s%20ability%20to%20closely%0Aadhere%20to%20textual%20guidance.%20This%20improvement%20underscores%20the%20pivotal%20role%0Aevolutionary%20computation%20can%20play%20in%20creating%20the%20datasets%20key%20to%20training%0Agenerative%20models%20for%20design.%20Web%20article%20at%20https%3A//tilegpt.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09997v1&entry.124074799=Read"},
{"title": "Driving-Video Dehazing with Non-Aligned Regularization for Safety\n  Assistance", "author": "Junkai Fan and Jiangwei Weng and Kun Wang and Yijun Yang and Jianjun Qian and Jun Li and Jian Yang", "abstract": "  Real driving-video dehazing poses a significant challenge due to the inherent\ndifficulty in acquiring precisely aligned hazy/clear video pairs for effective\nmodel training, especially in dynamic driving scenarios with unpredictable\nweather conditions. In this paper, we propose a pioneering approach that\naddresses this challenge through a nonaligned regularization strategy. Our core\nconcept involves identifying clear frames that closely match hazy frames,\nserving as references to supervise a video dehazing network. Our approach\ncomprises two key components: reference matching and video dehazing. Firstly,\nwe introduce a non-aligned reference frame matching module, leveraging an\nadaptive sliding window to match high-quality reference frames from clear\nvideos. Video dehazing incorporates flow-guided cosine attention sampler and\ndeformable cosine attention fusion modules to enhance spatial multiframe\nalignment and fuse their improved information. To validate our approach, we\ncollect a GoProHazy dataset captured effortlessly with GoPro cameras in diverse\nrural and urban road environments. Extensive experiments demonstrate the\nsuperiority of the proposed method over current state-of-the-art methods in the\nchallenging task of real driving-video dehazing. Project page.\n", "link": "http://arxiv.org/abs/2405.09996v1", "date": "2024-05-16", "relevancy": 2.2247, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5747}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5473}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving-Video%20Dehazing%20with%20Non-Aligned%20Regularization%20for%20Safety%0A%20%20Assistance&body=Title%3A%20Driving-Video%20Dehazing%20with%20Non-Aligned%20Regularization%20for%20Safety%0A%20%20Assistance%0AAuthor%3A%20Junkai%20Fan%20and%20Jiangwei%20Weng%20and%20Kun%20Wang%20and%20Yijun%20Yang%20and%20Jianjun%20Qian%20and%20Jun%20Li%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Real%20driving-video%20dehazing%20poses%20a%20significant%20challenge%20due%20to%20the%20inherent%0Adifficulty%20in%20acquiring%20precisely%20aligned%20hazy/clear%20video%20pairs%20for%20effective%0Amodel%20training%2C%20especially%20in%20dynamic%20driving%20scenarios%20with%20unpredictable%0Aweather%20conditions.%20In%20this%20paper%2C%20we%20propose%20a%20pioneering%20approach%20that%0Aaddresses%20this%20challenge%20through%20a%20nonaligned%20regularization%20strategy.%20Our%20core%0Aconcept%20involves%20identifying%20clear%20frames%20that%20closely%20match%20hazy%20frames%2C%0Aserving%20as%20references%20to%20supervise%20a%20video%20dehazing%20network.%20Our%20approach%0Acomprises%20two%20key%20components%3A%20reference%20matching%20and%20video%20dehazing.%20Firstly%2C%0Awe%20introduce%20a%20non-aligned%20reference%20frame%20matching%20module%2C%20leveraging%20an%0Aadaptive%20sliding%20window%20to%20match%20high-quality%20reference%20frames%20from%20clear%0Avideos.%20Video%20dehazing%20incorporates%20flow-guided%20cosine%20attention%20sampler%20and%0Adeformable%20cosine%20attention%20fusion%20modules%20to%20enhance%20spatial%20multiframe%0Aalignment%20and%20fuse%20their%20improved%20information.%20To%20validate%20our%20approach%2C%20we%0Acollect%20a%20GoProHazy%20dataset%20captured%20effortlessly%20with%20GoPro%20cameras%20in%20diverse%0Arural%20and%20urban%20road%20environments.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%20current%20state-of-the-art%20methods%20in%20the%0Achallenging%20task%20of%20real%20driving-video%20dehazing.%20Project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving-Video%2520Dehazing%2520with%2520Non-Aligned%2520Regularization%2520for%2520Safety%250A%2520%2520Assistance%26entry.906535625%3DJunkai%2520Fan%2520and%2520Jiangwei%2520Weng%2520and%2520Kun%2520Wang%2520and%2520Yijun%2520Yang%2520and%2520Jianjun%2520Qian%2520and%2520Jun%2520Li%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Real%2520driving-video%2520dehazing%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520inherent%250Adifficulty%2520in%2520acquiring%2520precisely%2520aligned%2520hazy/clear%2520video%2520pairs%2520for%2520effective%250Amodel%2520training%252C%2520especially%2520in%2520dynamic%2520driving%2520scenarios%2520with%2520unpredictable%250Aweather%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520pioneering%2520approach%2520that%250Aaddresses%2520this%2520challenge%2520through%2520a%2520nonaligned%2520regularization%2520strategy.%2520Our%2520core%250Aconcept%2520involves%2520identifying%2520clear%2520frames%2520that%2520closely%2520match%2520hazy%2520frames%252C%250Aserving%2520as%2520references%2520to%2520supervise%2520a%2520video%2520dehazing%2520network.%2520Our%2520approach%250Acomprises%2520two%2520key%2520components%253A%2520reference%2520matching%2520and%2520video%2520dehazing.%2520Firstly%252C%250Awe%2520introduce%2520a%2520non-aligned%2520reference%2520frame%2520matching%2520module%252C%2520leveraging%2520an%250Aadaptive%2520sliding%2520window%2520to%2520match%2520high-quality%2520reference%2520frames%2520from%2520clear%250Avideos.%2520Video%2520dehazing%2520incorporates%2520flow-guided%2520cosine%2520attention%2520sampler%2520and%250Adeformable%2520cosine%2520attention%2520fusion%2520modules%2520to%2520enhance%2520spatial%2520multiframe%250Aalignment%2520and%2520fuse%2520their%2520improved%2520information.%2520To%2520validate%2520our%2520approach%252C%2520we%250Acollect%2520a%2520GoProHazy%2520dataset%2520captured%2520effortlessly%2520with%2520GoPro%2520cameras%2520in%2520diverse%250Arural%2520and%2520urban%2520road%2520environments.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method%2520over%2520current%2520state-of-the-art%2520methods%2520in%2520the%250Achallenging%2520task%2520of%2520real%2520driving-video%2520dehazing.%2520Project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving-Video%20Dehazing%20with%20Non-Aligned%20Regularization%20for%20Safety%0A%20%20Assistance&entry.906535625=Junkai%20Fan%20and%20Jiangwei%20Weng%20and%20Kun%20Wang%20and%20Yijun%20Yang%20and%20Jianjun%20Qian%20and%20Jun%20Li%20and%20Jian%20Yang&entry.1292438233=%20%20Real%20driving-video%20dehazing%20poses%20a%20significant%20challenge%20due%20to%20the%20inherent%0Adifficulty%20in%20acquiring%20precisely%20aligned%20hazy/clear%20video%20pairs%20for%20effective%0Amodel%20training%2C%20especially%20in%20dynamic%20driving%20scenarios%20with%20unpredictable%0Aweather%20conditions.%20In%20this%20paper%2C%20we%20propose%20a%20pioneering%20approach%20that%0Aaddresses%20this%20challenge%20through%20a%20nonaligned%20regularization%20strategy.%20Our%20core%0Aconcept%20involves%20identifying%20clear%20frames%20that%20closely%20match%20hazy%20frames%2C%0Aserving%20as%20references%20to%20supervise%20a%20video%20dehazing%20network.%20Our%20approach%0Acomprises%20two%20key%20components%3A%20reference%20matching%20and%20video%20dehazing.%20Firstly%2C%0Awe%20introduce%20a%20non-aligned%20reference%20frame%20matching%20module%2C%20leveraging%20an%0Aadaptive%20sliding%20window%20to%20match%20high-quality%20reference%20frames%20from%20clear%0Avideos.%20Video%20dehazing%20incorporates%20flow-guided%20cosine%20attention%20sampler%20and%0Adeformable%20cosine%20attention%20fusion%20modules%20to%20enhance%20spatial%20multiframe%0Aalignment%20and%20fuse%20their%20improved%20information.%20To%20validate%20our%20approach%2C%20we%0Acollect%20a%20GoProHazy%20dataset%20captured%20effortlessly%20with%20GoPro%20cameras%20in%20diverse%0Arural%20and%20urban%20road%20environments.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20over%20current%20state-of-the-art%20methods%20in%20the%0Achallenging%20task%20of%20real%20driving-video%20dehazing.%20Project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09996v1&entry.124074799=Read"},
{"title": "Text-to-Vector Generation with Neural Path Representation", "author": "Peiying Zhang and Nanxuan Zhao and Jing Liao", "abstract": "  Vector graphics are widely used in digital art and highly favored by\ndesigners due to their scalability and layer-wise properties. However, the\nprocess of creating and editing vector graphics requires creativity and design\nexpertise, making it a time-consuming task. Recent advancements in\ntext-to-vector (T2V) generation have aimed to make this process more\naccessible. However, existing T2V methods directly optimize control points of\nvector graphics paths, often resulting in intersecting or jagged paths due to\nthe lack of geometry constraints. To overcome these limitations, we propose a\nnovel neural path representation by designing a dual-branch Variational\nAutoencoder (VAE) that learns the path latent space from both sequence and\nimage modalities. By optimizing the combination of neural paths, we can\nincorporate geometric constraints while preserving expressivity in generated\nSVGs. Furthermore, we introduce a two-stage path optimization method to improve\nthe visual and topological quality of generated SVGs. In the first stage, a\npre-trained text-to-image diffusion model guides the initial generation of\ncomplex vector graphics through the Variational Score Distillation (VSD)\nprocess. In the second stage, we refine the graphics using a layer-wise image\nvectorization strategy to achieve clearer elements and structure. We\ndemonstrate the effectiveness of our method through extensive experiments and\nshowcase various applications. The project page is\nhttps://intchous.github.io/T2V-NPR.\n", "link": "http://arxiv.org/abs/2405.10317v1", "date": "2024-05-16", "relevancy": 2.1958, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.557}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5529}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation&body=Title%3A%20Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation%0AAuthor%3A%20Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Jing%20Liao%0AAbstract%3A%20%20%20Vector%20graphics%20are%20widely%20used%20in%20digital%20art%20and%20highly%20favored%20by%0Adesigners%20due%20to%20their%20scalability%20and%20layer-wise%20properties.%20However%2C%20the%0Aprocess%20of%20creating%20and%20editing%20vector%20graphics%20requires%20creativity%20and%20design%0Aexpertise%2C%20making%20it%20a%20time-consuming%20task.%20Recent%20advancements%20in%0Atext-to-vector%20%28T2V%29%20generation%20have%20aimed%20to%20make%20this%20process%20more%0Aaccessible.%20However%2C%20existing%20T2V%20methods%20directly%20optimize%20control%20points%20of%0Avector%20graphics%20paths%2C%20often%20resulting%20in%20intersecting%20or%20jagged%20paths%20due%20to%0Athe%20lack%20of%20geometry%20constraints.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20neural%20path%20representation%20by%20designing%20a%20dual-branch%20Variational%0AAutoencoder%20%28VAE%29%20that%20learns%20the%20path%20latent%20space%20from%20both%20sequence%20and%0Aimage%20modalities.%20By%20optimizing%20the%20combination%20of%20neural%20paths%2C%20we%20can%0Aincorporate%20geometric%20constraints%20while%20preserving%20expressivity%20in%20generated%0ASVGs.%20Furthermore%2C%20we%20introduce%20a%20two-stage%20path%20optimization%20method%20to%20improve%0Athe%20visual%20and%20topological%20quality%20of%20generated%20SVGs.%20In%20the%20first%20stage%2C%20a%0Apre-trained%20text-to-image%20diffusion%20model%20guides%20the%20initial%20generation%20of%0Acomplex%20vector%20graphics%20through%20the%20Variational%20Score%20Distillation%20%28VSD%29%0Aprocess.%20In%20the%20second%20stage%2C%20we%20refine%20the%20graphics%20using%20a%20layer-wise%20image%0Avectorization%20strategy%20to%20achieve%20clearer%20elements%20and%20structure.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%20experiments%20and%0Ashowcase%20various%20applications.%20The%20project%20page%20is%0Ahttps%3A//intchous.github.io/T2V-NPR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Vector%2520Generation%2520with%2520Neural%2520Path%2520Representation%26entry.906535625%3DPeiying%2520Zhang%2520and%2520Nanxuan%2520Zhao%2520and%2520Jing%2520Liao%26entry.1292438233%3D%2520%2520Vector%2520graphics%2520are%2520widely%2520used%2520in%2520digital%2520art%2520and%2520highly%2520favored%2520by%250Adesigners%2520due%2520to%2520their%2520scalability%2520and%2520layer-wise%2520properties.%2520However%252C%2520the%250Aprocess%2520of%2520creating%2520and%2520editing%2520vector%2520graphics%2520requires%2520creativity%2520and%2520design%250Aexpertise%252C%2520making%2520it%2520a%2520time-consuming%2520task.%2520Recent%2520advancements%2520in%250Atext-to-vector%2520%2528T2V%2529%2520generation%2520have%2520aimed%2520to%2520make%2520this%2520process%2520more%250Aaccessible.%2520However%252C%2520existing%2520T2V%2520methods%2520directly%2520optimize%2520control%2520points%2520of%250Avector%2520graphics%2520paths%252C%2520often%2520resulting%2520in%2520intersecting%2520or%2520jagged%2520paths%2520due%2520to%250Athe%2520lack%2520of%2520geometry%2520constraints.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520neural%2520path%2520representation%2520by%2520designing%2520a%2520dual-branch%2520Variational%250AAutoencoder%2520%2528VAE%2529%2520that%2520learns%2520the%2520path%2520latent%2520space%2520from%2520both%2520sequence%2520and%250Aimage%2520modalities.%2520By%2520optimizing%2520the%2520combination%2520of%2520neural%2520paths%252C%2520we%2520can%250Aincorporate%2520geometric%2520constraints%2520while%2520preserving%2520expressivity%2520in%2520generated%250ASVGs.%2520Furthermore%252C%2520we%2520introduce%2520a%2520two-stage%2520path%2520optimization%2520method%2520to%2520improve%250Athe%2520visual%2520and%2520topological%2520quality%2520of%2520generated%2520SVGs.%2520In%2520the%2520first%2520stage%252C%2520a%250Apre-trained%2520text-to-image%2520diffusion%2520model%2520guides%2520the%2520initial%2520generation%2520of%250Acomplex%2520vector%2520graphics%2520through%2520the%2520Variational%2520Score%2520Distillation%2520%2528VSD%2529%250Aprocess.%2520In%2520the%2520second%2520stage%252C%2520we%2520refine%2520the%2520graphics%2520using%2520a%2520layer-wise%2520image%250Avectorization%2520strategy%2520to%2520achieve%2520clearer%2520elements%2520and%2520structure.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%2520extensive%2520experiments%2520and%250Ashowcase%2520various%2520applications.%2520The%2520project%2520page%2520is%250Ahttps%253A//intchous.github.io/T2V-NPR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Vector%20Generation%20with%20Neural%20Path%20Representation&entry.906535625=Peiying%20Zhang%20and%20Nanxuan%20Zhao%20and%20Jing%20Liao&entry.1292438233=%20%20Vector%20graphics%20are%20widely%20used%20in%20digital%20art%20and%20highly%20favored%20by%0Adesigners%20due%20to%20their%20scalability%20and%20layer-wise%20properties.%20However%2C%20the%0Aprocess%20of%20creating%20and%20editing%20vector%20graphics%20requires%20creativity%20and%20design%0Aexpertise%2C%20making%20it%20a%20time-consuming%20task.%20Recent%20advancements%20in%0Atext-to-vector%20%28T2V%29%20generation%20have%20aimed%20to%20make%20this%20process%20more%0Aaccessible.%20However%2C%20existing%20T2V%20methods%20directly%20optimize%20control%20points%20of%0Avector%20graphics%20paths%2C%20often%20resulting%20in%20intersecting%20or%20jagged%20paths%20due%20to%0Athe%20lack%20of%20geometry%20constraints.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20neural%20path%20representation%20by%20designing%20a%20dual-branch%20Variational%0AAutoencoder%20%28VAE%29%20that%20learns%20the%20path%20latent%20space%20from%20both%20sequence%20and%0Aimage%20modalities.%20By%20optimizing%20the%20combination%20of%20neural%20paths%2C%20we%20can%0Aincorporate%20geometric%20constraints%20while%20preserving%20expressivity%20in%20generated%0ASVGs.%20Furthermore%2C%20we%20introduce%20a%20two-stage%20path%20optimization%20method%20to%20improve%0Athe%20visual%20and%20topological%20quality%20of%20generated%20SVGs.%20In%20the%20first%20stage%2C%20a%0Apre-trained%20text-to-image%20diffusion%20model%20guides%20the%20initial%20generation%20of%0Acomplex%20vector%20graphics%20through%20the%20Variational%20Score%20Distillation%20%28VSD%29%0Aprocess.%20In%20the%20second%20stage%2C%20we%20refine%20the%20graphics%20using%20a%20layer-wise%20image%0Avectorization%20strategy%20to%20achieve%20clearer%20elements%20and%20structure.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20through%20extensive%20experiments%20and%0Ashowcase%20various%20applications.%20The%20project%20page%20is%0Ahttps%3A//intchous.github.io/T2V-NPR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10317v1&entry.124074799=Read"},
{"title": "SpecNeRF: Gaussian Directional Encoding for Specular Reflections", "author": "Li Ma and Vasu Agrawal and Haithem Turki and Changil Kim and Chen Gao and Pedro Sander and Michael Zollh\u00f6fer and Christian Richardt", "abstract": "  Neural radiance fields have achieved remarkable performance in modeling the\nappearance of 3D scenes. However, existing approaches still struggle with the\nview-dependent appearance of glossy surfaces, especially under complex lighting\nof indoor environments. Unlike existing methods, which typically assume distant\nlighting like an environment map, we propose a learnable Gaussian directional\nencoding to better model the view-dependent effects under near-field lighting\nconditions. Importantly, our new directional encoding captures the\nspatially-varying nature of near-field lighting and emulates the behavior of\nprefiltered environment maps. As a result, it enables the efficient evaluation\nof preconvolved specular color at any 3D location with varying roughness\ncoefficients. We further introduce a data-driven geometry prior that helps\nalleviate the shape radiance ambiguity in reflection modeling. We show that our\nGaussian directional encoding and geometry prior significantly improve the\nmodeling of challenging specular reflections in neural radiance fields, which\nhelps decompose appearance into more physically meaningful components.\n", "link": "http://arxiv.org/abs/2312.13102v3", "date": "2024-05-16", "relevancy": 2.1956, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5847}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5371}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecNeRF%3A%20Gaussian%20Directional%20Encoding%20for%20Specular%20Reflections&body=Title%3A%20SpecNeRF%3A%20Gaussian%20Directional%20Encoding%20for%20Specular%20Reflections%0AAuthor%3A%20Li%20Ma%20and%20Vasu%20Agrawal%20and%20Haithem%20Turki%20and%20Changil%20Kim%20and%20Chen%20Gao%20and%20Pedro%20Sander%20and%20Michael%20Zollh%C3%B6fer%20and%20Christian%20Richardt%0AAbstract%3A%20%20%20Neural%20radiance%20fields%20have%20achieved%20remarkable%20performance%20in%20modeling%20the%0Aappearance%20of%203D%20scenes.%20However%2C%20existing%20approaches%20still%20struggle%20with%20the%0Aview-dependent%20appearance%20of%20glossy%20surfaces%2C%20especially%20under%20complex%20lighting%0Aof%20indoor%20environments.%20Unlike%20existing%20methods%2C%20which%20typically%20assume%20distant%0Alighting%20like%20an%20environment%20map%2C%20we%20propose%20a%20learnable%20Gaussian%20directional%0Aencoding%20to%20better%20model%20the%20view-dependent%20effects%20under%20near-field%20lighting%0Aconditions.%20Importantly%2C%20our%20new%20directional%20encoding%20captures%20the%0Aspatially-varying%20nature%20of%20near-field%20lighting%20and%20emulates%20the%20behavior%20of%0Aprefiltered%20environment%20maps.%20As%20a%20result%2C%20it%20enables%20the%20efficient%20evaluation%0Aof%20preconvolved%20specular%20color%20at%20any%203D%20location%20with%20varying%20roughness%0Acoefficients.%20We%20further%20introduce%20a%20data-driven%20geometry%20prior%20that%20helps%0Aalleviate%20the%20shape%20radiance%20ambiguity%20in%20reflection%20modeling.%20We%20show%20that%20our%0AGaussian%20directional%20encoding%20and%20geometry%20prior%20significantly%20improve%20the%0Amodeling%20of%20challenging%20specular%20reflections%20in%20neural%20radiance%20fields%2C%20which%0Ahelps%20decompose%20appearance%20into%20more%20physically%20meaningful%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13102v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecNeRF%253A%2520Gaussian%2520Directional%2520Encoding%2520for%2520Specular%2520Reflections%26entry.906535625%3DLi%2520Ma%2520and%2520Vasu%2520Agrawal%2520and%2520Haithem%2520Turki%2520and%2520Changil%2520Kim%2520and%2520Chen%2520Gao%2520and%2520Pedro%2520Sander%2520and%2520Michael%2520Zollh%25C3%25B6fer%2520and%2520Christian%2520Richardt%26entry.1292438233%3D%2520%2520Neural%2520radiance%2520fields%2520have%2520achieved%2520remarkable%2520performance%2520in%2520modeling%2520the%250Aappearance%2520of%25203D%2520scenes.%2520However%252C%2520existing%2520approaches%2520still%2520struggle%2520with%2520the%250Aview-dependent%2520appearance%2520of%2520glossy%2520surfaces%252C%2520especially%2520under%2520complex%2520lighting%250Aof%2520indoor%2520environments.%2520Unlike%2520existing%2520methods%252C%2520which%2520typically%2520assume%2520distant%250Alighting%2520like%2520an%2520environment%2520map%252C%2520we%2520propose%2520a%2520learnable%2520Gaussian%2520directional%250Aencoding%2520to%2520better%2520model%2520the%2520view-dependent%2520effects%2520under%2520near-field%2520lighting%250Aconditions.%2520Importantly%252C%2520our%2520new%2520directional%2520encoding%2520captures%2520the%250Aspatially-varying%2520nature%2520of%2520near-field%2520lighting%2520and%2520emulates%2520the%2520behavior%2520of%250Aprefiltered%2520environment%2520maps.%2520As%2520a%2520result%252C%2520it%2520enables%2520the%2520efficient%2520evaluation%250Aof%2520preconvolved%2520specular%2520color%2520at%2520any%25203D%2520location%2520with%2520varying%2520roughness%250Acoefficients.%2520We%2520further%2520introduce%2520a%2520data-driven%2520geometry%2520prior%2520that%2520helps%250Aalleviate%2520the%2520shape%2520radiance%2520ambiguity%2520in%2520reflection%2520modeling.%2520We%2520show%2520that%2520our%250AGaussian%2520directional%2520encoding%2520and%2520geometry%2520prior%2520significantly%2520improve%2520the%250Amodeling%2520of%2520challenging%2520specular%2520reflections%2520in%2520neural%2520radiance%2520fields%252C%2520which%250Ahelps%2520decompose%2520appearance%2520into%2520more%2520physically%2520meaningful%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13102v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecNeRF%3A%20Gaussian%20Directional%20Encoding%20for%20Specular%20Reflections&entry.906535625=Li%20Ma%20and%20Vasu%20Agrawal%20and%20Haithem%20Turki%20and%20Changil%20Kim%20and%20Chen%20Gao%20and%20Pedro%20Sander%20and%20Michael%20Zollh%C3%B6fer%20and%20Christian%20Richardt&entry.1292438233=%20%20Neural%20radiance%20fields%20have%20achieved%20remarkable%20performance%20in%20modeling%20the%0Aappearance%20of%203D%20scenes.%20However%2C%20existing%20approaches%20still%20struggle%20with%20the%0Aview-dependent%20appearance%20of%20glossy%20surfaces%2C%20especially%20under%20complex%20lighting%0Aof%20indoor%20environments.%20Unlike%20existing%20methods%2C%20which%20typically%20assume%20distant%0Alighting%20like%20an%20environment%20map%2C%20we%20propose%20a%20learnable%20Gaussian%20directional%0Aencoding%20to%20better%20model%20the%20view-dependent%20effects%20under%20near-field%20lighting%0Aconditions.%20Importantly%2C%20our%20new%20directional%20encoding%20captures%20the%0Aspatially-varying%20nature%20of%20near-field%20lighting%20and%20emulates%20the%20behavior%20of%0Aprefiltered%20environment%20maps.%20As%20a%20result%2C%20it%20enables%20the%20efficient%20evaluation%0Aof%20preconvolved%20specular%20color%20at%20any%203D%20location%20with%20varying%20roughness%0Acoefficients.%20We%20further%20introduce%20a%20data-driven%20geometry%20prior%20that%20helps%0Aalleviate%20the%20shape%20radiance%20ambiguity%20in%20reflection%20modeling.%20We%20show%20that%20our%0AGaussian%20directional%20encoding%20and%20geometry%20prior%20significantly%20improve%20the%0Amodeling%20of%20challenging%20specular%20reflections%20in%20neural%20radiance%20fields%2C%20which%0Ahelps%20decompose%20appearance%20into%20more%20physically%20meaningful%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13102v3&entry.124074799=Read"},
{"title": "A Unified Deep Transfer Learning Model for Accurate IoT Localization in\n  Diverse Environments", "author": "Abdullahi Isa Ahmed and Yaya Etiabi and Ali Waqar Azim and El Mehdi Amhoud", "abstract": "  Internet of Things (IoT) is an ever-evolving technological paradigm that is\nreshaping industries and societies globally. Real-time data collection,\nanalysis, and decision-making facilitated by localization solutions form the\nfoundation for location-based services, enabling them to support critical\nfunctions within diverse IoT ecosystems. However, most existing works on\nlocalization focus on single environment, resulting in the development of\nmultiple models to support multiple environments. In the context of smart\ncities, these raise costs and complexity due to the dynamicity of such\nenvironments. To address these challenges, this paper presents a unified\nindoor-outdoor localization solution that leverages transfer learning (TL)\nschemes to build a single deep learning model. The model accurately predicts\nthe localization of IoT devices in diverse environments. The performance\nevaluation shows that by adopting an encoder-based TL scheme, we can improve\nthe baseline model by about 17.18% in indoor environments and 9.79% in outdoor\nenvironments.\n", "link": "http://arxiv.org/abs/2405.09960v1", "date": "2024-05-16", "relevancy": 2.1928, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Deep%20Transfer%20Learning%20Model%20for%20Accurate%20IoT%20Localization%20in%0A%20%20Diverse%20Environments&body=Title%3A%20A%20Unified%20Deep%20Transfer%20Learning%20Model%20for%20Accurate%20IoT%20Localization%20in%0A%20%20Diverse%20Environments%0AAuthor%3A%20Abdullahi%20Isa%20Ahmed%20and%20Yaya%20Etiabi%20and%20Ali%20Waqar%20Azim%20and%20El%20Mehdi%20Amhoud%0AAbstract%3A%20%20%20Internet%20of%20Things%20%28IoT%29%20is%20an%20ever-evolving%20technological%20paradigm%20that%20is%0Areshaping%20industries%20and%20societies%20globally.%20Real-time%20data%20collection%2C%0Aanalysis%2C%20and%20decision-making%20facilitated%20by%20localization%20solutions%20form%20the%0Afoundation%20for%20location-based%20services%2C%20enabling%20them%20to%20support%20critical%0Afunctions%20within%20diverse%20IoT%20ecosystems.%20However%2C%20most%20existing%20works%20on%0Alocalization%20focus%20on%20single%20environment%2C%20resulting%20in%20the%20development%20of%0Amultiple%20models%20to%20support%20multiple%20environments.%20In%20the%20context%20of%20smart%0Acities%2C%20these%20raise%20costs%20and%20complexity%20due%20to%20the%20dynamicity%20of%20such%0Aenvironments.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20a%20unified%0Aindoor-outdoor%20localization%20solution%20that%20leverages%20transfer%20learning%20%28TL%29%0Aschemes%20to%20build%20a%20single%20deep%20learning%20model.%20The%20model%20accurately%20predicts%0Athe%20localization%20of%20IoT%20devices%20in%20diverse%20environments.%20The%20performance%0Aevaluation%20shows%20that%20by%20adopting%20an%20encoder-based%20TL%20scheme%2C%20we%20can%20improve%0Athe%20baseline%20model%20by%20about%2017.18%25%20in%20indoor%20environments%20and%209.79%25%20in%20outdoor%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Deep%2520Transfer%2520Learning%2520Model%2520for%2520Accurate%2520IoT%2520Localization%2520in%250A%2520%2520Diverse%2520Environments%26entry.906535625%3DAbdullahi%2520Isa%2520Ahmed%2520and%2520Yaya%2520Etiabi%2520and%2520Ali%2520Waqar%2520Azim%2520and%2520El%2520Mehdi%2520Amhoud%26entry.1292438233%3D%2520%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520is%2520an%2520ever-evolving%2520technological%2520paradigm%2520that%2520is%250Areshaping%2520industries%2520and%2520societies%2520globally.%2520Real-time%2520data%2520collection%252C%250Aanalysis%252C%2520and%2520decision-making%2520facilitated%2520by%2520localization%2520solutions%2520form%2520the%250Afoundation%2520for%2520location-based%2520services%252C%2520enabling%2520them%2520to%2520support%2520critical%250Afunctions%2520within%2520diverse%2520IoT%2520ecosystems.%2520However%252C%2520most%2520existing%2520works%2520on%250Alocalization%2520focus%2520on%2520single%2520environment%252C%2520resulting%2520in%2520the%2520development%2520of%250Amultiple%2520models%2520to%2520support%2520multiple%2520environments.%2520In%2520the%2520context%2520of%2520smart%250Acities%252C%2520these%2520raise%2520costs%2520and%2520complexity%2520due%2520to%2520the%2520dynamicity%2520of%2520such%250Aenvironments.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520a%2520unified%250Aindoor-outdoor%2520localization%2520solution%2520that%2520leverages%2520transfer%2520learning%2520%2528TL%2529%250Aschemes%2520to%2520build%2520a%2520single%2520deep%2520learning%2520model.%2520The%2520model%2520accurately%2520predicts%250Athe%2520localization%2520of%2520IoT%2520devices%2520in%2520diverse%2520environments.%2520The%2520performance%250Aevaluation%2520shows%2520that%2520by%2520adopting%2520an%2520encoder-based%2520TL%2520scheme%252C%2520we%2520can%2520improve%250Athe%2520baseline%2520model%2520by%2520about%252017.18%2525%2520in%2520indoor%2520environments%2520and%25209.79%2525%2520in%2520outdoor%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Deep%20Transfer%20Learning%20Model%20for%20Accurate%20IoT%20Localization%20in%0A%20%20Diverse%20Environments&entry.906535625=Abdullahi%20Isa%20Ahmed%20and%20Yaya%20Etiabi%20and%20Ali%20Waqar%20Azim%20and%20El%20Mehdi%20Amhoud&entry.1292438233=%20%20Internet%20of%20Things%20%28IoT%29%20is%20an%20ever-evolving%20technological%20paradigm%20that%20is%0Areshaping%20industries%20and%20societies%20globally.%20Real-time%20data%20collection%2C%0Aanalysis%2C%20and%20decision-making%20facilitated%20by%20localization%20solutions%20form%20the%0Afoundation%20for%20location-based%20services%2C%20enabling%20them%20to%20support%20critical%0Afunctions%20within%20diverse%20IoT%20ecosystems.%20However%2C%20most%20existing%20works%20on%0Alocalization%20focus%20on%20single%20environment%2C%20resulting%20in%20the%20development%20of%0Amultiple%20models%20to%20support%20multiple%20environments.%20In%20the%20context%20of%20smart%0Acities%2C%20these%20raise%20costs%20and%20complexity%20due%20to%20the%20dynamicity%20of%20such%0Aenvironments.%20To%20address%20these%20challenges%2C%20this%20paper%20presents%20a%20unified%0Aindoor-outdoor%20localization%20solution%20that%20leverages%20transfer%20learning%20%28TL%29%0Aschemes%20to%20build%20a%20single%20deep%20learning%20model.%20The%20model%20accurately%20predicts%0Athe%20localization%20of%20IoT%20devices%20in%20diverse%20environments.%20The%20performance%0Aevaluation%20shows%20that%20by%20adopting%20an%20encoder-based%20TL%20scheme%2C%20we%20can%20improve%0Athe%20baseline%20model%20by%20about%2017.18%25%20in%20indoor%20environments%20and%209.79%25%20in%20outdoor%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09960v1&entry.124074799=Read"},
{"title": "GraCo: Granularity-Controllable Interactive Segmentation", "author": "Yian Zhao and Kehan Li and Zesen Cheng and Pengchong Qiao and Xiawu Zheng and Rongrong Ji and Chang Liu and Li Yuan and Jie Chen", "abstract": "  Interactive Segmentation (IS) segments specific objects or parts in the image\naccording to user input. Current IS pipelines fall into two categories:\nsingle-granularity output and multi-granularity output. The latter aims to\nalleviate the spatial ambiguity present in the former. However, the\nmulti-granularity output pipeline suffers from limited interaction flexibility\nand produces redundant results. In this work, we introduce\nGranularity-Controllable Interactive Segmentation (GraCo), a novel approach\nthat allows precise control of prediction granularity by introducing additional\nparameters to input. This enhances the customization of the interactive system\nand eliminates redundancy while resolving ambiguity. Nevertheless, the\nexorbitant cost of annotating multi-granularity masks and the lack of available\ndatasets with granularity annotations make it difficult for models to acquire\nthe necessary guidance to control output granularity. To address this problem,\nwe design an any-granularity mask generator that exploits the semantic property\nof the pre-trained IS model to automatically generate abundant mask-granularity\npairs without requiring additional manual annotation. Based on these pairs, we\npropose a granularity-controllable learning strategy that efficiently imparts\nthe granularity controllability to the IS model. Extensive experiments on\nintricate scenarios at object and part levels demonstrate that our GraCo has\nsignificant advantages over previous methods. This highlights the potential of\nGraCo to be a flexible annotation tool, capable of adapting to diverse\nsegmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.\n", "link": "http://arxiv.org/abs/2405.00587v2", "date": "2024-05-16", "relevancy": 2.1918, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5621}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5501}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation&body=Title%3A%20GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation%0AAuthor%3A%20Yian%20Zhao%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Pengchong%20Qiao%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Chang%20Liu%20and%20Li%20Yuan%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Interactive%20Segmentation%20%28IS%29%20segments%20specific%20objects%20or%20parts%20in%20the%20image%0Aaccording%20to%20user%20input.%20Current%20IS%20pipelines%20fall%20into%20two%20categories%3A%0Asingle-granularity%20output%20and%20multi-granularity%20output.%20The%20latter%20aims%20to%0Aalleviate%20the%20spatial%20ambiguity%20present%20in%20the%20former.%20However%2C%20the%0Amulti-granularity%20output%20pipeline%20suffers%20from%20limited%20interaction%20flexibility%0Aand%20produces%20redundant%20results.%20In%20this%20work%2C%20we%20introduce%0AGranularity-Controllable%20Interactive%20Segmentation%20%28GraCo%29%2C%20a%20novel%20approach%0Athat%20allows%20precise%20control%20of%20prediction%20granularity%20by%20introducing%20additional%0Aparameters%20to%20input.%20This%20enhances%20the%20customization%20of%20the%20interactive%20system%0Aand%20eliminates%20redundancy%20while%20resolving%20ambiguity.%20Nevertheless%2C%20the%0Aexorbitant%20cost%20of%20annotating%20multi-granularity%20masks%20and%20the%20lack%20of%20available%0Adatasets%20with%20granularity%20annotations%20make%20it%20difficult%20for%20models%20to%20acquire%0Athe%20necessary%20guidance%20to%20control%20output%20granularity.%20To%20address%20this%20problem%2C%0Awe%20design%20an%20any-granularity%20mask%20generator%20that%20exploits%20the%20semantic%20property%0Aof%20the%20pre-trained%20IS%20model%20to%20automatically%20generate%20abundant%20mask-granularity%0Apairs%20without%20requiring%20additional%20manual%20annotation.%20Based%20on%20these%20pairs%2C%20we%0Apropose%20a%20granularity-controllable%20learning%20strategy%20that%20efficiently%20imparts%0Athe%20granularity%20controllability%20to%20the%20IS%20model.%20Extensive%20experiments%20on%0Aintricate%20scenarios%20at%20object%20and%20part%20levels%20demonstrate%20that%20our%20GraCo%20has%0Asignificant%20advantages%20over%20previous%20methods.%20This%20highlights%20the%20potential%20of%0AGraCo%20to%20be%20a%20flexible%20annotation%20tool%2C%20capable%20of%20adapting%20to%20diverse%0Asegmentation%20scenarios.%20The%20project%20page%3A%20https%3A//zhao-yian.github.io/GraCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraCo%253A%2520Granularity-Controllable%2520Interactive%2520Segmentation%26entry.906535625%3DYian%2520Zhao%2520and%2520Kehan%2520Li%2520and%2520Zesen%2520Cheng%2520and%2520Pengchong%2520Qiao%2520and%2520Xiawu%2520Zheng%2520and%2520Rongrong%2520Ji%2520and%2520Chang%2520Liu%2520and%2520Li%2520Yuan%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520Interactive%2520Segmentation%2520%2528IS%2529%2520segments%2520specific%2520objects%2520or%2520parts%2520in%2520the%2520image%250Aaccording%2520to%2520user%2520input.%2520Current%2520IS%2520pipelines%2520fall%2520into%2520two%2520categories%253A%250Asingle-granularity%2520output%2520and%2520multi-granularity%2520output.%2520The%2520latter%2520aims%2520to%250Aalleviate%2520the%2520spatial%2520ambiguity%2520present%2520in%2520the%2520former.%2520However%252C%2520the%250Amulti-granularity%2520output%2520pipeline%2520suffers%2520from%2520limited%2520interaction%2520flexibility%250Aand%2520produces%2520redundant%2520results.%2520In%2520this%2520work%252C%2520we%2520introduce%250AGranularity-Controllable%2520Interactive%2520Segmentation%2520%2528GraCo%2529%252C%2520a%2520novel%2520approach%250Athat%2520allows%2520precise%2520control%2520of%2520prediction%2520granularity%2520by%2520introducing%2520additional%250Aparameters%2520to%2520input.%2520This%2520enhances%2520the%2520customization%2520of%2520the%2520interactive%2520system%250Aand%2520eliminates%2520redundancy%2520while%2520resolving%2520ambiguity.%2520Nevertheless%252C%2520the%250Aexorbitant%2520cost%2520of%2520annotating%2520multi-granularity%2520masks%2520and%2520the%2520lack%2520of%2520available%250Adatasets%2520with%2520granularity%2520annotations%2520make%2520it%2520difficult%2520for%2520models%2520to%2520acquire%250Athe%2520necessary%2520guidance%2520to%2520control%2520output%2520granularity.%2520To%2520address%2520this%2520problem%252C%250Awe%2520design%2520an%2520any-granularity%2520mask%2520generator%2520that%2520exploits%2520the%2520semantic%2520property%250Aof%2520the%2520pre-trained%2520IS%2520model%2520to%2520automatically%2520generate%2520abundant%2520mask-granularity%250Apairs%2520without%2520requiring%2520additional%2520manual%2520annotation.%2520Based%2520on%2520these%2520pairs%252C%2520we%250Apropose%2520a%2520granularity-controllable%2520learning%2520strategy%2520that%2520efficiently%2520imparts%250Athe%2520granularity%2520controllability%2520to%2520the%2520IS%2520model.%2520Extensive%2520experiments%2520on%250Aintricate%2520scenarios%2520at%2520object%2520and%2520part%2520levels%2520demonstrate%2520that%2520our%2520GraCo%2520has%250Asignificant%2520advantages%2520over%2520previous%2520methods.%2520This%2520highlights%2520the%2520potential%2520of%250AGraCo%2520to%2520be%2520a%2520flexible%2520annotation%2520tool%252C%2520capable%2520of%2520adapting%2520to%2520diverse%250Asegmentation%2520scenarios.%2520The%2520project%2520page%253A%2520https%253A//zhao-yian.github.io/GraCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraCo%3A%20Granularity-Controllable%20Interactive%20Segmentation&entry.906535625=Yian%20Zhao%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Pengchong%20Qiao%20and%20Xiawu%20Zheng%20and%20Rongrong%20Ji%20and%20Chang%20Liu%20and%20Li%20Yuan%20and%20Jie%20Chen&entry.1292438233=%20%20Interactive%20Segmentation%20%28IS%29%20segments%20specific%20objects%20or%20parts%20in%20the%20image%0Aaccording%20to%20user%20input.%20Current%20IS%20pipelines%20fall%20into%20two%20categories%3A%0Asingle-granularity%20output%20and%20multi-granularity%20output.%20The%20latter%20aims%20to%0Aalleviate%20the%20spatial%20ambiguity%20present%20in%20the%20former.%20However%2C%20the%0Amulti-granularity%20output%20pipeline%20suffers%20from%20limited%20interaction%20flexibility%0Aand%20produces%20redundant%20results.%20In%20this%20work%2C%20we%20introduce%0AGranularity-Controllable%20Interactive%20Segmentation%20%28GraCo%29%2C%20a%20novel%20approach%0Athat%20allows%20precise%20control%20of%20prediction%20granularity%20by%20introducing%20additional%0Aparameters%20to%20input.%20This%20enhances%20the%20customization%20of%20the%20interactive%20system%0Aand%20eliminates%20redundancy%20while%20resolving%20ambiguity.%20Nevertheless%2C%20the%0Aexorbitant%20cost%20of%20annotating%20multi-granularity%20masks%20and%20the%20lack%20of%20available%0Adatasets%20with%20granularity%20annotations%20make%20it%20difficult%20for%20models%20to%20acquire%0Athe%20necessary%20guidance%20to%20control%20output%20granularity.%20To%20address%20this%20problem%2C%0Awe%20design%20an%20any-granularity%20mask%20generator%20that%20exploits%20the%20semantic%20property%0Aof%20the%20pre-trained%20IS%20model%20to%20automatically%20generate%20abundant%20mask-granularity%0Apairs%20without%20requiring%20additional%20manual%20annotation.%20Based%20on%20these%20pairs%2C%20we%0Apropose%20a%20granularity-controllable%20learning%20strategy%20that%20efficiently%20imparts%0Athe%20granularity%20controllability%20to%20the%20IS%20model.%20Extensive%20experiments%20on%0Aintricate%20scenarios%20at%20object%20and%20part%20levels%20demonstrate%20that%20our%20GraCo%20has%0Asignificant%20advantages%20over%20previous%20methods.%20This%20highlights%20the%20potential%20of%0AGraCo%20to%20be%20a%20flexible%20annotation%20tool%2C%20capable%20of%20adapting%20to%20diverse%0Asegmentation%20scenarios.%20The%20project%20page%3A%20https%3A//zhao-yian.github.io/GraCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00587v2&entry.124074799=Read"},
{"title": "Sharpness-Aware Minimization in Genetic Programming", "author": "Illya Bakurov and Nathan Haut and Wolfgang Banzhaf", "abstract": "  Sharpness-Aware Minimization (SAM) was recently introduced as a\nregularization procedure for training deep neural networks. It simultaneously\nminimizes the fitness (or loss) function and the so-called fitness sharpness.\nThe latter serves as a %connection between the geometry of the fitness\nlandscape measure of the nonlinear behavior of a solution %and generalization\nand does so by finding solutions that lie in neighborhoods having uniformly\nsimilar loss values across all fitness cases. In this contribution, we adapt\nSAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoods\nof solutions using two simple approaches By capitalizing upon perturbing input\nand output of program trees, sharpness can be estimated and used as a second\noptimization criterion during the evolution. To better understand the impact of\nthis variant of SAM on TGP, we collect numerous indicators of the evolutionary\nprocess, including generalization ability, complexity, diversity, and a\nrecently proposed genotype-phenotype mapping to study the amount of redundancy\nin trees. The experimental results demonstrate that using any of the two\nproposed SAM adaptations in TGP allows (i) a significant reduction of tree\nsizes in the population and (ii) a decrease in redundancy of the trees. When\nassessed on real-world benchmarks, the generalization ability of the elite\nsolutions does not deteriorate.\n", "link": "http://arxiv.org/abs/2405.10267v1", "date": "2024-05-16", "relevancy": 2.1776, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4628}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4233}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharpness-Aware%20Minimization%20in%20Genetic%20Programming&body=Title%3A%20Sharpness-Aware%20Minimization%20in%20Genetic%20Programming%0AAuthor%3A%20Illya%20Bakurov%20and%20Nathan%20Haut%20and%20Wolfgang%20Banzhaf%0AAbstract%3A%20%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20was%20recently%20introduced%20as%20a%0Aregularization%20procedure%20for%20training%20deep%20neural%20networks.%20It%20simultaneously%0Aminimizes%20the%20fitness%20%28or%20loss%29%20function%20and%20the%20so-called%20fitness%20sharpness.%0AThe%20latter%20serves%20as%20a%20%25connection%20between%20the%20geometry%20of%20the%20fitness%0Alandscape%20measure%20of%20the%20nonlinear%20behavior%20of%20a%20solution%20%25and%20generalization%0Aand%20does%20so%20by%20finding%20solutions%20that%20lie%20in%20neighborhoods%20having%20uniformly%0Asimilar%20loss%20values%20across%20all%20fitness%20cases.%20In%20this%20contribution%2C%20we%20adapt%0ASAM%20for%20tree%20Genetic%20Programming%20%28TGP%29%20by%20exploring%20the%20semantic%20neighborhoods%0Aof%20solutions%20using%20two%20simple%20approaches%20By%20capitalizing%20upon%20perturbing%20input%0Aand%20output%20of%20program%20trees%2C%20sharpness%20can%20be%20estimated%20and%20used%20as%20a%20second%0Aoptimization%20criterion%20during%20the%20evolution.%20To%20better%20understand%20the%20impact%20of%0Athis%20variant%20of%20SAM%20on%20TGP%2C%20we%20collect%20numerous%20indicators%20of%20the%20evolutionary%0Aprocess%2C%20including%20generalization%20ability%2C%20complexity%2C%20diversity%2C%20and%20a%0Arecently%20proposed%20genotype-phenotype%20mapping%20to%20study%20the%20amount%20of%20redundancy%0Ain%20trees.%20The%20experimental%20results%20demonstrate%20that%20using%20any%20of%20the%20two%0Aproposed%20SAM%20adaptations%20in%20TGP%20allows%20%28i%29%20a%20significant%20reduction%20of%20tree%0Asizes%20in%20the%20population%20and%20%28ii%29%20a%20decrease%20in%20redundancy%20of%20the%20trees.%20When%0Aassessed%20on%20real-world%20benchmarks%2C%20the%20generalization%20ability%20of%20the%20elite%0Asolutions%20does%20not%20deteriorate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharpness-Aware%2520Minimization%2520in%2520Genetic%2520Programming%26entry.906535625%3DIllya%2520Bakurov%2520and%2520Nathan%2520Haut%2520and%2520Wolfgang%2520Banzhaf%26entry.1292438233%3D%2520%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%2520was%2520recently%2520introduced%2520as%2520a%250Aregularization%2520procedure%2520for%2520training%2520deep%2520neural%2520networks.%2520It%2520simultaneously%250Aminimizes%2520the%2520fitness%2520%2528or%2520loss%2529%2520function%2520and%2520the%2520so-called%2520fitness%2520sharpness.%250AThe%2520latter%2520serves%2520as%2520a%2520%2525connection%2520between%2520the%2520geometry%2520of%2520the%2520fitness%250Alandscape%2520measure%2520of%2520the%2520nonlinear%2520behavior%2520of%2520a%2520solution%2520%2525and%2520generalization%250Aand%2520does%2520so%2520by%2520finding%2520solutions%2520that%2520lie%2520in%2520neighborhoods%2520having%2520uniformly%250Asimilar%2520loss%2520values%2520across%2520all%2520fitness%2520cases.%2520In%2520this%2520contribution%252C%2520we%2520adapt%250ASAM%2520for%2520tree%2520Genetic%2520Programming%2520%2528TGP%2529%2520by%2520exploring%2520the%2520semantic%2520neighborhoods%250Aof%2520solutions%2520using%2520two%2520simple%2520approaches%2520By%2520capitalizing%2520upon%2520perturbing%2520input%250Aand%2520output%2520of%2520program%2520trees%252C%2520sharpness%2520can%2520be%2520estimated%2520and%2520used%2520as%2520a%2520second%250Aoptimization%2520criterion%2520during%2520the%2520evolution.%2520To%2520better%2520understand%2520the%2520impact%2520of%250Athis%2520variant%2520of%2520SAM%2520on%2520TGP%252C%2520we%2520collect%2520numerous%2520indicators%2520of%2520the%2520evolutionary%250Aprocess%252C%2520including%2520generalization%2520ability%252C%2520complexity%252C%2520diversity%252C%2520and%2520a%250Arecently%2520proposed%2520genotype-phenotype%2520mapping%2520to%2520study%2520the%2520amount%2520of%2520redundancy%250Ain%2520trees.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520using%2520any%2520of%2520the%2520two%250Aproposed%2520SAM%2520adaptations%2520in%2520TGP%2520allows%2520%2528i%2529%2520a%2520significant%2520reduction%2520of%2520tree%250Asizes%2520in%2520the%2520population%2520and%2520%2528ii%2529%2520a%2520decrease%2520in%2520redundancy%2520of%2520the%2520trees.%2520When%250Aassessed%2520on%2520real-world%2520benchmarks%252C%2520the%2520generalization%2520ability%2520of%2520the%2520elite%250Asolutions%2520does%2520not%2520deteriorate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharpness-Aware%20Minimization%20in%20Genetic%20Programming&entry.906535625=Illya%20Bakurov%20and%20Nathan%20Haut%20and%20Wolfgang%20Banzhaf&entry.1292438233=%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20was%20recently%20introduced%20as%20a%0Aregularization%20procedure%20for%20training%20deep%20neural%20networks.%20It%20simultaneously%0Aminimizes%20the%20fitness%20%28or%20loss%29%20function%20and%20the%20so-called%20fitness%20sharpness.%0AThe%20latter%20serves%20as%20a%20%25connection%20between%20the%20geometry%20of%20the%20fitness%0Alandscape%20measure%20of%20the%20nonlinear%20behavior%20of%20a%20solution%20%25and%20generalization%0Aand%20does%20so%20by%20finding%20solutions%20that%20lie%20in%20neighborhoods%20having%20uniformly%0Asimilar%20loss%20values%20across%20all%20fitness%20cases.%20In%20this%20contribution%2C%20we%20adapt%0ASAM%20for%20tree%20Genetic%20Programming%20%28TGP%29%20by%20exploring%20the%20semantic%20neighborhoods%0Aof%20solutions%20using%20two%20simple%20approaches%20By%20capitalizing%20upon%20perturbing%20input%0Aand%20output%20of%20program%20trees%2C%20sharpness%20can%20be%20estimated%20and%20used%20as%20a%20second%0Aoptimization%20criterion%20during%20the%20evolution.%20To%20better%20understand%20the%20impact%20of%0Athis%20variant%20of%20SAM%20on%20TGP%2C%20we%20collect%20numerous%20indicators%20of%20the%20evolutionary%0Aprocess%2C%20including%20generalization%20ability%2C%20complexity%2C%20diversity%2C%20and%20a%0Arecently%20proposed%20genotype-phenotype%20mapping%20to%20study%20the%20amount%20of%20redundancy%0Ain%20trees.%20The%20experimental%20results%20demonstrate%20that%20using%20any%20of%20the%20two%0Aproposed%20SAM%20adaptations%20in%20TGP%20allows%20%28i%29%20a%20significant%20reduction%20of%20tree%0Asizes%20in%20the%20population%20and%20%28ii%29%20a%20decrease%20in%20redundancy%20of%20the%20trees.%20When%0Aassessed%20on%20real-world%20benchmarks%2C%20the%20generalization%20ability%20of%20the%20elite%0Asolutions%20does%20not%20deteriorate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10267v1&entry.124074799=Read"},
{"title": "Cooperative Visual-LiDAR Extrinsic Calibration Technology for\n  Intersection Vehicle-Infrastructure: A review", "author": "Xinyu Zhang and Yijin Xiong and Qianxin Qu and Renjie Wang and Xin Gao and Jing Liu and Shichun Guo and Jun Li", "abstract": "  In the typical urban intersection scenario, both vehicles and infrastructures\nare equipped with visual and LiDAR sensors. By successfully integrating the\ndata from vehicle-side and road monitoring devices, a more comprehensive and\naccurate environmental perception and information acquisition can be achieved.\nThe Calibration of sensors, as an essential component of autonomous driving\ntechnology, has consistently drawn significant attention. Particularly in\nscenarios involving multiple sensors collaboratively perceiving and addressing\nlocalization challenges, the requirement for inter-sensor calibration becomes\ncrucial. Recent years have witnessed the emergence of the concept of multi-end\ncooperation, where infrastructure captures and transmits surrounding\nenvironment information to vehicles, bolstering their perception capabilities\nwhile mitigating costs. However, this also poses technical complexities,\nunderscoring the pressing need for diverse end calibration. Camera and LiDAR,\nthe bedrock sensors in autonomous driving, exhibit expansive applicability.\nThis paper comprehensively examines and analyzes the calibration of multi-end\ncamera-LiDAR setups from vehicle, roadside, and vehicle-road cooperation\nperspectives, outlining their relevant applications and profound significance.\nConcluding with a summary, we present our future-oriented ideas and hypotheses.\n", "link": "http://arxiv.org/abs/2405.10132v1", "date": "2024-05-16", "relevancy": 2.1601, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5564}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooperative%20Visual-LiDAR%20Extrinsic%20Calibration%20Technology%20for%0A%20%20Intersection%20Vehicle-Infrastructure%3A%20A%20review&body=Title%3A%20Cooperative%20Visual-LiDAR%20Extrinsic%20Calibration%20Technology%20for%0A%20%20Intersection%20Vehicle-Infrastructure%3A%20A%20review%0AAuthor%3A%20Xinyu%20Zhang%20and%20Yijin%20Xiong%20and%20Qianxin%20Qu%20and%20Renjie%20Wang%20and%20Xin%20Gao%20and%20Jing%20Liu%20and%20Shichun%20Guo%20and%20Jun%20Li%0AAbstract%3A%20%20%20In%20the%20typical%20urban%20intersection%20scenario%2C%20both%20vehicles%20and%20infrastructures%0Aare%20equipped%20with%20visual%20and%20LiDAR%20sensors.%20By%20successfully%20integrating%20the%0Adata%20from%20vehicle-side%20and%20road%20monitoring%20devices%2C%20a%20more%20comprehensive%20and%0Aaccurate%20environmental%20perception%20and%20information%20acquisition%20can%20be%20achieved.%0AThe%20Calibration%20of%20sensors%2C%20as%20an%20essential%20component%20of%20autonomous%20driving%0Atechnology%2C%20has%20consistently%20drawn%20significant%20attention.%20Particularly%20in%0Ascenarios%20involving%20multiple%20sensors%20collaboratively%20perceiving%20and%20addressing%0Alocalization%20challenges%2C%20the%20requirement%20for%20inter-sensor%20calibration%20becomes%0Acrucial.%20Recent%20years%20have%20witnessed%20the%20emergence%20of%20the%20concept%20of%20multi-end%0Acooperation%2C%20where%20infrastructure%20captures%20and%20transmits%20surrounding%0Aenvironment%20information%20to%20vehicles%2C%20bolstering%20their%20perception%20capabilities%0Awhile%20mitigating%20costs.%20However%2C%20this%20also%20poses%20technical%20complexities%2C%0Aunderscoring%20the%20pressing%20need%20for%20diverse%20end%20calibration.%20Camera%20and%20LiDAR%2C%0Athe%20bedrock%20sensors%20in%20autonomous%20driving%2C%20exhibit%20expansive%20applicability.%0AThis%20paper%20comprehensively%20examines%20and%20analyzes%20the%20calibration%20of%20multi-end%0Acamera-LiDAR%20setups%20from%20vehicle%2C%20roadside%2C%20and%20vehicle-road%20cooperation%0Aperspectives%2C%20outlining%20their%20relevant%20applications%20and%20profound%20significance.%0AConcluding%20with%20a%20summary%2C%20we%20present%20our%20future-oriented%20ideas%20and%20hypotheses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooperative%2520Visual-LiDAR%2520Extrinsic%2520Calibration%2520Technology%2520for%250A%2520%2520Intersection%2520Vehicle-Infrastructure%253A%2520A%2520review%26entry.906535625%3DXinyu%2520Zhang%2520and%2520Yijin%2520Xiong%2520and%2520Qianxin%2520Qu%2520and%2520Renjie%2520Wang%2520and%2520Xin%2520Gao%2520and%2520Jing%2520Liu%2520and%2520Shichun%2520Guo%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520In%2520the%2520typical%2520urban%2520intersection%2520scenario%252C%2520both%2520vehicles%2520and%2520infrastructures%250Aare%2520equipped%2520with%2520visual%2520and%2520LiDAR%2520sensors.%2520By%2520successfully%2520integrating%2520the%250Adata%2520from%2520vehicle-side%2520and%2520road%2520monitoring%2520devices%252C%2520a%2520more%2520comprehensive%2520and%250Aaccurate%2520environmental%2520perception%2520and%2520information%2520acquisition%2520can%2520be%2520achieved.%250AThe%2520Calibration%2520of%2520sensors%252C%2520as%2520an%2520essential%2520component%2520of%2520autonomous%2520driving%250Atechnology%252C%2520has%2520consistently%2520drawn%2520significant%2520attention.%2520Particularly%2520in%250Ascenarios%2520involving%2520multiple%2520sensors%2520collaboratively%2520perceiving%2520and%2520addressing%250Alocalization%2520challenges%252C%2520the%2520requirement%2520for%2520inter-sensor%2520calibration%2520becomes%250Acrucial.%2520Recent%2520years%2520have%2520witnessed%2520the%2520emergence%2520of%2520the%2520concept%2520of%2520multi-end%250Acooperation%252C%2520where%2520infrastructure%2520captures%2520and%2520transmits%2520surrounding%250Aenvironment%2520information%2520to%2520vehicles%252C%2520bolstering%2520their%2520perception%2520capabilities%250Awhile%2520mitigating%2520costs.%2520However%252C%2520this%2520also%2520poses%2520technical%2520complexities%252C%250Aunderscoring%2520the%2520pressing%2520need%2520for%2520diverse%2520end%2520calibration.%2520Camera%2520and%2520LiDAR%252C%250Athe%2520bedrock%2520sensors%2520in%2520autonomous%2520driving%252C%2520exhibit%2520expansive%2520applicability.%250AThis%2520paper%2520comprehensively%2520examines%2520and%2520analyzes%2520the%2520calibration%2520of%2520multi-end%250Acamera-LiDAR%2520setups%2520from%2520vehicle%252C%2520roadside%252C%2520and%2520vehicle-road%2520cooperation%250Aperspectives%252C%2520outlining%2520their%2520relevant%2520applications%2520and%2520profound%2520significance.%250AConcluding%2520with%2520a%2520summary%252C%2520we%2520present%2520our%2520future-oriented%2520ideas%2520and%2520hypotheses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooperative%20Visual-LiDAR%20Extrinsic%20Calibration%20Technology%20for%0A%20%20Intersection%20Vehicle-Infrastructure%3A%20A%20review&entry.906535625=Xinyu%20Zhang%20and%20Yijin%20Xiong%20and%20Qianxin%20Qu%20and%20Renjie%20Wang%20and%20Xin%20Gao%20and%20Jing%20Liu%20and%20Shichun%20Guo%20and%20Jun%20Li&entry.1292438233=%20%20In%20the%20typical%20urban%20intersection%20scenario%2C%20both%20vehicles%20and%20infrastructures%0Aare%20equipped%20with%20visual%20and%20LiDAR%20sensors.%20By%20successfully%20integrating%20the%0Adata%20from%20vehicle-side%20and%20road%20monitoring%20devices%2C%20a%20more%20comprehensive%20and%0Aaccurate%20environmental%20perception%20and%20information%20acquisition%20can%20be%20achieved.%0AThe%20Calibration%20of%20sensors%2C%20as%20an%20essential%20component%20of%20autonomous%20driving%0Atechnology%2C%20has%20consistently%20drawn%20significant%20attention.%20Particularly%20in%0Ascenarios%20involving%20multiple%20sensors%20collaboratively%20perceiving%20and%20addressing%0Alocalization%20challenges%2C%20the%20requirement%20for%20inter-sensor%20calibration%20becomes%0Acrucial.%20Recent%20years%20have%20witnessed%20the%20emergence%20of%20the%20concept%20of%20multi-end%0Acooperation%2C%20where%20infrastructure%20captures%20and%20transmits%20surrounding%0Aenvironment%20information%20to%20vehicles%2C%20bolstering%20their%20perception%20capabilities%0Awhile%20mitigating%20costs.%20However%2C%20this%20also%20poses%20technical%20complexities%2C%0Aunderscoring%20the%20pressing%20need%20for%20diverse%20end%20calibration.%20Camera%20and%20LiDAR%2C%0Athe%20bedrock%20sensors%20in%20autonomous%20driving%2C%20exhibit%20expansive%20applicability.%0AThis%20paper%20comprehensively%20examines%20and%20analyzes%20the%20calibration%20of%20multi-end%0Acamera-LiDAR%20setups%20from%20vehicle%2C%20roadside%2C%20and%20vehicle-road%20cooperation%0Aperspectives%2C%20outlining%20their%20relevant%20applications%20and%20profound%20significance.%0AConcluding%20with%20a%20summary%2C%20we%20present%20our%20future-oriented%20ideas%20and%20hypotheses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10132v1&entry.124074799=Read"},
{"title": "Optimizing Search and Rescue UAV Connectivity in Challenging Terrain\n  through Multi Q-Learning", "author": "Mohammed M. H. Qazzaz and Syed A. R. Zaidi and Desmond C. McLernon and Abdelaziz Salama and Aubida A. Al-Hameed", "abstract": "  Using Unmanned Aerial Vehicles (UAVs) in Search and rescue operations (SAR)\nto navigate challenging terrain while maintaining reliable communication with\nthe cellular network is a promising approach. This paper suggests a novel\ntechnique employing a reinforcement learning multi Q-learning algorithm to\noptimize UAV connectivity in such scenarios. We introduce a Strategic Planning\nAgent for efficient path planning and collision awareness and a Real-time\nAdaptive Agent to maintain optimal connection with the cellular base station.\nThe agents trained in a simulated environment using multi Q-learning,\nencouraging them to learn from experience and adjust their decision-making to\ndiverse terrain complexities and communication scenarios. Evaluation results\nreveal the significance of the approach, highlighting successful navigation in\nenvironments with varying obstacle densities and the ability to perform optimal\nconnectivity using different frequency bands. This work paves the way for\nenhanced UAV autonomy and enhanced communication reliability in search and\nrescue operations.\n", "link": "http://arxiv.org/abs/2405.10042v1", "date": "2024-05-16", "relevancy": 2.1591, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Search%20and%20Rescue%20UAV%20Connectivity%20in%20Challenging%20Terrain%0A%20%20through%20Multi%20Q-Learning&body=Title%3A%20Optimizing%20Search%20and%20Rescue%20UAV%20Connectivity%20in%20Challenging%20Terrain%0A%20%20through%20Multi%20Q-Learning%0AAuthor%3A%20Mohammed%20M.%20H.%20Qazzaz%20and%20Syed%20A.%20R.%20Zaidi%20and%20Desmond%20C.%20McLernon%20and%20Abdelaziz%20Salama%20and%20Aubida%20A.%20Al-Hameed%0AAbstract%3A%20%20%20Using%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20in%20Search%20and%20rescue%20operations%20%28SAR%29%0Ato%20navigate%20challenging%20terrain%20while%20maintaining%20reliable%20communication%20with%0Athe%20cellular%20network%20is%20a%20promising%20approach.%20This%20paper%20suggests%20a%20novel%0Atechnique%20employing%20a%20reinforcement%20learning%20multi%20Q-learning%20algorithm%20to%0Aoptimize%20UAV%20connectivity%20in%20such%20scenarios.%20We%20introduce%20a%20Strategic%20Planning%0AAgent%20for%20efficient%20path%20planning%20and%20collision%20awareness%20and%20a%20Real-time%0AAdaptive%20Agent%20to%20maintain%20optimal%20connection%20with%20the%20cellular%20base%20station.%0AThe%20agents%20trained%20in%20a%20simulated%20environment%20using%20multi%20Q-learning%2C%0Aencouraging%20them%20to%20learn%20from%20experience%20and%20adjust%20their%20decision-making%20to%0Adiverse%20terrain%20complexities%20and%20communication%20scenarios.%20Evaluation%20results%0Areveal%20the%20significance%20of%20the%20approach%2C%20highlighting%20successful%20navigation%20in%0Aenvironments%20with%20varying%20obstacle%20densities%20and%20the%20ability%20to%20perform%20optimal%0Aconnectivity%20using%20different%20frequency%20bands.%20This%20work%20paves%20the%20way%20for%0Aenhanced%20UAV%20autonomy%20and%20enhanced%20communication%20reliability%20in%20search%20and%0Arescue%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Search%2520and%2520Rescue%2520UAV%2520Connectivity%2520in%2520Challenging%2520Terrain%250A%2520%2520through%2520Multi%2520Q-Learning%26entry.906535625%3DMohammed%2520M.%2520H.%2520Qazzaz%2520and%2520Syed%2520A.%2520R.%2520Zaidi%2520and%2520Desmond%2520C.%2520McLernon%2520and%2520Abdelaziz%2520Salama%2520and%2520Aubida%2520A.%2520Al-Hameed%26entry.1292438233%3D%2520%2520Using%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520in%2520Search%2520and%2520rescue%2520operations%2520%2528SAR%2529%250Ato%2520navigate%2520challenging%2520terrain%2520while%2520maintaining%2520reliable%2520communication%2520with%250Athe%2520cellular%2520network%2520is%2520a%2520promising%2520approach.%2520This%2520paper%2520suggests%2520a%2520novel%250Atechnique%2520employing%2520a%2520reinforcement%2520learning%2520multi%2520Q-learning%2520algorithm%2520to%250Aoptimize%2520UAV%2520connectivity%2520in%2520such%2520scenarios.%2520We%2520introduce%2520a%2520Strategic%2520Planning%250AAgent%2520for%2520efficient%2520path%2520planning%2520and%2520collision%2520awareness%2520and%2520a%2520Real-time%250AAdaptive%2520Agent%2520to%2520maintain%2520optimal%2520connection%2520with%2520the%2520cellular%2520base%2520station.%250AThe%2520agents%2520trained%2520in%2520a%2520simulated%2520environment%2520using%2520multi%2520Q-learning%252C%250Aencouraging%2520them%2520to%2520learn%2520from%2520experience%2520and%2520adjust%2520their%2520decision-making%2520to%250Adiverse%2520terrain%2520complexities%2520and%2520communication%2520scenarios.%2520Evaluation%2520results%250Areveal%2520the%2520significance%2520of%2520the%2520approach%252C%2520highlighting%2520successful%2520navigation%2520in%250Aenvironments%2520with%2520varying%2520obstacle%2520densities%2520and%2520the%2520ability%2520to%2520perform%2520optimal%250Aconnectivity%2520using%2520different%2520frequency%2520bands.%2520This%2520work%2520paves%2520the%2520way%2520for%250Aenhanced%2520UAV%2520autonomy%2520and%2520enhanced%2520communication%2520reliability%2520in%2520search%2520and%250Arescue%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Search%20and%20Rescue%20UAV%20Connectivity%20in%20Challenging%20Terrain%0A%20%20through%20Multi%20Q-Learning&entry.906535625=Mohammed%20M.%20H.%20Qazzaz%20and%20Syed%20A.%20R.%20Zaidi%20and%20Desmond%20C.%20McLernon%20and%20Abdelaziz%20Salama%20and%20Aubida%20A.%20Al-Hameed&entry.1292438233=%20%20Using%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20in%20Search%20and%20rescue%20operations%20%28SAR%29%0Ato%20navigate%20challenging%20terrain%20while%20maintaining%20reliable%20communication%20with%0Athe%20cellular%20network%20is%20a%20promising%20approach.%20This%20paper%20suggests%20a%20novel%0Atechnique%20employing%20a%20reinforcement%20learning%20multi%20Q-learning%20algorithm%20to%0Aoptimize%20UAV%20connectivity%20in%20such%20scenarios.%20We%20introduce%20a%20Strategic%20Planning%0AAgent%20for%20efficient%20path%20planning%20and%20collision%20awareness%20and%20a%20Real-time%0AAdaptive%20Agent%20to%20maintain%20optimal%20connection%20with%20the%20cellular%20base%20station.%0AThe%20agents%20trained%20in%20a%20simulated%20environment%20using%20multi%20Q-learning%2C%0Aencouraging%20them%20to%20learn%20from%20experience%20and%20adjust%20their%20decision-making%20to%0Adiverse%20terrain%20complexities%20and%20communication%20scenarios.%20Evaluation%20results%0Areveal%20the%20significance%20of%20the%20approach%2C%20highlighting%20successful%20navigation%20in%0Aenvironments%20with%20varying%20obstacle%20densities%20and%20the%20ability%20to%20perform%20optimal%0Aconnectivity%20using%20different%20frequency%20bands.%20This%20work%20paves%20the%20way%20for%0Aenhanced%20UAV%20autonomy%20and%20enhanced%20communication%20reliability%20in%20search%20and%0Arescue%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10042v1&entry.124074799=Read"},
{"title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in\n  very strong Vision-Language models", "author": "Adrian Bulat and Yassine Ouali and Georgios Tzimiropoulos", "abstract": "  Despite noise and caption quality having been acknowledged as important\nfactors impacting vision-language contrastive pre-training, in this paper, we\nshow that the full potential of improving the training process by addressing\nsuch issues is yet to be realized. Specifically, we firstly study and analyze\ntwo issues affecting training: incorrect assignment of negative pairs, and low\ncaption quality and diversity. Then, we devise effective solutions for\naddressing both problems, which essentially require training with multiple true\npositive pairs. Finally, we propose training with sigmoid loss to address such\na requirement. We show very large gains over the current state-of-the-art for\nboth image recognition ($\\sim +6\\%$ on average over 11 datasets) and image\nretrieval ($\\sim +19\\%$ on Flickr30k and $\\sim +15\\%$ on MSCOCO).\n", "link": "http://arxiv.org/abs/2405.10286v1", "date": "2024-05-16", "relevancy": 2.1588, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.533}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FFF%3A%20Fixing%20Flawed%20Foundations%20in%20contrastive%20pre-training%20results%20in%0A%20%20very%20strong%20Vision-Language%20models&body=Title%3A%20FFF%3A%20Fixing%20Flawed%20Foundations%20in%20contrastive%20pre-training%20results%20in%0A%20%20very%20strong%20Vision-Language%20models%0AAuthor%3A%20Adrian%20Bulat%20and%20Yassine%20Ouali%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20Despite%20noise%20and%20caption%20quality%20having%20been%20acknowledged%20as%20important%0Afactors%20impacting%20vision-language%20contrastive%20pre-training%2C%20in%20this%20paper%2C%20we%0Ashow%20that%20the%20full%20potential%20of%20improving%20the%20training%20process%20by%20addressing%0Asuch%20issues%20is%20yet%20to%20be%20realized.%20Specifically%2C%20we%20firstly%20study%20and%20analyze%0Atwo%20issues%20affecting%20training%3A%20incorrect%20assignment%20of%20negative%20pairs%2C%20and%20low%0Acaption%20quality%20and%20diversity.%20Then%2C%20we%20devise%20effective%20solutions%20for%0Aaddressing%20both%20problems%2C%20which%20essentially%20require%20training%20with%20multiple%20true%0Apositive%20pairs.%20Finally%2C%20we%20propose%20training%20with%20sigmoid%20loss%20to%20address%20such%0Aa%20requirement.%20We%20show%20very%20large%20gains%20over%20the%20current%20state-of-the-art%20for%0Aboth%20image%20recognition%20%28%24%5Csim%20%2B6%5C%25%24%20on%20average%20over%2011%20datasets%29%20and%20image%0Aretrieval%20%28%24%5Csim%20%2B19%5C%25%24%20on%20Flickr30k%20and%20%24%5Csim%20%2B15%5C%25%24%20on%20MSCOCO%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFFF%253A%2520Fixing%2520Flawed%2520Foundations%2520in%2520contrastive%2520pre-training%2520results%2520in%250A%2520%2520very%2520strong%2520Vision-Language%2520models%26entry.906535625%3DAdrian%2520Bulat%2520and%2520Yassine%2520Ouali%2520and%2520Georgios%2520Tzimiropoulos%26entry.1292438233%3D%2520%2520Despite%2520noise%2520and%2520caption%2520quality%2520having%2520been%2520acknowledged%2520as%2520important%250Afactors%2520impacting%2520vision-language%2520contrastive%2520pre-training%252C%2520in%2520this%2520paper%252C%2520we%250Ashow%2520that%2520the%2520full%2520potential%2520of%2520improving%2520the%2520training%2520process%2520by%2520addressing%250Asuch%2520issues%2520is%2520yet%2520to%2520be%2520realized.%2520Specifically%252C%2520we%2520firstly%2520study%2520and%2520analyze%250Atwo%2520issues%2520affecting%2520training%253A%2520incorrect%2520assignment%2520of%2520negative%2520pairs%252C%2520and%2520low%250Acaption%2520quality%2520and%2520diversity.%2520Then%252C%2520we%2520devise%2520effective%2520solutions%2520for%250Aaddressing%2520both%2520problems%252C%2520which%2520essentially%2520require%2520training%2520with%2520multiple%2520true%250Apositive%2520pairs.%2520Finally%252C%2520we%2520propose%2520training%2520with%2520sigmoid%2520loss%2520to%2520address%2520such%250Aa%2520requirement.%2520We%2520show%2520very%2520large%2520gains%2520over%2520the%2520current%2520state-of-the-art%2520for%250Aboth%2520image%2520recognition%2520%2528%2524%255Csim%2520%252B6%255C%2525%2524%2520on%2520average%2520over%252011%2520datasets%2529%2520and%2520image%250Aretrieval%2520%2528%2524%255Csim%2520%252B19%255C%2525%2524%2520on%2520Flickr30k%2520and%2520%2524%255Csim%2520%252B15%255C%2525%2524%2520on%2520MSCOCO%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FFF%3A%20Fixing%20Flawed%20Foundations%20in%20contrastive%20pre-training%20results%20in%0A%20%20very%20strong%20Vision-Language%20models&entry.906535625=Adrian%20Bulat%20and%20Yassine%20Ouali%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20Despite%20noise%20and%20caption%20quality%20having%20been%20acknowledged%20as%20important%0Afactors%20impacting%20vision-language%20contrastive%20pre-training%2C%20in%20this%20paper%2C%20we%0Ashow%20that%20the%20full%20potential%20of%20improving%20the%20training%20process%20by%20addressing%0Asuch%20issues%20is%20yet%20to%20be%20realized.%20Specifically%2C%20we%20firstly%20study%20and%20analyze%0Atwo%20issues%20affecting%20training%3A%20incorrect%20assignment%20of%20negative%20pairs%2C%20and%20low%0Acaption%20quality%20and%20diversity.%20Then%2C%20we%20devise%20effective%20solutions%20for%0Aaddressing%20both%20problems%2C%20which%20essentially%20require%20training%20with%20multiple%20true%0Apositive%20pairs.%20Finally%2C%20we%20propose%20training%20with%20sigmoid%20loss%20to%20address%20such%0Aa%20requirement.%20We%20show%20very%20large%20gains%20over%20the%20current%20state-of-the-art%20for%0Aboth%20image%20recognition%20%28%24%5Csim%20%2B6%5C%25%24%20on%20average%20over%2011%20datasets%29%20and%20image%0Aretrieval%20%28%24%5Csim%20%2B19%5C%25%24%20on%20Flickr30k%20and%20%24%5Csim%20%2B15%5C%25%24%20on%20MSCOCO%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10286v1&entry.124074799=Read"},
{"title": "Geo-Localization Based on Dynamically Weighted Factor-Graph", "author": "Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n and Alejandro Olivas and Edison Velasco-S\u00e1nchez and Francisco A. Candelas and Fernando Torres", "abstract": "  Feature-based geo-localization relies on associating features extracted from\naerial imagery with those detected by the vehicle's sensors. This requires that\nthe type of landmarks must be observable from both sources. This lack of\nvariety of feature types generates poor representations that lead to outliers\nand deviations produced by ambiguities and lack of detections, respectively. To\nmitigate these drawbacks, in this paper, we present a dynamically weighted\nfactor graph model for the vehicle's trajectory estimation. The weight\nadjustment in this implementation depends on information quantification in the\ndetections performed using a LiDAR sensor. Also, a prior (GNSS-based) error\nestimation is included in the model. Then, when the representation becomes\nambiguous or sparse, the weights are dynamically adjusted to rely on the\ncorrected prior trajectory, mitigating outliers and deviations in this way. We\ncompare our method against state-of-the-art geo-localization ones in a\nchallenging and ambiguous environment, where we also cause detection losses. We\ndemonstrate mitigation of the mentioned drawbacks where the other methods fail.\n", "link": "http://arxiv.org/abs/2311.07301v2", "date": "2024-05-16", "relevancy": 2.158, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5606}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo-Localization%20Based%20on%20Dynamically%20Weighted%20Factor-Graph&body=Title%3A%20Geo-Localization%20Based%20on%20Dynamically%20Weighted%20Factor-Graph%0AAuthor%3A%20Miguel%20%C3%81ngel%20Mu%C3%B1oz-Ba%C3%B1%C3%B3n%20and%20Alejandro%20Olivas%20and%20Edison%20Velasco-S%C3%A1nchez%20and%20Francisco%20A.%20Candelas%20and%20Fernando%20Torres%0AAbstract%3A%20%20%20Feature-based%20geo-localization%20relies%20on%20associating%20features%20extracted%20from%0Aaerial%20imagery%20with%20those%20detected%20by%20the%20vehicle%27s%20sensors.%20This%20requires%20that%0Athe%20type%20of%20landmarks%20must%20be%20observable%20from%20both%20sources.%20This%20lack%20of%0Avariety%20of%20feature%20types%20generates%20poor%20representations%20that%20lead%20to%20outliers%0Aand%20deviations%20produced%20by%20ambiguities%20and%20lack%20of%20detections%2C%20respectively.%20To%0Amitigate%20these%20drawbacks%2C%20in%20this%20paper%2C%20we%20present%20a%20dynamically%20weighted%0Afactor%20graph%20model%20for%20the%20vehicle%27s%20trajectory%20estimation.%20The%20weight%0Aadjustment%20in%20this%20implementation%20depends%20on%20information%20quantification%20in%20the%0Adetections%20performed%20using%20a%20LiDAR%20sensor.%20Also%2C%20a%20prior%20%28GNSS-based%29%20error%0Aestimation%20is%20included%20in%20the%20model.%20Then%2C%20when%20the%20representation%20becomes%0Aambiguous%20or%20sparse%2C%20the%20weights%20are%20dynamically%20adjusted%20to%20rely%20on%20the%0Acorrected%20prior%20trajectory%2C%20mitigating%20outliers%20and%20deviations%20in%20this%20way.%20We%0Acompare%20our%20method%20against%20state-of-the-art%20geo-localization%20ones%20in%20a%0Achallenging%20and%20ambiguous%20environment%2C%20where%20we%20also%20cause%20detection%20losses.%20We%0Ademonstrate%20mitigation%20of%20the%20mentioned%20drawbacks%20where%20the%20other%20methods%20fail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo-Localization%2520Based%2520on%2520Dynamically%2520Weighted%2520Factor-Graph%26entry.906535625%3DMiguel%2520%25C3%2581ngel%2520Mu%25C3%25B1oz-Ba%25C3%25B1%25C3%25B3n%2520and%2520Alejandro%2520Olivas%2520and%2520Edison%2520Velasco-S%25C3%25A1nchez%2520and%2520Francisco%2520A.%2520Candelas%2520and%2520Fernando%2520Torres%26entry.1292438233%3D%2520%2520Feature-based%2520geo-localization%2520relies%2520on%2520associating%2520features%2520extracted%2520from%250Aaerial%2520imagery%2520with%2520those%2520detected%2520by%2520the%2520vehicle%2527s%2520sensors.%2520This%2520requires%2520that%250Athe%2520type%2520of%2520landmarks%2520must%2520be%2520observable%2520from%2520both%2520sources.%2520This%2520lack%2520of%250Avariety%2520of%2520feature%2520types%2520generates%2520poor%2520representations%2520that%2520lead%2520to%2520outliers%250Aand%2520deviations%2520produced%2520by%2520ambiguities%2520and%2520lack%2520of%2520detections%252C%2520respectively.%2520To%250Amitigate%2520these%2520drawbacks%252C%2520in%2520this%2520paper%252C%2520we%2520present%2520a%2520dynamically%2520weighted%250Afactor%2520graph%2520model%2520for%2520the%2520vehicle%2527s%2520trajectory%2520estimation.%2520The%2520weight%250Aadjustment%2520in%2520this%2520implementation%2520depends%2520on%2520information%2520quantification%2520in%2520the%250Adetections%2520performed%2520using%2520a%2520LiDAR%2520sensor.%2520Also%252C%2520a%2520prior%2520%2528GNSS-based%2529%2520error%250Aestimation%2520is%2520included%2520in%2520the%2520model.%2520Then%252C%2520when%2520the%2520representation%2520becomes%250Aambiguous%2520or%2520sparse%252C%2520the%2520weights%2520are%2520dynamically%2520adjusted%2520to%2520rely%2520on%2520the%250Acorrected%2520prior%2520trajectory%252C%2520mitigating%2520outliers%2520and%2520deviations%2520in%2520this%2520way.%2520We%250Acompare%2520our%2520method%2520against%2520state-of-the-art%2520geo-localization%2520ones%2520in%2520a%250Achallenging%2520and%2520ambiguous%2520environment%252C%2520where%2520we%2520also%2520cause%2520detection%2520losses.%2520We%250Ademonstrate%2520mitigation%2520of%2520the%2520mentioned%2520drawbacks%2520where%2520the%2520other%2520methods%2520fail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo-Localization%20Based%20on%20Dynamically%20Weighted%20Factor-Graph&entry.906535625=Miguel%20%C3%81ngel%20Mu%C3%B1oz-Ba%C3%B1%C3%B3n%20and%20Alejandro%20Olivas%20and%20Edison%20Velasco-S%C3%A1nchez%20and%20Francisco%20A.%20Candelas%20and%20Fernando%20Torres&entry.1292438233=%20%20Feature-based%20geo-localization%20relies%20on%20associating%20features%20extracted%20from%0Aaerial%20imagery%20with%20those%20detected%20by%20the%20vehicle%27s%20sensors.%20This%20requires%20that%0Athe%20type%20of%20landmarks%20must%20be%20observable%20from%20both%20sources.%20This%20lack%20of%0Avariety%20of%20feature%20types%20generates%20poor%20representations%20that%20lead%20to%20outliers%0Aand%20deviations%20produced%20by%20ambiguities%20and%20lack%20of%20detections%2C%20respectively.%20To%0Amitigate%20these%20drawbacks%2C%20in%20this%20paper%2C%20we%20present%20a%20dynamically%20weighted%0Afactor%20graph%20model%20for%20the%20vehicle%27s%20trajectory%20estimation.%20The%20weight%0Aadjustment%20in%20this%20implementation%20depends%20on%20information%20quantification%20in%20the%0Adetections%20performed%20using%20a%20LiDAR%20sensor.%20Also%2C%20a%20prior%20%28GNSS-based%29%20error%0Aestimation%20is%20included%20in%20the%20model.%20Then%2C%20when%20the%20representation%20becomes%0Aambiguous%20or%20sparse%2C%20the%20weights%20are%20dynamically%20adjusted%20to%20rely%20on%20the%0Acorrected%20prior%20trajectory%2C%20mitigating%20outliers%20and%20deviations%20in%20this%20way.%20We%0Acompare%20our%20method%20against%20state-of-the-art%20geo-localization%20ones%20in%20a%0Achallenging%20and%20ambiguous%20environment%2C%20where%20we%20also%20cause%20detection%20losses.%20We%0Ademonstrate%20mitigation%20of%20the%20mentioned%20drawbacks%20where%20the%20other%20methods%20fail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07301v2&entry.124074799=Read"},
{"title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction\n  Representation Learning", "author": "Jiancheng Pan and Muyuan Ma and Qing Ma and Cong Bai and Shengyong Chen", "abstract": "  Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, Vision Instruction Representation (VIR)\nbased on Spatial-PAE utilizes the prior-guided knowledge of the remote sensing\nscene recognition by building a belief matrix to select key features for\nreducing the impact of semantic noise. In text representation, Language Cycle\nAttention (LCA) based on Temporal-PAE uses the previous time step to cyclically\nactivate the current time step to enhance text representation capability. A\ncluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classes\nand to reduce the semantic confusion zones in the common subspace.\nComprehensive experiments demonstrate that PIR could enhance vision and text\nrepresentations and outperform the state-of-the-art methods of closed-domain\nand open-domain retrieval on two benchmark datasets, RSICD and RSITMD.\n", "link": "http://arxiv.org/abs/2405.10160v1", "date": "2024-05-16", "relevancy": 2.1529, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIR%3A%20Remote%20Sensing%20Image-Text%20Retrieval%20with%20Prior%20Instruction%0A%20%20Representation%20Learning&body=Title%3A%20PIR%3A%20Remote%20Sensing%20Image-Text%20Retrieval%20with%20Prior%20Instruction%0A%20%20Representation%20Learning%0AAuthor%3A%20Jiancheng%20Pan%20and%20Muyuan%20Ma%20and%20Qing%20Ma%20and%20Cong%20Bai%20and%20Shengyong%20Chen%0AAbstract%3A%20%20%20Remote%20sensing%20image-text%20retrieval%20constitutes%20a%20foundational%20aspect%20of%0Aremote%20sensing%20interpretation%20tasks%2C%20facilitating%20the%20alignment%20of%20vision%20and%0Alanguage%20representations.%20This%20paper%20introduces%20a%20prior%20instruction%0Arepresentation%20%28PIR%29%20learning%20paradigm%20that%20draws%20on%20prior%20knowledge%20to%0Ainstruct%20adaptive%20learning%20of%20vision%20and%20text%20representations.%20Based%20on%20PIR%2C%20a%0Adomain-adapted%20remote%20sensing%20image-text%20retrieval%20framework%20PIR-ITR%20is%0Adesigned%20to%20address%20semantic%20noise%20issues%20in%20vision-language%20understanding%0Atasks.%20However%2C%20with%20massive%20additional%20data%20for%20pre-training%20the%0Avision-language%20foundation%20model%2C%20remote%20sensing%20image-text%20retrieval%20is%0Afurther%20developed%20into%20an%20open-domain%20retrieval%20task.%20Continuing%20with%20the%0Aabove%2C%20we%20propose%20PIR-CLIP%2C%20a%20domain-specific%20CLIP-based%20framework%20for%20remote%0Asensing%20image-text%20retrieval%2C%20to%20address%20semantic%20noise%20in%20remote%20sensing%0Avision-language%20representations%20and%20further%20improve%20open-domain%20retrieval%0Aperformance.%20In%20vision%20representation%2C%20Vision%20Instruction%20Representation%20%28VIR%29%0Abased%20on%20Spatial-PAE%20utilizes%20the%20prior-guided%20knowledge%20of%20the%20remote%20sensing%0Ascene%20recognition%20by%20building%20a%20belief%20matrix%20to%20select%20key%20features%20for%0Areducing%20the%20impact%20of%20semantic%20noise.%20In%20text%20representation%2C%20Language%20Cycle%0AAttention%20%28LCA%29%20based%20on%20Temporal-PAE%20uses%20the%20previous%20time%20step%20to%20cyclically%0Aactivate%20the%20current%20time%20step%20to%20enhance%20text%20representation%20capability.%20A%0Acluster-wise%20Affiliation%20Loss%20%28AL%29%20is%20proposed%20to%20constrain%20the%20inter-classes%0Aand%20to%20reduce%20the%20semantic%20confusion%20zones%20in%20the%20common%20subspace.%0AComprehensive%20experiments%20demonstrate%20that%20PIR%20could%20enhance%20vision%20and%20text%0Arepresentations%20and%20outperform%20the%20state-of-the-art%20methods%20of%20closed-domain%0Aand%20open-domain%20retrieval%20on%20two%20benchmark%20datasets%2C%20RSICD%20and%20RSITMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIR%253A%2520Remote%2520Sensing%2520Image-Text%2520Retrieval%2520with%2520Prior%2520Instruction%250A%2520%2520Representation%2520Learning%26entry.906535625%3DJiancheng%2520Pan%2520and%2520Muyuan%2520Ma%2520and%2520Qing%2520Ma%2520and%2520Cong%2520Bai%2520and%2520Shengyong%2520Chen%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image-text%2520retrieval%2520constitutes%2520a%2520foundational%2520aspect%2520of%250Aremote%2520sensing%2520interpretation%2520tasks%252C%2520facilitating%2520the%2520alignment%2520of%2520vision%2520and%250Alanguage%2520representations.%2520This%2520paper%2520introduces%2520a%2520prior%2520instruction%250Arepresentation%2520%2528PIR%2529%2520learning%2520paradigm%2520that%2520draws%2520on%2520prior%2520knowledge%2520to%250Ainstruct%2520adaptive%2520learning%2520of%2520vision%2520and%2520text%2520representations.%2520Based%2520on%2520PIR%252C%2520a%250Adomain-adapted%2520remote%2520sensing%2520image-text%2520retrieval%2520framework%2520PIR-ITR%2520is%250Adesigned%2520to%2520address%2520semantic%2520noise%2520issues%2520in%2520vision-language%2520understanding%250Atasks.%2520However%252C%2520with%2520massive%2520additional%2520data%2520for%2520pre-training%2520the%250Avision-language%2520foundation%2520model%252C%2520remote%2520sensing%2520image-text%2520retrieval%2520is%250Afurther%2520developed%2520into%2520an%2520open-domain%2520retrieval%2520task.%2520Continuing%2520with%2520the%250Aabove%252C%2520we%2520propose%2520PIR-CLIP%252C%2520a%2520domain-specific%2520CLIP-based%2520framework%2520for%2520remote%250Asensing%2520image-text%2520retrieval%252C%2520to%2520address%2520semantic%2520noise%2520in%2520remote%2520sensing%250Avision-language%2520representations%2520and%2520further%2520improve%2520open-domain%2520retrieval%250Aperformance.%2520In%2520vision%2520representation%252C%2520Vision%2520Instruction%2520Representation%2520%2528VIR%2529%250Abased%2520on%2520Spatial-PAE%2520utilizes%2520the%2520prior-guided%2520knowledge%2520of%2520the%2520remote%2520sensing%250Ascene%2520recognition%2520by%2520building%2520a%2520belief%2520matrix%2520to%2520select%2520key%2520features%2520for%250Areducing%2520the%2520impact%2520of%2520semantic%2520noise.%2520In%2520text%2520representation%252C%2520Language%2520Cycle%250AAttention%2520%2528LCA%2529%2520based%2520on%2520Temporal-PAE%2520uses%2520the%2520previous%2520time%2520step%2520to%2520cyclically%250Aactivate%2520the%2520current%2520time%2520step%2520to%2520enhance%2520text%2520representation%2520capability.%2520A%250Acluster-wise%2520Affiliation%2520Loss%2520%2528AL%2529%2520is%2520proposed%2520to%2520constrain%2520the%2520inter-classes%250Aand%2520to%2520reduce%2520the%2520semantic%2520confusion%2520zones%2520in%2520the%2520common%2520subspace.%250AComprehensive%2520experiments%2520demonstrate%2520that%2520PIR%2520could%2520enhance%2520vision%2520and%2520text%250Arepresentations%2520and%2520outperform%2520the%2520state-of-the-art%2520methods%2520of%2520closed-domain%250Aand%2520open-domain%2520retrieval%2520on%2520two%2520benchmark%2520datasets%252C%2520RSICD%2520and%2520RSITMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIR%3A%20Remote%20Sensing%20Image-Text%20Retrieval%20with%20Prior%20Instruction%0A%20%20Representation%20Learning&entry.906535625=Jiancheng%20Pan%20and%20Muyuan%20Ma%20and%20Qing%20Ma%20and%20Cong%20Bai%20and%20Shengyong%20Chen&entry.1292438233=%20%20Remote%20sensing%20image-text%20retrieval%20constitutes%20a%20foundational%20aspect%20of%0Aremote%20sensing%20interpretation%20tasks%2C%20facilitating%20the%20alignment%20of%20vision%20and%0Alanguage%20representations.%20This%20paper%20introduces%20a%20prior%20instruction%0Arepresentation%20%28PIR%29%20learning%20paradigm%20that%20draws%20on%20prior%20knowledge%20to%0Ainstruct%20adaptive%20learning%20of%20vision%20and%20text%20representations.%20Based%20on%20PIR%2C%20a%0Adomain-adapted%20remote%20sensing%20image-text%20retrieval%20framework%20PIR-ITR%20is%0Adesigned%20to%20address%20semantic%20noise%20issues%20in%20vision-language%20understanding%0Atasks.%20However%2C%20with%20massive%20additional%20data%20for%20pre-training%20the%0Avision-language%20foundation%20model%2C%20remote%20sensing%20image-text%20retrieval%20is%0Afurther%20developed%20into%20an%20open-domain%20retrieval%20task.%20Continuing%20with%20the%0Aabove%2C%20we%20propose%20PIR-CLIP%2C%20a%20domain-specific%20CLIP-based%20framework%20for%20remote%0Asensing%20image-text%20retrieval%2C%20to%20address%20semantic%20noise%20in%20remote%20sensing%0Avision-language%20representations%20and%20further%20improve%20open-domain%20retrieval%0Aperformance.%20In%20vision%20representation%2C%20Vision%20Instruction%20Representation%20%28VIR%29%0Abased%20on%20Spatial-PAE%20utilizes%20the%20prior-guided%20knowledge%20of%20the%20remote%20sensing%0Ascene%20recognition%20by%20building%20a%20belief%20matrix%20to%20select%20key%20features%20for%0Areducing%20the%20impact%20of%20semantic%20noise.%20In%20text%20representation%2C%20Language%20Cycle%0AAttention%20%28LCA%29%20based%20on%20Temporal-PAE%20uses%20the%20previous%20time%20step%20to%20cyclically%0Aactivate%20the%20current%20time%20step%20to%20enhance%20text%20representation%20capability.%20A%0Acluster-wise%20Affiliation%20Loss%20%28AL%29%20is%20proposed%20to%20constrain%20the%20inter-classes%0Aand%20to%20reduce%20the%20semantic%20confusion%20zones%20in%20the%20common%20subspace.%0AComprehensive%20experiments%20demonstrate%20that%20PIR%20could%20enhance%20vision%20and%20text%0Arepresentations%20and%20outperform%20the%20state-of-the-art%20methods%20of%20closed-domain%0Aand%20open-domain%20retrieval%20on%20two%20benchmark%20datasets%2C%20RSICD%20and%20RSITMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10160v1&entry.124074799=Read"},
{"title": "Common Corruptions for Enhancing and Evaluating Robustness in Air-to-Air\n  Visual Object Detection", "author": "Anastasios Arsenos and Vasileios Karampinis and Evangelos Petrongonas and Christos Skliros and Dimitrios Kollias and Stefanos Kollias and Athanasios Voulodimos", "abstract": "  The main barrier to achieving fully autonomous flights lies in autonomous\naircraft navigation. Managing non-cooperative traffic presents the most\nimportant challenge in this problem. The most efficient strategy for handling\nnon-cooperative traffic is based on monocular video processing through deep\nlearning models. This study contributes to the vision-based deep learning\naircraft detection and tracking literature by investigating the impact of data\ncorruption arising from environmental and hardware conditions on the\neffectiveness of these methods. More specifically, we designed $7$ types of\ncommon corruptions for camera inputs taking into account real-world flight\nconditions. By applying these corruptions to the Airborne Object Tracking (AOT)\ndataset we constructed the first robustness benchmark dataset named AOT-C for\nair-to-air aerial object detection. The corruptions included in this dataset\ncover a wide range of challenging conditions such as adverse weather and sensor\nnoise. The second main contribution of this letter is to present an extensive\nexperimental evaluation involving $8$ diverse object detectors to explore the\ndegradation in the performance under escalating levels of corruptions (domain\nshifts). Based on the evaluation results, the key observations that emerge are\nthe following: 1) One-stage detectors of the YOLO family demonstrate better\nrobustness, 2) Transformer-based and multi-stage detectors like Faster R-CNN\nare extremely vulnerable to corruptions, 3) Robustness against corruptions is\nrelated to the generalization ability of models. The third main contribution is\nto present that finetuning on our augmented synthetic data results in\nimprovements in the generalisation ability of the object detector in real-world\nflight experiments.\n", "link": "http://arxiv.org/abs/2405.06765v2", "date": "2024-05-16", "relevancy": 2.1484, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Common%20Corruptions%20for%20Enhancing%20and%20Evaluating%20Robustness%20in%20Air-to-Air%0A%20%20Visual%20Object%20Detection&body=Title%3A%20Common%20Corruptions%20for%20Enhancing%20and%20Evaluating%20Robustness%20in%20Air-to-Air%0A%20%20Visual%20Object%20Detection%0AAuthor%3A%20Anastasios%20Arsenos%20and%20Vasileios%20Karampinis%20and%20Evangelos%20Petrongonas%20and%20Christos%20Skliros%20and%20Dimitrios%20Kollias%20and%20Stefanos%20Kollias%20and%20Athanasios%20Voulodimos%0AAbstract%3A%20%20%20The%20main%20barrier%20to%20achieving%20fully%20autonomous%20flights%20lies%20in%20autonomous%0Aaircraft%20navigation.%20Managing%20non-cooperative%20traffic%20presents%20the%20most%0Aimportant%20challenge%20in%20this%20problem.%20The%20most%20efficient%20strategy%20for%20handling%0Anon-cooperative%20traffic%20is%20based%20on%20monocular%20video%20processing%20through%20deep%0Alearning%20models.%20This%20study%20contributes%20to%20the%20vision-based%20deep%20learning%0Aaircraft%20detection%20and%20tracking%20literature%20by%20investigating%20the%20impact%20of%20data%0Acorruption%20arising%20from%20environmental%20and%20hardware%20conditions%20on%20the%0Aeffectiveness%20of%20these%20methods.%20More%20specifically%2C%20we%20designed%20%247%24%20types%20of%0Acommon%20corruptions%20for%20camera%20inputs%20taking%20into%20account%20real-world%20flight%0Aconditions.%20By%20applying%20these%20corruptions%20to%20the%20Airborne%20Object%20Tracking%20%28AOT%29%0Adataset%20we%20constructed%20the%20first%20robustness%20benchmark%20dataset%20named%20AOT-C%20for%0Aair-to-air%20aerial%20object%20detection.%20The%20corruptions%20included%20in%20this%20dataset%0Acover%20a%20wide%20range%20of%20challenging%20conditions%20such%20as%20adverse%20weather%20and%20sensor%0Anoise.%20The%20second%20main%20contribution%20of%20this%20letter%20is%20to%20present%20an%20extensive%0Aexperimental%20evaluation%20involving%20%248%24%20diverse%20object%20detectors%20to%20explore%20the%0Adegradation%20in%20the%20performance%20under%20escalating%20levels%20of%20corruptions%20%28domain%0Ashifts%29.%20Based%20on%20the%20evaluation%20results%2C%20the%20key%20observations%20that%20emerge%20are%0Athe%20following%3A%201%29%20One-stage%20detectors%20of%20the%20YOLO%20family%20demonstrate%20better%0Arobustness%2C%202%29%20Transformer-based%20and%20multi-stage%20detectors%20like%20Faster%20R-CNN%0Aare%20extremely%20vulnerable%20to%20corruptions%2C%203%29%20Robustness%20against%20corruptions%20is%0Arelated%20to%20the%20generalization%20ability%20of%20models.%20The%20third%20main%20contribution%20is%0Ato%20present%20that%20finetuning%20on%20our%20augmented%20synthetic%20data%20results%20in%0Aimprovements%20in%20the%20generalisation%20ability%20of%20the%20object%20detector%20in%20real-world%0Aflight%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommon%2520Corruptions%2520for%2520Enhancing%2520and%2520Evaluating%2520Robustness%2520in%2520Air-to-Air%250A%2520%2520Visual%2520Object%2520Detection%26entry.906535625%3DAnastasios%2520Arsenos%2520and%2520Vasileios%2520Karampinis%2520and%2520Evangelos%2520Petrongonas%2520and%2520Christos%2520Skliros%2520and%2520Dimitrios%2520Kollias%2520and%2520Stefanos%2520Kollias%2520and%2520Athanasios%2520Voulodimos%26entry.1292438233%3D%2520%2520The%2520main%2520barrier%2520to%2520achieving%2520fully%2520autonomous%2520flights%2520lies%2520in%2520autonomous%250Aaircraft%2520navigation.%2520Managing%2520non-cooperative%2520traffic%2520presents%2520the%2520most%250Aimportant%2520challenge%2520in%2520this%2520problem.%2520The%2520most%2520efficient%2520strategy%2520for%2520handling%250Anon-cooperative%2520traffic%2520is%2520based%2520on%2520monocular%2520video%2520processing%2520through%2520deep%250Alearning%2520models.%2520This%2520study%2520contributes%2520to%2520the%2520vision-based%2520deep%2520learning%250Aaircraft%2520detection%2520and%2520tracking%2520literature%2520by%2520investigating%2520the%2520impact%2520of%2520data%250Acorruption%2520arising%2520from%2520environmental%2520and%2520hardware%2520conditions%2520on%2520the%250Aeffectiveness%2520of%2520these%2520methods.%2520More%2520specifically%252C%2520we%2520designed%2520%25247%2524%2520types%2520of%250Acommon%2520corruptions%2520for%2520camera%2520inputs%2520taking%2520into%2520account%2520real-world%2520flight%250Aconditions.%2520By%2520applying%2520these%2520corruptions%2520to%2520the%2520Airborne%2520Object%2520Tracking%2520%2528AOT%2529%250Adataset%2520we%2520constructed%2520the%2520first%2520robustness%2520benchmark%2520dataset%2520named%2520AOT-C%2520for%250Aair-to-air%2520aerial%2520object%2520detection.%2520The%2520corruptions%2520included%2520in%2520this%2520dataset%250Acover%2520a%2520wide%2520range%2520of%2520challenging%2520conditions%2520such%2520as%2520adverse%2520weather%2520and%2520sensor%250Anoise.%2520The%2520second%2520main%2520contribution%2520of%2520this%2520letter%2520is%2520to%2520present%2520an%2520extensive%250Aexperimental%2520evaluation%2520involving%2520%25248%2524%2520diverse%2520object%2520detectors%2520to%2520explore%2520the%250Adegradation%2520in%2520the%2520performance%2520under%2520escalating%2520levels%2520of%2520corruptions%2520%2528domain%250Ashifts%2529.%2520Based%2520on%2520the%2520evaluation%2520results%252C%2520the%2520key%2520observations%2520that%2520emerge%2520are%250Athe%2520following%253A%25201%2529%2520One-stage%2520detectors%2520of%2520the%2520YOLO%2520family%2520demonstrate%2520better%250Arobustness%252C%25202%2529%2520Transformer-based%2520and%2520multi-stage%2520detectors%2520like%2520Faster%2520R-CNN%250Aare%2520extremely%2520vulnerable%2520to%2520corruptions%252C%25203%2529%2520Robustness%2520against%2520corruptions%2520is%250Arelated%2520to%2520the%2520generalization%2520ability%2520of%2520models.%2520The%2520third%2520main%2520contribution%2520is%250Ato%2520present%2520that%2520finetuning%2520on%2520our%2520augmented%2520synthetic%2520data%2520results%2520in%250Aimprovements%2520in%2520the%2520generalisation%2520ability%2520of%2520the%2520object%2520detector%2520in%2520real-world%250Aflight%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Common%20Corruptions%20for%20Enhancing%20and%20Evaluating%20Robustness%20in%20Air-to-Air%0A%20%20Visual%20Object%20Detection&entry.906535625=Anastasios%20Arsenos%20and%20Vasileios%20Karampinis%20and%20Evangelos%20Petrongonas%20and%20Christos%20Skliros%20and%20Dimitrios%20Kollias%20and%20Stefanos%20Kollias%20and%20Athanasios%20Voulodimos&entry.1292438233=%20%20The%20main%20barrier%20to%20achieving%20fully%20autonomous%20flights%20lies%20in%20autonomous%0Aaircraft%20navigation.%20Managing%20non-cooperative%20traffic%20presents%20the%20most%0Aimportant%20challenge%20in%20this%20problem.%20The%20most%20efficient%20strategy%20for%20handling%0Anon-cooperative%20traffic%20is%20based%20on%20monocular%20video%20processing%20through%20deep%0Alearning%20models.%20This%20study%20contributes%20to%20the%20vision-based%20deep%20learning%0Aaircraft%20detection%20and%20tracking%20literature%20by%20investigating%20the%20impact%20of%20data%0Acorruption%20arising%20from%20environmental%20and%20hardware%20conditions%20on%20the%0Aeffectiveness%20of%20these%20methods.%20More%20specifically%2C%20we%20designed%20%247%24%20types%20of%0Acommon%20corruptions%20for%20camera%20inputs%20taking%20into%20account%20real-world%20flight%0Aconditions.%20By%20applying%20these%20corruptions%20to%20the%20Airborne%20Object%20Tracking%20%28AOT%29%0Adataset%20we%20constructed%20the%20first%20robustness%20benchmark%20dataset%20named%20AOT-C%20for%0Aair-to-air%20aerial%20object%20detection.%20The%20corruptions%20included%20in%20this%20dataset%0Acover%20a%20wide%20range%20of%20challenging%20conditions%20such%20as%20adverse%20weather%20and%20sensor%0Anoise.%20The%20second%20main%20contribution%20of%20this%20letter%20is%20to%20present%20an%20extensive%0Aexperimental%20evaluation%20involving%20%248%24%20diverse%20object%20detectors%20to%20explore%20the%0Adegradation%20in%20the%20performance%20under%20escalating%20levels%20of%20corruptions%20%28domain%0Ashifts%29.%20Based%20on%20the%20evaluation%20results%2C%20the%20key%20observations%20that%20emerge%20are%0Athe%20following%3A%201%29%20One-stage%20detectors%20of%20the%20YOLO%20family%20demonstrate%20better%0Arobustness%2C%202%29%20Transformer-based%20and%20multi-stage%20detectors%20like%20Faster%20R-CNN%0Aare%20extremely%20vulnerable%20to%20corruptions%2C%203%29%20Robustness%20against%20corruptions%20is%0Arelated%20to%20the%20generalization%20ability%20of%20models.%20The%20third%20main%20contribution%20is%0Ato%20present%20that%20finetuning%20on%20our%20augmented%20synthetic%20data%20results%20in%0Aimprovements%20in%20the%20generalisation%20ability%20of%20the%20object%20detector%20in%20real-world%0Aflight%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06765v2&entry.124074799=Read"},
{"title": "Deepfake Generation and Detection: A Benchmark and Survey", "author": "Gan Pei and Jiangning Zhang and Menghan Hu and Zhenyu Zhang and Chengjie Wang and Yunsheng Wu and Guangtao Zhai and Jian Yang and Chunhua Shen and Dacheng Tao", "abstract": "  Deepfake is a technology dedicated to creating highly realistic facial images\nand videos under specific conditions, which has significant application\npotential in fields such as entertainment, movie production, digital human\ncreation, to name a few. With the advancements in deep learning, techniques\nprimarily represented by Variational Autoencoders and Generative Adversarial\nNetworks have achieved impressive generation results. More recently, the\nemergence of diffusion models with powerful generation capabilities has sparked\na renewed wave of research. In addition to deepfake generation, corresponding\ndetection technologies continuously evolve to regulate the potential misuse of\ndeepfakes, such as for privacy invasion and phishing attacks. This survey\ncomprehensively reviews the latest developments in deepfake generation and\ndetection, summarizing and analyzing current state-of-the-arts in this rapidly\nevolving field. We first unify task definitions, comprehensively introduce\ndatasets and metrics, and discuss developing technologies. Then, we discuss the\ndevelopment of several related sub-fields and focus on researching four\nrepresentative deepfake fields: face swapping, face reenactment, talking face\ngeneration, and facial attribute editing, as well as forgery detection.\nSubsequently, we comprehensively benchmark representative methods on popular\ndatasets for each field, fully evaluating the latest and influential published\nworks. Finally, we analyze challenges and future research directions of the\ndiscussed fields.\n", "link": "http://arxiv.org/abs/2403.17881v4", "date": "2024-05-16", "relevancy": 2.1296, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5436}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5349}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&body=Title%3A%20Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey%0AAuthor%3A%20Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Zhenyu%20Zhang%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Guangtao%20Zhai%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Deepfake%20is%20a%20technology%20dedicated%20to%20creating%20highly%20realistic%20facial%20images%0Aand%20videos%20under%20specific%20conditions%2C%20which%20has%20significant%20application%0Apotential%20in%20fields%20such%20as%20entertainment%2C%20movie%20production%2C%20digital%20human%0Acreation%2C%20to%20name%20a%20few.%20With%20the%20advancements%20in%20deep%20learning%2C%20techniques%0Aprimarily%20represented%20by%20Variational%20Autoencoders%20and%20Generative%20Adversarial%0ANetworks%20have%20achieved%20impressive%20generation%20results.%20More%20recently%2C%20the%0Aemergence%20of%20diffusion%20models%20with%20powerful%20generation%20capabilities%20has%20sparked%0Aa%20renewed%20wave%20of%20research.%20In%20addition%20to%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20continuously%20evolve%20to%20regulate%20the%20potential%20misuse%20of%0Adeepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%20survey%0Acomprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%20and%0Adetection%2C%20summarizing%20and%20analyzing%20current%20state-of-the-arts%20in%20this%20rapidly%0Aevolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%20introduce%0Adatasets%20and%20metrics%2C%20and%20discuss%20developing%20technologies.%20Then%2C%20we%20discuss%20the%0Adevelopment%20of%20several%20related%20sub-fields%20and%20focus%20on%20researching%20four%0Arepresentative%20deepfake%20fields%3A%20face%20swapping%2C%20face%20reenactment%2C%20talking%20face%0Ageneration%2C%20and%20facial%20attribute%20editing%2C%20as%20well%20as%20forgery%20detection.%0ASubsequently%2C%20we%20comprehensively%20benchmark%20representative%20methods%20on%20popular%0Adatasets%20for%20each%20field%2C%20fully%20evaluating%20the%20latest%20and%20influential%20published%0Aworks.%20Finally%2C%20we%20analyze%20challenges%20and%20future%20research%20directions%20of%20the%0Adiscussed%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17881v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepfake%2520Generation%2520and%2520Detection%253A%2520A%2520Benchmark%2520and%2520Survey%26entry.906535625%3DGan%2520Pei%2520and%2520Jiangning%2520Zhang%2520and%2520Menghan%2520Hu%2520and%2520Zhenyu%2520Zhang%2520and%2520Chengjie%2520Wang%2520and%2520Yunsheng%2520Wu%2520and%2520Guangtao%2520Zhai%2520and%2520Jian%2520Yang%2520and%2520Chunhua%2520Shen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Deepfake%2520is%2520a%2520technology%2520dedicated%2520to%2520creating%2520highly%2520realistic%2520facial%2520images%250Aand%2520videos%2520under%2520specific%2520conditions%252C%2520which%2520has%2520significant%2520application%250Apotential%2520in%2520fields%2520such%2520as%2520entertainment%252C%2520movie%2520production%252C%2520digital%2520human%250Acreation%252C%2520to%2520name%2520a%2520few.%2520With%2520the%2520advancements%2520in%2520deep%2520learning%252C%2520techniques%250Aprimarily%2520represented%2520by%2520Variational%2520Autoencoders%2520and%2520Generative%2520Adversarial%250ANetworks%2520have%2520achieved%2520impressive%2520generation%2520results.%2520More%2520recently%252C%2520the%250Aemergence%2520of%2520diffusion%2520models%2520with%2520powerful%2520generation%2520capabilities%2520has%2520sparked%250Aa%2520renewed%2520wave%2520of%2520research.%2520In%2520addition%2520to%2520deepfake%2520generation%252C%2520corresponding%250Adetection%2520technologies%2520continuously%2520evolve%2520to%2520regulate%2520the%2520potential%2520misuse%2520of%250Adeepfakes%252C%2520such%2520as%2520for%2520privacy%2520invasion%2520and%2520phishing%2520attacks.%2520This%2520survey%250Acomprehensively%2520reviews%2520the%2520latest%2520developments%2520in%2520deepfake%2520generation%2520and%250Adetection%252C%2520summarizing%2520and%2520analyzing%2520current%2520state-of-the-arts%2520in%2520this%2520rapidly%250Aevolving%2520field.%2520We%2520first%2520unify%2520task%2520definitions%252C%2520comprehensively%2520introduce%250Adatasets%2520and%2520metrics%252C%2520and%2520discuss%2520developing%2520technologies.%2520Then%252C%2520we%2520discuss%2520the%250Adevelopment%2520of%2520several%2520related%2520sub-fields%2520and%2520focus%2520on%2520researching%2520four%250Arepresentative%2520deepfake%2520fields%253A%2520face%2520swapping%252C%2520face%2520reenactment%252C%2520talking%2520face%250Ageneration%252C%2520and%2520facial%2520attribute%2520editing%252C%2520as%2520well%2520as%2520forgery%2520detection.%250ASubsequently%252C%2520we%2520comprehensively%2520benchmark%2520representative%2520methods%2520on%2520popular%250Adatasets%2520for%2520each%2520field%252C%2520fully%2520evaluating%2520the%2520latest%2520and%2520influential%2520published%250Aworks.%2520Finally%252C%2520we%2520analyze%2520challenges%2520and%2520future%2520research%2520directions%2520of%2520the%250Adiscussed%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17881v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deepfake%20Generation%20and%20Detection%3A%20A%20Benchmark%20and%20Survey&entry.906535625=Gan%20Pei%20and%20Jiangning%20Zhang%20and%20Menghan%20Hu%20and%20Zhenyu%20Zhang%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Guangtao%20Zhai%20and%20Jian%20Yang%20and%20Chunhua%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20Deepfake%20is%20a%20technology%20dedicated%20to%20creating%20highly%20realistic%20facial%20images%0Aand%20videos%20under%20specific%20conditions%2C%20which%20has%20significant%20application%0Apotential%20in%20fields%20such%20as%20entertainment%2C%20movie%20production%2C%20digital%20human%0Acreation%2C%20to%20name%20a%20few.%20With%20the%20advancements%20in%20deep%20learning%2C%20techniques%0Aprimarily%20represented%20by%20Variational%20Autoencoders%20and%20Generative%20Adversarial%0ANetworks%20have%20achieved%20impressive%20generation%20results.%20More%20recently%2C%20the%0Aemergence%20of%20diffusion%20models%20with%20powerful%20generation%20capabilities%20has%20sparked%0Aa%20renewed%20wave%20of%20research.%20In%20addition%20to%20deepfake%20generation%2C%20corresponding%0Adetection%20technologies%20continuously%20evolve%20to%20regulate%20the%20potential%20misuse%20of%0Adeepfakes%2C%20such%20as%20for%20privacy%20invasion%20and%20phishing%20attacks.%20This%20survey%0Acomprehensively%20reviews%20the%20latest%20developments%20in%20deepfake%20generation%20and%0Adetection%2C%20summarizing%20and%20analyzing%20current%20state-of-the-arts%20in%20this%20rapidly%0Aevolving%20field.%20We%20first%20unify%20task%20definitions%2C%20comprehensively%20introduce%0Adatasets%20and%20metrics%2C%20and%20discuss%20developing%20technologies.%20Then%2C%20we%20discuss%20the%0Adevelopment%20of%20several%20related%20sub-fields%20and%20focus%20on%20researching%20four%0Arepresentative%20deepfake%20fields%3A%20face%20swapping%2C%20face%20reenactment%2C%20talking%20face%0Ageneration%2C%20and%20facial%20attribute%20editing%2C%20as%20well%20as%20forgery%20detection.%0ASubsequently%2C%20we%20comprehensively%20benchmark%20representative%20methods%20on%20popular%0Adatasets%20for%20each%20field%2C%20fully%20evaluating%20the%20latest%20and%20influential%20published%0Aworks.%20Finally%2C%20we%20analyze%20challenges%20and%20future%20research%20directions%20of%20the%0Adiscussed%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17881v4&entry.124074799=Read"},
{"title": "RSDehamba: Lightweight Vision Mamba for Remote Sensing Satellite Image\n  Dehazing", "author": "Huiling Zhou and Xianhao Wu and Hongming Chen and Xiang Chen and Xin He", "abstract": "  Remote sensing image dehazing (RSID) aims to remove nonuniform and physically\nirregular haze factors for high-quality image restoration. The emergence of\nCNNs and Transformers has taken extraordinary strides in the RSID arena.\nHowever, these methods often struggle to demonstrate the balance of adequate\nlong-range dependency modeling and maintaining computational efficiency. To\nthis end, we propose the first lightweight network on the mamba-based model\ncalled RSDhamba in the field of RSID. Greatly inspired by the recent rise of\nSelective State Space Model (SSM) for its superior performance in modeling\nlinear complexity and remote dependencies, our designed RSDehamba integrates\nthe SSM framework into the U-Net architecture. Specifically, we propose the\nVision Dehamba Block (VDB) as the core component of the overall network, which\nutilizes the linear complexity of SSM to achieve the capability of global\ncontext encoding. Simultaneously, the Direction-aware Scan Module (DSM) is\ndesigned to dynamically aggregate feature exchanges over different directional\ndomains to effectively enhance the flexibility of sensing the spatially varying\ndistribution of haze. In this way, our RSDhamba fully demonstrates the\nsuperiority of spatial distance capture dependencies and channel information\nexchange for better extraction of haze features. Extensive experimental results\non widely used benchmarks validate the surpassing performance of our RSDehamba\nagainst existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.10030v1", "date": "2024-05-16", "relevancy": 2.1275, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5835}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5291}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSDehamba%3A%20Lightweight%20Vision%20Mamba%20for%20Remote%20Sensing%20Satellite%20Image%0A%20%20Dehazing&body=Title%3A%20RSDehamba%3A%20Lightweight%20Vision%20Mamba%20for%20Remote%20Sensing%20Satellite%20Image%0A%20%20Dehazing%0AAuthor%3A%20Huiling%20Zhou%20and%20Xianhao%20Wu%20and%20Hongming%20Chen%20and%20Xiang%20Chen%20and%20Xin%20He%0AAbstract%3A%20%20%20Remote%20sensing%20image%20dehazing%20%28RSID%29%20aims%20to%20remove%20nonuniform%20and%20physically%0Airregular%20haze%20factors%20for%20high-quality%20image%20restoration.%20The%20emergence%20of%0ACNNs%20and%20Transformers%20has%20taken%20extraordinary%20strides%20in%20the%20RSID%20arena.%0AHowever%2C%20these%20methods%20often%20struggle%20to%20demonstrate%20the%20balance%20of%20adequate%0Along-range%20dependency%20modeling%20and%20maintaining%20computational%20efficiency.%20To%0Athis%20end%2C%20we%20propose%20the%20first%20lightweight%20network%20on%20the%20mamba-based%20model%0Acalled%20RSDhamba%20in%20the%20field%20of%20RSID.%20Greatly%20inspired%20by%20the%20recent%20rise%20of%0ASelective%20State%20Space%20Model%20%28SSM%29%20for%20its%20superior%20performance%20in%20modeling%0Alinear%20complexity%20and%20remote%20dependencies%2C%20our%20designed%20RSDehamba%20integrates%0Athe%20SSM%20framework%20into%20the%20U-Net%20architecture.%20Specifically%2C%20we%20propose%20the%0AVision%20Dehamba%20Block%20%28VDB%29%20as%20the%20core%20component%20of%20the%20overall%20network%2C%20which%0Autilizes%20the%20linear%20complexity%20of%20SSM%20to%20achieve%20the%20capability%20of%20global%0Acontext%20encoding.%20Simultaneously%2C%20the%20Direction-aware%20Scan%20Module%20%28DSM%29%20is%0Adesigned%20to%20dynamically%20aggregate%20feature%20exchanges%20over%20different%20directional%0Adomains%20to%20effectively%20enhance%20the%20flexibility%20of%20sensing%20the%20spatially%20varying%0Adistribution%20of%20haze.%20In%20this%20way%2C%20our%20RSDhamba%20fully%20demonstrates%20the%0Asuperiority%20of%20spatial%20distance%20capture%20dependencies%20and%20channel%20information%0Aexchange%20for%20better%20extraction%20of%20haze%20features.%20Extensive%20experimental%20results%0Aon%20widely%20used%20benchmarks%20validate%20the%20surpassing%20performance%20of%20our%20RSDehamba%0Aagainst%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSDehamba%253A%2520Lightweight%2520Vision%2520Mamba%2520for%2520Remote%2520Sensing%2520Satellite%2520Image%250A%2520%2520Dehazing%26entry.906535625%3DHuiling%2520Zhou%2520and%2520Xianhao%2520Wu%2520and%2520Hongming%2520Chen%2520and%2520Xiang%2520Chen%2520and%2520Xin%2520He%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520dehazing%2520%2528RSID%2529%2520aims%2520to%2520remove%2520nonuniform%2520and%2520physically%250Airregular%2520haze%2520factors%2520for%2520high-quality%2520image%2520restoration.%2520The%2520emergence%2520of%250ACNNs%2520and%2520Transformers%2520has%2520taken%2520extraordinary%2520strides%2520in%2520the%2520RSID%2520arena.%250AHowever%252C%2520these%2520methods%2520often%2520struggle%2520to%2520demonstrate%2520the%2520balance%2520of%2520adequate%250Along-range%2520dependency%2520modeling%2520and%2520maintaining%2520computational%2520efficiency.%2520To%250Athis%2520end%252C%2520we%2520propose%2520the%2520first%2520lightweight%2520network%2520on%2520the%2520mamba-based%2520model%250Acalled%2520RSDhamba%2520in%2520the%2520field%2520of%2520RSID.%2520Greatly%2520inspired%2520by%2520the%2520recent%2520rise%2520of%250ASelective%2520State%2520Space%2520Model%2520%2528SSM%2529%2520for%2520its%2520superior%2520performance%2520in%2520modeling%250Alinear%2520complexity%2520and%2520remote%2520dependencies%252C%2520our%2520designed%2520RSDehamba%2520integrates%250Athe%2520SSM%2520framework%2520into%2520the%2520U-Net%2520architecture.%2520Specifically%252C%2520we%2520propose%2520the%250AVision%2520Dehamba%2520Block%2520%2528VDB%2529%2520as%2520the%2520core%2520component%2520of%2520the%2520overall%2520network%252C%2520which%250Autilizes%2520the%2520linear%2520complexity%2520of%2520SSM%2520to%2520achieve%2520the%2520capability%2520of%2520global%250Acontext%2520encoding.%2520Simultaneously%252C%2520the%2520Direction-aware%2520Scan%2520Module%2520%2528DSM%2529%2520is%250Adesigned%2520to%2520dynamically%2520aggregate%2520feature%2520exchanges%2520over%2520different%2520directional%250Adomains%2520to%2520effectively%2520enhance%2520the%2520flexibility%2520of%2520sensing%2520the%2520spatially%2520varying%250Adistribution%2520of%2520haze.%2520In%2520this%2520way%252C%2520our%2520RSDhamba%2520fully%2520demonstrates%2520the%250Asuperiority%2520of%2520spatial%2520distance%2520capture%2520dependencies%2520and%2520channel%2520information%250Aexchange%2520for%2520better%2520extraction%2520of%2520haze%2520features.%2520Extensive%2520experimental%2520results%250Aon%2520widely%2520used%2520benchmarks%2520validate%2520the%2520surpassing%2520performance%2520of%2520our%2520RSDehamba%250Aagainst%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSDehamba%3A%20Lightweight%20Vision%20Mamba%20for%20Remote%20Sensing%20Satellite%20Image%0A%20%20Dehazing&entry.906535625=Huiling%20Zhou%20and%20Xianhao%20Wu%20and%20Hongming%20Chen%20and%20Xiang%20Chen%20and%20Xin%20He&entry.1292438233=%20%20Remote%20sensing%20image%20dehazing%20%28RSID%29%20aims%20to%20remove%20nonuniform%20and%20physically%0Airregular%20haze%20factors%20for%20high-quality%20image%20restoration.%20The%20emergence%20of%0ACNNs%20and%20Transformers%20has%20taken%20extraordinary%20strides%20in%20the%20RSID%20arena.%0AHowever%2C%20these%20methods%20often%20struggle%20to%20demonstrate%20the%20balance%20of%20adequate%0Along-range%20dependency%20modeling%20and%20maintaining%20computational%20efficiency.%20To%0Athis%20end%2C%20we%20propose%20the%20first%20lightweight%20network%20on%20the%20mamba-based%20model%0Acalled%20RSDhamba%20in%20the%20field%20of%20RSID.%20Greatly%20inspired%20by%20the%20recent%20rise%20of%0ASelective%20State%20Space%20Model%20%28SSM%29%20for%20its%20superior%20performance%20in%20modeling%0Alinear%20complexity%20and%20remote%20dependencies%2C%20our%20designed%20RSDehamba%20integrates%0Athe%20SSM%20framework%20into%20the%20U-Net%20architecture.%20Specifically%2C%20we%20propose%20the%0AVision%20Dehamba%20Block%20%28VDB%29%20as%20the%20core%20component%20of%20the%20overall%20network%2C%20which%0Autilizes%20the%20linear%20complexity%20of%20SSM%20to%20achieve%20the%20capability%20of%20global%0Acontext%20encoding.%20Simultaneously%2C%20the%20Direction-aware%20Scan%20Module%20%28DSM%29%20is%0Adesigned%20to%20dynamically%20aggregate%20feature%20exchanges%20over%20different%20directional%0Adomains%20to%20effectively%20enhance%20the%20flexibility%20of%20sensing%20the%20spatially%20varying%0Adistribution%20of%20haze.%20In%20this%20way%2C%20our%20RSDhamba%20fully%20demonstrates%20the%0Asuperiority%20of%20spatial%20distance%20capture%20dependencies%20and%20channel%20information%0Aexchange%20for%20better%20extraction%20of%20haze%20features.%20Extensive%20experimental%20results%0Aon%20widely%20used%20benchmarks%20validate%20the%20surpassing%20performance%20of%20our%20RSDehamba%0Aagainst%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10030v1&entry.124074799=Read"},
{"title": "HecVL: Hierarchical Video-Language Pretraining for Zero-shot Surgical\n  Phase Recognition", "author": "Kun Yuan and Vinkle Srivastav and Nassir Navab and Nicolas Padoy", "abstract": "  Natural language could play an important role in developing generalist\nsurgical models by providing a broad source of supervision from raw texts. This\nflexible form of supervision can enable the model's transferability across\ndatasets and tasks as natural language can be used to reference learned visual\nconcepts or describe new ones. In this work, we present HecVL, a novel\nhierarchical video-language pretraining approach for building a generalist\nsurgical model. Specifically, we construct a hierarchical video-text paired\ndataset by pairing the surgical lecture video with three hierarchical levels of\ntexts: at clip-level, atomic actions using transcribed audio texts; at\nphase-level, conceptual text summaries; and at video-level, overall abstract\ntext of the surgical procedure. Then, we propose a novel fine-to-coarse\ncontrastive learning framework that learns separate embedding spaces for the\nthree video-text hierarchies using a single model. By disentangling embedding\nspaces of different hierarchical levels, the learned multi-modal\nrepresentations encode short-term and long-term surgical concepts in the same\nmodel. Thanks to the injected textual semantics, we demonstrate that the HecVL\napproach can enable zero-shot surgical phase recognition without any human\nannotation. Furthermore, we show that the same HecVL model for surgical phase\nrecognition can be transferred across different surgical procedures and medical\ncenters.\n", "link": "http://arxiv.org/abs/2405.10075v1", "date": "2024-05-16", "relevancy": 2.1156, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5299}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HecVL%3A%20Hierarchical%20Video-Language%20Pretraining%20for%20Zero-shot%20Surgical%0A%20%20Phase%20Recognition&body=Title%3A%20HecVL%3A%20Hierarchical%20Video-Language%20Pretraining%20for%20Zero-shot%20Surgical%0A%20%20Phase%20Recognition%0AAuthor%3A%20Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Natural%20language%20could%20play%20an%20important%20role%20in%20developing%20generalist%0Asurgical%20models%20by%20providing%20a%20broad%20source%20of%20supervision%20from%20raw%20texts.%20This%0Aflexible%20form%20of%20supervision%20can%20enable%20the%20model%27s%20transferability%20across%0Adatasets%20and%20tasks%20as%20natural%20language%20can%20be%20used%20to%20reference%20learned%20visual%0Aconcepts%20or%20describe%20new%20ones.%20In%20this%20work%2C%20we%20present%20HecVL%2C%20a%20novel%0Ahierarchical%20video-language%20pretraining%20approach%20for%20building%20a%20generalist%0Asurgical%20model.%20Specifically%2C%20we%20construct%20a%20hierarchical%20video-text%20paired%0Adataset%20by%20pairing%20the%20surgical%20lecture%20video%20with%20three%20hierarchical%20levels%20of%0Atexts%3A%20at%20clip-level%2C%20atomic%20actions%20using%20transcribed%20audio%20texts%3B%20at%0Aphase-level%2C%20conceptual%20text%20summaries%3B%20and%20at%20video-level%2C%20overall%20abstract%0Atext%20of%20the%20surgical%20procedure.%20Then%2C%20we%20propose%20a%20novel%20fine-to-coarse%0Acontrastive%20learning%20framework%20that%20learns%20separate%20embedding%20spaces%20for%20the%0Athree%20video-text%20hierarchies%20using%20a%20single%20model.%20By%20disentangling%20embedding%0Aspaces%20of%20different%20hierarchical%20levels%2C%20the%20learned%20multi-modal%0Arepresentations%20encode%20short-term%20and%20long-term%20surgical%20concepts%20in%20the%20same%0Amodel.%20Thanks%20to%20the%20injected%20textual%20semantics%2C%20we%20demonstrate%20that%20the%20HecVL%0Aapproach%20can%20enable%20zero-shot%20surgical%20phase%20recognition%20without%20any%20human%0Aannotation.%20Furthermore%2C%20we%20show%20that%20the%20same%20HecVL%20model%20for%20surgical%20phase%0Arecognition%20can%20be%20transferred%20across%20different%20surgical%20procedures%20and%20medical%0Acenters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHecVL%253A%2520Hierarchical%2520Video-Language%2520Pretraining%2520for%2520Zero-shot%2520Surgical%250A%2520%2520Phase%2520Recognition%26entry.906535625%3DKun%2520Yuan%2520and%2520Vinkle%2520Srivastav%2520and%2520Nassir%2520Navab%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Natural%2520language%2520could%2520play%2520an%2520important%2520role%2520in%2520developing%2520generalist%250Asurgical%2520models%2520by%2520providing%2520a%2520broad%2520source%2520of%2520supervision%2520from%2520raw%2520texts.%2520This%250Aflexible%2520form%2520of%2520supervision%2520can%2520enable%2520the%2520model%2527s%2520transferability%2520across%250Adatasets%2520and%2520tasks%2520as%2520natural%2520language%2520can%2520be%2520used%2520to%2520reference%2520learned%2520visual%250Aconcepts%2520or%2520describe%2520new%2520ones.%2520In%2520this%2520work%252C%2520we%2520present%2520HecVL%252C%2520a%2520novel%250Ahierarchical%2520video-language%2520pretraining%2520approach%2520for%2520building%2520a%2520generalist%250Asurgical%2520model.%2520Specifically%252C%2520we%2520construct%2520a%2520hierarchical%2520video-text%2520paired%250Adataset%2520by%2520pairing%2520the%2520surgical%2520lecture%2520video%2520with%2520three%2520hierarchical%2520levels%2520of%250Atexts%253A%2520at%2520clip-level%252C%2520atomic%2520actions%2520using%2520transcribed%2520audio%2520texts%253B%2520at%250Aphase-level%252C%2520conceptual%2520text%2520summaries%253B%2520and%2520at%2520video-level%252C%2520overall%2520abstract%250Atext%2520of%2520the%2520surgical%2520procedure.%2520Then%252C%2520we%2520propose%2520a%2520novel%2520fine-to-coarse%250Acontrastive%2520learning%2520framework%2520that%2520learns%2520separate%2520embedding%2520spaces%2520for%2520the%250Athree%2520video-text%2520hierarchies%2520using%2520a%2520single%2520model.%2520By%2520disentangling%2520embedding%250Aspaces%2520of%2520different%2520hierarchical%2520levels%252C%2520the%2520learned%2520multi-modal%250Arepresentations%2520encode%2520short-term%2520and%2520long-term%2520surgical%2520concepts%2520in%2520the%2520same%250Amodel.%2520Thanks%2520to%2520the%2520injected%2520textual%2520semantics%252C%2520we%2520demonstrate%2520that%2520the%2520HecVL%250Aapproach%2520can%2520enable%2520zero-shot%2520surgical%2520phase%2520recognition%2520without%2520any%2520human%250Aannotation.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520same%2520HecVL%2520model%2520for%2520surgical%2520phase%250Arecognition%2520can%2520be%2520transferred%2520across%2520different%2520surgical%2520procedures%2520and%2520medical%250Acenters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HecVL%3A%20Hierarchical%20Video-Language%20Pretraining%20for%20Zero-shot%20Surgical%0A%20%20Phase%20Recognition&entry.906535625=Kun%20Yuan%20and%20Vinkle%20Srivastav%20and%20Nassir%20Navab%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Natural%20language%20could%20play%20an%20important%20role%20in%20developing%20generalist%0Asurgical%20models%20by%20providing%20a%20broad%20source%20of%20supervision%20from%20raw%20texts.%20This%0Aflexible%20form%20of%20supervision%20can%20enable%20the%20model%27s%20transferability%20across%0Adatasets%20and%20tasks%20as%20natural%20language%20can%20be%20used%20to%20reference%20learned%20visual%0Aconcepts%20or%20describe%20new%20ones.%20In%20this%20work%2C%20we%20present%20HecVL%2C%20a%20novel%0Ahierarchical%20video-language%20pretraining%20approach%20for%20building%20a%20generalist%0Asurgical%20model.%20Specifically%2C%20we%20construct%20a%20hierarchical%20video-text%20paired%0Adataset%20by%20pairing%20the%20surgical%20lecture%20video%20with%20three%20hierarchical%20levels%20of%0Atexts%3A%20at%20clip-level%2C%20atomic%20actions%20using%20transcribed%20audio%20texts%3B%20at%0Aphase-level%2C%20conceptual%20text%20summaries%3B%20and%20at%20video-level%2C%20overall%20abstract%0Atext%20of%20the%20surgical%20procedure.%20Then%2C%20we%20propose%20a%20novel%20fine-to-coarse%0Acontrastive%20learning%20framework%20that%20learns%20separate%20embedding%20spaces%20for%20the%0Athree%20video-text%20hierarchies%20using%20a%20single%20model.%20By%20disentangling%20embedding%0Aspaces%20of%20different%20hierarchical%20levels%2C%20the%20learned%20multi-modal%0Arepresentations%20encode%20short-term%20and%20long-term%20surgical%20concepts%20in%20the%20same%0Amodel.%20Thanks%20to%20the%20injected%20textual%20semantics%2C%20we%20demonstrate%20that%20the%20HecVL%0Aapproach%20can%20enable%20zero-shot%20surgical%20phase%20recognition%20without%20any%20human%0Aannotation.%20Furthermore%2C%20we%20show%20that%20the%20same%20HecVL%20model%20for%20surgical%20phase%0Arecognition%20can%20be%20transferred%20across%20different%20surgical%20procedures%20and%20medical%0Acenters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10075v1&entry.124074799=Read"},
{"title": "MrRegNet: Multi-resolution Mask Guided Convolutional Neural Network for\n  Medical Image Registration with Large Deformations", "author": "Ruizhe Li and Grazziela Figueredo and Dorothee Auer and Christian Wagner and Xin Chen", "abstract": "  Deformable image registration (alignment) is highly sought after in numerous\nclinical applications, such as computer aided diagnosis and disease progression\nanalysis. Deep Convolutional Neural Network (DCNN)-based image registration\nmethods have demonstrated advantages in terms of registration accuracy and\ncomputational speed. However, while most methods excel at global alignment,\nthey often perform worse in aligning local regions. To address this challenge,\nthis paper proposes a mask-guided encoder-decoder DCNN-based image registration\nmethod, named as MrRegNet. This approach employs a multi-resolution encoder for\nfeature extraction and subsequently estimates multi-resolution displacement\nfields in the decoder to handle the substantial deformation of images.\nFurthermore, segmentation masks are employed to direct the model's attention\ntoward aligning local regions. The results show that the proposed method\noutperforms traditional methods like Demons and a well-known deep learning\nmethod, VoxelMorph, on a public 3D brain MRI dataset (OASIS) and a local 2D\nbrain MRI dataset with large deformations. Importantly, the image alignment\naccuracies are significantly improved at local regions guided by segmentation\nmasks. Github link:https://github.com/ruizhe-l/MrRegNet.\n", "link": "http://arxiv.org/abs/2405.10068v1", "date": "2024-05-16", "relevancy": 2.1004, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5044}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MrRegNet%3A%20Multi-resolution%20Mask%20Guided%20Convolutional%20Neural%20Network%20for%0A%20%20Medical%20Image%20Registration%20with%20Large%20Deformations&body=Title%3A%20MrRegNet%3A%20Multi-resolution%20Mask%20Guided%20Convolutional%20Neural%20Network%20for%0A%20%20Medical%20Image%20Registration%20with%20Large%20Deformations%0AAuthor%3A%20Ruizhe%20Li%20and%20Grazziela%20Figueredo%20and%20Dorothee%20Auer%20and%20Christian%20Wagner%20and%20Xin%20Chen%0AAbstract%3A%20%20%20Deformable%20image%20registration%20%28alignment%29%20is%20highly%20sought%20after%20in%20numerous%0Aclinical%20applications%2C%20such%20as%20computer%20aided%20diagnosis%20and%20disease%20progression%0Aanalysis.%20Deep%20Convolutional%20Neural%20Network%20%28DCNN%29-based%20image%20registration%0Amethods%20have%20demonstrated%20advantages%20in%20terms%20of%20registration%20accuracy%20and%0Acomputational%20speed.%20However%2C%20while%20most%20methods%20excel%20at%20global%20alignment%2C%0Athey%20often%20perform%20worse%20in%20aligning%20local%20regions.%20To%20address%20this%20challenge%2C%0Athis%20paper%20proposes%20a%20mask-guided%20encoder-decoder%20DCNN-based%20image%20registration%0Amethod%2C%20named%20as%20MrRegNet.%20This%20approach%20employs%20a%20multi-resolution%20encoder%20for%0Afeature%20extraction%20and%20subsequently%20estimates%20multi-resolution%20displacement%0Afields%20in%20the%20decoder%20to%20handle%20the%20substantial%20deformation%20of%20images.%0AFurthermore%2C%20segmentation%20masks%20are%20employed%20to%20direct%20the%20model%27s%20attention%0Atoward%20aligning%20local%20regions.%20The%20results%20show%20that%20the%20proposed%20method%0Aoutperforms%20traditional%20methods%20like%20Demons%20and%20a%20well-known%20deep%20learning%0Amethod%2C%20VoxelMorph%2C%20on%20a%20public%203D%20brain%20MRI%20dataset%20%28OASIS%29%20and%20a%20local%202D%0Abrain%20MRI%20dataset%20with%20large%20deformations.%20Importantly%2C%20the%20image%20alignment%0Aaccuracies%20are%20significantly%20improved%20at%20local%20regions%20guided%20by%20segmentation%0Amasks.%20Github%20link%3Ahttps%3A//github.com/ruizhe-l/MrRegNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMrRegNet%253A%2520Multi-resolution%2520Mask%2520Guided%2520Convolutional%2520Neural%2520Network%2520for%250A%2520%2520Medical%2520Image%2520Registration%2520with%2520Large%2520Deformations%26entry.906535625%3DRuizhe%2520Li%2520and%2520Grazziela%2520Figueredo%2520and%2520Dorothee%2520Auer%2520and%2520Christian%2520Wagner%2520and%2520Xin%2520Chen%26entry.1292438233%3D%2520%2520Deformable%2520image%2520registration%2520%2528alignment%2529%2520is%2520highly%2520sought%2520after%2520in%2520numerous%250Aclinical%2520applications%252C%2520such%2520as%2520computer%2520aided%2520diagnosis%2520and%2520disease%2520progression%250Aanalysis.%2520Deep%2520Convolutional%2520Neural%2520Network%2520%2528DCNN%2529-based%2520image%2520registration%250Amethods%2520have%2520demonstrated%2520advantages%2520in%2520terms%2520of%2520registration%2520accuracy%2520and%250Acomputational%2520speed.%2520However%252C%2520while%2520most%2520methods%2520excel%2520at%2520global%2520alignment%252C%250Athey%2520often%2520perform%2520worse%2520in%2520aligning%2520local%2520regions.%2520To%2520address%2520this%2520challenge%252C%250Athis%2520paper%2520proposes%2520a%2520mask-guided%2520encoder-decoder%2520DCNN-based%2520image%2520registration%250Amethod%252C%2520named%2520as%2520MrRegNet.%2520This%2520approach%2520employs%2520a%2520multi-resolution%2520encoder%2520for%250Afeature%2520extraction%2520and%2520subsequently%2520estimates%2520multi-resolution%2520displacement%250Afields%2520in%2520the%2520decoder%2520to%2520handle%2520the%2520substantial%2520deformation%2520of%2520images.%250AFurthermore%252C%2520segmentation%2520masks%2520are%2520employed%2520to%2520direct%2520the%2520model%2527s%2520attention%250Atoward%2520aligning%2520local%2520regions.%2520The%2520results%2520show%2520that%2520the%2520proposed%2520method%250Aoutperforms%2520traditional%2520methods%2520like%2520Demons%2520and%2520a%2520well-known%2520deep%2520learning%250Amethod%252C%2520VoxelMorph%252C%2520on%2520a%2520public%25203D%2520brain%2520MRI%2520dataset%2520%2528OASIS%2529%2520and%2520a%2520local%25202D%250Abrain%2520MRI%2520dataset%2520with%2520large%2520deformations.%2520Importantly%252C%2520the%2520image%2520alignment%250Aaccuracies%2520are%2520significantly%2520improved%2520at%2520local%2520regions%2520guided%2520by%2520segmentation%250Amasks.%2520Github%2520link%253Ahttps%253A//github.com/ruizhe-l/MrRegNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MrRegNet%3A%20Multi-resolution%20Mask%20Guided%20Convolutional%20Neural%20Network%20for%0A%20%20Medical%20Image%20Registration%20with%20Large%20Deformations&entry.906535625=Ruizhe%20Li%20and%20Grazziela%20Figueredo%20and%20Dorothee%20Auer%20and%20Christian%20Wagner%20and%20Xin%20Chen&entry.1292438233=%20%20Deformable%20image%20registration%20%28alignment%29%20is%20highly%20sought%20after%20in%20numerous%0Aclinical%20applications%2C%20such%20as%20computer%20aided%20diagnosis%20and%20disease%20progression%0Aanalysis.%20Deep%20Convolutional%20Neural%20Network%20%28DCNN%29-based%20image%20registration%0Amethods%20have%20demonstrated%20advantages%20in%20terms%20of%20registration%20accuracy%20and%0Acomputational%20speed.%20However%2C%20while%20most%20methods%20excel%20at%20global%20alignment%2C%0Athey%20often%20perform%20worse%20in%20aligning%20local%20regions.%20To%20address%20this%20challenge%2C%0Athis%20paper%20proposes%20a%20mask-guided%20encoder-decoder%20DCNN-based%20image%20registration%0Amethod%2C%20named%20as%20MrRegNet.%20This%20approach%20employs%20a%20multi-resolution%20encoder%20for%0Afeature%20extraction%20and%20subsequently%20estimates%20multi-resolution%20displacement%0Afields%20in%20the%20decoder%20to%20handle%20the%20substantial%20deformation%20of%20images.%0AFurthermore%2C%20segmentation%20masks%20are%20employed%20to%20direct%20the%20model%27s%20attention%0Atoward%20aligning%20local%20regions.%20The%20results%20show%20that%20the%20proposed%20method%0Aoutperforms%20traditional%20methods%20like%20Demons%20and%20a%20well-known%20deep%20learning%0Amethod%2C%20VoxelMorph%2C%20on%20a%20public%203D%20brain%20MRI%20dataset%20%28OASIS%29%20and%20a%20local%202D%0Abrain%20MRI%20dataset%20with%20large%20deformations.%20Importantly%2C%20the%20image%20alignment%0Aaccuracies%20are%20significantly%20improved%20at%20local%20regions%20guided%20by%20segmentation%0Amasks.%20Github%20link%3Ahttps%3A//github.com/ruizhe-l/MrRegNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10068v1&entry.124074799=Read"},
{"title": "Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving\n  Platforms", "author": "Pascal Goldschmid and Aamir Ahmad", "abstract": "  Multi-rotor UAVs suffer from a restricted range and flight duration due to\nlimited battery capacity. Autonomous landing on a 2D moving platform offers the\npossibility to replenish batteries and offload data, thus increasing the\nutility of the vehicle. Classical approaches rely on accurate, complex and\ndifficult-to-derive models of the vehicle and the environment. Reinforcement\nlearning (RL) provides an attractive alternative due to its ability to learn a\nsuitable control policy exclusively from data during a training procedure.\nHowever, current methods require several hours to train, have limited success\nrates and depend on hyperparameters that need to be tuned by trial-and-error.\nWe address all these issues in this work. First, we decompose the landing\nprocedure into a sequence of simpler, but similar learning tasks. This is\nenabled by applying two instances of the same RL based controller trained for\n1D motion for controlling the multi-rotor's movement in both the longitudinal\nand the lateral directions. Second, we introduce a powerful state space\ndiscretization technique that is based on i) kinematic modeling of the moving\nplatform to derive information about the state space topology and ii)\nstructuring the training as a sequential curriculum using transfer learning.\nThird, we leverage the kinematics model of the moving platform to also derive\ninterpretable hyperparameters for the training process that ensure sufficient\nmaneuverability of the multi-rotor vehicle. The training is performed using the\ntabular RL method Double Q-Learning. Through extensive simulations we show that\nthe presented method significantly increases the rate of successful landings,\nwhile requiring less training time compared to other deep RL approaches.\nFinally, we deploy and demonstrate our algorithm on real hardware. For all\nevaluation scenarios we provide statistics on the agent's performance.\n", "link": "http://arxiv.org/abs/2302.13192v3", "date": "2024-05-16", "relevancy": 2.0842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5814}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms&body=Title%3A%20Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms%0AAuthor%3A%20Pascal%20Goldschmid%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Multi-rotor%20UAVs%20suffer%20from%20a%20restricted%20range%20and%20flight%20duration%20due%20to%0Alimited%20battery%20capacity.%20Autonomous%20landing%20on%20a%202D%20moving%20platform%20offers%20the%0Apossibility%20to%20replenish%20batteries%20and%20offload%20data%2C%20thus%20increasing%20the%0Autility%20of%20the%20vehicle.%20Classical%20approaches%20rely%20on%20accurate%2C%20complex%20and%0Adifficult-to-derive%20models%20of%20the%20vehicle%20and%20the%20environment.%20Reinforcement%0Alearning%20%28RL%29%20provides%20an%20attractive%20alternative%20due%20to%20its%20ability%20to%20learn%20a%0Asuitable%20control%20policy%20exclusively%20from%20data%20during%20a%20training%20procedure.%0AHowever%2C%20current%20methods%20require%20several%20hours%20to%20train%2C%20have%20limited%20success%0Arates%20and%20depend%20on%20hyperparameters%20that%20need%20to%20be%20tuned%20by%20trial-and-error.%0AWe%20address%20all%20these%20issues%20in%20this%20work.%20First%2C%20we%20decompose%20the%20landing%0Aprocedure%20into%20a%20sequence%20of%20simpler%2C%20but%20similar%20learning%20tasks.%20This%20is%0Aenabled%20by%20applying%20two%20instances%20of%20the%20same%20RL%20based%20controller%20trained%20for%0A1D%20motion%20for%20controlling%20the%20multi-rotor%27s%20movement%20in%20both%20the%20longitudinal%0Aand%20the%20lateral%20directions.%20Second%2C%20we%20introduce%20a%20powerful%20state%20space%0Adiscretization%20technique%20that%20is%20based%20on%20i%29%20kinematic%20modeling%20of%20the%20moving%0Aplatform%20to%20derive%20information%20about%20the%20state%20space%20topology%20and%20ii%29%0Astructuring%20the%20training%20as%20a%20sequential%20curriculum%20using%20transfer%20learning.%0AThird%2C%20we%20leverage%20the%20kinematics%20model%20of%20the%20moving%20platform%20to%20also%20derive%0Ainterpretable%20hyperparameters%20for%20the%20training%20process%20that%20ensure%20sufficient%0Amaneuverability%20of%20the%20multi-rotor%20vehicle.%20The%20training%20is%20performed%20using%20the%0Atabular%20RL%20method%20Double%20Q-Learning.%20Through%20extensive%20simulations%20we%20show%20that%0Athe%20presented%20method%20significantly%20increases%20the%20rate%20of%20successful%20landings%2C%0Awhile%20requiring%20less%20training%20time%20compared%20to%20other%20deep%20RL%20approaches.%0AFinally%2C%20we%20deploy%20and%20demonstrate%20our%20algorithm%20on%20real%20hardware.%20For%20all%0Aevaluation%20scenarios%20we%20provide%20statistics%20on%20the%20agent%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.13192v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520based%2520Autonomous%2520Multi-Rotor%2520Landing%2520on%2520Moving%250A%2520%2520Platforms%26entry.906535625%3DPascal%2520Goldschmid%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Multi-rotor%2520UAVs%2520suffer%2520from%2520a%2520restricted%2520range%2520and%2520flight%2520duration%2520due%2520to%250Alimited%2520battery%2520capacity.%2520Autonomous%2520landing%2520on%2520a%25202D%2520moving%2520platform%2520offers%2520the%250Apossibility%2520to%2520replenish%2520batteries%2520and%2520offload%2520data%252C%2520thus%2520increasing%2520the%250Autility%2520of%2520the%2520vehicle.%2520Classical%2520approaches%2520rely%2520on%2520accurate%252C%2520complex%2520and%250Adifficult-to-derive%2520models%2520of%2520the%2520vehicle%2520and%2520the%2520environment.%2520Reinforcement%250Alearning%2520%2528RL%2529%2520provides%2520an%2520attractive%2520alternative%2520due%2520to%2520its%2520ability%2520to%2520learn%2520a%250Asuitable%2520control%2520policy%2520exclusively%2520from%2520data%2520during%2520a%2520training%2520procedure.%250AHowever%252C%2520current%2520methods%2520require%2520several%2520hours%2520to%2520train%252C%2520have%2520limited%2520success%250Arates%2520and%2520depend%2520on%2520hyperparameters%2520that%2520need%2520to%2520be%2520tuned%2520by%2520trial-and-error.%250AWe%2520address%2520all%2520these%2520issues%2520in%2520this%2520work.%2520First%252C%2520we%2520decompose%2520the%2520landing%250Aprocedure%2520into%2520a%2520sequence%2520of%2520simpler%252C%2520but%2520similar%2520learning%2520tasks.%2520This%2520is%250Aenabled%2520by%2520applying%2520two%2520instances%2520of%2520the%2520same%2520RL%2520based%2520controller%2520trained%2520for%250A1D%2520motion%2520for%2520controlling%2520the%2520multi-rotor%2527s%2520movement%2520in%2520both%2520the%2520longitudinal%250Aand%2520the%2520lateral%2520directions.%2520Second%252C%2520we%2520introduce%2520a%2520powerful%2520state%2520space%250Adiscretization%2520technique%2520that%2520is%2520based%2520on%2520i%2529%2520kinematic%2520modeling%2520of%2520the%2520moving%250Aplatform%2520to%2520derive%2520information%2520about%2520the%2520state%2520space%2520topology%2520and%2520ii%2529%250Astructuring%2520the%2520training%2520as%2520a%2520sequential%2520curriculum%2520using%2520transfer%2520learning.%250AThird%252C%2520we%2520leverage%2520the%2520kinematics%2520model%2520of%2520the%2520moving%2520platform%2520to%2520also%2520derive%250Ainterpretable%2520hyperparameters%2520for%2520the%2520training%2520process%2520that%2520ensure%2520sufficient%250Amaneuverability%2520of%2520the%2520multi-rotor%2520vehicle.%2520The%2520training%2520is%2520performed%2520using%2520the%250Atabular%2520RL%2520method%2520Double%2520Q-Learning.%2520Through%2520extensive%2520simulations%2520we%2520show%2520that%250Athe%2520presented%2520method%2520significantly%2520increases%2520the%2520rate%2520of%2520successful%2520landings%252C%250Awhile%2520requiring%2520less%2520training%2520time%2520compared%2520to%2520other%2520deep%2520RL%2520approaches.%250AFinally%252C%2520we%2520deploy%2520and%2520demonstrate%2520our%2520algorithm%2520on%2520real%2520hardware.%2520For%2520all%250Aevaluation%2520scenarios%2520we%2520provide%2520statistics%2520on%2520the%2520agent%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.13192v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20based%20Autonomous%20Multi-Rotor%20Landing%20on%20Moving%0A%20%20Platforms&entry.906535625=Pascal%20Goldschmid%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Multi-rotor%20UAVs%20suffer%20from%20a%20restricted%20range%20and%20flight%20duration%20due%20to%0Alimited%20battery%20capacity.%20Autonomous%20landing%20on%20a%202D%20moving%20platform%20offers%20the%0Apossibility%20to%20replenish%20batteries%20and%20offload%20data%2C%20thus%20increasing%20the%0Autility%20of%20the%20vehicle.%20Classical%20approaches%20rely%20on%20accurate%2C%20complex%20and%0Adifficult-to-derive%20models%20of%20the%20vehicle%20and%20the%20environment.%20Reinforcement%0Alearning%20%28RL%29%20provides%20an%20attractive%20alternative%20due%20to%20its%20ability%20to%20learn%20a%0Asuitable%20control%20policy%20exclusively%20from%20data%20during%20a%20training%20procedure.%0AHowever%2C%20current%20methods%20require%20several%20hours%20to%20train%2C%20have%20limited%20success%0Arates%20and%20depend%20on%20hyperparameters%20that%20need%20to%20be%20tuned%20by%20trial-and-error.%0AWe%20address%20all%20these%20issues%20in%20this%20work.%20First%2C%20we%20decompose%20the%20landing%0Aprocedure%20into%20a%20sequence%20of%20simpler%2C%20but%20similar%20learning%20tasks.%20This%20is%0Aenabled%20by%20applying%20two%20instances%20of%20the%20same%20RL%20based%20controller%20trained%20for%0A1D%20motion%20for%20controlling%20the%20multi-rotor%27s%20movement%20in%20both%20the%20longitudinal%0Aand%20the%20lateral%20directions.%20Second%2C%20we%20introduce%20a%20powerful%20state%20space%0Adiscretization%20technique%20that%20is%20based%20on%20i%29%20kinematic%20modeling%20of%20the%20moving%0Aplatform%20to%20derive%20information%20about%20the%20state%20space%20topology%20and%20ii%29%0Astructuring%20the%20training%20as%20a%20sequential%20curriculum%20using%20transfer%20learning.%0AThird%2C%20we%20leverage%20the%20kinematics%20model%20of%20the%20moving%20platform%20to%20also%20derive%0Ainterpretable%20hyperparameters%20for%20the%20training%20process%20that%20ensure%20sufficient%0Amaneuverability%20of%20the%20multi-rotor%20vehicle.%20The%20training%20is%20performed%20using%20the%0Atabular%20RL%20method%20Double%20Q-Learning.%20Through%20extensive%20simulations%20we%20show%20that%0Athe%20presented%20method%20significantly%20increases%20the%20rate%20of%20successful%20landings%2C%0Awhile%20requiring%20less%20training%20time%20compared%20to%20other%20deep%20RL%20approaches.%0AFinally%2C%20we%20deploy%20and%20demonstrate%20our%20algorithm%20on%20real%20hardware.%20For%20all%0Aevaluation%20scenarios%20we%20provide%20statistics%20on%20the%20agent%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.13192v3&entry.124074799=Read"},
{"title": "Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective", "author": "Sunwoong Yang and Hojin Kim and Yoonpyo Hong and Kwanjung Yee and Romit Maulik and Namwoo Kang", "abstract": "  This study explores the potential of physics-informed neural networks (PINNs)\nfor the realization of digital twins (DT) from various perspectives. First,\nvarious adaptive sampling approaches for collocation points are investigated to\nverify their effectiveness in the mesh-free framework of PINNs, which allows\nautomated construction of virtual representation without manual mesh\ngeneration. Then, the overall performance of the data-driven PINNs (DD-PINNs)\nframework is examined, which can utilize the acquired datasets in DT scenarios.\nIts scalability to more general physics is validated within parametric\nNavier-Stokes equations, where PINNs do not need to be retrained as the\nReynolds number varies. In addition, since datasets can be often collected from\ndifferent fidelity/sparsity in practice, multi-fidelity DD-PINNs are also\nproposed and evaluated. They show remarkable prediction performance even in the\nextrapolation tasks, with $42\\sim62\\%$ improvement over the single-fidelity\napproach. Finally, the uncertainty quantification performance of multi-fidelity\nDD-PINNs is investigated by the ensemble method to verify their potential in\nDT, where an accurate measure of predictive uncertainty is critical. The\nDD-PINN frameworks explored in this study are found to be more suitable for DT\nscenarios than traditional PINNs from the above perspectives, bringing\nengineers one step closer to seamless DT realization.\n", "link": "http://arxiv.org/abs/2401.08667v2", "date": "2024-05-16", "relevancy": 2.073, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5211}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5192}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Physics-Informed%20Neural%20Networks%3A%20A%20Digital%20Twin%20Perspective&body=Title%3A%20Data-Driven%20Physics-Informed%20Neural%20Networks%3A%20A%20Digital%20Twin%20Perspective%0AAuthor%3A%20Sunwoong%20Yang%20and%20Hojin%20Kim%20and%20Yoonpyo%20Hong%20and%20Kwanjung%20Yee%20and%20Romit%20Maulik%20and%20Namwoo%20Kang%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20of%20physics-informed%20neural%20networks%20%28PINNs%29%0Afor%20the%20realization%20of%20digital%20twins%20%28DT%29%20from%20various%20perspectives.%20First%2C%0Avarious%20adaptive%20sampling%20approaches%20for%20collocation%20points%20are%20investigated%20to%0Averify%20their%20effectiveness%20in%20the%20mesh-free%20framework%20of%20PINNs%2C%20which%20allows%0Aautomated%20construction%20of%20virtual%20representation%20without%20manual%20mesh%0Ageneration.%20Then%2C%20the%20overall%20performance%20of%20the%20data-driven%20PINNs%20%28DD-PINNs%29%0Aframework%20is%20examined%2C%20which%20can%20utilize%20the%20acquired%20datasets%20in%20DT%20scenarios.%0AIts%20scalability%20to%20more%20general%20physics%20is%20validated%20within%20parametric%0ANavier-Stokes%20equations%2C%20where%20PINNs%20do%20not%20need%20to%20be%20retrained%20as%20the%0AReynolds%20number%20varies.%20In%20addition%2C%20since%20datasets%20can%20be%20often%20collected%20from%0Adifferent%20fidelity/sparsity%20in%20practice%2C%20multi-fidelity%20DD-PINNs%20are%20also%0Aproposed%20and%20evaluated.%20They%20show%20remarkable%20prediction%20performance%20even%20in%20the%0Aextrapolation%20tasks%2C%20with%20%2442%5Csim62%5C%25%24%20improvement%20over%20the%20single-fidelity%0Aapproach.%20Finally%2C%20the%20uncertainty%20quantification%20performance%20of%20multi-fidelity%0ADD-PINNs%20is%20investigated%20by%20the%20ensemble%20method%20to%20verify%20their%20potential%20in%0ADT%2C%20where%20an%20accurate%20measure%20of%20predictive%20uncertainty%20is%20critical.%20The%0ADD-PINN%20frameworks%20explored%20in%20this%20study%20are%20found%20to%20be%20more%20suitable%20for%20DT%0Ascenarios%20than%20traditional%20PINNs%20from%20the%20above%20perspectives%2C%20bringing%0Aengineers%20one%20step%20closer%20to%20seamless%20DT%20realization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Physics-Informed%2520Neural%2520Networks%253A%2520A%2520Digital%2520Twin%2520Perspective%26entry.906535625%3DSunwoong%2520Yang%2520and%2520Hojin%2520Kim%2520and%2520Yoonpyo%2520Hong%2520and%2520Kwanjung%2520Yee%2520and%2520Romit%2520Maulik%2520and%2520Namwoo%2520Kang%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520of%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%250Afor%2520the%2520realization%2520of%2520digital%2520twins%2520%2528DT%2529%2520from%2520various%2520perspectives.%2520First%252C%250Avarious%2520adaptive%2520sampling%2520approaches%2520for%2520collocation%2520points%2520are%2520investigated%2520to%250Averify%2520their%2520effectiveness%2520in%2520the%2520mesh-free%2520framework%2520of%2520PINNs%252C%2520which%2520allows%250Aautomated%2520construction%2520of%2520virtual%2520representation%2520without%2520manual%2520mesh%250Ageneration.%2520Then%252C%2520the%2520overall%2520performance%2520of%2520the%2520data-driven%2520PINNs%2520%2528DD-PINNs%2529%250Aframework%2520is%2520examined%252C%2520which%2520can%2520utilize%2520the%2520acquired%2520datasets%2520in%2520DT%2520scenarios.%250AIts%2520scalability%2520to%2520more%2520general%2520physics%2520is%2520validated%2520within%2520parametric%250ANavier-Stokes%2520equations%252C%2520where%2520PINNs%2520do%2520not%2520need%2520to%2520be%2520retrained%2520as%2520the%250AReynolds%2520number%2520varies.%2520In%2520addition%252C%2520since%2520datasets%2520can%2520be%2520often%2520collected%2520from%250Adifferent%2520fidelity/sparsity%2520in%2520practice%252C%2520multi-fidelity%2520DD-PINNs%2520are%2520also%250Aproposed%2520and%2520evaluated.%2520They%2520show%2520remarkable%2520prediction%2520performance%2520even%2520in%2520the%250Aextrapolation%2520tasks%252C%2520with%2520%252442%255Csim62%255C%2525%2524%2520improvement%2520over%2520the%2520single-fidelity%250Aapproach.%2520Finally%252C%2520the%2520uncertainty%2520quantification%2520performance%2520of%2520multi-fidelity%250ADD-PINNs%2520is%2520investigated%2520by%2520the%2520ensemble%2520method%2520to%2520verify%2520their%2520potential%2520in%250ADT%252C%2520where%2520an%2520accurate%2520measure%2520of%2520predictive%2520uncertainty%2520is%2520critical.%2520The%250ADD-PINN%2520frameworks%2520explored%2520in%2520this%2520study%2520are%2520found%2520to%2520be%2520more%2520suitable%2520for%2520DT%250Ascenarios%2520than%2520traditional%2520PINNs%2520from%2520the%2520above%2520perspectives%252C%2520bringing%250Aengineers%2520one%2520step%2520closer%2520to%2520seamless%2520DT%2520realization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Physics-Informed%20Neural%20Networks%3A%20A%20Digital%20Twin%20Perspective&entry.906535625=Sunwoong%20Yang%20and%20Hojin%20Kim%20and%20Yoonpyo%20Hong%20and%20Kwanjung%20Yee%20and%20Romit%20Maulik%20and%20Namwoo%20Kang&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20of%20physics-informed%20neural%20networks%20%28PINNs%29%0Afor%20the%20realization%20of%20digital%20twins%20%28DT%29%20from%20various%20perspectives.%20First%2C%0Avarious%20adaptive%20sampling%20approaches%20for%20collocation%20points%20are%20investigated%20to%0Averify%20their%20effectiveness%20in%20the%20mesh-free%20framework%20of%20PINNs%2C%20which%20allows%0Aautomated%20construction%20of%20virtual%20representation%20without%20manual%20mesh%0Ageneration.%20Then%2C%20the%20overall%20performance%20of%20the%20data-driven%20PINNs%20%28DD-PINNs%29%0Aframework%20is%20examined%2C%20which%20can%20utilize%20the%20acquired%20datasets%20in%20DT%20scenarios.%0AIts%20scalability%20to%20more%20general%20physics%20is%20validated%20within%20parametric%0ANavier-Stokes%20equations%2C%20where%20PINNs%20do%20not%20need%20to%20be%20retrained%20as%20the%0AReynolds%20number%20varies.%20In%20addition%2C%20since%20datasets%20can%20be%20often%20collected%20from%0Adifferent%20fidelity/sparsity%20in%20practice%2C%20multi-fidelity%20DD-PINNs%20are%20also%0Aproposed%20and%20evaluated.%20They%20show%20remarkable%20prediction%20performance%20even%20in%20the%0Aextrapolation%20tasks%2C%20with%20%2442%5Csim62%5C%25%24%20improvement%20over%20the%20single-fidelity%0Aapproach.%20Finally%2C%20the%20uncertainty%20quantification%20performance%20of%20multi-fidelity%0ADD-PINNs%20is%20investigated%20by%20the%20ensemble%20method%20to%20verify%20their%20potential%20in%0ADT%2C%20where%20an%20accurate%20measure%20of%20predictive%20uncertainty%20is%20critical.%20The%0ADD-PINN%20frameworks%20explored%20in%20this%20study%20are%20found%20to%20be%20more%20suitable%20for%20DT%0Ascenarios%20than%20traditional%20PINNs%20from%20the%20above%20perspectives%2C%20bringing%0Aengineers%20one%20step%20closer%20to%20seamless%20DT%20realization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08667v2&entry.124074799=Read"},
{"title": "Global-Local Image Perceptual Score (GLIPS): Evaluating Photorealistic\n  Quality of AI-Generated Images", "author": "Memoona Aziz and Umair Rehman and Muhammad Umair Danish and Katarina Grolinger", "abstract": "  This paper introduces the Global-Local Image Perceptual Score (GLIPS), an\nimage metric designed to assess the photorealistic image quality of\nAI-generated images with a high degree of alignment to human visual perception.\nTraditional metrics such as FID and KID scores do not align closely with human\nevaluations. The proposed metric incorporates advanced transformer-based\nattention mechanisms to assess local similarity and Maximum Mean Discrepancy\n(MMD) to evaluate global distributional similarity. To evaluate the performance\nof GLIPS, we conducted a human study on photorealistic image quality.\nComprehensive tests across various generative models demonstrate that GLIPS\nconsistently outperforms existing metrics like FID, SSIM, and MS-SSIM in terms\nof correlation with human scores. Additionally, we introduce the Interpolative\nBinning Scale (IBS), a refined scaling method that enhances the\ninterpretability of metric scores by aligning them more closely with human\nevaluative standards. The proposed metric and scaling approach not only\nprovides more reliable assessments of AI-generated images but also suggest\npathways for future enhancements in image generation technologies.\n", "link": "http://arxiv.org/abs/2405.09426v2", "date": "2024-05-16", "relevancy": 2.0693, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images&body=Title%3A%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images%0AAuthor%3A%20Memoona%20Aziz%20and%20Umair%20Rehman%20and%20Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%2C%20an%0Aimage%20metric%20designed%20to%20assess%20the%20photorealistic%20image%20quality%20of%0AAI-generated%20images%20with%20a%20high%20degree%20of%20alignment%20to%20human%20visual%20perception.%0ATraditional%20metrics%20such%20as%20FID%20and%20KID%20scores%20do%20not%20align%20closely%20with%20human%0Aevaluations.%20The%20proposed%20metric%20incorporates%20advanced%20transformer-based%0Aattention%20mechanisms%20to%20assess%20local%20similarity%20and%20Maximum%20Mean%20Discrepancy%0A%28MMD%29%20to%20evaluate%20global%20distributional%20similarity.%20To%20evaluate%20the%20performance%0Aof%20GLIPS%2C%20we%20conducted%20a%20human%20study%20on%20photorealistic%20image%20quality.%0AComprehensive%20tests%20across%20various%20generative%20models%20demonstrate%20that%20GLIPS%0Aconsistently%20outperforms%20existing%20metrics%20like%20FID%2C%20SSIM%2C%20and%20MS-SSIM%20in%20terms%0Aof%20correlation%20with%20human%20scores.%20Additionally%2C%20we%20introduce%20the%20Interpolative%0ABinning%20Scale%20%28IBS%29%2C%20a%20refined%20scaling%20method%20that%20enhances%20the%0Ainterpretability%20of%20metric%20scores%20by%20aligning%20them%20more%20closely%20with%20human%0Aevaluative%20standards.%20The%20proposed%20metric%20and%20scaling%20approach%20not%20only%0Aprovides%20more%20reliable%20assessments%20of%20AI-generated%20images%20but%20also%20suggest%0Apathways%20for%20future%20enhancements%20in%20image%20generation%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-Local%2520Image%2520Perceptual%2520Score%2520%2528GLIPS%2529%253A%2520Evaluating%2520Photorealistic%250A%2520%2520Quality%2520of%2520AI-Generated%2520Images%26entry.906535625%3DMemoona%2520Aziz%2520and%2520Umair%2520Rehman%2520and%2520Muhammad%2520Umair%2520Danish%2520and%2520Katarina%2520Grolinger%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Global-Local%2520Image%2520Perceptual%2520Score%2520%2528GLIPS%2529%252C%2520an%250Aimage%2520metric%2520designed%2520to%2520assess%2520the%2520photorealistic%2520image%2520quality%2520of%250AAI-generated%2520images%2520with%2520a%2520high%2520degree%2520of%2520alignment%2520to%2520human%2520visual%2520perception.%250ATraditional%2520metrics%2520such%2520as%2520FID%2520and%2520KID%2520scores%2520do%2520not%2520align%2520closely%2520with%2520human%250Aevaluations.%2520The%2520proposed%2520metric%2520incorporates%2520advanced%2520transformer-based%250Aattention%2520mechanisms%2520to%2520assess%2520local%2520similarity%2520and%2520Maximum%2520Mean%2520Discrepancy%250A%2528MMD%2529%2520to%2520evaluate%2520global%2520distributional%2520similarity.%2520To%2520evaluate%2520the%2520performance%250Aof%2520GLIPS%252C%2520we%2520conducted%2520a%2520human%2520study%2520on%2520photorealistic%2520image%2520quality.%250AComprehensive%2520tests%2520across%2520various%2520generative%2520models%2520demonstrate%2520that%2520GLIPS%250Aconsistently%2520outperforms%2520existing%2520metrics%2520like%2520FID%252C%2520SSIM%252C%2520and%2520MS-SSIM%2520in%2520terms%250Aof%2520correlation%2520with%2520human%2520scores.%2520Additionally%252C%2520we%2520introduce%2520the%2520Interpolative%250ABinning%2520Scale%2520%2528IBS%2529%252C%2520a%2520refined%2520scaling%2520method%2520that%2520enhances%2520the%250Ainterpretability%2520of%2520metric%2520scores%2520by%2520aligning%2520them%2520more%2520closely%2520with%2520human%250Aevaluative%2520standards.%2520The%2520proposed%2520metric%2520and%2520scaling%2520approach%2520not%2520only%250Aprovides%2520more%2520reliable%2520assessments%2520of%2520AI-generated%2520images%2520but%2520also%2520suggest%250Apathways%2520for%2520future%2520enhancements%2520in%2520image%2520generation%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images&entry.906535625=Memoona%20Aziz%20and%20Umair%20Rehman%20and%20Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger&entry.1292438233=%20%20This%20paper%20introduces%20the%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%2C%20an%0Aimage%20metric%20designed%20to%20assess%20the%20photorealistic%20image%20quality%20of%0AAI-generated%20images%20with%20a%20high%20degree%20of%20alignment%20to%20human%20visual%20perception.%0ATraditional%20metrics%20such%20as%20FID%20and%20KID%20scores%20do%20not%20align%20closely%20with%20human%0Aevaluations.%20The%20proposed%20metric%20incorporates%20advanced%20transformer-based%0Aattention%20mechanisms%20to%20assess%20local%20similarity%20and%20Maximum%20Mean%20Discrepancy%0A%28MMD%29%20to%20evaluate%20global%20distributional%20similarity.%20To%20evaluate%20the%20performance%0Aof%20GLIPS%2C%20we%20conducted%20a%20human%20study%20on%20photorealistic%20image%20quality.%0AComprehensive%20tests%20across%20various%20generative%20models%20demonstrate%20that%20GLIPS%0Aconsistently%20outperforms%20existing%20metrics%20like%20FID%2C%20SSIM%2C%20and%20MS-SSIM%20in%20terms%0Aof%20correlation%20with%20human%20scores.%20Additionally%2C%20we%20introduce%20the%20Interpolative%0ABinning%20Scale%20%28IBS%29%2C%20a%20refined%20scaling%20method%20that%20enhances%20the%0Ainterpretability%20of%20metric%20scores%20by%20aligning%20them%20more%20closely%20with%20human%0Aevaluative%20standards.%20The%20proposed%20metric%20and%20scaling%20approach%20not%20only%0Aprovides%20more%20reliable%20assessments%20of%20AI-generated%20images%20but%20also%20suggest%0Apathways%20for%20future%20enhancements%20in%20image%20generation%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09426v2&entry.124074799=Read"},
{"title": "Solving the enigma: Deriving optimal explanations of deep networks", "author": "Michail Mamalakis and Antonios Mamalakis and Ingrid Agartz and Lynn Egeland M\u00f8rch-Johnsen and Graham Murray and John Suckling and Pietro Lio", "abstract": "  The accelerated progress of artificial intelligence (AI) has popularized deep\nlearning models across domains, yet their inherent opacity poses challenges,\nnotably in critical fields like healthcare, medicine and the geosciences.\nExplainable AI (XAI) has emerged to shed light on these \"black box\" models,\nhelping decipher their decision making process. Nevertheless, different XAI\nmethods yield highly different explanations. This inter-method variability\nincreases uncertainty and lowers trust in deep networks' predictions. In this\nstudy, for the first time, we propose a novel framework designed to enhance the\nexplainability of deep networks, by maximizing both the accuracy and the\ncomprehensibility of the explanations. Our framework integrates various\nexplanations from established XAI methods and employs a non-linear \"explanation\noptimizer\" to construct a unique and optimal explanation. Through experiments\non multi-class and binary classification tasks in 2D object and 3D neuroscience\nimaging, we validate the efficacy of our approach. Our explanation optimizer\nachieved superior faithfulness scores, averaging 155% and 63% higher than the\nbest performing XAI method in the 3D and 2D applications, respectively.\nAdditionally, our approach yielded lower complexity, increasing\ncomprehensibility. Our results suggest that optimal explanations based on\nspecific criteria are derivable and address the issue of inter-method\nvariability in the current XAI literature.\n", "link": "http://arxiv.org/abs/2405.10008v1", "date": "2024-05-16", "relevancy": 2.0461, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5277}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20enigma%3A%20Deriving%20optimal%20explanations%20of%20deep%20networks&body=Title%3A%20Solving%20the%20enigma%3A%20Deriving%20optimal%20explanations%20of%20deep%20networks%0AAuthor%3A%20Michail%20Mamalakis%20and%20Antonios%20Mamalakis%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Graham%20Murray%20and%20John%20Suckling%20and%20Pietro%20Lio%0AAbstract%3A%20%20%20The%20accelerated%20progress%20of%20artificial%20intelligence%20%28AI%29%20has%20popularized%20deep%0Alearning%20models%20across%20domains%2C%20yet%20their%20inherent%20opacity%20poses%20challenges%2C%0Anotably%20in%20critical%20fields%20like%20healthcare%2C%20medicine%20and%20the%20geosciences.%0AExplainable%20AI%20%28XAI%29%20has%20emerged%20to%20shed%20light%20on%20these%20%22black%20box%22%20models%2C%0Ahelping%20decipher%20their%20decision%20making%20process.%20Nevertheless%2C%20different%20XAI%0Amethods%20yield%20highly%20different%20explanations.%20This%20inter-method%20variability%0Aincreases%20uncertainty%20and%20lowers%20trust%20in%20deep%20networks%27%20predictions.%20In%20this%0Astudy%2C%20for%20the%20first%20time%2C%20we%20propose%20a%20novel%20framework%20designed%20to%20enhance%20the%0Aexplainability%20of%20deep%20networks%2C%20by%20maximizing%20both%20the%20accuracy%20and%20the%0Acomprehensibility%20of%20the%20explanations.%20Our%20framework%20integrates%20various%0Aexplanations%20from%20established%20XAI%20methods%20and%20employs%20a%20non-linear%20%22explanation%0Aoptimizer%22%20to%20construct%20a%20unique%20and%20optimal%20explanation.%20Through%20experiments%0Aon%20multi-class%20and%20binary%20classification%20tasks%20in%202D%20object%20and%203D%20neuroscience%0Aimaging%2C%20we%20validate%20the%20efficacy%20of%20our%20approach.%20Our%20explanation%20optimizer%0Aachieved%20superior%20faithfulness%20scores%2C%20averaging%20155%25%20and%2063%25%20higher%20than%20the%0Abest%20performing%20XAI%20method%20in%20the%203D%20and%202D%20applications%2C%20respectively.%0AAdditionally%2C%20our%20approach%20yielded%20lower%20complexity%2C%20increasing%0Acomprehensibility.%20Our%20results%20suggest%20that%20optimal%20explanations%20based%20on%0Aspecific%20criteria%20are%20derivable%20and%20address%20the%20issue%20of%20inter-method%0Avariability%20in%20the%20current%20XAI%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520enigma%253A%2520Deriving%2520optimal%2520explanations%2520of%2520deep%2520networks%26entry.906535625%3DMichail%2520Mamalakis%2520and%2520Antonios%2520Mamalakis%2520and%2520Ingrid%2520Agartz%2520and%2520Lynn%2520Egeland%2520M%25C3%25B8rch-Johnsen%2520and%2520Graham%2520Murray%2520and%2520John%2520Suckling%2520and%2520Pietro%2520Lio%26entry.1292438233%3D%2520%2520The%2520accelerated%2520progress%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520popularized%2520deep%250Alearning%2520models%2520across%2520domains%252C%2520yet%2520their%2520inherent%2520opacity%2520poses%2520challenges%252C%250Anotably%2520in%2520critical%2520fields%2520like%2520healthcare%252C%2520medicine%2520and%2520the%2520geosciences.%250AExplainable%2520AI%2520%2528XAI%2529%2520has%2520emerged%2520to%2520shed%2520light%2520on%2520these%2520%2522black%2520box%2522%2520models%252C%250Ahelping%2520decipher%2520their%2520decision%2520making%2520process.%2520Nevertheless%252C%2520different%2520XAI%250Amethods%2520yield%2520highly%2520different%2520explanations.%2520This%2520inter-method%2520variability%250Aincreases%2520uncertainty%2520and%2520lowers%2520trust%2520in%2520deep%2520networks%2527%2520predictions.%2520In%2520this%250Astudy%252C%2520for%2520the%2520first%2520time%252C%2520we%2520propose%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520the%250Aexplainability%2520of%2520deep%2520networks%252C%2520by%2520maximizing%2520both%2520the%2520accuracy%2520and%2520the%250Acomprehensibility%2520of%2520the%2520explanations.%2520Our%2520framework%2520integrates%2520various%250Aexplanations%2520from%2520established%2520XAI%2520methods%2520and%2520employs%2520a%2520non-linear%2520%2522explanation%250Aoptimizer%2522%2520to%2520construct%2520a%2520unique%2520and%2520optimal%2520explanation.%2520Through%2520experiments%250Aon%2520multi-class%2520and%2520binary%2520classification%2520tasks%2520in%25202D%2520object%2520and%25203D%2520neuroscience%250Aimaging%252C%2520we%2520validate%2520the%2520efficacy%2520of%2520our%2520approach.%2520Our%2520explanation%2520optimizer%250Aachieved%2520superior%2520faithfulness%2520scores%252C%2520averaging%2520155%2525%2520and%252063%2525%2520higher%2520than%2520the%250Abest%2520performing%2520XAI%2520method%2520in%2520the%25203D%2520and%25202D%2520applications%252C%2520respectively.%250AAdditionally%252C%2520our%2520approach%2520yielded%2520lower%2520complexity%252C%2520increasing%250Acomprehensibility.%2520Our%2520results%2520suggest%2520that%2520optimal%2520explanations%2520based%2520on%250Aspecific%2520criteria%2520are%2520derivable%2520and%2520address%2520the%2520issue%2520of%2520inter-method%250Avariability%2520in%2520the%2520current%2520XAI%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20enigma%3A%20Deriving%20optimal%20explanations%20of%20deep%20networks&entry.906535625=Michail%20Mamalakis%20and%20Antonios%20Mamalakis%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Graham%20Murray%20and%20John%20Suckling%20and%20Pietro%20Lio&entry.1292438233=%20%20The%20accelerated%20progress%20of%20artificial%20intelligence%20%28AI%29%20has%20popularized%20deep%0Alearning%20models%20across%20domains%2C%20yet%20their%20inherent%20opacity%20poses%20challenges%2C%0Anotably%20in%20critical%20fields%20like%20healthcare%2C%20medicine%20and%20the%20geosciences.%0AExplainable%20AI%20%28XAI%29%20has%20emerged%20to%20shed%20light%20on%20these%20%22black%20box%22%20models%2C%0Ahelping%20decipher%20their%20decision%20making%20process.%20Nevertheless%2C%20different%20XAI%0Amethods%20yield%20highly%20different%20explanations.%20This%20inter-method%20variability%0Aincreases%20uncertainty%20and%20lowers%20trust%20in%20deep%20networks%27%20predictions.%20In%20this%0Astudy%2C%20for%20the%20first%20time%2C%20we%20propose%20a%20novel%20framework%20designed%20to%20enhance%20the%0Aexplainability%20of%20deep%20networks%2C%20by%20maximizing%20both%20the%20accuracy%20and%20the%0Acomprehensibility%20of%20the%20explanations.%20Our%20framework%20integrates%20various%0Aexplanations%20from%20established%20XAI%20methods%20and%20employs%20a%20non-linear%20%22explanation%0Aoptimizer%22%20to%20construct%20a%20unique%20and%20optimal%20explanation.%20Through%20experiments%0Aon%20multi-class%20and%20binary%20classification%20tasks%20in%202D%20object%20and%203D%20neuroscience%0Aimaging%2C%20we%20validate%20the%20efficacy%20of%20our%20approach.%20Our%20explanation%20optimizer%0Aachieved%20superior%20faithfulness%20scores%2C%20averaging%20155%25%20and%2063%25%20higher%20than%20the%0Abest%20performing%20XAI%20method%20in%20the%203D%20and%202D%20applications%2C%20respectively.%0AAdditionally%2C%20our%20approach%20yielded%20lower%20complexity%2C%20increasing%0Acomprehensibility.%20Our%20results%20suggest%20that%20optimal%20explanations%20based%20on%0Aspecific%20criteria%20are%20derivable%20and%20address%20the%20issue%20of%20inter-method%0Avariability%20in%20the%20current%20XAI%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10008v1&entry.124074799=Read"},
{"title": "Autonomous Drone Racing: A Survey", "author": "Drew Hanover and Antonio Loquercio and Leonard Bauersfeld and Angel Romero and Robert Penicka and Yunlong Song and Giovanni Cioffi and Elia Kaufmann and Davide Scaramuzza", "abstract": "  Over the last decade, the use of autonomous drone systems for surveying,\nsearch and rescue, or last-mile delivery has increased exponentially. With the\nrise of these applications comes the need for highly robust, safety-critical\nalgorithms which can operate drones in complex and uncertain environments.\nAdditionally, flying fast enables drones to cover more ground which in turn\nincreases productivity and further strengthens their use case. One proxy for\ndeveloping algorithms used in high-speed navigation is the task of autonomous\ndrone racing, where researchers program drones to fly through a sequence of\ngates and avoid obstacles as quickly as possible using onboard sensors and\nlimited computational power. Speeds and accelerations exceed over 80 kph and 4\ng respectively, raising significant challenges across perception, planning,\ncontrol, and state estimation. To achieve maximum performance, systems require\nreal-time algorithms that are robust to motion blur, high dynamic range, model\nuncertainties, aerodynamic disturbances, and often unpredictable opponents.\nThis survey covers the progression of autonomous drone racing across\nmodel-based and learning-based approaches. We provide an overview of the field,\nits evolution over the years, and conclude with the biggest challenges and open\nquestions to be faced in the future.\n", "link": "http://arxiv.org/abs/2301.01755v3", "date": "2024-05-16", "relevancy": 2.0458, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5234}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Drone%20Racing%3A%20A%20Survey&body=Title%3A%20Autonomous%20Drone%20Racing%3A%20A%20Survey%0AAuthor%3A%20Drew%20Hanover%20and%20Antonio%20Loquercio%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Robert%20Penicka%20and%20Yunlong%20Song%20and%20Giovanni%20Cioffi%20and%20Elia%20Kaufmann%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Over%20the%20last%20decade%2C%20the%20use%20of%20autonomous%20drone%20systems%20for%20surveying%2C%0Asearch%20and%20rescue%2C%20or%20last-mile%20delivery%20has%20increased%20exponentially.%20With%20the%0Arise%20of%20these%20applications%20comes%20the%20need%20for%20highly%20robust%2C%20safety-critical%0Aalgorithms%20which%20can%20operate%20drones%20in%20complex%20and%20uncertain%20environments.%0AAdditionally%2C%20flying%20fast%20enables%20drones%20to%20cover%20more%20ground%20which%20in%20turn%0Aincreases%20productivity%20and%20further%20strengthens%20their%20use%20case.%20One%20proxy%20for%0Adeveloping%20algorithms%20used%20in%20high-speed%20navigation%20is%20the%20task%20of%20autonomous%0Adrone%20racing%2C%20where%20researchers%20program%20drones%20to%20fly%20through%20a%20sequence%20of%0Agates%20and%20avoid%20obstacles%20as%20quickly%20as%20possible%20using%20onboard%20sensors%20and%0Alimited%20computational%20power.%20Speeds%20and%20accelerations%20exceed%20over%2080%20kph%20and%204%0Ag%20respectively%2C%20raising%20significant%20challenges%20across%20perception%2C%20planning%2C%0Acontrol%2C%20and%20state%20estimation.%20To%20achieve%20maximum%20performance%2C%20systems%20require%0Areal-time%20algorithms%20that%20are%20robust%20to%20motion%20blur%2C%20high%20dynamic%20range%2C%20model%0Auncertainties%2C%20aerodynamic%20disturbances%2C%20and%20often%20unpredictable%20opponents.%0AThis%20survey%20covers%20the%20progression%20of%20autonomous%20drone%20racing%20across%0Amodel-based%20and%20learning-based%20approaches.%20We%20provide%20an%20overview%20of%20the%20field%2C%0Aits%20evolution%20over%20the%20years%2C%20and%20conclude%20with%20the%20biggest%20challenges%20and%20open%0Aquestions%20to%20be%20faced%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Drone%2520Racing%253A%2520A%2520Survey%26entry.906535625%3DDrew%2520Hanover%2520and%2520Antonio%2520Loquercio%2520and%2520Leonard%2520Bauersfeld%2520and%2520Angel%2520Romero%2520and%2520Robert%2520Penicka%2520and%2520Yunlong%2520Song%2520and%2520Giovanni%2520Cioffi%2520and%2520Elia%2520Kaufmann%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Over%2520the%2520last%2520decade%252C%2520the%2520use%2520of%2520autonomous%2520drone%2520systems%2520for%2520surveying%252C%250Asearch%2520and%2520rescue%252C%2520or%2520last-mile%2520delivery%2520has%2520increased%2520exponentially.%2520With%2520the%250Arise%2520of%2520these%2520applications%2520comes%2520the%2520need%2520for%2520highly%2520robust%252C%2520safety-critical%250Aalgorithms%2520which%2520can%2520operate%2520drones%2520in%2520complex%2520and%2520uncertain%2520environments.%250AAdditionally%252C%2520flying%2520fast%2520enables%2520drones%2520to%2520cover%2520more%2520ground%2520which%2520in%2520turn%250Aincreases%2520productivity%2520and%2520further%2520strengthens%2520their%2520use%2520case.%2520One%2520proxy%2520for%250Adeveloping%2520algorithms%2520used%2520in%2520high-speed%2520navigation%2520is%2520the%2520task%2520of%2520autonomous%250Adrone%2520racing%252C%2520where%2520researchers%2520program%2520drones%2520to%2520fly%2520through%2520a%2520sequence%2520of%250Agates%2520and%2520avoid%2520obstacles%2520as%2520quickly%2520as%2520possible%2520using%2520onboard%2520sensors%2520and%250Alimited%2520computational%2520power.%2520Speeds%2520and%2520accelerations%2520exceed%2520over%252080%2520kph%2520and%25204%250Ag%2520respectively%252C%2520raising%2520significant%2520challenges%2520across%2520perception%252C%2520planning%252C%250Acontrol%252C%2520and%2520state%2520estimation.%2520To%2520achieve%2520maximum%2520performance%252C%2520systems%2520require%250Areal-time%2520algorithms%2520that%2520are%2520robust%2520to%2520motion%2520blur%252C%2520high%2520dynamic%2520range%252C%2520model%250Auncertainties%252C%2520aerodynamic%2520disturbances%252C%2520and%2520often%2520unpredictable%2520opponents.%250AThis%2520survey%2520covers%2520the%2520progression%2520of%2520autonomous%2520drone%2520racing%2520across%250Amodel-based%2520and%2520learning-based%2520approaches.%2520We%2520provide%2520an%2520overview%2520of%2520the%2520field%252C%250Aits%2520evolution%2520over%2520the%2520years%252C%2520and%2520conclude%2520with%2520the%2520biggest%2520challenges%2520and%2520open%250Aquestions%2520to%2520be%2520faced%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.01755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Drone%20Racing%3A%20A%20Survey&entry.906535625=Drew%20Hanover%20and%20Antonio%20Loquercio%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Robert%20Penicka%20and%20Yunlong%20Song%20and%20Giovanni%20Cioffi%20and%20Elia%20Kaufmann%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Over%20the%20last%20decade%2C%20the%20use%20of%20autonomous%20drone%20systems%20for%20surveying%2C%0Asearch%20and%20rescue%2C%20or%20last-mile%20delivery%20has%20increased%20exponentially.%20With%20the%0Arise%20of%20these%20applications%20comes%20the%20need%20for%20highly%20robust%2C%20safety-critical%0Aalgorithms%20which%20can%20operate%20drones%20in%20complex%20and%20uncertain%20environments.%0AAdditionally%2C%20flying%20fast%20enables%20drones%20to%20cover%20more%20ground%20which%20in%20turn%0Aincreases%20productivity%20and%20further%20strengthens%20their%20use%20case.%20One%20proxy%20for%0Adeveloping%20algorithms%20used%20in%20high-speed%20navigation%20is%20the%20task%20of%20autonomous%0Adrone%20racing%2C%20where%20researchers%20program%20drones%20to%20fly%20through%20a%20sequence%20of%0Agates%20and%20avoid%20obstacles%20as%20quickly%20as%20possible%20using%20onboard%20sensors%20and%0Alimited%20computational%20power.%20Speeds%20and%20accelerations%20exceed%20over%2080%20kph%20and%204%0Ag%20respectively%2C%20raising%20significant%20challenges%20across%20perception%2C%20planning%2C%0Acontrol%2C%20and%20state%20estimation.%20To%20achieve%20maximum%20performance%2C%20systems%20require%0Areal-time%20algorithms%20that%20are%20robust%20to%20motion%20blur%2C%20high%20dynamic%20range%2C%20model%0Auncertainties%2C%20aerodynamic%20disturbances%2C%20and%20often%20unpredictable%20opponents.%0AThis%20survey%20covers%20the%20progression%20of%20autonomous%20drone%20racing%20across%0Amodel-based%20and%20learning-based%20approaches.%20We%20provide%20an%20overview%20of%20the%20field%2C%0Aits%20evolution%20over%20the%20years%2C%20and%20conclude%20with%20the%20biggest%20challenges%20and%20open%0Aquestions%20to%20be%20faced%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01755v3&entry.124074799=Read"},
{"title": "StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis", "author": "Chidimma Opara", "abstract": "  The emergence of large language models (LLMs) capable of generating realistic\ntexts and images has sparked ethical concerns across various sectors. In\nresponse, researchers in academia and industry are actively exploring methods\nto distinguish AI-generated content from human-authored material. However, a\ncrucial question remains: What are the unique characteristics of AI-generated\ntext? Addressing this gap, this study proposes StyloAI, a data-driven model\nthat uses 31 stylometric features to identify AI-generated texts by applying a\nRandom Forest classifier on two multi-domain datasets. StyloAI achieves\naccuracy rates of 81% and 98% on the test set of the AuTextification dataset\nand the Education dataset, respectively. This approach surpasses the\nperformance of existing state-of-the-art models and provides valuable insights\ninto the differences between AI-generated and human-authored texts.\n", "link": "http://arxiv.org/abs/2405.10129v1", "date": "2024-05-16", "relevancy": 2.012, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5278}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4886}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyloAI%3A%20Distinguishing%20AI-Generated%20Content%20with%20Stylometric%20Analysis&body=Title%3A%20StyloAI%3A%20Distinguishing%20AI-Generated%20Content%20with%20Stylometric%20Analysis%0AAuthor%3A%20Chidimma%20Opara%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20capable%20of%20generating%20realistic%0Atexts%20and%20images%20has%20sparked%20ethical%20concerns%20across%20various%20sectors.%20In%0Aresponse%2C%20researchers%20in%20academia%20and%20industry%20are%20actively%20exploring%20methods%0Ato%20distinguish%20AI-generated%20content%20from%20human-authored%20material.%20However%2C%20a%0Acrucial%20question%20remains%3A%20What%20are%20the%20unique%20characteristics%20of%20AI-generated%0Atext%3F%20Addressing%20this%20gap%2C%20this%20study%20proposes%20StyloAI%2C%20a%20data-driven%20model%0Athat%20uses%2031%20stylometric%20features%20to%20identify%20AI-generated%20texts%20by%20applying%20a%0ARandom%20Forest%20classifier%20on%20two%20multi-domain%20datasets.%20StyloAI%20achieves%0Aaccuracy%20rates%20of%2081%25%20and%2098%25%20on%20the%20test%20set%20of%20the%20AuTextification%20dataset%0Aand%20the%20Education%20dataset%2C%20respectively.%20This%20approach%20surpasses%20the%0Aperformance%20of%20existing%20state-of-the-art%20models%20and%20provides%20valuable%20insights%0Ainto%20the%20differences%20between%20AI-generated%20and%20human-authored%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyloAI%253A%2520Distinguishing%2520AI-Generated%2520Content%2520with%2520Stylometric%2520Analysis%26entry.906535625%3DChidimma%2520Opara%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520capable%2520of%2520generating%2520realistic%250Atexts%2520and%2520images%2520has%2520sparked%2520ethical%2520concerns%2520across%2520various%2520sectors.%2520In%250Aresponse%252C%2520researchers%2520in%2520academia%2520and%2520industry%2520are%2520actively%2520exploring%2520methods%250Ato%2520distinguish%2520AI-generated%2520content%2520from%2520human-authored%2520material.%2520However%252C%2520a%250Acrucial%2520question%2520remains%253A%2520What%2520are%2520the%2520unique%2520characteristics%2520of%2520AI-generated%250Atext%253F%2520Addressing%2520this%2520gap%252C%2520this%2520study%2520proposes%2520StyloAI%252C%2520a%2520data-driven%2520model%250Athat%2520uses%252031%2520stylometric%2520features%2520to%2520identify%2520AI-generated%2520texts%2520by%2520applying%2520a%250ARandom%2520Forest%2520classifier%2520on%2520two%2520multi-domain%2520datasets.%2520StyloAI%2520achieves%250Aaccuracy%2520rates%2520of%252081%2525%2520and%252098%2525%2520on%2520the%2520test%2520set%2520of%2520the%2520AuTextification%2520dataset%250Aand%2520the%2520Education%2520dataset%252C%2520respectively.%2520This%2520approach%2520surpasses%2520the%250Aperformance%2520of%2520existing%2520state-of-the-art%2520models%2520and%2520provides%2520valuable%2520insights%250Ainto%2520the%2520differences%2520between%2520AI-generated%2520and%2520human-authored%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyloAI%3A%20Distinguishing%20AI-Generated%20Content%20with%20Stylometric%20Analysis&entry.906535625=Chidimma%20Opara&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20capable%20of%20generating%20realistic%0Atexts%20and%20images%20has%20sparked%20ethical%20concerns%20across%20various%20sectors.%20In%0Aresponse%2C%20researchers%20in%20academia%20and%20industry%20are%20actively%20exploring%20methods%0Ato%20distinguish%20AI-generated%20content%20from%20human-authored%20material.%20However%2C%20a%0Acrucial%20question%20remains%3A%20What%20are%20the%20unique%20characteristics%20of%20AI-generated%0Atext%3F%20Addressing%20this%20gap%2C%20this%20study%20proposes%20StyloAI%2C%20a%20data-driven%20model%0Athat%20uses%2031%20stylometric%20features%20to%20identify%20AI-generated%20texts%20by%20applying%20a%0ARandom%20Forest%20classifier%20on%20two%20multi-domain%20datasets.%20StyloAI%20achieves%0Aaccuracy%20rates%20of%2081%25%20and%2098%25%20on%20the%20test%20set%20of%20the%20AuTextification%20dataset%0Aand%20the%20Education%20dataset%2C%20respectively.%20This%20approach%20surpasses%20the%0Aperformance%20of%20existing%20state-of-the-art%20models%20and%20provides%20valuable%20insights%0Ainto%20the%20differences%20between%20AI-generated%20and%20human-authored%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10129v1&entry.124074799=Read"},
{"title": "Global Benchmark Database", "author": "Markus Iser and Christoph Jabs", "abstract": "  This paper presents Global Benchmark Database (GBD), a comprehensive suite of\ntools for provisioning and sustainably maintaining benchmark instances and\ntheir metadata. The availability of benchmark metadata is essential for many\ntasks in empirical research, e.g., for the data-driven compilation of\nbenchmarks, the domain-specific analysis of runtime experiments, or the\ninstance-specific selection of solvers. In this paper, we introduce the data\nmodel of GBD as well as its interfaces and provide examples of how to interact\nwith them. We also demonstrate the integration of custom data sources and\nexplain how to extend GBD with additional problem domains, instance formats and\nfeature extractors.\n", "link": "http://arxiv.org/abs/2405.10045v1", "date": "2024-05-16", "relevancy": 2.0114, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4109}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3985}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Benchmark%20Database&body=Title%3A%20Global%20Benchmark%20Database%0AAuthor%3A%20Markus%20Iser%20and%20Christoph%20Jabs%0AAbstract%3A%20%20%20This%20paper%20presents%20Global%20Benchmark%20Database%20%28GBD%29%2C%20a%20comprehensive%20suite%20of%0Atools%20for%20provisioning%20and%20sustainably%20maintaining%20benchmark%20instances%20and%0Atheir%20metadata.%20The%20availability%20of%20benchmark%20metadata%20is%20essential%20for%20many%0Atasks%20in%20empirical%20research%2C%20e.g.%2C%20for%20the%20data-driven%20compilation%20of%0Abenchmarks%2C%20the%20domain-specific%20analysis%20of%20runtime%20experiments%2C%20or%20the%0Ainstance-specific%20selection%20of%20solvers.%20In%20this%20paper%2C%20we%20introduce%20the%20data%0Amodel%20of%20GBD%20as%20well%20as%20its%20interfaces%20and%20provide%20examples%20of%20how%20to%20interact%0Awith%20them.%20We%20also%20demonstrate%20the%20integration%20of%20custom%20data%20sources%20and%0Aexplain%20how%20to%20extend%20GBD%20with%20additional%20problem%20domains%2C%20instance%20formats%20and%0Afeature%20extractors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Benchmark%2520Database%26entry.906535625%3DMarkus%2520Iser%2520and%2520Christoph%2520Jabs%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Global%2520Benchmark%2520Database%2520%2528GBD%2529%252C%2520a%2520comprehensive%2520suite%2520of%250Atools%2520for%2520provisioning%2520and%2520sustainably%2520maintaining%2520benchmark%2520instances%2520and%250Atheir%2520metadata.%2520The%2520availability%2520of%2520benchmark%2520metadata%2520is%2520essential%2520for%2520many%250Atasks%2520in%2520empirical%2520research%252C%2520e.g.%252C%2520for%2520the%2520data-driven%2520compilation%2520of%250Abenchmarks%252C%2520the%2520domain-specific%2520analysis%2520of%2520runtime%2520experiments%252C%2520or%2520the%250Ainstance-specific%2520selection%2520of%2520solvers.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520data%250Amodel%2520of%2520GBD%2520as%2520well%2520as%2520its%2520interfaces%2520and%2520provide%2520examples%2520of%2520how%2520to%2520interact%250Awith%2520them.%2520We%2520also%2520demonstrate%2520the%2520integration%2520of%2520custom%2520data%2520sources%2520and%250Aexplain%2520how%2520to%2520extend%2520GBD%2520with%2520additional%2520problem%2520domains%252C%2520instance%2520formats%2520and%250Afeature%2520extractors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Benchmark%20Database&entry.906535625=Markus%20Iser%20and%20Christoph%20Jabs&entry.1292438233=%20%20This%20paper%20presents%20Global%20Benchmark%20Database%20%28GBD%29%2C%20a%20comprehensive%20suite%20of%0Atools%20for%20provisioning%20and%20sustainably%20maintaining%20benchmark%20instances%20and%0Atheir%20metadata.%20The%20availability%20of%20benchmark%20metadata%20is%20essential%20for%20many%0Atasks%20in%20empirical%20research%2C%20e.g.%2C%20for%20the%20data-driven%20compilation%20of%0Abenchmarks%2C%20the%20domain-specific%20analysis%20of%20runtime%20experiments%2C%20or%20the%0Ainstance-specific%20selection%20of%20solvers.%20In%20this%20paper%2C%20we%20introduce%20the%20data%0Amodel%20of%20GBD%20as%20well%20as%20its%20interfaces%20and%20provide%20examples%20of%20how%20to%20interact%0Awith%20them.%20We%20also%20demonstrate%20the%20integration%20of%20custom%20data%20sources%20and%0Aexplain%20how%20to%20extend%20GBD%20with%20additional%20problem%20domains%2C%20instance%20formats%20and%0Afeature%20extractors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10045v1&entry.124074799=Read"},
{"title": "Biasing & Debiasing based Approach Towards Fair Knowledge Transfer for\n  Equitable Skin Analysis", "author": "Anshul Pundhir and Balasubramanian Raman and Pravendra Singh", "abstract": "  Deep learning models, particularly Convolutional Neural Networks (CNNs), have\ndemonstrated exceptional performance in diagnosing skin diseases, often\noutperforming dermatologists. However, they have also unveiled biases linked to\nspecific demographic traits, notably concerning diverse skin tones or gender,\nprompting concerns regarding fairness and limiting their widespread deployment.\nResearchers are actively working to ensure fairness in AI-based solutions, but\nexisting methods incur an accuracy loss when striving for fairness. To solve\nthis issue, we propose a `two-biased teachers' (i.e., biased on different\nsensitive attributes) based approach to transfer fair knowledge into the\nstudent network. Our approach mitigates biases present in the student network\nwithout harming its predictive accuracy. In fact, in most cases, our approach\nimproves the accuracy of the baseline model. To achieve this goal, we developed\na weighted loss function comprising biasing and debiasing loss terms. We\nsurpassed available state-of-the-art approaches to attain fairness and also\nimproved the accuracy at the same time. The proposed approach has been\nevaluated and validated on two dermatology datasets using standard accuracy and\nfairness evaluation measures. We will make source code publicly available to\nfoster reproducibility and future research.\n", "link": "http://arxiv.org/abs/2405.10256v1", "date": "2024-05-16", "relevancy": 2.0066, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5222}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biasing%20%26%20Debiasing%20based%20Approach%20Towards%20Fair%20Knowledge%20Transfer%20for%0A%20%20Equitable%20Skin%20Analysis&body=Title%3A%20Biasing%20%26%20Debiasing%20based%20Approach%20Towards%20Fair%20Knowledge%20Transfer%20for%0A%20%20Equitable%20Skin%20Analysis%0AAuthor%3A%20Anshul%20Pundhir%20and%20Balasubramanian%20Raman%20and%20Pravendra%20Singh%0AAbstract%3A%20%20%20Deep%20learning%20models%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20have%0Ademonstrated%20exceptional%20performance%20in%20diagnosing%20skin%20diseases%2C%20often%0Aoutperforming%20dermatologists.%20However%2C%20they%20have%20also%20unveiled%20biases%20linked%20to%0Aspecific%20demographic%20traits%2C%20notably%20concerning%20diverse%20skin%20tones%20or%20gender%2C%0Aprompting%20concerns%20regarding%20fairness%20and%20limiting%20their%20widespread%20deployment.%0AResearchers%20are%20actively%20working%20to%20ensure%20fairness%20in%20AI-based%20solutions%2C%20but%0Aexisting%20methods%20incur%20an%20accuracy%20loss%20when%20striving%20for%20fairness.%20To%20solve%0Athis%20issue%2C%20we%20propose%20a%20%60two-biased%20teachers%27%20%28i.e.%2C%20biased%20on%20different%0Asensitive%20attributes%29%20based%20approach%20to%20transfer%20fair%20knowledge%20into%20the%0Astudent%20network.%20Our%20approach%20mitigates%20biases%20present%20in%20the%20student%20network%0Awithout%20harming%20its%20predictive%20accuracy.%20In%20fact%2C%20in%20most%20cases%2C%20our%20approach%0Aimproves%20the%20accuracy%20of%20the%20baseline%20model.%20To%20achieve%20this%20goal%2C%20we%20developed%0Aa%20weighted%20loss%20function%20comprising%20biasing%20and%20debiasing%20loss%20terms.%20We%0Asurpassed%20available%20state-of-the-art%20approaches%20to%20attain%20fairness%20and%20also%0Aimproved%20the%20accuracy%20at%20the%20same%20time.%20The%20proposed%20approach%20has%20been%0Aevaluated%20and%20validated%20on%20two%20dermatology%20datasets%20using%20standard%20accuracy%20and%0Afairness%20evaluation%20measures.%20We%20will%20make%20source%20code%20publicly%20available%20to%0Afoster%20reproducibility%20and%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiasing%2520%2526%2520Debiasing%2520based%2520Approach%2520Towards%2520Fair%2520Knowledge%2520Transfer%2520for%250A%2520%2520Equitable%2520Skin%2520Analysis%26entry.906535625%3DAnshul%2520Pundhir%2520and%2520Balasubramanian%2520Raman%2520and%2520Pravendra%2520Singh%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%252C%2520particularly%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520have%250Ademonstrated%2520exceptional%2520performance%2520in%2520diagnosing%2520skin%2520diseases%252C%2520often%250Aoutperforming%2520dermatologists.%2520However%252C%2520they%2520have%2520also%2520unveiled%2520biases%2520linked%2520to%250Aspecific%2520demographic%2520traits%252C%2520notably%2520concerning%2520diverse%2520skin%2520tones%2520or%2520gender%252C%250Aprompting%2520concerns%2520regarding%2520fairness%2520and%2520limiting%2520their%2520widespread%2520deployment.%250AResearchers%2520are%2520actively%2520working%2520to%2520ensure%2520fairness%2520in%2520AI-based%2520solutions%252C%2520but%250Aexisting%2520methods%2520incur%2520an%2520accuracy%2520loss%2520when%2520striving%2520for%2520fairness.%2520To%2520solve%250Athis%2520issue%252C%2520we%2520propose%2520a%2520%2560two-biased%2520teachers%2527%2520%2528i.e.%252C%2520biased%2520on%2520different%250Asensitive%2520attributes%2529%2520based%2520approach%2520to%2520transfer%2520fair%2520knowledge%2520into%2520the%250Astudent%2520network.%2520Our%2520approach%2520mitigates%2520biases%2520present%2520in%2520the%2520student%2520network%250Awithout%2520harming%2520its%2520predictive%2520accuracy.%2520In%2520fact%252C%2520in%2520most%2520cases%252C%2520our%2520approach%250Aimproves%2520the%2520accuracy%2520of%2520the%2520baseline%2520model.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520developed%250Aa%2520weighted%2520loss%2520function%2520comprising%2520biasing%2520and%2520debiasing%2520loss%2520terms.%2520We%250Asurpassed%2520available%2520state-of-the-art%2520approaches%2520to%2520attain%2520fairness%2520and%2520also%250Aimproved%2520the%2520accuracy%2520at%2520the%2520same%2520time.%2520The%2520proposed%2520approach%2520has%2520been%250Aevaluated%2520and%2520validated%2520on%2520two%2520dermatology%2520datasets%2520using%2520standard%2520accuracy%2520and%250Afairness%2520evaluation%2520measures.%2520We%2520will%2520make%2520source%2520code%2520publicly%2520available%2520to%250Afoster%2520reproducibility%2520and%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biasing%20%26%20Debiasing%20based%20Approach%20Towards%20Fair%20Knowledge%20Transfer%20for%0A%20%20Equitable%20Skin%20Analysis&entry.906535625=Anshul%20Pundhir%20and%20Balasubramanian%20Raman%20and%20Pravendra%20Singh&entry.1292438233=%20%20Deep%20learning%20models%2C%20particularly%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20have%0Ademonstrated%20exceptional%20performance%20in%20diagnosing%20skin%20diseases%2C%20often%0Aoutperforming%20dermatologists.%20However%2C%20they%20have%20also%20unveiled%20biases%20linked%20to%0Aspecific%20demographic%20traits%2C%20notably%20concerning%20diverse%20skin%20tones%20or%20gender%2C%0Aprompting%20concerns%20regarding%20fairness%20and%20limiting%20their%20widespread%20deployment.%0AResearchers%20are%20actively%20working%20to%20ensure%20fairness%20in%20AI-based%20solutions%2C%20but%0Aexisting%20methods%20incur%20an%20accuracy%20loss%20when%20striving%20for%20fairness.%20To%20solve%0Athis%20issue%2C%20we%20propose%20a%20%60two-biased%20teachers%27%20%28i.e.%2C%20biased%20on%20different%0Asensitive%20attributes%29%20based%20approach%20to%20transfer%20fair%20knowledge%20into%20the%0Astudent%20network.%20Our%20approach%20mitigates%20biases%20present%20in%20the%20student%20network%0Awithout%20harming%20its%20predictive%20accuracy.%20In%20fact%2C%20in%20most%20cases%2C%20our%20approach%0Aimproves%20the%20accuracy%20of%20the%20baseline%20model.%20To%20achieve%20this%20goal%2C%20we%20developed%0Aa%20weighted%20loss%20function%20comprising%20biasing%20and%20debiasing%20loss%20terms.%20We%0Asurpassed%20available%20state-of-the-art%20approaches%20to%20attain%20fairness%20and%20also%0Aimproved%20the%20accuracy%20at%20the%20same%20time.%20The%20proposed%20approach%20has%20been%0Aevaluated%20and%20validated%20on%20two%20dermatology%20datasets%20using%20standard%20accuracy%20and%0Afairness%20evaluation%20measures.%20We%20will%20make%20source%20code%20publicly%20available%20to%0Afoster%20reproducibility%20and%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10256v1&entry.124074799=Read"},
{"title": "Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD\n  with Near-perfect Representation Learning", "author": "Chendi Wang and Yuqing Zhu and Weijie J. Su and Yu-Xiang Wang", "abstract": "  A recent study by De et al. (2022) has reported that large-scale\nrepresentation learning through pre-training on a public dataset significantly\nenhances differentially private (DP) learning in downstream tasks, despite the\nhigh dimensionality of the feature space. To theoretically explain this\nphenomenon, we consider the setting of a layer-peeled model in representation\nlearning, which results in interesting phenomena related to learned features in\ndeep learning and transfer learning, known as Neural Collapse (NC).\n  Within the framework of NC, we establish an error bound indicating that the\nmisclassification error is independent of dimension when the distance between\nactual features and the ideal ones is smaller than a threshold. Additionally,\nthe quality of the features in the last layer is empirically evaluated under\ndifferent pre-trained models within the framework of NC, showing that a more\npowerful transformer leads to a better feature representation. Furthermore, we\nreveal that DP fine-tuning is less robust compared to fine-tuning without DP,\nparticularly in the presence of perturbations. These observations are supported\nby both theoretical analyses and experimental evaluation. Moreover, to enhance\nthe robustness of DP fine-tuning, we suggest several strategies, such as\nfeature normalization or employing dimension reduction methods like Principal\nComponent Analysis (PCA). Empirically, we demonstrate a significant improvement\nin testing accuracy by conducting PCA on the last-layer features.\n", "link": "http://arxiv.org/abs/2405.08920v2", "date": "2024-05-16", "relevancy": 1.9939, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4895}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Collapse%20Meets%20Differential%20Privacy%3A%20Curious%20Behaviors%20of%20NoisyGD%0A%20%20with%20Near-perfect%20Representation%20Learning&body=Title%3A%20Neural%20Collapse%20Meets%20Differential%20Privacy%3A%20Curious%20Behaviors%20of%20NoisyGD%0A%20%20with%20Near-perfect%20Representation%20Learning%0AAuthor%3A%20Chendi%20Wang%20and%20Yuqing%20Zhu%20and%20Weijie%20J.%20Su%20and%20Yu-Xiang%20Wang%0AAbstract%3A%20%20%20A%20recent%20study%20by%20De%20et%20al.%20%282022%29%20has%20reported%20that%20large-scale%0Arepresentation%20learning%20through%20pre-training%20on%20a%20public%20dataset%20significantly%0Aenhances%20differentially%20private%20%28DP%29%20learning%20in%20downstream%20tasks%2C%20despite%20the%0Ahigh%20dimensionality%20of%20the%20feature%20space.%20To%20theoretically%20explain%20this%0Aphenomenon%2C%20we%20consider%20the%20setting%20of%20a%20layer-peeled%20model%20in%20representation%0Alearning%2C%20which%20results%20in%20interesting%20phenomena%20related%20to%20learned%20features%20in%0Adeep%20learning%20and%20transfer%20learning%2C%20known%20as%20Neural%20Collapse%20%28NC%29.%0A%20%20Within%20the%20framework%20of%20NC%2C%20we%20establish%20an%20error%20bound%20indicating%20that%20the%0Amisclassification%20error%20is%20independent%20of%20dimension%20when%20the%20distance%20between%0Aactual%20features%20and%20the%20ideal%20ones%20is%20smaller%20than%20a%20threshold.%20Additionally%2C%0Athe%20quality%20of%20the%20features%20in%20the%20last%20layer%20is%20empirically%20evaluated%20under%0Adifferent%20pre-trained%20models%20within%20the%20framework%20of%20NC%2C%20showing%20that%20a%20more%0Apowerful%20transformer%20leads%20to%20a%20better%20feature%20representation.%20Furthermore%2C%20we%0Areveal%20that%20DP%20fine-tuning%20is%20less%20robust%20compared%20to%20fine-tuning%20without%20DP%2C%0Aparticularly%20in%20the%20presence%20of%20perturbations.%20These%20observations%20are%20supported%0Aby%20both%20theoretical%20analyses%20and%20experimental%20evaluation.%20Moreover%2C%20to%20enhance%0Athe%20robustness%20of%20DP%20fine-tuning%2C%20we%20suggest%20several%20strategies%2C%20such%20as%0Afeature%20normalization%20or%20employing%20dimension%20reduction%20methods%20like%20Principal%0AComponent%20Analysis%20%28PCA%29.%20Empirically%2C%20we%20demonstrate%20a%20significant%20improvement%0Ain%20testing%20accuracy%20by%20conducting%20PCA%20on%20the%20last-layer%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Collapse%2520Meets%2520Differential%2520Privacy%253A%2520Curious%2520Behaviors%2520of%2520NoisyGD%250A%2520%2520with%2520Near-perfect%2520Representation%2520Learning%26entry.906535625%3DChendi%2520Wang%2520and%2520Yuqing%2520Zhu%2520and%2520Weijie%2520J.%2520Su%2520and%2520Yu-Xiang%2520Wang%26entry.1292438233%3D%2520%2520A%2520recent%2520study%2520by%2520De%2520et%2520al.%2520%25282022%2529%2520has%2520reported%2520that%2520large-scale%250Arepresentation%2520learning%2520through%2520pre-training%2520on%2520a%2520public%2520dataset%2520significantly%250Aenhances%2520differentially%2520private%2520%2528DP%2529%2520learning%2520in%2520downstream%2520tasks%252C%2520despite%2520the%250Ahigh%2520dimensionality%2520of%2520the%2520feature%2520space.%2520To%2520theoretically%2520explain%2520this%250Aphenomenon%252C%2520we%2520consider%2520the%2520setting%2520of%2520a%2520layer-peeled%2520model%2520in%2520representation%250Alearning%252C%2520which%2520results%2520in%2520interesting%2520phenomena%2520related%2520to%2520learned%2520features%2520in%250Adeep%2520learning%2520and%2520transfer%2520learning%252C%2520known%2520as%2520Neural%2520Collapse%2520%2528NC%2529.%250A%2520%2520Within%2520the%2520framework%2520of%2520NC%252C%2520we%2520establish%2520an%2520error%2520bound%2520indicating%2520that%2520the%250Amisclassification%2520error%2520is%2520independent%2520of%2520dimension%2520when%2520the%2520distance%2520between%250Aactual%2520features%2520and%2520the%2520ideal%2520ones%2520is%2520smaller%2520than%2520a%2520threshold.%2520Additionally%252C%250Athe%2520quality%2520of%2520the%2520features%2520in%2520the%2520last%2520layer%2520is%2520empirically%2520evaluated%2520under%250Adifferent%2520pre-trained%2520models%2520within%2520the%2520framework%2520of%2520NC%252C%2520showing%2520that%2520a%2520more%250Apowerful%2520transformer%2520leads%2520to%2520a%2520better%2520feature%2520representation.%2520Furthermore%252C%2520we%250Areveal%2520that%2520DP%2520fine-tuning%2520is%2520less%2520robust%2520compared%2520to%2520fine-tuning%2520without%2520DP%252C%250Aparticularly%2520in%2520the%2520presence%2520of%2520perturbations.%2520These%2520observations%2520are%2520supported%250Aby%2520both%2520theoretical%2520analyses%2520and%2520experimental%2520evaluation.%2520Moreover%252C%2520to%2520enhance%250Athe%2520robustness%2520of%2520DP%2520fine-tuning%252C%2520we%2520suggest%2520several%2520strategies%252C%2520such%2520as%250Afeature%2520normalization%2520or%2520employing%2520dimension%2520reduction%2520methods%2520like%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529.%2520Empirically%252C%2520we%2520demonstrate%2520a%2520significant%2520improvement%250Ain%2520testing%2520accuracy%2520by%2520conducting%2520PCA%2520on%2520the%2520last-layer%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Collapse%20Meets%20Differential%20Privacy%3A%20Curious%20Behaviors%20of%20NoisyGD%0A%20%20with%20Near-perfect%20Representation%20Learning&entry.906535625=Chendi%20Wang%20and%20Yuqing%20Zhu%20and%20Weijie%20J.%20Su%20and%20Yu-Xiang%20Wang&entry.1292438233=%20%20A%20recent%20study%20by%20De%20et%20al.%20%282022%29%20has%20reported%20that%20large-scale%0Arepresentation%20learning%20through%20pre-training%20on%20a%20public%20dataset%20significantly%0Aenhances%20differentially%20private%20%28DP%29%20learning%20in%20downstream%20tasks%2C%20despite%20the%0Ahigh%20dimensionality%20of%20the%20feature%20space.%20To%20theoretically%20explain%20this%0Aphenomenon%2C%20we%20consider%20the%20setting%20of%20a%20layer-peeled%20model%20in%20representation%0Alearning%2C%20which%20results%20in%20interesting%20phenomena%20related%20to%20learned%20features%20in%0Adeep%20learning%20and%20transfer%20learning%2C%20known%20as%20Neural%20Collapse%20%28NC%29.%0A%20%20Within%20the%20framework%20of%20NC%2C%20we%20establish%20an%20error%20bound%20indicating%20that%20the%0Amisclassification%20error%20is%20independent%20of%20dimension%20when%20the%20distance%20between%0Aactual%20features%20and%20the%20ideal%20ones%20is%20smaller%20than%20a%20threshold.%20Additionally%2C%0Athe%20quality%20of%20the%20features%20in%20the%20last%20layer%20is%20empirically%20evaluated%20under%0Adifferent%20pre-trained%20models%20within%20the%20framework%20of%20NC%2C%20showing%20that%20a%20more%0Apowerful%20transformer%20leads%20to%20a%20better%20feature%20representation.%20Furthermore%2C%20we%0Areveal%20that%20DP%20fine-tuning%20is%20less%20robust%20compared%20to%20fine-tuning%20without%20DP%2C%0Aparticularly%20in%20the%20presence%20of%20perturbations.%20These%20observations%20are%20supported%0Aby%20both%20theoretical%20analyses%20and%20experimental%20evaluation.%20Moreover%2C%20to%20enhance%0Athe%20robustness%20of%20DP%20fine-tuning%2C%20we%20suggest%20several%20strategies%2C%20such%20as%0Afeature%20normalization%20or%20employing%20dimension%20reduction%20methods%20like%20Principal%0AComponent%20Analysis%20%28PCA%29.%20Empirically%2C%20we%20demonstrate%20a%20significant%20improvement%0Ain%20testing%20accuracy%20by%20conducting%20PCA%20on%20the%20last-layer%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08920v2&entry.124074799=Read"},
{"title": "TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based\n  BiLSTM and Twitter-RoBERTa", "author": "Md Abrar Jahin and Md Sakib Hossain Shovon and M. F. Mridha and Md Rashedul Islam and Yutaka Watanobe", "abstract": "  Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.\n", "link": "http://arxiv.org/abs/2404.00297v2", "date": "2024-05-16", "relevancy": 1.9804, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRABSA%3A%20Interpretable%20Sentiment%20Analysis%20of%20Tweets%20using%20Attention-based%0A%20%20BiLSTM%20and%20Twitter-RoBERTa&body=Title%3A%20TRABSA%3A%20Interpretable%20Sentiment%20Analysis%20of%20Tweets%20using%20Attention-based%0A%20%20BiLSTM%20and%20Twitter-RoBERTa%0AAuthor%3A%20Md%20Abrar%20Jahin%20and%20Md%20Sakib%20Hossain%20Shovon%20and%20M.%20F.%20Mridha%20and%20Md%20Rashedul%20Islam%20and%20Yutaka%20Watanobe%0AAbstract%3A%20%20%20Sentiment%20analysis%20is%20crucial%20for%20understanding%20public%20opinion%20and%20consumer%0Abehavior.%20Existing%20models%20face%20challenges%20with%20linguistic%20diversity%2C%0Ageneralizability%2C%20and%20explainability.%20We%20propose%20TRABSA%2C%20a%20hybrid%20framework%0Aintegrating%20transformer-based%20architectures%2C%20attention%20mechanisms%2C%20and%20BiLSTM%0Anetworks%20to%20address%20this.%20Leveraging%20RoBERTa-trained%20on%20124M%20tweets%2C%20we%20bridge%0Agaps%20in%20sentiment%20analysis%20benchmarks%2C%20ensuring%20state-of-the-art%20accuracy.%0AAugmenting%20datasets%20with%20tweets%20from%2032%20countries%20and%20US%20states%2C%20we%20compare%20six%0Aword-embedding%20techniques%20and%20three%20lexicon-based%20labeling%20techniques%2C%0Aselecting%20the%20best%20for%20optimal%20sentiment%20analysis.%20TRABSA%20outperforms%0Atraditional%20ML%20and%20deep%20learning%20models%20with%2094%25%20accuracy%20and%20significant%0Aprecision%2C%20recall%2C%20and%20F1-score%20gains.%20Evaluation%20across%20diverse%20datasets%0Ademonstrates%20consistent%20superiority%20and%20generalizability.%20SHAP%20and%20LIME%0Aanalyses%20enhance%20interpretability%2C%20improving%20confidence%20in%20predictions.%20Our%0Astudy%20facilitates%20pandemic%20resource%20management%2C%20aiding%20resource%20planning%2C%0Apolicy%20formation%2C%20and%20vaccination%20tactics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRABSA%253A%2520Interpretable%2520Sentiment%2520Analysis%2520of%2520Tweets%2520using%2520Attention-based%250A%2520%2520BiLSTM%2520and%2520Twitter-RoBERTa%26entry.906535625%3DMd%2520Abrar%2520Jahin%2520and%2520Md%2520Sakib%2520Hossain%2520Shovon%2520and%2520M.%2520F.%2520Mridha%2520and%2520Md%2520Rashedul%2520Islam%2520and%2520Yutaka%2520Watanobe%26entry.1292438233%3D%2520%2520Sentiment%2520analysis%2520is%2520crucial%2520for%2520understanding%2520public%2520opinion%2520and%2520consumer%250Abehavior.%2520Existing%2520models%2520face%2520challenges%2520with%2520linguistic%2520diversity%252C%250Ageneralizability%252C%2520and%2520explainability.%2520We%2520propose%2520TRABSA%252C%2520a%2520hybrid%2520framework%250Aintegrating%2520transformer-based%2520architectures%252C%2520attention%2520mechanisms%252C%2520and%2520BiLSTM%250Anetworks%2520to%2520address%2520this.%2520Leveraging%2520RoBERTa-trained%2520on%2520124M%2520tweets%252C%2520we%2520bridge%250Agaps%2520in%2520sentiment%2520analysis%2520benchmarks%252C%2520ensuring%2520state-of-the-art%2520accuracy.%250AAugmenting%2520datasets%2520with%2520tweets%2520from%252032%2520countries%2520and%2520US%2520states%252C%2520we%2520compare%2520six%250Aword-embedding%2520techniques%2520and%2520three%2520lexicon-based%2520labeling%2520techniques%252C%250Aselecting%2520the%2520best%2520for%2520optimal%2520sentiment%2520analysis.%2520TRABSA%2520outperforms%250Atraditional%2520ML%2520and%2520deep%2520learning%2520models%2520with%252094%2525%2520accuracy%2520and%2520significant%250Aprecision%252C%2520recall%252C%2520and%2520F1-score%2520gains.%2520Evaluation%2520across%2520diverse%2520datasets%250Ademonstrates%2520consistent%2520superiority%2520and%2520generalizability.%2520SHAP%2520and%2520LIME%250Aanalyses%2520enhance%2520interpretability%252C%2520improving%2520confidence%2520in%2520predictions.%2520Our%250Astudy%2520facilitates%2520pandemic%2520resource%2520management%252C%2520aiding%2520resource%2520planning%252C%250Apolicy%2520formation%252C%2520and%2520vaccination%2520tactics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRABSA%3A%20Interpretable%20Sentiment%20Analysis%20of%20Tweets%20using%20Attention-based%0A%20%20BiLSTM%20and%20Twitter-RoBERTa&entry.906535625=Md%20Abrar%20Jahin%20and%20Md%20Sakib%20Hossain%20Shovon%20and%20M.%20F.%20Mridha%20and%20Md%20Rashedul%20Islam%20and%20Yutaka%20Watanobe&entry.1292438233=%20%20Sentiment%20analysis%20is%20crucial%20for%20understanding%20public%20opinion%20and%20consumer%0Abehavior.%20Existing%20models%20face%20challenges%20with%20linguistic%20diversity%2C%0Ageneralizability%2C%20and%20explainability.%20We%20propose%20TRABSA%2C%20a%20hybrid%20framework%0Aintegrating%20transformer-based%20architectures%2C%20attention%20mechanisms%2C%20and%20BiLSTM%0Anetworks%20to%20address%20this.%20Leveraging%20RoBERTa-trained%20on%20124M%20tweets%2C%20we%20bridge%0Agaps%20in%20sentiment%20analysis%20benchmarks%2C%20ensuring%20state-of-the-art%20accuracy.%0AAugmenting%20datasets%20with%20tweets%20from%2032%20countries%20and%20US%20states%2C%20we%20compare%20six%0Aword-embedding%20techniques%20and%20three%20lexicon-based%20labeling%20techniques%2C%0Aselecting%20the%20best%20for%20optimal%20sentiment%20analysis.%20TRABSA%20outperforms%0Atraditional%20ML%20and%20deep%20learning%20models%20with%2094%25%20accuracy%20and%20significant%0Aprecision%2C%20recall%2C%20and%20F1-score%20gains.%20Evaluation%20across%20diverse%20datasets%0Ademonstrates%20consistent%20superiority%20and%20generalizability.%20SHAP%20and%20LIME%0Aanalyses%20enhance%20interpretability%2C%20improving%20confidence%20in%20predictions.%20Our%0Astudy%20facilitates%20pandemic%20resource%20management%2C%20aiding%20resource%20planning%2C%0Apolicy%20formation%2C%20and%20vaccination%20tactics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00297v2&entry.124074799=Read"},
{"title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation", "author": "Abhishek Divekar and Greg Durrett", "abstract": "  Large language models (LLMs) are versatile and can address many tasks, but\nfor computational efficiency, it is often desirable to distill their\ncapabilities into smaller student models. One way to do this for classification\ntasks is via dataset synthesis, which can be accomplished by generating\nexamples of each label from the LLM. Prior approaches to synthesis use few-shot\nprompting, which relies on the LLM's parametric knowledge to generate usable\nexamples. However, this leads to issues of repetition, bias towards popular\nentities, and stylistic differences from human text. In this work, we propose\nSynthesize by Retrieval and Refinement (SynthesizRR), which uses retrieval\naugmentation to introduce variety into the dataset synthesis process: as\nretrieved passages vary, the LLM is \"seeded\" with different content to generate\nits examples. We empirically study the synthesis of six datasets, covering\ntopic classification, sentiment analysis, tone detection, and humor, requiring\ncomplex synthesis strategies. We find SynthesizRR greatly improves lexical and\nsemantic diversity, similarity to human-written text, and distillation\nperformance, when compared to standard 32-shot prompting and six baseline\napproaches.\n", "link": "http://arxiv.org/abs/2405.10040v1", "date": "2024-05-16", "relevancy": 1.9719, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.496}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&body=Title%3A%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation%0AAuthor%3A%20Abhishek%20Divekar%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20versatile%20and%20can%20address%20many%20tasks%2C%20but%0Afor%20computational%20efficiency%2C%20it%20is%20often%20desirable%20to%20distill%20their%0Acapabilities%20into%20smaller%20student%20models.%20One%20way%20to%20do%20this%20for%20classification%0Atasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%20accomplished%20by%20generating%0Aexamples%20of%20each%20label%20from%20the%20LLM.%20Prior%20approaches%20to%20synthesis%20use%20few-shot%0Aprompting%2C%20which%20relies%20on%20the%20LLM%27s%20parametric%20knowledge%20to%20generate%20usable%0Aexamples.%20However%2C%20this%20leads%20to%20issues%20of%20repetition%2C%20bias%20towards%20popular%0Aentities%2C%20and%20stylistic%20differences%20from%20human%20text.%20In%20this%20work%2C%20we%20propose%0ASynthesize%20by%20Retrieval%20and%20Refinement%20%28SynthesizRR%29%2C%20which%20uses%20retrieval%0Aaugmentation%20to%20introduce%20variety%20into%20the%20dataset%20synthesis%20process%3A%20as%0Aretrieved%20passages%20vary%2C%20the%20LLM%20is%20%22seeded%22%20with%20different%20content%20to%20generate%0Aits%20examples.%20We%20empirically%20study%20the%20synthesis%20of%20six%20datasets%2C%20covering%0Atopic%20classification%2C%20sentiment%20analysis%2C%20tone%20detection%2C%20and%20humor%2C%20requiring%0Acomplex%20synthesis%20strategies.%20We%20find%20SynthesizRR%20greatly%20improves%20lexical%20and%0Asemantic%20diversity%2C%20similarity%20to%20human-written%20text%2C%20and%20distillation%0Aperformance%2C%20when%20compared%20to%20standard%2032-shot%20prompting%20and%20six%20baseline%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizRR%253A%2520Generating%2520Diverse%2520Datasets%2520with%2520Retrieval%2520Augmentation%26entry.906535625%3DAbhishek%2520Divekar%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520versatile%2520and%2520can%2520address%2520many%2520tasks%252C%2520but%250Afor%2520computational%2520efficiency%252C%2520it%2520is%2520often%2520desirable%2520to%2520distill%2520their%250Acapabilities%2520into%2520smaller%2520student%2520models.%2520One%2520way%2520to%2520do%2520this%2520for%2520classification%250Atasks%2520is%2520via%2520dataset%2520synthesis%252C%2520which%2520can%2520be%2520accomplished%2520by%2520generating%250Aexamples%2520of%2520each%2520label%2520from%2520the%2520LLM.%2520Prior%2520approaches%2520to%2520synthesis%2520use%2520few-shot%250Aprompting%252C%2520which%2520relies%2520on%2520the%2520LLM%2527s%2520parametric%2520knowledge%2520to%2520generate%2520usable%250Aexamples.%2520However%252C%2520this%2520leads%2520to%2520issues%2520of%2520repetition%252C%2520bias%2520towards%2520popular%250Aentities%252C%2520and%2520stylistic%2520differences%2520from%2520human%2520text.%2520In%2520this%2520work%252C%2520we%2520propose%250ASynthesize%2520by%2520Retrieval%2520and%2520Refinement%2520%2528SynthesizRR%2529%252C%2520which%2520uses%2520retrieval%250Aaugmentation%2520to%2520introduce%2520variety%2520into%2520the%2520dataset%2520synthesis%2520process%253A%2520as%250Aretrieved%2520passages%2520vary%252C%2520the%2520LLM%2520is%2520%2522seeded%2522%2520with%2520different%2520content%2520to%2520generate%250Aits%2520examples.%2520We%2520empirically%2520study%2520the%2520synthesis%2520of%2520six%2520datasets%252C%2520covering%250Atopic%2520classification%252C%2520sentiment%2520analysis%252C%2520tone%2520detection%252C%2520and%2520humor%252C%2520requiring%250Acomplex%2520synthesis%2520strategies.%2520We%2520find%2520SynthesizRR%2520greatly%2520improves%2520lexical%2520and%250Asemantic%2520diversity%252C%2520similarity%2520to%2520human-written%2520text%252C%2520and%2520distillation%250Aperformance%252C%2520when%2520compared%2520to%2520standard%252032-shot%2520prompting%2520and%2520six%2520baseline%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&entry.906535625=Abhishek%20Divekar%20and%20Greg%20Durrett&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20versatile%20and%20can%20address%20many%20tasks%2C%20but%0Afor%20computational%20efficiency%2C%20it%20is%20often%20desirable%20to%20distill%20their%0Acapabilities%20into%20smaller%20student%20models.%20One%20way%20to%20do%20this%20for%20classification%0Atasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%20accomplished%20by%20generating%0Aexamples%20of%20each%20label%20from%20the%20LLM.%20Prior%20approaches%20to%20synthesis%20use%20few-shot%0Aprompting%2C%20which%20relies%20on%20the%20LLM%27s%20parametric%20knowledge%20to%20generate%20usable%0Aexamples.%20However%2C%20this%20leads%20to%20issues%20of%20repetition%2C%20bias%20towards%20popular%0Aentities%2C%20and%20stylistic%20differences%20from%20human%20text.%20In%20this%20work%2C%20we%20propose%0ASynthesize%20by%20Retrieval%20and%20Refinement%20%28SynthesizRR%29%2C%20which%20uses%20retrieval%0Aaugmentation%20to%20introduce%20variety%20into%20the%20dataset%20synthesis%20process%3A%20as%0Aretrieved%20passages%20vary%2C%20the%20LLM%20is%20%22seeded%22%20with%20different%20content%20to%20generate%0Aits%20examples.%20We%20empirically%20study%20the%20synthesis%20of%20six%20datasets%2C%20covering%0Atopic%20classification%2C%20sentiment%20analysis%2C%20tone%20detection%2C%20and%20humor%2C%20requiring%0Acomplex%20synthesis%20strategies.%20We%20find%20SynthesizRR%20greatly%20improves%20lexical%20and%0Asemantic%20diversity%2C%20similarity%20to%20human-written%20text%2C%20and%20distillation%0Aperformance%2C%20when%20compared%20to%20standard%2032-shot%20prompting%20and%20six%20baseline%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10040v1&entry.124074799=Read"},
{"title": "A Framework for Improving the Reliability of Black-box Variational\n  Inference", "author": "Manushi Welandawe and Michael Riis Andersen and Aki Vehtari and Jonathan H. Huggins", "abstract": "  Black-box variational inference (BBVI) now sees widespread use in machine\nlearning and statistics as a fast yet flexible alternative to Markov chain\nMonte Carlo methods for approximate Bayesian inference. However, stochastic\noptimization methods for BBVI remain unreliable and require substantial\nexpertise and hand-tuning to apply effectively. In this paper, we propose\nRobust and Automated Black-box VI (RABVI), a framework for improving the\nreliability of BBVI optimization. RABVI is based on rigorously justified\nautomation techniques, includes just a small number of intuitive tuning\nparameters, and detects inaccurate estimates of the optimal variational\napproximation. RABVI adaptively decreases the learning rate by detecting\nconvergence of the fixed--learning-rate iterates, then estimates the\nsymmetrized Kullback--Leibler (KL) divergence between the current variational\napproximation and the optimal one. It also employs a novel optimization\ntermination criterion that enables the user to balance desired accuracy against\ncomputational cost by comparing (i) the predicted relative decrease in the\nsymmetrized KL divergence if a smaller learning were used and (ii) the\npredicted computation required to converge with the smaller learning rate. We\nvalidate the robustness and accuracy of RABVI through carefully designed\nsimulation studies and on a diverse set of real-world model and data examples.\n", "link": "http://arxiv.org/abs/2203.15945v2", "date": "2024-05-16", "relevancy": 1.9637, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Improving%20the%20Reliability%20of%20Black-box%20Variational%0A%20%20Inference&body=Title%3A%20A%20Framework%20for%20Improving%20the%20Reliability%20of%20Black-box%20Variational%0A%20%20Inference%0AAuthor%3A%20Manushi%20Welandawe%20and%20Michael%20Riis%20Andersen%20and%20Aki%20Vehtari%20and%20Jonathan%20H.%20Huggins%0AAbstract%3A%20%20%20Black-box%20variational%20inference%20%28BBVI%29%20now%20sees%20widespread%20use%20in%20machine%0Alearning%20and%20statistics%20as%20a%20fast%20yet%20flexible%20alternative%20to%20Markov%20chain%0AMonte%20Carlo%20methods%20for%20approximate%20Bayesian%20inference.%20However%2C%20stochastic%0Aoptimization%20methods%20for%20BBVI%20remain%20unreliable%20and%20require%20substantial%0Aexpertise%20and%20hand-tuning%20to%20apply%20effectively.%20In%20this%20paper%2C%20we%20propose%0ARobust%20and%20Automated%20Black-box%20VI%20%28RABVI%29%2C%20a%20framework%20for%20improving%20the%0Areliability%20of%20BBVI%20optimization.%20RABVI%20is%20based%20on%20rigorously%20justified%0Aautomation%20techniques%2C%20includes%20just%20a%20small%20number%20of%20intuitive%20tuning%0Aparameters%2C%20and%20detects%20inaccurate%20estimates%20of%20the%20optimal%20variational%0Aapproximation.%20RABVI%20adaptively%20decreases%20the%20learning%20rate%20by%20detecting%0Aconvergence%20of%20the%20fixed--learning-rate%20iterates%2C%20then%20estimates%20the%0Asymmetrized%20Kullback--Leibler%20%28KL%29%20divergence%20between%20the%20current%20variational%0Aapproximation%20and%20the%20optimal%20one.%20It%20also%20employs%20a%20novel%20optimization%0Atermination%20criterion%20that%20enables%20the%20user%20to%20balance%20desired%20accuracy%20against%0Acomputational%20cost%20by%20comparing%20%28i%29%20the%20predicted%20relative%20decrease%20in%20the%0Asymmetrized%20KL%20divergence%20if%20a%20smaller%20learning%20were%20used%20and%20%28ii%29%20the%0Apredicted%20computation%20required%20to%20converge%20with%20the%20smaller%20learning%20rate.%20We%0Avalidate%20the%20robustness%20and%20accuracy%20of%20RABVI%20through%20carefully%20designed%0Asimulation%20studies%20and%20on%20a%20diverse%20set%20of%20real-world%20model%20and%20data%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.15945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Improving%2520the%2520Reliability%2520of%2520Black-box%2520Variational%250A%2520%2520Inference%26entry.906535625%3DManushi%2520Welandawe%2520and%2520Michael%2520Riis%2520Andersen%2520and%2520Aki%2520Vehtari%2520and%2520Jonathan%2520H.%2520Huggins%26entry.1292438233%3D%2520%2520Black-box%2520variational%2520inference%2520%2528BBVI%2529%2520now%2520sees%2520widespread%2520use%2520in%2520machine%250Alearning%2520and%2520statistics%2520as%2520a%2520fast%2520yet%2520flexible%2520alternative%2520to%2520Markov%2520chain%250AMonte%2520Carlo%2520methods%2520for%2520approximate%2520Bayesian%2520inference.%2520However%252C%2520stochastic%250Aoptimization%2520methods%2520for%2520BBVI%2520remain%2520unreliable%2520and%2520require%2520substantial%250Aexpertise%2520and%2520hand-tuning%2520to%2520apply%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520propose%250ARobust%2520and%2520Automated%2520Black-box%2520VI%2520%2528RABVI%2529%252C%2520a%2520framework%2520for%2520improving%2520the%250Areliability%2520of%2520BBVI%2520optimization.%2520RABVI%2520is%2520based%2520on%2520rigorously%2520justified%250Aautomation%2520techniques%252C%2520includes%2520just%2520a%2520small%2520number%2520of%2520intuitive%2520tuning%250Aparameters%252C%2520and%2520detects%2520inaccurate%2520estimates%2520of%2520the%2520optimal%2520variational%250Aapproximation.%2520RABVI%2520adaptively%2520decreases%2520the%2520learning%2520rate%2520by%2520detecting%250Aconvergence%2520of%2520the%2520fixed--learning-rate%2520iterates%252C%2520then%2520estimates%2520the%250Asymmetrized%2520Kullback--Leibler%2520%2528KL%2529%2520divergence%2520between%2520the%2520current%2520variational%250Aapproximation%2520and%2520the%2520optimal%2520one.%2520It%2520also%2520employs%2520a%2520novel%2520optimization%250Atermination%2520criterion%2520that%2520enables%2520the%2520user%2520to%2520balance%2520desired%2520accuracy%2520against%250Acomputational%2520cost%2520by%2520comparing%2520%2528i%2529%2520the%2520predicted%2520relative%2520decrease%2520in%2520the%250Asymmetrized%2520KL%2520divergence%2520if%2520a%2520smaller%2520learning%2520were%2520used%2520and%2520%2528ii%2529%2520the%250Apredicted%2520computation%2520required%2520to%2520converge%2520with%2520the%2520smaller%2520learning%2520rate.%2520We%250Avalidate%2520the%2520robustness%2520and%2520accuracy%2520of%2520RABVI%2520through%2520carefully%2520designed%250Asimulation%2520studies%2520and%2520on%2520a%2520diverse%2520set%2520of%2520real-world%2520model%2520and%2520data%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.15945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Improving%20the%20Reliability%20of%20Black-box%20Variational%0A%20%20Inference&entry.906535625=Manushi%20Welandawe%20and%20Michael%20Riis%20Andersen%20and%20Aki%20Vehtari%20and%20Jonathan%20H.%20Huggins&entry.1292438233=%20%20Black-box%20variational%20inference%20%28BBVI%29%20now%20sees%20widespread%20use%20in%20machine%0Alearning%20and%20statistics%20as%20a%20fast%20yet%20flexible%20alternative%20to%20Markov%20chain%0AMonte%20Carlo%20methods%20for%20approximate%20Bayesian%20inference.%20However%2C%20stochastic%0Aoptimization%20methods%20for%20BBVI%20remain%20unreliable%20and%20require%20substantial%0Aexpertise%20and%20hand-tuning%20to%20apply%20effectively.%20In%20this%20paper%2C%20we%20propose%0ARobust%20and%20Automated%20Black-box%20VI%20%28RABVI%29%2C%20a%20framework%20for%20improving%20the%0Areliability%20of%20BBVI%20optimization.%20RABVI%20is%20based%20on%20rigorously%20justified%0Aautomation%20techniques%2C%20includes%20just%20a%20small%20number%20of%20intuitive%20tuning%0Aparameters%2C%20and%20detects%20inaccurate%20estimates%20of%20the%20optimal%20variational%0Aapproximation.%20RABVI%20adaptively%20decreases%20the%20learning%20rate%20by%20detecting%0Aconvergence%20of%20the%20fixed--learning-rate%20iterates%2C%20then%20estimates%20the%0Asymmetrized%20Kullback--Leibler%20%28KL%29%20divergence%20between%20the%20current%20variational%0Aapproximation%20and%20the%20optimal%20one.%20It%20also%20employs%20a%20novel%20optimization%0Atermination%20criterion%20that%20enables%20the%20user%20to%20balance%20desired%20accuracy%20against%0Acomputational%20cost%20by%20comparing%20%28i%29%20the%20predicted%20relative%20decrease%20in%20the%0Asymmetrized%20KL%20divergence%20if%20a%20smaller%20learning%20were%20used%20and%20%28ii%29%20the%0Apredicted%20computation%20required%20to%20converge%20with%20the%20smaller%20learning%20rate.%20We%0Avalidate%20the%20robustness%20and%20accuracy%20of%20RABVI%20through%20carefully%20designed%0Asimulation%20studies%20and%20on%20a%20diverse%20set%20of%20real-world%20model%20and%20data%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.15945v2&entry.124074799=Read"},
{"title": "Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN\n  Learning Over-Fitted Features", "author": "Junpeng Zhang and Qing Li and Liang Lin and Quanshi Zhang", "abstract": "  This paper investigates the dynamics of a deep neural network (DNN) learning\ninteractions. Previous studies have discovered and mathematically proven that\ngiven each input sample, a well-trained DNN usually only encodes a small number\nof interactions (non-linear relationships) between input variables in the\nsample. A series of theorems have been derived to prove that we can consider\nthe DNN's inference equivalent to using these interactions as primitive\npatterns for inference. In this paper, we discover the DNN learns interactions\nin two phases. The first phase mainly penalizes interactions of medium and high\norders, and the second phase mainly learns interactions of gradually increasing\norders. We can consider the two-phase phenomenon as the starting point of a DNN\nlearning over-fitted features. Such a phenomenon has been widely shared by DNNs\nwith various architectures trained for different tasks. Therefore, the\ndiscovery of the two-phase dynamics provides a detailed mechanism for how a DNN\ngradually learns different inference patterns (interactions). In particular, we\nhave also verified the claim that high-order interactions have weaker\ngeneralization power than low-order interactions. Thus, the discovered\ntwo-phase dynamics also explains how the generalization power of a DNN changes\nduring the training process.\n", "link": "http://arxiv.org/abs/2405.10262v1", "date": "2024-05-16", "relevancy": 1.9605, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5504}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4498}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Phase%20Dynamics%20of%20Interactions%20Explains%20the%20Starting%20Point%20of%20a%20DNN%0A%20%20Learning%20Over-Fitted%20Features&body=Title%3A%20Two-Phase%20Dynamics%20of%20Interactions%20Explains%20the%20Starting%20Point%20of%20a%20DNN%0A%20%20Learning%20Over-Fitted%20Features%0AAuthor%3A%20Junpeng%20Zhang%20and%20Qing%20Li%20and%20Liang%20Lin%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20dynamics%20of%20a%20deep%20neural%20network%20%28DNN%29%20learning%0Ainteractions.%20Previous%20studies%20have%20discovered%20and%20mathematically%20proven%20that%0Agiven%20each%20input%20sample%2C%20a%20well-trained%20DNN%20usually%20only%20encodes%20a%20small%20number%0Aof%20interactions%20%28non-linear%20relationships%29%20between%20input%20variables%20in%20the%0Asample.%20A%20series%20of%20theorems%20have%20been%20derived%20to%20prove%20that%20we%20can%20consider%0Athe%20DNN%27s%20inference%20equivalent%20to%20using%20these%20interactions%20as%20primitive%0Apatterns%20for%20inference.%20In%20this%20paper%2C%20we%20discover%20the%20DNN%20learns%20interactions%0Ain%20two%20phases.%20The%20first%20phase%20mainly%20penalizes%20interactions%20of%20medium%20and%20high%0Aorders%2C%20and%20the%20second%20phase%20mainly%20learns%20interactions%20of%20gradually%20increasing%0Aorders.%20We%20can%20consider%20the%20two-phase%20phenomenon%20as%20the%20starting%20point%20of%20a%20DNN%0Alearning%20over-fitted%20features.%20Such%20a%20phenomenon%20has%20been%20widely%20shared%20by%20DNNs%0Awith%20various%20architectures%20trained%20for%20different%20tasks.%20Therefore%2C%20the%0Adiscovery%20of%20the%20two-phase%20dynamics%20provides%20a%20detailed%20mechanism%20for%20how%20a%20DNN%0Agradually%20learns%20different%20inference%20patterns%20%28interactions%29.%20In%20particular%2C%20we%0Ahave%20also%20verified%20the%20claim%20that%20high-order%20interactions%20have%20weaker%0Ageneralization%20power%20than%20low-order%20interactions.%20Thus%2C%20the%20discovered%0Atwo-phase%20dynamics%20also%20explains%20how%20the%20generalization%20power%20of%20a%20DNN%20changes%0Aduring%20the%20training%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Phase%2520Dynamics%2520of%2520Interactions%2520Explains%2520the%2520Starting%2520Point%2520of%2520a%2520DNN%250A%2520%2520Learning%2520Over-Fitted%2520Features%26entry.906535625%3DJunpeng%2520Zhang%2520and%2520Qing%2520Li%2520and%2520Liang%2520Lin%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520dynamics%2520of%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520learning%250Ainteractions.%2520Previous%2520studies%2520have%2520discovered%2520and%2520mathematically%2520proven%2520that%250Agiven%2520each%2520input%2520sample%252C%2520a%2520well-trained%2520DNN%2520usually%2520only%2520encodes%2520a%2520small%2520number%250Aof%2520interactions%2520%2528non-linear%2520relationships%2529%2520between%2520input%2520variables%2520in%2520the%250Asample.%2520A%2520series%2520of%2520theorems%2520have%2520been%2520derived%2520to%2520prove%2520that%2520we%2520can%2520consider%250Athe%2520DNN%2527s%2520inference%2520equivalent%2520to%2520using%2520these%2520interactions%2520as%2520primitive%250Apatterns%2520for%2520inference.%2520In%2520this%2520paper%252C%2520we%2520discover%2520the%2520DNN%2520learns%2520interactions%250Ain%2520two%2520phases.%2520The%2520first%2520phase%2520mainly%2520penalizes%2520interactions%2520of%2520medium%2520and%2520high%250Aorders%252C%2520and%2520the%2520second%2520phase%2520mainly%2520learns%2520interactions%2520of%2520gradually%2520increasing%250Aorders.%2520We%2520can%2520consider%2520the%2520two-phase%2520phenomenon%2520as%2520the%2520starting%2520point%2520of%2520a%2520DNN%250Alearning%2520over-fitted%2520features.%2520Such%2520a%2520phenomenon%2520has%2520been%2520widely%2520shared%2520by%2520DNNs%250Awith%2520various%2520architectures%2520trained%2520for%2520different%2520tasks.%2520Therefore%252C%2520the%250Adiscovery%2520of%2520the%2520two-phase%2520dynamics%2520provides%2520a%2520detailed%2520mechanism%2520for%2520how%2520a%2520DNN%250Agradually%2520learns%2520different%2520inference%2520patterns%2520%2528interactions%2529.%2520In%2520particular%252C%2520we%250Ahave%2520also%2520verified%2520the%2520claim%2520that%2520high-order%2520interactions%2520have%2520weaker%250Ageneralization%2520power%2520than%2520low-order%2520interactions.%2520Thus%252C%2520the%2520discovered%250Atwo-phase%2520dynamics%2520also%2520explains%2520how%2520the%2520generalization%2520power%2520of%2520a%2520DNN%2520changes%250Aduring%2520the%2520training%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Phase%20Dynamics%20of%20Interactions%20Explains%20the%20Starting%20Point%20of%20a%20DNN%0A%20%20Learning%20Over-Fitted%20Features&entry.906535625=Junpeng%20Zhang%20and%20Qing%20Li%20and%20Liang%20Lin%20and%20Quanshi%20Zhang&entry.1292438233=%20%20This%20paper%20investigates%20the%20dynamics%20of%20a%20deep%20neural%20network%20%28DNN%29%20learning%0Ainteractions.%20Previous%20studies%20have%20discovered%20and%20mathematically%20proven%20that%0Agiven%20each%20input%20sample%2C%20a%20well-trained%20DNN%20usually%20only%20encodes%20a%20small%20number%0Aof%20interactions%20%28non-linear%20relationships%29%20between%20input%20variables%20in%20the%0Asample.%20A%20series%20of%20theorems%20have%20been%20derived%20to%20prove%20that%20we%20can%20consider%0Athe%20DNN%27s%20inference%20equivalent%20to%20using%20these%20interactions%20as%20primitive%0Apatterns%20for%20inference.%20In%20this%20paper%2C%20we%20discover%20the%20DNN%20learns%20interactions%0Ain%20two%20phases.%20The%20first%20phase%20mainly%20penalizes%20interactions%20of%20medium%20and%20high%0Aorders%2C%20and%20the%20second%20phase%20mainly%20learns%20interactions%20of%20gradually%20increasing%0Aorders.%20We%20can%20consider%20the%20two-phase%20phenomenon%20as%20the%20starting%20point%20of%20a%20DNN%0Alearning%20over-fitted%20features.%20Such%20a%20phenomenon%20has%20been%20widely%20shared%20by%20DNNs%0Awith%20various%20architectures%20trained%20for%20different%20tasks.%20Therefore%2C%20the%0Adiscovery%20of%20the%20two-phase%20dynamics%20provides%20a%20detailed%20mechanism%20for%20how%20a%20DNN%0Agradually%20learns%20different%20inference%20patterns%20%28interactions%29.%20In%20particular%2C%20we%0Ahave%20also%20verified%20the%20claim%20that%20high-order%20interactions%20have%20weaker%0Ageneralization%20power%20than%20low-order%20interactions.%20Thus%2C%20the%20discovered%0Atwo-phase%20dynamics%20also%20explains%20how%20the%20generalization%20power%20of%20a%20DNN%20changes%0Aduring%20the%20training%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10262v1&entry.124074799=Read"},
{"title": "Patient-Specific Real-Time Segmentation in Trackerless Brain Ultrasound", "author": "Reuben Dorent and Erickson Torio and Nazim Haouchine and Colin Galvin and Sarah Frisken and Alexandra Golby and Tina Kapur and William Wells", "abstract": "  Intraoperative ultrasound (iUS) imaging has the potential to improve surgical\noutcomes in brain surgery. However, its interpretation is challenging, even for\nexpert neurosurgeons. In this work, we designed the first patient-specific\nframework that performs brain tumor segmentation in trackerless iUS. To\ndisambiguate ultrasound imaging and adapt to the neurosurgeon's surgical\nobjective, a patient-specific real-time network is trained using synthetic\nultrasound data generated by simulating virtual iUS sweep acquisitions in\npre-operative MR data. Extensive experiments performed in real ultrasound data\ndemonstrate the effectiveness of the proposed approach, allowing for adapting\nto the surgeon's definition of surgical targets and outperforming\nnon-patient-specific models, neurosurgeon experts, and high-end tracking\nsystems. Our code is available at: \\url{https://github.com/ReubenDo/MHVAE-Seg}.\n", "link": "http://arxiv.org/abs/2405.09959v1", "date": "2024-05-16", "relevancy": 1.958, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4988}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patient-Specific%20Real-Time%20Segmentation%20in%20Trackerless%20Brain%20Ultrasound&body=Title%3A%20Patient-Specific%20Real-Time%20Segmentation%20in%20Trackerless%20Brain%20Ultrasound%0AAuthor%3A%20Reuben%20Dorent%20and%20Erickson%20Torio%20and%20Nazim%20Haouchine%20and%20Colin%20Galvin%20and%20Sarah%20Frisken%20and%20Alexandra%20Golby%20and%20Tina%20Kapur%20and%20William%20Wells%0AAbstract%3A%20%20%20Intraoperative%20ultrasound%20%28iUS%29%20imaging%20has%20the%20potential%20to%20improve%20surgical%0Aoutcomes%20in%20brain%20surgery.%20However%2C%20its%20interpretation%20is%20challenging%2C%20even%20for%0Aexpert%20neurosurgeons.%20In%20this%20work%2C%20we%20designed%20the%20first%20patient-specific%0Aframework%20that%20performs%20brain%20tumor%20segmentation%20in%20trackerless%20iUS.%20To%0Adisambiguate%20ultrasound%20imaging%20and%20adapt%20to%20the%20neurosurgeon%27s%20surgical%0Aobjective%2C%20a%20patient-specific%20real-time%20network%20is%20trained%20using%20synthetic%0Aultrasound%20data%20generated%20by%20simulating%20virtual%20iUS%20sweep%20acquisitions%20in%0Apre-operative%20MR%20data.%20Extensive%20experiments%20performed%20in%20real%20ultrasound%20data%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20allowing%20for%20adapting%0Ato%20the%20surgeon%27s%20definition%20of%20surgical%20targets%20and%20outperforming%0Anon-patient-specific%20models%2C%20neurosurgeon%20experts%2C%20and%20high-end%20tracking%0Asystems.%20Our%20code%20is%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/ReubenDo/MHVAE-Seg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatient-Specific%2520Real-Time%2520Segmentation%2520in%2520Trackerless%2520Brain%2520Ultrasound%26entry.906535625%3DReuben%2520Dorent%2520and%2520Erickson%2520Torio%2520and%2520Nazim%2520Haouchine%2520and%2520Colin%2520Galvin%2520and%2520Sarah%2520Frisken%2520and%2520Alexandra%2520Golby%2520and%2520Tina%2520Kapur%2520and%2520William%2520Wells%26entry.1292438233%3D%2520%2520Intraoperative%2520ultrasound%2520%2528iUS%2529%2520imaging%2520has%2520the%2520potential%2520to%2520improve%2520surgical%250Aoutcomes%2520in%2520brain%2520surgery.%2520However%252C%2520its%2520interpretation%2520is%2520challenging%252C%2520even%2520for%250Aexpert%2520neurosurgeons.%2520In%2520this%2520work%252C%2520we%2520designed%2520the%2520first%2520patient-specific%250Aframework%2520that%2520performs%2520brain%2520tumor%2520segmentation%2520in%2520trackerless%2520iUS.%2520To%250Adisambiguate%2520ultrasound%2520imaging%2520and%2520adapt%2520to%2520the%2520neurosurgeon%2527s%2520surgical%250Aobjective%252C%2520a%2520patient-specific%2520real-time%2520network%2520is%2520trained%2520using%2520synthetic%250Aultrasound%2520data%2520generated%2520by%2520simulating%2520virtual%2520iUS%2520sweep%2520acquisitions%2520in%250Apre-operative%2520MR%2520data.%2520Extensive%2520experiments%2520performed%2520in%2520real%2520ultrasound%2520data%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%252C%2520allowing%2520for%2520adapting%250Ato%2520the%2520surgeon%2527s%2520definition%2520of%2520surgical%2520targets%2520and%2520outperforming%250Anon-patient-specific%2520models%252C%2520neurosurgeon%2520experts%252C%2520and%2520high-end%2520tracking%250Asystems.%2520Our%2520code%2520is%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/ReubenDo/MHVAE-Seg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patient-Specific%20Real-Time%20Segmentation%20in%20Trackerless%20Brain%20Ultrasound&entry.906535625=Reuben%20Dorent%20and%20Erickson%20Torio%20and%20Nazim%20Haouchine%20and%20Colin%20Galvin%20and%20Sarah%20Frisken%20and%20Alexandra%20Golby%20and%20Tina%20Kapur%20and%20William%20Wells&entry.1292438233=%20%20Intraoperative%20ultrasound%20%28iUS%29%20imaging%20has%20the%20potential%20to%20improve%20surgical%0Aoutcomes%20in%20brain%20surgery.%20However%2C%20its%20interpretation%20is%20challenging%2C%20even%20for%0Aexpert%20neurosurgeons.%20In%20this%20work%2C%20we%20designed%20the%20first%20patient-specific%0Aframework%20that%20performs%20brain%20tumor%20segmentation%20in%20trackerless%20iUS.%20To%0Adisambiguate%20ultrasound%20imaging%20and%20adapt%20to%20the%20neurosurgeon%27s%20surgical%0Aobjective%2C%20a%20patient-specific%20real-time%20network%20is%20trained%20using%20synthetic%0Aultrasound%20data%20generated%20by%20simulating%20virtual%20iUS%20sweep%20acquisitions%20in%0Apre-operative%20MR%20data.%20Extensive%20experiments%20performed%20in%20real%20ultrasound%20data%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20allowing%20for%20adapting%0Ato%20the%20surgeon%27s%20definition%20of%20surgical%20targets%20and%20outperforming%0Anon-patient-specific%20models%2C%20neurosurgeon%20experts%2C%20and%20high-end%20tracking%0Asystems.%20Our%20code%20is%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/ReubenDo/MHVAE-Seg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09959v1&entry.124074799=Read"},
{"title": "Timeline-based Sentence Decomposition with In-Context Learning for\n  Temporal Fact Extraction", "author": "Jianhao Chen and Haoyuan Ouyang and Junyang Ren and Wentao Ding and Wei Hu and Yuzhong Qu", "abstract": "  Facts extraction is pivotal for constructing knowledge graphs. Recently, the\nincreasing demand for temporal facts in downstream tasks has led to the\nemergence of the task of temporal fact extraction. In this paper, we\nspecifically address the extraction of temporal facts from natural language\ntext. Previous studies fail to handle the challenge of establishing\ntime-to-fact correspondences in complex sentences. To overcome this hurdle, we\npropose a timeline-based sentence decomposition strategy using large language\nmodels (LLMs) with in-context learning, ensuring a fine-grained understanding\nof the timeline associated with various facts. In addition, we evaluate the\nperformance of LLMs for direct temporal fact extraction and get unsatisfactory\nresults. To this end, we introduce TSDRE, a method that incorporates the\ndecomposition capabilities of LLMs into the traditional fine-tuning of smaller\npre-trained language models (PLMs). To support the evaluation, we construct\nComplexTRED, a complex temporal fact extraction dataset. Our experiments show\nthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal and\nComplexTRED datasets.\n", "link": "http://arxiv.org/abs/2405.10288v1", "date": "2024-05-16", "relevancy": 1.9522, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4834}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timeline-based%20Sentence%20Decomposition%20with%20In-Context%20Learning%20for%0A%20%20Temporal%20Fact%20Extraction&body=Title%3A%20Timeline-based%20Sentence%20Decomposition%20with%20In-Context%20Learning%20for%0A%20%20Temporal%20Fact%20Extraction%0AAuthor%3A%20Jianhao%20Chen%20and%20Haoyuan%20Ouyang%20and%20Junyang%20Ren%20and%20Wentao%20Ding%20and%20Wei%20Hu%20and%20Yuzhong%20Qu%0AAbstract%3A%20%20%20Facts%20extraction%20is%20pivotal%20for%20constructing%20knowledge%20graphs.%20Recently%2C%20the%0Aincreasing%20demand%20for%20temporal%20facts%20in%20downstream%20tasks%20has%20led%20to%20the%0Aemergence%20of%20the%20task%20of%20temporal%20fact%20extraction.%20In%20this%20paper%2C%20we%0Aspecifically%20address%20the%20extraction%20of%20temporal%20facts%20from%20natural%20language%0Atext.%20Previous%20studies%20fail%20to%20handle%20the%20challenge%20of%20establishing%0Atime-to-fact%20correspondences%20in%20complex%20sentences.%20To%20overcome%20this%20hurdle%2C%20we%0Apropose%20a%20timeline-based%20sentence%20decomposition%20strategy%20using%20large%20language%0Amodels%20%28LLMs%29%20with%20in-context%20learning%2C%20ensuring%20a%20fine-grained%20understanding%0Aof%20the%20timeline%20associated%20with%20various%20facts.%20In%20addition%2C%20we%20evaluate%20the%0Aperformance%20of%20LLMs%20for%20direct%20temporal%20fact%20extraction%20and%20get%20unsatisfactory%0Aresults.%20To%20this%20end%2C%20we%20introduce%20TSDRE%2C%20a%20method%20that%20incorporates%20the%0Adecomposition%20capabilities%20of%20LLMs%20into%20the%20traditional%20fine-tuning%20of%20smaller%0Apre-trained%20language%20models%20%28PLMs%29.%20To%20support%20the%20evaluation%2C%20we%20construct%0AComplexTRED%2C%20a%20complex%20temporal%20fact%20extraction%20dataset.%20Our%20experiments%20show%0Athat%20TSDRE%20achieves%20state-of-the-art%20results%20on%20both%20HyperRED-Temporal%20and%0AComplexTRED%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeline-based%2520Sentence%2520Decomposition%2520with%2520In-Context%2520Learning%2520for%250A%2520%2520Temporal%2520Fact%2520Extraction%26entry.906535625%3DJianhao%2520Chen%2520and%2520Haoyuan%2520Ouyang%2520and%2520Junyang%2520Ren%2520and%2520Wentao%2520Ding%2520and%2520Wei%2520Hu%2520and%2520Yuzhong%2520Qu%26entry.1292438233%3D%2520%2520Facts%2520extraction%2520is%2520pivotal%2520for%2520constructing%2520knowledge%2520graphs.%2520Recently%252C%2520the%250Aincreasing%2520demand%2520for%2520temporal%2520facts%2520in%2520downstream%2520tasks%2520has%2520led%2520to%2520the%250Aemergence%2520of%2520the%2520task%2520of%2520temporal%2520fact%2520extraction.%2520In%2520this%2520paper%252C%2520we%250Aspecifically%2520address%2520the%2520extraction%2520of%2520temporal%2520facts%2520from%2520natural%2520language%250Atext.%2520Previous%2520studies%2520fail%2520to%2520handle%2520the%2520challenge%2520of%2520establishing%250Atime-to-fact%2520correspondences%2520in%2520complex%2520sentences.%2520To%2520overcome%2520this%2520hurdle%252C%2520we%250Apropose%2520a%2520timeline-based%2520sentence%2520decomposition%2520strategy%2520using%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520in-context%2520learning%252C%2520ensuring%2520a%2520fine-grained%2520understanding%250Aof%2520the%2520timeline%2520associated%2520with%2520various%2520facts.%2520In%2520addition%252C%2520we%2520evaluate%2520the%250Aperformance%2520of%2520LLMs%2520for%2520direct%2520temporal%2520fact%2520extraction%2520and%2520get%2520unsatisfactory%250Aresults.%2520To%2520this%2520end%252C%2520we%2520introduce%2520TSDRE%252C%2520a%2520method%2520that%2520incorporates%2520the%250Adecomposition%2520capabilities%2520of%2520LLMs%2520into%2520the%2520traditional%2520fine-tuning%2520of%2520smaller%250Apre-trained%2520language%2520models%2520%2528PLMs%2529.%2520To%2520support%2520the%2520evaluation%252C%2520we%2520construct%250AComplexTRED%252C%2520a%2520complex%2520temporal%2520fact%2520extraction%2520dataset.%2520Our%2520experiments%2520show%250Athat%2520TSDRE%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520HyperRED-Temporal%2520and%250AComplexTRED%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timeline-based%20Sentence%20Decomposition%20with%20In-Context%20Learning%20for%0A%20%20Temporal%20Fact%20Extraction&entry.906535625=Jianhao%20Chen%20and%20Haoyuan%20Ouyang%20and%20Junyang%20Ren%20and%20Wentao%20Ding%20and%20Wei%20Hu%20and%20Yuzhong%20Qu&entry.1292438233=%20%20Facts%20extraction%20is%20pivotal%20for%20constructing%20knowledge%20graphs.%20Recently%2C%20the%0Aincreasing%20demand%20for%20temporal%20facts%20in%20downstream%20tasks%20has%20led%20to%20the%0Aemergence%20of%20the%20task%20of%20temporal%20fact%20extraction.%20In%20this%20paper%2C%20we%0Aspecifically%20address%20the%20extraction%20of%20temporal%20facts%20from%20natural%20language%0Atext.%20Previous%20studies%20fail%20to%20handle%20the%20challenge%20of%20establishing%0Atime-to-fact%20correspondences%20in%20complex%20sentences.%20To%20overcome%20this%20hurdle%2C%20we%0Apropose%20a%20timeline-based%20sentence%20decomposition%20strategy%20using%20large%20language%0Amodels%20%28LLMs%29%20with%20in-context%20learning%2C%20ensuring%20a%20fine-grained%20understanding%0Aof%20the%20timeline%20associated%20with%20various%20facts.%20In%20addition%2C%20we%20evaluate%20the%0Aperformance%20of%20LLMs%20for%20direct%20temporal%20fact%20extraction%20and%20get%20unsatisfactory%0Aresults.%20To%20this%20end%2C%20we%20introduce%20TSDRE%2C%20a%20method%20that%20incorporates%20the%0Adecomposition%20capabilities%20of%20LLMs%20into%20the%20traditional%20fine-tuning%20of%20smaller%0Apre-trained%20language%20models%20%28PLMs%29.%20To%20support%20the%20evaluation%2C%20we%20construct%0AComplexTRED%2C%20a%20complex%20temporal%20fact%20extraction%20dataset.%20Our%20experiments%20show%0Athat%20TSDRE%20achieves%20state-of-the-art%20results%20on%20both%20HyperRED-Temporal%20and%0AComplexTRED%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10288v1&entry.124074799=Read"},
{"title": "Conformal Alignment: Knowing When to Trust Foundation Models with\n  Guarantees", "author": "Yu Gui and Ying Jin and Zhimei Ren", "abstract": "  Before deploying outputs from foundation models in high-stakes tasks, it is\nimperative to ensure that they align with human values. For instance, in\nradiology report generation, reports generated by a vision-language model must\nalign with human evaluations before their use in medical decision-making. This\npaper presents Conformal Alignment, a general framework for identifying units\nwhose outputs meet a user-specified alignment criterion. It is guaranteed that\non average, a prescribed fraction of selected units indeed meet the alignment\ncriterion, regardless of the foundation model or the data distribution. Given\nany pre-trained model and new units with model-generated outputs, Conformal\nAlignment leverages a set of reference data with ground-truth alignment status\nto train an alignment predictor. It then selects new units whose predicted\nalignment scores surpass a data-dependent threshold, certifying their\ncorresponding outputs as trustworthy. Through applications to question\nanswering and radiology report generation, we demonstrate that our method is\nable to accurately identify units with trustworthy outputs via lightweight\ntraining over a moderate amount of reference data. En route, we investigate the\ninformativeness of various features in alignment prediction and combine them\nwith standard models to construct the alignment predictor.\n", "link": "http://arxiv.org/abs/2405.10301v1", "date": "2024-05-16", "relevancy": 1.9375, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4855}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Alignment%3A%20Knowing%20When%20to%20Trust%20Foundation%20Models%20with%0A%20%20Guarantees&body=Title%3A%20Conformal%20Alignment%3A%20Knowing%20When%20to%20Trust%20Foundation%20Models%20with%0A%20%20Guarantees%0AAuthor%3A%20Yu%20Gui%20and%20Ying%20Jin%20and%20Zhimei%20Ren%0AAbstract%3A%20%20%20Before%20deploying%20outputs%20from%20foundation%20models%20in%20high-stakes%20tasks%2C%20it%20is%0Aimperative%20to%20ensure%20that%20they%20align%20with%20human%20values.%20For%20instance%2C%20in%0Aradiology%20report%20generation%2C%20reports%20generated%20by%20a%20vision-language%20model%20must%0Aalign%20with%20human%20evaluations%20before%20their%20use%20in%20medical%20decision-making.%20This%0Apaper%20presents%20Conformal%20Alignment%2C%20a%20general%20framework%20for%20identifying%20units%0Awhose%20outputs%20meet%20a%20user-specified%20alignment%20criterion.%20It%20is%20guaranteed%20that%0Aon%20average%2C%20a%20prescribed%20fraction%20of%20selected%20units%20indeed%20meet%20the%20alignment%0Acriterion%2C%20regardless%20of%20the%20foundation%20model%20or%20the%20data%20distribution.%20Given%0Aany%20pre-trained%20model%20and%20new%20units%20with%20model-generated%20outputs%2C%20Conformal%0AAlignment%20leverages%20a%20set%20of%20reference%20data%20with%20ground-truth%20alignment%20status%0Ato%20train%20an%20alignment%20predictor.%20It%20then%20selects%20new%20units%20whose%20predicted%0Aalignment%20scores%20surpass%20a%20data-dependent%20threshold%2C%20certifying%20their%0Acorresponding%20outputs%20as%20trustworthy.%20Through%20applications%20to%20question%0Aanswering%20and%20radiology%20report%20generation%2C%20we%20demonstrate%20that%20our%20method%20is%0Aable%20to%20accurately%20identify%20units%20with%20trustworthy%20outputs%20via%20lightweight%0Atraining%20over%20a%20moderate%20amount%20of%20reference%20data.%20En%20route%2C%20we%20investigate%20the%0Ainformativeness%20of%20various%20features%20in%20alignment%20prediction%20and%20combine%20them%0Awith%20standard%20models%20to%20construct%20the%20alignment%20predictor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Alignment%253A%2520Knowing%2520When%2520to%2520Trust%2520Foundation%2520Models%2520with%250A%2520%2520Guarantees%26entry.906535625%3DYu%2520Gui%2520and%2520Ying%2520Jin%2520and%2520Zhimei%2520Ren%26entry.1292438233%3D%2520%2520Before%2520deploying%2520outputs%2520from%2520foundation%2520models%2520in%2520high-stakes%2520tasks%252C%2520it%2520is%250Aimperative%2520to%2520ensure%2520that%2520they%2520align%2520with%2520human%2520values.%2520For%2520instance%252C%2520in%250Aradiology%2520report%2520generation%252C%2520reports%2520generated%2520by%2520a%2520vision-language%2520model%2520must%250Aalign%2520with%2520human%2520evaluations%2520before%2520their%2520use%2520in%2520medical%2520decision-making.%2520This%250Apaper%2520presents%2520Conformal%2520Alignment%252C%2520a%2520general%2520framework%2520for%2520identifying%2520units%250Awhose%2520outputs%2520meet%2520a%2520user-specified%2520alignment%2520criterion.%2520It%2520is%2520guaranteed%2520that%250Aon%2520average%252C%2520a%2520prescribed%2520fraction%2520of%2520selected%2520units%2520indeed%2520meet%2520the%2520alignment%250Acriterion%252C%2520regardless%2520of%2520the%2520foundation%2520model%2520or%2520the%2520data%2520distribution.%2520Given%250Aany%2520pre-trained%2520model%2520and%2520new%2520units%2520with%2520model-generated%2520outputs%252C%2520Conformal%250AAlignment%2520leverages%2520a%2520set%2520of%2520reference%2520data%2520with%2520ground-truth%2520alignment%2520status%250Ato%2520train%2520an%2520alignment%2520predictor.%2520It%2520then%2520selects%2520new%2520units%2520whose%2520predicted%250Aalignment%2520scores%2520surpass%2520a%2520data-dependent%2520threshold%252C%2520certifying%2520their%250Acorresponding%2520outputs%2520as%2520trustworthy.%2520Through%2520applications%2520to%2520question%250Aanswering%2520and%2520radiology%2520report%2520generation%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520is%250Aable%2520to%2520accurately%2520identify%2520units%2520with%2520trustworthy%2520outputs%2520via%2520lightweight%250Atraining%2520over%2520a%2520moderate%2520amount%2520of%2520reference%2520data.%2520En%2520route%252C%2520we%2520investigate%2520the%250Ainformativeness%2520of%2520various%2520features%2520in%2520alignment%2520prediction%2520and%2520combine%2520them%250Awith%2520standard%2520models%2520to%2520construct%2520the%2520alignment%2520predictor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Alignment%3A%20Knowing%20When%20to%20Trust%20Foundation%20Models%20with%0A%20%20Guarantees&entry.906535625=Yu%20Gui%20and%20Ying%20Jin%20and%20Zhimei%20Ren&entry.1292438233=%20%20Before%20deploying%20outputs%20from%20foundation%20models%20in%20high-stakes%20tasks%2C%20it%20is%0Aimperative%20to%20ensure%20that%20they%20align%20with%20human%20values.%20For%20instance%2C%20in%0Aradiology%20report%20generation%2C%20reports%20generated%20by%20a%20vision-language%20model%20must%0Aalign%20with%20human%20evaluations%20before%20their%20use%20in%20medical%20decision-making.%20This%0Apaper%20presents%20Conformal%20Alignment%2C%20a%20general%20framework%20for%20identifying%20units%0Awhose%20outputs%20meet%20a%20user-specified%20alignment%20criterion.%20It%20is%20guaranteed%20that%0Aon%20average%2C%20a%20prescribed%20fraction%20of%20selected%20units%20indeed%20meet%20the%20alignment%0Acriterion%2C%20regardless%20of%20the%20foundation%20model%20or%20the%20data%20distribution.%20Given%0Aany%20pre-trained%20model%20and%20new%20units%20with%20model-generated%20outputs%2C%20Conformal%0AAlignment%20leverages%20a%20set%20of%20reference%20data%20with%20ground-truth%20alignment%20status%0Ato%20train%20an%20alignment%20predictor.%20It%20then%20selects%20new%20units%20whose%20predicted%0Aalignment%20scores%20surpass%20a%20data-dependent%20threshold%2C%20certifying%20their%0Acorresponding%20outputs%20as%20trustworthy.%20Through%20applications%20to%20question%0Aanswering%20and%20radiology%20report%20generation%2C%20we%20demonstrate%20that%20our%20method%20is%0Aable%20to%20accurately%20identify%20units%20with%20trustworthy%20outputs%20via%20lightweight%0Atraining%20over%20a%20moderate%20amount%20of%20reference%20data.%20En%20route%2C%20we%20investigate%20the%0Ainformativeness%20of%20various%20features%20in%20alignment%20prediction%20and%20combine%20them%0Awith%20standard%20models%20to%20construct%20the%20alignment%20predictor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10301v1&entry.124074799=Read"},
{"title": "A blind spot for large language models: Supradiegetic linguistic\n  information", "author": "Julia Witte Zimmerman and Denis Hudon and Kathryn Cramer and Jonathan St. Onge and Mikaela Fudolig and Milo Z. Trujillo and Christopher M. Danforth and Peter Sheridan Dodds", "abstract": "  Large Language Models (LLMs) like ChatGPT reflect profound changes in the\nfield of Artificial Intelligence, achieving a linguistic fluency that is\nimpressively, even shockingly, human-like. The extent of their current and\npotential capabilities is an active area of investigation by no means limited\nto scientific researchers. It is common for people to frame the training data\nfor LLMs as \"text\" or even \"language\". We examine the details of this framing\nusing ideas from several areas, including linguistics, embodied cognition,\ncognitive science, mathematics, and history. We propose that considering what\nit is like to be an LLM like ChatGPT, as Nagel might have put it, can help us\ngain insight into its capabilities in general, and in particular, that its\nexposure to linguistic training data can be productively reframed as exposure\nto the diegetic information encoded in language, and its deficits can be\nreframed as ignorance of extradiegetic information, including supradiegetic\nlinguistic information. Supradiegetic linguistic information consists of those\narbitrary aspects of the physical form of language that are not derivable from\nthe one-dimensional relations of context -- frequency, adjacency, proximity,\nco-occurrence -- that LLMs like ChatGPT have access to. Roughly speaking, the\ndiegetic portion of a word can be thought of as its function, its meaning, as\nthe information in a theoretical vector in a word embedding, while the\nsupradiegetic portion of the word can be thought of as its form, like the\nshapes of its letters or the sounds of its syllables. We use these concepts to\ninvestigate why LLMs like ChatGPT have trouble handling palindromes, the visual\ncharacteristics of symbols, translating Sumerian cuneiform, and continuing\ninteger sequences.\n", "link": "http://arxiv.org/abs/2306.06794v3", "date": "2024-05-16", "relevancy": 1.9204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4717}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20blind%20spot%20for%20large%20language%20models%3A%20Supradiegetic%20linguistic%0A%20%20information&body=Title%3A%20A%20blind%20spot%20for%20large%20language%20models%3A%20Supradiegetic%20linguistic%0A%20%20information%0AAuthor%3A%20Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Jonathan%20St.%20Onge%20and%20Mikaela%20Fudolig%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20reflect%20profound%20changes%20in%20the%0Afield%20of%20Artificial%20Intelligence%2C%20achieving%20a%20linguistic%20fluency%20that%20is%0Aimpressively%2C%20even%20shockingly%2C%20human-like.%20The%20extent%20of%20their%20current%20and%0Apotential%20capabilities%20is%20an%20active%20area%20of%20investigation%20by%20no%20means%20limited%0Ato%20scientific%20researchers.%20It%20is%20common%20for%20people%20to%20frame%20the%20training%20data%0Afor%20LLMs%20as%20%22text%22%20or%20even%20%22language%22.%20We%20examine%20the%20details%20of%20this%20framing%0Ausing%20ideas%20from%20several%20areas%2C%20including%20linguistics%2C%20embodied%20cognition%2C%0Acognitive%20science%2C%20mathematics%2C%20and%20history.%20We%20propose%20that%20considering%20what%0Ait%20is%20like%20to%20be%20an%20LLM%20like%20ChatGPT%2C%20as%20Nagel%20might%20have%20put%20it%2C%20can%20help%20us%0Again%20insight%20into%20its%20capabilities%20in%20general%2C%20and%20in%20particular%2C%20that%20its%0Aexposure%20to%20linguistic%20training%20data%20can%20be%20productively%20reframed%20as%20exposure%0Ato%20the%20diegetic%20information%20encoded%20in%20language%2C%20and%20its%20deficits%20can%20be%0Areframed%20as%20ignorance%20of%20extradiegetic%20information%2C%20including%20supradiegetic%0Alinguistic%20information.%20Supradiegetic%20linguistic%20information%20consists%20of%20those%0Aarbitrary%20aspects%20of%20the%20physical%20form%20of%20language%20that%20are%20not%20derivable%20from%0Athe%20one-dimensional%20relations%20of%20context%20--%20frequency%2C%20adjacency%2C%20proximity%2C%0Aco-occurrence%20--%20that%20LLMs%20like%20ChatGPT%20have%20access%20to.%20Roughly%20speaking%2C%20the%0Adiegetic%20portion%20of%20a%20word%20can%20be%20thought%20of%20as%20its%20function%2C%20its%20meaning%2C%20as%0Athe%20information%20in%20a%20theoretical%20vector%20in%20a%20word%20embedding%2C%20while%20the%0Asupradiegetic%20portion%20of%20the%20word%20can%20be%20thought%20of%20as%20its%20form%2C%20like%20the%0Ashapes%20of%20its%20letters%20or%20the%20sounds%20of%20its%20syllables.%20We%20use%20these%20concepts%20to%0Ainvestigate%20why%20LLMs%20like%20ChatGPT%20have%20trouble%20handling%20palindromes%2C%20the%20visual%0Acharacteristics%20of%20symbols%2C%20translating%20Sumerian%20cuneiform%2C%20and%20continuing%0Ainteger%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06794v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520blind%2520spot%2520for%2520large%2520language%2520models%253A%2520Supradiegetic%2520linguistic%250A%2520%2520information%26entry.906535625%3DJulia%2520Witte%2520Zimmerman%2520and%2520Denis%2520Hudon%2520and%2520Kathryn%2520Cramer%2520and%2520Jonathan%2520St.%2520Onge%2520and%2520Mikaela%2520Fudolig%2520and%2520Milo%2520Z.%2520Trujillo%2520and%2520Christopher%2520M.%2520Danforth%2520and%2520Peter%2520Sheridan%2520Dodds%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520reflect%2520profound%2520changes%2520in%2520the%250Afield%2520of%2520Artificial%2520Intelligence%252C%2520achieving%2520a%2520linguistic%2520fluency%2520that%2520is%250Aimpressively%252C%2520even%2520shockingly%252C%2520human-like.%2520The%2520extent%2520of%2520their%2520current%2520and%250Apotential%2520capabilities%2520is%2520an%2520active%2520area%2520of%2520investigation%2520by%2520no%2520means%2520limited%250Ato%2520scientific%2520researchers.%2520It%2520is%2520common%2520for%2520people%2520to%2520frame%2520the%2520training%2520data%250Afor%2520LLMs%2520as%2520%2522text%2522%2520or%2520even%2520%2522language%2522.%2520We%2520examine%2520the%2520details%2520of%2520this%2520framing%250Ausing%2520ideas%2520from%2520several%2520areas%252C%2520including%2520linguistics%252C%2520embodied%2520cognition%252C%250Acognitive%2520science%252C%2520mathematics%252C%2520and%2520history.%2520We%2520propose%2520that%2520considering%2520what%250Ait%2520is%2520like%2520to%2520be%2520an%2520LLM%2520like%2520ChatGPT%252C%2520as%2520Nagel%2520might%2520have%2520put%2520it%252C%2520can%2520help%2520us%250Again%2520insight%2520into%2520its%2520capabilities%2520in%2520general%252C%2520and%2520in%2520particular%252C%2520that%2520its%250Aexposure%2520to%2520linguistic%2520training%2520data%2520can%2520be%2520productively%2520reframed%2520as%2520exposure%250Ato%2520the%2520diegetic%2520information%2520encoded%2520in%2520language%252C%2520and%2520its%2520deficits%2520can%2520be%250Areframed%2520as%2520ignorance%2520of%2520extradiegetic%2520information%252C%2520including%2520supradiegetic%250Alinguistic%2520information.%2520Supradiegetic%2520linguistic%2520information%2520consists%2520of%2520those%250Aarbitrary%2520aspects%2520of%2520the%2520physical%2520form%2520of%2520language%2520that%2520are%2520not%2520derivable%2520from%250Athe%2520one-dimensional%2520relations%2520of%2520context%2520--%2520frequency%252C%2520adjacency%252C%2520proximity%252C%250Aco-occurrence%2520--%2520that%2520LLMs%2520like%2520ChatGPT%2520have%2520access%2520to.%2520Roughly%2520speaking%252C%2520the%250Adiegetic%2520portion%2520of%2520a%2520word%2520can%2520be%2520thought%2520of%2520as%2520its%2520function%252C%2520its%2520meaning%252C%2520as%250Athe%2520information%2520in%2520a%2520theoretical%2520vector%2520in%2520a%2520word%2520embedding%252C%2520while%2520the%250Asupradiegetic%2520portion%2520of%2520the%2520word%2520can%2520be%2520thought%2520of%2520as%2520its%2520form%252C%2520like%2520the%250Ashapes%2520of%2520its%2520letters%2520or%2520the%2520sounds%2520of%2520its%2520syllables.%2520We%2520use%2520these%2520concepts%2520to%250Ainvestigate%2520why%2520LLMs%2520like%2520ChatGPT%2520have%2520trouble%2520handling%2520palindromes%252C%2520the%2520visual%250Acharacteristics%2520of%2520symbols%252C%2520translating%2520Sumerian%2520cuneiform%252C%2520and%2520continuing%250Ainteger%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06794v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20blind%20spot%20for%20large%20language%20models%3A%20Supradiegetic%20linguistic%0A%20%20information&entry.906535625=Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Jonathan%20St.%20Onge%20and%20Mikaela%20Fudolig%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20like%20ChatGPT%20reflect%20profound%20changes%20in%20the%0Afield%20of%20Artificial%20Intelligence%2C%20achieving%20a%20linguistic%20fluency%20that%20is%0Aimpressively%2C%20even%20shockingly%2C%20human-like.%20The%20extent%20of%20their%20current%20and%0Apotential%20capabilities%20is%20an%20active%20area%20of%20investigation%20by%20no%20means%20limited%0Ato%20scientific%20researchers.%20It%20is%20common%20for%20people%20to%20frame%20the%20training%20data%0Afor%20LLMs%20as%20%22text%22%20or%20even%20%22language%22.%20We%20examine%20the%20details%20of%20this%20framing%0Ausing%20ideas%20from%20several%20areas%2C%20including%20linguistics%2C%20embodied%20cognition%2C%0Acognitive%20science%2C%20mathematics%2C%20and%20history.%20We%20propose%20that%20considering%20what%0Ait%20is%20like%20to%20be%20an%20LLM%20like%20ChatGPT%2C%20as%20Nagel%20might%20have%20put%20it%2C%20can%20help%20us%0Again%20insight%20into%20its%20capabilities%20in%20general%2C%20and%20in%20particular%2C%20that%20its%0Aexposure%20to%20linguistic%20training%20data%20can%20be%20productively%20reframed%20as%20exposure%0Ato%20the%20diegetic%20information%20encoded%20in%20language%2C%20and%20its%20deficits%20can%20be%0Areframed%20as%20ignorance%20of%20extradiegetic%20information%2C%20including%20supradiegetic%0Alinguistic%20information.%20Supradiegetic%20linguistic%20information%20consists%20of%20those%0Aarbitrary%20aspects%20of%20the%20physical%20form%20of%20language%20that%20are%20not%20derivable%20from%0Athe%20one-dimensional%20relations%20of%20context%20--%20frequency%2C%20adjacency%2C%20proximity%2C%0Aco-occurrence%20--%20that%20LLMs%20like%20ChatGPT%20have%20access%20to.%20Roughly%20speaking%2C%20the%0Adiegetic%20portion%20of%20a%20word%20can%20be%20thought%20of%20as%20its%20function%2C%20its%20meaning%2C%20as%0Athe%20information%20in%20a%20theoretical%20vector%20in%20a%20word%20embedding%2C%20while%20the%0Asupradiegetic%20portion%20of%20the%20word%20can%20be%20thought%20of%20as%20its%20form%2C%20like%20the%0Ashapes%20of%20its%20letters%20or%20the%20sounds%20of%20its%20syllables.%20We%20use%20these%20concepts%20to%0Ainvestigate%20why%20LLMs%20like%20ChatGPT%20have%20trouble%20handling%20palindromes%2C%20the%20visual%0Acharacteristics%20of%20symbols%2C%20translating%20Sumerian%20cuneiform%2C%20and%20continuing%0Ainteger%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06794v3&entry.124074799=Read"},
{"title": "Dual-band feature selection for maturity classification of specialty\n  crops by hyperspectral imaging", "author": "Usman A. Zahidi and Krystian \u0141ukasik and Grzegorz Cielniak", "abstract": "  The maturity classification of specialty crops such as strawberries and\ntomatoes is an essential agricultural downstream activity for selective\nharvesting and quality control (QC) at production and packaging sites. Recent\nadvancements in Deep Learning (DL) have produced encouraging results in color\nimages for maturity classification applications. However, hyperspectral imaging\n(HSI) outperforms methods based on color vision. Multivariate analysis methods\nand Convolutional Neural Networks (CNN) deliver promising results; however, a\nlarge amount of input data and the associated preprocessing requirements cause\nhindrances in practical application. Conventionally, the reflectance intensity\nin a given electromagnetic spectrum is employed in estimating fruit maturity.\nWe present a feature extraction method to empirically demonstrate that the peak\nreflectance in subbands such as 500-670 nm (pigment band) and the wavelength of\nthe peak position, and contrarily, the trough reflectance and its corresponding\nwavelength within 671-790 nm (chlorophyll band) are convenient to compute yet\ndistinctive features for the maturity classification. The proposed feature\nselection method is beneficial because preprocessing, such as dimensionality\nreduction, is avoided before every prediction. The feature set is designed to\ncapture these traits. The best SOTA methods, among 3D-CNN, 1D-CNN, and SVM,\nachieve at most 90.0 % accuracy for strawberries and 92.0 % for tomatoes on our\ndataset. Results show that the proposed method outperforms the SOTA as it\nyields an accuracy above 98.0 % in strawberry and 96.0 % in tomato\nclassification. A comparative analysis of the time efficiency of these methods\nis also conducted, which shows the proposed method performs prediction at 13\nFrames Per Second (FPS) compared to the maximum 1.16 FPS attained by the\nfull-spectrum SVM classifier.\n", "link": "http://arxiv.org/abs/2405.09955v1", "date": "2024-05-16", "relevancy": 1.9194, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.494}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-band%20feature%20selection%20for%20maturity%20classification%20of%20specialty%0A%20%20crops%20by%20hyperspectral%20imaging&body=Title%3A%20Dual-band%20feature%20selection%20for%20maturity%20classification%20of%20specialty%0A%20%20crops%20by%20hyperspectral%20imaging%0AAuthor%3A%20Usman%20A.%20Zahidi%20and%20Krystian%20%C5%81ukasik%20and%20Grzegorz%20Cielniak%0AAbstract%3A%20%20%20The%20maturity%20classification%20of%20specialty%20crops%20such%20as%20strawberries%20and%0Atomatoes%20is%20an%20essential%20agricultural%20downstream%20activity%20for%20selective%0Aharvesting%20and%20quality%20control%20%28QC%29%20at%20production%20and%20packaging%20sites.%20Recent%0Aadvancements%20in%20Deep%20Learning%20%28DL%29%20have%20produced%20encouraging%20results%20in%20color%0Aimages%20for%20maturity%20classification%20applications.%20However%2C%20hyperspectral%20imaging%0A%28HSI%29%20outperforms%20methods%20based%20on%20color%20vision.%20Multivariate%20analysis%20methods%0Aand%20Convolutional%20Neural%20Networks%20%28CNN%29%20deliver%20promising%20results%3B%20however%2C%20a%0Alarge%20amount%20of%20input%20data%20and%20the%20associated%20preprocessing%20requirements%20cause%0Ahindrances%20in%20practical%20application.%20Conventionally%2C%20the%20reflectance%20intensity%0Ain%20a%20given%20electromagnetic%20spectrum%20is%20employed%20in%20estimating%20fruit%20maturity.%0AWe%20present%20a%20feature%20extraction%20method%20to%20empirically%20demonstrate%20that%20the%20peak%0Areflectance%20in%20subbands%20such%20as%20500-670%20nm%20%28pigment%20band%29%20and%20the%20wavelength%20of%0Athe%20peak%20position%2C%20and%20contrarily%2C%20the%20trough%20reflectance%20and%20its%20corresponding%0Awavelength%20within%20671-790%20nm%20%28chlorophyll%20band%29%20are%20convenient%20to%20compute%20yet%0Adistinctive%20features%20for%20the%20maturity%20classification.%20The%20proposed%20feature%0Aselection%20method%20is%20beneficial%20because%20preprocessing%2C%20such%20as%20dimensionality%0Areduction%2C%20is%20avoided%20before%20every%20prediction.%20The%20feature%20set%20is%20designed%20to%0Acapture%20these%20traits.%20The%20best%20SOTA%20methods%2C%20among%203D-CNN%2C%201D-CNN%2C%20and%20SVM%2C%0Aachieve%20at%20most%2090.0%20%25%20accuracy%20for%20strawberries%20and%2092.0%20%25%20for%20tomatoes%20on%20our%0Adataset.%20Results%20show%20that%20the%20proposed%20method%20outperforms%20the%20SOTA%20as%20it%0Ayields%20an%20accuracy%20above%2098.0%20%25%20in%20strawberry%20and%2096.0%20%25%20in%20tomato%0Aclassification.%20A%20comparative%20analysis%20of%20the%20time%20efficiency%20of%20these%20methods%0Ais%20also%20conducted%2C%20which%20shows%20the%20proposed%20method%20performs%20prediction%20at%2013%0AFrames%20Per%20Second%20%28FPS%29%20compared%20to%20the%20maximum%201.16%20FPS%20attained%20by%20the%0Afull-spectrum%20SVM%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-band%2520feature%2520selection%2520for%2520maturity%2520classification%2520of%2520specialty%250A%2520%2520crops%2520by%2520hyperspectral%2520imaging%26entry.906535625%3DUsman%2520A.%2520Zahidi%2520and%2520Krystian%2520%25C5%2581ukasik%2520and%2520Grzegorz%2520Cielniak%26entry.1292438233%3D%2520%2520The%2520maturity%2520classification%2520of%2520specialty%2520crops%2520such%2520as%2520strawberries%2520and%250Atomatoes%2520is%2520an%2520essential%2520agricultural%2520downstream%2520activity%2520for%2520selective%250Aharvesting%2520and%2520quality%2520control%2520%2528QC%2529%2520at%2520production%2520and%2520packaging%2520sites.%2520Recent%250Aadvancements%2520in%2520Deep%2520Learning%2520%2528DL%2529%2520have%2520produced%2520encouraging%2520results%2520in%2520color%250Aimages%2520for%2520maturity%2520classification%2520applications.%2520However%252C%2520hyperspectral%2520imaging%250A%2528HSI%2529%2520outperforms%2520methods%2520based%2520on%2520color%2520vision.%2520Multivariate%2520analysis%2520methods%250Aand%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%2520deliver%2520promising%2520results%253B%2520however%252C%2520a%250Alarge%2520amount%2520of%2520input%2520data%2520and%2520the%2520associated%2520preprocessing%2520requirements%2520cause%250Ahindrances%2520in%2520practical%2520application.%2520Conventionally%252C%2520the%2520reflectance%2520intensity%250Ain%2520a%2520given%2520electromagnetic%2520spectrum%2520is%2520employed%2520in%2520estimating%2520fruit%2520maturity.%250AWe%2520present%2520a%2520feature%2520extraction%2520method%2520to%2520empirically%2520demonstrate%2520that%2520the%2520peak%250Areflectance%2520in%2520subbands%2520such%2520as%2520500-670%2520nm%2520%2528pigment%2520band%2529%2520and%2520the%2520wavelength%2520of%250Athe%2520peak%2520position%252C%2520and%2520contrarily%252C%2520the%2520trough%2520reflectance%2520and%2520its%2520corresponding%250Awavelength%2520within%2520671-790%2520nm%2520%2528chlorophyll%2520band%2529%2520are%2520convenient%2520to%2520compute%2520yet%250Adistinctive%2520features%2520for%2520the%2520maturity%2520classification.%2520The%2520proposed%2520feature%250Aselection%2520method%2520is%2520beneficial%2520because%2520preprocessing%252C%2520such%2520as%2520dimensionality%250Areduction%252C%2520is%2520avoided%2520before%2520every%2520prediction.%2520The%2520feature%2520set%2520is%2520designed%2520to%250Acapture%2520these%2520traits.%2520The%2520best%2520SOTA%2520methods%252C%2520among%25203D-CNN%252C%25201D-CNN%252C%2520and%2520SVM%252C%250Aachieve%2520at%2520most%252090.0%2520%2525%2520accuracy%2520for%2520strawberries%2520and%252092.0%2520%2525%2520for%2520tomatoes%2520on%2520our%250Adataset.%2520Results%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%2520SOTA%2520as%2520it%250Ayields%2520an%2520accuracy%2520above%252098.0%2520%2525%2520in%2520strawberry%2520and%252096.0%2520%2525%2520in%2520tomato%250Aclassification.%2520A%2520comparative%2520analysis%2520of%2520the%2520time%2520efficiency%2520of%2520these%2520methods%250Ais%2520also%2520conducted%252C%2520which%2520shows%2520the%2520proposed%2520method%2520performs%2520prediction%2520at%252013%250AFrames%2520Per%2520Second%2520%2528FPS%2529%2520compared%2520to%2520the%2520maximum%25201.16%2520FPS%2520attained%2520by%2520the%250Afull-spectrum%2520SVM%2520classifier.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-band%20feature%20selection%20for%20maturity%20classification%20of%20specialty%0A%20%20crops%20by%20hyperspectral%20imaging&entry.906535625=Usman%20A.%20Zahidi%20and%20Krystian%20%C5%81ukasik%20and%20Grzegorz%20Cielniak&entry.1292438233=%20%20The%20maturity%20classification%20of%20specialty%20crops%20such%20as%20strawberries%20and%0Atomatoes%20is%20an%20essential%20agricultural%20downstream%20activity%20for%20selective%0Aharvesting%20and%20quality%20control%20%28QC%29%20at%20production%20and%20packaging%20sites.%20Recent%0Aadvancements%20in%20Deep%20Learning%20%28DL%29%20have%20produced%20encouraging%20results%20in%20color%0Aimages%20for%20maturity%20classification%20applications.%20However%2C%20hyperspectral%20imaging%0A%28HSI%29%20outperforms%20methods%20based%20on%20color%20vision.%20Multivariate%20analysis%20methods%0Aand%20Convolutional%20Neural%20Networks%20%28CNN%29%20deliver%20promising%20results%3B%20however%2C%20a%0Alarge%20amount%20of%20input%20data%20and%20the%20associated%20preprocessing%20requirements%20cause%0Ahindrances%20in%20practical%20application.%20Conventionally%2C%20the%20reflectance%20intensity%0Ain%20a%20given%20electromagnetic%20spectrum%20is%20employed%20in%20estimating%20fruit%20maturity.%0AWe%20present%20a%20feature%20extraction%20method%20to%20empirically%20demonstrate%20that%20the%20peak%0Areflectance%20in%20subbands%20such%20as%20500-670%20nm%20%28pigment%20band%29%20and%20the%20wavelength%20of%0Athe%20peak%20position%2C%20and%20contrarily%2C%20the%20trough%20reflectance%20and%20its%20corresponding%0Awavelength%20within%20671-790%20nm%20%28chlorophyll%20band%29%20are%20convenient%20to%20compute%20yet%0Adistinctive%20features%20for%20the%20maturity%20classification.%20The%20proposed%20feature%0Aselection%20method%20is%20beneficial%20because%20preprocessing%2C%20such%20as%20dimensionality%0Areduction%2C%20is%20avoided%20before%20every%20prediction.%20The%20feature%20set%20is%20designed%20to%0Acapture%20these%20traits.%20The%20best%20SOTA%20methods%2C%20among%203D-CNN%2C%201D-CNN%2C%20and%20SVM%2C%0Aachieve%20at%20most%2090.0%20%25%20accuracy%20for%20strawberries%20and%2092.0%20%25%20for%20tomatoes%20on%20our%0Adataset.%20Results%20show%20that%20the%20proposed%20method%20outperforms%20the%20SOTA%20as%20it%0Ayields%20an%20accuracy%20above%2098.0%20%25%20in%20strawberry%20and%2096.0%20%25%20in%20tomato%0Aclassification.%20A%20comparative%20analysis%20of%20the%20time%20efficiency%20of%20these%20methods%0Ais%20also%20conducted%2C%20which%20shows%20the%20proposed%20method%20performs%20prediction%20at%2013%0AFrames%20Per%20Second%20%28FPS%29%20compared%20to%20the%20maximum%201.16%20FPS%20attained%20by%20the%0Afull-spectrum%20SVM%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09955v1&entry.124074799=Read"},
{"title": "Automated Federated Learning via Informed Pruning", "author": "Christian Intern\u00f2 and Elena Raponi and Niki van Stein and Thomas B\u00e4ck and Markus Olhofer and Yaochu Jin and Barbara Hammer", "abstract": "  Federated learning (FL) represents a pivotal shift in machine learning (ML)\nas it enables collaborative training of local ML models coordinated by a\ncentral aggregator, all without the need to exchange local data. However, its\napplication on edge devices is hindered by limited computational capabilities\nand data communication challenges, compounded by the inherent complexity of\nDeep Learning (DL) models. Model pruning is identified as a key technique for\ncompressing DL models on devices with limited resources. Nonetheless,\nconventional pruning techniques typically rely on manually crafted heuristics\nand demand human expertise to achieve a balance between model size, speed, and\naccuracy, often resulting in sub-optimal solutions.\n  In this study, we introduce an automated federated learning approach\nutilizing informed pruning, called AutoFLIP, which dynamically prunes and\ncompresses DL models within both the local clients and the global server. It\nleverages a federated loss exploration phase to investigate model gradient\nbehavior across diverse datasets and losses, providing insights into parameter\nsignificance. Our experiments showcase notable enhancements in scenarios with\nstrong non-IID data, underscoring AutoFLIP's capacity to tackle computational\nconstraints and achieve superior global convergence.\n", "link": "http://arxiv.org/abs/2405.10271v1", "date": "2024-05-16", "relevancy": 1.9181, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4876}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4768}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Federated%20Learning%20via%20Informed%20Pruning&body=Title%3A%20Automated%20Federated%20Learning%20via%20Informed%20Pruning%0AAuthor%3A%20Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20represents%20a%20pivotal%20shift%20in%20machine%20learning%20%28ML%29%0Aas%20it%20enables%20collaborative%20training%20of%20local%20ML%20models%20coordinated%20by%20a%0Acentral%20aggregator%2C%20all%20without%20the%20need%20to%20exchange%20local%20data.%20However%2C%20its%0Aapplication%20on%20edge%20devices%20is%20hindered%20by%20limited%20computational%20capabilities%0Aand%20data%20communication%20challenges%2C%20compounded%20by%20the%20inherent%20complexity%20of%0ADeep%20Learning%20%28DL%29%20models.%20Model%20pruning%20is%20identified%20as%20a%20key%20technique%20for%0Acompressing%20DL%20models%20on%20devices%20with%20limited%20resources.%20Nonetheless%2C%0Aconventional%20pruning%20techniques%20typically%20rely%20on%20manually%20crafted%20heuristics%0Aand%20demand%20human%20expertise%20to%20achieve%20a%20balance%20between%20model%20size%2C%20speed%2C%20and%0Aaccuracy%2C%20often%20resulting%20in%20sub-optimal%20solutions.%0A%20%20In%20this%20study%2C%20we%20introduce%20an%20automated%20federated%20learning%20approach%0Autilizing%20informed%20pruning%2C%20called%20AutoFLIP%2C%20which%20dynamically%20prunes%20and%0Acompresses%20DL%20models%20within%20both%20the%20local%20clients%20and%20the%20global%20server.%20It%0Aleverages%20a%20federated%20loss%20exploration%20phase%20to%20investigate%20model%20gradient%0Abehavior%20across%20diverse%20datasets%20and%20losses%2C%20providing%20insights%20into%20parameter%0Asignificance.%20Our%20experiments%20showcase%20notable%20enhancements%20in%20scenarios%20with%0Astrong%20non-IID%20data%2C%20underscoring%20AutoFLIP%27s%20capacity%20to%20tackle%20computational%0Aconstraints%20and%20achieve%20superior%20global%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Federated%2520Learning%2520via%2520Informed%2520Pruning%26entry.906535625%3DChristian%2520Intern%25C3%25B2%2520and%2520Elena%2520Raponi%2520and%2520Niki%2520van%2520Stein%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Markus%2520Olhofer%2520and%2520Yaochu%2520Jin%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520represents%2520a%2520pivotal%2520shift%2520in%2520machine%2520learning%2520%2528ML%2529%250Aas%2520it%2520enables%2520collaborative%2520training%2520of%2520local%2520ML%2520models%2520coordinated%2520by%2520a%250Acentral%2520aggregator%252C%2520all%2520without%2520the%2520need%2520to%2520exchange%2520local%2520data.%2520However%252C%2520its%250Aapplication%2520on%2520edge%2520devices%2520is%2520hindered%2520by%2520limited%2520computational%2520capabilities%250Aand%2520data%2520communication%2520challenges%252C%2520compounded%2520by%2520the%2520inherent%2520complexity%2520of%250ADeep%2520Learning%2520%2528DL%2529%2520models.%2520Model%2520pruning%2520is%2520identified%2520as%2520a%2520key%2520technique%2520for%250Acompressing%2520DL%2520models%2520on%2520devices%2520with%2520limited%2520resources.%2520Nonetheless%252C%250Aconventional%2520pruning%2520techniques%2520typically%2520rely%2520on%2520manually%2520crafted%2520heuristics%250Aand%2520demand%2520human%2520expertise%2520to%2520achieve%2520a%2520balance%2520between%2520model%2520size%252C%2520speed%252C%2520and%250Aaccuracy%252C%2520often%2520resulting%2520in%2520sub-optimal%2520solutions.%250A%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520automated%2520federated%2520learning%2520approach%250Autilizing%2520informed%2520pruning%252C%2520called%2520AutoFLIP%252C%2520which%2520dynamically%2520prunes%2520and%250Acompresses%2520DL%2520models%2520within%2520both%2520the%2520local%2520clients%2520and%2520the%2520global%2520server.%2520It%250Aleverages%2520a%2520federated%2520loss%2520exploration%2520phase%2520to%2520investigate%2520model%2520gradient%250Abehavior%2520across%2520diverse%2520datasets%2520and%2520losses%252C%2520providing%2520insights%2520into%2520parameter%250Asignificance.%2520Our%2520experiments%2520showcase%2520notable%2520enhancements%2520in%2520scenarios%2520with%250Astrong%2520non-IID%2520data%252C%2520underscoring%2520AutoFLIP%2527s%2520capacity%2520to%2520tackle%2520computational%250Aconstraints%2520and%2520achieve%2520superior%2520global%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Federated%20Learning%20via%20Informed%20Pruning&entry.906535625=Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20represents%20a%20pivotal%20shift%20in%20machine%20learning%20%28ML%29%0Aas%20it%20enables%20collaborative%20training%20of%20local%20ML%20models%20coordinated%20by%20a%0Acentral%20aggregator%2C%20all%20without%20the%20need%20to%20exchange%20local%20data.%20However%2C%20its%0Aapplication%20on%20edge%20devices%20is%20hindered%20by%20limited%20computational%20capabilities%0Aand%20data%20communication%20challenges%2C%20compounded%20by%20the%20inherent%20complexity%20of%0ADeep%20Learning%20%28DL%29%20models.%20Model%20pruning%20is%20identified%20as%20a%20key%20technique%20for%0Acompressing%20DL%20models%20on%20devices%20with%20limited%20resources.%20Nonetheless%2C%0Aconventional%20pruning%20techniques%20typically%20rely%20on%20manually%20crafted%20heuristics%0Aand%20demand%20human%20expertise%20to%20achieve%20a%20balance%20between%20model%20size%2C%20speed%2C%20and%0Aaccuracy%2C%20often%20resulting%20in%20sub-optimal%20solutions.%0A%20%20In%20this%20study%2C%20we%20introduce%20an%20automated%20federated%20learning%20approach%0Autilizing%20informed%20pruning%2C%20called%20AutoFLIP%2C%20which%20dynamically%20prunes%20and%0Acompresses%20DL%20models%20within%20both%20the%20local%20clients%20and%20the%20global%20server.%20It%0Aleverages%20a%20federated%20loss%20exploration%20phase%20to%20investigate%20model%20gradient%0Abehavior%20across%20diverse%20datasets%20and%20losses%2C%20providing%20insights%20into%20parameter%0Asignificance.%20Our%20experiments%20showcase%20notable%20enhancements%20in%20scenarios%20with%0Astrong%20non-IID%20data%2C%20underscoring%20AutoFLIP%27s%20capacity%20to%20tackle%20computational%0Aconstraints%20and%20achieve%20superior%20global%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10271v1&entry.124074799=Read"},
{"title": "Goal-conditioned Offline Reinforcement Learning through State Space\n  Partitioning", "author": "Mianchu Wang and Yue Jin and Giovanni Montana", "abstract": "  Offline reinforcement learning (RL) aims to infer sequential decision\npolicies using only offline datasets. This is a particularly difficult setup,\nespecially when learning to achieve multiple different goals or outcomes under\na given scenario with only sparse rewards. For offline learning of\ngoal-conditioned policies via supervised learning, previous work has shown that\nan advantage weighted log-likelihood loss guarantees monotonic policy\nimprovement. In this work we argue that, despite its benefits, this approach is\nstill insufficient to fully address the distribution shift and multi-modality\nproblems. The latter is particularly severe in long-horizon tasks where finding\na unique and optimal policy that goes from a state to the desired goal is\nchallenging as there may be multiple and potentially conflicting solutions. To\ntackle these challenges, we propose a complementary advantage-based weighting\nscheme that introduces an additional source of inductive bias: given a\nvalue-based partitioning of the state space, the contribution of actions\nexpected to lead to target regions that are easier to reach, compared to the\nfinal goal, is further increased. Empirically, we demonstrate that the proposed\napproach, Dual-Advantage Weighted Offline Goal-conditioned RL (DAWOG),\noutperforms several competing offline algorithms in commonly used benchmarks.\nAnalytically, we offer a guarantee that the learnt policy is never worse than\nthe underlying behaviour policy.\n", "link": "http://arxiv.org/abs/2303.09367v2", "date": "2024-05-16", "relevancy": 1.9075, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal-conditioned%20Offline%20Reinforcement%20Learning%20through%20State%20Space%0A%20%20Partitioning&body=Title%3A%20Goal-conditioned%20Offline%20Reinforcement%20Learning%20through%20State%20Space%0A%20%20Partitioning%0AAuthor%3A%20Mianchu%20Wang%20and%20Yue%20Jin%20and%20Giovanni%20Montana%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20infer%20sequential%20decision%0Apolicies%20using%20only%20offline%20datasets.%20This%20is%20a%20particularly%20difficult%20setup%2C%0Aespecially%20when%20learning%20to%20achieve%20multiple%20different%20goals%20or%20outcomes%20under%0Aa%20given%20scenario%20with%20only%20sparse%20rewards.%20For%20offline%20learning%20of%0Agoal-conditioned%20policies%20via%20supervised%20learning%2C%20previous%20work%20has%20shown%20that%0Aan%20advantage%20weighted%20log-likelihood%20loss%20guarantees%20monotonic%20policy%0Aimprovement.%20In%20this%20work%20we%20argue%20that%2C%20despite%20its%20benefits%2C%20this%20approach%20is%0Astill%20insufficient%20to%20fully%20address%20the%20distribution%20shift%20and%20multi-modality%0Aproblems.%20The%20latter%20is%20particularly%20severe%20in%20long-horizon%20tasks%20where%20finding%0Aa%20unique%20and%20optimal%20policy%20that%20goes%20from%20a%20state%20to%20the%20desired%20goal%20is%0Achallenging%20as%20there%20may%20be%20multiple%20and%20potentially%20conflicting%20solutions.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20a%20complementary%20advantage-based%20weighting%0Ascheme%20that%20introduces%20an%20additional%20source%20of%20inductive%20bias%3A%20given%20a%0Avalue-based%20partitioning%20of%20the%20state%20space%2C%20the%20contribution%20of%20actions%0Aexpected%20to%20lead%20to%20target%20regions%20that%20are%20easier%20to%20reach%2C%20compared%20to%20the%0Afinal%20goal%2C%20is%20further%20increased.%20Empirically%2C%20we%20demonstrate%20that%20the%20proposed%0Aapproach%2C%20Dual-Advantage%20Weighted%20Offline%20Goal-conditioned%20RL%20%28DAWOG%29%2C%0Aoutperforms%20several%20competing%20offline%20algorithms%20in%20commonly%20used%20benchmarks.%0AAnalytically%2C%20we%20offer%20a%20guarantee%20that%20the%20learnt%20policy%20is%20never%20worse%20than%0Athe%20underlying%20behaviour%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal-conditioned%2520Offline%2520Reinforcement%2520Learning%2520through%2520State%2520Space%250A%2520%2520Partitioning%26entry.906535625%3DMianchu%2520Wang%2520and%2520Yue%2520Jin%2520and%2520Giovanni%2520Montana%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520aims%2520to%2520infer%2520sequential%2520decision%250Apolicies%2520using%2520only%2520offline%2520datasets.%2520This%2520is%2520a%2520particularly%2520difficult%2520setup%252C%250Aespecially%2520when%2520learning%2520to%2520achieve%2520multiple%2520different%2520goals%2520or%2520outcomes%2520under%250Aa%2520given%2520scenario%2520with%2520only%2520sparse%2520rewards.%2520For%2520offline%2520learning%2520of%250Agoal-conditioned%2520policies%2520via%2520supervised%2520learning%252C%2520previous%2520work%2520has%2520shown%2520that%250Aan%2520advantage%2520weighted%2520log-likelihood%2520loss%2520guarantees%2520monotonic%2520policy%250Aimprovement.%2520In%2520this%2520work%2520we%2520argue%2520that%252C%2520despite%2520its%2520benefits%252C%2520this%2520approach%2520is%250Astill%2520insufficient%2520to%2520fully%2520address%2520the%2520distribution%2520shift%2520and%2520multi-modality%250Aproblems.%2520The%2520latter%2520is%2520particularly%2520severe%2520in%2520long-horizon%2520tasks%2520where%2520finding%250Aa%2520unique%2520and%2520optimal%2520policy%2520that%2520goes%2520from%2520a%2520state%2520to%2520the%2520desired%2520goal%2520is%250Achallenging%2520as%2520there%2520may%2520be%2520multiple%2520and%2520potentially%2520conflicting%2520solutions.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520complementary%2520advantage-based%2520weighting%250Ascheme%2520that%2520introduces%2520an%2520additional%2520source%2520of%2520inductive%2520bias%253A%2520given%2520a%250Avalue-based%2520partitioning%2520of%2520the%2520state%2520space%252C%2520the%2520contribution%2520of%2520actions%250Aexpected%2520to%2520lead%2520to%2520target%2520regions%2520that%2520are%2520easier%2520to%2520reach%252C%2520compared%2520to%2520the%250Afinal%2520goal%252C%2520is%2520further%2520increased.%2520Empirically%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%252C%2520Dual-Advantage%2520Weighted%2520Offline%2520Goal-conditioned%2520RL%2520%2528DAWOG%2529%252C%250Aoutperforms%2520several%2520competing%2520offline%2520algorithms%2520in%2520commonly%2520used%2520benchmarks.%250AAnalytically%252C%2520we%2520offer%2520a%2520guarantee%2520that%2520the%2520learnt%2520policy%2520is%2520never%2520worse%2520than%250Athe%2520underlying%2520behaviour%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-conditioned%20Offline%20Reinforcement%20Learning%20through%20State%20Space%0A%20%20Partitioning&entry.906535625=Mianchu%20Wang%20and%20Yue%20Jin%20and%20Giovanni%20Montana&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20infer%20sequential%20decision%0Apolicies%20using%20only%20offline%20datasets.%20This%20is%20a%20particularly%20difficult%20setup%2C%0Aespecially%20when%20learning%20to%20achieve%20multiple%20different%20goals%20or%20outcomes%20under%0Aa%20given%20scenario%20with%20only%20sparse%20rewards.%20For%20offline%20learning%20of%0Agoal-conditioned%20policies%20via%20supervised%20learning%2C%20previous%20work%20has%20shown%20that%0Aan%20advantage%20weighted%20log-likelihood%20loss%20guarantees%20monotonic%20policy%0Aimprovement.%20In%20this%20work%20we%20argue%20that%2C%20despite%20its%20benefits%2C%20this%20approach%20is%0Astill%20insufficient%20to%20fully%20address%20the%20distribution%20shift%20and%20multi-modality%0Aproblems.%20The%20latter%20is%20particularly%20severe%20in%20long-horizon%20tasks%20where%20finding%0Aa%20unique%20and%20optimal%20policy%20that%20goes%20from%20a%20state%20to%20the%20desired%20goal%20is%0Achallenging%20as%20there%20may%20be%20multiple%20and%20potentially%20conflicting%20solutions.%20To%0Atackle%20these%20challenges%2C%20we%20propose%20a%20complementary%20advantage-based%20weighting%0Ascheme%20that%20introduces%20an%20additional%20source%20of%20inductive%20bias%3A%20given%20a%0Avalue-based%20partitioning%20of%20the%20state%20space%2C%20the%20contribution%20of%20actions%0Aexpected%20to%20lead%20to%20target%20regions%20that%20are%20easier%20to%20reach%2C%20compared%20to%20the%0Afinal%20goal%2C%20is%20further%20increased.%20Empirically%2C%20we%20demonstrate%20that%20the%20proposed%0Aapproach%2C%20Dual-Advantage%20Weighted%20Offline%20Goal-conditioned%20RL%20%28DAWOG%29%2C%0Aoutperforms%20several%20competing%20offline%20algorithms%20in%20commonly%20used%20benchmarks.%0AAnalytically%2C%20we%20offer%20a%20guarantee%20that%20the%20learnt%20policy%20is%20never%20worse%20than%0Athe%20underlying%20behaviour%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09367v2&entry.124074799=Read"},
{"title": "Revealing Hierarchical Structure of Leaf Venations in Plant Science via\n  Label-Efficient Segmentation: Dataset and Method", "author": "Weizhen Liu and Ao Li and Ze Wu and Yue Li and Baobin Ge and Guangyu Lan and Shilin Chen and Minghe Li and Yunfei Liu and Xiaohui Yuan and Nanqing Dong", "abstract": "  Hierarchical leaf vein segmentation is a crucial but under-explored task in\nagricultural sciences, where analysis of the hierarchical structure of plant\nleaf venation can contribute to plant breeding. While current segmentation\ntechniques rely on data-driven models, there is no publicly available dataset\nspecifically designed for hierarchical leaf vein segmentation. To address this\ngap, we introduce the HierArchical Leaf Vein Segmentation (HALVS) dataset, the\nfirst public hierarchical leaf vein segmentation dataset. HALVS comprises 5,057\nreal-scanned high-resolution leaf images collected from three plant species:\nsoybean, sweet cherry, and London planetree. It also includes human-annotated\nground truth for three orders of leaf veins, with a total labeling effort of\n83.8 person-days. Based on HALVS, we further develop a label-efficient learning\nparadigm that leverages partial label information, i.e. missing annotations for\ntertiary veins. Empirical studies are performed on HALVS, revealing new\nobservations, challenges, and research directions on leaf vein segmentation.\n", "link": "http://arxiv.org/abs/2405.10041v1", "date": "2024-05-16", "relevancy": 1.9071, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5103}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Hierarchical%20Structure%20of%20Leaf%20Venations%20in%20Plant%20Science%20via%0A%20%20Label-Efficient%20Segmentation%3A%20Dataset%20and%20Method&body=Title%3A%20Revealing%20Hierarchical%20Structure%20of%20Leaf%20Venations%20in%20Plant%20Science%20via%0A%20%20Label-Efficient%20Segmentation%3A%20Dataset%20and%20Method%0AAuthor%3A%20Weizhen%20Liu%20and%20Ao%20Li%20and%20Ze%20Wu%20and%20Yue%20Li%20and%20Baobin%20Ge%20and%20Guangyu%20Lan%20and%20Shilin%20Chen%20and%20Minghe%20Li%20and%20Yunfei%20Liu%20and%20Xiaohui%20Yuan%20and%20Nanqing%20Dong%0AAbstract%3A%20%20%20Hierarchical%20leaf%20vein%20segmentation%20is%20a%20crucial%20but%20under-explored%20task%20in%0Aagricultural%20sciences%2C%20where%20analysis%20of%20the%20hierarchical%20structure%20of%20plant%0Aleaf%20venation%20can%20contribute%20to%20plant%20breeding.%20While%20current%20segmentation%0Atechniques%20rely%20on%20data-driven%20models%2C%20there%20is%20no%20publicly%20available%20dataset%0Aspecifically%20designed%20for%20hierarchical%20leaf%20vein%20segmentation.%20To%20address%20this%0Agap%2C%20we%20introduce%20the%20HierArchical%20Leaf%20Vein%20Segmentation%20%28HALVS%29%20dataset%2C%20the%0Afirst%20public%20hierarchical%20leaf%20vein%20segmentation%20dataset.%20HALVS%20comprises%205%2C057%0Areal-scanned%20high-resolution%20leaf%20images%20collected%20from%20three%20plant%20species%3A%0Asoybean%2C%20sweet%20cherry%2C%20and%20London%20planetree.%20It%20also%20includes%20human-annotated%0Aground%20truth%20for%20three%20orders%20of%20leaf%20veins%2C%20with%20a%20total%20labeling%20effort%20of%0A83.8%20person-days.%20Based%20on%20HALVS%2C%20we%20further%20develop%20a%20label-efficient%20learning%0Aparadigm%20that%20leverages%20partial%20label%20information%2C%20i.e.%20missing%20annotations%20for%0Atertiary%20veins.%20Empirical%20studies%20are%20performed%20on%20HALVS%2C%20revealing%20new%0Aobservations%2C%20challenges%2C%20and%20research%20directions%20on%20leaf%20vein%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Hierarchical%2520Structure%2520of%2520Leaf%2520Venations%2520in%2520Plant%2520Science%2520via%250A%2520%2520Label-Efficient%2520Segmentation%253A%2520Dataset%2520and%2520Method%26entry.906535625%3DWeizhen%2520Liu%2520and%2520Ao%2520Li%2520and%2520Ze%2520Wu%2520and%2520Yue%2520Li%2520and%2520Baobin%2520Ge%2520and%2520Guangyu%2520Lan%2520and%2520Shilin%2520Chen%2520and%2520Minghe%2520Li%2520and%2520Yunfei%2520Liu%2520and%2520Xiaohui%2520Yuan%2520and%2520Nanqing%2520Dong%26entry.1292438233%3D%2520%2520Hierarchical%2520leaf%2520vein%2520segmentation%2520is%2520a%2520crucial%2520but%2520under-explored%2520task%2520in%250Aagricultural%2520sciences%252C%2520where%2520analysis%2520of%2520the%2520hierarchical%2520structure%2520of%2520plant%250Aleaf%2520venation%2520can%2520contribute%2520to%2520plant%2520breeding.%2520While%2520current%2520segmentation%250Atechniques%2520rely%2520on%2520data-driven%2520models%252C%2520there%2520is%2520no%2520publicly%2520available%2520dataset%250Aspecifically%2520designed%2520for%2520hierarchical%2520leaf%2520vein%2520segmentation.%2520To%2520address%2520this%250Agap%252C%2520we%2520introduce%2520the%2520HierArchical%2520Leaf%2520Vein%2520Segmentation%2520%2528HALVS%2529%2520dataset%252C%2520the%250Afirst%2520public%2520hierarchical%2520leaf%2520vein%2520segmentation%2520dataset.%2520HALVS%2520comprises%25205%252C057%250Areal-scanned%2520high-resolution%2520leaf%2520images%2520collected%2520from%2520three%2520plant%2520species%253A%250Asoybean%252C%2520sweet%2520cherry%252C%2520and%2520London%2520planetree.%2520It%2520also%2520includes%2520human-annotated%250Aground%2520truth%2520for%2520three%2520orders%2520of%2520leaf%2520veins%252C%2520with%2520a%2520total%2520labeling%2520effort%2520of%250A83.8%2520person-days.%2520Based%2520on%2520HALVS%252C%2520we%2520further%2520develop%2520a%2520label-efficient%2520learning%250Aparadigm%2520that%2520leverages%2520partial%2520label%2520information%252C%2520i.e.%2520missing%2520annotations%2520for%250Atertiary%2520veins.%2520Empirical%2520studies%2520are%2520performed%2520on%2520HALVS%252C%2520revealing%2520new%250Aobservations%252C%2520challenges%252C%2520and%2520research%2520directions%2520on%2520leaf%2520vein%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Hierarchical%20Structure%20of%20Leaf%20Venations%20in%20Plant%20Science%20via%0A%20%20Label-Efficient%20Segmentation%3A%20Dataset%20and%20Method&entry.906535625=Weizhen%20Liu%20and%20Ao%20Li%20and%20Ze%20Wu%20and%20Yue%20Li%20and%20Baobin%20Ge%20and%20Guangyu%20Lan%20and%20Shilin%20Chen%20and%20Minghe%20Li%20and%20Yunfei%20Liu%20and%20Xiaohui%20Yuan%20and%20Nanqing%20Dong&entry.1292438233=%20%20Hierarchical%20leaf%20vein%20segmentation%20is%20a%20crucial%20but%20under-explored%20task%20in%0Aagricultural%20sciences%2C%20where%20analysis%20of%20the%20hierarchical%20structure%20of%20plant%0Aleaf%20venation%20can%20contribute%20to%20plant%20breeding.%20While%20current%20segmentation%0Atechniques%20rely%20on%20data-driven%20models%2C%20there%20is%20no%20publicly%20available%20dataset%0Aspecifically%20designed%20for%20hierarchical%20leaf%20vein%20segmentation.%20To%20address%20this%0Agap%2C%20we%20introduce%20the%20HierArchical%20Leaf%20Vein%20Segmentation%20%28HALVS%29%20dataset%2C%20the%0Afirst%20public%20hierarchical%20leaf%20vein%20segmentation%20dataset.%20HALVS%20comprises%205%2C057%0Areal-scanned%20high-resolution%20leaf%20images%20collected%20from%20three%20plant%20species%3A%0Asoybean%2C%20sweet%20cherry%2C%20and%20London%20planetree.%20It%20also%20includes%20human-annotated%0Aground%20truth%20for%20three%20orders%20of%20leaf%20veins%2C%20with%20a%20total%20labeling%20effort%20of%0A83.8%20person-days.%20Based%20on%20HALVS%2C%20we%20further%20develop%20a%20label-efficient%20learning%0Aparadigm%20that%20leverages%20partial%20label%20information%2C%20i.e.%20missing%20annotations%20for%0Atertiary%20veins.%20Empirical%20studies%20are%20performed%20on%20HALVS%2C%20revealing%20new%0Aobservations%2C%20challenges%2C%20and%20research%20directions%20on%20leaf%20vein%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10041v1&entry.124074799=Read"},
{"title": "Random ReLU Neural Networks as Non-Gaussian Processes", "author": "Rahul Parhi and Pakshal Bohra and Ayoub El Biari and Mehrsa Pourya and Michael Unser", "abstract": "  We consider a large class of shallow neural networks with randomly\ninitialized parameters and rectified linear unit activation functions. We prove\nthat these random neural networks are well-defined non-Gaussian processes. As a\nby-product, we demonstrate that these networks are solutions to stochastic\ndifferential equations driven by impulsive white noise (combinations of random\nDirac measures). These processes are parameterized by the law of the weights\nand biases as well as the density of activation thresholds in each bounded\nregion of the input domain. We prove that these processes are isotropic and\nwide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkably\nsimple closed-form expression for their autocovariance function. Our results\nare fundamentally different from prior work in that we consider a\nnon-asymptotic viewpoint: The number of neurons in each bounded region of the\ninput domain (i.e., the width) is itself a random variable with a Poisson law\nwith mean proportional to the density parameter. Finally, we show that, under\nsuitable hypotheses, as the expected width tends to infinity, these processes\ncan converge in law not only to Gaussian processes, but also to non-Gaussian\nprocesses depending on the law of the weights. Our asymptotic results provide a\nnew take on several classical results (wide networks converge to Gaussian\nprocesses) as well as some new ones (wide networks can converge to non-Gaussian\nprocesses).\n", "link": "http://arxiv.org/abs/2405.10229v1", "date": "2024-05-16", "relevancy": 1.8827, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4989}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4761}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes&body=Title%3A%20Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes%0AAuthor%3A%20Rahul%20Parhi%20and%20Pakshal%20Bohra%20and%20Ayoub%20El%20Biari%20and%20Mehrsa%20Pourya%20and%20Michael%20Unser%0AAbstract%3A%20%20%20We%20consider%20a%20large%20class%20of%20shallow%20neural%20networks%20with%20randomly%0Ainitialized%20parameters%20and%20rectified%20linear%20unit%20activation%20functions.%20We%20prove%0Athat%20these%20random%20neural%20networks%20are%20well-defined%20non-Gaussian%20processes.%20As%20a%0Aby-product%2C%20we%20demonstrate%20that%20these%20networks%20are%20solutions%20to%20stochastic%0Adifferential%20equations%20driven%20by%20impulsive%20white%20noise%20%28combinations%20of%20random%0ADirac%20measures%29.%20These%20processes%20are%20parameterized%20by%20the%20law%20of%20the%20weights%0Aand%20biases%20as%20well%20as%20the%20density%20of%20activation%20thresholds%20in%20each%20bounded%0Aregion%20of%20the%20input%20domain.%20We%20prove%20that%20these%20processes%20are%20isotropic%20and%0Awide-sense%20self-similar%20with%20Hurst%20exponent%20%243/2%24.%20We%20also%20derive%20a%20remarkably%0Asimple%20closed-form%20expression%20for%20their%20autocovariance%20function.%20Our%20results%0Aare%20fundamentally%20different%20from%20prior%20work%20in%20that%20we%20consider%20a%0Anon-asymptotic%20viewpoint%3A%20The%20number%20of%20neurons%20in%20each%20bounded%20region%20of%20the%0Ainput%20domain%20%28i.e.%2C%20the%20width%29%20is%20itself%20a%20random%20variable%20with%20a%20Poisson%20law%0Awith%20mean%20proportional%20to%20the%20density%20parameter.%20Finally%2C%20we%20show%20that%2C%20under%0Asuitable%20hypotheses%2C%20as%20the%20expected%20width%20tends%20to%20infinity%2C%20these%20processes%0Acan%20converge%20in%20law%20not%20only%20to%20Gaussian%20processes%2C%20but%20also%20to%20non-Gaussian%0Aprocesses%20depending%20on%20the%20law%20of%20the%20weights.%20Our%20asymptotic%20results%20provide%20a%0Anew%20take%20on%20several%20classical%20results%20%28wide%20networks%20converge%20to%20Gaussian%0Aprocesses%29%20as%20well%20as%20some%20new%20ones%20%28wide%20networks%20can%20converge%20to%20non-Gaussian%0Aprocesses%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520ReLU%2520Neural%2520Networks%2520as%2520Non-Gaussian%2520Processes%26entry.906535625%3DRahul%2520Parhi%2520and%2520Pakshal%2520Bohra%2520and%2520Ayoub%2520El%2520Biari%2520and%2520Mehrsa%2520Pourya%2520and%2520Michael%2520Unser%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520large%2520class%2520of%2520shallow%2520neural%2520networks%2520with%2520randomly%250Ainitialized%2520parameters%2520and%2520rectified%2520linear%2520unit%2520activation%2520functions.%2520We%2520prove%250Athat%2520these%2520random%2520neural%2520networks%2520are%2520well-defined%2520non-Gaussian%2520processes.%2520As%2520a%250Aby-product%252C%2520we%2520demonstrate%2520that%2520these%2520networks%2520are%2520solutions%2520to%2520stochastic%250Adifferential%2520equations%2520driven%2520by%2520impulsive%2520white%2520noise%2520%2528combinations%2520of%2520random%250ADirac%2520measures%2529.%2520These%2520processes%2520are%2520parameterized%2520by%2520the%2520law%2520of%2520the%2520weights%250Aand%2520biases%2520as%2520well%2520as%2520the%2520density%2520of%2520activation%2520thresholds%2520in%2520each%2520bounded%250Aregion%2520of%2520the%2520input%2520domain.%2520We%2520prove%2520that%2520these%2520processes%2520are%2520isotropic%2520and%250Awide-sense%2520self-similar%2520with%2520Hurst%2520exponent%2520%25243/2%2524.%2520We%2520also%2520derive%2520a%2520remarkably%250Asimple%2520closed-form%2520expression%2520for%2520their%2520autocovariance%2520function.%2520Our%2520results%250Aare%2520fundamentally%2520different%2520from%2520prior%2520work%2520in%2520that%2520we%2520consider%2520a%250Anon-asymptotic%2520viewpoint%253A%2520The%2520number%2520of%2520neurons%2520in%2520each%2520bounded%2520region%2520of%2520the%250Ainput%2520domain%2520%2528i.e.%252C%2520the%2520width%2529%2520is%2520itself%2520a%2520random%2520variable%2520with%2520a%2520Poisson%2520law%250Awith%2520mean%2520proportional%2520to%2520the%2520density%2520parameter.%2520Finally%252C%2520we%2520show%2520that%252C%2520under%250Asuitable%2520hypotheses%252C%2520as%2520the%2520expected%2520width%2520tends%2520to%2520infinity%252C%2520these%2520processes%250Acan%2520converge%2520in%2520law%2520not%2520only%2520to%2520Gaussian%2520processes%252C%2520but%2520also%2520to%2520non-Gaussian%250Aprocesses%2520depending%2520on%2520the%2520law%2520of%2520the%2520weights.%2520Our%2520asymptotic%2520results%2520provide%2520a%250Anew%2520take%2520on%2520several%2520classical%2520results%2520%2528wide%2520networks%2520converge%2520to%2520Gaussian%250Aprocesses%2529%2520as%2520well%2520as%2520some%2520new%2520ones%2520%2528wide%2520networks%2520can%2520converge%2520to%2520non-Gaussian%250Aprocesses%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20ReLU%20Neural%20Networks%20as%20Non-Gaussian%20Processes&entry.906535625=Rahul%20Parhi%20and%20Pakshal%20Bohra%20and%20Ayoub%20El%20Biari%20and%20Mehrsa%20Pourya%20and%20Michael%20Unser&entry.1292438233=%20%20We%20consider%20a%20large%20class%20of%20shallow%20neural%20networks%20with%20randomly%0Ainitialized%20parameters%20and%20rectified%20linear%20unit%20activation%20functions.%20We%20prove%0Athat%20these%20random%20neural%20networks%20are%20well-defined%20non-Gaussian%20processes.%20As%20a%0Aby-product%2C%20we%20demonstrate%20that%20these%20networks%20are%20solutions%20to%20stochastic%0Adifferential%20equations%20driven%20by%20impulsive%20white%20noise%20%28combinations%20of%20random%0ADirac%20measures%29.%20These%20processes%20are%20parameterized%20by%20the%20law%20of%20the%20weights%0Aand%20biases%20as%20well%20as%20the%20density%20of%20activation%20thresholds%20in%20each%20bounded%0Aregion%20of%20the%20input%20domain.%20We%20prove%20that%20these%20processes%20are%20isotropic%20and%0Awide-sense%20self-similar%20with%20Hurst%20exponent%20%243/2%24.%20We%20also%20derive%20a%20remarkably%0Asimple%20closed-form%20expression%20for%20their%20autocovariance%20function.%20Our%20results%0Aare%20fundamentally%20different%20from%20prior%20work%20in%20that%20we%20consider%20a%0Anon-asymptotic%20viewpoint%3A%20The%20number%20of%20neurons%20in%20each%20bounded%20region%20of%20the%0Ainput%20domain%20%28i.e.%2C%20the%20width%29%20is%20itself%20a%20random%20variable%20with%20a%20Poisson%20law%0Awith%20mean%20proportional%20to%20the%20density%20parameter.%20Finally%2C%20we%20show%20that%2C%20under%0Asuitable%20hypotheses%2C%20as%20the%20expected%20width%20tends%20to%20infinity%2C%20these%20processes%0Acan%20converge%20in%20law%20not%20only%20to%20Gaussian%20processes%2C%20but%20also%20to%20non-Gaussian%0Aprocesses%20depending%20on%20the%20law%20of%20the%20weights.%20Our%20asymptotic%20results%20provide%20a%0Anew%20take%20on%20several%20classical%20results%20%28wide%20networks%20converge%20to%20Gaussian%0Aprocesses%29%20as%20well%20as%20some%20new%20ones%20%28wide%20networks%20can%20converge%20to%20non-Gaussian%0Aprocesses%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10229v1&entry.124074799=Read"},
{"title": "Red Teaming Language Models for Contradictory Dialogues", "author": "Xiaofei Wen and Bangzheng Li and Tenghao Huang and Muhao Chen", "abstract": "  Most language models currently available are prone to self-contradiction\nduring dialogues. To mitigate this issue, this study explores a novel\ncontradictory dialogue processing task that aims to detect and modify\ncontradictory statements in a conversation. This task is inspired by research\non context faithfulness and dialogue comprehension, which have demonstrated\nthat the detection and understanding of contradictions often necessitate\ndetailed explanations. We develop a dataset comprising contradictory dialogues,\nin which one side of the conversation contradicts itself. Each dialogue is\naccompanied by an explanatory label that highlights the location and details of\nthe contradiction. With this dataset, we present a Red Teaming framework for\ncontradictory dialogue processing. The framework detects and attempts to\nexplain the dialogue, then modifies the existing contradictory content using\nthe explanation. Our experiments demonstrate that the framework improves the\nability to detect contradictory dialogues and provides valid explanations.\nAdditionally, it showcases distinct capabilities for modifying such dialogues.\nOur study highlights the importance of the logical inconsistency problem in\nconversational AI.\n", "link": "http://arxiv.org/abs/2405.10128v1", "date": "2024-05-16", "relevancy": 1.8815, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4693}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Red%20Teaming%20Language%20Models%20for%20Contradictory%20Dialogues&body=Title%3A%20Red%20Teaming%20Language%20Models%20for%20Contradictory%20Dialogues%0AAuthor%3A%20Xiaofei%20Wen%20and%20Bangzheng%20Li%20and%20Tenghao%20Huang%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20Most%20language%20models%20currently%20available%20are%20prone%20to%20self-contradiction%0Aduring%20dialogues.%20To%20mitigate%20this%20issue%2C%20this%20study%20explores%20a%20novel%0Acontradictory%20dialogue%20processing%20task%20that%20aims%20to%20detect%20and%20modify%0Acontradictory%20statements%20in%20a%20conversation.%20This%20task%20is%20inspired%20by%20research%0Aon%20context%20faithfulness%20and%20dialogue%20comprehension%2C%20which%20have%20demonstrated%0Athat%20the%20detection%20and%20understanding%20of%20contradictions%20often%20necessitate%0Adetailed%20explanations.%20We%20develop%20a%20dataset%20comprising%20contradictory%20dialogues%2C%0Ain%20which%20one%20side%20of%20the%20conversation%20contradicts%20itself.%20Each%20dialogue%20is%0Aaccompanied%20by%20an%20explanatory%20label%20that%20highlights%20the%20location%20and%20details%20of%0Athe%20contradiction.%20With%20this%20dataset%2C%20we%20present%20a%20Red%20Teaming%20framework%20for%0Acontradictory%20dialogue%20processing.%20The%20framework%20detects%20and%20attempts%20to%0Aexplain%20the%20dialogue%2C%20then%20modifies%20the%20existing%20contradictory%20content%20using%0Athe%20explanation.%20Our%20experiments%20demonstrate%20that%20the%20framework%20improves%20the%0Aability%20to%20detect%20contradictory%20dialogues%20and%20provides%20valid%20explanations.%0AAdditionally%2C%20it%20showcases%20distinct%20capabilities%20for%20modifying%20such%20dialogues.%0AOur%20study%20highlights%20the%20importance%20of%20the%20logical%20inconsistency%20problem%20in%0Aconversational%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRed%2520Teaming%2520Language%2520Models%2520for%2520Contradictory%2520Dialogues%26entry.906535625%3DXiaofei%2520Wen%2520and%2520Bangzheng%2520Li%2520and%2520Tenghao%2520Huang%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520Most%2520language%2520models%2520currently%2520available%2520are%2520prone%2520to%2520self-contradiction%250Aduring%2520dialogues.%2520To%2520mitigate%2520this%2520issue%252C%2520this%2520study%2520explores%2520a%2520novel%250Acontradictory%2520dialogue%2520processing%2520task%2520that%2520aims%2520to%2520detect%2520and%2520modify%250Acontradictory%2520statements%2520in%2520a%2520conversation.%2520This%2520task%2520is%2520inspired%2520by%2520research%250Aon%2520context%2520faithfulness%2520and%2520dialogue%2520comprehension%252C%2520which%2520have%2520demonstrated%250Athat%2520the%2520detection%2520and%2520understanding%2520of%2520contradictions%2520often%2520necessitate%250Adetailed%2520explanations.%2520We%2520develop%2520a%2520dataset%2520comprising%2520contradictory%2520dialogues%252C%250Ain%2520which%2520one%2520side%2520of%2520the%2520conversation%2520contradicts%2520itself.%2520Each%2520dialogue%2520is%250Aaccompanied%2520by%2520an%2520explanatory%2520label%2520that%2520highlights%2520the%2520location%2520and%2520details%2520of%250Athe%2520contradiction.%2520With%2520this%2520dataset%252C%2520we%2520present%2520a%2520Red%2520Teaming%2520framework%2520for%250Acontradictory%2520dialogue%2520processing.%2520The%2520framework%2520detects%2520and%2520attempts%2520to%250Aexplain%2520the%2520dialogue%252C%2520then%2520modifies%2520the%2520existing%2520contradictory%2520content%2520using%250Athe%2520explanation.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520framework%2520improves%2520the%250Aability%2520to%2520detect%2520contradictory%2520dialogues%2520and%2520provides%2520valid%2520explanations.%250AAdditionally%252C%2520it%2520showcases%2520distinct%2520capabilities%2520for%2520modifying%2520such%2520dialogues.%250AOur%2520study%2520highlights%2520the%2520importance%2520of%2520the%2520logical%2520inconsistency%2520problem%2520in%250Aconversational%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Red%20Teaming%20Language%20Models%20for%20Contradictory%20Dialogues&entry.906535625=Xiaofei%20Wen%20and%20Bangzheng%20Li%20and%20Tenghao%20Huang%20and%20Muhao%20Chen&entry.1292438233=%20%20Most%20language%20models%20currently%20available%20are%20prone%20to%20self-contradiction%0Aduring%20dialogues.%20To%20mitigate%20this%20issue%2C%20this%20study%20explores%20a%20novel%0Acontradictory%20dialogue%20processing%20task%20that%20aims%20to%20detect%20and%20modify%0Acontradictory%20statements%20in%20a%20conversation.%20This%20task%20is%20inspired%20by%20research%0Aon%20context%20faithfulness%20and%20dialogue%20comprehension%2C%20which%20have%20demonstrated%0Athat%20the%20detection%20and%20understanding%20of%20contradictions%20often%20necessitate%0Adetailed%20explanations.%20We%20develop%20a%20dataset%20comprising%20contradictory%20dialogues%2C%0Ain%20which%20one%20side%20of%20the%20conversation%20contradicts%20itself.%20Each%20dialogue%20is%0Aaccompanied%20by%20an%20explanatory%20label%20that%20highlights%20the%20location%20and%20details%20of%0Athe%20contradiction.%20With%20this%20dataset%2C%20we%20present%20a%20Red%20Teaming%20framework%20for%0Acontradictory%20dialogue%20processing.%20The%20framework%20detects%20and%20attempts%20to%0Aexplain%20the%20dialogue%2C%20then%20modifies%20the%20existing%20contradictory%20content%20using%0Athe%20explanation.%20Our%20experiments%20demonstrate%20that%20the%20framework%20improves%20the%0Aability%20to%20detect%20contradictory%20dialogues%20and%20provides%20valid%20explanations.%0AAdditionally%2C%20it%20showcases%20distinct%20capabilities%20for%20modifying%20such%20dialogues.%0AOur%20study%20highlights%20the%20importance%20of%20the%20logical%20inconsistency%20problem%20in%0Aconversational%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10128v1&entry.124074799=Read"},
{"title": "SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection", "author": "Mingxuan Liu and Tyler L. Hayes and Elisa Ricci and Gabriela Csurka and Riccardo Volpi", "abstract": "  Open-vocabulary object detection (OvOD) has transformed detection into a\nlanguage-guided task, empowering users to freely define their class\nvocabularies of interest during inference. However, our initial investigation\nindicates that existing OvOD detectors exhibit significant variability when\ndealing with vocabularies across various semantic granularities, posing a\nconcern for real-world deployment. To this end, we introduce Semantic Hierarchy\nNexus (SHiNe), a novel classifier that uses semantic knowledge from class\nhierarchies. It runs offline in three steps: i) it retrieves relevant\nsuper-/sub-categories from a hierarchy for each target class; ii) it integrates\nthese categories into hierarchy-aware sentences; iii) it fuses these sentence\nembeddings to generate the nexus classifier vector. Our evaluation on various\ndetection benchmarks demonstrates that SHiNe enhances robustness across diverse\nvocabulary granularities, achieving up to +31.9% mAP50 with ground truth\nhierarchies, while retaining improvements using hierarchies generated by large\nlanguage models. Moreover, when applied to open-vocabulary classification on\nImageNet-1k, SHiNe improves the CLIP zero-shot baseline by +2.8% accuracy.\nSHiNe is training-free and can be seamlessly integrated with any off-the-shelf\nOvOD detector, without incurring additional computational overhead during\ninference. The code is open source.\n", "link": "http://arxiv.org/abs/2405.10053v1", "date": "2024-05-16", "relevancy": 1.8789, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHiNe%3A%20Semantic%20Hierarchy%20Nexus%20for%20Open-vocabulary%20Object%20Detection&body=Title%3A%20SHiNe%3A%20Semantic%20Hierarchy%20Nexus%20for%20Open-vocabulary%20Object%20Detection%0AAuthor%3A%20Mingxuan%20Liu%20and%20Tyler%20L.%20Hayes%20and%20Elisa%20Ricci%20and%20Gabriela%20Csurka%20and%20Riccardo%20Volpi%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20%28OvOD%29%20has%20transformed%20detection%20into%20a%0Alanguage-guided%20task%2C%20empowering%20users%20to%20freely%20define%20their%20class%0Avocabularies%20of%20interest%20during%20inference.%20However%2C%20our%20initial%20investigation%0Aindicates%20that%20existing%20OvOD%20detectors%20exhibit%20significant%20variability%20when%0Adealing%20with%20vocabularies%20across%20various%20semantic%20granularities%2C%20posing%20a%0Aconcern%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20introduce%20Semantic%20Hierarchy%0ANexus%20%28SHiNe%29%2C%20a%20novel%20classifier%20that%20uses%20semantic%20knowledge%20from%20class%0Ahierarchies.%20It%20runs%20offline%20in%20three%20steps%3A%20i%29%20it%20retrieves%20relevant%0Asuper-/sub-categories%20from%20a%20hierarchy%20for%20each%20target%20class%3B%20ii%29%20it%20integrates%0Athese%20categories%20into%20hierarchy-aware%20sentences%3B%20iii%29%20it%20fuses%20these%20sentence%0Aembeddings%20to%20generate%20the%20nexus%20classifier%20vector.%20Our%20evaluation%20on%20various%0Adetection%20benchmarks%20demonstrates%20that%20SHiNe%20enhances%20robustness%20across%20diverse%0Avocabulary%20granularities%2C%20achieving%20up%20to%20%2B31.9%25%20mAP50%20with%20ground%20truth%0Ahierarchies%2C%20while%20retaining%20improvements%20using%20hierarchies%20generated%20by%20large%0Alanguage%20models.%20Moreover%2C%20when%20applied%20to%20open-vocabulary%20classification%20on%0AImageNet-1k%2C%20SHiNe%20improves%20the%20CLIP%20zero-shot%20baseline%20by%20%2B2.8%25%20accuracy.%0ASHiNe%20is%20training-free%20and%20can%20be%20seamlessly%20integrated%20with%20any%20off-the-shelf%0AOvOD%20detector%2C%20without%20incurring%20additional%20computational%20overhead%20during%0Ainference.%20The%20code%20is%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHiNe%253A%2520Semantic%2520Hierarchy%2520Nexus%2520for%2520Open-vocabulary%2520Object%2520Detection%26entry.906535625%3DMingxuan%2520Liu%2520and%2520Tyler%2520L.%2520Hayes%2520and%2520Elisa%2520Ricci%2520and%2520Gabriela%2520Csurka%2520and%2520Riccardo%2520Volpi%26entry.1292438233%3D%2520%2520Open-vocabulary%2520object%2520detection%2520%2528OvOD%2529%2520has%2520transformed%2520detection%2520into%2520a%250Alanguage-guided%2520task%252C%2520empowering%2520users%2520to%2520freely%2520define%2520their%2520class%250Avocabularies%2520of%2520interest%2520during%2520inference.%2520However%252C%2520our%2520initial%2520investigation%250Aindicates%2520that%2520existing%2520OvOD%2520detectors%2520exhibit%2520significant%2520variability%2520when%250Adealing%2520with%2520vocabularies%2520across%2520various%2520semantic%2520granularities%252C%2520posing%2520a%250Aconcern%2520for%2520real-world%2520deployment.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Semantic%2520Hierarchy%250ANexus%2520%2528SHiNe%2529%252C%2520a%2520novel%2520classifier%2520that%2520uses%2520semantic%2520knowledge%2520from%2520class%250Ahierarchies.%2520It%2520runs%2520offline%2520in%2520three%2520steps%253A%2520i%2529%2520it%2520retrieves%2520relevant%250Asuper-/sub-categories%2520from%2520a%2520hierarchy%2520for%2520each%2520target%2520class%253B%2520ii%2529%2520it%2520integrates%250Athese%2520categories%2520into%2520hierarchy-aware%2520sentences%253B%2520iii%2529%2520it%2520fuses%2520these%2520sentence%250Aembeddings%2520to%2520generate%2520the%2520nexus%2520classifier%2520vector.%2520Our%2520evaluation%2520on%2520various%250Adetection%2520benchmarks%2520demonstrates%2520that%2520SHiNe%2520enhances%2520robustness%2520across%2520diverse%250Avocabulary%2520granularities%252C%2520achieving%2520up%2520to%2520%252B31.9%2525%2520mAP50%2520with%2520ground%2520truth%250Ahierarchies%252C%2520while%2520retaining%2520improvements%2520using%2520hierarchies%2520generated%2520by%2520large%250Alanguage%2520models.%2520Moreover%252C%2520when%2520applied%2520to%2520open-vocabulary%2520classification%2520on%250AImageNet-1k%252C%2520SHiNe%2520improves%2520the%2520CLIP%2520zero-shot%2520baseline%2520by%2520%252B2.8%2525%2520accuracy.%250ASHiNe%2520is%2520training-free%2520and%2520can%2520be%2520seamlessly%2520integrated%2520with%2520any%2520off-the-shelf%250AOvOD%2520detector%252C%2520without%2520incurring%2520additional%2520computational%2520overhead%2520during%250Ainference.%2520The%2520code%2520is%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHiNe%3A%20Semantic%20Hierarchy%20Nexus%20for%20Open-vocabulary%20Object%20Detection&entry.906535625=Mingxuan%20Liu%20and%20Tyler%20L.%20Hayes%20and%20Elisa%20Ricci%20and%20Gabriela%20Csurka%20and%20Riccardo%20Volpi&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20%28OvOD%29%20has%20transformed%20detection%20into%20a%0Alanguage-guided%20task%2C%20empowering%20users%20to%20freely%20define%20their%20class%0Avocabularies%20of%20interest%20during%20inference.%20However%2C%20our%20initial%20investigation%0Aindicates%20that%20existing%20OvOD%20detectors%20exhibit%20significant%20variability%20when%0Adealing%20with%20vocabularies%20across%20various%20semantic%20granularities%2C%20posing%20a%0Aconcern%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20introduce%20Semantic%20Hierarchy%0ANexus%20%28SHiNe%29%2C%20a%20novel%20classifier%20that%20uses%20semantic%20knowledge%20from%20class%0Ahierarchies.%20It%20runs%20offline%20in%20three%20steps%3A%20i%29%20it%20retrieves%20relevant%0Asuper-/sub-categories%20from%20a%20hierarchy%20for%20each%20target%20class%3B%20ii%29%20it%20integrates%0Athese%20categories%20into%20hierarchy-aware%20sentences%3B%20iii%29%20it%20fuses%20these%20sentence%0Aembeddings%20to%20generate%20the%20nexus%20classifier%20vector.%20Our%20evaluation%20on%20various%0Adetection%20benchmarks%20demonstrates%20that%20SHiNe%20enhances%20robustness%20across%20diverse%0Avocabulary%20granularities%2C%20achieving%20up%20to%20%2B31.9%25%20mAP50%20with%20ground%20truth%0Ahierarchies%2C%20while%20retaining%20improvements%20using%20hierarchies%20generated%20by%20large%0Alanguage%20models.%20Moreover%2C%20when%20applied%20to%20open-vocabulary%20classification%20on%0AImageNet-1k%2C%20SHiNe%20improves%20the%20CLIP%20zero-shot%20baseline%20by%20%2B2.8%25%20accuracy.%0ASHiNe%20is%20training-free%20and%20can%20be%20seamlessly%20integrated%20with%20any%20off-the-shelf%0AOvOD%20detector%2C%20without%20incurring%20additional%20computational%20overhead%20during%0Ainference.%20The%20code%20is%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10053v1&entry.124074799=Read"},
{"title": "Probabilities of the third type: Statistical Relational Learning and\n  Reasoning with Relative Frequencies", "author": "Felix Weitk\u00e4mper", "abstract": "  Dependencies on the relative frequency of a state in the domain are common\nwhen modelling probabilistic dependencies on relational data. For instance, the\nlikelihood of a school closure during an epidemic might depend on the\nproportion of infected pupils exceeding a threshold. Often, rather than\ndepending on discrete thresholds, dependencies are continuous: for instance,\nthe likelihood of any one mosquito bite transmitting an illness depends on the\nproportion of carrier mosquitoes. Current approaches usually only consider\nprobabilities over possible worlds rather than over domain elements themselves.\nAn exception are the recently introduced Lifted Bayesian Networks for\nConditional Probability Logic, which express discrete dependencies on\nprobabilistic data. We introduce functional lifted Bayesian networks, a\nformalism that explicitly incorporates continuous dependencies on relative\nfrequencies into statistical relational artificial intelligence. and compare\nand contrast them with ifted Bayesian Networks for Conditional Probability\nLogic. Incorporating relative frequencies is not only beneficial to modelling;\nit also provides a more rigorous approach to learning problems where training\nand test or application domains have different sizes. To this end, we provide a\nrepresentation of the asymptotic probability distributions induced by\nfunctional lifted Bayesian networks on domains of increasing sizes. Since that\nrepresentation has well-understood scaling behaviour across domain sizes, it\ncan be used to estimate parameters for a large domain consistently from\nrandomly sampled subpopulations. Furthermore, we show that in parametric\nfamilies of FLBN, convergence is uniform in the parameters, which ensures a\nmeaningful dependence of the asymptotic probabilities on the parameters of the\nmodel.\n", "link": "http://arxiv.org/abs/2202.10367v3", "date": "2024-05-16", "relevancy": 1.8784, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5312}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4922}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilities%20of%20the%20third%20type%3A%20Statistical%20Relational%20Learning%20and%0A%20%20Reasoning%20with%20Relative%20Frequencies&body=Title%3A%20Probabilities%20of%20the%20third%20type%3A%20Statistical%20Relational%20Learning%20and%0A%20%20Reasoning%20with%20Relative%20Frequencies%0AAuthor%3A%20Felix%20Weitk%C3%A4mper%0AAbstract%3A%20%20%20Dependencies%20on%20the%20relative%20frequency%20of%20a%20state%20in%20the%20domain%20are%20common%0Awhen%20modelling%20probabilistic%20dependencies%20on%20relational%20data.%20For%20instance%2C%20the%0Alikelihood%20of%20a%20school%20closure%20during%20an%20epidemic%20might%20depend%20on%20the%0Aproportion%20of%20infected%20pupils%20exceeding%20a%20threshold.%20Often%2C%20rather%20than%0Adepending%20on%20discrete%20thresholds%2C%20dependencies%20are%20continuous%3A%20for%20instance%2C%0Athe%20likelihood%20of%20any%20one%20mosquito%20bite%20transmitting%20an%20illness%20depends%20on%20the%0Aproportion%20of%20carrier%20mosquitoes.%20Current%20approaches%20usually%20only%20consider%0Aprobabilities%20over%20possible%20worlds%20rather%20than%20over%20domain%20elements%20themselves.%0AAn%20exception%20are%20the%20recently%20introduced%20Lifted%20Bayesian%20Networks%20for%0AConditional%20Probability%20Logic%2C%20which%20express%20discrete%20dependencies%20on%0Aprobabilistic%20data.%20We%20introduce%20functional%20lifted%20Bayesian%20networks%2C%20a%0Aformalism%20that%20explicitly%20incorporates%20continuous%20dependencies%20on%20relative%0Afrequencies%20into%20statistical%20relational%20artificial%20intelligence.%20and%20compare%0Aand%20contrast%20them%20with%20ifted%20Bayesian%20Networks%20for%20Conditional%20Probability%0ALogic.%20Incorporating%20relative%20frequencies%20is%20not%20only%20beneficial%20to%20modelling%3B%0Ait%20also%20provides%20a%20more%20rigorous%20approach%20to%20learning%20problems%20where%20training%0Aand%20test%20or%20application%20domains%20have%20different%20sizes.%20To%20this%20end%2C%20we%20provide%20a%0Arepresentation%20of%20the%20asymptotic%20probability%20distributions%20induced%20by%0Afunctional%20lifted%20Bayesian%20networks%20on%20domains%20of%20increasing%20sizes.%20Since%20that%0Arepresentation%20has%20well-understood%20scaling%20behaviour%20across%20domain%20sizes%2C%20it%0Acan%20be%20used%20to%20estimate%20parameters%20for%20a%20large%20domain%20consistently%20from%0Arandomly%20sampled%20subpopulations.%20Furthermore%2C%20we%20show%20that%20in%20parametric%0Afamilies%20of%20FLBN%2C%20convergence%20is%20uniform%20in%20the%20parameters%2C%20which%20ensures%20a%0Ameaningful%20dependence%20of%20the%20asymptotic%20probabilities%20on%20the%20parameters%20of%20the%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.10367v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilities%2520of%2520the%2520third%2520type%253A%2520Statistical%2520Relational%2520Learning%2520and%250A%2520%2520Reasoning%2520with%2520Relative%2520Frequencies%26entry.906535625%3DFelix%2520Weitk%25C3%25A4mper%26entry.1292438233%3D%2520%2520Dependencies%2520on%2520the%2520relative%2520frequency%2520of%2520a%2520state%2520in%2520the%2520domain%2520are%2520common%250Awhen%2520modelling%2520probabilistic%2520dependencies%2520on%2520relational%2520data.%2520For%2520instance%252C%2520the%250Alikelihood%2520of%2520a%2520school%2520closure%2520during%2520an%2520epidemic%2520might%2520depend%2520on%2520the%250Aproportion%2520of%2520infected%2520pupils%2520exceeding%2520a%2520threshold.%2520Often%252C%2520rather%2520than%250Adepending%2520on%2520discrete%2520thresholds%252C%2520dependencies%2520are%2520continuous%253A%2520for%2520instance%252C%250Athe%2520likelihood%2520of%2520any%2520one%2520mosquito%2520bite%2520transmitting%2520an%2520illness%2520depends%2520on%2520the%250Aproportion%2520of%2520carrier%2520mosquitoes.%2520Current%2520approaches%2520usually%2520only%2520consider%250Aprobabilities%2520over%2520possible%2520worlds%2520rather%2520than%2520over%2520domain%2520elements%2520themselves.%250AAn%2520exception%2520are%2520the%2520recently%2520introduced%2520Lifted%2520Bayesian%2520Networks%2520for%250AConditional%2520Probability%2520Logic%252C%2520which%2520express%2520discrete%2520dependencies%2520on%250Aprobabilistic%2520data.%2520We%2520introduce%2520functional%2520lifted%2520Bayesian%2520networks%252C%2520a%250Aformalism%2520that%2520explicitly%2520incorporates%2520continuous%2520dependencies%2520on%2520relative%250Afrequencies%2520into%2520statistical%2520relational%2520artificial%2520intelligence.%2520and%2520compare%250Aand%2520contrast%2520them%2520with%2520ifted%2520Bayesian%2520Networks%2520for%2520Conditional%2520Probability%250ALogic.%2520Incorporating%2520relative%2520frequencies%2520is%2520not%2520only%2520beneficial%2520to%2520modelling%253B%250Ait%2520also%2520provides%2520a%2520more%2520rigorous%2520approach%2520to%2520learning%2520problems%2520where%2520training%250Aand%2520test%2520or%2520application%2520domains%2520have%2520different%2520sizes.%2520To%2520this%2520end%252C%2520we%2520provide%2520a%250Arepresentation%2520of%2520the%2520asymptotic%2520probability%2520distributions%2520induced%2520by%250Afunctional%2520lifted%2520Bayesian%2520networks%2520on%2520domains%2520of%2520increasing%2520sizes.%2520Since%2520that%250Arepresentation%2520has%2520well-understood%2520scaling%2520behaviour%2520across%2520domain%2520sizes%252C%2520it%250Acan%2520be%2520used%2520to%2520estimate%2520parameters%2520for%2520a%2520large%2520domain%2520consistently%2520from%250Arandomly%2520sampled%2520subpopulations.%2520Furthermore%252C%2520we%2520show%2520that%2520in%2520parametric%250Afamilies%2520of%2520FLBN%252C%2520convergence%2520is%2520uniform%2520in%2520the%2520parameters%252C%2520which%2520ensures%2520a%250Ameaningful%2520dependence%2520of%2520the%2520asymptotic%2520probabilities%2520on%2520the%2520parameters%2520of%2520the%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.10367v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilities%20of%20the%20third%20type%3A%20Statistical%20Relational%20Learning%20and%0A%20%20Reasoning%20with%20Relative%20Frequencies&entry.906535625=Felix%20Weitk%C3%A4mper&entry.1292438233=%20%20Dependencies%20on%20the%20relative%20frequency%20of%20a%20state%20in%20the%20domain%20are%20common%0Awhen%20modelling%20probabilistic%20dependencies%20on%20relational%20data.%20For%20instance%2C%20the%0Alikelihood%20of%20a%20school%20closure%20during%20an%20epidemic%20might%20depend%20on%20the%0Aproportion%20of%20infected%20pupils%20exceeding%20a%20threshold.%20Often%2C%20rather%20than%0Adepending%20on%20discrete%20thresholds%2C%20dependencies%20are%20continuous%3A%20for%20instance%2C%0Athe%20likelihood%20of%20any%20one%20mosquito%20bite%20transmitting%20an%20illness%20depends%20on%20the%0Aproportion%20of%20carrier%20mosquitoes.%20Current%20approaches%20usually%20only%20consider%0Aprobabilities%20over%20possible%20worlds%20rather%20than%20over%20domain%20elements%20themselves.%0AAn%20exception%20are%20the%20recently%20introduced%20Lifted%20Bayesian%20Networks%20for%0AConditional%20Probability%20Logic%2C%20which%20express%20discrete%20dependencies%20on%0Aprobabilistic%20data.%20We%20introduce%20functional%20lifted%20Bayesian%20networks%2C%20a%0Aformalism%20that%20explicitly%20incorporates%20continuous%20dependencies%20on%20relative%0Afrequencies%20into%20statistical%20relational%20artificial%20intelligence.%20and%20compare%0Aand%20contrast%20them%20with%20ifted%20Bayesian%20Networks%20for%20Conditional%20Probability%0ALogic.%20Incorporating%20relative%20frequencies%20is%20not%20only%20beneficial%20to%20modelling%3B%0Ait%20also%20provides%20a%20more%20rigorous%20approach%20to%20learning%20problems%20where%20training%0Aand%20test%20or%20application%20domains%20have%20different%20sizes.%20To%20this%20end%2C%20we%20provide%20a%0Arepresentation%20of%20the%20asymptotic%20probability%20distributions%20induced%20by%0Afunctional%20lifted%20Bayesian%20networks%20on%20domains%20of%20increasing%20sizes.%20Since%20that%0Arepresentation%20has%20well-understood%20scaling%20behaviour%20across%20domain%20sizes%2C%20it%0Acan%20be%20used%20to%20estimate%20parameters%20for%20a%20large%20domain%20consistently%20from%0Arandomly%20sampled%20subpopulations.%20Furthermore%2C%20we%20show%20that%20in%20parametric%0Afamilies%20of%20FLBN%2C%20convergence%20is%20uniform%20in%20the%20parameters%2C%20which%20ensures%20a%0Ameaningful%20dependence%20of%20the%20asymptotic%20probabilities%20on%20the%20parameters%20of%20the%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.10367v3&entry.124074799=Read"},
{"title": "Distribution of Test Statistic for Euclidean Distance Matrices", "author": "Dawson Beatty", "abstract": "  Methods for global navigation satellite system fault detection using\nEuclidean Distance Matrices have been presented recently in the literature.\nPublished methods define a test statistic in terms of eigenvalues of a certain\nmatrix, but the distribution of the test statistic was not known, which\npresented a barrier to practical implementation. This document was a personal\ncorrespondence from Beatty to Derek Knowles. It includes a brief derivation of\nthe distribution of the test statistic and a representative case showing that\nthe theoretical distribution closely matches a simulated empirical\ndistribution.\n", "link": "http://arxiv.org/abs/2405.10049v1", "date": "2024-05-16", "relevancy": 1.876, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3789}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution%20of%20Test%20Statistic%20for%20Euclidean%20Distance%20Matrices&body=Title%3A%20Distribution%20of%20Test%20Statistic%20for%20Euclidean%20Distance%20Matrices%0AAuthor%3A%20Dawson%20Beatty%0AAbstract%3A%20%20%20Methods%20for%20global%20navigation%20satellite%20system%20fault%20detection%20using%0AEuclidean%20Distance%20Matrices%20have%20been%20presented%20recently%20in%20the%20literature.%0APublished%20methods%20define%20a%20test%20statistic%20in%20terms%20of%20eigenvalues%20of%20a%20certain%0Amatrix%2C%20but%20the%20distribution%20of%20the%20test%20statistic%20was%20not%20known%2C%20which%0Apresented%20a%20barrier%20to%20practical%20implementation.%20This%20document%20was%20a%20personal%0Acorrespondence%20from%20Beatty%20to%20Derek%20Knowles.%20It%20includes%20a%20brief%20derivation%20of%0Athe%20distribution%20of%20the%20test%20statistic%20and%20a%20representative%20case%20showing%20that%0Athe%20theoretical%20distribution%20closely%20matches%20a%20simulated%20empirical%0Adistribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution%2520of%2520Test%2520Statistic%2520for%2520Euclidean%2520Distance%2520Matrices%26entry.906535625%3DDawson%2520Beatty%26entry.1292438233%3D%2520%2520Methods%2520for%2520global%2520navigation%2520satellite%2520system%2520fault%2520detection%2520using%250AEuclidean%2520Distance%2520Matrices%2520have%2520been%2520presented%2520recently%2520in%2520the%2520literature.%250APublished%2520methods%2520define%2520a%2520test%2520statistic%2520in%2520terms%2520of%2520eigenvalues%2520of%2520a%2520certain%250Amatrix%252C%2520but%2520the%2520distribution%2520of%2520the%2520test%2520statistic%2520was%2520not%2520known%252C%2520which%250Apresented%2520a%2520barrier%2520to%2520practical%2520implementation.%2520This%2520document%2520was%2520a%2520personal%250Acorrespondence%2520from%2520Beatty%2520to%2520Derek%2520Knowles.%2520It%2520includes%2520a%2520brief%2520derivation%2520of%250Athe%2520distribution%2520of%2520the%2520test%2520statistic%2520and%2520a%2520representative%2520case%2520showing%2520that%250Athe%2520theoretical%2520distribution%2520closely%2520matches%2520a%2520simulated%2520empirical%250Adistribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution%20of%20Test%20Statistic%20for%20Euclidean%20Distance%20Matrices&entry.906535625=Dawson%20Beatty&entry.1292438233=%20%20Methods%20for%20global%20navigation%20satellite%20system%20fault%20detection%20using%0AEuclidean%20Distance%20Matrices%20have%20been%20presented%20recently%20in%20the%20literature.%0APublished%20methods%20define%20a%20test%20statistic%20in%20terms%20of%20eigenvalues%20of%20a%20certain%0Amatrix%2C%20but%20the%20distribution%20of%20the%20test%20statistic%20was%20not%20known%2C%20which%0Apresented%20a%20barrier%20to%20practical%20implementation.%20This%20document%20was%20a%20personal%0Acorrespondence%20from%20Beatty%20to%20Derek%20Knowles.%20It%20includes%20a%20brief%20derivation%20of%0Athe%20distribution%20of%20the%20test%20statistic%20and%20a%20representative%20case%20showing%20that%0Athe%20theoretical%20distribution%20closely%20matches%20a%20simulated%20empirical%0Adistribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10049v1&entry.124074799=Read"},
{"title": "Listen Again and Choose the Right Answer: A New Paradigm for Automatic\n  Speech Recognition with Large Language Models", "author": "Yuchen Hu and Chen Chen and Chengwei Qin and Qiushi Zhu and Eng Siong Chng and Ruizhe Li", "abstract": "  Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which aims to\npredict the ground-truth transcription from the decoded N-best hypotheses.\nThanks to the strong language generation ability of LLMs and rich information\nin the N-best list, GER shows great effectiveness in enhancing ASR results.\nHowever, it still suffers from two limitations: 1) LLMs are unaware of the\nsource speech during GER, which may lead to results that are grammatically\ncorrect but violate the source speech content, 2) N-best hypotheses usually\nonly vary in a few tokens, making it redundant to send all of them for GER,\nwhich could confuse LLM about which tokens to focus on and thus lead to\nincreased miscorrection. In this paper, we propose ClozeGER, a new paradigm for\nASR generative error correction. First, we introduce a multimodal LLM (i.e.,\nSpeechGPT) to receive source speech as extra input to improve the fidelity of\ncorrection output. Then, we reformat GER as a cloze test with logits\ncalibration to remove the input information redundancy and simplify GER with\nclear instructions. Experiments show that ClozeGER achieves a new breakthrough\nover vanilla GER on 9 popular ASR datasets.\n", "link": "http://arxiv.org/abs/2405.10025v1", "date": "2024-05-16", "relevancy": 1.8665, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4808}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Listen%20Again%20and%20Choose%20the%20Right%20Answer%3A%20A%20New%20Paradigm%20for%20Automatic%0A%20%20Speech%20Recognition%20with%20Large%20Language%20Models&body=Title%3A%20Listen%20Again%20and%20Choose%20the%20Right%20Answer%3A%20A%20New%20Paradigm%20for%20Automatic%0A%20%20Speech%20Recognition%20with%20Large%20Language%20Models%0AAuthor%3A%20Yuchen%20Hu%20and%20Chen%20Chen%20and%20Chengwei%20Qin%20and%20Qiushi%20Zhu%20and%20Eng%20Siong%20Chng%20and%20Ruizhe%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20promoted%20generative%0Aerror%20correction%20%28GER%29%20for%20automatic%20speech%20recognition%20%28ASR%29%2C%20which%20aims%20to%0Apredict%20the%20ground-truth%20transcription%20from%20the%20decoded%20N-best%20hypotheses.%0AThanks%20to%20the%20strong%20language%20generation%20ability%20of%20LLMs%20and%20rich%20information%0Ain%20the%20N-best%20list%2C%20GER%20shows%20great%20effectiveness%20in%20enhancing%20ASR%20results.%0AHowever%2C%20it%20still%20suffers%20from%20two%20limitations%3A%201%29%20LLMs%20are%20unaware%20of%20the%0Asource%20speech%20during%20GER%2C%20which%20may%20lead%20to%20results%20that%20are%20grammatically%0Acorrect%20but%20violate%20the%20source%20speech%20content%2C%202%29%20N-best%20hypotheses%20usually%0Aonly%20vary%20in%20a%20few%20tokens%2C%20making%20it%20redundant%20to%20send%20all%20of%20them%20for%20GER%2C%0Awhich%20could%20confuse%20LLM%20about%20which%20tokens%20to%20focus%20on%20and%20thus%20lead%20to%0Aincreased%20miscorrection.%20In%20this%20paper%2C%20we%20propose%20ClozeGER%2C%20a%20new%20paradigm%20for%0AASR%20generative%20error%20correction.%20First%2C%20we%20introduce%20a%20multimodal%20LLM%20%28i.e.%2C%0ASpeechGPT%29%20to%20receive%20source%20speech%20as%20extra%20input%20to%20improve%20the%20fidelity%20of%0Acorrection%20output.%20Then%2C%20we%20reformat%20GER%20as%20a%20cloze%20test%20with%20logits%0Acalibration%20to%20remove%20the%20input%20information%20redundancy%20and%20simplify%20GER%20with%0Aclear%20instructions.%20Experiments%20show%20that%20ClozeGER%20achieves%20a%20new%20breakthrough%0Aover%20vanilla%20GER%20on%209%20popular%20ASR%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DListen%2520Again%2520and%2520Choose%2520the%2520Right%2520Answer%253A%2520A%2520New%2520Paradigm%2520for%2520Automatic%250A%2520%2520Speech%2520Recognition%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DYuchen%2520Hu%2520and%2520Chen%2520Chen%2520and%2520Chengwei%2520Qin%2520and%2520Qiushi%2520Zhu%2520and%2520Eng%2520Siong%2520Chng%2520and%2520Ruizhe%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520promoted%2520generative%250Aerror%2520correction%2520%2528GER%2529%2520for%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%252C%2520which%2520aims%2520to%250Apredict%2520the%2520ground-truth%2520transcription%2520from%2520the%2520decoded%2520N-best%2520hypotheses.%250AThanks%2520to%2520the%2520strong%2520language%2520generation%2520ability%2520of%2520LLMs%2520and%2520rich%2520information%250Ain%2520the%2520N-best%2520list%252C%2520GER%2520shows%2520great%2520effectiveness%2520in%2520enhancing%2520ASR%2520results.%250AHowever%252C%2520it%2520still%2520suffers%2520from%2520two%2520limitations%253A%25201%2529%2520LLMs%2520are%2520unaware%2520of%2520the%250Asource%2520speech%2520during%2520GER%252C%2520which%2520may%2520lead%2520to%2520results%2520that%2520are%2520grammatically%250Acorrect%2520but%2520violate%2520the%2520source%2520speech%2520content%252C%25202%2529%2520N-best%2520hypotheses%2520usually%250Aonly%2520vary%2520in%2520a%2520few%2520tokens%252C%2520making%2520it%2520redundant%2520to%2520send%2520all%2520of%2520them%2520for%2520GER%252C%250Awhich%2520could%2520confuse%2520LLM%2520about%2520which%2520tokens%2520to%2520focus%2520on%2520and%2520thus%2520lead%2520to%250Aincreased%2520miscorrection.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ClozeGER%252C%2520a%2520new%2520paradigm%2520for%250AASR%2520generative%2520error%2520correction.%2520First%252C%2520we%2520introduce%2520a%2520multimodal%2520LLM%2520%2528i.e.%252C%250ASpeechGPT%2529%2520to%2520receive%2520source%2520speech%2520as%2520extra%2520input%2520to%2520improve%2520the%2520fidelity%2520of%250Acorrection%2520output.%2520Then%252C%2520we%2520reformat%2520GER%2520as%2520a%2520cloze%2520test%2520with%2520logits%250Acalibration%2520to%2520remove%2520the%2520input%2520information%2520redundancy%2520and%2520simplify%2520GER%2520with%250Aclear%2520instructions.%2520Experiments%2520show%2520that%2520ClozeGER%2520achieves%2520a%2520new%2520breakthrough%250Aover%2520vanilla%2520GER%2520on%25209%2520popular%2520ASR%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Listen%20Again%20and%20Choose%20the%20Right%20Answer%3A%20A%20New%20Paradigm%20for%20Automatic%0A%20%20Speech%20Recognition%20with%20Large%20Language%20Models&entry.906535625=Yuchen%20Hu%20and%20Chen%20Chen%20and%20Chengwei%20Qin%20and%20Qiushi%20Zhu%20and%20Eng%20Siong%20Chng%20and%20Ruizhe%20Li&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20promoted%20generative%0Aerror%20correction%20%28GER%29%20for%20automatic%20speech%20recognition%20%28ASR%29%2C%20which%20aims%20to%0Apredict%20the%20ground-truth%20transcription%20from%20the%20decoded%20N-best%20hypotheses.%0AThanks%20to%20the%20strong%20language%20generation%20ability%20of%20LLMs%20and%20rich%20information%0Ain%20the%20N-best%20list%2C%20GER%20shows%20great%20effectiveness%20in%20enhancing%20ASR%20results.%0AHowever%2C%20it%20still%20suffers%20from%20two%20limitations%3A%201%29%20LLMs%20are%20unaware%20of%20the%0Asource%20speech%20during%20GER%2C%20which%20may%20lead%20to%20results%20that%20are%20grammatically%0Acorrect%20but%20violate%20the%20source%20speech%20content%2C%202%29%20N-best%20hypotheses%20usually%0Aonly%20vary%20in%20a%20few%20tokens%2C%20making%20it%20redundant%20to%20send%20all%20of%20them%20for%20GER%2C%0Awhich%20could%20confuse%20LLM%20about%20which%20tokens%20to%20focus%20on%20and%20thus%20lead%20to%0Aincreased%20miscorrection.%20In%20this%20paper%2C%20we%20propose%20ClozeGER%2C%20a%20new%20paradigm%20for%0AASR%20generative%20error%20correction.%20First%2C%20we%20introduce%20a%20multimodal%20LLM%20%28i.e.%2C%0ASpeechGPT%29%20to%20receive%20source%20speech%20as%20extra%20input%20to%20improve%20the%20fidelity%20of%0Acorrection%20output.%20Then%2C%20we%20reformat%20GER%20as%20a%20cloze%20test%20with%20logits%0Acalibration%20to%20remove%20the%20input%20information%20redundancy%20and%20simplify%20GER%20with%0Aclear%20instructions.%20Experiments%20show%20that%20ClozeGER%20achieves%20a%20new%20breakthrough%0Aover%20vanilla%20GER%20on%209%20popular%20ASR%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10025v1&entry.124074799=Read"},
{"title": "ROCOv2: Radiology Objects in COntext Version 2, an Updated Multimodal\n  Image Dataset", "author": "Johannes R\u00fcckert and Louise Bloch and Raphael Br\u00fcngel and Ahmad Idrissi-Yaghir and Henning Sch\u00e4fer and Cynthia S. Schmidt and Sven Koitka and Obioma Pelka and Asma Ben Abacha and Alba G. Seco de Herrera and Henning M\u00fcller and Peter A. Horn and Felix Nensa and Christoph M. Friedrich", "abstract": "  Automated medical image analysis systems often require large amounts of\ntraining data with high quality labels, which are difficult and time consuming\nto generate. This paper introduces Radiology Object in COntext version 2\n(ROCOv2), a multimodal dataset consisting of radiological images and associated\nmedical concepts and captions extracted from the PMC Open Access subset. It is\nan updated version of the ROCO dataset published in 2018, and adds 35,705 new\nimages added to PMC since 2018. It further provides manually curated concepts\nfor imaging modalities with additional anatomical and directional concepts for\nX-rays. The dataset consists of 79,789 images and has been used, with minor\nmodifications, in the concept detection and caption prediction tasks of\nImageCLEFmedical Caption 2023. The dataset is suitable for training image\nannotation models based on image-caption pairs, or for multi-label image\nclassification using Unified Medical Language System (UMLS) concepts provided\nwith each image. In addition, it can serve for pre-training of medical domain\nmodels, and evaluation of deep learning models for multi-task learning.\n", "link": "http://arxiv.org/abs/2405.10004v1", "date": "2024-05-16", "relevancy": 1.8649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4676}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset&body=Title%3A%20ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset%0AAuthor%3A%20Johannes%20R%C3%BCckert%20and%20Louise%20Bloch%20and%20Raphael%20Br%C3%BCngel%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Cynthia%20S.%20Schmidt%20and%20Sven%20Koitka%20and%20Obioma%20Pelka%20and%20Asma%20Ben%20Abacha%20and%20Alba%20G.%20Seco%20de%20Herrera%20and%20Henning%20M%C3%BCller%20and%20Peter%20A.%20Horn%20and%20Felix%20Nensa%20and%20Christoph%20M.%20Friedrich%0AAbstract%3A%20%20%20Automated%20medical%20image%20analysis%20systems%20often%20require%20large%20amounts%20of%0Atraining%20data%20with%20high%20quality%20labels%2C%20which%20are%20difficult%20and%20time%20consuming%0Ato%20generate.%20This%20paper%20introduces%20Radiology%20Object%20in%20COntext%20version%202%0A%28ROCOv2%29%2C%20a%20multimodal%20dataset%20consisting%20of%20radiological%20images%20and%20associated%0Amedical%20concepts%20and%20captions%20extracted%20from%20the%20PMC%20Open%20Access%20subset.%20It%20is%0Aan%20updated%20version%20of%20the%20ROCO%20dataset%20published%20in%202018%2C%20and%20adds%2035%2C705%20new%0Aimages%20added%20to%20PMC%20since%202018.%20It%20further%20provides%20manually%20curated%20concepts%0Afor%20imaging%20modalities%20with%20additional%20anatomical%20and%20directional%20concepts%20for%0AX-rays.%20The%20dataset%20consists%20of%2079%2C789%20images%20and%20has%20been%20used%2C%20with%20minor%0Amodifications%2C%20in%20the%20concept%20detection%20and%20caption%20prediction%20tasks%20of%0AImageCLEFmedical%20Caption%202023.%20The%20dataset%20is%20suitable%20for%20training%20image%0Aannotation%20models%20based%20on%20image-caption%20pairs%2C%20or%20for%20multi-label%20image%0Aclassification%20using%20Unified%20Medical%20Language%20System%20%28UMLS%29%20concepts%20provided%0Awith%20each%20image.%20In%20addition%2C%20it%20can%20serve%20for%20pre-training%20of%20medical%20domain%0Amodels%2C%20and%20evaluation%20of%20deep%20learning%20models%20for%20multi-task%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROCOv2%253A%2520Radiology%2520Objects%2520in%2520COntext%2520Version%25202%252C%2520an%2520Updated%2520Multimodal%250A%2520%2520Image%2520Dataset%26entry.906535625%3DJohannes%2520R%25C3%25BCckert%2520and%2520Louise%2520Bloch%2520and%2520Raphael%2520Br%25C3%25BCngel%2520and%2520Ahmad%2520Idrissi-Yaghir%2520and%2520Henning%2520Sch%25C3%25A4fer%2520and%2520Cynthia%2520S.%2520Schmidt%2520and%2520Sven%2520Koitka%2520and%2520Obioma%2520Pelka%2520and%2520Asma%2520Ben%2520Abacha%2520and%2520Alba%2520G.%2520Seco%2520de%2520Herrera%2520and%2520Henning%2520M%25C3%25BCller%2520and%2520Peter%2520A.%2520Horn%2520and%2520Felix%2520Nensa%2520and%2520Christoph%2520M.%2520Friedrich%26entry.1292438233%3D%2520%2520Automated%2520medical%2520image%2520analysis%2520systems%2520often%2520require%2520large%2520amounts%2520of%250Atraining%2520data%2520with%2520high%2520quality%2520labels%252C%2520which%2520are%2520difficult%2520and%2520time%2520consuming%250Ato%2520generate.%2520This%2520paper%2520introduces%2520Radiology%2520Object%2520in%2520COntext%2520version%25202%250A%2528ROCOv2%2529%252C%2520a%2520multimodal%2520dataset%2520consisting%2520of%2520radiological%2520images%2520and%2520associated%250Amedical%2520concepts%2520and%2520captions%2520extracted%2520from%2520the%2520PMC%2520Open%2520Access%2520subset.%2520It%2520is%250Aan%2520updated%2520version%2520of%2520the%2520ROCO%2520dataset%2520published%2520in%25202018%252C%2520and%2520adds%252035%252C705%2520new%250Aimages%2520added%2520to%2520PMC%2520since%25202018.%2520It%2520further%2520provides%2520manually%2520curated%2520concepts%250Afor%2520imaging%2520modalities%2520with%2520additional%2520anatomical%2520and%2520directional%2520concepts%2520for%250AX-rays.%2520The%2520dataset%2520consists%2520of%252079%252C789%2520images%2520and%2520has%2520been%2520used%252C%2520with%2520minor%250Amodifications%252C%2520in%2520the%2520concept%2520detection%2520and%2520caption%2520prediction%2520tasks%2520of%250AImageCLEFmedical%2520Caption%25202023.%2520The%2520dataset%2520is%2520suitable%2520for%2520training%2520image%250Aannotation%2520models%2520based%2520on%2520image-caption%2520pairs%252C%2520or%2520for%2520multi-label%2520image%250Aclassification%2520using%2520Unified%2520Medical%2520Language%2520System%2520%2528UMLS%2529%2520concepts%2520provided%250Awith%2520each%2520image.%2520In%2520addition%252C%2520it%2520can%2520serve%2520for%2520pre-training%2520of%2520medical%2520domain%250Amodels%252C%2520and%2520evaluation%2520of%2520deep%2520learning%2520models%2520for%2520multi-task%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROCOv2%3A%20Radiology%20Objects%20in%20COntext%20Version%202%2C%20an%20Updated%20Multimodal%0A%20%20Image%20Dataset&entry.906535625=Johannes%20R%C3%BCckert%20and%20Louise%20Bloch%20and%20Raphael%20Br%C3%BCngel%20and%20Ahmad%20Idrissi-Yaghir%20and%20Henning%20Sch%C3%A4fer%20and%20Cynthia%20S.%20Schmidt%20and%20Sven%20Koitka%20and%20Obioma%20Pelka%20and%20Asma%20Ben%20Abacha%20and%20Alba%20G.%20Seco%20de%20Herrera%20and%20Henning%20M%C3%BCller%20and%20Peter%20A.%20Horn%20and%20Felix%20Nensa%20and%20Christoph%20M.%20Friedrich&entry.1292438233=%20%20Automated%20medical%20image%20analysis%20systems%20often%20require%20large%20amounts%20of%0Atraining%20data%20with%20high%20quality%20labels%2C%20which%20are%20difficult%20and%20time%20consuming%0Ato%20generate.%20This%20paper%20introduces%20Radiology%20Object%20in%20COntext%20version%202%0A%28ROCOv2%29%2C%20a%20multimodal%20dataset%20consisting%20of%20radiological%20images%20and%20associated%0Amedical%20concepts%20and%20captions%20extracted%20from%20the%20PMC%20Open%20Access%20subset.%20It%20is%0Aan%20updated%20version%20of%20the%20ROCO%20dataset%20published%20in%202018%2C%20and%20adds%2035%2C705%20new%0Aimages%20added%20to%20PMC%20since%202018.%20It%20further%20provides%20manually%20curated%20concepts%0Afor%20imaging%20modalities%20with%20additional%20anatomical%20and%20directional%20concepts%20for%0AX-rays.%20The%20dataset%20consists%20of%2079%2C789%20images%20and%20has%20been%20used%2C%20with%20minor%0Amodifications%2C%20in%20the%20concept%20detection%20and%20caption%20prediction%20tasks%20of%0AImageCLEFmedical%20Caption%202023.%20The%20dataset%20is%20suitable%20for%20training%20image%0Aannotation%20models%20based%20on%20image-caption%20pairs%2C%20or%20for%20multi-label%20image%0Aclassification%20using%20Unified%20Medical%20Language%20System%20%28UMLS%29%20concepts%20provided%0Awith%20each%20image.%20In%20addition%2C%20it%20can%20serve%20for%20pre-training%20of%20medical%20domain%0Amodels%2C%20and%20evaluation%20of%20deep%20learning%20models%20for%20multi-task%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10004v1&entry.124074799=Read"},
{"title": "The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows", "author": "Humberto Reyes-Gonzalez and Riccardo Torre", "abstract": "  We propose the NFLikelihood, an unsupervised version, based on Normalizing\nFlows, of the DNNLikelihood proposed in Ref.[1]. We show, through realistic\nexamples, how Autoregressive Flows, based on affine and rational quadratic\nspline bijectors, are able to learn complicated high-dimensional Likelihoods\narising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysis\nexample already considered in the literature and on two Effective Field Theory\nfits of flavor and electroweak observables, whose samples have been obtained\nthrought the HEPFit code. We discuss advantages and disadvantages of the\nunsupervised approach with respect to the supervised one and discuss possible\ninterplays of the two.\n", "link": "http://arxiv.org/abs/2309.09743v3", "date": "2024-05-16", "relevancy": 1.8601, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4751}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows&body=Title%3A%20The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows%0AAuthor%3A%20Humberto%20Reyes-Gonzalez%20and%20Riccardo%20Torre%0AAbstract%3A%20%20%20We%20propose%20the%20NFLikelihood%2C%20an%20unsupervised%20version%2C%20based%20on%20Normalizing%0AFlows%2C%20of%20the%20DNNLikelihood%20proposed%20in%20Ref.%5B1%5D.%20We%20show%2C%20through%20realistic%0Aexamples%2C%20how%20Autoregressive%20Flows%2C%20based%20on%20affine%20and%20rational%20quadratic%0Aspline%20bijectors%2C%20are%20able%20to%20learn%20complicated%20high-dimensional%20Likelihoods%0Aarising%20in%20High%20Energy%20Physics%20%28HEP%29%20analyses.%20We%20focus%20on%20a%20toy%20LHC%20analysis%0Aexample%20already%20considered%20in%20the%20literature%20and%20on%20two%20Effective%20Field%20Theory%0Afits%20of%20flavor%20and%20electroweak%20observables%2C%20whose%20samples%20have%20been%20obtained%0Athrought%20the%20HEPFit%20code.%20We%20discuss%20advantages%20and%20disadvantages%20of%20the%0Aunsupervised%20approach%20with%20respect%20to%20the%20supervised%20one%20and%20discuss%20possible%0Ainterplays%20of%20the%20two.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520NFLikelihood%253A%2520an%2520unsupervised%2520DNNLikelihood%2520from%2520Normalizing%2520Flows%26entry.906535625%3DHumberto%2520Reyes-Gonzalez%2520and%2520Riccardo%2520Torre%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520NFLikelihood%252C%2520an%2520unsupervised%2520version%252C%2520based%2520on%2520Normalizing%250AFlows%252C%2520of%2520the%2520DNNLikelihood%2520proposed%2520in%2520Ref.%255B1%255D.%2520We%2520show%252C%2520through%2520realistic%250Aexamples%252C%2520how%2520Autoregressive%2520Flows%252C%2520based%2520on%2520affine%2520and%2520rational%2520quadratic%250Aspline%2520bijectors%252C%2520are%2520able%2520to%2520learn%2520complicated%2520high-dimensional%2520Likelihoods%250Aarising%2520in%2520High%2520Energy%2520Physics%2520%2528HEP%2529%2520analyses.%2520We%2520focus%2520on%2520a%2520toy%2520LHC%2520analysis%250Aexample%2520already%2520considered%2520in%2520the%2520literature%2520and%2520on%2520two%2520Effective%2520Field%2520Theory%250Afits%2520of%2520flavor%2520and%2520electroweak%2520observables%252C%2520whose%2520samples%2520have%2520been%2520obtained%250Athrought%2520the%2520HEPFit%2520code.%2520We%2520discuss%2520advantages%2520and%2520disadvantages%2520of%2520the%250Aunsupervised%2520approach%2520with%2520respect%2520to%2520the%2520supervised%2520one%2520and%2520discuss%2520possible%250Ainterplays%2520of%2520the%2520two.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NFLikelihood%3A%20an%20unsupervised%20DNNLikelihood%20from%20Normalizing%20Flows&entry.906535625=Humberto%20Reyes-Gonzalez%20and%20Riccardo%20Torre&entry.1292438233=%20%20We%20propose%20the%20NFLikelihood%2C%20an%20unsupervised%20version%2C%20based%20on%20Normalizing%0AFlows%2C%20of%20the%20DNNLikelihood%20proposed%20in%20Ref.%5B1%5D.%20We%20show%2C%20through%20realistic%0Aexamples%2C%20how%20Autoregressive%20Flows%2C%20based%20on%20affine%20and%20rational%20quadratic%0Aspline%20bijectors%2C%20are%20able%20to%20learn%20complicated%20high-dimensional%20Likelihoods%0Aarising%20in%20High%20Energy%20Physics%20%28HEP%29%20analyses.%20We%20focus%20on%20a%20toy%20LHC%20analysis%0Aexample%20already%20considered%20in%20the%20literature%20and%20on%20two%20Effective%20Field%20Theory%0Afits%20of%20flavor%20and%20electroweak%20observables%2C%20whose%20samples%20have%20been%20obtained%0Athrought%20the%20HEPFit%20code.%20We%20discuss%20advantages%20and%20disadvantages%20of%20the%0Aunsupervised%20approach%20with%20respect%20to%20the%20supervised%20one%20and%20discuss%20possible%0Ainterplays%20of%20the%20two.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09743v3&entry.124074799=Read"},
{"title": "Invariant Risk Minimization Is A Total Variation Model", "author": "Zhao-Rong Lai and Weiwen Wang", "abstract": "  Invariant risk minimization (IRM) is an arising approach to generalize\ninvariant features to different environments in machine learning. While most\nrelated works focus on new IRM settings or new application scenarios, the\nmathematical essence of IRM remains to be properly explained. We verify that\nIRM is essentially a total variation based on $L^2$ norm (TV-$\\ell_2$) of the\nlearning risk with respect to the classifier variable. Moreover, we propose a\nnovel IRM framework based on the TV-$\\ell_1$ model. It not only expands the\nclasses of functions that can be used as the learning risk, but also has robust\nperformance in denoising and invariant feature preservation based on the coarea\nformula. We also illustrate some requirements for IRM-TV-$\\ell_1$ to achieve\nout-of-distribution generalization. Experimental results show that the proposed\nframework achieves competitive performance in several benchmark machine\nlearning scenarios.\n", "link": "http://arxiv.org/abs/2405.01389v4", "date": "2024-05-16", "relevancy": 1.8494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4653}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4622}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&body=Title%3A%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model%0AAuthor%3A%20Zhao-Rong%20Lai%20and%20Weiwen%20Wang%0AAbstract%3A%20%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01389v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Risk%2520Minimization%2520Is%2520A%2520Total%2520Variation%2520Model%26entry.906535625%3DZhao-Rong%2520Lai%2520and%2520Weiwen%2520Wang%26entry.1292438233%3D%2520%2520Invariant%2520risk%2520minimization%2520%2528IRM%2529%2520is%2520an%2520arising%2520approach%2520to%2520generalize%250Ainvariant%2520features%2520to%2520different%2520environments%2520in%2520machine%2520learning.%2520While%2520most%250Arelated%2520works%2520focus%2520on%2520new%2520IRM%2520settings%2520or%2520new%2520application%2520scenarios%252C%2520the%250Amathematical%2520essence%2520of%2520IRM%2520remains%2520to%2520be%2520properly%2520explained.%2520We%2520verify%2520that%250AIRM%2520is%2520essentially%2520a%2520total%2520variation%2520based%2520on%2520%2524L%255E2%2524%2520norm%2520%2528TV-%2524%255Cell_2%2524%2529%2520of%2520the%250Alearning%2520risk%2520with%2520respect%2520to%2520the%2520classifier%2520variable.%2520Moreover%252C%2520we%2520propose%2520a%250Anovel%2520IRM%2520framework%2520based%2520on%2520the%2520TV-%2524%255Cell_1%2524%2520model.%2520It%2520not%2520only%2520expands%2520the%250Aclasses%2520of%2520functions%2520that%2520can%2520be%2520used%2520as%2520the%2520learning%2520risk%252C%2520but%2520also%2520has%2520robust%250Aperformance%2520in%2520denoising%2520and%2520invariant%2520feature%2520preservation%2520based%2520on%2520the%2520coarea%250Aformula.%2520We%2520also%2520illustrate%2520some%2520requirements%2520for%2520IRM-TV-%2524%255Cell_1%2524%2520to%2520achieve%250Aout-of-distribution%2520generalization.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Aframework%2520achieves%2520competitive%2520performance%2520in%2520several%2520benchmark%2520machine%250Alearning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01389v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&entry.906535625=Zhao-Rong%20Lai%20and%20Weiwen%20Wang&entry.1292438233=%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01389v4&entry.124074799=Read"},
{"title": "Frequency-Domain Refinement with Multiscale Diffusion for Super\n  Resolution", "author": "Xingjian Wang and Li Chai and Jiming Chen", "abstract": "  The performance of single image super-resolution depends heavily on how to\ngenerate and complement high-frequency details to low-resolution images.\nRecently, diffusion-based models exhibit great potential in generating\nhigh-quality images for super-resolution tasks. However, existing models\nencounter difficulties in directly predicting high-frequency information of\nwide bandwidth by solely utilizing the high-resolution ground truth as the\ntarget for all sampling timesteps. To tackle this problem and achieve\nhigher-quality super-resolution, we propose a novel Frequency Domain-guided\nmultiscale Diffusion model (FDDiff), which decomposes the high-frequency\ninformation complementing process into finer-grained steps. In particular, a\nwavelet packet-based frequency complement chain is developed to provide\nmultiscale intermediate targets with increasing bandwidth for reverse diffusion\nprocess. Then FDDiff guides reverse diffusion process to progressively\ncomplement the missing high-frequency details over timesteps. Moreover, we\ndesign a multiscale frequency refinement network to predict the required\nhigh-frequency components at multiple scales within one unified network.\nComprehensive evaluations on popular benchmarks are conducted, and demonstrate\nthat FDDiff outperforms prior generative methods with higher-fidelity\nsuper-resolution results.\n", "link": "http://arxiv.org/abs/2405.10014v1", "date": "2024-05-16", "relevancy": 1.8474, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6437}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6094}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Domain%20Refinement%20with%20Multiscale%20Diffusion%20for%20Super%0A%20%20Resolution&body=Title%3A%20Frequency-Domain%20Refinement%20with%20Multiscale%20Diffusion%20for%20Super%0A%20%20Resolution%0AAuthor%3A%20Xingjian%20Wang%20and%20Li%20Chai%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20The%20performance%20of%20single%20image%20super-resolution%20depends%20heavily%20on%20how%20to%0Agenerate%20and%20complement%20high-frequency%20details%20to%20low-resolution%20images.%0ARecently%2C%20diffusion-based%20models%20exhibit%20great%20potential%20in%20generating%0Ahigh-quality%20images%20for%20super-resolution%20tasks.%20However%2C%20existing%20models%0Aencounter%20difficulties%20in%20directly%20predicting%20high-frequency%20information%20of%0Awide%20bandwidth%20by%20solely%20utilizing%20the%20high-resolution%20ground%20truth%20as%20the%0Atarget%20for%20all%20sampling%20timesteps.%20To%20tackle%20this%20problem%20and%20achieve%0Ahigher-quality%20super-resolution%2C%20we%20propose%20a%20novel%20Frequency%20Domain-guided%0Amultiscale%20Diffusion%20model%20%28FDDiff%29%2C%20which%20decomposes%20the%20high-frequency%0Ainformation%20complementing%20process%20into%20finer-grained%20steps.%20In%20particular%2C%20a%0Awavelet%20packet-based%20frequency%20complement%20chain%20is%20developed%20to%20provide%0Amultiscale%20intermediate%20targets%20with%20increasing%20bandwidth%20for%20reverse%20diffusion%0Aprocess.%20Then%20FDDiff%20guides%20reverse%20diffusion%20process%20to%20progressively%0Acomplement%20the%20missing%20high-frequency%20details%20over%20timesteps.%20Moreover%2C%20we%0Adesign%20a%20multiscale%20frequency%20refinement%20network%20to%20predict%20the%20required%0Ahigh-frequency%20components%20at%20multiple%20scales%20within%20one%20unified%20network.%0AComprehensive%20evaluations%20on%20popular%20benchmarks%20are%20conducted%2C%20and%20demonstrate%0Athat%20FDDiff%20outperforms%20prior%20generative%20methods%20with%20higher-fidelity%0Asuper-resolution%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Domain%2520Refinement%2520with%2520Multiscale%2520Diffusion%2520for%2520Super%250A%2520%2520Resolution%26entry.906535625%3DXingjian%2520Wang%2520and%2520Li%2520Chai%2520and%2520Jiming%2520Chen%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520single%2520image%2520super-resolution%2520depends%2520heavily%2520on%2520how%2520to%250Agenerate%2520and%2520complement%2520high-frequency%2520details%2520to%2520low-resolution%2520images.%250ARecently%252C%2520diffusion-based%2520models%2520exhibit%2520great%2520potential%2520in%2520generating%250Ahigh-quality%2520images%2520for%2520super-resolution%2520tasks.%2520However%252C%2520existing%2520models%250Aencounter%2520difficulties%2520in%2520directly%2520predicting%2520high-frequency%2520information%2520of%250Awide%2520bandwidth%2520by%2520solely%2520utilizing%2520the%2520high-resolution%2520ground%2520truth%2520as%2520the%250Atarget%2520for%2520all%2520sampling%2520timesteps.%2520To%2520tackle%2520this%2520problem%2520and%2520achieve%250Ahigher-quality%2520super-resolution%252C%2520we%2520propose%2520a%2520novel%2520Frequency%2520Domain-guided%250Amultiscale%2520Diffusion%2520model%2520%2528FDDiff%2529%252C%2520which%2520decomposes%2520the%2520high-frequency%250Ainformation%2520complementing%2520process%2520into%2520finer-grained%2520steps.%2520In%2520particular%252C%2520a%250Awavelet%2520packet-based%2520frequency%2520complement%2520chain%2520is%2520developed%2520to%2520provide%250Amultiscale%2520intermediate%2520targets%2520with%2520increasing%2520bandwidth%2520for%2520reverse%2520diffusion%250Aprocess.%2520Then%2520FDDiff%2520guides%2520reverse%2520diffusion%2520process%2520to%2520progressively%250Acomplement%2520the%2520missing%2520high-frequency%2520details%2520over%2520timesteps.%2520Moreover%252C%2520we%250Adesign%2520a%2520multiscale%2520frequency%2520refinement%2520network%2520to%2520predict%2520the%2520required%250Ahigh-frequency%2520components%2520at%2520multiple%2520scales%2520within%2520one%2520unified%2520network.%250AComprehensive%2520evaluations%2520on%2520popular%2520benchmarks%2520are%2520conducted%252C%2520and%2520demonstrate%250Athat%2520FDDiff%2520outperforms%2520prior%2520generative%2520methods%2520with%2520higher-fidelity%250Asuper-resolution%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Domain%20Refinement%20with%20Multiscale%20Diffusion%20for%20Super%0A%20%20Resolution&entry.906535625=Xingjian%20Wang%20and%20Li%20Chai%20and%20Jiming%20Chen&entry.1292438233=%20%20The%20performance%20of%20single%20image%20super-resolution%20depends%20heavily%20on%20how%20to%0Agenerate%20and%20complement%20high-frequency%20details%20to%20low-resolution%20images.%0ARecently%2C%20diffusion-based%20models%20exhibit%20great%20potential%20in%20generating%0Ahigh-quality%20images%20for%20super-resolution%20tasks.%20However%2C%20existing%20models%0Aencounter%20difficulties%20in%20directly%20predicting%20high-frequency%20information%20of%0Awide%20bandwidth%20by%20solely%20utilizing%20the%20high-resolution%20ground%20truth%20as%20the%0Atarget%20for%20all%20sampling%20timesteps.%20To%20tackle%20this%20problem%20and%20achieve%0Ahigher-quality%20super-resolution%2C%20we%20propose%20a%20novel%20Frequency%20Domain-guided%0Amultiscale%20Diffusion%20model%20%28FDDiff%29%2C%20which%20decomposes%20the%20high-frequency%0Ainformation%20complementing%20process%20into%20finer-grained%20steps.%20In%20particular%2C%20a%0Awavelet%20packet-based%20frequency%20complement%20chain%20is%20developed%20to%20provide%0Amultiscale%20intermediate%20targets%20with%20increasing%20bandwidth%20for%20reverse%20diffusion%0Aprocess.%20Then%20FDDiff%20guides%20reverse%20diffusion%20process%20to%20progressively%0Acomplement%20the%20missing%20high-frequency%20details%20over%20timesteps.%20Moreover%2C%20we%0Adesign%20a%20multiscale%20frequency%20refinement%20network%20to%20predict%20the%20required%0Ahigh-frequency%20components%20at%20multiple%20scales%20within%20one%20unified%20network.%0AComprehensive%20evaluations%20on%20popular%20benchmarks%20are%20conducted%2C%20and%20demonstrate%0Athat%20FDDiff%20outperforms%20prior%20generative%20methods%20with%20higher-fidelity%0Asuper-resolution%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10014v1&entry.124074799=Read"},
{"title": "Histopathology Foundation Models Enable Accurate Ovarian Cancer Subtype\n  Classification", "author": "Jack Breen and Katie Allen and Kieran Zucker and Lucy Godson and Nicolas M. Orsi and Nishant Ravikumar", "abstract": "  Large pretrained transformers are increasingly being developed as generalised\nfoundation models which can underpin powerful task-specific artificial\nintelligence models. Histopathology foundation models show promise across many\ntasks, but analyses have been limited by arbitrary hyperparameters that were\nnot tuned to the specific task/dataset. We report the most rigorous single-task\nvalidation conducted to date of a histopathology foundation model, and the\nfirst performed in ovarian cancer subtyping. Attention-based multiple instance\nlearning classifiers were compared using vision transformer and ResNet features\ngenerated through varied preprocessing and pretraining procedures. The training\nset consisted of 1864 whole slide images from 434 ovarian carcinoma cases at\nLeeds Hospitals. Five-class classification performance was evaluated through\nfive-fold cross-validation, and these cross-validation models were ensembled\nfor evaluation on a hold-out test set and an external set from the\nTranscanadian study. Reporting followed the TRIPOD+AI checklist. The vision\ntransformer-based histopathology foundation model, UNI, performed best in every\nevaluation, with five-class balanced accuracies of 88% and 93% in hold-out\ninternal and external testing, compared to the best ResNet model scores of 68%\nand 81%, respectively. Normalisations and augmentations aided the\ngeneralisability of ResNet-based models, but these still did not match the\nperformance of UNI, which gave the best external performance in any ovarian\ncancer subtyping study to date. Histopathology foundation models offer a clear\nbenefit to subtyping, improving classification performance to a degree where\nclinical utility is tangible, albeit with an increased computational burden.\nSuch models could provide a second opinion in challenging cases and may improve\nthe accuracy, objectivity, and efficiency of pathological diagnoses overall.\n", "link": "http://arxiv.org/abs/2405.09990v1", "date": "2024-05-16", "relevancy": 1.8333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4703}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histopathology%20Foundation%20Models%20Enable%20Accurate%20Ovarian%20Cancer%20Subtype%0A%20%20Classification&body=Title%3A%20Histopathology%20Foundation%20Models%20Enable%20Accurate%20Ovarian%20Cancer%20Subtype%0A%20%20Classification%0AAuthor%3A%20Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Lucy%20Godson%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar%0AAbstract%3A%20%20%20Large%20pretrained%20transformers%20are%20increasingly%20being%20developed%20as%20generalised%0Afoundation%20models%20which%20can%20underpin%20powerful%20task-specific%20artificial%0Aintelligence%20models.%20Histopathology%20foundation%20models%20show%20promise%20across%20many%0Atasks%2C%20but%20analyses%20have%20been%20limited%20by%20arbitrary%20hyperparameters%20that%20were%0Anot%20tuned%20to%20the%20specific%20task/dataset.%20We%20report%20the%20most%20rigorous%20single-task%0Avalidation%20conducted%20to%20date%20of%20a%20histopathology%20foundation%20model%2C%20and%20the%0Afirst%20performed%20in%20ovarian%20cancer%20subtyping.%20Attention-based%20multiple%20instance%0Alearning%20classifiers%20were%20compared%20using%20vision%20transformer%20and%20ResNet%20features%0Agenerated%20through%20varied%20preprocessing%20and%20pretraining%20procedures.%20The%20training%0Aset%20consisted%20of%201864%20whole%20slide%20images%20from%20434%20ovarian%20carcinoma%20cases%20at%0ALeeds%20Hospitals.%20Five-class%20classification%20performance%20was%20evaluated%20through%0Afive-fold%20cross-validation%2C%20and%20these%20cross-validation%20models%20were%20ensembled%0Afor%20evaluation%20on%20a%20hold-out%20test%20set%20and%20an%20external%20set%20from%20the%0ATranscanadian%20study.%20Reporting%20followed%20the%20TRIPOD%2BAI%20checklist.%20The%20vision%0Atransformer-based%20histopathology%20foundation%20model%2C%20UNI%2C%20performed%20best%20in%20every%0Aevaluation%2C%20with%20five-class%20balanced%20accuracies%20of%2088%25%20and%2093%25%20in%20hold-out%0Ainternal%20and%20external%20testing%2C%20compared%20to%20the%20best%20ResNet%20model%20scores%20of%2068%25%0Aand%2081%25%2C%20respectively.%20Normalisations%20and%20augmentations%20aided%20the%0Ageneralisability%20of%20ResNet-based%20models%2C%20but%20these%20still%20did%20not%20match%20the%0Aperformance%20of%20UNI%2C%20which%20gave%20the%20best%20external%20performance%20in%20any%20ovarian%0Acancer%20subtyping%20study%20to%20date.%20Histopathology%20foundation%20models%20offer%20a%20clear%0Abenefit%20to%20subtyping%2C%20improving%20classification%20performance%20to%20a%20degree%20where%0Aclinical%20utility%20is%20tangible%2C%20albeit%20with%20an%20increased%20computational%20burden.%0ASuch%20models%20could%20provide%20a%20second%20opinion%20in%20challenging%20cases%20and%20may%20improve%0Athe%20accuracy%2C%20objectivity%2C%20and%20efficiency%20of%20pathological%20diagnoses%20overall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistopathology%2520Foundation%2520Models%2520Enable%2520Accurate%2520Ovarian%2520Cancer%2520Subtype%250A%2520%2520Classification%26entry.906535625%3DJack%2520Breen%2520and%2520Katie%2520Allen%2520and%2520Kieran%2520Zucker%2520and%2520Lucy%2520Godson%2520and%2520Nicolas%2520M.%2520Orsi%2520and%2520Nishant%2520Ravikumar%26entry.1292438233%3D%2520%2520Large%2520pretrained%2520transformers%2520are%2520increasingly%2520being%2520developed%2520as%2520generalised%250Afoundation%2520models%2520which%2520can%2520underpin%2520powerful%2520task-specific%2520artificial%250Aintelligence%2520models.%2520Histopathology%2520foundation%2520models%2520show%2520promise%2520across%2520many%250Atasks%252C%2520but%2520analyses%2520have%2520been%2520limited%2520by%2520arbitrary%2520hyperparameters%2520that%2520were%250Anot%2520tuned%2520to%2520the%2520specific%2520task/dataset.%2520We%2520report%2520the%2520most%2520rigorous%2520single-task%250Avalidation%2520conducted%2520to%2520date%2520of%2520a%2520histopathology%2520foundation%2520model%252C%2520and%2520the%250Afirst%2520performed%2520in%2520ovarian%2520cancer%2520subtyping.%2520Attention-based%2520multiple%2520instance%250Alearning%2520classifiers%2520were%2520compared%2520using%2520vision%2520transformer%2520and%2520ResNet%2520features%250Agenerated%2520through%2520varied%2520preprocessing%2520and%2520pretraining%2520procedures.%2520The%2520training%250Aset%2520consisted%2520of%25201864%2520whole%2520slide%2520images%2520from%2520434%2520ovarian%2520carcinoma%2520cases%2520at%250ALeeds%2520Hospitals.%2520Five-class%2520classification%2520performance%2520was%2520evaluated%2520through%250Afive-fold%2520cross-validation%252C%2520and%2520these%2520cross-validation%2520models%2520were%2520ensembled%250Afor%2520evaluation%2520on%2520a%2520hold-out%2520test%2520set%2520and%2520an%2520external%2520set%2520from%2520the%250ATranscanadian%2520study.%2520Reporting%2520followed%2520the%2520TRIPOD%252BAI%2520checklist.%2520The%2520vision%250Atransformer-based%2520histopathology%2520foundation%2520model%252C%2520UNI%252C%2520performed%2520best%2520in%2520every%250Aevaluation%252C%2520with%2520five-class%2520balanced%2520accuracies%2520of%252088%2525%2520and%252093%2525%2520in%2520hold-out%250Ainternal%2520and%2520external%2520testing%252C%2520compared%2520to%2520the%2520best%2520ResNet%2520model%2520scores%2520of%252068%2525%250Aand%252081%2525%252C%2520respectively.%2520Normalisations%2520and%2520augmentations%2520aided%2520the%250Ageneralisability%2520of%2520ResNet-based%2520models%252C%2520but%2520these%2520still%2520did%2520not%2520match%2520the%250Aperformance%2520of%2520UNI%252C%2520which%2520gave%2520the%2520best%2520external%2520performance%2520in%2520any%2520ovarian%250Acancer%2520subtyping%2520study%2520to%2520date.%2520Histopathology%2520foundation%2520models%2520offer%2520a%2520clear%250Abenefit%2520to%2520subtyping%252C%2520improving%2520classification%2520performance%2520to%2520a%2520degree%2520where%250Aclinical%2520utility%2520is%2520tangible%252C%2520albeit%2520with%2520an%2520increased%2520computational%2520burden.%250ASuch%2520models%2520could%2520provide%2520a%2520second%2520opinion%2520in%2520challenging%2520cases%2520and%2520may%2520improve%250Athe%2520accuracy%252C%2520objectivity%252C%2520and%2520efficiency%2520of%2520pathological%2520diagnoses%2520overall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histopathology%20Foundation%20Models%20Enable%20Accurate%20Ovarian%20Cancer%20Subtype%0A%20%20Classification&entry.906535625=Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Lucy%20Godson%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar&entry.1292438233=%20%20Large%20pretrained%20transformers%20are%20increasingly%20being%20developed%20as%20generalised%0Afoundation%20models%20which%20can%20underpin%20powerful%20task-specific%20artificial%0Aintelligence%20models.%20Histopathology%20foundation%20models%20show%20promise%20across%20many%0Atasks%2C%20but%20analyses%20have%20been%20limited%20by%20arbitrary%20hyperparameters%20that%20were%0Anot%20tuned%20to%20the%20specific%20task/dataset.%20We%20report%20the%20most%20rigorous%20single-task%0Avalidation%20conducted%20to%20date%20of%20a%20histopathology%20foundation%20model%2C%20and%20the%0Afirst%20performed%20in%20ovarian%20cancer%20subtyping.%20Attention-based%20multiple%20instance%0Alearning%20classifiers%20were%20compared%20using%20vision%20transformer%20and%20ResNet%20features%0Agenerated%20through%20varied%20preprocessing%20and%20pretraining%20procedures.%20The%20training%0Aset%20consisted%20of%201864%20whole%20slide%20images%20from%20434%20ovarian%20carcinoma%20cases%20at%0ALeeds%20Hospitals.%20Five-class%20classification%20performance%20was%20evaluated%20through%0Afive-fold%20cross-validation%2C%20and%20these%20cross-validation%20models%20were%20ensembled%0Afor%20evaluation%20on%20a%20hold-out%20test%20set%20and%20an%20external%20set%20from%20the%0ATranscanadian%20study.%20Reporting%20followed%20the%20TRIPOD%2BAI%20checklist.%20The%20vision%0Atransformer-based%20histopathology%20foundation%20model%2C%20UNI%2C%20performed%20best%20in%20every%0Aevaluation%2C%20with%20five-class%20balanced%20accuracies%20of%2088%25%20and%2093%25%20in%20hold-out%0Ainternal%20and%20external%20testing%2C%20compared%20to%20the%20best%20ResNet%20model%20scores%20of%2068%25%0Aand%2081%25%2C%20respectively.%20Normalisations%20and%20augmentations%20aided%20the%0Ageneralisability%20of%20ResNet-based%20models%2C%20but%20these%20still%20did%20not%20match%20the%0Aperformance%20of%20UNI%2C%20which%20gave%20the%20best%20external%20performance%20in%20any%20ovarian%0Acancer%20subtyping%20study%20to%20date.%20Histopathology%20foundation%20models%20offer%20a%20clear%0Abenefit%20to%20subtyping%2C%20improving%20classification%20performance%20to%20a%20degree%20where%0Aclinical%20utility%20is%20tangible%2C%20albeit%20with%20an%20increased%20computational%20burden.%0ASuch%20models%20could%20provide%20a%20second%20opinion%20in%20challenging%20cases%20and%20may%20improve%0Athe%20accuracy%2C%20objectivity%2C%20and%20efficiency%20of%20pathological%20diagnoses%20overall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09990v1&entry.124074799=Read"},
{"title": "Keep It Private: Unsupervised Privatization of Online Text", "author": "Calvin Bao and Marine Carpuat", "abstract": "  Authorship obfuscation techniques hold the promise of helping people protect\ntheir privacy in online communications by automatically rewriting text to hide\nthe identity of the original author. However, obfuscation has been evaluated in\nnarrow settings in the NLP literature and has primarily been addressed with\nsuperficial edit operations that can lead to unnatural outputs. In this work,\nwe introduce an automatic text privatization framework that fine-tunes a large\nlanguage model via reinforcement learning to produce rewrites that balance\nsoundness, sense, and privacy. We evaluate it extensively on a large-scale test\nset of English Reddit posts by 68k authors composed of short-medium length\ntexts. We study how the performance changes among evaluative conditions\nincluding authorial profile length and authorship detection strategy. Our\nmethod maintains high text quality according to both automated metrics and\nhuman evaluation, and successfully evades several automated authorship attacks.\n", "link": "http://arxiv.org/abs/2405.10260v1", "date": "2024-05-16", "relevancy": 1.8224, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4588}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4567}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20It%20Private%3A%20Unsupervised%20Privatization%20of%20Online%20Text&body=Title%3A%20Keep%20It%20Private%3A%20Unsupervised%20Privatization%20of%20Online%20Text%0AAuthor%3A%20Calvin%20Bao%20and%20Marine%20Carpuat%0AAbstract%3A%20%20%20Authorship%20obfuscation%20techniques%20hold%20the%20promise%20of%20helping%20people%20protect%0Atheir%20privacy%20in%20online%20communications%20by%20automatically%20rewriting%20text%20to%20hide%0Athe%20identity%20of%20the%20original%20author.%20However%2C%20obfuscation%20has%20been%20evaluated%20in%0Anarrow%20settings%20in%20the%20NLP%20literature%20and%20has%20primarily%20been%20addressed%20with%0Asuperficial%20edit%20operations%20that%20can%20lead%20to%20unnatural%20outputs.%20In%20this%20work%2C%0Awe%20introduce%20an%20automatic%20text%20privatization%20framework%20that%20fine-tunes%20a%20large%0Alanguage%20model%20via%20reinforcement%20learning%20to%20produce%20rewrites%20that%20balance%0Asoundness%2C%20sense%2C%20and%20privacy.%20We%20evaluate%20it%20extensively%20on%20a%20large-scale%20test%0Aset%20of%20English%20Reddit%20posts%20by%2068k%20authors%20composed%20of%20short-medium%20length%0Atexts.%20We%20study%20how%20the%20performance%20changes%20among%20evaluative%20conditions%0Aincluding%20authorial%20profile%20length%20and%20authorship%20detection%20strategy.%20Our%0Amethod%20maintains%20high%20text%20quality%20according%20to%20both%20automated%20metrics%20and%0Ahuman%20evaluation%2C%20and%20successfully%20evades%20several%20automated%20authorship%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520It%2520Private%253A%2520Unsupervised%2520Privatization%2520of%2520Online%2520Text%26entry.906535625%3DCalvin%2520Bao%2520and%2520Marine%2520Carpuat%26entry.1292438233%3D%2520%2520Authorship%2520obfuscation%2520techniques%2520hold%2520the%2520promise%2520of%2520helping%2520people%2520protect%250Atheir%2520privacy%2520in%2520online%2520communications%2520by%2520automatically%2520rewriting%2520text%2520to%2520hide%250Athe%2520identity%2520of%2520the%2520original%2520author.%2520However%252C%2520obfuscation%2520has%2520been%2520evaluated%2520in%250Anarrow%2520settings%2520in%2520the%2520NLP%2520literature%2520and%2520has%2520primarily%2520been%2520addressed%2520with%250Asuperficial%2520edit%2520operations%2520that%2520can%2520lead%2520to%2520unnatural%2520outputs.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520an%2520automatic%2520text%2520privatization%2520framework%2520that%2520fine-tunes%2520a%2520large%250Alanguage%2520model%2520via%2520reinforcement%2520learning%2520to%2520produce%2520rewrites%2520that%2520balance%250Asoundness%252C%2520sense%252C%2520and%2520privacy.%2520We%2520evaluate%2520it%2520extensively%2520on%2520a%2520large-scale%2520test%250Aset%2520of%2520English%2520Reddit%2520posts%2520by%252068k%2520authors%2520composed%2520of%2520short-medium%2520length%250Atexts.%2520We%2520study%2520how%2520the%2520performance%2520changes%2520among%2520evaluative%2520conditions%250Aincluding%2520authorial%2520profile%2520length%2520and%2520authorship%2520detection%2520strategy.%2520Our%250Amethod%2520maintains%2520high%2520text%2520quality%2520according%2520to%2520both%2520automated%2520metrics%2520and%250Ahuman%2520evaluation%252C%2520and%2520successfully%2520evades%2520several%2520automated%2520authorship%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20It%20Private%3A%20Unsupervised%20Privatization%20of%20Online%20Text&entry.906535625=Calvin%20Bao%20and%20Marine%20Carpuat&entry.1292438233=%20%20Authorship%20obfuscation%20techniques%20hold%20the%20promise%20of%20helping%20people%20protect%0Atheir%20privacy%20in%20online%20communications%20by%20automatically%20rewriting%20text%20to%20hide%0Athe%20identity%20of%20the%20original%20author.%20However%2C%20obfuscation%20has%20been%20evaluated%20in%0Anarrow%20settings%20in%20the%20NLP%20literature%20and%20has%20primarily%20been%20addressed%20with%0Asuperficial%20edit%20operations%20that%20can%20lead%20to%20unnatural%20outputs.%20In%20this%20work%2C%0Awe%20introduce%20an%20automatic%20text%20privatization%20framework%20that%20fine-tunes%20a%20large%0Alanguage%20model%20via%20reinforcement%20learning%20to%20produce%20rewrites%20that%20balance%0Asoundness%2C%20sense%2C%20and%20privacy.%20We%20evaluate%20it%20extensively%20on%20a%20large-scale%20test%0Aset%20of%20English%20Reddit%20posts%20by%2068k%20authors%20composed%20of%20short-medium%20length%0Atexts.%20We%20study%20how%20the%20performance%20changes%20among%20evaluative%20conditions%0Aincluding%20authorial%20profile%20length%20and%20authorship%20detection%20strategy.%20Our%0Amethod%20maintains%20high%20text%20quality%20according%20to%20both%20automated%20metrics%20and%0Ahuman%20evaluation%2C%20and%20successfully%20evades%20several%20automated%20authorship%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10260v1&entry.124074799=Read"},
{"title": "Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain\n  Modality Forecasting", "author": "Divij Gupta and Anubhav Bhatti and Suraj Parmar and Chen Dan and Yuwei Liu and Bingjie Shen and San Lee", "abstract": "  Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large\npre-trained or foundational models across different modalities and tasks.\nHowever, its application to time series data, particularly within foundational\nmodels, remains underexplored. This paper examines the impact of LoRA on\ncontemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos.\nWe demonstrate LoRA's fine-tuning potential for forecasting the vital signs of\nsepsis patients in intensive care units (ICUs), emphasizing the models'\nadaptability to previously unseen, out-of-domain modalities. Integrating LoRA\naims to enhance forecasting performance while reducing inefficiencies\nassociated with fine-tuning large models on limited domain-specific data. Our\nexperiments show that LoRA fine-tuning of time series foundational models\nsignificantly improves forecasting, achieving results comparable to\nstate-of-the-art models trained from scratch on similar modalities. We conduct\ncomprehensive ablation studies to demonstrate the trade-offs between the number\nof tunable parameters and forecasting performance and assess the impact of\nvarying LoRA matrix ranks on model performance.\n", "link": "http://arxiv.org/abs/2405.10216v1", "date": "2024-05-16", "relevancy": 1.8123, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4551}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4526}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Adaptation%20of%20Time%20Series%20Foundational%20Models%20for%20Out-of-Domain%0A%20%20Modality%20Forecasting&body=Title%3A%20Low-Rank%20Adaptation%20of%20Time%20Series%20Foundational%20Models%20for%20Out-of-Domain%0A%20%20Modality%20Forecasting%0AAuthor%3A%20Divij%20Gupta%20and%20Anubhav%20Bhatti%20and%20Suraj%20Parmar%20and%20Chen%20Dan%20and%20Yuwei%20Liu%20and%20Bingjie%20Shen%20and%20San%20Lee%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20is%20a%20widely%20used%20technique%20for%20fine-tuning%20large%0Apre-trained%20or%20foundational%20models%20across%20different%20modalities%20and%20tasks.%0AHowever%2C%20its%20application%20to%20time%20series%20data%2C%20particularly%20within%20foundational%0Amodels%2C%20remains%20underexplored.%20This%20paper%20examines%20the%20impact%20of%20LoRA%20on%0Acontemporary%20time%20series%20foundational%20models%3A%20Lag-Llama%2C%20MOIRAI%2C%20and%20Chronos.%0AWe%20demonstrate%20LoRA%27s%20fine-tuning%20potential%20for%20forecasting%20the%20vital%20signs%20of%0Asepsis%20patients%20in%20intensive%20care%20units%20%28ICUs%29%2C%20emphasizing%20the%20models%27%0Aadaptability%20to%20previously%20unseen%2C%20out-of-domain%20modalities.%20Integrating%20LoRA%0Aaims%20to%20enhance%20forecasting%20performance%20while%20reducing%20inefficiencies%0Aassociated%20with%20fine-tuning%20large%20models%20on%20limited%20domain-specific%20data.%20Our%0Aexperiments%20show%20that%20LoRA%20fine-tuning%20of%20time%20series%20foundational%20models%0Asignificantly%20improves%20forecasting%2C%20achieving%20results%20comparable%20to%0Astate-of-the-art%20models%20trained%20from%20scratch%20on%20similar%20modalities.%20We%20conduct%0Acomprehensive%20ablation%20studies%20to%20demonstrate%20the%20trade-offs%20between%20the%20number%0Aof%20tunable%20parameters%20and%20forecasting%20performance%20and%20assess%20the%20impact%20of%0Avarying%20LoRA%20matrix%20ranks%20on%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Adaptation%2520of%2520Time%2520Series%2520Foundational%2520Models%2520for%2520Out-of-Domain%250A%2520%2520Modality%2520Forecasting%26entry.906535625%3DDivij%2520Gupta%2520and%2520Anubhav%2520Bhatti%2520and%2520Suraj%2520Parmar%2520and%2520Chen%2520Dan%2520and%2520Yuwei%2520Liu%2520and%2520Bingjie%2520Shen%2520and%2520San%2520Lee%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520is%2520a%2520widely%2520used%2520technique%2520for%2520fine-tuning%2520large%250Apre-trained%2520or%2520foundational%2520models%2520across%2520different%2520modalities%2520and%2520tasks.%250AHowever%252C%2520its%2520application%2520to%2520time%2520series%2520data%252C%2520particularly%2520within%2520foundational%250Amodels%252C%2520remains%2520underexplored.%2520This%2520paper%2520examines%2520the%2520impact%2520of%2520LoRA%2520on%250Acontemporary%2520time%2520series%2520foundational%2520models%253A%2520Lag-Llama%252C%2520MOIRAI%252C%2520and%2520Chronos.%250AWe%2520demonstrate%2520LoRA%2527s%2520fine-tuning%2520potential%2520for%2520forecasting%2520the%2520vital%2520signs%2520of%250Asepsis%2520patients%2520in%2520intensive%2520care%2520units%2520%2528ICUs%2529%252C%2520emphasizing%2520the%2520models%2527%250Aadaptability%2520to%2520previously%2520unseen%252C%2520out-of-domain%2520modalities.%2520Integrating%2520LoRA%250Aaims%2520to%2520enhance%2520forecasting%2520performance%2520while%2520reducing%2520inefficiencies%250Aassociated%2520with%2520fine-tuning%2520large%2520models%2520on%2520limited%2520domain-specific%2520data.%2520Our%250Aexperiments%2520show%2520that%2520LoRA%2520fine-tuning%2520of%2520time%2520series%2520foundational%2520models%250Asignificantly%2520improves%2520forecasting%252C%2520achieving%2520results%2520comparable%2520to%250Astate-of-the-art%2520models%2520trained%2520from%2520scratch%2520on%2520similar%2520modalities.%2520We%2520conduct%250Acomprehensive%2520ablation%2520studies%2520to%2520demonstrate%2520the%2520trade-offs%2520between%2520the%2520number%250Aof%2520tunable%2520parameters%2520and%2520forecasting%2520performance%2520and%2520assess%2520the%2520impact%2520of%250Avarying%2520LoRA%2520matrix%2520ranks%2520on%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Adaptation%20of%20Time%20Series%20Foundational%20Models%20for%20Out-of-Domain%0A%20%20Modality%20Forecasting&entry.906535625=Divij%20Gupta%20and%20Anubhav%20Bhatti%20and%20Suraj%20Parmar%20and%20Chen%20Dan%20and%20Yuwei%20Liu%20and%20Bingjie%20Shen%20and%20San%20Lee&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20is%20a%20widely%20used%20technique%20for%20fine-tuning%20large%0Apre-trained%20or%20foundational%20models%20across%20different%20modalities%20and%20tasks.%0AHowever%2C%20its%20application%20to%20time%20series%20data%2C%20particularly%20within%20foundational%0Amodels%2C%20remains%20underexplored.%20This%20paper%20examines%20the%20impact%20of%20LoRA%20on%0Acontemporary%20time%20series%20foundational%20models%3A%20Lag-Llama%2C%20MOIRAI%2C%20and%20Chronos.%0AWe%20demonstrate%20LoRA%27s%20fine-tuning%20potential%20for%20forecasting%20the%20vital%20signs%20of%0Asepsis%20patients%20in%20intensive%20care%20units%20%28ICUs%29%2C%20emphasizing%20the%20models%27%0Aadaptability%20to%20previously%20unseen%2C%20out-of-domain%20modalities.%20Integrating%20LoRA%0Aaims%20to%20enhance%20forecasting%20performance%20while%20reducing%20inefficiencies%0Aassociated%20with%20fine-tuning%20large%20models%20on%20limited%20domain-specific%20data.%20Our%0Aexperiments%20show%20that%20LoRA%20fine-tuning%20of%20time%20series%20foundational%20models%0Asignificantly%20improves%20forecasting%2C%20achieving%20results%20comparable%20to%0Astate-of-the-art%20models%20trained%20from%20scratch%20on%20similar%20modalities.%20We%20conduct%0Acomprehensive%20ablation%20studies%20to%20demonstrate%20the%20trade-offs%20between%20the%20number%0Aof%20tunable%20parameters%20and%20forecasting%20performance%20and%20assess%20the%20impact%20of%0Avarying%20LoRA%20matrix%20ranks%20on%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10216v1&entry.124074799=Read"},
{"title": "How Far Are We From AGI", "author": "Tao Feng and Chuanyang Jin and Jingyu Liu and Kunlun Zhu and Haoqin Tu and Zirui Cheng and Guanyu Lin and Jiaxuan You", "abstract": "  The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. Yet, the\nescalating demands on AI have highlighted the limitations of AI's current\nofferings, catalyzing a movement towards Artificial General Intelligence (AGI).\nAGI, distinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing works have summarized\nspecific recent advancements of AI, they lack a comprehensive discussion of\nAGI's definitions, goals, and developmental trajectories. Different from\nexisting survey papers, this paper delves into the pivotal questions of our\nproximity to AGI and the strategies necessary for its realization through\nextensive surveys, discussions, and original perspectives. We start by\narticulating the requisite capability frameworks for AGI, integrating the\ninternal, interface, and system dimensions. As the realization of AGI requires\nmore advanced capabilities and adherence to stringent constraints, we further\ndiscuss necessary AGI alignment technologies to harmonize these factors.\nNotably, we emphasize the importance of approaching AGI responsibly by first\ndefining the key levels of AGI progression, followed by the evaluation\nframework that situates the status-quo, and finally giving our roadmap of how\nto reach the pinnacle of AGI. Moreover, to give tangible insights into the\nubiquitous impact of the integration of AI, we outline existing challenges and\npotential pathways toward AGI in multiple domains. In sum, serving as a\npioneering exploration into the current state and future trajectory of AGI,\nthis paper aims to foster a collective comprehension and catalyze broader\npublic discussions among researchers and practitioners on AGI.\n", "link": "http://arxiv.org/abs/2405.10313v1", "date": "2024-05-16", "relevancy": 1.8117, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4725}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4586}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20Are%20We%20From%20AGI&body=Title%3A%20How%20Far%20Are%20We%20From%20AGI%0AAuthor%3A%20Tao%20Feng%20and%20Chuanyang%20Jin%20and%20Jingyu%20Liu%20and%20Kunlun%20Zhu%20and%20Haoqin%20Tu%20and%20Zirui%20Cheng%20and%20Guanyu%20Lin%20and%20Jiaxuan%20You%0AAbstract%3A%20%20%20The%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20profoundly%20impacted%20human%0Asociety%2C%20driving%20significant%20advancements%20in%20multiple%20sectors.%20Yet%2C%20the%0Aescalating%20demands%20on%20AI%20have%20highlighted%20the%20limitations%20of%20AI%27s%20current%0Aofferings%2C%20catalyzing%20a%20movement%20towards%20Artificial%20General%20Intelligence%20%28AGI%29.%0AAGI%2C%20distinguished%20by%20its%20ability%20to%20execute%20diverse%20real-world%20tasks%20with%0Aefficiency%20and%20effectiveness%20comparable%20to%20human%20intelligence%2C%20reflects%20a%0Aparamount%20milestone%20in%20AI%20evolution.%20While%20existing%20works%20have%20summarized%0Aspecific%20recent%20advancements%20of%20AI%2C%20they%20lack%20a%20comprehensive%20discussion%20of%0AAGI%27s%20definitions%2C%20goals%2C%20and%20developmental%20trajectories.%20Different%20from%0Aexisting%20survey%20papers%2C%20this%20paper%20delves%20into%20the%20pivotal%20questions%20of%20our%0Aproximity%20to%20AGI%20and%20the%20strategies%20necessary%20for%20its%20realization%20through%0Aextensive%20surveys%2C%20discussions%2C%20and%20original%20perspectives.%20We%20start%20by%0Aarticulating%20the%20requisite%20capability%20frameworks%20for%20AGI%2C%20integrating%20the%0Ainternal%2C%20interface%2C%20and%20system%20dimensions.%20As%20the%20realization%20of%20AGI%20requires%0Amore%20advanced%20capabilities%20and%20adherence%20to%20stringent%20constraints%2C%20we%20further%0Adiscuss%20necessary%20AGI%20alignment%20technologies%20to%20harmonize%20these%20factors.%0ANotably%2C%20we%20emphasize%20the%20importance%20of%20approaching%20AGI%20responsibly%20by%20first%0Adefining%20the%20key%20levels%20of%20AGI%20progression%2C%20followed%20by%20the%20evaluation%0Aframework%20that%20situates%20the%20status-quo%2C%20and%20finally%20giving%20our%20roadmap%20of%20how%0Ato%20reach%20the%20pinnacle%20of%20AGI.%20Moreover%2C%20to%20give%20tangible%20insights%20into%20the%0Aubiquitous%20impact%20of%20the%20integration%20of%20AI%2C%20we%20outline%20existing%20challenges%20and%0Apotential%20pathways%20toward%20AGI%20in%20multiple%20domains.%20In%20sum%2C%20serving%20as%20a%0Apioneering%20exploration%20into%20the%20current%20state%20and%20future%20trajectory%20of%20AGI%2C%0Athis%20paper%20aims%20to%20foster%20a%20collective%20comprehension%20and%20catalyze%20broader%0Apublic%20discussions%20among%20researchers%20and%20practitioners%20on%20AGI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520Are%2520We%2520From%2520AGI%26entry.906535625%3DTao%2520Feng%2520and%2520Chuanyang%2520Jin%2520and%2520Jingyu%2520Liu%2520and%2520Kunlun%2520Zhu%2520and%2520Haoqin%2520Tu%2520and%2520Zirui%2520Cheng%2520and%2520Guanyu%2520Lin%2520and%2520Jiaxuan%2520You%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520profoundly%2520impacted%2520human%250Asociety%252C%2520driving%2520significant%2520advancements%2520in%2520multiple%2520sectors.%2520Yet%252C%2520the%250Aescalating%2520demands%2520on%2520AI%2520have%2520highlighted%2520the%2520limitations%2520of%2520AI%2527s%2520current%250Aofferings%252C%2520catalyzing%2520a%2520movement%2520towards%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529.%250AAGI%252C%2520distinguished%2520by%2520its%2520ability%2520to%2520execute%2520diverse%2520real-world%2520tasks%2520with%250Aefficiency%2520and%2520effectiveness%2520comparable%2520to%2520human%2520intelligence%252C%2520reflects%2520a%250Aparamount%2520milestone%2520in%2520AI%2520evolution.%2520While%2520existing%2520works%2520have%2520summarized%250Aspecific%2520recent%2520advancements%2520of%2520AI%252C%2520they%2520lack%2520a%2520comprehensive%2520discussion%2520of%250AAGI%2527s%2520definitions%252C%2520goals%252C%2520and%2520developmental%2520trajectories.%2520Different%2520from%250Aexisting%2520survey%2520papers%252C%2520this%2520paper%2520delves%2520into%2520the%2520pivotal%2520questions%2520of%2520our%250Aproximity%2520to%2520AGI%2520and%2520the%2520strategies%2520necessary%2520for%2520its%2520realization%2520through%250Aextensive%2520surveys%252C%2520discussions%252C%2520and%2520original%2520perspectives.%2520We%2520start%2520by%250Aarticulating%2520the%2520requisite%2520capability%2520frameworks%2520for%2520AGI%252C%2520integrating%2520the%250Ainternal%252C%2520interface%252C%2520and%2520system%2520dimensions.%2520As%2520the%2520realization%2520of%2520AGI%2520requires%250Amore%2520advanced%2520capabilities%2520and%2520adherence%2520to%2520stringent%2520constraints%252C%2520we%2520further%250Adiscuss%2520necessary%2520AGI%2520alignment%2520technologies%2520to%2520harmonize%2520these%2520factors.%250ANotably%252C%2520we%2520emphasize%2520the%2520importance%2520of%2520approaching%2520AGI%2520responsibly%2520by%2520first%250Adefining%2520the%2520key%2520levels%2520of%2520AGI%2520progression%252C%2520followed%2520by%2520the%2520evaluation%250Aframework%2520that%2520situates%2520the%2520status-quo%252C%2520and%2520finally%2520giving%2520our%2520roadmap%2520of%2520how%250Ato%2520reach%2520the%2520pinnacle%2520of%2520AGI.%2520Moreover%252C%2520to%2520give%2520tangible%2520insights%2520into%2520the%250Aubiquitous%2520impact%2520of%2520the%2520integration%2520of%2520AI%252C%2520we%2520outline%2520existing%2520challenges%2520and%250Apotential%2520pathways%2520toward%2520AGI%2520in%2520multiple%2520domains.%2520In%2520sum%252C%2520serving%2520as%2520a%250Apioneering%2520exploration%2520into%2520the%2520current%2520state%2520and%2520future%2520trajectory%2520of%2520AGI%252C%250Athis%2520paper%2520aims%2520to%2520foster%2520a%2520collective%2520comprehension%2520and%2520catalyze%2520broader%250Apublic%2520discussions%2520among%2520researchers%2520and%2520practitioners%2520on%2520AGI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20Are%20We%20From%20AGI&entry.906535625=Tao%20Feng%20and%20Chuanyang%20Jin%20and%20Jingyu%20Liu%20and%20Kunlun%20Zhu%20and%20Haoqin%20Tu%20and%20Zirui%20Cheng%20and%20Guanyu%20Lin%20and%20Jiaxuan%20You&entry.1292438233=%20%20The%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20profoundly%20impacted%20human%0Asociety%2C%20driving%20significant%20advancements%20in%20multiple%20sectors.%20Yet%2C%20the%0Aescalating%20demands%20on%20AI%20have%20highlighted%20the%20limitations%20of%20AI%27s%20current%0Aofferings%2C%20catalyzing%20a%20movement%20towards%20Artificial%20General%20Intelligence%20%28AGI%29.%0AAGI%2C%20distinguished%20by%20its%20ability%20to%20execute%20diverse%20real-world%20tasks%20with%0Aefficiency%20and%20effectiveness%20comparable%20to%20human%20intelligence%2C%20reflects%20a%0Aparamount%20milestone%20in%20AI%20evolution.%20While%20existing%20works%20have%20summarized%0Aspecific%20recent%20advancements%20of%20AI%2C%20they%20lack%20a%20comprehensive%20discussion%20of%0AAGI%27s%20definitions%2C%20goals%2C%20and%20developmental%20trajectories.%20Different%20from%0Aexisting%20survey%20papers%2C%20this%20paper%20delves%20into%20the%20pivotal%20questions%20of%20our%0Aproximity%20to%20AGI%20and%20the%20strategies%20necessary%20for%20its%20realization%20through%0Aextensive%20surveys%2C%20discussions%2C%20and%20original%20perspectives.%20We%20start%20by%0Aarticulating%20the%20requisite%20capability%20frameworks%20for%20AGI%2C%20integrating%20the%0Ainternal%2C%20interface%2C%20and%20system%20dimensions.%20As%20the%20realization%20of%20AGI%20requires%0Amore%20advanced%20capabilities%20and%20adherence%20to%20stringent%20constraints%2C%20we%20further%0Adiscuss%20necessary%20AGI%20alignment%20technologies%20to%20harmonize%20these%20factors.%0ANotably%2C%20we%20emphasize%20the%20importance%20of%20approaching%20AGI%20responsibly%20by%20first%0Adefining%20the%20key%20levels%20of%20AGI%20progression%2C%20followed%20by%20the%20evaluation%0Aframework%20that%20situates%20the%20status-quo%2C%20and%20finally%20giving%20our%20roadmap%20of%20how%0Ato%20reach%20the%20pinnacle%20of%20AGI.%20Moreover%2C%20to%20give%20tangible%20insights%20into%20the%0Aubiquitous%20impact%20of%20the%20integration%20of%20AI%2C%20we%20outline%20existing%20challenges%20and%0Apotential%20pathways%20toward%20AGI%20in%20multiple%20domains.%20In%20sum%2C%20serving%20as%20a%0Apioneering%20exploration%20into%20the%20current%20state%20and%20future%20trajectory%20of%20AGI%2C%0Athis%20paper%20aims%20to%20foster%20a%20collective%20comprehension%20and%20catalyze%20broader%0Apublic%20discussions%20among%20researchers%20and%20practitioners%20on%20AGI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10313v1&entry.124074799=Read"},
{"title": "The Real Price of Bandit Information in Multiclass Classification", "author": "Liad Erez and Alon Cohen and Tomer Koren and Yishay Mansour and Shay Moran", "abstract": "  We revisit the classical problem of multiclass classification with bandit\nfeedback (Kakade, Shalev-Shwartz and Tewari, 2008), where each input classifies\nto one of $K$ possible labels and feedback is restricted to whether the\npredicted label is correct or not. Our primary inquiry is with regard to the\ndependency on the number of labels $K$, and whether $T$-step regret bounds in\nthis setting can be improved beyond the $\\smash{\\sqrt{KT}}$ dependence\nexhibited by existing algorithms. Our main contribution is in showing that the\nminimax regret of bandit multiclass is in fact more nuanced, and is of the form\n$\\smash{\\widetilde{\\Theta}\\left(\\min \\left\\{|\\mathcal{H}| + \\sqrt{T}, \\sqrt{KT\n\\log |{\\mathcal{H}|}} \\right\\} \\right) }$, where $\\mathcal{H}$ is the\nunderlying (finite) hypothesis class. In particular, we present a new bandit\nclassification algorithm that guarantees regret\n$\\smash{\\widetilde{O}(|\\mathcal{H}|+\\sqrt{T})}$, improving over classical\nalgorithms for moderately-sized hypothesis classes, and give a matching lower\nbound establishing tightness of the upper bounds (up to log-factors) in all\nparameter regimes.\n", "link": "http://arxiv.org/abs/2405.10027v1", "date": "2024-05-16", "relevancy": 1.803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4772}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.471}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Real%20Price%20of%20Bandit%20Information%20in%20Multiclass%20Classification&body=Title%3A%20The%20Real%20Price%20of%20Bandit%20Information%20in%20Multiclass%20Classification%0AAuthor%3A%20Liad%20Erez%20and%20Alon%20Cohen%20and%20Tomer%20Koren%20and%20Yishay%20Mansour%20and%20Shay%20Moran%0AAbstract%3A%20%20%20We%20revisit%20the%20classical%20problem%20of%20multiclass%20classification%20with%20bandit%0Afeedback%20%28Kakade%2C%20Shalev-Shwartz%20and%20Tewari%2C%202008%29%2C%20where%20each%20input%20classifies%0Ato%20one%20of%20%24K%24%20possible%20labels%20and%20feedback%20is%20restricted%20to%20whether%20the%0Apredicted%20label%20is%20correct%20or%20not.%20Our%20primary%20inquiry%20is%20with%20regard%20to%20the%0Adependency%20on%20the%20number%20of%20labels%20%24K%24%2C%20and%20whether%20%24T%24-step%20regret%20bounds%20in%0Athis%20setting%20can%20be%20improved%20beyond%20the%20%24%5Csmash%7B%5Csqrt%7BKT%7D%7D%24%20dependence%0Aexhibited%20by%20existing%20algorithms.%20Our%20main%20contribution%20is%20in%20showing%20that%20the%0Aminimax%20regret%20of%20bandit%20multiclass%20is%20in%20fact%20more%20nuanced%2C%20and%20is%20of%20the%20form%0A%24%5Csmash%7B%5Cwidetilde%7B%5CTheta%7D%5Cleft%28%5Cmin%20%5Cleft%5C%7B%7C%5Cmathcal%7BH%7D%7C%20%2B%20%5Csqrt%7BT%7D%2C%20%5Csqrt%7BKT%0A%5Clog%20%7C%7B%5Cmathcal%7BH%7D%7C%7D%7D%20%5Cright%5C%7D%20%5Cright%29%20%7D%24%2C%20where%20%24%5Cmathcal%7BH%7D%24%20is%20the%0Aunderlying%20%28finite%29%20hypothesis%20class.%20In%20particular%2C%20we%20present%20a%20new%20bandit%0Aclassification%20algorithm%20that%20guarantees%20regret%0A%24%5Csmash%7B%5Cwidetilde%7BO%7D%28%7C%5Cmathcal%7BH%7D%7C%2B%5Csqrt%7BT%7D%29%7D%24%2C%20improving%20over%20classical%0Aalgorithms%20for%20moderately-sized%20hypothesis%20classes%2C%20and%20give%20a%20matching%20lower%0Abound%20establishing%20tightness%20of%20the%20upper%20bounds%20%28up%20to%20log-factors%29%20in%20all%0Aparameter%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Real%2520Price%2520of%2520Bandit%2520Information%2520in%2520Multiclass%2520Classification%26entry.906535625%3DLiad%2520Erez%2520and%2520Alon%2520Cohen%2520and%2520Tomer%2520Koren%2520and%2520Yishay%2520Mansour%2520and%2520Shay%2520Moran%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520classical%2520problem%2520of%2520multiclass%2520classification%2520with%2520bandit%250Afeedback%2520%2528Kakade%252C%2520Shalev-Shwartz%2520and%2520Tewari%252C%25202008%2529%252C%2520where%2520each%2520input%2520classifies%250Ato%2520one%2520of%2520%2524K%2524%2520possible%2520labels%2520and%2520feedback%2520is%2520restricted%2520to%2520whether%2520the%250Apredicted%2520label%2520is%2520correct%2520or%2520not.%2520Our%2520primary%2520inquiry%2520is%2520with%2520regard%2520to%2520the%250Adependency%2520on%2520the%2520number%2520of%2520labels%2520%2524K%2524%252C%2520and%2520whether%2520%2524T%2524-step%2520regret%2520bounds%2520in%250Athis%2520setting%2520can%2520be%2520improved%2520beyond%2520the%2520%2524%255Csmash%257B%255Csqrt%257BKT%257D%257D%2524%2520dependence%250Aexhibited%2520by%2520existing%2520algorithms.%2520Our%2520main%2520contribution%2520is%2520in%2520showing%2520that%2520the%250Aminimax%2520regret%2520of%2520bandit%2520multiclass%2520is%2520in%2520fact%2520more%2520nuanced%252C%2520and%2520is%2520of%2520the%2520form%250A%2524%255Csmash%257B%255Cwidetilde%257B%255CTheta%257D%255Cleft%2528%255Cmin%2520%255Cleft%255C%257B%257C%255Cmathcal%257BH%257D%257C%2520%252B%2520%255Csqrt%257BT%257D%252C%2520%255Csqrt%257BKT%250A%255Clog%2520%257C%257B%255Cmathcal%257BH%257D%257C%257D%257D%2520%255Cright%255C%257D%2520%255Cright%2529%2520%257D%2524%252C%2520where%2520%2524%255Cmathcal%257BH%257D%2524%2520is%2520the%250Aunderlying%2520%2528finite%2529%2520hypothesis%2520class.%2520In%2520particular%252C%2520we%2520present%2520a%2520new%2520bandit%250Aclassification%2520algorithm%2520that%2520guarantees%2520regret%250A%2524%255Csmash%257B%255Cwidetilde%257BO%257D%2528%257C%255Cmathcal%257BH%257D%257C%252B%255Csqrt%257BT%257D%2529%257D%2524%252C%2520improving%2520over%2520classical%250Aalgorithms%2520for%2520moderately-sized%2520hypothesis%2520classes%252C%2520and%2520give%2520a%2520matching%2520lower%250Abound%2520establishing%2520tightness%2520of%2520the%2520upper%2520bounds%2520%2528up%2520to%2520log-factors%2529%2520in%2520all%250Aparameter%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Real%20Price%20of%20Bandit%20Information%20in%20Multiclass%20Classification&entry.906535625=Liad%20Erez%20and%20Alon%20Cohen%20and%20Tomer%20Koren%20and%20Yishay%20Mansour%20and%20Shay%20Moran&entry.1292438233=%20%20We%20revisit%20the%20classical%20problem%20of%20multiclass%20classification%20with%20bandit%0Afeedback%20%28Kakade%2C%20Shalev-Shwartz%20and%20Tewari%2C%202008%29%2C%20where%20each%20input%20classifies%0Ato%20one%20of%20%24K%24%20possible%20labels%20and%20feedback%20is%20restricted%20to%20whether%20the%0Apredicted%20label%20is%20correct%20or%20not.%20Our%20primary%20inquiry%20is%20with%20regard%20to%20the%0Adependency%20on%20the%20number%20of%20labels%20%24K%24%2C%20and%20whether%20%24T%24-step%20regret%20bounds%20in%0Athis%20setting%20can%20be%20improved%20beyond%20the%20%24%5Csmash%7B%5Csqrt%7BKT%7D%7D%24%20dependence%0Aexhibited%20by%20existing%20algorithms.%20Our%20main%20contribution%20is%20in%20showing%20that%20the%0Aminimax%20regret%20of%20bandit%20multiclass%20is%20in%20fact%20more%20nuanced%2C%20and%20is%20of%20the%20form%0A%24%5Csmash%7B%5Cwidetilde%7B%5CTheta%7D%5Cleft%28%5Cmin%20%5Cleft%5C%7B%7C%5Cmathcal%7BH%7D%7C%20%2B%20%5Csqrt%7BT%7D%2C%20%5Csqrt%7BKT%0A%5Clog%20%7C%7B%5Cmathcal%7BH%7D%7C%7D%7D%20%5Cright%5C%7D%20%5Cright%29%20%7D%24%2C%20where%20%24%5Cmathcal%7BH%7D%24%20is%20the%0Aunderlying%20%28finite%29%20hypothesis%20class.%20In%20particular%2C%20we%20present%20a%20new%20bandit%0Aclassification%20algorithm%20that%20guarantees%20regret%0A%24%5Csmash%7B%5Cwidetilde%7BO%7D%28%7C%5Cmathcal%7BH%7D%7C%2B%5Csqrt%7BT%7D%29%7D%24%2C%20improving%20over%20classical%0Aalgorithms%20for%20moderately-sized%20hypothesis%20classes%2C%20and%20give%20a%20matching%20lower%0Abound%20establishing%20tightness%20of%20the%20upper%20bounds%20%28up%20to%20log-factors%29%20in%20all%0Aparameter%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10027v1&entry.124074799=Read"},
{"title": "Ensuring UAV Safety: A Vision-only and Real-time Framework for Collision\n  Avoidance Through Object Detection, Tracking, and Distance Estimation", "author": "Vasileios Karampinis and Anastasios Arsenos and Orfeas Filippopoulos and Evangelos Petrongonas and Christos Skliros and Dimitrios Kollias and Stefanos Kollias and Athanasios Voulodimos", "abstract": "  In the last twenty years, unmanned aerial vehicles (UAVs) have garnered\ngrowing interest due to their expanding applications in both military and\ncivilian domains. Detecting non-cooperative aerial vehicles with efficiency and\nestimating collisions accurately are pivotal for achieving fully autonomous\naircraft and facilitating Advanced Air Mobility (AAM). This paper presents a\ndeep-learning framework that utilizes optical sensors for the detection,\ntracking, and distance estimation of non-cooperative aerial vehicles. In\nimplementing this comprehensive sensing framework, the availability of depth\ninformation is essential for enabling autonomous aerial vehicles to perceive\nand navigate around obstacles. In this work, we propose a method for estimating\nthe distance information of a detected aerial object in real time using only\nthe input of a monocular camera. In order to train our deep learning components\nfor the object detection, tracking and depth estimation tasks we utilize the\nAmazon Airborne Object Tracking (AOT) Dataset. In contrast to previous\napproaches that integrate the depth estimation module into the object detector,\nour method formulates the problem as image-to-image translation. We employ a\nseparate lightweight encoder-decoder network for efficient and robust depth\nestimation. In a nutshell, the object detection module identifies and localizes\nobstacles, conveying this information to both the tracking module for\nmonitoring obstacle movement and the depth estimation module for calculating\ndistances. Our approach is evaluated on the Airborne Object Tracking (AOT)\ndataset which is the largest (to the best of our knowledge) air-to-air airborne\nobject dataset.\n", "link": "http://arxiv.org/abs/2405.06749v2", "date": "2024-05-16", "relevancy": 1.7958, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6194}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5802}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensuring%20UAV%20Safety%3A%20A%20Vision-only%20and%20Real-time%20Framework%20for%20Collision%0A%20%20Avoidance%20Through%20Object%20Detection%2C%20Tracking%2C%20and%20Distance%20Estimation&body=Title%3A%20Ensuring%20UAV%20Safety%3A%20A%20Vision-only%20and%20Real-time%20Framework%20for%20Collision%0A%20%20Avoidance%20Through%20Object%20Detection%2C%20Tracking%2C%20and%20Distance%20Estimation%0AAuthor%3A%20Vasileios%20Karampinis%20and%20Anastasios%20Arsenos%20and%20Orfeas%20Filippopoulos%20and%20Evangelos%20Petrongonas%20and%20Christos%20Skliros%20and%20Dimitrios%20Kollias%20and%20Stefanos%20Kollias%20and%20Athanasios%20Voulodimos%0AAbstract%3A%20%20%20In%20the%20last%20twenty%20years%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20have%20garnered%0Agrowing%20interest%20due%20to%20their%20expanding%20applications%20in%20both%20military%20and%0Acivilian%20domains.%20Detecting%20non-cooperative%20aerial%20vehicles%20with%20efficiency%20and%0Aestimating%20collisions%20accurately%20are%20pivotal%20for%20achieving%20fully%20autonomous%0Aaircraft%20and%20facilitating%20Advanced%20Air%20Mobility%20%28AAM%29.%20This%20paper%20presents%20a%0Adeep-learning%20framework%20that%20utilizes%20optical%20sensors%20for%20the%20detection%2C%0Atracking%2C%20and%20distance%20estimation%20of%20non-cooperative%20aerial%20vehicles.%20In%0Aimplementing%20this%20comprehensive%20sensing%20framework%2C%20the%20availability%20of%20depth%0Ainformation%20is%20essential%20for%20enabling%20autonomous%20aerial%20vehicles%20to%20perceive%0Aand%20navigate%20around%20obstacles.%20In%20this%20work%2C%20we%20propose%20a%20method%20for%20estimating%0Athe%20distance%20information%20of%20a%20detected%20aerial%20object%20in%20real%20time%20using%20only%0Athe%20input%20of%20a%20monocular%20camera.%20In%20order%20to%20train%20our%20deep%20learning%20components%0Afor%20the%20object%20detection%2C%20tracking%20and%20depth%20estimation%20tasks%20we%20utilize%20the%0AAmazon%20Airborne%20Object%20Tracking%20%28AOT%29%20Dataset.%20In%20contrast%20to%20previous%0Aapproaches%20that%20integrate%20the%20depth%20estimation%20module%20into%20the%20object%20detector%2C%0Aour%20method%20formulates%20the%20problem%20as%20image-to-image%20translation.%20We%20employ%20a%0Aseparate%20lightweight%20encoder-decoder%20network%20for%20efficient%20and%20robust%20depth%0Aestimation.%20In%20a%20nutshell%2C%20the%20object%20detection%20module%20identifies%20and%20localizes%0Aobstacles%2C%20conveying%20this%20information%20to%20both%20the%20tracking%20module%20for%0Amonitoring%20obstacle%20movement%20and%20the%20depth%20estimation%20module%20for%20calculating%0Adistances.%20Our%20approach%20is%20evaluated%20on%20the%20Airborne%20Object%20Tracking%20%28AOT%29%0Adataset%20which%20is%20the%20largest%20%28to%20the%20best%20of%20our%20knowledge%29%20air-to-air%20airborne%0Aobject%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsuring%2520UAV%2520Safety%253A%2520A%2520Vision-only%2520and%2520Real-time%2520Framework%2520for%2520Collision%250A%2520%2520Avoidance%2520Through%2520Object%2520Detection%252C%2520Tracking%252C%2520and%2520Distance%2520Estimation%26entry.906535625%3DVasileios%2520Karampinis%2520and%2520Anastasios%2520Arsenos%2520and%2520Orfeas%2520Filippopoulos%2520and%2520Evangelos%2520Petrongonas%2520and%2520Christos%2520Skliros%2520and%2520Dimitrios%2520Kollias%2520and%2520Stefanos%2520Kollias%2520and%2520Athanasios%2520Voulodimos%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520twenty%2520years%252C%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520have%2520garnered%250Agrowing%2520interest%2520due%2520to%2520their%2520expanding%2520applications%2520in%2520both%2520military%2520and%250Acivilian%2520domains.%2520Detecting%2520non-cooperative%2520aerial%2520vehicles%2520with%2520efficiency%2520and%250Aestimating%2520collisions%2520accurately%2520are%2520pivotal%2520for%2520achieving%2520fully%2520autonomous%250Aaircraft%2520and%2520facilitating%2520Advanced%2520Air%2520Mobility%2520%2528AAM%2529.%2520This%2520paper%2520presents%2520a%250Adeep-learning%2520framework%2520that%2520utilizes%2520optical%2520sensors%2520for%2520the%2520detection%252C%250Atracking%252C%2520and%2520distance%2520estimation%2520of%2520non-cooperative%2520aerial%2520vehicles.%2520In%250Aimplementing%2520this%2520comprehensive%2520sensing%2520framework%252C%2520the%2520availability%2520of%2520depth%250Ainformation%2520is%2520essential%2520for%2520enabling%2520autonomous%2520aerial%2520vehicles%2520to%2520perceive%250Aand%2520navigate%2520around%2520obstacles.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520for%2520estimating%250Athe%2520distance%2520information%2520of%2520a%2520detected%2520aerial%2520object%2520in%2520real%2520time%2520using%2520only%250Athe%2520input%2520of%2520a%2520monocular%2520camera.%2520In%2520order%2520to%2520train%2520our%2520deep%2520learning%2520components%250Afor%2520the%2520object%2520detection%252C%2520tracking%2520and%2520depth%2520estimation%2520tasks%2520we%2520utilize%2520the%250AAmazon%2520Airborne%2520Object%2520Tracking%2520%2528AOT%2529%2520Dataset.%2520In%2520contrast%2520to%2520previous%250Aapproaches%2520that%2520integrate%2520the%2520depth%2520estimation%2520module%2520into%2520the%2520object%2520detector%252C%250Aour%2520method%2520formulates%2520the%2520problem%2520as%2520image-to-image%2520translation.%2520We%2520employ%2520a%250Aseparate%2520lightweight%2520encoder-decoder%2520network%2520for%2520efficient%2520and%2520robust%2520depth%250Aestimation.%2520In%2520a%2520nutshell%252C%2520the%2520object%2520detection%2520module%2520identifies%2520and%2520localizes%250Aobstacles%252C%2520conveying%2520this%2520information%2520to%2520both%2520the%2520tracking%2520module%2520for%250Amonitoring%2520obstacle%2520movement%2520and%2520the%2520depth%2520estimation%2520module%2520for%2520calculating%250Adistances.%2520Our%2520approach%2520is%2520evaluated%2520on%2520the%2520Airborne%2520Object%2520Tracking%2520%2528AOT%2529%250Adataset%2520which%2520is%2520the%2520largest%2520%2528to%2520the%2520best%2520of%2520our%2520knowledge%2529%2520air-to-air%2520airborne%250Aobject%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensuring%20UAV%20Safety%3A%20A%20Vision-only%20and%20Real-time%20Framework%20for%20Collision%0A%20%20Avoidance%20Through%20Object%20Detection%2C%20Tracking%2C%20and%20Distance%20Estimation&entry.906535625=Vasileios%20Karampinis%20and%20Anastasios%20Arsenos%20and%20Orfeas%20Filippopoulos%20and%20Evangelos%20Petrongonas%20and%20Christos%20Skliros%20and%20Dimitrios%20Kollias%20and%20Stefanos%20Kollias%20and%20Athanasios%20Voulodimos&entry.1292438233=%20%20In%20the%20last%20twenty%20years%2C%20unmanned%20aerial%20vehicles%20%28UAVs%29%20have%20garnered%0Agrowing%20interest%20due%20to%20their%20expanding%20applications%20in%20both%20military%20and%0Acivilian%20domains.%20Detecting%20non-cooperative%20aerial%20vehicles%20with%20efficiency%20and%0Aestimating%20collisions%20accurately%20are%20pivotal%20for%20achieving%20fully%20autonomous%0Aaircraft%20and%20facilitating%20Advanced%20Air%20Mobility%20%28AAM%29.%20This%20paper%20presents%20a%0Adeep-learning%20framework%20that%20utilizes%20optical%20sensors%20for%20the%20detection%2C%0Atracking%2C%20and%20distance%20estimation%20of%20non-cooperative%20aerial%20vehicles.%20In%0Aimplementing%20this%20comprehensive%20sensing%20framework%2C%20the%20availability%20of%20depth%0Ainformation%20is%20essential%20for%20enabling%20autonomous%20aerial%20vehicles%20to%20perceive%0Aand%20navigate%20around%20obstacles.%20In%20this%20work%2C%20we%20propose%20a%20method%20for%20estimating%0Athe%20distance%20information%20of%20a%20detected%20aerial%20object%20in%20real%20time%20using%20only%0Athe%20input%20of%20a%20monocular%20camera.%20In%20order%20to%20train%20our%20deep%20learning%20components%0Afor%20the%20object%20detection%2C%20tracking%20and%20depth%20estimation%20tasks%20we%20utilize%20the%0AAmazon%20Airborne%20Object%20Tracking%20%28AOT%29%20Dataset.%20In%20contrast%20to%20previous%0Aapproaches%20that%20integrate%20the%20depth%20estimation%20module%20into%20the%20object%20detector%2C%0Aour%20method%20formulates%20the%20problem%20as%20image-to-image%20translation.%20We%20employ%20a%0Aseparate%20lightweight%20encoder-decoder%20network%20for%20efficient%20and%20robust%20depth%0Aestimation.%20In%20a%20nutshell%2C%20the%20object%20detection%20module%20identifies%20and%20localizes%0Aobstacles%2C%20conveying%20this%20information%20to%20both%20the%20tracking%20module%20for%0Amonitoring%20obstacle%20movement%20and%20the%20depth%20estimation%20module%20for%20calculating%0Adistances.%20Our%20approach%20is%20evaluated%20on%20the%20Airborne%20Object%20Tracking%20%28AOT%29%0Adataset%20which%20is%20the%20largest%20%28to%20the%20best%20of%20our%20knowledge%29%20air-to-air%20airborne%0Aobject%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06749v2&entry.124074799=Read"},
{"title": "Protecting Your LLMs with Information Bottleneck", "author": "Zichuan Liu and Zefan Wang and Linjie Xu and Jinyu Wang and Lei Song and Tianchun Wang and Chunlin Chen and Wei Cheng and Jiang Bian", "abstract": "  The advent of large language models (LLMs) has revolutionized the field of\nnatural language processing, yet they might be attacked to produce harmful\ncontent. Despite efforts to ethically align LLMs, these are often fragile and\ncan be circumvented by jailbreaking attacks through optimized or manual\nadversarial prompts. To address this, we introduce the Information Bottleneck\nProtector (IBProtector), a defense mechanism grounded in the information\nbottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a\nlightweight and trainable extractor, preserving only essential information for\nthe target LLMs to respond with the expected answer. Moreover, we further\nconsider a situation where the gradient is not visible to be compatible with\nany LLM. Our empirical evaluations show that IBProtector outperforms current\ndefense methods in mitigating jailbreak attempts, without overly affecting\nresponse quality or inference speed. Its effectiveness and adaptability across\nvarious attack methods and target LLMs underscore the potential of IBProtector\nas a novel, transferable defense that bolsters the security of LLMs without\nrequiring modifications to the underlying models.\n", "link": "http://arxiv.org/abs/2404.13968v2", "date": "2024-05-16", "relevancy": 1.787, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4815}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4409}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protecting%20Your%20LLMs%20with%20Information%20Bottleneck&body=Title%3A%20Protecting%20Your%20LLMs%20with%20Information%20Bottleneck%0AAuthor%3A%20Zichuan%20Liu%20and%20Zefan%20Wang%20and%20Linjie%20Xu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Tianchun%20Wang%20and%20Chunlin%20Chen%20and%20Wei%20Cheng%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20revolutionized%20the%20field%20of%0Anatural%20language%20processing%2C%20yet%20they%20might%20be%20attacked%20to%20produce%20harmful%0Acontent.%20Despite%20efforts%20to%20ethically%20align%20LLMs%2C%20these%20are%20often%20fragile%20and%0Acan%20be%20circumvented%20by%20jailbreaking%20attacks%20through%20optimized%20or%20manual%0Aadversarial%20prompts.%20To%20address%20this%2C%20we%20introduce%20the%20Information%20Bottleneck%0AProtector%20%28IBProtector%29%2C%20a%20defense%20mechanism%20grounded%20in%20the%20information%0Abottleneck%20principle%2C%20and%20we%20modify%20the%20objective%20to%20avoid%20trivial%20solutions.%0AThe%20IBProtector%20selectively%20compresses%20and%20perturbs%20prompts%2C%20facilitated%20by%20a%0Alightweight%20and%20trainable%20extractor%2C%20preserving%20only%20essential%20information%20for%0Athe%20target%20LLMs%20to%20respond%20with%20the%20expected%20answer.%20Moreover%2C%20we%20further%0Aconsider%20a%20situation%20where%20the%20gradient%20is%20not%20visible%20to%20be%20compatible%20with%0Aany%20LLM.%20Our%20empirical%20evaluations%20show%20that%20IBProtector%20outperforms%20current%0Adefense%20methods%20in%20mitigating%20jailbreak%20attempts%2C%20without%20overly%20affecting%0Aresponse%20quality%20or%20inference%20speed.%20Its%20effectiveness%20and%20adaptability%20across%0Avarious%20attack%20methods%20and%20target%20LLMs%20underscore%20the%20potential%20of%20IBProtector%0Aas%20a%20novel%2C%20transferable%20defense%20that%20bolsters%20the%20security%20of%20LLMs%20without%0Arequiring%20modifications%20to%20the%20underlying%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtecting%2520Your%2520LLMs%2520with%2520Information%2520Bottleneck%26entry.906535625%3DZichuan%2520Liu%2520and%2520Zefan%2520Wang%2520and%2520Linjie%2520Xu%2520and%2520Jinyu%2520Wang%2520and%2520Lei%2520Song%2520and%2520Tianchun%2520Wang%2520and%2520Chunlin%2520Chen%2520and%2520Wei%2520Cheng%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520revolutionized%2520the%2520field%2520of%250Anatural%2520language%2520processing%252C%2520yet%2520they%2520might%2520be%2520attacked%2520to%2520produce%2520harmful%250Acontent.%2520Despite%2520efforts%2520to%2520ethically%2520align%2520LLMs%252C%2520these%2520are%2520often%2520fragile%2520and%250Acan%2520be%2520circumvented%2520by%2520jailbreaking%2520attacks%2520through%2520optimized%2520or%2520manual%250Aadversarial%2520prompts.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Information%2520Bottleneck%250AProtector%2520%2528IBProtector%2529%252C%2520a%2520defense%2520mechanism%2520grounded%2520in%2520the%2520information%250Abottleneck%2520principle%252C%2520and%2520we%2520modify%2520the%2520objective%2520to%2520avoid%2520trivial%2520solutions.%250AThe%2520IBProtector%2520selectively%2520compresses%2520and%2520perturbs%2520prompts%252C%2520facilitated%2520by%2520a%250Alightweight%2520and%2520trainable%2520extractor%252C%2520preserving%2520only%2520essential%2520information%2520for%250Athe%2520target%2520LLMs%2520to%2520respond%2520with%2520the%2520expected%2520answer.%2520Moreover%252C%2520we%2520further%250Aconsider%2520a%2520situation%2520where%2520the%2520gradient%2520is%2520not%2520visible%2520to%2520be%2520compatible%2520with%250Aany%2520LLM.%2520Our%2520empirical%2520evaluations%2520show%2520that%2520IBProtector%2520outperforms%2520current%250Adefense%2520methods%2520in%2520mitigating%2520jailbreak%2520attempts%252C%2520without%2520overly%2520affecting%250Aresponse%2520quality%2520or%2520inference%2520speed.%2520Its%2520effectiveness%2520and%2520adaptability%2520across%250Avarious%2520attack%2520methods%2520and%2520target%2520LLMs%2520underscore%2520the%2520potential%2520of%2520IBProtector%250Aas%2520a%2520novel%252C%2520transferable%2520defense%2520that%2520bolsters%2520the%2520security%2520of%2520LLMs%2520without%250Arequiring%2520modifications%2520to%2520the%2520underlying%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protecting%20Your%20LLMs%20with%20Information%20Bottleneck&entry.906535625=Zichuan%20Liu%20and%20Zefan%20Wang%20and%20Linjie%20Xu%20and%20Jinyu%20Wang%20and%20Lei%20Song%20and%20Tianchun%20Wang%20and%20Chunlin%20Chen%20and%20Wei%20Cheng%20and%20Jiang%20Bian&entry.1292438233=%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20revolutionized%20the%20field%20of%0Anatural%20language%20processing%2C%20yet%20they%20might%20be%20attacked%20to%20produce%20harmful%0Acontent.%20Despite%20efforts%20to%20ethically%20align%20LLMs%2C%20these%20are%20often%20fragile%20and%0Acan%20be%20circumvented%20by%20jailbreaking%20attacks%20through%20optimized%20or%20manual%0Aadversarial%20prompts.%20To%20address%20this%2C%20we%20introduce%20the%20Information%20Bottleneck%0AProtector%20%28IBProtector%29%2C%20a%20defense%20mechanism%20grounded%20in%20the%20information%0Abottleneck%20principle%2C%20and%20we%20modify%20the%20objective%20to%20avoid%20trivial%20solutions.%0AThe%20IBProtector%20selectively%20compresses%20and%20perturbs%20prompts%2C%20facilitated%20by%20a%0Alightweight%20and%20trainable%20extractor%2C%20preserving%20only%20essential%20information%20for%0Athe%20target%20LLMs%20to%20respond%20with%20the%20expected%20answer.%20Moreover%2C%20we%20further%0Aconsider%20a%20situation%20where%20the%20gradient%20is%20not%20visible%20to%20be%20compatible%20with%0Aany%20LLM.%20Our%20empirical%20evaluations%20show%20that%20IBProtector%20outperforms%20current%0Adefense%20methods%20in%20mitigating%20jailbreak%20attempts%2C%20without%20overly%20affecting%0Aresponse%20quality%20or%20inference%20speed.%20Its%20effectiveness%20and%20adaptability%20across%0Avarious%20attack%20methods%20and%20target%20LLMs%20underscore%20the%20potential%20of%20IBProtector%0Aas%20a%20novel%2C%20transferable%20defense%20that%20bolsters%20the%20security%20of%20LLMs%20without%0Arequiring%20modifications%20to%20the%20underlying%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13968v2&entry.124074799=Read"},
{"title": "Zero-Shot Hierarchical Classification on the Common Procurement\n  Vocabulary Taxonomy", "author": "Federico Moiraghi and Matteo Palmonari and Davide Allavena and Federico Morando", "abstract": "  Classifying public tenders is a useful task for both companies that are\ninvited to participate and for inspecting fraudulent activities. To facilitate\nthe task for both participants and public administrations, the European Union\npresented a common taxonomy (\\textit{Common Procurement Vocabulary}, CPV) which\nis mandatory for tenders of certain importance; however, the contracts in which\na CPV label is mandatory are the minority compared to all the Public\nAdministrations activities. Classifying over a real-world taxonomy introduces\nsome difficulties that can not be ignored. First of all, some fine-grained\nclasses have an insufficient (if any) number of observations in the training\nset, while other classes are far more frequent (even thousands of times) than\nthe average. To overcome those difficulties, we present a zero-shot approach,\nbased on a pre-trained language model that relies only on label description and\nrespects the label taxonomy. To train our proposed model, we used industrial\ndata, which comes from \\url{contrattipubblici.org}, a service by\n\\href{https://spaziodati.eu}{SpazioDati s.r.l}. that collects public contracts\nstipulated in Italy in the last 25 years. Results show that the proposed model\nachieves better performance in classifying low-frequent classes compared to\nthree different baselines, and is also able to predict never-seen classes.\n", "link": "http://arxiv.org/abs/2405.09983v1", "date": "2024-05-16", "relevancy": 1.7638, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.445}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Hierarchical%20Classification%20on%20the%20Common%20Procurement%0A%20%20Vocabulary%20Taxonomy&body=Title%3A%20Zero-Shot%20Hierarchical%20Classification%20on%20the%20Common%20Procurement%0A%20%20Vocabulary%20Taxonomy%0AAuthor%3A%20Federico%20Moiraghi%20and%20Matteo%20Palmonari%20and%20Davide%20Allavena%20and%20Federico%20Morando%0AAbstract%3A%20%20%20Classifying%20public%20tenders%20is%20a%20useful%20task%20for%20both%20companies%20that%20are%0Ainvited%20to%20participate%20and%20for%20inspecting%20fraudulent%20activities.%20To%20facilitate%0Athe%20task%20for%20both%20participants%20and%20public%20administrations%2C%20the%20European%20Union%0Apresented%20a%20common%20taxonomy%20%28%5Ctextit%7BCommon%20Procurement%20Vocabulary%7D%2C%20CPV%29%20which%0Ais%20mandatory%20for%20tenders%20of%20certain%20importance%3B%20however%2C%20the%20contracts%20in%20which%0Aa%20CPV%20label%20is%20mandatory%20are%20the%20minority%20compared%20to%20all%20the%20Public%0AAdministrations%20activities.%20Classifying%20over%20a%20real-world%20taxonomy%20introduces%0Asome%20difficulties%20that%20can%20not%20be%20ignored.%20First%20of%20all%2C%20some%20fine-grained%0Aclasses%20have%20an%20insufficient%20%28if%20any%29%20number%20of%20observations%20in%20the%20training%0Aset%2C%20while%20other%20classes%20are%20far%20more%20frequent%20%28even%20thousands%20of%20times%29%20than%0Athe%20average.%20To%20overcome%20those%20difficulties%2C%20we%20present%20a%20zero-shot%20approach%2C%0Abased%20on%20a%20pre-trained%20language%20model%20that%20relies%20only%20on%20label%20description%20and%0Arespects%20the%20label%20taxonomy.%20To%20train%20our%20proposed%20model%2C%20we%20used%20industrial%0Adata%2C%20which%20comes%20from%20%5Curl%7Bcontrattipubblici.org%7D%2C%20a%20service%20by%0A%5Chref%7Bhttps%3A//spaziodati.eu%7D%7BSpazioDati%20s.r.l%7D.%20that%20collects%20public%20contracts%0Astipulated%20in%20Italy%20in%20the%20last%2025%20years.%20Results%20show%20that%20the%20proposed%20model%0Aachieves%20better%20performance%20in%20classifying%20low-frequent%20classes%20compared%20to%0Athree%20different%20baselines%2C%20and%20is%20also%20able%20to%20predict%20never-seen%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Hierarchical%2520Classification%2520on%2520the%2520Common%2520Procurement%250A%2520%2520Vocabulary%2520Taxonomy%26entry.906535625%3DFederico%2520Moiraghi%2520and%2520Matteo%2520Palmonari%2520and%2520Davide%2520Allavena%2520and%2520Federico%2520Morando%26entry.1292438233%3D%2520%2520Classifying%2520public%2520tenders%2520is%2520a%2520useful%2520task%2520for%2520both%2520companies%2520that%2520are%250Ainvited%2520to%2520participate%2520and%2520for%2520inspecting%2520fraudulent%2520activities.%2520To%2520facilitate%250Athe%2520task%2520for%2520both%2520participants%2520and%2520public%2520administrations%252C%2520the%2520European%2520Union%250Apresented%2520a%2520common%2520taxonomy%2520%2528%255Ctextit%257BCommon%2520Procurement%2520Vocabulary%257D%252C%2520CPV%2529%2520which%250Ais%2520mandatory%2520for%2520tenders%2520of%2520certain%2520importance%253B%2520however%252C%2520the%2520contracts%2520in%2520which%250Aa%2520CPV%2520label%2520is%2520mandatory%2520are%2520the%2520minority%2520compared%2520to%2520all%2520the%2520Public%250AAdministrations%2520activities.%2520Classifying%2520over%2520a%2520real-world%2520taxonomy%2520introduces%250Asome%2520difficulties%2520that%2520can%2520not%2520be%2520ignored.%2520First%2520of%2520all%252C%2520some%2520fine-grained%250Aclasses%2520have%2520an%2520insufficient%2520%2528if%2520any%2529%2520number%2520of%2520observations%2520in%2520the%2520training%250Aset%252C%2520while%2520other%2520classes%2520are%2520far%2520more%2520frequent%2520%2528even%2520thousands%2520of%2520times%2529%2520than%250Athe%2520average.%2520To%2520overcome%2520those%2520difficulties%252C%2520we%2520present%2520a%2520zero-shot%2520approach%252C%250Abased%2520on%2520a%2520pre-trained%2520language%2520model%2520that%2520relies%2520only%2520on%2520label%2520description%2520and%250Arespects%2520the%2520label%2520taxonomy.%2520To%2520train%2520our%2520proposed%2520model%252C%2520we%2520used%2520industrial%250Adata%252C%2520which%2520comes%2520from%2520%255Curl%257Bcontrattipubblici.org%257D%252C%2520a%2520service%2520by%250A%255Chref%257Bhttps%253A//spaziodati.eu%257D%257BSpazioDati%2520s.r.l%257D.%2520that%2520collects%2520public%2520contracts%250Astipulated%2520in%2520Italy%2520in%2520the%2520last%252025%2520years.%2520Results%2520show%2520that%2520the%2520proposed%2520model%250Aachieves%2520better%2520performance%2520in%2520classifying%2520low-frequent%2520classes%2520compared%2520to%250Athree%2520different%2520baselines%252C%2520and%2520is%2520also%2520able%2520to%2520predict%2520never-seen%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Hierarchical%20Classification%20on%20the%20Common%20Procurement%0A%20%20Vocabulary%20Taxonomy&entry.906535625=Federico%20Moiraghi%20and%20Matteo%20Palmonari%20and%20Davide%20Allavena%20and%20Federico%20Morando&entry.1292438233=%20%20Classifying%20public%20tenders%20is%20a%20useful%20task%20for%20both%20companies%20that%20are%0Ainvited%20to%20participate%20and%20for%20inspecting%20fraudulent%20activities.%20To%20facilitate%0Athe%20task%20for%20both%20participants%20and%20public%20administrations%2C%20the%20European%20Union%0Apresented%20a%20common%20taxonomy%20%28%5Ctextit%7BCommon%20Procurement%20Vocabulary%7D%2C%20CPV%29%20which%0Ais%20mandatory%20for%20tenders%20of%20certain%20importance%3B%20however%2C%20the%20contracts%20in%20which%0Aa%20CPV%20label%20is%20mandatory%20are%20the%20minority%20compared%20to%20all%20the%20Public%0AAdministrations%20activities.%20Classifying%20over%20a%20real-world%20taxonomy%20introduces%0Asome%20difficulties%20that%20can%20not%20be%20ignored.%20First%20of%20all%2C%20some%20fine-grained%0Aclasses%20have%20an%20insufficient%20%28if%20any%29%20number%20of%20observations%20in%20the%20training%0Aset%2C%20while%20other%20classes%20are%20far%20more%20frequent%20%28even%20thousands%20of%20times%29%20than%0Athe%20average.%20To%20overcome%20those%20difficulties%2C%20we%20present%20a%20zero-shot%20approach%2C%0Abased%20on%20a%20pre-trained%20language%20model%20that%20relies%20only%20on%20label%20description%20and%0Arespects%20the%20label%20taxonomy.%20To%20train%20our%20proposed%20model%2C%20we%20used%20industrial%0Adata%2C%20which%20comes%20from%20%5Curl%7Bcontrattipubblici.org%7D%2C%20a%20service%20by%0A%5Chref%7Bhttps%3A//spaziodati.eu%7D%7BSpazioDati%20s.r.l%7D.%20that%20collects%20public%20contracts%0Astipulated%20in%20Italy%20in%20the%20last%2025%20years.%20Results%20show%20that%20the%20proposed%20model%0Aachieves%20better%20performance%20in%20classifying%20low-frequent%20classes%20compared%20to%0Athree%20different%20baselines%2C%20and%20is%20also%20able%20to%20predict%20never-seen%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09983v1&entry.124074799=Read"},
{"title": "A finite-sample generalization bound for stable LPV systems", "author": "Daniel Racz and Martin Gonzalez and Mihaly Petreczky and Andras Benczur and Balint Daroczy", "abstract": "  One of the main theoretical challenges in learning dynamical systems from\ndata is providing upper bounds on the generalization error, that is, the\ndifference between the expected prediction error and the empirical prediction\nerror measured on some finite sample. In machine learning, a popular class of\nsuch bounds are the so-called Probably Approximately Correct (PAC) bounds. In\nthis paper, we derive a PAC bound for stable continuous-time linear\nparameter-varying (LPV) systems. Our bound depends on the H2 norm of the chosen\nclass of the LPV systems, but does not depend on the time interval for which\nthe signals are considered.\n", "link": "http://arxiv.org/abs/2405.10054v1", "date": "2024-05-16", "relevancy": 1.7501, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20finite-sample%20generalization%20bound%20for%20stable%20LPV%20systems&body=Title%3A%20A%20finite-sample%20generalization%20bound%20for%20stable%20LPV%20systems%0AAuthor%3A%20Daniel%20Racz%20and%20Martin%20Gonzalez%20and%20Mihaly%20Petreczky%20and%20Andras%20Benczur%20and%20Balint%20Daroczy%0AAbstract%3A%20%20%20One%20of%20the%20main%20theoretical%20challenges%20in%20learning%20dynamical%20systems%20from%0Adata%20is%20providing%20upper%20bounds%20on%20the%20generalization%20error%2C%20that%20is%2C%20the%0Adifference%20between%20the%20expected%20prediction%20error%20and%20the%20empirical%20prediction%0Aerror%20measured%20on%20some%20finite%20sample.%20In%20machine%20learning%2C%20a%20popular%20class%20of%0Asuch%20bounds%20are%20the%20so-called%20Probably%20Approximately%20Correct%20%28PAC%29%20bounds.%20In%0Athis%20paper%2C%20we%20derive%20a%20PAC%20bound%20for%20stable%20continuous-time%20linear%0Aparameter-varying%20%28LPV%29%20systems.%20Our%20bound%20depends%20on%20the%20H2%20norm%20of%20the%20chosen%0Aclass%20of%20the%20LPV%20systems%2C%20but%20does%20not%20depend%20on%20the%20time%20interval%20for%20which%0Athe%20signals%20are%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520finite-sample%2520generalization%2520bound%2520for%2520stable%2520LPV%2520systems%26entry.906535625%3DDaniel%2520Racz%2520and%2520Martin%2520Gonzalez%2520and%2520Mihaly%2520Petreczky%2520and%2520Andras%2520Benczur%2520and%2520Balint%2520Daroczy%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520main%2520theoretical%2520challenges%2520in%2520learning%2520dynamical%2520systems%2520from%250Adata%2520is%2520providing%2520upper%2520bounds%2520on%2520the%2520generalization%2520error%252C%2520that%2520is%252C%2520the%250Adifference%2520between%2520the%2520expected%2520prediction%2520error%2520and%2520the%2520empirical%2520prediction%250Aerror%2520measured%2520on%2520some%2520finite%2520sample.%2520In%2520machine%2520learning%252C%2520a%2520popular%2520class%2520of%250Asuch%2520bounds%2520are%2520the%2520so-called%2520Probably%2520Approximately%2520Correct%2520%2528PAC%2529%2520bounds.%2520In%250Athis%2520paper%252C%2520we%2520derive%2520a%2520PAC%2520bound%2520for%2520stable%2520continuous-time%2520linear%250Aparameter-varying%2520%2528LPV%2529%2520systems.%2520Our%2520bound%2520depends%2520on%2520the%2520H2%2520norm%2520of%2520the%2520chosen%250Aclass%2520of%2520the%2520LPV%2520systems%252C%2520but%2520does%2520not%2520depend%2520on%2520the%2520time%2520interval%2520for%2520which%250Athe%2520signals%2520are%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20finite-sample%20generalization%20bound%20for%20stable%20LPV%20systems&entry.906535625=Daniel%20Racz%20and%20Martin%20Gonzalez%20and%20Mihaly%20Petreczky%20and%20Andras%20Benczur%20and%20Balint%20Daroczy&entry.1292438233=%20%20One%20of%20the%20main%20theoretical%20challenges%20in%20learning%20dynamical%20systems%20from%0Adata%20is%20providing%20upper%20bounds%20on%20the%20generalization%20error%2C%20that%20is%2C%20the%0Adifference%20between%20the%20expected%20prediction%20error%20and%20the%20empirical%20prediction%0Aerror%20measured%20on%20some%20finite%20sample.%20In%20machine%20learning%2C%20a%20popular%20class%20of%0Asuch%20bounds%20are%20the%20so-called%20Probably%20Approximately%20Correct%20%28PAC%29%20bounds.%20In%0Athis%20paper%2C%20we%20derive%20a%20PAC%20bound%20for%20stable%20continuous-time%20linear%0Aparameter-varying%20%28LPV%29%20systems.%20Our%20bound%20depends%20on%20the%20H2%20norm%20of%20the%20chosen%0Aclass%20of%20the%20LPV%20systems%2C%20but%20does%20not%20depend%20on%20the%20time%20interval%20for%20which%0Athe%20signals%20are%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10054v1&entry.124074799=Read"},
{"title": "Natural Language Can Help Bridge the Sim2Real Gap", "author": "Albert Yu and Adeline Foote and Raymond Mooney and Roberto Mart\u00edn-Mart\u00edn", "abstract": "  The main challenge in learning image-conditioned robotic policies is\nacquiring a visual representation conducive to low-level control. Due to the\nhigh dimensionality of the image space, learning a good visual representation\nrequires a considerable amount of visual data. However, when learning in the\nreal world, data is expensive. Sim2Real is a promising paradigm for overcoming\ndata scarcity in the real-world target domain by using a simulator to collect\nlarge amounts of cheap data closely related to the target task. However, it is\ndifficult to transfer an image-conditioned policy from sim to real when the\ndomains are very visually dissimilar. To bridge the sim2real visual gap, we\npropose using natural language descriptions of images as a unifying signal\nacross domains that captures the underlying task-relevant semantics. Our key\ninsight is that if two image observations from different domains are labeled\nwith similar language, the policy should predict similar action distributions\nfor both images. We demonstrate that training the image encoder to predict the\nlanguage description or the distance between descriptions of a sim or real\nimage serves as a useful, data-efficient pretraining step that helps learn a\ndomain-invariant image representation. We can then use this image encoder as\nthe backbone of an IL policy trained simultaneously on a large amount of\nsimulated and a handful of real demonstrations. Our approach outperforms widely\nused prior sim2real methods and strong vision-language pretraining baselines\nlike CLIP and R3M by 25 to 40%.\n", "link": "http://arxiv.org/abs/2405.10020v1", "date": "2024-05-16", "relevancy": 1.7497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5865}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Can%20Help%20Bridge%20the%20Sim2Real%20Gap&body=Title%3A%20Natural%20Language%20Can%20Help%20Bridge%20the%20Sim2Real%20Gap%0AAuthor%3A%20Albert%20Yu%20and%20Adeline%20Foote%20and%20Raymond%20Mooney%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn%0AAbstract%3A%20%20%20The%20main%20challenge%20in%20learning%20image-conditioned%20robotic%20policies%20is%0Aacquiring%20a%20visual%20representation%20conducive%20to%20low-level%20control.%20Due%20to%20the%0Ahigh%20dimensionality%20of%20the%20image%20space%2C%20learning%20a%20good%20visual%20representation%0Arequires%20a%20considerable%20amount%20of%20visual%20data.%20However%2C%20when%20learning%20in%20the%0Areal%20world%2C%20data%20is%20expensive.%20Sim2Real%20is%20a%20promising%20paradigm%20for%20overcoming%0Adata%20scarcity%20in%20the%20real-world%20target%20domain%20by%20using%20a%20simulator%20to%20collect%0Alarge%20amounts%20of%20cheap%20data%20closely%20related%20to%20the%20target%20task.%20However%2C%20it%20is%0Adifficult%20to%20transfer%20an%20image-conditioned%20policy%20from%20sim%20to%20real%20when%20the%0Adomains%20are%20very%20visually%20dissimilar.%20To%20bridge%20the%20sim2real%20visual%20gap%2C%20we%0Apropose%20using%20natural%20language%20descriptions%20of%20images%20as%20a%20unifying%20signal%0Aacross%20domains%20that%20captures%20the%20underlying%20task-relevant%20semantics.%20Our%20key%0Ainsight%20is%20that%20if%20two%20image%20observations%20from%20different%20domains%20are%20labeled%0Awith%20similar%20language%2C%20the%20policy%20should%20predict%20similar%20action%20distributions%0Afor%20both%20images.%20We%20demonstrate%20that%20training%20the%20image%20encoder%20to%20predict%20the%0Alanguage%20description%20or%20the%20distance%20between%20descriptions%20of%20a%20sim%20or%20real%0Aimage%20serves%20as%20a%20useful%2C%20data-efficient%20pretraining%20step%20that%20helps%20learn%20a%0Adomain-invariant%20image%20representation.%20We%20can%20then%20use%20this%20image%20encoder%20as%0Athe%20backbone%20of%20an%20IL%20policy%20trained%20simultaneously%20on%20a%20large%20amount%20of%0Asimulated%20and%20a%20handful%20of%20real%20demonstrations.%20Our%20approach%20outperforms%20widely%0Aused%20prior%20sim2real%20methods%20and%20strong%20vision-language%20pretraining%20baselines%0Alike%20CLIP%20and%20R3M%20by%2025%20to%2040%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Can%2520Help%2520Bridge%2520the%2520Sim2Real%2520Gap%26entry.906535625%3DAlbert%2520Yu%2520and%2520Adeline%2520Foote%2520and%2520Raymond%2520Mooney%2520and%2520Roberto%2520Mart%25C3%25ADn-Mart%25C3%25ADn%26entry.1292438233%3D%2520%2520The%2520main%2520challenge%2520in%2520learning%2520image-conditioned%2520robotic%2520policies%2520is%250Aacquiring%2520a%2520visual%2520representation%2520conducive%2520to%2520low-level%2520control.%2520Due%2520to%2520the%250Ahigh%2520dimensionality%2520of%2520the%2520image%2520space%252C%2520learning%2520a%2520good%2520visual%2520representation%250Arequires%2520a%2520considerable%2520amount%2520of%2520visual%2520data.%2520However%252C%2520when%2520learning%2520in%2520the%250Areal%2520world%252C%2520data%2520is%2520expensive.%2520Sim2Real%2520is%2520a%2520promising%2520paradigm%2520for%2520overcoming%250Adata%2520scarcity%2520in%2520the%2520real-world%2520target%2520domain%2520by%2520using%2520a%2520simulator%2520to%2520collect%250Alarge%2520amounts%2520of%2520cheap%2520data%2520closely%2520related%2520to%2520the%2520target%2520task.%2520However%252C%2520it%2520is%250Adifficult%2520to%2520transfer%2520an%2520image-conditioned%2520policy%2520from%2520sim%2520to%2520real%2520when%2520the%250Adomains%2520are%2520very%2520visually%2520dissimilar.%2520To%2520bridge%2520the%2520sim2real%2520visual%2520gap%252C%2520we%250Apropose%2520using%2520natural%2520language%2520descriptions%2520of%2520images%2520as%2520a%2520unifying%2520signal%250Aacross%2520domains%2520that%2520captures%2520the%2520underlying%2520task-relevant%2520semantics.%2520Our%2520key%250Ainsight%2520is%2520that%2520if%2520two%2520image%2520observations%2520from%2520different%2520domains%2520are%2520labeled%250Awith%2520similar%2520language%252C%2520the%2520policy%2520should%2520predict%2520similar%2520action%2520distributions%250Afor%2520both%2520images.%2520We%2520demonstrate%2520that%2520training%2520the%2520image%2520encoder%2520to%2520predict%2520the%250Alanguage%2520description%2520or%2520the%2520distance%2520between%2520descriptions%2520of%2520a%2520sim%2520or%2520real%250Aimage%2520serves%2520as%2520a%2520useful%252C%2520data-efficient%2520pretraining%2520step%2520that%2520helps%2520learn%2520a%250Adomain-invariant%2520image%2520representation.%2520We%2520can%2520then%2520use%2520this%2520image%2520encoder%2520as%250Athe%2520backbone%2520of%2520an%2520IL%2520policy%2520trained%2520simultaneously%2520on%2520a%2520large%2520amount%2520of%250Asimulated%2520and%2520a%2520handful%2520of%2520real%2520demonstrations.%2520Our%2520approach%2520outperforms%2520widely%250Aused%2520prior%2520sim2real%2520methods%2520and%2520strong%2520vision-language%2520pretraining%2520baselines%250Alike%2520CLIP%2520and%2520R3M%2520by%252025%2520to%252040%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Can%20Help%20Bridge%20the%20Sim2Real%20Gap&entry.906535625=Albert%20Yu%20and%20Adeline%20Foote%20and%20Raymond%20Mooney%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn&entry.1292438233=%20%20The%20main%20challenge%20in%20learning%20image-conditioned%20robotic%20policies%20is%0Aacquiring%20a%20visual%20representation%20conducive%20to%20low-level%20control.%20Due%20to%20the%0Ahigh%20dimensionality%20of%20the%20image%20space%2C%20learning%20a%20good%20visual%20representation%0Arequires%20a%20considerable%20amount%20of%20visual%20data.%20However%2C%20when%20learning%20in%20the%0Areal%20world%2C%20data%20is%20expensive.%20Sim2Real%20is%20a%20promising%20paradigm%20for%20overcoming%0Adata%20scarcity%20in%20the%20real-world%20target%20domain%20by%20using%20a%20simulator%20to%20collect%0Alarge%20amounts%20of%20cheap%20data%20closely%20related%20to%20the%20target%20task.%20However%2C%20it%20is%0Adifficult%20to%20transfer%20an%20image-conditioned%20policy%20from%20sim%20to%20real%20when%20the%0Adomains%20are%20very%20visually%20dissimilar.%20To%20bridge%20the%20sim2real%20visual%20gap%2C%20we%0Apropose%20using%20natural%20language%20descriptions%20of%20images%20as%20a%20unifying%20signal%0Aacross%20domains%20that%20captures%20the%20underlying%20task-relevant%20semantics.%20Our%20key%0Ainsight%20is%20that%20if%20two%20image%20observations%20from%20different%20domains%20are%20labeled%0Awith%20similar%20language%2C%20the%20policy%20should%20predict%20similar%20action%20distributions%0Afor%20both%20images.%20We%20demonstrate%20that%20training%20the%20image%20encoder%20to%20predict%20the%0Alanguage%20description%20or%20the%20distance%20between%20descriptions%20of%20a%20sim%20or%20real%0Aimage%20serves%20as%20a%20useful%2C%20data-efficient%20pretraining%20step%20that%20helps%20learn%20a%0Adomain-invariant%20image%20representation.%20We%20can%20then%20use%20this%20image%20encoder%20as%0Athe%20backbone%20of%20an%20IL%20policy%20trained%20simultaneously%20on%20a%20large%20amount%20of%0Asimulated%20and%20a%20handful%20of%20real%20demonstrations.%20Our%20approach%20outperforms%20widely%0Aused%20prior%20sim2real%20methods%20and%20strong%20vision-language%20pretraining%20baselines%0Alike%20CLIP%20and%20R3M%20by%2025%20to%2040%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10020v1&entry.124074799=Read"},
{"title": "Synthpop++: A Hybrid Framework for Generating A Country-scale Synthetic\n  Population", "author": "Bhavesh Neekhra and Kshitij Kapoor and Debayan Gupta", "abstract": "  Population censuses are vital to public policy decision-making. They provide\ninsight into human resources, demography, culture, and economic structure at\nlocal, regional, and national levels. However, such surveys are very expensive\n(especially for low and middle-income countries with high populations, such as\nIndia), time-consuming, and may also raise privacy concerns, depending upon the\nkinds of data collected.\n  In light of these issues, we introduce SynthPop++, a novel hybrid framework,\nwhich can combine data from multiple real-world surveys (with different,\npartially overlapping sets of attributes) to produce a real-scale synthetic\npopulation of humans. Critically, our population maintains family structures\ncomprising individuals with demographic, socioeconomic, health, and geolocation\nattributes: this means that our ``fake'' people live in realistic locations,\nhave realistic families, etc. Such data can be used for a variety of purposes:\nwe explore one such use case, Agent-based modelling of infectious disease in\nIndia.\n  To gauge the quality of our synthetic population, we use both machine\nlearning and statistical metrics. Our experimental results show that synthetic\npopulation can realistically simulate the population for various administrative\nunits of India, producing real-scale, detailed data at the desired level of\nzoom -- from cities, to districts, to states, eventually combining to form a\ncountry-scale synthetic population.\n", "link": "http://arxiv.org/abs/2304.12284v2", "date": "2024-05-16", "relevancy": 1.7475, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4436}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4407}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthpop%2B%2B%3A%20A%20Hybrid%20Framework%20for%20Generating%20A%20Country-scale%20Synthetic%0A%20%20Population&body=Title%3A%20Synthpop%2B%2B%3A%20A%20Hybrid%20Framework%20for%20Generating%20A%20Country-scale%20Synthetic%0A%20%20Population%0AAuthor%3A%20Bhavesh%20Neekhra%20and%20Kshitij%20Kapoor%20and%20Debayan%20Gupta%0AAbstract%3A%20%20%20Population%20censuses%20are%20vital%20to%20public%20policy%20decision-making.%20They%20provide%0Ainsight%20into%20human%20resources%2C%20demography%2C%20culture%2C%20and%20economic%20structure%20at%0Alocal%2C%20regional%2C%20and%20national%20levels.%20However%2C%20such%20surveys%20are%20very%20expensive%0A%28especially%20for%20low%20and%20middle-income%20countries%20with%20high%20populations%2C%20such%20as%0AIndia%29%2C%20time-consuming%2C%20and%20may%20also%20raise%20privacy%20concerns%2C%20depending%20upon%20the%0Akinds%20of%20data%20collected.%0A%20%20In%20light%20of%20these%20issues%2C%20we%20introduce%20SynthPop%2B%2B%2C%20a%20novel%20hybrid%20framework%2C%0Awhich%20can%20combine%20data%20from%20multiple%20real-world%20surveys%20%28with%20different%2C%0Apartially%20overlapping%20sets%20of%20attributes%29%20to%20produce%20a%20real-scale%20synthetic%0Apopulation%20of%20humans.%20Critically%2C%20our%20population%20maintains%20family%20structures%0Acomprising%20individuals%20with%20demographic%2C%20socioeconomic%2C%20health%2C%20and%20geolocation%0Aattributes%3A%20this%20means%20that%20our%20%60%60fake%27%27%20people%20live%20in%20realistic%20locations%2C%0Ahave%20realistic%20families%2C%20etc.%20Such%20data%20can%20be%20used%20for%20a%20variety%20of%20purposes%3A%0Awe%20explore%20one%20such%20use%20case%2C%20Agent-based%20modelling%20of%20infectious%20disease%20in%0AIndia.%0A%20%20To%20gauge%20the%20quality%20of%20our%20synthetic%20population%2C%20we%20use%20both%20machine%0Alearning%20and%20statistical%20metrics.%20Our%20experimental%20results%20show%20that%20synthetic%0Apopulation%20can%20realistically%20simulate%20the%20population%20for%20various%20administrative%0Aunits%20of%20India%2C%20producing%20real-scale%2C%20detailed%20data%20at%20the%20desired%20level%20of%0Azoom%20--%20from%20cities%2C%20to%20districts%2C%20to%20states%2C%20eventually%20combining%20to%20form%20a%0Acountry-scale%20synthetic%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthpop%252B%252B%253A%2520A%2520Hybrid%2520Framework%2520for%2520Generating%2520A%2520Country-scale%2520Synthetic%250A%2520%2520Population%26entry.906535625%3DBhavesh%2520Neekhra%2520and%2520Kshitij%2520Kapoor%2520and%2520Debayan%2520Gupta%26entry.1292438233%3D%2520%2520Population%2520censuses%2520are%2520vital%2520to%2520public%2520policy%2520decision-making.%2520They%2520provide%250Ainsight%2520into%2520human%2520resources%252C%2520demography%252C%2520culture%252C%2520and%2520economic%2520structure%2520at%250Alocal%252C%2520regional%252C%2520and%2520national%2520levels.%2520However%252C%2520such%2520surveys%2520are%2520very%2520expensive%250A%2528especially%2520for%2520low%2520and%2520middle-income%2520countries%2520with%2520high%2520populations%252C%2520such%2520as%250AIndia%2529%252C%2520time-consuming%252C%2520and%2520may%2520also%2520raise%2520privacy%2520concerns%252C%2520depending%2520upon%2520the%250Akinds%2520of%2520data%2520collected.%250A%2520%2520In%2520light%2520of%2520these%2520issues%252C%2520we%2520introduce%2520SynthPop%252B%252B%252C%2520a%2520novel%2520hybrid%2520framework%252C%250Awhich%2520can%2520combine%2520data%2520from%2520multiple%2520real-world%2520surveys%2520%2528with%2520different%252C%250Apartially%2520overlapping%2520sets%2520of%2520attributes%2529%2520to%2520produce%2520a%2520real-scale%2520synthetic%250Apopulation%2520of%2520humans.%2520Critically%252C%2520our%2520population%2520maintains%2520family%2520structures%250Acomprising%2520individuals%2520with%2520demographic%252C%2520socioeconomic%252C%2520health%252C%2520and%2520geolocation%250Aattributes%253A%2520this%2520means%2520that%2520our%2520%2560%2560fake%2527%2527%2520people%2520live%2520in%2520realistic%2520locations%252C%250Ahave%2520realistic%2520families%252C%2520etc.%2520Such%2520data%2520can%2520be%2520used%2520for%2520a%2520variety%2520of%2520purposes%253A%250Awe%2520explore%2520one%2520such%2520use%2520case%252C%2520Agent-based%2520modelling%2520of%2520infectious%2520disease%2520in%250AIndia.%250A%2520%2520To%2520gauge%2520the%2520quality%2520of%2520our%2520synthetic%2520population%252C%2520we%2520use%2520both%2520machine%250Alearning%2520and%2520statistical%2520metrics.%2520Our%2520experimental%2520results%2520show%2520that%2520synthetic%250Apopulation%2520can%2520realistically%2520simulate%2520the%2520population%2520for%2520various%2520administrative%250Aunits%2520of%2520India%252C%2520producing%2520real-scale%252C%2520detailed%2520data%2520at%2520the%2520desired%2520level%2520of%250Azoom%2520--%2520from%2520cities%252C%2520to%2520districts%252C%2520to%2520states%252C%2520eventually%2520combining%2520to%2520form%2520a%250Acountry-scale%2520synthetic%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.12284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthpop%2B%2B%3A%20A%20Hybrid%20Framework%20for%20Generating%20A%20Country-scale%20Synthetic%0A%20%20Population&entry.906535625=Bhavesh%20Neekhra%20and%20Kshitij%20Kapoor%20and%20Debayan%20Gupta&entry.1292438233=%20%20Population%20censuses%20are%20vital%20to%20public%20policy%20decision-making.%20They%20provide%0Ainsight%20into%20human%20resources%2C%20demography%2C%20culture%2C%20and%20economic%20structure%20at%0Alocal%2C%20regional%2C%20and%20national%20levels.%20However%2C%20such%20surveys%20are%20very%20expensive%0A%28especially%20for%20low%20and%20middle-income%20countries%20with%20high%20populations%2C%20such%20as%0AIndia%29%2C%20time-consuming%2C%20and%20may%20also%20raise%20privacy%20concerns%2C%20depending%20upon%20the%0Akinds%20of%20data%20collected.%0A%20%20In%20light%20of%20these%20issues%2C%20we%20introduce%20SynthPop%2B%2B%2C%20a%20novel%20hybrid%20framework%2C%0Awhich%20can%20combine%20data%20from%20multiple%20real-world%20surveys%20%28with%20different%2C%0Apartially%20overlapping%20sets%20of%20attributes%29%20to%20produce%20a%20real-scale%20synthetic%0Apopulation%20of%20humans.%20Critically%2C%20our%20population%20maintains%20family%20structures%0Acomprising%20individuals%20with%20demographic%2C%20socioeconomic%2C%20health%2C%20and%20geolocation%0Aattributes%3A%20this%20means%20that%20our%20%60%60fake%27%27%20people%20live%20in%20realistic%20locations%2C%0Ahave%20realistic%20families%2C%20etc.%20Such%20data%20can%20be%20used%20for%20a%20variety%20of%20purposes%3A%0Awe%20explore%20one%20such%20use%20case%2C%20Agent-based%20modelling%20of%20infectious%20disease%20in%0AIndia.%0A%20%20To%20gauge%20the%20quality%20of%20our%20synthetic%20population%2C%20we%20use%20both%20machine%0Alearning%20and%20statistical%20metrics.%20Our%20experimental%20results%20show%20that%20synthetic%0Apopulation%20can%20realistically%20simulate%20the%20population%20for%20various%20administrative%0Aunits%20of%20India%2C%20producing%20real-scale%2C%20detailed%20data%20at%20the%20desired%20level%20of%0Azoom%20--%20from%20cities%2C%20to%20districts%2C%20to%20states%2C%20eventually%20combining%20to%20form%20a%0Acountry-scale%20synthetic%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12284v2&entry.124074799=Read"},
{"title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks\n  via Multi-modal Large Language Models", "author": "Xianzheng Ma and Yash Bhalgat and Brandon Smart and Shuai Chen and Xinghui Li and Jian Ding and Jindong Gu and Dave Zhenyu Chen and Songyou Peng and Jia-Wang Bian and Philip H Torr and Marc Pollefeys and Matthias Nie\u00dfner and Ian D Reid and Angel X. Chang and Iro Laina and Victor Adrian Prisacariu", "abstract": "  As large language models (LLMs) evolve, their integration with 3D spatial\ndata (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for\nunderstanding and interacting with physical spaces. This survey provides a\ncomprehensive overview of the methodologies enabling LLMs to process,\nunderstand, and generate 3D data. Highlighting the unique advantages of LLMs,\nsuch as in-context learning, step-by-step reasoning, open-vocabulary\ncapabilities, and extensive world knowledge, we underscore their potential to\nsignificantly advance spatial comprehension and interaction within embodied\nArtificial Intelligence (AI) systems. Our investigation spans various 3D data\nrepresentations, from point clouds to Neural Radiance Fields (NeRFs). It\nexamines their integration with LLMs for tasks such as 3D scene understanding,\ncaptioning, question-answering, and dialogue, as well as LLM-based agents for\nspatial reasoning, planning, and navigation. The paper also includes a brief\nreview of other methods that integrate 3D and language. The meta-analysis\npresented in this paper reveals significant progress yet underscores the\nnecessity for novel approaches to harness the full potential of 3D-LLMs. Hence,\nwith this paper, we aim to chart a course for future research that explores and\nexpands the capabilities of 3D-LLMs in understanding and interacting with the\ncomplex 3D world. To support this survey, we have established a project page\nwhere papers related to our topic are organized and listed:\nhttps://github.com/ActiveVisionLab/Awesome-LLM-3D.\n", "link": "http://arxiv.org/abs/2405.10255v1", "date": "2024-05-16", "relevancy": 1.7414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6024}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LLMs%20step%20into%20the%203D%20World%3A%20A%20Survey%20and%20Meta-Analysis%20of%203D%20Tasks%0A%20%20via%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20When%20LLMs%20step%20into%20the%203D%20World%3A%20A%20Survey%20and%20Meta-Analysis%20of%203D%20Tasks%0A%20%20via%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Xianzheng%20Ma%20and%20Yash%20Bhalgat%20and%20Brandon%20Smart%20and%20Shuai%20Chen%20and%20Xinghui%20Li%20and%20Jian%20Ding%20and%20Jindong%20Gu%20and%20Dave%20Zhenyu%20Chen%20and%20Songyou%20Peng%20and%20Jia-Wang%20Bian%20and%20Philip%20H%20Torr%20and%20Marc%20Pollefeys%20and%20Matthias%20Nie%C3%9Fner%20and%20Ian%20D%20Reid%20and%20Angel%20X.%20Chang%20and%20Iro%20Laina%20and%20Victor%20Adrian%20Prisacariu%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20evolve%2C%20their%20integration%20with%203D%20spatial%0Adata%20%283D-LLMs%29%20has%20seen%20rapid%20progress%2C%20offering%20unprecedented%20capabilities%20for%0Aunderstanding%20and%20interacting%20with%20physical%20spaces.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20the%20methodologies%20enabling%20LLMs%20to%20process%2C%0Aunderstand%2C%20and%20generate%203D%20data.%20Highlighting%20the%20unique%20advantages%20of%20LLMs%2C%0Asuch%20as%20in-context%20learning%2C%20step-by-step%20reasoning%2C%20open-vocabulary%0Acapabilities%2C%20and%20extensive%20world%20knowledge%2C%20we%20underscore%20their%20potential%20to%0Asignificantly%20advance%20spatial%20comprehension%20and%20interaction%20within%20embodied%0AArtificial%20Intelligence%20%28AI%29%20systems.%20Our%20investigation%20spans%20various%203D%20data%0Arepresentations%2C%20from%20point%20clouds%20to%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20It%0Aexamines%20their%20integration%20with%20LLMs%20for%20tasks%20such%20as%203D%20scene%20understanding%2C%0Acaptioning%2C%20question-answering%2C%20and%20dialogue%2C%20as%20well%20as%20LLM-based%20agents%20for%0Aspatial%20reasoning%2C%20planning%2C%20and%20navigation.%20The%20paper%20also%20includes%20a%20brief%0Areview%20of%20other%20methods%20that%20integrate%203D%20and%20language.%20The%20meta-analysis%0Apresented%20in%20this%20paper%20reveals%20significant%20progress%20yet%20underscores%20the%0Anecessity%20for%20novel%20approaches%20to%20harness%20the%20full%20potential%20of%203D-LLMs.%20Hence%2C%0Awith%20this%20paper%2C%20we%20aim%20to%20chart%20a%20course%20for%20future%20research%20that%20explores%20and%0Aexpands%20the%20capabilities%20of%203D-LLMs%20in%20understanding%20and%20interacting%20with%20the%0Acomplex%203D%20world.%20To%20support%20this%20survey%2C%20we%20have%20established%20a%20project%20page%0Awhere%20papers%20related%20to%20our%20topic%20are%20organized%20and%20listed%3A%0Ahttps%3A//github.com/ActiveVisionLab/Awesome-LLM-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LLMs%2520step%2520into%2520the%25203D%2520World%253A%2520A%2520Survey%2520and%2520Meta-Analysis%2520of%25203D%2520Tasks%250A%2520%2520via%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DXianzheng%2520Ma%2520and%2520Yash%2520Bhalgat%2520and%2520Brandon%2520Smart%2520and%2520Shuai%2520Chen%2520and%2520Xinghui%2520Li%2520and%2520Jian%2520Ding%2520and%2520Jindong%2520Gu%2520and%2520Dave%2520Zhenyu%2520Chen%2520and%2520Songyou%2520Peng%2520and%2520Jia-Wang%2520Bian%2520and%2520Philip%2520H%2520Torr%2520and%2520Marc%2520Pollefeys%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Ian%2520D%2520Reid%2520and%2520Angel%2520X.%2520Chang%2520and%2520Iro%2520Laina%2520and%2520Victor%2520Adrian%2520Prisacariu%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520evolve%252C%2520their%2520integration%2520with%25203D%2520spatial%250Adata%2520%25283D-LLMs%2529%2520has%2520seen%2520rapid%2520progress%252C%2520offering%2520unprecedented%2520capabilities%2520for%250Aunderstanding%2520and%2520interacting%2520with%2520physical%2520spaces.%2520This%2520survey%2520provides%2520a%250Acomprehensive%2520overview%2520of%2520the%2520methodologies%2520enabling%2520LLMs%2520to%2520process%252C%250Aunderstand%252C%2520and%2520generate%25203D%2520data.%2520Highlighting%2520the%2520unique%2520advantages%2520of%2520LLMs%252C%250Asuch%2520as%2520in-context%2520learning%252C%2520step-by-step%2520reasoning%252C%2520open-vocabulary%250Acapabilities%252C%2520and%2520extensive%2520world%2520knowledge%252C%2520we%2520underscore%2520their%2520potential%2520to%250Asignificantly%2520advance%2520spatial%2520comprehension%2520and%2520interaction%2520within%2520embodied%250AArtificial%2520Intelligence%2520%2528AI%2529%2520systems.%2520Our%2520investigation%2520spans%2520various%25203D%2520data%250Arepresentations%252C%2520from%2520point%2520clouds%2520to%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529.%2520It%250Aexamines%2520their%2520integration%2520with%2520LLMs%2520for%2520tasks%2520such%2520as%25203D%2520scene%2520understanding%252C%250Acaptioning%252C%2520question-answering%252C%2520and%2520dialogue%252C%2520as%2520well%2520as%2520LLM-based%2520agents%2520for%250Aspatial%2520reasoning%252C%2520planning%252C%2520and%2520navigation.%2520The%2520paper%2520also%2520includes%2520a%2520brief%250Areview%2520of%2520other%2520methods%2520that%2520integrate%25203D%2520and%2520language.%2520The%2520meta-analysis%250Apresented%2520in%2520this%2520paper%2520reveals%2520significant%2520progress%2520yet%2520underscores%2520the%250Anecessity%2520for%2520novel%2520approaches%2520to%2520harness%2520the%2520full%2520potential%2520of%25203D-LLMs.%2520Hence%252C%250Awith%2520this%2520paper%252C%2520we%2520aim%2520to%2520chart%2520a%2520course%2520for%2520future%2520research%2520that%2520explores%2520and%250Aexpands%2520the%2520capabilities%2520of%25203D-LLMs%2520in%2520understanding%2520and%2520interacting%2520with%2520the%250Acomplex%25203D%2520world.%2520To%2520support%2520this%2520survey%252C%2520we%2520have%2520established%2520a%2520project%2520page%250Awhere%2520papers%2520related%2520to%2520our%2520topic%2520are%2520organized%2520and%2520listed%253A%250Ahttps%253A//github.com/ActiveVisionLab/Awesome-LLM-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LLMs%20step%20into%20the%203D%20World%3A%20A%20Survey%20and%20Meta-Analysis%20of%203D%20Tasks%0A%20%20via%20Multi-modal%20Large%20Language%20Models&entry.906535625=Xianzheng%20Ma%20and%20Yash%20Bhalgat%20and%20Brandon%20Smart%20and%20Shuai%20Chen%20and%20Xinghui%20Li%20and%20Jian%20Ding%20and%20Jindong%20Gu%20and%20Dave%20Zhenyu%20Chen%20and%20Songyou%20Peng%20and%20Jia-Wang%20Bian%20and%20Philip%20H%20Torr%20and%20Marc%20Pollefeys%20and%20Matthias%20Nie%C3%9Fner%20and%20Ian%20D%20Reid%20and%20Angel%20X.%20Chang%20and%20Iro%20Laina%20and%20Victor%20Adrian%20Prisacariu&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20evolve%2C%20their%20integration%20with%203D%20spatial%0Adata%20%283D-LLMs%29%20has%20seen%20rapid%20progress%2C%20offering%20unprecedented%20capabilities%20for%0Aunderstanding%20and%20interacting%20with%20physical%20spaces.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20the%20methodologies%20enabling%20LLMs%20to%20process%2C%0Aunderstand%2C%20and%20generate%203D%20data.%20Highlighting%20the%20unique%20advantages%20of%20LLMs%2C%0Asuch%20as%20in-context%20learning%2C%20step-by-step%20reasoning%2C%20open-vocabulary%0Acapabilities%2C%20and%20extensive%20world%20knowledge%2C%20we%20underscore%20their%20potential%20to%0Asignificantly%20advance%20spatial%20comprehension%20and%20interaction%20within%20embodied%0AArtificial%20Intelligence%20%28AI%29%20systems.%20Our%20investigation%20spans%20various%203D%20data%0Arepresentations%2C%20from%20point%20clouds%20to%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20It%0Aexamines%20their%20integration%20with%20LLMs%20for%20tasks%20such%20as%203D%20scene%20understanding%2C%0Acaptioning%2C%20question-answering%2C%20and%20dialogue%2C%20as%20well%20as%20LLM-based%20agents%20for%0Aspatial%20reasoning%2C%20planning%2C%20and%20navigation.%20The%20paper%20also%20includes%20a%20brief%0Areview%20of%20other%20methods%20that%20integrate%203D%20and%20language.%20The%20meta-analysis%0Apresented%20in%20this%20paper%20reveals%20significant%20progress%20yet%20underscores%20the%0Anecessity%20for%20novel%20approaches%20to%20harness%20the%20full%20potential%20of%203D-LLMs.%20Hence%2C%0Awith%20this%20paper%2C%20we%20aim%20to%20chart%20a%20course%20for%20future%20research%20that%20explores%20and%0Aexpands%20the%20capabilities%20of%203D-LLMs%20in%20understanding%20and%20interacting%20with%20the%0Acomplex%203D%20world.%20To%20support%20this%20survey%2C%20we%20have%20established%20a%20project%20page%0Awhere%20papers%20related%20to%20our%20topic%20are%20organized%20and%20listed%3A%0Ahttps%3A//github.com/ActiveVisionLab/Awesome-LLM-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10255v1&entry.124074799=Read"},
{"title": "On Partially Unitary Learning", "author": "Mikhail Gennadievich Belov and Vladislav Gennadievich Malyshkin", "abstract": "  The problem of an optimal mapping between Hilbert spaces $IN$ of\n$\\left|\\psi\\right\\rangle$ and $OUT$ of $\\left|\\phi\\right\\rangle$ based on a set\nof wavefunction measurements (within a phase) $\\psi_l \\to \\phi_l$, $l=1\\dots\nM$, is formulated as an optimization problem maximizing the total fidelity\n$\\sum_{l=1}^{M} \\omega^{(l)}\n\\left|\\langle\\phi_l|\\mathcal{U}|\\psi_l\\rangle\\right|^2$ subject to probability\npreservation constraints on $\\mathcal{U}$ (partial unitarity). Constructed\noperator $\\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; it\nis a partially unitary rectangular matrix of the dimension $\\dim(OUT) \\times\n\\dim(IN)$ transforming operators as $A^{OUT}=\\mathcal{U} A^{IN}\n\\mathcal{U}^{\\dagger}$. An iteration algorithm finding the global maximum of\nthis optimization problem is developed and it's application to a number of\nproblems is demonstrated. A software product implementing the algorithm is\navailable from the authors.\n", "link": "http://arxiv.org/abs/2405.10263v1", "date": "2024-05-16", "relevancy": 1.7049, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Partially%20Unitary%20Learning&body=Title%3A%20On%20Partially%20Unitary%20Learning%0AAuthor%3A%20Mikhail%20Gennadievich%20Belov%20and%20Vladislav%20Gennadievich%20Malyshkin%0AAbstract%3A%20%20%20The%20problem%20of%20an%20optimal%20mapping%20between%20Hilbert%20spaces%20%24IN%24%20of%0A%24%5Cleft%7C%5Cpsi%5Cright%5Crangle%24%20and%20%24OUT%24%20of%20%24%5Cleft%7C%5Cphi%5Cright%5Crangle%24%20based%20on%20a%20set%0Aof%20wavefunction%20measurements%20%28within%20a%20phase%29%20%24%5Cpsi_l%20%5Cto%20%5Cphi_l%24%2C%20%24l%3D1%5Cdots%0AM%24%2C%20is%20formulated%20as%20an%20optimization%20problem%20maximizing%20the%20total%20fidelity%0A%24%5Csum_%7Bl%3D1%7D%5E%7BM%7D%20%5Comega%5E%7B%28l%29%7D%0A%5Cleft%7C%5Clangle%5Cphi_l%7C%5Cmathcal%7BU%7D%7C%5Cpsi_l%5Crangle%5Cright%7C%5E2%24%20subject%20to%20probability%0Apreservation%20constraints%20on%20%24%5Cmathcal%7BU%7D%24%20%28partial%20unitarity%29.%20Constructed%0Aoperator%20%24%5Cmathcal%7BU%7D%24%20can%20be%20considered%20as%20a%20%24IN%24%20to%20%24OUT%24%20quantum%20channel%3B%20it%0Ais%20a%20partially%20unitary%20rectangular%20matrix%20of%20the%20dimension%20%24%5Cdim%28OUT%29%20%5Ctimes%0A%5Cdim%28IN%29%24%20transforming%20operators%20as%20%24A%5E%7BOUT%7D%3D%5Cmathcal%7BU%7D%20A%5E%7BIN%7D%0A%5Cmathcal%7BU%7D%5E%7B%5Cdagger%7D%24.%20An%20iteration%20algorithm%20finding%20the%20global%20maximum%20of%0Athis%20optimization%20problem%20is%20developed%20and%20it%27s%20application%20to%20a%20number%20of%0Aproblems%20is%20demonstrated.%20A%20software%20product%20implementing%20the%20algorithm%20is%0Aavailable%20from%20the%20authors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Partially%2520Unitary%2520Learning%26entry.906535625%3DMikhail%2520Gennadievich%2520Belov%2520and%2520Vladislav%2520Gennadievich%2520Malyshkin%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520an%2520optimal%2520mapping%2520between%2520Hilbert%2520spaces%2520%2524IN%2524%2520of%250A%2524%255Cleft%257C%255Cpsi%255Cright%255Crangle%2524%2520and%2520%2524OUT%2524%2520of%2520%2524%255Cleft%257C%255Cphi%255Cright%255Crangle%2524%2520based%2520on%2520a%2520set%250Aof%2520wavefunction%2520measurements%2520%2528within%2520a%2520phase%2529%2520%2524%255Cpsi_l%2520%255Cto%2520%255Cphi_l%2524%252C%2520%2524l%253D1%255Cdots%250AM%2524%252C%2520is%2520formulated%2520as%2520an%2520optimization%2520problem%2520maximizing%2520the%2520total%2520fidelity%250A%2524%255Csum_%257Bl%253D1%257D%255E%257BM%257D%2520%255Comega%255E%257B%2528l%2529%257D%250A%255Cleft%257C%255Clangle%255Cphi_l%257C%255Cmathcal%257BU%257D%257C%255Cpsi_l%255Crangle%255Cright%257C%255E2%2524%2520subject%2520to%2520probability%250Apreservation%2520constraints%2520on%2520%2524%255Cmathcal%257BU%257D%2524%2520%2528partial%2520unitarity%2529.%2520Constructed%250Aoperator%2520%2524%255Cmathcal%257BU%257D%2524%2520can%2520be%2520considered%2520as%2520a%2520%2524IN%2524%2520to%2520%2524OUT%2524%2520quantum%2520channel%253B%2520it%250Ais%2520a%2520partially%2520unitary%2520rectangular%2520matrix%2520of%2520the%2520dimension%2520%2524%255Cdim%2528OUT%2529%2520%255Ctimes%250A%255Cdim%2528IN%2529%2524%2520transforming%2520operators%2520as%2520%2524A%255E%257BOUT%257D%253D%255Cmathcal%257BU%257D%2520A%255E%257BIN%257D%250A%255Cmathcal%257BU%257D%255E%257B%255Cdagger%257D%2524.%2520An%2520iteration%2520algorithm%2520finding%2520the%2520global%2520maximum%2520of%250Athis%2520optimization%2520problem%2520is%2520developed%2520and%2520it%2527s%2520application%2520to%2520a%2520number%2520of%250Aproblems%2520is%2520demonstrated.%2520A%2520software%2520product%2520implementing%2520the%2520algorithm%2520is%250Aavailable%2520from%2520the%2520authors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Partially%20Unitary%20Learning&entry.906535625=Mikhail%20Gennadievich%20Belov%20and%20Vladislav%20Gennadievich%20Malyshkin&entry.1292438233=%20%20The%20problem%20of%20an%20optimal%20mapping%20between%20Hilbert%20spaces%20%24IN%24%20of%0A%24%5Cleft%7C%5Cpsi%5Cright%5Crangle%24%20and%20%24OUT%24%20of%20%24%5Cleft%7C%5Cphi%5Cright%5Crangle%24%20based%20on%20a%20set%0Aof%20wavefunction%20measurements%20%28within%20a%20phase%29%20%24%5Cpsi_l%20%5Cto%20%5Cphi_l%24%2C%20%24l%3D1%5Cdots%0AM%24%2C%20is%20formulated%20as%20an%20optimization%20problem%20maximizing%20the%20total%20fidelity%0A%24%5Csum_%7Bl%3D1%7D%5E%7BM%7D%20%5Comega%5E%7B%28l%29%7D%0A%5Cleft%7C%5Clangle%5Cphi_l%7C%5Cmathcal%7BU%7D%7C%5Cpsi_l%5Crangle%5Cright%7C%5E2%24%20subject%20to%20probability%0Apreservation%20constraints%20on%20%24%5Cmathcal%7BU%7D%24%20%28partial%20unitarity%29.%20Constructed%0Aoperator%20%24%5Cmathcal%7BU%7D%24%20can%20be%20considered%20as%20a%20%24IN%24%20to%20%24OUT%24%20quantum%20channel%3B%20it%0Ais%20a%20partially%20unitary%20rectangular%20matrix%20of%20the%20dimension%20%24%5Cdim%28OUT%29%20%5Ctimes%0A%5Cdim%28IN%29%24%20transforming%20operators%20as%20%24A%5E%7BOUT%7D%3D%5Cmathcal%7BU%7D%20A%5E%7BIN%7D%0A%5Cmathcal%7BU%7D%5E%7B%5Cdagger%7D%24.%20An%20iteration%20algorithm%20finding%20the%20global%20maximum%20of%0Athis%20optimization%20problem%20is%20developed%20and%20it%27s%20application%20to%20a%20number%20of%0Aproblems%20is%20demonstrated.%20A%20software%20product%20implementing%20the%20algorithm%20is%0Aavailable%20from%20the%20authors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10263v1&entry.124074799=Read"},
{"title": "Generating Coherent Sequences of Visual Illustrations for Real-World\n  Manual Tasks", "author": "Jo\u00e3o Bordalo and Vasco Ramos and Rodrigo Val\u00e9rio and Diogo Gl\u00f3ria-Silva and Yonatan Bitton and Michal Yarom and Idan Szpektor and Joao Magalhaes", "abstract": "  Multistep instructions, such as recipes and how-to guides, greatly benefit\nfrom visual aids, such as a series of images that accompany the instruction\nsteps. While Large Language Models (LLMs) have become adept at generating\ncoherent textual steps, Large Vision/Language Models (LVLMs) are less capable\nof generating accompanying image sequences. The most challenging aspect is that\neach generated image needs to adhere to the relevant textual step instruction,\nas well as be visually consistent with earlier images in the sequence. To\naddress this problem, we propose an approach for generating consistent image\nsequences, which integrates a Latent Diffusion Model (LDM) with an LLM to\ntransform the sequence into a caption to maintain the semantic coherence of the\nsequence. In addition, to maintain the visual coherence of the image sequence,\nwe introduce a copy mechanism to initialise reverse diffusion processes with a\nlatent vector iteration from a previously generated image from a relevant step.\nBoth strategies will condition the reverse diffusion process on the sequence of\ninstruction steps and tie the contents of the current image to previous\ninstruction steps and corresponding images. Experiments show that the proposed\napproach is preferred by humans in 46.6% of the cases against 26.6% for the\nsecond best method. In addition, automatic metrics showed that the proposed\nmethod maintains semantic coherence and visual consistency across steps in both\ndomains.\n", "link": "http://arxiv.org/abs/2405.10122v1", "date": "2024-05-16", "relevancy": 1.7043, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5832}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5642}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Coherent%20Sequences%20of%20Visual%20Illustrations%20for%20Real-World%0A%20%20Manual%20Tasks&body=Title%3A%20Generating%20Coherent%20Sequences%20of%20Visual%20Illustrations%20for%20Real-World%0A%20%20Manual%20Tasks%0AAuthor%3A%20Jo%C3%A3o%20Bordalo%20and%20Vasco%20Ramos%20and%20Rodrigo%20Val%C3%A9rio%20and%20Diogo%20Gl%C3%B3ria-Silva%20and%20Yonatan%20Bitton%20and%20Michal%20Yarom%20and%20Idan%20Szpektor%20and%20Joao%20Magalhaes%0AAbstract%3A%20%20%20Multistep%20instructions%2C%20such%20as%20recipes%20and%20how-to%20guides%2C%20greatly%20benefit%0Afrom%20visual%20aids%2C%20such%20as%20a%20series%20of%20images%20that%20accompany%20the%20instruction%0Asteps.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20adept%20at%20generating%0Acoherent%20textual%20steps%2C%20Large%20Vision/Language%20Models%20%28LVLMs%29%20are%20less%20capable%0Aof%20generating%20accompanying%20image%20sequences.%20The%20most%20challenging%20aspect%20is%20that%0Aeach%20generated%20image%20needs%20to%20adhere%20to%20the%20relevant%20textual%20step%20instruction%2C%0Aas%20well%20as%20be%20visually%20consistent%20with%20earlier%20images%20in%20the%20sequence.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20an%20approach%20for%20generating%20consistent%20image%0Asequences%2C%20which%20integrates%20a%20Latent%20Diffusion%20Model%20%28LDM%29%20with%20an%20LLM%20to%0Atransform%20the%20sequence%20into%20a%20caption%20to%20maintain%20the%20semantic%20coherence%20of%20the%0Asequence.%20In%20addition%2C%20to%20maintain%20the%20visual%20coherence%20of%20the%20image%20sequence%2C%0Awe%20introduce%20a%20copy%20mechanism%20to%20initialise%20reverse%20diffusion%20processes%20with%20a%0Alatent%20vector%20iteration%20from%20a%20previously%20generated%20image%20from%20a%20relevant%20step.%0ABoth%20strategies%20will%20condition%20the%20reverse%20diffusion%20process%20on%20the%20sequence%20of%0Ainstruction%20steps%20and%20tie%20the%20contents%20of%20the%20current%20image%20to%20previous%0Ainstruction%20steps%20and%20corresponding%20images.%20Experiments%20show%20that%20the%20proposed%0Aapproach%20is%20preferred%20by%20humans%20in%2046.6%25%20of%20the%20cases%20against%2026.6%25%20for%20the%0Asecond%20best%20method.%20In%20addition%2C%20automatic%20metrics%20showed%20that%20the%20proposed%0Amethod%20maintains%20semantic%20coherence%20and%20visual%20consistency%20across%20steps%20in%20both%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Coherent%2520Sequences%2520of%2520Visual%2520Illustrations%2520for%2520Real-World%250A%2520%2520Manual%2520Tasks%26entry.906535625%3DJo%25C3%25A3o%2520Bordalo%2520and%2520Vasco%2520Ramos%2520and%2520Rodrigo%2520Val%25C3%25A9rio%2520and%2520Diogo%2520Gl%25C3%25B3ria-Silva%2520and%2520Yonatan%2520Bitton%2520and%2520Michal%2520Yarom%2520and%2520Idan%2520Szpektor%2520and%2520Joao%2520Magalhaes%26entry.1292438233%3D%2520%2520Multistep%2520instructions%252C%2520such%2520as%2520recipes%2520and%2520how-to%2520guides%252C%2520greatly%2520benefit%250Afrom%2520visual%2520aids%252C%2520such%2520as%2520a%2520series%2520of%2520images%2520that%2520accompany%2520the%2520instruction%250Asteps.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520adept%2520at%2520generating%250Acoherent%2520textual%2520steps%252C%2520Large%2520Vision/Language%2520Models%2520%2528LVLMs%2529%2520are%2520less%2520capable%250Aof%2520generating%2520accompanying%2520image%2520sequences.%2520The%2520most%2520challenging%2520aspect%2520is%2520that%250Aeach%2520generated%2520image%2520needs%2520to%2520adhere%2520to%2520the%2520relevant%2520textual%2520step%2520instruction%252C%250Aas%2520well%2520as%2520be%2520visually%2520consistent%2520with%2520earlier%2520images%2520in%2520the%2520sequence.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520an%2520approach%2520for%2520generating%2520consistent%2520image%250Asequences%252C%2520which%2520integrates%2520a%2520Latent%2520Diffusion%2520Model%2520%2528LDM%2529%2520with%2520an%2520LLM%2520to%250Atransform%2520the%2520sequence%2520into%2520a%2520caption%2520to%2520maintain%2520the%2520semantic%2520coherence%2520of%2520the%250Asequence.%2520In%2520addition%252C%2520to%2520maintain%2520the%2520visual%2520coherence%2520of%2520the%2520image%2520sequence%252C%250Awe%2520introduce%2520a%2520copy%2520mechanism%2520to%2520initialise%2520reverse%2520diffusion%2520processes%2520with%2520a%250Alatent%2520vector%2520iteration%2520from%2520a%2520previously%2520generated%2520image%2520from%2520a%2520relevant%2520step.%250ABoth%2520strategies%2520will%2520condition%2520the%2520reverse%2520diffusion%2520process%2520on%2520the%2520sequence%2520of%250Ainstruction%2520steps%2520and%2520tie%2520the%2520contents%2520of%2520the%2520current%2520image%2520to%2520previous%250Ainstruction%2520steps%2520and%2520corresponding%2520images.%2520Experiments%2520show%2520that%2520the%2520proposed%250Aapproach%2520is%2520preferred%2520by%2520humans%2520in%252046.6%2525%2520of%2520the%2520cases%2520against%252026.6%2525%2520for%2520the%250Asecond%2520best%2520method.%2520In%2520addition%252C%2520automatic%2520metrics%2520showed%2520that%2520the%2520proposed%250Amethod%2520maintains%2520semantic%2520coherence%2520and%2520visual%2520consistency%2520across%2520steps%2520in%2520both%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Coherent%20Sequences%20of%20Visual%20Illustrations%20for%20Real-World%0A%20%20Manual%20Tasks&entry.906535625=Jo%C3%A3o%20Bordalo%20and%20Vasco%20Ramos%20and%20Rodrigo%20Val%C3%A9rio%20and%20Diogo%20Gl%C3%B3ria-Silva%20and%20Yonatan%20Bitton%20and%20Michal%20Yarom%20and%20Idan%20Szpektor%20and%20Joao%20Magalhaes&entry.1292438233=%20%20Multistep%20instructions%2C%20such%20as%20recipes%20and%20how-to%20guides%2C%20greatly%20benefit%0Afrom%20visual%20aids%2C%20such%20as%20a%20series%20of%20images%20that%20accompany%20the%20instruction%0Asteps.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20adept%20at%20generating%0Acoherent%20textual%20steps%2C%20Large%20Vision/Language%20Models%20%28LVLMs%29%20are%20less%20capable%0Aof%20generating%20accompanying%20image%20sequences.%20The%20most%20challenging%20aspect%20is%20that%0Aeach%20generated%20image%20needs%20to%20adhere%20to%20the%20relevant%20textual%20step%20instruction%2C%0Aas%20well%20as%20be%20visually%20consistent%20with%20earlier%20images%20in%20the%20sequence.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20an%20approach%20for%20generating%20consistent%20image%0Asequences%2C%20which%20integrates%20a%20Latent%20Diffusion%20Model%20%28LDM%29%20with%20an%20LLM%20to%0Atransform%20the%20sequence%20into%20a%20caption%20to%20maintain%20the%20semantic%20coherence%20of%20the%0Asequence.%20In%20addition%2C%20to%20maintain%20the%20visual%20coherence%20of%20the%20image%20sequence%2C%0Awe%20introduce%20a%20copy%20mechanism%20to%20initialise%20reverse%20diffusion%20processes%20with%20a%0Alatent%20vector%20iteration%20from%20a%20previously%20generated%20image%20from%20a%20relevant%20step.%0ABoth%20strategies%20will%20condition%20the%20reverse%20diffusion%20process%20on%20the%20sequence%20of%0Ainstruction%20steps%20and%20tie%20the%20contents%20of%20the%20current%20image%20to%20previous%0Ainstruction%20steps%20and%20corresponding%20images.%20Experiments%20show%20that%20the%20proposed%0Aapproach%20is%20preferred%20by%20humans%20in%2046.6%25%20of%20the%20cases%20against%2026.6%25%20for%20the%0Asecond%20best%20method.%20In%20addition%2C%20automatic%20metrics%20showed%20that%20the%20proposed%0Amethod%20maintains%20semantic%20coherence%20and%20visual%20consistency%20across%20steps%20in%20both%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10122v1&entry.124074799=Read"},
{"title": "Analogist: Out-of-the-box Visual In-Context Learning with Image\n  Diffusion Model", "author": "Zheng Gu and Shiyuan Yang and Jing Liao and Jing Huo and Yang Gao", "abstract": "  Visual In-Context Learning (ICL) has emerged as a promising research area due\nto its capability to accomplish various tasks with limited example pairs\nthrough analogical reasoning. However, training-based visual ICL has\nlimitations in its ability to generalize to unseen tasks and requires the\ncollection of a diverse task dataset. On the other hand, existing methods in\nthe inference-based visual ICL category solely rely on textual prompts, which\nfail to capture fine-grained contextual information from given examples and can\nbe time-consuming when converting from images to text prompts. To address these\nchallenges, we propose Analogist, a novel inference-based visual ICL approach\nthat exploits both visual and textual prompting techniques using a\ntext-to-image diffusion model pretrained for image inpainting. For visual\nprompting, we propose a self-attention cloning (SAC) method to guide the\nfine-grained structural-level analogy between image examples. For textual\nprompting, we leverage GPT-4V's visual reasoning capability to efficiently\ngenerate text prompts and introduce a cross-attention masking (CAM) operation\nto enhance the accuracy of semantic-level analogy guided by text prompts. Our\nmethod is out-of-the-box and does not require fine-tuning or optimization. It\nis also generic and flexible, enabling a wide range of visual tasks to be\nperformed in an in-context manner. Extensive experiments demonstrate the\nsuperiority of our method over existing approaches, both qualitatively and\nquantitatively.\n", "link": "http://arxiv.org/abs/2405.10316v1", "date": "2024-05-16", "relevancy": 1.6804, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5787}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.563}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analogist%3A%20Out-of-the-box%20Visual%20In-Context%20Learning%20with%20Image%0A%20%20Diffusion%20Model&body=Title%3A%20Analogist%3A%20Out-of-the-box%20Visual%20In-Context%20Learning%20with%20Image%0A%20%20Diffusion%20Model%0AAuthor%3A%20Zheng%20Gu%20and%20Shiyuan%20Yang%20and%20Jing%20Liao%20and%20Jing%20Huo%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Visual%20In-Context%20Learning%20%28ICL%29%20has%20emerged%20as%20a%20promising%20research%20area%20due%0Ato%20its%20capability%20to%20accomplish%20various%20tasks%20with%20limited%20example%20pairs%0Athrough%20analogical%20reasoning.%20However%2C%20training-based%20visual%20ICL%20has%0Alimitations%20in%20its%20ability%20to%20generalize%20to%20unseen%20tasks%20and%20requires%20the%0Acollection%20of%20a%20diverse%20task%20dataset.%20On%20the%20other%20hand%2C%20existing%20methods%20in%0Athe%20inference-based%20visual%20ICL%20category%20solely%20rely%20on%20textual%20prompts%2C%20which%0Afail%20to%20capture%20fine-grained%20contextual%20information%20from%20given%20examples%20and%20can%0Abe%20time-consuming%20when%20converting%20from%20images%20to%20text%20prompts.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Analogist%2C%20a%20novel%20inference-based%20visual%20ICL%20approach%0Athat%20exploits%20both%20visual%20and%20textual%20prompting%20techniques%20using%20a%0Atext-to-image%20diffusion%20model%20pretrained%20for%20image%20inpainting.%20For%20visual%0Aprompting%2C%20we%20propose%20a%20self-attention%20cloning%20%28SAC%29%20method%20to%20guide%20the%0Afine-grained%20structural-level%20analogy%20between%20image%20examples.%20For%20textual%0Aprompting%2C%20we%20leverage%20GPT-4V%27s%20visual%20reasoning%20capability%20to%20efficiently%0Agenerate%20text%20prompts%20and%20introduce%20a%20cross-attention%20masking%20%28CAM%29%20operation%0Ato%20enhance%20the%20accuracy%20of%20semantic-level%20analogy%20guided%20by%20text%20prompts.%20Our%0Amethod%20is%20out-of-the-box%20and%20does%20not%20require%20fine-tuning%20or%20optimization.%20It%0Ais%20also%20generic%20and%20flexible%2C%20enabling%20a%20wide%20range%20of%20visual%20tasks%20to%20be%0Aperformed%20in%20an%20in-context%20manner.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20existing%20approaches%2C%20both%20qualitatively%20and%0Aquantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalogist%253A%2520Out-of-the-box%2520Visual%2520In-Context%2520Learning%2520with%2520Image%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DZheng%2520Gu%2520and%2520Shiyuan%2520Yang%2520and%2520Jing%2520Liao%2520and%2520Jing%2520Huo%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Visual%2520In-Context%2520Learning%2520%2528ICL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520research%2520area%2520due%250Ato%2520its%2520capability%2520to%2520accomplish%2520various%2520tasks%2520with%2520limited%2520example%2520pairs%250Athrough%2520analogical%2520reasoning.%2520However%252C%2520training-based%2520visual%2520ICL%2520has%250Alimitations%2520in%2520its%2520ability%2520to%2520generalize%2520to%2520unseen%2520tasks%2520and%2520requires%2520the%250Acollection%2520of%2520a%2520diverse%2520task%2520dataset.%2520On%2520the%2520other%2520hand%252C%2520existing%2520methods%2520in%250Athe%2520inference-based%2520visual%2520ICL%2520category%2520solely%2520rely%2520on%2520textual%2520prompts%252C%2520which%250Afail%2520to%2520capture%2520fine-grained%2520contextual%2520information%2520from%2520given%2520examples%2520and%2520can%250Abe%2520time-consuming%2520when%2520converting%2520from%2520images%2520to%2520text%2520prompts.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520Analogist%252C%2520a%2520novel%2520inference-based%2520visual%2520ICL%2520approach%250Athat%2520exploits%2520both%2520visual%2520and%2520textual%2520prompting%2520techniques%2520using%2520a%250Atext-to-image%2520diffusion%2520model%2520pretrained%2520for%2520image%2520inpainting.%2520For%2520visual%250Aprompting%252C%2520we%2520propose%2520a%2520self-attention%2520cloning%2520%2528SAC%2529%2520method%2520to%2520guide%2520the%250Afine-grained%2520structural-level%2520analogy%2520between%2520image%2520examples.%2520For%2520textual%250Aprompting%252C%2520we%2520leverage%2520GPT-4V%2527s%2520visual%2520reasoning%2520capability%2520to%2520efficiently%250Agenerate%2520text%2520prompts%2520and%2520introduce%2520a%2520cross-attention%2520masking%2520%2528CAM%2529%2520operation%250Ato%2520enhance%2520the%2520accuracy%2520of%2520semantic-level%2520analogy%2520guided%2520by%2520text%2520prompts.%2520Our%250Amethod%2520is%2520out-of-the-box%2520and%2520does%2520not%2520require%2520fine-tuning%2520or%2520optimization.%2520It%250Ais%2520also%2520generic%2520and%2520flexible%252C%2520enabling%2520a%2520wide%2520range%2520of%2520visual%2520tasks%2520to%2520be%250Aperformed%2520in%2520an%2520in-context%2520manner.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520over%2520existing%2520approaches%252C%2520both%2520qualitatively%2520and%250Aquantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analogist%3A%20Out-of-the-box%20Visual%20In-Context%20Learning%20with%20Image%0A%20%20Diffusion%20Model&entry.906535625=Zheng%20Gu%20and%20Shiyuan%20Yang%20and%20Jing%20Liao%20and%20Jing%20Huo%20and%20Yang%20Gao&entry.1292438233=%20%20Visual%20In-Context%20Learning%20%28ICL%29%20has%20emerged%20as%20a%20promising%20research%20area%20due%0Ato%20its%20capability%20to%20accomplish%20various%20tasks%20with%20limited%20example%20pairs%0Athrough%20analogical%20reasoning.%20However%2C%20training-based%20visual%20ICL%20has%0Alimitations%20in%20its%20ability%20to%20generalize%20to%20unseen%20tasks%20and%20requires%20the%0Acollection%20of%20a%20diverse%20task%20dataset.%20On%20the%20other%20hand%2C%20existing%20methods%20in%0Athe%20inference-based%20visual%20ICL%20category%20solely%20rely%20on%20textual%20prompts%2C%20which%0Afail%20to%20capture%20fine-grained%20contextual%20information%20from%20given%20examples%20and%20can%0Abe%20time-consuming%20when%20converting%20from%20images%20to%20text%20prompts.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Analogist%2C%20a%20novel%20inference-based%20visual%20ICL%20approach%0Athat%20exploits%20both%20visual%20and%20textual%20prompting%20techniques%20using%20a%0Atext-to-image%20diffusion%20model%20pretrained%20for%20image%20inpainting.%20For%20visual%0Aprompting%2C%20we%20propose%20a%20self-attention%20cloning%20%28SAC%29%20method%20to%20guide%20the%0Afine-grained%20structural-level%20analogy%20between%20image%20examples.%20For%20textual%0Aprompting%2C%20we%20leverage%20GPT-4V%27s%20visual%20reasoning%20capability%20to%20efficiently%0Agenerate%20text%20prompts%20and%20introduce%20a%20cross-attention%20masking%20%28CAM%29%20operation%0Ato%20enhance%20the%20accuracy%20of%20semantic-level%20analogy%20guided%20by%20text%20prompts.%20Our%0Amethod%20is%20out-of-the-box%20and%20does%20not%20require%20fine-tuning%20or%20optimization.%20It%0Ais%20also%20generic%20and%20flexible%2C%20enabling%20a%20wide%20range%20of%20visual%20tasks%20to%20be%0Aperformed%20in%20an%20in-context%20manner.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20existing%20approaches%2C%20both%20qualitatively%20and%0Aquantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10316v1&entry.124074799=Read"},
{"title": "Towards Consistent and Explainable Motion Prediction using Heterogeneous\n  Graph Attention", "author": "Tobias Demmler and Andreas Tamke and Thao Dang and Karsten Haug and Lars Mikelsons", "abstract": "  In autonomous driving, accurately interpreting the movements of other road\nusers and leveraging this knowledge to forecast future trajectories is crucial.\nThis is typically achieved through the integration of map data and tracked\ntrajectories of various agents. Numerous methodologies combine this information\ninto a singular embedding for each agent, which is then utilized to predict\nfuture behavior. However, these approaches have a notable drawback in that they\nmay lose exact location information during the encoding process. The encoding\nstill includes general map information. However, the generation of valid and\nconsistent trajectories is not guaranteed. This can cause the predicted\ntrajectories to stray from the actual lanes. This paper introduces a new\nrefinement module designed to project the predicted trajectories back onto the\nactual map, rectifying these discrepancies and leading towards more consistent\npredictions. This versatile module can be readily incorporated into a wide\nrange of architectures. Additionally, we propose a novel scene encoder that\nhandles all relations between agents and their environment in a single unified\nheterogeneous graph attention network. By analyzing the attention values on the\ndifferent edges in this graph, we can gain unique insights into the neural\nnetwork's inner workings leading towards a more explainable prediction.\n", "link": "http://arxiv.org/abs/2405.10134v1", "date": "2024-05-16", "relevancy": 1.6676, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5997}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Consistent%20and%20Explainable%20Motion%20Prediction%20using%20Heterogeneous%0A%20%20Graph%20Attention&body=Title%3A%20Towards%20Consistent%20and%20Explainable%20Motion%20Prediction%20using%20Heterogeneous%0A%20%20Graph%20Attention%0AAuthor%3A%20Tobias%20Demmler%20and%20Andreas%20Tamke%20and%20Thao%20Dang%20and%20Karsten%20Haug%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20accurately%20interpreting%20the%20movements%20of%20other%20road%0Ausers%20and%20leveraging%20this%20knowledge%20to%20forecast%20future%20trajectories%20is%20crucial.%0AThis%20is%20typically%20achieved%20through%20the%20integration%20of%20map%20data%20and%20tracked%0Atrajectories%20of%20various%20agents.%20Numerous%20methodologies%20combine%20this%20information%0Ainto%20a%20singular%20embedding%20for%20each%20agent%2C%20which%20is%20then%20utilized%20to%20predict%0Afuture%20behavior.%20However%2C%20these%20approaches%20have%20a%20notable%20drawback%20in%20that%20they%0Amay%20lose%20exact%20location%20information%20during%20the%20encoding%20process.%20The%20encoding%0Astill%20includes%20general%20map%20information.%20However%2C%20the%20generation%20of%20valid%20and%0Aconsistent%20trajectories%20is%20not%20guaranteed.%20This%20can%20cause%20the%20predicted%0Atrajectories%20to%20stray%20from%20the%20actual%20lanes.%20This%20paper%20introduces%20a%20new%0Arefinement%20module%20designed%20to%20project%20the%20predicted%20trajectories%20back%20onto%20the%0Aactual%20map%2C%20rectifying%20these%20discrepancies%20and%20leading%20towards%20more%20consistent%0Apredictions.%20This%20versatile%20module%20can%20be%20readily%20incorporated%20into%20a%20wide%0Arange%20of%20architectures.%20Additionally%2C%20we%20propose%20a%20novel%20scene%20encoder%20that%0Ahandles%20all%20relations%20between%20agents%20and%20their%20environment%20in%20a%20single%20unified%0Aheterogeneous%20graph%20attention%20network.%20By%20analyzing%20the%20attention%20values%20on%20the%0Adifferent%20edges%20in%20this%20graph%2C%20we%20can%20gain%20unique%20insights%20into%20the%20neural%0Anetwork%27s%20inner%20workings%20leading%20towards%20a%20more%20explainable%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Consistent%2520and%2520Explainable%2520Motion%2520Prediction%2520using%2520Heterogeneous%250A%2520%2520Graph%2520Attention%26entry.906535625%3DTobias%2520Demmler%2520and%2520Andreas%2520Tamke%2520and%2520Thao%2520Dang%2520and%2520Karsten%2520Haug%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520accurately%2520interpreting%2520the%2520movements%2520of%2520other%2520road%250Ausers%2520and%2520leveraging%2520this%2520knowledge%2520to%2520forecast%2520future%2520trajectories%2520is%2520crucial.%250AThis%2520is%2520typically%2520achieved%2520through%2520the%2520integration%2520of%2520map%2520data%2520and%2520tracked%250Atrajectories%2520of%2520various%2520agents.%2520Numerous%2520methodologies%2520combine%2520this%2520information%250Ainto%2520a%2520singular%2520embedding%2520for%2520each%2520agent%252C%2520which%2520is%2520then%2520utilized%2520to%2520predict%250Afuture%2520behavior.%2520However%252C%2520these%2520approaches%2520have%2520a%2520notable%2520drawback%2520in%2520that%2520they%250Amay%2520lose%2520exact%2520location%2520information%2520during%2520the%2520encoding%2520process.%2520The%2520encoding%250Astill%2520includes%2520general%2520map%2520information.%2520However%252C%2520the%2520generation%2520of%2520valid%2520and%250Aconsistent%2520trajectories%2520is%2520not%2520guaranteed.%2520This%2520can%2520cause%2520the%2520predicted%250Atrajectories%2520to%2520stray%2520from%2520the%2520actual%2520lanes.%2520This%2520paper%2520introduces%2520a%2520new%250Arefinement%2520module%2520designed%2520to%2520project%2520the%2520predicted%2520trajectories%2520back%2520onto%2520the%250Aactual%2520map%252C%2520rectifying%2520these%2520discrepancies%2520and%2520leading%2520towards%2520more%2520consistent%250Apredictions.%2520This%2520versatile%2520module%2520can%2520be%2520readily%2520incorporated%2520into%2520a%2520wide%250Arange%2520of%2520architectures.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520scene%2520encoder%2520that%250Ahandles%2520all%2520relations%2520between%2520agents%2520and%2520their%2520environment%2520in%2520a%2520single%2520unified%250Aheterogeneous%2520graph%2520attention%2520network.%2520By%2520analyzing%2520the%2520attention%2520values%2520on%2520the%250Adifferent%2520edges%2520in%2520this%2520graph%252C%2520we%2520can%2520gain%2520unique%2520insights%2520into%2520the%2520neural%250Anetwork%2527s%2520inner%2520workings%2520leading%2520towards%2520a%2520more%2520explainable%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Consistent%20and%20Explainable%20Motion%20Prediction%20using%20Heterogeneous%0A%20%20Graph%20Attention&entry.906535625=Tobias%20Demmler%20and%20Andreas%20Tamke%20and%20Thao%20Dang%20and%20Karsten%20Haug%20and%20Lars%20Mikelsons&entry.1292438233=%20%20In%20autonomous%20driving%2C%20accurately%20interpreting%20the%20movements%20of%20other%20road%0Ausers%20and%20leveraging%20this%20knowledge%20to%20forecast%20future%20trajectories%20is%20crucial.%0AThis%20is%20typically%20achieved%20through%20the%20integration%20of%20map%20data%20and%20tracked%0Atrajectories%20of%20various%20agents.%20Numerous%20methodologies%20combine%20this%20information%0Ainto%20a%20singular%20embedding%20for%20each%20agent%2C%20which%20is%20then%20utilized%20to%20predict%0Afuture%20behavior.%20However%2C%20these%20approaches%20have%20a%20notable%20drawback%20in%20that%20they%0Amay%20lose%20exact%20location%20information%20during%20the%20encoding%20process.%20The%20encoding%0Astill%20includes%20general%20map%20information.%20However%2C%20the%20generation%20of%20valid%20and%0Aconsistent%20trajectories%20is%20not%20guaranteed.%20This%20can%20cause%20the%20predicted%0Atrajectories%20to%20stray%20from%20the%20actual%20lanes.%20This%20paper%20introduces%20a%20new%0Arefinement%20module%20designed%20to%20project%20the%20predicted%20trajectories%20back%20onto%20the%0Aactual%20map%2C%20rectifying%20these%20discrepancies%20and%20leading%20towards%20more%20consistent%0Apredictions.%20This%20versatile%20module%20can%20be%20readily%20incorporated%20into%20a%20wide%0Arange%20of%20architectures.%20Additionally%2C%20we%20propose%20a%20novel%20scene%20encoder%20that%0Ahandles%20all%20relations%20between%20agents%20and%20their%20environment%20in%20a%20single%20unified%0Aheterogeneous%20graph%20attention%20network.%20By%20analyzing%20the%20attention%20values%20on%20the%0Adifferent%20edges%20in%20this%20graph%2C%20we%20can%20gain%20unique%20insights%20into%20the%20neural%0Anetwork%27s%20inner%20workings%20leading%20towards%20a%20more%20explainable%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10134v1&entry.124074799=Read"},
{"title": "AI-Cybersecurity Education Through Designing AI-based Cyberharassment\n  Detection Lab", "author": "Ebuka Okpala and Nishant Vishwamitra and Keyan Guo and Song Liao and Long Cheng and Hongxin Hu and Yongkai Wu and Xiaohong Yuan and Jeannette Wade and Sajad Khorsandroo", "abstract": "  Cyberharassment is a critical, socially relevant cybersecurity problem\nbecause of the adverse effects it can have on targeted groups or individuals.\nWhile progress has been made in understanding cyber-harassment, its detection,\nattacks on artificial intelligence (AI) based cyberharassment systems, and the\nsocial problems in cyberharassment detectors, little has been done in designing\nexperiential learning educational materials that engage students in this\nemerging social cybersecurity in the era of AI. Experiential learning\nopportunities are usually provided through capstone projects and engineering\ndesign courses in STEM programs such as computer science. While capstone\nprojects are an excellent example of experiential learning, given the\ninterdisciplinary nature of this emerging social cybersecurity problem, it can\nbe challenging to use them to engage non-computing students without prior\nknowledge of AI. Because of this, we were motivated to develop a hands-on lab\nplatform that provided experiential learning experiences to non-computing\nstudents with little or no background knowledge in AI and discussed the lessons\nlearned in developing this lab. In this lab used by social science students at\nNorth Carolina A&T State University across two semesters (spring and fall) in\n2022, students are given a detailed lab manual and are to complete a set of\nwell-detailed tasks. Through this process, students learn AI concepts and the\napplication of AI for cyberharassment detection. Using pre- and post-surveys,\nwe asked students to rate their knowledge or skills in AI and their\nunderstanding of the concepts learned. The results revealed that the students\nmoderately understood the concepts of AI and cyberharassment.\n", "link": "http://arxiv.org/abs/2405.08125v2", "date": "2024-05-16", "relevancy": 1.6611, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4253}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4236}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Cybersecurity%20Education%20Through%20Designing%20AI-based%20Cyberharassment%0A%20%20Detection%20Lab&body=Title%3A%20AI-Cybersecurity%20Education%20Through%20Designing%20AI-based%20Cyberharassment%0A%20%20Detection%20Lab%0AAuthor%3A%20Ebuka%20Okpala%20and%20Nishant%20Vishwamitra%20and%20Keyan%20Guo%20and%20Song%20Liao%20and%20Long%20Cheng%20and%20Hongxin%20Hu%20and%20Yongkai%20Wu%20and%20Xiaohong%20Yuan%20and%20Jeannette%20Wade%20and%20Sajad%20Khorsandroo%0AAbstract%3A%20%20%20Cyberharassment%20is%20a%20critical%2C%20socially%20relevant%20cybersecurity%20problem%0Abecause%20of%20the%20adverse%20effects%20it%20can%20have%20on%20targeted%20groups%20or%20individuals.%0AWhile%20progress%20has%20been%20made%20in%20understanding%20cyber-harassment%2C%20its%20detection%2C%0Aattacks%20on%20artificial%20intelligence%20%28AI%29%20based%20cyberharassment%20systems%2C%20and%20the%0Asocial%20problems%20in%20cyberharassment%20detectors%2C%20little%20has%20been%20done%20in%20designing%0Aexperiential%20learning%20educational%20materials%20that%20engage%20students%20in%20this%0Aemerging%20social%20cybersecurity%20in%20the%20era%20of%20AI.%20Experiential%20learning%0Aopportunities%20are%20usually%20provided%20through%20capstone%20projects%20and%20engineering%0Adesign%20courses%20in%20STEM%20programs%20such%20as%20computer%20science.%20While%20capstone%0Aprojects%20are%20an%20excellent%20example%20of%20experiential%20learning%2C%20given%20the%0Ainterdisciplinary%20nature%20of%20this%20emerging%20social%20cybersecurity%20problem%2C%20it%20can%0Abe%20challenging%20to%20use%20them%20to%20engage%20non-computing%20students%20without%20prior%0Aknowledge%20of%20AI.%20Because%20of%20this%2C%20we%20were%20motivated%20to%20develop%20a%20hands-on%20lab%0Aplatform%20that%20provided%20experiential%20learning%20experiences%20to%20non-computing%0Astudents%20with%20little%20or%20no%20background%20knowledge%20in%20AI%20and%20discussed%20the%20lessons%0Alearned%20in%20developing%20this%20lab.%20In%20this%20lab%20used%20by%20social%20science%20students%20at%0ANorth%20Carolina%20A%26T%20State%20University%20across%20two%20semesters%20%28spring%20and%20fall%29%20in%0A2022%2C%20students%20are%20given%20a%20detailed%20lab%20manual%20and%20are%20to%20complete%20a%20set%20of%0Awell-detailed%20tasks.%20Through%20this%20process%2C%20students%20learn%20AI%20concepts%20and%20the%0Aapplication%20of%20AI%20for%20cyberharassment%20detection.%20Using%20pre-%20and%20post-surveys%2C%0Awe%20asked%20students%20to%20rate%20their%20knowledge%20or%20skills%20in%20AI%20and%20their%0Aunderstanding%20of%20the%20concepts%20learned.%20The%20results%20revealed%20that%20the%20students%0Amoderately%20understood%20the%20concepts%20of%20AI%20and%20cyberharassment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Cybersecurity%2520Education%2520Through%2520Designing%2520AI-based%2520Cyberharassment%250A%2520%2520Detection%2520Lab%26entry.906535625%3DEbuka%2520Okpala%2520and%2520Nishant%2520Vishwamitra%2520and%2520Keyan%2520Guo%2520and%2520Song%2520Liao%2520and%2520Long%2520Cheng%2520and%2520Hongxin%2520Hu%2520and%2520Yongkai%2520Wu%2520and%2520Xiaohong%2520Yuan%2520and%2520Jeannette%2520Wade%2520and%2520Sajad%2520Khorsandroo%26entry.1292438233%3D%2520%2520Cyberharassment%2520is%2520a%2520critical%252C%2520socially%2520relevant%2520cybersecurity%2520problem%250Abecause%2520of%2520the%2520adverse%2520effects%2520it%2520can%2520have%2520on%2520targeted%2520groups%2520or%2520individuals.%250AWhile%2520progress%2520has%2520been%2520made%2520in%2520understanding%2520cyber-harassment%252C%2520its%2520detection%252C%250Aattacks%2520on%2520artificial%2520intelligence%2520%2528AI%2529%2520based%2520cyberharassment%2520systems%252C%2520and%2520the%250Asocial%2520problems%2520in%2520cyberharassment%2520detectors%252C%2520little%2520has%2520been%2520done%2520in%2520designing%250Aexperiential%2520learning%2520educational%2520materials%2520that%2520engage%2520students%2520in%2520this%250Aemerging%2520social%2520cybersecurity%2520in%2520the%2520era%2520of%2520AI.%2520Experiential%2520learning%250Aopportunities%2520are%2520usually%2520provided%2520through%2520capstone%2520projects%2520and%2520engineering%250Adesign%2520courses%2520in%2520STEM%2520programs%2520such%2520as%2520computer%2520science.%2520While%2520capstone%250Aprojects%2520are%2520an%2520excellent%2520example%2520of%2520experiential%2520learning%252C%2520given%2520the%250Ainterdisciplinary%2520nature%2520of%2520this%2520emerging%2520social%2520cybersecurity%2520problem%252C%2520it%2520can%250Abe%2520challenging%2520to%2520use%2520them%2520to%2520engage%2520non-computing%2520students%2520without%2520prior%250Aknowledge%2520of%2520AI.%2520Because%2520of%2520this%252C%2520we%2520were%2520motivated%2520to%2520develop%2520a%2520hands-on%2520lab%250Aplatform%2520that%2520provided%2520experiential%2520learning%2520experiences%2520to%2520non-computing%250Astudents%2520with%2520little%2520or%2520no%2520background%2520knowledge%2520in%2520AI%2520and%2520discussed%2520the%2520lessons%250Alearned%2520in%2520developing%2520this%2520lab.%2520In%2520this%2520lab%2520used%2520by%2520social%2520science%2520students%2520at%250ANorth%2520Carolina%2520A%2526T%2520State%2520University%2520across%2520two%2520semesters%2520%2528spring%2520and%2520fall%2529%2520in%250A2022%252C%2520students%2520are%2520given%2520a%2520detailed%2520lab%2520manual%2520and%2520are%2520to%2520complete%2520a%2520set%2520of%250Awell-detailed%2520tasks.%2520Through%2520this%2520process%252C%2520students%2520learn%2520AI%2520concepts%2520and%2520the%250Aapplication%2520of%2520AI%2520for%2520cyberharassment%2520detection.%2520Using%2520pre-%2520and%2520post-surveys%252C%250Awe%2520asked%2520students%2520to%2520rate%2520their%2520knowledge%2520or%2520skills%2520in%2520AI%2520and%2520their%250Aunderstanding%2520of%2520the%2520concepts%2520learned.%2520The%2520results%2520revealed%2520that%2520the%2520students%250Amoderately%2520understood%2520the%2520concepts%2520of%2520AI%2520and%2520cyberharassment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Cybersecurity%20Education%20Through%20Designing%20AI-based%20Cyberharassment%0A%20%20Detection%20Lab&entry.906535625=Ebuka%20Okpala%20and%20Nishant%20Vishwamitra%20and%20Keyan%20Guo%20and%20Song%20Liao%20and%20Long%20Cheng%20and%20Hongxin%20Hu%20and%20Yongkai%20Wu%20and%20Xiaohong%20Yuan%20and%20Jeannette%20Wade%20and%20Sajad%20Khorsandroo&entry.1292438233=%20%20Cyberharassment%20is%20a%20critical%2C%20socially%20relevant%20cybersecurity%20problem%0Abecause%20of%20the%20adverse%20effects%20it%20can%20have%20on%20targeted%20groups%20or%20individuals.%0AWhile%20progress%20has%20been%20made%20in%20understanding%20cyber-harassment%2C%20its%20detection%2C%0Aattacks%20on%20artificial%20intelligence%20%28AI%29%20based%20cyberharassment%20systems%2C%20and%20the%0Asocial%20problems%20in%20cyberharassment%20detectors%2C%20little%20has%20been%20done%20in%20designing%0Aexperiential%20learning%20educational%20materials%20that%20engage%20students%20in%20this%0Aemerging%20social%20cybersecurity%20in%20the%20era%20of%20AI.%20Experiential%20learning%0Aopportunities%20are%20usually%20provided%20through%20capstone%20projects%20and%20engineering%0Adesign%20courses%20in%20STEM%20programs%20such%20as%20computer%20science.%20While%20capstone%0Aprojects%20are%20an%20excellent%20example%20of%20experiential%20learning%2C%20given%20the%0Ainterdisciplinary%20nature%20of%20this%20emerging%20social%20cybersecurity%20problem%2C%20it%20can%0Abe%20challenging%20to%20use%20them%20to%20engage%20non-computing%20students%20without%20prior%0Aknowledge%20of%20AI.%20Because%20of%20this%2C%20we%20were%20motivated%20to%20develop%20a%20hands-on%20lab%0Aplatform%20that%20provided%20experiential%20learning%20experiences%20to%20non-computing%0Astudents%20with%20little%20or%20no%20background%20knowledge%20in%20AI%20and%20discussed%20the%20lessons%0Alearned%20in%20developing%20this%20lab.%20In%20this%20lab%20used%20by%20social%20science%20students%20at%0ANorth%20Carolina%20A%26T%20State%20University%20across%20two%20semesters%20%28spring%20and%20fall%29%20in%0A2022%2C%20students%20are%20given%20a%20detailed%20lab%20manual%20and%20are%20to%20complete%20a%20set%20of%0Awell-detailed%20tasks.%20Through%20this%20process%2C%20students%20learn%20AI%20concepts%20and%20the%0Aapplication%20of%20AI%20for%20cyberharassment%20detection.%20Using%20pre-%20and%20post-surveys%2C%0Awe%20asked%20students%20to%20rate%20their%20knowledge%20or%20skills%20in%20AI%20and%20their%0Aunderstanding%20of%20the%20concepts%20learned.%20The%20results%20revealed%20that%20the%20students%0Amoderately%20understood%20the%20concepts%20of%20AI%20and%20cyberharassment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08125v2&entry.124074799=Read"},
{"title": "Should agentic conversational AI change how we think about ethics?\n  Characterising an interactional ethics centred on respect", "author": "Lize Alberts and Geoff Keeling and Amanda McCroskery", "abstract": "  With the growing popularity of conversational agents based on large language\nmodels (LLMs), we need to ensure their behaviour is ethical and appropriate.\nWork in this area largely centres around the 'HHH' criteria: making outputs\nmore helpful and honest, and avoiding harmful (biased, toxic, or inaccurate)\nstatements. Whilst this semantic focus is useful when viewing LLM agents as\nmere mediums or output-generating systems, it fails to account for pragmatic\nfactors that can make the same speech act seem more or less tactless or\ninconsiderate in different social situations. With the push towards agentic AI,\nwherein systems become increasingly proactive in chasing goals and performing\nactions in the world, considering the pragmatics of interaction becomes\nessential. We propose an interactional approach to ethics that is centred on\nrelational and situational factors. We explore what it means for a system, as a\nsocial actor, to treat an individual respectfully in a (series of)\ninteraction(s). Our work anticipates a set of largely unexplored risks at the\nlevel of situated social interaction, and offers practical suggestions to help\nagentic LLM technologies treat people well.\n", "link": "http://arxiv.org/abs/2401.09082v2", "date": "2024-05-16", "relevancy": 1.6541, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4441}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4116}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Should%20agentic%20conversational%20AI%20change%20how%20we%20think%20about%20ethics%3F%0A%20%20Characterising%20an%20interactional%20ethics%20centred%20on%20respect&body=Title%3A%20Should%20agentic%20conversational%20AI%20change%20how%20we%20think%20about%20ethics%3F%0A%20%20Characterising%20an%20interactional%20ethics%20centred%20on%20respect%0AAuthor%3A%20Lize%20Alberts%20and%20Geoff%20Keeling%20and%20Amanda%20McCroskery%0AAbstract%3A%20%20%20With%20the%20growing%20popularity%20of%20conversational%20agents%20based%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20need%20to%20ensure%20their%20behaviour%20is%20ethical%20and%20appropriate.%0AWork%20in%20this%20area%20largely%20centres%20around%20the%20%27HHH%27%20criteria%3A%20making%20outputs%0Amore%20helpful%20and%20honest%2C%20and%20avoiding%20harmful%20%28biased%2C%20toxic%2C%20or%20inaccurate%29%0Astatements.%20Whilst%20this%20semantic%20focus%20is%20useful%20when%20viewing%20LLM%20agents%20as%0Amere%20mediums%20or%20output-generating%20systems%2C%20it%20fails%20to%20account%20for%20pragmatic%0Afactors%20that%20can%20make%20the%20same%20speech%20act%20seem%20more%20or%20less%20tactless%20or%0Ainconsiderate%20in%20different%20social%20situations.%20With%20the%20push%20towards%20agentic%20AI%2C%0Awherein%20systems%20become%20increasingly%20proactive%20in%20chasing%20goals%20and%20performing%0Aactions%20in%20the%20world%2C%20considering%20the%20pragmatics%20of%20interaction%20becomes%0Aessential.%20We%20propose%20an%20interactional%20approach%20to%20ethics%20that%20is%20centred%20on%0Arelational%20and%20situational%20factors.%20We%20explore%20what%20it%20means%20for%20a%20system%2C%20as%20a%0Asocial%20actor%2C%20to%20treat%20an%20individual%20respectfully%20in%20a%20%28series%20of%29%0Ainteraction%28s%29.%20Our%20work%20anticipates%20a%20set%20of%20largely%20unexplored%20risks%20at%20the%0Alevel%20of%20situated%20social%20interaction%2C%20and%20offers%20practical%20suggestions%20to%20help%0Aagentic%20LLM%20technologies%20treat%20people%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShould%2520agentic%2520conversational%2520AI%2520change%2520how%2520we%2520think%2520about%2520ethics%253F%250A%2520%2520Characterising%2520an%2520interactional%2520ethics%2520centred%2520on%2520respect%26entry.906535625%3DLize%2520Alberts%2520and%2520Geoff%2520Keeling%2520and%2520Amanda%2520McCroskery%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520popularity%2520of%2520conversational%2520agents%2520based%2520on%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520we%2520need%2520to%2520ensure%2520their%2520behaviour%2520is%2520ethical%2520and%2520appropriate.%250AWork%2520in%2520this%2520area%2520largely%2520centres%2520around%2520the%2520%2527HHH%2527%2520criteria%253A%2520making%2520outputs%250Amore%2520helpful%2520and%2520honest%252C%2520and%2520avoiding%2520harmful%2520%2528biased%252C%2520toxic%252C%2520or%2520inaccurate%2529%250Astatements.%2520Whilst%2520this%2520semantic%2520focus%2520is%2520useful%2520when%2520viewing%2520LLM%2520agents%2520as%250Amere%2520mediums%2520or%2520output-generating%2520systems%252C%2520it%2520fails%2520to%2520account%2520for%2520pragmatic%250Afactors%2520that%2520can%2520make%2520the%2520same%2520speech%2520act%2520seem%2520more%2520or%2520less%2520tactless%2520or%250Ainconsiderate%2520in%2520different%2520social%2520situations.%2520With%2520the%2520push%2520towards%2520agentic%2520AI%252C%250Awherein%2520systems%2520become%2520increasingly%2520proactive%2520in%2520chasing%2520goals%2520and%2520performing%250Aactions%2520in%2520the%2520world%252C%2520considering%2520the%2520pragmatics%2520of%2520interaction%2520becomes%250Aessential.%2520We%2520propose%2520an%2520interactional%2520approach%2520to%2520ethics%2520that%2520is%2520centred%2520on%250Arelational%2520and%2520situational%2520factors.%2520We%2520explore%2520what%2520it%2520means%2520for%2520a%2520system%252C%2520as%2520a%250Asocial%2520actor%252C%2520to%2520treat%2520an%2520individual%2520respectfully%2520in%2520a%2520%2528series%2520of%2529%250Ainteraction%2528s%2529.%2520Our%2520work%2520anticipates%2520a%2520set%2520of%2520largely%2520unexplored%2520risks%2520at%2520the%250Alevel%2520of%2520situated%2520social%2520interaction%252C%2520and%2520offers%2520practical%2520suggestions%2520to%2520help%250Aagentic%2520LLM%2520technologies%2520treat%2520people%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Should%20agentic%20conversational%20AI%20change%20how%20we%20think%20about%20ethics%3F%0A%20%20Characterising%20an%20interactional%20ethics%20centred%20on%20respect&entry.906535625=Lize%20Alberts%20and%20Geoff%20Keeling%20and%20Amanda%20McCroskery&entry.1292438233=%20%20With%20the%20growing%20popularity%20of%20conversational%20agents%20based%20on%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20need%20to%20ensure%20their%20behaviour%20is%20ethical%20and%20appropriate.%0AWork%20in%20this%20area%20largely%20centres%20around%20the%20%27HHH%27%20criteria%3A%20making%20outputs%0Amore%20helpful%20and%20honest%2C%20and%20avoiding%20harmful%20%28biased%2C%20toxic%2C%20or%20inaccurate%29%0Astatements.%20Whilst%20this%20semantic%20focus%20is%20useful%20when%20viewing%20LLM%20agents%20as%0Amere%20mediums%20or%20output-generating%20systems%2C%20it%20fails%20to%20account%20for%20pragmatic%0Afactors%20that%20can%20make%20the%20same%20speech%20act%20seem%20more%20or%20less%20tactless%20or%0Ainconsiderate%20in%20different%20social%20situations.%20With%20the%20push%20towards%20agentic%20AI%2C%0Awherein%20systems%20become%20increasingly%20proactive%20in%20chasing%20goals%20and%20performing%0Aactions%20in%20the%20world%2C%20considering%20the%20pragmatics%20of%20interaction%20becomes%0Aessential.%20We%20propose%20an%20interactional%20approach%20to%20ethics%20that%20is%20centred%20on%0Arelational%20and%20situational%20factors.%20We%20explore%20what%20it%20means%20for%20a%20system%2C%20as%20a%0Asocial%20actor%2C%20to%20treat%20an%20individual%20respectfully%20in%20a%20%28series%20of%29%0Ainteraction%28s%29.%20Our%20work%20anticipates%20a%20set%20of%20largely%20unexplored%20risks%20at%20the%0Alevel%20of%20situated%20social%20interaction%2C%20and%20offers%20practical%20suggestions%20to%20help%0Aagentic%20LLM%20technologies%20treat%20people%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09082v2&entry.124074799=Read"},
{"title": "MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion\n  Recognition", "author": "Peihao Xiang and Chaohao Lin and Kaida Wu and Ou Bai", "abstract": "  This paper presents a novel approach to processing multimodal data for\ndynamic emotion recognition, named as the Multimodal Masked Autoencoder for\nDynamic Emotion Recognition (MultiMAE-DER). The MultiMAE-DER leverages the\nclosely correlated representation information within spatiotemporal sequences\nacross visual and audio modalities. By utilizing a pre-trained masked\nautoencoder model, the MultiMAEDER is accomplished through simple,\nstraightforward finetuning. The performance of the MultiMAE-DER is enhanced by\noptimizing six fusion strategies for multimodal input sequences. These\nstrategies address dynamic feature correlations within cross-domain data across\nspatial, temporal, and spatiotemporal sequences. In comparison to\nstate-of-the-art multimodal supervised learning models for dynamic emotion\nrecognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41%\non the RAVDESS dataset and by 2.06% on the CREMAD. Furthermore, when compared\nwith the state-of-the-art model of multimodal self-supervised learning,\nMultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset.\n", "link": "http://arxiv.org/abs/2404.18327v2", "date": "2024-05-16", "relevancy": 1.6467, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5508}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiMAE-DER%3A%20Multimodal%20Masked%20Autoencoder%20for%20Dynamic%20Emotion%0A%20%20Recognition&body=Title%3A%20MultiMAE-DER%3A%20Multimodal%20Masked%20Autoencoder%20for%20Dynamic%20Emotion%0A%20%20Recognition%0AAuthor%3A%20Peihao%20Xiang%20and%20Chaohao%20Lin%20and%20Kaida%20Wu%20and%20Ou%20Bai%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20processing%20multimodal%20data%20for%0Adynamic%20emotion%20recognition%2C%20named%20as%20the%20Multimodal%20Masked%20Autoencoder%20for%0ADynamic%20Emotion%20Recognition%20%28MultiMAE-DER%29.%20The%20MultiMAE-DER%20leverages%20the%0Aclosely%20correlated%20representation%20information%20within%20spatiotemporal%20sequences%0Aacross%20visual%20and%20audio%20modalities.%20By%20utilizing%20a%20pre-trained%20masked%0Aautoencoder%20model%2C%20the%20MultiMAEDER%20is%20accomplished%20through%20simple%2C%0Astraightforward%20finetuning.%20The%20performance%20of%20the%20MultiMAE-DER%20is%20enhanced%20by%0Aoptimizing%20six%20fusion%20strategies%20for%20multimodal%20input%20sequences.%20These%0Astrategies%20address%20dynamic%20feature%20correlations%20within%20cross-domain%20data%20across%0Aspatial%2C%20temporal%2C%20and%20spatiotemporal%20sequences.%20In%20comparison%20to%0Astate-of-the-art%20multimodal%20supervised%20learning%20models%20for%20dynamic%20emotion%0Arecognition%2C%20MultiMAE-DER%20enhances%20the%20weighted%20average%20recall%20%28WAR%29%20by%204.41%25%0Aon%20the%20RAVDESS%20dataset%20and%20by%202.06%25%20on%20the%20CREMAD.%20Furthermore%2C%20when%20compared%0Awith%20the%20state-of-the-art%20model%20of%20multimodal%20self-supervised%20learning%2C%0AMultiMAE-DER%20achieves%20a%201.86%25%20higher%20WAR%20on%20the%20IEMOCAP%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiMAE-DER%253A%2520Multimodal%2520Masked%2520Autoencoder%2520for%2520Dynamic%2520Emotion%250A%2520%2520Recognition%26entry.906535625%3DPeihao%2520Xiang%2520and%2520Chaohao%2520Lin%2520and%2520Kaida%2520Wu%2520and%2520Ou%2520Bai%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520processing%2520multimodal%2520data%2520for%250Adynamic%2520emotion%2520recognition%252C%2520named%2520as%2520the%2520Multimodal%2520Masked%2520Autoencoder%2520for%250ADynamic%2520Emotion%2520Recognition%2520%2528MultiMAE-DER%2529.%2520The%2520MultiMAE-DER%2520leverages%2520the%250Aclosely%2520correlated%2520representation%2520information%2520within%2520spatiotemporal%2520sequences%250Aacross%2520visual%2520and%2520audio%2520modalities.%2520By%2520utilizing%2520a%2520pre-trained%2520masked%250Aautoencoder%2520model%252C%2520the%2520MultiMAEDER%2520is%2520accomplished%2520through%2520simple%252C%250Astraightforward%2520finetuning.%2520The%2520performance%2520of%2520the%2520MultiMAE-DER%2520is%2520enhanced%2520by%250Aoptimizing%2520six%2520fusion%2520strategies%2520for%2520multimodal%2520input%2520sequences.%2520These%250Astrategies%2520address%2520dynamic%2520feature%2520correlations%2520within%2520cross-domain%2520data%2520across%250Aspatial%252C%2520temporal%252C%2520and%2520spatiotemporal%2520sequences.%2520In%2520comparison%2520to%250Astate-of-the-art%2520multimodal%2520supervised%2520learning%2520models%2520for%2520dynamic%2520emotion%250Arecognition%252C%2520MultiMAE-DER%2520enhances%2520the%2520weighted%2520average%2520recall%2520%2528WAR%2529%2520by%25204.41%2525%250Aon%2520the%2520RAVDESS%2520dataset%2520and%2520by%25202.06%2525%2520on%2520the%2520CREMAD.%2520Furthermore%252C%2520when%2520compared%250Awith%2520the%2520state-of-the-art%2520model%2520of%2520multimodal%2520self-supervised%2520learning%252C%250AMultiMAE-DER%2520achieves%2520a%25201.86%2525%2520higher%2520WAR%2520on%2520the%2520IEMOCAP%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiMAE-DER%3A%20Multimodal%20Masked%20Autoencoder%20for%20Dynamic%20Emotion%0A%20%20Recognition&entry.906535625=Peihao%20Xiang%20and%20Chaohao%20Lin%20and%20Kaida%20Wu%20and%20Ou%20Bai&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20processing%20multimodal%20data%20for%0Adynamic%20emotion%20recognition%2C%20named%20as%20the%20Multimodal%20Masked%20Autoencoder%20for%0ADynamic%20Emotion%20Recognition%20%28MultiMAE-DER%29.%20The%20MultiMAE-DER%20leverages%20the%0Aclosely%20correlated%20representation%20information%20within%20spatiotemporal%20sequences%0Aacross%20visual%20and%20audio%20modalities.%20By%20utilizing%20a%20pre-trained%20masked%0Aautoencoder%20model%2C%20the%20MultiMAEDER%20is%20accomplished%20through%20simple%2C%0Astraightforward%20finetuning.%20The%20performance%20of%20the%20MultiMAE-DER%20is%20enhanced%20by%0Aoptimizing%20six%20fusion%20strategies%20for%20multimodal%20input%20sequences.%20These%0Astrategies%20address%20dynamic%20feature%20correlations%20within%20cross-domain%20data%20across%0Aspatial%2C%20temporal%2C%20and%20spatiotemporal%20sequences.%20In%20comparison%20to%0Astate-of-the-art%20multimodal%20supervised%20learning%20models%20for%20dynamic%20emotion%0Arecognition%2C%20MultiMAE-DER%20enhances%20the%20weighted%20average%20recall%20%28WAR%29%20by%204.41%25%0Aon%20the%20RAVDESS%20dataset%20and%20by%202.06%25%20on%20the%20CREMAD.%20Furthermore%2C%20when%20compared%0Awith%20the%20state-of-the-art%20model%20of%20multimodal%20self-supervised%20learning%2C%0AMultiMAE-DER%20achieves%20a%201.86%25%20higher%20WAR%20on%20the%20IEMOCAP%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18327v2&entry.124074799=Read"},
{"title": "A Preprocessing and Postprocessing Voxel-based Method for LiDAR Semantic\n  Segmentation Improvement in Long Distance", "author": "Andrea Matteazzi and Pascal Colling and Michael Arnold and Dietmar Tutsch", "abstract": "  In recent years considerable research in LiDAR semantic segmentation was\nconducted, introducing several new state of the art models. However, most\nresearch focuses on single-scan point clouds, limiting performance especially\nin long distance outdoor scenarios, by omitting time-sequential information.\nMoreover, varying-density and occlusions constitute significant challenges in\nsingle-scan approaches. In this paper we propose a LiDAR point cloud\npreprocessing and postprocessing method. This multi-stage approach, in\nconjunction with state of the art models in a multi-scan setting, aims to solve\nthose challenges. We demonstrate the benefits of our method through\nquantitative evaluation with the given models in single-scan settings. In\nparticular, we achieve significant improvements in mIoU performance of over 5\npercentage point in medium range and over 10 percentage point in far range.\nThis is essential for 3D semantic scene understanding in long distance as well\nas for applications where offline processing is permissible.\n", "link": "http://arxiv.org/abs/2405.10046v1", "date": "2024-05-16", "relevancy": 1.6435, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5608}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Preprocessing%20and%20Postprocessing%20Voxel-based%20Method%20for%20LiDAR%20Semantic%0A%20%20Segmentation%20Improvement%20in%20Long%20Distance&body=Title%3A%20A%20Preprocessing%20and%20Postprocessing%20Voxel-based%20Method%20for%20LiDAR%20Semantic%0A%20%20Segmentation%20Improvement%20in%20Long%20Distance%0AAuthor%3A%20Andrea%20Matteazzi%20and%20Pascal%20Colling%20and%20Michael%20Arnold%20and%20Dietmar%20Tutsch%0AAbstract%3A%20%20%20In%20recent%20years%20considerable%20research%20in%20LiDAR%20semantic%20segmentation%20was%0Aconducted%2C%20introducing%20several%20new%20state%20of%20the%20art%20models.%20However%2C%20most%0Aresearch%20focuses%20on%20single-scan%20point%20clouds%2C%20limiting%20performance%20especially%0Ain%20long%20distance%20outdoor%20scenarios%2C%20by%20omitting%20time-sequential%20information.%0AMoreover%2C%20varying-density%20and%20occlusions%20constitute%20significant%20challenges%20in%0Asingle-scan%20approaches.%20In%20this%20paper%20we%20propose%20a%20LiDAR%20point%20cloud%0Apreprocessing%20and%20postprocessing%20method.%20This%20multi-stage%20approach%2C%20in%0Aconjunction%20with%20state%20of%20the%20art%20models%20in%20a%20multi-scan%20setting%2C%20aims%20to%20solve%0Athose%20challenges.%20We%20demonstrate%20the%20benefits%20of%20our%20method%20through%0Aquantitative%20evaluation%20with%20the%20given%20models%20in%20single-scan%20settings.%20In%0Aparticular%2C%20we%20achieve%20significant%20improvements%20in%20mIoU%20performance%20of%20over%205%0Apercentage%20point%20in%20medium%20range%20and%20over%2010%20percentage%20point%20in%20far%20range.%0AThis%20is%20essential%20for%203D%20semantic%20scene%20understanding%20in%20long%20distance%20as%20well%0Aas%20for%20applications%20where%20offline%20processing%20is%20permissible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Preprocessing%2520and%2520Postprocessing%2520Voxel-based%2520Method%2520for%2520LiDAR%2520Semantic%250A%2520%2520Segmentation%2520Improvement%2520in%2520Long%2520Distance%26entry.906535625%3DAndrea%2520Matteazzi%2520and%2520Pascal%2520Colling%2520and%2520Michael%2520Arnold%2520and%2520Dietmar%2520Tutsch%26entry.1292438233%3D%2520%2520In%2520recent%2520years%2520considerable%2520research%2520in%2520LiDAR%2520semantic%2520segmentation%2520was%250Aconducted%252C%2520introducing%2520several%2520new%2520state%2520of%2520the%2520art%2520models.%2520However%252C%2520most%250Aresearch%2520focuses%2520on%2520single-scan%2520point%2520clouds%252C%2520limiting%2520performance%2520especially%250Ain%2520long%2520distance%2520outdoor%2520scenarios%252C%2520by%2520omitting%2520time-sequential%2520information.%250AMoreover%252C%2520varying-density%2520and%2520occlusions%2520constitute%2520significant%2520challenges%2520in%250Asingle-scan%2520approaches.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520LiDAR%2520point%2520cloud%250Apreprocessing%2520and%2520postprocessing%2520method.%2520This%2520multi-stage%2520approach%252C%2520in%250Aconjunction%2520with%2520state%2520of%2520the%2520art%2520models%2520in%2520a%2520multi-scan%2520setting%252C%2520aims%2520to%2520solve%250Athose%2520challenges.%2520We%2520demonstrate%2520the%2520benefits%2520of%2520our%2520method%2520through%250Aquantitative%2520evaluation%2520with%2520the%2520given%2520models%2520in%2520single-scan%2520settings.%2520In%250Aparticular%252C%2520we%2520achieve%2520significant%2520improvements%2520in%2520mIoU%2520performance%2520of%2520over%25205%250Apercentage%2520point%2520in%2520medium%2520range%2520and%2520over%252010%2520percentage%2520point%2520in%2520far%2520range.%250AThis%2520is%2520essential%2520for%25203D%2520semantic%2520scene%2520understanding%2520in%2520long%2520distance%2520as%2520well%250Aas%2520for%2520applications%2520where%2520offline%2520processing%2520is%2520permissible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Preprocessing%20and%20Postprocessing%20Voxel-based%20Method%20for%20LiDAR%20Semantic%0A%20%20Segmentation%20Improvement%20in%20Long%20Distance&entry.906535625=Andrea%20Matteazzi%20and%20Pascal%20Colling%20and%20Michael%20Arnold%20and%20Dietmar%20Tutsch&entry.1292438233=%20%20In%20recent%20years%20considerable%20research%20in%20LiDAR%20semantic%20segmentation%20was%0Aconducted%2C%20introducing%20several%20new%20state%20of%20the%20art%20models.%20However%2C%20most%0Aresearch%20focuses%20on%20single-scan%20point%20clouds%2C%20limiting%20performance%20especially%0Ain%20long%20distance%20outdoor%20scenarios%2C%20by%20omitting%20time-sequential%20information.%0AMoreover%2C%20varying-density%20and%20occlusions%20constitute%20significant%20challenges%20in%0Asingle-scan%20approaches.%20In%20this%20paper%20we%20propose%20a%20LiDAR%20point%20cloud%0Apreprocessing%20and%20postprocessing%20method.%20This%20multi-stage%20approach%2C%20in%0Aconjunction%20with%20state%20of%20the%20art%20models%20in%20a%20multi-scan%20setting%2C%20aims%20to%20solve%0Athose%20challenges.%20We%20demonstrate%20the%20benefits%20of%20our%20method%20through%0Aquantitative%20evaluation%20with%20the%20given%20models%20in%20single-scan%20settings.%20In%0Aparticular%2C%20we%20achieve%20significant%20improvements%20in%20mIoU%20performance%20of%20over%205%0Apercentage%20point%20in%20medium%20range%20and%20over%2010%20percentage%20point%20in%20far%20range.%0AThis%20is%20essential%20for%203D%20semantic%20scene%20understanding%20in%20long%20distance%20as%20well%0Aas%20for%20applications%20where%20offline%20processing%20is%20permissible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10046v1&entry.124074799=Read"},
{"title": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction", "author": "Yunfan Jiang and Chen Wang and Ruohan Zhang and Jiajun Wu and Li Fei-Fei", "abstract": "  Learning in simulation and transferring the learned policy to the real world\nhas the potential to enable generalist robots. The key challenge of this\napproach is to address simulation-to-reality (sim-to-real) gaps. Previous\nmethods often require domain-specific knowledge a priori. We argue that a\nstraightforward way to obtain such knowledge is by asking humans to observe and\nassist robot policy execution in the real world. The robots can then learn from\nhumans to close various sim-to-real gaps. We propose TRANSIC, a data-driven\napproach to enable successful sim-to-real transfer based on a human-in-the-loop\nframework. TRANSIC allows humans to augment simulation policies to overcome\nvarious unmodeled sim-to-real gaps holistically through intervention and online\ncorrection. Residual policies can be learned from human corrections and\nintegrated with simulation policies for autonomous execution. We show that our\napproach can achieve successful sim-to-real transfer in complex and\ncontact-rich manipulation tasks such as furniture assembly. Through synergistic\nintegration of policies learned in simulation and from humans, TRANSIC is\neffective as a holistic approach to addressing various, often coexisting\nsim-to-real gaps. It displays attractive properties such as scaling with human\neffort. Videos and code are available at https://transic-robot.github.io/\n", "link": "http://arxiv.org/abs/2405.10315v1", "date": "2024-05-16", "relevancy": 1.6245, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5473}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRANSIC%3A%20Sim-to-Real%20Policy%20Transfer%20by%20Learning%20from%20Online%20Correction&body=Title%3A%20TRANSIC%3A%20Sim-to-Real%20Policy%20Transfer%20by%20Learning%20from%20Online%20Correction%0AAuthor%3A%20Yunfan%20Jiang%20and%20Chen%20Wang%20and%20Ruohan%20Zhang%20and%20Jiajun%20Wu%20and%20Li%20Fei-Fei%0AAbstract%3A%20%20%20Learning%20in%20simulation%20and%20transferring%20the%20learned%20policy%20to%20the%20real%20world%0Ahas%20the%20potential%20to%20enable%20generalist%20robots.%20The%20key%20challenge%20of%20this%0Aapproach%20is%20to%20address%20simulation-to-reality%20%28sim-to-real%29%20gaps.%20Previous%0Amethods%20often%20require%20domain-specific%20knowledge%20a%20priori.%20We%20argue%20that%20a%0Astraightforward%20way%20to%20obtain%20such%20knowledge%20is%20by%20asking%20humans%20to%20observe%20and%0Aassist%20robot%20policy%20execution%20in%20the%20real%20world.%20The%20robots%20can%20then%20learn%20from%0Ahumans%20to%20close%20various%20sim-to-real%20gaps.%20We%20propose%20TRANSIC%2C%20a%20data-driven%0Aapproach%20to%20enable%20successful%20sim-to-real%20transfer%20based%20on%20a%20human-in-the-loop%0Aframework.%20TRANSIC%20allows%20humans%20to%20augment%20simulation%20policies%20to%20overcome%0Avarious%20unmodeled%20sim-to-real%20gaps%20holistically%20through%20intervention%20and%20online%0Acorrection.%20Residual%20policies%20can%20be%20learned%20from%20human%20corrections%20and%0Aintegrated%20with%20simulation%20policies%20for%20autonomous%20execution.%20We%20show%20that%20our%0Aapproach%20can%20achieve%20successful%20sim-to-real%20transfer%20in%20complex%20and%0Acontact-rich%20manipulation%20tasks%20such%20as%20furniture%20assembly.%20Through%20synergistic%0Aintegration%20of%20policies%20learned%20in%20simulation%20and%20from%20humans%2C%20TRANSIC%20is%0Aeffective%20as%20a%20holistic%20approach%20to%20addressing%20various%2C%20often%20coexisting%0Asim-to-real%20gaps.%20It%20displays%20attractive%20properties%20such%20as%20scaling%20with%20human%0Aeffort.%20Videos%20and%20code%20are%20available%20at%20https%3A//transic-robot.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRANSIC%253A%2520Sim-to-Real%2520Policy%2520Transfer%2520by%2520Learning%2520from%2520Online%2520Correction%26entry.906535625%3DYunfan%2520Jiang%2520and%2520Chen%2520Wang%2520and%2520Ruohan%2520Zhang%2520and%2520Jiajun%2520Wu%2520and%2520Li%2520Fei-Fei%26entry.1292438233%3D%2520%2520Learning%2520in%2520simulation%2520and%2520transferring%2520the%2520learned%2520policy%2520to%2520the%2520real%2520world%250Ahas%2520the%2520potential%2520to%2520enable%2520generalist%2520robots.%2520The%2520key%2520challenge%2520of%2520this%250Aapproach%2520is%2520to%2520address%2520simulation-to-reality%2520%2528sim-to-real%2529%2520gaps.%2520Previous%250Amethods%2520often%2520require%2520domain-specific%2520knowledge%2520a%2520priori.%2520We%2520argue%2520that%2520a%250Astraightforward%2520way%2520to%2520obtain%2520such%2520knowledge%2520is%2520by%2520asking%2520humans%2520to%2520observe%2520and%250Aassist%2520robot%2520policy%2520execution%2520in%2520the%2520real%2520world.%2520The%2520robots%2520can%2520then%2520learn%2520from%250Ahumans%2520to%2520close%2520various%2520sim-to-real%2520gaps.%2520We%2520propose%2520TRANSIC%252C%2520a%2520data-driven%250Aapproach%2520to%2520enable%2520successful%2520sim-to-real%2520transfer%2520based%2520on%2520a%2520human-in-the-loop%250Aframework.%2520TRANSIC%2520allows%2520humans%2520to%2520augment%2520simulation%2520policies%2520to%2520overcome%250Avarious%2520unmodeled%2520sim-to-real%2520gaps%2520holistically%2520through%2520intervention%2520and%2520online%250Acorrection.%2520Residual%2520policies%2520can%2520be%2520learned%2520from%2520human%2520corrections%2520and%250Aintegrated%2520with%2520simulation%2520policies%2520for%2520autonomous%2520execution.%2520We%2520show%2520that%2520our%250Aapproach%2520can%2520achieve%2520successful%2520sim-to-real%2520transfer%2520in%2520complex%2520and%250Acontact-rich%2520manipulation%2520tasks%2520such%2520as%2520furniture%2520assembly.%2520Through%2520synergistic%250Aintegration%2520of%2520policies%2520learned%2520in%2520simulation%2520and%2520from%2520humans%252C%2520TRANSIC%2520is%250Aeffective%2520as%2520a%2520holistic%2520approach%2520to%2520addressing%2520various%252C%2520often%2520coexisting%250Asim-to-real%2520gaps.%2520It%2520displays%2520attractive%2520properties%2520such%2520as%2520scaling%2520with%2520human%250Aeffort.%2520Videos%2520and%2520code%2520are%2520available%2520at%2520https%253A//transic-robot.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRANSIC%3A%20Sim-to-Real%20Policy%20Transfer%20by%20Learning%20from%20Online%20Correction&entry.906535625=Yunfan%20Jiang%20and%20Chen%20Wang%20and%20Ruohan%20Zhang%20and%20Jiajun%20Wu%20and%20Li%20Fei-Fei&entry.1292438233=%20%20Learning%20in%20simulation%20and%20transferring%20the%20learned%20policy%20to%20the%20real%20world%0Ahas%20the%20potential%20to%20enable%20generalist%20robots.%20The%20key%20challenge%20of%20this%0Aapproach%20is%20to%20address%20simulation-to-reality%20%28sim-to-real%29%20gaps.%20Previous%0Amethods%20often%20require%20domain-specific%20knowledge%20a%20priori.%20We%20argue%20that%20a%0Astraightforward%20way%20to%20obtain%20such%20knowledge%20is%20by%20asking%20humans%20to%20observe%20and%0Aassist%20robot%20policy%20execution%20in%20the%20real%20world.%20The%20robots%20can%20then%20learn%20from%0Ahumans%20to%20close%20various%20sim-to-real%20gaps.%20We%20propose%20TRANSIC%2C%20a%20data-driven%0Aapproach%20to%20enable%20successful%20sim-to-real%20transfer%20based%20on%20a%20human-in-the-loop%0Aframework.%20TRANSIC%20allows%20humans%20to%20augment%20simulation%20policies%20to%20overcome%0Avarious%20unmodeled%20sim-to-real%20gaps%20holistically%20through%20intervention%20and%20online%0Acorrection.%20Residual%20policies%20can%20be%20learned%20from%20human%20corrections%20and%0Aintegrated%20with%20simulation%20policies%20for%20autonomous%20execution.%20We%20show%20that%20our%0Aapproach%20can%20achieve%20successful%20sim-to-real%20transfer%20in%20complex%20and%0Acontact-rich%20manipulation%20tasks%20such%20as%20furniture%20assembly.%20Through%20synergistic%0Aintegration%20of%20policies%20learned%20in%20simulation%20and%20from%20humans%2C%20TRANSIC%20is%0Aeffective%20as%20a%20holistic%20approach%20to%20addressing%20various%2C%20often%20coexisting%0Asim-to-real%20gaps.%20It%20displays%20attractive%20properties%20such%20as%20scaling%20with%20human%0Aeffort.%20Videos%20and%20code%20are%20available%20at%20https%3A//transic-robot.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10315v1&entry.124074799=Read"},
{"title": "Language-Oriented Semantic Latent Representation for Image Transmission", "author": "Giordano Cicchetti and Eleonora Grassucci and Jihong Park and Jinho Choi and Sergio Barbarossa and Danilo Comminiello", "abstract": "  In the new paradigm of semantic communication (SC), the focus is on\ndelivering meanings behind bits by extracting semantic information from raw\ndata. Recent advances in data-to-text models facilitate language-oriented SC,\nparticularly for text-transformed image communication via image-to-text (I2T)\nencoding and text-to-image (T2I) decoding. However, although semantically\naligned, the text is too coarse to precisely capture sophisticated visual\nfeatures such as spatial locations, color, and texture, incurring a significant\nperceptual difference between intended and reconstructed images. To address\nthis limitation, in this paper, we propose a novel language-oriented SC\nframework that communicates both text and a compressed image embedding and\ncombines them using a latent diffusion model to reconstruct the intended image.\nExperimental results validate the potential of our approach, which transmits\nonly 2.09\\% of the original image size while achieving higher perceptual\nsimilarities in noisy communication channels compared to a baseline SC method\nthat communicates only through text.The code is available at\nhttps://github.com/ispamm/Img2Img-SC/ .\n", "link": "http://arxiv.org/abs/2405.09976v1", "date": "2024-05-16", "relevancy": 1.6338, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5888}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Oriented%20Semantic%20Latent%20Representation%20for%20Image%20Transmission&body=Title%3A%20Language-Oriented%20Semantic%20Latent%20Representation%20for%20Image%20Transmission%0AAuthor%3A%20Giordano%20Cicchetti%20and%20Eleonora%20Grassucci%20and%20Jihong%20Park%20and%20Jinho%20Choi%20and%20Sergio%20Barbarossa%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20In%20the%20new%20paradigm%20of%20semantic%20communication%20%28SC%29%2C%20the%20focus%20is%20on%0Adelivering%20meanings%20behind%20bits%20by%20extracting%20semantic%20information%20from%20raw%0Adata.%20Recent%20advances%20in%20data-to-text%20models%20facilitate%20language-oriented%20SC%2C%0Aparticularly%20for%20text-transformed%20image%20communication%20via%20image-to-text%20%28I2T%29%0Aencoding%20and%20text-to-image%20%28T2I%29%20decoding.%20However%2C%20although%20semantically%0Aaligned%2C%20the%20text%20is%20too%20coarse%20to%20precisely%20capture%20sophisticated%20visual%0Afeatures%20such%20as%20spatial%20locations%2C%20color%2C%20and%20texture%2C%20incurring%20a%20significant%0Aperceptual%20difference%20between%20intended%20and%20reconstructed%20images.%20To%20address%0Athis%20limitation%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20language-oriented%20SC%0Aframework%20that%20communicates%20both%20text%20and%20a%20compressed%20image%20embedding%20and%0Acombines%20them%20using%20a%20latent%20diffusion%20model%20to%20reconstruct%20the%20intended%20image.%0AExperimental%20results%20validate%20the%20potential%20of%20our%20approach%2C%20which%20transmits%0Aonly%202.09%5C%25%20of%20the%20original%20image%20size%20while%20achieving%20higher%20perceptual%0Asimilarities%20in%20noisy%20communication%20channels%20compared%20to%20a%20baseline%20SC%20method%0Athat%20communicates%20only%20through%20text.The%20code%20is%20available%20at%0Ahttps%3A//github.com/ispamm/Img2Img-SC/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Oriented%2520Semantic%2520Latent%2520Representation%2520for%2520Image%2520Transmission%26entry.906535625%3DGiordano%2520Cicchetti%2520and%2520Eleonora%2520Grassucci%2520and%2520Jihong%2520Park%2520and%2520Jinho%2520Choi%2520and%2520Sergio%2520Barbarossa%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520In%2520the%2520new%2520paradigm%2520of%2520semantic%2520communication%2520%2528SC%2529%252C%2520the%2520focus%2520is%2520on%250Adelivering%2520meanings%2520behind%2520bits%2520by%2520extracting%2520semantic%2520information%2520from%2520raw%250Adata.%2520Recent%2520advances%2520in%2520data-to-text%2520models%2520facilitate%2520language-oriented%2520SC%252C%250Aparticularly%2520for%2520text-transformed%2520image%2520communication%2520via%2520image-to-text%2520%2528I2T%2529%250Aencoding%2520and%2520text-to-image%2520%2528T2I%2529%2520decoding.%2520However%252C%2520although%2520semantically%250Aaligned%252C%2520the%2520text%2520is%2520too%2520coarse%2520to%2520precisely%2520capture%2520sophisticated%2520visual%250Afeatures%2520such%2520as%2520spatial%2520locations%252C%2520color%252C%2520and%2520texture%252C%2520incurring%2520a%2520significant%250Aperceptual%2520difference%2520between%2520intended%2520and%2520reconstructed%2520images.%2520To%2520address%250Athis%2520limitation%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520language-oriented%2520SC%250Aframework%2520that%2520communicates%2520both%2520text%2520and%2520a%2520compressed%2520image%2520embedding%2520and%250Acombines%2520them%2520using%2520a%2520latent%2520diffusion%2520model%2520to%2520reconstruct%2520the%2520intended%2520image.%250AExperimental%2520results%2520validate%2520the%2520potential%2520of%2520our%2520approach%252C%2520which%2520transmits%250Aonly%25202.09%255C%2525%2520of%2520the%2520original%2520image%2520size%2520while%2520achieving%2520higher%2520perceptual%250Asimilarities%2520in%2520noisy%2520communication%2520channels%2520compared%2520to%2520a%2520baseline%2520SC%2520method%250Athat%2520communicates%2520only%2520through%2520text.The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ispamm/Img2Img-SC/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Oriented%20Semantic%20Latent%20Representation%20for%20Image%20Transmission&entry.906535625=Giordano%20Cicchetti%20and%20Eleonora%20Grassucci%20and%20Jihong%20Park%20and%20Jinho%20Choi%20and%20Sergio%20Barbarossa%20and%20Danilo%20Comminiello&entry.1292438233=%20%20In%20the%20new%20paradigm%20of%20semantic%20communication%20%28SC%29%2C%20the%20focus%20is%20on%0Adelivering%20meanings%20behind%20bits%20by%20extracting%20semantic%20information%20from%20raw%0Adata.%20Recent%20advances%20in%20data-to-text%20models%20facilitate%20language-oriented%20SC%2C%0Aparticularly%20for%20text-transformed%20image%20communication%20via%20image-to-text%20%28I2T%29%0Aencoding%20and%20text-to-image%20%28T2I%29%20decoding.%20However%2C%20although%20semantically%0Aaligned%2C%20the%20text%20is%20too%20coarse%20to%20precisely%20capture%20sophisticated%20visual%0Afeatures%20such%20as%20spatial%20locations%2C%20color%2C%20and%20texture%2C%20incurring%20a%20significant%0Aperceptual%20difference%20between%20intended%20and%20reconstructed%20images.%20To%20address%0Athis%20limitation%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20language-oriented%20SC%0Aframework%20that%20communicates%20both%20text%20and%20a%20compressed%20image%20embedding%20and%0Acombines%20them%20using%20a%20latent%20diffusion%20model%20to%20reconstruct%20the%20intended%20image.%0AExperimental%20results%20validate%20the%20potential%20of%20our%20approach%2C%20which%20transmits%0Aonly%202.09%5C%25%20of%20the%20original%20image%20size%20while%20achieving%20higher%20perceptual%0Asimilarities%20in%20noisy%20communication%20channels%20compared%20to%20a%20baseline%20SC%20method%0Athat%20communicates%20only%20through%20text.The%20code%20is%20available%20at%0Ahttps%3A//github.com/ispamm/Img2Img-SC/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09976v1&entry.124074799=Read"},
{"title": "Optimal Aggregation of Prediction Intervals under Unsupervised Domain\n  Shift", "author": "Jiawei Ge and Debarghya Mukherjee and Jianqing Fan", "abstract": "  As machine learning models are increasingly deployed in dynamic environments,\nit becomes paramount to assess and quantify uncertainties associated with\ndistribution shifts. A distribution shift occurs when the underlying\ndata-generating process changes, leading to a deviation in the model's\nperformance. The prediction interval, which captures the range of likely\noutcomes for a given prediction, serves as a crucial tool for characterizing\nuncertainties induced by their underlying distribution. In this paper, we\npropose methodologies for aggregating prediction intervals to obtain one with\nminimal width and adequate coverage on the target domain under unsupervised\ndomain shift, under which we have labeled samples from a related source domain\nand unlabeled covariates from the target domain. Our analysis encompasses\nscenarios where the source and the target domain are related via i) a bounded\ndensity ratio, and ii) a measure-preserving transformation. Our proposed\nmethodologies are computationally efficient and easy to implement. Beyond\nillustrating the performance of our method through a real-world dataset, we\nalso delve into the theoretical details. This includes establishing rigorous\ntheoretical guarantees, coupled with finite sample bounds, regarding the\ncoverage and width of our prediction intervals. Our approach excels in\npractical applications and is underpinned by a solid theoretical framework,\nensuring its reliability and effectiveness across diverse contexts.\n", "link": "http://arxiv.org/abs/2405.10302v1", "date": "2024-05-16", "relevancy": 1.4564, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4871}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Aggregation%20of%20Prediction%20Intervals%20under%20Unsupervised%20Domain%0A%20%20Shift&body=Title%3A%20Optimal%20Aggregation%20of%20Prediction%20Intervals%20under%20Unsupervised%20Domain%0A%20%20Shift%0AAuthor%3A%20Jiawei%20Ge%20and%20Debarghya%20Mukherjee%20and%20Jianqing%20Fan%0AAbstract%3A%20%20%20As%20machine%20learning%20models%20are%20increasingly%20deployed%20in%20dynamic%20environments%2C%0Ait%20becomes%20paramount%20to%20assess%20and%20quantify%20uncertainties%20associated%20with%0Adistribution%20shifts.%20A%20distribution%20shift%20occurs%20when%20the%20underlying%0Adata-generating%20process%20changes%2C%20leading%20to%20a%20deviation%20in%20the%20model%27s%0Aperformance.%20The%20prediction%20interval%2C%20which%20captures%20the%20range%20of%20likely%0Aoutcomes%20for%20a%20given%20prediction%2C%20serves%20as%20a%20crucial%20tool%20for%20characterizing%0Auncertainties%20induced%20by%20their%20underlying%20distribution.%20In%20this%20paper%2C%20we%0Apropose%20methodologies%20for%20aggregating%20prediction%20intervals%20to%20obtain%20one%20with%0Aminimal%20width%20and%20adequate%20coverage%20on%20the%20target%20domain%20under%20unsupervised%0Adomain%20shift%2C%20under%20which%20we%20have%20labeled%20samples%20from%20a%20related%20source%20domain%0Aand%20unlabeled%20covariates%20from%20the%20target%20domain.%20Our%20analysis%20encompasses%0Ascenarios%20where%20the%20source%20and%20the%20target%20domain%20are%20related%20via%20i%29%20a%20bounded%0Adensity%20ratio%2C%20and%20ii%29%20a%20measure-preserving%20transformation.%20Our%20proposed%0Amethodologies%20are%20computationally%20efficient%20and%20easy%20to%20implement.%20Beyond%0Aillustrating%20the%20performance%20of%20our%20method%20through%20a%20real-world%20dataset%2C%20we%0Aalso%20delve%20into%20the%20theoretical%20details.%20This%20includes%20establishing%20rigorous%0Atheoretical%20guarantees%2C%20coupled%20with%20finite%20sample%20bounds%2C%20regarding%20the%0Acoverage%20and%20width%20of%20our%20prediction%20intervals.%20Our%20approach%20excels%20in%0Apractical%20applications%20and%20is%20underpinned%20by%20a%20solid%20theoretical%20framework%2C%0Aensuring%20its%20reliability%20and%20effectiveness%20across%20diverse%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Aggregation%2520of%2520Prediction%2520Intervals%2520under%2520Unsupervised%2520Domain%250A%2520%2520Shift%26entry.906535625%3DJiawei%2520Ge%2520and%2520Debarghya%2520Mukherjee%2520and%2520Jianqing%2520Fan%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520models%2520are%2520increasingly%2520deployed%2520in%2520dynamic%2520environments%252C%250Ait%2520becomes%2520paramount%2520to%2520assess%2520and%2520quantify%2520uncertainties%2520associated%2520with%250Adistribution%2520shifts.%2520A%2520distribution%2520shift%2520occurs%2520when%2520the%2520underlying%250Adata-generating%2520process%2520changes%252C%2520leading%2520to%2520a%2520deviation%2520in%2520the%2520model%2527s%250Aperformance.%2520The%2520prediction%2520interval%252C%2520which%2520captures%2520the%2520range%2520of%2520likely%250Aoutcomes%2520for%2520a%2520given%2520prediction%252C%2520serves%2520as%2520a%2520crucial%2520tool%2520for%2520characterizing%250Auncertainties%2520induced%2520by%2520their%2520underlying%2520distribution.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520methodologies%2520for%2520aggregating%2520prediction%2520intervals%2520to%2520obtain%2520one%2520with%250Aminimal%2520width%2520and%2520adequate%2520coverage%2520on%2520the%2520target%2520domain%2520under%2520unsupervised%250Adomain%2520shift%252C%2520under%2520which%2520we%2520have%2520labeled%2520samples%2520from%2520a%2520related%2520source%2520domain%250Aand%2520unlabeled%2520covariates%2520from%2520the%2520target%2520domain.%2520Our%2520analysis%2520encompasses%250Ascenarios%2520where%2520the%2520source%2520and%2520the%2520target%2520domain%2520are%2520related%2520via%2520i%2529%2520a%2520bounded%250Adensity%2520ratio%252C%2520and%2520ii%2529%2520a%2520measure-preserving%2520transformation.%2520Our%2520proposed%250Amethodologies%2520are%2520computationally%2520efficient%2520and%2520easy%2520to%2520implement.%2520Beyond%250Aillustrating%2520the%2520performance%2520of%2520our%2520method%2520through%2520a%2520real-world%2520dataset%252C%2520we%250Aalso%2520delve%2520into%2520the%2520theoretical%2520details.%2520This%2520includes%2520establishing%2520rigorous%250Atheoretical%2520guarantees%252C%2520coupled%2520with%2520finite%2520sample%2520bounds%252C%2520regarding%2520the%250Acoverage%2520and%2520width%2520of%2520our%2520prediction%2520intervals.%2520Our%2520approach%2520excels%2520in%250Apractical%2520applications%2520and%2520is%2520underpinned%2520by%2520a%2520solid%2520theoretical%2520framework%252C%250Aensuring%2520its%2520reliability%2520and%2520effectiveness%2520across%2520diverse%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Aggregation%20of%20Prediction%20Intervals%20under%20Unsupervised%20Domain%0A%20%20Shift&entry.906535625=Jiawei%20Ge%20and%20Debarghya%20Mukherjee%20and%20Jianqing%20Fan&entry.1292438233=%20%20As%20machine%20learning%20models%20are%20increasingly%20deployed%20in%20dynamic%20environments%2C%0Ait%20becomes%20paramount%20to%20assess%20and%20quantify%20uncertainties%20associated%20with%0Adistribution%20shifts.%20A%20distribution%20shift%20occurs%20when%20the%20underlying%0Adata-generating%20process%20changes%2C%20leading%20to%20a%20deviation%20in%20the%20model%27s%0Aperformance.%20The%20prediction%20interval%2C%20which%20captures%20the%20range%20of%20likely%0Aoutcomes%20for%20a%20given%20prediction%2C%20serves%20as%20a%20crucial%20tool%20for%20characterizing%0Auncertainties%20induced%20by%20their%20underlying%20distribution.%20In%20this%20paper%2C%20we%0Apropose%20methodologies%20for%20aggregating%20prediction%20intervals%20to%20obtain%20one%20with%0Aminimal%20width%20and%20adequate%20coverage%20on%20the%20target%20domain%20under%20unsupervised%0Adomain%20shift%2C%20under%20which%20we%20have%20labeled%20samples%20from%20a%20related%20source%20domain%0Aand%20unlabeled%20covariates%20from%20the%20target%20domain.%20Our%20analysis%20encompasses%0Ascenarios%20where%20the%20source%20and%20the%20target%20domain%20are%20related%20via%20i%29%20a%20bounded%0Adensity%20ratio%2C%20and%20ii%29%20a%20measure-preserving%20transformation.%20Our%20proposed%0Amethodologies%20are%20computationally%20efficient%20and%20easy%20to%20implement.%20Beyond%0Aillustrating%20the%20performance%20of%20our%20method%20through%20a%20real-world%20dataset%2C%20we%0Aalso%20delve%20into%20the%20theoretical%20details.%20This%20includes%20establishing%20rigorous%0Atheoretical%20guarantees%2C%20coupled%20with%20finite%20sample%20bounds%2C%20regarding%20the%0Acoverage%20and%20width%20of%20our%20prediction%20intervals.%20Our%20approach%20excels%20in%0Apractical%20applications%20and%20is%20underpinned%20by%20a%20solid%20theoretical%20framework%2C%0Aensuring%20its%20reliability%20and%20effectiveness%20across%20diverse%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10302v1&entry.124074799=Read"},
{"title": "Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey", "author": "Lauren Nicole DeLong and Ramon Fern\u00e1ndez Mir and Jacques D. Fleuriot", "abstract": "  Neurosymbolic AI is an increasingly active area of research that combines\nsymbolic reasoning methods with deep learning to leverage their complementary\nbenefits. As knowledge graphs are becoming a popular way to represent\nheterogeneous and multi-relational data, methods for reasoning on graph\nstructures have attempted to follow this neurosymbolic paradigm. Traditionally,\nsuch approaches have utilized either rule-based inference or generated\nrepresentative numerical embeddings from which patterns could be extracted.\nHowever, several recent studies have attempted to bridge this dichotomy to\ngenerate models that facilitate interpretability, maintain competitive\nperformance, and integrate expert knowledge. Therefore, we survey methods that\nperform neurosymbolic reasoning tasks on knowledge graphs and propose a novel\ntaxonomy by which we can classify them. Specifically, we propose three major\ncategories: (1) logically-informed embedding approaches, (2) embedding\napproaches with logical constraints, and (3) rule learning approaches.\nAlongside the taxonomy, we provide a tabular overview of the approaches and\nlinks to their source code, if available, for more direct comparison. Finally,\nwe discuss the unique characteristics and limitations of these methods, then\npropose several prospective directions toward which this field of research\ncould evolve.\n", "link": "http://arxiv.org/abs/2302.07200v3", "date": "2024-05-16", "relevancy": 1.4453, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5014}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neurosymbolic%20AI%20for%20Reasoning%20over%20Knowledge%20Graphs%3A%20A%20Survey&body=Title%3A%20Neurosymbolic%20AI%20for%20Reasoning%20over%20Knowledge%20Graphs%3A%20A%20Survey%0AAuthor%3A%20Lauren%20Nicole%20DeLong%20and%20Ramon%20Fern%C3%A1ndez%20Mir%20and%20Jacques%20D.%20Fleuriot%0AAbstract%3A%20%20%20Neurosymbolic%20AI%20is%20an%20increasingly%20active%20area%20of%20research%20that%20combines%0Asymbolic%20reasoning%20methods%20with%20deep%20learning%20to%20leverage%20their%20complementary%0Abenefits.%20As%20knowledge%20graphs%20are%20becoming%20a%20popular%20way%20to%20represent%0Aheterogeneous%20and%20multi-relational%20data%2C%20methods%20for%20reasoning%20on%20graph%0Astructures%20have%20attempted%20to%20follow%20this%20neurosymbolic%20paradigm.%20Traditionally%2C%0Asuch%20approaches%20have%20utilized%20either%20rule-based%20inference%20or%20generated%0Arepresentative%20numerical%20embeddings%20from%20which%20patterns%20could%20be%20extracted.%0AHowever%2C%20several%20recent%20studies%20have%20attempted%20to%20bridge%20this%20dichotomy%20to%0Agenerate%20models%20that%20facilitate%20interpretability%2C%20maintain%20competitive%0Aperformance%2C%20and%20integrate%20expert%20knowledge.%20Therefore%2C%20we%20survey%20methods%20that%0Aperform%20neurosymbolic%20reasoning%20tasks%20on%20knowledge%20graphs%20and%20propose%20a%20novel%0Ataxonomy%20by%20which%20we%20can%20classify%20them.%20Specifically%2C%20we%20propose%20three%20major%0Acategories%3A%20%281%29%20logically-informed%20embedding%20approaches%2C%20%282%29%20embedding%0Aapproaches%20with%20logical%20constraints%2C%20and%20%283%29%20rule%20learning%20approaches.%0AAlongside%20the%20taxonomy%2C%20we%20provide%20a%20tabular%20overview%20of%20the%20approaches%20and%0Alinks%20to%20their%20source%20code%2C%20if%20available%2C%20for%20more%20direct%20comparison.%20Finally%2C%0Awe%20discuss%20the%20unique%20characteristics%20and%20limitations%20of%20these%20methods%2C%20then%0Apropose%20several%20prospective%20directions%20toward%20which%20this%20field%20of%20research%0Acould%20evolve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.07200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurosymbolic%2520AI%2520for%2520Reasoning%2520over%2520Knowledge%2520Graphs%253A%2520A%2520Survey%26entry.906535625%3DLauren%2520Nicole%2520DeLong%2520and%2520Ramon%2520Fern%25C3%25A1ndez%2520Mir%2520and%2520Jacques%2520D.%2520Fleuriot%26entry.1292438233%3D%2520%2520Neurosymbolic%2520AI%2520is%2520an%2520increasingly%2520active%2520area%2520of%2520research%2520that%2520combines%250Asymbolic%2520reasoning%2520methods%2520with%2520deep%2520learning%2520to%2520leverage%2520their%2520complementary%250Abenefits.%2520As%2520knowledge%2520graphs%2520are%2520becoming%2520a%2520popular%2520way%2520to%2520represent%250Aheterogeneous%2520and%2520multi-relational%2520data%252C%2520methods%2520for%2520reasoning%2520on%2520graph%250Astructures%2520have%2520attempted%2520to%2520follow%2520this%2520neurosymbolic%2520paradigm.%2520Traditionally%252C%250Asuch%2520approaches%2520have%2520utilized%2520either%2520rule-based%2520inference%2520or%2520generated%250Arepresentative%2520numerical%2520embeddings%2520from%2520which%2520patterns%2520could%2520be%2520extracted.%250AHowever%252C%2520several%2520recent%2520studies%2520have%2520attempted%2520to%2520bridge%2520this%2520dichotomy%2520to%250Agenerate%2520models%2520that%2520facilitate%2520interpretability%252C%2520maintain%2520competitive%250Aperformance%252C%2520and%2520integrate%2520expert%2520knowledge.%2520Therefore%252C%2520we%2520survey%2520methods%2520that%250Aperform%2520neurosymbolic%2520reasoning%2520tasks%2520on%2520knowledge%2520graphs%2520and%2520propose%2520a%2520novel%250Ataxonomy%2520by%2520which%2520we%2520can%2520classify%2520them.%2520Specifically%252C%2520we%2520propose%2520three%2520major%250Acategories%253A%2520%25281%2529%2520logically-informed%2520embedding%2520approaches%252C%2520%25282%2529%2520embedding%250Aapproaches%2520with%2520logical%2520constraints%252C%2520and%2520%25283%2529%2520rule%2520learning%2520approaches.%250AAlongside%2520the%2520taxonomy%252C%2520we%2520provide%2520a%2520tabular%2520overview%2520of%2520the%2520approaches%2520and%250Alinks%2520to%2520their%2520source%2520code%252C%2520if%2520available%252C%2520for%2520more%2520direct%2520comparison.%2520Finally%252C%250Awe%2520discuss%2520the%2520unique%2520characteristics%2520and%2520limitations%2520of%2520these%2520methods%252C%2520then%250Apropose%2520several%2520prospective%2520directions%2520toward%2520which%2520this%2520field%2520of%2520research%250Acould%2520evolve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.07200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neurosymbolic%20AI%20for%20Reasoning%20over%20Knowledge%20Graphs%3A%20A%20Survey&entry.906535625=Lauren%20Nicole%20DeLong%20and%20Ramon%20Fern%C3%A1ndez%20Mir%20and%20Jacques%20D.%20Fleuriot&entry.1292438233=%20%20Neurosymbolic%20AI%20is%20an%20increasingly%20active%20area%20of%20research%20that%20combines%0Asymbolic%20reasoning%20methods%20with%20deep%20learning%20to%20leverage%20their%20complementary%0Abenefits.%20As%20knowledge%20graphs%20are%20becoming%20a%20popular%20way%20to%20represent%0Aheterogeneous%20and%20multi-relational%20data%2C%20methods%20for%20reasoning%20on%20graph%0Astructures%20have%20attempted%20to%20follow%20this%20neurosymbolic%20paradigm.%20Traditionally%2C%0Asuch%20approaches%20have%20utilized%20either%20rule-based%20inference%20or%20generated%0Arepresentative%20numerical%20embeddings%20from%20which%20patterns%20could%20be%20extracted.%0AHowever%2C%20several%20recent%20studies%20have%20attempted%20to%20bridge%20this%20dichotomy%20to%0Agenerate%20models%20that%20facilitate%20interpretability%2C%20maintain%20competitive%0Aperformance%2C%20and%20integrate%20expert%20knowledge.%20Therefore%2C%20we%20survey%20methods%20that%0Aperform%20neurosymbolic%20reasoning%20tasks%20on%20knowledge%20graphs%20and%20propose%20a%20novel%0Ataxonomy%20by%20which%20we%20can%20classify%20them.%20Specifically%2C%20we%20propose%20three%20major%0Acategories%3A%20%281%29%20logically-informed%20embedding%20approaches%2C%20%282%29%20embedding%0Aapproaches%20with%20logical%20constraints%2C%20and%20%283%29%20rule%20learning%20approaches.%0AAlongside%20the%20taxonomy%2C%20we%20provide%20a%20tabular%20overview%20of%20the%20approaches%20and%0Alinks%20to%20their%20source%20code%2C%20if%20available%2C%20for%20more%20direct%20comparison.%20Finally%2C%0Awe%20discuss%20the%20unique%20characteristics%20and%20limitations%20of%20these%20methods%2C%20then%0Apropose%20several%20prospective%20directions%20toward%20which%20this%20field%20of%20research%0Acould%20evolve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07200v3&entry.124074799=Read"},
{"title": "Graph Attention-Based Symmetry Constraint Extraction for Analog Circuits", "author": "Qi Xu and Lijie Wang and Jing Wang and Lin Cheng and Song Chen and Yi Kang", "abstract": "  In recent years, analog circuits have received extensive attention and are\nwidely used in many emerging applications. The high demand for analog circuits\nnecessitates shorter circuit design cycles. To achieve the desired performance\nand specifications, various geometrical symmetry constraints must be carefully\nconsidered during the analog layout process. However, the manual labeling of\nthese constraints by experienced analog engineers is a laborious and\ntime-consuming process. To handle the costly runtime issue, we propose a\ngraph-based learning framework to automatically extract symmetric constraints\nin analog circuit layout. The proposed framework leverages the connection\ncharacteristics of circuits and the devices' information to learn the general\nrules of symmetric constraints, which effectively facilitates the extraction of\ndevice-level constraints on circuit netlists. The experimental results\ndemonstrate that compared to state-of-the-art symmetric constraint detection\napproaches, our framework achieves higher accuracy and F1-score.\n", "link": "http://arxiv.org/abs/2312.14405v2", "date": "2024-05-16", "relevancy": 1.4353, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5223}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4326}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Attention-Based%20Symmetry%20Constraint%20Extraction%20for%20Analog%20Circuits&body=Title%3A%20Graph%20Attention-Based%20Symmetry%20Constraint%20Extraction%20for%20Analog%20Circuits%0AAuthor%3A%20Qi%20Xu%20and%20Lijie%20Wang%20and%20Jing%20Wang%20and%20Lin%20Cheng%20and%20Song%20Chen%20and%20Yi%20Kang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20analog%20circuits%20have%20received%20extensive%20attention%20and%20are%0Awidely%20used%20in%20many%20emerging%20applications.%20The%20high%20demand%20for%20analog%20circuits%0Anecessitates%20shorter%20circuit%20design%20cycles.%20To%20achieve%20the%20desired%20performance%0Aand%20specifications%2C%20various%20geometrical%20symmetry%20constraints%20must%20be%20carefully%0Aconsidered%20during%20the%20analog%20layout%20process.%20However%2C%20the%20manual%20labeling%20of%0Athese%20constraints%20by%20experienced%20analog%20engineers%20is%20a%20laborious%20and%0Atime-consuming%20process.%20To%20handle%20the%20costly%20runtime%20issue%2C%20we%20propose%20a%0Agraph-based%20learning%20framework%20to%20automatically%20extract%20symmetric%20constraints%0Ain%20analog%20circuit%20layout.%20The%20proposed%20framework%20leverages%20the%20connection%0Acharacteristics%20of%20circuits%20and%20the%20devices%27%20information%20to%20learn%20the%20general%0Arules%20of%20symmetric%20constraints%2C%20which%20effectively%20facilitates%20the%20extraction%20of%0Adevice-level%20constraints%20on%20circuit%20netlists.%20The%20experimental%20results%0Ademonstrate%20that%20compared%20to%20state-of-the-art%20symmetric%20constraint%20detection%0Aapproaches%2C%20our%20framework%20achieves%20higher%20accuracy%20and%20F1-score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Attention-Based%2520Symmetry%2520Constraint%2520Extraction%2520for%2520Analog%2520Circuits%26entry.906535625%3DQi%2520Xu%2520and%2520Lijie%2520Wang%2520and%2520Jing%2520Wang%2520and%2520Lin%2520Cheng%2520and%2520Song%2520Chen%2520and%2520Yi%2520Kang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520analog%2520circuits%2520have%2520received%2520extensive%2520attention%2520and%2520are%250Awidely%2520used%2520in%2520many%2520emerging%2520applications.%2520The%2520high%2520demand%2520for%2520analog%2520circuits%250Anecessitates%2520shorter%2520circuit%2520design%2520cycles.%2520To%2520achieve%2520the%2520desired%2520performance%250Aand%2520specifications%252C%2520various%2520geometrical%2520symmetry%2520constraints%2520must%2520be%2520carefully%250Aconsidered%2520during%2520the%2520analog%2520layout%2520process.%2520However%252C%2520the%2520manual%2520labeling%2520of%250Athese%2520constraints%2520by%2520experienced%2520analog%2520engineers%2520is%2520a%2520laborious%2520and%250Atime-consuming%2520process.%2520To%2520handle%2520the%2520costly%2520runtime%2520issue%252C%2520we%2520propose%2520a%250Agraph-based%2520learning%2520framework%2520to%2520automatically%2520extract%2520symmetric%2520constraints%250Ain%2520analog%2520circuit%2520layout.%2520The%2520proposed%2520framework%2520leverages%2520the%2520connection%250Acharacteristics%2520of%2520circuits%2520and%2520the%2520devices%2527%2520information%2520to%2520learn%2520the%2520general%250Arules%2520of%2520symmetric%2520constraints%252C%2520which%2520effectively%2520facilitates%2520the%2520extraction%2520of%250Adevice-level%2520constraints%2520on%2520circuit%2520netlists.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520compared%2520to%2520state-of-the-art%2520symmetric%2520constraint%2520detection%250Aapproaches%252C%2520our%2520framework%2520achieves%2520higher%2520accuracy%2520and%2520F1-score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Attention-Based%20Symmetry%20Constraint%20Extraction%20for%20Analog%20Circuits&entry.906535625=Qi%20Xu%20and%20Lijie%20Wang%20and%20Jing%20Wang%20and%20Lin%20Cheng%20and%20Song%20Chen%20and%20Yi%20Kang&entry.1292438233=%20%20In%20recent%20years%2C%20analog%20circuits%20have%20received%20extensive%20attention%20and%20are%0Awidely%20used%20in%20many%20emerging%20applications.%20The%20high%20demand%20for%20analog%20circuits%0Anecessitates%20shorter%20circuit%20design%20cycles.%20To%20achieve%20the%20desired%20performance%0Aand%20specifications%2C%20various%20geometrical%20symmetry%20constraints%20must%20be%20carefully%0Aconsidered%20during%20the%20analog%20layout%20process.%20However%2C%20the%20manual%20labeling%20of%0Athese%20constraints%20by%20experienced%20analog%20engineers%20is%20a%20laborious%20and%0Atime-consuming%20process.%20To%20handle%20the%20costly%20runtime%20issue%2C%20we%20propose%20a%0Agraph-based%20learning%20framework%20to%20automatically%20extract%20symmetric%20constraints%0Ain%20analog%20circuit%20layout.%20The%20proposed%20framework%20leverages%20the%20connection%0Acharacteristics%20of%20circuits%20and%20the%20devices%27%20information%20to%20learn%20the%20general%0Arules%20of%20symmetric%20constraints%2C%20which%20effectively%20facilitates%20the%20extraction%20of%0Adevice-level%20constraints%20on%20circuit%20netlists.%20The%20experimental%20results%0Ademonstrate%20that%20compared%20to%20state-of-the-art%20symmetric%20constraint%20detection%0Aapproaches%2C%20our%20framework%20achieves%20higher%20accuracy%20and%20F1-score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14405v2&entry.124074799=Read"},
{"title": "Grounding DINO 1.5: Advance the \"Edge\" of Open-Set Object Detection", "author": "Tianhe Ren and Qing Jiang and Shilong Liu and Zhaoyang Zeng and Wenlong Liu and Han Gao and Hongjie Huang and Zhengyu Ma and Xiaoke Jiang and Yihao Chen and Yuda Xiong and Hao Zhang and Feng Li and Peijun Tang and Kent Yu and Lei Zhang", "abstract": "  This paper introduces Grounding DINO 1.5, a suite of advanced open-set object\ndetection models developed by IDEA Research, which aims to advance the \"Edge\"\nof open-set object detection. The suite encompasses two models: Grounding DINO\n1.5 Pro, a high-performance model designed for stronger generalization\ncapability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an\nefficient model optimized for faster speed demanded in many applications\nrequiring edge deployment. The Grounding DINO 1.5 Pro model advances its\npredecessor by scaling up the model architecture, integrating an enhanced\nvision backbone, and expanding the training dataset to over 20 million images\nwith grounding annotations, thereby achieving a richer semantic understanding.\nThe Grounding DINO 1.5 Edge model, while designed for efficiency with reduced\nfeature scales, maintains robust detection capabilities by being trained on the\nsame comprehensive dataset. Empirical results demonstrate the effectiveness of\nGrounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP\non the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot\ntransfer benchmark, setting new records for open-set object detection.\nFurthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT,\nachieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP\non the LVIS-minival benchmark, making it more suitable for edge computing\nscenarios. Model examples and demos with API will be released at\nhttps://github.com/IDEA-Research/Grounding-DINO-1.5-API\n", "link": "http://arxiv.org/abs/2405.10300v1", "date": "2024-05-16", "relevancy": 1.6026, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5409}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5264}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20DINO%201.5%3A%20Advance%20the%20%22Edge%22%20of%20Open-Set%20Object%20Detection&body=Title%3A%20Grounding%20DINO%201.5%3A%20Advance%20the%20%22Edge%22%20of%20Open-Set%20Object%20Detection%0AAuthor%3A%20Tianhe%20Ren%20and%20Qing%20Jiang%20and%20Shilong%20Liu%20and%20Zhaoyang%20Zeng%20and%20Wenlong%20Liu%20and%20Han%20Gao%20and%20Hongjie%20Huang%20and%20Zhengyu%20Ma%20and%20Xiaoke%20Jiang%20and%20Yihao%20Chen%20and%20Yuda%20Xiong%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Peijun%20Tang%20and%20Kent%20Yu%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20Grounding%20DINO%201.5%2C%20a%20suite%20of%20advanced%20open-set%20object%0Adetection%20models%20developed%20by%20IDEA%20Research%2C%20which%20aims%20to%20advance%20the%20%22Edge%22%0Aof%20open-set%20object%20detection.%20The%20suite%20encompasses%20two%20models%3A%20Grounding%20DINO%0A1.5%20Pro%2C%20a%20high-performance%20model%20designed%20for%20stronger%20generalization%0Acapability%20across%20a%20wide%20range%20of%20scenarios%2C%20and%20Grounding%20DINO%201.5%20Edge%2C%20an%0Aefficient%20model%20optimized%20for%20faster%20speed%20demanded%20in%20many%20applications%0Arequiring%20edge%20deployment.%20The%20Grounding%20DINO%201.5%20Pro%20model%20advances%20its%0Apredecessor%20by%20scaling%20up%20the%20model%20architecture%2C%20integrating%20an%20enhanced%0Avision%20backbone%2C%20and%20expanding%20the%20training%20dataset%20to%20over%2020%20million%20images%0Awith%20grounding%20annotations%2C%20thereby%20achieving%20a%20richer%20semantic%20understanding.%0AThe%20Grounding%20DINO%201.5%20Edge%20model%2C%20while%20designed%20for%20efficiency%20with%20reduced%0Afeature%20scales%2C%20maintains%20robust%20detection%20capabilities%20by%20being%20trained%20on%20the%0Asame%20comprehensive%20dataset.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0AGrounding%20DINO%201.5%2C%20with%20the%20Grounding%20DINO%201.5%20Pro%20model%20attaining%20a%2054.3%20AP%0Aon%20the%20COCO%20detection%20benchmark%20and%20a%2055.7%20AP%20on%20the%20LVIS-minival%20zero-shot%0Atransfer%20benchmark%2C%20setting%20new%20records%20for%20open-set%20object%20detection.%0AFurthermore%2C%20the%20Grounding%20DINO%201.5%20Edge%20model%2C%20when%20optimized%20with%20TensorRT%2C%0Aachieves%20a%20speed%20of%2075.2%20FPS%20while%20attaining%20a%20zero-shot%20performance%20of%2036.2%20AP%0Aon%20the%20LVIS-minival%20benchmark%2C%20making%20it%20more%20suitable%20for%20edge%20computing%0Ascenarios.%20Model%20examples%20and%20demos%20with%20API%20will%20be%20released%20at%0Ahttps%3A//github.com/IDEA-Research/Grounding-DINO-1.5-API%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520DINO%25201.5%253A%2520Advance%2520the%2520%2522Edge%2522%2520of%2520Open-Set%2520Object%2520Detection%26entry.906535625%3DTianhe%2520Ren%2520and%2520Qing%2520Jiang%2520and%2520Shilong%2520Liu%2520and%2520Zhaoyang%2520Zeng%2520and%2520Wenlong%2520Liu%2520and%2520Han%2520Gao%2520and%2520Hongjie%2520Huang%2520and%2520Zhengyu%2520Ma%2520and%2520Xiaoke%2520Jiang%2520and%2520Yihao%2520Chen%2520and%2520Yuda%2520Xiong%2520and%2520Hao%2520Zhang%2520and%2520Feng%2520Li%2520and%2520Peijun%2520Tang%2520and%2520Kent%2520Yu%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Grounding%2520DINO%25201.5%252C%2520a%2520suite%2520of%2520advanced%2520open-set%2520object%250Adetection%2520models%2520developed%2520by%2520IDEA%2520Research%252C%2520which%2520aims%2520to%2520advance%2520the%2520%2522Edge%2522%250Aof%2520open-set%2520object%2520detection.%2520The%2520suite%2520encompasses%2520two%2520models%253A%2520Grounding%2520DINO%250A1.5%2520Pro%252C%2520a%2520high-performance%2520model%2520designed%2520for%2520stronger%2520generalization%250Acapability%2520across%2520a%2520wide%2520range%2520of%2520scenarios%252C%2520and%2520Grounding%2520DINO%25201.5%2520Edge%252C%2520an%250Aefficient%2520model%2520optimized%2520for%2520faster%2520speed%2520demanded%2520in%2520many%2520applications%250Arequiring%2520edge%2520deployment.%2520The%2520Grounding%2520DINO%25201.5%2520Pro%2520model%2520advances%2520its%250Apredecessor%2520by%2520scaling%2520up%2520the%2520model%2520architecture%252C%2520integrating%2520an%2520enhanced%250Avision%2520backbone%252C%2520and%2520expanding%2520the%2520training%2520dataset%2520to%2520over%252020%2520million%2520images%250Awith%2520grounding%2520annotations%252C%2520thereby%2520achieving%2520a%2520richer%2520semantic%2520understanding.%250AThe%2520Grounding%2520DINO%25201.5%2520Edge%2520model%252C%2520while%2520designed%2520for%2520efficiency%2520with%2520reduced%250Afeature%2520scales%252C%2520maintains%2520robust%2520detection%2520capabilities%2520by%2520being%2520trained%2520on%2520the%250Asame%2520comprehensive%2520dataset.%2520Empirical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250AGrounding%2520DINO%25201.5%252C%2520with%2520the%2520Grounding%2520DINO%25201.5%2520Pro%2520model%2520attaining%2520a%252054.3%2520AP%250Aon%2520the%2520COCO%2520detection%2520benchmark%2520and%2520a%252055.7%2520AP%2520on%2520the%2520LVIS-minival%2520zero-shot%250Atransfer%2520benchmark%252C%2520setting%2520new%2520records%2520for%2520open-set%2520object%2520detection.%250AFurthermore%252C%2520the%2520Grounding%2520DINO%25201.5%2520Edge%2520model%252C%2520when%2520optimized%2520with%2520TensorRT%252C%250Aachieves%2520a%2520speed%2520of%252075.2%2520FPS%2520while%2520attaining%2520a%2520zero-shot%2520performance%2520of%252036.2%2520AP%250Aon%2520the%2520LVIS-minival%2520benchmark%252C%2520making%2520it%2520more%2520suitable%2520for%2520edge%2520computing%250Ascenarios.%2520Model%2520examples%2520and%2520demos%2520with%2520API%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/IDEA-Research/Grounding-DINO-1.5-API%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20DINO%201.5%3A%20Advance%20the%20%22Edge%22%20of%20Open-Set%20Object%20Detection&entry.906535625=Tianhe%20Ren%20and%20Qing%20Jiang%20and%20Shilong%20Liu%20and%20Zhaoyang%20Zeng%20and%20Wenlong%20Liu%20and%20Han%20Gao%20and%20Hongjie%20Huang%20and%20Zhengyu%20Ma%20and%20Xiaoke%20Jiang%20and%20Yihao%20Chen%20and%20Yuda%20Xiong%20and%20Hao%20Zhang%20and%20Feng%20Li%20and%20Peijun%20Tang%20and%20Kent%20Yu%20and%20Lei%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20Grounding%20DINO%201.5%2C%20a%20suite%20of%20advanced%20open-set%20object%0Adetection%20models%20developed%20by%20IDEA%20Research%2C%20which%20aims%20to%20advance%20the%20%22Edge%22%0Aof%20open-set%20object%20detection.%20The%20suite%20encompasses%20two%20models%3A%20Grounding%20DINO%0A1.5%20Pro%2C%20a%20high-performance%20model%20designed%20for%20stronger%20generalization%0Acapability%20across%20a%20wide%20range%20of%20scenarios%2C%20and%20Grounding%20DINO%201.5%20Edge%2C%20an%0Aefficient%20model%20optimized%20for%20faster%20speed%20demanded%20in%20many%20applications%0Arequiring%20edge%20deployment.%20The%20Grounding%20DINO%201.5%20Pro%20model%20advances%20its%0Apredecessor%20by%20scaling%20up%20the%20model%20architecture%2C%20integrating%20an%20enhanced%0Avision%20backbone%2C%20and%20expanding%20the%20training%20dataset%20to%20over%2020%20million%20images%0Awith%20grounding%20annotations%2C%20thereby%20achieving%20a%20richer%20semantic%20understanding.%0AThe%20Grounding%20DINO%201.5%20Edge%20model%2C%20while%20designed%20for%20efficiency%20with%20reduced%0Afeature%20scales%2C%20maintains%20robust%20detection%20capabilities%20by%20being%20trained%20on%20the%0Asame%20comprehensive%20dataset.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0AGrounding%20DINO%201.5%2C%20with%20the%20Grounding%20DINO%201.5%20Pro%20model%20attaining%20a%2054.3%20AP%0Aon%20the%20COCO%20detection%20benchmark%20and%20a%2055.7%20AP%20on%20the%20LVIS-minival%20zero-shot%0Atransfer%20benchmark%2C%20setting%20new%20records%20for%20open-set%20object%20detection.%0AFurthermore%2C%20the%20Grounding%20DINO%201.5%20Edge%20model%2C%20when%20optimized%20with%20TensorRT%2C%0Aachieves%20a%20speed%20of%2075.2%20FPS%20while%20attaining%20a%20zero-shot%20performance%20of%2036.2%20AP%0Aon%20the%20LVIS-minival%20benchmark%2C%20making%20it%20more%20suitable%20for%20edge%20computing%0Ascenarios.%20Model%20examples%20and%20demos%20with%20API%20will%20be%20released%20at%0Ahttps%3A//github.com/IDEA-Research/Grounding-DINO-1.5-API%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10300v1&entry.124074799=Read"},
{"title": "Architectures and random properties of symplectic quantum circuits", "author": "Diego Garc\u00eda-Mart\u00edn and Paolo Braccia and M. Cerezo", "abstract": "  Parametrized and random unitary (or orthogonal) $n$-qubit circuits play a\ncentral role in quantum information. As such, one could naturally assume that\ncircuits implementing symplectic transformation would attract similar\nattention. However, this is not the case, as $\\mathbb{SP}(d/2)$ -- the group of\n$d\\times d$ unitary symplectic matrices -- has thus far been overlooked. In\nthis work, we aim at starting to right this wrong. We begin by presenting a\nuniversal set of generators $\\mathcal{G}$ for the symplectic algebra\n$i\\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators acting\non neighboring sites in a one-dimensional lattice. Here, we uncover two\ncritical differences between such set, and equivalent ones for unitary and\northogonal circuits. Namely, we find that the operators in $\\mathcal{G}$ cannot\ngenerate arbitrary local symplectic unitaries and that they are not\ntranslationally invariant. We then review the Schur-Weyl duality between the\nsymplectic group and the Brauer algebra, and use tools from Weingarten calculus\nto prove that Pauli measurements at the output of Haar random symplectic\ncircuits can converge to Gaussian processes. As a by-product, such analysis\nprovides us with concentration bounds for Pauli measurements in circuits that\nform $t$-designs over $\\mathbb{SP}(d/2)$. To finish, we present tensor-network\ntools to analyze shallow random symplectic circuits, and we use these to\nnumerically show that computational-basis measurements anti-concentrate at\nlogarithmic depth.\n", "link": "http://arxiv.org/abs/2405.10264v1", "date": "2024-05-16", "relevancy": 1.5353, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3869}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectures%20and%20random%20properties%20of%20symplectic%20quantum%20circuits&body=Title%3A%20Architectures%20and%20random%20properties%20of%20symplectic%20quantum%20circuits%0AAuthor%3A%20Diego%20Garc%C3%ADa-Mart%C3%ADn%20and%20Paolo%20Braccia%20and%20M.%20Cerezo%0AAbstract%3A%20%20%20Parametrized%20and%20random%20unitary%20%28or%20orthogonal%29%20%24n%24-qubit%20circuits%20play%20a%0Acentral%20role%20in%20quantum%20information.%20As%20such%2C%20one%20could%20naturally%20assume%20that%0Acircuits%20implementing%20symplectic%20transformation%20would%20attract%20similar%0Aattention.%20However%2C%20this%20is%20not%20the%20case%2C%20as%20%24%5Cmathbb%7BSP%7D%28d/2%29%24%20--%20the%20group%20of%0A%24d%5Ctimes%20d%24%20unitary%20symplectic%20matrices%20--%20has%20thus%20far%20been%20overlooked.%20In%0Athis%20work%2C%20we%20aim%20at%20starting%20to%20right%20this%20wrong.%20We%20begin%20by%20presenting%20a%0Auniversal%20set%20of%20generators%20%24%5Cmathcal%7BG%7D%24%20for%20the%20symplectic%20algebra%0A%24i%5Cmathfrak%7Bsp%7D%28d/2%29%24%2C%20consisting%20of%20one-%20and%20two-qubit%20Pauli%20operators%20acting%0Aon%20neighboring%20sites%20in%20a%20one-dimensional%20lattice.%20Here%2C%20we%20uncover%20two%0Acritical%20differences%20between%20such%20set%2C%20and%20equivalent%20ones%20for%20unitary%20and%0Aorthogonal%20circuits.%20Namely%2C%20we%20find%20that%20the%20operators%20in%20%24%5Cmathcal%7BG%7D%24%20cannot%0Agenerate%20arbitrary%20local%20symplectic%20unitaries%20and%20that%20they%20are%20not%0Atranslationally%20invariant.%20We%20then%20review%20the%20Schur-Weyl%20duality%20between%20the%0Asymplectic%20group%20and%20the%20Brauer%20algebra%2C%20and%20use%20tools%20from%20Weingarten%20calculus%0Ato%20prove%20that%20Pauli%20measurements%20at%20the%20output%20of%20Haar%20random%20symplectic%0Acircuits%20can%20converge%20to%20Gaussian%20processes.%20As%20a%20by-product%2C%20such%20analysis%0Aprovides%20us%20with%20concentration%20bounds%20for%20Pauli%20measurements%20in%20circuits%20that%0Aform%20%24t%24-designs%20over%20%24%5Cmathbb%7BSP%7D%28d/2%29%24.%20To%20finish%2C%20we%20present%20tensor-network%0Atools%20to%20analyze%20shallow%20random%20symplectic%20circuits%2C%20and%20we%20use%20these%20to%0Anumerically%20show%20that%20computational-basis%20measurements%20anti-concentrate%20at%0Alogarithmic%20depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectures%2520and%2520random%2520properties%2520of%2520symplectic%2520quantum%2520circuits%26entry.906535625%3DDiego%2520Garc%25C3%25ADa-Mart%25C3%25ADn%2520and%2520Paolo%2520Braccia%2520and%2520M.%2520Cerezo%26entry.1292438233%3D%2520%2520Parametrized%2520and%2520random%2520unitary%2520%2528or%2520orthogonal%2529%2520%2524n%2524-qubit%2520circuits%2520play%2520a%250Acentral%2520role%2520in%2520quantum%2520information.%2520As%2520such%252C%2520one%2520could%2520naturally%2520assume%2520that%250Acircuits%2520implementing%2520symplectic%2520transformation%2520would%2520attract%2520similar%250Aattention.%2520However%252C%2520this%2520is%2520not%2520the%2520case%252C%2520as%2520%2524%255Cmathbb%257BSP%257D%2528d/2%2529%2524%2520--%2520the%2520group%2520of%250A%2524d%255Ctimes%2520d%2524%2520unitary%2520symplectic%2520matrices%2520--%2520has%2520thus%2520far%2520been%2520overlooked.%2520In%250Athis%2520work%252C%2520we%2520aim%2520at%2520starting%2520to%2520right%2520this%2520wrong.%2520We%2520begin%2520by%2520presenting%2520a%250Auniversal%2520set%2520of%2520generators%2520%2524%255Cmathcal%257BG%257D%2524%2520for%2520the%2520symplectic%2520algebra%250A%2524i%255Cmathfrak%257Bsp%257D%2528d/2%2529%2524%252C%2520consisting%2520of%2520one-%2520and%2520two-qubit%2520Pauli%2520operators%2520acting%250Aon%2520neighboring%2520sites%2520in%2520a%2520one-dimensional%2520lattice.%2520Here%252C%2520we%2520uncover%2520two%250Acritical%2520differences%2520between%2520such%2520set%252C%2520and%2520equivalent%2520ones%2520for%2520unitary%2520and%250Aorthogonal%2520circuits.%2520Namely%252C%2520we%2520find%2520that%2520the%2520operators%2520in%2520%2524%255Cmathcal%257BG%257D%2524%2520cannot%250Agenerate%2520arbitrary%2520local%2520symplectic%2520unitaries%2520and%2520that%2520they%2520are%2520not%250Atranslationally%2520invariant.%2520We%2520then%2520review%2520the%2520Schur-Weyl%2520duality%2520between%2520the%250Asymplectic%2520group%2520and%2520the%2520Brauer%2520algebra%252C%2520and%2520use%2520tools%2520from%2520Weingarten%2520calculus%250Ato%2520prove%2520that%2520Pauli%2520measurements%2520at%2520the%2520output%2520of%2520Haar%2520random%2520symplectic%250Acircuits%2520can%2520converge%2520to%2520Gaussian%2520processes.%2520As%2520a%2520by-product%252C%2520such%2520analysis%250Aprovides%2520us%2520with%2520concentration%2520bounds%2520for%2520Pauli%2520measurements%2520in%2520circuits%2520that%250Aform%2520%2524t%2524-designs%2520over%2520%2524%255Cmathbb%257BSP%257D%2528d/2%2529%2524.%2520To%2520finish%252C%2520we%2520present%2520tensor-network%250Atools%2520to%2520analyze%2520shallow%2520random%2520symplectic%2520circuits%252C%2520and%2520we%2520use%2520these%2520to%250Anumerically%2520show%2520that%2520computational-basis%2520measurements%2520anti-concentrate%2520at%250Alogarithmic%2520depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectures%20and%20random%20properties%20of%20symplectic%20quantum%20circuits&entry.906535625=Diego%20Garc%C3%ADa-Mart%C3%ADn%20and%20Paolo%20Braccia%20and%20M.%20Cerezo&entry.1292438233=%20%20Parametrized%20and%20random%20unitary%20%28or%20orthogonal%29%20%24n%24-qubit%20circuits%20play%20a%0Acentral%20role%20in%20quantum%20information.%20As%20such%2C%20one%20could%20naturally%20assume%20that%0Acircuits%20implementing%20symplectic%20transformation%20would%20attract%20similar%0Aattention.%20However%2C%20this%20is%20not%20the%20case%2C%20as%20%24%5Cmathbb%7BSP%7D%28d/2%29%24%20--%20the%20group%20of%0A%24d%5Ctimes%20d%24%20unitary%20symplectic%20matrices%20--%20has%20thus%20far%20been%20overlooked.%20In%0Athis%20work%2C%20we%20aim%20at%20starting%20to%20right%20this%20wrong.%20We%20begin%20by%20presenting%20a%0Auniversal%20set%20of%20generators%20%24%5Cmathcal%7BG%7D%24%20for%20the%20symplectic%20algebra%0A%24i%5Cmathfrak%7Bsp%7D%28d/2%29%24%2C%20consisting%20of%20one-%20and%20two-qubit%20Pauli%20operators%20acting%0Aon%20neighboring%20sites%20in%20a%20one-dimensional%20lattice.%20Here%2C%20we%20uncover%20two%0Acritical%20differences%20between%20such%20set%2C%20and%20equivalent%20ones%20for%20unitary%20and%0Aorthogonal%20circuits.%20Namely%2C%20we%20find%20that%20the%20operators%20in%20%24%5Cmathcal%7BG%7D%24%20cannot%0Agenerate%20arbitrary%20local%20symplectic%20unitaries%20and%20that%20they%20are%20not%0Atranslationally%20invariant.%20We%20then%20review%20the%20Schur-Weyl%20duality%20between%20the%0Asymplectic%20group%20and%20the%20Brauer%20algebra%2C%20and%20use%20tools%20from%20Weingarten%20calculus%0Ato%20prove%20that%20Pauli%20measurements%20at%20the%20output%20of%20Haar%20random%20symplectic%0Acircuits%20can%20converge%20to%20Gaussian%20processes.%20As%20a%20by-product%2C%20such%20analysis%0Aprovides%20us%20with%20concentration%20bounds%20for%20Pauli%20measurements%20in%20circuits%20that%0Aform%20%24t%24-designs%20over%20%24%5Cmathbb%7BSP%7D%28d/2%29%24.%20To%20finish%2C%20we%20present%20tensor-network%0Atools%20to%20analyze%20shallow%20random%20symplectic%20circuits%2C%20and%20we%20use%20these%20to%0Anumerically%20show%20that%20computational-basis%20measurements%20anti-concentrate%20at%0Alogarithmic%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10264v1&entry.124074799=Read"},
{"title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language\n  Models", "author": "Kuofeng Gao and Yang Bai and Jiawang Bai and Yong Yang and Shu-Tao Xia", "abstract": "  Multi-modal Large Language Models (MLLMs) have recently achieved enhanced\nperformance across various vision-language tasks including visual grounding\ncapabilities. However, the adversarial robustness of visual grounding remains\nunexplored in MLLMs. To fill this gap, we use referring expression\ncomprehension (REC) as an example task in visual grounding and propose three\nadversarial attack paradigms as follows. Firstly, untargeted adversarial\nattacks induce MLLMs to generate incorrect bounding boxes for each object.\nBesides, exclusive targeted adversarial attacks cause all generated outputs to\nthe same target bounding box. In addition, permuted targeted adversarial\nattacks aim to permute all bounding boxes among different objects within a\nsingle image. Extensive experiments demonstrate that the proposed methods can\nsuccessfully attack visual grounding capabilities of MLLMs. Our methods not\nonly provide a new perspective for designing novel attacks but also serve as a\nstrong baseline for improving the adversarial robustness for visual grounding\nof MLLMs.\n", "link": "http://arxiv.org/abs/2405.09981v1", "date": "2024-05-16", "relevancy": 1.6347, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20for%20Visual%20Grounding%20of%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Adversarial%20Robustness%20for%20Visual%20Grounding%20of%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20enhanced%0Aperformance%20across%20various%20vision-language%20tasks%20including%20visual%20grounding%0Acapabilities.%20However%2C%20the%20adversarial%20robustness%20of%20visual%20grounding%20remains%0Aunexplored%20in%20MLLMs.%20To%20fill%20this%20gap%2C%20we%20use%20referring%20expression%0Acomprehension%20%28REC%29%20as%20an%20example%20task%20in%20visual%20grounding%20and%20propose%20three%0Aadversarial%20attack%20paradigms%20as%20follows.%20Firstly%2C%20untargeted%20adversarial%0Aattacks%20induce%20MLLMs%20to%20generate%20incorrect%20bounding%20boxes%20for%20each%20object.%0ABesides%2C%20exclusive%20targeted%20adversarial%20attacks%20cause%20all%20generated%20outputs%20to%0Athe%20same%20target%20bounding%20box.%20In%20addition%2C%20permuted%20targeted%20adversarial%0Aattacks%20aim%20to%20permute%20all%20bounding%20boxes%20among%20different%20objects%20within%20a%0Asingle%20image.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20methods%20can%0Asuccessfully%20attack%20visual%20grounding%20capabilities%20of%20MLLMs.%20Our%20methods%20not%0Aonly%20provide%20a%20new%20perspective%20for%20designing%20novel%20attacks%20but%20also%20serve%20as%20a%0Astrong%20baseline%20for%20improving%20the%20adversarial%20robustness%20for%20visual%20grounding%0Aof%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520for%2520Visual%2520Grounding%2520of%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Yang%2520Bai%2520and%2520Jiawang%2520Bai%2520and%2520Yong%2520Yang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520achieved%2520enhanced%250Aperformance%2520across%2520various%2520vision-language%2520tasks%2520including%2520visual%2520grounding%250Acapabilities.%2520However%252C%2520the%2520adversarial%2520robustness%2520of%2520visual%2520grounding%2520remains%250Aunexplored%2520in%2520MLLMs.%2520To%2520fill%2520this%2520gap%252C%2520we%2520use%2520referring%2520expression%250Acomprehension%2520%2528REC%2529%2520as%2520an%2520example%2520task%2520in%2520visual%2520grounding%2520and%2520propose%2520three%250Aadversarial%2520attack%2520paradigms%2520as%2520follows.%2520Firstly%252C%2520untargeted%2520adversarial%250Aattacks%2520induce%2520MLLMs%2520to%2520generate%2520incorrect%2520bounding%2520boxes%2520for%2520each%2520object.%250ABesides%252C%2520exclusive%2520targeted%2520adversarial%2520attacks%2520cause%2520all%2520generated%2520outputs%2520to%250Athe%2520same%2520target%2520bounding%2520box.%2520In%2520addition%252C%2520permuted%2520targeted%2520adversarial%250Aattacks%2520aim%2520to%2520permute%2520all%2520bounding%2520boxes%2520among%2520different%2520objects%2520within%2520a%250Asingle%2520image.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520methods%2520can%250Asuccessfully%2520attack%2520visual%2520grounding%2520capabilities%2520of%2520MLLMs.%2520Our%2520methods%2520not%250Aonly%2520provide%2520a%2520new%2520perspective%2520for%2520designing%2520novel%2520attacks%2520but%2520also%2520serve%2520as%2520a%250Astrong%2520baseline%2520for%2520improving%2520the%2520adversarial%2520robustness%2520for%2520visual%2520grounding%250Aof%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20for%20Visual%20Grounding%20of%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Kuofeng%20Gao%20and%20Yang%20Bai%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20achieved%20enhanced%0Aperformance%20across%20various%20vision-language%20tasks%20including%20visual%20grounding%0Acapabilities.%20However%2C%20the%20adversarial%20robustness%20of%20visual%20grounding%20remains%0Aunexplored%20in%20MLLMs.%20To%20fill%20this%20gap%2C%20we%20use%20referring%20expression%0Acomprehension%20%28REC%29%20as%20an%20example%20task%20in%20visual%20grounding%20and%20propose%20three%0Aadversarial%20attack%20paradigms%20as%20follows.%20Firstly%2C%20untargeted%20adversarial%0Aattacks%20induce%20MLLMs%20to%20generate%20incorrect%20bounding%20boxes%20for%20each%20object.%0ABesides%2C%20exclusive%20targeted%20adversarial%20attacks%20cause%20all%20generated%20outputs%20to%0Athe%20same%20target%20bounding%20box.%20In%20addition%2C%20permuted%20targeted%20adversarial%0Aattacks%20aim%20to%20permute%20all%20bounding%20boxes%20among%20different%20objects%20within%20a%0Asingle%20image.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20methods%20can%0Asuccessfully%20attack%20visual%20grounding%20capabilities%20of%20MLLMs.%20Our%20methods%20not%0Aonly%20provide%20a%20new%20perspective%20for%20designing%20novel%20attacks%20but%20also%20serve%20as%20a%0Astrong%20baseline%20for%20improving%20the%20adversarial%20robustness%20for%20visual%20grounding%0Aof%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09981v1&entry.124074799=Read"},
{"title": "DocuMint: Docstring Generation for Python using Small Language Models", "author": "Bibek Poudel and Adam Cook and Sekou Traore and Shelah Ameli", "abstract": "  Effective communication, specifically through documentation, is the beating\nheart of collaboration among contributors in software development. Recent\nadvancements in language models (LMs) have enabled the introduction of a new\ntype of actor in that ecosystem: LM-powered assistants capable of code\ngeneration, optimization, and maintenance. Our study investigates the efficacy\nof small language models (SLMs) for generating high-quality docstrings by\nassessing accuracy, conciseness, and clarity, benchmarking performance\nquantitatively through mathematical formulas and qualitatively through human\nevaluation using Likert scale. Further, we introduce DocuMint, as a large-scale\nsupervised fine-tuning dataset with 100,000 samples. In quantitative\nexperiments, Llama 3 8B achieved the best performance across all metrics, with\nconciseness and clarity scores of 0.605 and 64.88, respectively. However, under\nhuman evaluation, CodeGemma 7B achieved the highest overall score with an\naverage of 8.3 out of 10 across all metrics. Fine-tuning the CodeGemma 2B model\nusing the DocuMint dataset led to significant improvements in performance\nacross all metrics, with gains of up to 22.5% in conciseness. The fine-tuned\nmodel and the dataset can be found in HuggingFace and the code can be found in\nthe repository.\n", "link": "http://arxiv.org/abs/2405.10243v1", "date": "2024-05-16", "relevancy": 1.6409, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4171}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocuMint%3A%20Docstring%20Generation%20for%20Python%20using%20Small%20Language%20Models&body=Title%3A%20DocuMint%3A%20Docstring%20Generation%20for%20Python%20using%20Small%20Language%20Models%0AAuthor%3A%20Bibek%20Poudel%20and%20Adam%20Cook%20and%20Sekou%20Traore%20and%20Shelah%20Ameli%0AAbstract%3A%20%20%20Effective%20communication%2C%20specifically%20through%20documentation%2C%20is%20the%20beating%0Aheart%20of%20collaboration%20among%20contributors%20in%20software%20development.%20Recent%0Aadvancements%20in%20language%20models%20%28LMs%29%20have%20enabled%20the%20introduction%20of%20a%20new%0Atype%20of%20actor%20in%20that%20ecosystem%3A%20LM-powered%20assistants%20capable%20of%20code%0Ageneration%2C%20optimization%2C%20and%20maintenance.%20Our%20study%20investigates%20the%20efficacy%0Aof%20small%20language%20models%20%28SLMs%29%20for%20generating%20high-quality%20docstrings%20by%0Aassessing%20accuracy%2C%20conciseness%2C%20and%20clarity%2C%20benchmarking%20performance%0Aquantitatively%20through%20mathematical%20formulas%20and%20qualitatively%20through%20human%0Aevaluation%20using%20Likert%20scale.%20Further%2C%20we%20introduce%20DocuMint%2C%20as%20a%20large-scale%0Asupervised%20fine-tuning%20dataset%20with%20100%2C000%20samples.%20In%20quantitative%0Aexperiments%2C%20Llama%203%208B%20achieved%20the%20best%20performance%20across%20all%20metrics%2C%20with%0Aconciseness%20and%20clarity%20scores%20of%200.605%20and%2064.88%2C%20respectively.%20However%2C%20under%0Ahuman%20evaluation%2C%20CodeGemma%207B%20achieved%20the%20highest%20overall%20score%20with%20an%0Aaverage%20of%208.3%20out%20of%2010%20across%20all%20metrics.%20Fine-tuning%20the%20CodeGemma%202B%20model%0Ausing%20the%20DocuMint%20dataset%20led%20to%20significant%20improvements%20in%20performance%0Aacross%20all%20metrics%2C%20with%20gains%20of%20up%20to%2022.5%25%20in%20conciseness.%20The%20fine-tuned%0Amodel%20and%20the%20dataset%20can%20be%20found%20in%20HuggingFace%20and%20the%20code%20can%20be%20found%20in%0Athe%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocuMint%253A%2520Docstring%2520Generation%2520for%2520Python%2520using%2520Small%2520Language%2520Models%26entry.906535625%3DBibek%2520Poudel%2520and%2520Adam%2520Cook%2520and%2520Sekou%2520Traore%2520and%2520Shelah%2520Ameli%26entry.1292438233%3D%2520%2520Effective%2520communication%252C%2520specifically%2520through%2520documentation%252C%2520is%2520the%2520beating%250Aheart%2520of%2520collaboration%2520among%2520contributors%2520in%2520software%2520development.%2520Recent%250Aadvancements%2520in%2520language%2520models%2520%2528LMs%2529%2520have%2520enabled%2520the%2520introduction%2520of%2520a%2520new%250Atype%2520of%2520actor%2520in%2520that%2520ecosystem%253A%2520LM-powered%2520assistants%2520capable%2520of%2520code%250Ageneration%252C%2520optimization%252C%2520and%2520maintenance.%2520Our%2520study%2520investigates%2520the%2520efficacy%250Aof%2520small%2520language%2520models%2520%2528SLMs%2529%2520for%2520generating%2520high-quality%2520docstrings%2520by%250Aassessing%2520accuracy%252C%2520conciseness%252C%2520and%2520clarity%252C%2520benchmarking%2520performance%250Aquantitatively%2520through%2520mathematical%2520formulas%2520and%2520qualitatively%2520through%2520human%250Aevaluation%2520using%2520Likert%2520scale.%2520Further%252C%2520we%2520introduce%2520DocuMint%252C%2520as%2520a%2520large-scale%250Asupervised%2520fine-tuning%2520dataset%2520with%2520100%252C000%2520samples.%2520In%2520quantitative%250Aexperiments%252C%2520Llama%25203%25208B%2520achieved%2520the%2520best%2520performance%2520across%2520all%2520metrics%252C%2520with%250Aconciseness%2520and%2520clarity%2520scores%2520of%25200.605%2520and%252064.88%252C%2520respectively.%2520However%252C%2520under%250Ahuman%2520evaluation%252C%2520CodeGemma%25207B%2520achieved%2520the%2520highest%2520overall%2520score%2520with%2520an%250Aaverage%2520of%25208.3%2520out%2520of%252010%2520across%2520all%2520metrics.%2520Fine-tuning%2520the%2520CodeGemma%25202B%2520model%250Ausing%2520the%2520DocuMint%2520dataset%2520led%2520to%2520significant%2520improvements%2520in%2520performance%250Aacross%2520all%2520metrics%252C%2520with%2520gains%2520of%2520up%2520to%252022.5%2525%2520in%2520conciseness.%2520The%2520fine-tuned%250Amodel%2520and%2520the%2520dataset%2520can%2520be%2520found%2520in%2520HuggingFace%2520and%2520the%2520code%2520can%2520be%2520found%2520in%250Athe%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocuMint%3A%20Docstring%20Generation%20for%20Python%20using%20Small%20Language%20Models&entry.906535625=Bibek%20Poudel%20and%20Adam%20Cook%20and%20Sekou%20Traore%20and%20Shelah%20Ameli&entry.1292438233=%20%20Effective%20communication%2C%20specifically%20through%20documentation%2C%20is%20the%20beating%0Aheart%20of%20collaboration%20among%20contributors%20in%20software%20development.%20Recent%0Aadvancements%20in%20language%20models%20%28LMs%29%20have%20enabled%20the%20introduction%20of%20a%20new%0Atype%20of%20actor%20in%20that%20ecosystem%3A%20LM-powered%20assistants%20capable%20of%20code%0Ageneration%2C%20optimization%2C%20and%20maintenance.%20Our%20study%20investigates%20the%20efficacy%0Aof%20small%20language%20models%20%28SLMs%29%20for%20generating%20high-quality%20docstrings%20by%0Aassessing%20accuracy%2C%20conciseness%2C%20and%20clarity%2C%20benchmarking%20performance%0Aquantitatively%20through%20mathematical%20formulas%20and%20qualitatively%20through%20human%0Aevaluation%20using%20Likert%20scale.%20Further%2C%20we%20introduce%20DocuMint%2C%20as%20a%20large-scale%0Asupervised%20fine-tuning%20dataset%20with%20100%2C000%20samples.%20In%20quantitative%0Aexperiments%2C%20Llama%203%208B%20achieved%20the%20best%20performance%20across%20all%20metrics%2C%20with%0Aconciseness%20and%20clarity%20scores%20of%200.605%20and%2064.88%2C%20respectively.%20However%2C%20under%0Ahuman%20evaluation%2C%20CodeGemma%207B%20achieved%20the%20highest%20overall%20score%20with%20an%0Aaverage%20of%208.3%20out%20of%2010%20across%20all%20metrics.%20Fine-tuning%20the%20CodeGemma%202B%20model%0Ausing%20the%20DocuMint%20dataset%20led%20to%20significant%20improvements%20in%20performance%0Aacross%20all%20metrics%2C%20with%20gains%20of%20up%20to%2022.5%25%20in%20conciseness.%20The%20fine-tuned%0Amodel%20and%20the%20dataset%20can%20be%20found%20in%20HuggingFace%20and%20the%20code%20can%20be%20found%20in%0Athe%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10243v1&entry.124074799=Read"},
{"title": "Influencer Cartels", "author": "Marit Hinnosaar and Toomas Hinnosaar", "abstract": "  Social media influencers account for a growing share of marketing worldwide.\nWe demonstrate the existence of a novel form of market failure in this\nadvertising market: influencer cartels, where groups of influencers collude to\nincrease their advertising revenue by inflating their engagement. Our\ntheoretical model shows that influencer cartels can improve consumer welfare if\nthey expand social media engagement to the target audience, or reduce welfare\nif they divert engagement to less relevant audiences. We validate the model\nempirically using novel data on influencer cartels combined with machine\nlearning tools, and derive policy implications for how to maximize consumer\nwelfare.\n", "link": "http://arxiv.org/abs/2405.10231v1", "date": "2024-05-16", "relevancy": 1.1127, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3799}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3693}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influencer%20Cartels&body=Title%3A%20Influencer%20Cartels%0AAuthor%3A%20Marit%20Hinnosaar%20and%20Toomas%20Hinnosaar%0AAbstract%3A%20%20%20Social%20media%20influencers%20account%20for%20a%20growing%20share%20of%20marketing%20worldwide.%0AWe%20demonstrate%20the%20existence%20of%20a%20novel%20form%20of%20market%20failure%20in%20this%0Aadvertising%20market%3A%20influencer%20cartels%2C%20where%20groups%20of%20influencers%20collude%20to%0Aincrease%20their%20advertising%20revenue%20by%20inflating%20their%20engagement.%20Our%0Atheoretical%20model%20shows%20that%20influencer%20cartels%20can%20improve%20consumer%20welfare%20if%0Athey%20expand%20social%20media%20engagement%20to%20the%20target%20audience%2C%20or%20reduce%20welfare%0Aif%20they%20divert%20engagement%20to%20less%20relevant%20audiences.%20We%20validate%20the%20model%0Aempirically%20using%20novel%20data%20on%20influencer%20cartels%20combined%20with%20machine%0Alearning%20tools%2C%20and%20derive%20policy%20implications%20for%20how%20to%20maximize%20consumer%0Awelfare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluencer%2520Cartels%26entry.906535625%3DMarit%2520Hinnosaar%2520and%2520Toomas%2520Hinnosaar%26entry.1292438233%3D%2520%2520Social%2520media%2520influencers%2520account%2520for%2520a%2520growing%2520share%2520of%2520marketing%2520worldwide.%250AWe%2520demonstrate%2520the%2520existence%2520of%2520a%2520novel%2520form%2520of%2520market%2520failure%2520in%2520this%250Aadvertising%2520market%253A%2520influencer%2520cartels%252C%2520where%2520groups%2520of%2520influencers%2520collude%2520to%250Aincrease%2520their%2520advertising%2520revenue%2520by%2520inflating%2520their%2520engagement.%2520Our%250Atheoretical%2520model%2520shows%2520that%2520influencer%2520cartels%2520can%2520improve%2520consumer%2520welfare%2520if%250Athey%2520expand%2520social%2520media%2520engagement%2520to%2520the%2520target%2520audience%252C%2520or%2520reduce%2520welfare%250Aif%2520they%2520divert%2520engagement%2520to%2520less%2520relevant%2520audiences.%2520We%2520validate%2520the%2520model%250Aempirically%2520using%2520novel%2520data%2520on%2520influencer%2520cartels%2520combined%2520with%2520machine%250Alearning%2520tools%252C%2520and%2520derive%2520policy%2520implications%2520for%2520how%2520to%2520maximize%2520consumer%250Awelfare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influencer%20Cartels&entry.906535625=Marit%20Hinnosaar%20and%20Toomas%20Hinnosaar&entry.1292438233=%20%20Social%20media%20influencers%20account%20for%20a%20growing%20share%20of%20marketing%20worldwide.%0AWe%20demonstrate%20the%20existence%20of%20a%20novel%20form%20of%20market%20failure%20in%20this%0Aadvertising%20market%3A%20influencer%20cartels%2C%20where%20groups%20of%20influencers%20collude%20to%0Aincrease%20their%20advertising%20revenue%20by%20inflating%20their%20engagement.%20Our%0Atheoretical%20model%20shows%20that%20influencer%20cartels%20can%20improve%20consumer%20welfare%20if%0Athey%20expand%20social%20media%20engagement%20to%20the%20target%20audience%2C%20or%20reduce%20welfare%0Aif%20they%20divert%20engagement%20to%20less%20relevant%20audiences.%20We%20validate%20the%20model%0Aempirically%20using%20novel%20data%20on%20influencer%20cartels%20combined%20with%20machine%0Alearning%20tools%2C%20and%20derive%20policy%20implications%20for%20how%20to%20maximize%20consumer%0Awelfare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10231v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


