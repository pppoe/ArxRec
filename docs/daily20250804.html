<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250803.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar", "author": "Dongbin Zhang and Yunfei Liu and Lijian Lin and Ye Zhu and Yang Li and Minghan Qin and Yu Li and Haoqian Wang", "abstract": "  Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.\n", "link": "http://arxiv.org/abs/2505.03351v2", "date": "2025-08-01", "relevancy": 3.716, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7611}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7611}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUAVA%3A%20Generalizable%20Upper%20Body%203D%20Gaussian%20Avatar&body=Title%3A%20GUAVA%3A%20Generalizable%20Upper%20Body%203D%20Gaussian%20Avatar%0AAuthor%3A%20Dongbin%20Zhang%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yang%20Li%20and%20Minghan%20Qin%20and%20Yu%20Li%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%20Reconstructing%20a%20high-quality%2C%20animatable%203D%20human%20avatar%20with%20expressive%0Afacial%20and%20hand%20motions%20from%20a%20single%20image%20has%20gained%20significant%20attention%0Adue%20to%20its%20broad%20application%20potential.%203D%20human%20avatar%20reconstruction%0Atypically%20requires%20multi-view%20or%20monocular%20videos%20and%20training%20on%20individual%0AIDs%2C%20which%20is%20both%20complex%20and%20time-consuming.%20Furthermore%2C%20limited%20by%20SMPLX%27s%0Aexpressiveness%2C%20these%20methods%20often%20focus%20on%20body%20motion%20but%20struggle%20with%0Afacial%20expressions.%20To%20address%20these%20challenges%2C%20we%20first%20introduce%20an%0Aexpressive%20human%20model%20%28EHM%29%20to%20enhance%20facial%20expression%20capabilities%20and%0Adevelop%20an%20accurate%20tracking%20method.%20Based%20on%20this%20template%20model%2C%20we%20propose%0AGUAVA%2C%20the%20first%20framework%20for%20fast%20animatable%20upper-body%203D%20Gaussian%20avatar%0Areconstruction.%20We%20leverage%20inverse%20texture%20mapping%20and%20projection%20sampling%0Atechniques%20to%20infer%20Ubody%20%28upper-body%29%20Gaussians%20from%20a%20single%20image.%20The%0Arendered%20images%20are%20refined%20through%20a%20neural%20refiner.%20Experimental%20results%0Ademonstrate%20that%20GUAVA%20significantly%20outperforms%20previous%20methods%20in%20rendering%0Aquality%20and%20offers%20significant%20speed%20improvements%2C%20with%20reconstruction%20times%20in%0Athe%20sub-second%20range%20%280.1s%29%2C%20and%20supports%20real-time%20animation%20and%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUAVA%253A%2520Generalizable%2520Upper%2520Body%25203D%2520Gaussian%2520Avatar%26entry.906535625%3DDongbin%2520Zhang%2520and%2520Yunfei%2520Liu%2520and%2520Lijian%2520Lin%2520and%2520Ye%2520Zhu%2520and%2520Yang%2520Li%2520and%2520Minghan%2520Qin%2520and%2520Yu%2520Li%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%2520a%2520high-quality%252C%2520animatable%25203D%2520human%2520avatar%2520with%2520expressive%250Afacial%2520and%2520hand%2520motions%2520from%2520a%2520single%2520image%2520has%2520gained%2520significant%2520attention%250Adue%2520to%2520its%2520broad%2520application%2520potential.%25203D%2520human%2520avatar%2520reconstruction%250Atypically%2520requires%2520multi-view%2520or%2520monocular%2520videos%2520and%2520training%2520on%2520individual%250AIDs%252C%2520which%2520is%2520both%2520complex%2520and%2520time-consuming.%2520Furthermore%252C%2520limited%2520by%2520SMPLX%2527s%250Aexpressiveness%252C%2520these%2520methods%2520often%2520focus%2520on%2520body%2520motion%2520but%2520struggle%2520with%250Afacial%2520expressions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520first%2520introduce%2520an%250Aexpressive%2520human%2520model%2520%2528EHM%2529%2520to%2520enhance%2520facial%2520expression%2520capabilities%2520and%250Adevelop%2520an%2520accurate%2520tracking%2520method.%2520Based%2520on%2520this%2520template%2520model%252C%2520we%2520propose%250AGUAVA%252C%2520the%2520first%2520framework%2520for%2520fast%2520animatable%2520upper-body%25203D%2520Gaussian%2520avatar%250Areconstruction.%2520We%2520leverage%2520inverse%2520texture%2520mapping%2520and%2520projection%2520sampling%250Atechniques%2520to%2520infer%2520Ubody%2520%2528upper-body%2529%2520Gaussians%2520from%2520a%2520single%2520image.%2520The%250Arendered%2520images%2520are%2520refined%2520through%2520a%2520neural%2520refiner.%2520Experimental%2520results%250Ademonstrate%2520that%2520GUAVA%2520significantly%2520outperforms%2520previous%2520methods%2520in%2520rendering%250Aquality%2520and%2520offers%2520significant%2520speed%2520improvements%252C%2520with%2520reconstruction%2520times%2520in%250Athe%2520sub-second%2520range%2520%25280.1s%2529%252C%2520and%2520supports%2520real-time%2520animation%2520and%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUAVA%3A%20Generalizable%20Upper%20Body%203D%20Gaussian%20Avatar&entry.906535625=Dongbin%20Zhang%20and%20Yunfei%20Liu%20and%20Lijian%20Lin%20and%20Ye%20Zhu%20and%20Yang%20Li%20and%20Minghan%20Qin%20and%20Yu%20Li%20and%20Haoqian%20Wang&entry.1292438233=%20%20Reconstructing%20a%20high-quality%2C%20animatable%203D%20human%20avatar%20with%20expressive%0Afacial%20and%20hand%20motions%20from%20a%20single%20image%20has%20gained%20significant%20attention%0Adue%20to%20its%20broad%20application%20potential.%203D%20human%20avatar%20reconstruction%0Atypically%20requires%20multi-view%20or%20monocular%20videos%20and%20training%20on%20individual%0AIDs%2C%20which%20is%20both%20complex%20and%20time-consuming.%20Furthermore%2C%20limited%20by%20SMPLX%27s%0Aexpressiveness%2C%20these%20methods%20often%20focus%20on%20body%20motion%20but%20struggle%20with%0Afacial%20expressions.%20To%20address%20these%20challenges%2C%20we%20first%20introduce%20an%0Aexpressive%20human%20model%20%28EHM%29%20to%20enhance%20facial%20expression%20capabilities%20and%0Adevelop%20an%20accurate%20tracking%20method.%20Based%20on%20this%20template%20model%2C%20we%20propose%0AGUAVA%2C%20the%20first%20framework%20for%20fast%20animatable%20upper-body%203D%20Gaussian%20avatar%0Areconstruction.%20We%20leverage%20inverse%20texture%20mapping%20and%20projection%20sampling%0Atechniques%20to%20infer%20Ubody%20%28upper-body%29%20Gaussians%20from%20a%20single%20image.%20The%0Arendered%20images%20are%20refined%20through%20a%20neural%20refiner.%20Experimental%20results%0Ademonstrate%20that%20GUAVA%20significantly%20outperforms%20previous%20methods%20in%20rendering%0Aquality%20and%20offers%20significant%20speed%20improvements%2C%20with%20reconstruction%20times%20in%0Athe%20sub-second%20range%20%280.1s%29%2C%20and%20supports%20real-time%20animation%20and%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03351v2&entry.124074799=Read"},
{"title": "Gaga: Group Any Gaussians via 3D-aware Memory Bank", "author": "Weijie Lyu and Xueting Li and Abhijit Kundu and Yi-Hsuan Tsai and Ming-Hsuan Yang", "abstract": "  We introduce Gaga, a framework that reconstructs and segments open-world 3D\nscenes by leveraging inconsistent 2D masks predicted by zero-shot\nclass-agnostic segmentation models. Contrasted to prior 3D scene segmentation\napproaches that rely on video object tracking or contrastive learning methods,\nGaga utilizes spatial information and effectively associates object masks\nacross diverse camera poses through a novel 3D-aware memory bank. By\neliminating the assumption of continuous view changes in training images, Gaga\ndemonstrates robustness to variations in camera poses, particularly beneficial\nfor sparsely sampled images, ensuring precise mask label consistency.\nFurthermore, Gaga accommodates 2D segmentation masks from diverse sources and\ndemonstrates robust performance with different open-world zero-shot\nclass-agnostic segmentation models, significantly enhancing its versatility.\nExtensive qualitative and quantitative evaluations demonstrate that Gaga\nperforms favorably against state-of-the-art methods, emphasizing its potential\nfor real-world applications such as 3D scene understanding and manipulation.\n", "link": "http://arxiv.org/abs/2404.07977v3", "date": "2025-08-01", "relevancy": 3.198, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6489}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6423}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank&body=Title%3A%20Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank%0AAuthor%3A%20Weijie%20Lyu%20and%20Xueting%20Li%20and%20Abhijit%20Kundu%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20We%20introduce%20Gaga%2C%20a%20framework%20that%20reconstructs%20and%20segments%20open-world%203D%0Ascenes%20by%20leveraging%20inconsistent%202D%20masks%20predicted%20by%20zero-shot%0Aclass-agnostic%20segmentation%20models.%20Contrasted%20to%20prior%203D%20scene%20segmentation%0Aapproaches%20that%20rely%20on%20video%20object%20tracking%20or%20contrastive%20learning%20methods%2C%0AGaga%20utilizes%20spatial%20information%20and%20effectively%20associates%20object%20masks%0Aacross%20diverse%20camera%20poses%20through%20a%20novel%203D-aware%20memory%20bank.%20By%0Aeliminating%20the%20assumption%20of%20continuous%20view%20changes%20in%20training%20images%2C%20Gaga%0Ademonstrates%20robustness%20to%20variations%20in%20camera%20poses%2C%20particularly%20beneficial%0Afor%20sparsely%20sampled%20images%2C%20ensuring%20precise%20mask%20label%20consistency.%0AFurthermore%2C%20Gaga%20accommodates%202D%20segmentation%20masks%20from%20diverse%20sources%20and%0Ademonstrates%20robust%20performance%20with%20different%20open-world%20zero-shot%0Aclass-agnostic%20segmentation%20models%2C%20significantly%20enhancing%20its%20versatility.%0AExtensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%20Gaga%0Aperforms%20favorably%20against%20state-of-the-art%20methods%2C%20emphasizing%20its%20potential%0Afor%20real-world%20applications%20such%20as%203D%20scene%20understanding%20and%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07977v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaga%253A%2520Group%2520Any%2520Gaussians%2520via%25203D-aware%2520Memory%2520Bank%26entry.906535625%3DWeijie%2520Lyu%2520and%2520Xueting%2520Li%2520and%2520Abhijit%2520Kundu%2520and%2520Yi-Hsuan%2520Tsai%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Gaga%252C%2520a%2520framework%2520that%2520reconstructs%2520and%2520segments%2520open-world%25203D%250Ascenes%2520by%2520leveraging%2520inconsistent%25202D%2520masks%2520predicted%2520by%2520zero-shot%250Aclass-agnostic%2520segmentation%2520models.%2520Contrasted%2520to%2520prior%25203D%2520scene%2520segmentation%250Aapproaches%2520that%2520rely%2520on%2520video%2520object%2520tracking%2520or%2520contrastive%2520learning%2520methods%252C%250AGaga%2520utilizes%2520spatial%2520information%2520and%2520effectively%2520associates%2520object%2520masks%250Aacross%2520diverse%2520camera%2520poses%2520through%2520a%2520novel%25203D-aware%2520memory%2520bank.%2520By%250Aeliminating%2520the%2520assumption%2520of%2520continuous%2520view%2520changes%2520in%2520training%2520images%252C%2520Gaga%250Ademonstrates%2520robustness%2520to%2520variations%2520in%2520camera%2520poses%252C%2520particularly%2520beneficial%250Afor%2520sparsely%2520sampled%2520images%252C%2520ensuring%2520precise%2520mask%2520label%2520consistency.%250AFurthermore%252C%2520Gaga%2520accommodates%25202D%2520segmentation%2520masks%2520from%2520diverse%2520sources%2520and%250Ademonstrates%2520robust%2520performance%2520with%2520different%2520open-world%2520zero-shot%250Aclass-agnostic%2520segmentation%2520models%252C%2520significantly%2520enhancing%2520its%2520versatility.%250AExtensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520demonstrate%2520that%2520Gaga%250Aperforms%2520favorably%2520against%2520state-of-the-art%2520methods%252C%2520emphasizing%2520its%2520potential%250Afor%2520real-world%2520applications%2520such%2520as%25203D%2520scene%2520understanding%2520and%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07977v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaga%3A%20Group%20Any%20Gaussians%20via%203D-aware%20Memory%20Bank&entry.906535625=Weijie%20Lyu%20and%20Xueting%20Li%20and%20Abhijit%20Kundu%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20We%20introduce%20Gaga%2C%20a%20framework%20that%20reconstructs%20and%20segments%20open-world%203D%0Ascenes%20by%20leveraging%20inconsistent%202D%20masks%20predicted%20by%20zero-shot%0Aclass-agnostic%20segmentation%20models.%20Contrasted%20to%20prior%203D%20scene%20segmentation%0Aapproaches%20that%20rely%20on%20video%20object%20tracking%20or%20contrastive%20learning%20methods%2C%0AGaga%20utilizes%20spatial%20information%20and%20effectively%20associates%20object%20masks%0Aacross%20diverse%20camera%20poses%20through%20a%20novel%203D-aware%20memory%20bank.%20By%0Aeliminating%20the%20assumption%20of%20continuous%20view%20changes%20in%20training%20images%2C%20Gaga%0Ademonstrates%20robustness%20to%20variations%20in%20camera%20poses%2C%20particularly%20beneficial%0Afor%20sparsely%20sampled%20images%2C%20ensuring%20precise%20mask%20label%20consistency.%0AFurthermore%2C%20Gaga%20accommodates%202D%20segmentation%20masks%20from%20diverse%20sources%20and%0Ademonstrates%20robust%20performance%20with%20different%20open-world%20zero-shot%0Aclass-agnostic%20segmentation%20models%2C%20significantly%20enhancing%20its%20versatility.%0AExtensive%20qualitative%20and%20quantitative%20evaluations%20demonstrate%20that%20Gaga%0Aperforms%20favorably%20against%20state-of-the-art%20methods%2C%20emphasizing%20its%20potential%0Afor%20real-world%20applications%20such%20as%203D%20scene%20understanding%20and%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07977v3&entry.124074799=Read"},
{"title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "author": "Junzhe Lu and Jing Lin and Hongkun Dou and Ailing Zeng and Yue Deng and Xian Liu and Zhongang Cai and Lei Yang and Yulun Zhang and Haoqian Wang and Ziwei Liu", "abstract": "  We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.\n", "link": "http://arxiv.org/abs/2508.00599v1", "date": "2025-08-01", "relevancy": 3.1962, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7156}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.609}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPoser-X%3A%20Diffusion%20Model%20as%20Robust%203D%20Whole-body%20Human%20Pose%20Prior&body=Title%3A%20DPoser-X%3A%20Diffusion%20Model%20as%20Robust%203D%20Whole-body%20Human%20Pose%20Prior%0AAuthor%3A%20Junzhe%20Lu%20and%20Jing%20Lin%20and%20Hongkun%20Dou%20and%20Ailing%20Zeng%20and%20Yue%20Deng%20and%20Xian%20Liu%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Yulun%20Zhang%20and%20Haoqian%20Wang%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20DPoser-X%2C%20a%20diffusion-based%20prior%20model%20for%203D%20whole-body%20human%0Aposes.%20Building%20a%20versatile%20and%20robust%20full-body%20human%20pose%20prior%20remains%0Achallenging%20due%20to%20the%20inherent%20complexity%20of%20articulated%20human%20poses%20and%20the%0Ascarcity%20of%20high-quality%20whole-body%20pose%20datasets.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20Diffusion%20model%20as%20body%20Pose%20prior%20%28DPoser%29%20and%0Aextend%20it%20to%20DPoser-X%20for%20expressive%20whole-body%20human%20pose%20modeling.%20Our%0Aapproach%20unifies%20various%20pose-centric%20tasks%20as%20inverse%20problems%2C%20solving%20them%0Athrough%20variational%20diffusion%20sampling.%20To%20enhance%20performance%20on%20downstream%0Aapplications%2C%20we%20introduce%20a%20novel%20truncated%20timestep%20scheduling%20method%0Aspecifically%20designed%20for%20pose%20data%20characteristics.%20We%20also%20propose%20a%20masked%0Atraining%20mechanism%20that%20effectively%20combines%20whole-body%20and%20part-specific%0Adatasets%2C%20enabling%20our%20model%20to%20capture%20interdependencies%20between%20body%20parts%0Awhile%20avoiding%20overfitting%20to%20specific%20actions.%20Extensive%20experiments%0Ademonstrate%20DPoser-X%27s%20robustness%20and%20versatility%20across%20multiple%20benchmarks%0Afor%20body%2C%20hand%2C%20face%2C%20and%20full-body%20pose%20modeling.%20Our%20model%20consistently%0Aoutperforms%20state-of-the-art%20alternatives%2C%20establishing%20a%20new%20benchmark%20for%0Awhole-body%20human%20pose%20prior%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPoser-X%253A%2520Diffusion%2520Model%2520as%2520Robust%25203D%2520Whole-body%2520Human%2520Pose%2520Prior%26entry.906535625%3DJunzhe%2520Lu%2520and%2520Jing%2520Lin%2520and%2520Hongkun%2520Dou%2520and%2520Ailing%2520Zeng%2520and%2520Yue%2520Deng%2520and%2520Xian%2520Liu%2520and%2520Zhongang%2520Cai%2520and%2520Lei%2520Yang%2520and%2520Yulun%2520Zhang%2520and%2520Haoqian%2520Wang%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520DPoser-X%252C%2520a%2520diffusion-based%2520prior%2520model%2520for%25203D%2520whole-body%2520human%250Aposes.%2520Building%2520a%2520versatile%2520and%2520robust%2520full-body%2520human%2520pose%2520prior%2520remains%250Achallenging%2520due%2520to%2520the%2520inherent%2520complexity%2520of%2520articulated%2520human%2520poses%2520and%2520the%250Ascarcity%2520of%2520high-quality%2520whole-body%2520pose%2520datasets.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520Diffusion%2520model%2520as%2520body%2520Pose%2520prior%2520%2528DPoser%2529%2520and%250Aextend%2520it%2520to%2520DPoser-X%2520for%2520expressive%2520whole-body%2520human%2520pose%2520modeling.%2520Our%250Aapproach%2520unifies%2520various%2520pose-centric%2520tasks%2520as%2520inverse%2520problems%252C%2520solving%2520them%250Athrough%2520variational%2520diffusion%2520sampling.%2520To%2520enhance%2520performance%2520on%2520downstream%250Aapplications%252C%2520we%2520introduce%2520a%2520novel%2520truncated%2520timestep%2520scheduling%2520method%250Aspecifically%2520designed%2520for%2520pose%2520data%2520characteristics.%2520We%2520also%2520propose%2520a%2520masked%250Atraining%2520mechanism%2520that%2520effectively%2520combines%2520whole-body%2520and%2520part-specific%250Adatasets%252C%2520enabling%2520our%2520model%2520to%2520capture%2520interdependencies%2520between%2520body%2520parts%250Awhile%2520avoiding%2520overfitting%2520to%2520specific%2520actions.%2520Extensive%2520experiments%250Ademonstrate%2520DPoser-X%2527s%2520robustness%2520and%2520versatility%2520across%2520multiple%2520benchmarks%250Afor%2520body%252C%2520hand%252C%2520face%252C%2520and%2520full-body%2520pose%2520modeling.%2520Our%2520model%2520consistently%250Aoutperforms%2520state-of-the-art%2520alternatives%252C%2520establishing%2520a%2520new%2520benchmark%2520for%250Awhole-body%2520human%2520pose%2520prior%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPoser-X%3A%20Diffusion%20Model%20as%20Robust%203D%20Whole-body%20Human%20Pose%20Prior&entry.906535625=Junzhe%20Lu%20and%20Jing%20Lin%20and%20Hongkun%20Dou%20and%20Ailing%20Zeng%20and%20Yue%20Deng%20and%20Xian%20Liu%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Yulun%20Zhang%20and%20Haoqian%20Wang%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20DPoser-X%2C%20a%20diffusion-based%20prior%20model%20for%203D%20whole-body%20human%0Aposes.%20Building%20a%20versatile%20and%20robust%20full-body%20human%20pose%20prior%20remains%0Achallenging%20due%20to%20the%20inherent%20complexity%20of%20articulated%20human%20poses%20and%20the%0Ascarcity%20of%20high-quality%20whole-body%20pose%20datasets.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20Diffusion%20model%20as%20body%20Pose%20prior%20%28DPoser%29%20and%0Aextend%20it%20to%20DPoser-X%20for%20expressive%20whole-body%20human%20pose%20modeling.%20Our%0Aapproach%20unifies%20various%20pose-centric%20tasks%20as%20inverse%20problems%2C%20solving%20them%0Athrough%20variational%20diffusion%20sampling.%20To%20enhance%20performance%20on%20downstream%0Aapplications%2C%20we%20introduce%20a%20novel%20truncated%20timestep%20scheduling%20method%0Aspecifically%20designed%20for%20pose%20data%20characteristics.%20We%20also%20propose%20a%20masked%0Atraining%20mechanism%20that%20effectively%20combines%20whole-body%20and%20part-specific%0Adatasets%2C%20enabling%20our%20model%20to%20capture%20interdependencies%20between%20body%20parts%0Awhile%20avoiding%20overfitting%20to%20specific%20actions.%20Extensive%20experiments%0Ademonstrate%20DPoser-X%27s%20robustness%20and%20versatility%20across%20multiple%20benchmarks%0Afor%20body%2C%20hand%2C%20face%2C%20and%20full-body%20pose%20modeling.%20Our%20model%20consistently%0Aoutperforms%20state-of-the-art%20alternatives%2C%20establishing%20a%20new%20benchmark%20for%0Awhole-body%20human%20pose%20prior%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00599v1&entry.124074799=Read"},
{"title": "Is It Really You? Exploring Biometric Verification Scenarios in\n  Photorealistic Talking-Head Avatar Videos", "author": "Laura Pedrouzo-Rodriguez and Pedro Delgado-DeRobles and Luis F. Gomez and Ruben Tolosana and Ruben Vera-Rodriguez and Aythami Morales and Julian Fierrez", "abstract": "  Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.\n", "link": "http://arxiv.org/abs/2508.00748v1", "date": "2025-08-01", "relevancy": 3.0342, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6224}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6224}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos&body=Title%3A%20Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos%0AAuthor%3A%20Laura%20Pedrouzo-Rodriguez%20and%20Pedro%20Delgado-DeRobles%20and%20Luis%20F.%20Gomez%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%0AAbstract%3A%20%20%20Photorealistic%20talking-head%20avatars%20are%20becoming%20increasingly%20common%20in%0Avirtual%20meetings%2C%20gaming%2C%20and%20social%20platforms.%20These%20avatars%20allow%20for%20more%0Aimmersive%20communication%2C%20but%20they%20also%20introduce%20serious%20security%20risks.%20One%0Aemerging%20threat%20is%20impersonation%3A%20an%20attacker%20can%20steal%20a%20user%27s%0Aavatar-preserving%20their%20appearance%20and%20voice-making%20it%20nearly%20impossible%20to%0Adetect%20its%20fraudulent%20usage%20by%20sight%20or%20sound%20alone.%20In%20this%20paper%2C%20we%20explore%0Athe%20challenge%20of%20biometric%20verification%20in%20such%20avatar-mediated%20scenarios.%20Our%0Amain%20question%20is%20whether%20an%20individual%27s%20facial%20motion%20patterns%20can%20serve%20as%0Areliable%20behavioral%20biometrics%20to%20verify%20their%20identity%20when%20the%20avatar%27s%0Avisual%20appearance%20is%20a%20facsimile%20of%20its%20owner.%20To%20answer%20this%20question%2C%20we%0Aintroduce%20a%20new%20dataset%20of%20realistic%20avatar%20videos%20created%20using%20a%0Astate-of-the-art%20one-shot%20avatar%20generation%20model%2C%20GAGAvatar%2C%20with%20genuine%20and%0Aimpostor%20avatar%20videos.%20We%20also%20propose%20a%20lightweight%2C%20explainable%0Aspatio-temporal%20Graph%20Convolutional%20Network%20architecture%20with%20temporal%0Aattention%20pooling%2C%20that%20uses%20only%20facial%20landmarks%20to%20model%20dynamic%20facial%0Agestures.%20Experimental%20results%20demonstrate%20that%20facial%20motion%20cues%20enable%0Ameaningful%20identity%20verification%20with%20AUC%20values%20approaching%2080%25.%20The%20proposed%0Abenchmark%20and%20biometric%20system%20are%20available%20for%20the%20research%20community%20in%0Aorder%20to%20bring%20attention%20to%20the%20urgent%20need%20for%20more%20advanced%20behavioral%0Abiometric%20defenses%20in%20avatar-based%20communication%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520It%2520Really%2520You%253F%2520Exploring%2520Biometric%2520Verification%2520Scenarios%2520in%250A%2520%2520Photorealistic%2520Talking-Head%2520Avatar%2520Videos%26entry.906535625%3DLaura%2520Pedrouzo-Rodriguez%2520and%2520Pedro%2520Delgado-DeRobles%2520and%2520Luis%2520F.%2520Gomez%2520and%2520Ruben%2520Tolosana%2520and%2520Ruben%2520Vera-Rodriguez%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%26entry.1292438233%3D%2520%2520Photorealistic%2520talking-head%2520avatars%2520are%2520becoming%2520increasingly%2520common%2520in%250Avirtual%2520meetings%252C%2520gaming%252C%2520and%2520social%2520platforms.%2520These%2520avatars%2520allow%2520for%2520more%250Aimmersive%2520communication%252C%2520but%2520they%2520also%2520introduce%2520serious%2520security%2520risks.%2520One%250Aemerging%2520threat%2520is%2520impersonation%253A%2520an%2520attacker%2520can%2520steal%2520a%2520user%2527s%250Aavatar-preserving%2520their%2520appearance%2520and%2520voice-making%2520it%2520nearly%2520impossible%2520to%250Adetect%2520its%2520fraudulent%2520usage%2520by%2520sight%2520or%2520sound%2520alone.%2520In%2520this%2520paper%252C%2520we%2520explore%250Athe%2520challenge%2520of%2520biometric%2520verification%2520in%2520such%2520avatar-mediated%2520scenarios.%2520Our%250Amain%2520question%2520is%2520whether%2520an%2520individual%2527s%2520facial%2520motion%2520patterns%2520can%2520serve%2520as%250Areliable%2520behavioral%2520biometrics%2520to%2520verify%2520their%2520identity%2520when%2520the%2520avatar%2527s%250Avisual%2520appearance%2520is%2520a%2520facsimile%2520of%2520its%2520owner.%2520To%2520answer%2520this%2520question%252C%2520we%250Aintroduce%2520a%2520new%2520dataset%2520of%2520realistic%2520avatar%2520videos%2520created%2520using%2520a%250Astate-of-the-art%2520one-shot%2520avatar%2520generation%2520model%252C%2520GAGAvatar%252C%2520with%2520genuine%2520and%250Aimpostor%2520avatar%2520videos.%2520We%2520also%2520propose%2520a%2520lightweight%252C%2520explainable%250Aspatio-temporal%2520Graph%2520Convolutional%2520Network%2520architecture%2520with%2520temporal%250Aattention%2520pooling%252C%2520that%2520uses%2520only%2520facial%2520landmarks%2520to%2520model%2520dynamic%2520facial%250Agestures.%2520Experimental%2520results%2520demonstrate%2520that%2520facial%2520motion%2520cues%2520enable%250Ameaningful%2520identity%2520verification%2520with%2520AUC%2520values%2520approaching%252080%2525.%2520The%2520proposed%250Abenchmark%2520and%2520biometric%2520system%2520are%2520available%2520for%2520the%2520research%2520community%2520in%250Aorder%2520to%2520bring%2520attention%2520to%2520the%2520urgent%2520need%2520for%2520more%2520advanced%2520behavioral%250Abiometric%2520defenses%2520in%2520avatar-based%2520communication%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20It%20Really%20You%3F%20Exploring%20Biometric%20Verification%20Scenarios%20in%0A%20%20Photorealistic%20Talking-Head%20Avatar%20Videos&entry.906535625=Laura%20Pedrouzo-Rodriguez%20and%20Pedro%20Delgado-DeRobles%20and%20Luis%20F.%20Gomez%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez&entry.1292438233=%20%20Photorealistic%20talking-head%20avatars%20are%20becoming%20increasingly%20common%20in%0Avirtual%20meetings%2C%20gaming%2C%20and%20social%20platforms.%20These%20avatars%20allow%20for%20more%0Aimmersive%20communication%2C%20but%20they%20also%20introduce%20serious%20security%20risks.%20One%0Aemerging%20threat%20is%20impersonation%3A%20an%20attacker%20can%20steal%20a%20user%27s%0Aavatar-preserving%20their%20appearance%20and%20voice-making%20it%20nearly%20impossible%20to%0Adetect%20its%20fraudulent%20usage%20by%20sight%20or%20sound%20alone.%20In%20this%20paper%2C%20we%20explore%0Athe%20challenge%20of%20biometric%20verification%20in%20such%20avatar-mediated%20scenarios.%20Our%0Amain%20question%20is%20whether%20an%20individual%27s%20facial%20motion%20patterns%20can%20serve%20as%0Areliable%20behavioral%20biometrics%20to%20verify%20their%20identity%20when%20the%20avatar%27s%0Avisual%20appearance%20is%20a%20facsimile%20of%20its%20owner.%20To%20answer%20this%20question%2C%20we%0Aintroduce%20a%20new%20dataset%20of%20realistic%20avatar%20videos%20created%20using%20a%0Astate-of-the-art%20one-shot%20avatar%20generation%20model%2C%20GAGAvatar%2C%20with%20genuine%20and%0Aimpostor%20avatar%20videos.%20We%20also%20propose%20a%20lightweight%2C%20explainable%0Aspatio-temporal%20Graph%20Convolutional%20Network%20architecture%20with%20temporal%0Aattention%20pooling%2C%20that%20uses%20only%20facial%20landmarks%20to%20model%20dynamic%20facial%0Agestures.%20Experimental%20results%20demonstrate%20that%20facial%20motion%20cues%20enable%0Ameaningful%20identity%20verification%20with%20AUC%20values%20approaching%2080%25.%20The%20proposed%0Abenchmark%20and%20biometric%20system%20are%20available%20for%20the%20research%20community%20in%0Aorder%20to%20bring%20attention%20to%20the%20urgent%20need%20for%20more%20advanced%20behavioral%0Abiometric%20defenses%20in%20avatar-based%20communication%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00748v1&entry.124074799=Read"},
{"title": "D3: Training-Free AI-Generated Video Detection Using Second-Order\n  Features", "author": "Chende Zheng and Ruiqi suo and Chenhao Lin and Zhengyu Zhao and Le Yang and Shuai Liu and Minghui Yang and Cong Wang and Chao Shen", "abstract": "  The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.\n", "link": "http://arxiv.org/abs/2508.00701v1", "date": "2025-08-01", "relevancy": 2.9779, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6062}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5972}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D3%3A%20Training-Free%20AI-Generated%20Video%20Detection%20Using%20Second-Order%0A%20%20Features&body=Title%3A%20D3%3A%20Training-Free%20AI-Generated%20Video%20Detection%20Using%20Second-Order%0A%20%20Features%0AAuthor%3A%20Chende%20Zheng%20and%20Ruiqi%20suo%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Le%20Yang%20and%20Shuai%20Liu%20and%20Minghui%20Yang%20and%20Cong%20Wang%20and%20Chao%20Shen%0AAbstract%3A%20%20%20The%20evolution%20of%20video%20generation%20techniques%2C%20such%20as%20Sora%2C%20has%20made%20it%0Aincreasingly%20easy%20to%20produce%20high-fidelity%20AI-generated%20videos%2C%20raising%20public%0Aconcern%20over%20the%20dissemination%20of%20synthetic%20content.%20However%2C%20existing%0Adetection%20methodologies%20remain%20limited%20by%20their%20insufficient%20exploration%20of%0Atemporal%20artifacts%20in%20synthetic%20videos.%20To%20bridge%20this%20gap%2C%20we%20establish%20a%0Atheoretical%20framework%20through%20second-order%20dynamical%20analysis%20under%20Newtonian%0Amechanics%2C%20subsequently%20extending%20the%20Second-order%20Central%20Difference%20features%0Atailored%20for%20temporal%20artifact%20detection.%20Building%20on%20this%20theoretical%0Afoundation%2C%20we%20reveal%20a%20fundamental%20divergence%20in%20second-order%20feature%0Adistributions%20between%20real%20and%20AI-generated%20videos.%20Concretely%2C%20we%20propose%0ADetection%20by%20Difference%20of%20Differences%20%28D3%29%2C%20a%20novel%20training-free%20detection%0Amethod%20that%20leverages%20the%20above%20second-order%20temporal%20discrepancies.%20We%0Avalidate%20the%20superiority%20of%20our%20D3%20on%204%20open-source%20datasets%20%28Gen-Video%2C%0AVideoPhy%2C%20EvalCrafter%2C%20VidProM%29%2C%2040%20subsets%20in%20total.%20For%20example%2C%20on%20GenVideo%2C%0AD3%20outperforms%20the%20previous%20best%20method%20by%2010.39%25%20%28absolute%29%20mean%20Average%0APrecision.%20Additional%20experiments%20on%20time%20cost%20and%20post-processing%20operations%0Ademonstrate%20D3%27s%20exceptional%20computational%20efficiency%20and%20strong%20robust%0Aperformance.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zig-HS/D3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD3%253A%2520Training-Free%2520AI-Generated%2520Video%2520Detection%2520Using%2520Second-Order%250A%2520%2520Features%26entry.906535625%3DChende%2520Zheng%2520and%2520Ruiqi%2520suo%2520and%2520Chenhao%2520Lin%2520and%2520Zhengyu%2520Zhao%2520and%2520Le%2520Yang%2520and%2520Shuai%2520Liu%2520and%2520Minghui%2520Yang%2520and%2520Cong%2520Wang%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520video%2520generation%2520techniques%252C%2520such%2520as%2520Sora%252C%2520has%2520made%2520it%250Aincreasingly%2520easy%2520to%2520produce%2520high-fidelity%2520AI-generated%2520videos%252C%2520raising%2520public%250Aconcern%2520over%2520the%2520dissemination%2520of%2520synthetic%2520content.%2520However%252C%2520existing%250Adetection%2520methodologies%2520remain%2520limited%2520by%2520their%2520insufficient%2520exploration%2520of%250Atemporal%2520artifacts%2520in%2520synthetic%2520videos.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520establish%2520a%250Atheoretical%2520framework%2520through%2520second-order%2520dynamical%2520analysis%2520under%2520Newtonian%250Amechanics%252C%2520subsequently%2520extending%2520the%2520Second-order%2520Central%2520Difference%2520features%250Atailored%2520for%2520temporal%2520artifact%2520detection.%2520Building%2520on%2520this%2520theoretical%250Afoundation%252C%2520we%2520reveal%2520a%2520fundamental%2520divergence%2520in%2520second-order%2520feature%250Adistributions%2520between%2520real%2520and%2520AI-generated%2520videos.%2520Concretely%252C%2520we%2520propose%250ADetection%2520by%2520Difference%2520of%2520Differences%2520%2528D3%2529%252C%2520a%2520novel%2520training-free%2520detection%250Amethod%2520that%2520leverages%2520the%2520above%2520second-order%2520temporal%2520discrepancies.%2520We%250Avalidate%2520the%2520superiority%2520of%2520our%2520D3%2520on%25204%2520open-source%2520datasets%2520%2528Gen-Video%252C%250AVideoPhy%252C%2520EvalCrafter%252C%2520VidProM%2529%252C%252040%2520subsets%2520in%2520total.%2520For%2520example%252C%2520on%2520GenVideo%252C%250AD3%2520outperforms%2520the%2520previous%2520best%2520method%2520by%252010.39%2525%2520%2528absolute%2529%2520mean%2520Average%250APrecision.%2520Additional%2520experiments%2520on%2520time%2520cost%2520and%2520post-processing%2520operations%250Ademonstrate%2520D3%2527s%2520exceptional%2520computational%2520efficiency%2520and%2520strong%2520robust%250Aperformance.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Zig-HS/D3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3%3A%20Training-Free%20AI-Generated%20Video%20Detection%20Using%20Second-Order%0A%20%20Features&entry.906535625=Chende%20Zheng%20and%20Ruiqi%20suo%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Le%20Yang%20and%20Shuai%20Liu%20and%20Minghui%20Yang%20and%20Cong%20Wang%20and%20Chao%20Shen&entry.1292438233=%20%20The%20evolution%20of%20video%20generation%20techniques%2C%20such%20as%20Sora%2C%20has%20made%20it%0Aincreasingly%20easy%20to%20produce%20high-fidelity%20AI-generated%20videos%2C%20raising%20public%0Aconcern%20over%20the%20dissemination%20of%20synthetic%20content.%20However%2C%20existing%0Adetection%20methodologies%20remain%20limited%20by%20their%20insufficient%20exploration%20of%0Atemporal%20artifacts%20in%20synthetic%20videos.%20To%20bridge%20this%20gap%2C%20we%20establish%20a%0Atheoretical%20framework%20through%20second-order%20dynamical%20analysis%20under%20Newtonian%0Amechanics%2C%20subsequently%20extending%20the%20Second-order%20Central%20Difference%20features%0Atailored%20for%20temporal%20artifact%20detection.%20Building%20on%20this%20theoretical%0Afoundation%2C%20we%20reveal%20a%20fundamental%20divergence%20in%20second-order%20feature%0Adistributions%20between%20real%20and%20AI-generated%20videos.%20Concretely%2C%20we%20propose%0ADetection%20by%20Difference%20of%20Differences%20%28D3%29%2C%20a%20novel%20training-free%20detection%0Amethod%20that%20leverages%20the%20above%20second-order%20temporal%20discrepancies.%20We%0Avalidate%20the%20superiority%20of%20our%20D3%20on%204%20open-source%20datasets%20%28Gen-Video%2C%0AVideoPhy%2C%20EvalCrafter%2C%20VidProM%29%2C%2040%20subsets%20in%20total.%20For%20example%2C%20on%20GenVideo%2C%0AD3%20outperforms%20the%20previous%20best%20method%20by%2010.39%25%20%28absolute%29%20mean%20Average%0APrecision.%20Additional%20experiments%20on%20time%20cost%20and%20post-processing%20operations%0Ademonstrate%20D3%27s%20exceptional%20computational%20efficiency%20and%20strong%20robust%0Aperformance.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zig-HS/D3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00701v1&entry.124074799=Read"},
{"title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "author": "Regine Hartwig and Dominik Muhle and Riccardo Marin and Daniel Cremers", "abstract": "  Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/\n", "link": "http://arxiv.org/abs/2508.00746v1", "date": "2025-08-01", "relevancy": 2.8567, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5791}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GECO%3A%20Geometrically%20Consistent%20Embedding%20with%20Lightspeed%20Inference&body=Title%3A%20GECO%3A%20Geometrically%20Consistent%20Embedding%20with%20Lightspeed%20Inference%0AAuthor%3A%20Regine%20Hartwig%20and%20Dominik%20Muhle%20and%20Riccardo%20Marin%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Recent%20advances%20in%20feature%20learning%20have%20shown%20that%20self-supervised%20vision%0Afoundation%20models%20can%20capture%20semantic%20correspondences%20but%20often%20lack%20awareness%0Aof%20underlying%203D%20geometry.%20GECO%20addresses%20this%20gap%20by%20producing%20geometrically%0Acoherent%20features%20that%20semantically%20distinguish%20parts%20based%20on%20geometry%20%28e.g.%2C%0Aleft/right%20eyes%2C%20front/back%20legs%29.%20We%20propose%20a%20training%20framework%20based%20on%0Aoptimal%20transport%2C%20enabling%20supervision%20beyond%20keypoints%2C%20even%20under%20occlusions%0Aand%20disocclusions.%20With%20a%20lightweight%20architecture%2C%20GECO%20runs%20at%2030%20fps%2C%2098.2%25%0Afaster%20than%20prior%20methods%2C%20while%20achieving%20state-of-the-art%20performance%20on%0APFPascal%2C%20APK%2C%20and%20CUB%2C%20improving%20PCK%20by%206.0%25%2C%206.2%25%2C%20and%204.1%25%2C%20respectively.%0AFinally%2C%20we%20show%20that%20PCK%20alone%20is%20insufficient%20to%20capture%20geometric%20quality%0Aand%20introduce%20new%20metrics%20and%20insights%20for%20more%20geometry-aware%20feature%0Alearning.%20Link%20to%20project%20page%3A%0Ahttps%3A//reginehartwig.github.io/publications/geco/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGECO%253A%2520Geometrically%2520Consistent%2520Embedding%2520with%2520Lightspeed%2520Inference%26entry.906535625%3DRegine%2520Hartwig%2520and%2520Dominik%2520Muhle%2520and%2520Riccardo%2520Marin%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520feature%2520learning%2520have%2520shown%2520that%2520self-supervised%2520vision%250Afoundation%2520models%2520can%2520capture%2520semantic%2520correspondences%2520but%2520often%2520lack%2520awareness%250Aof%2520underlying%25203D%2520geometry.%2520GECO%2520addresses%2520this%2520gap%2520by%2520producing%2520geometrically%250Acoherent%2520features%2520that%2520semantically%2520distinguish%2520parts%2520based%2520on%2520geometry%2520%2528e.g.%252C%250Aleft/right%2520eyes%252C%2520front/back%2520legs%2529.%2520We%2520propose%2520a%2520training%2520framework%2520based%2520on%250Aoptimal%2520transport%252C%2520enabling%2520supervision%2520beyond%2520keypoints%252C%2520even%2520under%2520occlusions%250Aand%2520disocclusions.%2520With%2520a%2520lightweight%2520architecture%252C%2520GECO%2520runs%2520at%252030%2520fps%252C%252098.2%2525%250Afaster%2520than%2520prior%2520methods%252C%2520while%2520achieving%2520state-of-the-art%2520performance%2520on%250APFPascal%252C%2520APK%252C%2520and%2520CUB%252C%2520improving%2520PCK%2520by%25206.0%2525%252C%25206.2%2525%252C%2520and%25204.1%2525%252C%2520respectively.%250AFinally%252C%2520we%2520show%2520that%2520PCK%2520alone%2520is%2520insufficient%2520to%2520capture%2520geometric%2520quality%250Aand%2520introduce%2520new%2520metrics%2520and%2520insights%2520for%2520more%2520geometry-aware%2520feature%250Alearning.%2520Link%2520to%2520project%2520page%253A%250Ahttps%253A//reginehartwig.github.io/publications/geco/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GECO%3A%20Geometrically%20Consistent%20Embedding%20with%20Lightspeed%20Inference&entry.906535625=Regine%20Hartwig%20and%20Dominik%20Muhle%20and%20Riccardo%20Marin%20and%20Daniel%20Cremers&entry.1292438233=%20%20Recent%20advances%20in%20feature%20learning%20have%20shown%20that%20self-supervised%20vision%0Afoundation%20models%20can%20capture%20semantic%20correspondences%20but%20often%20lack%20awareness%0Aof%20underlying%203D%20geometry.%20GECO%20addresses%20this%20gap%20by%20producing%20geometrically%0Acoherent%20features%20that%20semantically%20distinguish%20parts%20based%20on%20geometry%20%28e.g.%2C%0Aleft/right%20eyes%2C%20front/back%20legs%29.%20We%20propose%20a%20training%20framework%20based%20on%0Aoptimal%20transport%2C%20enabling%20supervision%20beyond%20keypoints%2C%20even%20under%20occlusions%0Aand%20disocclusions.%20With%20a%20lightweight%20architecture%2C%20GECO%20runs%20at%2030%20fps%2C%2098.2%25%0Afaster%20than%20prior%20methods%2C%20while%20achieving%20state-of-the-art%20performance%20on%0APFPascal%2C%20APK%2C%20and%20CUB%2C%20improving%20PCK%20by%206.0%25%2C%206.2%25%2C%20and%204.1%25%2C%20respectively.%0AFinally%2C%20we%20show%20that%20PCK%20alone%20is%20insufficient%20to%20capture%20geometric%20quality%0Aand%20introduce%20new%20metrics%20and%20insights%20for%20more%20geometry-aware%20feature%0Alearning.%20Link%20to%20project%20page%3A%0Ahttps%3A//reginehartwig.github.io/publications/geco/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00746v1&entry.124074799=Read"},
{"title": "HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial,\n  Appearance, and Motion Anomaly", "author": "Chang Liu and Yunfan Ye and Fan Zhang and Qingyang Zhou and Yuchuan Luo and Zhiping Cai", "abstract": "  Numerous synthesized videos from generative models, especially human-centric\nones that simulate realistic human actions, pose significant threats to human\ninformation security and authenticity. While progress has been made in binary\nforgery video detection, the lack of fine-grained understanding of forgery\ntypes raises concerns regarding both reliability and interpretability, which\nare critical for real-world applications. To address this limitation, we\npropose HumanSAM, a new framework that builds upon the fundamental challenges\nof video generation models. Specifically, HumanSAM aims to classify\nhuman-centric forgeries into three distinct types of artifacts commonly\nobserved in generated content: spatial, appearance, and motion anomaly. To\nbetter capture the features of geometry, semantics and spatiotemporal\nconsistency, we propose to generate the human forgery representation by fusing\ntwo branches of video understanding and spatial depth. We also adopt a\nrank-based confidence enhancement strategy during the training process to learn\nmore robust representation by introducing three prior scores. For training and\nevaluation, we construct the first public benchmark, the Human-centric Forgery\nVideo (HFV) dataset, with all types of forgeries carefully annotated\nsemi-automatically. In our experiments, HumanSAM yields promising results in\ncomparison with state-of-the-art methods, both in binary and multi-class\nforgery classification.\n", "link": "http://arxiv.org/abs/2507.19924v2", "date": "2025-08-01", "relevancy": 2.7902, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.568}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5538}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanSAM%3A%20Classifying%20Human-centric%20Forgery%20Videos%20in%20Human%20Spatial%2C%0A%20%20Appearance%2C%20and%20Motion%20Anomaly&body=Title%3A%20HumanSAM%3A%20Classifying%20Human-centric%20Forgery%20Videos%20in%20Human%20Spatial%2C%0A%20%20Appearance%2C%20and%20Motion%20Anomaly%0AAuthor%3A%20Chang%20Liu%20and%20Yunfan%20Ye%20and%20Fan%20Zhang%20and%20Qingyang%20Zhou%20and%20Yuchuan%20Luo%20and%20Zhiping%20Cai%0AAbstract%3A%20%20%20Numerous%20synthesized%20videos%20from%20generative%20models%2C%20especially%20human-centric%0Aones%20that%20simulate%20realistic%20human%20actions%2C%20pose%20significant%20threats%20to%20human%0Ainformation%20security%20and%20authenticity.%20While%20progress%20has%20been%20made%20in%20binary%0Aforgery%20video%20detection%2C%20the%20lack%20of%20fine-grained%20understanding%20of%20forgery%0Atypes%20raises%20concerns%20regarding%20both%20reliability%20and%20interpretability%2C%20which%0Aare%20critical%20for%20real-world%20applications.%20To%20address%20this%20limitation%2C%20we%0Apropose%20HumanSAM%2C%20a%20new%20framework%20that%20builds%20upon%20the%20fundamental%20challenges%0Aof%20video%20generation%20models.%20Specifically%2C%20HumanSAM%20aims%20to%20classify%0Ahuman-centric%20forgeries%20into%20three%20distinct%20types%20of%20artifacts%20commonly%0Aobserved%20in%20generated%20content%3A%20spatial%2C%20appearance%2C%20and%20motion%20anomaly.%20To%0Abetter%20capture%20the%20features%20of%20geometry%2C%20semantics%20and%20spatiotemporal%0Aconsistency%2C%20we%20propose%20to%20generate%20the%20human%20forgery%20representation%20by%20fusing%0Atwo%20branches%20of%20video%20understanding%20and%20spatial%20depth.%20We%20also%20adopt%20a%0Arank-based%20confidence%20enhancement%20strategy%20during%20the%20training%20process%20to%20learn%0Amore%20robust%20representation%20by%20introducing%20three%20prior%20scores.%20For%20training%20and%0Aevaluation%2C%20we%20construct%20the%20first%20public%20benchmark%2C%20the%20Human-centric%20Forgery%0AVideo%20%28HFV%29%20dataset%2C%20with%20all%20types%20of%20forgeries%20carefully%20annotated%0Asemi-automatically.%20In%20our%20experiments%2C%20HumanSAM%20yields%20promising%20results%20in%0Acomparison%20with%20state-of-the-art%20methods%2C%20both%20in%20binary%20and%20multi-class%0Aforgery%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanSAM%253A%2520Classifying%2520Human-centric%2520Forgery%2520Videos%2520in%2520Human%2520Spatial%252C%250A%2520%2520Appearance%252C%2520and%2520Motion%2520Anomaly%26entry.906535625%3DChang%2520Liu%2520and%2520Yunfan%2520Ye%2520and%2520Fan%2520Zhang%2520and%2520Qingyang%2520Zhou%2520and%2520Yuchuan%2520Luo%2520and%2520Zhiping%2520Cai%26entry.1292438233%3D%2520%2520Numerous%2520synthesized%2520videos%2520from%2520generative%2520models%252C%2520especially%2520human-centric%250Aones%2520that%2520simulate%2520realistic%2520human%2520actions%252C%2520pose%2520significant%2520threats%2520to%2520human%250Ainformation%2520security%2520and%2520authenticity.%2520While%2520progress%2520has%2520been%2520made%2520in%2520binary%250Aforgery%2520video%2520detection%252C%2520the%2520lack%2520of%2520fine-grained%2520understanding%2520of%2520forgery%250Atypes%2520raises%2520concerns%2520regarding%2520both%2520reliability%2520and%2520interpretability%252C%2520which%250Aare%2520critical%2520for%2520real-world%2520applications.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520HumanSAM%252C%2520a%2520new%2520framework%2520that%2520builds%2520upon%2520the%2520fundamental%2520challenges%250Aof%2520video%2520generation%2520models.%2520Specifically%252C%2520HumanSAM%2520aims%2520to%2520classify%250Ahuman-centric%2520forgeries%2520into%2520three%2520distinct%2520types%2520of%2520artifacts%2520commonly%250Aobserved%2520in%2520generated%2520content%253A%2520spatial%252C%2520appearance%252C%2520and%2520motion%2520anomaly.%2520To%250Abetter%2520capture%2520the%2520features%2520of%2520geometry%252C%2520semantics%2520and%2520spatiotemporal%250Aconsistency%252C%2520we%2520propose%2520to%2520generate%2520the%2520human%2520forgery%2520representation%2520by%2520fusing%250Atwo%2520branches%2520of%2520video%2520understanding%2520and%2520spatial%2520depth.%2520We%2520also%2520adopt%2520a%250Arank-based%2520confidence%2520enhancement%2520strategy%2520during%2520the%2520training%2520process%2520to%2520learn%250Amore%2520robust%2520representation%2520by%2520introducing%2520three%2520prior%2520scores.%2520For%2520training%2520and%250Aevaluation%252C%2520we%2520construct%2520the%2520first%2520public%2520benchmark%252C%2520the%2520Human-centric%2520Forgery%250AVideo%2520%2528HFV%2529%2520dataset%252C%2520with%2520all%2520types%2520of%2520forgeries%2520carefully%2520annotated%250Asemi-automatically.%2520In%2520our%2520experiments%252C%2520HumanSAM%2520yields%2520promising%2520results%2520in%250Acomparison%2520with%2520state-of-the-art%2520methods%252C%2520both%2520in%2520binary%2520and%2520multi-class%250Aforgery%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanSAM%3A%20Classifying%20Human-centric%20Forgery%20Videos%20in%20Human%20Spatial%2C%0A%20%20Appearance%2C%20and%20Motion%20Anomaly&entry.906535625=Chang%20Liu%20and%20Yunfan%20Ye%20and%20Fan%20Zhang%20and%20Qingyang%20Zhou%20and%20Yuchuan%20Luo%20and%20Zhiping%20Cai&entry.1292438233=%20%20Numerous%20synthesized%20videos%20from%20generative%20models%2C%20especially%20human-centric%0Aones%20that%20simulate%20realistic%20human%20actions%2C%20pose%20significant%20threats%20to%20human%0Ainformation%20security%20and%20authenticity.%20While%20progress%20has%20been%20made%20in%20binary%0Aforgery%20video%20detection%2C%20the%20lack%20of%20fine-grained%20understanding%20of%20forgery%0Atypes%20raises%20concerns%20regarding%20both%20reliability%20and%20interpretability%2C%20which%0Aare%20critical%20for%20real-world%20applications.%20To%20address%20this%20limitation%2C%20we%0Apropose%20HumanSAM%2C%20a%20new%20framework%20that%20builds%20upon%20the%20fundamental%20challenges%0Aof%20video%20generation%20models.%20Specifically%2C%20HumanSAM%20aims%20to%20classify%0Ahuman-centric%20forgeries%20into%20three%20distinct%20types%20of%20artifacts%20commonly%0Aobserved%20in%20generated%20content%3A%20spatial%2C%20appearance%2C%20and%20motion%20anomaly.%20To%0Abetter%20capture%20the%20features%20of%20geometry%2C%20semantics%20and%20spatiotemporal%0Aconsistency%2C%20we%20propose%20to%20generate%20the%20human%20forgery%20representation%20by%20fusing%0Atwo%20branches%20of%20video%20understanding%20and%20spatial%20depth.%20We%20also%20adopt%20a%0Arank-based%20confidence%20enhancement%20strategy%20during%20the%20training%20process%20to%20learn%0Amore%20robust%20representation%20by%20introducing%20three%20prior%20scores.%20For%20training%20and%0Aevaluation%2C%20we%20construct%20the%20first%20public%20benchmark%2C%20the%20Human-centric%20Forgery%0AVideo%20%28HFV%29%20dataset%2C%20with%20all%20types%20of%20forgeries%20carefully%20annotated%0Asemi-automatically.%20In%20our%20experiments%2C%20HumanSAM%20yields%20promising%20results%20in%0Acomparison%20with%20state-of-the-art%20methods%2C%20both%20in%20binary%20and%20multi-class%0Aforgery%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19924v2&entry.124074799=Read"},
{"title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic\n  Prototypes", "author": "Jialun Zheng and Jie Liu and Jiannong Cao and Xiao Wang and Hanchen Yang and Yankai Chen and Philip S. Yu", "abstract": "  Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.\n", "link": "http://arxiv.org/abs/2508.00664v1", "date": "2025-08-01", "relevancy": 2.7718, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5594}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-DGAD%3A%20A%20Generalist%20Dynamic%20Graph%20Anomaly%20Detector%20with%20Dynamic%0A%20%20Prototypes&body=Title%3A%20DP-DGAD%3A%20A%20Generalist%20Dynamic%20Graph%20Anomaly%20Detector%20with%20Dynamic%0A%20%20Prototypes%0AAuthor%3A%20Jialun%20Zheng%20and%20Jie%20Liu%20and%20Jiannong%20Cao%20and%20Xiao%20Wang%20and%20Hanchen%20Yang%20and%20Yankai%20Chen%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Dynamic%20graph%20anomaly%20detection%20%28DGAD%29%20is%20essential%20for%20identifying%20anomalies%0Ain%20evolving%20graphs%20across%20domains%20such%20as%20finance%2C%20traffic%2C%20and%20social%0Anetworks.%20Recently%2C%20generalist%20graph%20anomaly%20detection%20%28GAD%29%20models%20have%20shown%0Apromising%20results.%20They%20are%20pretrained%20on%20multiple%20source%20datasets%20and%0Ageneralize%20across%20domains.%20While%20effective%20on%20static%20graphs%2C%20they%20struggle%20to%0Acapture%20evolving%20anomalies%20in%20dynamic%20graphs.%20Moreover%2C%20the%20continuous%0Aemergence%20of%20new%20domains%20and%20the%20lack%20of%20labeled%20data%20further%20challenge%0Ageneralist%20DGAD.%20Effective%20cross-domain%20DGAD%20requires%20both%20domain-specific%20and%0Adomain-agnostic%20anomalous%20patterns.%20Importantly%2C%20these%20patterns%20evolve%0Atemporally%20within%20and%20across%20domains.%20Building%20on%20these%20insights%2C%20we%20propose%20a%0ADGAD%20model%20with%20Dynamic%20Prototypes%20%28DP%29%20to%20capture%20evolving%20domain-specific%20and%0Adomain-agnostic%20patterns.%20Firstly%2C%20DP-DGAD%20extracts%20dynamic%20prototypes%2C%20i.e.%2C%0Aevolving%20representations%20of%20normal%20and%20anomalous%20patterns%2C%20from%20temporal%0Aego-graphs%20and%20stores%20them%20in%20a%20memory%20buffer.%20The%20buffer%20is%20selectively%0Aupdated%20to%20retain%20general%2C%20domain-agnostic%20patterns%20while%20incorporating%20new%0Adomain-specific%20ones.%20Then%2C%20an%20anomaly%20scorer%20compares%20incoming%20data%20with%0Adynamic%20prototypes%20to%20flag%20both%20general%20and%20domain-specific%20anomalies.%20Finally%2C%0ADP-DGAD%20employs%20confidence-based%20pseudo-labeling%20for%20effective%20self-supervised%0Aadaptation%20in%20target%20domains.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20performance%20across%20ten%20real-world%20datasets%20from%20different%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-DGAD%253A%2520A%2520Generalist%2520Dynamic%2520Graph%2520Anomaly%2520Detector%2520with%2520Dynamic%250A%2520%2520Prototypes%26entry.906535625%3DJialun%2520Zheng%2520and%2520Jie%2520Liu%2520and%2520Jiannong%2520Cao%2520and%2520Xiao%2520Wang%2520and%2520Hanchen%2520Yang%2520and%2520Yankai%2520Chen%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520anomaly%2520detection%2520%2528DGAD%2529%2520is%2520essential%2520for%2520identifying%2520anomalies%250Ain%2520evolving%2520graphs%2520across%2520domains%2520such%2520as%2520finance%252C%2520traffic%252C%2520and%2520social%250Anetworks.%2520Recently%252C%2520generalist%2520graph%2520anomaly%2520detection%2520%2528GAD%2529%2520models%2520have%2520shown%250Apromising%2520results.%2520They%2520are%2520pretrained%2520on%2520multiple%2520source%2520datasets%2520and%250Ageneralize%2520across%2520domains.%2520While%2520effective%2520on%2520static%2520graphs%252C%2520they%2520struggle%2520to%250Acapture%2520evolving%2520anomalies%2520in%2520dynamic%2520graphs.%2520Moreover%252C%2520the%2520continuous%250Aemergence%2520of%2520new%2520domains%2520and%2520the%2520lack%2520of%2520labeled%2520data%2520further%2520challenge%250Ageneralist%2520DGAD.%2520Effective%2520cross-domain%2520DGAD%2520requires%2520both%2520domain-specific%2520and%250Adomain-agnostic%2520anomalous%2520patterns.%2520Importantly%252C%2520these%2520patterns%2520evolve%250Atemporally%2520within%2520and%2520across%2520domains.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%250ADGAD%2520model%2520with%2520Dynamic%2520Prototypes%2520%2528DP%2529%2520to%2520capture%2520evolving%2520domain-specific%2520and%250Adomain-agnostic%2520patterns.%2520Firstly%252C%2520DP-DGAD%2520extracts%2520dynamic%2520prototypes%252C%2520i.e.%252C%250Aevolving%2520representations%2520of%2520normal%2520and%2520anomalous%2520patterns%252C%2520from%2520temporal%250Aego-graphs%2520and%2520stores%2520them%2520in%2520a%2520memory%2520buffer.%2520The%2520buffer%2520is%2520selectively%250Aupdated%2520to%2520retain%2520general%252C%2520domain-agnostic%2520patterns%2520while%2520incorporating%2520new%250Adomain-specific%2520ones.%2520Then%252C%2520an%2520anomaly%2520scorer%2520compares%2520incoming%2520data%2520with%250Adynamic%2520prototypes%2520to%2520flag%2520both%2520general%2520and%2520domain-specific%2520anomalies.%2520Finally%252C%250ADP-DGAD%2520employs%2520confidence-based%2520pseudo-labeling%2520for%2520effective%2520self-supervised%250Aadaptation%2520in%2520target%2520domains.%2520Extensive%2520experiments%2520demonstrate%250Astate-of-the-art%2520performance%2520across%2520ten%2520real-world%2520datasets%2520from%2520different%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-DGAD%3A%20A%20Generalist%20Dynamic%20Graph%20Anomaly%20Detector%20with%20Dynamic%0A%20%20Prototypes&entry.906535625=Jialun%20Zheng%20and%20Jie%20Liu%20and%20Jiannong%20Cao%20and%20Xiao%20Wang%20and%20Hanchen%20Yang%20and%20Yankai%20Chen%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Dynamic%20graph%20anomaly%20detection%20%28DGAD%29%20is%20essential%20for%20identifying%20anomalies%0Ain%20evolving%20graphs%20across%20domains%20such%20as%20finance%2C%20traffic%2C%20and%20social%0Anetworks.%20Recently%2C%20generalist%20graph%20anomaly%20detection%20%28GAD%29%20models%20have%20shown%0Apromising%20results.%20They%20are%20pretrained%20on%20multiple%20source%20datasets%20and%0Ageneralize%20across%20domains.%20While%20effective%20on%20static%20graphs%2C%20they%20struggle%20to%0Acapture%20evolving%20anomalies%20in%20dynamic%20graphs.%20Moreover%2C%20the%20continuous%0Aemergence%20of%20new%20domains%20and%20the%20lack%20of%20labeled%20data%20further%20challenge%0Ageneralist%20DGAD.%20Effective%20cross-domain%20DGAD%20requires%20both%20domain-specific%20and%0Adomain-agnostic%20anomalous%20patterns.%20Importantly%2C%20these%20patterns%20evolve%0Atemporally%20within%20and%20across%20domains.%20Building%20on%20these%20insights%2C%20we%20propose%20a%0ADGAD%20model%20with%20Dynamic%20Prototypes%20%28DP%29%20to%20capture%20evolving%20domain-specific%20and%0Adomain-agnostic%20patterns.%20Firstly%2C%20DP-DGAD%20extracts%20dynamic%20prototypes%2C%20i.e.%2C%0Aevolving%20representations%20of%20normal%20and%20anomalous%20patterns%2C%20from%20temporal%0Aego-graphs%20and%20stores%20them%20in%20a%20memory%20buffer.%20The%20buffer%20is%20selectively%0Aupdated%20to%20retain%20general%2C%20domain-agnostic%20patterns%20while%20incorporating%20new%0Adomain-specific%20ones.%20Then%2C%20an%20anomaly%20scorer%20compares%20incoming%20data%20with%0Adynamic%20prototypes%20to%20flag%20both%20general%20and%20domain-specific%20anomalies.%20Finally%2C%0ADP-DGAD%20employs%20confidence-based%20pseudo-labeling%20for%20effective%20self-supervised%0Aadaptation%20in%20target%20domains.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20performance%20across%20ten%20real-world%20datasets%20from%20different%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00664v1&entry.124074799=Read"},
{"title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts\n  for Two-View Geometry", "author": "Jiajun Le and Jiayi Ma", "abstract": "  Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.\n", "link": "http://arxiv.org/abs/2508.00592v1", "date": "2025-08-01", "relevancy": 2.7455, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5502}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMoE%3A%20Divide-and-Conquer%20Motion%20Field%20Modeling%20with%20Mixture-of-Experts%0A%20%20for%20Two-View%20Geometry&body=Title%3A%20GeoMoE%3A%20Divide-and-Conquer%20Motion%20Field%20Modeling%20with%20Mixture-of-Experts%0A%20%20for%20Two-View%20Geometry%0AAuthor%3A%20Jiajun%20Le%20and%20Jiayi%20Ma%0AAbstract%3A%20%20%20Recent%20progress%20in%20two-view%20geometry%20increasingly%20emphasizes%20enforcing%0Asmoothness%20and%20global%20consistency%20priors%20when%20estimating%20motion%20fields%20between%0Apairs%20of%20images.%20However%2C%20in%20complex%20real-world%20scenes%2C%20characterized%20by%0Aextreme%20viewpoint%20and%20scale%20changes%20as%20well%20as%20pronounced%20depth%0Adiscontinuities%2C%20the%20motion%20field%20often%20exhibits%20diverse%20and%20heterogeneous%0Amotion%20patterns.%20Most%20existing%20methods%20lack%20targeted%20modeling%20strategies%20and%0Afail%20to%20explicitly%20account%20for%20this%20variability%2C%20resulting%20in%20estimated%20motion%0Afields%20that%20diverge%20from%20their%20true%20underlying%20structure%20and%20distribution.%20We%0Aobserve%20that%20Mixture-of-Experts%20%28MoE%29%20can%20assign%20dedicated%20experts%20to%20motion%0Asub-fields%2C%20enabling%20a%20divide-and-conquer%20strategy%20for%20heterogeneous%20motion%0Apatterns.%20Building%20on%20this%20insight%2C%20we%20re-architect%20motion%20field%20modeling%20in%0Atwo-view%20geometry%20with%20GeoMoE%2C%20a%20streamlined%20framework.%20Specifically%2C%20we%20first%0Adevise%20a%20Probabilistic%20Prior-Guided%20Decomposition%20strategy%20that%20exploits%20inlier%0Aprobability%20signals%20to%20perform%20a%20structure-aware%20decomposition%20of%20the%20motion%0Afield%20into%20heterogeneous%20sub-fields%2C%20sharply%20curbing%20outlier-induced%20bias.%0ANext%2C%20we%20introduce%20an%20MoE-Enhanced%20Bi-Path%20Rectifier%20that%20enhances%20each%0Asub-field%20along%20spatial-context%20and%20channel-semantic%20paths%20and%20routes%20it%20to%20a%0Acustomized%20expert%20for%20targeted%20modeling%2C%20thereby%20decoupling%20heterogeneous%0Amotion%20regimes%2C%20suppressing%20cross-sub-field%20interference%20and%20representational%0Aentanglement%2C%20and%20yielding%20fine-grained%20motion-field%20rectification.%20With%20this%0Aminimalist%20design%2C%20GeoMoE%20outperforms%20prior%20state-of-the-art%20methods%20in%0Arelative%20pose%20and%20homography%20estimation%20and%20shows%20strong%20generalization.%20The%0Asource%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/JiajunLe/GeoMoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMoE%253A%2520Divide-and-Conquer%2520Motion%2520Field%2520Modeling%2520with%2520Mixture-of-Experts%250A%2520%2520for%2520Two-View%2520Geometry%26entry.906535625%3DJiajun%2520Le%2520and%2520Jiayi%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520two-view%2520geometry%2520increasingly%2520emphasizes%2520enforcing%250Asmoothness%2520and%2520global%2520consistency%2520priors%2520when%2520estimating%2520motion%2520fields%2520between%250Apairs%2520of%2520images.%2520However%252C%2520in%2520complex%2520real-world%2520scenes%252C%2520characterized%2520by%250Aextreme%2520viewpoint%2520and%2520scale%2520changes%2520as%2520well%2520as%2520pronounced%2520depth%250Adiscontinuities%252C%2520the%2520motion%2520field%2520often%2520exhibits%2520diverse%2520and%2520heterogeneous%250Amotion%2520patterns.%2520Most%2520existing%2520methods%2520lack%2520targeted%2520modeling%2520strategies%2520and%250Afail%2520to%2520explicitly%2520account%2520for%2520this%2520variability%252C%2520resulting%2520in%2520estimated%2520motion%250Afields%2520that%2520diverge%2520from%2520their%2520true%2520underlying%2520structure%2520and%2520distribution.%2520We%250Aobserve%2520that%2520Mixture-of-Experts%2520%2528MoE%2529%2520can%2520assign%2520dedicated%2520experts%2520to%2520motion%250Asub-fields%252C%2520enabling%2520a%2520divide-and-conquer%2520strategy%2520for%2520heterogeneous%2520motion%250Apatterns.%2520Building%2520on%2520this%2520insight%252C%2520we%2520re-architect%2520motion%2520field%2520modeling%2520in%250Atwo-view%2520geometry%2520with%2520GeoMoE%252C%2520a%2520streamlined%2520framework.%2520Specifically%252C%2520we%2520first%250Adevise%2520a%2520Probabilistic%2520Prior-Guided%2520Decomposition%2520strategy%2520that%2520exploits%2520inlier%250Aprobability%2520signals%2520to%2520perform%2520a%2520structure-aware%2520decomposition%2520of%2520the%2520motion%250Afield%2520into%2520heterogeneous%2520sub-fields%252C%2520sharply%2520curbing%2520outlier-induced%2520bias.%250ANext%252C%2520we%2520introduce%2520an%2520MoE-Enhanced%2520Bi-Path%2520Rectifier%2520that%2520enhances%2520each%250Asub-field%2520along%2520spatial-context%2520and%2520channel-semantic%2520paths%2520and%2520routes%2520it%2520to%2520a%250Acustomized%2520expert%2520for%2520targeted%2520modeling%252C%2520thereby%2520decoupling%2520heterogeneous%250Amotion%2520regimes%252C%2520suppressing%2520cross-sub-field%2520interference%2520and%2520representational%250Aentanglement%252C%2520and%2520yielding%2520fine-grained%2520motion-field%2520rectification.%2520With%2520this%250Aminimalist%2520design%252C%2520GeoMoE%2520outperforms%2520prior%2520state-of-the-art%2520methods%2520in%250Arelative%2520pose%2520and%2520homography%2520estimation%2520and%2520shows%2520strong%2520generalization.%2520The%250Asource%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/JiajunLe/GeoMoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMoE%3A%20Divide-and-Conquer%20Motion%20Field%20Modeling%20with%20Mixture-of-Experts%0A%20%20for%20Two-View%20Geometry&entry.906535625=Jiajun%20Le%20and%20Jiayi%20Ma&entry.1292438233=%20%20Recent%20progress%20in%20two-view%20geometry%20increasingly%20emphasizes%20enforcing%0Asmoothness%20and%20global%20consistency%20priors%20when%20estimating%20motion%20fields%20between%0Apairs%20of%20images.%20However%2C%20in%20complex%20real-world%20scenes%2C%20characterized%20by%0Aextreme%20viewpoint%20and%20scale%20changes%20as%20well%20as%20pronounced%20depth%0Adiscontinuities%2C%20the%20motion%20field%20often%20exhibits%20diverse%20and%20heterogeneous%0Amotion%20patterns.%20Most%20existing%20methods%20lack%20targeted%20modeling%20strategies%20and%0Afail%20to%20explicitly%20account%20for%20this%20variability%2C%20resulting%20in%20estimated%20motion%0Afields%20that%20diverge%20from%20their%20true%20underlying%20structure%20and%20distribution.%20We%0Aobserve%20that%20Mixture-of-Experts%20%28MoE%29%20can%20assign%20dedicated%20experts%20to%20motion%0Asub-fields%2C%20enabling%20a%20divide-and-conquer%20strategy%20for%20heterogeneous%20motion%0Apatterns.%20Building%20on%20this%20insight%2C%20we%20re-architect%20motion%20field%20modeling%20in%0Atwo-view%20geometry%20with%20GeoMoE%2C%20a%20streamlined%20framework.%20Specifically%2C%20we%20first%0Adevise%20a%20Probabilistic%20Prior-Guided%20Decomposition%20strategy%20that%20exploits%20inlier%0Aprobability%20signals%20to%20perform%20a%20structure-aware%20decomposition%20of%20the%20motion%0Afield%20into%20heterogeneous%20sub-fields%2C%20sharply%20curbing%20outlier-induced%20bias.%0ANext%2C%20we%20introduce%20an%20MoE-Enhanced%20Bi-Path%20Rectifier%20that%20enhances%20each%0Asub-field%20along%20spatial-context%20and%20channel-semantic%20paths%20and%20routes%20it%20to%20a%0Acustomized%20expert%20for%20targeted%20modeling%2C%20thereby%20decoupling%20heterogeneous%0Amotion%20regimes%2C%20suppressing%20cross-sub-field%20interference%20and%20representational%0Aentanglement%2C%20and%20yielding%20fine-grained%20motion-field%20rectification.%20With%20this%0Aminimalist%20design%2C%20GeoMoE%20outperforms%20prior%20state-of-the-art%20methods%20in%0Arelative%20pose%20and%20homography%20estimation%20and%20shows%20strong%20generalization.%20The%0Asource%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/JiajunLe/GeoMoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00592v1&entry.124074799=Read"},
{"title": "SCORE: Saturated Consensus Relocalization in Semantic Line Maps", "author": "Haodong Jiang and Xiang Zheng and Yanglin Zhang and Qingcheng Zeng and Yiqian Li and Ziyang Hong and Junfeng Wu", "abstract": "  We present SCORE, a visual relocalization system that achieves unprecedented\nmap compactness by adopting semantically labeled 3D line maps. SCORE requires\nonly 0.01\\%-0.1\\% of the storage needed by structure-based or learning-based\nbaselines, while maintaining practical accuracy and comparable runtime. The key\ninnovation is a novel robust estimation mechanism, Saturated Consensus\nMaximization (Sat-CM), which generalizes classical Consensus Maximization (CM)\nby assigning diminishing weights to inlier associations according to maximum\nlikelihood with probabilistic justification. Under extreme outlier ratios (up\nto 99.5\\%) arising from one-to-many ambiguity in semantic matching, Sat-CM\nenables accurate estimation when CM fails. To ensure computational efficiency,\nwe propose an accelerating framework for globally solving Sat-CM formulations\nand specialize it for the Perspective-n-Lines problem at the core of SCORE.\n", "link": "http://arxiv.org/abs/2503.03254v2", "date": "2025-08-01", "relevancy": 2.7208, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCORE%3A%20Saturated%20Consensus%20Relocalization%20in%20Semantic%20Line%20Maps&body=Title%3A%20SCORE%3A%20Saturated%20Consensus%20Relocalization%20in%20Semantic%20Line%20Maps%0AAuthor%3A%20Haodong%20Jiang%20and%20Xiang%20Zheng%20and%20Yanglin%20Zhang%20and%20Qingcheng%20Zeng%20and%20Yiqian%20Li%20and%20Ziyang%20Hong%20and%20Junfeng%20Wu%0AAbstract%3A%20%20%20We%20present%20SCORE%2C%20a%20visual%20relocalization%20system%20that%20achieves%20unprecedented%0Amap%20compactness%20by%20adopting%20semantically%20labeled%203D%20line%20maps.%20SCORE%20requires%0Aonly%200.01%5C%25-0.1%5C%25%20of%20the%20storage%20needed%20by%20structure-based%20or%20learning-based%0Abaselines%2C%20while%20maintaining%20practical%20accuracy%20and%20comparable%20runtime.%20The%20key%0Ainnovation%20is%20a%20novel%20robust%20estimation%20mechanism%2C%20Saturated%20Consensus%0AMaximization%20%28Sat-CM%29%2C%20which%20generalizes%20classical%20Consensus%20Maximization%20%28CM%29%0Aby%20assigning%20diminishing%20weights%20to%20inlier%20associations%20according%20to%20maximum%0Alikelihood%20with%20probabilistic%20justification.%20Under%20extreme%20outlier%20ratios%20%28up%0Ato%2099.5%5C%25%29%20arising%20from%20one-to-many%20ambiguity%20in%20semantic%20matching%2C%20Sat-CM%0Aenables%20accurate%20estimation%20when%20CM%20fails.%20To%20ensure%20computational%20efficiency%2C%0Awe%20propose%20an%20accelerating%20framework%20for%20globally%20solving%20Sat-CM%20formulations%0Aand%20specialize%20it%20for%20the%20Perspective-n-Lines%20problem%20at%20the%20core%20of%20SCORE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCORE%253A%2520Saturated%2520Consensus%2520Relocalization%2520in%2520Semantic%2520Line%2520Maps%26entry.906535625%3DHaodong%2520Jiang%2520and%2520Xiang%2520Zheng%2520and%2520Yanglin%2520Zhang%2520and%2520Qingcheng%2520Zeng%2520and%2520Yiqian%2520Li%2520and%2520Ziyang%2520Hong%2520and%2520Junfeng%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520SCORE%252C%2520a%2520visual%2520relocalization%2520system%2520that%2520achieves%2520unprecedented%250Amap%2520compactness%2520by%2520adopting%2520semantically%2520labeled%25203D%2520line%2520maps.%2520SCORE%2520requires%250Aonly%25200.01%255C%2525-0.1%255C%2525%2520of%2520the%2520storage%2520needed%2520by%2520structure-based%2520or%2520learning-based%250Abaselines%252C%2520while%2520maintaining%2520practical%2520accuracy%2520and%2520comparable%2520runtime.%2520The%2520key%250Ainnovation%2520is%2520a%2520novel%2520robust%2520estimation%2520mechanism%252C%2520Saturated%2520Consensus%250AMaximization%2520%2528Sat-CM%2529%252C%2520which%2520generalizes%2520classical%2520Consensus%2520Maximization%2520%2528CM%2529%250Aby%2520assigning%2520diminishing%2520weights%2520to%2520inlier%2520associations%2520according%2520to%2520maximum%250Alikelihood%2520with%2520probabilistic%2520justification.%2520Under%2520extreme%2520outlier%2520ratios%2520%2528up%250Ato%252099.5%255C%2525%2529%2520arising%2520from%2520one-to-many%2520ambiguity%2520in%2520semantic%2520matching%252C%2520Sat-CM%250Aenables%2520accurate%2520estimation%2520when%2520CM%2520fails.%2520To%2520ensure%2520computational%2520efficiency%252C%250Awe%2520propose%2520an%2520accelerating%2520framework%2520for%2520globally%2520solving%2520Sat-CM%2520formulations%250Aand%2520specialize%2520it%2520for%2520the%2520Perspective-n-Lines%2520problem%2520at%2520the%2520core%2520of%2520SCORE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCORE%3A%20Saturated%20Consensus%20Relocalization%20in%20Semantic%20Line%20Maps&entry.906535625=Haodong%20Jiang%20and%20Xiang%20Zheng%20and%20Yanglin%20Zhang%20and%20Qingcheng%20Zeng%20and%20Yiqian%20Li%20and%20Ziyang%20Hong%20and%20Junfeng%20Wu&entry.1292438233=%20%20We%20present%20SCORE%2C%20a%20visual%20relocalization%20system%20that%20achieves%20unprecedented%0Amap%20compactness%20by%20adopting%20semantically%20labeled%203D%20line%20maps.%20SCORE%20requires%0Aonly%200.01%5C%25-0.1%5C%25%20of%20the%20storage%20needed%20by%20structure-based%20or%20learning-based%0Abaselines%2C%20while%20maintaining%20practical%20accuracy%20and%20comparable%20runtime.%20The%20key%0Ainnovation%20is%20a%20novel%20robust%20estimation%20mechanism%2C%20Saturated%20Consensus%0AMaximization%20%28Sat-CM%29%2C%20which%20generalizes%20classical%20Consensus%20Maximization%20%28CM%29%0Aby%20assigning%20diminishing%20weights%20to%20inlier%20associations%20according%20to%20maximum%0Alikelihood%20with%20probabilistic%20justification.%20Under%20extreme%20outlier%20ratios%20%28up%0Ato%2099.5%5C%25%29%20arising%20from%20one-to-many%20ambiguity%20in%20semantic%20matching%2C%20Sat-CM%0Aenables%20accurate%20estimation%20when%20CM%20fails.%20To%20ensure%20computational%20efficiency%2C%0Awe%20propose%20an%20accelerating%20framework%20for%20globally%20solving%20Sat-CM%20formulations%0Aand%20specialize%20it%20for%20the%20Perspective-n-Lines%20problem%20at%20the%20core%20of%20SCORE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03254v2&entry.124074799=Read"},
{"title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts", "author": "Peijie Wang and Zhong-Zhi Li and Fei Yin and Xin Yang and Dekang Ran and Cheng-Lin Liu", "abstract": "  Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.\n", "link": "http://arxiv.org/abs/2502.20808v6", "date": "2025-08-01", "relevancy": 2.669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts&body=Title%3A%20MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts%0AAuthor%3A%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Fei%20Yin%20and%20Xin%20Yang%20and%20Dekang%20Ran%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20capabilities%20in%0Amathematical%20reasoning%20within%20visual%20contexts%20across%20various%20datasets.%20However%2C%0Amost%20existing%20multimodal%20math%20benchmarks%20are%20limited%20to%20single-visual%20contexts%2C%0Awhich%20diverges%20from%20the%20multi-visual%20scenarios%20commonly%20encountered%20in%0Areal-world%20mathematical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%0AMV-MATH%3A%20a%20meticulously%20curated%20dataset%20of%202%2C009%20high-quality%20mathematical%0Aproblems.%20Each%20problem%20integrates%20multiple%20images%20interleaved%20with%20text%2C%0Aderived%20from%20authentic%20K-12%20scenarios%2C%20and%20enriched%20with%20detailed%20annotations.%0AMV-MATH%20includes%20multiple-choice%2C%20free-form%2C%20and%20multi-step%20questions%2C%20covering%0A11%20subject%20areas%20across%203%20difficulty%20levels%2C%20and%20serves%20as%20a%20comprehensive%20and%0Arigorous%20benchmark%20for%20assessing%20MLLMs%27%20mathematical%20reasoning%20in%20multi-visual%0Acontexts.%20Through%20extensive%20experimentation%2C%20we%20observe%20that%20MLLMs%20encounter%0Asubstantial%20challenges%20in%20multi-visual%20math%20tasks%2C%20with%20a%20considerable%0Aperformance%20gap%20relative%20to%20human%20capabilities%20on%20MV-MATH.%20Furthermore%2C%20we%0Aanalyze%20the%20performance%20and%20error%20patterns%20of%20various%20models%2C%20providing%0Ainsights%20into%20MLLMs%27%20mathematical%20reasoning%20capabilities%20within%20multi-visual%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20808v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-MATH%253A%2520Evaluating%2520Multimodal%2520Math%2520Reasoning%2520in%2520Multi-Visual%2520Contexts%26entry.906535625%3DPeijie%2520Wang%2520and%2520Zhong-Zhi%2520Li%2520and%2520Fei%2520Yin%2520and%2520Xin%2520Yang%2520and%2520Dekang%2520Ran%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520shown%2520promising%2520capabilities%2520in%250Amathematical%2520reasoning%2520within%2520visual%2520contexts%2520across%2520various%2520datasets.%2520However%252C%250Amost%2520existing%2520multimodal%2520math%2520benchmarks%2520are%2520limited%2520to%2520single-visual%2520contexts%252C%250Awhich%2520diverges%2520from%2520the%2520multi-visual%2520scenarios%2520commonly%2520encountered%2520in%250Areal-world%2520mathematical%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AMV-MATH%253A%2520a%2520meticulously%2520curated%2520dataset%2520of%25202%252C009%2520high-quality%2520mathematical%250Aproblems.%2520Each%2520problem%2520integrates%2520multiple%2520images%2520interleaved%2520with%2520text%252C%250Aderived%2520from%2520authentic%2520K-12%2520scenarios%252C%2520and%2520enriched%2520with%2520detailed%2520annotations.%250AMV-MATH%2520includes%2520multiple-choice%252C%2520free-form%252C%2520and%2520multi-step%2520questions%252C%2520covering%250A11%2520subject%2520areas%2520across%25203%2520difficulty%2520levels%252C%2520and%2520serves%2520as%2520a%2520comprehensive%2520and%250Arigorous%2520benchmark%2520for%2520assessing%2520MLLMs%2527%2520mathematical%2520reasoning%2520in%2520multi-visual%250Acontexts.%2520Through%2520extensive%2520experimentation%252C%2520we%2520observe%2520that%2520MLLMs%2520encounter%250Asubstantial%2520challenges%2520in%2520multi-visual%2520math%2520tasks%252C%2520with%2520a%2520considerable%250Aperformance%2520gap%2520relative%2520to%2520human%2520capabilities%2520on%2520MV-MATH.%2520Furthermore%252C%2520we%250Aanalyze%2520the%2520performance%2520and%2520error%2520patterns%2520of%2520various%2520models%252C%2520providing%250Ainsights%2520into%2520MLLMs%2527%2520mathematical%2520reasoning%2520capabilities%2520within%2520multi-visual%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20808v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-MATH%3A%20Evaluating%20Multimodal%20Math%20Reasoning%20in%20Multi-Visual%20Contexts&entry.906535625=Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Fei%20Yin%20and%20Xin%20Yang%20and%20Dekang%20Ran%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20shown%20promising%20capabilities%20in%0Amathematical%20reasoning%20within%20visual%20contexts%20across%20various%20datasets.%20However%2C%0Amost%20existing%20multimodal%20math%20benchmarks%20are%20limited%20to%20single-visual%20contexts%2C%0Awhich%20diverges%20from%20the%20multi-visual%20scenarios%20commonly%20encountered%20in%0Areal-world%20mathematical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%0AMV-MATH%3A%20a%20meticulously%20curated%20dataset%20of%202%2C009%20high-quality%20mathematical%0Aproblems.%20Each%20problem%20integrates%20multiple%20images%20interleaved%20with%20text%2C%0Aderived%20from%20authentic%20K-12%20scenarios%2C%20and%20enriched%20with%20detailed%20annotations.%0AMV-MATH%20includes%20multiple-choice%2C%20free-form%2C%20and%20multi-step%20questions%2C%20covering%0A11%20subject%20areas%20across%203%20difficulty%20levels%2C%20and%20serves%20as%20a%20comprehensive%20and%0Arigorous%20benchmark%20for%20assessing%20MLLMs%27%20mathematical%20reasoning%20in%20multi-visual%0Acontexts.%20Through%20extensive%20experimentation%2C%20we%20observe%20that%20MLLMs%20encounter%0Asubstantial%20challenges%20in%20multi-visual%20math%20tasks%2C%20with%20a%20considerable%0Aperformance%20gap%20relative%20to%20human%20capabilities%20on%20MV-MATH.%20Furthermore%2C%20we%0Aanalyze%20the%20performance%20and%20error%20patterns%20of%20various%20models%2C%20providing%0Ainsights%20into%20MLLMs%27%20mathematical%20reasoning%20capabilities%20within%20multi-visual%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20808v6&entry.124074799=Read"},
{"title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited\n  computing resources", "author": "Paul Tresson and Pierre Le Coz and Hadrien Tulet and Anthony Malkassian and Maxime R\u00e9jou M\u00e9chain", "abstract": "  Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.\n", "link": "http://arxiv.org/abs/2508.00627v1", "date": "2025-08-01", "relevancy": 2.6617, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5563}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAMAP%3A%20Unlocking%20Deep%20Learning%20in%20QGIS%20for%20non-coders%20and%20limited%0A%20%20computing%20resources&body=Title%3A%20IAMAP%3A%20Unlocking%20Deep%20Learning%20in%20QGIS%20for%20non-coders%20and%20limited%0A%20%20computing%20resources%0AAuthor%3A%20Paul%20Tresson%20and%20Pierre%20Le%20Coz%20and%20Hadrien%20Tulet%20and%20Anthony%20Malkassian%20and%20Maxime%20R%C3%A9jou%20M%C3%A9chain%0AAbstract%3A%20%20%20Remote%20sensing%20has%20entered%20a%20new%20era%20with%20the%20rapid%20development%20of%20artificial%0Aintelligence%20approaches.%20However%2C%20the%20implementation%20of%20deep%20learning%20has%0Alargely%20remained%20restricted%20to%20specialists%20and%20has%20been%20impractical%20because%20it%0Aoften%20requires%20%28i%29%20large%20reference%20datasets%20for%20model%20training%20and%20validation%3B%0A%28ii%29%20substantial%20computing%20resources%3B%20and%20%28iii%29%20strong%20coding%20skills.%20Here%2C%20we%0Aintroduce%20IAMAP%2C%20a%20user-friendly%20QGIS%20plugin%20that%20addresses%20these%20three%0Achallenges%20in%20an%20easy%20yet%20flexible%20way.%20IAMAP%20builds%20on%20recent%20advancements%20in%0Aself-supervised%20learning%20strategies%2C%20which%20now%20provide%20robust%20feature%0Aextractors%2C%20often%20referred%20to%20as%20foundation%20models.%20These%20generalist%20models%20can%0Aoften%20be%20reliably%20used%20in%20few-shot%20or%20zero-shot%20scenarios%20%28i.e.%2C%20with%20little%20to%0Ano%20fine-tuning%29.%20IAMAP%27s%20interface%20allows%20users%20to%20streamline%20several%20key%20steps%0Ain%20remote%20sensing%20image%20analysis%3A%20%28i%29%20extracting%20image%20features%20using%20a%20wide%0Arange%20of%20deep%20learning%20architectures%3B%20%28ii%29%20reducing%20dimensionality%20with%0Abuilt-in%20algorithms%3B%20%28iii%29%20performing%20clustering%20on%20features%20or%20their%20reduced%0Arepresentations%3B%20%28iv%29%20generating%20feature%20similarity%20maps%3B%20and%20%28v%29%20calibrating%0Aand%20validating%20supervised%20machine%20learning%20models%20for%20prediction.%20By%20enabling%0Anon-AI%20specialists%20to%20leverage%20the%20high-quality%20features%20provided%20by%20recent%0Adeep%20learning%20approaches%20without%20requiring%20GPU%20capacity%20or%20extensive%20reference%0Adatasets%2C%20IAMAP%20contributes%20to%20the%20democratization%20of%20computationally%20efficient%0Aand%20energy-conscious%20deep%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAMAP%253A%2520Unlocking%2520Deep%2520Learning%2520in%2520QGIS%2520for%2520non-coders%2520and%2520limited%250A%2520%2520computing%2520resources%26entry.906535625%3DPaul%2520Tresson%2520and%2520Pierre%2520Le%2520Coz%2520and%2520Hadrien%2520Tulet%2520and%2520Anthony%2520Malkassian%2520and%2520Maxime%2520R%25C3%25A9jou%2520M%25C3%25A9chain%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520has%2520entered%2520a%2520new%2520era%2520with%2520the%2520rapid%2520development%2520of%2520artificial%250Aintelligence%2520approaches.%2520However%252C%2520the%2520implementation%2520of%2520deep%2520learning%2520has%250Alargely%2520remained%2520restricted%2520to%2520specialists%2520and%2520has%2520been%2520impractical%2520because%2520it%250Aoften%2520requires%2520%2528i%2529%2520large%2520reference%2520datasets%2520for%2520model%2520training%2520and%2520validation%253B%250A%2528ii%2529%2520substantial%2520computing%2520resources%253B%2520and%2520%2528iii%2529%2520strong%2520coding%2520skills.%2520Here%252C%2520we%250Aintroduce%2520IAMAP%252C%2520a%2520user-friendly%2520QGIS%2520plugin%2520that%2520addresses%2520these%2520three%250Achallenges%2520in%2520an%2520easy%2520yet%2520flexible%2520way.%2520IAMAP%2520builds%2520on%2520recent%2520advancements%2520in%250Aself-supervised%2520learning%2520strategies%252C%2520which%2520now%2520provide%2520robust%2520feature%250Aextractors%252C%2520often%2520referred%2520to%2520as%2520foundation%2520models.%2520These%2520generalist%2520models%2520can%250Aoften%2520be%2520reliably%2520used%2520in%2520few-shot%2520or%2520zero-shot%2520scenarios%2520%2528i.e.%252C%2520with%2520little%2520to%250Ano%2520fine-tuning%2529.%2520IAMAP%2527s%2520interface%2520allows%2520users%2520to%2520streamline%2520several%2520key%2520steps%250Ain%2520remote%2520sensing%2520image%2520analysis%253A%2520%2528i%2529%2520extracting%2520image%2520features%2520using%2520a%2520wide%250Arange%2520of%2520deep%2520learning%2520architectures%253B%2520%2528ii%2529%2520reducing%2520dimensionality%2520with%250Abuilt-in%2520algorithms%253B%2520%2528iii%2529%2520performing%2520clustering%2520on%2520features%2520or%2520their%2520reduced%250Arepresentations%253B%2520%2528iv%2529%2520generating%2520feature%2520similarity%2520maps%253B%2520and%2520%2528v%2529%2520calibrating%250Aand%2520validating%2520supervised%2520machine%2520learning%2520models%2520for%2520prediction.%2520By%2520enabling%250Anon-AI%2520specialists%2520to%2520leverage%2520the%2520high-quality%2520features%2520provided%2520by%2520recent%250Adeep%2520learning%2520approaches%2520without%2520requiring%2520GPU%2520capacity%2520or%2520extensive%2520reference%250Adatasets%252C%2520IAMAP%2520contributes%2520to%2520the%2520democratization%2520of%2520computationally%2520efficient%250Aand%2520energy-conscious%2520deep%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAMAP%3A%20Unlocking%20Deep%20Learning%20in%20QGIS%20for%20non-coders%20and%20limited%0A%20%20computing%20resources&entry.906535625=Paul%20Tresson%20and%20Pierre%20Le%20Coz%20and%20Hadrien%20Tulet%20and%20Anthony%20Malkassian%20and%20Maxime%20R%C3%A9jou%20M%C3%A9chain&entry.1292438233=%20%20Remote%20sensing%20has%20entered%20a%20new%20era%20with%20the%20rapid%20development%20of%20artificial%0Aintelligence%20approaches.%20However%2C%20the%20implementation%20of%20deep%20learning%20has%0Alargely%20remained%20restricted%20to%20specialists%20and%20has%20been%20impractical%20because%20it%0Aoften%20requires%20%28i%29%20large%20reference%20datasets%20for%20model%20training%20and%20validation%3B%0A%28ii%29%20substantial%20computing%20resources%3B%20and%20%28iii%29%20strong%20coding%20skills.%20Here%2C%20we%0Aintroduce%20IAMAP%2C%20a%20user-friendly%20QGIS%20plugin%20that%20addresses%20these%20three%0Achallenges%20in%20an%20easy%20yet%20flexible%20way.%20IAMAP%20builds%20on%20recent%20advancements%20in%0Aself-supervised%20learning%20strategies%2C%20which%20now%20provide%20robust%20feature%0Aextractors%2C%20often%20referred%20to%20as%20foundation%20models.%20These%20generalist%20models%20can%0Aoften%20be%20reliably%20used%20in%20few-shot%20or%20zero-shot%20scenarios%20%28i.e.%2C%20with%20little%20to%0Ano%20fine-tuning%29.%20IAMAP%27s%20interface%20allows%20users%20to%20streamline%20several%20key%20steps%0Ain%20remote%20sensing%20image%20analysis%3A%20%28i%29%20extracting%20image%20features%20using%20a%20wide%0Arange%20of%20deep%20learning%20architectures%3B%20%28ii%29%20reducing%20dimensionality%20with%0Abuilt-in%20algorithms%3B%20%28iii%29%20performing%20clustering%20on%20features%20or%20their%20reduced%0Arepresentations%3B%20%28iv%29%20generating%20feature%20similarity%20maps%3B%20and%20%28v%29%20calibrating%0Aand%20validating%20supervised%20machine%20learning%20models%20for%20prediction.%20By%20enabling%0Anon-AI%20specialists%20to%20leverage%20the%20high-quality%20features%20provided%20by%20recent%0Adeep%20learning%20approaches%20without%20requiring%20GPU%20capacity%20or%20extensive%20reference%0Adatasets%2C%20IAMAP%20contributes%20to%20the%20democratization%20of%20computationally%20efficient%0Aand%20energy-conscious%20deep%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00627v1&entry.124074799=Read"},
{"title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "author": "Adwait Chandorkar and Hasan Tercan and Tobias Meisen", "abstract": "  Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.\n", "link": "http://arxiv.org/abs/2508.00744v1", "date": "2025-08-01", "relevancy": 2.6529, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR&body=Title%3A%20Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR%0AAuthor%3A%20Adwait%20Chandorkar%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20LiDAR-based%203D%20object%20detection%20have%20significantly%0Aaccelerated%20progress%20toward%20the%20realization%20of%20fully%20autonomous%20driving%20in%0Areal-world%20environments.%20Despite%20achieving%20high%20detection%20performance%2C%20most%20of%0Athe%20approaches%20still%20rely%20on%20a%20VGG-based%20or%20ResNet-based%20backbone%20for%20feature%0Aexploration%2C%20which%20increases%20the%20model%20complexity.%20Lightweight%20backbone%20design%0Ais%20well-explored%20for%202D%20object%20detection%2C%20but%20research%20on%203D%20object%20detection%0Astill%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20Dense%20Backbone%2C%20a%20lightweight%0Abackbone%20that%20combines%20the%20benefits%20of%20high%20processing%20speed%2C%20lightweight%0Aarchitecture%2C%20and%20robust%20detection%20accuracy.%20We%20adapt%20multiple%20SoTA%203d%20object%0Adetectors%2C%20such%20as%20PillarNet%2C%20with%20our%20backbone%20and%20show%20that%20with%20our%0Abackbone%2C%20these%20models%20retain%20most%20of%20their%20detection%20capability%20at%20a%0Asignificantly%20reduced%20computational%20cost.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Adense-layer-based%20backbone%20tailored%20specifically%20for%203D%20object%20detection%20from%0Apoint%20cloud%20data.%20DensePillarNet%2C%20our%20adaptation%20of%20PillarNet%2C%20achieves%20a%2029%25%0Areduction%20in%20model%20parameters%20and%20a%2028%25%20reduction%20in%20latency%20with%20just%20a%202%25%0Adrop%20in%20detection%20accuracy%20on%20the%20nuScenes%20test%20set.%20Furthermore%2C%20Dense%0ABackbone%27s%20plug-and-play%20design%20allows%20straightforward%20integration%20into%0Aexisting%20architectures%2C%20requiring%20no%20modifications%20to%20other%20network%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Backbone%2520Design%2520for%2520Lightweight%25203D%2520Object%2520Detection%2520in%2520LiDAR%26entry.906535625%3DAdwait%2520Chandorkar%2520and%2520Hasan%2520Tercan%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520LiDAR-based%25203D%2520object%2520detection%2520have%2520significantly%250Aaccelerated%2520progress%2520toward%2520the%2520realization%2520of%2520fully%2520autonomous%2520driving%2520in%250Areal-world%2520environments.%2520Despite%2520achieving%2520high%2520detection%2520performance%252C%2520most%2520of%250Athe%2520approaches%2520still%2520rely%2520on%2520a%2520VGG-based%2520or%2520ResNet-based%2520backbone%2520for%2520feature%250Aexploration%252C%2520which%2520increases%2520the%2520model%2520complexity.%2520Lightweight%2520backbone%2520design%250Ais%2520well-explored%2520for%25202D%2520object%2520detection%252C%2520but%2520research%2520on%25203D%2520object%2520detection%250Astill%2520remains%2520limited.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Dense%2520Backbone%252C%2520a%2520lightweight%250Abackbone%2520that%2520combines%2520the%2520benefits%2520of%2520high%2520processing%2520speed%252C%2520lightweight%250Aarchitecture%252C%2520and%2520robust%2520detection%2520accuracy.%2520We%2520adapt%2520multiple%2520SoTA%25203d%2520object%250Adetectors%252C%2520such%2520as%2520PillarNet%252C%2520with%2520our%2520backbone%2520and%2520show%2520that%2520with%2520our%250Abackbone%252C%2520these%2520models%2520retain%2520most%2520of%2520their%2520detection%2520capability%2520at%2520a%250Asignificantly%2520reduced%2520computational%2520cost.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Adense-layer-based%2520backbone%2520tailored%2520specifically%2520for%25203D%2520object%2520detection%2520from%250Apoint%2520cloud%2520data.%2520DensePillarNet%252C%2520our%2520adaptation%2520of%2520PillarNet%252C%2520achieves%2520a%252029%2525%250Areduction%2520in%2520model%2520parameters%2520and%2520a%252028%2525%2520reduction%2520in%2520latency%2520with%2520just%2520a%25202%2525%250Adrop%2520in%2520detection%2520accuracy%2520on%2520the%2520nuScenes%2520test%2520set.%2520Furthermore%252C%2520Dense%250ABackbone%2527s%2520plug-and-play%2520design%2520allows%2520straightforward%2520integration%2520into%250Aexisting%2520architectures%252C%2520requiring%2520no%2520modifications%2520to%2520other%2520network%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Backbone%20Design%20for%20Lightweight%203D%20Object%20Detection%20in%20LiDAR&entry.906535625=Adwait%20Chandorkar%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen&entry.1292438233=%20%20Recent%20advancements%20in%20LiDAR-based%203D%20object%20detection%20have%20significantly%0Aaccelerated%20progress%20toward%20the%20realization%20of%20fully%20autonomous%20driving%20in%0Areal-world%20environments.%20Despite%20achieving%20high%20detection%20performance%2C%20most%20of%0Athe%20approaches%20still%20rely%20on%20a%20VGG-based%20or%20ResNet-based%20backbone%20for%20feature%0Aexploration%2C%20which%20increases%20the%20model%20complexity.%20Lightweight%20backbone%20design%0Ais%20well-explored%20for%202D%20object%20detection%2C%20but%20research%20on%203D%20object%20detection%0Astill%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20Dense%20Backbone%2C%20a%20lightweight%0Abackbone%20that%20combines%20the%20benefits%20of%20high%20processing%20speed%2C%20lightweight%0Aarchitecture%2C%20and%20robust%20detection%20accuracy.%20We%20adapt%20multiple%20SoTA%203d%20object%0Adetectors%2C%20such%20as%20PillarNet%2C%20with%20our%20backbone%20and%20show%20that%20with%20our%0Abackbone%2C%20these%20models%20retain%20most%20of%20their%20detection%20capability%20at%20a%0Asignificantly%20reduced%20computational%20cost.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Adense-layer-based%20backbone%20tailored%20specifically%20for%203D%20object%20detection%20from%0Apoint%20cloud%20data.%20DensePillarNet%2C%20our%20adaptation%20of%20PillarNet%2C%20achieves%20a%2029%25%0Areduction%20in%20model%20parameters%20and%20a%2028%25%20reduction%20in%20latency%20with%20just%20a%202%25%0Adrop%20in%20detection%20accuracy%20on%20the%20nuScenes%20test%20set.%20Furthermore%2C%20Dense%0ABackbone%27s%20plug-and-play%20design%20allows%20straightforward%20integration%20into%0Aexisting%20architectures%2C%20requiring%20no%20modifications%20to%20other%20network%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00744v1&entry.124074799=Read"},
{"title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting\n  with CounterNet", "author": "Xiaoyu Zhang and Zhifeng Bao and Hai Dong and Ziwei Wang and Jiajun Liu", "abstract": "  Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.\n", "link": "http://arxiv.org/abs/2507.19209v2", "date": "2025-08-01", "relevancy": 2.6192, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5115}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Querying%20Autonomous%20Vehicle%20Point%20Clouds%3A%20Enhanced%20by%203D%20Object%20Counting%0A%20%20with%20CounterNet&body=Title%3A%20Querying%20Autonomous%20Vehicle%20Point%20Clouds%3A%20Enhanced%20by%203D%20Object%20Counting%0A%20%20with%20CounterNet%0AAuthor%3A%20Xiaoyu%20Zhang%20and%20Zhifeng%20Bao%20and%20Hai%20Dong%20and%20Ziwei%20Wang%20and%20Jiajun%20Liu%0AAbstract%3A%20%20%20Autonomous%20vehicles%20generate%20massive%20volumes%20of%20point%20cloud%20data%2C%20yet%20only%20a%0Asubset%20is%20relevant%20for%20specific%20tasks%20such%20as%20collision%20detection%2C%20traffic%0Aanalysis%2C%20or%20congestion%20monitoring.%20Effectively%20querying%20this%20data%20is%20essential%0Ato%20enable%20targeted%20analytics.%20In%20this%20work%2C%20we%20formalize%20point%20cloud%20querying%0Aby%20defining%20three%20core%20query%20types%3A%20RETRIEVAL%2C%20COUNT%2C%20and%20AGGREGATION%2C%20each%0Aaligned%20with%20distinct%20analytical%20scenarios.%20All%20these%20queries%20rely%20heavily%20on%0Aaccurate%20object%20counts%20to%20produce%20meaningful%20results%2C%20making%20precise%20object%0Acounting%20a%20critical%20component%20of%20query%20execution.%20Prior%20work%20has%20focused%20on%0Aindexing%20techniques%20for%202D%20video%20data%2C%20assuming%20detection%20models%20provide%0Aaccurate%20counting%20information.%20However%2C%20when%20applied%20to%203D%20point%20cloud%20data%2C%0Astate-of-the-art%20detection%20models%20often%20fail%20to%20generate%20reliable%20object%0Acounts%2C%20leading%20to%20substantial%20errors%20in%20query%20results.%20To%20address%20this%0Alimitation%2C%20we%20propose%20CounterNet%2C%20a%20heatmap-based%20network%20designed%20for%0Aaccurate%20object%20counting%20in%20large-scale%20point%20cloud%20data.%20Rather%20than%20focusing%0Aon%20accurate%20object%20localization%2C%20CounterNet%20detects%20object%20presence%20by%20finding%0Aobject%20centers%20to%20improve%20counting%20accuracy.%20We%20further%20enhance%20its%20performance%0Awith%20a%20feature%20map%20partitioning%20strategy%20using%20overlapping%20regions%2C%20enabling%0Abetter%20handling%20of%20both%20small%20and%20large%20objects%20in%20complex%20traffic%20scenes.%20To%0Aadapt%20to%20varying%20frame%20characteristics%2C%20we%20introduce%20a%20per-frame%20dynamic%20model%0Aselection%20strategy%20that%20selects%20the%20most%20effective%20configuration%20for%20each%0Ainput.%20Evaluations%20on%20three%20real-world%20autonomous%20vehicle%20datasets%20show%20that%0ACounterNet%20improves%20counting%20accuracy%20by%205%25%20to%2020%25%20across%20object%20categories%2C%0Aresulting%20in%20more%20reliable%20query%20outcomes%20across%20all%20supported%20query%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuerying%2520Autonomous%2520Vehicle%2520Point%2520Clouds%253A%2520Enhanced%2520by%25203D%2520Object%2520Counting%250A%2520%2520with%2520CounterNet%26entry.906535625%3DXiaoyu%2520Zhang%2520and%2520Zhifeng%2520Bao%2520and%2520Hai%2520Dong%2520and%2520Ziwei%2520Wang%2520and%2520Jiajun%2520Liu%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520generate%2520massive%2520volumes%2520of%2520point%2520cloud%2520data%252C%2520yet%2520only%2520a%250Asubset%2520is%2520relevant%2520for%2520specific%2520tasks%2520such%2520as%2520collision%2520detection%252C%2520traffic%250Aanalysis%252C%2520or%2520congestion%2520monitoring.%2520Effectively%2520querying%2520this%2520data%2520is%2520essential%250Ato%2520enable%2520targeted%2520analytics.%2520In%2520this%2520work%252C%2520we%2520formalize%2520point%2520cloud%2520querying%250Aby%2520defining%2520three%2520core%2520query%2520types%253A%2520RETRIEVAL%252C%2520COUNT%252C%2520and%2520AGGREGATION%252C%2520each%250Aaligned%2520with%2520distinct%2520analytical%2520scenarios.%2520All%2520these%2520queries%2520rely%2520heavily%2520on%250Aaccurate%2520object%2520counts%2520to%2520produce%2520meaningful%2520results%252C%2520making%2520precise%2520object%250Acounting%2520a%2520critical%2520component%2520of%2520query%2520execution.%2520Prior%2520work%2520has%2520focused%2520on%250Aindexing%2520techniques%2520for%25202D%2520video%2520data%252C%2520assuming%2520detection%2520models%2520provide%250Aaccurate%2520counting%2520information.%2520However%252C%2520when%2520applied%2520to%25203D%2520point%2520cloud%2520data%252C%250Astate-of-the-art%2520detection%2520models%2520often%2520fail%2520to%2520generate%2520reliable%2520object%250Acounts%252C%2520leading%2520to%2520substantial%2520errors%2520in%2520query%2520results.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520CounterNet%252C%2520a%2520heatmap-based%2520network%2520designed%2520for%250Aaccurate%2520object%2520counting%2520in%2520large-scale%2520point%2520cloud%2520data.%2520Rather%2520than%2520focusing%250Aon%2520accurate%2520object%2520localization%252C%2520CounterNet%2520detects%2520object%2520presence%2520by%2520finding%250Aobject%2520centers%2520to%2520improve%2520counting%2520accuracy.%2520We%2520further%2520enhance%2520its%2520performance%250Awith%2520a%2520feature%2520map%2520partitioning%2520strategy%2520using%2520overlapping%2520regions%252C%2520enabling%250Abetter%2520handling%2520of%2520both%2520small%2520and%2520large%2520objects%2520in%2520complex%2520traffic%2520scenes.%2520To%250Aadapt%2520to%2520varying%2520frame%2520characteristics%252C%2520we%2520introduce%2520a%2520per-frame%2520dynamic%2520model%250Aselection%2520strategy%2520that%2520selects%2520the%2520most%2520effective%2520configuration%2520for%2520each%250Ainput.%2520Evaluations%2520on%2520three%2520real-world%2520autonomous%2520vehicle%2520datasets%2520show%2520that%250ACounterNet%2520improves%2520counting%2520accuracy%2520by%25205%2525%2520to%252020%2525%2520across%2520object%2520categories%252C%250Aresulting%2520in%2520more%2520reliable%2520query%2520outcomes%2520across%2520all%2520supported%2520query%2520types.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Querying%20Autonomous%20Vehicle%20Point%20Clouds%3A%20Enhanced%20by%203D%20Object%20Counting%0A%20%20with%20CounterNet&entry.906535625=Xiaoyu%20Zhang%20and%20Zhifeng%20Bao%20and%20Hai%20Dong%20and%20Ziwei%20Wang%20and%20Jiajun%20Liu&entry.1292438233=%20%20Autonomous%20vehicles%20generate%20massive%20volumes%20of%20point%20cloud%20data%2C%20yet%20only%20a%0Asubset%20is%20relevant%20for%20specific%20tasks%20such%20as%20collision%20detection%2C%20traffic%0Aanalysis%2C%20or%20congestion%20monitoring.%20Effectively%20querying%20this%20data%20is%20essential%0Ato%20enable%20targeted%20analytics.%20In%20this%20work%2C%20we%20formalize%20point%20cloud%20querying%0Aby%20defining%20three%20core%20query%20types%3A%20RETRIEVAL%2C%20COUNT%2C%20and%20AGGREGATION%2C%20each%0Aaligned%20with%20distinct%20analytical%20scenarios.%20All%20these%20queries%20rely%20heavily%20on%0Aaccurate%20object%20counts%20to%20produce%20meaningful%20results%2C%20making%20precise%20object%0Acounting%20a%20critical%20component%20of%20query%20execution.%20Prior%20work%20has%20focused%20on%0Aindexing%20techniques%20for%202D%20video%20data%2C%20assuming%20detection%20models%20provide%0Aaccurate%20counting%20information.%20However%2C%20when%20applied%20to%203D%20point%20cloud%20data%2C%0Astate-of-the-art%20detection%20models%20often%20fail%20to%20generate%20reliable%20object%0Acounts%2C%20leading%20to%20substantial%20errors%20in%20query%20results.%20To%20address%20this%0Alimitation%2C%20we%20propose%20CounterNet%2C%20a%20heatmap-based%20network%20designed%20for%0Aaccurate%20object%20counting%20in%20large-scale%20point%20cloud%20data.%20Rather%20than%20focusing%0Aon%20accurate%20object%20localization%2C%20CounterNet%20detects%20object%20presence%20by%20finding%0Aobject%20centers%20to%20improve%20counting%20accuracy.%20We%20further%20enhance%20its%20performance%0Awith%20a%20feature%20map%20partitioning%20strategy%20using%20overlapping%20regions%2C%20enabling%0Abetter%20handling%20of%20both%20small%20and%20large%20objects%20in%20complex%20traffic%20scenes.%20To%0Aadapt%20to%20varying%20frame%20characteristics%2C%20we%20introduce%20a%20per-frame%20dynamic%20model%0Aselection%20strategy%20that%20selects%20the%20most%20effective%20configuration%20for%20each%0Ainput.%20Evaluations%20on%20three%20real-world%20autonomous%20vehicle%20datasets%20show%20that%0ACounterNet%20improves%20counting%20accuracy%20by%205%25%20to%2020%25%20across%20object%20categories%2C%0Aresulting%20in%20more%20reliable%20query%20outcomes%20across%20all%20supported%20query%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19209v2&entry.124074799=Read"},
{"title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "author": "Benedikt Blumenstiel and Paolo Fraccaro and Valerio Marsocci and Johannes Jakubik and Stefano Maurogiovanni and Mikolaj Czerkawski and Rocco Sedona and Gabriele Cavallaro and Thomas Brunschwiler and Juan Bernabe-Moreno and Nicolas Long\u00e9p\u00e9", "abstract": "  Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9~million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training. We provide detailed data processing steps,\ncomprehensive statistics, and empirical evidence demonstrating improved model\nperformance when pre-trained on TerraMesh. The dataset is hosted at\nhttps://huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.\n", "link": "http://arxiv.org/abs/2504.11172v2", "date": "2025-08-01", "relevancy": 2.5805, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TerraMesh%3A%20A%20Planetary%20Mosaic%20of%20Multimodal%20Earth%20Observation%20Data&body=Title%3A%20TerraMesh%3A%20A%20Planetary%20Mosaic%20of%20Multimodal%20Earth%20Observation%20Data%0AAuthor%3A%20Benedikt%20Blumenstiel%20and%20Paolo%20Fraccaro%20and%20Valerio%20Marsocci%20and%20Johannes%20Jakubik%20and%20Stefano%20Maurogiovanni%20and%20Mikolaj%20Czerkawski%20and%20Rocco%20Sedona%20and%20Gabriele%20Cavallaro%20and%20Thomas%20Brunschwiler%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20Large-scale%20foundation%20models%20in%20Earth%20Observation%20can%20learn%20versatile%2C%0Alabel-efficient%20representations%20by%20leveraging%20massive%20amounts%20of%20unlabeled%0Adata.%20However%2C%20existing%20public%20datasets%20are%20often%20limited%20in%20scale%2C%20geographic%0Acoverage%2C%20or%20sensor%20variety.%20We%20introduce%20TerraMesh%2C%20a%20new%20globally%20diverse%2C%0Amultimodal%20dataset%20combining%20optical%2C%20synthetic%20aperture%20radar%2C%20elevation%2C%20and%0Aland-cover%20modalities%20in%20an%20Analysis-Ready%20Data%20format.%20TerraMesh%20includes%20over%0A9~million%20samples%20with%20eight%20spatiotemporal%20aligned%20modalities%2C%20enabling%0Alarge-scale%20pre-training.%20We%20provide%20detailed%20data%20processing%20steps%2C%0Acomprehensive%20statistics%2C%20and%20empirical%20evidence%20demonstrating%20improved%20model%0Aperformance%20when%20pre-trained%20on%20TerraMesh.%20The%20dataset%20is%20hosted%20at%0Ahttps%3A//huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerraMesh%253A%2520A%2520Planetary%2520Mosaic%2520of%2520Multimodal%2520Earth%2520Observation%2520Data%26entry.906535625%3DBenedikt%2520Blumenstiel%2520and%2520Paolo%2520Fraccaro%2520and%2520Valerio%2520Marsocci%2520and%2520Johannes%2520Jakubik%2520and%2520Stefano%2520Maurogiovanni%2520and%2520Mikolaj%2520Czerkawski%2520and%2520Rocco%2520Sedona%2520and%2520Gabriele%2520Cavallaro%2520and%2520Thomas%2520Brunschwiler%2520and%2520Juan%2520Bernabe-Moreno%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520Large-scale%2520foundation%2520models%2520in%2520Earth%2520Observation%2520can%2520learn%2520versatile%252C%250Alabel-efficient%2520representations%2520by%2520leveraging%2520massive%2520amounts%2520of%2520unlabeled%250Adata.%2520However%252C%2520existing%2520public%2520datasets%2520are%2520often%2520limited%2520in%2520scale%252C%2520geographic%250Acoverage%252C%2520or%2520sensor%2520variety.%2520We%2520introduce%2520TerraMesh%252C%2520a%2520new%2520globally%2520diverse%252C%250Amultimodal%2520dataset%2520combining%2520optical%252C%2520synthetic%2520aperture%2520radar%252C%2520elevation%252C%2520and%250Aland-cover%2520modalities%2520in%2520an%2520Analysis-Ready%2520Data%2520format.%2520TerraMesh%2520includes%2520over%250A9~million%2520samples%2520with%2520eight%2520spatiotemporal%2520aligned%2520modalities%252C%2520enabling%250Alarge-scale%2520pre-training.%2520We%2520provide%2520detailed%2520data%2520processing%2520steps%252C%250Acomprehensive%2520statistics%252C%2520and%2520empirical%2520evidence%2520demonstrating%2520improved%2520model%250Aperformance%2520when%2520pre-trained%2520on%2520TerraMesh.%2520The%2520dataset%2520is%2520hosted%2520at%250Ahttps%253A//huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TerraMesh%3A%20A%20Planetary%20Mosaic%20of%20Multimodal%20Earth%20Observation%20Data&entry.906535625=Benedikt%20Blumenstiel%20and%20Paolo%20Fraccaro%20and%20Valerio%20Marsocci%20and%20Johannes%20Jakubik%20and%20Stefano%20Maurogiovanni%20and%20Mikolaj%20Czerkawski%20and%20Rocco%20Sedona%20and%20Gabriele%20Cavallaro%20and%20Thomas%20Brunschwiler%20and%20Juan%20Bernabe-Moreno%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20Large-scale%20foundation%20models%20in%20Earth%20Observation%20can%20learn%20versatile%2C%0Alabel-efficient%20representations%20by%20leveraging%20massive%20amounts%20of%20unlabeled%0Adata.%20However%2C%20existing%20public%20datasets%20are%20often%20limited%20in%20scale%2C%20geographic%0Acoverage%2C%20or%20sensor%20variety.%20We%20introduce%20TerraMesh%2C%20a%20new%20globally%20diverse%2C%0Amultimodal%20dataset%20combining%20optical%2C%20synthetic%20aperture%20radar%2C%20elevation%2C%20and%0Aland-cover%20modalities%20in%20an%20Analysis-Ready%20Data%20format.%20TerraMesh%20includes%20over%0A9~million%20samples%20with%20eight%20spatiotemporal%20aligned%20modalities%2C%20enabling%0Alarge-scale%20pre-training.%20We%20provide%20detailed%20data%20processing%20steps%2C%0Acomprehensive%20statistics%2C%20and%20empirical%20evidence%20demonstrating%20improved%20model%0Aperformance%20when%20pre-trained%20on%20TerraMesh.%20The%20dataset%20is%20hosted%20at%0Ahttps%3A//huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11172v2&entry.124074799=Read"},
{"title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural\n  Representations from Extracellular Recordings", "author": "Feng Cao and Zishuo Feng and Wei Shi and Jicong Zhang", "abstract": "  Extracellular recordings are transient voltage fluctuations in the vicinity\nof neurons, serving as a fundamental modality in neuroscience for decoding\nbrain activity at single-neuron resolution. Spike sorting, the process of\nattributing each detected spike to its corresponding neuron, is a pivotal step\nin brain sensing pipelines. However, it remains challenging under low\nsignal-to-noise ratio (SNR), electrode drift, and cross-session variability. In\nthis paper, we propose HuiduRep, a robust self-supervised representation\nlearning framework that extracts discriminative and generalizable features from\nextracellular recordings. By integrating contrastive learning with a denoising\nautoencoder, HuiduRep learns latent representations robust to noise and drift.\nWith HuiduRep, we develop a spike sorting pipeline that clusters spike\nrepresentations without ground truth labels. Experiments on hybrid and\nreal-world datasets demonstrate that HuiduRep achieves strong robustness.\nFurthermore, the pipeline significantly outperforms state-of-the-art tools such\nas KiloSort4 and MountainSort5 on accuracy and precision on diverse datasets.\nThese findings demonstrate the potential of self-supervised spike\nrepresentation learning as a foundational tool for robust and generalizable\nprocessing of extracellular recordings. Code is available at:\nhttps://github.com/IgarashiAkatuki/HuiduRep\n", "link": "http://arxiv.org/abs/2507.17224v2", "date": "2025-08-01", "relevancy": 2.5596, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5339}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5084}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HuiduRep%3A%20A%20Robust%20Self-Supervised%20Framework%20for%20Learning%20Neural%0A%20%20Representations%20from%20Extracellular%20Recordings&body=Title%3A%20HuiduRep%3A%20A%20Robust%20Self-Supervised%20Framework%20for%20Learning%20Neural%0A%20%20Representations%20from%20Extracellular%20Recordings%0AAuthor%3A%20Feng%20Cao%20and%20Zishuo%20Feng%20and%20Wei%20Shi%20and%20Jicong%20Zhang%0AAbstract%3A%20%20%20Extracellular%20recordings%20are%20transient%20voltage%20fluctuations%20in%20the%20vicinity%0Aof%20neurons%2C%20serving%20as%20a%20fundamental%20modality%20in%20neuroscience%20for%20decoding%0Abrain%20activity%20at%20single-neuron%20resolution.%20Spike%20sorting%2C%20the%20process%20of%0Aattributing%20each%20detected%20spike%20to%20its%20corresponding%20neuron%2C%20is%20a%20pivotal%20step%0Ain%20brain%20sensing%20pipelines.%20However%2C%20it%20remains%20challenging%20under%20low%0Asignal-to-noise%20ratio%20%28SNR%29%2C%20electrode%20drift%2C%20and%20cross-session%20variability.%20In%0Athis%20paper%2C%20we%20propose%20HuiduRep%2C%20a%20robust%20self-supervised%20representation%0Alearning%20framework%20that%20extracts%20discriminative%20and%20generalizable%20features%20from%0Aextracellular%20recordings.%20By%20integrating%20contrastive%20learning%20with%20a%20denoising%0Aautoencoder%2C%20HuiduRep%20learns%20latent%20representations%20robust%20to%20noise%20and%20drift.%0AWith%20HuiduRep%2C%20we%20develop%20a%20spike%20sorting%20pipeline%20that%20clusters%20spike%0Arepresentations%20without%20ground%20truth%20labels.%20Experiments%20on%20hybrid%20and%0Areal-world%20datasets%20demonstrate%20that%20HuiduRep%20achieves%20strong%20robustness.%0AFurthermore%2C%20the%20pipeline%20significantly%20outperforms%20state-of-the-art%20tools%20such%0Aas%20KiloSort4%20and%20MountainSort5%20on%20accuracy%20and%20precision%20on%20diverse%20datasets.%0AThese%20findings%20demonstrate%20the%20potential%20of%20self-supervised%20spike%0Arepresentation%20learning%20as%20a%20foundational%20tool%20for%20robust%20and%20generalizable%0Aprocessing%20of%20extracellular%20recordings.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IgarashiAkatuki/HuiduRep%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuiduRep%253A%2520A%2520Robust%2520Self-Supervised%2520Framework%2520for%2520Learning%2520Neural%250A%2520%2520Representations%2520from%2520Extracellular%2520Recordings%26entry.906535625%3DFeng%2520Cao%2520and%2520Zishuo%2520Feng%2520and%2520Wei%2520Shi%2520and%2520Jicong%2520Zhang%26entry.1292438233%3D%2520%2520Extracellular%2520recordings%2520are%2520transient%2520voltage%2520fluctuations%2520in%2520the%2520vicinity%250Aof%2520neurons%252C%2520serving%2520as%2520a%2520fundamental%2520modality%2520in%2520neuroscience%2520for%2520decoding%250Abrain%2520activity%2520at%2520single-neuron%2520resolution.%2520Spike%2520sorting%252C%2520the%2520process%2520of%250Aattributing%2520each%2520detected%2520spike%2520to%2520its%2520corresponding%2520neuron%252C%2520is%2520a%2520pivotal%2520step%250Ain%2520brain%2520sensing%2520pipelines.%2520However%252C%2520it%2520remains%2520challenging%2520under%2520low%250Asignal-to-noise%2520ratio%2520%2528SNR%2529%252C%2520electrode%2520drift%252C%2520and%2520cross-session%2520variability.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520HuiduRep%252C%2520a%2520robust%2520self-supervised%2520representation%250Alearning%2520framework%2520that%2520extracts%2520discriminative%2520and%2520generalizable%2520features%2520from%250Aextracellular%2520recordings.%2520By%2520integrating%2520contrastive%2520learning%2520with%2520a%2520denoising%250Aautoencoder%252C%2520HuiduRep%2520learns%2520latent%2520representations%2520robust%2520to%2520noise%2520and%2520drift.%250AWith%2520HuiduRep%252C%2520we%2520develop%2520a%2520spike%2520sorting%2520pipeline%2520that%2520clusters%2520spike%250Arepresentations%2520without%2520ground%2520truth%2520labels.%2520Experiments%2520on%2520hybrid%2520and%250Areal-world%2520datasets%2520demonstrate%2520that%2520HuiduRep%2520achieves%2520strong%2520robustness.%250AFurthermore%252C%2520the%2520pipeline%2520significantly%2520outperforms%2520state-of-the-art%2520tools%2520such%250Aas%2520KiloSort4%2520and%2520MountainSort5%2520on%2520accuracy%2520and%2520precision%2520on%2520diverse%2520datasets.%250AThese%2520findings%2520demonstrate%2520the%2520potential%2520of%2520self-supervised%2520spike%250Arepresentation%2520learning%2520as%2520a%2520foundational%2520tool%2520for%2520robust%2520and%2520generalizable%250Aprocessing%2520of%2520extracellular%2520recordings.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/IgarashiAkatuki/HuiduRep%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HuiduRep%3A%20A%20Robust%20Self-Supervised%20Framework%20for%20Learning%20Neural%0A%20%20Representations%20from%20Extracellular%20Recordings&entry.906535625=Feng%20Cao%20and%20Zishuo%20Feng%20and%20Wei%20Shi%20and%20Jicong%20Zhang&entry.1292438233=%20%20Extracellular%20recordings%20are%20transient%20voltage%20fluctuations%20in%20the%20vicinity%0Aof%20neurons%2C%20serving%20as%20a%20fundamental%20modality%20in%20neuroscience%20for%20decoding%0Abrain%20activity%20at%20single-neuron%20resolution.%20Spike%20sorting%2C%20the%20process%20of%0Aattributing%20each%20detected%20spike%20to%20its%20corresponding%20neuron%2C%20is%20a%20pivotal%20step%0Ain%20brain%20sensing%20pipelines.%20However%2C%20it%20remains%20challenging%20under%20low%0Asignal-to-noise%20ratio%20%28SNR%29%2C%20electrode%20drift%2C%20and%20cross-session%20variability.%20In%0Athis%20paper%2C%20we%20propose%20HuiduRep%2C%20a%20robust%20self-supervised%20representation%0Alearning%20framework%20that%20extracts%20discriminative%20and%20generalizable%20features%20from%0Aextracellular%20recordings.%20By%20integrating%20contrastive%20learning%20with%20a%20denoising%0Aautoencoder%2C%20HuiduRep%20learns%20latent%20representations%20robust%20to%20noise%20and%20drift.%0AWith%20HuiduRep%2C%20we%20develop%20a%20spike%20sorting%20pipeline%20that%20clusters%20spike%0Arepresentations%20without%20ground%20truth%20labels.%20Experiments%20on%20hybrid%20and%0Areal-world%20datasets%20demonstrate%20that%20HuiduRep%20achieves%20strong%20robustness.%0AFurthermore%2C%20the%20pipeline%20significantly%20outperforms%20state-of-the-art%20tools%20such%0Aas%20KiloSort4%20and%20MountainSort5%20on%20accuracy%20and%20precision%20on%20diverse%20datasets.%0AThese%20findings%20demonstrate%20the%20potential%20of%20self-supervised%20spike%0Arepresentation%20learning%20as%20a%20foundational%20tool%20for%20robust%20and%20generalizable%0Aprocessing%20of%20extracellular%20recordings.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IgarashiAkatuki/HuiduRep%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17224v2&entry.124074799=Read"},
{"title": "Video Generators are Robot Policies", "author": "Junbang Liang and Pavel Tokmakov and Ruoshi Liu and Sruthi Sudhakar and Paarth Shah and Rares Ambrus and Carl Vondrick", "abstract": "  Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.\n", "link": "http://arxiv.org/abs/2508.00795v1", "date": "2025-08-01", "relevancy": 2.5524, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6696}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6289}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Generators%20are%20Robot%20Policies&body=Title%3A%20Video%20Generators%20are%20Robot%20Policies%0AAuthor%3A%20Junbang%20Liang%20and%20Pavel%20Tokmakov%20and%20Ruoshi%20Liu%20and%20Sruthi%20Sudhakar%20and%20Paarth%20Shah%20and%20Rares%20Ambrus%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20Despite%20tremendous%20progress%20in%20dexterous%20manipulation%2C%20current%20visuomotor%0Apolicies%20remain%20fundamentally%20limited%20by%20two%20challenges%3A%20they%20struggle%20to%0Ageneralize%20under%20perceptual%20or%20behavioral%20distribution%20shifts%2C%20and%20their%0Aperformance%20is%20constrained%20by%20the%20size%20of%20human%20demonstration%20data.%20In%20this%0Apaper%2C%20we%20use%20video%20generation%20as%20a%20proxy%20for%20robot%20policy%20learning%20to%20address%0Aboth%20limitations%20simultaneously.%20We%20propose%20Video%20Policy%2C%20a%20modular%20framework%0Athat%20combines%20video%20and%20action%20generation%20that%20can%20be%20trained%20end-to-end.%20Our%0Aresults%20demonstrate%20that%20learning%20to%20generate%20videos%20of%20robot%20behavior%20allows%0Afor%20the%20extraction%20of%20policies%20with%20minimal%20demonstration%20data%2C%20significantly%0Aimproving%20robustness%20and%20sample%20efficiency.%20Our%20method%20shows%20strong%0Ageneralization%20to%20unseen%20objects%2C%20backgrounds%2C%20and%20tasks%2C%20both%20in%20simulation%0Aand%20the%20real%20world.%20We%20further%20highlight%20that%20task%20success%20is%20closely%20tied%20to%0Athe%20generated%20video%2C%20with%20action-free%20video%20data%20providing%20critical%20benefits%0Afor%20generalizing%20to%20novel%20tasks.%20By%20leveraging%20large-scale%20video%20generative%0Amodels%2C%20we%20achieve%20superior%20performance%20compared%20to%20traditional%20behavior%0Acloning%2C%20paving%20the%20way%20for%20more%20scalable%20and%20data-efficient%20robot%20policy%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Generators%2520are%2520Robot%2520Policies%26entry.906535625%3DJunbang%2520Liang%2520and%2520Pavel%2520Tokmakov%2520and%2520Ruoshi%2520Liu%2520and%2520Sruthi%2520Sudhakar%2520and%2520Paarth%2520Shah%2520and%2520Rares%2520Ambrus%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520Despite%2520tremendous%2520progress%2520in%2520dexterous%2520manipulation%252C%2520current%2520visuomotor%250Apolicies%2520remain%2520fundamentally%2520limited%2520by%2520two%2520challenges%253A%2520they%2520struggle%2520to%250Ageneralize%2520under%2520perceptual%2520or%2520behavioral%2520distribution%2520shifts%252C%2520and%2520their%250Aperformance%2520is%2520constrained%2520by%2520the%2520size%2520of%2520human%2520demonstration%2520data.%2520In%2520this%250Apaper%252C%2520we%2520use%2520video%2520generation%2520as%2520a%2520proxy%2520for%2520robot%2520policy%2520learning%2520to%2520address%250Aboth%2520limitations%2520simultaneously.%2520We%2520propose%2520Video%2520Policy%252C%2520a%2520modular%2520framework%250Athat%2520combines%2520video%2520and%2520action%2520generation%2520that%2520can%2520be%2520trained%2520end-to-end.%2520Our%250Aresults%2520demonstrate%2520that%2520learning%2520to%2520generate%2520videos%2520of%2520robot%2520behavior%2520allows%250Afor%2520the%2520extraction%2520of%2520policies%2520with%2520minimal%2520demonstration%2520data%252C%2520significantly%250Aimproving%2520robustness%2520and%2520sample%2520efficiency.%2520Our%2520method%2520shows%2520strong%250Ageneralization%2520to%2520unseen%2520objects%252C%2520backgrounds%252C%2520and%2520tasks%252C%2520both%2520in%2520simulation%250Aand%2520the%2520real%2520world.%2520We%2520further%2520highlight%2520that%2520task%2520success%2520is%2520closely%2520tied%2520to%250Athe%2520generated%2520video%252C%2520with%2520action-free%2520video%2520data%2520providing%2520critical%2520benefits%250Afor%2520generalizing%2520to%2520novel%2520tasks.%2520By%2520leveraging%2520large-scale%2520video%2520generative%250Amodels%252C%2520we%2520achieve%2520superior%2520performance%2520compared%2520to%2520traditional%2520behavior%250Acloning%252C%2520paving%2520the%2520way%2520for%2520more%2520scalable%2520and%2520data-efficient%2520robot%2520policy%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Generators%20are%20Robot%20Policies&entry.906535625=Junbang%20Liang%20and%20Pavel%20Tokmakov%20and%20Ruoshi%20Liu%20and%20Sruthi%20Sudhakar%20and%20Paarth%20Shah%20and%20Rares%20Ambrus%20and%20Carl%20Vondrick&entry.1292438233=%20%20Despite%20tremendous%20progress%20in%20dexterous%20manipulation%2C%20current%20visuomotor%0Apolicies%20remain%20fundamentally%20limited%20by%20two%20challenges%3A%20they%20struggle%20to%0Ageneralize%20under%20perceptual%20or%20behavioral%20distribution%20shifts%2C%20and%20their%0Aperformance%20is%20constrained%20by%20the%20size%20of%20human%20demonstration%20data.%20In%20this%0Apaper%2C%20we%20use%20video%20generation%20as%20a%20proxy%20for%20robot%20policy%20learning%20to%20address%0Aboth%20limitations%20simultaneously.%20We%20propose%20Video%20Policy%2C%20a%20modular%20framework%0Athat%20combines%20video%20and%20action%20generation%20that%20can%20be%20trained%20end-to-end.%20Our%0Aresults%20demonstrate%20that%20learning%20to%20generate%20videos%20of%20robot%20behavior%20allows%0Afor%20the%20extraction%20of%20policies%20with%20minimal%20demonstration%20data%2C%20significantly%0Aimproving%20robustness%20and%20sample%20efficiency.%20Our%20method%20shows%20strong%0Ageneralization%20to%20unseen%20objects%2C%20backgrounds%2C%20and%20tasks%2C%20both%20in%20simulation%0Aand%20the%20real%20world.%20We%20further%20highlight%20that%20task%20success%20is%20closely%20tied%20to%0Athe%20generated%20video%2C%20with%20action-free%20video%20data%20providing%20critical%20benefits%0Afor%20generalizing%20to%20novel%20tasks.%20By%20leveraging%20large-scale%20video%20generative%0Amodels%2C%20we%20achieve%20superior%20performance%20compared%20to%20traditional%20behavior%0Acloning%2C%20paving%20the%20way%20for%20more%20scalable%20and%20data-efficient%20robot%20policy%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00795v1&entry.124074799=Read"},
{"title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like\n  Artificial Nighttime Light Image Reconstruction (1986-2024)", "author": "Yihe Tian and Kwan Man Cheng and Zhengbo Zhang and Tao Zhang and Suju Li and Dongmei Yan and Bing Xu", "abstract": "  Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.\n", "link": "http://arxiv.org/abs/2508.00590v1", "date": "2025-08-01", "relevancy": 2.5228, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5163}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4987}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Modeling%20Framework%20and%20Data%20Product%20for%20Extended%20VIIRS-like%0A%20%20Artificial%20Nighttime%20Light%20Image%20Reconstruction%20%281986-2024%29&body=Title%3A%20A%20Novel%20Modeling%20Framework%20and%20Data%20Product%20for%20Extended%20VIIRS-like%0A%20%20Artificial%20Nighttime%20Light%20Image%20Reconstruction%20%281986-2024%29%0AAuthor%3A%20Yihe%20Tian%20and%20Kwan%20Man%20Cheng%20and%20Zhengbo%20Zhang%20and%20Tao%20Zhang%20and%20Suju%20Li%20and%20Dongmei%20Yan%20and%20Bing%20Xu%0AAbstract%3A%20%20%20Artificial%20Night-Time%20Light%20%28NTL%29%20remote%20sensing%20is%20a%20vital%20proxy%20for%0Aquantifying%20the%20intensity%20and%20spatial%20distribution%20of%20human%20activities.%0AAlthough%20the%20NPP-VIIRS%20sensor%20provides%20high-quality%20NTL%20observations%2C%20its%0Atemporal%20coverage%2C%20which%20begins%20in%202012%2C%20restricts%20long-term%20time-series%0Astudies%20that%20extend%20to%20earlier%20periods.%20Despite%20the%20progress%20in%20extending%0AVIIRS-like%20NTL%20time-series%2C%20current%20methods%20still%20suffer%20from%20two%20significant%0Ashortcomings%3A%20the%20underestimation%20of%20light%20intensity%20and%20the%20structural%0Aomission.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20reconstruction%0Aframework%20consisting%20of%20a%20two-stage%20process%3A%20construction%20and%20refinement.%20The%0Aconstruction%20stage%20features%20a%20Hierarchical%20Fusion%20Decoder%20%28HFD%29%20designed%20to%0Aenhance%20the%20fidelity%20of%20the%20initial%20reconstruction.%20The%20refinement%20stage%0Aemploys%20a%20Dual%20Feature%20Refiner%20%28DFR%29%2C%20which%20leverages%20high-resolution%0Aimpervious%20surface%20masks%20to%20guide%20and%20enhance%20fine-grained%20structural%20details.%0ABased%20on%20this%20framework%2C%20we%20developed%20the%20Extended%20VIIRS-like%20Artificial%0ANighttime%20Light%20%28EVAL%29%20product%20for%20China%2C%20extending%20the%20standard%20data%20record%0Abackwards%20by%2026%20years%20to%20begin%20in%201986.%20Quantitative%20evaluation%20shows%20that%20EVAL%0Asignificantly%20outperforms%20existing%20state-of-the-art%20products%2C%20boosting%20the%0A%24%5Ctext%7BR%7D%5E2%24%20from%200.68%20to%200.80%20while%20lowering%20the%20RMSE%20from%201.27%20to%200.99.%0AFurthermore%2C%20EVAL%20exhibits%20excellent%20temporal%20consistency%20and%20maintains%20a%20high%0Acorrelation%20with%20socioeconomic%20parameters%2C%20confirming%20its%20reliability%20for%0Along-term%20analysis.%20The%20resulting%20EVAL%20dataset%20provides%20a%20valuable%20new%20resource%0Afor%20the%20research%20community%20and%20is%20publicly%20available%20at%0Ahttps%3A//doi.org/10.11888/HumanNat.tpdc.302930.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Modeling%2520Framework%2520and%2520Data%2520Product%2520for%2520Extended%2520VIIRS-like%250A%2520%2520Artificial%2520Nighttime%2520Light%2520Image%2520Reconstruction%2520%25281986-2024%2529%26entry.906535625%3DYihe%2520Tian%2520and%2520Kwan%2520Man%2520Cheng%2520and%2520Zhengbo%2520Zhang%2520and%2520Tao%2520Zhang%2520and%2520Suju%2520Li%2520and%2520Dongmei%2520Yan%2520and%2520Bing%2520Xu%26entry.1292438233%3D%2520%2520Artificial%2520Night-Time%2520Light%2520%2528NTL%2529%2520remote%2520sensing%2520is%2520a%2520vital%2520proxy%2520for%250Aquantifying%2520the%2520intensity%2520and%2520spatial%2520distribution%2520of%2520human%2520activities.%250AAlthough%2520the%2520NPP-VIIRS%2520sensor%2520provides%2520high-quality%2520NTL%2520observations%252C%2520its%250Atemporal%2520coverage%252C%2520which%2520begins%2520in%25202012%252C%2520restricts%2520long-term%2520time-series%250Astudies%2520that%2520extend%2520to%2520earlier%2520periods.%2520Despite%2520the%2520progress%2520in%2520extending%250AVIIRS-like%2520NTL%2520time-series%252C%2520current%2520methods%2520still%2520suffer%2520from%2520two%2520significant%250Ashortcomings%253A%2520the%2520underestimation%2520of%2520light%2520intensity%2520and%2520the%2520structural%250Aomission.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520reconstruction%250Aframework%2520consisting%2520of%2520a%2520two-stage%2520process%253A%2520construction%2520and%2520refinement.%2520The%250Aconstruction%2520stage%2520features%2520a%2520Hierarchical%2520Fusion%2520Decoder%2520%2528HFD%2529%2520designed%2520to%250Aenhance%2520the%2520fidelity%2520of%2520the%2520initial%2520reconstruction.%2520The%2520refinement%2520stage%250Aemploys%2520a%2520Dual%2520Feature%2520Refiner%2520%2528DFR%2529%252C%2520which%2520leverages%2520high-resolution%250Aimpervious%2520surface%2520masks%2520to%2520guide%2520and%2520enhance%2520fine-grained%2520structural%2520details.%250ABased%2520on%2520this%2520framework%252C%2520we%2520developed%2520the%2520Extended%2520VIIRS-like%2520Artificial%250ANighttime%2520Light%2520%2528EVAL%2529%2520product%2520for%2520China%252C%2520extending%2520the%2520standard%2520data%2520record%250Abackwards%2520by%252026%2520years%2520to%2520begin%2520in%25201986.%2520Quantitative%2520evaluation%2520shows%2520that%2520EVAL%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520products%252C%2520boosting%2520the%250A%2524%255Ctext%257BR%257D%255E2%2524%2520from%25200.68%2520to%25200.80%2520while%2520lowering%2520the%2520RMSE%2520from%25201.27%2520to%25200.99.%250AFurthermore%252C%2520EVAL%2520exhibits%2520excellent%2520temporal%2520consistency%2520and%2520maintains%2520a%2520high%250Acorrelation%2520with%2520socioeconomic%2520parameters%252C%2520confirming%2520its%2520reliability%2520for%250Along-term%2520analysis.%2520The%2520resulting%2520EVAL%2520dataset%2520provides%2520a%2520valuable%2520new%2520resource%250Afor%2520the%2520research%2520community%2520and%2520is%2520publicly%2520available%2520at%250Ahttps%253A//doi.org/10.11888/HumanNat.tpdc.302930.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Modeling%20Framework%20and%20Data%20Product%20for%20Extended%20VIIRS-like%0A%20%20Artificial%20Nighttime%20Light%20Image%20Reconstruction%20%281986-2024%29&entry.906535625=Yihe%20Tian%20and%20Kwan%20Man%20Cheng%20and%20Zhengbo%20Zhang%20and%20Tao%20Zhang%20and%20Suju%20Li%20and%20Dongmei%20Yan%20and%20Bing%20Xu&entry.1292438233=%20%20Artificial%20Night-Time%20Light%20%28NTL%29%20remote%20sensing%20is%20a%20vital%20proxy%20for%0Aquantifying%20the%20intensity%20and%20spatial%20distribution%20of%20human%20activities.%0AAlthough%20the%20NPP-VIIRS%20sensor%20provides%20high-quality%20NTL%20observations%2C%20its%0Atemporal%20coverage%2C%20which%20begins%20in%202012%2C%20restricts%20long-term%20time-series%0Astudies%20that%20extend%20to%20earlier%20periods.%20Despite%20the%20progress%20in%20extending%0AVIIRS-like%20NTL%20time-series%2C%20current%20methods%20still%20suffer%20from%20two%20significant%0Ashortcomings%3A%20the%20underestimation%20of%20light%20intensity%20and%20the%20structural%0Aomission.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20novel%20reconstruction%0Aframework%20consisting%20of%20a%20two-stage%20process%3A%20construction%20and%20refinement.%20The%0Aconstruction%20stage%20features%20a%20Hierarchical%20Fusion%20Decoder%20%28HFD%29%20designed%20to%0Aenhance%20the%20fidelity%20of%20the%20initial%20reconstruction.%20The%20refinement%20stage%0Aemploys%20a%20Dual%20Feature%20Refiner%20%28DFR%29%2C%20which%20leverages%20high-resolution%0Aimpervious%20surface%20masks%20to%20guide%20and%20enhance%20fine-grained%20structural%20details.%0ABased%20on%20this%20framework%2C%20we%20developed%20the%20Extended%20VIIRS-like%20Artificial%0ANighttime%20Light%20%28EVAL%29%20product%20for%20China%2C%20extending%20the%20standard%20data%20record%0Abackwards%20by%2026%20years%20to%20begin%20in%201986.%20Quantitative%20evaluation%20shows%20that%20EVAL%0Asignificantly%20outperforms%20existing%20state-of-the-art%20products%2C%20boosting%20the%0A%24%5Ctext%7BR%7D%5E2%24%20from%200.68%20to%200.80%20while%20lowering%20the%20RMSE%20from%201.27%20to%200.99.%0AFurthermore%2C%20EVAL%20exhibits%20excellent%20temporal%20consistency%20and%20maintains%20a%20high%0Acorrelation%20with%20socioeconomic%20parameters%2C%20confirming%20its%20reliability%20for%0Along-term%20analysis.%20The%20resulting%20EVAL%20dataset%20provides%20a%20valuable%20new%20resource%0Afor%20the%20research%20community%20and%20is%20publicly%20available%20at%0Ahttps%3A//doi.org/10.11888/HumanNat.tpdc.302930.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00590v1&entry.124074799=Read"},
{"title": "How LLMs are Shaping the Future of Virtual Reality", "author": "S\u00fceda \u00d6zkaya and Santiago Berrezueta-Guzman and Stefan Wagner", "abstract": "  The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.\n", "link": "http://arxiv.org/abs/2508.00737v1", "date": "2025-08-01", "relevancy": 2.5099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20LLMs%20are%20Shaping%20the%20Future%20of%20Virtual%20Reality&body=Title%3A%20How%20LLMs%20are%20Shaping%20the%20Future%20of%20Virtual%20Reality%0AAuthor%3A%20S%C3%BCeda%20%C3%96zkaya%20and%20Santiago%20Berrezueta-Guzman%20and%20Stefan%20Wagner%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20Virtual%20Reality%20%28VR%29%0Agames%20marks%20a%20paradigm%20shift%20in%20the%20design%20of%20immersive%2C%20adaptive%2C%20and%0Aintelligent%20digital%20experiences.%20This%20paper%20presents%20a%20comprehensive%20review%20of%0Arecent%20research%20at%20the%20intersection%20of%20LLMs%20and%20VR%2C%20examining%20how%20these%20models%0Aare%20transforming%20narrative%20generation%2C%20non-player%20character%20%28NPC%29%20interactions%2C%0Aaccessibility%2C%20personalization%2C%20and%20game%20mastering.%20Drawing%20from%20an%20analysis%20of%0A62%20peer%20reviewed%20studies%20published%20between%202018%20and%202025%2C%20we%20identify%20key%0Aapplication%20domains%20ranging%20from%20emotionally%20intelligent%20NPCs%20and%20procedurally%0Agenerated%20storytelling%20to%20AI-driven%20adaptive%20systems%20and%20inclusive%20gameplay%0Ainterfaces.%20We%20also%20address%20the%20major%20challenges%20facing%20this%20convergence%2C%0Aincluding%20real-time%20performance%20constraints%2C%20memory%20limitations%2C%20ethical%20risks%2C%0Aand%20scalability%20barriers.%20Our%20findings%20highlight%20that%20while%20LLMs%20significantly%0Aenhance%20realism%2C%20creativity%2C%20and%20user%20engagement%20in%20VR%20environments%2C%20their%0Aeffective%20deployment%20requires%20robust%20design%20strategies%20that%20integrate%0Amultimodal%20interaction%2C%20hybrid%20AI%20architectures%2C%20and%20ethical%20safeguards.%20The%0Apaper%20concludes%20by%20outlining%20future%20research%20directions%20in%20multimodal%20AI%2C%0Aaffective%20computing%2C%20reinforcement%20learning%2C%20and%20open-source%20development%2C%0Aaiming%20to%20guide%20the%20responsible%20advancement%20of%20intelligent%20and%20inclusive%20VR%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520LLMs%2520are%2520Shaping%2520the%2520Future%2520of%2520Virtual%2520Reality%26entry.906535625%3DS%25C3%25BCeda%2520%25C3%2596zkaya%2520and%2520Santiago%2520Berrezueta-Guzman%2520and%2520Stefan%2520Wagner%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520Virtual%2520Reality%2520%2528VR%2529%250Agames%2520marks%2520a%2520paradigm%2520shift%2520in%2520the%2520design%2520of%2520immersive%252C%2520adaptive%252C%2520and%250Aintelligent%2520digital%2520experiences.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%250Arecent%2520research%2520at%2520the%2520intersection%2520of%2520LLMs%2520and%2520VR%252C%2520examining%2520how%2520these%2520models%250Aare%2520transforming%2520narrative%2520generation%252C%2520non-player%2520character%2520%2528NPC%2529%2520interactions%252C%250Aaccessibility%252C%2520personalization%252C%2520and%2520game%2520mastering.%2520Drawing%2520from%2520an%2520analysis%2520of%250A62%2520peer%2520reviewed%2520studies%2520published%2520between%25202018%2520and%25202025%252C%2520we%2520identify%2520key%250Aapplication%2520domains%2520ranging%2520from%2520emotionally%2520intelligent%2520NPCs%2520and%2520procedurally%250Agenerated%2520storytelling%2520to%2520AI-driven%2520adaptive%2520systems%2520and%2520inclusive%2520gameplay%250Ainterfaces.%2520We%2520also%2520address%2520the%2520major%2520challenges%2520facing%2520this%2520convergence%252C%250Aincluding%2520real-time%2520performance%2520constraints%252C%2520memory%2520limitations%252C%2520ethical%2520risks%252C%250Aand%2520scalability%2520barriers.%2520Our%2520findings%2520highlight%2520that%2520while%2520LLMs%2520significantly%250Aenhance%2520realism%252C%2520creativity%252C%2520and%2520user%2520engagement%2520in%2520VR%2520environments%252C%2520their%250Aeffective%2520deployment%2520requires%2520robust%2520design%2520strategies%2520that%2520integrate%250Amultimodal%2520interaction%252C%2520hybrid%2520AI%2520architectures%252C%2520and%2520ethical%2520safeguards.%2520The%250Apaper%2520concludes%2520by%2520outlining%2520future%2520research%2520directions%2520in%2520multimodal%2520AI%252C%250Aaffective%2520computing%252C%2520reinforcement%2520learning%252C%2520and%2520open-source%2520development%252C%250Aaiming%2520to%2520guide%2520the%2520responsible%2520advancement%2520of%2520intelligent%2520and%2520inclusive%2520VR%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20LLMs%20are%20Shaping%20the%20Future%20of%20Virtual%20Reality&entry.906535625=S%C3%BCeda%20%C3%96zkaya%20and%20Santiago%20Berrezueta-Guzman%20and%20Stefan%20Wagner&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20Virtual%20Reality%20%28VR%29%0Agames%20marks%20a%20paradigm%20shift%20in%20the%20design%20of%20immersive%2C%20adaptive%2C%20and%0Aintelligent%20digital%20experiences.%20This%20paper%20presents%20a%20comprehensive%20review%20of%0Arecent%20research%20at%20the%20intersection%20of%20LLMs%20and%20VR%2C%20examining%20how%20these%20models%0Aare%20transforming%20narrative%20generation%2C%20non-player%20character%20%28NPC%29%20interactions%2C%0Aaccessibility%2C%20personalization%2C%20and%20game%20mastering.%20Drawing%20from%20an%20analysis%20of%0A62%20peer%20reviewed%20studies%20published%20between%202018%20and%202025%2C%20we%20identify%20key%0Aapplication%20domains%20ranging%20from%20emotionally%20intelligent%20NPCs%20and%20procedurally%0Agenerated%20storytelling%20to%20AI-driven%20adaptive%20systems%20and%20inclusive%20gameplay%0Ainterfaces.%20We%20also%20address%20the%20major%20challenges%20facing%20this%20convergence%2C%0Aincluding%20real-time%20performance%20constraints%2C%20memory%20limitations%2C%20ethical%20risks%2C%0Aand%20scalability%20barriers.%20Our%20findings%20highlight%20that%20while%20LLMs%20significantly%0Aenhance%20realism%2C%20creativity%2C%20and%20user%20engagement%20in%20VR%20environments%2C%20their%0Aeffective%20deployment%20requires%20robust%20design%20strategies%20that%20integrate%0Amultimodal%20interaction%2C%20hybrid%20AI%20architectures%2C%20and%20ethical%20safeguards.%20The%0Apaper%20concludes%20by%20outlining%20future%20research%20directions%20in%20multimodal%20AI%2C%0Aaffective%20computing%2C%20reinforcement%20learning%2C%20and%20open-source%20development%2C%0Aaiming%20to%20guide%20the%20responsible%20advancement%20of%20intelligent%20and%20inclusive%20VR%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00737v1&entry.124074799=Read"},
{"title": "Applying Psychometrics to Large Language Model Simulated Populations:\n  Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "author": "Sarah Mercer and Daniel P. Martin and Phil Swatton", "abstract": "  Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.\n", "link": "http://arxiv.org/abs/2508.00742v1", "date": "2025-08-01", "relevancy": 2.5031, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5356}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4835}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20Psychometrics%20to%20Large%20Language%20Model%20Simulated%20Populations%3A%0A%20%20Recreating%20the%20HEXACO%20Personality%20Inventory%20Experiment%20with%20Generative%20Agents&body=Title%3A%20Applying%20Psychometrics%20to%20Large%20Language%20Model%20Simulated%20Populations%3A%0A%20%20Recreating%20the%20HEXACO%20Personality%20Inventory%20Experiment%20with%20Generative%20Agents%0AAuthor%3A%20Sarah%20Mercer%20and%20Daniel%20P.%20Martin%20and%20Phil%20Swatton%0AAbstract%3A%20%20%20Generative%20agents%20powered%20by%20Large%20Language%20Models%20demonstrate%20human-like%0Acharacteristics%20through%20sophisticated%20natural%20language%20interactions.%20Their%0Aability%20to%20assume%20roles%20and%20personalities%20based%20on%20predefined%20character%0Abiographies%20has%20positioned%20them%20as%20cost-effective%20substitutes%20for%20human%0Aparticipants%20in%20social%20science%20research.%20This%20paper%20explores%20the%20validity%20of%0Asuch%20persona-based%20agents%20in%20representing%20human%20populations%3B%20we%20recreate%20the%0AHEXACO%20personality%20inventory%20experiment%20by%20surveying%20310%20GPT-4%20powered%20agents%2C%0Aconducting%20factor%20analysis%20on%20their%20responses%2C%20and%20comparing%20these%20results%20to%0Athe%20original%20findings%20presented%20by%20Ashton%2C%20Lee%2C%20%26%20Goldberg%20in%202004.%20Our%20results%0Afound%201%29%20a%20coherent%20and%20reliable%20personality%20structure%20was%20recoverable%20from%20the%0Aagents%27%20responses%20demonstrating%20partial%20alignment%20to%20the%20HEXACO%20framework.%202%29%0Athe%20derived%20personality%20dimensions%20were%20consistent%20and%20reliable%20within%20GPT-4%2C%0Awhen%20coupled%20with%20a%20sufficiently%20curated%20population%2C%20and%203%29%20cross-model%0Aanalysis%20revealed%20variability%20in%20personality%20profiling%2C%20suggesting%0Amodel-specific%20biases%20and%20limitations.%20We%20discuss%20the%20practical%20considerations%0Aand%20challenges%20encountered%20during%20the%20experiment.%20This%20study%20contributes%20to%20the%0Aongoing%20discourse%20on%20the%20potential%20benefits%20and%20limitations%20of%20using%20generative%0Aagents%20in%20social%20science%20research%20and%20provides%20useful%20guidance%20on%20designing%0Aconsistent%20and%20representative%20agent%20personas%20to%20maximise%20coverage%20and%0Arepresentation%20of%20human%20personality%20traits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520Psychometrics%2520to%2520Large%2520Language%2520Model%2520Simulated%2520Populations%253A%250A%2520%2520Recreating%2520the%2520HEXACO%2520Personality%2520Inventory%2520Experiment%2520with%2520Generative%2520Agents%26entry.906535625%3DSarah%2520Mercer%2520and%2520Daniel%2520P.%2520Martin%2520and%2520Phil%2520Swatton%26entry.1292438233%3D%2520%2520Generative%2520agents%2520powered%2520by%2520Large%2520Language%2520Models%2520demonstrate%2520human-like%250Acharacteristics%2520through%2520sophisticated%2520natural%2520language%2520interactions.%2520Their%250Aability%2520to%2520assume%2520roles%2520and%2520personalities%2520based%2520on%2520predefined%2520character%250Abiographies%2520has%2520positioned%2520them%2520as%2520cost-effective%2520substitutes%2520for%2520human%250Aparticipants%2520in%2520social%2520science%2520research.%2520This%2520paper%2520explores%2520the%2520validity%2520of%250Asuch%2520persona-based%2520agents%2520in%2520representing%2520human%2520populations%253B%2520we%2520recreate%2520the%250AHEXACO%2520personality%2520inventory%2520experiment%2520by%2520surveying%2520310%2520GPT-4%2520powered%2520agents%252C%250Aconducting%2520factor%2520analysis%2520on%2520their%2520responses%252C%2520and%2520comparing%2520these%2520results%2520to%250Athe%2520original%2520findings%2520presented%2520by%2520Ashton%252C%2520Lee%252C%2520%2526%2520Goldberg%2520in%25202004.%2520Our%2520results%250Afound%25201%2529%2520a%2520coherent%2520and%2520reliable%2520personality%2520structure%2520was%2520recoverable%2520from%2520the%250Aagents%2527%2520responses%2520demonstrating%2520partial%2520alignment%2520to%2520the%2520HEXACO%2520framework.%25202%2529%250Athe%2520derived%2520personality%2520dimensions%2520were%2520consistent%2520and%2520reliable%2520within%2520GPT-4%252C%250Awhen%2520coupled%2520with%2520a%2520sufficiently%2520curated%2520population%252C%2520and%25203%2529%2520cross-model%250Aanalysis%2520revealed%2520variability%2520in%2520personality%2520profiling%252C%2520suggesting%250Amodel-specific%2520biases%2520and%2520limitations.%2520We%2520discuss%2520the%2520practical%2520considerations%250Aand%2520challenges%2520encountered%2520during%2520the%2520experiment.%2520This%2520study%2520contributes%2520to%2520the%250Aongoing%2520discourse%2520on%2520the%2520potential%2520benefits%2520and%2520limitations%2520of%2520using%2520generative%250Aagents%2520in%2520social%2520science%2520research%2520and%2520provides%2520useful%2520guidance%2520on%2520designing%250Aconsistent%2520and%2520representative%2520agent%2520personas%2520to%2520maximise%2520coverage%2520and%250Arepresentation%2520of%2520human%2520personality%2520traits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20Psychometrics%20to%20Large%20Language%20Model%20Simulated%20Populations%3A%0A%20%20Recreating%20the%20HEXACO%20Personality%20Inventory%20Experiment%20with%20Generative%20Agents&entry.906535625=Sarah%20Mercer%20and%20Daniel%20P.%20Martin%20and%20Phil%20Swatton&entry.1292438233=%20%20Generative%20agents%20powered%20by%20Large%20Language%20Models%20demonstrate%20human-like%0Acharacteristics%20through%20sophisticated%20natural%20language%20interactions.%20Their%0Aability%20to%20assume%20roles%20and%20personalities%20based%20on%20predefined%20character%0Abiographies%20has%20positioned%20them%20as%20cost-effective%20substitutes%20for%20human%0Aparticipants%20in%20social%20science%20research.%20This%20paper%20explores%20the%20validity%20of%0Asuch%20persona-based%20agents%20in%20representing%20human%20populations%3B%20we%20recreate%20the%0AHEXACO%20personality%20inventory%20experiment%20by%20surveying%20310%20GPT-4%20powered%20agents%2C%0Aconducting%20factor%20analysis%20on%20their%20responses%2C%20and%20comparing%20these%20results%20to%0Athe%20original%20findings%20presented%20by%20Ashton%2C%20Lee%2C%20%26%20Goldberg%20in%202004.%20Our%20results%0Afound%201%29%20a%20coherent%20and%20reliable%20personality%20structure%20was%20recoverable%20from%20the%0Aagents%27%20responses%20demonstrating%20partial%20alignment%20to%20the%20HEXACO%20framework.%202%29%0Athe%20derived%20personality%20dimensions%20were%20consistent%20and%20reliable%20within%20GPT-4%2C%0Awhen%20coupled%20with%20a%20sufficiently%20curated%20population%2C%20and%203%29%20cross-model%0Aanalysis%20revealed%20variability%20in%20personality%20profiling%2C%20suggesting%0Amodel-specific%20biases%20and%20limitations.%20We%20discuss%20the%20practical%20considerations%0Aand%20challenges%20encountered%20during%20the%20experiment.%20This%20study%20contributes%20to%20the%0Aongoing%20discourse%20on%20the%20potential%20benefits%20and%20limitations%20of%20using%20generative%0Aagents%20in%20social%20science%20research%20and%20provides%20useful%20guidance%20on%20designing%0Aconsistent%20and%20representative%20agent%20personas%20to%20maximise%20coverage%20and%0Arepresentation%20of%20human%20personality%20traits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00742v1&entry.124074799=Read"},
{"title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "author": "Wenxuan Guo and Xiuwei Xu and Hang Yin and Ziwei Wang and Jianjiang Feng and Jie Zhou and Jiwen Lu", "abstract": "  Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.\n", "link": "http://arxiv.org/abs/2508.00823v1", "date": "2025-08-01", "relevancy": 2.5001, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6318}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6245}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGL-Nav%3A%20Incremental%203D%20Gaussian%20Localization%20for%20Image-goal%20Navigation&body=Title%3A%20IGL-Nav%3A%20Incremental%203D%20Gaussian%20Localization%20for%20Image-goal%20Navigation%0AAuthor%3A%20Wenxuan%20Guo%20and%20Xiuwei%20Xu%20and%20Hang%20Yin%20and%20Ziwei%20Wang%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Visual%20navigation%20with%20an%20image%20as%20goal%20is%20a%20fundamental%20and%20challenging%0Aproblem.%20Conventional%20methods%20either%20rely%20on%20end-to-end%20RL%20learning%20or%0Amodular-based%20policy%20with%20topological%20graph%20or%20BEV%20map%20as%20memory%2C%20which%20cannot%0Afully%20model%20the%20geometric%20relationship%20between%20the%20explored%203D%20environment%20and%0Athe%20goal%20image.%20In%20order%20to%20efficiently%20and%20accurately%20localize%20the%20goal%20image%0Ain%203D%20space%2C%20we%20build%20our%20navigation%20system%20upon%20the%20renderable%203D%20gaussian%0A%283DGS%29%20representation.%20However%2C%20due%20to%20the%20computational%20intensity%20of%203DGS%0Aoptimization%20and%20the%20large%20search%20space%20of%206-DoF%20camera%20pose%2C%20directly%0Aleveraging%203DGS%20for%20image%20localization%20during%20agent%20exploration%20process%20is%0Aprohibitively%20inefficient.%20To%20this%20end%2C%20we%20propose%20IGL-Nav%2C%20an%20Incremental%203D%0AGaussian%20Localization%20framework%20for%20efficient%20and%203D-aware%20image-goal%0Anavigation.%20Specifically%2C%20we%20incrementally%20update%20the%20scene%20representation%20as%0Anew%20images%20arrive%20with%20feed-forward%20monocular%20prediction.%20Then%20we%20coarsely%0Alocalize%20the%20goal%20by%20leveraging%20the%20geometric%20information%20for%20discrete%20space%0Amatching%2C%20which%20can%20be%20equivalent%20to%20efficient%203D%20convolution.%20When%20the%20agent%0Ais%20close%20to%20the%20goal%2C%20we%20finally%20solve%20the%20fine%20target%20pose%20with%20optimization%0Avia%20differentiable%20rendering.%20The%20proposed%20IGL-Nav%20outperforms%20existing%0Astate-of-the-art%20methods%20by%20a%20large%20margin%20across%20diverse%20experimental%0Aconfigurations.%20It%20can%20also%20handle%20the%20more%20challenging%20free-view%20image-goal%0Asetting%20and%20be%20deployed%20on%20real-world%20robotic%20platform%20using%20a%20cellphone%20to%0Acapture%20goal%20image%20at%20arbitrary%20pose.%20Project%20page%3A%0Ahttps%3A//gwxuan.github.io/IGL-Nav/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGL-Nav%253A%2520Incremental%25203D%2520Gaussian%2520Localization%2520for%2520Image-goal%2520Navigation%26entry.906535625%3DWenxuan%2520Guo%2520and%2520Xiuwei%2520Xu%2520and%2520Hang%2520Yin%2520and%2520Ziwei%2520Wang%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Visual%2520navigation%2520with%2520an%2520image%2520as%2520goal%2520is%2520a%2520fundamental%2520and%2520challenging%250Aproblem.%2520Conventional%2520methods%2520either%2520rely%2520on%2520end-to-end%2520RL%2520learning%2520or%250Amodular-based%2520policy%2520with%2520topological%2520graph%2520or%2520BEV%2520map%2520as%2520memory%252C%2520which%2520cannot%250Afully%2520model%2520the%2520geometric%2520relationship%2520between%2520the%2520explored%25203D%2520environment%2520and%250Athe%2520goal%2520image.%2520In%2520order%2520to%2520efficiently%2520and%2520accurately%2520localize%2520the%2520goal%2520image%250Ain%25203D%2520space%252C%2520we%2520build%2520our%2520navigation%2520system%2520upon%2520the%2520renderable%25203D%2520gaussian%250A%25283DGS%2529%2520representation.%2520However%252C%2520due%2520to%2520the%2520computational%2520intensity%2520of%25203DGS%250Aoptimization%2520and%2520the%2520large%2520search%2520space%2520of%25206-DoF%2520camera%2520pose%252C%2520directly%250Aleveraging%25203DGS%2520for%2520image%2520localization%2520during%2520agent%2520exploration%2520process%2520is%250Aprohibitively%2520inefficient.%2520To%2520this%2520end%252C%2520we%2520propose%2520IGL-Nav%252C%2520an%2520Incremental%25203D%250AGaussian%2520Localization%2520framework%2520for%2520efficient%2520and%25203D-aware%2520image-goal%250Anavigation.%2520Specifically%252C%2520we%2520incrementally%2520update%2520the%2520scene%2520representation%2520as%250Anew%2520images%2520arrive%2520with%2520feed-forward%2520monocular%2520prediction.%2520Then%2520we%2520coarsely%250Alocalize%2520the%2520goal%2520by%2520leveraging%2520the%2520geometric%2520information%2520for%2520discrete%2520space%250Amatching%252C%2520which%2520can%2520be%2520equivalent%2520to%2520efficient%25203D%2520convolution.%2520When%2520the%2520agent%250Ais%2520close%2520to%2520the%2520goal%252C%2520we%2520finally%2520solve%2520the%2520fine%2520target%2520pose%2520with%2520optimization%250Avia%2520differentiable%2520rendering.%2520The%2520proposed%2520IGL-Nav%2520outperforms%2520existing%250Astate-of-the-art%2520methods%2520by%2520a%2520large%2520margin%2520across%2520diverse%2520experimental%250Aconfigurations.%2520It%2520can%2520also%2520handle%2520the%2520more%2520challenging%2520free-view%2520image-goal%250Asetting%2520and%2520be%2520deployed%2520on%2520real-world%2520robotic%2520platform%2520using%2520a%2520cellphone%2520to%250Acapture%2520goal%2520image%2520at%2520arbitrary%2520pose.%2520Project%2520page%253A%250Ahttps%253A//gwxuan.github.io/IGL-Nav/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGL-Nav%3A%20Incremental%203D%20Gaussian%20Localization%20for%20Image-goal%20Navigation&entry.906535625=Wenxuan%20Guo%20and%20Xiuwei%20Xu%20and%20Hang%20Yin%20and%20Ziwei%20Wang%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Visual%20navigation%20with%20an%20image%20as%20goal%20is%20a%20fundamental%20and%20challenging%0Aproblem.%20Conventional%20methods%20either%20rely%20on%20end-to-end%20RL%20learning%20or%0Amodular-based%20policy%20with%20topological%20graph%20or%20BEV%20map%20as%20memory%2C%20which%20cannot%0Afully%20model%20the%20geometric%20relationship%20between%20the%20explored%203D%20environment%20and%0Athe%20goal%20image.%20In%20order%20to%20efficiently%20and%20accurately%20localize%20the%20goal%20image%0Ain%203D%20space%2C%20we%20build%20our%20navigation%20system%20upon%20the%20renderable%203D%20gaussian%0A%283DGS%29%20representation.%20However%2C%20due%20to%20the%20computational%20intensity%20of%203DGS%0Aoptimization%20and%20the%20large%20search%20space%20of%206-DoF%20camera%20pose%2C%20directly%0Aleveraging%203DGS%20for%20image%20localization%20during%20agent%20exploration%20process%20is%0Aprohibitively%20inefficient.%20To%20this%20end%2C%20we%20propose%20IGL-Nav%2C%20an%20Incremental%203D%0AGaussian%20Localization%20framework%20for%20efficient%20and%203D-aware%20image-goal%0Anavigation.%20Specifically%2C%20we%20incrementally%20update%20the%20scene%20representation%20as%0Anew%20images%20arrive%20with%20feed-forward%20monocular%20prediction.%20Then%20we%20coarsely%0Alocalize%20the%20goal%20by%20leveraging%20the%20geometric%20information%20for%20discrete%20space%0Amatching%2C%20which%20can%20be%20equivalent%20to%20efficient%203D%20convolution.%20When%20the%20agent%0Ais%20close%20to%20the%20goal%2C%20we%20finally%20solve%20the%20fine%20target%20pose%20with%20optimization%0Avia%20differentiable%20rendering.%20The%20proposed%20IGL-Nav%20outperforms%20existing%0Astate-of-the-art%20methods%20by%20a%20large%20margin%20across%20diverse%20experimental%0Aconfigurations.%20It%20can%20also%20handle%20the%20more%20challenging%20free-view%20image-goal%0Asetting%20and%20be%20deployed%20on%20real-world%20robotic%20platform%20using%20a%20cellphone%20to%0Acapture%20goal%20image%20at%20arbitrary%20pose.%20Project%20page%3A%0Ahttps%3A//gwxuan.github.io/IGL-Nav/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00823v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise\n  Out-of-Distribution Detection", "author": "Marc H\u00f6lle and Walter Kellermann and Vasileios Belagiannis", "abstract": "  Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.\n", "link": "http://arxiv.org/abs/2508.00587v1", "date": "2025-08-01", "relevancy": 2.479, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6611}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6312}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Likelihood%20Ratio%20Estimation%20for%20Pixel-Wise%0A%20%20Out-of-Distribution%20Detection&body=Title%3A%20Uncertainty-Aware%20Likelihood%20Ratio%20Estimation%20for%20Pixel-Wise%0A%20%20Out-of-Distribution%20Detection%0AAuthor%3A%20Marc%20H%C3%B6lle%20and%20Walter%20Kellermann%20and%20Vasileios%20Belagiannis%0AAbstract%3A%20%20%20Semantic%20segmentation%20models%20trained%20on%20known%20object%20classes%20often%20fail%20in%0Areal-world%20autonomous%20driving%20scenarios%20by%20confidently%20misclassifying%20unknown%0Aobjects.%20While%20pixel-wise%20out-of-distribution%20detection%20can%20identify%20unknown%0Aobjects%2C%20existing%20methods%20struggle%20in%20complex%20scenes%20where%20rare%20object%20classes%0Aare%20often%20confused%20with%20truly%20unknown%20objects.%20We%20introduce%20an%0Auncertainty-aware%20likelihood%20ratio%20estimation%20method%20that%20addresses%20these%0Alimitations.%20Our%20approach%20uses%20an%20evidential%20classifier%20within%20a%20likelihood%0Aratio%20test%20to%20distinguish%20between%20known%20and%20unknown%20pixel%20features%20from%20a%0Asemantic%20segmentation%20model%2C%20while%20explicitly%20accounting%20for%20uncertainty.%0AInstead%20of%20producing%20point%20estimates%2C%20our%20method%20outputs%20probability%0Adistributions%20that%20capture%20uncertainty%20from%20both%20rare%20training%20examples%20and%0Aimperfect%20synthetic%20outliers.%20We%20show%20that%20by%20incorporating%20uncertainty%20in%20this%0Away%2C%20outlier%20exposure%20can%20be%20leveraged%20more%20effectively.%20Evaluated%20on%20five%0Astandard%20benchmark%20datasets%2C%20our%20method%20achieves%20the%20lowest%20average%20false%0Apositive%20rate%20%282.5%25%29%20among%20state-of-the-art%20while%20maintaining%20high%20average%0Aprecision%20%2890.91%25%29%20and%20incurring%20only%20negligible%20computational%20overhead.%20Code%0Ais%20available%20at%20https%3A//github.com/glasbruch/ULRE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Likelihood%2520Ratio%2520Estimation%2520for%2520Pixel-Wise%250A%2520%2520Out-of-Distribution%2520Detection%26entry.906535625%3DMarc%2520H%25C3%25B6lle%2520and%2520Walter%2520Kellermann%2520and%2520Vasileios%2520Belagiannis%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520models%2520trained%2520on%2520known%2520object%2520classes%2520often%2520fail%2520in%250Areal-world%2520autonomous%2520driving%2520scenarios%2520by%2520confidently%2520misclassifying%2520unknown%250Aobjects.%2520While%2520pixel-wise%2520out-of-distribution%2520detection%2520can%2520identify%2520unknown%250Aobjects%252C%2520existing%2520methods%2520struggle%2520in%2520complex%2520scenes%2520where%2520rare%2520object%2520classes%250Aare%2520often%2520confused%2520with%2520truly%2520unknown%2520objects.%2520We%2520introduce%2520an%250Auncertainty-aware%2520likelihood%2520ratio%2520estimation%2520method%2520that%2520addresses%2520these%250Alimitations.%2520Our%2520approach%2520uses%2520an%2520evidential%2520classifier%2520within%2520a%2520likelihood%250Aratio%2520test%2520to%2520distinguish%2520between%2520known%2520and%2520unknown%2520pixel%2520features%2520from%2520a%250Asemantic%2520segmentation%2520model%252C%2520while%2520explicitly%2520accounting%2520for%2520uncertainty.%250AInstead%2520of%2520producing%2520point%2520estimates%252C%2520our%2520method%2520outputs%2520probability%250Adistributions%2520that%2520capture%2520uncertainty%2520from%2520both%2520rare%2520training%2520examples%2520and%250Aimperfect%2520synthetic%2520outliers.%2520We%2520show%2520that%2520by%2520incorporating%2520uncertainty%2520in%2520this%250Away%252C%2520outlier%2520exposure%2520can%2520be%2520leveraged%2520more%2520effectively.%2520Evaluated%2520on%2520five%250Astandard%2520benchmark%2520datasets%252C%2520our%2520method%2520achieves%2520the%2520lowest%2520average%2520false%250Apositive%2520rate%2520%25282.5%2525%2529%2520among%2520state-of-the-art%2520while%2520maintaining%2520high%2520average%250Aprecision%2520%252890.91%2525%2529%2520and%2520incurring%2520only%2520negligible%2520computational%2520overhead.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/glasbruch/ULRE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Likelihood%20Ratio%20Estimation%20for%20Pixel-Wise%0A%20%20Out-of-Distribution%20Detection&entry.906535625=Marc%20H%C3%B6lle%20and%20Walter%20Kellermann%20and%20Vasileios%20Belagiannis&entry.1292438233=%20%20Semantic%20segmentation%20models%20trained%20on%20known%20object%20classes%20often%20fail%20in%0Areal-world%20autonomous%20driving%20scenarios%20by%20confidently%20misclassifying%20unknown%0Aobjects.%20While%20pixel-wise%20out-of-distribution%20detection%20can%20identify%20unknown%0Aobjects%2C%20existing%20methods%20struggle%20in%20complex%20scenes%20where%20rare%20object%20classes%0Aare%20often%20confused%20with%20truly%20unknown%20objects.%20We%20introduce%20an%0Auncertainty-aware%20likelihood%20ratio%20estimation%20method%20that%20addresses%20these%0Alimitations.%20Our%20approach%20uses%20an%20evidential%20classifier%20within%20a%20likelihood%0Aratio%20test%20to%20distinguish%20between%20known%20and%20unknown%20pixel%20features%20from%20a%0Asemantic%20segmentation%20model%2C%20while%20explicitly%20accounting%20for%20uncertainty.%0AInstead%20of%20producing%20point%20estimates%2C%20our%20method%20outputs%20probability%0Adistributions%20that%20capture%20uncertainty%20from%20both%20rare%20training%20examples%20and%0Aimperfect%20synthetic%20outliers.%20We%20show%20that%20by%20incorporating%20uncertainty%20in%20this%0Away%2C%20outlier%20exposure%20can%20be%20leveraged%20more%20effectively.%20Evaluated%20on%20five%0Astandard%20benchmark%20datasets%2C%20our%20method%20achieves%20the%20lowest%20average%20false%0Apositive%20rate%20%282.5%25%29%20among%20state-of-the-art%20while%20maintaining%20high%20average%0Aprecision%20%2890.91%25%29%20and%20incurring%20only%20negligible%20computational%20overhead.%20Code%0Ais%20available%20at%20https%3A//github.com/glasbruch/ULRE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00587v1&entry.124074799=Read"},
{"title": "Scalable Outdoors Autonomous Drone Flight with Visual-Inertial SLAM and\n  Dense Submaps Built without LiDAR", "author": "Sebasti\u00e1n Barbas Laina and Simon Boche and Sotiris Papatheodorou and Dimos Tzoumanikas and Simon Schaefer and Hanzhi Chen and Stefan Leutenegger", "abstract": "  Autonomous navigation is needed for several robotics applications. In this\npaper we present an autonomous Micro Aerial Vehicle (MAV) system which purely\nrelies on cost-effective and light-weight passive visual and inertial sensors\nto perform large-scale autonomous navigation in outdoor,unstructured and\ncluttered environments. We leverage visual-inertial simultaneous localization\nand mapping (VI-SLAM) for accurate MAV state estimates and couple it with a\nvolumetric occupancy submapping system to achieve a scalable mapping framework\nwhich can be directly used for path planning. To ensure the safety of the MAV\nduring navigation, we also propose a novel reference trajectory anchoring\nscheme that deforms the reference trajectory the MAV is tracking upon state\nupdates from the VI-SLAM system in a consistent way, even upon large state\nupdates due to loop-closures. We thoroughly validate our system in both real\nand simulated forest environments and at peak velocities up to 3 m/s while not\nencountering a single collision or system failure. To the best of our\nknowledge, this is the first system which achieves this level of performance in\nsuch an unstructured environment using low-cost passive visual sensors and\nfully on-board computation, including VI-SLAM.\n", "link": "http://arxiv.org/abs/2403.09596v2", "date": "2025-08-01", "relevancy": 2.4323, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6262}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Outdoors%20Autonomous%20Drone%20Flight%20with%20Visual-Inertial%20SLAM%20and%0A%20%20Dense%20Submaps%20Built%20without%20LiDAR&body=Title%3A%20Scalable%20Outdoors%20Autonomous%20Drone%20Flight%20with%20Visual-Inertial%20SLAM%20and%0A%20%20Dense%20Submaps%20Built%20without%20LiDAR%0AAuthor%3A%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Dimos%20Tzoumanikas%20and%20Simon%20Schaefer%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20Autonomous%20navigation%20is%20needed%20for%20several%20robotics%20applications.%20In%20this%0Apaper%20we%20present%20an%20autonomous%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%20which%20purely%0Arelies%20on%20cost-effective%20and%20light-weight%20passive%20visual%20and%20inertial%20sensors%0Ato%20perform%20large-scale%20autonomous%20navigation%20in%20outdoor%2Cunstructured%20and%0Acluttered%20environments.%20We%20leverage%20visual-inertial%20simultaneous%20localization%0Aand%20mapping%20%28VI-SLAM%29%20for%20accurate%20MAV%20state%20estimates%20and%20couple%20it%20with%20a%0Avolumetric%20occupancy%20submapping%20system%20to%20achieve%20a%20scalable%20mapping%20framework%0Awhich%20can%20be%20directly%20used%20for%20path%20planning.%20To%20ensure%20the%20safety%20of%20the%20MAV%0Aduring%20navigation%2C%20we%20also%20propose%20a%20novel%20reference%20trajectory%20anchoring%0Ascheme%20that%20deforms%20the%20reference%20trajectory%20the%20MAV%20is%20tracking%20upon%20state%0Aupdates%20from%20the%20VI-SLAM%20system%20in%20a%20consistent%20way%2C%20even%20upon%20large%20state%0Aupdates%20due%20to%20loop-closures.%20We%20thoroughly%20validate%20our%20system%20in%20both%20real%0Aand%20simulated%20forest%20environments%20and%20at%20peak%20velocities%20up%20to%203%20m/s%20while%20not%0Aencountering%20a%20single%20collision%20or%20system%20failure.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20system%20which%20achieves%20this%20level%20of%20performance%20in%0Asuch%20an%20unstructured%20environment%20using%20low-cost%20passive%20visual%20sensors%20and%0Afully%20on-board%20computation%2C%20including%20VI-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Outdoors%2520Autonomous%2520Drone%2520Flight%2520with%2520Visual-Inertial%2520SLAM%2520and%250A%2520%2520Dense%2520Submaps%2520Built%2520without%2520LiDAR%26entry.906535625%3DSebasti%25C3%25A1n%2520Barbas%2520Laina%2520and%2520Simon%2520Boche%2520and%2520Sotiris%2520Papatheodorou%2520and%2520Dimos%2520Tzoumanikas%2520and%2520Simon%2520Schaefer%2520and%2520Hanzhi%2520Chen%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520is%2520needed%2520for%2520several%2520robotics%2520applications.%2520In%2520this%250Apaper%2520we%2520present%2520an%2520autonomous%2520Micro%2520Aerial%2520Vehicle%2520%2528MAV%2529%2520system%2520which%2520purely%250Arelies%2520on%2520cost-effective%2520and%2520light-weight%2520passive%2520visual%2520and%2520inertial%2520sensors%250Ato%2520perform%2520large-scale%2520autonomous%2520navigation%2520in%2520outdoor%252Cunstructured%2520and%250Acluttered%2520environments.%2520We%2520leverage%2520visual-inertial%2520simultaneous%2520localization%250Aand%2520mapping%2520%2528VI-SLAM%2529%2520for%2520accurate%2520MAV%2520state%2520estimates%2520and%2520couple%2520it%2520with%2520a%250Avolumetric%2520occupancy%2520submapping%2520system%2520to%2520achieve%2520a%2520scalable%2520mapping%2520framework%250Awhich%2520can%2520be%2520directly%2520used%2520for%2520path%2520planning.%2520To%2520ensure%2520the%2520safety%2520of%2520the%2520MAV%250Aduring%2520navigation%252C%2520we%2520also%2520propose%2520a%2520novel%2520reference%2520trajectory%2520anchoring%250Ascheme%2520that%2520deforms%2520the%2520reference%2520trajectory%2520the%2520MAV%2520is%2520tracking%2520upon%2520state%250Aupdates%2520from%2520the%2520VI-SLAM%2520system%2520in%2520a%2520consistent%2520way%252C%2520even%2520upon%2520large%2520state%250Aupdates%2520due%2520to%2520loop-closures.%2520We%2520thoroughly%2520validate%2520our%2520system%2520in%2520both%2520real%250Aand%2520simulated%2520forest%2520environments%2520and%2520at%2520peak%2520velocities%2520up%2520to%25203%2520m/s%2520while%2520not%250Aencountering%2520a%2520single%2520collision%2520or%2520system%2520failure.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520system%2520which%2520achieves%2520this%2520level%2520of%2520performance%2520in%250Asuch%2520an%2520unstructured%2520environment%2520using%2520low-cost%2520passive%2520visual%2520sensors%2520and%250Afully%2520on-board%2520computation%252C%2520including%2520VI-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Outdoors%20Autonomous%20Drone%20Flight%20with%20Visual-Inertial%20SLAM%20and%0A%20%20Dense%20Submaps%20Built%20without%20LiDAR&entry.906535625=Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Dimos%20Tzoumanikas%20and%20Simon%20Schaefer%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20Autonomous%20navigation%20is%20needed%20for%20several%20robotics%20applications.%20In%20this%0Apaper%20we%20present%20an%20autonomous%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%20which%20purely%0Arelies%20on%20cost-effective%20and%20light-weight%20passive%20visual%20and%20inertial%20sensors%0Ato%20perform%20large-scale%20autonomous%20navigation%20in%20outdoor%2Cunstructured%20and%0Acluttered%20environments.%20We%20leverage%20visual-inertial%20simultaneous%20localization%0Aand%20mapping%20%28VI-SLAM%29%20for%20accurate%20MAV%20state%20estimates%20and%20couple%20it%20with%20a%0Avolumetric%20occupancy%20submapping%20system%20to%20achieve%20a%20scalable%20mapping%20framework%0Awhich%20can%20be%20directly%20used%20for%20path%20planning.%20To%20ensure%20the%20safety%20of%20the%20MAV%0Aduring%20navigation%2C%20we%20also%20propose%20a%20novel%20reference%20trajectory%20anchoring%0Ascheme%20that%20deforms%20the%20reference%20trajectory%20the%20MAV%20is%20tracking%20upon%20state%0Aupdates%20from%20the%20VI-SLAM%20system%20in%20a%20consistent%20way%2C%20even%20upon%20large%20state%0Aupdates%20due%20to%20loop-closures.%20We%20thoroughly%20validate%20our%20system%20in%20both%20real%0Aand%20simulated%20forest%20environments%20and%20at%20peak%20velocities%20up%20to%203%20m/s%20while%20not%0Aencountering%20a%20single%20collision%20or%20system%20failure.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20system%20which%20achieves%20this%20level%20of%20performance%20in%0Asuch%20an%20unstructured%20environment%20using%20low-cost%20passive%20visual%20sensors%20and%0Afully%20on-board%20computation%2C%20including%20VI-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09596v2&entry.124074799=Read"},
{"title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on\n  Planetary Rovers Using RGB, Depth, and Thermal Imagery", "author": "Raul Castilla-Arquillo and Carlos Perez-del-Pulgar and Levin Gerdes and Alfonso Garcia-Cerezo and Miguel A. Olivares-Mendez", "abstract": "  Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.\n", "link": "http://arxiv.org/abs/2508.00580v1", "date": "2025-08-01", "relevancy": 2.377, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6134}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniUnet%3A%20A%20Multimodal%20Network%20for%20Unstructured%20Terrain%20Segmentation%20on%0A%20%20Planetary%20Rovers%20Using%20RGB%2C%20Depth%2C%20and%20Thermal%20Imagery&body=Title%3A%20OmniUnet%3A%20A%20Multimodal%20Network%20for%20Unstructured%20Terrain%20Segmentation%20on%0A%20%20Planetary%20Rovers%20Using%20RGB%2C%20Depth%2C%20and%20Thermal%20Imagery%0AAuthor%3A%20Raul%20Castilla-Arquillo%20and%20Carlos%20Perez-del-Pulgar%20and%20Levin%20Gerdes%20and%20Alfonso%20Garcia-Cerezo%20and%20Miguel%20A.%20Olivares-Mendez%0AAbstract%3A%20%20%20Robot%20navigation%20in%20unstructured%20environments%20requires%20multimodal%20perception%0Asystems%20that%20can%20support%20safe%20navigation.%20Multimodality%20enables%20the%20integration%0Aof%20complementary%20information%20collected%20by%20different%20sensors.%20However%2C%20this%0Ainformation%20must%20be%20processed%20by%20machine%20learning%20algorithms%20specifically%0Adesigned%20to%20leverage%20heterogeneous%20data.%20Furthermore%2C%20it%20is%20necessary%20to%0Aidentify%20which%20sensor%20modalities%20are%20most%20informative%20for%20navigation%20in%20the%0Atarget%20environment.%20In%20Martian%20exploration%2C%20thermal%20imagery%20has%20proven%20valuable%0Afor%20assessing%20terrain%20safety%20due%20to%20differences%20in%20thermal%20behaviour%20between%0Asoil%20types.%20This%20work%20presents%20OmniUnet%2C%20a%20transformer-based%20neural%20network%0Aarchitecture%20for%20semantic%20segmentation%20using%20RGB%2C%20depth%2C%20and%20thermal%20%28RGB-D-T%29%0Aimagery.%20A%20custom%20multimodal%20sensor%20housing%20was%20developed%20using%203D%20printing%20and%0Amounted%20on%20the%20Martian%20Rover%20Testbed%20for%20Autonomy%20%28MaRTA%29%20to%20collect%20a%0Amultimodal%20dataset%20in%20the%20Bardenas%20semi-desert%20in%20northern%20Spain.%20This%20location%0Aserves%20as%20a%20representative%20environment%20of%20the%20Martian%20surface%2C%20featuring%0Aterrain%20types%20such%20as%20sand%2C%20bedrock%2C%20and%20compact%20soil.%20A%20subset%20of%20this%20dataset%0Awas%20manually%20labeled%20to%20support%20supervised%20training%20of%20the%20network.%20The%20model%0Awas%20evaluated%20both%20quantitatively%20and%20qualitatively%2C%20achieving%20a%20pixel%20accuracy%0Aof%2080.37%25%20and%20demonstrating%20strong%20performance%20in%20segmenting%20complex%0Aunstructured%20terrain.%20Inference%20tests%20yielded%20an%20average%20prediction%20time%20of%20673%0Ams%20on%20a%20resource-constrained%20computer%20%28Jetson%20Orin%20Nano%29%2C%20confirming%20its%0Asuitability%20for%20on-robot%20deployment.%20The%20software%20implementation%20of%20the%20network%0Aand%20the%20labeled%20dataset%20have%20been%20made%20publicly%20available%20to%20support%20future%0Aresearch%20in%20multimodal%20terrain%20perception%20for%20planetary%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniUnet%253A%2520A%2520Multimodal%2520Network%2520for%2520Unstructured%2520Terrain%2520Segmentation%2520on%250A%2520%2520Planetary%2520Rovers%2520Using%2520RGB%252C%2520Depth%252C%2520and%2520Thermal%2520Imagery%26entry.906535625%3DRaul%2520Castilla-Arquillo%2520and%2520Carlos%2520Perez-del-Pulgar%2520and%2520Levin%2520Gerdes%2520and%2520Alfonso%2520Garcia-Cerezo%2520and%2520Miguel%2520A.%2520Olivares-Mendez%26entry.1292438233%3D%2520%2520Robot%2520navigation%2520in%2520unstructured%2520environments%2520requires%2520multimodal%2520perception%250Asystems%2520that%2520can%2520support%2520safe%2520navigation.%2520Multimodality%2520enables%2520the%2520integration%250Aof%2520complementary%2520information%2520collected%2520by%2520different%2520sensors.%2520However%252C%2520this%250Ainformation%2520must%2520be%2520processed%2520by%2520machine%2520learning%2520algorithms%2520specifically%250Adesigned%2520to%2520leverage%2520heterogeneous%2520data.%2520Furthermore%252C%2520it%2520is%2520necessary%2520to%250Aidentify%2520which%2520sensor%2520modalities%2520are%2520most%2520informative%2520for%2520navigation%2520in%2520the%250Atarget%2520environment.%2520In%2520Martian%2520exploration%252C%2520thermal%2520imagery%2520has%2520proven%2520valuable%250Afor%2520assessing%2520terrain%2520safety%2520due%2520to%2520differences%2520in%2520thermal%2520behaviour%2520between%250Asoil%2520types.%2520This%2520work%2520presents%2520OmniUnet%252C%2520a%2520transformer-based%2520neural%2520network%250Aarchitecture%2520for%2520semantic%2520segmentation%2520using%2520RGB%252C%2520depth%252C%2520and%2520thermal%2520%2528RGB-D-T%2529%250Aimagery.%2520A%2520custom%2520multimodal%2520sensor%2520housing%2520was%2520developed%2520using%25203D%2520printing%2520and%250Amounted%2520on%2520the%2520Martian%2520Rover%2520Testbed%2520for%2520Autonomy%2520%2528MaRTA%2529%2520to%2520collect%2520a%250Amultimodal%2520dataset%2520in%2520the%2520Bardenas%2520semi-desert%2520in%2520northern%2520Spain.%2520This%2520location%250Aserves%2520as%2520a%2520representative%2520environment%2520of%2520the%2520Martian%2520surface%252C%2520featuring%250Aterrain%2520types%2520such%2520as%2520sand%252C%2520bedrock%252C%2520and%2520compact%2520soil.%2520A%2520subset%2520of%2520this%2520dataset%250Awas%2520manually%2520labeled%2520to%2520support%2520supervised%2520training%2520of%2520the%2520network.%2520The%2520model%250Awas%2520evaluated%2520both%2520quantitatively%2520and%2520qualitatively%252C%2520achieving%2520a%2520pixel%2520accuracy%250Aof%252080.37%2525%2520and%2520demonstrating%2520strong%2520performance%2520in%2520segmenting%2520complex%250Aunstructured%2520terrain.%2520Inference%2520tests%2520yielded%2520an%2520average%2520prediction%2520time%2520of%2520673%250Ams%2520on%2520a%2520resource-constrained%2520computer%2520%2528Jetson%2520Orin%2520Nano%2529%252C%2520confirming%2520its%250Asuitability%2520for%2520on-robot%2520deployment.%2520The%2520software%2520implementation%2520of%2520the%2520network%250Aand%2520the%2520labeled%2520dataset%2520have%2520been%2520made%2520publicly%2520available%2520to%2520support%2520future%250Aresearch%2520in%2520multimodal%2520terrain%2520perception%2520for%2520planetary%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniUnet%3A%20A%20Multimodal%20Network%20for%20Unstructured%20Terrain%20Segmentation%20on%0A%20%20Planetary%20Rovers%20Using%20RGB%2C%20Depth%2C%20and%20Thermal%20Imagery&entry.906535625=Raul%20Castilla-Arquillo%20and%20Carlos%20Perez-del-Pulgar%20and%20Levin%20Gerdes%20and%20Alfonso%20Garcia-Cerezo%20and%20Miguel%20A.%20Olivares-Mendez&entry.1292438233=%20%20Robot%20navigation%20in%20unstructured%20environments%20requires%20multimodal%20perception%0Asystems%20that%20can%20support%20safe%20navigation.%20Multimodality%20enables%20the%20integration%0Aof%20complementary%20information%20collected%20by%20different%20sensors.%20However%2C%20this%0Ainformation%20must%20be%20processed%20by%20machine%20learning%20algorithms%20specifically%0Adesigned%20to%20leverage%20heterogeneous%20data.%20Furthermore%2C%20it%20is%20necessary%20to%0Aidentify%20which%20sensor%20modalities%20are%20most%20informative%20for%20navigation%20in%20the%0Atarget%20environment.%20In%20Martian%20exploration%2C%20thermal%20imagery%20has%20proven%20valuable%0Afor%20assessing%20terrain%20safety%20due%20to%20differences%20in%20thermal%20behaviour%20between%0Asoil%20types.%20This%20work%20presents%20OmniUnet%2C%20a%20transformer-based%20neural%20network%0Aarchitecture%20for%20semantic%20segmentation%20using%20RGB%2C%20depth%2C%20and%20thermal%20%28RGB-D-T%29%0Aimagery.%20A%20custom%20multimodal%20sensor%20housing%20was%20developed%20using%203D%20printing%20and%0Amounted%20on%20the%20Martian%20Rover%20Testbed%20for%20Autonomy%20%28MaRTA%29%20to%20collect%20a%0Amultimodal%20dataset%20in%20the%20Bardenas%20semi-desert%20in%20northern%20Spain.%20This%20location%0Aserves%20as%20a%20representative%20environment%20of%20the%20Martian%20surface%2C%20featuring%0Aterrain%20types%20such%20as%20sand%2C%20bedrock%2C%20and%20compact%20soil.%20A%20subset%20of%20this%20dataset%0Awas%20manually%20labeled%20to%20support%20supervised%20training%20of%20the%20network.%20The%20model%0Awas%20evaluated%20both%20quantitatively%20and%20qualitatively%2C%20achieving%20a%20pixel%20accuracy%0Aof%2080.37%25%20and%20demonstrating%20strong%20performance%20in%20segmenting%20complex%0Aunstructured%20terrain.%20Inference%20tests%20yielded%20an%20average%20prediction%20time%20of%20673%0Ams%20on%20a%20resource-constrained%20computer%20%28Jetson%20Orin%20Nano%29%2C%20confirming%20its%0Asuitability%20for%20on-robot%20deployment.%20The%20software%20implementation%20of%20the%20network%0Aand%20the%20labeled%20dataset%20have%20been%20made%20publicly%20available%20to%20support%20future%0Aresearch%20in%20multimodal%20terrain%20perception%20for%20planetary%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00580v1&entry.124074799=Read"},
{"title": "Towards Fair In-Context Learning with Tabular Foundation Models", "author": "Patrik Kenfack and Samira Ebrahimi Kahou and Ulrich A\u00efvodji", "abstract": "  Transformer-based tabular foundation models have recently demonstrated\npromising in-context learning (ICL) performance on structured data, emerging as\ncompetitive alternatives to gradient-boosted trees. However, the fairness\nimplications of this new paradigm remain largely unexplored. We present the\nfirst investigation of fairness in tabular ICL, evaluating three recently\nproposed foundation models -- TabPFNv2, TabICL, and TabDPT -- on multiple\nbenchmark datasets. To mitigate biases, we explore three pre-processing\nfairness-enhancing methods: correlation removal (decorrelating input features\nfrom the sensitive attribute), group-balanced sample selection (ensuring equal\nrepresentation of protected groups in context examples), and uncertainty-based\nsample selection (prioritizing context examples with high sensitive-attribute\nprediction uncertainty). Our experiments show that the uncertainty-based\nstrategy consistently improves group fairness metrics (e.g., demographic\nparity, equalized odds, and equal opportunity) with minimal impact on\npredictive accuracy. We release our code to facilitate reproducibility\n(https://github.com/patrikken/Fair-TabICL)\n", "link": "http://arxiv.org/abs/2505.09503v3", "date": "2025-08-01", "relevancy": 2.376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&body=Title%3A%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models%0AAuthor%3A%20Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kahou%20and%20Ulrich%20A%C3%AFvodji%0AAbstract%3A%20%20%20Transformer-based%20tabular%20foundation%20models%20have%20recently%20demonstrated%0Apromising%20in-context%20learning%20%28ICL%29%20performance%20on%20structured%20data%2C%20emerging%20as%0Acompetitive%20alternatives%20to%20gradient-boosted%20trees.%20However%2C%20the%20fairness%0Aimplications%20of%20this%20new%20paradigm%20remain%20largely%20unexplored.%20We%20present%20the%0Afirst%20investigation%20of%20fairness%20in%20tabular%20ICL%2C%20evaluating%20three%20recently%0Aproposed%20foundation%20models%20--%20TabPFNv2%2C%20TabICL%2C%20and%20TabDPT%20--%20on%20multiple%0Abenchmark%20datasets.%20To%20mitigate%20biases%2C%20we%20explore%20three%20pre-processing%0Afairness-enhancing%20methods%3A%20correlation%20removal%20%28decorrelating%20input%20features%0Afrom%20the%20sensitive%20attribute%29%2C%20group-balanced%20sample%20selection%20%28ensuring%20equal%0Arepresentation%20of%20protected%20groups%20in%20context%20examples%29%2C%20and%20uncertainty-based%0Asample%20selection%20%28prioritizing%20context%20examples%20with%20high%20sensitive-attribute%0Aprediction%20uncertainty%29.%20Our%20experiments%20show%20that%20the%20uncertainty-based%0Astrategy%20consistently%20improves%20group%20fairness%20metrics%20%28e.g.%2C%20demographic%0Aparity%2C%20equalized%20odds%2C%20and%20equal%20opportunity%29%20with%20minimal%20impact%20on%0Apredictive%20accuracy.%20We%20release%20our%20code%20to%20facilitate%20reproducibility%0A%28https%3A//github.com/patrikken/Fair-TabICL%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520In-Context%2520Learning%2520with%2520Tabular%2520Foundation%2520Models%26entry.906535625%3DPatrik%2520Kenfack%2520and%2520Samira%2520Ebrahimi%2520Kahou%2520and%2520Ulrich%2520A%25C3%25AFvodji%26entry.1292438233%3D%2520%2520Transformer-based%2520tabular%2520foundation%2520models%2520have%2520recently%2520demonstrated%250Apromising%2520in-context%2520learning%2520%2528ICL%2529%2520performance%2520on%2520structured%2520data%252C%2520emerging%2520as%250Acompetitive%2520alternatives%2520to%2520gradient-boosted%2520trees.%2520However%252C%2520the%2520fairness%250Aimplications%2520of%2520this%2520new%2520paradigm%2520remain%2520largely%2520unexplored.%2520We%2520present%2520the%250Afirst%2520investigation%2520of%2520fairness%2520in%2520tabular%2520ICL%252C%2520evaluating%2520three%2520recently%250Aproposed%2520foundation%2520models%2520--%2520TabPFNv2%252C%2520TabICL%252C%2520and%2520TabDPT%2520--%2520on%2520multiple%250Abenchmark%2520datasets.%2520To%2520mitigate%2520biases%252C%2520we%2520explore%2520three%2520pre-processing%250Afairness-enhancing%2520methods%253A%2520correlation%2520removal%2520%2528decorrelating%2520input%2520features%250Afrom%2520the%2520sensitive%2520attribute%2529%252C%2520group-balanced%2520sample%2520selection%2520%2528ensuring%2520equal%250Arepresentation%2520of%2520protected%2520groups%2520in%2520context%2520examples%2529%252C%2520and%2520uncertainty-based%250Asample%2520selection%2520%2528prioritizing%2520context%2520examples%2520with%2520high%2520sensitive-attribute%250Aprediction%2520uncertainty%2529.%2520Our%2520experiments%2520show%2520that%2520the%2520uncertainty-based%250Astrategy%2520consistently%2520improves%2520group%2520fairness%2520metrics%2520%2528e.g.%252C%2520demographic%250Aparity%252C%2520equalized%2520odds%252C%2520and%2520equal%2520opportunity%2529%2520with%2520minimal%2520impact%2520on%250Apredictive%2520accuracy.%2520We%2520release%2520our%2520code%2520to%2520facilitate%2520reproducibility%250A%2528https%253A//github.com/patrikken/Fair-TabICL%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&entry.906535625=Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kahou%20and%20Ulrich%20A%C3%AFvodji&entry.1292438233=%20%20Transformer-based%20tabular%20foundation%20models%20have%20recently%20demonstrated%0Apromising%20in-context%20learning%20%28ICL%29%20performance%20on%20structured%20data%2C%20emerging%20as%0Acompetitive%20alternatives%20to%20gradient-boosted%20trees.%20However%2C%20the%20fairness%0Aimplications%20of%20this%20new%20paradigm%20remain%20largely%20unexplored.%20We%20present%20the%0Afirst%20investigation%20of%20fairness%20in%20tabular%20ICL%2C%20evaluating%20three%20recently%0Aproposed%20foundation%20models%20--%20TabPFNv2%2C%20TabICL%2C%20and%20TabDPT%20--%20on%20multiple%0Abenchmark%20datasets.%20To%20mitigate%20biases%2C%20we%20explore%20three%20pre-processing%0Afairness-enhancing%20methods%3A%20correlation%20removal%20%28decorrelating%20input%20features%0Afrom%20the%20sensitive%20attribute%29%2C%20group-balanced%20sample%20selection%20%28ensuring%20equal%0Arepresentation%20of%20protected%20groups%20in%20context%20examples%29%2C%20and%20uncertainty-based%0Asample%20selection%20%28prioritizing%20context%20examples%20with%20high%20sensitive-attribute%0Aprediction%20uncertainty%29.%20Our%20experiments%20show%20that%20the%20uncertainty-based%0Astrategy%20consistently%20improves%20group%20fairness%20metrics%20%28e.g.%2C%20demographic%0Aparity%2C%20equalized%20odds%2C%20and%20equal%20opportunity%29%20with%20minimal%20impact%20on%0Apredictive%20accuracy.%20We%20release%20our%20code%20to%20facilitate%20reproducibility%0A%28https%3A//github.com/patrikken/Fair-TabICL%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09503v3&entry.124074799=Read"},
{"title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation", "author": "Kien T. Pham and Yingqing He and Yazhou Xing and Qifeng Chen and Long Chen", "abstract": "  Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.\n", "link": "http://arxiv.org/abs/2508.00782v1", "date": "2025-08-01", "relevancy": 2.2964, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5898}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5746}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpA2V%3A%20Harnessing%20Spatial%20Auditory%20Cues%20for%20Audio-driven%20Spatially-aware%0A%20%20Video%20Generation&body=Title%3A%20SpA2V%3A%20Harnessing%20Spatial%20Auditory%20Cues%20for%20Audio-driven%20Spatially-aware%0A%20%20Video%20Generation%0AAuthor%3A%20Kien%20T.%20Pham%20and%20Yingqing%20He%20and%20Yazhou%20Xing%20and%20Qifeng%20Chen%20and%20Long%20Chen%0AAbstract%3A%20%20%20Audio-driven%20video%20generation%20aims%20to%20synthesize%20realistic%20videos%20that%20align%0Awith%20input%20audio%20recordings%2C%20akin%20to%20the%20human%20ability%20to%20visualize%20scenes%20from%0Aauditory%20input.%20However%2C%20existing%20approaches%20predominantly%20focus%20on%20exploring%0Asemantic%20information%2C%20such%20as%20the%20classes%20of%20sounding%20sources%20present%20in%20the%0Aaudio%2C%20limiting%20their%20ability%20to%20generate%20videos%20with%20accurate%20content%20and%0Aspatial%20composition.%20In%20contrast%2C%20we%20humans%20can%20not%20only%20naturally%20identify%20the%0Asemantic%20categories%20of%20sounding%20sources%20but%20also%20determine%20their%20deeply%20encoded%0Aspatial%20attributes%2C%20including%20locations%20and%20movement%20directions.%20This%20useful%0Ainformation%20can%20be%20elucidated%20by%20considering%20specific%20spatial%20indicators%0Aderived%20from%20the%20inherent%20physical%20properties%20of%20sound%2C%20such%20as%20loudness%20or%0Afrequency.%20As%20prior%20methods%20largely%20ignore%20this%20factor%2C%20we%20present%20SpA2V%2C%20the%0Afirst%20framework%20explicitly%20exploits%20these%20spatial%20auditory%20cues%20from%20audios%20to%0Agenerate%20videos%20with%20high%20semantic%20and%20spatial%20correspondence.%20SpA2V%20decomposes%0Athe%20generation%20process%20into%20two%20stages%3A%201%29%20Audio-guided%20Video%20Planning%3A%20We%0Ameticulously%20adapt%20a%20state-of-the-art%20MLLM%20for%20a%20novel%20task%20of%20harnessing%0Aspatial%20and%20semantic%20cues%20from%20input%20audio%20to%20construct%20Video%20Scene%20Layouts%0A%28VSLs%29.%20This%20serves%20as%20an%20intermediate%20representation%20to%20bridge%20the%20gap%20between%0Athe%20audio%20and%20video%20modalities.%202%29%20Layout-grounded%20Video%20Generation%3A%20We%20develop%0Aan%20efficient%20and%20effective%20approach%20to%20seamlessly%20integrate%20VSLs%20as%20conditional%0Aguidance%20into%20pre-trained%20diffusion%20models%2C%20enabling%20VSL-grounded%20video%0Ageneration%20in%20a%20training-free%20manner.%20Extensive%20experiments%20demonstrate%20that%0ASpA2V%20excels%20in%20generating%20realistic%20videos%20with%20semantic%20and%20spatial%20alignment%0Ato%20the%20input%20audios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpA2V%253A%2520Harnessing%2520Spatial%2520Auditory%2520Cues%2520for%2520Audio-driven%2520Spatially-aware%250A%2520%2520Video%2520Generation%26entry.906535625%3DKien%2520T.%2520Pham%2520and%2520Yingqing%2520He%2520and%2520Yazhou%2520Xing%2520and%2520Qifeng%2520Chen%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Audio-driven%2520video%2520generation%2520aims%2520to%2520synthesize%2520realistic%2520videos%2520that%2520align%250Awith%2520input%2520audio%2520recordings%252C%2520akin%2520to%2520the%2520human%2520ability%2520to%2520visualize%2520scenes%2520from%250Aauditory%2520input.%2520However%252C%2520existing%2520approaches%2520predominantly%2520focus%2520on%2520exploring%250Asemantic%2520information%252C%2520such%2520as%2520the%2520classes%2520of%2520sounding%2520sources%2520present%2520in%2520the%250Aaudio%252C%2520limiting%2520their%2520ability%2520to%2520generate%2520videos%2520with%2520accurate%2520content%2520and%250Aspatial%2520composition.%2520In%2520contrast%252C%2520we%2520humans%2520can%2520not%2520only%2520naturally%2520identify%2520the%250Asemantic%2520categories%2520of%2520sounding%2520sources%2520but%2520also%2520determine%2520their%2520deeply%2520encoded%250Aspatial%2520attributes%252C%2520including%2520locations%2520and%2520movement%2520directions.%2520This%2520useful%250Ainformation%2520can%2520be%2520elucidated%2520by%2520considering%2520specific%2520spatial%2520indicators%250Aderived%2520from%2520the%2520inherent%2520physical%2520properties%2520of%2520sound%252C%2520such%2520as%2520loudness%2520or%250Afrequency.%2520As%2520prior%2520methods%2520largely%2520ignore%2520this%2520factor%252C%2520we%2520present%2520SpA2V%252C%2520the%250Afirst%2520framework%2520explicitly%2520exploits%2520these%2520spatial%2520auditory%2520cues%2520from%2520audios%2520to%250Agenerate%2520videos%2520with%2520high%2520semantic%2520and%2520spatial%2520correspondence.%2520SpA2V%2520decomposes%250Athe%2520generation%2520process%2520into%2520two%2520stages%253A%25201%2529%2520Audio-guided%2520Video%2520Planning%253A%2520We%250Ameticulously%2520adapt%2520a%2520state-of-the-art%2520MLLM%2520for%2520a%2520novel%2520task%2520of%2520harnessing%250Aspatial%2520and%2520semantic%2520cues%2520from%2520input%2520audio%2520to%2520construct%2520Video%2520Scene%2520Layouts%250A%2528VSLs%2529.%2520This%2520serves%2520as%2520an%2520intermediate%2520representation%2520to%2520bridge%2520the%2520gap%2520between%250Athe%2520audio%2520and%2520video%2520modalities.%25202%2529%2520Layout-grounded%2520Video%2520Generation%253A%2520We%2520develop%250Aan%2520efficient%2520and%2520effective%2520approach%2520to%2520seamlessly%2520integrate%2520VSLs%2520as%2520conditional%250Aguidance%2520into%2520pre-trained%2520diffusion%2520models%252C%2520enabling%2520VSL-grounded%2520video%250Ageneration%2520in%2520a%2520training-free%2520manner.%2520Extensive%2520experiments%2520demonstrate%2520that%250ASpA2V%2520excels%2520in%2520generating%2520realistic%2520videos%2520with%2520semantic%2520and%2520spatial%2520alignment%250Ato%2520the%2520input%2520audios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpA2V%3A%20Harnessing%20Spatial%20Auditory%20Cues%20for%20Audio-driven%20Spatially-aware%0A%20%20Video%20Generation&entry.906535625=Kien%20T.%20Pham%20and%20Yingqing%20He%20and%20Yazhou%20Xing%20and%20Qifeng%20Chen%20and%20Long%20Chen&entry.1292438233=%20%20Audio-driven%20video%20generation%20aims%20to%20synthesize%20realistic%20videos%20that%20align%0Awith%20input%20audio%20recordings%2C%20akin%20to%20the%20human%20ability%20to%20visualize%20scenes%20from%0Aauditory%20input.%20However%2C%20existing%20approaches%20predominantly%20focus%20on%20exploring%0Asemantic%20information%2C%20such%20as%20the%20classes%20of%20sounding%20sources%20present%20in%20the%0Aaudio%2C%20limiting%20their%20ability%20to%20generate%20videos%20with%20accurate%20content%20and%0Aspatial%20composition.%20In%20contrast%2C%20we%20humans%20can%20not%20only%20naturally%20identify%20the%0Asemantic%20categories%20of%20sounding%20sources%20but%20also%20determine%20their%20deeply%20encoded%0Aspatial%20attributes%2C%20including%20locations%20and%20movement%20directions.%20This%20useful%0Ainformation%20can%20be%20elucidated%20by%20considering%20specific%20spatial%20indicators%0Aderived%20from%20the%20inherent%20physical%20properties%20of%20sound%2C%20such%20as%20loudness%20or%0Afrequency.%20As%20prior%20methods%20largely%20ignore%20this%20factor%2C%20we%20present%20SpA2V%2C%20the%0Afirst%20framework%20explicitly%20exploits%20these%20spatial%20auditory%20cues%20from%20audios%20to%0Agenerate%20videos%20with%20high%20semantic%20and%20spatial%20correspondence.%20SpA2V%20decomposes%0Athe%20generation%20process%20into%20two%20stages%3A%201%29%20Audio-guided%20Video%20Planning%3A%20We%0Ameticulously%20adapt%20a%20state-of-the-art%20MLLM%20for%20a%20novel%20task%20of%20harnessing%0Aspatial%20and%20semantic%20cues%20from%20input%20audio%20to%20construct%20Video%20Scene%20Layouts%0A%28VSLs%29.%20This%20serves%20as%20an%20intermediate%20representation%20to%20bridge%20the%20gap%20between%0Athe%20audio%20and%20video%20modalities.%202%29%20Layout-grounded%20Video%20Generation%3A%20We%20develop%0Aan%20efficient%20and%20effective%20approach%20to%20seamlessly%20integrate%20VSLs%20as%20conditional%0Aguidance%20into%20pre-trained%20diffusion%20models%2C%20enabling%20VSL-grounded%20video%0Ageneration%20in%20a%20training-free%20manner.%20Extensive%20experiments%20demonstrate%20that%0ASpA2V%20excels%20in%20generating%20realistic%20videos%20with%20semantic%20and%20spatial%20alignment%0Ato%20the%20input%20audios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00782v1&entry.124074799=Read"},
{"title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for\n  Video-Synchronized Audio, Speech, and Song Generation", "author": "Le Wang and Jun Wang and Feng Deng and Chen Zhang and Kun Gai and Di Zhang", "abstract": "  We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.\n", "link": "http://arxiv.org/abs/2508.00733v1", "date": "2025-08-01", "relevancy": 2.2833, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5796}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5733}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioGen-Omni%3A%20A%20Unified%20Multimodal%20Diffusion%20Transformer%20for%0A%20%20Video-Synchronized%20Audio%2C%20Speech%2C%20and%20Song%20Generation&body=Title%3A%20AudioGen-Omni%3A%20A%20Unified%20Multimodal%20Diffusion%20Transformer%20for%0A%20%20Video-Synchronized%20Audio%2C%20Speech%2C%20and%20Song%20Generation%0AAuthor%3A%20Le%20Wang%20and%20Jun%20Wang%20and%20Feng%20Deng%20and%20Chen%20Zhang%20and%20Kun%20Gai%20and%20Di%20Zhang%0AAbstract%3A%20%20%20We%20present%20AudioGen-Omni%20-%20a%20unified%20approach%20based%20on%20multimodal%20diffusion%0Atransformers%20%28MMDit%29%2C%20capable%20of%20generating%20high-fidelity%20audio%2C%20speech%2C%20and%0Asongs%20coherently%20synchronized%20with%20the%20input%20video.%20AudioGen-Omni%20introduces%20a%0Anovel%20joint%20training%20paradigm%20that%20seamlessly%20integrates%20large-scale%0Avideo-text-audio%20corpora%2C%20enabling%20a%20model%20capable%20of%20generating%20semantically%0Arich%2C%20acoustically%20diverse%20audio%20conditioned%20on%20multimodal%20inputs%20and%20adaptable%0Ato%20a%20wide%20range%20of%20audio%20generation%20tasks.%20AudioGen-Omni%20employs%20a%20unified%0Alyrics-transcription%20encoder%20that%20encodes%20graphemes%20and%20phonemes%20from%20both%20sung%0Aand%20spoken%20inputs%20into%20dense%20frame-level%20representations.%20Dense%20frame-level%0Arepresentations%20are%20fused%20using%20an%20AdaLN-based%20joint%20attention%20mechanism%0Aenhanced%20with%20phase-aligned%20anisotropic%20positional%20infusion%20%28PAAPI%29%2C%20wherein%0ARoPE%20is%20selectively%20applied%20to%20temporally%20structured%20modalities%20to%20ensure%0Aprecise%20and%20robust%20cross-modal%20alignment.%20By%20unfreezing%20all%20modalities%20and%0Amasking%20missing%20inputs%2C%20AudioGen-Omni%20mitigates%20the%20semantic%20constraints%20of%0Atext-frozen%20paradigms%2C%20enabling%20effective%20cross-modal%20conditioning.%20This%20joint%0Atraining%20approach%20enhances%20audio%20quality%2C%20semantic%20alignment%2C%20and%20lip-sync%0Aaccuracy%2C%20while%20also%20achieving%20state-of-the-art%20results%20on%0AText-to-Audio/Speech/Song%20tasks.%20With%20an%20inference%20time%20of%201.91%20seconds%20for%208%0Aseconds%20of%20audio%2C%20it%20offers%20substantial%20improvements%20in%20both%20efficiency%20and%0Agenerality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioGen-Omni%253A%2520A%2520Unified%2520Multimodal%2520Diffusion%2520Transformer%2520for%250A%2520%2520Video-Synchronized%2520Audio%252C%2520Speech%252C%2520and%2520Song%2520Generation%26entry.906535625%3DLe%2520Wang%2520and%2520Jun%2520Wang%2520and%2520Feng%2520Deng%2520and%2520Chen%2520Zhang%2520and%2520Kun%2520Gai%2520and%2520Di%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520AudioGen-Omni%2520-%2520a%2520unified%2520approach%2520based%2520on%2520multimodal%2520diffusion%250Atransformers%2520%2528MMDit%2529%252C%2520capable%2520of%2520generating%2520high-fidelity%2520audio%252C%2520speech%252C%2520and%250Asongs%2520coherently%2520synchronized%2520with%2520the%2520input%2520video.%2520AudioGen-Omni%2520introduces%2520a%250Anovel%2520joint%2520training%2520paradigm%2520that%2520seamlessly%2520integrates%2520large-scale%250Avideo-text-audio%2520corpora%252C%2520enabling%2520a%2520model%2520capable%2520of%2520generating%2520semantically%250Arich%252C%2520acoustically%2520diverse%2520audio%2520conditioned%2520on%2520multimodal%2520inputs%2520and%2520adaptable%250Ato%2520a%2520wide%2520range%2520of%2520audio%2520generation%2520tasks.%2520AudioGen-Omni%2520employs%2520a%2520unified%250Alyrics-transcription%2520encoder%2520that%2520encodes%2520graphemes%2520and%2520phonemes%2520from%2520both%2520sung%250Aand%2520spoken%2520inputs%2520into%2520dense%2520frame-level%2520representations.%2520Dense%2520frame-level%250Arepresentations%2520are%2520fused%2520using%2520an%2520AdaLN-based%2520joint%2520attention%2520mechanism%250Aenhanced%2520with%2520phase-aligned%2520anisotropic%2520positional%2520infusion%2520%2528PAAPI%2529%252C%2520wherein%250ARoPE%2520is%2520selectively%2520applied%2520to%2520temporally%2520structured%2520modalities%2520to%2520ensure%250Aprecise%2520and%2520robust%2520cross-modal%2520alignment.%2520By%2520unfreezing%2520all%2520modalities%2520and%250Amasking%2520missing%2520inputs%252C%2520AudioGen-Omni%2520mitigates%2520the%2520semantic%2520constraints%2520of%250Atext-frozen%2520paradigms%252C%2520enabling%2520effective%2520cross-modal%2520conditioning.%2520This%2520joint%250Atraining%2520approach%2520enhances%2520audio%2520quality%252C%2520semantic%2520alignment%252C%2520and%2520lip-sync%250Aaccuracy%252C%2520while%2520also%2520achieving%2520state-of-the-art%2520results%2520on%250AText-to-Audio/Speech/Song%2520tasks.%2520With%2520an%2520inference%2520time%2520of%25201.91%2520seconds%2520for%25208%250Aseconds%2520of%2520audio%252C%2520it%2520offers%2520substantial%2520improvements%2520in%2520both%2520efficiency%2520and%250Agenerality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioGen-Omni%3A%20A%20Unified%20Multimodal%20Diffusion%20Transformer%20for%0A%20%20Video-Synchronized%20Audio%2C%20Speech%2C%20and%20Song%20Generation&entry.906535625=Le%20Wang%20and%20Jun%20Wang%20and%20Feng%20Deng%20and%20Chen%20Zhang%20and%20Kun%20Gai%20and%20Di%20Zhang&entry.1292438233=%20%20We%20present%20AudioGen-Omni%20-%20a%20unified%20approach%20based%20on%20multimodal%20diffusion%0Atransformers%20%28MMDit%29%2C%20capable%20of%20generating%20high-fidelity%20audio%2C%20speech%2C%20and%0Asongs%20coherently%20synchronized%20with%20the%20input%20video.%20AudioGen-Omni%20introduces%20a%0Anovel%20joint%20training%20paradigm%20that%20seamlessly%20integrates%20large-scale%0Avideo-text-audio%20corpora%2C%20enabling%20a%20model%20capable%20of%20generating%20semantically%0Arich%2C%20acoustically%20diverse%20audio%20conditioned%20on%20multimodal%20inputs%20and%20adaptable%0Ato%20a%20wide%20range%20of%20audio%20generation%20tasks.%20AudioGen-Omni%20employs%20a%20unified%0Alyrics-transcription%20encoder%20that%20encodes%20graphemes%20and%20phonemes%20from%20both%20sung%0Aand%20spoken%20inputs%20into%20dense%20frame-level%20representations.%20Dense%20frame-level%0Arepresentations%20are%20fused%20using%20an%20AdaLN-based%20joint%20attention%20mechanism%0Aenhanced%20with%20phase-aligned%20anisotropic%20positional%20infusion%20%28PAAPI%29%2C%20wherein%0ARoPE%20is%20selectively%20applied%20to%20temporally%20structured%20modalities%20to%20ensure%0Aprecise%20and%20robust%20cross-modal%20alignment.%20By%20unfreezing%20all%20modalities%20and%0Amasking%20missing%20inputs%2C%20AudioGen-Omni%20mitigates%20the%20semantic%20constraints%20of%0Atext-frozen%20paradigms%2C%20enabling%20effective%20cross-modal%20conditioning.%20This%20joint%0Atraining%20approach%20enhances%20audio%20quality%2C%20semantic%20alignment%2C%20and%20lip-sync%0Aaccuracy%2C%20while%20also%20achieving%20state-of-the-art%20results%20on%0AText-to-Audio/Speech/Song%20tasks.%20With%20an%20inference%20time%20of%201.91%20seconds%20for%208%0Aseconds%20of%20audio%2C%20it%20offers%20substantial%20improvements%20in%20both%20efficiency%20and%0Agenerality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00733v1&entry.124074799=Read"},
{"title": "Context-based Motion Retrieval using Open Vocabulary Methods for\n  Autonomous Driving", "author": "Stefan Englmeier and Max A. B\u00fcttner and Katharina Winter and Fabian B. Flohr", "abstract": "  Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.\n", "link": "http://arxiv.org/abs/2508.00589v1", "date": "2025-08-01", "relevancy": 2.2832, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5958}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5781}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-based%20Motion%20Retrieval%20using%20Open%20Vocabulary%20Methods%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20Context-based%20Motion%20Retrieval%20using%20Open%20Vocabulary%20Methods%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Stefan%20Englmeier%20and%20Max%20A.%20B%C3%BCttner%20and%20Katharina%20Winter%20and%20Fabian%20B.%20Flohr%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20must%20operate%20reliably%20in%20safety-critical%0Ascenarios%2C%20particularly%20those%20involving%20unusual%20or%20complex%20behavior%20by%0AVulnerable%20Road%20Users%20%28VRUs%29.%20Identifying%20these%20edge%20cases%20in%20driving%20datasets%0Ais%20essential%20for%20robust%20evaluation%20and%20generalization%2C%20but%20retrieving%20such%20rare%0Ahuman%20behavior%20scenarios%20within%20the%20long%20tail%20of%20large-scale%20datasets%20is%0Achallenging.%20To%20support%20targeted%20evaluation%20of%20autonomous%20driving%20systems%20in%0Adiverse%2C%20human-centered%20scenarios%2C%20we%20propose%20a%20novel%20context-aware%20motion%0Aretrieval%20framework.%20Our%20method%20combines%20Skinned%20Multi-Person%20Linear%0A%28SMPL%29-based%20motion%20sequences%20and%20corresponding%20video%20frames%20before%20encoding%0Athem%20into%20a%20shared%20multimodal%20embedding%20space%20aligned%20with%20natural%20language.%0AOur%20approach%20enables%20the%20scalable%20retrieval%20of%20human%20behavior%20and%20their%20context%0Athrough%20text%20queries.%20This%20work%20also%20introduces%20our%20dataset%20WayMoCo%2C%20an%0Aextension%20of%20the%20Waymo%20Open%20Dataset.%20It%20contains%20automatically%20labeled%20motion%0Aand%20scene%20context%20descriptions%20derived%20from%20generated%20pseudo-ground-truth%20SMPL%0Asequences%20and%20corresponding%20image%20data.%20Our%20approach%20outperforms%0Astate-of-the-art%20models%20by%20up%20to%2027.5%25%20accuracy%20in%20motion-context%20retrieval%2C%0Awhen%20evaluated%20on%20the%20WayMoCo%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-based%2520Motion%2520Retrieval%2520using%2520Open%2520Vocabulary%2520Methods%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DStefan%2520Englmeier%2520and%2520Max%2520A.%2520B%25C3%25BCttner%2520and%2520Katharina%2520Winter%2520and%2520Fabian%2520B.%2520Flohr%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520must%2520operate%2520reliably%2520in%2520safety-critical%250Ascenarios%252C%2520particularly%2520those%2520involving%2520unusual%2520or%2520complex%2520behavior%2520by%250AVulnerable%2520Road%2520Users%2520%2528VRUs%2529.%2520Identifying%2520these%2520edge%2520cases%2520in%2520driving%2520datasets%250Ais%2520essential%2520for%2520robust%2520evaluation%2520and%2520generalization%252C%2520but%2520retrieving%2520such%2520rare%250Ahuman%2520behavior%2520scenarios%2520within%2520the%2520long%2520tail%2520of%2520large-scale%2520datasets%2520is%250Achallenging.%2520To%2520support%2520targeted%2520evaluation%2520of%2520autonomous%2520driving%2520systems%2520in%250Adiverse%252C%2520human-centered%2520scenarios%252C%2520we%2520propose%2520a%2520novel%2520context-aware%2520motion%250Aretrieval%2520framework.%2520Our%2520method%2520combines%2520Skinned%2520Multi-Person%2520Linear%250A%2528SMPL%2529-based%2520motion%2520sequences%2520and%2520corresponding%2520video%2520frames%2520before%2520encoding%250Athem%2520into%2520a%2520shared%2520multimodal%2520embedding%2520space%2520aligned%2520with%2520natural%2520language.%250AOur%2520approach%2520enables%2520the%2520scalable%2520retrieval%2520of%2520human%2520behavior%2520and%2520their%2520context%250Athrough%2520text%2520queries.%2520This%2520work%2520also%2520introduces%2520our%2520dataset%2520WayMoCo%252C%2520an%250Aextension%2520of%2520the%2520Waymo%2520Open%2520Dataset.%2520It%2520contains%2520automatically%2520labeled%2520motion%250Aand%2520scene%2520context%2520descriptions%2520derived%2520from%2520generated%2520pseudo-ground-truth%2520SMPL%250Asequences%2520and%2520corresponding%2520image%2520data.%2520Our%2520approach%2520outperforms%250Astate-of-the-art%2520models%2520by%2520up%2520to%252027.5%2525%2520accuracy%2520in%2520motion-context%2520retrieval%252C%250Awhen%2520evaluated%2520on%2520the%2520WayMoCo%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-based%20Motion%20Retrieval%20using%20Open%20Vocabulary%20Methods%20for%0A%20%20Autonomous%20Driving&entry.906535625=Stefan%20Englmeier%20and%20Max%20A.%20B%C3%BCttner%20and%20Katharina%20Winter%20and%20Fabian%20B.%20Flohr&entry.1292438233=%20%20Autonomous%20driving%20systems%20must%20operate%20reliably%20in%20safety-critical%0Ascenarios%2C%20particularly%20those%20involving%20unusual%20or%20complex%20behavior%20by%0AVulnerable%20Road%20Users%20%28VRUs%29.%20Identifying%20these%20edge%20cases%20in%20driving%20datasets%0Ais%20essential%20for%20robust%20evaluation%20and%20generalization%2C%20but%20retrieving%20such%20rare%0Ahuman%20behavior%20scenarios%20within%20the%20long%20tail%20of%20large-scale%20datasets%20is%0Achallenging.%20To%20support%20targeted%20evaluation%20of%20autonomous%20driving%20systems%20in%0Adiverse%2C%20human-centered%20scenarios%2C%20we%20propose%20a%20novel%20context-aware%20motion%0Aretrieval%20framework.%20Our%20method%20combines%20Skinned%20Multi-Person%20Linear%0A%28SMPL%29-based%20motion%20sequences%20and%20corresponding%20video%20frames%20before%20encoding%0Athem%20into%20a%20shared%20multimodal%20embedding%20space%20aligned%20with%20natural%20language.%0AOur%20approach%20enables%20the%20scalable%20retrieval%20of%20human%20behavior%20and%20their%20context%0Athrough%20text%20queries.%20This%20work%20also%20introduces%20our%20dataset%20WayMoCo%2C%20an%0Aextension%20of%20the%20Waymo%20Open%20Dataset.%20It%20contains%20automatically%20labeled%20motion%0Aand%20scene%20context%20descriptions%20derived%20from%20generated%20pseudo-ground-truth%20SMPL%0Asequences%20and%20corresponding%20image%20data.%20Our%20approach%20outperforms%0Astate-of-the-art%20models%20by%20up%20to%2027.5%25%20accuracy%20in%20motion-context%20retrieval%2C%0Awhen%20evaluated%20on%20the%20WayMoCo%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00589v1&entry.124074799=Read"},
{"title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel\n  environments", "author": "Federica Di Lauro and Domenico G. Sorrenti and Miguel Angel Sotelo", "abstract": "  Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.\n", "link": "http://arxiv.org/abs/2507.21553v3", "date": "2025-08-01", "relevancy": 2.283, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5785}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments&body=Title%3A%20Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments%0AAuthor%3A%20Federica%20Di%20Lauro%20and%20Domenico%20G.%20Sorrenti%20and%20Miguel%20Angel%20Sotelo%0AAbstract%3A%20%20%20Multi-robot%20SLAM%20aims%20at%20localizing%20and%20building%20a%20map%20with%20multiple%20robots%2C%0Ainteracting%20with%20each%20other.%20In%20the%20work%20described%20in%20this%20article%2C%20we%20analyze%0Athe%20pipeline%20of%20a%20decentralized%20LiDAR%20SLAM%20system%20to%20study%20the%20current%0Alimitations%20of%20the%20state%20of%20the%20art%2C%20and%20we%20discover%20a%20significant%20source%20of%0Afailures%2C%20i.e.%2C%20that%20the%20loop%20detection%20is%20the%20source%20of%20too%20many%20false%0Apositives.%20We%20therefore%20develop%20and%20propose%20a%20new%20heuristic%20to%20overcome%20these%0Alimitations.%20The%20environment%20taken%20as%20reference%20in%20this%20work%20is%20the%20highly%0Achallenging%20case%20of%20underground%20tunnels.%20We%20also%20highlight%20potential%20new%0Aresearch%20areas%20still%20under-explored.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-robot%2520LiDAR%2520SLAM%253A%2520a%2520practical%2520case%2520study%2520in%2520underground%2520tunnel%250A%2520%2520environments%26entry.906535625%3DFederica%2520Di%2520Lauro%2520and%2520Domenico%2520G.%2520Sorrenti%2520and%2520Miguel%2520Angel%2520Sotelo%26entry.1292438233%3D%2520%2520Multi-robot%2520SLAM%2520aims%2520at%2520localizing%2520and%2520building%2520a%2520map%2520with%2520multiple%2520robots%252C%250Ainteracting%2520with%2520each%2520other.%2520In%2520the%2520work%2520described%2520in%2520this%2520article%252C%2520we%2520analyze%250Athe%2520pipeline%2520of%2520a%2520decentralized%2520LiDAR%2520SLAM%2520system%2520to%2520study%2520the%2520current%250Alimitations%2520of%2520the%2520state%2520of%2520the%2520art%252C%2520and%2520we%2520discover%2520a%2520significant%2520source%2520of%250Afailures%252C%2520i.e.%252C%2520that%2520the%2520loop%2520detection%2520is%2520the%2520source%2520of%2520too%2520many%2520false%250Apositives.%2520We%2520therefore%2520develop%2520and%2520propose%2520a%2520new%2520heuristic%2520to%2520overcome%2520these%250Alimitations.%2520The%2520environment%2520taken%2520as%2520reference%2520in%2520this%2520work%2520is%2520the%2520highly%250Achallenging%2520case%2520of%2520underground%2520tunnels.%2520We%2520also%2520highlight%2520potential%2520new%250Aresearch%2520areas%2520still%2520under-explored.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-robot%20LiDAR%20SLAM%3A%20a%20practical%20case%20study%20in%20underground%20tunnel%0A%20%20environments&entry.906535625=Federica%20Di%20Lauro%20and%20Domenico%20G.%20Sorrenti%20and%20Miguel%20Angel%20Sotelo&entry.1292438233=%20%20Multi-robot%20SLAM%20aims%20at%20localizing%20and%20building%20a%20map%20with%20multiple%20robots%2C%0Ainteracting%20with%20each%20other.%20In%20the%20work%20described%20in%20this%20article%2C%20we%20analyze%0Athe%20pipeline%20of%20a%20decentralized%20LiDAR%20SLAM%20system%20to%20study%20the%20current%0Alimitations%20of%20the%20state%20of%20the%20art%2C%20and%20we%20discover%20a%20significant%20source%20of%0Afailures%2C%20i.e.%2C%20that%20the%20loop%20detection%20is%20the%20source%20of%20too%20many%20false%0Apositives.%20We%20therefore%20develop%20and%20propose%20a%20new%20heuristic%20to%20overcome%20these%0Alimitations.%20The%20environment%20taken%20as%20reference%20in%20this%20work%20is%20the%20highly%0Achallenging%20case%20of%20underground%20tunnels.%20We%20also%20highlight%20potential%20new%0Aresearch%20areas%20still%20under-explored.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21553v3&entry.124074799=Read"},
{"title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST\n  Point Cloud City Datasets for 3D Deep Learning", "author": "Alexander Nikitas Dimopoulos and Joseph Grasso", "abstract": "  This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.\n", "link": "http://arxiv.org/abs/2508.00822v1", "date": "2025-08-01", "relevancy": 2.2707, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Dataset%20Semantic%20Segmentation%20Performance%20Analysis%3A%20Unifying%20NIST%0A%20%20Point%20Cloud%20City%20Datasets%20for%203D%20Deep%20Learning&body=Title%3A%20Cross-Dataset%20Semantic%20Segmentation%20Performance%20Analysis%3A%20Unifying%20NIST%0A%20%20Point%20Cloud%20City%20Datasets%20for%203D%20Deep%20Learning%0AAuthor%3A%20Alexander%20Nikitas%20Dimopoulos%20and%20Joseph%20Grasso%0AAbstract%3A%20%20%20This%20study%20analyzes%20semantic%20segmentation%20performance%20across%20heterogeneously%0Alabeled%20point-cloud%20datasets%20relevant%20to%20public%20safety%20applications%2C%20including%0Apre-incident%20planning%20systems%20derived%20from%20lidar%20scans.%20Using%20NIST%27s%20Point%0ACloud%20City%20dataset%20%28Enfield%20and%20Memphis%20collections%29%2C%20we%20investigate%20challenges%0Ain%20unifying%20differently%20labeled%203D%20data.%20Our%20methodology%20employs%20a%20graded%0Aschema%20with%20the%20KPConv%20architecture%2C%20evaluating%20performance%20through%20IoU%20metrics%0Aon%20safety-relevant%20features.%20Results%20indicate%20performance%20variability%3A%0Ageometrically%20large%20objects%20%28e.g.%20stairs%2C%20windows%29%20achieve%20higher%20segmentation%0Aperformance%2C%20suggesting%20potential%20for%20navigational%20context%2C%20while%20smaller%0Asafety-critical%20features%20exhibit%20lower%20recognition%20rates.%20Performance%20is%0Aimpacted%20by%20class%20imbalance%20and%20the%20limited%20geometric%20distinction%20of%20smaller%0Aobjects%20in%20typical%20lidar%20scans%2C%20indicating%20limitations%20in%20detecting%20certain%0Asafety-relevant%20features%20using%20current%20point-cloud%20methods.%20Key%20identified%0Achallenges%20include%20insufficient%20labeled%20data%2C%20difficulties%20in%20unifying%20class%0Alabels%20across%20datasets%2C%20and%20the%20need%20for%20standardization.%20Potential%20directions%0Ainclude%20automated%20labeling%20and%20multi-dataset%20learning%20strategies.%20We%20conclude%0Athat%20reliable%20point-cloud%20semantic%20segmentation%20for%20public%20safety%20necessitates%0Astandardized%20annotation%20protocols%20and%20improved%20labeling%20techniques%20to%20address%0Adata%20heterogeneity%20and%20the%20detection%20of%20small%2C%20safety-critical%20elements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Dataset%2520Semantic%2520Segmentation%2520Performance%2520Analysis%253A%2520Unifying%2520NIST%250A%2520%2520Point%2520Cloud%2520City%2520Datasets%2520for%25203D%2520Deep%2520Learning%26entry.906535625%3DAlexander%2520Nikitas%2520Dimopoulos%2520and%2520Joseph%2520Grasso%26entry.1292438233%3D%2520%2520This%2520study%2520analyzes%2520semantic%2520segmentation%2520performance%2520across%2520heterogeneously%250Alabeled%2520point-cloud%2520datasets%2520relevant%2520to%2520public%2520safety%2520applications%252C%2520including%250Apre-incident%2520planning%2520systems%2520derived%2520from%2520lidar%2520scans.%2520Using%2520NIST%2527s%2520Point%250ACloud%2520City%2520dataset%2520%2528Enfield%2520and%2520Memphis%2520collections%2529%252C%2520we%2520investigate%2520challenges%250Ain%2520unifying%2520differently%2520labeled%25203D%2520data.%2520Our%2520methodology%2520employs%2520a%2520graded%250Aschema%2520with%2520the%2520KPConv%2520architecture%252C%2520evaluating%2520performance%2520through%2520IoU%2520metrics%250Aon%2520safety-relevant%2520features.%2520Results%2520indicate%2520performance%2520variability%253A%250Ageometrically%2520large%2520objects%2520%2528e.g.%2520stairs%252C%2520windows%2529%2520achieve%2520higher%2520segmentation%250Aperformance%252C%2520suggesting%2520potential%2520for%2520navigational%2520context%252C%2520while%2520smaller%250Asafety-critical%2520features%2520exhibit%2520lower%2520recognition%2520rates.%2520Performance%2520is%250Aimpacted%2520by%2520class%2520imbalance%2520and%2520the%2520limited%2520geometric%2520distinction%2520of%2520smaller%250Aobjects%2520in%2520typical%2520lidar%2520scans%252C%2520indicating%2520limitations%2520in%2520detecting%2520certain%250Asafety-relevant%2520features%2520using%2520current%2520point-cloud%2520methods.%2520Key%2520identified%250Achallenges%2520include%2520insufficient%2520labeled%2520data%252C%2520difficulties%2520in%2520unifying%2520class%250Alabels%2520across%2520datasets%252C%2520and%2520the%2520need%2520for%2520standardization.%2520Potential%2520directions%250Ainclude%2520automated%2520labeling%2520and%2520multi-dataset%2520learning%2520strategies.%2520We%2520conclude%250Athat%2520reliable%2520point-cloud%2520semantic%2520segmentation%2520for%2520public%2520safety%2520necessitates%250Astandardized%2520annotation%2520protocols%2520and%2520improved%2520labeling%2520techniques%2520to%2520address%250Adata%2520heterogeneity%2520and%2520the%2520detection%2520of%2520small%252C%2520safety-critical%2520elements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Dataset%20Semantic%20Segmentation%20Performance%20Analysis%3A%20Unifying%20NIST%0A%20%20Point%20Cloud%20City%20Datasets%20for%203D%20Deep%20Learning&entry.906535625=Alexander%20Nikitas%20Dimopoulos%20and%20Joseph%20Grasso&entry.1292438233=%20%20This%20study%20analyzes%20semantic%20segmentation%20performance%20across%20heterogeneously%0Alabeled%20point-cloud%20datasets%20relevant%20to%20public%20safety%20applications%2C%20including%0Apre-incident%20planning%20systems%20derived%20from%20lidar%20scans.%20Using%20NIST%27s%20Point%0ACloud%20City%20dataset%20%28Enfield%20and%20Memphis%20collections%29%2C%20we%20investigate%20challenges%0Ain%20unifying%20differently%20labeled%203D%20data.%20Our%20methodology%20employs%20a%20graded%0Aschema%20with%20the%20KPConv%20architecture%2C%20evaluating%20performance%20through%20IoU%20metrics%0Aon%20safety-relevant%20features.%20Results%20indicate%20performance%20variability%3A%0Ageometrically%20large%20objects%20%28e.g.%20stairs%2C%20windows%29%20achieve%20higher%20segmentation%0Aperformance%2C%20suggesting%20potential%20for%20navigational%20context%2C%20while%20smaller%0Asafety-critical%20features%20exhibit%20lower%20recognition%20rates.%20Performance%20is%0Aimpacted%20by%20class%20imbalance%20and%20the%20limited%20geometric%20distinction%20of%20smaller%0Aobjects%20in%20typical%20lidar%20scans%2C%20indicating%20limitations%20in%20detecting%20certain%0Asafety-relevant%20features%20using%20current%20point-cloud%20methods.%20Key%20identified%0Achallenges%20include%20insufficient%20labeled%20data%2C%20difficulties%20in%20unifying%20class%0Alabels%20across%20datasets%2C%20and%20the%20need%20for%20standardization.%20Potential%20directions%0Ainclude%20automated%20labeling%20and%20multi-dataset%20learning%20strategies.%20We%20conclude%0Athat%20reliable%20point-cloud%20semantic%20segmentation%20for%20public%20safety%20necessitates%0Astandardized%20annotation%20protocols%20and%20improved%20labeling%20techniques%20to%20address%0Adata%20heterogeneity%20and%20the%20detection%20of%20small%2C%20safety-critical%20elements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00822v1&entry.124074799=Read"},
{"title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source\n  Synthetic$\\unicode{x2013}$Data SDK", "author": "Ivona Krchova and Mariana Vargas Vieyra and Mario Scriminaci and Andrey Sidorenko", "abstract": "  Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.\n", "link": "http://arxiv.org/abs/2508.00718v1", "date": "2025-08-01", "relevancy": 2.2623, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4561}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4561}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%0A%20%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK&body=Title%3A%20Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%0A%20%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK%0AAuthor%3A%20Ivona%20Krchova%20and%20Mariana%20Vargas%20Vieyra%20and%20Mario%20Scriminaci%20and%20Andrey%20Sidorenko%0AAbstract%3A%20%20%20Machine%20learning%20development%20critically%20depends%20on%20access%20to%20high-quality%0Adata.%20However%2C%20increasing%20restrictions%20due%20to%20privacy%2C%20proprietary%20interests%2C%0Aand%20ethical%20concerns%20have%20created%20significant%20barriers%20to%20data%20accessibility.%0ASynthetic%20data%20offers%20a%20viable%20solution%20by%20enabling%20safe%2C%20broad%20data%20usage%0Awithout%20compromising%20sensitive%20information.%20This%20paper%20presents%20the%20MOSTLY%20AI%0ASynthetic%20Data%20Software%20Development%20Kit%20%28SDK%29%2C%20an%20open-source%20toolkit%20designed%0Aspecifically%20for%20synthesizing%20high-quality%20tabular%20data.%20The%20SDK%20integrates%0Arobust%20features%20such%20as%20differential%20privacy%20guarantees%2C%20fairness-aware%20data%0Ageneration%2C%20and%20automated%20quality%20assurance%20into%20a%20flexible%20and%20accessible%0APython%20interface.%20Leveraging%20the%20TabularARGN%20autoregressive%20framework%2C%20the%20SDK%0Asupports%20diverse%20data%20types%20and%20complex%20multi-table%20and%20sequential%20datasets%2C%0Adelivering%20competitive%20performance%20with%20notable%20improvements%20in%20speed%20and%0Ausability.%20Currently%20deployed%20both%20as%20a%20cloud%20service%20and%20locally%20installable%0Asoftware%2C%20the%20SDK%20has%20seen%20rapid%20adoption%2C%20highlighting%20its%20practicality%20in%0Aaddressing%20real-world%20data%20bottlenecks%20and%20promoting%20widespread%20data%0Ademocratization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemocratizing%2520Tabular%2520Data%2520Access%2520with%2520an%2520Open%2524%255Cunicode%257Bx2013%257D%2524Source%250A%2520%2520Synthetic%2524%255Cunicode%257Bx2013%257D%2524Data%2520SDK%26entry.906535625%3DIvona%2520Krchova%2520and%2520Mariana%2520Vargas%2520Vieyra%2520and%2520Mario%2520Scriminaci%2520and%2520Andrey%2520Sidorenko%26entry.1292438233%3D%2520%2520Machine%2520learning%2520development%2520critically%2520depends%2520on%2520access%2520to%2520high-quality%250Adata.%2520However%252C%2520increasing%2520restrictions%2520due%2520to%2520privacy%252C%2520proprietary%2520interests%252C%250Aand%2520ethical%2520concerns%2520have%2520created%2520significant%2520barriers%2520to%2520data%2520accessibility.%250ASynthetic%2520data%2520offers%2520a%2520viable%2520solution%2520by%2520enabling%2520safe%252C%2520broad%2520data%2520usage%250Awithout%2520compromising%2520sensitive%2520information.%2520This%2520paper%2520presents%2520the%2520MOSTLY%2520AI%250ASynthetic%2520Data%2520Software%2520Development%2520Kit%2520%2528SDK%2529%252C%2520an%2520open-source%2520toolkit%2520designed%250Aspecifically%2520for%2520synthesizing%2520high-quality%2520tabular%2520data.%2520The%2520SDK%2520integrates%250Arobust%2520features%2520such%2520as%2520differential%2520privacy%2520guarantees%252C%2520fairness-aware%2520data%250Ageneration%252C%2520and%2520automated%2520quality%2520assurance%2520into%2520a%2520flexible%2520and%2520accessible%250APython%2520interface.%2520Leveraging%2520the%2520TabularARGN%2520autoregressive%2520framework%252C%2520the%2520SDK%250Asupports%2520diverse%2520data%2520types%2520and%2520complex%2520multi-table%2520and%2520sequential%2520datasets%252C%250Adelivering%2520competitive%2520performance%2520with%2520notable%2520improvements%2520in%2520speed%2520and%250Ausability.%2520Currently%2520deployed%2520both%2520as%2520a%2520cloud%2520service%2520and%2520locally%2520installable%250Asoftware%252C%2520the%2520SDK%2520has%2520seen%2520rapid%2520adoption%252C%2520highlighting%2520its%2520practicality%2520in%250Aaddressing%2520real-world%2520data%2520bottlenecks%2520and%2520promoting%2520widespread%2520data%250Ademocratization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%0A%20%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK&entry.906535625=Ivona%20Krchova%20and%20Mariana%20Vargas%20Vieyra%20and%20Mario%20Scriminaci%20and%20Andrey%20Sidorenko&entry.1292438233=%20%20Machine%20learning%20development%20critically%20depends%20on%20access%20to%20high-quality%0Adata.%20However%2C%20increasing%20restrictions%20due%20to%20privacy%2C%20proprietary%20interests%2C%0Aand%20ethical%20concerns%20have%20created%20significant%20barriers%20to%20data%20accessibility.%0ASynthetic%20data%20offers%20a%20viable%20solution%20by%20enabling%20safe%2C%20broad%20data%20usage%0Awithout%20compromising%20sensitive%20information.%20This%20paper%20presents%20the%20MOSTLY%20AI%0ASynthetic%20Data%20Software%20Development%20Kit%20%28SDK%29%2C%20an%20open-source%20toolkit%20designed%0Aspecifically%20for%20synthesizing%20high-quality%20tabular%20data.%20The%20SDK%20integrates%0Arobust%20features%20such%20as%20differential%20privacy%20guarantees%2C%20fairness-aware%20data%0Ageneration%2C%20and%20automated%20quality%20assurance%20into%20a%20flexible%20and%20accessible%0APython%20interface.%20Leveraging%20the%20TabularARGN%20autoregressive%20framework%2C%20the%20SDK%0Asupports%20diverse%20data%20types%20and%20complex%20multi-table%20and%20sequential%20datasets%2C%0Adelivering%20competitive%20performance%20with%20notable%20improvements%20in%20speed%20and%0Ausability.%20Currently%20deployed%20both%20as%20a%20cloud%20service%20and%20locally%20installable%0Asoftware%2C%20the%20SDK%20has%20seen%20rapid%20adoption%2C%20highlighting%20its%20practicality%20in%0Aaddressing%20real-world%20data%20bottlenecks%20and%20promoting%20widespread%20data%0Ademocratization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00718v1&entry.124074799=Read"},
{"title": "FalconGym: A Photorealistic Simulation Framework for Zero-Shot\n  Sim-to-Real Vision-Based Quadrotor Navigation", "author": "Yan Miao and Will Shen and Sayan Mitra", "abstract": "  We present a novel framework demonstrating zero-shot sim-to-real transfer of\nvisual control policies learned in a Neural Radiance Field (NeRF) environment\nfor quadrotors to fly through racing gates. Robust transfer from simulation to\nreal flight poses a major challenge, as standard simulators often lack\nsufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates.\n", "link": "http://arxiv.org/abs/2503.02198v2", "date": "2025-08-01", "relevancy": 2.2609, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5599}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FalconGym%3A%20A%20Photorealistic%20Simulation%20Framework%20for%20Zero-Shot%0A%20%20Sim-to-Real%20Vision-Based%20Quadrotor%20Navigation&body=Title%3A%20FalconGym%3A%20A%20Photorealistic%20Simulation%20Framework%20for%20Zero-Shot%0A%20%20Sim-to-Real%20Vision-Based%20Quadrotor%20Navigation%0AAuthor%3A%20Yan%20Miao%20and%20Will%20Shen%20and%20Sayan%20Mitra%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20demonstrating%20zero-shot%20sim-to-real%20transfer%20of%0Avisual%20control%20policies%20learned%20in%20a%20Neural%20Radiance%20Field%20%28NeRF%29%20environment%0Afor%20quadrotors%20to%20fly%20through%20racing%20gates.%20Robust%20transfer%20from%20simulation%20to%0Areal%20flight%20poses%20a%20major%20challenge%2C%20as%20standard%20simulators%20often%20lack%0Asufficient%20visual%20fidelity.%20To%20address%20this%2C%20we%20construct%20a%20photorealistic%0Asimulation%20environment%20of%20quadrotor%20racing%20tracks%2C%20called%20FalconGym%2C%20which%0Aprovides%20effectively%20unlimited%20synthetic%20images%20for%20training.%20Within%20FalconGym%2C%0Awe%20develop%20a%20pipelined%20approach%20for%20crossing%20gates%20that%20combines%20%28i%29%20a%20Neural%0APose%20Estimator%20%28NPE%29%20coupled%20with%20a%20Kalman%20filter%20to%20reliably%20infer%20quadrotor%0Aposes%20from%20single-frame%20RGB%20images%20and%20IMU%20data%2C%20and%20%28ii%29%20a%0Aself-attention-based%20multi-modal%20controller%20that%20adaptively%20integrates%20visual%0Afeatures%20and%20pose%20estimation.%20This%20multi-modal%20design%20compensates%20for%0Aperception%20noise%20and%20intermittent%20gate%20visibility.%20We%20train%20this%20controller%0Apurely%20in%20FalconGym%20with%20imitation%20learning%20and%20deploy%20the%20resulting%20policy%20to%0Areal%20hardware%20with%20no%20additional%20fine-tuning.%20Simulation%20experiments%20on%20three%0Adistinct%20tracks%20%28circle%2C%20U-turn%20and%20figure-8%29%20demonstrate%20that%20our%20controller%0Aoutperforms%20a%20vision-only%20state-of-the-art%20baseline%20in%20both%20success%20rate%20and%0Agate-crossing%20accuracy.%20In%2030%20live%20hardware%20flights%20spanning%20three%20tracks%20and%0A120%20gates%2C%20our%20controller%20achieves%20a%2095.8%25%20success%20rate%20and%20an%20average%20error%20of%0Ajust%2010%20cm%20when%20flying%20through%2038%20cm-radius%20gates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFalconGym%253A%2520A%2520Photorealistic%2520Simulation%2520Framework%2520for%2520Zero-Shot%250A%2520%2520Sim-to-Real%2520Vision-Based%2520Quadrotor%2520Navigation%26entry.906535625%3DYan%2520Miao%2520and%2520Will%2520Shen%2520and%2520Sayan%2520Mitra%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520demonstrating%2520zero-shot%2520sim-to-real%2520transfer%2520of%250Avisual%2520control%2520policies%2520learned%2520in%2520a%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520environment%250Afor%2520quadrotors%2520to%2520fly%2520through%2520racing%2520gates.%2520Robust%2520transfer%2520from%2520simulation%2520to%250Areal%2520flight%2520poses%2520a%2520major%2520challenge%252C%2520as%2520standard%2520simulators%2520often%2520lack%250Asufficient%2520visual%2520fidelity.%2520To%2520address%2520this%252C%2520we%2520construct%2520a%2520photorealistic%250Asimulation%2520environment%2520of%2520quadrotor%2520racing%2520tracks%252C%2520called%2520FalconGym%252C%2520which%250Aprovides%2520effectively%2520unlimited%2520synthetic%2520images%2520for%2520training.%2520Within%2520FalconGym%252C%250Awe%2520develop%2520a%2520pipelined%2520approach%2520for%2520crossing%2520gates%2520that%2520combines%2520%2528i%2529%2520a%2520Neural%250APose%2520Estimator%2520%2528NPE%2529%2520coupled%2520with%2520a%2520Kalman%2520filter%2520to%2520reliably%2520infer%2520quadrotor%250Aposes%2520from%2520single-frame%2520RGB%2520images%2520and%2520IMU%2520data%252C%2520and%2520%2528ii%2529%2520a%250Aself-attention-based%2520multi-modal%2520controller%2520that%2520adaptively%2520integrates%2520visual%250Afeatures%2520and%2520pose%2520estimation.%2520This%2520multi-modal%2520design%2520compensates%2520for%250Aperception%2520noise%2520and%2520intermittent%2520gate%2520visibility.%2520We%2520train%2520this%2520controller%250Apurely%2520in%2520FalconGym%2520with%2520imitation%2520learning%2520and%2520deploy%2520the%2520resulting%2520policy%2520to%250Areal%2520hardware%2520with%2520no%2520additional%2520fine-tuning.%2520Simulation%2520experiments%2520on%2520three%250Adistinct%2520tracks%2520%2528circle%252C%2520U-turn%2520and%2520figure-8%2529%2520demonstrate%2520that%2520our%2520controller%250Aoutperforms%2520a%2520vision-only%2520state-of-the-art%2520baseline%2520in%2520both%2520success%2520rate%2520and%250Agate-crossing%2520accuracy.%2520In%252030%2520live%2520hardware%2520flights%2520spanning%2520three%2520tracks%2520and%250A120%2520gates%252C%2520our%2520controller%2520achieves%2520a%252095.8%2525%2520success%2520rate%2520and%2520an%2520average%2520error%2520of%250Ajust%252010%2520cm%2520when%2520flying%2520through%252038%2520cm-radius%2520gates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FalconGym%3A%20A%20Photorealistic%20Simulation%20Framework%20for%20Zero-Shot%0A%20%20Sim-to-Real%20Vision-Based%20Quadrotor%20Navigation&entry.906535625=Yan%20Miao%20and%20Will%20Shen%20and%20Sayan%20Mitra&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20demonstrating%20zero-shot%20sim-to-real%20transfer%20of%0Avisual%20control%20policies%20learned%20in%20a%20Neural%20Radiance%20Field%20%28NeRF%29%20environment%0Afor%20quadrotors%20to%20fly%20through%20racing%20gates.%20Robust%20transfer%20from%20simulation%20to%0Areal%20flight%20poses%20a%20major%20challenge%2C%20as%20standard%20simulators%20often%20lack%0Asufficient%20visual%20fidelity.%20To%20address%20this%2C%20we%20construct%20a%20photorealistic%0Asimulation%20environment%20of%20quadrotor%20racing%20tracks%2C%20called%20FalconGym%2C%20which%0Aprovides%20effectively%20unlimited%20synthetic%20images%20for%20training.%20Within%20FalconGym%2C%0Awe%20develop%20a%20pipelined%20approach%20for%20crossing%20gates%20that%20combines%20%28i%29%20a%20Neural%0APose%20Estimator%20%28NPE%29%20coupled%20with%20a%20Kalman%20filter%20to%20reliably%20infer%20quadrotor%0Aposes%20from%20single-frame%20RGB%20images%20and%20IMU%20data%2C%20and%20%28ii%29%20a%0Aself-attention-based%20multi-modal%20controller%20that%20adaptively%20integrates%20visual%0Afeatures%20and%20pose%20estimation.%20This%20multi-modal%20design%20compensates%20for%0Aperception%20noise%20and%20intermittent%20gate%20visibility.%20We%20train%20this%20controller%0Apurely%20in%20FalconGym%20with%20imitation%20learning%20and%20deploy%20the%20resulting%20policy%20to%0Areal%20hardware%20with%20no%20additional%20fine-tuning.%20Simulation%20experiments%20on%20three%0Adistinct%20tracks%20%28circle%2C%20U-turn%20and%20figure-8%29%20demonstrate%20that%20our%20controller%0Aoutperforms%20a%20vision-only%20state-of-the-art%20baseline%20in%20both%20success%20rate%20and%0Agate-crossing%20accuracy.%20In%2030%20live%20hardware%20flights%20spanning%20three%20tracks%20and%0A120%20gates%2C%20our%20controller%20achieves%20a%2095.8%25%20success%20rate%20and%20an%20average%20error%20of%0Ajust%2010%20cm%20when%20flying%20through%2038%20cm-radius%20gates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02198v2&entry.124074799=Read"},
{"title": "Similarity-Based Self-Construct Graph Model for Predicting Patient\n  Criticalness Using Graph Neural Networks and EHR Data", "author": "Mukesh Kumar Sahu and Pinki Roy", "abstract": "  Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.\n", "link": "http://arxiv.org/abs/2508.00615v1", "date": "2025-08-01", "relevancy": 2.2421, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4577}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4474}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity-Based%20Self-Construct%20Graph%20Model%20for%20Predicting%20Patient%0A%20%20Criticalness%20Using%20Graph%20Neural%20Networks%20and%20EHR%20Data&body=Title%3A%20Similarity-Based%20Self-Construct%20Graph%20Model%20for%20Predicting%20Patient%0A%20%20Criticalness%20Using%20Graph%20Neural%20Networks%20and%20EHR%20Data%0AAuthor%3A%20Mukesh%20Kumar%20Sahu%20and%20Pinki%20Roy%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20criticalness%20of%20ICU%20patients%20%28such%20as%20in-ICU%0Amortality%20risk%29%20is%20vital%20for%20early%20intervention%20in%20critical%20care.%20However%2C%0Aconventional%20models%20often%20treat%20each%20patient%20in%20isolation%20and%20struggle%20to%0Aexploit%20the%20relational%20structure%20in%20Electronic%20Health%20Records%20%28EHR%29.%20We%20propose%0Aa%20Similarity-Based%20Self-Construct%20Graph%20Model%20%28SBSCGM%29%20that%20dynamically%20builds%0Aa%20patient%20similarity%20graph%20from%20multi-modal%20EHR%20data%2C%20and%20a%20HybridGraphMedGNN%0Aarchitecture%20that%20operates%20on%20this%20graph%20to%20predict%20patient%20mortality%20and%20a%0Acontinuous%20criticalness%20score.%20SBSCGM%20uses%20a%20hybrid%20similarity%20measure%0A%28combining%20feature-based%20and%20structural%20similarities%29%20to%20connect%20patients%20with%0Aanalogous%20clinical%20profiles%20in%20real-time.%20The%20HybridGraphMedGNN%20integrates%0AGraph%20Convolutional%20Network%20%28GCN%29%2C%20GraphSAGE%2C%20and%20Graph%20Attention%20Network%20%28GAT%29%0Alayers%20to%20learn%20robust%20patient%20representations%2C%20leveraging%20both%20local%20and%0Aglobal%20graph%20patterns.%20In%20experiments%20on%206%2C000%20ICU%20stays%20from%20the%20MIMIC-III%0Adataset%2C%20our%20model%20achieves%20state-of-the-art%20performance%20%28AUC-ROC%20%240.94%24%29%0Aoutperforming%20baseline%20classifiers%20and%20single-type%20GNN%20models.%20We%20also%0Ademonstrate%20improved%20precision/recall%20and%20show%20that%20the%20attention%20mechanism%0Aprovides%20interpretable%20insights%20into%20model%20predictions.%20Our%20framework%20offers%20a%0Ascalable%20and%20interpretable%20solution%20for%20critical%20care%20risk%20prediction%2C%20with%0Apotential%20to%20support%20clinicians%20in%20real-world%20ICU%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity-Based%2520Self-Construct%2520Graph%2520Model%2520for%2520Predicting%2520Patient%250A%2520%2520Criticalness%2520Using%2520Graph%2520Neural%2520Networks%2520and%2520EHR%2520Data%26entry.906535625%3DMukesh%2520Kumar%2520Sahu%2520and%2520Pinki%2520Roy%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520criticalness%2520of%2520ICU%2520patients%2520%2528such%2520as%2520in-ICU%250Amortality%2520risk%2529%2520is%2520vital%2520for%2520early%2520intervention%2520in%2520critical%2520care.%2520However%252C%250Aconventional%2520models%2520often%2520treat%2520each%2520patient%2520in%2520isolation%2520and%2520struggle%2520to%250Aexploit%2520the%2520relational%2520structure%2520in%2520Electronic%2520Health%2520Records%2520%2528EHR%2529.%2520We%2520propose%250Aa%2520Similarity-Based%2520Self-Construct%2520Graph%2520Model%2520%2528SBSCGM%2529%2520that%2520dynamically%2520builds%250Aa%2520patient%2520similarity%2520graph%2520from%2520multi-modal%2520EHR%2520data%252C%2520and%2520a%2520HybridGraphMedGNN%250Aarchitecture%2520that%2520operates%2520on%2520this%2520graph%2520to%2520predict%2520patient%2520mortality%2520and%2520a%250Acontinuous%2520criticalness%2520score.%2520SBSCGM%2520uses%2520a%2520hybrid%2520similarity%2520measure%250A%2528combining%2520feature-based%2520and%2520structural%2520similarities%2529%2520to%2520connect%2520patients%2520with%250Aanalogous%2520clinical%2520profiles%2520in%2520real-time.%2520The%2520HybridGraphMedGNN%2520integrates%250AGraph%2520Convolutional%2520Network%2520%2528GCN%2529%252C%2520GraphSAGE%252C%2520and%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%250Alayers%2520to%2520learn%2520robust%2520patient%2520representations%252C%2520leveraging%2520both%2520local%2520and%250Aglobal%2520graph%2520patterns.%2520In%2520experiments%2520on%25206%252C000%2520ICU%2520stays%2520from%2520the%2520MIMIC-III%250Adataset%252C%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%2520%2528AUC-ROC%2520%25240.94%2524%2529%250Aoutperforming%2520baseline%2520classifiers%2520and%2520single-type%2520GNN%2520models.%2520We%2520also%250Ademonstrate%2520improved%2520precision/recall%2520and%2520show%2520that%2520the%2520attention%2520mechanism%250Aprovides%2520interpretable%2520insights%2520into%2520model%2520predictions.%2520Our%2520framework%2520offers%2520a%250Ascalable%2520and%2520interpretable%2520solution%2520for%2520critical%2520care%2520risk%2520prediction%252C%2520with%250Apotential%2520to%2520support%2520clinicians%2520in%2520real-world%2520ICU%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity-Based%20Self-Construct%20Graph%20Model%20for%20Predicting%20Patient%0A%20%20Criticalness%20Using%20Graph%20Neural%20Networks%20and%20EHR%20Data&entry.906535625=Mukesh%20Kumar%20Sahu%20and%20Pinki%20Roy&entry.1292438233=%20%20Accurately%20predicting%20the%20criticalness%20of%20ICU%20patients%20%28such%20as%20in-ICU%0Amortality%20risk%29%20is%20vital%20for%20early%20intervention%20in%20critical%20care.%20However%2C%0Aconventional%20models%20often%20treat%20each%20patient%20in%20isolation%20and%20struggle%20to%0Aexploit%20the%20relational%20structure%20in%20Electronic%20Health%20Records%20%28EHR%29.%20We%20propose%0Aa%20Similarity-Based%20Self-Construct%20Graph%20Model%20%28SBSCGM%29%20that%20dynamically%20builds%0Aa%20patient%20similarity%20graph%20from%20multi-modal%20EHR%20data%2C%20and%20a%20HybridGraphMedGNN%0Aarchitecture%20that%20operates%20on%20this%20graph%20to%20predict%20patient%20mortality%20and%20a%0Acontinuous%20criticalness%20score.%20SBSCGM%20uses%20a%20hybrid%20similarity%20measure%0A%28combining%20feature-based%20and%20structural%20similarities%29%20to%20connect%20patients%20with%0Aanalogous%20clinical%20profiles%20in%20real-time.%20The%20HybridGraphMedGNN%20integrates%0AGraph%20Convolutional%20Network%20%28GCN%29%2C%20GraphSAGE%2C%20and%20Graph%20Attention%20Network%20%28GAT%29%0Alayers%20to%20learn%20robust%20patient%20representations%2C%20leveraging%20both%20local%20and%0Aglobal%20graph%20patterns.%20In%20experiments%20on%206%2C000%20ICU%20stays%20from%20the%20MIMIC-III%0Adataset%2C%20our%20model%20achieves%20state-of-the-art%20performance%20%28AUC-ROC%20%240.94%24%29%0Aoutperforming%20baseline%20classifiers%20and%20single-type%20GNN%20models.%20We%20also%0Ademonstrate%20improved%20precision/recall%20and%20show%20that%20the%20attention%20mechanism%0Aprovides%20interpretable%20insights%20into%20model%20predictions.%20Our%20framework%20offers%20a%0Ascalable%20and%20interpretable%20solution%20for%20critical%20care%20risk%20prediction%2C%20with%0Apotential%20to%20support%20clinicians%20in%20real-world%20ICU%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00615v1&entry.124074799=Read"},
{"title": "PanoLlama: Generating Endless and Coherent Panoramas with\n  Next-Token-Prediction LLMs", "author": "Teng Zhou and Xiaoyu Zhang and Yongchuan Tang", "abstract": "  Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama.\n", "link": "http://arxiv.org/abs/2411.15867v3", "date": "2025-08-01", "relevancy": 2.212, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5794}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoLlama%3A%20Generating%20Endless%20and%20Coherent%20Panoramas%20with%0A%20%20Next-Token-Prediction%20LLMs&body=Title%3A%20PanoLlama%3A%20Generating%20Endless%20and%20Coherent%20Panoramas%20with%0A%20%20Next-Token-Prediction%20LLMs%0AAuthor%3A%20Teng%20Zhou%20and%20Xiaoyu%20Zhang%20and%20Yongchuan%20Tang%0AAbstract%3A%20%20%20Panoramic%20Image%20Generation%20%28PIG%29%20aims%20to%20create%20coherent%20images%20of%20arbitrary%0Alengths.%20Most%20existing%20methods%20fall%20in%20the%20joint%20diffusion%20paradigm%2C%20but%20their%0Acomplex%20and%20heuristic%20crop%20connection%20designs%20often%20limit%20their%20ability%20to%0Aachieve%20multilevel%20coherence.%20By%20deconstructing%20this%20challenge%20into%20its%20core%0Acomponents%2C%20we%20find%20it%20naturally%20aligns%20with%20next-token%20prediction%2C%20leading%20us%0Ato%20adopt%20an%20autoregressive%20%28AR%29%20paradigm%20for%20PIG%20modeling.%20However%2C%20existing%0Avisual%20AR%20%28VAR%29%20models%20are%20limited%20to%20fixed-size%20generation%2C%20lacking%20the%0Acapability%20to%20produce%20panoramic%20images.%20In%20this%20paper%2C%20we%20propose%20PanoLlama%2C%20a%0Anovel%20framework%20that%20achieves%20endless%20and%20coherent%20panorama%20generation%20with%20the%0Aautoregressive%20paradigm.%20Our%20approach%20develops%20a%20training-free%20strategy%20that%0Autilizes%20token%20redirection%20to%20overcome%20the%20size%20limitations%20of%20existing%20VAR%0Amodels%2C%20enabling%20next-crop%20prediction%20in%20both%20horizontal%20and%20vertical%0Adirections.%20This%20refreshes%20the%20PIG%20pipeline%20while%20achieving%20SOTA%20performance%20in%0Acoherence%20%2847.50%25%29%2C%20fidelity%2828.16%25%29%2C%20and%20aesthetics%20%2815%25%29.%20Additionally%2C%0APanoLlama%20supports%20applications%20other%20PIG%20methods%20cannot%20achieve%2C%20including%0Amask-free%20layout%20control%2C%20multi-scale%20and%20multi-guidance%20synthesis.%20To%0Afacilitate%20standardized%20evaluation%2C%20we%20also%20establish%20a%20dataset%20with%201%2C000%0Aprompts%20spanning%20100%2B%20themes%2C%20providing%20a%20new%20testing%20benchmark%20for%20PIG%0Aresearch.%20The%20code%20is%20available%20at%20https%3A//github.com/0606zt/PanoLlama.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15867v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoLlama%253A%2520Generating%2520Endless%2520and%2520Coherent%2520Panoramas%2520with%250A%2520%2520Next-Token-Prediction%2520LLMs%26entry.906535625%3DTeng%2520Zhou%2520and%2520Xiaoyu%2520Zhang%2520and%2520Yongchuan%2520Tang%26entry.1292438233%3D%2520%2520Panoramic%2520Image%2520Generation%2520%2528PIG%2529%2520aims%2520to%2520create%2520coherent%2520images%2520of%2520arbitrary%250Alengths.%2520Most%2520existing%2520methods%2520fall%2520in%2520the%2520joint%2520diffusion%2520paradigm%252C%2520but%2520their%250Acomplex%2520and%2520heuristic%2520crop%2520connection%2520designs%2520often%2520limit%2520their%2520ability%2520to%250Aachieve%2520multilevel%2520coherence.%2520By%2520deconstructing%2520this%2520challenge%2520into%2520its%2520core%250Acomponents%252C%2520we%2520find%2520it%2520naturally%2520aligns%2520with%2520next-token%2520prediction%252C%2520leading%2520us%250Ato%2520adopt%2520an%2520autoregressive%2520%2528AR%2529%2520paradigm%2520for%2520PIG%2520modeling.%2520However%252C%2520existing%250Avisual%2520AR%2520%2528VAR%2529%2520models%2520are%2520limited%2520to%2520fixed-size%2520generation%252C%2520lacking%2520the%250Acapability%2520to%2520produce%2520panoramic%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PanoLlama%252C%2520a%250Anovel%2520framework%2520that%2520achieves%2520endless%2520and%2520coherent%2520panorama%2520generation%2520with%2520the%250Aautoregressive%2520paradigm.%2520Our%2520approach%2520develops%2520a%2520training-free%2520strategy%2520that%250Autilizes%2520token%2520redirection%2520to%2520overcome%2520the%2520size%2520limitations%2520of%2520existing%2520VAR%250Amodels%252C%2520enabling%2520next-crop%2520prediction%2520in%2520both%2520horizontal%2520and%2520vertical%250Adirections.%2520This%2520refreshes%2520the%2520PIG%2520pipeline%2520while%2520achieving%2520SOTA%2520performance%2520in%250Acoherence%2520%252847.50%2525%2529%252C%2520fidelity%252828.16%2525%2529%252C%2520and%2520aesthetics%2520%252815%2525%2529.%2520Additionally%252C%250APanoLlama%2520supports%2520applications%2520other%2520PIG%2520methods%2520cannot%2520achieve%252C%2520including%250Amask-free%2520layout%2520control%252C%2520multi-scale%2520and%2520multi-guidance%2520synthesis.%2520To%250Afacilitate%2520standardized%2520evaluation%252C%2520we%2520also%2520establish%2520a%2520dataset%2520with%25201%252C000%250Aprompts%2520spanning%2520100%252B%2520themes%252C%2520providing%2520a%2520new%2520testing%2520benchmark%2520for%2520PIG%250Aresearch.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/0606zt/PanoLlama.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15867v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoLlama%3A%20Generating%20Endless%20and%20Coherent%20Panoramas%20with%0A%20%20Next-Token-Prediction%20LLMs&entry.906535625=Teng%20Zhou%20and%20Xiaoyu%20Zhang%20and%20Yongchuan%20Tang&entry.1292438233=%20%20Panoramic%20Image%20Generation%20%28PIG%29%20aims%20to%20create%20coherent%20images%20of%20arbitrary%0Alengths.%20Most%20existing%20methods%20fall%20in%20the%20joint%20diffusion%20paradigm%2C%20but%20their%0Acomplex%20and%20heuristic%20crop%20connection%20designs%20often%20limit%20their%20ability%20to%0Aachieve%20multilevel%20coherence.%20By%20deconstructing%20this%20challenge%20into%20its%20core%0Acomponents%2C%20we%20find%20it%20naturally%20aligns%20with%20next-token%20prediction%2C%20leading%20us%0Ato%20adopt%20an%20autoregressive%20%28AR%29%20paradigm%20for%20PIG%20modeling.%20However%2C%20existing%0Avisual%20AR%20%28VAR%29%20models%20are%20limited%20to%20fixed-size%20generation%2C%20lacking%20the%0Acapability%20to%20produce%20panoramic%20images.%20In%20this%20paper%2C%20we%20propose%20PanoLlama%2C%20a%0Anovel%20framework%20that%20achieves%20endless%20and%20coherent%20panorama%20generation%20with%20the%0Aautoregressive%20paradigm.%20Our%20approach%20develops%20a%20training-free%20strategy%20that%0Autilizes%20token%20redirection%20to%20overcome%20the%20size%20limitations%20of%20existing%20VAR%0Amodels%2C%20enabling%20next-crop%20prediction%20in%20both%20horizontal%20and%20vertical%0Adirections.%20This%20refreshes%20the%20PIG%20pipeline%20while%20achieving%20SOTA%20performance%20in%0Acoherence%20%2847.50%25%29%2C%20fidelity%2828.16%25%29%2C%20and%20aesthetics%20%2815%25%29.%20Additionally%2C%0APanoLlama%20supports%20applications%20other%20PIG%20methods%20cannot%20achieve%2C%20including%0Amask-free%20layout%20control%2C%20multi-scale%20and%20multi-guidance%20synthesis.%20To%0Afacilitate%20standardized%20evaluation%2C%20we%20also%20establish%20a%20dataset%20with%201%2C000%0Aprompts%20spanning%20100%2B%20themes%2C%20providing%20a%20new%20testing%20benchmark%20for%20PIG%0Aresearch.%20The%20code%20is%20available%20at%20https%3A//github.com/0606zt/PanoLlama.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15867v3&entry.124074799=Read"},
{"title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "author": "Hongfei Zhang and Kun Zhou and Ruizheng Wu and Jiangbo Lu", "abstract": "  Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.\n", "link": "http://arxiv.org/abs/2508.00698v1", "date": "2025-08-01", "relevancy": 2.1836, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Pretrained%20Depth%20Estimation%20Models%20Help%20With%20Image%20Dehazing%3F&body=Title%3A%20Can%20Large%20Pretrained%20Depth%20Estimation%20Models%20Help%20With%20Image%20Dehazing%3F%0AAuthor%3A%20Hongfei%20Zhang%20and%20Kun%20Zhou%20and%20Ruizheng%20Wu%20and%20Jiangbo%20Lu%0AAbstract%3A%20%20%20Image%20dehazing%20remains%20a%20challenging%20problem%20due%20to%20the%20spatially%20varying%0Anature%20of%20haze%20in%20real-world%20scenes.%20While%20existing%20methods%20have%20demonstrated%0Athe%20promise%20of%20large-scale%20pretrained%20models%20for%20image%20dehazing%2C%20their%0Aarchitecture-specific%20designs%20hinder%20adaptability%20across%20diverse%20scenarios%20with%0Adifferent%20accuracy%20and%20efficiency%20requirements.%20In%20this%20work%2C%20we%20systematically%0Ainvestigate%20the%20generalization%20capability%20of%20pretrained%20depth%0Arepresentations-learned%20from%20millions%20of%20diverse%20images-for%20image%20dehazing.%20Our%0Aempirical%20analysis%20reveals%20that%20the%20learned%20deep%20depth%20features%20maintain%0Aremarkable%20consistency%20across%20varying%20haze%20levels.%20Building%20on%20this%20insight%2C%20we%0Apropose%20a%20plug-and-play%20RGB-D%20fusion%20module%20that%20seamlessly%20integrates%20with%0Adiverse%20dehazing%20architectures.%20Extensive%20experiments%20across%20multiple%0Abenchmarks%20validate%20both%20the%20effectiveness%20and%20broad%20applicability%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Pretrained%2520Depth%2520Estimation%2520Models%2520Help%2520With%2520Image%2520Dehazing%253F%26entry.906535625%3DHongfei%2520Zhang%2520and%2520Kun%2520Zhou%2520and%2520Ruizheng%2520Wu%2520and%2520Jiangbo%2520Lu%26entry.1292438233%3D%2520%2520Image%2520dehazing%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520spatially%2520varying%250Anature%2520of%2520haze%2520in%2520real-world%2520scenes.%2520While%2520existing%2520methods%2520have%2520demonstrated%250Athe%2520promise%2520of%2520large-scale%2520pretrained%2520models%2520for%2520image%2520dehazing%252C%2520their%250Aarchitecture-specific%2520designs%2520hinder%2520adaptability%2520across%2520diverse%2520scenarios%2520with%250Adifferent%2520accuracy%2520and%2520efficiency%2520requirements.%2520In%2520this%2520work%252C%2520we%2520systematically%250Ainvestigate%2520the%2520generalization%2520capability%2520of%2520pretrained%2520depth%250Arepresentations-learned%2520from%2520millions%2520of%2520diverse%2520images-for%2520image%2520dehazing.%2520Our%250Aempirical%2520analysis%2520reveals%2520that%2520the%2520learned%2520deep%2520depth%2520features%2520maintain%250Aremarkable%2520consistency%2520across%2520varying%2520haze%2520levels.%2520Building%2520on%2520this%2520insight%252C%2520we%250Apropose%2520a%2520plug-and-play%2520RGB-D%2520fusion%2520module%2520that%2520seamlessly%2520integrates%2520with%250Adiverse%2520dehazing%2520architectures.%2520Extensive%2520experiments%2520across%2520multiple%250Abenchmarks%2520validate%2520both%2520the%2520effectiveness%2520and%2520broad%2520applicability%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Pretrained%20Depth%20Estimation%20Models%20Help%20With%20Image%20Dehazing%3F&entry.906535625=Hongfei%20Zhang%20and%20Kun%20Zhou%20and%20Ruizheng%20Wu%20and%20Jiangbo%20Lu&entry.1292438233=%20%20Image%20dehazing%20remains%20a%20challenging%20problem%20due%20to%20the%20spatially%20varying%0Anature%20of%20haze%20in%20real-world%20scenes.%20While%20existing%20methods%20have%20demonstrated%0Athe%20promise%20of%20large-scale%20pretrained%20models%20for%20image%20dehazing%2C%20their%0Aarchitecture-specific%20designs%20hinder%20adaptability%20across%20diverse%20scenarios%20with%0Adifferent%20accuracy%20and%20efficiency%20requirements.%20In%20this%20work%2C%20we%20systematically%0Ainvestigate%20the%20generalization%20capability%20of%20pretrained%20depth%0Arepresentations-learned%20from%20millions%20of%20diverse%20images-for%20image%20dehazing.%20Our%0Aempirical%20analysis%20reveals%20that%20the%20learned%20deep%20depth%20features%20maintain%0Aremarkable%20consistency%20across%20varying%20haze%20levels.%20Building%20on%20this%20insight%2C%20we%0Apropose%20a%20plug-and-play%20RGB-D%20fusion%20module%20that%20seamlessly%20integrates%20with%0Adiverse%20dehazing%20architectures.%20Extensive%20experiments%20across%20multiple%0Abenchmarks%20validate%20both%20the%20effectiveness%20and%20broad%20applicability%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00698v1&entry.124074799=Read"},
{"title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast\n  Representations", "author": "Mehmet Yigit Avci and Pedro Borges and Paul Wright and Mehmet Yigitsoy and Sebastien Ourselin and Jorge Cardoso", "abstract": "  Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.\n", "link": "http://arxiv.org/abs/2507.00043v2", "date": "2025-08-01", "relevancy": 2.1771, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5961}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR-CLIP%3A%20Efficient%20Metadata-Guided%20Learning%20of%20MRI%20Contrast%0A%20%20Representations&body=Title%3A%20MR-CLIP%3A%20Efficient%20Metadata-Guided%20Learning%20of%20MRI%20Contrast%0A%20%20Representations%0AAuthor%3A%20Mehmet%20Yigit%20Avci%20and%20Pedro%20Borges%20and%20Paul%20Wright%20and%20Mehmet%20Yigitsoy%20and%20Sebastien%20Ourselin%20and%20Jorge%20Cardoso%0AAbstract%3A%20%20%20Accurate%20interpretation%20of%20Magnetic%20Resonance%20Imaging%20scans%20in%20clinical%0Asystems%20is%20based%20on%20a%20precise%20understanding%20of%20image%20contrast.%20This%20contrast%20is%0Aprimarily%20governed%20by%20acquisition%20parameters%2C%20such%20as%20echo%20time%20and%20repetition%0Atime%2C%20which%20are%20stored%20in%20the%20DICOM%20metadata.%20To%20simplify%20contrast%0Aidentification%2C%20broad%20labels%20such%20as%20T1-weighted%20or%20T2-weighted%20are%20commonly%0Aused%2C%20but%20these%20offer%20only%20a%20coarse%20approximation%20of%20the%20underlying%20acquisition%0Asettings.%20In%20many%20real-world%20datasets%2C%20such%20labels%20are%20entirely%20missing%2C%0Aleaving%20raw%20acquisition%20parameters%20as%20the%20only%20indicators%20of%20contrast.%20Adding%0Ato%20this%20challenge%2C%20the%20available%20metadata%20is%20often%20incomplete%2C%20noisy%2C%20or%0Ainconsistent.%20The%20lack%20of%20reliable%20and%20standardized%20metadata%20complicates%20tasks%0Asuch%20as%20image%20interpretation%2C%20retrieval%2C%20and%20integration%20into%20clinical%0Aworkflows.%20Furthermore%2C%20robust%20contrast-aware%20representations%20are%20essential%20to%0Aenable%20more%20advanced%20clinical%20applications%2C%20such%20as%20achieving%0Amodality-invariant%20representations%20and%20data%20harmonization.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MR-CLIP%2C%20a%20multimodal%20contrastive%20learning%20framework%0Athat%20aligns%20MR%20images%20with%20their%20DICOM%20metadata%20to%20learn%20contrast-aware%0Arepresentations%2C%20without%20relying%20on%20manual%20labels.%20Trained%20on%20a%20diverse%0Aclinical%20dataset%20that%20spans%20various%20scanners%20and%20protocols%2C%20MR-CLIP%20captures%0Acontrast%20variations%20across%20acquisitions%20and%20within%20scans%2C%20enabling%0Aanatomy-invariant%20representations.%20We%20demonstrate%20its%20effectiveness%20in%0Across-modal%20retrieval%20and%20contrast%20classification%2C%20highlighting%20its%20scalability%0Aand%20potential%20for%20further%20clinical%20applications.%20The%20code%20and%20weights%20are%0Apublicly%20available%20at%20https%3A//github.com/myigitavci/MR-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR-CLIP%253A%2520Efficient%2520Metadata-Guided%2520Learning%2520of%2520MRI%2520Contrast%250A%2520%2520Representations%26entry.906535625%3DMehmet%2520Yigit%2520Avci%2520and%2520Pedro%2520Borges%2520and%2520Paul%2520Wright%2520and%2520Mehmet%2520Yigitsoy%2520and%2520Sebastien%2520Ourselin%2520and%2520Jorge%2520Cardoso%26entry.1292438233%3D%2520%2520Accurate%2520interpretation%2520of%2520Magnetic%2520Resonance%2520Imaging%2520scans%2520in%2520clinical%250Asystems%2520is%2520based%2520on%2520a%2520precise%2520understanding%2520of%2520image%2520contrast.%2520This%2520contrast%2520is%250Aprimarily%2520governed%2520by%2520acquisition%2520parameters%252C%2520such%2520as%2520echo%2520time%2520and%2520repetition%250Atime%252C%2520which%2520are%2520stored%2520in%2520the%2520DICOM%2520metadata.%2520To%2520simplify%2520contrast%250Aidentification%252C%2520broad%2520labels%2520such%2520as%2520T1-weighted%2520or%2520T2-weighted%2520are%2520commonly%250Aused%252C%2520but%2520these%2520offer%2520only%2520a%2520coarse%2520approximation%2520of%2520the%2520underlying%2520acquisition%250Asettings.%2520In%2520many%2520real-world%2520datasets%252C%2520such%2520labels%2520are%2520entirely%2520missing%252C%250Aleaving%2520raw%2520acquisition%2520parameters%2520as%2520the%2520only%2520indicators%2520of%2520contrast.%2520Adding%250Ato%2520this%2520challenge%252C%2520the%2520available%2520metadata%2520is%2520often%2520incomplete%252C%2520noisy%252C%2520or%250Ainconsistent.%2520The%2520lack%2520of%2520reliable%2520and%2520standardized%2520metadata%2520complicates%2520tasks%250Asuch%2520as%2520image%2520interpretation%252C%2520retrieval%252C%2520and%2520integration%2520into%2520clinical%250Aworkflows.%2520Furthermore%252C%2520robust%2520contrast-aware%2520representations%2520are%2520essential%2520to%250Aenable%2520more%2520advanced%2520clinical%2520applications%252C%2520such%2520as%2520achieving%250Amodality-invariant%2520representations%2520and%2520data%2520harmonization.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520MR-CLIP%252C%2520a%2520multimodal%2520contrastive%2520learning%2520framework%250Athat%2520aligns%2520MR%2520images%2520with%2520their%2520DICOM%2520metadata%2520to%2520learn%2520contrast-aware%250Arepresentations%252C%2520without%2520relying%2520on%2520manual%2520labels.%2520Trained%2520on%2520a%2520diverse%250Aclinical%2520dataset%2520that%2520spans%2520various%2520scanners%2520and%2520protocols%252C%2520MR-CLIP%2520captures%250Acontrast%2520variations%2520across%2520acquisitions%2520and%2520within%2520scans%252C%2520enabling%250Aanatomy-invariant%2520representations.%2520We%2520demonstrate%2520its%2520effectiveness%2520in%250Across-modal%2520retrieval%2520and%2520contrast%2520classification%252C%2520highlighting%2520its%2520scalability%250Aand%2520potential%2520for%2520further%2520clinical%2520applications.%2520The%2520code%2520and%2520weights%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/myigitavci/MR-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR-CLIP%3A%20Efficient%20Metadata-Guided%20Learning%20of%20MRI%20Contrast%0A%20%20Representations&entry.906535625=Mehmet%20Yigit%20Avci%20and%20Pedro%20Borges%20and%20Paul%20Wright%20and%20Mehmet%20Yigitsoy%20and%20Sebastien%20Ourselin%20and%20Jorge%20Cardoso&entry.1292438233=%20%20Accurate%20interpretation%20of%20Magnetic%20Resonance%20Imaging%20scans%20in%20clinical%0Asystems%20is%20based%20on%20a%20precise%20understanding%20of%20image%20contrast.%20This%20contrast%20is%0Aprimarily%20governed%20by%20acquisition%20parameters%2C%20such%20as%20echo%20time%20and%20repetition%0Atime%2C%20which%20are%20stored%20in%20the%20DICOM%20metadata.%20To%20simplify%20contrast%0Aidentification%2C%20broad%20labels%20such%20as%20T1-weighted%20or%20T2-weighted%20are%20commonly%0Aused%2C%20but%20these%20offer%20only%20a%20coarse%20approximation%20of%20the%20underlying%20acquisition%0Asettings.%20In%20many%20real-world%20datasets%2C%20such%20labels%20are%20entirely%20missing%2C%0Aleaving%20raw%20acquisition%20parameters%20as%20the%20only%20indicators%20of%20contrast.%20Adding%0Ato%20this%20challenge%2C%20the%20available%20metadata%20is%20often%20incomplete%2C%20noisy%2C%20or%0Ainconsistent.%20The%20lack%20of%20reliable%20and%20standardized%20metadata%20complicates%20tasks%0Asuch%20as%20image%20interpretation%2C%20retrieval%2C%20and%20integration%20into%20clinical%0Aworkflows.%20Furthermore%2C%20robust%20contrast-aware%20representations%20are%20essential%20to%0Aenable%20more%20advanced%20clinical%20applications%2C%20such%20as%20achieving%0Amodality-invariant%20representations%20and%20data%20harmonization.%20To%20address%20these%0Achallenges%2C%20we%20propose%20MR-CLIP%2C%20a%20multimodal%20contrastive%20learning%20framework%0Athat%20aligns%20MR%20images%20with%20their%20DICOM%20metadata%20to%20learn%20contrast-aware%0Arepresentations%2C%20without%20relying%20on%20manual%20labels.%20Trained%20on%20a%20diverse%0Aclinical%20dataset%20that%20spans%20various%20scanners%20and%20protocols%2C%20MR-CLIP%20captures%0Acontrast%20variations%20across%20acquisitions%20and%20within%20scans%2C%20enabling%0Aanatomy-invariant%20representations.%20We%20demonstrate%20its%20effectiveness%20in%0Across-modal%20retrieval%20and%20contrast%20classification%2C%20highlighting%20its%20scalability%0Aand%20potential%20for%20further%20clinical%20applications.%20The%20code%20and%20weights%20are%0Apublicly%20available%20at%20https%3A//github.com/myigitavci/MR-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00043v2&entry.124074799=Read"},
{"title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "author": "Fabian C. Weigend and Dabin K. Choe and Santiago Canete and Conor J. Walsh", "abstract": "  Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.\n", "link": "http://arxiv.org/abs/2508.00691v1", "date": "2025-08-01", "relevancy": 2.1705, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Data-Driven%20Adaptive%20Exoskeleton%20Assistance%20for%20Post-stroke%20Gait&body=Title%3A%20Towards%20Data-Driven%20Adaptive%20Exoskeleton%20Assistance%20for%20Post-stroke%20Gait%0AAuthor%3A%20Fabian%20C.%20Weigend%20and%20Dabin%20K.%20Choe%20and%20Santiago%20Canete%20and%20Conor%20J.%20Walsh%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20exoskeletons%20controlled%20through%20data-driven%0Amethods%20can%20dynamically%20adapt%20assistance%20to%20various%20tasks%20for%20healthy%20young%0Aadults.%20However%2C%20applying%20these%20methods%20to%20populations%20with%20neuromotor%20gait%0Adeficits%2C%20such%20as%20post-stroke%20hemiparesis%2C%20is%20challenging.%20This%20is%20due%20not%20only%0Ato%20high%20population%20heterogeneity%20and%20gait%20variability%20but%20also%20to%20a%20lack%20of%0Apost-stroke%20gait%20datasets%20to%20train%20accurate%20models.%20Despite%20these%20challenges%2C%0Adata-driven%20methods%20offer%20a%20promising%20avenue%20for%20control%2C%20potentially%20allowing%0Aexoskeletons%20to%20function%20safely%20and%20effectively%20in%20unstructured%20community%0Asettings.%20This%20work%20presents%20a%20first%20step%20towards%20enabling%20adaptive%0Aplantarflexion%20and%20dorsiflexion%20assistance%20from%20data-driven%20torque%20estimation%0Aduring%20post-stroke%20walking.%20We%20trained%20a%20multi-task%20Temporal%20Convolutional%0ANetwork%20%28TCN%29%20using%20collected%20data%20from%20four%20post-stroke%20participants%20walking%0Aon%20a%20treadmill%20%28%24R%5E2%24%20of%20%240.74%20%5Cpm%200.13%24%29.%20The%20model%20uses%20data%20from%20three%0Ainertial%20measurement%20units%20%28IMU%29%20and%20was%20pretrained%20on%20healthy%20walking%20data%0Afrom%206%20participants.%20We%20implemented%20a%20wearable%20prototype%20for%20our%20ankle%20torque%0Aestimation%20approach%20for%20exoskeleton%20control%20and%20demonstrated%20the%20viability%20of%0Areal-time%20sensing%2C%20estimation%2C%20and%20actuation%20with%20one%20post-stroke%20participant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Data-Driven%2520Adaptive%2520Exoskeleton%2520Assistance%2520for%2520Post-stroke%2520Gait%26entry.906535625%3DFabian%2520C.%2520Weigend%2520and%2520Dabin%2520K.%2520Choe%2520and%2520Santiago%2520Canete%2520and%2520Conor%2520J.%2520Walsh%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520exoskeletons%2520controlled%2520through%2520data-driven%250Amethods%2520can%2520dynamically%2520adapt%2520assistance%2520to%2520various%2520tasks%2520for%2520healthy%2520young%250Aadults.%2520However%252C%2520applying%2520these%2520methods%2520to%2520populations%2520with%2520neuromotor%2520gait%250Adeficits%252C%2520such%2520as%2520post-stroke%2520hemiparesis%252C%2520is%2520challenging.%2520This%2520is%2520due%2520not%2520only%250Ato%2520high%2520population%2520heterogeneity%2520and%2520gait%2520variability%2520but%2520also%2520to%2520a%2520lack%2520of%250Apost-stroke%2520gait%2520datasets%2520to%2520train%2520accurate%2520models.%2520Despite%2520these%2520challenges%252C%250Adata-driven%2520methods%2520offer%2520a%2520promising%2520avenue%2520for%2520control%252C%2520potentially%2520allowing%250Aexoskeletons%2520to%2520function%2520safely%2520and%2520effectively%2520in%2520unstructured%2520community%250Asettings.%2520This%2520work%2520presents%2520a%2520first%2520step%2520towards%2520enabling%2520adaptive%250Aplantarflexion%2520and%2520dorsiflexion%2520assistance%2520from%2520data-driven%2520torque%2520estimation%250Aduring%2520post-stroke%2520walking.%2520We%2520trained%2520a%2520multi-task%2520Temporal%2520Convolutional%250ANetwork%2520%2528TCN%2529%2520using%2520collected%2520data%2520from%2520four%2520post-stroke%2520participants%2520walking%250Aon%2520a%2520treadmill%2520%2528%2524R%255E2%2524%2520of%2520%25240.74%2520%255Cpm%25200.13%2524%2529.%2520The%2520model%2520uses%2520data%2520from%2520three%250Ainertial%2520measurement%2520units%2520%2528IMU%2529%2520and%2520was%2520pretrained%2520on%2520healthy%2520walking%2520data%250Afrom%25206%2520participants.%2520We%2520implemented%2520a%2520wearable%2520prototype%2520for%2520our%2520ankle%2520torque%250Aestimation%2520approach%2520for%2520exoskeleton%2520control%2520and%2520demonstrated%2520the%2520viability%2520of%250Areal-time%2520sensing%252C%2520estimation%252C%2520and%2520actuation%2520with%2520one%2520post-stroke%2520participant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Data-Driven%20Adaptive%20Exoskeleton%20Assistance%20for%20Post-stroke%20Gait&entry.906535625=Fabian%20C.%20Weigend%20and%20Dabin%20K.%20Choe%20and%20Santiago%20Canete%20and%20Conor%20J.%20Walsh&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20exoskeletons%20controlled%20through%20data-driven%0Amethods%20can%20dynamically%20adapt%20assistance%20to%20various%20tasks%20for%20healthy%20young%0Aadults.%20However%2C%20applying%20these%20methods%20to%20populations%20with%20neuromotor%20gait%0Adeficits%2C%20such%20as%20post-stroke%20hemiparesis%2C%20is%20challenging.%20This%20is%20due%20not%20only%0Ato%20high%20population%20heterogeneity%20and%20gait%20variability%20but%20also%20to%20a%20lack%20of%0Apost-stroke%20gait%20datasets%20to%20train%20accurate%20models.%20Despite%20these%20challenges%2C%0Adata-driven%20methods%20offer%20a%20promising%20avenue%20for%20control%2C%20potentially%20allowing%0Aexoskeletons%20to%20function%20safely%20and%20effectively%20in%20unstructured%20community%0Asettings.%20This%20work%20presents%20a%20first%20step%20towards%20enabling%20adaptive%0Aplantarflexion%20and%20dorsiflexion%20assistance%20from%20data-driven%20torque%20estimation%0Aduring%20post-stroke%20walking.%20We%20trained%20a%20multi-task%20Temporal%20Convolutional%0ANetwork%20%28TCN%29%20using%20collected%20data%20from%20four%20post-stroke%20participants%20walking%0Aon%20a%20treadmill%20%28%24R%5E2%24%20of%20%240.74%20%5Cpm%200.13%24%29.%20The%20model%20uses%20data%20from%20three%0Ainertial%20measurement%20units%20%28IMU%29%20and%20was%20pretrained%20on%20healthy%20walking%20data%0Afrom%206%20participants.%20We%20implemented%20a%20wearable%20prototype%20for%20our%20ankle%20torque%0Aestimation%20approach%20for%20exoskeleton%20control%20and%20demonstrated%20the%20viability%20of%0Areal-time%20sensing%2C%20estimation%2C%20and%20actuation%20with%20one%20post-stroke%20participant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00691v1&entry.124074799=Read"},
{"title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large\n  Language Models", "author": "Mingruo Yuan and Shuyi Zhang and Ben Kao", "abstract": "  Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.\n", "link": "http://arxiv.org/abs/2508.00600v1", "date": "2025-08-01", "relevancy": 2.1625, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Context-Aware%20Dual-Metric%20Framework%20for%20Confidence%20Estimation%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20A%20Context-Aware%20Dual-Metric%20Framework%20for%20Confidence%20Estimation%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Mingruo%20Yuan%20and%20Shuyi%20Zhang%20and%20Ben%20Kao%0AAbstract%3A%20%20%20Accurate%20confidence%20estimation%20is%20essential%20for%20trustworthy%20large%20language%0Amodels%20%28LLMs%29%20systems%2C%20as%20it%20empowers%20the%20user%20to%20determine%20when%20to%20trust%0Aoutputs%20and%20enables%20reliable%20deployment%20in%20safety-critical%20applications.%0ACurrent%20confidence%20estimation%20methods%20for%20LLMs%20neglect%20the%20relevance%20between%0Aresponses%20and%20contextual%20information%2C%20a%20crucial%20factor%20in%20output%20quality%0Aevaluation%2C%20particularly%20in%20scenarios%20where%20background%20knowledge%20is%20provided.%0ATo%20bridge%20this%20gap%2C%20we%20propose%20CRUX%20%28Context-aware%20entropy%20Reduction%20and%0AUnified%20consistency%20eXamination%29%2C%20the%20first%20framework%20that%20integrates%20context%0Afaithfulness%20and%20consistency%20for%20confidence%20estimation%20via%20two%20novel%20metrics.%0AFirst%2C%20contextual%20entropy%20reduction%20represents%20data%20uncertainty%20with%20the%0Ainformation%20gain%20through%20contrastive%20sampling%20with%20and%20without%20context.%20Second%2C%0Aunified%20consistency%20examination%20captures%20potential%20model%20uncertainty%20through%0Athe%20global%20consistency%20of%20the%20generated%20answers%20with%20and%20without%20context.%0AExperiments%20across%20three%20benchmark%20datasets%20%28CoQA%2C%20SQuAD%2C%20QuAC%29%20and%20two%0Adomain-specific%20datasets%20%28BioASQ%2C%20EduQG%29%20demonstrate%20CRUX%27s%20effectiveness%2C%0Aachieving%20the%20highest%20AUROC%20than%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Context-Aware%2520Dual-Metric%2520Framework%2520for%2520Confidence%2520Estimation%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMingruo%2520Yuan%2520and%2520Shuyi%2520Zhang%2520and%2520Ben%2520Kao%26entry.1292438233%3D%2520%2520Accurate%2520confidence%2520estimation%2520is%2520essential%2520for%2520trustworthy%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520systems%252C%2520as%2520it%2520empowers%2520the%2520user%2520to%2520determine%2520when%2520to%2520trust%250Aoutputs%2520and%2520enables%2520reliable%2520deployment%2520in%2520safety-critical%2520applications.%250ACurrent%2520confidence%2520estimation%2520methods%2520for%2520LLMs%2520neglect%2520the%2520relevance%2520between%250Aresponses%2520and%2520contextual%2520information%252C%2520a%2520crucial%2520factor%2520in%2520output%2520quality%250Aevaluation%252C%2520particularly%2520in%2520scenarios%2520where%2520background%2520knowledge%2520is%2520provided.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520CRUX%2520%2528Context-aware%2520entropy%2520Reduction%2520and%250AUnified%2520consistency%2520eXamination%2529%252C%2520the%2520first%2520framework%2520that%2520integrates%2520context%250Afaithfulness%2520and%2520consistency%2520for%2520confidence%2520estimation%2520via%2520two%2520novel%2520metrics.%250AFirst%252C%2520contextual%2520entropy%2520reduction%2520represents%2520data%2520uncertainty%2520with%2520the%250Ainformation%2520gain%2520through%2520contrastive%2520sampling%2520with%2520and%2520without%2520context.%2520Second%252C%250Aunified%2520consistency%2520examination%2520captures%2520potential%2520model%2520uncertainty%2520through%250Athe%2520global%2520consistency%2520of%2520the%2520generated%2520answers%2520with%2520and%2520without%2520context.%250AExperiments%2520across%2520three%2520benchmark%2520datasets%2520%2528CoQA%252C%2520SQuAD%252C%2520QuAC%2529%2520and%2520two%250Adomain-specific%2520datasets%2520%2528BioASQ%252C%2520EduQG%2529%2520demonstrate%2520CRUX%2527s%2520effectiveness%252C%250Aachieving%2520the%2520highest%2520AUROC%2520than%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Context-Aware%20Dual-Metric%20Framework%20for%20Confidence%20Estimation%20in%20Large%0A%20%20Language%20Models&entry.906535625=Mingruo%20Yuan%20and%20Shuyi%20Zhang%20and%20Ben%20Kao&entry.1292438233=%20%20Accurate%20confidence%20estimation%20is%20essential%20for%20trustworthy%20large%20language%0Amodels%20%28LLMs%29%20systems%2C%20as%20it%20empowers%20the%20user%20to%20determine%20when%20to%20trust%0Aoutputs%20and%20enables%20reliable%20deployment%20in%20safety-critical%20applications.%0ACurrent%20confidence%20estimation%20methods%20for%20LLMs%20neglect%20the%20relevance%20between%0Aresponses%20and%20contextual%20information%2C%20a%20crucial%20factor%20in%20output%20quality%0Aevaluation%2C%20particularly%20in%20scenarios%20where%20background%20knowledge%20is%20provided.%0ATo%20bridge%20this%20gap%2C%20we%20propose%20CRUX%20%28Context-aware%20entropy%20Reduction%20and%0AUnified%20consistency%20eXamination%29%2C%20the%20first%20framework%20that%20integrates%20context%0Afaithfulness%20and%20consistency%20for%20confidence%20estimation%20via%20two%20novel%20metrics.%0AFirst%2C%20contextual%20entropy%20reduction%20represents%20data%20uncertainty%20with%20the%0Ainformation%20gain%20through%20contrastive%20sampling%20with%20and%20without%20context.%20Second%2C%0Aunified%20consistency%20examination%20captures%20potential%20model%20uncertainty%20through%0Athe%20global%20consistency%20of%20the%20generated%20answers%20with%20and%20without%20context.%0AExperiments%20across%20three%20benchmark%20datasets%20%28CoQA%2C%20SQuAD%2C%20QuAC%29%20and%20two%0Adomain-specific%20datasets%20%28BioASQ%2C%20EduQG%29%20demonstrate%20CRUX%27s%20effectiveness%2C%0Aachieving%20the%20highest%20AUROC%20than%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00600v1&entry.124074799=Read"},
{"title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "author": "Irene Iele and Francesco Di Feola and Valerio Guarrasi and Paolo Soda", "abstract": "  Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.\n", "link": "http://arxiv.org/abs/2508.00766v1", "date": "2025-08-01", "relevancy": 2.1521, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5337}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation&body=Title%3A%20Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation%0AAuthor%3A%20Irene%20Iele%20and%20Francesco%20Di%20Feola%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Image-to-image%20translation%20has%20emerged%20as%20a%20powerful%20technique%20in%20medical%0Aimaging%2C%20enabling%20tasks%20such%20as%20image%20denoising%20and%20cross-modality%20conversion.%0AHowever%2C%20it%20suffers%20from%20limitations%20in%20handling%20out-of-distribution%20samples%0Awithout%20causing%20performance%20degradation.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20novel%20Test-Time%20Adaptation%20%28TTA%29%20framework%20that%20dynamically%20adjusts%20the%0Atranslation%20process%20based%20on%20the%20characteristics%20of%20each%20test%20sample.%20Our%0Amethod%20introduces%20a%20Reconstruction%20Module%20to%20quantify%20the%20domain%20shift%20and%20a%0ADynamic%20Adaptation%20Block%20that%20selectively%20modifies%20the%20internal%20features%20of%20a%0Apretrained%20translation%20model%20to%20mitigate%20the%20shift%20without%20compromising%20the%0Aperformance%20on%20in-distribution%20samples%20that%20do%20not%20require%20adaptation.%20We%0Aevaluate%20our%20approach%20on%20two%20medical%20image-to-image%20translation%20tasks%3A%20low-dose%0ACT%20denoising%20and%20T1%20to%20T2%20MRI%20translation%2C%20showing%20consistent%20improvements%20over%0Aboth%20the%20baseline%20translation%20model%20without%20TTA%20and%20prior%20TTA%20methods.%20Our%0Aanalysis%20highlights%20the%20limitations%20of%20the%20state-of-the-art%20that%20uniformly%0Aapply%20the%20adaptation%20to%20both%20out-of-distribution%20and%20in-distribution%20samples%2C%0Ademonstrating%20that%20dynamic%2C%20sample-specific%20adjustment%20offers%20a%20promising%20path%0Ato%20improve%20model%20resilience%20in%20real-world%20scenarios.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/cosbidev/Sample-Aware_TTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Aware%2520Test-Time%2520Adaptation%2520for%2520Medical%2520Image-to-Image%2520Translation%26entry.906535625%3DIrene%2520Iele%2520and%2520Francesco%2520Di%2520Feola%2520and%2520Valerio%2520Guarrasi%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520Image-to-image%2520translation%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520in%2520medical%250Aimaging%252C%2520enabling%2520tasks%2520such%2520as%2520image%2520denoising%2520and%2520cross-modality%2520conversion.%250AHowever%252C%2520it%2520suffers%2520from%2520limitations%2520in%2520handling%2520out-of-distribution%2520samples%250Awithout%2520causing%2520performance%2520degradation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250Aa%2520novel%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520framework%2520that%2520dynamically%2520adjusts%2520the%250Atranslation%2520process%2520based%2520on%2520the%2520characteristics%2520of%2520each%2520test%2520sample.%2520Our%250Amethod%2520introduces%2520a%2520Reconstruction%2520Module%2520to%2520quantify%2520the%2520domain%2520shift%2520and%2520a%250ADynamic%2520Adaptation%2520Block%2520that%2520selectively%2520modifies%2520the%2520internal%2520features%2520of%2520a%250Apretrained%2520translation%2520model%2520to%2520mitigate%2520the%2520shift%2520without%2520compromising%2520the%250Aperformance%2520on%2520in-distribution%2520samples%2520that%2520do%2520not%2520require%2520adaptation.%2520We%250Aevaluate%2520our%2520approach%2520on%2520two%2520medical%2520image-to-image%2520translation%2520tasks%253A%2520low-dose%250ACT%2520denoising%2520and%2520T1%2520to%2520T2%2520MRI%2520translation%252C%2520showing%2520consistent%2520improvements%2520over%250Aboth%2520the%2520baseline%2520translation%2520model%2520without%2520TTA%2520and%2520prior%2520TTA%2520methods.%2520Our%250Aanalysis%2520highlights%2520the%2520limitations%2520of%2520the%2520state-of-the-art%2520that%2520uniformly%250Aapply%2520the%2520adaptation%2520to%2520both%2520out-of-distribution%2520and%2520in-distribution%2520samples%252C%250Ademonstrating%2520that%2520dynamic%252C%2520sample-specific%2520adjustment%2520offers%2520a%2520promising%2520path%250Ato%2520improve%2520model%2520resilience%2520in%2520real-world%2520scenarios.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/cosbidev/Sample-Aware_TTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation&entry.906535625=Irene%20Iele%20and%20Francesco%20Di%20Feola%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20Image-to-image%20translation%20has%20emerged%20as%20a%20powerful%20technique%20in%20medical%0Aimaging%2C%20enabling%20tasks%20such%20as%20image%20denoising%20and%20cross-modality%20conversion.%0AHowever%2C%20it%20suffers%20from%20limitations%20in%20handling%20out-of-distribution%20samples%0Awithout%20causing%20performance%20degradation.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20novel%20Test-Time%20Adaptation%20%28TTA%29%20framework%20that%20dynamically%20adjusts%20the%0Atranslation%20process%20based%20on%20the%20characteristics%20of%20each%20test%20sample.%20Our%0Amethod%20introduces%20a%20Reconstruction%20Module%20to%20quantify%20the%20domain%20shift%20and%20a%0ADynamic%20Adaptation%20Block%20that%20selectively%20modifies%20the%20internal%20features%20of%20a%0Apretrained%20translation%20model%20to%20mitigate%20the%20shift%20without%20compromising%20the%0Aperformance%20on%20in-distribution%20samples%20that%20do%20not%20require%20adaptation.%20We%0Aevaluate%20our%20approach%20on%20two%20medical%20image-to-image%20translation%20tasks%3A%20low-dose%0ACT%20denoising%20and%20T1%20to%20T2%20MRI%20translation%2C%20showing%20consistent%20improvements%20over%0Aboth%20the%20baseline%20translation%20model%20without%20TTA%20and%20prior%20TTA%20methods.%20Our%0Aanalysis%20highlights%20the%20limitations%20of%20the%20state-of-the-art%20that%20uniformly%0Aapply%20the%20adaptation%20to%20both%20out-of-distribution%20and%20in-distribution%20samples%2C%0Ademonstrating%20that%20dynamic%2C%20sample-specific%20adjustment%20offers%20a%20promising%20path%0Ato%20improve%20model%20resilience%20in%20real-world%20scenarios.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/cosbidev/Sample-Aware_TTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00766v1&entry.124074799=Read"},
{"title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory\n  Management", "author": "Ping Chen and Zhuohong Deng and Ping Li and Shuibing He and Hongzi Zhu and Yi Zheng and Zhefeng Wang and Baoxing Huai and Minyi Guo", "abstract": "  Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.\n", "link": "http://arxiv.org/abs/2508.00806v1", "date": "2025-08-01", "relevancy": 2.1377, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5367}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adacc%3A%20Adaptive%20Compression%20and%20Activation%20Checkpointing%20for%20LLM%20Memory%0A%20%20Management&body=Title%3A%20Adacc%3A%20Adaptive%20Compression%20and%20Activation%20Checkpointing%20for%20LLM%20Memory%0A%20%20Management%0AAuthor%3A%20Ping%20Chen%20and%20Zhuohong%20Deng%20and%20Ping%20Li%20and%20Shuibing%20He%20and%20Hongzi%20Zhu%20and%20Yi%20Zheng%20and%20Zhefeng%20Wang%20and%20Baoxing%20Huai%20and%20Minyi%20Guo%0AAbstract%3A%20%20%20Training%20large%20language%20models%20often%20employs%20recomputation%20to%20alleviate%0Amemory%20pressure%2C%20which%20can%20introduce%20up%20to%2030%25%20overhead%20in%20real-world%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20Adacc%2C%20a%20novel%20memory%20management%20framework%0Athat%20combines%20adaptive%20compression%20and%20activation%20checkpointing%20to%20reduce%20the%0AGPU%20memory%20footprint.%20It%20comprises%20three%20modules%3A%20%281%29%20We%20design%20layer-specific%0Acompression%20algorithms%20that%20account%20for%20outliers%20in%20LLM%20tensors%2C%20instead%20of%0Adirectly%20quantizing%20floats%20from%20FP16%20to%20INT4%2C%20to%20ensure%20model%20accuracy.%20%282%29%20We%0Apropose%20an%20optimal%20scheduling%20policy%20that%20employs%20MILP%20to%20determine%20the%20best%0Amemory%20optimization%20for%20each%20tensor.%20%283%29%20To%20accommodate%20changes%20in%20training%0Atensors%2C%20we%20introduce%20an%20adaptive%20policy%20evolution%20mechanism%20that%20adjusts%20the%0Apolicy%20during%20training%20to%20enhance%20throughput.%20Experimental%20results%20show%20that%0AAdacc%20can%20accelerate%20the%20LLM%20training%20by%201.01x%20to%201.37x%20compared%20to%0Astate-of-the-art%20frameworks%2C%20while%20maintaining%20comparable%20model%20accuracy%20to%20the%0ABaseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdacc%253A%2520Adaptive%2520Compression%2520and%2520Activation%2520Checkpointing%2520for%2520LLM%2520Memory%250A%2520%2520Management%26entry.906535625%3DPing%2520Chen%2520and%2520Zhuohong%2520Deng%2520and%2520Ping%2520Li%2520and%2520Shuibing%2520He%2520and%2520Hongzi%2520Zhu%2520and%2520Yi%2520Zheng%2520and%2520Zhefeng%2520Wang%2520and%2520Baoxing%2520Huai%2520and%2520Minyi%2520Guo%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520often%2520employs%2520recomputation%2520to%2520alleviate%250Amemory%2520pressure%252C%2520which%2520can%2520introduce%2520up%2520to%252030%2525%2520overhead%2520in%2520real-world%250Ascenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Adacc%252C%2520a%2520novel%2520memory%2520management%2520framework%250Athat%2520combines%2520adaptive%2520compression%2520and%2520activation%2520checkpointing%2520to%2520reduce%2520the%250AGPU%2520memory%2520footprint.%2520It%2520comprises%2520three%2520modules%253A%2520%25281%2529%2520We%2520design%2520layer-specific%250Acompression%2520algorithms%2520that%2520account%2520for%2520outliers%2520in%2520LLM%2520tensors%252C%2520instead%2520of%250Adirectly%2520quantizing%2520floats%2520from%2520FP16%2520to%2520INT4%252C%2520to%2520ensure%2520model%2520accuracy.%2520%25282%2529%2520We%250Apropose%2520an%2520optimal%2520scheduling%2520policy%2520that%2520employs%2520MILP%2520to%2520determine%2520the%2520best%250Amemory%2520optimization%2520for%2520each%2520tensor.%2520%25283%2529%2520To%2520accommodate%2520changes%2520in%2520training%250Atensors%252C%2520we%2520introduce%2520an%2520adaptive%2520policy%2520evolution%2520mechanism%2520that%2520adjusts%2520the%250Apolicy%2520during%2520training%2520to%2520enhance%2520throughput.%2520Experimental%2520results%2520show%2520that%250AAdacc%2520can%2520accelerate%2520the%2520LLM%2520training%2520by%25201.01x%2520to%25201.37x%2520compared%2520to%250Astate-of-the-art%2520frameworks%252C%2520while%2520maintaining%2520comparable%2520model%2520accuracy%2520to%2520the%250ABaseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adacc%3A%20Adaptive%20Compression%20and%20Activation%20Checkpointing%20for%20LLM%20Memory%0A%20%20Management&entry.906535625=Ping%20Chen%20and%20Zhuohong%20Deng%20and%20Ping%20Li%20and%20Shuibing%20He%20and%20Hongzi%20Zhu%20and%20Yi%20Zheng%20and%20Zhefeng%20Wang%20and%20Baoxing%20Huai%20and%20Minyi%20Guo&entry.1292438233=%20%20Training%20large%20language%20models%20often%20employs%20recomputation%20to%20alleviate%0Amemory%20pressure%2C%20which%20can%20introduce%20up%20to%2030%25%20overhead%20in%20real-world%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20Adacc%2C%20a%20novel%20memory%20management%20framework%0Athat%20combines%20adaptive%20compression%20and%20activation%20checkpointing%20to%20reduce%20the%0AGPU%20memory%20footprint.%20It%20comprises%20three%20modules%3A%20%281%29%20We%20design%20layer-specific%0Acompression%20algorithms%20that%20account%20for%20outliers%20in%20LLM%20tensors%2C%20instead%20of%0Adirectly%20quantizing%20floats%20from%20FP16%20to%20INT4%2C%20to%20ensure%20model%20accuracy.%20%282%29%20We%0Apropose%20an%20optimal%20scheduling%20policy%20that%20employs%20MILP%20to%20determine%20the%20best%0Amemory%20optimization%20for%20each%20tensor.%20%283%29%20To%20accommodate%20changes%20in%20training%0Atensors%2C%20we%20introduce%20an%20adaptive%20policy%20evolution%20mechanism%20that%20adjusts%20the%0Apolicy%20during%20training%20to%20enhance%20throughput.%20Experimental%20results%20show%20that%0AAdacc%20can%20accelerate%20the%20LLM%20training%20by%201.01x%20to%201.37x%20compared%20to%0Astate-of-the-art%20frameworks%2C%20while%20maintaining%20comparable%20model%20accuracy%20to%20the%0ABaseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00806v1&entry.124074799=Read"},
{"title": "LLaVA-Video: Video Instruction Tuning With Synthetic Data", "author": "Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li", "abstract": "  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n", "link": "http://arxiv.org/abs/2410.02713v3", "date": "2025-08-01", "relevancy": 2.1375, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5507}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-Video%3A%20Video%20Instruction%20Tuning%20With%20Synthetic%20Data&body=Title%3A%20LLaVA-Video%3A%20Video%20Instruction%20Tuning%20With%20Synthetic%20Data%0AAuthor%3A%20Yuanhan%20Zhang%20and%20Jinming%20Wu%20and%20Wei%20Li%20and%20Bo%20Li%20and%20Zejun%20Ma%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li%0AAbstract%3A%20%20%20The%20development%20of%20video%20large%20multimodal%20models%20%28LMMs%29%20has%20been%20hindered%20by%0Athe%20difficulty%20of%20curating%20large%20amounts%20of%20high-quality%20raw%20data%20from%20the%20web.%0ATo%20address%20this%2C%20we%20propose%20an%20alternative%20approach%20by%20creating%20a%20high-quality%0Asynthetic%20dataset%20specifically%20for%20video%20instruction-following%2C%20namely%0ALLaVA-Video-178K.%20This%20dataset%20includes%20key%20tasks%20such%20as%20detailed%20captioning%2C%0Aopen-ended%20question-answering%20%28QA%29%2C%20and%20multiple-choice%20QA.%20By%20training%20on%20this%0Adataset%2C%20in%20combination%20with%20existing%20visual%20instruction%20tuning%20data%2C%20we%0Aintroduce%20LLaVA-Video%2C%20a%20new%20video%20LMM.%20Our%20experiments%20demonstrate%20that%0ALLaVA-Video%20achieves%20strong%20performance%20across%20various%20video%20benchmarks%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20dataset.%20We%20plan%20to%20release%20the%20dataset%2C%0Aits%20generation%20pipeline%2C%20and%20the%20model%20checkpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-Video%253A%2520Video%2520Instruction%2520Tuning%2520With%2520Synthetic%2520Data%26entry.906535625%3DYuanhan%2520Zhang%2520and%2520Jinming%2520Wu%2520and%2520Wei%2520Li%2520and%2520Bo%2520Li%2520and%2520Zejun%2520Ma%2520and%2520Ziwei%2520Liu%2520and%2520Chunyuan%2520Li%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520video%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520has%2520been%2520hindered%2520by%250Athe%2520difficulty%2520of%2520curating%2520large%2520amounts%2520of%2520high-quality%2520raw%2520data%2520from%2520the%2520web.%250ATo%2520address%2520this%252C%2520we%2520propose%2520an%2520alternative%2520approach%2520by%2520creating%2520a%2520high-quality%250Asynthetic%2520dataset%2520specifically%2520for%2520video%2520instruction-following%252C%2520namely%250ALLaVA-Video-178K.%2520This%2520dataset%2520includes%2520key%2520tasks%2520such%2520as%2520detailed%2520captioning%252C%250Aopen-ended%2520question-answering%2520%2528QA%2529%252C%2520and%2520multiple-choice%2520QA.%2520By%2520training%2520on%2520this%250Adataset%252C%2520in%2520combination%2520with%2520existing%2520visual%2520instruction%2520tuning%2520data%252C%2520we%250Aintroduce%2520LLaVA-Video%252C%2520a%2520new%2520video%2520LMM.%2520Our%2520experiments%2520demonstrate%2520that%250ALLaVA-Video%2520achieves%2520strong%2520performance%2520across%2520various%2520video%2520benchmarks%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520our%2520dataset.%2520We%2520plan%2520to%2520release%2520the%2520dataset%252C%250Aits%2520generation%2520pipeline%252C%2520and%2520the%2520model%2520checkpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-Video%3A%20Video%20Instruction%20Tuning%20With%20Synthetic%20Data&entry.906535625=Yuanhan%20Zhang%20and%20Jinming%20Wu%20and%20Wei%20Li%20and%20Bo%20Li%20and%20Zejun%20Ma%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li&entry.1292438233=%20%20The%20development%20of%20video%20large%20multimodal%20models%20%28LMMs%29%20has%20been%20hindered%20by%0Athe%20difficulty%20of%20curating%20large%20amounts%20of%20high-quality%20raw%20data%20from%20the%20web.%0ATo%20address%20this%2C%20we%20propose%20an%20alternative%20approach%20by%20creating%20a%20high-quality%0Asynthetic%20dataset%20specifically%20for%20video%20instruction-following%2C%20namely%0ALLaVA-Video-178K.%20This%20dataset%20includes%20key%20tasks%20such%20as%20detailed%20captioning%2C%0Aopen-ended%20question-answering%20%28QA%29%2C%20and%20multiple-choice%20QA.%20By%20training%20on%20this%0Adataset%2C%20in%20combination%20with%20existing%20visual%20instruction%20tuning%20data%2C%20we%0Aintroduce%20LLaVA-Video%2C%20a%20new%20video%20LMM.%20Our%20experiments%20demonstrate%20that%0ALLaVA-Video%20achieves%20strong%20performance%20across%20various%20video%20benchmarks%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20dataset.%20We%20plan%20to%20release%20the%20dataset%2C%0Aits%20generation%20pipeline%2C%20and%20the%20model%20checkpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02713v3&entry.124074799=Read"},
{"title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language\n  Models", "author": "Shantanu Thorat and Andrew Caines", "abstract": "  Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.\n", "link": "http://arxiv.org/abs/2508.00619v1", "date": "2025-08-01", "relevancy": 2.1321, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5453}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5327}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DACTYL%3A%20Diverse%20Adversarial%20Corpus%20of%20Texts%20Yielded%20from%20Large%20Language%0A%20%20Models&body=Title%3A%20DACTYL%3A%20Diverse%20Adversarial%20Corpus%20of%20Texts%20Yielded%20from%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Shantanu%20Thorat%20and%20Andrew%20Caines%0AAbstract%3A%20%20%20Existing%20AIG%20%28AI-generated%29%20text%20detectors%20struggle%20in%20real-world%20settings%0Adespite%20succeeding%20in%20internal%20testing%2C%20suggesting%20that%20they%20may%20not%20be%20robust%0Aenough.%20We%20rigorously%20examine%20the%20machine-learning%20procedure%20to%20build%20these%0Adetectors%20to%20address%20this.%20Most%20current%20AIG%20text%20detection%20datasets%20focus%20on%0Azero-shot%20generations%2C%20but%20little%20work%20has%20been%20done%20on%20few-shot%20or%20one-shot%0Agenerations%2C%20where%20LLMs%20are%20given%20human%20texts%20as%20an%20example.%20In%20response%2C%20we%0Aintroduce%20the%20Diverse%20Adversarial%20Corpus%20of%20Texts%20Yielded%20from%20Language%20models%0A%28DACTYL%29%2C%20a%20challenging%20AIG%20text%20detection%20dataset%20focusing%20on%0Aone-shot/few-shot%20generations.%20We%20also%20include%20texts%20from%20domain-specific%0Acontinued-pre-trained%20%28CPT%29%20language%20models%2C%20where%20we%20fully%20train%20all%0Aparameters%20using%20a%20memory-efficient%20optimization%20approach.%20Many%20existing%20AIG%0Atext%20detectors%20struggle%20significantly%20on%20our%20dataset%2C%20indicating%20a%20potential%0Avulnerability%20to%20one-shot/few-shot%20and%20CPT-generated%20texts.%20We%20also%20train%20our%0Aown%20classifiers%20using%20two%20approaches%3A%20standard%20binary%20cross-entropy%20%28BCE%29%0Aoptimization%20and%20a%20more%20recent%20approach%2C%20deep%20X-risk%20optimization%20%28DXO%29.%20While%0ABCE-trained%20classifiers%20marginally%20outperform%20DXO%20classifiers%20on%20the%20DACTYL%0Atest%20set%2C%20the%20latter%20excels%20on%20out-of-distribution%20%28OOD%29%20texts.%20In%20our%20mock%0Adeployment%20scenario%20in%20student%20essay%20detection%20with%20an%20OOD%20student%20essay%0Adataset%2C%20the%20best%20DXO%20classifier%20outscored%20the%20best%20BCE-trained%20classifier%20by%0A50.56%20macro-F1%20score%20points%20at%20the%20lowest%20false%20positive%20rates%20for%20both.%20Our%0Aresults%20indicate%20that%20DXO%20classifiers%20generalize%20better%20without%20overfitting%20to%0Athe%20test%20set.%20Our%20experiments%20highlight%20several%20areas%20of%20improvement%20for%20AIG%0Atext%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDACTYL%253A%2520Diverse%2520Adversarial%2520Corpus%2520of%2520Texts%2520Yielded%2520from%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DShantanu%2520Thorat%2520and%2520Andrew%2520Caines%26entry.1292438233%3D%2520%2520Existing%2520AIG%2520%2528AI-generated%2529%2520text%2520detectors%2520struggle%2520in%2520real-world%2520settings%250Adespite%2520succeeding%2520in%2520internal%2520testing%252C%2520suggesting%2520that%2520they%2520may%2520not%2520be%2520robust%250Aenough.%2520We%2520rigorously%2520examine%2520the%2520machine-learning%2520procedure%2520to%2520build%2520these%250Adetectors%2520to%2520address%2520this.%2520Most%2520current%2520AIG%2520text%2520detection%2520datasets%2520focus%2520on%250Azero-shot%2520generations%252C%2520but%2520little%2520work%2520has%2520been%2520done%2520on%2520few-shot%2520or%2520one-shot%250Agenerations%252C%2520where%2520LLMs%2520are%2520given%2520human%2520texts%2520as%2520an%2520example.%2520In%2520response%252C%2520we%250Aintroduce%2520the%2520Diverse%2520Adversarial%2520Corpus%2520of%2520Texts%2520Yielded%2520from%2520Language%2520models%250A%2528DACTYL%2529%252C%2520a%2520challenging%2520AIG%2520text%2520detection%2520dataset%2520focusing%2520on%250Aone-shot/few-shot%2520generations.%2520We%2520also%2520include%2520texts%2520from%2520domain-specific%250Acontinued-pre-trained%2520%2528CPT%2529%2520language%2520models%252C%2520where%2520we%2520fully%2520train%2520all%250Aparameters%2520using%2520a%2520memory-efficient%2520optimization%2520approach.%2520Many%2520existing%2520AIG%250Atext%2520detectors%2520struggle%2520significantly%2520on%2520our%2520dataset%252C%2520indicating%2520a%2520potential%250Avulnerability%2520to%2520one-shot/few-shot%2520and%2520CPT-generated%2520texts.%2520We%2520also%2520train%2520our%250Aown%2520classifiers%2520using%2520two%2520approaches%253A%2520standard%2520binary%2520cross-entropy%2520%2528BCE%2529%250Aoptimization%2520and%2520a%2520more%2520recent%2520approach%252C%2520deep%2520X-risk%2520optimization%2520%2528DXO%2529.%2520While%250ABCE-trained%2520classifiers%2520marginally%2520outperform%2520DXO%2520classifiers%2520on%2520the%2520DACTYL%250Atest%2520set%252C%2520the%2520latter%2520excels%2520on%2520out-of-distribution%2520%2528OOD%2529%2520texts.%2520In%2520our%2520mock%250Adeployment%2520scenario%2520in%2520student%2520essay%2520detection%2520with%2520an%2520OOD%2520student%2520essay%250Adataset%252C%2520the%2520best%2520DXO%2520classifier%2520outscored%2520the%2520best%2520BCE-trained%2520classifier%2520by%250A50.56%2520macro-F1%2520score%2520points%2520at%2520the%2520lowest%2520false%2520positive%2520rates%2520for%2520both.%2520Our%250Aresults%2520indicate%2520that%2520DXO%2520classifiers%2520generalize%2520better%2520without%2520overfitting%2520to%250Athe%2520test%2520set.%2520Our%2520experiments%2520highlight%2520several%2520areas%2520of%2520improvement%2520for%2520AIG%250Atext%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DACTYL%3A%20Diverse%20Adversarial%20Corpus%20of%20Texts%20Yielded%20from%20Large%20Language%0A%20%20Models&entry.906535625=Shantanu%20Thorat%20and%20Andrew%20Caines&entry.1292438233=%20%20Existing%20AIG%20%28AI-generated%29%20text%20detectors%20struggle%20in%20real-world%20settings%0Adespite%20succeeding%20in%20internal%20testing%2C%20suggesting%20that%20they%20may%20not%20be%20robust%0Aenough.%20We%20rigorously%20examine%20the%20machine-learning%20procedure%20to%20build%20these%0Adetectors%20to%20address%20this.%20Most%20current%20AIG%20text%20detection%20datasets%20focus%20on%0Azero-shot%20generations%2C%20but%20little%20work%20has%20been%20done%20on%20few-shot%20or%20one-shot%0Agenerations%2C%20where%20LLMs%20are%20given%20human%20texts%20as%20an%20example.%20In%20response%2C%20we%0Aintroduce%20the%20Diverse%20Adversarial%20Corpus%20of%20Texts%20Yielded%20from%20Language%20models%0A%28DACTYL%29%2C%20a%20challenging%20AIG%20text%20detection%20dataset%20focusing%20on%0Aone-shot/few-shot%20generations.%20We%20also%20include%20texts%20from%20domain-specific%0Acontinued-pre-trained%20%28CPT%29%20language%20models%2C%20where%20we%20fully%20train%20all%0Aparameters%20using%20a%20memory-efficient%20optimization%20approach.%20Many%20existing%20AIG%0Atext%20detectors%20struggle%20significantly%20on%20our%20dataset%2C%20indicating%20a%20potential%0Avulnerability%20to%20one-shot/few-shot%20and%20CPT-generated%20texts.%20We%20also%20train%20our%0Aown%20classifiers%20using%20two%20approaches%3A%20standard%20binary%20cross-entropy%20%28BCE%29%0Aoptimization%20and%20a%20more%20recent%20approach%2C%20deep%20X-risk%20optimization%20%28DXO%29.%20While%0ABCE-trained%20classifiers%20marginally%20outperform%20DXO%20classifiers%20on%20the%20DACTYL%0Atest%20set%2C%20the%20latter%20excels%20on%20out-of-distribution%20%28OOD%29%20texts.%20In%20our%20mock%0Adeployment%20scenario%20in%20student%20essay%20detection%20with%20an%20OOD%20student%20essay%0Adataset%2C%20the%20best%20DXO%20classifier%20outscored%20the%20best%20BCE-trained%20classifier%20by%0A50.56%20macro-F1%20score%20points%20at%20the%20lowest%20false%20positive%20rates%20for%20both.%20Our%0Aresults%20indicate%20that%20DXO%20classifiers%20generalize%20better%20without%20overfitting%20to%0Athe%20test%20set.%20Our%20experiments%20highlight%20several%20areas%20of%20improvement%20for%20AIG%0Atext%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00619v1&entry.124074799=Read"},
{"title": "Un-mixing Test-time Adaptation under Heterogeneous Data Streams", "author": "Zixian Su and Jingwei Guo and Xi Yang and Qiufeng Wang and Kaizhu Huang", "abstract": "  Deploying deep models in real-world scenarios remains challenging due to\nsignificant performance drops under distribution shifts between training and\ndeployment environments. Test-Time Adaptation (TTA) has recently emerged as a\npromising solution, enabling on-the-fly model adaptation without access to\nsource data. However, its effectiveness degrades significantly in the presence\nof complex, mixed distribution shifts - common in practical settings - where\nmultiple latent domains coexist. Adapting under such intrinsic heterogeneity,\nespecially in unlabeled and online conditions, remains an open and\nunderexplored challenge. In this paper, we study TTA under mixed distribution\nshifts and move beyond conventional homogeneous adaptation paradigms. By\nrevisiting TTA from a frequency-domain perspective, we observe that\ndistribution heterogeneity often manifests in Fourier space - for instance,\nhigh-frequency components tend to carry domain-specific variations. This\nmotivates us to perform domain-aware separation using high-frequency texture\ncues, making diverse shift patterns more tractable. To this end, we propose\nFreDA, a novel Frequency-based Decentralized Adaptation framework that\ndecomposes globally heterogeneous data into locally homogeneous components in\nthe frequency domain. It further employs decentralized learning and\naugmentation strategies to robustly adapt under complex, evolving shifts.\nExtensive experiments across various environments (corrupted, natural, and\nmedical) demonstrate the superiority of our proposed framework over the\nstate-of-the-arts.\n", "link": "http://arxiv.org/abs/2411.15173v2", "date": "2025-08-01", "relevancy": 2.1132, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5367}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Un-mixing%20Test-time%20Adaptation%20under%20Heterogeneous%20Data%20Streams&body=Title%3A%20Un-mixing%20Test-time%20Adaptation%20under%20Heterogeneous%20Data%20Streams%0AAuthor%3A%20Zixian%20Su%20and%20Jingwei%20Guo%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20Deploying%20deep%20models%20in%20real-world%20scenarios%20remains%20challenging%20due%20to%0Asignificant%20performance%20drops%20under%20distribution%20shifts%20between%20training%20and%0Adeployment%20environments.%20Test-Time%20Adaptation%20%28TTA%29%20has%20recently%20emerged%20as%20a%0Apromising%20solution%2C%20enabling%20on-the-fly%20model%20adaptation%20without%20access%20to%0Asource%20data.%20However%2C%20its%20effectiveness%20degrades%20significantly%20in%20the%20presence%0Aof%20complex%2C%20mixed%20distribution%20shifts%20-%20common%20in%20practical%20settings%20-%20where%0Amultiple%20latent%20domains%20coexist.%20Adapting%20under%20such%20intrinsic%20heterogeneity%2C%0Aespecially%20in%20unlabeled%20and%20online%20conditions%2C%20remains%20an%20open%20and%0Aunderexplored%20challenge.%20In%20this%20paper%2C%20we%20study%20TTA%20under%20mixed%20distribution%0Ashifts%20and%20move%20beyond%20conventional%20homogeneous%20adaptation%20paradigms.%20By%0Arevisiting%20TTA%20from%20a%20frequency-domain%20perspective%2C%20we%20observe%20that%0Adistribution%20heterogeneity%20often%20manifests%20in%20Fourier%20space%20-%20for%20instance%2C%0Ahigh-frequency%20components%20tend%20to%20carry%20domain-specific%20variations.%20This%0Amotivates%20us%20to%20perform%20domain-aware%20separation%20using%20high-frequency%20texture%0Acues%2C%20making%20diverse%20shift%20patterns%20more%20tractable.%20To%20this%20end%2C%20we%20propose%0AFreDA%2C%20a%20novel%20Frequency-based%20Decentralized%20Adaptation%20framework%20that%0Adecomposes%20globally%20heterogeneous%20data%20into%20locally%20homogeneous%20components%20in%0Athe%20frequency%20domain.%20It%20further%20employs%20decentralized%20learning%20and%0Aaugmentation%20strategies%20to%20robustly%20adapt%20under%20complex%2C%20evolving%20shifts.%0AExtensive%20experiments%20across%20various%20environments%20%28corrupted%2C%20natural%2C%20and%0Amedical%29%20demonstrate%20the%20superiority%20of%20our%20proposed%20framework%20over%20the%0Astate-of-the-arts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUn-mixing%2520Test-time%2520Adaptation%2520under%2520Heterogeneous%2520Data%2520Streams%26entry.906535625%3DZixian%2520Su%2520and%2520Jingwei%2520Guo%2520and%2520Xi%2520Yang%2520and%2520Qiufeng%2520Wang%2520and%2520Kaizhu%2520Huang%26entry.1292438233%3D%2520%2520Deploying%2520deep%2520models%2520in%2520real-world%2520scenarios%2520remains%2520challenging%2520due%2520to%250Asignificant%2520performance%2520drops%2520under%2520distribution%2520shifts%2520between%2520training%2520and%250Adeployment%2520environments.%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520has%2520recently%2520emerged%2520as%2520a%250Apromising%2520solution%252C%2520enabling%2520on-the-fly%2520model%2520adaptation%2520without%2520access%2520to%250Asource%2520data.%2520However%252C%2520its%2520effectiveness%2520degrades%2520significantly%2520in%2520the%2520presence%250Aof%2520complex%252C%2520mixed%2520distribution%2520shifts%2520-%2520common%2520in%2520practical%2520settings%2520-%2520where%250Amultiple%2520latent%2520domains%2520coexist.%2520Adapting%2520under%2520such%2520intrinsic%2520heterogeneity%252C%250Aespecially%2520in%2520unlabeled%2520and%2520online%2520conditions%252C%2520remains%2520an%2520open%2520and%250Aunderexplored%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520study%2520TTA%2520under%2520mixed%2520distribution%250Ashifts%2520and%2520move%2520beyond%2520conventional%2520homogeneous%2520adaptation%2520paradigms.%2520By%250Arevisiting%2520TTA%2520from%2520a%2520frequency-domain%2520perspective%252C%2520we%2520observe%2520that%250Adistribution%2520heterogeneity%2520often%2520manifests%2520in%2520Fourier%2520space%2520-%2520for%2520instance%252C%250Ahigh-frequency%2520components%2520tend%2520to%2520carry%2520domain-specific%2520variations.%2520This%250Amotivates%2520us%2520to%2520perform%2520domain-aware%2520separation%2520using%2520high-frequency%2520texture%250Acues%252C%2520making%2520diverse%2520shift%2520patterns%2520more%2520tractable.%2520To%2520this%2520end%252C%2520we%2520propose%250AFreDA%252C%2520a%2520novel%2520Frequency-based%2520Decentralized%2520Adaptation%2520framework%2520that%250Adecomposes%2520globally%2520heterogeneous%2520data%2520into%2520locally%2520homogeneous%2520components%2520in%250Athe%2520frequency%2520domain.%2520It%2520further%2520employs%2520decentralized%2520learning%2520and%250Aaugmentation%2520strategies%2520to%2520robustly%2520adapt%2520under%2520complex%252C%2520evolving%2520shifts.%250AExtensive%2520experiments%2520across%2520various%2520environments%2520%2528corrupted%252C%2520natural%252C%2520and%250Amedical%2529%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520framework%2520over%2520the%250Astate-of-the-arts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Un-mixing%20Test-time%20Adaptation%20under%20Heterogeneous%20Data%20Streams&entry.906535625=Zixian%20Su%20and%20Jingwei%20Guo%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Kaizhu%20Huang&entry.1292438233=%20%20Deploying%20deep%20models%20in%20real-world%20scenarios%20remains%20challenging%20due%20to%0Asignificant%20performance%20drops%20under%20distribution%20shifts%20between%20training%20and%0Adeployment%20environments.%20Test-Time%20Adaptation%20%28TTA%29%20has%20recently%20emerged%20as%20a%0Apromising%20solution%2C%20enabling%20on-the-fly%20model%20adaptation%20without%20access%20to%0Asource%20data.%20However%2C%20its%20effectiveness%20degrades%20significantly%20in%20the%20presence%0Aof%20complex%2C%20mixed%20distribution%20shifts%20-%20common%20in%20practical%20settings%20-%20where%0Amultiple%20latent%20domains%20coexist.%20Adapting%20under%20such%20intrinsic%20heterogeneity%2C%0Aespecially%20in%20unlabeled%20and%20online%20conditions%2C%20remains%20an%20open%20and%0Aunderexplored%20challenge.%20In%20this%20paper%2C%20we%20study%20TTA%20under%20mixed%20distribution%0Ashifts%20and%20move%20beyond%20conventional%20homogeneous%20adaptation%20paradigms.%20By%0Arevisiting%20TTA%20from%20a%20frequency-domain%20perspective%2C%20we%20observe%20that%0Adistribution%20heterogeneity%20often%20manifests%20in%20Fourier%20space%20-%20for%20instance%2C%0Ahigh-frequency%20components%20tend%20to%20carry%20domain-specific%20variations.%20This%0Amotivates%20us%20to%20perform%20domain-aware%20separation%20using%20high-frequency%20texture%0Acues%2C%20making%20diverse%20shift%20patterns%20more%20tractable.%20To%20this%20end%2C%20we%20propose%0AFreDA%2C%20a%20novel%20Frequency-based%20Decentralized%20Adaptation%20framework%20that%0Adecomposes%20globally%20heterogeneous%20data%20into%20locally%20homogeneous%20components%20in%0Athe%20frequency%20domain.%20It%20further%20employs%20decentralized%20learning%20and%0Aaugmentation%20strategies%20to%20robustly%20adapt%20under%20complex%2C%20evolving%20shifts.%0AExtensive%20experiments%20across%20various%20environments%20%28corrupted%2C%20natural%2C%20and%0Amedical%29%20demonstrate%20the%20superiority%20of%20our%20proposed%20framework%20over%20the%0Astate-of-the-arts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15173v2&entry.124074799=Read"},
{"title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of\n  Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "author": "Prerana Ramkumar", "abstract": "  Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.\n", "link": "http://arxiv.org/abs/2508.00750v1", "date": "2025-08-01", "relevancy": 2.1089, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5206}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SU-ESRGAN%3A%20Semantic%20and%20Uncertainty-Aware%20ESRGAN%20for%20Super-Resolution%20of%0A%20%20Satellite%20and%20Drone%20Imagery%20with%20Fine-Tuning%20for%20Cross%20Domain%20Evaluation&body=Title%3A%20SU-ESRGAN%3A%20Semantic%20and%20Uncertainty-Aware%20ESRGAN%20for%20Super-Resolution%20of%0A%20%20Satellite%20and%20Drone%20Imagery%20with%20Fine-Tuning%20for%20Cross%20Domain%20Evaluation%0AAuthor%3A%20Prerana%20Ramkumar%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20achieved%20realistic%0Asuper-resolution%20%28SR%29%20of%20images%20however%2C%20they%20lack%20semantic%20consistency%20and%0Aper-pixel%20confidence%2C%20limiting%20their%20credibility%20in%20critical%20remote%20sensing%0Aapplications%20such%20as%20disaster%20response%2C%20urban%20planning%20and%20agriculture.%20This%0Apaper%20introduces%20Semantic%20and%20Uncertainty-Aware%20ESRGAN%20%28SU-ESRGAN%29%2C%20the%20first%0ASR%20framework%20designed%20for%20satellite%20imagery%20to%20integrate%20the%20ESRGAN%2C%0Asegmentation%20loss%20via%20DeepLabv3%20for%20class%20detail%20preservation%20and%20Monte%20Carlo%0Adropout%20to%20produce%20pixel-wise%20uncertainty%20maps.%20The%20SU-ESRGAN%20produces%20results%0A%28PSNR%2C%20SSIM%2C%20LPIPS%29%20comparable%20to%20the%20Baseline%20ESRGAN%20on%20aerial%20imagery.%20This%0Anovel%20model%20is%20valuable%20in%20satellite%20systems%20or%20UAVs%20that%20use%20wide%0Afield-of-view%20%28FoV%29%20cameras%2C%20trading%20off%20spatial%20resolution%20for%20coverage.%20The%0Amodular%20design%20allows%20integration%20in%20UAV%20data%20pipelines%20for%20on-board%20or%0Apost-processing%20SR%20to%20enhance%20imagery%20resulting%20due%20to%20motion%20blur%2C%20compression%0Aand%20sensor%20limitations.%20Further%2C%20the%20model%20is%20fine-tuned%20to%20evaluate%20its%0Aperformance%20on%20cross%20domain%20applications.%20The%20tests%20are%20conducted%20on%20two%20drone%0Abased%20datasets%20which%20differ%20in%20altitude%20and%20imaging%20perspective.%20Performance%0Aevaluation%20of%20the%20fine-tuned%20models%20show%20a%20stronger%20adaptation%20to%20the%20Aerial%0AMaritime%20Drone%20Dataset%2C%20whose%20imaging%20characteristics%20align%20with%20the%20training%0Adata%2C%20highlighting%20the%20importance%20of%20domain-aware%20training%20in%20SR-applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSU-ESRGAN%253A%2520Semantic%2520and%2520Uncertainty-Aware%2520ESRGAN%2520for%2520Super-Resolution%2520of%250A%2520%2520Satellite%2520and%2520Drone%2520Imagery%2520with%2520Fine-Tuning%2520for%2520Cross%2520Domain%2520Evaluation%26entry.906535625%3DPrerana%2520Ramkumar%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520achieved%2520realistic%250Asuper-resolution%2520%2528SR%2529%2520of%2520images%2520however%252C%2520they%2520lack%2520semantic%2520consistency%2520and%250Aper-pixel%2520confidence%252C%2520limiting%2520their%2520credibility%2520in%2520critical%2520remote%2520sensing%250Aapplications%2520such%2520as%2520disaster%2520response%252C%2520urban%2520planning%2520and%2520agriculture.%2520This%250Apaper%2520introduces%2520Semantic%2520and%2520Uncertainty-Aware%2520ESRGAN%2520%2528SU-ESRGAN%2529%252C%2520the%2520first%250ASR%2520framework%2520designed%2520for%2520satellite%2520imagery%2520to%2520integrate%2520the%2520ESRGAN%252C%250Asegmentation%2520loss%2520via%2520DeepLabv3%2520for%2520class%2520detail%2520preservation%2520and%2520Monte%2520Carlo%250Adropout%2520to%2520produce%2520pixel-wise%2520uncertainty%2520maps.%2520The%2520SU-ESRGAN%2520produces%2520results%250A%2528PSNR%252C%2520SSIM%252C%2520LPIPS%2529%2520comparable%2520to%2520the%2520Baseline%2520ESRGAN%2520on%2520aerial%2520imagery.%2520This%250Anovel%2520model%2520is%2520valuable%2520in%2520satellite%2520systems%2520or%2520UAVs%2520that%2520use%2520wide%250Afield-of-view%2520%2528FoV%2529%2520cameras%252C%2520trading%2520off%2520spatial%2520resolution%2520for%2520coverage.%2520The%250Amodular%2520design%2520allows%2520integration%2520in%2520UAV%2520data%2520pipelines%2520for%2520on-board%2520or%250Apost-processing%2520SR%2520to%2520enhance%2520imagery%2520resulting%2520due%2520to%2520motion%2520blur%252C%2520compression%250Aand%2520sensor%2520limitations.%2520Further%252C%2520the%2520model%2520is%2520fine-tuned%2520to%2520evaluate%2520its%250Aperformance%2520on%2520cross%2520domain%2520applications.%2520The%2520tests%2520are%2520conducted%2520on%2520two%2520drone%250Abased%2520datasets%2520which%2520differ%2520in%2520altitude%2520and%2520imaging%2520perspective.%2520Performance%250Aevaluation%2520of%2520the%2520fine-tuned%2520models%2520show%2520a%2520stronger%2520adaptation%2520to%2520the%2520Aerial%250AMaritime%2520Drone%2520Dataset%252C%2520whose%2520imaging%2520characteristics%2520align%2520with%2520the%2520training%250Adata%252C%2520highlighting%2520the%2520importance%2520of%2520domain-aware%2520training%2520in%2520SR-applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SU-ESRGAN%3A%20Semantic%20and%20Uncertainty-Aware%20ESRGAN%20for%20Super-Resolution%20of%0A%20%20Satellite%20and%20Drone%20Imagery%20with%20Fine-Tuning%20for%20Cross%20Domain%20Evaluation&entry.906535625=Prerana%20Ramkumar&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20achieved%20realistic%0Asuper-resolution%20%28SR%29%20of%20images%20however%2C%20they%20lack%20semantic%20consistency%20and%0Aper-pixel%20confidence%2C%20limiting%20their%20credibility%20in%20critical%20remote%20sensing%0Aapplications%20such%20as%20disaster%20response%2C%20urban%20planning%20and%20agriculture.%20This%0Apaper%20introduces%20Semantic%20and%20Uncertainty-Aware%20ESRGAN%20%28SU-ESRGAN%29%2C%20the%20first%0ASR%20framework%20designed%20for%20satellite%20imagery%20to%20integrate%20the%20ESRGAN%2C%0Asegmentation%20loss%20via%20DeepLabv3%20for%20class%20detail%20preservation%20and%20Monte%20Carlo%0Adropout%20to%20produce%20pixel-wise%20uncertainty%20maps.%20The%20SU-ESRGAN%20produces%20results%0A%28PSNR%2C%20SSIM%2C%20LPIPS%29%20comparable%20to%20the%20Baseline%20ESRGAN%20on%20aerial%20imagery.%20This%0Anovel%20model%20is%20valuable%20in%20satellite%20systems%20or%20UAVs%20that%20use%20wide%0Afield-of-view%20%28FoV%29%20cameras%2C%20trading%20off%20spatial%20resolution%20for%20coverage.%20The%0Amodular%20design%20allows%20integration%20in%20UAV%20data%20pipelines%20for%20on-board%20or%0Apost-processing%20SR%20to%20enhance%20imagery%20resulting%20due%20to%20motion%20blur%2C%20compression%0Aand%20sensor%20limitations.%20Further%2C%20the%20model%20is%20fine-tuned%20to%20evaluate%20its%0Aperformance%20on%20cross%20domain%20applications.%20The%20tests%20are%20conducted%20on%20two%20drone%0Abased%20datasets%20which%20differ%20in%20altitude%20and%20imaging%20perspective.%20Performance%0Aevaluation%20of%20the%20fine-tuned%20models%20show%20a%20stronger%20adaptation%20to%20the%20Aerial%0AMaritime%20Drone%20Dataset%2C%20whose%20imaging%20characteristics%20align%20with%20the%20training%0Adata%2C%20highlighting%20the%20importance%20of%20domain-aware%20training%20in%20SR-applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00750v1&entry.124074799=Read"},
{"title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image\n  systems", "author": "Mingrui Liu and Sixiao Zhang and Cheng Long", "abstract": "  Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.\n", "link": "http://arxiv.org/abs/2508.00591v1", "date": "2025-08-01", "relevancy": 2.1042, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5434}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5278}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wukong%20Framework%20for%20Not%20Safe%20For%20Work%20Detection%20in%20Text-to-Image%0A%20%20systems&body=Title%3A%20Wukong%20Framework%20for%20Not%20Safe%20For%20Work%20Detection%20in%20Text-to-Image%0A%20%20systems%0AAuthor%3A%20Mingrui%20Liu%20and%20Sixiao%20Zhang%20and%20Cheng%20Long%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20generation%20is%20a%20popular%20AI-generated%20content%20%28AIGC%29%0Atechnology%20enabling%20diverse%20and%20creative%20image%20synthesis.%20However%2C%20some%20outputs%0Amay%20contain%20Not%20Safe%20For%20Work%20%28NSFW%29%20content%20%28e.g.%2C%20violence%29%2C%20violating%0Acommunity%20guidelines.%20Detecting%20NSFW%20content%20efficiently%20and%20accurately%2C%20known%0Aas%20external%20safeguarding%2C%20is%20essential.%20Existing%20external%20safeguards%20fall%20into%0Atwo%20types%3A%20text%20filters%2C%20which%20analyze%20user%20prompts%20but%20overlook%20T2I%0Amodel-specific%20variations%20and%20are%20prone%20to%20adversarial%20attacks%3B%20and%20image%0Afilters%2C%20which%20analyze%20final%20generated%20images%20but%20are%20computationally%20costly%0Aand%20introduce%20latency.%20Diffusion%20models%2C%20the%20foundation%20of%20modern%20T2I%20systems%0Alike%20Stable%20Diffusion%2C%20generate%20images%20through%20iterative%20denoising%20using%20a%0AU-Net%20architecture%20with%20ResNet%20and%20Transformer%20blocks.%20We%20observe%20that%3A%20%281%29%0Aearly%20denoising%20steps%20define%20the%20semantic%20layout%20of%20the%20image%2C%20and%20%282%29%0Across-attention%20layers%20in%20U-Net%20are%20crucial%20for%20aligning%20text%20and%20image%0Aregions.%20Based%20on%20these%20insights%2C%20we%20propose%20Wukong%2C%20a%20transformer-based%20NSFW%0Adetection%20framework%20that%20leverages%20intermediate%20outputs%20from%20early%20denoising%0Asteps%20and%20reuses%20U-Net%27s%20pre-trained%20cross-attention%20parameters.%20Wukong%0Aoperates%20within%20the%20diffusion%20process%2C%20enabling%20early%20detection%20without%20waiting%0Afor%20full%20image%20generation.%20We%20also%20introduce%20a%20new%20dataset%20containing%20prompts%2C%0Aseeds%2C%20and%20image-specific%20NSFW%20labels%2C%20and%20evaluate%20Wukong%20on%20this%20and%20two%0Apublic%20benchmarks.%20Results%20show%20that%20Wukong%20significantly%20outperforms%0Atext-based%20safeguards%20and%20achieves%20comparable%20accuracy%20of%20image%20filters%2C%20while%0Aoffering%20much%20greater%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWukong%2520Framework%2520for%2520Not%2520Safe%2520For%2520Work%2520Detection%2520in%2520Text-to-Image%250A%2520%2520systems%26entry.906535625%3DMingrui%2520Liu%2520and%2520Sixiao%2520Zhang%2520and%2520Cheng%2520Long%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520is%2520a%2520popular%2520AI-generated%2520content%2520%2528AIGC%2529%250Atechnology%2520enabling%2520diverse%2520and%2520creative%2520image%2520synthesis.%2520However%252C%2520some%2520outputs%250Amay%2520contain%2520Not%2520Safe%2520For%2520Work%2520%2528NSFW%2529%2520content%2520%2528e.g.%252C%2520violence%2529%252C%2520violating%250Acommunity%2520guidelines.%2520Detecting%2520NSFW%2520content%2520efficiently%2520and%2520accurately%252C%2520known%250Aas%2520external%2520safeguarding%252C%2520is%2520essential.%2520Existing%2520external%2520safeguards%2520fall%2520into%250Atwo%2520types%253A%2520text%2520filters%252C%2520which%2520analyze%2520user%2520prompts%2520but%2520overlook%2520T2I%250Amodel-specific%2520variations%2520and%2520are%2520prone%2520to%2520adversarial%2520attacks%253B%2520and%2520image%250Afilters%252C%2520which%2520analyze%2520final%2520generated%2520images%2520but%2520are%2520computationally%2520costly%250Aand%2520introduce%2520latency.%2520Diffusion%2520models%252C%2520the%2520foundation%2520of%2520modern%2520T2I%2520systems%250Alike%2520Stable%2520Diffusion%252C%2520generate%2520images%2520through%2520iterative%2520denoising%2520using%2520a%250AU-Net%2520architecture%2520with%2520ResNet%2520and%2520Transformer%2520blocks.%2520We%2520observe%2520that%253A%2520%25281%2529%250Aearly%2520denoising%2520steps%2520define%2520the%2520semantic%2520layout%2520of%2520the%2520image%252C%2520and%2520%25282%2529%250Across-attention%2520layers%2520in%2520U-Net%2520are%2520crucial%2520for%2520aligning%2520text%2520and%2520image%250Aregions.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520Wukong%252C%2520a%2520transformer-based%2520NSFW%250Adetection%2520framework%2520that%2520leverages%2520intermediate%2520outputs%2520from%2520early%2520denoising%250Asteps%2520and%2520reuses%2520U-Net%2527s%2520pre-trained%2520cross-attention%2520parameters.%2520Wukong%250Aoperates%2520within%2520the%2520diffusion%2520process%252C%2520enabling%2520early%2520detection%2520without%2520waiting%250Afor%2520full%2520image%2520generation.%2520We%2520also%2520introduce%2520a%2520new%2520dataset%2520containing%2520prompts%252C%250Aseeds%252C%2520and%2520image-specific%2520NSFW%2520labels%252C%2520and%2520evaluate%2520Wukong%2520on%2520this%2520and%2520two%250Apublic%2520benchmarks.%2520Results%2520show%2520that%2520Wukong%2520significantly%2520outperforms%250Atext-based%2520safeguards%2520and%2520achieves%2520comparable%2520accuracy%2520of%2520image%2520filters%252C%2520while%250Aoffering%2520much%2520greater%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wukong%20Framework%20for%20Not%20Safe%20For%20Work%20Detection%20in%20Text-to-Image%0A%20%20systems&entry.906535625=Mingrui%20Liu%20and%20Sixiao%20Zhang%20and%20Cheng%20Long&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20generation%20is%20a%20popular%20AI-generated%20content%20%28AIGC%29%0Atechnology%20enabling%20diverse%20and%20creative%20image%20synthesis.%20However%2C%20some%20outputs%0Amay%20contain%20Not%20Safe%20For%20Work%20%28NSFW%29%20content%20%28e.g.%2C%20violence%29%2C%20violating%0Acommunity%20guidelines.%20Detecting%20NSFW%20content%20efficiently%20and%20accurately%2C%20known%0Aas%20external%20safeguarding%2C%20is%20essential.%20Existing%20external%20safeguards%20fall%20into%0Atwo%20types%3A%20text%20filters%2C%20which%20analyze%20user%20prompts%20but%20overlook%20T2I%0Amodel-specific%20variations%20and%20are%20prone%20to%20adversarial%20attacks%3B%20and%20image%0Afilters%2C%20which%20analyze%20final%20generated%20images%20but%20are%20computationally%20costly%0Aand%20introduce%20latency.%20Diffusion%20models%2C%20the%20foundation%20of%20modern%20T2I%20systems%0Alike%20Stable%20Diffusion%2C%20generate%20images%20through%20iterative%20denoising%20using%20a%0AU-Net%20architecture%20with%20ResNet%20and%20Transformer%20blocks.%20We%20observe%20that%3A%20%281%29%0Aearly%20denoising%20steps%20define%20the%20semantic%20layout%20of%20the%20image%2C%20and%20%282%29%0Across-attention%20layers%20in%20U-Net%20are%20crucial%20for%20aligning%20text%20and%20image%0Aregions.%20Based%20on%20these%20insights%2C%20we%20propose%20Wukong%2C%20a%20transformer-based%20NSFW%0Adetection%20framework%20that%20leverages%20intermediate%20outputs%20from%20early%20denoising%0Asteps%20and%20reuses%20U-Net%27s%20pre-trained%20cross-attention%20parameters.%20Wukong%0Aoperates%20within%20the%20diffusion%20process%2C%20enabling%20early%20detection%20without%20waiting%0Afor%20full%20image%20generation.%20We%20also%20introduce%20a%20new%20dataset%20containing%20prompts%2C%0Aseeds%2C%20and%20image-specific%20NSFW%20labels%2C%20and%20evaluate%20Wukong%20on%20this%20and%20two%0Apublic%20benchmarks.%20Results%20show%20that%20Wukong%20significantly%20outperforms%0Atext-based%20safeguards%20and%20achieves%20comparable%20accuracy%20of%20image%20filters%2C%20while%0Aoffering%20much%20greater%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00591v1&entry.124074799=Read"},
{"title": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded\n  Interpretations", "author": "Bradley P. Allen and Prateek Chhikara and Thomas Macaulay Ferguson and Filip Ilievski and Paul Groth", "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.\n", "link": "http://arxiv.org/abs/2507.09751v2", "date": "2025-08-01", "relevancy": 2.1018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sound%20and%20Complete%20Neurosymbolic%20Reasoning%20with%20LLM-Grounded%0A%20%20Interpretations&body=Title%3A%20Sound%20and%20Complete%20Neurosymbolic%20Reasoning%20with%20LLM-Grounded%0A%20%20Interpretations%0AAuthor%3A%20Bradley%20P.%20Allen%20and%20Prateek%20Chhikara%20and%20Thomas%20Macaulay%20Ferguson%20and%20Filip%20Ilievski%20and%20Paul%20Groth%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Anatural%20language%20understanding%20and%20generation%2C%20but%20they%20exhibit%20problems%20with%0Alogical%20consistency%20in%20the%20output%20they%20generate.%20How%20can%20we%20harness%20LLMs%27%0Abroad-coverage%20parametric%20knowledge%20in%20formal%20reasoning%20despite%20their%0Ainconsistency%3F%20We%20present%20a%20method%20for%20directly%20integrating%20an%20LLM%20into%20the%0Ainterpretation%20function%20of%20the%20formal%20semantics%20for%20a%20paraconsistent%20logic.%20We%0Aprovide%20experimental%20evidence%20for%20the%20feasibility%20of%20the%20method%20by%20evaluating%0Athe%20function%20using%20datasets%20created%20from%20several%20short-form%20factuality%0Abenchmarks.%20Unlike%20prior%20work%2C%20our%20method%20offers%20a%20theoretical%20framework%20for%0Aneurosymbolic%20reasoning%20that%20leverages%20an%20LLM%27s%20knowledge%20while%20preserving%20the%0Aunderlying%20logic%27s%20soundness%20and%20completeness%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSound%2520and%2520Complete%2520Neurosymbolic%2520Reasoning%2520with%2520LLM-Grounded%250A%2520%2520Interpretations%26entry.906535625%3DBradley%2520P.%2520Allen%2520and%2520Prateek%2520Chhikara%2520and%2520Thomas%2520Macaulay%2520Ferguson%2520and%2520Filip%2520Ilievski%2520and%2520Paul%2520Groth%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Anatural%2520language%2520understanding%2520and%2520generation%252C%2520but%2520they%2520exhibit%2520problems%2520with%250Alogical%2520consistency%2520in%2520the%2520output%2520they%2520generate.%2520How%2520can%2520we%2520harness%2520LLMs%2527%250Abroad-coverage%2520parametric%2520knowledge%2520in%2520formal%2520reasoning%2520despite%2520their%250Ainconsistency%253F%2520We%2520present%2520a%2520method%2520for%2520directly%2520integrating%2520an%2520LLM%2520into%2520the%250Ainterpretation%2520function%2520of%2520the%2520formal%2520semantics%2520for%2520a%2520paraconsistent%2520logic.%2520We%250Aprovide%2520experimental%2520evidence%2520for%2520the%2520feasibility%2520of%2520the%2520method%2520by%2520evaluating%250Athe%2520function%2520using%2520datasets%2520created%2520from%2520several%2520short-form%2520factuality%250Abenchmarks.%2520Unlike%2520prior%2520work%252C%2520our%2520method%2520offers%2520a%2520theoretical%2520framework%2520for%250Aneurosymbolic%2520reasoning%2520that%2520leverages%2520an%2520LLM%2527s%2520knowledge%2520while%2520preserving%2520the%250Aunderlying%2520logic%2527s%2520soundness%2520and%2520completeness%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sound%20and%20Complete%20Neurosymbolic%20Reasoning%20with%20LLM-Grounded%0A%20%20Interpretations&entry.906535625=Bradley%20P.%20Allen%20and%20Prateek%20Chhikara%20and%20Thomas%20Macaulay%20Ferguson%20and%20Filip%20Ilievski%20and%20Paul%20Groth&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Anatural%20language%20understanding%20and%20generation%2C%20but%20they%20exhibit%20problems%20with%0Alogical%20consistency%20in%20the%20output%20they%20generate.%20How%20can%20we%20harness%20LLMs%27%0Abroad-coverage%20parametric%20knowledge%20in%20formal%20reasoning%20despite%20their%0Ainconsistency%3F%20We%20present%20a%20method%20for%20directly%20integrating%20an%20LLM%20into%20the%0Ainterpretation%20function%20of%20the%20formal%20semantics%20for%20a%20paraconsistent%20logic.%20We%0Aprovide%20experimental%20evidence%20for%20the%20feasibility%20of%20the%20method%20by%20evaluating%0Athe%20function%20using%20datasets%20created%20from%20several%20short-form%20factuality%0Abenchmarks.%20Unlike%20prior%20work%2C%20our%20method%20offers%20a%20theoretical%20framework%20for%0Aneurosymbolic%20reasoning%20that%20leverages%20an%20LLM%27s%20knowledge%20while%20preserving%20the%0Aunderlying%20logic%27s%20soundness%20and%20completeness%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09751v2&entry.124074799=Read"},
{"title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data\n  Leveraging Declarative Facts in Earlier Training Data", "author": "Sohaib Imran and Rob Lamb and Peter M. Atkinson", "abstract": "  Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.\n", "link": "http://arxiv.org/abs/2508.00741v1", "date": "2025-08-01", "relevancy": 2.0898, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-Context%20Abduction%3A%20LLMs%20Make%20Inferences%20About%20Procedural%20Data%0A%20%20Leveraging%20Declarative%20Facts%20in%20Earlier%20Training%20Data&body=Title%3A%20Out-of-Context%20Abduction%3A%20LLMs%20Make%20Inferences%20About%20Procedural%20Data%0A%20%20Leveraging%20Declarative%20Facts%20in%20Earlier%20Training%20Data%0AAuthor%3A%20Sohaib%20Imran%20and%20Rob%20Lamb%20and%20Peter%20M.%20Atkinson%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20large%20corpora%2C%20yet%20it%20is%20unclear%0Awhether%20they%20can%20reason%20about%20the%20information%20present%20within%20their%20training%0Adata.%20We%20design%20experiments%20to%20study%20out-of-context%20abduction%20in%20LLMs%2C%20the%0Aability%20to%20infer%20the%20most%20plausible%20explanations%20for%20observations%20using%0Arelevant%20facts%20present%20in%20training%20data.%20We%20train%20treatment%20LLMs%20on%20names%20and%0Abehavior%20descriptions%20of%20fictitious%20chatbots%2C%20but%20not%20on%20examples%20of%20dialogue%0Awith%20the%20chatbots.%20We%20find%20that%20OpenAI%27s%20GPT%204o%20LLM%20can%20correctly%20infer%20at%0Aleast%20one%20chatbot%27s%20name%20after%20observing%20example%20responses%20characteristic%20of%0Athat%20chatbot.%20We%20also%20find%20that%20previously%20training%20GPT%204o%20on%20descriptions%20of%20a%0Achatbot%27s%20behavior%20allows%20it%20to%20display%20behaviors%20more%20characteristic%20of%20the%0Achatbot%20when%20iteratively%20trained%20to%20display%20such%20behaviors.%20Our%20results%20have%0Aimplications%20for%20situational%20awareness%20in%20LLMs%20and%2C%20therefore%2C%20for%20AI%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-Context%2520Abduction%253A%2520LLMs%2520Make%2520Inferences%2520About%2520Procedural%2520Data%250A%2520%2520Leveraging%2520Declarative%2520Facts%2520in%2520Earlier%2520Training%2520Data%26entry.906535625%3DSohaib%2520Imran%2520and%2520Rob%2520Lamb%2520and%2520Peter%2520M.%2520Atkinson%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520large%2520corpora%252C%2520yet%2520it%2520is%2520unclear%250Awhether%2520they%2520can%2520reason%2520about%2520the%2520information%2520present%2520within%2520their%2520training%250Adata.%2520We%2520design%2520experiments%2520to%2520study%2520out-of-context%2520abduction%2520in%2520LLMs%252C%2520the%250Aability%2520to%2520infer%2520the%2520most%2520plausible%2520explanations%2520for%2520observations%2520using%250Arelevant%2520facts%2520present%2520in%2520training%2520data.%2520We%2520train%2520treatment%2520LLMs%2520on%2520names%2520and%250Abehavior%2520descriptions%2520of%2520fictitious%2520chatbots%252C%2520but%2520not%2520on%2520examples%2520of%2520dialogue%250Awith%2520the%2520chatbots.%2520We%2520find%2520that%2520OpenAI%2527s%2520GPT%25204o%2520LLM%2520can%2520correctly%2520infer%2520at%250Aleast%2520one%2520chatbot%2527s%2520name%2520after%2520observing%2520example%2520responses%2520characteristic%2520of%250Athat%2520chatbot.%2520We%2520also%2520find%2520that%2520previously%2520training%2520GPT%25204o%2520on%2520descriptions%2520of%2520a%250Achatbot%2527s%2520behavior%2520allows%2520it%2520to%2520display%2520behaviors%2520more%2520characteristic%2520of%2520the%250Achatbot%2520when%2520iteratively%2520trained%2520to%2520display%2520such%2520behaviors.%2520Our%2520results%2520have%250Aimplications%2520for%2520situational%2520awareness%2520in%2520LLMs%2520and%252C%2520therefore%252C%2520for%2520AI%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Context%20Abduction%3A%20LLMs%20Make%20Inferences%20About%20Procedural%20Data%0A%20%20Leveraging%20Declarative%20Facts%20in%20Earlier%20Training%20Data&entry.906535625=Sohaib%20Imran%20and%20Rob%20Lamb%20and%20Peter%20M.%20Atkinson&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20trained%20on%20large%20corpora%2C%20yet%20it%20is%20unclear%0Awhether%20they%20can%20reason%20about%20the%20information%20present%20within%20their%20training%0Adata.%20We%20design%20experiments%20to%20study%20out-of-context%20abduction%20in%20LLMs%2C%20the%0Aability%20to%20infer%20the%20most%20plausible%20explanations%20for%20observations%20using%0Arelevant%20facts%20present%20in%20training%20data.%20We%20train%20treatment%20LLMs%20on%20names%20and%0Abehavior%20descriptions%20of%20fictitious%20chatbots%2C%20but%20not%20on%20examples%20of%20dialogue%0Awith%20the%20chatbots.%20We%20find%20that%20OpenAI%27s%20GPT%204o%20LLM%20can%20correctly%20infer%20at%0Aleast%20one%20chatbot%27s%20name%20after%20observing%20example%20responses%20characteristic%20of%0Athat%20chatbot.%20We%20also%20find%20that%20previously%20training%20GPT%204o%20on%20descriptions%20of%20a%0Achatbot%27s%20behavior%20allows%20it%20to%20display%20behaviors%20more%20characteristic%20of%20the%0Achatbot%20when%20iteratively%20trained%20to%20display%20such%20behaviors.%20Our%20results%20have%0Aimplications%20for%20situational%20awareness%20in%20LLMs%20and%2C%20therefore%2C%20for%20AI%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00741v1&entry.124074799=Read"},
{"title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications", "author": "Wenxuan Wang and Zizhan Ma and Meidan Ding and Shiyi Zheng and Shengyuan Liu and Jie Liu and Jiaming Ji and Wenting Chen and Xiang Li and Linlin Shen and Yixuan Yuan", "abstract": "  The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.\n", "link": "http://arxiv.org/abs/2508.00669v1", "date": "2025-08-01", "relevancy": 2.0852, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20Reasoning%20in%20the%20Era%20of%20LLMs%3A%20A%20Systematic%20Review%20of%20Enhancement%0A%20%20Techniques%20and%20Applications&body=Title%3A%20Medical%20Reasoning%20in%20the%20Era%20of%20LLMs%3A%20A%20Systematic%20Review%20of%20Enhancement%0A%20%20Techniques%20and%20Applications%0AAuthor%3A%20Wenxuan%20Wang%20and%20Zizhan%20Ma%20and%20Meidan%20Ding%20and%20Shiyi%20Zheng%20and%20Shengyuan%20Liu%20and%20Jie%20Liu%20and%20Jiaming%20Ji%20and%20Wenting%20Chen%20and%20Xiang%20Li%20and%20Linlin%20Shen%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20medicine%20has%20enabled%0Aimpressive%20capabilities%2C%20yet%20a%20critical%20gap%20remains%20in%20their%20ability%20to%20perform%0Asystematic%2C%20transparent%2C%20and%20verifiable%20reasoning%2C%20a%20cornerstone%20of%20clinical%0Apractice.%20This%20has%20catalyzed%20a%20shift%20from%20single-step%20answer%20generation%20to%20the%0Adevelopment%20of%20LLMs%20explicitly%20designed%20for%20medical%20reasoning.%20This%20paper%0Aprovides%20the%20first%20systematic%20review%20of%20this%20emerging%20field.%20We%20propose%20a%0Ataxonomy%20of%20reasoning%20enhancement%20techniques%2C%20categorized%20into%20training-time%0Astrategies%20%28e.g.%2C%20supervised%20fine-tuning%2C%20reinforcement%20learning%29%20and%20test-time%0Amechanisms%20%28e.g.%2C%20prompt%20engineering%2C%20multi-agent%20systems%29.%20We%20analyze%20how%0Athese%20techniques%20are%20applied%20across%20different%20data%20modalities%20%28text%2C%20image%2C%0Acode%29%20and%20in%20key%20clinical%20applications%20such%20as%20diagnosis%2C%20education%2C%20and%0Atreatment%20planning.%20Furthermore%2C%20we%20survey%20the%20evolution%20of%20evaluation%0Abenchmarks%20from%20simple%20accuracy%20metrics%20to%20sophisticated%20assessments%20of%0Areasoning%20quality%20and%20visual%20interpretability.%20Based%20on%20an%20analysis%20of%2060%0Aseminal%20studies%20from%202022-2025%2C%20we%20conclude%20by%20identifying%20critical%20challenges%2C%0Aincluding%20the%20faithfulness-plausibility%20gap%20and%20the%20need%20for%20native%20multimodal%0Areasoning%2C%20and%20outlining%20future%20directions%20toward%20building%20efficient%2C%20robust%2C%0Aand%20sociotechnically%20responsible%20medical%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520Reasoning%2520in%2520the%2520Era%2520of%2520LLMs%253A%2520A%2520Systematic%2520Review%2520of%2520Enhancement%250A%2520%2520Techniques%2520and%2520Applications%26entry.906535625%3DWenxuan%2520Wang%2520and%2520Zizhan%2520Ma%2520and%2520Meidan%2520Ding%2520and%2520Shiyi%2520Zheng%2520and%2520Shengyuan%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Jiaming%2520Ji%2520and%2520Wenting%2520Chen%2520and%2520Xiang%2520Li%2520and%2520Linlin%2520Shen%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520medicine%2520has%2520enabled%250Aimpressive%2520capabilities%252C%2520yet%2520a%2520critical%2520gap%2520remains%2520in%2520their%2520ability%2520to%2520perform%250Asystematic%252C%2520transparent%252C%2520and%2520verifiable%2520reasoning%252C%2520a%2520cornerstone%2520of%2520clinical%250Apractice.%2520This%2520has%2520catalyzed%2520a%2520shift%2520from%2520single-step%2520answer%2520generation%2520to%2520the%250Adevelopment%2520of%2520LLMs%2520explicitly%2520designed%2520for%2520medical%2520reasoning.%2520This%2520paper%250Aprovides%2520the%2520first%2520systematic%2520review%2520of%2520this%2520emerging%2520field.%2520We%2520propose%2520a%250Ataxonomy%2520of%2520reasoning%2520enhancement%2520techniques%252C%2520categorized%2520into%2520training-time%250Astrategies%2520%2528e.g.%252C%2520supervised%2520fine-tuning%252C%2520reinforcement%2520learning%2529%2520and%2520test-time%250Amechanisms%2520%2528e.g.%252C%2520prompt%2520engineering%252C%2520multi-agent%2520systems%2529.%2520We%2520analyze%2520how%250Athese%2520techniques%2520are%2520applied%2520across%2520different%2520data%2520modalities%2520%2528text%252C%2520image%252C%250Acode%2529%2520and%2520in%2520key%2520clinical%2520applications%2520such%2520as%2520diagnosis%252C%2520education%252C%2520and%250Atreatment%2520planning.%2520Furthermore%252C%2520we%2520survey%2520the%2520evolution%2520of%2520evaluation%250Abenchmarks%2520from%2520simple%2520accuracy%2520metrics%2520to%2520sophisticated%2520assessments%2520of%250Areasoning%2520quality%2520and%2520visual%2520interpretability.%2520Based%2520on%2520an%2520analysis%2520of%252060%250Aseminal%2520studies%2520from%25202022-2025%252C%2520we%2520conclude%2520by%2520identifying%2520critical%2520challenges%252C%250Aincluding%2520the%2520faithfulness-plausibility%2520gap%2520and%2520the%2520need%2520for%2520native%2520multimodal%250Areasoning%252C%2520and%2520outlining%2520future%2520directions%2520toward%2520building%2520efficient%252C%2520robust%252C%250Aand%2520sociotechnically%2520responsible%2520medical%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Reasoning%20in%20the%20Era%20of%20LLMs%3A%20A%20Systematic%20Review%20of%20Enhancement%0A%20%20Techniques%20and%20Applications&entry.906535625=Wenxuan%20Wang%20and%20Zizhan%20Ma%20and%20Meidan%20Ding%20and%20Shiyi%20Zheng%20and%20Shengyuan%20Liu%20and%20Jie%20Liu%20and%20Jiaming%20Ji%20and%20Wenting%20Chen%20and%20Xiang%20Li%20and%20Linlin%20Shen%20and%20Yixuan%20Yuan&entry.1292438233=%20%20The%20proliferation%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20medicine%20has%20enabled%0Aimpressive%20capabilities%2C%20yet%20a%20critical%20gap%20remains%20in%20their%20ability%20to%20perform%0Asystematic%2C%20transparent%2C%20and%20verifiable%20reasoning%2C%20a%20cornerstone%20of%20clinical%0Apractice.%20This%20has%20catalyzed%20a%20shift%20from%20single-step%20answer%20generation%20to%20the%0Adevelopment%20of%20LLMs%20explicitly%20designed%20for%20medical%20reasoning.%20This%20paper%0Aprovides%20the%20first%20systematic%20review%20of%20this%20emerging%20field.%20We%20propose%20a%0Ataxonomy%20of%20reasoning%20enhancement%20techniques%2C%20categorized%20into%20training-time%0Astrategies%20%28e.g.%2C%20supervised%20fine-tuning%2C%20reinforcement%20learning%29%20and%20test-time%0Amechanisms%20%28e.g.%2C%20prompt%20engineering%2C%20multi-agent%20systems%29.%20We%20analyze%20how%0Athese%20techniques%20are%20applied%20across%20different%20data%20modalities%20%28text%2C%20image%2C%0Acode%29%20and%20in%20key%20clinical%20applications%20such%20as%20diagnosis%2C%20education%2C%20and%0Atreatment%20planning.%20Furthermore%2C%20we%20survey%20the%20evolution%20of%20evaluation%0Abenchmarks%20from%20simple%20accuracy%20metrics%20to%20sophisticated%20assessments%20of%0Areasoning%20quality%20and%20visual%20interpretability.%20Based%20on%20an%20analysis%20of%2060%0Aseminal%20studies%20from%202022-2025%2C%20we%20conclude%20by%20identifying%20critical%20challenges%2C%0Aincluding%20the%20faithfulness-plausibility%20gap%20and%20the%20need%20for%20native%20multimodal%0Areasoning%2C%20and%20outlining%20future%20directions%20toward%20building%20efficient%2C%20robust%2C%0Aand%20sociotechnically%20responsible%20medical%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00669v1&entry.124074799=Read"},
{"title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence", "author": "Huan-ang Gao and Jiayi Geng and Wenyue Hua and Mengkang Hu and Xinzhe Juan and Hongzhang Liu and Shilong Liu and Jiahao Qiu and Xuan Qi and Yiran Wu and Hongru Wang and Han Xiao and Yuhang Zhou and Shaokun Zhang and Jiayi Zhang and Jinyu Xiang and Yixiong Fang and Qiwen Zhao and Dongrui Liu and Qihan Ren and Cheng Qian and Zhenhailong Wang and Minda Hu and Huazheng Wang and Qingyun Wu and Heng Ji and Mengdi Wang", "abstract": "  Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.\n", "link": "http://arxiv.org/abs/2507.21046v3", "date": "2025-08-01", "relevancy": 2.0711, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5337}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5195}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&body=Title%3A%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence%0AAuthor%3A%20Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenhailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21046v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Self-Evolving%2520Agents%253A%2520On%2520Path%2520to%2520Artificial%2520Super%250A%2520%2520Intelligence%26entry.906535625%3DHuan-ang%2520Gao%2520and%2520Jiayi%2520Geng%2520and%2520Wenyue%2520Hua%2520and%2520Mengkang%2520Hu%2520and%2520Xinzhe%2520Juan%2520and%2520Hongzhang%2520Liu%2520and%2520Shilong%2520Liu%2520and%2520Jiahao%2520Qiu%2520and%2520Xuan%2520Qi%2520and%2520Yiran%2520Wu%2520and%2520Hongru%2520Wang%2520and%2520Han%2520Xiao%2520and%2520Yuhang%2520Zhou%2520and%2520Shaokun%2520Zhang%2520and%2520Jiayi%2520Zhang%2520and%2520Jinyu%2520Xiang%2520and%2520Yixiong%2520Fang%2520and%2520Qiwen%2520Zhao%2520and%2520Dongrui%2520Liu%2520and%2520Qihan%2520Ren%2520and%2520Cheng%2520Qian%2520and%2520Zhenhailong%2520Wang%2520and%2520Minda%2520Hu%2520and%2520Huazheng%2520Wang%2520and%2520Qingyun%2520Wu%2520and%2520Heng%2520Ji%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520but%2520remain%250Afundamentally%2520static%252C%2520unable%2520to%2520adapt%2520their%2520internal%2520parameters%2520to%2520novel%2520tasks%252C%250Aevolving%2520knowledge%2520domains%252C%2520or%2520dynamic%2520interaction%2520contexts.%2520As%2520LLMs%2520are%250Aincreasingly%2520deployed%2520in%2520open-ended%252C%2520interactive%2520environments%252C%2520this%2520static%250Anature%2520has%2520become%2520a%2520critical%2520bottleneck%252C%2520necessitating%2520agents%2520that%2520can%250Aadaptively%2520reason%252C%2520act%252C%2520and%2520evolve%2520in%2520real%2520time.%2520This%2520paradigm%2520shift%2520--%2520from%250Ascaling%2520static%2520models%2520to%2520developing%2520self-evolving%2520agents%2520--%2520has%2520sparked%2520growing%250Ainterest%2520in%2520architectures%2520and%2520methods%2520enabling%2520continual%2520learning%2520and%250Aadaptation%2520from%2520data%252C%2520interactions%252C%2520and%2520experiences.%2520This%2520survey%2520provides%2520the%250Afirst%2520systematic%2520and%2520comprehensive%2520review%2520of%2520self-evolving%2520agents%252C%2520organized%250Aaround%2520three%2520foundational%2520dimensions%2520--%2520what%2520to%2520evolve%252C%2520when%2520to%2520evolve%252C%2520and%2520how%250Ato%2520evolve.%2520We%2520examine%2520evolutionary%2520mechanisms%2520across%2520agent%2520components%2520%2528e.g.%252C%250Amodels%252C%2520memory%252C%2520tools%252C%2520architecture%2529%252C%2520categorize%2520adaptation%2520methods%2520by%2520stages%250A%2528e.g.%252C%2520intra-test-time%252C%2520inter-test-time%2529%252C%2520and%2520analyze%2520the%2520algorithmic%2520and%250Aarchitectural%2520designs%2520that%2520guide%2520evolutionary%2520adaptation%2520%2528e.g.%252C%2520scalar%2520rewards%252C%250Atextual%2520feedback%252C%2520single-agent%2520and%2520multi-agent%2520systems%2529.%2520Additionally%252C%2520we%250Aanalyze%2520evaluation%2520metrics%2520and%2520benchmarks%2520tailored%2520for%2520self-evolving%2520agents%252C%250Ahighlight%2520applications%2520in%2520domains%2520such%2520as%2520coding%252C%2520education%252C%2520and%2520healthcare%252C%250Aand%2520identify%2520critical%2520challenges%2520and%2520research%2520directions%2520in%2520safety%252C%250Ascalability%252C%2520and%2520co-evolutionary%2520dynamics.%2520By%2520providing%2520a%2520structured%2520framework%250Afor%2520understanding%2520and%2520designing%2520self-evolving%2520agents%252C%2520this%2520survey%2520establishes%2520a%250Aroadmap%2520for%2520advancing%2520adaptive%2520agentic%2520systems%2520in%2520both%2520research%2520and%2520real-world%250Adeployments%252C%2520ultimately%2520shedding%2520lights%2520to%2520pave%2520the%2520way%2520for%2520the%2520realization%2520of%250AArtificial%2520Super%2520Intelligence%2520%2528ASI%2529%252C%2520where%2520agents%2520evolve%2520autonomously%252C%250Aperforming%2520at%2520or%2520beyond%2520human-level%2520intelligence%2520across%2520a%2520wide%2520array%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21046v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&entry.906535625=Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenhailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21046v3&entry.124074799=Read"},
{"title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun\n  Handling in Large Language Models", "author": "Xushuo Tang and Yi Ding and Zhengyi Yang and Yin Chen and Yongrui Gu and Wenke Yang and Mingchen Ju and Xin Cao and Yongfei Liu and Wenjie Zhang", "abstract": "  Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.\n", "link": "http://arxiv.org/abs/2508.00788v1", "date": "2025-08-01", "relevancy": 2.0633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20They%20Understand%20Them%3F%20An%20Updated%20Evaluation%20on%20Nonbinary%20Pronoun%0A%20%20Handling%20in%20Large%20Language%20Models&body=Title%3A%20Do%20They%20Understand%20Them%3F%20An%20Updated%20Evaluation%20on%20Nonbinary%20Pronoun%0A%20%20Handling%20in%20Large%20Language%20Models%0AAuthor%3A%20Xushuo%20Tang%20and%20Yi%20Ding%20and%20Zhengyi%20Yang%20and%20Yin%20Chen%20and%20Yongrui%20Gu%20and%20Wenke%20Yang%20and%20Mingchen%20Ju%20and%20Xin%20Cao%20and%20Yongfei%20Liu%20and%20Wenjie%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%20contexts%0Awhere%20fairness%20and%20inclusivity%20are%20critical.%20Pronoun%20usage%2C%20especially%0Aconcerning%20gender-neutral%20and%20neopronouns%2C%20remains%20a%20key%20challenge%20for%0Aresponsible%20AI.%20Prior%20work%2C%20such%20as%20the%20MISGENDERED%20benchmark%2C%20revealed%0Asignificant%20limitations%20in%20earlier%20LLMs%27%20handling%20of%20inclusive%20pronouns%2C%20but%0Awas%20constrained%20to%20outdated%20models%20and%20limited%20evaluations.%20In%20this%20study%2C%20we%0Aintroduce%20MISGENDERED%2B%2C%20an%20extended%20and%20updated%20benchmark%20for%20evaluating%20LLMs%27%0Apronoun%20fidelity.%20We%20benchmark%20five%20representative%20LLMs%2C%20GPT-4o%2C%20Claude%204%2C%0ADeepSeek-V3%2C%20Qwen%20Turbo%2C%20and%20Qwen2.5%2C%20across%20zero-shot%2C%20few-shot%2C%20and%20gender%0Aidentity%20inference.%20Our%20results%20show%20notable%20improvements%20compared%20with%0Aprevious%20studies%2C%20especially%20in%20binary%20and%20gender-neutral%20pronoun%20accuracy.%0AHowever%2C%20accuracy%20on%20neopronouns%20and%20reverse%20inference%20tasks%20remains%0Ainconsistent%2C%20underscoring%20persistent%20gaps%20in%20identity-sensitive%20reasoning.%20We%0Adiscuss%20implications%2C%20model-specific%20observations%2C%20and%20avenues%20for%20future%0Ainclusive%20AI%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520They%2520Understand%2520Them%253F%2520An%2520Updated%2520Evaluation%2520on%2520Nonbinary%2520Pronoun%250A%2520%2520Handling%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXushuo%2520Tang%2520and%2520Yi%2520Ding%2520and%2520Zhengyi%2520Yang%2520and%2520Yin%2520Chen%2520and%2520Yongrui%2520Gu%2520and%2520Wenke%2520Yang%2520and%2520Mingchen%2520Ju%2520and%2520Xin%2520Cao%2520and%2520Yongfei%2520Liu%2520and%2520Wenjie%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520sensitive%2520contexts%250Awhere%2520fairness%2520and%2520inclusivity%2520are%2520critical.%2520Pronoun%2520usage%252C%2520especially%250Aconcerning%2520gender-neutral%2520and%2520neopronouns%252C%2520remains%2520a%2520key%2520challenge%2520for%250Aresponsible%2520AI.%2520Prior%2520work%252C%2520such%2520as%2520the%2520MISGENDERED%2520benchmark%252C%2520revealed%250Asignificant%2520limitations%2520in%2520earlier%2520LLMs%2527%2520handling%2520of%2520inclusive%2520pronouns%252C%2520but%250Awas%2520constrained%2520to%2520outdated%2520models%2520and%2520limited%2520evaluations.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520MISGENDERED%252B%252C%2520an%2520extended%2520and%2520updated%2520benchmark%2520for%2520evaluating%2520LLMs%2527%250Apronoun%2520fidelity.%2520We%2520benchmark%2520five%2520representative%2520LLMs%252C%2520GPT-4o%252C%2520Claude%25204%252C%250ADeepSeek-V3%252C%2520Qwen%2520Turbo%252C%2520and%2520Qwen2.5%252C%2520across%2520zero-shot%252C%2520few-shot%252C%2520and%2520gender%250Aidentity%2520inference.%2520Our%2520results%2520show%2520notable%2520improvements%2520compared%2520with%250Aprevious%2520studies%252C%2520especially%2520in%2520binary%2520and%2520gender-neutral%2520pronoun%2520accuracy.%250AHowever%252C%2520accuracy%2520on%2520neopronouns%2520and%2520reverse%2520inference%2520tasks%2520remains%250Ainconsistent%252C%2520underscoring%2520persistent%2520gaps%2520in%2520identity-sensitive%2520reasoning.%2520We%250Adiscuss%2520implications%252C%2520model-specific%2520observations%252C%2520and%2520avenues%2520for%2520future%250Ainclusive%2520AI%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20They%20Understand%20Them%3F%20An%20Updated%20Evaluation%20on%20Nonbinary%20Pronoun%0A%20%20Handling%20in%20Large%20Language%20Models&entry.906535625=Xushuo%20Tang%20and%20Yi%20Ding%20and%20Zhengyi%20Yang%20and%20Yin%20Chen%20and%20Yongrui%20Gu%20and%20Wenke%20Yang%20and%20Mingchen%20Ju%20and%20Xin%20Cao%20and%20Yongfei%20Liu%20and%20Wenjie%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20sensitive%20contexts%0Awhere%20fairness%20and%20inclusivity%20are%20critical.%20Pronoun%20usage%2C%20especially%0Aconcerning%20gender-neutral%20and%20neopronouns%2C%20remains%20a%20key%20challenge%20for%0Aresponsible%20AI.%20Prior%20work%2C%20such%20as%20the%20MISGENDERED%20benchmark%2C%20revealed%0Asignificant%20limitations%20in%20earlier%20LLMs%27%20handling%20of%20inclusive%20pronouns%2C%20but%0Awas%20constrained%20to%20outdated%20models%20and%20limited%20evaluations.%20In%20this%20study%2C%20we%0Aintroduce%20MISGENDERED%2B%2C%20an%20extended%20and%20updated%20benchmark%20for%20evaluating%20LLMs%27%0Apronoun%20fidelity.%20We%20benchmark%20five%20representative%20LLMs%2C%20GPT-4o%2C%20Claude%204%2C%0ADeepSeek-V3%2C%20Qwen%20Turbo%2C%20and%20Qwen2.5%2C%20across%20zero-shot%2C%20few-shot%2C%20and%20gender%0Aidentity%20inference.%20Our%20results%20show%20notable%20improvements%20compared%20with%0Aprevious%20studies%2C%20especially%20in%20binary%20and%20gender-neutral%20pronoun%20accuracy.%0AHowever%2C%20accuracy%20on%20neopronouns%20and%20reverse%20inference%20tasks%20remains%0Ainconsistent%2C%20underscoring%20persistent%20gaps%20in%20identity-sensitive%20reasoning.%20We%0Adiscuss%20implications%2C%20model-specific%20observations%2C%20and%20avenues%20for%20future%0Ainclusive%20AI%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00788v1&entry.124074799=Read"},
{"title": "Federated Cross-Training Learners for Robust Generalization under Data\n  Heterogeneity", "author": "Zhuang Qi and Lei Meng and Ruohan Zhang and Yu Wang and Xin Qi and Xiangxu Meng and Han Yu and Qiang Yang", "abstract": "  Federated learning benefits from cross-training strategies, which enables\nmodels to train on data from distinct sources to improve generalization\ncapability. However, due to inherent differences in data distributions, the\noptimization goals of local models remain misaligned, and this mismatch\ncontinues to manifest as feature space heterogeneity even after cross-training.\nWe argue that knowledge distillation from the personalized view preserves\nclient-specific characteristics and expands the local knowledge base, while\ndistillation from the global view provides consistent semantic anchors that\nfacilitate feature alignment across clients. To achieve this goal, this paper\npresents a cross-training scheme, termed FedCT, includes three main modules,\nwhere the consistency-aware knowledge broadcasting module aims to optimize\nmodel assignment strategies, which enhances collaborative advantages between\nclients and achieves an efficient federated learning process. The multi-view\nknowledge-guided representation learning module leverages fused prototypical\nknowledge from both global and local views to enhance the preservation of local\nknowledge before and after model exchange, as well as to ensure consistency\nbetween local and global knowledge. The mixup-based feature augmentation module\naggregates rich information to further increase the diversity of feature\nspaces, which enables the model to better discriminate complex samples.\nExtensive experiments were conducted on four datasets in terms of performance\ncomparison, ablation study, in-depth analysis and case study. The results\ndemonstrated that FedCT alleviates knowledge forgetting from both local and\nglobal views, which enables it outperform state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.20046v2", "date": "2025-08-01", "relevancy": 2.0522, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Cross-Training%20Learners%20for%20Robust%20Generalization%20under%20Data%0A%20%20Heterogeneity&body=Title%3A%20Federated%20Cross-Training%20Learners%20for%20Robust%20Generalization%20under%20Data%0A%20%20Heterogeneity%0AAuthor%3A%20Zhuang%20Qi%20and%20Lei%20Meng%20and%20Ruohan%20Zhang%20and%20Yu%20Wang%20and%20Xin%20Qi%20and%20Xiangxu%20Meng%20and%20Han%20Yu%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20Federated%20learning%20benefits%20from%20cross-training%20strategies%2C%20which%20enables%0Amodels%20to%20train%20on%20data%20from%20distinct%20sources%20to%20improve%20generalization%0Acapability.%20However%2C%20due%20to%20inherent%20differences%20in%20data%20distributions%2C%20the%0Aoptimization%20goals%20of%20local%20models%20remain%20misaligned%2C%20and%20this%20mismatch%0Acontinues%20to%20manifest%20as%20feature%20space%20heterogeneity%20even%20after%20cross-training.%0AWe%20argue%20that%20knowledge%20distillation%20from%20the%20personalized%20view%20preserves%0Aclient-specific%20characteristics%20and%20expands%20the%20local%20knowledge%20base%2C%20while%0Adistillation%20from%20the%20global%20view%20provides%20consistent%20semantic%20anchors%20that%0Afacilitate%20feature%20alignment%20across%20clients.%20To%20achieve%20this%20goal%2C%20this%20paper%0Apresents%20a%20cross-training%20scheme%2C%20termed%20FedCT%2C%20includes%20three%20main%20modules%2C%0Awhere%20the%20consistency-aware%20knowledge%20broadcasting%20module%20aims%20to%20optimize%0Amodel%20assignment%20strategies%2C%20which%20enhances%20collaborative%20advantages%20between%0Aclients%20and%20achieves%20an%20efficient%20federated%20learning%20process.%20The%20multi-view%0Aknowledge-guided%20representation%20learning%20module%20leverages%20fused%20prototypical%0Aknowledge%20from%20both%20global%20and%20local%20views%20to%20enhance%20the%20preservation%20of%20local%0Aknowledge%20before%20and%20after%20model%20exchange%2C%20as%20well%20as%20to%20ensure%20consistency%0Abetween%20local%20and%20global%20knowledge.%20The%20mixup-based%20feature%20augmentation%20module%0Aaggregates%20rich%20information%20to%20further%20increase%20the%20diversity%20of%20feature%0Aspaces%2C%20which%20enables%20the%20model%20to%20better%20discriminate%20complex%20samples.%0AExtensive%20experiments%20were%20conducted%20on%20four%20datasets%20in%20terms%20of%20performance%0Acomparison%2C%20ablation%20study%2C%20in-depth%20analysis%20and%20case%20study.%20The%20results%0Ademonstrated%20that%20FedCT%20alleviates%20knowledge%20forgetting%20from%20both%20local%20and%0Aglobal%20views%2C%20which%20enables%20it%20outperform%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Cross-Training%2520Learners%2520for%2520Robust%2520Generalization%2520under%2520Data%250A%2520%2520Heterogeneity%26entry.906535625%3DZhuang%2520Qi%2520and%2520Lei%2520Meng%2520and%2520Ruohan%2520Zhang%2520and%2520Yu%2520Wang%2520and%2520Xin%2520Qi%2520and%2520Xiangxu%2520Meng%2520and%2520Han%2520Yu%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520benefits%2520from%2520cross-training%2520strategies%252C%2520which%2520enables%250Amodels%2520to%2520train%2520on%2520data%2520from%2520distinct%2520sources%2520to%2520improve%2520generalization%250Acapability.%2520However%252C%2520due%2520to%2520inherent%2520differences%2520in%2520data%2520distributions%252C%2520the%250Aoptimization%2520goals%2520of%2520local%2520models%2520remain%2520misaligned%252C%2520and%2520this%2520mismatch%250Acontinues%2520to%2520manifest%2520as%2520feature%2520space%2520heterogeneity%2520even%2520after%2520cross-training.%250AWe%2520argue%2520that%2520knowledge%2520distillation%2520from%2520the%2520personalized%2520view%2520preserves%250Aclient-specific%2520characteristics%2520and%2520expands%2520the%2520local%2520knowledge%2520base%252C%2520while%250Adistillation%2520from%2520the%2520global%2520view%2520provides%2520consistent%2520semantic%2520anchors%2520that%250Afacilitate%2520feature%2520alignment%2520across%2520clients.%2520To%2520achieve%2520this%2520goal%252C%2520this%2520paper%250Apresents%2520a%2520cross-training%2520scheme%252C%2520termed%2520FedCT%252C%2520includes%2520three%2520main%2520modules%252C%250Awhere%2520the%2520consistency-aware%2520knowledge%2520broadcasting%2520module%2520aims%2520to%2520optimize%250Amodel%2520assignment%2520strategies%252C%2520which%2520enhances%2520collaborative%2520advantages%2520between%250Aclients%2520and%2520achieves%2520an%2520efficient%2520federated%2520learning%2520process.%2520The%2520multi-view%250Aknowledge-guided%2520representation%2520learning%2520module%2520leverages%2520fused%2520prototypical%250Aknowledge%2520from%2520both%2520global%2520and%2520local%2520views%2520to%2520enhance%2520the%2520preservation%2520of%2520local%250Aknowledge%2520before%2520and%2520after%2520model%2520exchange%252C%2520as%2520well%2520as%2520to%2520ensure%2520consistency%250Abetween%2520local%2520and%2520global%2520knowledge.%2520The%2520mixup-based%2520feature%2520augmentation%2520module%250Aaggregates%2520rich%2520information%2520to%2520further%2520increase%2520the%2520diversity%2520of%2520feature%250Aspaces%252C%2520which%2520enables%2520the%2520model%2520to%2520better%2520discriminate%2520complex%2520samples.%250AExtensive%2520experiments%2520were%2520conducted%2520on%2520four%2520datasets%2520in%2520terms%2520of%2520performance%250Acomparison%252C%2520ablation%2520study%252C%2520in-depth%2520analysis%2520and%2520case%2520study.%2520The%2520results%250Ademonstrated%2520that%2520FedCT%2520alleviates%2520knowledge%2520forgetting%2520from%2520both%2520local%2520and%250Aglobal%2520views%252C%2520which%2520enables%2520it%2520outperform%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Cross-Training%20Learners%20for%20Robust%20Generalization%20under%20Data%0A%20%20Heterogeneity&entry.906535625=Zhuang%20Qi%20and%20Lei%20Meng%20and%20Ruohan%20Zhang%20and%20Yu%20Wang%20and%20Xin%20Qi%20and%20Xiangxu%20Meng%20and%20Han%20Yu%20and%20Qiang%20Yang&entry.1292438233=%20%20Federated%20learning%20benefits%20from%20cross-training%20strategies%2C%20which%20enables%0Amodels%20to%20train%20on%20data%20from%20distinct%20sources%20to%20improve%20generalization%0Acapability.%20However%2C%20due%20to%20inherent%20differences%20in%20data%20distributions%2C%20the%0Aoptimization%20goals%20of%20local%20models%20remain%20misaligned%2C%20and%20this%20mismatch%0Acontinues%20to%20manifest%20as%20feature%20space%20heterogeneity%20even%20after%20cross-training.%0AWe%20argue%20that%20knowledge%20distillation%20from%20the%20personalized%20view%20preserves%0Aclient-specific%20characteristics%20and%20expands%20the%20local%20knowledge%20base%2C%20while%0Adistillation%20from%20the%20global%20view%20provides%20consistent%20semantic%20anchors%20that%0Afacilitate%20feature%20alignment%20across%20clients.%20To%20achieve%20this%20goal%2C%20this%20paper%0Apresents%20a%20cross-training%20scheme%2C%20termed%20FedCT%2C%20includes%20three%20main%20modules%2C%0Awhere%20the%20consistency-aware%20knowledge%20broadcasting%20module%20aims%20to%20optimize%0Amodel%20assignment%20strategies%2C%20which%20enhances%20collaborative%20advantages%20between%0Aclients%20and%20achieves%20an%20efficient%20federated%20learning%20process.%20The%20multi-view%0Aknowledge-guided%20representation%20learning%20module%20leverages%20fused%20prototypical%0Aknowledge%20from%20both%20global%20and%20local%20views%20to%20enhance%20the%20preservation%20of%20local%0Aknowledge%20before%20and%20after%20model%20exchange%2C%20as%20well%20as%20to%20ensure%20consistency%0Abetween%20local%20and%20global%20knowledge.%20The%20mixup-based%20feature%20augmentation%20module%0Aaggregates%20rich%20information%20to%20further%20increase%20the%20diversity%20of%20feature%0Aspaces%2C%20which%20enables%20the%20model%20to%20better%20discriminate%20complex%20samples.%0AExtensive%20experiments%20were%20conducted%20on%20four%20datasets%20in%20terms%20of%20performance%0Acomparison%2C%20ablation%20study%2C%20in-depth%20analysis%20and%20case%20study.%20The%20results%0Ademonstrated%20that%20FedCT%20alleviates%20knowledge%20forgetting%20from%20both%20local%20and%0Aglobal%20views%2C%20which%20enables%20it%20outperform%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20046v2&entry.124074799=Read"},
{"title": "Model Stock: All we need is just a few fine-tuned models", "author": "Dong-Hwan Jang and Sangdoo Yun and Dongyoon Han", "abstract": "  This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.\n", "link": "http://arxiv.org/abs/2403.19522v2", "date": "2025-08-01", "relevancy": 2.0493, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5216}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5067}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models&body=Title%3A%20Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models%0AAuthor%3A%20Dong-Hwan%20Jang%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20efficient%20fine-tuning%20method%20for%20large%20pre-trained%0Amodels%2C%20offering%20strong%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Aperformance.%20Breaking%20away%20from%20traditional%20practices%20that%20need%20a%20multitude%20of%0Afine-tuned%20models%20for%20averaging%2C%20our%20approach%20employs%20significantly%20fewer%0Amodels%20to%20achieve%20final%20weights%20yet%20yield%20superior%20accuracy.%20Drawing%20from%20key%0Ainsights%20in%20the%20weight%20space%20of%20fine-tuned%20weights%2C%20we%20uncover%20a%20strong%20link%0Abetween%20the%20performance%20and%20proximity%20to%20the%20center%20of%20weight%20space.%20Based%20on%0Athis%2C%20we%20introduce%20a%20method%20that%20approximates%20a%20center-close%20weight%20using%20only%0Atwo%20fine-tuned%20models%2C%20applicable%20during%20or%20after%20training.%20Our%20innovative%0Alayer-wise%20weight%20averaging%20technique%20surpasses%20state-of-the-art%20model%20methods%0Asuch%20as%20Model%20Soup%2C%20utilizing%20only%20two%20fine-tuned%20models.%20This%20strategy%20can%20be%0Aaptly%20coined%20Model%20Stock%2C%20highlighting%20its%20reliance%20on%20selecting%20a%20minimal%0Anumber%20of%20models%20to%20draw%20a%20more%20optimized-averaged%20model.%20We%20demonstrate%20the%0Aefficacy%20of%20Model%20Stock%20with%20fine-tuned%20models%20based%20upon%20pre-trained%20CLIP%0Aarchitectures%2C%20achieving%20remarkable%20performance%20on%20both%20ID%20and%20OOD%20tasks%20on%20the%0Astandard%20benchmarks%2C%20all%20while%20barely%20bringing%20extra%20computational%20demands.%20Our%0Acode%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/naver-ai/model-stock.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Stock%253A%2520All%2520we%2520need%2520is%2520just%2520a%2520few%2520fine-tuned%2520models%26entry.906535625%3DDong-Hwan%2520Jang%2520and%2520Sangdoo%2520Yun%2520and%2520Dongyoon%2520Han%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520efficient%2520fine-tuning%2520method%2520for%2520large%2520pre-trained%250Amodels%252C%2520offering%2520strong%2520in-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%250Aperformance.%2520Breaking%2520away%2520from%2520traditional%2520practices%2520that%2520need%2520a%2520multitude%2520of%250Afine-tuned%2520models%2520for%2520averaging%252C%2520our%2520approach%2520employs%2520significantly%2520fewer%250Amodels%2520to%2520achieve%2520final%2520weights%2520yet%2520yield%2520superior%2520accuracy.%2520Drawing%2520from%2520key%250Ainsights%2520in%2520the%2520weight%2520space%2520of%2520fine-tuned%2520weights%252C%2520we%2520uncover%2520a%2520strong%2520link%250Abetween%2520the%2520performance%2520and%2520proximity%2520to%2520the%2520center%2520of%2520weight%2520space.%2520Based%2520on%250Athis%252C%2520we%2520introduce%2520a%2520method%2520that%2520approximates%2520a%2520center-close%2520weight%2520using%2520only%250Atwo%2520fine-tuned%2520models%252C%2520applicable%2520during%2520or%2520after%2520training.%2520Our%2520innovative%250Alayer-wise%2520weight%2520averaging%2520technique%2520surpasses%2520state-of-the-art%2520model%2520methods%250Asuch%2520as%2520Model%2520Soup%252C%2520utilizing%2520only%2520two%2520fine-tuned%2520models.%2520This%2520strategy%2520can%2520be%250Aaptly%2520coined%2520Model%2520Stock%252C%2520highlighting%2520its%2520reliance%2520on%2520selecting%2520a%2520minimal%250Anumber%2520of%2520models%2520to%2520draw%2520a%2520more%2520optimized-averaged%2520model.%2520We%2520demonstrate%2520the%250Aefficacy%2520of%2520Model%2520Stock%2520with%2520fine-tuned%2520models%2520based%2520upon%2520pre-trained%2520CLIP%250Aarchitectures%252C%2520achieving%2520remarkable%2520performance%2520on%2520both%2520ID%2520and%2520OOD%2520tasks%2520on%2520the%250Astandard%2520benchmarks%252C%2520all%2520while%2520barely%2520bringing%2520extra%2520computational%2520demands.%2520Our%250Acode%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/naver-ai/model-stock.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models&entry.906535625=Dong-Hwan%20Jang%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han&entry.1292438233=%20%20This%20paper%20introduces%20an%20efficient%20fine-tuning%20method%20for%20large%20pre-trained%0Amodels%2C%20offering%20strong%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Aperformance.%20Breaking%20away%20from%20traditional%20practices%20that%20need%20a%20multitude%20of%0Afine-tuned%20models%20for%20averaging%2C%20our%20approach%20employs%20significantly%20fewer%0Amodels%20to%20achieve%20final%20weights%20yet%20yield%20superior%20accuracy.%20Drawing%20from%20key%0Ainsights%20in%20the%20weight%20space%20of%20fine-tuned%20weights%2C%20we%20uncover%20a%20strong%20link%0Abetween%20the%20performance%20and%20proximity%20to%20the%20center%20of%20weight%20space.%20Based%20on%0Athis%2C%20we%20introduce%20a%20method%20that%20approximates%20a%20center-close%20weight%20using%20only%0Atwo%20fine-tuned%20models%2C%20applicable%20during%20or%20after%20training.%20Our%20innovative%0Alayer-wise%20weight%20averaging%20technique%20surpasses%20state-of-the-art%20model%20methods%0Asuch%20as%20Model%20Soup%2C%20utilizing%20only%20two%20fine-tuned%20models.%20This%20strategy%20can%20be%0Aaptly%20coined%20Model%20Stock%2C%20highlighting%20its%20reliance%20on%20selecting%20a%20minimal%0Anumber%20of%20models%20to%20draw%20a%20more%20optimized-averaged%20model.%20We%20demonstrate%20the%0Aefficacy%20of%20Model%20Stock%20with%20fine-tuned%20models%20based%20upon%20pre-trained%20CLIP%0Aarchitectures%2C%20achieving%20remarkable%20performance%20on%20both%20ID%20and%20OOD%20tasks%20on%20the%0Astandard%20benchmarks%2C%20all%20while%20barely%20bringing%20extra%20computational%20demands.%20Our%0Acode%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/naver-ai/model-stock.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19522v2&entry.124074799=Read"},
{"title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "author": "Zihan Wang and Samira Ebrahimi Kahou and Narges Armanfard", "abstract": "  Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.\n", "link": "http://arxiv.org/abs/2508.00777v1", "date": "2025-08-01", "relevancy": 2.0468, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Anomaly%20Detection%20with%20Dual-Branch%20Prompt%20Learning&body=Title%3A%20Zero-Shot%20Anomaly%20Detection%20with%20Dual-Branch%20Prompt%20Learning%0AAuthor%3A%20Zihan%20Wang%20and%20Samira%20Ebrahimi%20Kahou%20and%20Narges%20Armanfard%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20enables%20identifying%20and%20localizing%20defects%0Ain%20unseen%20categories%20by%20relying%20solely%20on%20generalizable%20features%20rather%20than%0Arequiring%20any%20labeled%20examples%20of%20anomalies.%20However%2C%20existing%20ZSAD%20methods%2C%0Awhether%20using%20fixed%20or%20learned%20prompts%2C%20struggle%20under%20domain%20shifts%20because%0Atheir%20training%20data%20are%20derived%20from%20limited%20training%20domains%20and%20fail%20to%0Ageneralize%20to%20new%20distributions.%20In%20this%20paper%2C%20we%20introduce%20PILOT%2C%20a%20framework%0Adesigned%20to%20overcome%20these%20challenges%20through%20two%20key%20innovations%3A%20%281%29%20a%20novel%0Adual-branch%20prompt%20learning%20mechanism%20that%20dynamically%20integrates%20a%20pool%20of%0Alearnable%20prompts%20with%20structured%20semantic%20attributes%2C%20enabling%20the%20model%20to%0Aadaptively%20weight%20the%20most%20relevant%20anomaly%20cues%20for%20each%20input%20image%3B%20and%20%282%29%0Aa%20label-free%20test-time%20adaptation%20strategy%20that%20updates%20the%20learnable%20prompt%0Aparameters%20using%20high-confidence%20pseudo-labels%20from%20unlabeled%20test%20data.%0AExtensive%20experiments%20on%2013%20industrial%20and%20medical%20benchmarks%20demonstrate%20that%0APILOT%20achieves%20state-of-the-art%20performance%20in%20both%20anomaly%20detection%20and%0Alocalization%20under%20domain%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Anomaly%2520Detection%2520with%2520Dual-Branch%2520Prompt%2520Learning%26entry.906535625%3DZihan%2520Wang%2520and%2520Samira%2520Ebrahimi%2520Kahou%2520and%2520Narges%2520Armanfard%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520enables%2520identifying%2520and%2520localizing%2520defects%250Ain%2520unseen%2520categories%2520by%2520relying%2520solely%2520on%2520generalizable%2520features%2520rather%2520than%250Arequiring%2520any%2520labeled%2520examples%2520of%2520anomalies.%2520However%252C%2520existing%2520ZSAD%2520methods%252C%250Awhether%2520using%2520fixed%2520or%2520learned%2520prompts%252C%2520struggle%2520under%2520domain%2520shifts%2520because%250Atheir%2520training%2520data%2520are%2520derived%2520from%2520limited%2520training%2520domains%2520and%2520fail%2520to%250Ageneralize%2520to%2520new%2520distributions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PILOT%252C%2520a%2520framework%250Adesigned%2520to%2520overcome%2520these%2520challenges%2520through%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520novel%250Adual-branch%2520prompt%2520learning%2520mechanism%2520that%2520dynamically%2520integrates%2520a%2520pool%2520of%250Alearnable%2520prompts%2520with%2520structured%2520semantic%2520attributes%252C%2520enabling%2520the%2520model%2520to%250Aadaptively%2520weight%2520the%2520most%2520relevant%2520anomaly%2520cues%2520for%2520each%2520input%2520image%253B%2520and%2520%25282%2529%250Aa%2520label-free%2520test-time%2520adaptation%2520strategy%2520that%2520updates%2520the%2520learnable%2520prompt%250Aparameters%2520using%2520high-confidence%2520pseudo-labels%2520from%2520unlabeled%2520test%2520data.%250AExtensive%2520experiments%2520on%252013%2520industrial%2520and%2520medical%2520benchmarks%2520demonstrate%2520that%250APILOT%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520anomaly%2520detection%2520and%250Alocalization%2520under%2520domain%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Anomaly%20Detection%20with%20Dual-Branch%20Prompt%20Learning&entry.906535625=Zihan%20Wang%20and%20Samira%20Ebrahimi%20Kahou%20and%20Narges%20Armanfard&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20enables%20identifying%20and%20localizing%20defects%0Ain%20unseen%20categories%20by%20relying%20solely%20on%20generalizable%20features%20rather%20than%0Arequiring%20any%20labeled%20examples%20of%20anomalies.%20However%2C%20existing%20ZSAD%20methods%2C%0Awhether%20using%20fixed%20or%20learned%20prompts%2C%20struggle%20under%20domain%20shifts%20because%0Atheir%20training%20data%20are%20derived%20from%20limited%20training%20domains%20and%20fail%20to%0Ageneralize%20to%20new%20distributions.%20In%20this%20paper%2C%20we%20introduce%20PILOT%2C%20a%20framework%0Adesigned%20to%20overcome%20these%20challenges%20through%20two%20key%20innovations%3A%20%281%29%20a%20novel%0Adual-branch%20prompt%20learning%20mechanism%20that%20dynamically%20integrates%20a%20pool%20of%0Alearnable%20prompts%20with%20structured%20semantic%20attributes%2C%20enabling%20the%20model%20to%0Aadaptively%20weight%20the%20most%20relevant%20anomaly%20cues%20for%20each%20input%20image%3B%20and%20%282%29%0Aa%20label-free%20test-time%20adaptation%20strategy%20that%20updates%20the%20learnable%20prompt%0Aparameters%20using%20high-confidence%20pseudo-labels%20from%20unlabeled%20test%20data.%0AExtensive%20experiments%20on%2013%20industrial%20and%20medical%20benchmarks%20demonstrate%20that%0APILOT%20achieves%20state-of-the-art%20performance%20in%20both%20anomaly%20detection%20and%0Alocalization%20under%20domain%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00777v1&entry.124074799=Read"},
{"title": "Sampling-enabled scalable manifold learning unveils discriminative\n  cluster structure of high-dimensional data", "author": "Dehua Peng and Zhipeng Gui and Wenzhang Wei and Fa Li and Jie Gui and Huayi Wu and Jianya Gong", "abstract": "  As a pivotal branch of machine learning, manifold learning uncovers the\nintrinsic low-dimensional structure within complex nonlinear manifolds in\nhigh-dimensional space for visualization, classification, clustering, and\ngaining key insights. Although existing techniques have achieved remarkable\nsuccesses, they suffer from extensive distortions of cluster structure, which\nhinders the understanding of underlying patterns. Scalability issues also limit\ntheir applicability for handling large-scale data. We hence propose a\nsampling-based Scalable manifold learning technique that enables Uniform and\nDiscriminative Embedding, namely SUDE, for large-scale and high-dimensional\ndata. It starts by seeking a set of landmarks to construct the low-dimensional\nskeleton of the entire data, and then incorporates the non-landmarks into the\nlearned space based on the constrained locally linear embedding (CLLE). We\nempirically validated the effectiveness of SUDE on synthetic datasets and\nreal-world benchmarks, and applied it to analyze single-cell data and detect\nanomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage\nin scalability with respect to data size and embedding dimension, and has\npromising performance in cluster separation, integrity, and global structure\npreservation. The experiments also demonstrate notable robustness in embedding\nquality as the sampling rate decreases.\n", "link": "http://arxiv.org/abs/2401.01100v4", "date": "2025-08-01", "relevancy": 2.0409, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5233}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5053}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-enabled%20scalable%20manifold%20learning%20unveils%20discriminative%0A%20%20cluster%20structure%20of%20high-dimensional%20data&body=Title%3A%20Sampling-enabled%20scalable%20manifold%20learning%20unveils%20discriminative%0A%20%20cluster%20structure%20of%20high-dimensional%20data%0AAuthor%3A%20Dehua%20Peng%20and%20Zhipeng%20Gui%20and%20Wenzhang%20Wei%20and%20Fa%20Li%20and%20Jie%20Gui%20and%20Huayi%20Wu%20and%20Jianya%20Gong%0AAbstract%3A%20%20%20As%20a%20pivotal%20branch%20of%20machine%20learning%2C%20manifold%20learning%20uncovers%20the%0Aintrinsic%20low-dimensional%20structure%20within%20complex%20nonlinear%20manifolds%20in%0Ahigh-dimensional%20space%20for%20visualization%2C%20classification%2C%20clustering%2C%20and%0Againing%20key%20insights.%20Although%20existing%20techniques%20have%20achieved%20remarkable%0Asuccesses%2C%20they%20suffer%20from%20extensive%20distortions%20of%20cluster%20structure%2C%20which%0Ahinders%20the%20understanding%20of%20underlying%20patterns.%20Scalability%20issues%20also%20limit%0Atheir%20applicability%20for%20handling%20large-scale%20data.%20We%20hence%20propose%20a%0Asampling-based%20Scalable%20manifold%20learning%20technique%20that%20enables%20Uniform%20and%0ADiscriminative%20Embedding%2C%20namely%20SUDE%2C%20for%20large-scale%20and%20high-dimensional%0Adata.%20It%20starts%20by%20seeking%20a%20set%20of%20landmarks%20to%20construct%20the%20low-dimensional%0Askeleton%20of%20the%20entire%20data%2C%20and%20then%20incorporates%20the%20non-landmarks%20into%20the%0Alearned%20space%20based%20on%20the%20constrained%20locally%20linear%20embedding%20%28CLLE%29.%20We%0Aempirically%20validated%20the%20effectiveness%20of%20SUDE%20on%20synthetic%20datasets%20and%0Areal-world%20benchmarks%2C%20and%20applied%20it%20to%20analyze%20single-cell%20data%20and%20detect%0Aanomalies%20in%20electrocardiogram%20%28ECG%29%20signals.%20SUDE%20exhibits%20distinct%20advantage%0Ain%20scalability%20with%20respect%20to%20data%20size%20and%20embedding%20dimension%2C%20and%20has%0Apromising%20performance%20in%20cluster%20separation%2C%20integrity%2C%20and%20global%20structure%0Apreservation.%20The%20experiments%20also%20demonstrate%20notable%20robustness%20in%20embedding%0Aquality%20as%20the%20sampling%20rate%20decreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01100v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-enabled%2520scalable%2520manifold%2520learning%2520unveils%2520discriminative%250A%2520%2520cluster%2520structure%2520of%2520high-dimensional%2520data%26entry.906535625%3DDehua%2520Peng%2520and%2520Zhipeng%2520Gui%2520and%2520Wenzhang%2520Wei%2520and%2520Fa%2520Li%2520and%2520Jie%2520Gui%2520and%2520Huayi%2520Wu%2520and%2520Jianya%2520Gong%26entry.1292438233%3D%2520%2520As%2520a%2520pivotal%2520branch%2520of%2520machine%2520learning%252C%2520manifold%2520learning%2520uncovers%2520the%250Aintrinsic%2520low-dimensional%2520structure%2520within%2520complex%2520nonlinear%2520manifolds%2520in%250Ahigh-dimensional%2520space%2520for%2520visualization%252C%2520classification%252C%2520clustering%252C%2520and%250Againing%2520key%2520insights.%2520Although%2520existing%2520techniques%2520have%2520achieved%2520remarkable%250Asuccesses%252C%2520they%2520suffer%2520from%2520extensive%2520distortions%2520of%2520cluster%2520structure%252C%2520which%250Ahinders%2520the%2520understanding%2520of%2520underlying%2520patterns.%2520Scalability%2520issues%2520also%2520limit%250Atheir%2520applicability%2520for%2520handling%2520large-scale%2520data.%2520We%2520hence%2520propose%2520a%250Asampling-based%2520Scalable%2520manifold%2520learning%2520technique%2520that%2520enables%2520Uniform%2520and%250ADiscriminative%2520Embedding%252C%2520namely%2520SUDE%252C%2520for%2520large-scale%2520and%2520high-dimensional%250Adata.%2520It%2520starts%2520by%2520seeking%2520a%2520set%2520of%2520landmarks%2520to%2520construct%2520the%2520low-dimensional%250Askeleton%2520of%2520the%2520entire%2520data%252C%2520and%2520then%2520incorporates%2520the%2520non-landmarks%2520into%2520the%250Alearned%2520space%2520based%2520on%2520the%2520constrained%2520locally%2520linear%2520embedding%2520%2528CLLE%2529.%2520We%250Aempirically%2520validated%2520the%2520effectiveness%2520of%2520SUDE%2520on%2520synthetic%2520datasets%2520and%250Areal-world%2520benchmarks%252C%2520and%2520applied%2520it%2520to%2520analyze%2520single-cell%2520data%2520and%2520detect%250Aanomalies%2520in%2520electrocardiogram%2520%2528ECG%2529%2520signals.%2520SUDE%2520exhibits%2520distinct%2520advantage%250Ain%2520scalability%2520with%2520respect%2520to%2520data%2520size%2520and%2520embedding%2520dimension%252C%2520and%2520has%250Apromising%2520performance%2520in%2520cluster%2520separation%252C%2520integrity%252C%2520and%2520global%2520structure%250Apreservation.%2520The%2520experiments%2520also%2520demonstrate%2520notable%2520robustness%2520in%2520embedding%250Aquality%2520as%2520the%2520sampling%2520rate%2520decreases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01100v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-enabled%20scalable%20manifold%20learning%20unveils%20discriminative%0A%20%20cluster%20structure%20of%20high-dimensional%20data&entry.906535625=Dehua%20Peng%20and%20Zhipeng%20Gui%20and%20Wenzhang%20Wei%20and%20Fa%20Li%20and%20Jie%20Gui%20and%20Huayi%20Wu%20and%20Jianya%20Gong&entry.1292438233=%20%20As%20a%20pivotal%20branch%20of%20machine%20learning%2C%20manifold%20learning%20uncovers%20the%0Aintrinsic%20low-dimensional%20structure%20within%20complex%20nonlinear%20manifolds%20in%0Ahigh-dimensional%20space%20for%20visualization%2C%20classification%2C%20clustering%2C%20and%0Againing%20key%20insights.%20Although%20existing%20techniques%20have%20achieved%20remarkable%0Asuccesses%2C%20they%20suffer%20from%20extensive%20distortions%20of%20cluster%20structure%2C%20which%0Ahinders%20the%20understanding%20of%20underlying%20patterns.%20Scalability%20issues%20also%20limit%0Atheir%20applicability%20for%20handling%20large-scale%20data.%20We%20hence%20propose%20a%0Asampling-based%20Scalable%20manifold%20learning%20technique%20that%20enables%20Uniform%20and%0ADiscriminative%20Embedding%2C%20namely%20SUDE%2C%20for%20large-scale%20and%20high-dimensional%0Adata.%20It%20starts%20by%20seeking%20a%20set%20of%20landmarks%20to%20construct%20the%20low-dimensional%0Askeleton%20of%20the%20entire%20data%2C%20and%20then%20incorporates%20the%20non-landmarks%20into%20the%0Alearned%20space%20based%20on%20the%20constrained%20locally%20linear%20embedding%20%28CLLE%29.%20We%0Aempirically%20validated%20the%20effectiveness%20of%20SUDE%20on%20synthetic%20datasets%20and%0Areal-world%20benchmarks%2C%20and%20applied%20it%20to%20analyze%20single-cell%20data%20and%20detect%0Aanomalies%20in%20electrocardiogram%20%28ECG%29%20signals.%20SUDE%20exhibits%20distinct%20advantage%0Ain%20scalability%20with%20respect%20to%20data%20size%20and%20embedding%20dimension%2C%20and%20has%0Apromising%20performance%20in%20cluster%20separation%2C%20integrity%2C%20and%20global%20structure%0Apreservation.%20The%20experiments%20also%20demonstrate%20notable%20robustness%20in%20embedding%0Aquality%20as%20the%20sampling%20rate%20decreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01100v4&entry.124074799=Read"},
{"title": "FakeIDet: Exploring Patches for Privacy-Preserving Fake ID Detection", "author": "Javier Mu\u00f1oz-Haro and Ruben Tolosana and Ruben Vera-Rodriguez and Aythami Morales and Julian Fierrez", "abstract": "  Verifying the authenticity of identity documents (IDs) has become a critical\nchallenge for real-life applications such as digital banking, crypto-exchanges,\nrenting, etc. This study focuses on the topic of fake ID detection, covering\nseveral limitations in the field. In particular, there are no publicly\navailable data from real IDs for proper research in this area, and most\npublished studies rely on proprietary internal databases that are not available\nfor privacy reasons. In order to advance this critical challenge of real data\nscarcity that makes it so difficult to advance the technology of machine\nlearning-based fake ID detection, we introduce a new patch-based methodology\nthat trades off privacy and performance, and propose a novel patch-wise\napproach for privacy-aware fake ID detection: FakeIDet. In our experiments, we\nexplore: i) two levels of anonymization for an ID (i.e., fully- and\npseudo-anonymized), and ii) different patch size configurations, varying the\namount of sensitive data visible in the patch image. State-of-the-art methods,\nsuch as vision transformers and foundation models, are considered as backbones.\nOur results show that, on an unseen database (DLC-2021), our proposal for fake\nID detection achieves 13.91% and 0% EERs at the patch and the whole ID level,\nshowing a good generalization to other databases. In addition to the path-based\nmethodology introduced and the new FakeIDet method based on it, another key\ncontribution of our article is the release of the first publicly available\ndatabase that contains 48,400 patches from real and fake IDs, called\nFakeIDet-db, together with the experimental framework.\n", "link": "http://arxiv.org/abs/2504.07761v2", "date": "2025-08-01", "relevancy": 2.039, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5416}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5022}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeIDet%3A%20Exploring%20Patches%20for%20Privacy-Preserving%20Fake%20ID%20Detection&body=Title%3A%20FakeIDet%3A%20Exploring%20Patches%20for%20Privacy-Preserving%20Fake%20ID%20Detection%0AAuthor%3A%20Javier%20Mu%C3%B1oz-Haro%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%0AAbstract%3A%20%20%20Verifying%20the%20authenticity%20of%20identity%20documents%20%28IDs%29%20has%20become%20a%20critical%0Achallenge%20for%20real-life%20applications%20such%20as%20digital%20banking%2C%20crypto-exchanges%2C%0Arenting%2C%20etc.%20This%20study%20focuses%20on%20the%20topic%20of%20fake%20ID%20detection%2C%20covering%0Aseveral%20limitations%20in%20the%20field.%20In%20particular%2C%20there%20are%20no%20publicly%0Aavailable%20data%20from%20real%20IDs%20for%20proper%20research%20in%20this%20area%2C%20and%20most%0Apublished%20studies%20rely%20on%20proprietary%20internal%20databases%20that%20are%20not%20available%0Afor%20privacy%20reasons.%20In%20order%20to%20advance%20this%20critical%20challenge%20of%20real%20data%0Ascarcity%20that%20makes%20it%20so%20difficult%20to%20advance%20the%20technology%20of%20machine%0Alearning-based%20fake%20ID%20detection%2C%20we%20introduce%20a%20new%20patch-based%20methodology%0Athat%20trades%20off%20privacy%20and%20performance%2C%20and%20propose%20a%20novel%20patch-wise%0Aapproach%20for%20privacy-aware%20fake%20ID%20detection%3A%20FakeIDet.%20In%20our%20experiments%2C%20we%0Aexplore%3A%20i%29%20two%20levels%20of%20anonymization%20for%20an%20ID%20%28i.e.%2C%20fully-%20and%0Apseudo-anonymized%29%2C%20and%20ii%29%20different%20patch%20size%20configurations%2C%20varying%20the%0Aamount%20of%20sensitive%20data%20visible%20in%20the%20patch%20image.%20State-of-the-art%20methods%2C%0Asuch%20as%20vision%20transformers%20and%20foundation%20models%2C%20are%20considered%20as%20backbones.%0AOur%20results%20show%20that%2C%20on%20an%20unseen%20database%20%28DLC-2021%29%2C%20our%20proposal%20for%20fake%0AID%20detection%20achieves%2013.91%25%20and%200%25%20EERs%20at%20the%20patch%20and%20the%20whole%20ID%20level%2C%0Ashowing%20a%20good%20generalization%20to%20other%20databases.%20In%20addition%20to%20the%20path-based%0Amethodology%20introduced%20and%20the%20new%20FakeIDet%20method%20based%20on%20it%2C%20another%20key%0Acontribution%20of%20our%20article%20is%20the%20release%20of%20the%20first%20publicly%20available%0Adatabase%20that%20contains%2048%2C400%20patches%20from%20real%20and%20fake%20IDs%2C%20called%0AFakeIDet-db%2C%20together%20with%20the%20experimental%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeIDet%253A%2520Exploring%2520Patches%2520for%2520Privacy-Preserving%2520Fake%2520ID%2520Detection%26entry.906535625%3DJavier%2520Mu%25C3%25B1oz-Haro%2520and%2520Ruben%2520Tolosana%2520and%2520Ruben%2520Vera-Rodriguez%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%26entry.1292438233%3D%2520%2520Verifying%2520the%2520authenticity%2520of%2520identity%2520documents%2520%2528IDs%2529%2520has%2520become%2520a%2520critical%250Achallenge%2520for%2520real-life%2520applications%2520such%2520as%2520digital%2520banking%252C%2520crypto-exchanges%252C%250Arenting%252C%2520etc.%2520This%2520study%2520focuses%2520on%2520the%2520topic%2520of%2520fake%2520ID%2520detection%252C%2520covering%250Aseveral%2520limitations%2520in%2520the%2520field.%2520In%2520particular%252C%2520there%2520are%2520no%2520publicly%250Aavailable%2520data%2520from%2520real%2520IDs%2520for%2520proper%2520research%2520in%2520this%2520area%252C%2520and%2520most%250Apublished%2520studies%2520rely%2520on%2520proprietary%2520internal%2520databases%2520that%2520are%2520not%2520available%250Afor%2520privacy%2520reasons.%2520In%2520order%2520to%2520advance%2520this%2520critical%2520challenge%2520of%2520real%2520data%250Ascarcity%2520that%2520makes%2520it%2520so%2520difficult%2520to%2520advance%2520the%2520technology%2520of%2520machine%250Alearning-based%2520fake%2520ID%2520detection%252C%2520we%2520introduce%2520a%2520new%2520patch-based%2520methodology%250Athat%2520trades%2520off%2520privacy%2520and%2520performance%252C%2520and%2520propose%2520a%2520novel%2520patch-wise%250Aapproach%2520for%2520privacy-aware%2520fake%2520ID%2520detection%253A%2520FakeIDet.%2520In%2520our%2520experiments%252C%2520we%250Aexplore%253A%2520i%2529%2520two%2520levels%2520of%2520anonymization%2520for%2520an%2520ID%2520%2528i.e.%252C%2520fully-%2520and%250Apseudo-anonymized%2529%252C%2520and%2520ii%2529%2520different%2520patch%2520size%2520configurations%252C%2520varying%2520the%250Aamount%2520of%2520sensitive%2520data%2520visible%2520in%2520the%2520patch%2520image.%2520State-of-the-art%2520methods%252C%250Asuch%2520as%2520vision%2520transformers%2520and%2520foundation%2520models%252C%2520are%2520considered%2520as%2520backbones.%250AOur%2520results%2520show%2520that%252C%2520on%2520an%2520unseen%2520database%2520%2528DLC-2021%2529%252C%2520our%2520proposal%2520for%2520fake%250AID%2520detection%2520achieves%252013.91%2525%2520and%25200%2525%2520EERs%2520at%2520the%2520patch%2520and%2520the%2520whole%2520ID%2520level%252C%250Ashowing%2520a%2520good%2520generalization%2520to%2520other%2520databases.%2520In%2520addition%2520to%2520the%2520path-based%250Amethodology%2520introduced%2520and%2520the%2520new%2520FakeIDet%2520method%2520based%2520on%2520it%252C%2520another%2520key%250Acontribution%2520of%2520our%2520article%2520is%2520the%2520release%2520of%2520the%2520first%2520publicly%2520available%250Adatabase%2520that%2520contains%252048%252C400%2520patches%2520from%2520real%2520and%2520fake%2520IDs%252C%2520called%250AFakeIDet-db%252C%2520together%2520with%2520the%2520experimental%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeIDet%3A%20Exploring%20Patches%20for%20Privacy-Preserving%20Fake%20ID%20Detection&entry.906535625=Javier%20Mu%C3%B1oz-Haro%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Aythami%20Morales%20and%20Julian%20Fierrez&entry.1292438233=%20%20Verifying%20the%20authenticity%20of%20identity%20documents%20%28IDs%29%20has%20become%20a%20critical%0Achallenge%20for%20real-life%20applications%20such%20as%20digital%20banking%2C%20crypto-exchanges%2C%0Arenting%2C%20etc.%20This%20study%20focuses%20on%20the%20topic%20of%20fake%20ID%20detection%2C%20covering%0Aseveral%20limitations%20in%20the%20field.%20In%20particular%2C%20there%20are%20no%20publicly%0Aavailable%20data%20from%20real%20IDs%20for%20proper%20research%20in%20this%20area%2C%20and%20most%0Apublished%20studies%20rely%20on%20proprietary%20internal%20databases%20that%20are%20not%20available%0Afor%20privacy%20reasons.%20In%20order%20to%20advance%20this%20critical%20challenge%20of%20real%20data%0Ascarcity%20that%20makes%20it%20so%20difficult%20to%20advance%20the%20technology%20of%20machine%0Alearning-based%20fake%20ID%20detection%2C%20we%20introduce%20a%20new%20patch-based%20methodology%0Athat%20trades%20off%20privacy%20and%20performance%2C%20and%20propose%20a%20novel%20patch-wise%0Aapproach%20for%20privacy-aware%20fake%20ID%20detection%3A%20FakeIDet.%20In%20our%20experiments%2C%20we%0Aexplore%3A%20i%29%20two%20levels%20of%20anonymization%20for%20an%20ID%20%28i.e.%2C%20fully-%20and%0Apseudo-anonymized%29%2C%20and%20ii%29%20different%20patch%20size%20configurations%2C%20varying%20the%0Aamount%20of%20sensitive%20data%20visible%20in%20the%20patch%20image.%20State-of-the-art%20methods%2C%0Asuch%20as%20vision%20transformers%20and%20foundation%20models%2C%20are%20considered%20as%20backbones.%0AOur%20results%20show%20that%2C%20on%20an%20unseen%20database%20%28DLC-2021%29%2C%20our%20proposal%20for%20fake%0AID%20detection%20achieves%2013.91%25%20and%200%25%20EERs%20at%20the%20patch%20and%20the%20whole%20ID%20level%2C%0Ashowing%20a%20good%20generalization%20to%20other%20databases.%20In%20addition%20to%20the%20path-based%0Amethodology%20introduced%20and%20the%20new%20FakeIDet%20method%20based%20on%20it%2C%20another%20key%0Acontribution%20of%20our%20article%20is%20the%20release%20of%20the%20first%20publicly%20available%0Adatabase%20that%20contains%2048%2C400%20patches%20from%20real%20and%20fake%20IDs%2C%20called%0AFakeIDet-db%2C%20together%20with%20the%20experimental%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07761v2&entry.124074799=Read"},
{"title": "Evaluating Angle and Amplitude Encoding Strategies for Variational\n  Quantum Machine Learning: their impact on model's accuracy", "author": "Antonio Tudisco and Andrea Marchesin and Maurizio Zamboni and Mariagrazia Graziano and Giovanna Turvani", "abstract": "  Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.\n", "link": "http://arxiv.org/abs/2508.00768v1", "date": "2025-08-01", "relevancy": 2.0186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Angle%20and%20Amplitude%20Encoding%20Strategies%20for%20Variational%0A%20%20Quantum%20Machine%20Learning%3A%20their%20impact%20on%20model%27s%20accuracy&body=Title%3A%20Evaluating%20Angle%20and%20Amplitude%20Encoding%20Strategies%20for%20Variational%0A%20%20Quantum%20Machine%20Learning%3A%20their%20impact%20on%20model%27s%20accuracy%0AAuthor%3A%20Antonio%20Tudisco%20and%20Andrea%20Marchesin%20and%20Maurizio%20Zamboni%20and%20Mariagrazia%20Graziano%20and%20Giovanna%20Turvani%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Quantum%20Computing%20and%20Machine%20Learning%20have%20increased%0Aattention%20to%20Quantum%20Machine%20Learning%20%28QML%29%2C%20which%20aims%20to%20develop%20machine%0Alearning%20models%20by%20exploiting%20the%20quantum%20computing%20paradigm.%20One%20of%20the%20widely%0Aused%20models%20in%20this%20area%20is%20the%20Variational%20Quantum%20Circuit%20%28VQC%29%2C%20a%20hybrid%0Amodel%20where%20the%20quantum%20circuit%20handles%20data%20inference%20while%20classical%0Aoptimization%20adjusts%20the%20parameters%20of%20the%20circuit.%20The%20quantum%20circuit%0Aconsists%20of%20an%20encoding%20layer%2C%20which%20loads%20data%20into%20the%20circuit%2C%20and%20a%0Atemplate%20circuit%2C%20known%20as%20the%20ansatz%2C%20responsible%20for%20processing%20the%20data.%0AThis%20work%20involves%20performing%20an%20analysis%20by%20considering%20both%20Amplitude-%20and%0AAngle-encoding%20models%2C%20and%20examining%20how%20the%20type%20of%20rotational%20gate%20applied%0Aaffects%20the%20classification%20performance%20of%20the%20model.%20This%20comparison%20is%20carried%0Aout%20by%20training%20the%20different%20models%20on%20two%20datasets%2C%20Wine%20and%20Diabetes%2C%20and%0Aevaluating%20their%20performance.%20The%20study%20demonstrates%20that%2C%20under%20identical%0Amodel%20topologies%2C%20the%20difference%20in%20accuracy%20between%20the%20best%20and%20worst%20models%0Aranges%20from%2010%25%20to%2030%25%2C%20with%20differences%20reaching%20up%20to%2041%25.%20Moreover%2C%20the%0Aresults%20highlight%20how%20the%20choice%20of%20rotational%20gates%20used%20in%20encoding%20can%0Asignificantly%20impact%20the%20model%27s%20classification%20performance.%20The%20findings%0Aconfirm%20that%20the%20embedding%20represents%20a%20hyperparameter%20for%20VQC%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Angle%2520and%2520Amplitude%2520Encoding%2520Strategies%2520for%2520Variational%250A%2520%2520Quantum%2520Machine%2520Learning%253A%2520their%2520impact%2520on%2520model%2527s%2520accuracy%26entry.906535625%3DAntonio%2520Tudisco%2520and%2520Andrea%2520Marchesin%2520and%2520Maurizio%2520Zamboni%2520and%2520Mariagrazia%2520Graziano%2520and%2520Giovanna%2520Turvani%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Quantum%2520Computing%2520and%2520Machine%2520Learning%2520have%2520increased%250Aattention%2520to%2520Quantum%2520Machine%2520Learning%2520%2528QML%2529%252C%2520which%2520aims%2520to%2520develop%2520machine%250Alearning%2520models%2520by%2520exploiting%2520the%2520quantum%2520computing%2520paradigm.%2520One%2520of%2520the%2520widely%250Aused%2520models%2520in%2520this%2520area%2520is%2520the%2520Variational%2520Quantum%2520Circuit%2520%2528VQC%2529%252C%2520a%2520hybrid%250Amodel%2520where%2520the%2520quantum%2520circuit%2520handles%2520data%2520inference%2520while%2520classical%250Aoptimization%2520adjusts%2520the%2520parameters%2520of%2520the%2520circuit.%2520The%2520quantum%2520circuit%250Aconsists%2520of%2520an%2520encoding%2520layer%252C%2520which%2520loads%2520data%2520into%2520the%2520circuit%252C%2520and%2520a%250Atemplate%2520circuit%252C%2520known%2520as%2520the%2520ansatz%252C%2520responsible%2520for%2520processing%2520the%2520data.%250AThis%2520work%2520involves%2520performing%2520an%2520analysis%2520by%2520considering%2520both%2520Amplitude-%2520and%250AAngle-encoding%2520models%252C%2520and%2520examining%2520how%2520the%2520type%2520of%2520rotational%2520gate%2520applied%250Aaffects%2520the%2520classification%2520performance%2520of%2520the%2520model.%2520This%2520comparison%2520is%2520carried%250Aout%2520by%2520training%2520the%2520different%2520models%2520on%2520two%2520datasets%252C%2520Wine%2520and%2520Diabetes%252C%2520and%250Aevaluating%2520their%2520performance.%2520The%2520study%2520demonstrates%2520that%252C%2520under%2520identical%250Amodel%2520topologies%252C%2520the%2520difference%2520in%2520accuracy%2520between%2520the%2520best%2520and%2520worst%2520models%250Aranges%2520from%252010%2525%2520to%252030%2525%252C%2520with%2520differences%2520reaching%2520up%2520to%252041%2525.%2520Moreover%252C%2520the%250Aresults%2520highlight%2520how%2520the%2520choice%2520of%2520rotational%2520gates%2520used%2520in%2520encoding%2520can%250Asignificantly%2520impact%2520the%2520model%2527s%2520classification%2520performance.%2520The%2520findings%250Aconfirm%2520that%2520the%2520embedding%2520represents%2520a%2520hyperparameter%2520for%2520VQC%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Angle%20and%20Amplitude%20Encoding%20Strategies%20for%20Variational%0A%20%20Quantum%20Machine%20Learning%3A%20their%20impact%20on%20model%27s%20accuracy&entry.906535625=Antonio%20Tudisco%20and%20Andrea%20Marchesin%20and%20Maurizio%20Zamboni%20and%20Mariagrazia%20Graziano%20and%20Giovanna%20Turvani&entry.1292438233=%20%20Recent%20advancements%20in%20Quantum%20Computing%20and%20Machine%20Learning%20have%20increased%0Aattention%20to%20Quantum%20Machine%20Learning%20%28QML%29%2C%20which%20aims%20to%20develop%20machine%0Alearning%20models%20by%20exploiting%20the%20quantum%20computing%20paradigm.%20One%20of%20the%20widely%0Aused%20models%20in%20this%20area%20is%20the%20Variational%20Quantum%20Circuit%20%28VQC%29%2C%20a%20hybrid%0Amodel%20where%20the%20quantum%20circuit%20handles%20data%20inference%20while%20classical%0Aoptimization%20adjusts%20the%20parameters%20of%20the%20circuit.%20The%20quantum%20circuit%0Aconsists%20of%20an%20encoding%20layer%2C%20which%20loads%20data%20into%20the%20circuit%2C%20and%20a%0Atemplate%20circuit%2C%20known%20as%20the%20ansatz%2C%20responsible%20for%20processing%20the%20data.%0AThis%20work%20involves%20performing%20an%20analysis%20by%20considering%20both%20Amplitude-%20and%0AAngle-encoding%20models%2C%20and%20examining%20how%20the%20type%20of%20rotational%20gate%20applied%0Aaffects%20the%20classification%20performance%20of%20the%20model.%20This%20comparison%20is%20carried%0Aout%20by%20training%20the%20different%20models%20on%20two%20datasets%2C%20Wine%20and%20Diabetes%2C%20and%0Aevaluating%20their%20performance.%20The%20study%20demonstrates%20that%2C%20under%20identical%0Amodel%20topologies%2C%20the%20difference%20in%20accuracy%20between%20the%20best%20and%20worst%20models%0Aranges%20from%2010%25%20to%2030%25%2C%20with%20differences%20reaching%20up%20to%2041%25.%20Moreover%2C%20the%0Aresults%20highlight%20how%20the%20choice%20of%20rotational%20gates%20used%20in%20encoding%20can%0Asignificantly%20impact%20the%20model%27s%20classification%20performance.%20The%20findings%0Aconfirm%20that%20the%20embedding%20represents%20a%20hyperparameter%20for%20VQC%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00768v1&entry.124074799=Read"},
{"title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "author": "Maryam Mosleh and Marie Devlin and Ellis Solaiman", "abstract": "  Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.\n", "link": "http://arxiv.org/abs/2508.00665v1", "date": "2025-08-01", "relevancy": 2.0153, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5033}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transparent%20Adaptive%20Learning%20via%20Data-Centric%20Multimodal%20Explainable%20AI&body=Title%3A%20Transparent%20Adaptive%20Learning%20via%20Data-Centric%20Multimodal%20Explainable%20AI%0AAuthor%3A%20Maryam%20Mosleh%20and%20Marie%20Devlin%20and%20Ellis%20Solaiman%0AAbstract%3A%20%20%20Artificial%20intelligence-driven%20adaptive%20learning%20systems%20are%20reshaping%0Aeducation%20through%20data-driven%20adaptation%20of%20learning%20experiences.%20Yet%20many%20of%0Athese%20systems%20lack%20transparency%2C%20offering%20limited%20insight%20into%20how%20decisions%0Aare%20made.%20Most%20explainable%20AI%20%28XAI%29%20techniques%20focus%20on%20technical%20outputs%20but%0Aneglect%20user%20roles%20and%20comprehension.%20This%20paper%20proposes%20a%20hybrid%20framework%0Athat%20integrates%20traditional%20XAI%20techniques%20with%20generative%20AI%20models%20and%20user%0Apersonalisation%20to%20generate%20multimodal%2C%20personalised%20explanations%20tailored%20to%0Auser%20needs.%20We%20redefine%20explainability%20as%20a%20dynamic%20communication%20process%0Atailored%20to%20user%20roles%20and%20learning%20goals.%20We%20outline%20the%20framework%27s%20design%2C%0Akey%20XAI%20limitations%20in%20education%2C%20and%20research%20directions%20on%20accuracy%2C%0Afairness%2C%20and%20personalisation.%20Our%20aim%20is%20to%20move%20towards%20explainable%20AI%20that%0Aenhances%20transparency%20while%20supporting%20user-centred%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransparent%2520Adaptive%2520Learning%2520via%2520Data-Centric%2520Multimodal%2520Explainable%2520AI%26entry.906535625%3DMaryam%2520Mosleh%2520and%2520Marie%2520Devlin%2520and%2520Ellis%2520Solaiman%26entry.1292438233%3D%2520%2520Artificial%2520intelligence-driven%2520adaptive%2520learning%2520systems%2520are%2520reshaping%250Aeducation%2520through%2520data-driven%2520adaptation%2520of%2520learning%2520experiences.%2520Yet%2520many%2520of%250Athese%2520systems%2520lack%2520transparency%252C%2520offering%2520limited%2520insight%2520into%2520how%2520decisions%250Aare%2520made.%2520Most%2520explainable%2520AI%2520%2528XAI%2529%2520techniques%2520focus%2520on%2520technical%2520outputs%2520but%250Aneglect%2520user%2520roles%2520and%2520comprehension.%2520This%2520paper%2520proposes%2520a%2520hybrid%2520framework%250Athat%2520integrates%2520traditional%2520XAI%2520techniques%2520with%2520generative%2520AI%2520models%2520and%2520user%250Apersonalisation%2520to%2520generate%2520multimodal%252C%2520personalised%2520explanations%2520tailored%2520to%250Auser%2520needs.%2520We%2520redefine%2520explainability%2520as%2520a%2520dynamic%2520communication%2520process%250Atailored%2520to%2520user%2520roles%2520and%2520learning%2520goals.%2520We%2520outline%2520the%2520framework%2527s%2520design%252C%250Akey%2520XAI%2520limitations%2520in%2520education%252C%2520and%2520research%2520directions%2520on%2520accuracy%252C%250Afairness%252C%2520and%2520personalisation.%2520Our%2520aim%2520is%2520to%2520move%2520towards%2520explainable%2520AI%2520that%250Aenhances%2520transparency%2520while%2520supporting%2520user-centred%2520experiences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transparent%20Adaptive%20Learning%20via%20Data-Centric%20Multimodal%20Explainable%20AI&entry.906535625=Maryam%20Mosleh%20and%20Marie%20Devlin%20and%20Ellis%20Solaiman&entry.1292438233=%20%20Artificial%20intelligence-driven%20adaptive%20learning%20systems%20are%20reshaping%0Aeducation%20through%20data-driven%20adaptation%20of%20learning%20experiences.%20Yet%20many%20of%0Athese%20systems%20lack%20transparency%2C%20offering%20limited%20insight%20into%20how%20decisions%0Aare%20made.%20Most%20explainable%20AI%20%28XAI%29%20techniques%20focus%20on%20technical%20outputs%20but%0Aneglect%20user%20roles%20and%20comprehension.%20This%20paper%20proposes%20a%20hybrid%20framework%0Athat%20integrates%20traditional%20XAI%20techniques%20with%20generative%20AI%20models%20and%20user%0Apersonalisation%20to%20generate%20multimodal%2C%20personalised%20explanations%20tailored%20to%0Auser%20needs.%20We%20redefine%20explainability%20as%20a%20dynamic%20communication%20process%0Atailored%20to%20user%20roles%20and%20learning%20goals.%20We%20outline%20the%20framework%27s%20design%2C%0Akey%20XAI%20limitations%20in%20education%2C%20and%20research%20directions%20on%20accuracy%2C%0Afairness%2C%20and%20personalisation.%20Our%20aim%20is%20to%20move%20towards%20explainable%20AI%20that%0Aenhances%20transparency%20while%20supporting%20user-centred%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00665v1&entry.124074799=Read"},
{"title": "Agentic large language models improve retrieval-based radiology question\n  answering", "author": "Sebastian Wind and Jeta Sopa and Daniel Truhn and Mahshad Lotfinia and Tri-Thien Nguyen and Keno Bressem and Lisa Adams and Mirabela Rusu and Harald K\u00f6stler and Gerhard Wellein and Andreas Maier and Soroosh Tayebi Arasteh", "abstract": "  Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.\n", "link": "http://arxiv.org/abs/2508.00743v1", "date": "2025-08-01", "relevancy": 1.9933, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20large%20language%20models%20improve%20retrieval-based%20radiology%20question%0A%20%20answering&body=Title%3A%20Agentic%20large%20language%20models%20improve%20retrieval-based%20radiology%20question%0A%20%20answering%0AAuthor%3A%20Sebastian%20Wind%20and%20Jeta%20Sopa%20and%20Daniel%20Truhn%20and%20Mahshad%20Lotfinia%20and%20Tri-Thien%20Nguyen%20and%20Keno%20Bressem%20and%20Lisa%20Adams%20and%20Mirabela%20Rusu%20and%20Harald%20K%C3%B6stler%20and%20Gerhard%20Wellein%20and%20Andreas%20Maier%20and%20Soroosh%20Tayebi%20Arasteh%0AAbstract%3A%20%20%20Clinical%20decision-making%20in%20radiology%20increasingly%20benefits%20from%20artificial%0Aintelligence%20%28AI%29%2C%20particularly%20through%20large%20language%20models%20%28LLMs%29.%20However%2C%0Atraditional%20retrieval-augmented%20generation%20%28RAG%29%20systems%20for%20radiology%20question%0Aanswering%20%28QA%29%20typically%20rely%20on%20single-step%20retrieval%2C%20limiting%20their%20ability%0Ato%20handle%20complex%20clinical%20reasoning%20tasks.%20Here%20we%20propose%20an%20agentic%20RAG%0Aframework%20enabling%20LLMs%20to%20autonomously%20decompose%20radiology%20questions%2C%0Aiteratively%20retrieve%20targeted%20clinical%20evidence%20from%20Radiopaedia%2C%20and%0Adynamically%20synthesize%20evidence-based%20responses.%20We%20evaluated%2024%20LLMs%20spanning%0Adiverse%20architectures%2C%20parameter%20scales%20%280.5B%20to%20%3E670B%29%2C%20and%20training%20paradigms%0A%28general-purpose%2C%20reasoning-optimized%2C%20clinically%20fine-tuned%29%2C%20using%20104%0Aexpert-curated%20radiology%20questions%20from%20previously%20established%20RSNA-RadioQA%20and%0AExtendedQA%20datasets.%20Agentic%20retrieval%20significantly%20improved%20mean%20diagnostic%0Aaccuracy%20over%20zero-shot%20prompting%20%2873%25%20vs.%2064%25%3B%20P%3C0.001%29%20and%20conventional%0Aonline%20RAG%20%2873%25%20vs.%2068%25%3B%20P%3C0.001%29.%20The%20greatest%20gains%20occurred%20in%20mid-sized%0Amodels%20%28e.g.%2C%20Mistral%20Large%20improved%20from%2072%25%20to%2081%25%29%20and%20small-scale%20models%0A%28e.g.%2C%20Qwen%202.5-7B%20improved%20from%2055%25%20to%2071%25%29%2C%20while%20very%20large%20models%20%28%3E200B%0Aparameters%29%20demonstrated%20minimal%20changes%20%28%3C2%25%20improvement%29.%20Additionally%2C%0Aagentic%20retrieval%20reduced%20hallucinations%20%28mean%209.4%25%29%20and%20retrieved%20clinically%0Arelevant%20context%20in%2046%25%20of%20cases%2C%20substantially%20aiding%20factual%20grounding.%20Even%0Aclinically%20fine-tuned%20models%20exhibited%20meaningful%20improvements%20%28e.g.%2C%0AMedGemma-27B%20improved%20from%2071%25%20to%2081%25%29%2C%20indicating%20complementary%20roles%20of%0Aretrieval%20and%20fine-tuning.%20These%20results%20highlight%20the%20potential%20of%20agentic%0Aframeworks%20to%20enhance%20factuality%20and%20diagnostic%20accuracy%20in%20radiology%20QA%2C%0Aparticularly%20among%20mid-sized%20LLMs%2C%20warranting%20future%20studies%20to%20validate%20their%0Aclinical%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520large%2520language%2520models%2520improve%2520retrieval-based%2520radiology%2520question%250A%2520%2520answering%26entry.906535625%3DSebastian%2520Wind%2520and%2520Jeta%2520Sopa%2520and%2520Daniel%2520Truhn%2520and%2520Mahshad%2520Lotfinia%2520and%2520Tri-Thien%2520Nguyen%2520and%2520Keno%2520Bressem%2520and%2520Lisa%2520Adams%2520and%2520Mirabela%2520Rusu%2520and%2520Harald%2520K%25C3%25B6stler%2520and%2520Gerhard%2520Wellein%2520and%2520Andreas%2520Maier%2520and%2520Soroosh%2520Tayebi%2520Arasteh%26entry.1292438233%3D%2520%2520Clinical%2520decision-making%2520in%2520radiology%2520increasingly%2520benefits%2520from%2520artificial%250Aintelligence%2520%2528AI%2529%252C%2520particularly%2520through%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%250Atraditional%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520systems%2520for%2520radiology%2520question%250Aanswering%2520%2528QA%2529%2520typically%2520rely%2520on%2520single-step%2520retrieval%252C%2520limiting%2520their%2520ability%250Ato%2520handle%2520complex%2520clinical%2520reasoning%2520tasks.%2520Here%2520we%2520propose%2520an%2520agentic%2520RAG%250Aframework%2520enabling%2520LLMs%2520to%2520autonomously%2520decompose%2520radiology%2520questions%252C%250Aiteratively%2520retrieve%2520targeted%2520clinical%2520evidence%2520from%2520Radiopaedia%252C%2520and%250Adynamically%2520synthesize%2520evidence-based%2520responses.%2520We%2520evaluated%252024%2520LLMs%2520spanning%250Adiverse%2520architectures%252C%2520parameter%2520scales%2520%25280.5B%2520to%2520%253E670B%2529%252C%2520and%2520training%2520paradigms%250A%2528general-purpose%252C%2520reasoning-optimized%252C%2520clinically%2520fine-tuned%2529%252C%2520using%2520104%250Aexpert-curated%2520radiology%2520questions%2520from%2520previously%2520established%2520RSNA-RadioQA%2520and%250AExtendedQA%2520datasets.%2520Agentic%2520retrieval%2520significantly%2520improved%2520mean%2520diagnostic%250Aaccuracy%2520over%2520zero-shot%2520prompting%2520%252873%2525%2520vs.%252064%2525%253B%2520P%253C0.001%2529%2520and%2520conventional%250Aonline%2520RAG%2520%252873%2525%2520vs.%252068%2525%253B%2520P%253C0.001%2529.%2520The%2520greatest%2520gains%2520occurred%2520in%2520mid-sized%250Amodels%2520%2528e.g.%252C%2520Mistral%2520Large%2520improved%2520from%252072%2525%2520to%252081%2525%2529%2520and%2520small-scale%2520models%250A%2528e.g.%252C%2520Qwen%25202.5-7B%2520improved%2520from%252055%2525%2520to%252071%2525%2529%252C%2520while%2520very%2520large%2520models%2520%2528%253E200B%250Aparameters%2529%2520demonstrated%2520minimal%2520changes%2520%2528%253C2%2525%2520improvement%2529.%2520Additionally%252C%250Aagentic%2520retrieval%2520reduced%2520hallucinations%2520%2528mean%25209.4%2525%2529%2520and%2520retrieved%2520clinically%250Arelevant%2520context%2520in%252046%2525%2520of%2520cases%252C%2520substantially%2520aiding%2520factual%2520grounding.%2520Even%250Aclinically%2520fine-tuned%2520models%2520exhibited%2520meaningful%2520improvements%2520%2528e.g.%252C%250AMedGemma-27B%2520improved%2520from%252071%2525%2520to%252081%2525%2529%252C%2520indicating%2520complementary%2520roles%2520of%250Aretrieval%2520and%2520fine-tuning.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520agentic%250Aframeworks%2520to%2520enhance%2520factuality%2520and%2520diagnostic%2520accuracy%2520in%2520radiology%2520QA%252C%250Aparticularly%2520among%2520mid-sized%2520LLMs%252C%2520warranting%2520future%2520studies%2520to%2520validate%2520their%250Aclinical%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20large%20language%20models%20improve%20retrieval-based%20radiology%20question%0A%20%20answering&entry.906535625=Sebastian%20Wind%20and%20Jeta%20Sopa%20and%20Daniel%20Truhn%20and%20Mahshad%20Lotfinia%20and%20Tri-Thien%20Nguyen%20and%20Keno%20Bressem%20and%20Lisa%20Adams%20and%20Mirabela%20Rusu%20and%20Harald%20K%C3%B6stler%20and%20Gerhard%20Wellein%20and%20Andreas%20Maier%20and%20Soroosh%20Tayebi%20Arasteh&entry.1292438233=%20%20Clinical%20decision-making%20in%20radiology%20increasingly%20benefits%20from%20artificial%0Aintelligence%20%28AI%29%2C%20particularly%20through%20large%20language%20models%20%28LLMs%29.%20However%2C%0Atraditional%20retrieval-augmented%20generation%20%28RAG%29%20systems%20for%20radiology%20question%0Aanswering%20%28QA%29%20typically%20rely%20on%20single-step%20retrieval%2C%20limiting%20their%20ability%0Ato%20handle%20complex%20clinical%20reasoning%20tasks.%20Here%20we%20propose%20an%20agentic%20RAG%0Aframework%20enabling%20LLMs%20to%20autonomously%20decompose%20radiology%20questions%2C%0Aiteratively%20retrieve%20targeted%20clinical%20evidence%20from%20Radiopaedia%2C%20and%0Adynamically%20synthesize%20evidence-based%20responses.%20We%20evaluated%2024%20LLMs%20spanning%0Adiverse%20architectures%2C%20parameter%20scales%20%280.5B%20to%20%3E670B%29%2C%20and%20training%20paradigms%0A%28general-purpose%2C%20reasoning-optimized%2C%20clinically%20fine-tuned%29%2C%20using%20104%0Aexpert-curated%20radiology%20questions%20from%20previously%20established%20RSNA-RadioQA%20and%0AExtendedQA%20datasets.%20Agentic%20retrieval%20significantly%20improved%20mean%20diagnostic%0Aaccuracy%20over%20zero-shot%20prompting%20%2873%25%20vs.%2064%25%3B%20P%3C0.001%29%20and%20conventional%0Aonline%20RAG%20%2873%25%20vs.%2068%25%3B%20P%3C0.001%29.%20The%20greatest%20gains%20occurred%20in%20mid-sized%0Amodels%20%28e.g.%2C%20Mistral%20Large%20improved%20from%2072%25%20to%2081%25%29%20and%20small-scale%20models%0A%28e.g.%2C%20Qwen%202.5-7B%20improved%20from%2055%25%20to%2071%25%29%2C%20while%20very%20large%20models%20%28%3E200B%0Aparameters%29%20demonstrated%20minimal%20changes%20%28%3C2%25%20improvement%29.%20Additionally%2C%0Aagentic%20retrieval%20reduced%20hallucinations%20%28mean%209.4%25%29%20and%20retrieved%20clinically%0Arelevant%20context%20in%2046%25%20of%20cases%2C%20substantially%20aiding%20factual%20grounding.%20Even%0Aclinically%20fine-tuned%20models%20exhibited%20meaningful%20improvements%20%28e.g.%2C%0AMedGemma-27B%20improved%20from%2071%25%20to%2081%25%29%2C%20indicating%20complementary%20roles%20of%0Aretrieval%20and%20fine-tuning.%20These%20results%20highlight%20the%20potential%20of%20agentic%0Aframeworks%20to%20enhance%20factuality%20and%20diagnostic%20accuracy%20in%20radiology%20QA%2C%0Aparticularly%20among%20mid-sized%20LLMs%2C%20warranting%20future%20studies%20to%20validate%20their%0Aclinical%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00743v1&entry.124074799=Read"},
{"title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese\n  Hate Speech Detection under Cloaking Perturbations", "author": "Qiyao Xue and Yuchen Dou and Ryan Shi and Xiang Lorraine Li and Wei Gao", "abstract": "  Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.\n", "link": "http://arxiv.org/abs/2508.00760v1", "date": "2025-08-01", "relevancy": 1.9862, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5087}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMBERT%3A%20Scaled%20Mixture-of-Experts%20Multimodal%20BERT%20for%20Robust%20Chinese%0A%20%20Hate%20Speech%20Detection%20under%20Cloaking%20Perturbations&body=Title%3A%20MMBERT%3A%20Scaled%20Mixture-of-Experts%20Multimodal%20BERT%20for%20Robust%20Chinese%0A%20%20Hate%20Speech%20Detection%20under%20Cloaking%20Perturbations%0AAuthor%3A%20Qiyao%20Xue%20and%20Yuchen%20Dou%20and%20Ryan%20Shi%20and%20Xiang%20Lorraine%20Li%20and%20Wei%20Gao%0AAbstract%3A%20%20%20Hate%20speech%20detection%20on%20Chinese%20social%20networks%20presents%20distinct%0Achallenges%2C%20particularly%20due%20to%20the%20widespread%20use%20of%20cloaking%20techniques%0Adesigned%20to%20evade%20conventional%20text-based%20detection%20systems.%20Although%20large%0Alanguage%20models%20%28LLMs%29%20have%20recently%20improved%20hate%20speech%20detection%0Acapabilities%2C%20the%20majority%20of%20existing%20work%20has%20concentrated%20on%20English%0Adatasets%2C%20with%20limited%20attention%20given%20to%20multimodal%20strategies%20in%20the%20Chinese%0Acontext.%20In%20this%20study%2C%20we%20propose%20MMBERT%2C%20a%20novel%20BERT-based%20multimodal%0Aframework%20that%20integrates%20textual%2C%20speech%2C%20and%20visual%20modalities%20through%20a%0AMixture-of-Experts%20%28MoE%29%20architecture.%20To%20address%20the%20instability%20associated%0Awith%20directly%20integrating%20MoE%20into%20BERT-based%20models%2C%20we%20develop%20a%20progressive%0Athree-stage%20training%20paradigm.%20MMBERT%20incorporates%20modality-specific%20experts%2C%20a%0Ashared%20self-attention%20mechanism%2C%20and%20a%20router-based%20expert%20allocation%20strategy%0Ato%20enhance%20robustness%20against%20adversarial%20perturbations.%20Empirical%20results%20in%0Aseveral%20Chinese%20hate%20speech%20datasets%20show%20that%20MMBERT%20significantly%20surpasses%0Afine-tuned%20BERT-based%20encoder%20models%2C%20fine-tuned%20LLMs%2C%20and%20LLMs%20utilizing%0Ain-context%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMBERT%253A%2520Scaled%2520Mixture-of-Experts%2520Multimodal%2520BERT%2520for%2520Robust%2520Chinese%250A%2520%2520Hate%2520Speech%2520Detection%2520under%2520Cloaking%2520Perturbations%26entry.906535625%3DQiyao%2520Xue%2520and%2520Yuchen%2520Dou%2520and%2520Ryan%2520Shi%2520and%2520Xiang%2520Lorraine%2520Li%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520Hate%2520speech%2520detection%2520on%2520Chinese%2520social%2520networks%2520presents%2520distinct%250Achallenges%252C%2520particularly%2520due%2520to%2520the%2520widespread%2520use%2520of%2520cloaking%2520techniques%250Adesigned%2520to%2520evade%2520conventional%2520text-based%2520detection%2520systems.%2520Although%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520recently%2520improved%2520hate%2520speech%2520detection%250Acapabilities%252C%2520the%2520majority%2520of%2520existing%2520work%2520has%2520concentrated%2520on%2520English%250Adatasets%252C%2520with%2520limited%2520attention%2520given%2520to%2520multimodal%2520strategies%2520in%2520the%2520Chinese%250Acontext.%2520In%2520this%2520study%252C%2520we%2520propose%2520MMBERT%252C%2520a%2520novel%2520BERT-based%2520multimodal%250Aframework%2520that%2520integrates%2520textual%252C%2520speech%252C%2520and%2520visual%2520modalities%2520through%2520a%250AMixture-of-Experts%2520%2528MoE%2529%2520architecture.%2520To%2520address%2520the%2520instability%2520associated%250Awith%2520directly%2520integrating%2520MoE%2520into%2520BERT-based%2520models%252C%2520we%2520develop%2520a%2520progressive%250Athree-stage%2520training%2520paradigm.%2520MMBERT%2520incorporates%2520modality-specific%2520experts%252C%2520a%250Ashared%2520self-attention%2520mechanism%252C%2520and%2520a%2520router-based%2520expert%2520allocation%2520strategy%250Ato%2520enhance%2520robustness%2520against%2520adversarial%2520perturbations.%2520Empirical%2520results%2520in%250Aseveral%2520Chinese%2520hate%2520speech%2520datasets%2520show%2520that%2520MMBERT%2520significantly%2520surpasses%250Afine-tuned%2520BERT-based%2520encoder%2520models%252C%2520fine-tuned%2520LLMs%252C%2520and%2520LLMs%2520utilizing%250Ain-context%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMBERT%3A%20Scaled%20Mixture-of-Experts%20Multimodal%20BERT%20for%20Robust%20Chinese%0A%20%20Hate%20Speech%20Detection%20under%20Cloaking%20Perturbations&entry.906535625=Qiyao%20Xue%20and%20Yuchen%20Dou%20and%20Ryan%20Shi%20and%20Xiang%20Lorraine%20Li%20and%20Wei%20Gao&entry.1292438233=%20%20Hate%20speech%20detection%20on%20Chinese%20social%20networks%20presents%20distinct%0Achallenges%2C%20particularly%20due%20to%20the%20widespread%20use%20of%20cloaking%20techniques%0Adesigned%20to%20evade%20conventional%20text-based%20detection%20systems.%20Although%20large%0Alanguage%20models%20%28LLMs%29%20have%20recently%20improved%20hate%20speech%20detection%0Acapabilities%2C%20the%20majority%20of%20existing%20work%20has%20concentrated%20on%20English%0Adatasets%2C%20with%20limited%20attention%20given%20to%20multimodal%20strategies%20in%20the%20Chinese%0Acontext.%20In%20this%20study%2C%20we%20propose%20MMBERT%2C%20a%20novel%20BERT-based%20multimodal%0Aframework%20that%20integrates%20textual%2C%20speech%2C%20and%20visual%20modalities%20through%20a%0AMixture-of-Experts%20%28MoE%29%20architecture.%20To%20address%20the%20instability%20associated%0Awith%20directly%20integrating%20MoE%20into%20BERT-based%20models%2C%20we%20develop%20a%20progressive%0Athree-stage%20training%20paradigm.%20MMBERT%20incorporates%20modality-specific%20experts%2C%20a%0Ashared%20self-attention%20mechanism%2C%20and%20a%20router-based%20expert%20allocation%20strategy%0Ato%20enhance%20robustness%20against%20adversarial%20perturbations.%20Empirical%20results%20in%0Aseveral%20Chinese%20hate%20speech%20datasets%20show%20that%20MMBERT%20significantly%20surpasses%0Afine-tuned%20BERT-based%20encoder%20models%2C%20fine-tuned%20LLMs%2C%20and%20LLMs%20utilizing%0Ain-context%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00760v1&entry.124074799=Read"},
{"title": "A Large Sensor Foundation Model Pretrained on Continuous Glucose Monitor\n  Data for Diabetes Management", "author": "Junjie Luo and Abhimanyu Kumbara and Mansur Shomali and Rui Han and Anand Iyer and Ritu Agarwal and Gordon Gao", "abstract": "  Continuous glucose monitoring (CGM) combined with AI offers new opportunities\nfor proactive diabetes management through real-time glucose forecasting.\nHowever, most existing models are task-specific and lack generalization across\npatient populations. Inspired by the autoregressive paradigm of large language\nmodels, we introduce CGM-LSM, a Transformer decoder-based Large Sensor Model\n(LSM) pretrained on 1.6 million CGM records from patients with different\ndiabetes types, ages, and genders. We model patients as sequences of glucose\ntime steps to learn latent knowledge embedded in CGM data and apply it to the\nprediction of glucose readings for a 2-hour horizon. Compared with prior\nmethods, CGM-LSM significantly improves prediction accuracy and robustness: a\n48.51% reduction in root mean square error in one-hour horizon forecasting and\nconsistent zero-shot prediction performance across held-out patient groups. We\nanalyze model performance variations across patient subgroups and prediction\nscenarios and outline key opportunities and challenges for advancing CGM\nfoundation models.\n", "link": "http://arxiv.org/abs/2412.09727v3", "date": "2025-08-01", "relevancy": 1.9841, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5187}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large%20Sensor%20Foundation%20Model%20Pretrained%20on%20Continuous%20Glucose%20Monitor%0A%20%20Data%20for%20Diabetes%20Management&body=Title%3A%20A%20Large%20Sensor%20Foundation%20Model%20Pretrained%20on%20Continuous%20Glucose%20Monitor%0A%20%20Data%20for%20Diabetes%20Management%0AAuthor%3A%20Junjie%20Luo%20and%20Abhimanyu%20Kumbara%20and%20Mansur%20Shomali%20and%20Rui%20Han%20and%20Anand%20Iyer%20and%20Ritu%20Agarwal%20and%20Gordon%20Gao%0AAbstract%3A%20%20%20Continuous%20glucose%20monitoring%20%28CGM%29%20combined%20with%20AI%20offers%20new%20opportunities%0Afor%20proactive%20diabetes%20management%20through%20real-time%20glucose%20forecasting.%0AHowever%2C%20most%20existing%20models%20are%20task-specific%20and%20lack%20generalization%20across%0Apatient%20populations.%20Inspired%20by%20the%20autoregressive%20paradigm%20of%20large%20language%0Amodels%2C%20we%20introduce%20CGM-LSM%2C%20a%20Transformer%20decoder-based%20Large%20Sensor%20Model%0A%28LSM%29%20pretrained%20on%201.6%20million%20CGM%20records%20from%20patients%20with%20different%0Adiabetes%20types%2C%20ages%2C%20and%20genders.%20We%20model%20patients%20as%20sequences%20of%20glucose%0Atime%20steps%20to%20learn%20latent%20knowledge%20embedded%20in%20CGM%20data%20and%20apply%20it%20to%20the%0Aprediction%20of%20glucose%20readings%20for%20a%202-hour%20horizon.%20Compared%20with%20prior%0Amethods%2C%20CGM-LSM%20significantly%20improves%20prediction%20accuracy%20and%20robustness%3A%20a%0A48.51%25%20reduction%20in%20root%20mean%20square%20error%20in%20one-hour%20horizon%20forecasting%20and%0Aconsistent%20zero-shot%20prediction%20performance%20across%20held-out%20patient%20groups.%20We%0Aanalyze%20model%20performance%20variations%20across%20patient%20subgroups%20and%20prediction%0Ascenarios%20and%20outline%20key%20opportunities%20and%20challenges%20for%20advancing%20CGM%0Afoundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09727v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large%2520Sensor%2520Foundation%2520Model%2520Pretrained%2520on%2520Continuous%2520Glucose%2520Monitor%250A%2520%2520Data%2520for%2520Diabetes%2520Management%26entry.906535625%3DJunjie%2520Luo%2520and%2520Abhimanyu%2520Kumbara%2520and%2520Mansur%2520Shomali%2520and%2520Rui%2520Han%2520and%2520Anand%2520Iyer%2520and%2520Ritu%2520Agarwal%2520and%2520Gordon%2520Gao%26entry.1292438233%3D%2520%2520Continuous%2520glucose%2520monitoring%2520%2528CGM%2529%2520combined%2520with%2520AI%2520offers%2520new%2520opportunities%250Afor%2520proactive%2520diabetes%2520management%2520through%2520real-time%2520glucose%2520forecasting.%250AHowever%252C%2520most%2520existing%2520models%2520are%2520task-specific%2520and%2520lack%2520generalization%2520across%250Apatient%2520populations.%2520Inspired%2520by%2520the%2520autoregressive%2520paradigm%2520of%2520large%2520language%250Amodels%252C%2520we%2520introduce%2520CGM-LSM%252C%2520a%2520Transformer%2520decoder-based%2520Large%2520Sensor%2520Model%250A%2528LSM%2529%2520pretrained%2520on%25201.6%2520million%2520CGM%2520records%2520from%2520patients%2520with%2520different%250Adiabetes%2520types%252C%2520ages%252C%2520and%2520genders.%2520We%2520model%2520patients%2520as%2520sequences%2520of%2520glucose%250Atime%2520steps%2520to%2520learn%2520latent%2520knowledge%2520embedded%2520in%2520CGM%2520data%2520and%2520apply%2520it%2520to%2520the%250Aprediction%2520of%2520glucose%2520readings%2520for%2520a%25202-hour%2520horizon.%2520Compared%2520with%2520prior%250Amethods%252C%2520CGM-LSM%2520significantly%2520improves%2520prediction%2520accuracy%2520and%2520robustness%253A%2520a%250A48.51%2525%2520reduction%2520in%2520root%2520mean%2520square%2520error%2520in%2520one-hour%2520horizon%2520forecasting%2520and%250Aconsistent%2520zero-shot%2520prediction%2520performance%2520across%2520held-out%2520patient%2520groups.%2520We%250Aanalyze%2520model%2520performance%2520variations%2520across%2520patient%2520subgroups%2520and%2520prediction%250Ascenarios%2520and%2520outline%2520key%2520opportunities%2520and%2520challenges%2520for%2520advancing%2520CGM%250Afoundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09727v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large%20Sensor%20Foundation%20Model%20Pretrained%20on%20Continuous%20Glucose%20Monitor%0A%20%20Data%20for%20Diabetes%20Management&entry.906535625=Junjie%20Luo%20and%20Abhimanyu%20Kumbara%20and%20Mansur%20Shomali%20and%20Rui%20Han%20and%20Anand%20Iyer%20and%20Ritu%20Agarwal%20and%20Gordon%20Gao&entry.1292438233=%20%20Continuous%20glucose%20monitoring%20%28CGM%29%20combined%20with%20AI%20offers%20new%20opportunities%0Afor%20proactive%20diabetes%20management%20through%20real-time%20glucose%20forecasting.%0AHowever%2C%20most%20existing%20models%20are%20task-specific%20and%20lack%20generalization%20across%0Apatient%20populations.%20Inspired%20by%20the%20autoregressive%20paradigm%20of%20large%20language%0Amodels%2C%20we%20introduce%20CGM-LSM%2C%20a%20Transformer%20decoder-based%20Large%20Sensor%20Model%0A%28LSM%29%20pretrained%20on%201.6%20million%20CGM%20records%20from%20patients%20with%20different%0Adiabetes%20types%2C%20ages%2C%20and%20genders.%20We%20model%20patients%20as%20sequences%20of%20glucose%0Atime%20steps%20to%20learn%20latent%20knowledge%20embedded%20in%20CGM%20data%20and%20apply%20it%20to%20the%0Aprediction%20of%20glucose%20readings%20for%20a%202-hour%20horizon.%20Compared%20with%20prior%0Amethods%2C%20CGM-LSM%20significantly%20improves%20prediction%20accuracy%20and%20robustness%3A%20a%0A48.51%25%20reduction%20in%20root%20mean%20square%20error%20in%20one-hour%20horizon%20forecasting%20and%0Aconsistent%20zero-shot%20prediction%20performance%20across%20held-out%20patient%20groups.%20We%0Aanalyze%20model%20performance%20variations%20across%20patient%20subgroups%20and%20prediction%0Ascenarios%20and%20outline%20key%20opportunities%20and%20challenges%20for%20advancing%20CGM%0Afoundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09727v3&entry.124074799=Read"},
{"title": "The Role of Active Learning in Modern Machine Learning", "author": "Thorben Werner and Lars Schmidt-Thieme and Vijaya Krishna Yalavarthi", "abstract": "  Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.\n", "link": "http://arxiv.org/abs/2508.00586v1", "date": "2025-08-01", "relevancy": 1.9811, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5108}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Active%20Learning%20in%20Modern%20Machine%20Learning&body=Title%3A%20The%20Role%20of%20Active%20Learning%20in%20Modern%20Machine%20Learning%0AAuthor%3A%20Thorben%20Werner%20and%20Lars%20Schmidt-Thieme%20and%20Vijaya%20Krishna%20Yalavarthi%0AAbstract%3A%20%20%20Even%20though%20Active%20Learning%20%28AL%29%20is%20widely%20studied%2C%20it%20is%20rarely%20applied%20in%0Acontexts%20outside%20its%20own%20scientific%20literature.%20We%20posit%20that%20the%20reason%20for%0Athis%20is%20AL%27s%20high%20computational%20cost%20coupled%20with%20the%20comparatively%20small%20lifts%0Ait%20is%20typically%20able%20to%20generate%20in%20scenarios%20with%20few%20labeled%20points.%20In%20this%0Awork%20we%20study%20the%20impact%20of%20different%20methods%20to%20combat%20this%20low%20data%20scenario%2C%0Anamely%20data%20augmentation%20%28DA%29%2C%20semi-supervised%20learning%20%28SSL%29%20and%20AL.%20We%20find%0Athat%20AL%20is%20by%20far%20the%20least%20efficient%20method%20of%20solving%20the%20low%20data%20problem%2C%0Agenerating%20a%20lift%20of%20only%201-4%5C%25%20over%20random%20sampling%2C%20while%20DA%20and%20SSL%20methods%0Acan%20generate%20up%20to%2060%5C%25%20lift%20in%20combination%20with%20random%20sampling.%20However%2C%20when%0AAL%20is%20combined%20with%20strong%20DA%20and%20SSL%20techniques%2C%20it%20surprisingly%20is%20still%20able%0Ato%20provide%20improvements.%20Based%20on%20these%20results%2C%20we%20frame%20AL%20not%20as%20a%20method%20to%0Acombat%20missing%20labels%2C%20but%20as%20the%20final%20building%20block%20to%20squeeze%20the%20last%20bits%0Aof%20performance%20out%20of%20data%20after%20appropriate%20DA%20and%20SSL%20methods%20as%20been%0Aapplied.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Active%2520Learning%2520in%2520Modern%2520Machine%2520Learning%26entry.906535625%3DThorben%2520Werner%2520and%2520Lars%2520Schmidt-Thieme%2520and%2520Vijaya%2520Krishna%2520Yalavarthi%26entry.1292438233%3D%2520%2520Even%2520though%2520Active%2520Learning%2520%2528AL%2529%2520is%2520widely%2520studied%252C%2520it%2520is%2520rarely%2520applied%2520in%250Acontexts%2520outside%2520its%2520own%2520scientific%2520literature.%2520We%2520posit%2520that%2520the%2520reason%2520for%250Athis%2520is%2520AL%2527s%2520high%2520computational%2520cost%2520coupled%2520with%2520the%2520comparatively%2520small%2520lifts%250Ait%2520is%2520typically%2520able%2520to%2520generate%2520in%2520scenarios%2520with%2520few%2520labeled%2520points.%2520In%2520this%250Awork%2520we%2520study%2520the%2520impact%2520of%2520different%2520methods%2520to%2520combat%2520this%2520low%2520data%2520scenario%252C%250Anamely%2520data%2520augmentation%2520%2528DA%2529%252C%2520semi-supervised%2520learning%2520%2528SSL%2529%2520and%2520AL.%2520We%2520find%250Athat%2520AL%2520is%2520by%2520far%2520the%2520least%2520efficient%2520method%2520of%2520solving%2520the%2520low%2520data%2520problem%252C%250Agenerating%2520a%2520lift%2520of%2520only%25201-4%255C%2525%2520over%2520random%2520sampling%252C%2520while%2520DA%2520and%2520SSL%2520methods%250Acan%2520generate%2520up%2520to%252060%255C%2525%2520lift%2520in%2520combination%2520with%2520random%2520sampling.%2520However%252C%2520when%250AAL%2520is%2520combined%2520with%2520strong%2520DA%2520and%2520SSL%2520techniques%252C%2520it%2520surprisingly%2520is%2520still%2520able%250Ato%2520provide%2520improvements.%2520Based%2520on%2520these%2520results%252C%2520we%2520frame%2520AL%2520not%2520as%2520a%2520method%2520to%250Acombat%2520missing%2520labels%252C%2520but%2520as%2520the%2520final%2520building%2520block%2520to%2520squeeze%2520the%2520last%2520bits%250Aof%2520performance%2520out%2520of%2520data%2520after%2520appropriate%2520DA%2520and%2520SSL%2520methods%2520as%2520been%250Aapplied.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Active%20Learning%20in%20Modern%20Machine%20Learning&entry.906535625=Thorben%20Werner%20and%20Lars%20Schmidt-Thieme%20and%20Vijaya%20Krishna%20Yalavarthi&entry.1292438233=%20%20Even%20though%20Active%20Learning%20%28AL%29%20is%20widely%20studied%2C%20it%20is%20rarely%20applied%20in%0Acontexts%20outside%20its%20own%20scientific%20literature.%20We%20posit%20that%20the%20reason%20for%0Athis%20is%20AL%27s%20high%20computational%20cost%20coupled%20with%20the%20comparatively%20small%20lifts%0Ait%20is%20typically%20able%20to%20generate%20in%20scenarios%20with%20few%20labeled%20points.%20In%20this%0Awork%20we%20study%20the%20impact%20of%20different%20methods%20to%20combat%20this%20low%20data%20scenario%2C%0Anamely%20data%20augmentation%20%28DA%29%2C%20semi-supervised%20learning%20%28SSL%29%20and%20AL.%20We%20find%0Athat%20AL%20is%20by%20far%20the%20least%20efficient%20method%20of%20solving%20the%20low%20data%20problem%2C%0Agenerating%20a%20lift%20of%20only%201-4%5C%25%20over%20random%20sampling%2C%20while%20DA%20and%20SSL%20methods%0Acan%20generate%20up%20to%2060%5C%25%20lift%20in%20combination%20with%20random%20sampling.%20However%2C%20when%0AAL%20is%20combined%20with%20strong%20DA%20and%20SSL%20techniques%2C%20it%20surprisingly%20is%20still%20able%0Ato%20provide%20improvements.%20Based%20on%20these%20results%2C%20we%20frame%20AL%20not%20as%20a%20method%20to%0Acombat%20missing%20labels%2C%20but%20as%20the%20final%20building%20block%20to%20squeeze%20the%20last%20bits%0Aof%20performance%20out%20of%20data%20after%20appropriate%20DA%20and%20SSL%20methods%20as%20been%0Aapplied.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00586v1&entry.124074799=Read"},
{"title": "Convergence of Implicit Gradient Descent for Training Two-Layer\n  Physics-Informed Neural Networks", "author": "Xianliang Xu and Ting Du and Wang Kong and Bin Shan and Ye Li and Zhongyi Huang", "abstract": "  The optimization algorithms are crucial in training physics-informed neural\nnetworks (PINNs), as unsuitable methods may lead to poor solutions. Compared to\nthe common gradient descent (GD) algorithm, implicit gradient descent (IGD)\noutperforms it in handling certain multi-scale problems. In this paper, we\nprovide convergence analysis for the IGD in training over-parameterized\ntwo-layer PINNs. We first derive the training dynamics of IGD in training\ntwo-layer PINNs. Then, over-parameterization allows us to prove that the\nrandomly initialized IGD converges to a globally optimal solution at a linear\nconvergence rate. Moreover, due to the distinct training dynamics of IGD\ncompared to GD, the learning rate can be selected independently of the sample\nsize and the least eigenvalue of the Gram matrix. Additionally, the novel\napproach used in our convergence analysis imposes a milder requirement on the\nnetwork width. Finally, empirical results validate our theoretical findings.\n", "link": "http://arxiv.org/abs/2407.02827v3", "date": "2025-08-01", "relevancy": 1.9752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5161}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4786}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20Implicit%20Gradient%20Descent%20for%20Training%20Two-Layer%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Convergence%20of%20Implicit%20Gradient%20Descent%20for%20Training%20Two-Layer%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Bin%20Shan%20and%20Ye%20Li%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20The%20optimization%20algorithms%20are%20crucial%20in%20training%20physics-informed%20neural%0Anetworks%20%28PINNs%29%2C%20as%20unsuitable%20methods%20may%20lead%20to%20poor%20solutions.%20Compared%20to%0Athe%20common%20gradient%20descent%20%28GD%29%20algorithm%2C%20implicit%20gradient%20descent%20%28IGD%29%0Aoutperforms%20it%20in%20handling%20certain%20multi-scale%20problems.%20In%20this%20paper%2C%20we%0Aprovide%20convergence%20analysis%20for%20the%20IGD%20in%20training%20over-parameterized%0Atwo-layer%20PINNs.%20We%20first%20derive%20the%20training%20dynamics%20of%20IGD%20in%20training%0Atwo-layer%20PINNs.%20Then%2C%20over-parameterization%20allows%20us%20to%20prove%20that%20the%0Arandomly%20initialized%20IGD%20converges%20to%20a%20globally%20optimal%20solution%20at%20a%20linear%0Aconvergence%20rate.%20Moreover%2C%20due%20to%20the%20distinct%20training%20dynamics%20of%20IGD%0Acompared%20to%20GD%2C%20the%20learning%20rate%20can%20be%20selected%20independently%20of%20the%20sample%0Asize%20and%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix.%20Additionally%2C%20the%20novel%0Aapproach%20used%20in%20our%20convergence%20analysis%20imposes%20a%20milder%20requirement%20on%20the%0Anetwork%20width.%20Finally%2C%20empirical%20results%20validate%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02827v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520Implicit%2520Gradient%2520Descent%2520for%2520Training%2520Two-Layer%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DXianliang%2520Xu%2520and%2520Ting%2520Du%2520and%2520Wang%2520Kong%2520and%2520Bin%2520Shan%2520and%2520Ye%2520Li%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520The%2520optimization%2520algorithms%2520are%2520crucial%2520in%2520training%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529%252C%2520as%2520unsuitable%2520methods%2520may%2520lead%2520to%2520poor%2520solutions.%2520Compared%2520to%250Athe%2520common%2520gradient%2520descent%2520%2528GD%2529%2520algorithm%252C%2520implicit%2520gradient%2520descent%2520%2528IGD%2529%250Aoutperforms%2520it%2520in%2520handling%2520certain%2520multi-scale%2520problems.%2520In%2520this%2520paper%252C%2520we%250Aprovide%2520convergence%2520analysis%2520for%2520the%2520IGD%2520in%2520training%2520over-parameterized%250Atwo-layer%2520PINNs.%2520We%2520first%2520derive%2520the%2520training%2520dynamics%2520of%2520IGD%2520in%2520training%250Atwo-layer%2520PINNs.%2520Then%252C%2520over-parameterization%2520allows%2520us%2520to%2520prove%2520that%2520the%250Arandomly%2520initialized%2520IGD%2520converges%2520to%2520a%2520globally%2520optimal%2520solution%2520at%2520a%2520linear%250Aconvergence%2520rate.%2520Moreover%252C%2520due%2520to%2520the%2520distinct%2520training%2520dynamics%2520of%2520IGD%250Acompared%2520to%2520GD%252C%2520the%2520learning%2520rate%2520can%2520be%2520selected%2520independently%2520of%2520the%2520sample%250Asize%2520and%2520the%2520least%2520eigenvalue%2520of%2520the%2520Gram%2520matrix.%2520Additionally%252C%2520the%2520novel%250Aapproach%2520used%2520in%2520our%2520convergence%2520analysis%2520imposes%2520a%2520milder%2520requirement%2520on%2520the%250Anetwork%2520width.%2520Finally%252C%2520empirical%2520results%2520validate%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02827v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20Implicit%20Gradient%20Descent%20for%20Training%20Two-Layer%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Bin%20Shan%20and%20Ye%20Li%20and%20Zhongyi%20Huang&entry.1292438233=%20%20The%20optimization%20algorithms%20are%20crucial%20in%20training%20physics-informed%20neural%0Anetworks%20%28PINNs%29%2C%20as%20unsuitable%20methods%20may%20lead%20to%20poor%20solutions.%20Compared%20to%0Athe%20common%20gradient%20descent%20%28GD%29%20algorithm%2C%20implicit%20gradient%20descent%20%28IGD%29%0Aoutperforms%20it%20in%20handling%20certain%20multi-scale%20problems.%20In%20this%20paper%2C%20we%0Aprovide%20convergence%20analysis%20for%20the%20IGD%20in%20training%20over-parameterized%0Atwo-layer%20PINNs.%20We%20first%20derive%20the%20training%20dynamics%20of%20IGD%20in%20training%0Atwo-layer%20PINNs.%20Then%2C%20over-parameterization%20allows%20us%20to%20prove%20that%20the%0Arandomly%20initialized%20IGD%20converges%20to%20a%20globally%20optimal%20solution%20at%20a%20linear%0Aconvergence%20rate.%20Moreover%2C%20due%20to%20the%20distinct%20training%20dynamics%20of%20IGD%0Acompared%20to%20GD%2C%20the%20learning%20rate%20can%20be%20selected%20independently%20of%20the%20sample%0Asize%20and%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix.%20Additionally%2C%20the%20novel%0Aapproach%20used%20in%20our%20convergence%20analysis%20imposes%20a%20milder%20requirement%20on%20the%0Anetwork%20width.%20Finally%2C%20empirical%20results%20validate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02827v3&entry.124074799=Read"},
{"title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting\n  System using Gaussian Processes as Surrogate Models", "author": "Felix Kronenwett and Georg Maier and Thomas L\u00e4ngle", "abstract": "  Sensor-based sorting systems enable the physical separation of a material\nstream into two fractions. The sorting decision is based on the image data\nevaluation of the sensors used and is carried out using actuators. Various\nprocess parameters must be set depending on the properties of the material\nstream, the dimensioning of the system, and the required sorting accuracy.\nHowever, continuous verification and re-adjustment are necessary due to\nchanging requirements and material stream compositions. In this paper, we\nintroduce an approach for optimizing, recurrently monitoring and adjusting the\nprocess parameters of a sensor-based sorting system. Based on Bayesian\nOptimization, Gaussian process regression models are used as surrogate models\nto achieve specific requirements for system behavior with the uncertainties\ncontained therein. This method minimizes the number of necessary experiments\nwhile simultaneously considering two possible optimization targets based on the\nrequirements for both material output streams. In addition, uncertainties are\nconsidered during determining sorting accuracies in the model calculation. We\nevaluated the method with three example process parameters.\n", "link": "http://arxiv.org/abs/2507.22766v2", "date": "2025-08-01", "relevancy": 1.962, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5574}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4778}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20of%20Process%20Parameters%20of%20a%20Sensor-Based%20Sorting%0A%20%20System%20using%20Gaussian%20Processes%20as%20Surrogate%20Models&body=Title%3A%20Bayesian%20Optimization%20of%20Process%20Parameters%20of%20a%20Sensor-Based%20Sorting%0A%20%20System%20using%20Gaussian%20Processes%20as%20Surrogate%20Models%0AAuthor%3A%20Felix%20Kronenwett%20and%20Georg%20Maier%20and%20Thomas%20L%C3%A4ngle%0AAbstract%3A%20%20%20Sensor-based%20sorting%20systems%20enable%20the%20physical%20separation%20of%20a%20material%0Astream%20into%20two%20fractions.%20The%20sorting%20decision%20is%20based%20on%20the%20image%20data%0Aevaluation%20of%20the%20sensors%20used%20and%20is%20carried%20out%20using%20actuators.%20Various%0Aprocess%20parameters%20must%20be%20set%20depending%20on%20the%20properties%20of%20the%20material%0Astream%2C%20the%20dimensioning%20of%20the%20system%2C%20and%20the%20required%20sorting%20accuracy.%0AHowever%2C%20continuous%20verification%20and%20re-adjustment%20are%20necessary%20due%20to%0Achanging%20requirements%20and%20material%20stream%20compositions.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20approach%20for%20optimizing%2C%20recurrently%20monitoring%20and%20adjusting%20the%0Aprocess%20parameters%20of%20a%20sensor-based%20sorting%20system.%20Based%20on%20Bayesian%0AOptimization%2C%20Gaussian%20process%20regression%20models%20are%20used%20as%20surrogate%20models%0Ato%20achieve%20specific%20requirements%20for%20system%20behavior%20with%20the%20uncertainties%0Acontained%20therein.%20This%20method%20minimizes%20the%20number%20of%20necessary%20experiments%0Awhile%20simultaneously%20considering%20two%20possible%20optimization%20targets%20based%20on%20the%0Arequirements%20for%20both%20material%20output%20streams.%20In%20addition%2C%20uncertainties%20are%0Aconsidered%20during%20determining%20sorting%20accuracies%20in%20the%20model%20calculation.%20We%0Aevaluated%20the%20method%20with%20three%20example%20process%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520of%2520Process%2520Parameters%2520of%2520a%2520Sensor-Based%2520Sorting%250A%2520%2520System%2520using%2520Gaussian%2520Processes%2520as%2520Surrogate%2520Models%26entry.906535625%3DFelix%2520Kronenwett%2520and%2520Georg%2520Maier%2520and%2520Thomas%2520L%25C3%25A4ngle%26entry.1292438233%3D%2520%2520Sensor-based%2520sorting%2520systems%2520enable%2520the%2520physical%2520separation%2520of%2520a%2520material%250Astream%2520into%2520two%2520fractions.%2520The%2520sorting%2520decision%2520is%2520based%2520on%2520the%2520image%2520data%250Aevaluation%2520of%2520the%2520sensors%2520used%2520and%2520is%2520carried%2520out%2520using%2520actuators.%2520Various%250Aprocess%2520parameters%2520must%2520be%2520set%2520depending%2520on%2520the%2520properties%2520of%2520the%2520material%250Astream%252C%2520the%2520dimensioning%2520of%2520the%2520system%252C%2520and%2520the%2520required%2520sorting%2520accuracy.%250AHowever%252C%2520continuous%2520verification%2520and%2520re-adjustment%2520are%2520necessary%2520due%2520to%250Achanging%2520requirements%2520and%2520material%2520stream%2520compositions.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520an%2520approach%2520for%2520optimizing%252C%2520recurrently%2520monitoring%2520and%2520adjusting%2520the%250Aprocess%2520parameters%2520of%2520a%2520sensor-based%2520sorting%2520system.%2520Based%2520on%2520Bayesian%250AOptimization%252C%2520Gaussian%2520process%2520regression%2520models%2520are%2520used%2520as%2520surrogate%2520models%250Ato%2520achieve%2520specific%2520requirements%2520for%2520system%2520behavior%2520with%2520the%2520uncertainties%250Acontained%2520therein.%2520This%2520method%2520minimizes%2520the%2520number%2520of%2520necessary%2520experiments%250Awhile%2520simultaneously%2520considering%2520two%2520possible%2520optimization%2520targets%2520based%2520on%2520the%250Arequirements%2520for%2520both%2520material%2520output%2520streams.%2520In%2520addition%252C%2520uncertainties%2520are%250Aconsidered%2520during%2520determining%2520sorting%2520accuracies%2520in%2520the%2520model%2520calculation.%2520We%250Aevaluated%2520the%2520method%2520with%2520three%2520example%2520process%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20of%20Process%20Parameters%20of%20a%20Sensor-Based%20Sorting%0A%20%20System%20using%20Gaussian%20Processes%20as%20Surrogate%20Models&entry.906535625=Felix%20Kronenwett%20and%20Georg%20Maier%20and%20Thomas%20L%C3%A4ngle&entry.1292438233=%20%20Sensor-based%20sorting%20systems%20enable%20the%20physical%20separation%20of%20a%20material%0Astream%20into%20two%20fractions.%20The%20sorting%20decision%20is%20based%20on%20the%20image%20data%0Aevaluation%20of%20the%20sensors%20used%20and%20is%20carried%20out%20using%20actuators.%20Various%0Aprocess%20parameters%20must%20be%20set%20depending%20on%20the%20properties%20of%20the%20material%0Astream%2C%20the%20dimensioning%20of%20the%20system%2C%20and%20the%20required%20sorting%20accuracy.%0AHowever%2C%20continuous%20verification%20and%20re-adjustment%20are%20necessary%20due%20to%0Achanging%20requirements%20and%20material%20stream%20compositions.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20approach%20for%20optimizing%2C%20recurrently%20monitoring%20and%20adjusting%20the%0Aprocess%20parameters%20of%20a%20sensor-based%20sorting%20system.%20Based%20on%20Bayesian%0AOptimization%2C%20Gaussian%20process%20regression%20models%20are%20used%20as%20surrogate%20models%0Ato%20achieve%20specific%20requirements%20for%20system%20behavior%20with%20the%20uncertainties%0Acontained%20therein.%20This%20method%20minimizes%20the%20number%20of%20necessary%20experiments%0Awhile%20simultaneously%20considering%20two%20possible%20optimization%20targets%20based%20on%20the%0Arequirements%20for%20both%20material%20output%20streams.%20In%20addition%2C%20uncertainties%20are%0Aconsidered%20during%20determining%20sorting%20accuracies%20in%20the%20model%20calculation.%20We%0Aevaluated%20the%20method%20with%20three%20example%20process%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22766v2&entry.124074799=Read"},
{"title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation\n  Learning", "author": "Yingxu Wang and Mengzhu Wang and Zhichao Huang and Suyu Liu", "abstract": "  Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.\n", "link": "http://arxiv.org/abs/2508.00716v1", "date": "2025-08-01", "relevancy": 1.9566, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5056}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning&body=Title%3A%20Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning%0AAuthor%3A%20Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Zhichao%20Huang%20and%20Suyu%20Liu%0AAbstract%3A%20%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20facilitates%20knowledge%20transfer%20from%20labeled%0Asource%20graphs%20to%20unlabeled%20target%20graphs%20by%20learning%20domain-invariant%0Arepresentations%2C%20which%20is%20essential%20in%20applications%20such%20as%20molecular%20property%0Aprediction%20and%20social%20network%20analysis.%20However%2C%20most%20existing%20GDA%20methods%20rely%0Aon%20the%20assumption%20of%20clean%20source%20labels%2C%20which%20rarely%20holds%20in%20real-world%0Ascenarios%20where%20annotation%20noise%20is%20pervasive.%20This%20label%20noise%20severely%0Aimpairs%20feature%20alignment%20and%20degrades%20adaptation%20performance%20under%20domain%0Ashifts.%20To%20address%20this%20challenge%2C%20we%20propose%20Nested%20Graph%20Pseudo-Label%0ARefinement%20%28NeGPR%29%2C%20a%20novel%20framework%20tailored%20for%20graph-level%20domain%0Aadaptation%20with%20noisy%20labels.%20NeGPR%20first%20pretrains%20dual%20branches%2C%20i.e.%2C%0Asemantic%20and%20topology%20branches%2C%20by%20enforcing%20neighborhood%20consistency%20in%20the%0Afeature%20space%2C%20thereby%20reducing%20the%20influence%20of%20noisy%20supervision.%20To%20bridge%0Adomain%20gaps%2C%20NeGPR%20employs%20a%20nested%20refinement%20mechanism%20in%20which%20one%20branch%0Aselects%20high-confidence%20target%20samples%20to%20guide%20the%20adaptation%20of%20the%20other%2C%0Aenabling%20progressive%20cross-domain%20learning.%20Furthermore%2C%20since%20pseudo-labels%0Amay%20still%20contain%20noise%20and%20the%20pre-trained%20branches%20are%20already%20overfitted%20to%0Athe%20noisy%20labels%20in%20the%20source%20domain%2C%20NeGPR%20incorporates%20a%20noise-aware%0Aregularization%20strategy.%20This%20regularization%20is%20theoretically%20proven%20to%0Amitigate%20the%20adverse%20effects%20of%20pseudo-label%20noise%2C%20even%20under%20the%20presence%20of%0Asource%20overfitting%2C%20thus%20enhancing%20the%20robustness%20of%20the%20adaptation%20process.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20NeGPR%20consistently%0Aoutperforms%20state-of-the-art%20methods%20under%20severe%20label%20noise%2C%20achieving%20gains%0Aof%20up%20to%2012.7%25%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNested%2520Graph%2520Pseudo-Label%2520Refinement%2520for%2520Noisy%2520Label%2520Domain%2520Adaptation%250A%2520%2520Learning%26entry.906535625%3DYingxu%2520Wang%2520and%2520Mengzhu%2520Wang%2520and%2520Zhichao%2520Huang%2520and%2520Suyu%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520Domain%2520Adaptation%2520%2528GDA%2529%2520facilitates%2520knowledge%2520transfer%2520from%2520labeled%250Asource%2520graphs%2520to%2520unlabeled%2520target%2520graphs%2520by%2520learning%2520domain-invariant%250Arepresentations%252C%2520which%2520is%2520essential%2520in%2520applications%2520such%2520as%2520molecular%2520property%250Aprediction%2520and%2520social%2520network%2520analysis.%2520However%252C%2520most%2520existing%2520GDA%2520methods%2520rely%250Aon%2520the%2520assumption%2520of%2520clean%2520source%2520labels%252C%2520which%2520rarely%2520holds%2520in%2520real-world%250Ascenarios%2520where%2520annotation%2520noise%2520is%2520pervasive.%2520This%2520label%2520noise%2520severely%250Aimpairs%2520feature%2520alignment%2520and%2520degrades%2520adaptation%2520performance%2520under%2520domain%250Ashifts.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Nested%2520Graph%2520Pseudo-Label%250ARefinement%2520%2528NeGPR%2529%252C%2520a%2520novel%2520framework%2520tailored%2520for%2520graph-level%2520domain%250Aadaptation%2520with%2520noisy%2520labels.%2520NeGPR%2520first%2520pretrains%2520dual%2520branches%252C%2520i.e.%252C%250Asemantic%2520and%2520topology%2520branches%252C%2520by%2520enforcing%2520neighborhood%2520consistency%2520in%2520the%250Afeature%2520space%252C%2520thereby%2520reducing%2520the%2520influence%2520of%2520noisy%2520supervision.%2520To%2520bridge%250Adomain%2520gaps%252C%2520NeGPR%2520employs%2520a%2520nested%2520refinement%2520mechanism%2520in%2520which%2520one%2520branch%250Aselects%2520high-confidence%2520target%2520samples%2520to%2520guide%2520the%2520adaptation%2520of%2520the%2520other%252C%250Aenabling%2520progressive%2520cross-domain%2520learning.%2520Furthermore%252C%2520since%2520pseudo-labels%250Amay%2520still%2520contain%2520noise%2520and%2520the%2520pre-trained%2520branches%2520are%2520already%2520overfitted%2520to%250Athe%2520noisy%2520labels%2520in%2520the%2520source%2520domain%252C%2520NeGPR%2520incorporates%2520a%2520noise-aware%250Aregularization%2520strategy.%2520This%2520regularization%2520is%2520theoretically%2520proven%2520to%250Amitigate%2520the%2520adverse%2520effects%2520of%2520pseudo-label%2520noise%252C%2520even%2520under%2520the%2520presence%2520of%250Asource%2520overfitting%252C%2520thus%2520enhancing%2520the%2520robustness%2520of%2520the%2520adaptation%2520process.%250AExtensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520NeGPR%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520under%2520severe%2520label%2520noise%252C%2520achieving%2520gains%250Aof%2520up%2520to%252012.7%2525%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nested%20Graph%20Pseudo-Label%20Refinement%20for%20Noisy%20Label%20Domain%20Adaptation%0A%20%20Learning&entry.906535625=Yingxu%20Wang%20and%20Mengzhu%20Wang%20and%20Zhichao%20Huang%20and%20Suyu%20Liu&entry.1292438233=%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20facilitates%20knowledge%20transfer%20from%20labeled%0Asource%20graphs%20to%20unlabeled%20target%20graphs%20by%20learning%20domain-invariant%0Arepresentations%2C%20which%20is%20essential%20in%20applications%20such%20as%20molecular%20property%0Aprediction%20and%20social%20network%20analysis.%20However%2C%20most%20existing%20GDA%20methods%20rely%0Aon%20the%20assumption%20of%20clean%20source%20labels%2C%20which%20rarely%20holds%20in%20real-world%0Ascenarios%20where%20annotation%20noise%20is%20pervasive.%20This%20label%20noise%20severely%0Aimpairs%20feature%20alignment%20and%20degrades%20adaptation%20performance%20under%20domain%0Ashifts.%20To%20address%20this%20challenge%2C%20we%20propose%20Nested%20Graph%20Pseudo-Label%0ARefinement%20%28NeGPR%29%2C%20a%20novel%20framework%20tailored%20for%20graph-level%20domain%0Aadaptation%20with%20noisy%20labels.%20NeGPR%20first%20pretrains%20dual%20branches%2C%20i.e.%2C%0Asemantic%20and%20topology%20branches%2C%20by%20enforcing%20neighborhood%20consistency%20in%20the%0Afeature%20space%2C%20thereby%20reducing%20the%20influence%20of%20noisy%20supervision.%20To%20bridge%0Adomain%20gaps%2C%20NeGPR%20employs%20a%20nested%20refinement%20mechanism%20in%20which%20one%20branch%0Aselects%20high-confidence%20target%20samples%20to%20guide%20the%20adaptation%20of%20the%20other%2C%0Aenabling%20progressive%20cross-domain%20learning.%20Furthermore%2C%20since%20pseudo-labels%0Amay%20still%20contain%20noise%20and%20the%20pre-trained%20branches%20are%20already%20overfitted%20to%0Athe%20noisy%20labels%20in%20the%20source%20domain%2C%20NeGPR%20incorporates%20a%20noise-aware%0Aregularization%20strategy.%20This%20regularization%20is%20theoretically%20proven%20to%0Amitigate%20the%20adverse%20effects%20of%20pseudo-label%20noise%2C%20even%20under%20the%20presence%20of%0Asource%20overfitting%2C%20thus%20enhancing%20the%20robustness%20of%20the%20adaptation%20process.%0AExtensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20NeGPR%20consistently%0Aoutperforms%20state-of-the-art%20methods%20under%20severe%20label%20noise%2C%20achieving%20gains%0Aof%20up%20to%2012.7%25%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00716v1&entry.124074799=Read"},
{"title": "Learning Network Dismantling without Handcrafted Inputs", "author": "Haozhe Tian and Pietro Ferraro and Robert Shorten and Mahdi Jalili and Homayoun Hamedmoghadam", "abstract": "  The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.\n", "link": "http://arxiv.org/abs/2508.00706v1", "date": "2025-08-01", "relevancy": 1.9535, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4945}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Network%20Dismantling%20without%20Handcrafted%20Inputs&body=Title%3A%20Learning%20Network%20Dismantling%20without%20Handcrafted%20Inputs%0AAuthor%3A%20Haozhe%20Tian%20and%20Pietro%20Ferraro%20and%20Robert%20Shorten%20and%20Mahdi%20Jalili%20and%20Homayoun%20Hamedmoghadam%0AAbstract%3A%20%20%20The%20application%20of%20message-passing%20Graph%20Neural%20Networks%20has%20been%20a%0Abreakthrough%20for%20important%20network%20science%20problems.%20However%2C%20the%20competitive%0Aperformance%20often%20relies%20on%20using%20handcrafted%20structural%20features%20as%20inputs%2C%0Awhich%20increases%20computational%20cost%20and%20introduces%20bias%20into%20the%20otherwise%0Apurely%20data-driven%20network%20representations.%20Here%2C%20we%20eliminate%20the%20need%20for%0Ahandcrafted%20features%20by%20introducing%20an%20attention%20mechanism%20and%20utilizing%0Amessage-iteration%20profiles%2C%20in%20addition%20to%20an%20effective%20algorithmic%20approach%20to%0Agenerate%20a%20structurally%20diverse%20training%20set%20of%20small%20synthetic%20networks.%0AThereby%2C%20we%20build%20an%20expressive%20message-passing%20framework%20and%20use%20it%20to%0Aefficiently%20solve%20the%20NP-hard%20problem%20of%20Network%20Dismantling%2C%20virtually%0Aequivalent%20to%20vital%20node%20identification%2C%20with%20significant%20real-world%0Aapplications.%20Trained%20solely%20on%20diversified%20synthetic%20networks%2C%20our%20proposed%0Amodel%20--%20MIND%3A%20Message%20Iteration%20Network%20Dismantler%20--%20generalizes%20to%20large%2C%0Aunseen%20real%20networks%20with%20millions%20of%20nodes%2C%20outperforming%20state-of-the-art%0Anetwork%20dismantling%20methods.%20Increased%20efficiency%20and%20generalizability%20of%20the%0Aproposed%20model%20can%20be%20leveraged%20beyond%20dismantling%20in%20a%20range%20of%20complex%0Anetwork%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Network%2520Dismantling%2520without%2520Handcrafted%2520Inputs%26entry.906535625%3DHaozhe%2520Tian%2520and%2520Pietro%2520Ferraro%2520and%2520Robert%2520Shorten%2520and%2520Mahdi%2520Jalili%2520and%2520Homayoun%2520Hamedmoghadam%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520message-passing%2520Graph%2520Neural%2520Networks%2520has%2520been%2520a%250Abreakthrough%2520for%2520important%2520network%2520science%2520problems.%2520However%252C%2520the%2520competitive%250Aperformance%2520often%2520relies%2520on%2520using%2520handcrafted%2520structural%2520features%2520as%2520inputs%252C%250Awhich%2520increases%2520computational%2520cost%2520and%2520introduces%2520bias%2520into%2520the%2520otherwise%250Apurely%2520data-driven%2520network%2520representations.%2520Here%252C%2520we%2520eliminate%2520the%2520need%2520for%250Ahandcrafted%2520features%2520by%2520introducing%2520an%2520attention%2520mechanism%2520and%2520utilizing%250Amessage-iteration%2520profiles%252C%2520in%2520addition%2520to%2520an%2520effective%2520algorithmic%2520approach%2520to%250Agenerate%2520a%2520structurally%2520diverse%2520training%2520set%2520of%2520small%2520synthetic%2520networks.%250AThereby%252C%2520we%2520build%2520an%2520expressive%2520message-passing%2520framework%2520and%2520use%2520it%2520to%250Aefficiently%2520solve%2520the%2520NP-hard%2520problem%2520of%2520Network%2520Dismantling%252C%2520virtually%250Aequivalent%2520to%2520vital%2520node%2520identification%252C%2520with%2520significant%2520real-world%250Aapplications.%2520Trained%2520solely%2520on%2520diversified%2520synthetic%2520networks%252C%2520our%2520proposed%250Amodel%2520--%2520MIND%253A%2520Message%2520Iteration%2520Network%2520Dismantler%2520--%2520generalizes%2520to%2520large%252C%250Aunseen%2520real%2520networks%2520with%2520millions%2520of%2520nodes%252C%2520outperforming%2520state-of-the-art%250Anetwork%2520dismantling%2520methods.%2520Increased%2520efficiency%2520and%2520generalizability%2520of%2520the%250Aproposed%2520model%2520can%2520be%2520leveraged%2520beyond%2520dismantling%2520in%2520a%2520range%2520of%2520complex%250Anetwork%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Network%20Dismantling%20without%20Handcrafted%20Inputs&entry.906535625=Haozhe%20Tian%20and%20Pietro%20Ferraro%20and%20Robert%20Shorten%20and%20Mahdi%20Jalili%20and%20Homayoun%20Hamedmoghadam&entry.1292438233=%20%20The%20application%20of%20message-passing%20Graph%20Neural%20Networks%20has%20been%20a%0Abreakthrough%20for%20important%20network%20science%20problems.%20However%2C%20the%20competitive%0Aperformance%20often%20relies%20on%20using%20handcrafted%20structural%20features%20as%20inputs%2C%0Awhich%20increases%20computational%20cost%20and%20introduces%20bias%20into%20the%20otherwise%0Apurely%20data-driven%20network%20representations.%20Here%2C%20we%20eliminate%20the%20need%20for%0Ahandcrafted%20features%20by%20introducing%20an%20attention%20mechanism%20and%20utilizing%0Amessage-iteration%20profiles%2C%20in%20addition%20to%20an%20effective%20algorithmic%20approach%20to%0Agenerate%20a%20structurally%20diverse%20training%20set%20of%20small%20synthetic%20networks.%0AThereby%2C%20we%20build%20an%20expressive%20message-passing%20framework%20and%20use%20it%20to%0Aefficiently%20solve%20the%20NP-hard%20problem%20of%20Network%20Dismantling%2C%20virtually%0Aequivalent%20to%20vital%20node%20identification%2C%20with%20significant%20real-world%0Aapplications.%20Trained%20solely%20on%20diversified%20synthetic%20networks%2C%20our%20proposed%0Amodel%20--%20MIND%3A%20Message%20Iteration%20Network%20Dismantler%20--%20generalizes%20to%20large%2C%0Aunseen%20real%20networks%20with%20millions%20of%20nodes%2C%20outperforming%20state-of-the-art%0Anetwork%20dismantling%20methods.%20Increased%20efficiency%20and%20generalizability%20of%20the%0Aproposed%20model%20can%20be%20leveraged%20beyond%20dismantling%20in%20a%20range%20of%20complex%0Anetwork%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00706v1&entry.124074799=Read"},
{"title": "Backdoor Attacks on Deep Learning Face Detection", "author": "Quentin Le Roux and Yannick Teglia and Teddy Furon and Philippe Loubet-Moundi", "abstract": "  Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.\n", "link": "http://arxiv.org/abs/2508.00620v1", "date": "2025-08-01", "relevancy": 1.9452, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5347}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4828}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backdoor%20Attacks%20on%20Deep%20Learning%20Face%20Detection&body=Title%3A%20Backdoor%20Attacks%20on%20Deep%20Learning%20Face%20Detection%0AAuthor%3A%20Quentin%20Le%20Roux%20and%20Yannick%20Teglia%20and%20Teddy%20Furon%20and%20Philippe%20Loubet-Moundi%0AAbstract%3A%20%20%20Face%20Recognition%20Systems%20that%20operate%20in%20unconstrained%20environments%20capture%0Aimages%20under%20varying%20conditions%2Csuch%20as%20inconsistent%20lighting%2C%20or%20diverse%20face%0Aposes.%20These%20challenges%20require%20including%20a%20Face%20Detection%20module%20that%0Aregresses%20bounding%20boxes%20and%20landmark%20coordinates%20for%20proper%20Face%20Alignment.%0AThis%20paper%20shows%20the%20effectiveness%20of%20Object%20Generation%20Attacks%20on%20Face%0ADetection%2C%20dubbed%20Face%20Generation%20Attacks%2C%20and%20demonstrates%20for%20the%20first%20time%0Aa%20Landmark%20Shift%20Attack%20that%20backdoors%20the%20coordinate%20regression%20task%20performed%0Aby%20face%20detectors.%20We%20then%20offer%20mitigations%20against%20these%20vulnerabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackdoor%2520Attacks%2520on%2520Deep%2520Learning%2520Face%2520Detection%26entry.906535625%3DQuentin%2520Le%2520Roux%2520and%2520Yannick%2520Teglia%2520and%2520Teddy%2520Furon%2520and%2520Philippe%2520Loubet-Moundi%26entry.1292438233%3D%2520%2520Face%2520Recognition%2520Systems%2520that%2520operate%2520in%2520unconstrained%2520environments%2520capture%250Aimages%2520under%2520varying%2520conditions%252Csuch%2520as%2520inconsistent%2520lighting%252C%2520or%2520diverse%2520face%250Aposes.%2520These%2520challenges%2520require%2520including%2520a%2520Face%2520Detection%2520module%2520that%250Aregresses%2520bounding%2520boxes%2520and%2520landmark%2520coordinates%2520for%2520proper%2520Face%2520Alignment.%250AThis%2520paper%2520shows%2520the%2520effectiveness%2520of%2520Object%2520Generation%2520Attacks%2520on%2520Face%250ADetection%252C%2520dubbed%2520Face%2520Generation%2520Attacks%252C%2520and%2520demonstrates%2520for%2520the%2520first%2520time%250Aa%2520Landmark%2520Shift%2520Attack%2520that%2520backdoors%2520the%2520coordinate%2520regression%2520task%2520performed%250Aby%2520face%2520detectors.%2520We%2520then%2520offer%2520mitigations%2520against%2520these%2520vulnerabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backdoor%20Attacks%20on%20Deep%20Learning%20Face%20Detection&entry.906535625=Quentin%20Le%20Roux%20and%20Yannick%20Teglia%20and%20Teddy%20Furon%20and%20Philippe%20Loubet-Moundi&entry.1292438233=%20%20Face%20Recognition%20Systems%20that%20operate%20in%20unconstrained%20environments%20capture%0Aimages%20under%20varying%20conditions%2Csuch%20as%20inconsistent%20lighting%2C%20or%20diverse%20face%0Aposes.%20These%20challenges%20require%20including%20a%20Face%20Detection%20module%20that%0Aregresses%20bounding%20boxes%20and%20landmark%20coordinates%20for%20proper%20Face%20Alignment.%0AThis%20paper%20shows%20the%20effectiveness%20of%20Object%20Generation%20Attacks%20on%20Face%0ADetection%2C%20dubbed%20Face%20Generation%20Attacks%2C%20and%20demonstrates%20for%20the%20first%20time%0Aa%20Landmark%20Shift%20Attack%20that%20backdoors%20the%20coordinate%20regression%20task%20performed%0Aby%20face%20detectors.%20We%20then%20offer%20mitigations%20against%20these%20vulnerabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00620v1&entry.124074799=Read"},
{"title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "author": "Sakshi Arya", "abstract": "  We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.\n", "link": "http://arxiv.org/abs/2505.10498v2", "date": "2025-08-01", "relevancy": 1.9423, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4862}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Batched%20Nonparametric%20Bandits%20via%20k-Nearest%20Neighbor%20UCB&body=Title%3A%20Batched%20Nonparametric%20Bandits%20via%20k-Nearest%20Neighbor%20UCB%0AAuthor%3A%20Sakshi%20Arya%0AAbstract%3A%20%20%20We%20study%20sequential%20decision-making%20in%20batched%20nonparametric%20contextual%0Abandits%2C%20where%20actions%20are%20selected%20over%20a%20finite%20horizon%20divided%20into%20a%20small%0Anumber%20of%20batches.%20Motivated%20by%20constraints%20in%20domains%20such%20as%20medicine%20and%0Amarketing%20--%20where%20online%20feedback%20is%20limited%20--%20we%20propose%20a%20nonparametric%0Aalgorithm%20that%20combines%20adaptive%20k-nearest%20neighbor%20%28k-NN%29%20regression%20with%20the%0Aupper%20confidence%20bound%20%28UCB%29%20principle.%20Our%20method%2C%20BaNk-UCB%2C%20is%20fully%0Anonparametric%2C%20adapts%20to%20the%20context%20dimension%2C%20and%20is%20simple%20to%20implement.%0AUnlike%20prior%20work%20relying%20on%20parametric%20or%20binning-based%20estimators%2C%20BaNk-UCB%0Auses%20local%20geometry%20to%20estimate%20rewards%20and%20adaptively%20balances%20exploration%20and%0Aexploitation.%20We%20provide%20near-optimal%20regret%20guarantees%20under%20standard%0ALipschitz%20smoothness%20and%20margin%20assumptions%2C%20using%20a%20theoretically%20motivated%0Abatch%20schedule%20that%20balances%20regret%20across%20batches%20and%20achieves%20minimax-optimal%0Arates.%20Empirical%20evaluations%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20BaNk-UCB%20consistently%20outperforms%20binning-based%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatched%2520Nonparametric%2520Bandits%2520via%2520k-Nearest%2520Neighbor%2520UCB%26entry.906535625%3DSakshi%2520Arya%26entry.1292438233%3D%2520%2520We%2520study%2520sequential%2520decision-making%2520in%2520batched%2520nonparametric%2520contextual%250Abandits%252C%2520where%2520actions%2520are%2520selected%2520over%2520a%2520finite%2520horizon%2520divided%2520into%2520a%2520small%250Anumber%2520of%2520batches.%2520Motivated%2520by%2520constraints%2520in%2520domains%2520such%2520as%2520medicine%2520and%250Amarketing%2520--%2520where%2520online%2520feedback%2520is%2520limited%2520--%2520we%2520propose%2520a%2520nonparametric%250Aalgorithm%2520that%2520combines%2520adaptive%2520k-nearest%2520neighbor%2520%2528k-NN%2529%2520regression%2520with%2520the%250Aupper%2520confidence%2520bound%2520%2528UCB%2529%2520principle.%2520Our%2520method%252C%2520BaNk-UCB%252C%2520is%2520fully%250Anonparametric%252C%2520adapts%2520to%2520the%2520context%2520dimension%252C%2520and%2520is%2520simple%2520to%2520implement.%250AUnlike%2520prior%2520work%2520relying%2520on%2520parametric%2520or%2520binning-based%2520estimators%252C%2520BaNk-UCB%250Auses%2520local%2520geometry%2520to%2520estimate%2520rewards%2520and%2520adaptively%2520balances%2520exploration%2520and%250Aexploitation.%2520We%2520provide%2520near-optimal%2520regret%2520guarantees%2520under%2520standard%250ALipschitz%2520smoothness%2520and%2520margin%2520assumptions%252C%2520using%2520a%2520theoretically%2520motivated%250Abatch%2520schedule%2520that%2520balances%2520regret%2520across%2520batches%2520and%2520achieves%2520minimax-optimal%250Arates.%2520Empirical%2520evaluations%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%250Athat%2520BaNk-UCB%2520consistently%2520outperforms%2520binning-based%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batched%20Nonparametric%20Bandits%20via%20k-Nearest%20Neighbor%20UCB&entry.906535625=Sakshi%20Arya&entry.1292438233=%20%20We%20study%20sequential%20decision-making%20in%20batched%20nonparametric%20contextual%0Abandits%2C%20where%20actions%20are%20selected%20over%20a%20finite%20horizon%20divided%20into%20a%20small%0Anumber%20of%20batches.%20Motivated%20by%20constraints%20in%20domains%20such%20as%20medicine%20and%0Amarketing%20--%20where%20online%20feedback%20is%20limited%20--%20we%20propose%20a%20nonparametric%0Aalgorithm%20that%20combines%20adaptive%20k-nearest%20neighbor%20%28k-NN%29%20regression%20with%20the%0Aupper%20confidence%20bound%20%28UCB%29%20principle.%20Our%20method%2C%20BaNk-UCB%2C%20is%20fully%0Anonparametric%2C%20adapts%20to%20the%20context%20dimension%2C%20and%20is%20simple%20to%20implement.%0AUnlike%20prior%20work%20relying%20on%20parametric%20or%20binning-based%20estimators%2C%20BaNk-UCB%0Auses%20local%20geometry%20to%20estimate%20rewards%20and%20adaptively%20balances%20exploration%20and%0Aexploitation.%20We%20provide%20near-optimal%20regret%20guarantees%20under%20standard%0ALipschitz%20smoothness%20and%20margin%20assumptions%2C%20using%20a%20theoretically%20motivated%0Abatch%20schedule%20that%20balances%20regret%20across%20batches%20and%20achieves%20minimax-optimal%0Arates.%20Empirical%20evaluations%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%0Athat%20BaNk-UCB%20consistently%20outperforms%20binning-based%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10498v2&entry.124074799=Read"},
{"title": "From Press to Pixels: Evolving Urdu Text Recognition", "author": "Samee Arif and Sualeha Farid", "abstract": "  This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR.\n", "link": "http://arxiv.org/abs/2505.13943v2", "date": "2025-08-01", "relevancy": 1.9367, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4827}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Press%20to%20Pixels%3A%20Evolving%20Urdu%20Text%20Recognition&body=Title%3A%20From%20Press%20to%20Pixels%3A%20Evolving%20Urdu%20Text%20Recognition%0AAuthor%3A%20Samee%20Arif%20and%20Sualeha%20Farid%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20end-to-end%20pipeline%20for%20Optical%20Character%0ARecognition%20%28OCR%29%20on%20Urdu%20newspapers%2C%20addressing%20challenges%20posed%20by%20complex%0Amulti-column%20layouts%2C%20low-resolution%20scans%2C%20and%20the%20stylistic%20variability%20of%0Athe%20Nastaliq%20script.%20Our%20system%20comprises%20four%20modules%3A%20%281%29%20article%0Asegmentation%2C%20%282%29%20image%20super-resolution%2C%20%283%29%20column%20segmentation%2C%20and%20%284%29%20text%0Arecognition.%20We%20fine-tune%20YOLOv11x%20for%20segmentation%2C%20achieving%200.963%20precision%0Afor%20articles%20and%200.970%20for%20columns.%20A%20SwinIR-based%20super-resolution%20model%0Aboosts%20LLM%20text%20recognition%20accuracy%20by%2025-70%25.%20We%20also%20introduce%20the%20Urdu%0ANewspaper%20Benchmark%20%28UNB%29%2C%20a%20manually%20annotated%20dataset%20for%20Urdu%20OCR.%20Using%20UNB%0Aand%20the%20OpenITI%20corpus%2C%20we%20compare%20traditional%20CNN%2BRNN-based%20OCR%20models%20with%0Amodern%20LLMs.%20Gemini-2.5-Pro%20achieves%20the%20best%20performance%20with%20a%20WER%20of%200.133.%0AWe%20further%20analyze%20LLM%20outputs%20via%20insertion%2C%20deletion%2C%20and%20substitution%20error%0Abreakdowns%2C%20as%20well%20as%20character-level%20confusion%20analysis.%20Finally%2C%20we%20show%0Athat%20fine-tuning%20on%20just%20500%20samples%20yields%20a%206.13%25%20WER%20improvement%2C%0Ahighlighting%20the%20adaptability%20of%20LLMs%20for%20Urdu%20OCR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Press%2520to%2520Pixels%253A%2520Evolving%2520Urdu%2520Text%2520Recognition%26entry.906535625%3DSamee%2520Arif%2520and%2520Sualeha%2520Farid%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520end-to-end%2520pipeline%2520for%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520on%2520Urdu%2520newspapers%252C%2520addressing%2520challenges%2520posed%2520by%2520complex%250Amulti-column%2520layouts%252C%2520low-resolution%2520scans%252C%2520and%2520the%2520stylistic%2520variability%2520of%250Athe%2520Nastaliq%2520script.%2520Our%2520system%2520comprises%2520four%2520modules%253A%2520%25281%2529%2520article%250Asegmentation%252C%2520%25282%2529%2520image%2520super-resolution%252C%2520%25283%2529%2520column%2520segmentation%252C%2520and%2520%25284%2529%2520text%250Arecognition.%2520We%2520fine-tune%2520YOLOv11x%2520for%2520segmentation%252C%2520achieving%25200.963%2520precision%250Afor%2520articles%2520and%25200.970%2520for%2520columns.%2520A%2520SwinIR-based%2520super-resolution%2520model%250Aboosts%2520LLM%2520text%2520recognition%2520accuracy%2520by%252025-70%2525.%2520We%2520also%2520introduce%2520the%2520Urdu%250ANewspaper%2520Benchmark%2520%2528UNB%2529%252C%2520a%2520manually%2520annotated%2520dataset%2520for%2520Urdu%2520OCR.%2520Using%2520UNB%250Aand%2520the%2520OpenITI%2520corpus%252C%2520we%2520compare%2520traditional%2520CNN%252BRNN-based%2520OCR%2520models%2520with%250Amodern%2520LLMs.%2520Gemini-2.5-Pro%2520achieves%2520the%2520best%2520performance%2520with%2520a%2520WER%2520of%25200.133.%250AWe%2520further%2520analyze%2520LLM%2520outputs%2520via%2520insertion%252C%2520deletion%252C%2520and%2520substitution%2520error%250Abreakdowns%252C%2520as%2520well%2520as%2520character-level%2520confusion%2520analysis.%2520Finally%252C%2520we%2520show%250Athat%2520fine-tuning%2520on%2520just%2520500%2520samples%2520yields%2520a%25206.13%2525%2520WER%2520improvement%252C%250Ahighlighting%2520the%2520adaptability%2520of%2520LLMs%2520for%2520Urdu%2520OCR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Press%20to%20Pixels%3A%20Evolving%20Urdu%20Text%20Recognition&entry.906535625=Samee%20Arif%20and%20Sualeha%20Farid&entry.1292438233=%20%20This%20paper%20introduces%20an%20end-to-end%20pipeline%20for%20Optical%20Character%0ARecognition%20%28OCR%29%20on%20Urdu%20newspapers%2C%20addressing%20challenges%20posed%20by%20complex%0Amulti-column%20layouts%2C%20low-resolution%20scans%2C%20and%20the%20stylistic%20variability%20of%0Athe%20Nastaliq%20script.%20Our%20system%20comprises%20four%20modules%3A%20%281%29%20article%0Asegmentation%2C%20%282%29%20image%20super-resolution%2C%20%283%29%20column%20segmentation%2C%20and%20%284%29%20text%0Arecognition.%20We%20fine-tune%20YOLOv11x%20for%20segmentation%2C%20achieving%200.963%20precision%0Afor%20articles%20and%200.970%20for%20columns.%20A%20SwinIR-based%20super-resolution%20model%0Aboosts%20LLM%20text%20recognition%20accuracy%20by%2025-70%25.%20We%20also%20introduce%20the%20Urdu%0ANewspaper%20Benchmark%20%28UNB%29%2C%20a%20manually%20annotated%20dataset%20for%20Urdu%20OCR.%20Using%20UNB%0Aand%20the%20OpenITI%20corpus%2C%20we%20compare%20traditional%20CNN%2BRNN-based%20OCR%20models%20with%0Amodern%20LLMs.%20Gemini-2.5-Pro%20achieves%20the%20best%20performance%20with%20a%20WER%20of%200.133.%0AWe%20further%20analyze%20LLM%20outputs%20via%20insertion%2C%20deletion%2C%20and%20substitution%20error%0Abreakdowns%2C%20as%20well%20as%20character-level%20confusion%20analysis.%20Finally%2C%20we%20show%0Athat%20fine-tuning%20on%20just%20500%20samples%20yields%20a%206.13%25%20WER%20improvement%2C%0Ahighlighting%20the%20adaptability%20of%20LLMs%20for%20Urdu%20OCR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13943v2&entry.124074799=Read"},
{"title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for\n  Failure Analysis of Nonlinear Stochastic Systems", "author": "Liuyun Xu and Seymour M. J. Spence", "abstract": "  Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.\n", "link": "http://arxiv.org/abs/2508.00734v1", "date": "2025-08-01", "relevancy": 1.9337, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4817}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Machine%20Learning-Driven%20Multi-Fidelity%20Stratified%20Sampling%20for%0A%20%20Failure%20Analysis%20of%20Nonlinear%20Stochastic%20Systems&body=Title%3A%20Adaptive%20Machine%20Learning-Driven%20Multi-Fidelity%20Stratified%20Sampling%20for%0A%20%20Failure%20Analysis%20of%20Nonlinear%20Stochastic%20Systems%0AAuthor%3A%20Liuyun%20Xu%20and%20Seymour%20M.%20J.%20Spence%0AAbstract%3A%20%20%20Existing%20variance%20reduction%20techniques%20used%20in%20stochastic%20simulations%20for%0Arare%20event%20analysis%20still%20require%20a%20substantial%20number%20of%20model%20evaluations%20to%0Aestimate%20small%20failure%20probabilities.%20In%20the%20context%20of%20complex%2C%20nonlinear%0Afinite%20element%20modeling%20environments%2C%20this%20can%20become%20computationally%0Achallenging-particularly%20for%20systems%20subjected%20to%20stochastic%20excitation.%20To%0Aaddress%20this%20challenge%2C%20a%20multi-fidelity%20stratified%20sampling%20scheme%20with%0Aadaptive%20machine%20learning%20metamodels%20is%20introduced%20for%20efficiently%20propagating%0Auncertainties%20and%20estimating%20small%20failure%20probabilities.%20In%20this%20approach%2C%20a%0Ahigh-fidelity%20dataset%20generated%20through%20stratified%20sampling%20is%20used%20to%20train%20a%0Adeep%20learning-based%20metamodel%2C%20which%20then%20serves%20as%20a%20cost-effective%20and%20highly%0Acorrelated%20low-fidelity%20model.%20An%20adaptive%20training%20scheme%20is%20proposed%20to%0Abalance%20the%20trade-off%20between%20approximation%20quality%20and%20computational%20demand%0Aassociated%20with%20the%20development%20of%20the%20low-fidelity%20model.%20By%20integrating%20the%0Alow-fidelity%20outputs%20with%20additional%20high-fidelity%20results%2C%20an%20unbiased%0Aestimate%20of%20the%20strata-wise%20failure%20probabilities%20is%20obtained%20using%20a%0Amulti-fidelity%20Monte%20Carlo%20framework.%20The%20overall%20probability%20of%20failure%20is%0Athen%20computed%20using%20the%20total%20probability%20theorem.%20Application%20to%20a%20full-scale%0Ahigh-rise%20steel%20building%20subjected%20to%20stochastic%20wind%20excitation%20demonstrates%0Athat%20the%20proposed%20scheme%20can%20accurately%20estimate%20exceedance%20probability%20curves%0Afor%20nonlinear%20responses%20of%20interest%2C%20while%20achieving%20significant%20computational%0Asavings%20compared%20to%20single-fidelity%20variance%20reduction%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Machine%2520Learning-Driven%2520Multi-Fidelity%2520Stratified%2520Sampling%2520for%250A%2520%2520Failure%2520Analysis%2520of%2520Nonlinear%2520Stochastic%2520Systems%26entry.906535625%3DLiuyun%2520Xu%2520and%2520Seymour%2520M.%2520J.%2520Spence%26entry.1292438233%3D%2520%2520Existing%2520variance%2520reduction%2520techniques%2520used%2520in%2520stochastic%2520simulations%2520for%250Arare%2520event%2520analysis%2520still%2520require%2520a%2520substantial%2520number%2520of%2520model%2520evaluations%2520to%250Aestimate%2520small%2520failure%2520probabilities.%2520In%2520the%2520context%2520of%2520complex%252C%2520nonlinear%250Afinite%2520element%2520modeling%2520environments%252C%2520this%2520can%2520become%2520computationally%250Achallenging-particularly%2520for%2520systems%2520subjected%2520to%2520stochastic%2520excitation.%2520To%250Aaddress%2520this%2520challenge%252C%2520a%2520multi-fidelity%2520stratified%2520sampling%2520scheme%2520with%250Aadaptive%2520machine%2520learning%2520metamodels%2520is%2520introduced%2520for%2520efficiently%2520propagating%250Auncertainties%2520and%2520estimating%2520small%2520failure%2520probabilities.%2520In%2520this%2520approach%252C%2520a%250Ahigh-fidelity%2520dataset%2520generated%2520through%2520stratified%2520sampling%2520is%2520used%2520to%2520train%2520a%250Adeep%2520learning-based%2520metamodel%252C%2520which%2520then%2520serves%2520as%2520a%2520cost-effective%2520and%2520highly%250Acorrelated%2520low-fidelity%2520model.%2520An%2520adaptive%2520training%2520scheme%2520is%2520proposed%2520to%250Abalance%2520the%2520trade-off%2520between%2520approximation%2520quality%2520and%2520computational%2520demand%250Aassociated%2520with%2520the%2520development%2520of%2520the%2520low-fidelity%2520model.%2520By%2520integrating%2520the%250Alow-fidelity%2520outputs%2520with%2520additional%2520high-fidelity%2520results%252C%2520an%2520unbiased%250Aestimate%2520of%2520the%2520strata-wise%2520failure%2520probabilities%2520is%2520obtained%2520using%2520a%250Amulti-fidelity%2520Monte%2520Carlo%2520framework.%2520The%2520overall%2520probability%2520of%2520failure%2520is%250Athen%2520computed%2520using%2520the%2520total%2520probability%2520theorem.%2520Application%2520to%2520a%2520full-scale%250Ahigh-rise%2520steel%2520building%2520subjected%2520to%2520stochastic%2520wind%2520excitation%2520demonstrates%250Athat%2520the%2520proposed%2520scheme%2520can%2520accurately%2520estimate%2520exceedance%2520probability%2520curves%250Afor%2520nonlinear%2520responses%2520of%2520interest%252C%2520while%2520achieving%2520significant%2520computational%250Asavings%2520compared%2520to%2520single-fidelity%2520variance%2520reduction%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Machine%20Learning-Driven%20Multi-Fidelity%20Stratified%20Sampling%20for%0A%20%20Failure%20Analysis%20of%20Nonlinear%20Stochastic%20Systems&entry.906535625=Liuyun%20Xu%20and%20Seymour%20M.%20J.%20Spence&entry.1292438233=%20%20Existing%20variance%20reduction%20techniques%20used%20in%20stochastic%20simulations%20for%0Arare%20event%20analysis%20still%20require%20a%20substantial%20number%20of%20model%20evaluations%20to%0Aestimate%20small%20failure%20probabilities.%20In%20the%20context%20of%20complex%2C%20nonlinear%0Afinite%20element%20modeling%20environments%2C%20this%20can%20become%20computationally%0Achallenging-particularly%20for%20systems%20subjected%20to%20stochastic%20excitation.%20To%0Aaddress%20this%20challenge%2C%20a%20multi-fidelity%20stratified%20sampling%20scheme%20with%0Aadaptive%20machine%20learning%20metamodels%20is%20introduced%20for%20efficiently%20propagating%0Auncertainties%20and%20estimating%20small%20failure%20probabilities.%20In%20this%20approach%2C%20a%0Ahigh-fidelity%20dataset%20generated%20through%20stratified%20sampling%20is%20used%20to%20train%20a%0Adeep%20learning-based%20metamodel%2C%20which%20then%20serves%20as%20a%20cost-effective%20and%20highly%0Acorrelated%20low-fidelity%20model.%20An%20adaptive%20training%20scheme%20is%20proposed%20to%0Abalance%20the%20trade-off%20between%20approximation%20quality%20and%20computational%20demand%0Aassociated%20with%20the%20development%20of%20the%20low-fidelity%20model.%20By%20integrating%20the%0Alow-fidelity%20outputs%20with%20additional%20high-fidelity%20results%2C%20an%20unbiased%0Aestimate%20of%20the%20strata-wise%20failure%20probabilities%20is%20obtained%20using%20a%0Amulti-fidelity%20Monte%20Carlo%20framework.%20The%20overall%20probability%20of%20failure%20is%0Athen%20computed%20using%20the%20total%20probability%20theorem.%20Application%20to%20a%20full-scale%0Ahigh-rise%20steel%20building%20subjected%20to%20stochastic%20wind%20excitation%20demonstrates%0Athat%20the%20proposed%20scheme%20can%20accurately%20estimate%20exceedance%20probability%20curves%0Afor%20nonlinear%20responses%20of%20interest%2C%20while%20achieving%20significant%20computational%0Asavings%20compared%20to%20single-fidelity%20variance%20reduction%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00734v1&entry.124074799=Read"},
{"title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms", "author": "Shiyi Liu and Buwen Liang and Yuetong Fang and Zixuan Jiang and Renjing Xu", "abstract": "  Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.\n", "link": "http://arxiv.org/abs/2507.02724v2", "date": "2025-08-01", "relevancy": 1.9309, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms&body=Title%3A%20Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms%0AAuthor%3A%20Shiyi%20Liu%20and%20Buwen%20Liang%20and%20Yuetong%20Fang%20and%20Zixuan%20Jiang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI%20for%20science%20have%20highlighted%20the%20power%20of%20contrastive%0Alearning%20in%20bridging%20heterogeneous%20biological%20data%20modalities.%20Building%20on%20this%0Aparadigm%2C%20we%20propose%20HIPPO%20%28HIerarchical%20Protein-Protein%20interaction%20prediction%0Aacross%20Organisms%29%2C%20a%20hierarchical%20contrastive%20framework%20for%20protein-protein%0Ainteraction%28PPI%29%20prediction%2C%20where%20protein%20sequences%20and%20their%20hierarchical%0Aattributes%20are%20aligned%20through%20multi-tiered%20biological%20representation%20matching.%0AThe%20proposed%20approach%20incorporates%20hierarchical%20contrastive%20loss%20functions%20that%0Aemulate%20the%20structured%20relationship%20among%20functional%20classes%20of%20proteins.%20The%0Aframework%20adaptively%20incorporates%20domain%20and%20family%20knowledge%20through%20a%0Adata-driven%20penalty%20mechanism%2C%20enforcing%20consistency%20between%20the%20learned%0Aembedding%20space%20and%20the%20intrinsic%20hierarchy%20of%20protein%20functions.%20Experiments%0Aon%20benchmark%20datasets%20demonstrate%20that%20HIPPO%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20existing%20methods%20and%20showing%20robustness%20in%20low-data%0Aregimes.%20Notably%2C%20the%20model%20demonstrates%20strong%20zero-shot%20transferability%20to%0Aother%20species%20without%20retraining%2C%20enabling%20reliable%20PPI%20prediction%20and%0Afunctional%20inference%20even%20in%20less%20characterized%20or%20rare%20organisms%20where%0Aexperimental%20data%20are%20limited.%20Further%20analysis%20reveals%20that%20hierarchical%0Afeature%20fusion%20is%20critical%20for%20capturing%20conserved%20interaction%20determinants%2C%0Asuch%20as%20binding%20motifs%20and%20functional%20annotations.%20This%20work%20advances%0Across-species%20PPI%20prediction%20and%20provides%20a%20unified%20framework%20for%20interaction%0Aprediction%20in%20scenarios%20with%20sparse%20or%20imbalanced%20multi-species%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multi-Label%2520Contrastive%2520Learning%2520for%2520Protein-Protein%250A%2520%2520Interaction%2520Prediction%2520Across%2520Organisms%26entry.906535625%3DShiyi%2520Liu%2520and%2520Buwen%2520Liang%2520and%2520Yuetong%2520Fang%2520and%2520Zixuan%2520Jiang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI%2520for%2520science%2520have%2520highlighted%2520the%2520power%2520of%2520contrastive%250Alearning%2520in%2520bridging%2520heterogeneous%2520biological%2520data%2520modalities.%2520Building%2520on%2520this%250Aparadigm%252C%2520we%2520propose%2520HIPPO%2520%2528HIerarchical%2520Protein-Protein%2520interaction%2520prediction%250Aacross%2520Organisms%2529%252C%2520a%2520hierarchical%2520contrastive%2520framework%2520for%2520protein-protein%250Ainteraction%2528PPI%2529%2520prediction%252C%2520where%2520protein%2520sequences%2520and%2520their%2520hierarchical%250Aattributes%2520are%2520aligned%2520through%2520multi-tiered%2520biological%2520representation%2520matching.%250AThe%2520proposed%2520approach%2520incorporates%2520hierarchical%2520contrastive%2520loss%2520functions%2520that%250Aemulate%2520the%2520structured%2520relationship%2520among%2520functional%2520classes%2520of%2520proteins.%2520The%250Aframework%2520adaptively%2520incorporates%2520domain%2520and%2520family%2520knowledge%2520through%2520a%250Adata-driven%2520penalty%2520mechanism%252C%2520enforcing%2520consistency%2520between%2520the%2520learned%250Aembedding%2520space%2520and%2520the%2520intrinsic%2520hierarchy%2520of%2520protein%2520functions.%2520Experiments%250Aon%2520benchmark%2520datasets%2520demonstrate%2520that%2520HIPPO%2520achieves%2520state-of-the-art%250Aperformance%252C%2520outperforming%2520existing%2520methods%2520and%2520showing%2520robustness%2520in%2520low-data%250Aregimes.%2520Notably%252C%2520the%2520model%2520demonstrates%2520strong%2520zero-shot%2520transferability%2520to%250Aother%2520species%2520without%2520retraining%252C%2520enabling%2520reliable%2520PPI%2520prediction%2520and%250Afunctional%2520inference%2520even%2520in%2520less%2520characterized%2520or%2520rare%2520organisms%2520where%250Aexperimental%2520data%2520are%2520limited.%2520Further%2520analysis%2520reveals%2520that%2520hierarchical%250Afeature%2520fusion%2520is%2520critical%2520for%2520capturing%2520conserved%2520interaction%2520determinants%252C%250Asuch%2520as%2520binding%2520motifs%2520and%2520functional%2520annotations.%2520This%2520work%2520advances%250Across-species%2520PPI%2520prediction%2520and%2520provides%2520a%2520unified%2520framework%2520for%2520interaction%250Aprediction%2520in%2520scenarios%2520with%2520sparse%2520or%2520imbalanced%2520multi-species%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multi-Label%20Contrastive%20Learning%20for%20Protein-Protein%0A%20%20Interaction%20Prediction%20Across%20Organisms&entry.906535625=Shiyi%20Liu%20and%20Buwen%20Liang%20and%20Yuetong%20Fang%20and%20Zixuan%20Jiang%20and%20Renjing%20Xu&entry.1292438233=%20%20Recent%20advances%20in%20AI%20for%20science%20have%20highlighted%20the%20power%20of%20contrastive%0Alearning%20in%20bridging%20heterogeneous%20biological%20data%20modalities.%20Building%20on%20this%0Aparadigm%2C%20we%20propose%20HIPPO%20%28HIerarchical%20Protein-Protein%20interaction%20prediction%0Aacross%20Organisms%29%2C%20a%20hierarchical%20contrastive%20framework%20for%20protein-protein%0Ainteraction%28PPI%29%20prediction%2C%20where%20protein%20sequences%20and%20their%20hierarchical%0Aattributes%20are%20aligned%20through%20multi-tiered%20biological%20representation%20matching.%0AThe%20proposed%20approach%20incorporates%20hierarchical%20contrastive%20loss%20functions%20that%0Aemulate%20the%20structured%20relationship%20among%20functional%20classes%20of%20proteins.%20The%0Aframework%20adaptively%20incorporates%20domain%20and%20family%20knowledge%20through%20a%0Adata-driven%20penalty%20mechanism%2C%20enforcing%20consistency%20between%20the%20learned%0Aembedding%20space%20and%20the%20intrinsic%20hierarchy%20of%20protein%20functions.%20Experiments%0Aon%20benchmark%20datasets%20demonstrate%20that%20HIPPO%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20existing%20methods%20and%20showing%20robustness%20in%20low-data%0Aregimes.%20Notably%2C%20the%20model%20demonstrates%20strong%20zero-shot%20transferability%20to%0Aother%20species%20without%20retraining%2C%20enabling%20reliable%20PPI%20prediction%20and%0Afunctional%20inference%20even%20in%20less%20characterized%20or%20rare%20organisms%20where%0Aexperimental%20data%20are%20limited.%20Further%20analysis%20reveals%20that%20hierarchical%0Afeature%20fusion%20is%20critical%20for%20capturing%20conserved%20interaction%20determinants%2C%0Asuch%20as%20binding%20motifs%20and%20functional%20annotations.%20This%20work%20advances%0Across-species%20PPI%20prediction%20and%20provides%20a%20unified%20framework%20for%20interaction%0Aprediction%20in%20scenarios%20with%20sparse%20or%20imbalanced%20multi-species%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02724v2&entry.124074799=Read"},
{"title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation", "author": "Jiakai Tang and Sunhao Dai and Teng Shi and Jun Xu and Xu Chen and Wen Chen and Jian Wu and Yuning Jiang", "abstract": "  Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.\n", "link": "http://arxiv.org/abs/2503.22675v3", "date": "2025-08-01", "relevancy": 1.9254, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Before%20Recommend%3A%20Unleashing%20the%20Latent%20Reasoning%20Power%20for%0A%20%20Sequential%20Recommendation&body=Title%3A%20Think%20Before%20Recommend%3A%20Unleashing%20the%20Latent%20Reasoning%20Power%20for%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Jiakai%20Tang%20and%20Sunhao%20Dai%20and%20Teng%20Shi%20and%20Jun%20Xu%20and%20Xu%20Chen%20and%20Wen%20Chen%20and%20Jian%20Wu%20and%20Yuning%20Jiang%0AAbstract%3A%20%20%20Sequential%20Recommendation%20%28SeqRec%29%20aims%20to%20predict%20the%20next%20item%20by%20capturing%0Asequential%20patterns%20from%20users%27%20historical%20interactions%2C%20playing%20a%20crucial%20role%0Ain%20many%20real-world%20recommender%20systems.%20However%2C%20existing%20approaches%0Apredominantly%20adopt%20a%20direct%20forward%20computation%20paradigm%2C%20where%20the%20final%0Ahidden%20state%20of%20the%20sequence%20encoder%20serves%20as%20the%20user%20representation.%20We%0Aargue%20that%20this%20inference%20paradigm%2C%20due%20to%20its%20limited%20computational%20depth%2C%0Astruggles%20to%20model%20the%20complex%20evolving%20nature%20of%20user%20preferences%20and%20lacks%20a%0Anuanced%20understanding%20of%20long-tail%20items%2C%20leading%20to%20suboptimal%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20%5Ctextbf%7BReaRec%7D%2C%20the%20first%20inference-time%0Acomputing%20framework%20for%20recommender%20systems%2C%20which%20enhances%20user%0Arepresentations%20through%20implicit%20multi-step%20reasoning.%20Specifically%2C%20ReaRec%0Aautoregressively%20feeds%20the%20sequence%27s%20last%20hidden%20state%20into%20the%20sequential%0Arecommender%20while%20incorporating%20special%20reasoning%20position%20embeddings%20to%0Adecouple%20the%20original%20item%20encoding%20space%20from%20the%20multi-step%20reasoning%20space.%0AMoreover%2C%20we%20introduce%20two%20lightweight%20reasoning-based%20learning%20methods%2C%0AEnsemble%20Reasoning%20Learning%20%28ERL%29%20and%20Progressive%20Reasoning%20Learning%20%28PRL%29%2C%20to%0Afurther%20effectively%20exploit%20ReaRec%27s%20reasoning%20potential.%20Extensive%20experiments%0Aon%20five%20public%20real-world%20datasets%20and%20different%20SeqRec%20architectures%0Ademonstrate%20the%20generality%20and%20effectiveness%20of%20our%20proposed%20ReaRec.%0ARemarkably%2C%20post-hoc%20analyses%20reveal%20that%20ReaRec%20significantly%20elevates%20the%0Aperformance%20ceiling%20of%20multiple%20sequential%20recommendation%20backbones%20by%0Aapproximately%2030%5C%25-50%5C%25.%20Thus%2C%20we%20believe%20this%20work%20can%20open%20a%20new%20and%0Apromising%20avenue%20for%20future%20research%20in%20inference-time%20computing%20for%20sequential%0Arecommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22675v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Before%2520Recommend%253A%2520Unleashing%2520the%2520Latent%2520Reasoning%2520Power%2520for%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DJiakai%2520Tang%2520and%2520Sunhao%2520Dai%2520and%2520Teng%2520Shi%2520and%2520Jun%2520Xu%2520and%2520Xu%2520Chen%2520and%2520Wen%2520Chen%2520and%2520Jian%2520Wu%2520and%2520Yuning%2520Jiang%26entry.1292438233%3D%2520%2520Sequential%2520Recommendation%2520%2528SeqRec%2529%2520aims%2520to%2520predict%2520the%2520next%2520item%2520by%2520capturing%250Asequential%2520patterns%2520from%2520users%2527%2520historical%2520interactions%252C%2520playing%2520a%2520crucial%2520role%250Ain%2520many%2520real-world%2520recommender%2520systems.%2520However%252C%2520existing%2520approaches%250Apredominantly%2520adopt%2520a%2520direct%2520forward%2520computation%2520paradigm%252C%2520where%2520the%2520final%250Ahidden%2520state%2520of%2520the%2520sequence%2520encoder%2520serves%2520as%2520the%2520user%2520representation.%2520We%250Aargue%2520that%2520this%2520inference%2520paradigm%252C%2520due%2520to%2520its%2520limited%2520computational%2520depth%252C%250Astruggles%2520to%2520model%2520the%2520complex%2520evolving%2520nature%2520of%2520user%2520preferences%2520and%2520lacks%2520a%250Anuanced%2520understanding%2520of%2520long-tail%2520items%252C%2520leading%2520to%2520suboptimal%2520performance.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520%255Ctextbf%257BReaRec%257D%252C%2520the%2520first%2520inference-time%250Acomputing%2520framework%2520for%2520recommender%2520systems%252C%2520which%2520enhances%2520user%250Arepresentations%2520through%2520implicit%2520multi-step%2520reasoning.%2520Specifically%252C%2520ReaRec%250Aautoregressively%2520feeds%2520the%2520sequence%2527s%2520last%2520hidden%2520state%2520into%2520the%2520sequential%250Arecommender%2520while%2520incorporating%2520special%2520reasoning%2520position%2520embeddings%2520to%250Adecouple%2520the%2520original%2520item%2520encoding%2520space%2520from%2520the%2520multi-step%2520reasoning%2520space.%250AMoreover%252C%2520we%2520introduce%2520two%2520lightweight%2520reasoning-based%2520learning%2520methods%252C%250AEnsemble%2520Reasoning%2520Learning%2520%2528ERL%2529%2520and%2520Progressive%2520Reasoning%2520Learning%2520%2528PRL%2529%252C%2520to%250Afurther%2520effectively%2520exploit%2520ReaRec%2527s%2520reasoning%2520potential.%2520Extensive%2520experiments%250Aon%2520five%2520public%2520real-world%2520datasets%2520and%2520different%2520SeqRec%2520architectures%250Ademonstrate%2520the%2520generality%2520and%2520effectiveness%2520of%2520our%2520proposed%2520ReaRec.%250ARemarkably%252C%2520post-hoc%2520analyses%2520reveal%2520that%2520ReaRec%2520significantly%2520elevates%2520the%250Aperformance%2520ceiling%2520of%2520multiple%2520sequential%2520recommendation%2520backbones%2520by%250Aapproximately%252030%255C%2525-50%255C%2525.%2520Thus%252C%2520we%2520believe%2520this%2520work%2520can%2520open%2520a%2520new%2520and%250Apromising%2520avenue%2520for%2520future%2520research%2520in%2520inference-time%2520computing%2520for%2520sequential%250Arecommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22675v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Before%20Recommend%3A%20Unleashing%20the%20Latent%20Reasoning%20Power%20for%0A%20%20Sequential%20Recommendation&entry.906535625=Jiakai%20Tang%20and%20Sunhao%20Dai%20and%20Teng%20Shi%20and%20Jun%20Xu%20and%20Xu%20Chen%20and%20Wen%20Chen%20and%20Jian%20Wu%20and%20Yuning%20Jiang&entry.1292438233=%20%20Sequential%20Recommendation%20%28SeqRec%29%20aims%20to%20predict%20the%20next%20item%20by%20capturing%0Asequential%20patterns%20from%20users%27%20historical%20interactions%2C%20playing%20a%20crucial%20role%0Ain%20many%20real-world%20recommender%20systems.%20However%2C%20existing%20approaches%0Apredominantly%20adopt%20a%20direct%20forward%20computation%20paradigm%2C%20where%20the%20final%0Ahidden%20state%20of%20the%20sequence%20encoder%20serves%20as%20the%20user%20representation.%20We%0Aargue%20that%20this%20inference%20paradigm%2C%20due%20to%20its%20limited%20computational%20depth%2C%0Astruggles%20to%20model%20the%20complex%20evolving%20nature%20of%20user%20preferences%20and%20lacks%20a%0Anuanced%20understanding%20of%20long-tail%20items%2C%20leading%20to%20suboptimal%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20%5Ctextbf%7BReaRec%7D%2C%20the%20first%20inference-time%0Acomputing%20framework%20for%20recommender%20systems%2C%20which%20enhances%20user%0Arepresentations%20through%20implicit%20multi-step%20reasoning.%20Specifically%2C%20ReaRec%0Aautoregressively%20feeds%20the%20sequence%27s%20last%20hidden%20state%20into%20the%20sequential%0Arecommender%20while%20incorporating%20special%20reasoning%20position%20embeddings%20to%0Adecouple%20the%20original%20item%20encoding%20space%20from%20the%20multi-step%20reasoning%20space.%0AMoreover%2C%20we%20introduce%20two%20lightweight%20reasoning-based%20learning%20methods%2C%0AEnsemble%20Reasoning%20Learning%20%28ERL%29%20and%20Progressive%20Reasoning%20Learning%20%28PRL%29%2C%20to%0Afurther%20effectively%20exploit%20ReaRec%27s%20reasoning%20potential.%20Extensive%20experiments%0Aon%20five%20public%20real-world%20datasets%20and%20different%20SeqRec%20architectures%0Ademonstrate%20the%20generality%20and%20effectiveness%20of%20our%20proposed%20ReaRec.%0ARemarkably%2C%20post-hoc%20analyses%20reveal%20that%20ReaRec%20significantly%20elevates%20the%0Aperformance%20ceiling%20of%20multiple%20sequential%20recommendation%20backbones%20by%0Aapproximately%2030%5C%25-50%5C%25.%20Thus%2C%20we%20believe%20this%20work%20can%20open%20a%20new%20and%0Apromising%20avenue%20for%20future%20research%20in%20inference-time%20computing%20for%20sequential%0Arecommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22675v3&entry.124074799=Read"},
{"title": "E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking", "author": "Kejia Gao and Liguo Zhou and Mingjun Liu and Alois Knoll", "abstract": "  End-to-end learning has shown great potential in autonomous parking, yet the\nlack of publicly available datasets limits reproducibility and benchmarking.\nWhile prior work introduced a visual-based parking model and a pipeline for\ndata generation, training, and close-loop test, the dataset itself was not\nreleased. To bridge this gap, we create and open-source a high-quality dataset\nfor end-to-end autonomous parking. Using the original model, we achieve an\noverall success rate of 85.16% with lower average position and orientation\nerrors (0.24 meters and 0.34 degrees).\n", "link": "http://arxiv.org/abs/2504.10812v2", "date": "2025-08-01", "relevancy": 1.9193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4905}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4756}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2E%20Parking%20Dataset%3A%20An%20Open%20Benchmark%20for%20End-to-End%20Autonomous%20Parking&body=Title%3A%20E2E%20Parking%20Dataset%3A%20An%20Open%20Benchmark%20for%20End-to-End%20Autonomous%20Parking%0AAuthor%3A%20Kejia%20Gao%20and%20Liguo%20Zhou%20and%20Mingjun%20Liu%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20End-to-end%20learning%20has%20shown%20great%20potential%20in%20autonomous%20parking%2C%20yet%20the%0Alack%20of%20publicly%20available%20datasets%20limits%20reproducibility%20and%20benchmarking.%0AWhile%20prior%20work%20introduced%20a%20visual-based%20parking%20model%20and%20a%20pipeline%20for%0Adata%20generation%2C%20training%2C%20and%20close-loop%20test%2C%20the%20dataset%20itself%20was%20not%0Areleased.%20To%20bridge%20this%20gap%2C%20we%20create%20and%20open-source%20a%20high-quality%20dataset%0Afor%20end-to-end%20autonomous%20parking.%20Using%20the%20original%20model%2C%20we%20achieve%20an%0Aoverall%20success%20rate%20of%2085.16%25%20with%20lower%20average%20position%20and%20orientation%0Aerrors%20%280.24%20meters%20and%200.34%20degrees%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2E%2520Parking%2520Dataset%253A%2520An%2520Open%2520Benchmark%2520for%2520End-to-End%2520Autonomous%2520Parking%26entry.906535625%3DKejia%2520Gao%2520and%2520Liguo%2520Zhou%2520and%2520Mingjun%2520Liu%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520End-to-end%2520learning%2520has%2520shown%2520great%2520potential%2520in%2520autonomous%2520parking%252C%2520yet%2520the%250Alack%2520of%2520publicly%2520available%2520datasets%2520limits%2520reproducibility%2520and%2520benchmarking.%250AWhile%2520prior%2520work%2520introduced%2520a%2520visual-based%2520parking%2520model%2520and%2520a%2520pipeline%2520for%250Adata%2520generation%252C%2520training%252C%2520and%2520close-loop%2520test%252C%2520the%2520dataset%2520itself%2520was%2520not%250Areleased.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520create%2520and%2520open-source%2520a%2520high-quality%2520dataset%250Afor%2520end-to-end%2520autonomous%2520parking.%2520Using%2520the%2520original%2520model%252C%2520we%2520achieve%2520an%250Aoverall%2520success%2520rate%2520of%252085.16%2525%2520with%2520lower%2520average%2520position%2520and%2520orientation%250Aerrors%2520%25280.24%2520meters%2520and%25200.34%2520degrees%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2E%20Parking%20Dataset%3A%20An%20Open%20Benchmark%20for%20End-to-End%20Autonomous%20Parking&entry.906535625=Kejia%20Gao%20and%20Liguo%20Zhou%20and%20Mingjun%20Liu%20and%20Alois%20Knoll&entry.1292438233=%20%20End-to-end%20learning%20has%20shown%20great%20potential%20in%20autonomous%20parking%2C%20yet%20the%0Alack%20of%20publicly%20available%20datasets%20limits%20reproducibility%20and%20benchmarking.%0AWhile%20prior%20work%20introduced%20a%20visual-based%20parking%20model%20and%20a%20pipeline%20for%0Adata%20generation%2C%20training%2C%20and%20close-loop%20test%2C%20the%20dataset%20itself%20was%20not%0Areleased.%20To%20bridge%20this%20gap%2C%20we%20create%20and%20open-source%20a%20high-quality%20dataset%0Afor%20end-to-end%20autonomous%20parking.%20Using%20the%20original%20model%2C%20we%20achieve%20an%0Aoverall%20success%20rate%20of%2085.16%25%20with%20lower%20average%20position%20and%20orientation%0Aerrors%20%280.24%20meters%20and%200.34%20degrees%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10812v2&entry.124074799=Read"},
{"title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning\n  Approach for High-Frequency PDEs", "author": "Xiong Xiong and Zhuo Zhang and Rongchun Hu and Chen Gao and Zichen Deng", "abstract": "  Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.\n", "link": "http://arxiv.org/abs/2508.00628v1", "date": "2025-08-01", "relevancy": 1.9154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4827}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4816}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separated-Variable%20Spectral%20Neural%20Networks%3A%20A%20Physics-Informed%20Learning%0A%20%20Approach%20for%20High-Frequency%20PDEs&body=Title%3A%20Separated-Variable%20Spectral%20Neural%20Networks%3A%20A%20Physics-Informed%20Learning%0A%20%20Approach%20for%20High-Frequency%20PDEs%0AAuthor%3A%20Xiong%20Xiong%20and%20Zhuo%20Zhang%20and%20Rongchun%20Hu%20and%20Chen%20Gao%20and%20Zichen%20Deng%0AAbstract%3A%20%20%20Solving%20high-frequency%20oscillatory%20partial%20differential%20equations%20%28PDEs%29%20is%20a%0Acritical%20challenge%20in%20scientific%20computing%2C%20with%20applications%20in%20fluid%0Amechanics%2C%20quantum%20mechanics%2C%20and%20electromagnetic%20wave%20propagation.%20Traditional%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20suffer%20from%20spectral%20bias%2C%20limiting%0Atheir%20ability%20to%20capture%20high-frequency%20solution%20components.%20We%20introduce%0ASeparated-Variable%20Spectral%20Neural%20Networks%20%28SV-SNN%29%2C%20a%20novel%20framework%20that%0Aaddresses%20these%20limitations%20by%20integrating%20separation%20of%20variables%20with%0Aadaptive%20spectral%20methods.%20Our%20approach%20features%20three%20key%20innovations%3A%20%281%29%0Adecomposition%20of%20multivariate%20functions%20into%20univariate%20function%20products%2C%0Aenabling%20independent%20spatial%20and%20temporal%20networks%3B%20%282%29%20adaptive%20Fourier%0Aspectral%20features%20with%20learnable%20frequency%20parameters%20for%20high-frequency%0Acapture%3B%20and%20%283%29%20theoretical%20framework%20based%20on%20singular%20value%20decomposition%20to%0Aquantify%20spectral%20bias.%20Comprehensive%20evaluation%20on%20benchmark%20problems%0Aincluding%20Heat%20equation%2C%20Helmholtz%20equation%2C%20Poisson%20equations%20and%0ANavier-Stokes%20equations%20demonstrates%20that%20SV-SNN%20achieves%201-3%20orders%20of%0Amagnitude%20improvement%20in%20accuracy%20while%20reducing%20parameter%20count%20by%20over%2090%5C%25%0Aand%20training%20time%20by%2060%5C%25.%20These%20results%20establish%20SV-SNN%20as%20an%20effective%0Asolution%20to%20the%20spectral%20bias%20problem%20in%20neural%20PDE%20solving.%20The%20implementation%0Awill%20be%20made%20publicly%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/xgxgnpu/SV-SNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparated-Variable%2520Spectral%2520Neural%2520Networks%253A%2520A%2520Physics-Informed%2520Learning%250A%2520%2520Approach%2520for%2520High-Frequency%2520PDEs%26entry.906535625%3DXiong%2520Xiong%2520and%2520Zhuo%2520Zhang%2520and%2520Rongchun%2520Hu%2520and%2520Chen%2520Gao%2520and%2520Zichen%2520Deng%26entry.1292438233%3D%2520%2520Solving%2520high-frequency%2520oscillatory%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%2520a%250Acritical%2520challenge%2520in%2520scientific%2520computing%252C%2520with%2520applications%2520in%2520fluid%250Amechanics%252C%2520quantum%2520mechanics%252C%2520and%2520electromagnetic%2520wave%2520propagation.%2520Traditional%250Aphysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520suffer%2520from%2520spectral%2520bias%252C%2520limiting%250Atheir%2520ability%2520to%2520capture%2520high-frequency%2520solution%2520components.%2520We%2520introduce%250ASeparated-Variable%2520Spectral%2520Neural%2520Networks%2520%2528SV-SNN%2529%252C%2520a%2520novel%2520framework%2520that%250Aaddresses%2520these%2520limitations%2520by%2520integrating%2520separation%2520of%2520variables%2520with%250Aadaptive%2520spectral%2520methods.%2520Our%2520approach%2520features%2520three%2520key%2520innovations%253A%2520%25281%2529%250Adecomposition%2520of%2520multivariate%2520functions%2520into%2520univariate%2520function%2520products%252C%250Aenabling%2520independent%2520spatial%2520and%2520temporal%2520networks%253B%2520%25282%2529%2520adaptive%2520Fourier%250Aspectral%2520features%2520with%2520learnable%2520frequency%2520parameters%2520for%2520high-frequency%250Acapture%253B%2520and%2520%25283%2529%2520theoretical%2520framework%2520based%2520on%2520singular%2520value%2520decomposition%2520to%250Aquantify%2520spectral%2520bias.%2520Comprehensive%2520evaluation%2520on%2520benchmark%2520problems%250Aincluding%2520Heat%2520equation%252C%2520Helmholtz%2520equation%252C%2520Poisson%2520equations%2520and%250ANavier-Stokes%2520equations%2520demonstrates%2520that%2520SV-SNN%2520achieves%25201-3%2520orders%2520of%250Amagnitude%2520improvement%2520in%2520accuracy%2520while%2520reducing%2520parameter%2520count%2520by%2520over%252090%255C%2525%250Aand%2520training%2520time%2520by%252060%255C%2525.%2520These%2520results%2520establish%2520SV-SNN%2520as%2520an%2520effective%250Asolution%2520to%2520the%2520spectral%2520bias%2520problem%2520in%2520neural%2520PDE%2520solving.%2520The%2520implementation%250Awill%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance%2520at%250Ahttps%253A//github.com/xgxgnpu/SV-SNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separated-Variable%20Spectral%20Neural%20Networks%3A%20A%20Physics-Informed%20Learning%0A%20%20Approach%20for%20High-Frequency%20PDEs&entry.906535625=Xiong%20Xiong%20and%20Zhuo%20Zhang%20and%20Rongchun%20Hu%20and%20Chen%20Gao%20and%20Zichen%20Deng&entry.1292438233=%20%20Solving%20high-frequency%20oscillatory%20partial%20differential%20equations%20%28PDEs%29%20is%20a%0Acritical%20challenge%20in%20scientific%20computing%2C%20with%20applications%20in%20fluid%0Amechanics%2C%20quantum%20mechanics%2C%20and%20electromagnetic%20wave%20propagation.%20Traditional%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20suffer%20from%20spectral%20bias%2C%20limiting%0Atheir%20ability%20to%20capture%20high-frequency%20solution%20components.%20We%20introduce%0ASeparated-Variable%20Spectral%20Neural%20Networks%20%28SV-SNN%29%2C%20a%20novel%20framework%20that%0Aaddresses%20these%20limitations%20by%20integrating%20separation%20of%20variables%20with%0Aadaptive%20spectral%20methods.%20Our%20approach%20features%20three%20key%20innovations%3A%20%281%29%0Adecomposition%20of%20multivariate%20functions%20into%20univariate%20function%20products%2C%0Aenabling%20independent%20spatial%20and%20temporal%20networks%3B%20%282%29%20adaptive%20Fourier%0Aspectral%20features%20with%20learnable%20frequency%20parameters%20for%20high-frequency%0Acapture%3B%20and%20%283%29%20theoretical%20framework%20based%20on%20singular%20value%20decomposition%20to%0Aquantify%20spectral%20bias.%20Comprehensive%20evaluation%20on%20benchmark%20problems%0Aincluding%20Heat%20equation%2C%20Helmholtz%20equation%2C%20Poisson%20equations%20and%0ANavier-Stokes%20equations%20demonstrates%20that%20SV-SNN%20achieves%201-3%20orders%20of%0Amagnitude%20improvement%20in%20accuracy%20while%20reducing%20parameter%20count%20by%20over%2090%5C%25%0Aand%20training%20time%20by%2060%5C%25.%20These%20results%20establish%20SV-SNN%20as%20an%20effective%0Asolution%20to%20the%20spectral%20bias%20problem%20in%20neural%20PDE%20solving.%20The%20implementation%0Awill%20be%20made%20publicly%20available%20upon%20acceptance%20at%0Ahttps%3A//github.com/xgxgnpu/SV-SNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00628v1&entry.124074799=Read"},
{"title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified\n  Evaluation, Large-Scale Dataset, and New Insights", "author": "Junhao Zheng and Jiahao Sun and Chenhao Lin and Zhengyu Zhao and Chen Ma and Chong Zhang and Cong Wang and Qian Wang and Chao Shen", "abstract": "  Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.\n", "link": "http://arxiv.org/abs/2508.00649v1", "date": "2025-08-01", "relevancy": 1.8924, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Adversarial%20Patch%20Defenses%20on%20Object%20Detectors%3A%20Unified%0A%20%20Evaluation%2C%20Large-Scale%20Dataset%2C%20and%20New%20Insights&body=Title%3A%20Revisiting%20Adversarial%20Patch%20Defenses%20on%20Object%20Detectors%3A%20Unified%0A%20%20Evaluation%2C%20Large-Scale%20Dataset%2C%20and%20New%20Insights%0AAuthor%3A%20Junhao%20Zheng%20and%20Jiahao%20Sun%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Chen%20Ma%20and%20Chong%20Zhang%20and%20Cong%20Wang%20and%20Qian%20Wang%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Developing%20reliable%20defenses%20against%20patch%20attacks%20on%20object%20detectors%20has%0Aattracted%20increasing%20interest.%20However%2C%20we%20identify%20that%20existing%20defense%0Aevaluations%20lack%20a%20unified%20and%20comprehensive%20framework%2C%20resulting%20in%0Ainconsistent%20and%20incomplete%20assessments%20of%20current%20methods.%20To%20address%20this%0Aissue%2C%20we%20revisit%2011%20representative%20defenses%20and%20present%20the%20first%20patch%0Adefense%20benchmark%2C%20involving%202%20attack%20goals%2C%2013%20patch%20attacks%2C%2011%20object%0Adetectors%2C%20and%204%20diverse%20metrics.%20This%20leads%20to%20the%20large-scale%20adversarial%0Apatch%20dataset%20with%2094%20types%20of%20patches%20and%2094%2C000%20images.%20Our%20comprehensive%0Aanalyses%20reveal%20new%20insights%3A%20%281%29%20The%20difficulty%20in%20defending%20against%0Anaturalistic%20patches%20lies%20in%20the%20data%20distribution%2C%20rather%20than%20the%20commonly%0Abelieved%20high%20frequencies.%20Our%20new%20dataset%20with%20diverse%20patch%20distributions%20can%0Abe%20used%20to%20improve%20existing%20defenses%20by%2015.09%25%20AP%400.5.%20%282%29%20The%20average%0Aprecision%20of%20the%20attacked%20object%2C%20rather%20than%20the%20commonly%20pursued%20patch%0Adetection%20accuracy%2C%20shows%20high%20consistency%20with%20defense%20performance.%20%283%29%0AAdaptive%20attacks%20can%20substantially%20bypass%20existing%20defenses%2C%20and%20defenses%20with%0Acomplex/stochastic%20models%20or%20universal%20patch%20properties%20are%20relatively%20robust.%0AWe%20hope%20that%20our%20analyses%20will%20serve%20as%20guidance%20on%20properly%20evaluating%20patch%0Aattacks/defenses%20and%20advancing%20their%20design.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Gandolfczjh/APDE%2C%20where%20we%20will%20keep%20integrating%20new%0Aattacks/defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Adversarial%2520Patch%2520Defenses%2520on%2520Object%2520Detectors%253A%2520Unified%250A%2520%2520Evaluation%252C%2520Large-Scale%2520Dataset%252C%2520and%2520New%2520Insights%26entry.906535625%3DJunhao%2520Zheng%2520and%2520Jiahao%2520Sun%2520and%2520Chenhao%2520Lin%2520and%2520Zhengyu%2520Zhao%2520and%2520Chen%2520Ma%2520and%2520Chong%2520Zhang%2520and%2520Cong%2520Wang%2520and%2520Qian%2520Wang%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520Developing%2520reliable%2520defenses%2520against%2520patch%2520attacks%2520on%2520object%2520detectors%2520has%250Aattracted%2520increasing%2520interest.%2520However%252C%2520we%2520identify%2520that%2520existing%2520defense%250Aevaluations%2520lack%2520a%2520unified%2520and%2520comprehensive%2520framework%252C%2520resulting%2520in%250Ainconsistent%2520and%2520incomplete%2520assessments%2520of%2520current%2520methods.%2520To%2520address%2520this%250Aissue%252C%2520we%2520revisit%252011%2520representative%2520defenses%2520and%2520present%2520the%2520first%2520patch%250Adefense%2520benchmark%252C%2520involving%25202%2520attack%2520goals%252C%252013%2520patch%2520attacks%252C%252011%2520object%250Adetectors%252C%2520and%25204%2520diverse%2520metrics.%2520This%2520leads%2520to%2520the%2520large-scale%2520adversarial%250Apatch%2520dataset%2520with%252094%2520types%2520of%2520patches%2520and%252094%252C000%2520images.%2520Our%2520comprehensive%250Aanalyses%2520reveal%2520new%2520insights%253A%2520%25281%2529%2520The%2520difficulty%2520in%2520defending%2520against%250Anaturalistic%2520patches%2520lies%2520in%2520the%2520data%2520distribution%252C%2520rather%2520than%2520the%2520commonly%250Abelieved%2520high%2520frequencies.%2520Our%2520new%2520dataset%2520with%2520diverse%2520patch%2520distributions%2520can%250Abe%2520used%2520to%2520improve%2520existing%2520defenses%2520by%252015.09%2525%2520AP%25400.5.%2520%25282%2529%2520The%2520average%250Aprecision%2520of%2520the%2520attacked%2520object%252C%2520rather%2520than%2520the%2520commonly%2520pursued%2520patch%250Adetection%2520accuracy%252C%2520shows%2520high%2520consistency%2520with%2520defense%2520performance.%2520%25283%2529%250AAdaptive%2520attacks%2520can%2520substantially%2520bypass%2520existing%2520defenses%252C%2520and%2520defenses%2520with%250Acomplex/stochastic%2520models%2520or%2520universal%2520patch%2520properties%2520are%2520relatively%2520robust.%250AWe%2520hope%2520that%2520our%2520analyses%2520will%2520serve%2520as%2520guidance%2520on%2520properly%2520evaluating%2520patch%250Aattacks/defenses%2520and%2520advancing%2520their%2520design.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/Gandolfczjh/APDE%252C%2520where%2520we%2520will%2520keep%2520integrating%2520new%250Aattacks/defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Adversarial%20Patch%20Defenses%20on%20Object%20Detectors%3A%20Unified%0A%20%20Evaluation%2C%20Large-Scale%20Dataset%2C%20and%20New%20Insights&entry.906535625=Junhao%20Zheng%20and%20Jiahao%20Sun%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Chen%20Ma%20and%20Chong%20Zhang%20and%20Cong%20Wang%20and%20Qian%20Wang%20and%20Chao%20Shen&entry.1292438233=%20%20Developing%20reliable%20defenses%20against%20patch%20attacks%20on%20object%20detectors%20has%0Aattracted%20increasing%20interest.%20However%2C%20we%20identify%20that%20existing%20defense%0Aevaluations%20lack%20a%20unified%20and%20comprehensive%20framework%2C%20resulting%20in%0Ainconsistent%20and%20incomplete%20assessments%20of%20current%20methods.%20To%20address%20this%0Aissue%2C%20we%20revisit%2011%20representative%20defenses%20and%20present%20the%20first%20patch%0Adefense%20benchmark%2C%20involving%202%20attack%20goals%2C%2013%20patch%20attacks%2C%2011%20object%0Adetectors%2C%20and%204%20diverse%20metrics.%20This%20leads%20to%20the%20large-scale%20adversarial%0Apatch%20dataset%20with%2094%20types%20of%20patches%20and%2094%2C000%20images.%20Our%20comprehensive%0Aanalyses%20reveal%20new%20insights%3A%20%281%29%20The%20difficulty%20in%20defending%20against%0Anaturalistic%20patches%20lies%20in%20the%20data%20distribution%2C%20rather%20than%20the%20commonly%0Abelieved%20high%20frequencies.%20Our%20new%20dataset%20with%20diverse%20patch%20distributions%20can%0Abe%20used%20to%20improve%20existing%20defenses%20by%2015.09%25%20AP%400.5.%20%282%29%20The%20average%0Aprecision%20of%20the%20attacked%20object%2C%20rather%20than%20the%20commonly%20pursued%20patch%0Adetection%20accuracy%2C%20shows%20high%20consistency%20with%20defense%20performance.%20%283%29%0AAdaptive%20attacks%20can%20substantially%20bypass%20existing%20defenses%2C%20and%20defenses%20with%0Acomplex/stochastic%20models%20or%20universal%20patch%20properties%20are%20relatively%20robust.%0AWe%20hope%20that%20our%20analyses%20will%20serve%20as%20guidance%20on%20properly%20evaluating%20patch%0Aattacks/defenses%20and%20advancing%20their%20design.%20Code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Gandolfczjh/APDE%2C%20where%20we%20will%20keep%20integrating%20new%0Aattacks/defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00649v1&entry.124074799=Read"},
{"title": "Better Embeddings with Coupled Adam", "author": "Felix Stollenwerk and Tobias Stollenwerk", "abstract": "  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n", "link": "http://arxiv.org/abs/2502.08441v3", "date": "2025-08-01", "relevancy": 1.887, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4534}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Embeddings%20with%20Coupled%20Adam&body=Title%3A%20Better%20Embeddings%20with%20Coupled%20Adam%0AAuthor%3A%20Felix%20Stollenwerk%20and%20Tobias%20Stollenwerk%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20capabilities%2C%20LLMs%20learn%20word%20representations%20that%0Aexhibit%20the%20undesirable%20yet%20poorly%20understood%20feature%20of%20anisotropy.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20second%20moment%20in%20Adam%20is%20a%20cause%20of%20anisotropic%0Aembeddings%2C%20and%20suggest%20a%20modified%20optimizer%20called%20Coupled%20Adam%20to%20mitigate%0Athe%20problem.%20Our%20experiments%20demonstrate%20that%20Coupled%20Adam%20significantly%0Aimproves%20the%20quality%20of%20embeddings%2C%20while%20also%20leading%20to%20better%20upstream%20and%0Adownstream%20performance%20on%20large%20enough%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Embeddings%2520with%2520Coupled%2520Adam%26entry.906535625%3DFelix%2520Stollenwerk%2520and%2520Tobias%2520Stollenwerk%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520capabilities%252C%2520LLMs%2520learn%2520word%2520representations%2520that%250Aexhibit%2520the%2520undesirable%2520yet%2520poorly%2520understood%2520feature%2520of%2520anisotropy.%2520In%2520this%250Apaper%252C%2520we%2520argue%2520that%2520the%2520second%2520moment%2520in%2520Adam%2520is%2520a%2520cause%2520of%2520anisotropic%250Aembeddings%252C%2520and%2520suggest%2520a%2520modified%2520optimizer%2520called%2520Coupled%2520Adam%2520to%2520mitigate%250Athe%2520problem.%2520Our%2520experiments%2520demonstrate%2520that%2520Coupled%2520Adam%2520significantly%250Aimproves%2520the%2520quality%2520of%2520embeddings%252C%2520while%2520also%2520leading%2520to%2520better%2520upstream%2520and%250Adownstream%2520performance%2520on%2520large%2520enough%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Embeddings%20with%20Coupled%20Adam&entry.906535625=Felix%20Stollenwerk%20and%20Tobias%20Stollenwerk&entry.1292438233=%20%20Despite%20their%20remarkable%20capabilities%2C%20LLMs%20learn%20word%20representations%20that%0Aexhibit%20the%20undesirable%20yet%20poorly%20understood%20feature%20of%20anisotropy.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20second%20moment%20in%20Adam%20is%20a%20cause%20of%20anisotropic%0Aembeddings%2C%20and%20suggest%20a%20modified%20optimizer%20called%20Coupled%20Adam%20to%20mitigate%0Athe%20problem.%20Our%20experiments%20demonstrate%20that%20Coupled%20Adam%20significantly%0Aimproves%20the%20quality%20of%20embeddings%2C%20while%20also%20leading%20to%20better%20upstream%20and%0Adownstream%20performance%20on%20large%20enough%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08441v3&entry.124074799=Read"},
{"title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection\n  and Leakage Attacks", "author": "Francesco Panebianco and Stefano Bonfanti and Francesco Trov\u00f2 and Michele Carminati", "abstract": "  The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.\n", "link": "http://arxiv.org/abs/2508.00602v1", "date": "2025-08-01", "relevancy": 1.8822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4636}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeakSealer%3A%20A%20Semisupervised%20Defense%20for%20LLMs%20Against%20Prompt%20Injection%0A%20%20and%20Leakage%20Attacks&body=Title%3A%20LeakSealer%3A%20A%20Semisupervised%20Defense%20for%20LLMs%20Against%20Prompt%20Injection%0A%20%20and%20Leakage%20Attacks%0AAuthor%3A%20Francesco%20Panebianco%20and%20Stefano%20Bonfanti%20and%20Francesco%20Trov%C3%B2%20and%20Michele%20Carminati%0AAbstract%3A%20%20%20The%20generalization%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%0Atheir%20widespread%20deployment%20across%20various%20applications.%20However%2C%20this%0Aincreased%20adoption%20has%20introduced%20several%20security%20threats%2C%20notably%20in%20the%0Aforms%20of%20jailbreaking%20and%20data%20leakage%20attacks.%20Additionally%2C%20Retrieval%0AAugmented%20Generation%20%28RAG%29%2C%20while%20enhancing%20context-awareness%20in%20LLM%20responses%2C%0Ahas%20inadvertently%20introduced%20vulnerabilities%20that%20can%20result%20in%20the%20leakage%20of%0Asensitive%20information.%20Our%20contributions%20are%20twofold.%20First%2C%20we%20introduce%20a%0Amethodology%20to%20analyze%20historical%20interaction%20data%20from%20an%20LLM%20system%2C%20enabling%0Athe%20generation%20of%20usage%20maps%20categorized%20by%20topics%20%28including%20adversarial%0Ainteractions%29.%20This%20approach%20further%20provides%20forensic%20insights%20for%20tracking%0Athe%20evolution%20of%20jailbreaking%20attack%20patterns.%20Second%2C%20we%20propose%20LeakSealer%2C%20a%0Amodel-agnostic%20framework%20that%20combines%20static%20analysis%20for%20forensic%20insights%0Awith%20dynamic%20defenses%20in%20a%20Human-In-The-Loop%20%28HITL%29%20pipeline.%20This%20technique%0Aidentifies%20topic%20groups%20and%20detects%20anomalous%20patterns%2C%20allowing%20for%20proactive%0Adefense%20mechanisms.%20We%20empirically%20evaluate%20LeakSealer%20under%20two%20scenarios%3A%20%281%29%0Ajailbreak%20attempts%2C%20employing%20a%20public%20benchmark%20dataset%2C%20and%20%282%29%20PII%20leakage%2C%0Asupported%20by%20a%20curated%20dataset%20of%20labeled%20LLM%20interactions.%20In%20the%20static%0Asetting%2C%20LeakSealer%20achieves%20the%20highest%20precision%20and%20recall%20on%20the%20ToxicChat%0Adataset%20when%20identifying%20prompt%20injection.%20In%20the%20dynamic%20setting%2C%20PII%20leakage%0Adetection%20achieves%20an%20AUPRC%20of%20%240.97%24%2C%20significantly%20outperforming%20baselines%0Asuch%20as%20Llama%20Guard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeakSealer%253A%2520A%2520Semisupervised%2520Defense%2520for%2520LLMs%2520Against%2520Prompt%2520Injection%250A%2520%2520and%2520Leakage%2520Attacks%26entry.906535625%3DFrancesco%2520Panebianco%2520and%2520Stefano%2520Bonfanti%2520and%2520Francesco%2520Trov%25C3%25B2%2520and%2520Michele%2520Carminati%26entry.1292438233%3D%2520%2520The%2520generalization%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520led%2520to%250Atheir%2520widespread%2520deployment%2520across%2520various%2520applications.%2520However%252C%2520this%250Aincreased%2520adoption%2520has%2520introduced%2520several%2520security%2520threats%252C%2520notably%2520in%2520the%250Aforms%2520of%2520jailbreaking%2520and%2520data%2520leakage%2520attacks.%2520Additionally%252C%2520Retrieval%250AAugmented%2520Generation%2520%2528RAG%2529%252C%2520while%2520enhancing%2520context-awareness%2520in%2520LLM%2520responses%252C%250Ahas%2520inadvertently%2520introduced%2520vulnerabilities%2520that%2520can%2520result%2520in%2520the%2520leakage%2520of%250Asensitive%2520information.%2520Our%2520contributions%2520are%2520twofold.%2520First%252C%2520we%2520introduce%2520a%250Amethodology%2520to%2520analyze%2520historical%2520interaction%2520data%2520from%2520an%2520LLM%2520system%252C%2520enabling%250Athe%2520generation%2520of%2520usage%2520maps%2520categorized%2520by%2520topics%2520%2528including%2520adversarial%250Ainteractions%2529.%2520This%2520approach%2520further%2520provides%2520forensic%2520insights%2520for%2520tracking%250Athe%2520evolution%2520of%2520jailbreaking%2520attack%2520patterns.%2520Second%252C%2520we%2520propose%2520LeakSealer%252C%2520a%250Amodel-agnostic%2520framework%2520that%2520combines%2520static%2520analysis%2520for%2520forensic%2520insights%250Awith%2520dynamic%2520defenses%2520in%2520a%2520Human-In-The-Loop%2520%2528HITL%2529%2520pipeline.%2520This%2520technique%250Aidentifies%2520topic%2520groups%2520and%2520detects%2520anomalous%2520patterns%252C%2520allowing%2520for%2520proactive%250Adefense%2520mechanisms.%2520We%2520empirically%2520evaluate%2520LeakSealer%2520under%2520two%2520scenarios%253A%2520%25281%2529%250Ajailbreak%2520attempts%252C%2520employing%2520a%2520public%2520benchmark%2520dataset%252C%2520and%2520%25282%2529%2520PII%2520leakage%252C%250Asupported%2520by%2520a%2520curated%2520dataset%2520of%2520labeled%2520LLM%2520interactions.%2520In%2520the%2520static%250Asetting%252C%2520LeakSealer%2520achieves%2520the%2520highest%2520precision%2520and%2520recall%2520on%2520the%2520ToxicChat%250Adataset%2520when%2520identifying%2520prompt%2520injection.%2520In%2520the%2520dynamic%2520setting%252C%2520PII%2520leakage%250Adetection%2520achieves%2520an%2520AUPRC%2520of%2520%25240.97%2524%252C%2520significantly%2520outperforming%2520baselines%250Asuch%2520as%2520Llama%2520Guard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeakSealer%3A%20A%20Semisupervised%20Defense%20for%20LLMs%20Against%20Prompt%20Injection%0A%20%20and%20Leakage%20Attacks&entry.906535625=Francesco%20Panebianco%20and%20Stefano%20Bonfanti%20and%20Francesco%20Trov%C3%B2%20and%20Michele%20Carminati&entry.1292438233=%20%20The%20generalization%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%0Atheir%20widespread%20deployment%20across%20various%20applications.%20However%2C%20this%0Aincreased%20adoption%20has%20introduced%20several%20security%20threats%2C%20notably%20in%20the%0Aforms%20of%20jailbreaking%20and%20data%20leakage%20attacks.%20Additionally%2C%20Retrieval%0AAugmented%20Generation%20%28RAG%29%2C%20while%20enhancing%20context-awareness%20in%20LLM%20responses%2C%0Ahas%20inadvertently%20introduced%20vulnerabilities%20that%20can%20result%20in%20the%20leakage%20of%0Asensitive%20information.%20Our%20contributions%20are%20twofold.%20First%2C%20we%20introduce%20a%0Amethodology%20to%20analyze%20historical%20interaction%20data%20from%20an%20LLM%20system%2C%20enabling%0Athe%20generation%20of%20usage%20maps%20categorized%20by%20topics%20%28including%20adversarial%0Ainteractions%29.%20This%20approach%20further%20provides%20forensic%20insights%20for%20tracking%0Athe%20evolution%20of%20jailbreaking%20attack%20patterns.%20Second%2C%20we%20propose%20LeakSealer%2C%20a%0Amodel-agnostic%20framework%20that%20combines%20static%20analysis%20for%20forensic%20insights%0Awith%20dynamic%20defenses%20in%20a%20Human-In-The-Loop%20%28HITL%29%20pipeline.%20This%20technique%0Aidentifies%20topic%20groups%20and%20detects%20anomalous%20patterns%2C%20allowing%20for%20proactive%0Adefense%20mechanisms.%20We%20empirically%20evaluate%20LeakSealer%20under%20two%20scenarios%3A%20%281%29%0Ajailbreak%20attempts%2C%20employing%20a%20public%20benchmark%20dataset%2C%20and%20%282%29%20PII%20leakage%2C%0Asupported%20by%20a%20curated%20dataset%20of%20labeled%20LLM%20interactions.%20In%20the%20static%0Asetting%2C%20LeakSealer%20achieves%20the%20highest%20precision%20and%20recall%20on%20the%20ToxicChat%0Adataset%20when%20identifying%20prompt%20injection.%20In%20the%20dynamic%20setting%2C%20PII%20leakage%0Adetection%20achieves%20an%20AUPRC%20of%20%240.97%24%2C%20significantly%20outperforming%20baselines%0Asuch%20as%20Llama%20Guard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00602v1&entry.124074799=Read"},
{"title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical\n  Role-Based Queries", "author": "Shubham Kumar Nigam and Tanmay Dubey and Noel Shallum and Arnab Bhattacharya", "abstract": "  Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.\n", "link": "http://arxiv.org/abs/2508.00679v1", "date": "2025-08-01", "relevancy": 1.8751, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20First%2C%20Retrieve%20Better%3A%20Realistic%20Legal%20Search%20via%20Rhetorical%0A%20%20Role-Based%20Queries&body=Title%3A%20Segment%20First%2C%20Retrieve%20Better%3A%20Realistic%20Legal%20Search%20via%20Rhetorical%0A%20%20Role-Based%20Queries%0AAuthor%3A%20Shubham%20Kumar%20Nigam%20and%20Tanmay%20Dubey%20and%20Noel%20Shallum%20and%20Arnab%20Bhattacharya%0AAbstract%3A%20%20%20Legal%20precedent%20retrieval%20is%20a%20cornerstone%20of%20the%20common%20law%20system%2C%20governed%0Aby%20the%20principle%20of%20stare%20decisis%2C%20which%20demands%20consistency%20in%20judicial%0Adecisions.%20However%2C%20the%20growing%20complexity%20and%20volume%20of%20legal%20documents%0Achallenge%20traditional%20retrieval%20methods.%20TraceRetriever%20mirrors%20real-world%0Alegal%20search%20by%20operating%20with%20limited%20case%20information%2C%20extracting%20only%0Arhetorically%20significant%20segments%20instead%20of%20requiring%20complete%20documents.%20Our%0Apipeline%20integrates%20BM25%2C%20Vector%20Database%2C%20and%20Cross-Encoder%20models%2C%20combining%0Ainitial%20results%20through%20Reciprocal%20Rank%20Fusion%20before%20final%20re-ranking.%0ARhetorical%20annotations%20are%20generated%20using%20a%20Hierarchical%20BiLSTM%20CRF%20classifier%0Atrained%20on%20Indian%20judgments.%20Evaluated%20on%20IL-PCR%20and%20COLIEE%202025%20datasets%2C%0ATraceRetriever%20addresses%20growing%20document%20volume%20challenges%20while%20aligning%20with%0Apractical%20search%20constraints%2C%20reliable%20and%20scalable%20foundation%20for%20precedent%0Aretrieval%20enhancing%20legal%20research%20when%20only%20partial%20case%20knowledge%20is%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520First%252C%2520Retrieve%2520Better%253A%2520Realistic%2520Legal%2520Search%2520via%2520Rhetorical%250A%2520%2520Role-Based%2520Queries%26entry.906535625%3DShubham%2520Kumar%2520Nigam%2520and%2520Tanmay%2520Dubey%2520and%2520Noel%2520Shallum%2520and%2520Arnab%2520Bhattacharya%26entry.1292438233%3D%2520%2520Legal%2520precedent%2520retrieval%2520is%2520a%2520cornerstone%2520of%2520the%2520common%2520law%2520system%252C%2520governed%250Aby%2520the%2520principle%2520of%2520stare%2520decisis%252C%2520which%2520demands%2520consistency%2520in%2520judicial%250Adecisions.%2520However%252C%2520the%2520growing%2520complexity%2520and%2520volume%2520of%2520legal%2520documents%250Achallenge%2520traditional%2520retrieval%2520methods.%2520TraceRetriever%2520mirrors%2520real-world%250Alegal%2520search%2520by%2520operating%2520with%2520limited%2520case%2520information%252C%2520extracting%2520only%250Arhetorically%2520significant%2520segments%2520instead%2520of%2520requiring%2520complete%2520documents.%2520Our%250Apipeline%2520integrates%2520BM25%252C%2520Vector%2520Database%252C%2520and%2520Cross-Encoder%2520models%252C%2520combining%250Ainitial%2520results%2520through%2520Reciprocal%2520Rank%2520Fusion%2520before%2520final%2520re-ranking.%250ARhetorical%2520annotations%2520are%2520generated%2520using%2520a%2520Hierarchical%2520BiLSTM%2520CRF%2520classifier%250Atrained%2520on%2520Indian%2520judgments.%2520Evaluated%2520on%2520IL-PCR%2520and%2520COLIEE%25202025%2520datasets%252C%250ATraceRetriever%2520addresses%2520growing%2520document%2520volume%2520challenges%2520while%2520aligning%2520with%250Apractical%2520search%2520constraints%252C%2520reliable%2520and%2520scalable%2520foundation%2520for%2520precedent%250Aretrieval%2520enhancing%2520legal%2520research%2520when%2520only%2520partial%2520case%2520knowledge%2520is%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20First%2C%20Retrieve%20Better%3A%20Realistic%20Legal%20Search%20via%20Rhetorical%0A%20%20Role-Based%20Queries&entry.906535625=Shubham%20Kumar%20Nigam%20and%20Tanmay%20Dubey%20and%20Noel%20Shallum%20and%20Arnab%20Bhattacharya&entry.1292438233=%20%20Legal%20precedent%20retrieval%20is%20a%20cornerstone%20of%20the%20common%20law%20system%2C%20governed%0Aby%20the%20principle%20of%20stare%20decisis%2C%20which%20demands%20consistency%20in%20judicial%0Adecisions.%20However%2C%20the%20growing%20complexity%20and%20volume%20of%20legal%20documents%0Achallenge%20traditional%20retrieval%20methods.%20TraceRetriever%20mirrors%20real-world%0Alegal%20search%20by%20operating%20with%20limited%20case%20information%2C%20extracting%20only%0Arhetorically%20significant%20segments%20instead%20of%20requiring%20complete%20documents.%20Our%0Apipeline%20integrates%20BM25%2C%20Vector%20Database%2C%20and%20Cross-Encoder%20models%2C%20combining%0Ainitial%20results%20through%20Reciprocal%20Rank%20Fusion%20before%20final%20re-ranking.%0ARhetorical%20annotations%20are%20generated%20using%20a%20Hierarchical%20BiLSTM%20CRF%20classifier%0Atrained%20on%20Indian%20judgments.%20Evaluated%20on%20IL-PCR%20and%20COLIEE%202025%20datasets%2C%0ATraceRetriever%20addresses%20growing%20document%20volume%20challenges%20while%20aligning%20with%0Apractical%20search%20constraints%2C%20reliable%20and%20scalable%20foundation%20for%20precedent%0Aretrieval%20enhancing%20legal%20research%20when%20only%20partial%20case%20knowledge%20is%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00679v1&entry.124074799=Read"},
{"title": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage", "author": "Fan Bu and Zheng Wang and Siyi Wang and Ziyao Liu", "abstract": "  As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.\n", "link": "http://arxiv.org/abs/2501.02039v3", "date": "2025-08-01", "relevancy": 1.8694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Investigation%20into%20Value%20Misalignment%20in%20LLM-Generated%20Texts%20for%0A%20%20Cultural%20Heritage&body=Title%3A%20An%20Investigation%20into%20Value%20Misalignment%20in%20LLM-Generated%20Texts%20for%0A%20%20Cultural%20Heritage%0AAuthor%3A%20Fan%20Bu%20and%20Zheng%20Wang%20and%20Siyi%20Wang%20and%20Ziyao%20Liu%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20prevalent%20in%20tasks%0Arelated%20to%20cultural%20heritage%2C%20such%20as%20generating%20descriptions%20of%20historical%0Amonuments%2C%20translating%20ancient%20texts%2C%20preserving%20oral%20traditions%2C%20and%20creating%0Aeducational%20content%2C%20their%20ability%20to%20produce%20accurate%20and%20culturally%20aligned%0Atexts%20is%20being%20increasingly%20relied%20upon%20by%20users%20and%20researchers.%20However%2C%0Acultural%20value%20misalignments%20may%20exist%20in%20generated%20texts%2C%20such%20as%20the%0Amisrepresentation%20of%20historical%20facts%2C%20the%20erosion%20of%20cultural%20identity%2C%20and%0Athe%20oversimplification%20of%20complex%20cultural%20narratives%2C%20which%20may%20lead%20to%20severe%0Aconsequences.%20Therefore%2C%20investigating%20value%20misalignment%20in%20the%20context%20of%20LLM%0Afor%20cultural%20heritage%20is%20crucial%20for%20mitigating%20these%20risks%2C%20yet%20there%20has%20been%0Aa%20significant%20lack%20of%20systematic%20and%20comprehensive%20study%20and%20investigation%20in%0Athis%20area.%20To%20fill%20this%20gap%2C%20we%20systematically%20assess%20the%20reliability%20of%20LLMs%0Ain%20generating%20culturally%20aligned%20texts%20for%20cultural%20heritage-related%20tasks.%20We%0Aconduct%20a%20comprehensive%20evaluation%20by%20compiling%20an%20extensive%20set%20of%201066%20query%0Atasks%20covering%205%20widely%20recognized%20categories%20with%2017%20aspects%20within%20the%0Aknowledge%20framework%20of%20cultural%20heritage%20across%205%20open-source%20LLMs%2C%20and%20examine%0Aboth%20the%20type%20and%20rate%20of%20cultural%20value%20misalignments%20in%20the%20generated%20texts.%0AUsing%20both%20automated%20and%20manual%20approaches%2C%20we%20effectively%20detect%20and%20analyze%0Athe%20cultural%20value%20misalignments%20in%20LLM-generated%20texts.%20Our%20findings%20are%0Aconcerning%3A%20over%2065%25%20of%20the%20generated%20texts%20exhibit%20notable%20cultural%0Amisalignments%2C%20with%20certain%20tasks%20demonstrating%20almost%20complete%20misalignment%0Awith%20key%20cultural%20values.%20Beyond%20these%20findings%2C%20this%20paper%20introduces%20a%0Abenchmark%20dataset%20and%20a%20comprehensive%20evaluation%20workflow%20that%20can%20serve%20as%20a%0Avaluable%20resource%20for%20future%20research%20aimed%20at%20enhancing%20the%20cultural%0Asensitivity%20and%20reliability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Investigation%2520into%2520Value%2520Misalignment%2520in%2520LLM-Generated%2520Texts%2520for%250A%2520%2520Cultural%2520Heritage%26entry.906535625%3DFan%2520Bu%2520and%2520Zheng%2520Wang%2520and%2520Siyi%2520Wang%2520and%2520Ziyao%2520Liu%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520increasingly%2520prevalent%2520in%2520tasks%250Arelated%2520to%2520cultural%2520heritage%252C%2520such%2520as%2520generating%2520descriptions%2520of%2520historical%250Amonuments%252C%2520translating%2520ancient%2520texts%252C%2520preserving%2520oral%2520traditions%252C%2520and%2520creating%250Aeducational%2520content%252C%2520their%2520ability%2520to%2520produce%2520accurate%2520and%2520culturally%2520aligned%250Atexts%2520is%2520being%2520increasingly%2520relied%2520upon%2520by%2520users%2520and%2520researchers.%2520However%252C%250Acultural%2520value%2520misalignments%2520may%2520exist%2520in%2520generated%2520texts%252C%2520such%2520as%2520the%250Amisrepresentation%2520of%2520historical%2520facts%252C%2520the%2520erosion%2520of%2520cultural%2520identity%252C%2520and%250Athe%2520oversimplification%2520of%2520complex%2520cultural%2520narratives%252C%2520which%2520may%2520lead%2520to%2520severe%250Aconsequences.%2520Therefore%252C%2520investigating%2520value%2520misalignment%2520in%2520the%2520context%2520of%2520LLM%250Afor%2520cultural%2520heritage%2520is%2520crucial%2520for%2520mitigating%2520these%2520risks%252C%2520yet%2520there%2520has%2520been%250Aa%2520significant%2520lack%2520of%2520systematic%2520and%2520comprehensive%2520study%2520and%2520investigation%2520in%250Athis%2520area.%2520To%2520fill%2520this%2520gap%252C%2520we%2520systematically%2520assess%2520the%2520reliability%2520of%2520LLMs%250Ain%2520generating%2520culturally%2520aligned%2520texts%2520for%2520cultural%2520heritage-related%2520tasks.%2520We%250Aconduct%2520a%2520comprehensive%2520evaluation%2520by%2520compiling%2520an%2520extensive%2520set%2520of%25201066%2520query%250Atasks%2520covering%25205%2520widely%2520recognized%2520categories%2520with%252017%2520aspects%2520within%2520the%250Aknowledge%2520framework%2520of%2520cultural%2520heritage%2520across%25205%2520open-source%2520LLMs%252C%2520and%2520examine%250Aboth%2520the%2520type%2520and%2520rate%2520of%2520cultural%2520value%2520misalignments%2520in%2520the%2520generated%2520texts.%250AUsing%2520both%2520automated%2520and%2520manual%2520approaches%252C%2520we%2520effectively%2520detect%2520and%2520analyze%250Athe%2520cultural%2520value%2520misalignments%2520in%2520LLM-generated%2520texts.%2520Our%2520findings%2520are%250Aconcerning%253A%2520over%252065%2525%2520of%2520the%2520generated%2520texts%2520exhibit%2520notable%2520cultural%250Amisalignments%252C%2520with%2520certain%2520tasks%2520demonstrating%2520almost%2520complete%2520misalignment%250Awith%2520key%2520cultural%2520values.%2520Beyond%2520these%2520findings%252C%2520this%2520paper%2520introduces%2520a%250Abenchmark%2520dataset%2520and%2520a%2520comprehensive%2520evaluation%2520workflow%2520that%2520can%2520serve%2520as%2520a%250Avaluable%2520resource%2520for%2520future%2520research%2520aimed%2520at%2520enhancing%2520the%2520cultural%250Asensitivity%2520and%2520reliability%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Investigation%20into%20Value%20Misalignment%20in%20LLM-Generated%20Texts%20for%0A%20%20Cultural%20Heritage&entry.906535625=Fan%20Bu%20and%20Zheng%20Wang%20and%20Siyi%20Wang%20and%20Ziyao%20Liu&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20increasingly%20prevalent%20in%20tasks%0Arelated%20to%20cultural%20heritage%2C%20such%20as%20generating%20descriptions%20of%20historical%0Amonuments%2C%20translating%20ancient%20texts%2C%20preserving%20oral%20traditions%2C%20and%20creating%0Aeducational%20content%2C%20their%20ability%20to%20produce%20accurate%20and%20culturally%20aligned%0Atexts%20is%20being%20increasingly%20relied%20upon%20by%20users%20and%20researchers.%20However%2C%0Acultural%20value%20misalignments%20may%20exist%20in%20generated%20texts%2C%20such%20as%20the%0Amisrepresentation%20of%20historical%20facts%2C%20the%20erosion%20of%20cultural%20identity%2C%20and%0Athe%20oversimplification%20of%20complex%20cultural%20narratives%2C%20which%20may%20lead%20to%20severe%0Aconsequences.%20Therefore%2C%20investigating%20value%20misalignment%20in%20the%20context%20of%20LLM%0Afor%20cultural%20heritage%20is%20crucial%20for%20mitigating%20these%20risks%2C%20yet%20there%20has%20been%0Aa%20significant%20lack%20of%20systematic%20and%20comprehensive%20study%20and%20investigation%20in%0Athis%20area.%20To%20fill%20this%20gap%2C%20we%20systematically%20assess%20the%20reliability%20of%20LLMs%0Ain%20generating%20culturally%20aligned%20texts%20for%20cultural%20heritage-related%20tasks.%20We%0Aconduct%20a%20comprehensive%20evaluation%20by%20compiling%20an%20extensive%20set%20of%201066%20query%0Atasks%20covering%205%20widely%20recognized%20categories%20with%2017%20aspects%20within%20the%0Aknowledge%20framework%20of%20cultural%20heritage%20across%205%20open-source%20LLMs%2C%20and%20examine%0Aboth%20the%20type%20and%20rate%20of%20cultural%20value%20misalignments%20in%20the%20generated%20texts.%0AUsing%20both%20automated%20and%20manual%20approaches%2C%20we%20effectively%20detect%20and%20analyze%0Athe%20cultural%20value%20misalignments%20in%20LLM-generated%20texts.%20Our%20findings%20are%0Aconcerning%3A%20over%2065%25%20of%20the%20generated%20texts%20exhibit%20notable%20cultural%0Amisalignments%2C%20with%20certain%20tasks%20demonstrating%20almost%20complete%20misalignment%0Awith%20key%20cultural%20values.%20Beyond%20these%20findings%2C%20this%20paper%20introduces%20a%0Abenchmark%20dataset%20and%20a%20comprehensive%20evaluation%20workflow%20that%20can%20serve%20as%20a%0Avaluable%20resource%20for%20future%20research%20aimed%20at%20enhancing%20the%20cultural%0Asensitivity%20and%20reliability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02039v3&entry.124074799=Read"},
{"title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated\n  Pre-Consultation Questionnaire Generation", "author": "Ruiqing Ding and Qianfang Sun and Yongkang Leng and Hui Yin and Xiaojian Li", "abstract": "  Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.\n", "link": "http://arxiv.org/abs/2508.00581v1", "date": "2025-08-01", "relevancy": 1.8565, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20EMR%20Data%20to%20Clinical%20Insight%3A%20An%20LLM-Driven%20Framework%20for%20Automated%0A%20%20Pre-Consultation%20Questionnaire%20Generation&body=Title%3A%20From%20EMR%20Data%20to%20Clinical%20Insight%3A%20An%20LLM-Driven%20Framework%20for%20Automated%0A%20%20Pre-Consultation%20Questionnaire%20Generation%0AAuthor%3A%20Ruiqing%20Ding%20and%20Qianfang%20Sun%20and%20Yongkang%20Leng%20and%20Hui%20Yin%20and%20Xiaojian%20Li%0AAbstract%3A%20%20%20Pre-consultation%20is%20a%20critical%20component%20of%20effective%20healthcare%20delivery.%0AHowever%2C%20generating%20comprehensive%20pre-consultation%20questionnaires%20from%20complex%2C%0Avoluminous%20Electronic%20Medical%20Records%20%28EMRs%29%20is%20a%20challenging%20task.%20Direct%0ALarge%20Language%20Model%20%28LLM%29%20approaches%20face%20difficulties%20in%20this%20task%2C%0Aparticularly%20regarding%20information%20completeness%2C%20logical%20order%2C%20and%0Adisease-level%20synthesis.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20multi-stage%0ALLM-driven%20framework%3A%20Stage%201%20extracts%20atomic%20assertions%20%28key%20facts%20with%0Atiming%29%20from%20EMRs%3B%20Stage%202%20constructs%20personal%20causal%20networks%20and%20synthesizes%0Adisease%20knowledge%20by%20clustering%20representative%20networks%20from%20an%20EMR%20corpus%3B%0AStage%203%20generates%20tailored%20personal%20and%20standardized%20disease-specific%0Aquestionnaires%20based%20on%20these%20structured%20representations.%20This%20framework%0Aovercomes%20limitations%20of%20direct%20methods%20by%20building%20explicit%20clinical%0Aknowledge.%20Evaluated%20on%20a%20real-world%20EMR%20dataset%20and%20validated%20by%20clinical%0Aexperts%2C%20our%20method%20demonstrates%20superior%20performance%20in%20information%20coverage%2C%0Adiagnostic%20relevance%2C%20understandability%2C%20and%20generation%20time%2C%20highlighting%20its%0Apractical%20potential%20to%20enhance%20patient%20information%20collection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520EMR%2520Data%2520to%2520Clinical%2520Insight%253A%2520An%2520LLM-Driven%2520Framework%2520for%2520Automated%250A%2520%2520Pre-Consultation%2520Questionnaire%2520Generation%26entry.906535625%3DRuiqing%2520Ding%2520and%2520Qianfang%2520Sun%2520and%2520Yongkang%2520Leng%2520and%2520Hui%2520Yin%2520and%2520Xiaojian%2520Li%26entry.1292438233%3D%2520%2520Pre-consultation%2520is%2520a%2520critical%2520component%2520of%2520effective%2520healthcare%2520delivery.%250AHowever%252C%2520generating%2520comprehensive%2520pre-consultation%2520questionnaires%2520from%2520complex%252C%250Avoluminous%2520Electronic%2520Medical%2520Records%2520%2528EMRs%2529%2520is%2520a%2520challenging%2520task.%2520Direct%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520approaches%2520face%2520difficulties%2520in%2520this%2520task%252C%250Aparticularly%2520regarding%2520information%2520completeness%252C%2520logical%2520order%252C%2520and%250Adisease-level%2520synthesis.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520multi-stage%250ALLM-driven%2520framework%253A%2520Stage%25201%2520extracts%2520atomic%2520assertions%2520%2528key%2520facts%2520with%250Atiming%2529%2520from%2520EMRs%253B%2520Stage%25202%2520constructs%2520personal%2520causal%2520networks%2520and%2520synthesizes%250Adisease%2520knowledge%2520by%2520clustering%2520representative%2520networks%2520from%2520an%2520EMR%2520corpus%253B%250AStage%25203%2520generates%2520tailored%2520personal%2520and%2520standardized%2520disease-specific%250Aquestionnaires%2520based%2520on%2520these%2520structured%2520representations.%2520This%2520framework%250Aovercomes%2520limitations%2520of%2520direct%2520methods%2520by%2520building%2520explicit%2520clinical%250Aknowledge.%2520Evaluated%2520on%2520a%2520real-world%2520EMR%2520dataset%2520and%2520validated%2520by%2520clinical%250Aexperts%252C%2520our%2520method%2520demonstrates%2520superior%2520performance%2520in%2520information%2520coverage%252C%250Adiagnostic%2520relevance%252C%2520understandability%252C%2520and%2520generation%2520time%252C%2520highlighting%2520its%250Apractical%2520potential%2520to%2520enhance%2520patient%2520information%2520collection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20EMR%20Data%20to%20Clinical%20Insight%3A%20An%20LLM-Driven%20Framework%20for%20Automated%0A%20%20Pre-Consultation%20Questionnaire%20Generation&entry.906535625=Ruiqing%20Ding%20and%20Qianfang%20Sun%20and%20Yongkang%20Leng%20and%20Hui%20Yin%20and%20Xiaojian%20Li&entry.1292438233=%20%20Pre-consultation%20is%20a%20critical%20component%20of%20effective%20healthcare%20delivery.%0AHowever%2C%20generating%20comprehensive%20pre-consultation%20questionnaires%20from%20complex%2C%0Avoluminous%20Electronic%20Medical%20Records%20%28EMRs%29%20is%20a%20challenging%20task.%20Direct%0ALarge%20Language%20Model%20%28LLM%29%20approaches%20face%20difficulties%20in%20this%20task%2C%0Aparticularly%20regarding%20information%20completeness%2C%20logical%20order%2C%20and%0Adisease-level%20synthesis.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20multi-stage%0ALLM-driven%20framework%3A%20Stage%201%20extracts%20atomic%20assertions%20%28key%20facts%20with%0Atiming%29%20from%20EMRs%3B%20Stage%202%20constructs%20personal%20causal%20networks%20and%20synthesizes%0Adisease%20knowledge%20by%20clustering%20representative%20networks%20from%20an%20EMR%20corpus%3B%0AStage%203%20generates%20tailored%20personal%20and%20standardized%20disease-specific%0Aquestionnaires%20based%20on%20these%20structured%20representations.%20This%20framework%0Aovercomes%20limitations%20of%20direct%20methods%20by%20building%20explicit%20clinical%0Aknowledge.%20Evaluated%20on%20a%20real-world%20EMR%20dataset%20and%20validated%20by%20clinical%0Aexperts%2C%20our%20method%20demonstrates%20superior%20performance%20in%20information%20coverage%2C%0Adiagnostic%20relevance%2C%20understandability%2C%20and%20generation%20time%2C%20highlighting%20its%0Apractical%20potential%20to%20enhance%20patient%20information%20collection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00581v1&entry.124074799=Read"},
{"title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions", "author": "Babak Taati and Muhammad Muzammil and Yasamin Zarghami and Abhishek Moturu and Amirhossein Kazerouni and Hailey Reimer and Alex Mihailidis and Thomas Hadjistavropoulos", "abstract": "  Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP\n", "link": "http://arxiv.org/abs/2507.19673v2", "date": "2025-08-01", "relevancy": 1.8505, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4864}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4493}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynPAIN%3A%20A%20Synthetic%20Dataset%20of%20Pain%20and%20Non-Pain%20Facial%20Expressions&body=Title%3A%20SynPAIN%3A%20A%20Synthetic%20Dataset%20of%20Pain%20and%20Non-Pain%20Facial%20Expressions%0AAuthor%3A%20Babak%20Taati%20and%20Muhammad%20Muzammil%20and%20Yasamin%20Zarghami%20and%20Abhishek%20Moturu%20and%20Amirhossein%20Kazerouni%20and%20Hailey%20Reimer%20and%20Alex%20Mihailidis%20and%20Thomas%20Hadjistavropoulos%0AAbstract%3A%20%20%20Accurate%20pain%20assessment%20in%20patients%20with%20limited%20ability%20to%20communicate%2C%0Asuch%20as%20older%20adults%20with%20dementia%2C%20represents%20a%20critical%20healthcare%20challenge.%0ARobust%20automated%20systems%20of%20pain%20detection%20may%20facilitate%20such%20assessments.%0AExisting%20pain%20detection%20datasets%2C%20however%2C%20suffer%20from%20limited%20ethnic/racial%0Adiversity%2C%20privacy%20constraints%2C%20and%20underrepresentation%20of%20older%20adults%20who%20are%0Athe%20primary%20target%20population%20for%20clinical%20deployment.%20We%20present%20SynPAIN%2C%20a%0Alarge-scale%20synthetic%20dataset%20containing%2010%2C710%20facial%20expression%20images%20%285%2C355%0Aneutral/expressive%20pairs%29%20across%20five%20ethnicities/races%2C%20two%20age%20groups%20%28young%3A%0A20-35%2C%20old%3A%2075%2B%29%2C%20and%20two%20genders.%20Using%20commercial%20generative%20AI%20tools%2C%20we%0Acreated%20demographically%20balanced%20synthetic%20identities%20with%20clinically%0Ameaningful%20pain%20expressions.%20Our%20validation%20demonstrates%20that%20synthetic%20pain%0Aexpressions%20exhibit%20expected%20pain%20patterns%2C%20scoring%20significantly%20higher%20than%0Aneutral%20and%20non-pain%20expressions%20using%20clinically%20validated%20pain%20assessment%0Atools%20based%20on%20facial%20action%20unit%20analysis.%20We%20experimentally%20demonstrate%0ASynPAIN%27s%20utility%20in%20identifying%20algorithmic%20bias%20in%20existing%20pain%20detection%0Amodels.%20Through%20comprehensive%20bias%20evaluation%2C%20we%20reveal%20substantial%0Aperformance%20disparities%20across%20demographic%20characteristics.%20These%20performance%0Adisparities%20were%20previously%20undetectable%20with%20smaller%2C%20less%20diverse%20datasets.%0AFurthermore%2C%20we%20demonstrate%20that%20age-matched%20synthetic%20data%20augmentation%0Aimproves%20pain%20detection%20performance%20on%20real%20clinical%20data%2C%20achieving%20a%207.0%25%0Aimprovement%20in%20average%20precision.%20SynPAIN%20addresses%20critical%20gaps%20in%20pain%0Aassessment%20research%20by%20providing%20the%20first%20publicly%20available%2C%20demographically%0Adiverse%20synthetic%20dataset%20specifically%20designed%20for%20older%20adult%20pain%20detection%2C%0Awhile%20establishing%20a%20framework%20for%20measuring%20and%20mitigating%20algorithmic%20bias.%0AThe%20dataset%20is%20available%20at%20https%3A//doi.org/10.5683/SP3/WCXMAP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynPAIN%253A%2520A%2520Synthetic%2520Dataset%2520of%2520Pain%2520and%2520Non-Pain%2520Facial%2520Expressions%26entry.906535625%3DBabak%2520Taati%2520and%2520Muhammad%2520Muzammil%2520and%2520Yasamin%2520Zarghami%2520and%2520Abhishek%2520Moturu%2520and%2520Amirhossein%2520Kazerouni%2520and%2520Hailey%2520Reimer%2520and%2520Alex%2520Mihailidis%2520and%2520Thomas%2520Hadjistavropoulos%26entry.1292438233%3D%2520%2520Accurate%2520pain%2520assessment%2520in%2520patients%2520with%2520limited%2520ability%2520to%2520communicate%252C%250Asuch%2520as%2520older%2520adults%2520with%2520dementia%252C%2520represents%2520a%2520critical%2520healthcare%2520challenge.%250ARobust%2520automated%2520systems%2520of%2520pain%2520detection%2520may%2520facilitate%2520such%2520assessments.%250AExisting%2520pain%2520detection%2520datasets%252C%2520however%252C%2520suffer%2520from%2520limited%2520ethnic/racial%250Adiversity%252C%2520privacy%2520constraints%252C%2520and%2520underrepresentation%2520of%2520older%2520adults%2520who%2520are%250Athe%2520primary%2520target%2520population%2520for%2520clinical%2520deployment.%2520We%2520present%2520SynPAIN%252C%2520a%250Alarge-scale%2520synthetic%2520dataset%2520containing%252010%252C710%2520facial%2520expression%2520images%2520%25285%252C355%250Aneutral/expressive%2520pairs%2529%2520across%2520five%2520ethnicities/races%252C%2520two%2520age%2520groups%2520%2528young%253A%250A20-35%252C%2520old%253A%252075%252B%2529%252C%2520and%2520two%2520genders.%2520Using%2520commercial%2520generative%2520AI%2520tools%252C%2520we%250Acreated%2520demographically%2520balanced%2520synthetic%2520identities%2520with%2520clinically%250Ameaningful%2520pain%2520expressions.%2520Our%2520validation%2520demonstrates%2520that%2520synthetic%2520pain%250Aexpressions%2520exhibit%2520expected%2520pain%2520patterns%252C%2520scoring%2520significantly%2520higher%2520than%250Aneutral%2520and%2520non-pain%2520expressions%2520using%2520clinically%2520validated%2520pain%2520assessment%250Atools%2520based%2520on%2520facial%2520action%2520unit%2520analysis.%2520We%2520experimentally%2520demonstrate%250ASynPAIN%2527s%2520utility%2520in%2520identifying%2520algorithmic%2520bias%2520in%2520existing%2520pain%2520detection%250Amodels.%2520Through%2520comprehensive%2520bias%2520evaluation%252C%2520we%2520reveal%2520substantial%250Aperformance%2520disparities%2520across%2520demographic%2520characteristics.%2520These%2520performance%250Adisparities%2520were%2520previously%2520undetectable%2520with%2520smaller%252C%2520less%2520diverse%2520datasets.%250AFurthermore%252C%2520we%2520demonstrate%2520that%2520age-matched%2520synthetic%2520data%2520augmentation%250Aimproves%2520pain%2520detection%2520performance%2520on%2520real%2520clinical%2520data%252C%2520achieving%2520a%25207.0%2525%250Aimprovement%2520in%2520average%2520precision.%2520SynPAIN%2520addresses%2520critical%2520gaps%2520in%2520pain%250Aassessment%2520research%2520by%2520providing%2520the%2520first%2520publicly%2520available%252C%2520demographically%250Adiverse%2520synthetic%2520dataset%2520specifically%2520designed%2520for%2520older%2520adult%2520pain%2520detection%252C%250Awhile%2520establishing%2520a%2520framework%2520for%2520measuring%2520and%2520mitigating%2520algorithmic%2520bias.%250AThe%2520dataset%2520is%2520available%2520at%2520https%253A//doi.org/10.5683/SP3/WCXMAP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynPAIN%3A%20A%20Synthetic%20Dataset%20of%20Pain%20and%20Non-Pain%20Facial%20Expressions&entry.906535625=Babak%20Taati%20and%20Muhammad%20Muzammil%20and%20Yasamin%20Zarghami%20and%20Abhishek%20Moturu%20and%20Amirhossein%20Kazerouni%20and%20Hailey%20Reimer%20and%20Alex%20Mihailidis%20and%20Thomas%20Hadjistavropoulos&entry.1292438233=%20%20Accurate%20pain%20assessment%20in%20patients%20with%20limited%20ability%20to%20communicate%2C%0Asuch%20as%20older%20adults%20with%20dementia%2C%20represents%20a%20critical%20healthcare%20challenge.%0ARobust%20automated%20systems%20of%20pain%20detection%20may%20facilitate%20such%20assessments.%0AExisting%20pain%20detection%20datasets%2C%20however%2C%20suffer%20from%20limited%20ethnic/racial%0Adiversity%2C%20privacy%20constraints%2C%20and%20underrepresentation%20of%20older%20adults%20who%20are%0Athe%20primary%20target%20population%20for%20clinical%20deployment.%20We%20present%20SynPAIN%2C%20a%0Alarge-scale%20synthetic%20dataset%20containing%2010%2C710%20facial%20expression%20images%20%285%2C355%0Aneutral/expressive%20pairs%29%20across%20five%20ethnicities/races%2C%20two%20age%20groups%20%28young%3A%0A20-35%2C%20old%3A%2075%2B%29%2C%20and%20two%20genders.%20Using%20commercial%20generative%20AI%20tools%2C%20we%0Acreated%20demographically%20balanced%20synthetic%20identities%20with%20clinically%0Ameaningful%20pain%20expressions.%20Our%20validation%20demonstrates%20that%20synthetic%20pain%0Aexpressions%20exhibit%20expected%20pain%20patterns%2C%20scoring%20significantly%20higher%20than%0Aneutral%20and%20non-pain%20expressions%20using%20clinically%20validated%20pain%20assessment%0Atools%20based%20on%20facial%20action%20unit%20analysis.%20We%20experimentally%20demonstrate%0ASynPAIN%27s%20utility%20in%20identifying%20algorithmic%20bias%20in%20existing%20pain%20detection%0Amodels.%20Through%20comprehensive%20bias%20evaluation%2C%20we%20reveal%20substantial%0Aperformance%20disparities%20across%20demographic%20characteristics.%20These%20performance%0Adisparities%20were%20previously%20undetectable%20with%20smaller%2C%20less%20diverse%20datasets.%0AFurthermore%2C%20we%20demonstrate%20that%20age-matched%20synthetic%20data%20augmentation%0Aimproves%20pain%20detection%20performance%20on%20real%20clinical%20data%2C%20achieving%20a%207.0%25%0Aimprovement%20in%20average%20precision.%20SynPAIN%20addresses%20critical%20gaps%20in%20pain%0Aassessment%20research%20by%20providing%20the%20first%20publicly%20available%2C%20demographically%0Adiverse%20synthetic%20dataset%20specifically%20designed%20for%20older%20adult%20pain%20detection%2C%0Awhile%20establishing%20a%20framework%20for%20measuring%20and%20mitigating%20algorithmic%20bias.%0AThe%20dataset%20is%20available%20at%20https%3A//doi.org/10.5683/SP3/WCXMAP%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19673v2&entry.124074799=Read"},
{"title": "How Far Are AI Scientists from Changing the World?", "author": "Qiujie Xie and Yixuan Weng and Minjun Zhu and Fuchen Shen and Shulin Huang and Zhen Lin and Jiahui Zhou and Zilan Mao and Zijie Yang and Linyi Yang and Jian Wu and Yue Zhang", "abstract": "  The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.\n", "link": "http://arxiv.org/abs/2507.23276v2", "date": "2025-08-01", "relevancy": 1.847, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4655}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20Are%20AI%20Scientists%20from%20Changing%20the%20World%3F&body=Title%3A%20How%20Far%20Are%20AI%20Scientists%20from%20Changing%20the%20World%3F%0AAuthor%3A%20Qiujie%20Xie%20and%20Yixuan%20Weng%20and%20Minjun%20Zhu%20and%20Fuchen%20Shen%20and%20Shulin%20Huang%20and%20Zhen%20Lin%20and%20Jiahui%20Zhou%20and%20Zilan%20Mao%20and%20Zijie%20Yang%20and%20Linyi%20Yang%20and%20Jian%20Wu%20and%20Yue%20Zhang%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20is%20propelling%20automated%0Ascientific%20discovery%20to%20the%20next%20level%2C%20with%20LLM-based%20Artificial%20Intelligence%0A%28AI%29%20Scientist%20systems%20now%20taking%20the%20lead%20in%20scientific%20research.%20Several%0Ainfluential%20works%20have%20already%20appeared%20in%20the%20field%20of%20AI%20Scientist%20systems%2C%0Awith%20AI-generated%20research%20papers%20having%20been%20accepted%20at%20the%20ICLR%202025%0Aworkshop%2C%20suggesting%20that%20a%20human-level%20AI%20Scientist%20capable%20of%20uncovering%0Aphenomena%20previously%20unknown%20to%20humans%2C%20may%20soon%20become%20a%20reality.%20In%20this%0Asurvey%2C%20we%20focus%20on%20the%20central%20question%3A%20How%20far%20are%20AI%20scientists%20from%0Achanging%20the%20world%20and%20reshaping%20the%20scientific%20research%20paradigm%3F%20To%20answer%0Athis%20question%2C%20we%20provide%20a%20prospect-driven%20review%20that%20comprehensively%0Aanalyzes%20the%20current%20achievements%20of%20AI%20Scientist%20systems%2C%20identifying%20key%0Abottlenecks%20and%20the%20critical%20components%20required%20for%20the%20emergence%20of%20a%0Ascientific%20agent%20capable%20of%20producing%20ground-breaking%20discoveries%20that%20solve%0Agrand%20challenges.%20We%20hope%20this%20survey%20will%20contribute%20to%20a%20clearer%0Aunderstanding%20of%20limitations%20of%20current%20AI%20Scientist%20systems%2C%20showing%20where%20we%0Aare%2C%20what%20is%20missing%2C%20and%20what%20the%20ultimate%20goals%20for%20scientific%20AI%20should%20be.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520Are%2520AI%2520Scientists%2520from%2520Changing%2520the%2520World%253F%26entry.906535625%3DQiujie%2520Xie%2520and%2520Yixuan%2520Weng%2520and%2520Minjun%2520Zhu%2520and%2520Fuchen%2520Shen%2520and%2520Shulin%2520Huang%2520and%2520Zhen%2520Lin%2520and%2520Jiahui%2520Zhou%2520and%2520Zilan%2520Mao%2520and%2520Zijie%2520Yang%2520and%2520Linyi%2520Yang%2520and%2520Jian%2520Wu%2520and%2520Yue%2520Zhang%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520propelling%2520automated%250Ascientific%2520discovery%2520to%2520the%2520next%2520level%252C%2520with%2520LLM-based%2520Artificial%2520Intelligence%250A%2528AI%2529%2520Scientist%2520systems%2520now%2520taking%2520the%2520lead%2520in%2520scientific%2520research.%2520Several%250Ainfluential%2520works%2520have%2520already%2520appeared%2520in%2520the%2520field%2520of%2520AI%2520Scientist%2520systems%252C%250Awith%2520AI-generated%2520research%2520papers%2520having%2520been%2520accepted%2520at%2520the%2520ICLR%25202025%250Aworkshop%252C%2520suggesting%2520that%2520a%2520human-level%2520AI%2520Scientist%2520capable%2520of%2520uncovering%250Aphenomena%2520previously%2520unknown%2520to%2520humans%252C%2520may%2520soon%2520become%2520a%2520reality.%2520In%2520this%250Asurvey%252C%2520we%2520focus%2520on%2520the%2520central%2520question%253A%2520How%2520far%2520are%2520AI%2520scientists%2520from%250Achanging%2520the%2520world%2520and%2520reshaping%2520the%2520scientific%2520research%2520paradigm%253F%2520To%2520answer%250Athis%2520question%252C%2520we%2520provide%2520a%2520prospect-driven%2520review%2520that%2520comprehensively%250Aanalyzes%2520the%2520current%2520achievements%2520of%2520AI%2520Scientist%2520systems%252C%2520identifying%2520key%250Abottlenecks%2520and%2520the%2520critical%2520components%2520required%2520for%2520the%2520emergence%2520of%2520a%250Ascientific%2520agent%2520capable%2520of%2520producing%2520ground-breaking%2520discoveries%2520that%2520solve%250Agrand%2520challenges.%2520We%2520hope%2520this%2520survey%2520will%2520contribute%2520to%2520a%2520clearer%250Aunderstanding%2520of%2520limitations%2520of%2520current%2520AI%2520Scientist%2520systems%252C%2520showing%2520where%2520we%250Aare%252C%2520what%2520is%2520missing%252C%2520and%2520what%2520the%2520ultimate%2520goals%2520for%2520scientific%2520AI%2520should%2520be.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20Are%20AI%20Scientists%20from%20Changing%20the%20World%3F&entry.906535625=Qiujie%20Xie%20and%20Yixuan%20Weng%20and%20Minjun%20Zhu%20and%20Fuchen%20Shen%20and%20Shulin%20Huang%20and%20Zhen%20Lin%20and%20Jiahui%20Zhou%20and%20Zilan%20Mao%20and%20Zijie%20Yang%20and%20Linyi%20Yang%20and%20Jian%20Wu%20and%20Yue%20Zhang&entry.1292438233=%20%20The%20emergence%20of%20large%20language%20models%20%28LLMs%29%20is%20propelling%20automated%0Ascientific%20discovery%20to%20the%20next%20level%2C%20with%20LLM-based%20Artificial%20Intelligence%0A%28AI%29%20Scientist%20systems%20now%20taking%20the%20lead%20in%20scientific%20research.%20Several%0Ainfluential%20works%20have%20already%20appeared%20in%20the%20field%20of%20AI%20Scientist%20systems%2C%0Awith%20AI-generated%20research%20papers%20having%20been%20accepted%20at%20the%20ICLR%202025%0Aworkshop%2C%20suggesting%20that%20a%20human-level%20AI%20Scientist%20capable%20of%20uncovering%0Aphenomena%20previously%20unknown%20to%20humans%2C%20may%20soon%20become%20a%20reality.%20In%20this%0Asurvey%2C%20we%20focus%20on%20the%20central%20question%3A%20How%20far%20are%20AI%20scientists%20from%0Achanging%20the%20world%20and%20reshaping%20the%20scientific%20research%20paradigm%3F%20To%20answer%0Athis%20question%2C%20we%20provide%20a%20prospect-driven%20review%20that%20comprehensively%0Aanalyzes%20the%20current%20achievements%20of%20AI%20Scientist%20systems%2C%20identifying%20key%0Abottlenecks%20and%20the%20critical%20components%20required%20for%20the%20emergence%20of%20a%0Ascientific%20agent%20capable%20of%20producing%20ground-breaking%20discoveries%20that%20solve%0Agrand%20challenges.%20We%20hope%20this%20survey%20will%20contribute%20to%20a%20clearer%0Aunderstanding%20of%20limitations%20of%20current%20AI%20Scientist%20systems%2C%20showing%20where%20we%0Aare%2C%20what%20is%20missing%2C%20and%20what%20the%20ultimate%20goals%20for%20scientific%20AI%20should%20be.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23276v2&entry.124074799=Read"},
{"title": "Discovering the underlying analytic structure within Standard Model\n  constants using artificial intelligence", "author": "S. V. Chekanov and H. Kjellerstrand", "abstract": "  This paper presents a search for underlying analytic structures among the\nfundamental parameters of the Standard Model (SM) using symbolic regression and\ngenetic programming. We identify the simplest analytic relationships connecting\npairs of these constants and report several notable expressions obtained with\nrelative precision better than 1%. These results may serve as valuable inputs\nfor model builders and artificial intelligence methods aimed at uncovering\nhidden patterns among the SM constants, or potentially used as building blocks\nfor a deeper underlying law that connects all parameters of the SM through a\nsmall set of fundamental constants.\n", "link": "http://arxiv.org/abs/2507.00225v2", "date": "2025-08-01", "relevancy": 1.8154, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20the%20underlying%20analytic%20structure%20within%20Standard%20Model%0A%20%20constants%20using%20artificial%20intelligence&body=Title%3A%20Discovering%20the%20underlying%20analytic%20structure%20within%20Standard%20Model%0A%20%20constants%20using%20artificial%20intelligence%0AAuthor%3A%20S.%20V.%20Chekanov%20and%20H.%20Kjellerstrand%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20search%20for%20underlying%20analytic%20structures%20among%20the%0Afundamental%20parameters%20of%20the%20Standard%20Model%20%28SM%29%20using%20symbolic%20regression%20and%0Agenetic%20programming.%20We%20identify%20the%20simplest%20analytic%20relationships%20connecting%0Apairs%20of%20these%20constants%20and%20report%20several%20notable%20expressions%20obtained%20with%0Arelative%20precision%20better%20than%201%25.%20These%20results%20may%20serve%20as%20valuable%20inputs%0Afor%20model%20builders%20and%20artificial%20intelligence%20methods%20aimed%20at%20uncovering%0Ahidden%20patterns%20among%20the%20SM%20constants%2C%20or%20potentially%20used%20as%20building%20blocks%0Afor%20a%20deeper%20underlying%20law%20that%20connects%20all%20parameters%20of%20the%20SM%20through%20a%0Asmall%20set%20of%20fundamental%20constants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520the%2520underlying%2520analytic%2520structure%2520within%2520Standard%2520Model%250A%2520%2520constants%2520using%2520artificial%2520intelligence%26entry.906535625%3DS.%2520V.%2520Chekanov%2520and%2520H.%2520Kjellerstrand%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520search%2520for%2520underlying%2520analytic%2520structures%2520among%2520the%250Afundamental%2520parameters%2520of%2520the%2520Standard%2520Model%2520%2528SM%2529%2520using%2520symbolic%2520regression%2520and%250Agenetic%2520programming.%2520We%2520identify%2520the%2520simplest%2520analytic%2520relationships%2520connecting%250Apairs%2520of%2520these%2520constants%2520and%2520report%2520several%2520notable%2520expressions%2520obtained%2520with%250Arelative%2520precision%2520better%2520than%25201%2525.%2520These%2520results%2520may%2520serve%2520as%2520valuable%2520inputs%250Afor%2520model%2520builders%2520and%2520artificial%2520intelligence%2520methods%2520aimed%2520at%2520uncovering%250Ahidden%2520patterns%2520among%2520the%2520SM%2520constants%252C%2520or%2520potentially%2520used%2520as%2520building%2520blocks%250Afor%2520a%2520deeper%2520underlying%2520law%2520that%2520connects%2520all%2520parameters%2520of%2520the%2520SM%2520through%2520a%250Asmall%2520set%2520of%2520fundamental%2520constants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20the%20underlying%20analytic%20structure%20within%20Standard%20Model%0A%20%20constants%20using%20artificial%20intelligence&entry.906535625=S.%20V.%20Chekanov%20and%20H.%20Kjellerstrand&entry.1292438233=%20%20This%20paper%20presents%20a%20search%20for%20underlying%20analytic%20structures%20among%20the%0Afundamental%20parameters%20of%20the%20Standard%20Model%20%28SM%29%20using%20symbolic%20regression%20and%0Agenetic%20programming.%20We%20identify%20the%20simplest%20analytic%20relationships%20connecting%0Apairs%20of%20these%20constants%20and%20report%20several%20notable%20expressions%20obtained%20with%0Arelative%20precision%20better%20than%201%25.%20These%20results%20may%20serve%20as%20valuable%20inputs%0Afor%20model%20builders%20and%20artificial%20intelligence%20methods%20aimed%20at%20uncovering%0Ahidden%20patterns%20among%20the%20SM%20constants%2C%20or%20potentially%20used%20as%20building%20blocks%0Afor%20a%20deeper%20underlying%20law%20that%20connects%20all%20parameters%20of%20the%20SM%20through%20a%0Asmall%20set%20of%20fundamental%20constants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00225v2&entry.124074799=Read"},
{"title": "Learning to optimize with guarantees: a complete characterization of\n  linearly convergent algorithms", "author": "Andrea Martin and Ian R. Manchester and Luca Furieri", "abstract": "  In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.\n", "link": "http://arxiv.org/abs/2508.00775v1", "date": "2025-08-01", "relevancy": 1.8129, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4665}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4556}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20optimize%20with%20guarantees%3A%20a%20complete%20characterization%20of%0A%20%20linearly%20convergent%20algorithms&body=Title%3A%20Learning%20to%20optimize%20with%20guarantees%3A%20a%20complete%20characterization%20of%0A%20%20linearly%20convergent%20algorithms%0AAuthor%3A%20Andrea%20Martin%20and%20Ian%20R.%20Manchester%20and%20Luca%20Furieri%0AAbstract%3A%20%20%20In%20high-stakes%20engineering%20applications%2C%20optimization%20algorithms%20must%20come%0Awith%20provable%20worst-case%20guarantees%20over%20a%20mathematically%20defined%20class%20of%0Aproblems.%20Designing%20for%20the%20worst%20case%2C%20however%2C%20inevitably%20sacrifices%0Aperformance%20on%20the%20specific%20problem%20instances%20that%20often%20occur%20in%20practice.%20We%0Aaddress%20the%20problem%20of%20augmenting%20a%20given%20linearly%20convergent%20algorithm%20to%0Aimprove%20its%20average-case%20performance%20on%20a%20restricted%20set%20of%20target%20problems%20-%0Afor%20example%2C%20tailoring%20an%20off-the-shelf%20solver%20for%20model%20predictive%20control%0A%28MPC%29%20for%20an%20application%20to%20a%20specific%20dynamical%20system%20-%20while%20preserving%20its%0Aworst-case%20guarantees%20across%20the%20entire%20problem%20class.%20Toward%20this%20goal%2C%20we%0Acharacterize%20the%20class%20of%20algorithms%20that%20achieve%20linear%20convergence%20for%0Aclasses%20of%20nonsmooth%20composite%20optimization%20problems.%20In%20particular%2C%20starting%0Afrom%20a%20baseline%20linearly%20convergent%20algorithm%2C%20we%20derive%20all%20-%20and%20only%20-%20the%0Amodifications%20to%20its%20update%20rule%20that%20maintain%20its%20convergence%20properties.%20Our%0Aresults%20apply%20to%20augmenting%20legacy%20algorithms%20such%20as%20gradient%20descent%20for%0Anonconvex%2C%20gradient-dominated%20functions%3B%20Nesterov%27s%20accelerated%20method%20for%0Astrongly%20convex%20functions%3B%20and%20projected%20methods%20for%20optimization%20over%0Apolyhedral%20feasibility%20sets.%20We%20showcase%20effectiveness%20of%20the%20approach%20on%0Asolving%20optimization%20problems%20with%20tight%20iteration%20budgets%20in%20application%20to%0Aill-conditioned%20systems%20of%20linear%20equations%20and%20MPC%20for%20linear%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520optimize%2520with%2520guarantees%253A%2520a%2520complete%2520characterization%2520of%250A%2520%2520linearly%2520convergent%2520algorithms%26entry.906535625%3DAndrea%2520Martin%2520and%2520Ian%2520R.%2520Manchester%2520and%2520Luca%2520Furieri%26entry.1292438233%3D%2520%2520In%2520high-stakes%2520engineering%2520applications%252C%2520optimization%2520algorithms%2520must%2520come%250Awith%2520provable%2520worst-case%2520guarantees%2520over%2520a%2520mathematically%2520defined%2520class%2520of%250Aproblems.%2520Designing%2520for%2520the%2520worst%2520case%252C%2520however%252C%2520inevitably%2520sacrifices%250Aperformance%2520on%2520the%2520specific%2520problem%2520instances%2520that%2520often%2520occur%2520in%2520practice.%2520We%250Aaddress%2520the%2520problem%2520of%2520augmenting%2520a%2520given%2520linearly%2520convergent%2520algorithm%2520to%250Aimprove%2520its%2520average-case%2520performance%2520on%2520a%2520restricted%2520set%2520of%2520target%2520problems%2520-%250Afor%2520example%252C%2520tailoring%2520an%2520off-the-shelf%2520solver%2520for%2520model%2520predictive%2520control%250A%2528MPC%2529%2520for%2520an%2520application%2520to%2520a%2520specific%2520dynamical%2520system%2520-%2520while%2520preserving%2520its%250Aworst-case%2520guarantees%2520across%2520the%2520entire%2520problem%2520class.%2520Toward%2520this%2520goal%252C%2520we%250Acharacterize%2520the%2520class%2520of%2520algorithms%2520that%2520achieve%2520linear%2520convergence%2520for%250Aclasses%2520of%2520nonsmooth%2520composite%2520optimization%2520problems.%2520In%2520particular%252C%2520starting%250Afrom%2520a%2520baseline%2520linearly%2520convergent%2520algorithm%252C%2520we%2520derive%2520all%2520-%2520and%2520only%2520-%2520the%250Amodifications%2520to%2520its%2520update%2520rule%2520that%2520maintain%2520its%2520convergence%2520properties.%2520Our%250Aresults%2520apply%2520to%2520augmenting%2520legacy%2520algorithms%2520such%2520as%2520gradient%2520descent%2520for%250Anonconvex%252C%2520gradient-dominated%2520functions%253B%2520Nesterov%2527s%2520accelerated%2520method%2520for%250Astrongly%2520convex%2520functions%253B%2520and%2520projected%2520methods%2520for%2520optimization%2520over%250Apolyhedral%2520feasibility%2520sets.%2520We%2520showcase%2520effectiveness%2520of%2520the%2520approach%2520on%250Asolving%2520optimization%2520problems%2520with%2520tight%2520iteration%2520budgets%2520in%2520application%2520to%250Aill-conditioned%2520systems%2520of%2520linear%2520equations%2520and%2520MPC%2520for%2520linear%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20optimize%20with%20guarantees%3A%20a%20complete%20characterization%20of%0A%20%20linearly%20convergent%20algorithms&entry.906535625=Andrea%20Martin%20and%20Ian%20R.%20Manchester%20and%20Luca%20Furieri&entry.1292438233=%20%20In%20high-stakes%20engineering%20applications%2C%20optimization%20algorithms%20must%20come%0Awith%20provable%20worst-case%20guarantees%20over%20a%20mathematically%20defined%20class%20of%0Aproblems.%20Designing%20for%20the%20worst%20case%2C%20however%2C%20inevitably%20sacrifices%0Aperformance%20on%20the%20specific%20problem%20instances%20that%20often%20occur%20in%20practice.%20We%0Aaddress%20the%20problem%20of%20augmenting%20a%20given%20linearly%20convergent%20algorithm%20to%0Aimprove%20its%20average-case%20performance%20on%20a%20restricted%20set%20of%20target%20problems%20-%0Afor%20example%2C%20tailoring%20an%20off-the-shelf%20solver%20for%20model%20predictive%20control%0A%28MPC%29%20for%20an%20application%20to%20a%20specific%20dynamical%20system%20-%20while%20preserving%20its%0Aworst-case%20guarantees%20across%20the%20entire%20problem%20class.%20Toward%20this%20goal%2C%20we%0Acharacterize%20the%20class%20of%20algorithms%20that%20achieve%20linear%20convergence%20for%0Aclasses%20of%20nonsmooth%20composite%20optimization%20problems.%20In%20particular%2C%20starting%0Afrom%20a%20baseline%20linearly%20convergent%20algorithm%2C%20we%20derive%20all%20-%20and%20only%20-%20the%0Amodifications%20to%20its%20update%20rule%20that%20maintain%20its%20convergence%20properties.%20Our%0Aresults%20apply%20to%20augmenting%20legacy%20algorithms%20such%20as%20gradient%20descent%20for%0Anonconvex%2C%20gradient-dominated%20functions%3B%20Nesterov%27s%20accelerated%20method%20for%0Astrongly%20convex%20functions%3B%20and%20projected%20methods%20for%20optimization%20over%0Apolyhedral%20feasibility%20sets.%20We%20showcase%20effectiveness%20of%20the%20approach%20on%0Asolving%20optimization%20problems%20with%20tight%20iteration%20budgets%20in%20application%20to%0Aill-conditioned%20systems%20of%20linear%20equations%20and%20MPC%20for%20linear%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00775v1&entry.124074799=Read"},
{"title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven\n  Image Generation", "author": "Ruoyu Wang and Huayang Huang and Ye Zhu and Olga Russakovsky and Yu Wu", "abstract": "  In this work, we introduce NoiseQuery as a novel method for enhanced noise\ninitialization in versatile goal-driven text-to-image (T2I) generation.\nSpecifically, we propose to leverage an aligned Gaussian noise as implicit\nguidance to complement explicit user-defined inputs, such as text prompts, for\nbetter generation quality and controllability. Unlike existing noise\noptimization methods designed for specific models, our approach is grounded in\na fundamental examination of the generic finite-step noise scheduler design in\ndiffusion formulation, allowing better generalization across different\ndiffusion-based architectures in a tuning-free manner. This model-agnostic\nnature allows us to construct a reusable noise library compatible with multiple\nT2I models and enhancement techniques, serving as a foundational layer for more\neffective generation. Extensive experiments demonstrate that NoiseQuery enables\nfine-grained control and yields significant performance boosts not only over\nhigh-level semantics but also over low-level visual attributes, which are\ntypically difficult to specify through text alone, with seamless integration\ninto current workflows with minimal computational overhead.\n", "link": "http://arxiv.org/abs/2412.05101v3", "date": "2025-08-01", "relevancy": 1.8114, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6256}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5786}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Silent%20Assistant%3A%20NoiseQuery%20as%20Implicit%20Guidance%20for%20Goal-Driven%0A%20%20Image%20Generation&body=Title%3A%20The%20Silent%20Assistant%3A%20NoiseQuery%20as%20Implicit%20Guidance%20for%20Goal-Driven%0A%20%20Image%20Generation%0AAuthor%3A%20Ruoyu%20Wang%20and%20Huayang%20Huang%20and%20Ye%20Zhu%20and%20Olga%20Russakovsky%20and%20Yu%20Wu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20NoiseQuery%20as%20a%20novel%20method%20for%20enhanced%20noise%0Ainitialization%20in%20versatile%20goal-driven%20text-to-image%20%28T2I%29%20generation.%0ASpecifically%2C%20we%20propose%20to%20leverage%20an%20aligned%20Gaussian%20noise%20as%20implicit%0Aguidance%20to%20complement%20explicit%20user-defined%20inputs%2C%20such%20as%20text%20prompts%2C%20for%0Abetter%20generation%20quality%20and%20controllability.%20Unlike%20existing%20noise%0Aoptimization%20methods%20designed%20for%20specific%20models%2C%20our%20approach%20is%20grounded%20in%0Aa%20fundamental%20examination%20of%20the%20generic%20finite-step%20noise%20scheduler%20design%20in%0Adiffusion%20formulation%2C%20allowing%20better%20generalization%20across%20different%0Adiffusion-based%20architectures%20in%20a%20tuning-free%20manner.%20This%20model-agnostic%0Anature%20allows%20us%20to%20construct%20a%20reusable%20noise%20library%20compatible%20with%20multiple%0AT2I%20models%20and%20enhancement%20techniques%2C%20serving%20as%20a%20foundational%20layer%20for%20more%0Aeffective%20generation.%20Extensive%20experiments%20demonstrate%20that%20NoiseQuery%20enables%0Afine-grained%20control%20and%20yields%20significant%20performance%20boosts%20not%20only%20over%0Ahigh-level%20semantics%20but%20also%20over%20low-level%20visual%20attributes%2C%20which%20are%0Atypically%20difficult%20to%20specify%20through%20text%20alone%2C%20with%20seamless%20integration%0Ainto%20current%20workflows%20with%20minimal%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Silent%2520Assistant%253A%2520NoiseQuery%2520as%2520Implicit%2520Guidance%2520for%2520Goal-Driven%250A%2520%2520Image%2520Generation%26entry.906535625%3DRuoyu%2520Wang%2520and%2520Huayang%2520Huang%2520and%2520Ye%2520Zhu%2520and%2520Olga%2520Russakovsky%2520and%2520Yu%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520NoiseQuery%2520as%2520a%2520novel%2520method%2520for%2520enhanced%2520noise%250Ainitialization%2520in%2520versatile%2520goal-driven%2520text-to-image%2520%2528T2I%2529%2520generation.%250ASpecifically%252C%2520we%2520propose%2520to%2520leverage%2520an%2520aligned%2520Gaussian%2520noise%2520as%2520implicit%250Aguidance%2520to%2520complement%2520explicit%2520user-defined%2520inputs%252C%2520such%2520as%2520text%2520prompts%252C%2520for%250Abetter%2520generation%2520quality%2520and%2520controllability.%2520Unlike%2520existing%2520noise%250Aoptimization%2520methods%2520designed%2520for%2520specific%2520models%252C%2520our%2520approach%2520is%2520grounded%2520in%250Aa%2520fundamental%2520examination%2520of%2520the%2520generic%2520finite-step%2520noise%2520scheduler%2520design%2520in%250Adiffusion%2520formulation%252C%2520allowing%2520better%2520generalization%2520across%2520different%250Adiffusion-based%2520architectures%2520in%2520a%2520tuning-free%2520manner.%2520This%2520model-agnostic%250Anature%2520allows%2520us%2520to%2520construct%2520a%2520reusable%2520noise%2520library%2520compatible%2520with%2520multiple%250AT2I%2520models%2520and%2520enhancement%2520techniques%252C%2520serving%2520as%2520a%2520foundational%2520layer%2520for%2520more%250Aeffective%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NoiseQuery%2520enables%250Afine-grained%2520control%2520and%2520yields%2520significant%2520performance%2520boosts%2520not%2520only%2520over%250Ahigh-level%2520semantics%2520but%2520also%2520over%2520low-level%2520visual%2520attributes%252C%2520which%2520are%250Atypically%2520difficult%2520to%2520specify%2520through%2520text%2520alone%252C%2520with%2520seamless%2520integration%250Ainto%2520current%2520workflows%2520with%2520minimal%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Silent%20Assistant%3A%20NoiseQuery%20as%20Implicit%20Guidance%20for%20Goal-Driven%0A%20%20Image%20Generation&entry.906535625=Ruoyu%20Wang%20and%20Huayang%20Huang%20and%20Ye%20Zhu%20and%20Olga%20Russakovsky%20and%20Yu%20Wu&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20NoiseQuery%20as%20a%20novel%20method%20for%20enhanced%20noise%0Ainitialization%20in%20versatile%20goal-driven%20text-to-image%20%28T2I%29%20generation.%0ASpecifically%2C%20we%20propose%20to%20leverage%20an%20aligned%20Gaussian%20noise%20as%20implicit%0Aguidance%20to%20complement%20explicit%20user-defined%20inputs%2C%20such%20as%20text%20prompts%2C%20for%0Abetter%20generation%20quality%20and%20controllability.%20Unlike%20existing%20noise%0Aoptimization%20methods%20designed%20for%20specific%20models%2C%20our%20approach%20is%20grounded%20in%0Aa%20fundamental%20examination%20of%20the%20generic%20finite-step%20noise%20scheduler%20design%20in%0Adiffusion%20formulation%2C%20allowing%20better%20generalization%20across%20different%0Adiffusion-based%20architectures%20in%20a%20tuning-free%20manner.%20This%20model-agnostic%0Anature%20allows%20us%20to%20construct%20a%20reusable%20noise%20library%20compatible%20with%20multiple%0AT2I%20models%20and%20enhancement%20techniques%2C%20serving%20as%20a%20foundational%20layer%20for%20more%0Aeffective%20generation.%20Extensive%20experiments%20demonstrate%20that%20NoiseQuery%20enables%0Afine-grained%20control%20and%20yields%20significant%20performance%20boosts%20not%20only%20over%0Ahigh-level%20semantics%20but%20also%20over%20low-level%20visual%20attributes%2C%20which%20are%0Atypically%20difficult%20to%20specify%20through%20text%20alone%2C%20with%20seamless%20integration%0Ainto%20current%20workflows%20with%20minimal%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05101v3&entry.124074799=Read"},
{"title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions\n  in Peptides", "author": "Marlen Neubert and Patrick Reiser and Frauke Gr\u00e4ter and Pascal Friederich", "abstract": "  Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.\n", "link": "http://arxiv.org/abs/2508.00578v1", "date": "2025-08-01", "relevancy": 1.7887, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4662}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4548}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Potential%20Energy%20Surfaces%20of%20Hydrogen%20Atom%20Transfer%20Reactions%0A%20%20in%20Peptides&body=Title%3A%20Learning%20Potential%20Energy%20Surfaces%20of%20Hydrogen%20Atom%20Transfer%20Reactions%0A%20%20in%20Peptides%0AAuthor%3A%20Marlen%20Neubert%20and%20Patrick%20Reiser%20and%20Frauke%20Gr%C3%A4ter%20and%20Pascal%20Friederich%0AAbstract%3A%20%20%20Hydrogen%20atom%20transfer%20%28HAT%29%20reactions%20are%20essential%20in%20many%20biological%0Aprocesses%2C%20such%20as%20radical%20migration%20in%20damaged%20proteins%2C%20but%20their%20mechanistic%0Apathways%20remain%20incompletely%20understood.%20Simulating%20HAT%20is%20challenging%20due%20to%0Athe%20need%20for%20quantum%20chemical%20accuracy%20at%20biologically%20relevant%20scales%3B%20thus%2C%0Aneither%20classical%20force%20fields%20nor%20DFT-based%20molecular%20dynamics%20are%20applicable.%0AMachine-learned%20potentials%20offer%20an%20alternative%2C%20able%20to%20learn%20potential%20energy%0Asurfaces%20%28PESs%29%20with%20near-quantum%20accuracy.%20However%2C%20training%20these%20models%20to%0Ageneralize%20across%20diverse%20HAT%20configurations%2C%20especially%20at%20radical%20positions%0Ain%20proteins%2C%20requires%20tailored%20data%20generation%20and%20careful%20model%20selection.%0AHere%2C%20we%20systematically%20generate%20HAT%20configurations%20in%20peptides%20to%20build%20large%0Adatasets%20using%20semiempirical%20methods%20and%20DFT.%20We%20benchmark%20three%20graph%20neural%0Anetwork%20architectures%20%28SchNet%2C%20Allegro%2C%20and%20MACE%29%20on%20their%20ability%20to%20learn%20HAT%0APESs%20and%20indirectly%20predict%20reaction%20barriers%20from%20energy%20predictions.%20MACE%0Aconsistently%20outperforms%20the%20others%20in%20energy%2C%20force%2C%20and%20barrier%20prediction%2C%0Aachieving%20a%20mean%20absolute%20error%20of%201.13%20kcal/mol%20on%20out-of-distribution%20DFT%0Abarrier%20predictions.%20This%20accuracy%20enables%20integration%20of%20ML%20potentials%20into%0Alarge-scale%20collagen%20simulations%20to%20compute%20reaction%20rates%20from%20predicted%0Abarriers%2C%20advancing%20mechanistic%20understanding%20of%20HAT%20and%20radical%20migration%20in%0Apeptides.%20We%20analyze%20scaling%20laws%2C%20model%20transferability%2C%20and%20cost-performance%0Atrade-offs%2C%20and%20outline%20strategies%20for%20improvement%20by%20combining%20ML%20potentials%0Awith%20transition%20state%20search%20algorithms%20and%20active%20learning.%20Our%20approach%20is%0Ageneralizable%20to%20other%20biomolecular%20systems%2C%20enabling%20quantum-accurate%0Asimulations%20of%20chemical%20reactivity%20in%20complex%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Potential%2520Energy%2520Surfaces%2520of%2520Hydrogen%2520Atom%2520Transfer%2520Reactions%250A%2520%2520in%2520Peptides%26entry.906535625%3DMarlen%2520Neubert%2520and%2520Patrick%2520Reiser%2520and%2520Frauke%2520Gr%25C3%25A4ter%2520and%2520Pascal%2520Friederich%26entry.1292438233%3D%2520%2520Hydrogen%2520atom%2520transfer%2520%2528HAT%2529%2520reactions%2520are%2520essential%2520in%2520many%2520biological%250Aprocesses%252C%2520such%2520as%2520radical%2520migration%2520in%2520damaged%2520proteins%252C%2520but%2520their%2520mechanistic%250Apathways%2520remain%2520incompletely%2520understood.%2520Simulating%2520HAT%2520is%2520challenging%2520due%2520to%250Athe%2520need%2520for%2520quantum%2520chemical%2520accuracy%2520at%2520biologically%2520relevant%2520scales%253B%2520thus%252C%250Aneither%2520classical%2520force%2520fields%2520nor%2520DFT-based%2520molecular%2520dynamics%2520are%2520applicable.%250AMachine-learned%2520potentials%2520offer%2520an%2520alternative%252C%2520able%2520to%2520learn%2520potential%2520energy%250Asurfaces%2520%2528PESs%2529%2520with%2520near-quantum%2520accuracy.%2520However%252C%2520training%2520these%2520models%2520to%250Ageneralize%2520across%2520diverse%2520HAT%2520configurations%252C%2520especially%2520at%2520radical%2520positions%250Ain%2520proteins%252C%2520requires%2520tailored%2520data%2520generation%2520and%2520careful%2520model%2520selection.%250AHere%252C%2520we%2520systematically%2520generate%2520HAT%2520configurations%2520in%2520peptides%2520to%2520build%2520large%250Adatasets%2520using%2520semiempirical%2520methods%2520and%2520DFT.%2520We%2520benchmark%2520three%2520graph%2520neural%250Anetwork%2520architectures%2520%2528SchNet%252C%2520Allegro%252C%2520and%2520MACE%2529%2520on%2520their%2520ability%2520to%2520learn%2520HAT%250APESs%2520and%2520indirectly%2520predict%2520reaction%2520barriers%2520from%2520energy%2520predictions.%2520MACE%250Aconsistently%2520outperforms%2520the%2520others%2520in%2520energy%252C%2520force%252C%2520and%2520barrier%2520prediction%252C%250Aachieving%2520a%2520mean%2520absolute%2520error%2520of%25201.13%2520kcal/mol%2520on%2520out-of-distribution%2520DFT%250Abarrier%2520predictions.%2520This%2520accuracy%2520enables%2520integration%2520of%2520ML%2520potentials%2520into%250Alarge-scale%2520collagen%2520simulations%2520to%2520compute%2520reaction%2520rates%2520from%2520predicted%250Abarriers%252C%2520advancing%2520mechanistic%2520understanding%2520of%2520HAT%2520and%2520radical%2520migration%2520in%250Apeptides.%2520We%2520analyze%2520scaling%2520laws%252C%2520model%2520transferability%252C%2520and%2520cost-performance%250Atrade-offs%252C%2520and%2520outline%2520strategies%2520for%2520improvement%2520by%2520combining%2520ML%2520potentials%250Awith%2520transition%2520state%2520search%2520algorithms%2520and%2520active%2520learning.%2520Our%2520approach%2520is%250Ageneralizable%2520to%2520other%2520biomolecular%2520systems%252C%2520enabling%2520quantum-accurate%250Asimulations%2520of%2520chemical%2520reactivity%2520in%2520complex%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Potential%20Energy%20Surfaces%20of%20Hydrogen%20Atom%20Transfer%20Reactions%0A%20%20in%20Peptides&entry.906535625=Marlen%20Neubert%20and%20Patrick%20Reiser%20and%20Frauke%20Gr%C3%A4ter%20and%20Pascal%20Friederich&entry.1292438233=%20%20Hydrogen%20atom%20transfer%20%28HAT%29%20reactions%20are%20essential%20in%20many%20biological%0Aprocesses%2C%20such%20as%20radical%20migration%20in%20damaged%20proteins%2C%20but%20their%20mechanistic%0Apathways%20remain%20incompletely%20understood.%20Simulating%20HAT%20is%20challenging%20due%20to%0Athe%20need%20for%20quantum%20chemical%20accuracy%20at%20biologically%20relevant%20scales%3B%20thus%2C%0Aneither%20classical%20force%20fields%20nor%20DFT-based%20molecular%20dynamics%20are%20applicable.%0AMachine-learned%20potentials%20offer%20an%20alternative%2C%20able%20to%20learn%20potential%20energy%0Asurfaces%20%28PESs%29%20with%20near-quantum%20accuracy.%20However%2C%20training%20these%20models%20to%0Ageneralize%20across%20diverse%20HAT%20configurations%2C%20especially%20at%20radical%20positions%0Ain%20proteins%2C%20requires%20tailored%20data%20generation%20and%20careful%20model%20selection.%0AHere%2C%20we%20systematically%20generate%20HAT%20configurations%20in%20peptides%20to%20build%20large%0Adatasets%20using%20semiempirical%20methods%20and%20DFT.%20We%20benchmark%20three%20graph%20neural%0Anetwork%20architectures%20%28SchNet%2C%20Allegro%2C%20and%20MACE%29%20on%20their%20ability%20to%20learn%20HAT%0APESs%20and%20indirectly%20predict%20reaction%20barriers%20from%20energy%20predictions.%20MACE%0Aconsistently%20outperforms%20the%20others%20in%20energy%2C%20force%2C%20and%20barrier%20prediction%2C%0Aachieving%20a%20mean%20absolute%20error%20of%201.13%20kcal/mol%20on%20out-of-distribution%20DFT%0Abarrier%20predictions.%20This%20accuracy%20enables%20integration%20of%20ML%20potentials%20into%0Alarge-scale%20collagen%20simulations%20to%20compute%20reaction%20rates%20from%20predicted%0Abarriers%2C%20advancing%20mechanistic%20understanding%20of%20HAT%20and%20radical%20migration%20in%0Apeptides.%20We%20analyze%20scaling%20laws%2C%20model%20transferability%2C%20and%20cost-performance%0Atrade-offs%2C%20and%20outline%20strategies%20for%20improvement%20by%20combining%20ML%20potentials%0Awith%20transition%20state%20search%20algorithms%20and%20active%20learning.%20Our%20approach%20is%0Ageneralizable%20to%20other%20biomolecular%20systems%2C%20enabling%20quantum-accurate%0Asimulations%20of%20chemical%20reactivity%20in%20complex%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00578v1&entry.124074799=Read"},
{"title": "Learning Goal-Directed Object Pushing in Cluttered Scenes With\n  Location-Based Attention", "author": "Nils Dengler and Juan Del Aguila Ferrandis and Jo\u00e3o Moura and Sethu Vijayakumar and Maren Bennewitz", "abstract": "  In complex scenarios where typical pick-and-place techniques are\ninsufficient, often non-prehensile manipulation can ensure that a robot is able\nto fulfill its task. However, non-prehensile manipulation is challenging due to\nits underactuated nature with hybrid-dynamics, where a robot needs to reason\nabout an object's long-term behavior and contact-switching, while being robust\nto contact uncertainty. The presence of clutter in the workspace further\ncomplicates this task, introducing the need to include more advanced spatial\nanalysis to avoid unwanted collisions. Building upon prior work on\nreinforcement learning with multimodal categorical exploration for planar\npushing, we propose to incorporate location-based attention to enable robust\nmanipulation in cluttered scenes. Unlike previous approaches addressing this\nobstacle avoiding pushing task, our framework requires no predefined global\npaths and considers the desired target orientation of the manipulated object.\nExperimental results in simulation as well as with a real KUKA iiwa robot arm\ndemonstrate that our learned policy manipulates objects successfully while\navoiding collisions through complex obstacle configurations, including dynamic\nobstacles, to reach the desired target pose.\n", "link": "http://arxiv.org/abs/2403.17667v3", "date": "2025-08-01", "relevancy": 1.7871, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6054}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Goal-Directed%20Object%20Pushing%20in%20Cluttered%20Scenes%20With%0A%20%20Location-Based%20Attention&body=Title%3A%20Learning%20Goal-Directed%20Object%20Pushing%20in%20Cluttered%20Scenes%20With%0A%20%20Location-Based%20Attention%0AAuthor%3A%20Nils%20Dengler%20and%20Juan%20Del%20Aguila%20Ferrandis%20and%20Jo%C3%A3o%20Moura%20and%20Sethu%20Vijayakumar%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20In%20complex%20scenarios%20where%20typical%20pick-and-place%20techniques%20are%0Ainsufficient%2C%20often%20non-prehensile%20manipulation%20can%20ensure%20that%20a%20robot%20is%20able%0Ato%20fulfill%20its%20task.%20However%2C%20non-prehensile%20manipulation%20is%20challenging%20due%20to%0Aits%20underactuated%20nature%20with%20hybrid-dynamics%2C%20where%20a%20robot%20needs%20to%20reason%0Aabout%20an%20object%27s%20long-term%20behavior%20and%20contact-switching%2C%20while%20being%20robust%0Ato%20contact%20uncertainty.%20The%20presence%20of%20clutter%20in%20the%20workspace%20further%0Acomplicates%20this%20task%2C%20introducing%20the%20need%20to%20include%20more%20advanced%20spatial%0Aanalysis%20to%20avoid%20unwanted%20collisions.%20Building%20upon%20prior%20work%20on%0Areinforcement%20learning%20with%20multimodal%20categorical%20exploration%20for%20planar%0Apushing%2C%20we%20propose%20to%20incorporate%20location-based%20attention%20to%20enable%20robust%0Amanipulation%20in%20cluttered%20scenes.%20Unlike%20previous%20approaches%20addressing%20this%0Aobstacle%20avoiding%20pushing%20task%2C%20our%20framework%20requires%20no%20predefined%20global%0Apaths%20and%20considers%20the%20desired%20target%20orientation%20of%20the%20manipulated%20object.%0AExperimental%20results%20in%20simulation%20as%20well%20as%20with%20a%20real%20KUKA%20iiwa%20robot%20arm%0Ademonstrate%20that%20our%20learned%20policy%20manipulates%20objects%20successfully%20while%0Aavoiding%20collisions%20through%20complex%20obstacle%20configurations%2C%20including%20dynamic%0Aobstacles%2C%20to%20reach%20the%20desired%20target%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17667v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Goal-Directed%2520Object%2520Pushing%2520in%2520Cluttered%2520Scenes%2520With%250A%2520%2520Location-Based%2520Attention%26entry.906535625%3DNils%2520Dengler%2520and%2520Juan%2520Del%2520Aguila%2520Ferrandis%2520and%2520Jo%25C3%25A3o%2520Moura%2520and%2520Sethu%2520Vijayakumar%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520In%2520complex%2520scenarios%2520where%2520typical%2520pick-and-place%2520techniques%2520are%250Ainsufficient%252C%2520often%2520non-prehensile%2520manipulation%2520can%2520ensure%2520that%2520a%2520robot%2520is%2520able%250Ato%2520fulfill%2520its%2520task.%2520However%252C%2520non-prehensile%2520manipulation%2520is%2520challenging%2520due%2520to%250Aits%2520underactuated%2520nature%2520with%2520hybrid-dynamics%252C%2520where%2520a%2520robot%2520needs%2520to%2520reason%250Aabout%2520an%2520object%2527s%2520long-term%2520behavior%2520and%2520contact-switching%252C%2520while%2520being%2520robust%250Ato%2520contact%2520uncertainty.%2520The%2520presence%2520of%2520clutter%2520in%2520the%2520workspace%2520further%250Acomplicates%2520this%2520task%252C%2520introducing%2520the%2520need%2520to%2520include%2520more%2520advanced%2520spatial%250Aanalysis%2520to%2520avoid%2520unwanted%2520collisions.%2520Building%2520upon%2520prior%2520work%2520on%250Areinforcement%2520learning%2520with%2520multimodal%2520categorical%2520exploration%2520for%2520planar%250Apushing%252C%2520we%2520propose%2520to%2520incorporate%2520location-based%2520attention%2520to%2520enable%2520robust%250Amanipulation%2520in%2520cluttered%2520scenes.%2520Unlike%2520previous%2520approaches%2520addressing%2520this%250Aobstacle%2520avoiding%2520pushing%2520task%252C%2520our%2520framework%2520requires%2520no%2520predefined%2520global%250Apaths%2520and%2520considers%2520the%2520desired%2520target%2520orientation%2520of%2520the%2520manipulated%2520object.%250AExperimental%2520results%2520in%2520simulation%2520as%2520well%2520as%2520with%2520a%2520real%2520KUKA%2520iiwa%2520robot%2520arm%250Ademonstrate%2520that%2520our%2520learned%2520policy%2520manipulates%2520objects%2520successfully%2520while%250Aavoiding%2520collisions%2520through%2520complex%2520obstacle%2520configurations%252C%2520including%2520dynamic%250Aobstacles%252C%2520to%2520reach%2520the%2520desired%2520target%2520pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17667v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Goal-Directed%20Object%20Pushing%20in%20Cluttered%20Scenes%20With%0A%20%20Location-Based%20Attention&entry.906535625=Nils%20Dengler%20and%20Juan%20Del%20Aguila%20Ferrandis%20and%20Jo%C3%A3o%20Moura%20and%20Sethu%20Vijayakumar%20and%20Maren%20Bennewitz&entry.1292438233=%20%20In%20complex%20scenarios%20where%20typical%20pick-and-place%20techniques%20are%0Ainsufficient%2C%20often%20non-prehensile%20manipulation%20can%20ensure%20that%20a%20robot%20is%20able%0Ato%20fulfill%20its%20task.%20However%2C%20non-prehensile%20manipulation%20is%20challenging%20due%20to%0Aits%20underactuated%20nature%20with%20hybrid-dynamics%2C%20where%20a%20robot%20needs%20to%20reason%0Aabout%20an%20object%27s%20long-term%20behavior%20and%20contact-switching%2C%20while%20being%20robust%0Ato%20contact%20uncertainty.%20The%20presence%20of%20clutter%20in%20the%20workspace%20further%0Acomplicates%20this%20task%2C%20introducing%20the%20need%20to%20include%20more%20advanced%20spatial%0Aanalysis%20to%20avoid%20unwanted%20collisions.%20Building%20upon%20prior%20work%20on%0Areinforcement%20learning%20with%20multimodal%20categorical%20exploration%20for%20planar%0Apushing%2C%20we%20propose%20to%20incorporate%20location-based%20attention%20to%20enable%20robust%0Amanipulation%20in%20cluttered%20scenes.%20Unlike%20previous%20approaches%20addressing%20this%0Aobstacle%20avoiding%20pushing%20task%2C%20our%20framework%20requires%20no%20predefined%20global%0Apaths%20and%20considers%20the%20desired%20target%20orientation%20of%20the%20manipulated%20object.%0AExperimental%20results%20in%20simulation%20as%20well%20as%20with%20a%20real%20KUKA%20iiwa%20robot%20arm%0Ademonstrate%20that%20our%20learned%20policy%20manipulates%20objects%20successfully%20while%0Aavoiding%20collisions%20through%20complex%20obstacle%20configurations%2C%20including%20dynamic%0Aobstacles%2C%20to%20reach%20the%20desired%20target%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17667v3&entry.124074799=Read"},
{"title": "UTrace: Poisoning Forensics for Private Collaborative Learning", "author": "Evan Rose and Hidde Lycklama and Harsh Chaudhari and Anwar Hithnawi and Alina Oprea", "abstract": "  Privacy-preserving machine learning (PPML) enables multiple data owners to\ncontribute their data privately to a set of servers that run a secure\nmulti-party computation (MPC) protocol to train a joint ML model. In these\nprotocols, the input data remains private throughout the training process, and\nonly the resulting model is made available. While this approach benefits\nprivacy, it also exacerbates the risks of data poisoning, where compromised\ndata owners induce undesirable model behavior by contributing malicious\ndatasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but\nthese measures are not exhaustive. To complement existing poisoning defenses,\nwe introduce UTrace: a framework for User-level Traceback of poisoning attacks\nin PPML. Utrace computes user responsibility scores using gradient similarity\nmetrics aggregated across the most relevant samples in an owner's dataset.\nUTrace is effective at low poisoning rates and is resilient to poisoning\nattacks distributed across multiple data owners, unlike existing\nunlearning-based methods. We introduce methods for checkpointing gradients with\nlow storage overhead, enabling traceback in the absence of data owners at\ndeployment time. We also design several optimizations that reduce traceback\ntime and communication in MPC. We provide a comprehensive evaluation of UTrace\nacross four datasets from three data modalities (vision, text, and malware) and\nshow its effectiveness against 10 poisoning attacks.\n", "link": "http://arxiv.org/abs/2409.15126v2", "date": "2025-08-01", "relevancy": 1.7699, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UTrace%3A%20Poisoning%20Forensics%20for%20Private%20Collaborative%20Learning&body=Title%3A%20UTrace%3A%20Poisoning%20Forensics%20for%20Private%20Collaborative%20Learning%0AAuthor%3A%20Evan%20Rose%20and%20Hidde%20Lycklama%20and%20Harsh%20Chaudhari%20and%20Anwar%20Hithnawi%20and%20Alina%20Oprea%0AAbstract%3A%20%20%20Privacy-preserving%20machine%20learning%20%28PPML%29%20enables%20multiple%20data%20owners%20to%0Acontribute%20their%20data%20privately%20to%20a%20set%20of%20servers%20that%20run%20a%20secure%0Amulti-party%20computation%20%28MPC%29%20protocol%20to%20train%20a%20joint%20ML%20model.%20In%20these%0Aprotocols%2C%20the%20input%20data%20remains%20private%20throughout%20the%20training%20process%2C%20and%0Aonly%20the%20resulting%20model%20is%20made%20available.%20While%20this%20approach%20benefits%0Aprivacy%2C%20it%20also%20exacerbates%20the%20risks%20of%20data%20poisoning%2C%20where%20compromised%0Adata%20owners%20induce%20undesirable%20model%20behavior%20by%20contributing%20malicious%0Adatasets.%20Existing%20MPC%20mechanisms%20can%20mitigate%20certain%20poisoning%20attacks%2C%20but%0Athese%20measures%20are%20not%20exhaustive.%20To%20complement%20existing%20poisoning%20defenses%2C%0Awe%20introduce%20UTrace%3A%20a%20framework%20for%20User-level%20Traceback%20of%20poisoning%20attacks%0Ain%20PPML.%20Utrace%20computes%20user%20responsibility%20scores%20using%20gradient%20similarity%0Ametrics%20aggregated%20across%20the%20most%20relevant%20samples%20in%20an%20owner%27s%20dataset.%0AUTrace%20is%20effective%20at%20low%20poisoning%20rates%20and%20is%20resilient%20to%20poisoning%0Aattacks%20distributed%20across%20multiple%20data%20owners%2C%20unlike%20existing%0Aunlearning-based%20methods.%20We%20introduce%20methods%20for%20checkpointing%20gradients%20with%0Alow%20storage%20overhead%2C%20enabling%20traceback%20in%20the%20absence%20of%20data%20owners%20at%0Adeployment%20time.%20We%20also%20design%20several%20optimizations%20that%20reduce%20traceback%0Atime%20and%20communication%20in%20MPC.%20We%20provide%20a%20comprehensive%20evaluation%20of%20UTrace%0Aacross%20four%20datasets%20from%20three%20data%20modalities%20%28vision%2C%20text%2C%20and%20malware%29%20and%0Ashow%20its%20effectiveness%20against%2010%20poisoning%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUTrace%253A%2520Poisoning%2520Forensics%2520for%2520Private%2520Collaborative%2520Learning%26entry.906535625%3DEvan%2520Rose%2520and%2520Hidde%2520Lycklama%2520and%2520Harsh%2520Chaudhari%2520and%2520Anwar%2520Hithnawi%2520and%2520Alina%2520Oprea%26entry.1292438233%3D%2520%2520Privacy-preserving%2520machine%2520learning%2520%2528PPML%2529%2520enables%2520multiple%2520data%2520owners%2520to%250Acontribute%2520their%2520data%2520privately%2520to%2520a%2520set%2520of%2520servers%2520that%2520run%2520a%2520secure%250Amulti-party%2520computation%2520%2528MPC%2529%2520protocol%2520to%2520train%2520a%2520joint%2520ML%2520model.%2520In%2520these%250Aprotocols%252C%2520the%2520input%2520data%2520remains%2520private%2520throughout%2520the%2520training%2520process%252C%2520and%250Aonly%2520the%2520resulting%2520model%2520is%2520made%2520available.%2520While%2520this%2520approach%2520benefits%250Aprivacy%252C%2520it%2520also%2520exacerbates%2520the%2520risks%2520of%2520data%2520poisoning%252C%2520where%2520compromised%250Adata%2520owners%2520induce%2520undesirable%2520model%2520behavior%2520by%2520contributing%2520malicious%250Adatasets.%2520Existing%2520MPC%2520mechanisms%2520can%2520mitigate%2520certain%2520poisoning%2520attacks%252C%2520but%250Athese%2520measures%2520are%2520not%2520exhaustive.%2520To%2520complement%2520existing%2520poisoning%2520defenses%252C%250Awe%2520introduce%2520UTrace%253A%2520a%2520framework%2520for%2520User-level%2520Traceback%2520of%2520poisoning%2520attacks%250Ain%2520PPML.%2520Utrace%2520computes%2520user%2520responsibility%2520scores%2520using%2520gradient%2520similarity%250Ametrics%2520aggregated%2520across%2520the%2520most%2520relevant%2520samples%2520in%2520an%2520owner%2527s%2520dataset.%250AUTrace%2520is%2520effective%2520at%2520low%2520poisoning%2520rates%2520and%2520is%2520resilient%2520to%2520poisoning%250Aattacks%2520distributed%2520across%2520multiple%2520data%2520owners%252C%2520unlike%2520existing%250Aunlearning-based%2520methods.%2520We%2520introduce%2520methods%2520for%2520checkpointing%2520gradients%2520with%250Alow%2520storage%2520overhead%252C%2520enabling%2520traceback%2520in%2520the%2520absence%2520of%2520data%2520owners%2520at%250Adeployment%2520time.%2520We%2520also%2520design%2520several%2520optimizations%2520that%2520reduce%2520traceback%250Atime%2520and%2520communication%2520in%2520MPC.%2520We%2520provide%2520a%2520comprehensive%2520evaluation%2520of%2520UTrace%250Aacross%2520four%2520datasets%2520from%2520three%2520data%2520modalities%2520%2528vision%252C%2520text%252C%2520and%2520malware%2529%2520and%250Ashow%2520its%2520effectiveness%2520against%252010%2520poisoning%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UTrace%3A%20Poisoning%20Forensics%20for%20Private%20Collaborative%20Learning&entry.906535625=Evan%20Rose%20and%20Hidde%20Lycklama%20and%20Harsh%20Chaudhari%20and%20Anwar%20Hithnawi%20and%20Alina%20Oprea&entry.1292438233=%20%20Privacy-preserving%20machine%20learning%20%28PPML%29%20enables%20multiple%20data%20owners%20to%0Acontribute%20their%20data%20privately%20to%20a%20set%20of%20servers%20that%20run%20a%20secure%0Amulti-party%20computation%20%28MPC%29%20protocol%20to%20train%20a%20joint%20ML%20model.%20In%20these%0Aprotocols%2C%20the%20input%20data%20remains%20private%20throughout%20the%20training%20process%2C%20and%0Aonly%20the%20resulting%20model%20is%20made%20available.%20While%20this%20approach%20benefits%0Aprivacy%2C%20it%20also%20exacerbates%20the%20risks%20of%20data%20poisoning%2C%20where%20compromised%0Adata%20owners%20induce%20undesirable%20model%20behavior%20by%20contributing%20malicious%0Adatasets.%20Existing%20MPC%20mechanisms%20can%20mitigate%20certain%20poisoning%20attacks%2C%20but%0Athese%20measures%20are%20not%20exhaustive.%20To%20complement%20existing%20poisoning%20defenses%2C%0Awe%20introduce%20UTrace%3A%20a%20framework%20for%20User-level%20Traceback%20of%20poisoning%20attacks%0Ain%20PPML.%20Utrace%20computes%20user%20responsibility%20scores%20using%20gradient%20similarity%0Ametrics%20aggregated%20across%20the%20most%20relevant%20samples%20in%20an%20owner%27s%20dataset.%0AUTrace%20is%20effective%20at%20low%20poisoning%20rates%20and%20is%20resilient%20to%20poisoning%0Aattacks%20distributed%20across%20multiple%20data%20owners%2C%20unlike%20existing%0Aunlearning-based%20methods.%20We%20introduce%20methods%20for%20checkpointing%20gradients%20with%0Alow%20storage%20overhead%2C%20enabling%20traceback%20in%20the%20absence%20of%20data%20owners%20at%0Adeployment%20time.%20We%20also%20design%20several%20optimizations%20that%20reduce%20traceback%0Atime%20and%20communication%20in%20MPC.%20We%20provide%20a%20comprehensive%20evaluation%20of%20UTrace%0Aacross%20four%20datasets%20from%20three%20data%20modalities%20%28vision%2C%20text%2C%20and%20malware%29%20and%0Ashow%20its%20effectiveness%20against%2010%20poisoning%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15126v2&entry.124074799=Read"},
{"title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal\n  Interactions in Multimodal AI Models", "author": "Zhanliang Wang and Kai Wang", "abstract": "  Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.\n", "link": "http://arxiv.org/abs/2508.00576v1", "date": "2025-08-01", "relevancy": 1.7689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5684}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiSHAP%3A%20A%20Shapley-Based%20Framework%20for%20Explaining%20Cross-Modal%0A%20%20Interactions%20in%20Multimodal%20AI%20Models&body=Title%3A%20MultiSHAP%3A%20A%20Shapley-Based%20Framework%20for%20Explaining%20Cross-Modal%0A%20%20Interactions%20in%20Multimodal%20AI%20Models%0AAuthor%3A%20Zhanliang%20Wang%20and%20Kai%20Wang%0AAbstract%3A%20%20%20Multimodal%20AI%20models%20have%20achieved%20impressive%20performance%20in%20tasks%20that%0Arequire%20integrating%20information%20from%20multiple%20modalities%2C%20such%20as%20vision%20and%0Alanguage.%20However%2C%20their%20%22black-box%22%20nature%20poses%20a%20major%20barrier%20to%20deployment%0Ain%20high-stakes%20applications%20where%20interpretability%20and%20trustworthiness%20are%0Aessential.%20How%20to%20explain%20cross-modal%20interactions%20in%20multimodal%20AI%20models%0Aremains%20a%20major%20challenge.%20While%20existing%20model%20explanation%20methods%2C%20such%20as%0Aattention%20map%20and%20Grad-CAM%2C%20offer%20coarse%20insights%20into%20cross-modal%0Arelationships%2C%20they%20cannot%20precisely%20quantify%20the%20synergistic%20effects%20between%0Amodalities%2C%20and%20are%20limited%20to%20open-source%20models%20with%20accessible%20internal%0Aweights.%20Here%20we%20introduce%20MultiSHAP%2C%20a%20model-agnostic%20interpretability%0Aframework%20that%20leverages%20the%20Shapley%20Interaction%20Index%20to%20attribute%20multimodal%0Apredictions%20to%20pairwise%20interactions%20between%20fine-grained%20visual%20and%20textual%0Aelements%20%28such%20as%20image%20patches%20and%20text%20tokens%29%2C%20while%20being%20applicable%20to%0Aboth%20open-%20and%20closed-source%20models.%20Our%20approach%20provides%3A%20%281%29%20instance-level%0Aexplanations%20that%20reveal%20synergistic%20and%20suppressive%20cross-modal%20effects%20for%0Aindividual%20samples%20-%20%22why%20the%20model%20makes%20a%20specific%20prediction%20on%20this%20input%22%2C%0Aand%20%282%29%20dataset-level%20explanation%20that%20uncovers%20generalizable%20interaction%0Apatterns%20across%20samples%20-%20%22how%20the%20model%20integrates%20information%20across%0Amodalities%22.%20Experiments%20on%20public%20multimodal%20benchmarks%20confirm%20that%20MultiSHAP%0Afaithfully%20captures%20cross-modal%20reasoning%20mechanisms%2C%20while%20real-world%20case%0Astudies%20demonstrate%20its%20practical%20utility.%20Our%20framework%20is%20extensible%20beyond%0Atwo%20modalities%2C%20offering%20a%20general%20solution%20for%20interpreting%20complex%20multimodal%0AAI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiSHAP%253A%2520A%2520Shapley-Based%2520Framework%2520for%2520Explaining%2520Cross-Modal%250A%2520%2520Interactions%2520in%2520Multimodal%2520AI%2520Models%26entry.906535625%3DZhanliang%2520Wang%2520and%2520Kai%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520AI%2520models%2520have%2520achieved%2520impressive%2520performance%2520in%2520tasks%2520that%250Arequire%2520integrating%2520information%2520from%2520multiple%2520modalities%252C%2520such%2520as%2520vision%2520and%250Alanguage.%2520However%252C%2520their%2520%2522black-box%2522%2520nature%2520poses%2520a%2520major%2520barrier%2520to%2520deployment%250Ain%2520high-stakes%2520applications%2520where%2520interpretability%2520and%2520trustworthiness%2520are%250Aessential.%2520How%2520to%2520explain%2520cross-modal%2520interactions%2520in%2520multimodal%2520AI%2520models%250Aremains%2520a%2520major%2520challenge.%2520While%2520existing%2520model%2520explanation%2520methods%252C%2520such%2520as%250Aattention%2520map%2520and%2520Grad-CAM%252C%2520offer%2520coarse%2520insights%2520into%2520cross-modal%250Arelationships%252C%2520they%2520cannot%2520precisely%2520quantify%2520the%2520synergistic%2520effects%2520between%250Amodalities%252C%2520and%2520are%2520limited%2520to%2520open-source%2520models%2520with%2520accessible%2520internal%250Aweights.%2520Here%2520we%2520introduce%2520MultiSHAP%252C%2520a%2520model-agnostic%2520interpretability%250Aframework%2520that%2520leverages%2520the%2520Shapley%2520Interaction%2520Index%2520to%2520attribute%2520multimodal%250Apredictions%2520to%2520pairwise%2520interactions%2520between%2520fine-grained%2520visual%2520and%2520textual%250Aelements%2520%2528such%2520as%2520image%2520patches%2520and%2520text%2520tokens%2529%252C%2520while%2520being%2520applicable%2520to%250Aboth%2520open-%2520and%2520closed-source%2520models.%2520Our%2520approach%2520provides%253A%2520%25281%2529%2520instance-level%250Aexplanations%2520that%2520reveal%2520synergistic%2520and%2520suppressive%2520cross-modal%2520effects%2520for%250Aindividual%2520samples%2520-%2520%2522why%2520the%2520model%2520makes%2520a%2520specific%2520prediction%2520on%2520this%2520input%2522%252C%250Aand%2520%25282%2529%2520dataset-level%2520explanation%2520that%2520uncovers%2520generalizable%2520interaction%250Apatterns%2520across%2520samples%2520-%2520%2522how%2520the%2520model%2520integrates%2520information%2520across%250Amodalities%2522.%2520Experiments%2520on%2520public%2520multimodal%2520benchmarks%2520confirm%2520that%2520MultiSHAP%250Afaithfully%2520captures%2520cross-modal%2520reasoning%2520mechanisms%252C%2520while%2520real-world%2520case%250Astudies%2520demonstrate%2520its%2520practical%2520utility.%2520Our%2520framework%2520is%2520extensible%2520beyond%250Atwo%2520modalities%252C%2520offering%2520a%2520general%2520solution%2520for%2520interpreting%2520complex%2520multimodal%250AAI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiSHAP%3A%20A%20Shapley-Based%20Framework%20for%20Explaining%20Cross-Modal%0A%20%20Interactions%20in%20Multimodal%20AI%20Models&entry.906535625=Zhanliang%20Wang%20and%20Kai%20Wang&entry.1292438233=%20%20Multimodal%20AI%20models%20have%20achieved%20impressive%20performance%20in%20tasks%20that%0Arequire%20integrating%20information%20from%20multiple%20modalities%2C%20such%20as%20vision%20and%0Alanguage.%20However%2C%20their%20%22black-box%22%20nature%20poses%20a%20major%20barrier%20to%20deployment%0Ain%20high-stakes%20applications%20where%20interpretability%20and%20trustworthiness%20are%0Aessential.%20How%20to%20explain%20cross-modal%20interactions%20in%20multimodal%20AI%20models%0Aremains%20a%20major%20challenge.%20While%20existing%20model%20explanation%20methods%2C%20such%20as%0Aattention%20map%20and%20Grad-CAM%2C%20offer%20coarse%20insights%20into%20cross-modal%0Arelationships%2C%20they%20cannot%20precisely%20quantify%20the%20synergistic%20effects%20between%0Amodalities%2C%20and%20are%20limited%20to%20open-source%20models%20with%20accessible%20internal%0Aweights.%20Here%20we%20introduce%20MultiSHAP%2C%20a%20model-agnostic%20interpretability%0Aframework%20that%20leverages%20the%20Shapley%20Interaction%20Index%20to%20attribute%20multimodal%0Apredictions%20to%20pairwise%20interactions%20between%20fine-grained%20visual%20and%20textual%0Aelements%20%28such%20as%20image%20patches%20and%20text%20tokens%29%2C%20while%20being%20applicable%20to%0Aboth%20open-%20and%20closed-source%20models.%20Our%20approach%20provides%3A%20%281%29%20instance-level%0Aexplanations%20that%20reveal%20synergistic%20and%20suppressive%20cross-modal%20effects%20for%0Aindividual%20samples%20-%20%22why%20the%20model%20makes%20a%20specific%20prediction%20on%20this%20input%22%2C%0Aand%20%282%29%20dataset-level%20explanation%20that%20uncovers%20generalizable%20interaction%0Apatterns%20across%20samples%20-%20%22how%20the%20model%20integrates%20information%20across%0Amodalities%22.%20Experiments%20on%20public%20multimodal%20benchmarks%20confirm%20that%20MultiSHAP%0Afaithfully%20captures%20cross-modal%20reasoning%20mechanisms%2C%20while%20real-world%20case%0Astudies%20demonstrate%20its%20practical%20utility.%20Our%20framework%20is%20extensible%20beyond%0Atwo%20modalities%2C%20offering%20a%20general%20solution%20for%20interpreting%20complex%20multimodal%0AAI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00576v1&entry.124074799=Read"},
{"title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of\n  State-of-the-Art Generative Models for OCR", "author": "Peirong Zhang and Haowei Xu and Jiaxin Zhang and Guitao Xu and Xuhan Zheng and Zhenhua Yang and Junle Liu and Yuyi Zhang and Lianwen Jin", "abstract": "  Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.\n", "link": "http://arxiv.org/abs/2507.15085v2", "date": "2025-08-01", "relevancy": 1.7657, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6002}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5981}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aesthetics%20is%20Cheap%2C%20Show%20me%20the%20Text%3A%20An%20Empirical%20Evaluation%20of%0A%20%20State-of-the-Art%20Generative%20Models%20for%20OCR&body=Title%3A%20Aesthetics%20is%20Cheap%2C%20Show%20me%20the%20Text%3A%20An%20Empirical%20Evaluation%20of%0A%20%20State-of-the-Art%20Generative%20Models%20for%20OCR%0AAuthor%3A%20Peirong%20Zhang%20and%20Haowei%20Xu%20and%20Jiaxin%20Zhang%20and%20Guitao%20Xu%20and%20Xuhan%20Zheng%20and%20Zhenhua%20Yang%20and%20Junle%20Liu%20and%20Yuyi%20Zhang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Text%20image%20is%20a%20unique%20and%20crucial%20information%20medium%20that%20integrates%20visual%0Aaesthetics%20and%20linguistic%20semantics%20in%20modern%20e-society.%20Due%20to%20their%20subtlety%0Aand%20complexity%2C%20the%20generation%20of%20text%20images%20represents%20a%20challenging%20and%0Aevolving%20frontier%20in%20the%20image%20generation%20field.%20The%20recent%20surge%20of%0Aspecialized%20image%20generators%20%28%5Cemph%7Be.g.%7D%2C%20Flux-series%29%20and%20unified%20generative%0Amodels%20%28%5Cemph%7Be.g.%7D%2C%20GPT-4o%29%2C%20which%20demonstrate%20exceptional%20fidelity%2C%20raises%20a%0Anatural%20question%3A%20can%20they%20master%20the%20intricacies%20of%20text%20image%20generation%20and%0Aediting%3F%20Motivated%20by%20this%2C%20we%20assess%20current%20state-of-the-art%20generative%0Amodels%27%20capabilities%20in%20terms%20of%20text%20image%20generation%20and%20editing.%20We%0Aincorporate%20various%20typical%20optical%20character%20recognition%20%28OCR%29%20tasks%20into%20our%0Aevaluation%20and%20broaden%20the%20concept%20of%20text-based%20generation%20tasks%20into%20OCR%0Agenerative%20tasks.%20We%20select%2033%20representative%20tasks%20and%20categorize%20them%20into%0Afive%20categories%3A%20document%2C%20handwritten%20text%2C%20scene%20text%2C%20artistic%20text%2C%20and%0Acomplex%20%5C%26%20layout-rich%20text.%20For%20comprehensive%20evaluation%2C%20we%20examine%20six%0Amodels%20across%20both%20closed-source%20and%20open-source%20domains%2C%20using%20tailored%2C%0Ahigh-quality%20image%20inputs%20and%20prompts.%20Through%20this%20evaluation%2C%20we%20draw%20crucial%0Aobservations%20and%20identify%20the%20weaknesses%20of%20current%20generative%20models%20for%20OCR%0Atasks.%20We%20argue%20that%20photorealistic%20text%20image%20generation%20and%20editing%20should%20be%0Ainternalized%20as%20foundational%20skills%20into%20general-domain%20generative%20models%2C%0Arather%20than%20being%20delegated%20to%20specialized%20solutions%2C%20and%20we%20hope%20this%0Aempirical%20analysis%20can%20provide%20valuable%20insights%20for%20the%20community%20to%20achieve%0Athis%20goal.%20This%20evaluation%20is%20online%20and%20will%20be%20continuously%20updated%20at%20our%0AGitHub%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAesthetics%2520is%2520Cheap%252C%2520Show%2520me%2520the%2520Text%253A%2520An%2520Empirical%2520Evaluation%2520of%250A%2520%2520State-of-the-Art%2520Generative%2520Models%2520for%2520OCR%26entry.906535625%3DPeirong%2520Zhang%2520and%2520Haowei%2520Xu%2520and%2520Jiaxin%2520Zhang%2520and%2520Guitao%2520Xu%2520and%2520Xuhan%2520Zheng%2520and%2520Zhenhua%2520Yang%2520and%2520Junle%2520Liu%2520and%2520Yuyi%2520Zhang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Text%2520image%2520is%2520a%2520unique%2520and%2520crucial%2520information%2520medium%2520that%2520integrates%2520visual%250Aaesthetics%2520and%2520linguistic%2520semantics%2520in%2520modern%2520e-society.%2520Due%2520to%2520their%2520subtlety%250Aand%2520complexity%252C%2520the%2520generation%2520of%2520text%2520images%2520represents%2520a%2520challenging%2520and%250Aevolving%2520frontier%2520in%2520the%2520image%2520generation%2520field.%2520The%2520recent%2520surge%2520of%250Aspecialized%2520image%2520generators%2520%2528%255Cemph%257Be.g.%257D%252C%2520Flux-series%2529%2520and%2520unified%2520generative%250Amodels%2520%2528%255Cemph%257Be.g.%257D%252C%2520GPT-4o%2529%252C%2520which%2520demonstrate%2520exceptional%2520fidelity%252C%2520raises%2520a%250Anatural%2520question%253A%2520can%2520they%2520master%2520the%2520intricacies%2520of%2520text%2520image%2520generation%2520and%250Aediting%253F%2520Motivated%2520by%2520this%252C%2520we%2520assess%2520current%2520state-of-the-art%2520generative%250Amodels%2527%2520capabilities%2520in%2520terms%2520of%2520text%2520image%2520generation%2520and%2520editing.%2520We%250Aincorporate%2520various%2520typical%2520optical%2520character%2520recognition%2520%2528OCR%2529%2520tasks%2520into%2520our%250Aevaluation%2520and%2520broaden%2520the%2520concept%2520of%2520text-based%2520generation%2520tasks%2520into%2520OCR%250Agenerative%2520tasks.%2520We%2520select%252033%2520representative%2520tasks%2520and%2520categorize%2520them%2520into%250Afive%2520categories%253A%2520document%252C%2520handwritten%2520text%252C%2520scene%2520text%252C%2520artistic%2520text%252C%2520and%250Acomplex%2520%255C%2526%2520layout-rich%2520text.%2520For%2520comprehensive%2520evaluation%252C%2520we%2520examine%2520six%250Amodels%2520across%2520both%2520closed-source%2520and%2520open-source%2520domains%252C%2520using%2520tailored%252C%250Ahigh-quality%2520image%2520inputs%2520and%2520prompts.%2520Through%2520this%2520evaluation%252C%2520we%2520draw%2520crucial%250Aobservations%2520and%2520identify%2520the%2520weaknesses%2520of%2520current%2520generative%2520models%2520for%2520OCR%250Atasks.%2520We%2520argue%2520that%2520photorealistic%2520text%2520image%2520generation%2520and%2520editing%2520should%2520be%250Ainternalized%2520as%2520foundational%2520skills%2520into%2520general-domain%2520generative%2520models%252C%250Arather%2520than%2520being%2520delegated%2520to%2520specialized%2520solutions%252C%2520and%2520we%2520hope%2520this%250Aempirical%2520analysis%2520can%2520provide%2520valuable%2520insights%2520for%2520the%2520community%2520to%2520achieve%250Athis%2520goal.%2520This%2520evaluation%2520is%2520online%2520and%2520will%2520be%2520continuously%2520updated%2520at%2520our%250AGitHub%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aesthetics%20is%20Cheap%2C%20Show%20me%20the%20Text%3A%20An%20Empirical%20Evaluation%20of%0A%20%20State-of-the-Art%20Generative%20Models%20for%20OCR&entry.906535625=Peirong%20Zhang%20and%20Haowei%20Xu%20and%20Jiaxin%20Zhang%20and%20Guitao%20Xu%20and%20Xuhan%20Zheng%20and%20Zhenhua%20Yang%20and%20Junle%20Liu%20and%20Yuyi%20Zhang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Text%20image%20is%20a%20unique%20and%20crucial%20information%20medium%20that%20integrates%20visual%0Aaesthetics%20and%20linguistic%20semantics%20in%20modern%20e-society.%20Due%20to%20their%20subtlety%0Aand%20complexity%2C%20the%20generation%20of%20text%20images%20represents%20a%20challenging%20and%0Aevolving%20frontier%20in%20the%20image%20generation%20field.%20The%20recent%20surge%20of%0Aspecialized%20image%20generators%20%28%5Cemph%7Be.g.%7D%2C%20Flux-series%29%20and%20unified%20generative%0Amodels%20%28%5Cemph%7Be.g.%7D%2C%20GPT-4o%29%2C%20which%20demonstrate%20exceptional%20fidelity%2C%20raises%20a%0Anatural%20question%3A%20can%20they%20master%20the%20intricacies%20of%20text%20image%20generation%20and%0Aediting%3F%20Motivated%20by%20this%2C%20we%20assess%20current%20state-of-the-art%20generative%0Amodels%27%20capabilities%20in%20terms%20of%20text%20image%20generation%20and%20editing.%20We%0Aincorporate%20various%20typical%20optical%20character%20recognition%20%28OCR%29%20tasks%20into%20our%0Aevaluation%20and%20broaden%20the%20concept%20of%20text-based%20generation%20tasks%20into%20OCR%0Agenerative%20tasks.%20We%20select%2033%20representative%20tasks%20and%20categorize%20them%20into%0Afive%20categories%3A%20document%2C%20handwritten%20text%2C%20scene%20text%2C%20artistic%20text%2C%20and%0Acomplex%20%5C%26%20layout-rich%20text.%20For%20comprehensive%20evaluation%2C%20we%20examine%20six%0Amodels%20across%20both%20closed-source%20and%20open-source%20domains%2C%20using%20tailored%2C%0Ahigh-quality%20image%20inputs%20and%20prompts.%20Through%20this%20evaluation%2C%20we%20draw%20crucial%0Aobservations%20and%20identify%20the%20weaknesses%20of%20current%20generative%20models%20for%20OCR%0Atasks.%20We%20argue%20that%20photorealistic%20text%20image%20generation%20and%20editing%20should%20be%0Ainternalized%20as%20foundational%20skills%20into%20general-domain%20generative%20models%2C%0Arather%20than%20being%20delegated%20to%20specialized%20solutions%2C%20and%20we%20hope%20this%0Aempirical%20analysis%20can%20provide%20valuable%20insights%20for%20the%20community%20to%20achieve%0Athis%20goal.%20This%20evaluation%20is%20online%20and%20will%20be%20continuously%20updated%20at%20our%0AGitHub%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15085v2&entry.124074799=Read"},
{"title": "Advancing Quantum Information Science Pre-College Education: The Case\n  for Learning Sciences Collaboration", "author": "Raquel Coelho and Roy Pea and Christian Schunn and Jinglei Cheng and Junyu Liu", "abstract": "  As quantum information science advances and the need for pre-college\nengagement grows, a critical question remains: How can young learners be\nprepared to participate in a field so radically different from what they have\nencountered before? This paper argues that meeting this challenge will require\nstrong interdisciplinary collaboration with the Learning Sciences (LS), a field\ndedicated to understanding how people learn and designing theory-guided\nenvironments to support learning. Drawing on lessons from previous STEM\neducation efforts, we discuss two key contributions of the learning sciences to\nquantum information science (QIS) education. The first is design-based\nresearch, the signature methodology of learning sciences, which can inform the\ndevelopment, refinement, and scaling of effective QIS learning experiences. The\nsecond is a framework for reshaping how learners reason about, learn and\nparticipate in QIS practices through shifts in knowledge representations that\nprovide new forms of engagement and associated learning. We call for a two-way\npartnership between quantum information science and the learning sciences, one\nthat not only supports learning in quantum concepts and practices but also\nimproves our understanding of how to teach and support learning in highly\ncomplex domains. We also consider potential questions involved in bridging\nthese disciplinary communities and argue that the theoretical and practical\nbenefits justify the effort.\n", "link": "http://arxiv.org/abs/2508.00668v1", "date": "2025-08-01", "relevancy": 1.7496, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4442}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Quantum%20Information%20Science%20Pre-College%20Education%3A%20The%20Case%0A%20%20for%20Learning%20Sciences%20Collaboration&body=Title%3A%20Advancing%20Quantum%20Information%20Science%20Pre-College%20Education%3A%20The%20Case%0A%20%20for%20Learning%20Sciences%20Collaboration%0AAuthor%3A%20Raquel%20Coelho%20and%20Roy%20Pea%20and%20Christian%20Schunn%20and%20Jinglei%20Cheng%20and%20Junyu%20Liu%0AAbstract%3A%20%20%20As%20quantum%20information%20science%20advances%20and%20the%20need%20for%20pre-college%0Aengagement%20grows%2C%20a%20critical%20question%20remains%3A%20How%20can%20young%20learners%20be%0Aprepared%20to%20participate%20in%20a%20field%20so%20radically%20different%20from%20what%20they%20have%0Aencountered%20before%3F%20This%20paper%20argues%20that%20meeting%20this%20challenge%20will%20require%0Astrong%20interdisciplinary%20collaboration%20with%20the%20Learning%20Sciences%20%28LS%29%2C%20a%20field%0Adedicated%20to%20understanding%20how%20people%20learn%20and%20designing%20theory-guided%0Aenvironments%20to%20support%20learning.%20Drawing%20on%20lessons%20from%20previous%20STEM%0Aeducation%20efforts%2C%20we%20discuss%20two%20key%20contributions%20of%20the%20learning%20sciences%20to%0Aquantum%20information%20science%20%28QIS%29%20education.%20The%20first%20is%20design-based%0Aresearch%2C%20the%20signature%20methodology%20of%20learning%20sciences%2C%20which%20can%20inform%20the%0Adevelopment%2C%20refinement%2C%20and%20scaling%20of%20effective%20QIS%20learning%20experiences.%20The%0Asecond%20is%20a%20framework%20for%20reshaping%20how%20learners%20reason%20about%2C%20learn%20and%0Aparticipate%20in%20QIS%20practices%20through%20shifts%20in%20knowledge%20representations%20that%0Aprovide%20new%20forms%20of%20engagement%20and%20associated%20learning.%20We%20call%20for%20a%20two-way%0Apartnership%20between%20quantum%20information%20science%20and%20the%20learning%20sciences%2C%20one%0Athat%20not%20only%20supports%20learning%20in%20quantum%20concepts%20and%20practices%20but%20also%0Aimproves%20our%20understanding%20of%20how%20to%20teach%20and%20support%20learning%20in%20highly%0Acomplex%20domains.%20We%20also%20consider%20potential%20questions%20involved%20in%20bridging%0Athese%20disciplinary%20communities%20and%20argue%20that%20the%20theoretical%20and%20practical%0Abenefits%20justify%20the%20effort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Quantum%2520Information%2520Science%2520Pre-College%2520Education%253A%2520The%2520Case%250A%2520%2520for%2520Learning%2520Sciences%2520Collaboration%26entry.906535625%3DRaquel%2520Coelho%2520and%2520Roy%2520Pea%2520and%2520Christian%2520Schunn%2520and%2520Jinglei%2520Cheng%2520and%2520Junyu%2520Liu%26entry.1292438233%3D%2520%2520As%2520quantum%2520information%2520science%2520advances%2520and%2520the%2520need%2520for%2520pre-college%250Aengagement%2520grows%252C%2520a%2520critical%2520question%2520remains%253A%2520How%2520can%2520young%2520learners%2520be%250Aprepared%2520to%2520participate%2520in%2520a%2520field%2520so%2520radically%2520different%2520from%2520what%2520they%2520have%250Aencountered%2520before%253F%2520This%2520paper%2520argues%2520that%2520meeting%2520this%2520challenge%2520will%2520require%250Astrong%2520interdisciplinary%2520collaboration%2520with%2520the%2520Learning%2520Sciences%2520%2528LS%2529%252C%2520a%2520field%250Adedicated%2520to%2520understanding%2520how%2520people%2520learn%2520and%2520designing%2520theory-guided%250Aenvironments%2520to%2520support%2520learning.%2520Drawing%2520on%2520lessons%2520from%2520previous%2520STEM%250Aeducation%2520efforts%252C%2520we%2520discuss%2520two%2520key%2520contributions%2520of%2520the%2520learning%2520sciences%2520to%250Aquantum%2520information%2520science%2520%2528QIS%2529%2520education.%2520The%2520first%2520is%2520design-based%250Aresearch%252C%2520the%2520signature%2520methodology%2520of%2520learning%2520sciences%252C%2520which%2520can%2520inform%2520the%250Adevelopment%252C%2520refinement%252C%2520and%2520scaling%2520of%2520effective%2520QIS%2520learning%2520experiences.%2520The%250Asecond%2520is%2520a%2520framework%2520for%2520reshaping%2520how%2520learners%2520reason%2520about%252C%2520learn%2520and%250Aparticipate%2520in%2520QIS%2520practices%2520through%2520shifts%2520in%2520knowledge%2520representations%2520that%250Aprovide%2520new%2520forms%2520of%2520engagement%2520and%2520associated%2520learning.%2520We%2520call%2520for%2520a%2520two-way%250Apartnership%2520between%2520quantum%2520information%2520science%2520and%2520the%2520learning%2520sciences%252C%2520one%250Athat%2520not%2520only%2520supports%2520learning%2520in%2520quantum%2520concepts%2520and%2520practices%2520but%2520also%250Aimproves%2520our%2520understanding%2520of%2520how%2520to%2520teach%2520and%2520support%2520learning%2520in%2520highly%250Acomplex%2520domains.%2520We%2520also%2520consider%2520potential%2520questions%2520involved%2520in%2520bridging%250Athese%2520disciplinary%2520communities%2520and%2520argue%2520that%2520the%2520theoretical%2520and%2520practical%250Abenefits%2520justify%2520the%2520effort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Quantum%20Information%20Science%20Pre-College%20Education%3A%20The%20Case%0A%20%20for%20Learning%20Sciences%20Collaboration&entry.906535625=Raquel%20Coelho%20and%20Roy%20Pea%20and%20Christian%20Schunn%20and%20Jinglei%20Cheng%20and%20Junyu%20Liu&entry.1292438233=%20%20As%20quantum%20information%20science%20advances%20and%20the%20need%20for%20pre-college%0Aengagement%20grows%2C%20a%20critical%20question%20remains%3A%20How%20can%20young%20learners%20be%0Aprepared%20to%20participate%20in%20a%20field%20so%20radically%20different%20from%20what%20they%20have%0Aencountered%20before%3F%20This%20paper%20argues%20that%20meeting%20this%20challenge%20will%20require%0Astrong%20interdisciplinary%20collaboration%20with%20the%20Learning%20Sciences%20%28LS%29%2C%20a%20field%0Adedicated%20to%20understanding%20how%20people%20learn%20and%20designing%20theory-guided%0Aenvironments%20to%20support%20learning.%20Drawing%20on%20lessons%20from%20previous%20STEM%0Aeducation%20efforts%2C%20we%20discuss%20two%20key%20contributions%20of%20the%20learning%20sciences%20to%0Aquantum%20information%20science%20%28QIS%29%20education.%20The%20first%20is%20design-based%0Aresearch%2C%20the%20signature%20methodology%20of%20learning%20sciences%2C%20which%20can%20inform%20the%0Adevelopment%2C%20refinement%2C%20and%20scaling%20of%20effective%20QIS%20learning%20experiences.%20The%0Asecond%20is%20a%20framework%20for%20reshaping%20how%20learners%20reason%20about%2C%20learn%20and%0Aparticipate%20in%20QIS%20practices%20through%20shifts%20in%20knowledge%20representations%20that%0Aprovide%20new%20forms%20of%20engagement%20and%20associated%20learning.%20We%20call%20for%20a%20two-way%0Apartnership%20between%20quantum%20information%20science%20and%20the%20learning%20sciences%2C%20one%0Athat%20not%20only%20supports%20learning%20in%20quantum%20concepts%20and%20practices%20but%20also%0Aimproves%20our%20understanding%20of%20how%20to%20teach%20and%20support%20learning%20in%20highly%0Acomplex%20domains.%20We%20also%20consider%20potential%20questions%20involved%20in%20bridging%0Athese%20disciplinary%20communities%20and%20argue%20that%20the%20theoretical%20and%20practical%0Abenefits%20justify%20the%20effort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00668v1&entry.124074799=Read"},
{"title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for\n  Better Synthetic Content Forensics", "author": "Tom Or and Omri Azencot", "abstract": "  Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.\n", "link": "http://arxiv.org/abs/2508.00784v1", "date": "2025-08-01", "relevancy": 1.1238, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5665}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5632}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Hidden%20Representations%3A%20A%20Multi-Modal%20Layer%20Analysis%20for%0A%20%20Better%20Synthetic%20Content%20Forensics&body=Title%3A%20Unraveling%20Hidden%20Representations%3A%20A%20Multi-Modal%20Layer%20Analysis%20for%0A%20%20Better%20Synthetic%20Content%20Forensics%0AAuthor%3A%20Tom%20Or%20and%20Omri%20Azencot%0AAbstract%3A%20%20%20Generative%20models%20achieve%20remarkable%20results%20in%20multiple%20data%20domains%2C%0Aincluding%20images%20and%20texts%2C%20among%20other%20examples.%20Unfortunately%2C%20malicious%0Ausers%20exploit%20synthetic%20media%20for%20spreading%20misinformation%20and%20disseminating%0Adeepfakes.%20Consequently%2C%20the%20need%20for%20robust%20and%20stable%20fake%20detectors%20is%0Apressing%2C%20especially%20when%20new%20generative%20models%20appear%20everyday.%20While%20the%0Amajority%20of%20existing%20work%20train%20classifiers%20that%20discriminate%20between%20real%20and%0Afake%20information%2C%20such%20tools%20typically%20generalize%20only%20within%20the%20same%20family%0Aof%20generators%20and%20data%20modalities%2C%20yielding%20poor%20results%20on%20other%20generative%0Aclasses%20and%20data%20domains.%20Towards%20a%20universal%20classifier%2C%20we%20propose%20the%20use%20of%0Alarge%20pre-trained%20multi-modal%20models%20for%20the%20detection%20of%20generative%20content.%0AEffectively%2C%20we%20show%20that%20the%20latent%20code%20of%20these%20models%20naturally%20captures%0Ainformation%20discriminating%20real%20from%20fake.%20Building%20on%20this%20observation%2C%20we%0Ademonstrate%20that%20linear%20classifiers%20trained%20on%20these%20features%20can%20achieve%0Astate-of-the-art%20results%20across%20various%20modalities%2C%20while%20remaining%0Acomputationally%20efficient%2C%20fast%20to%20train%2C%20and%20effective%20even%20in%20few-shot%0Asettings.%20Our%20work%20primarily%20focuses%20on%20fake%20detection%20in%20audio%20and%20images%2C%0Aachieving%20performance%20that%20surpasses%20or%20matches%20that%20of%20strong%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Hidden%2520Representations%253A%2520A%2520Multi-Modal%2520Layer%2520Analysis%2520for%250A%2520%2520Better%2520Synthetic%2520Content%2520Forensics%26entry.906535625%3DTom%2520Or%2520and%2520Omri%2520Azencot%26entry.1292438233%3D%2520%2520Generative%2520models%2520achieve%2520remarkable%2520results%2520in%2520multiple%2520data%2520domains%252C%250Aincluding%2520images%2520and%2520texts%252C%2520among%2520other%2520examples.%2520Unfortunately%252C%2520malicious%250Ausers%2520exploit%2520synthetic%2520media%2520for%2520spreading%2520misinformation%2520and%2520disseminating%250Adeepfakes.%2520Consequently%252C%2520the%2520need%2520for%2520robust%2520and%2520stable%2520fake%2520detectors%2520is%250Apressing%252C%2520especially%2520when%2520new%2520generative%2520models%2520appear%2520everyday.%2520While%2520the%250Amajority%2520of%2520existing%2520work%2520train%2520classifiers%2520that%2520discriminate%2520between%2520real%2520and%250Afake%2520information%252C%2520such%2520tools%2520typically%2520generalize%2520only%2520within%2520the%2520same%2520family%250Aof%2520generators%2520and%2520data%2520modalities%252C%2520yielding%2520poor%2520results%2520on%2520other%2520generative%250Aclasses%2520and%2520data%2520domains.%2520Towards%2520a%2520universal%2520classifier%252C%2520we%2520propose%2520the%2520use%2520of%250Alarge%2520pre-trained%2520multi-modal%2520models%2520for%2520the%2520detection%2520of%2520generative%2520content.%250AEffectively%252C%2520we%2520show%2520that%2520the%2520latent%2520code%2520of%2520these%2520models%2520naturally%2520captures%250Ainformation%2520discriminating%2520real%2520from%2520fake.%2520Building%2520on%2520this%2520observation%252C%2520we%250Ademonstrate%2520that%2520linear%2520classifiers%2520trained%2520on%2520these%2520features%2520can%2520achieve%250Astate-of-the-art%2520results%2520across%2520various%2520modalities%252C%2520while%2520remaining%250Acomputationally%2520efficient%252C%2520fast%2520to%2520train%252C%2520and%2520effective%2520even%2520in%2520few-shot%250Asettings.%2520Our%2520work%2520primarily%2520focuses%2520on%2520fake%2520detection%2520in%2520audio%2520and%2520images%252C%250Aachieving%2520performance%2520that%2520surpasses%2520or%2520matches%2520that%2520of%2520strong%2520baseline%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Hidden%20Representations%3A%20A%20Multi-Modal%20Layer%20Analysis%20for%0A%20%20Better%20Synthetic%20Content%20Forensics&entry.906535625=Tom%20Or%20and%20Omri%20Azencot&entry.1292438233=%20%20Generative%20models%20achieve%20remarkable%20results%20in%20multiple%20data%20domains%2C%0Aincluding%20images%20and%20texts%2C%20among%20other%20examples.%20Unfortunately%2C%20malicious%0Ausers%20exploit%20synthetic%20media%20for%20spreading%20misinformation%20and%20disseminating%0Adeepfakes.%20Consequently%2C%20the%20need%20for%20robust%20and%20stable%20fake%20detectors%20is%0Apressing%2C%20especially%20when%20new%20generative%20models%20appear%20everyday.%20While%20the%0Amajority%20of%20existing%20work%20train%20classifiers%20that%20discriminate%20between%20real%20and%0Afake%20information%2C%20such%20tools%20typically%20generalize%20only%20within%20the%20same%20family%0Aof%20generators%20and%20data%20modalities%2C%20yielding%20poor%20results%20on%20other%20generative%0Aclasses%20and%20data%20domains.%20Towards%20a%20universal%20classifier%2C%20we%20propose%20the%20use%20of%0Alarge%20pre-trained%20multi-modal%20models%20for%20the%20detection%20of%20generative%20content.%0AEffectively%2C%20we%20show%20that%20the%20latent%20code%20of%20these%20models%20naturally%20captures%0Ainformation%20discriminating%20real%20from%20fake.%20Building%20on%20this%20observation%2C%20we%0Ademonstrate%20that%20linear%20classifiers%20trained%20on%20these%20features%20can%20achieve%0Astate-of-the-art%20results%20across%20various%20modalities%2C%20while%20remaining%0Acomputationally%20efficient%2C%20fast%20to%20train%2C%20and%20effective%20even%20in%20few-shot%0Asettings.%20Our%20work%20primarily%20focuses%20on%20fake%20detection%20in%20audio%20and%20images%2C%0Aachieving%20performance%20that%20surpasses%20or%20matches%20that%20of%20strong%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00784v1&entry.124074799=Read"},
{"title": "The Second Machine Turn: From Checking Proofs to Creating Concepts", "author": "Asvin G", "abstract": "  We identify a second machine turn in the process of mathematical discovery:\nafter automating proof-checking, AI is now poised to automate the *creation* of\nmathematical concepts themselves. We discuss the current state of the art,\nobstacles and potential solutions as well as a preliminary attempt at\nmathematizing the creation of concepts itself. The paper ends with an\nassessment of how these capabilities could reshape mathematics and\nhuman-machine collaboration, and a few different futures we might find\nourselves in.\n", "link": "http://arxiv.org/abs/2507.10179v2", "date": "2025-08-01", "relevancy": 1.2333, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.418}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4095}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Second%20Machine%20Turn%3A%20From%20Checking%20Proofs%20to%20Creating%20Concepts&body=Title%3A%20The%20Second%20Machine%20Turn%3A%20From%20Checking%20Proofs%20to%20Creating%20Concepts%0AAuthor%3A%20Asvin%20G%0AAbstract%3A%20%20%20We%20identify%20a%20second%20machine%20turn%20in%20the%20process%20of%20mathematical%20discovery%3A%0Aafter%20automating%20proof-checking%2C%20AI%20is%20now%20poised%20to%20automate%20the%20%2Acreation%2A%20of%0Amathematical%20concepts%20themselves.%20We%20discuss%20the%20current%20state%20of%20the%20art%2C%0Aobstacles%20and%20potential%20solutions%20as%20well%20as%20a%20preliminary%20attempt%20at%0Amathematizing%20the%20creation%20of%20concepts%20itself.%20The%20paper%20ends%20with%20an%0Aassessment%20of%20how%20these%20capabilities%20could%20reshape%20mathematics%20and%0Ahuman-machine%20collaboration%2C%20and%20a%20few%20different%20futures%20we%20might%20find%0Aourselves%20in.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Second%2520Machine%2520Turn%253A%2520From%2520Checking%2520Proofs%2520to%2520Creating%2520Concepts%26entry.906535625%3DAsvin%2520G%26entry.1292438233%3D%2520%2520We%2520identify%2520a%2520second%2520machine%2520turn%2520in%2520the%2520process%2520of%2520mathematical%2520discovery%253A%250Aafter%2520automating%2520proof-checking%252C%2520AI%2520is%2520now%2520poised%2520to%2520automate%2520the%2520%252Acreation%252A%2520of%250Amathematical%2520concepts%2520themselves.%2520We%2520discuss%2520the%2520current%2520state%2520of%2520the%2520art%252C%250Aobstacles%2520and%2520potential%2520solutions%2520as%2520well%2520as%2520a%2520preliminary%2520attempt%2520at%250Amathematizing%2520the%2520creation%2520of%2520concepts%2520itself.%2520The%2520paper%2520ends%2520with%2520an%250Aassessment%2520of%2520how%2520these%2520capabilities%2520could%2520reshape%2520mathematics%2520and%250Ahuman-machine%2520collaboration%252C%2520and%2520a%2520few%2520different%2520futures%2520we%2520might%2520find%250Aourselves%2520in.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Second%20Machine%20Turn%3A%20From%20Checking%20Proofs%20to%20Creating%20Concepts&entry.906535625=Asvin%20G&entry.1292438233=%20%20We%20identify%20a%20second%20machine%20turn%20in%20the%20process%20of%20mathematical%20discovery%3A%0Aafter%20automating%20proof-checking%2C%20AI%20is%20now%20poised%20to%20automate%20the%20%2Acreation%2A%20of%0Amathematical%20concepts%20themselves.%20We%20discuss%20the%20current%20state%20of%20the%20art%2C%0Aobstacles%20and%20potential%20solutions%20as%20well%20as%20a%20preliminary%20attempt%20at%0Amathematizing%20the%20creation%20of%20concepts%20itself.%20The%20paper%20ends%20with%20an%0Aassessment%20of%20how%20these%20capabilities%20could%20reshape%20mathematics%20and%0Ahuman-machine%20collaboration%2C%20and%20a%20few%20different%20futures%20we%20might%20find%0Aourselves%20in.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10179v2&entry.124074799=Read"},
{"title": "JSON-Bag: A generic game trajectory representation", "author": "Dien Nguyen and Diego Perez-Liebana and Simon Lucas", "abstract": "  We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.\n", "link": "http://arxiv.org/abs/2508.00712v1", "date": "2025-08-01", "relevancy": 1.4254, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5372}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4685}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JSON-Bag%3A%20A%20generic%20game%20trajectory%20representation&body=Title%3A%20JSON-Bag%3A%20A%20generic%20game%20trajectory%20representation%0AAuthor%3A%20Dien%20Nguyen%20and%20Diego%20Perez-Liebana%20and%20Simon%20Lucas%0AAbstract%3A%20%20%20We%20introduce%20JSON%20Bag-of-Tokens%20model%20%28JSON-Bag%29%20as%20a%20method%20to%20generically%0Arepresent%20game%20trajectories%20by%20tokenizing%20their%20JSON%20descriptions%20and%20apply%0AJensen-Shannon%20distance%20%28JSD%29%20as%20distance%20metric%20for%20them.%20Using%20a%0Aprototype-based%20nearest-neighbor%20search%20%28P-NNS%29%2C%20we%20evaluate%20the%20validity%20of%0AJSON-Bag%20with%20JSD%20on%20six%20tabletop%20games%20--%20%5Ctextit%7B7%20Wonders%7D%2C%0A%5Ctextit%7BDominion%7D%2C%20%5Ctextit%7BSea%20Salt%20and%20Paper%7D%2C%20%5Ctextit%7BCan%27t%20Stop%7D%2C%0A%5Ctextit%7BConnect4%7D%2C%20%5Ctextit%7BDots%20and%20boxes%7D%20--%20each%20over%20three%20game%20trajectory%0Aclassification%20tasks%3A%20classifying%20the%20playing%20agents%2C%20game%20parameters%2C%20or%20game%0Aseeds%20that%20were%20used%20to%20generate%20the%20trajectories.%0A%20%20Our%20approach%20outperforms%20a%20baseline%20using%20hand-crafted%20features%20in%20the%0Amajority%20of%20tasks.%20Evaluating%20on%20N-shot%20classification%20suggests%20using%20JSON-Bag%0Aprototype%20to%20represent%20game%20trajectory%20classes%20is%20also%20sample%20efficient.%0AAdditionally%2C%20we%20demonstrate%20JSON-Bag%20ability%20for%20automatic%20feature%20extraction%0Aby%20treating%20tokens%20as%20individual%20features%20to%20be%20used%20in%20Random%20Forest%20to%20solve%0Athe%20tasks%20above%2C%20which%20significantly%20improves%20accuracy%20on%20underperforming%0Atasks.%20Finally%2C%20we%20show%20that%2C%20across%20all%20six%20games%2C%20the%20JSD%20between%20JSON-Bag%0Aprototypes%20of%20agent%20classes%20highly%20correlates%20with%20the%20distances%20between%0Aagents%27%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJSON-Bag%253A%2520A%2520generic%2520game%2520trajectory%2520representation%26entry.906535625%3DDien%2520Nguyen%2520and%2520Diego%2520Perez-Liebana%2520and%2520Simon%2520Lucas%26entry.1292438233%3D%2520%2520We%2520introduce%2520JSON%2520Bag-of-Tokens%2520model%2520%2528JSON-Bag%2529%2520as%2520a%2520method%2520to%2520generically%250Arepresent%2520game%2520trajectories%2520by%2520tokenizing%2520their%2520JSON%2520descriptions%2520and%2520apply%250AJensen-Shannon%2520distance%2520%2528JSD%2529%2520as%2520distance%2520metric%2520for%2520them.%2520Using%2520a%250Aprototype-based%2520nearest-neighbor%2520search%2520%2528P-NNS%2529%252C%2520we%2520evaluate%2520the%2520validity%2520of%250AJSON-Bag%2520with%2520JSD%2520on%2520six%2520tabletop%2520games%2520--%2520%255Ctextit%257B7%2520Wonders%257D%252C%250A%255Ctextit%257BDominion%257D%252C%2520%255Ctextit%257BSea%2520Salt%2520and%2520Paper%257D%252C%2520%255Ctextit%257BCan%2527t%2520Stop%257D%252C%250A%255Ctextit%257BConnect4%257D%252C%2520%255Ctextit%257BDots%2520and%2520boxes%257D%2520--%2520each%2520over%2520three%2520game%2520trajectory%250Aclassification%2520tasks%253A%2520classifying%2520the%2520playing%2520agents%252C%2520game%2520parameters%252C%2520or%2520game%250Aseeds%2520that%2520were%2520used%2520to%2520generate%2520the%2520trajectories.%250A%2520%2520Our%2520approach%2520outperforms%2520a%2520baseline%2520using%2520hand-crafted%2520features%2520in%2520the%250Amajority%2520of%2520tasks.%2520Evaluating%2520on%2520N-shot%2520classification%2520suggests%2520using%2520JSON-Bag%250Aprototype%2520to%2520represent%2520game%2520trajectory%2520classes%2520is%2520also%2520sample%2520efficient.%250AAdditionally%252C%2520we%2520demonstrate%2520JSON-Bag%2520ability%2520for%2520automatic%2520feature%2520extraction%250Aby%2520treating%2520tokens%2520as%2520individual%2520features%2520to%2520be%2520used%2520in%2520Random%2520Forest%2520to%2520solve%250Athe%2520tasks%2520above%252C%2520which%2520significantly%2520improves%2520accuracy%2520on%2520underperforming%250Atasks.%2520Finally%252C%2520we%2520show%2520that%252C%2520across%2520all%2520six%2520games%252C%2520the%2520JSD%2520between%2520JSON-Bag%250Aprototypes%2520of%2520agent%2520classes%2520highly%2520correlates%2520with%2520the%2520distances%2520between%250Aagents%2527%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JSON-Bag%3A%20A%20generic%20game%20trajectory%20representation&entry.906535625=Dien%20Nguyen%20and%20Diego%20Perez-Liebana%20and%20Simon%20Lucas&entry.1292438233=%20%20We%20introduce%20JSON%20Bag-of-Tokens%20model%20%28JSON-Bag%29%20as%20a%20method%20to%20generically%0Arepresent%20game%20trajectories%20by%20tokenizing%20their%20JSON%20descriptions%20and%20apply%0AJensen-Shannon%20distance%20%28JSD%29%20as%20distance%20metric%20for%20them.%20Using%20a%0Aprototype-based%20nearest-neighbor%20search%20%28P-NNS%29%2C%20we%20evaluate%20the%20validity%20of%0AJSON-Bag%20with%20JSD%20on%20six%20tabletop%20games%20--%20%5Ctextit%7B7%20Wonders%7D%2C%0A%5Ctextit%7BDominion%7D%2C%20%5Ctextit%7BSea%20Salt%20and%20Paper%7D%2C%20%5Ctextit%7BCan%27t%20Stop%7D%2C%0A%5Ctextit%7BConnect4%7D%2C%20%5Ctextit%7BDots%20and%20boxes%7D%20--%20each%20over%20three%20game%20trajectory%0Aclassification%20tasks%3A%20classifying%20the%20playing%20agents%2C%20game%20parameters%2C%20or%20game%0Aseeds%20that%20were%20used%20to%20generate%20the%20trajectories.%0A%20%20Our%20approach%20outperforms%20a%20baseline%20using%20hand-crafted%20features%20in%20the%0Amajority%20of%20tasks.%20Evaluating%20on%20N-shot%20classification%20suggests%20using%20JSON-Bag%0Aprototype%20to%20represent%20game%20trajectory%20classes%20is%20also%20sample%20efficient.%0AAdditionally%2C%20we%20demonstrate%20JSON-Bag%20ability%20for%20automatic%20feature%20extraction%0Aby%20treating%20tokens%20as%20individual%20features%20to%20be%20used%20in%20Random%20Forest%20to%20solve%0Athe%20tasks%20above%2C%20which%20significantly%20improves%20accuracy%20on%20underperforming%0Atasks.%20Finally%2C%20we%20show%20that%2C%20across%20all%20six%20games%2C%20the%20JSD%20between%20JSON-Bag%0Aprototypes%20of%20agent%20classes%20highly%20correlates%20with%20the%20distances%20between%0Aagents%27%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00712v1&entry.124074799=Read"},
{"title": "Large Deviations of Gaussian Neural Networks with ReLU activation", "author": "Quirin Vogel", "abstract": "  We prove a large deviation principle for deep neural networks with Gaussian\nweights and at most linearly growing activation functions, such as ReLU. This\ngeneralises earlier work, in which bounded and continuous activation functions\nwere considered. In practice, linearly growing activation functions such as\nReLU are most commonly used. We furthermore simplify previous expressions for\nthe rate function and provide a power-series expansions for the ReLU case.\n", "link": "http://arxiv.org/abs/2405.16958v2", "date": "2025-08-01", "relevancy": 1.7481, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4507}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4367}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Deviations%20of%20Gaussian%20Neural%20Networks%20with%20ReLU%20activation&body=Title%3A%20Large%20Deviations%20of%20Gaussian%20Neural%20Networks%20with%20ReLU%20activation%0AAuthor%3A%20Quirin%20Vogel%0AAbstract%3A%20%20%20We%20prove%20a%20large%20deviation%20principle%20for%20deep%20neural%20networks%20with%20Gaussian%0Aweights%20and%20at%20most%20linearly%20growing%20activation%20functions%2C%20such%20as%20ReLU.%20This%0Ageneralises%20earlier%20work%2C%20in%20which%20bounded%20and%20continuous%20activation%20functions%0Awere%20considered.%20In%20practice%2C%20linearly%20growing%20activation%20functions%20such%20as%0AReLU%20are%20most%20commonly%20used.%20We%20furthermore%20simplify%20previous%20expressions%20for%0Athe%20rate%20function%20and%20provide%20a%20power-series%20expansions%20for%20the%20ReLU%20case.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Deviations%2520of%2520Gaussian%2520Neural%2520Networks%2520with%2520ReLU%2520activation%26entry.906535625%3DQuirin%2520Vogel%26entry.1292438233%3D%2520%2520We%2520prove%2520a%2520large%2520deviation%2520principle%2520for%2520deep%2520neural%2520networks%2520with%2520Gaussian%250Aweights%2520and%2520at%2520most%2520linearly%2520growing%2520activation%2520functions%252C%2520such%2520as%2520ReLU.%2520This%250Ageneralises%2520earlier%2520work%252C%2520in%2520which%2520bounded%2520and%2520continuous%2520activation%2520functions%250Awere%2520considered.%2520In%2520practice%252C%2520linearly%2520growing%2520activation%2520functions%2520such%2520as%250AReLU%2520are%2520most%2520commonly%2520used.%2520We%2520furthermore%2520simplify%2520previous%2520expressions%2520for%250Athe%2520rate%2520function%2520and%2520provide%2520a%2520power-series%2520expansions%2520for%2520the%2520ReLU%2520case.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Deviations%20of%20Gaussian%20Neural%20Networks%20with%20ReLU%20activation&entry.906535625=Quirin%20Vogel&entry.1292438233=%20%20We%20prove%20a%20large%20deviation%20principle%20for%20deep%20neural%20networks%20with%20Gaussian%0Aweights%20and%20at%20most%20linearly%20growing%20activation%20functions%2C%20such%20as%20ReLU.%20This%0Ageneralises%20earlier%20work%2C%20in%20which%20bounded%20and%20continuous%20activation%20functions%0Awere%20considered.%20In%20practice%2C%20linearly%20growing%20activation%20functions%20such%20as%0AReLU%20are%20most%20commonly%20used.%20We%20furthermore%20simplify%20previous%20expressions%20for%0Athe%20rate%20function%20and%20provide%20a%20power-series%20expansions%20for%20the%20ReLU%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16958v2&entry.124074799=Read"},
{"title": "Context-Aware Visualization for Explainable AI Recommendations in Social\n  Media: A Vision for User-Aligned Explanations", "author": "Banan Alkhateeb and Ellis Solaiman", "abstract": "  Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.\n", "link": "http://arxiv.org/abs/2508.00674v1", "date": "2025-08-01", "relevancy": 1.4655, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4919}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Visualization%20for%20Explainable%20AI%20Recommendations%20in%20Social%0A%20%20Media%3A%20A%20Vision%20for%20User-Aligned%20Explanations&body=Title%3A%20Context-Aware%20Visualization%20for%20Explainable%20AI%20Recommendations%20in%20Social%0A%20%20Media%3A%20A%20Vision%20for%20User-Aligned%20Explanations%0AAuthor%3A%20Banan%20Alkhateeb%20and%20Ellis%20Solaiman%0AAbstract%3A%20%20%20Social%20media%20platforms%20today%20strive%20to%20improve%20user%20experience%20through%20AI%0Arecommendations%2C%20yet%20the%20value%20of%20such%20recommendations%20vanishes%20as%20users%20do%20not%0Aunderstand%20the%20reasons%20behind%20them.%20This%20issue%20arises%20because%20explainability%20in%0Asocial%20media%20is%20general%20and%20lacks%20alignment%20with%20user-specific%20needs.%20In%20this%0Avision%20paper%2C%20we%20outline%20a%20user-segmented%20and%20context-aware%20explanation%20layer%0Aby%20proposing%20a%20visual%20explanation%20system%20with%20diverse%20explanation%20methods.%20The%0Aproposed%20system%20is%20framed%20by%20the%20variety%20of%20user%20needs%20and%20contexts%2C%20showing%0Aexplanations%20in%20different%20visualized%20forms%2C%20including%20a%20technically%20detailed%0Aversion%20for%20AI%20experts%20and%20a%20simplified%20one%20for%20lay%20users.%20Our%20framework%20is%20the%0Afirst%20to%20jointly%20adapt%20explanation%20style%20%28visual%20vs.%20numeric%29%20and%20granularity%0A%28expert%20vs.%20lay%29%20inside%20a%20single%20pipeline.%20A%20public%20pilot%20with%2030%20X%20users%20will%0Avalidate%20its%20impact%20on%20decision-making%20and%20trust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Visualization%2520for%2520Explainable%2520AI%2520Recommendations%2520in%2520Social%250A%2520%2520Media%253A%2520A%2520Vision%2520for%2520User-Aligned%2520Explanations%26entry.906535625%3DBanan%2520Alkhateeb%2520and%2520Ellis%2520Solaiman%26entry.1292438233%3D%2520%2520Social%2520media%2520platforms%2520today%2520strive%2520to%2520improve%2520user%2520experience%2520through%2520AI%250Arecommendations%252C%2520yet%2520the%2520value%2520of%2520such%2520recommendations%2520vanishes%2520as%2520users%2520do%2520not%250Aunderstand%2520the%2520reasons%2520behind%2520them.%2520This%2520issue%2520arises%2520because%2520explainability%2520in%250Asocial%2520media%2520is%2520general%2520and%2520lacks%2520alignment%2520with%2520user-specific%2520needs.%2520In%2520this%250Avision%2520paper%252C%2520we%2520outline%2520a%2520user-segmented%2520and%2520context-aware%2520explanation%2520layer%250Aby%2520proposing%2520a%2520visual%2520explanation%2520system%2520with%2520diverse%2520explanation%2520methods.%2520The%250Aproposed%2520system%2520is%2520framed%2520by%2520the%2520variety%2520of%2520user%2520needs%2520and%2520contexts%252C%2520showing%250Aexplanations%2520in%2520different%2520visualized%2520forms%252C%2520including%2520a%2520technically%2520detailed%250Aversion%2520for%2520AI%2520experts%2520and%2520a%2520simplified%2520one%2520for%2520lay%2520users.%2520Our%2520framework%2520is%2520the%250Afirst%2520to%2520jointly%2520adapt%2520explanation%2520style%2520%2528visual%2520vs.%2520numeric%2529%2520and%2520granularity%250A%2528expert%2520vs.%2520lay%2529%2520inside%2520a%2520single%2520pipeline.%2520A%2520public%2520pilot%2520with%252030%2520X%2520users%2520will%250Avalidate%2520its%2520impact%2520on%2520decision-making%2520and%2520trust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Visualization%20for%20Explainable%20AI%20Recommendations%20in%20Social%0A%20%20Media%3A%20A%20Vision%20for%20User-Aligned%20Explanations&entry.906535625=Banan%20Alkhateeb%20and%20Ellis%20Solaiman&entry.1292438233=%20%20Social%20media%20platforms%20today%20strive%20to%20improve%20user%20experience%20through%20AI%0Arecommendations%2C%20yet%20the%20value%20of%20such%20recommendations%20vanishes%20as%20users%20do%20not%0Aunderstand%20the%20reasons%20behind%20them.%20This%20issue%20arises%20because%20explainability%20in%0Asocial%20media%20is%20general%20and%20lacks%20alignment%20with%20user-specific%20needs.%20In%20this%0Avision%20paper%2C%20we%20outline%20a%20user-segmented%20and%20context-aware%20explanation%20layer%0Aby%20proposing%20a%20visual%20explanation%20system%20with%20diverse%20explanation%20methods.%20The%0Aproposed%20system%20is%20framed%20by%20the%20variety%20of%20user%20needs%20and%20contexts%2C%20showing%0Aexplanations%20in%20different%20visualized%20forms%2C%20including%20a%20technically%20detailed%0Aversion%20for%20AI%20experts%20and%20a%20simplified%20one%20for%20lay%20users.%20Our%20framework%20is%20the%0Afirst%20to%20jointly%20adapt%20explanation%20style%20%28visual%20vs.%20numeric%29%20and%20granularity%0A%28expert%20vs.%20lay%29%20inside%20a%20single%20pipeline.%20A%20public%20pilot%20with%2030%20X%20users%20will%0Avalidate%20its%20impact%20on%20decision-making%20and%20trust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00674v1&entry.124074799=Read"},
{"title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable\n  Heterogeneous AGV Systems", "author": "Boyu Li and Zhengchen Li and Weimin Wu and Mengchu Zhou", "abstract": "  The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.\n", "link": "http://arxiv.org/abs/2508.00724v1", "date": "2025-08-01", "relevancy": 1.3429, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4631}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4581}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Petri%20Net%20Modeling%20and%20Deadlock-Free%20Scheduling%20of%20Attachable%0A%20%20Heterogeneous%20AGV%20Systems&body=Title%3A%20Petri%20Net%20Modeling%20and%20Deadlock-Free%20Scheduling%20of%20Attachable%0A%20%20Heterogeneous%20AGV%20Systems%0AAuthor%3A%20Boyu%20Li%20and%20Zhengchen%20Li%20and%20Weimin%20Wu%20and%20Mengchu%20Zhou%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20automation%20and%20flexibility%20drives%20the%20widespread%0Aadoption%20of%20heterogeneous%20automated%20guided%20vehicles%20%28AGVs%29.%20This%20work%20intends%0Ato%20investigate%20a%20new%20scheduling%20problem%20in%20a%20material%20transportation%20system%0Aconsisting%20of%20attachable%20heterogeneous%20AGVs%2C%20namely%20carriers%20and%20shuttles.%20They%0Acan%20flexibly%20attach%20to%20and%20detach%20from%20each%20other%20to%20cooperatively%20execute%0Acomplex%20transportation%20tasks.%20While%20such%20collaboration%20enhances%20operational%0Aefficiency%2C%20the%20attachment-induced%20synchronization%20and%20interdependence%20render%0Athe%20scheduling%20coupled%20and%20susceptible%20to%20deadlock.%20To%20tackle%20this%20challenge%2C%0APetri%20nets%20are%20introduced%20to%20model%20AGV%20schedules%2C%20well%20describing%20the%0Aconcurrent%20and%20sequential%20task%20execution%20and%20carrier-shuttle%20synchronization.%0ABased%20on%20Petri%20net%20theory%2C%20a%20firing-driven%20decoding%20method%20is%20proposed%2C%20along%0Awith%20deadlock%20detection%20and%20prevention%20strategies%20to%20ensure%20deadlock-free%0Aschedules.%20Furthermore%2C%20a%20Petri%20net-based%20metaheuristic%20is%20developed%20in%20an%0Aadaptive%20large%20neighborhood%20search%20framework%20and%20incorporates%20an%20effective%0Aacceleration%20method%20to%20enhance%20computational%20efficiency.%20Finally%2C%20numerical%0Aexperiments%20using%20real-world%20industrial%20data%20validate%20the%20effectiveness%20of%20the%0Aproposed%20algorithm%20against%20the%20scheduling%20policy%20applied%20in%20engineering%0Apractice%2C%20an%20exact%20solver%2C%20and%20four%20state-of-the-art%20metaheuristics.%20A%0Asensitivity%20analysis%20is%20also%20conducted%20to%20provide%20managerial%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPetri%2520Net%2520Modeling%2520and%2520Deadlock-Free%2520Scheduling%2520of%2520Attachable%250A%2520%2520Heterogeneous%2520AGV%2520Systems%26entry.906535625%3DBoyu%2520Li%2520and%2520Zhengchen%2520Li%2520and%2520Weimin%2520Wu%2520and%2520Mengchu%2520Zhou%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520automation%2520and%2520flexibility%2520drives%2520the%2520widespread%250Aadoption%2520of%2520heterogeneous%2520automated%2520guided%2520vehicles%2520%2528AGVs%2529.%2520This%2520work%2520intends%250Ato%2520investigate%2520a%2520new%2520scheduling%2520problem%2520in%2520a%2520material%2520transportation%2520system%250Aconsisting%2520of%2520attachable%2520heterogeneous%2520AGVs%252C%2520namely%2520carriers%2520and%2520shuttles.%2520They%250Acan%2520flexibly%2520attach%2520to%2520and%2520detach%2520from%2520each%2520other%2520to%2520cooperatively%2520execute%250Acomplex%2520transportation%2520tasks.%2520While%2520such%2520collaboration%2520enhances%2520operational%250Aefficiency%252C%2520the%2520attachment-induced%2520synchronization%2520and%2520interdependence%2520render%250Athe%2520scheduling%2520coupled%2520and%2520susceptible%2520to%2520deadlock.%2520To%2520tackle%2520this%2520challenge%252C%250APetri%2520nets%2520are%2520introduced%2520to%2520model%2520AGV%2520schedules%252C%2520well%2520describing%2520the%250Aconcurrent%2520and%2520sequential%2520task%2520execution%2520and%2520carrier-shuttle%2520synchronization.%250ABased%2520on%2520Petri%2520net%2520theory%252C%2520a%2520firing-driven%2520decoding%2520method%2520is%2520proposed%252C%2520along%250Awith%2520deadlock%2520detection%2520and%2520prevention%2520strategies%2520to%2520ensure%2520deadlock-free%250Aschedules.%2520Furthermore%252C%2520a%2520Petri%2520net-based%2520metaheuristic%2520is%2520developed%2520in%2520an%250Aadaptive%2520large%2520neighborhood%2520search%2520framework%2520and%2520incorporates%2520an%2520effective%250Aacceleration%2520method%2520to%2520enhance%2520computational%2520efficiency.%2520Finally%252C%2520numerical%250Aexperiments%2520using%2520real-world%2520industrial%2520data%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520algorithm%2520against%2520the%2520scheduling%2520policy%2520applied%2520in%2520engineering%250Apractice%252C%2520an%2520exact%2520solver%252C%2520and%2520four%2520state-of-the-art%2520metaheuristics.%2520A%250Asensitivity%2520analysis%2520is%2520also%2520conducted%2520to%2520provide%2520managerial%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Petri%20Net%20Modeling%20and%20Deadlock-Free%20Scheduling%20of%20Attachable%0A%20%20Heterogeneous%20AGV%20Systems&entry.906535625=Boyu%20Li%20and%20Zhengchen%20Li%20and%20Weimin%20Wu%20and%20Mengchu%20Zhou&entry.1292438233=%20%20The%20increasing%20demand%20for%20automation%20and%20flexibility%20drives%20the%20widespread%0Aadoption%20of%20heterogeneous%20automated%20guided%20vehicles%20%28AGVs%29.%20This%20work%20intends%0Ato%20investigate%20a%20new%20scheduling%20problem%20in%20a%20material%20transportation%20system%0Aconsisting%20of%20attachable%20heterogeneous%20AGVs%2C%20namely%20carriers%20and%20shuttles.%20They%0Acan%20flexibly%20attach%20to%20and%20detach%20from%20each%20other%20to%20cooperatively%20execute%0Acomplex%20transportation%20tasks.%20While%20such%20collaboration%20enhances%20operational%0Aefficiency%2C%20the%20attachment-induced%20synchronization%20and%20interdependence%20render%0Athe%20scheduling%20coupled%20and%20susceptible%20to%20deadlock.%20To%20tackle%20this%20challenge%2C%0APetri%20nets%20are%20introduced%20to%20model%20AGV%20schedules%2C%20well%20describing%20the%0Aconcurrent%20and%20sequential%20task%20execution%20and%20carrier-shuttle%20synchronization.%0ABased%20on%20Petri%20net%20theory%2C%20a%20firing-driven%20decoding%20method%20is%20proposed%2C%20along%0Awith%20deadlock%20detection%20and%20prevention%20strategies%20to%20ensure%20deadlock-free%0Aschedules.%20Furthermore%2C%20a%20Petri%20net-based%20metaheuristic%20is%20developed%20in%20an%0Aadaptive%20large%20neighborhood%20search%20framework%20and%20incorporates%20an%20effective%0Aacceleration%20method%20to%20enhance%20computational%20efficiency.%20Finally%2C%20numerical%0Aexperiments%20using%20real-world%20industrial%20data%20validate%20the%20effectiveness%20of%20the%0Aproposed%20algorithm%20against%20the%20scheduling%20policy%20applied%20in%20engineering%0Apractice%2C%20an%20exact%20solver%2C%20and%20four%20state-of-the-art%20metaheuristics.%20A%0Asensitivity%20analysis%20is%20also%20conducted%20to%20provide%20managerial%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00724v1&entry.124074799=Read"},
{"title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and\n  Context-Aware KGQA", "author": "Yingxu Wang and Shiqi Fan and Mengzhu Wang and Siwei Liu", "abstract": "  Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.00719v1", "date": "2025-08-01", "relevancy": 1.0325, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5513}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamically%20Adaptive%20Reasoning%20via%20LLM-Guided%20MCTS%20for%20Efficient%20and%0A%20%20Context-Aware%20KGQA&body=Title%3A%20Dynamically%20Adaptive%20Reasoning%20via%20LLM-Guided%20MCTS%20for%20Efficient%20and%0A%20%20Context-Aware%20KGQA%0AAuthor%3A%20Yingxu%20Wang%20and%20Shiqi%20Fan%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu%0AAbstract%3A%20%20%20Knowledge%20Graph%20Question%20Answering%20%28KGQA%29%20aims%20to%20interpret%20natural%20language%0Aqueries%20and%20perform%20structured%20reasoning%20over%20knowledge%20graphs%20by%20leveraging%0Atheir%20relational%20and%20semantic%20structures%20to%20retrieve%20accurate%20answers.%20Recent%0AKGQA%20methods%20primarily%20follow%20either%20retrieve-then-reason%20paradigm%2C%20relying%20on%0AGNNs%20or%20heuristic%20rules%20for%20static%20paths%20extraction%2C%20or%20dynamic%20path%20generation%0Astrategies%20that%20use%20large%20language%20models%20%28LLMs%29%20with%20prompting%20to%20jointly%0Aperform%20retrieval%20and%20reasoning.%20However%2C%20the%20former%20suffers%20from%20limited%0Aadaptability%20due%20to%20static%20path%20extraction%20and%20lack%20of%20contextual%20refinement%2C%0Awhile%20the%20latter%20incurs%20high%20computational%20costs%20and%20struggles%20with%20accurate%0Apath%20evaluation%20due%20to%20reliance%20on%20fixed%20scoring%20functions%20and%20extensive%20LLM%0Acalls.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20Dynamically%20Adaptive%0AMCTS-based%20Reasoning%20%28DAMR%29%2C%20a%20novel%20framework%20that%20integrates%20symbolic%20search%0Awith%20adaptive%20path%20evaluation%20for%20efficient%20and%20context-aware%20KGQA.%20DAMR%0Aemploys%20a%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20backbone%20guided%20by%20an%20LLM-based%0Aplanner%2C%20which%20selects%20top-%24k%24%20relevant%20relations%20at%20each%20step%20to%20reduce%20search%0Aspace.%20To%20improve%20path%20evaluation%20accuracy%2C%20we%20introduce%20a%20lightweight%0ATransformer-based%20scorer%20that%20performs%20context-aware%20plausibility%20estimation%20by%0Ajointly%20encoding%20the%20question%20and%20relation%20sequence%20through%20cross-attention%2C%0Aenabling%20the%20model%20to%20capture%20fine-grained%20semantic%20shifts%20during%20multi-hop%0Areasoning.%20Furthermore%2C%20to%20alleviate%20the%20scarcity%20of%20high-quality%20supervision%2C%0ADAMR%20incorporates%20a%20dynamic%20pseudo-path%20refinement%20mechanism%20that%20periodically%0Agenerates%20training%20signals%20from%20partial%20paths%20explored%20during%20search%2C%20allowing%0Athe%20scorer%20to%20continuously%20adapt%20to%20the%20evolving%20distribution%20of%20reasoning%0Atrajectories.%20Extensive%20experiments%20on%20multiple%20KGQA%20benchmarks%20show%20that%20DAMR%0Asignificantly%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamically%2520Adaptive%2520Reasoning%2520via%2520LLM-Guided%2520MCTS%2520for%2520Efficient%2520and%250A%2520%2520Context-Aware%2520KGQA%26entry.906535625%3DYingxu%2520Wang%2520and%2520Shiqi%2520Fan%2520and%2520Mengzhu%2520Wang%2520and%2520Siwei%2520Liu%26entry.1292438233%3D%2520%2520Knowledge%2520Graph%2520Question%2520Answering%2520%2528KGQA%2529%2520aims%2520to%2520interpret%2520natural%2520language%250Aqueries%2520and%2520perform%2520structured%2520reasoning%2520over%2520knowledge%2520graphs%2520by%2520leveraging%250Atheir%2520relational%2520and%2520semantic%2520structures%2520to%2520retrieve%2520accurate%2520answers.%2520Recent%250AKGQA%2520methods%2520primarily%2520follow%2520either%2520retrieve-then-reason%2520paradigm%252C%2520relying%2520on%250AGNNs%2520or%2520heuristic%2520rules%2520for%2520static%2520paths%2520extraction%252C%2520or%2520dynamic%2520path%2520generation%250Astrategies%2520that%2520use%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520prompting%2520to%2520jointly%250Aperform%2520retrieval%2520and%2520reasoning.%2520However%252C%2520the%2520former%2520suffers%2520from%2520limited%250Aadaptability%2520due%2520to%2520static%2520path%2520extraction%2520and%2520lack%2520of%2520contextual%2520refinement%252C%250Awhile%2520the%2520latter%2520incurs%2520high%2520computational%2520costs%2520and%2520struggles%2520with%2520accurate%250Apath%2520evaluation%2520due%2520to%2520reliance%2520on%2520fixed%2520scoring%2520functions%2520and%2520extensive%2520LLM%250Acalls.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520Dynamically%2520Adaptive%250AMCTS-based%2520Reasoning%2520%2528DAMR%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520symbolic%2520search%250Awith%2520adaptive%2520path%2520evaluation%2520for%2520efficient%2520and%2520context-aware%2520KGQA.%2520DAMR%250Aemploys%2520a%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%2520backbone%2520guided%2520by%2520an%2520LLM-based%250Aplanner%252C%2520which%2520selects%2520top-%2524k%2524%2520relevant%2520relations%2520at%2520each%2520step%2520to%2520reduce%2520search%250Aspace.%2520To%2520improve%2520path%2520evaluation%2520accuracy%252C%2520we%2520introduce%2520a%2520lightweight%250ATransformer-based%2520scorer%2520that%2520performs%2520context-aware%2520plausibility%2520estimation%2520by%250Ajointly%2520encoding%2520the%2520question%2520and%2520relation%2520sequence%2520through%2520cross-attention%252C%250Aenabling%2520the%2520model%2520to%2520capture%2520fine-grained%2520semantic%2520shifts%2520during%2520multi-hop%250Areasoning.%2520Furthermore%252C%2520to%2520alleviate%2520the%2520scarcity%2520of%2520high-quality%2520supervision%252C%250ADAMR%2520incorporates%2520a%2520dynamic%2520pseudo-path%2520refinement%2520mechanism%2520that%2520periodically%250Agenerates%2520training%2520signals%2520from%2520partial%2520paths%2520explored%2520during%2520search%252C%2520allowing%250Athe%2520scorer%2520to%2520continuously%2520adapt%2520to%2520the%2520evolving%2520distribution%2520of%2520reasoning%250Atrajectories.%2520Extensive%2520experiments%2520on%2520multiple%2520KGQA%2520benchmarks%2520show%2520that%2520DAMR%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamically%20Adaptive%20Reasoning%20via%20LLM-Guided%20MCTS%20for%20Efficient%20and%0A%20%20Context-Aware%20KGQA&entry.906535625=Yingxu%20Wang%20and%20Shiqi%20Fan%20and%20Mengzhu%20Wang%20and%20Siwei%20Liu&entry.1292438233=%20%20Knowledge%20Graph%20Question%20Answering%20%28KGQA%29%20aims%20to%20interpret%20natural%20language%0Aqueries%20and%20perform%20structured%20reasoning%20over%20knowledge%20graphs%20by%20leveraging%0Atheir%20relational%20and%20semantic%20structures%20to%20retrieve%20accurate%20answers.%20Recent%0AKGQA%20methods%20primarily%20follow%20either%20retrieve-then-reason%20paradigm%2C%20relying%20on%0AGNNs%20or%20heuristic%20rules%20for%20static%20paths%20extraction%2C%20or%20dynamic%20path%20generation%0Astrategies%20that%20use%20large%20language%20models%20%28LLMs%29%20with%20prompting%20to%20jointly%0Aperform%20retrieval%20and%20reasoning.%20However%2C%20the%20former%20suffers%20from%20limited%0Aadaptability%20due%20to%20static%20path%20extraction%20and%20lack%20of%20contextual%20refinement%2C%0Awhile%20the%20latter%20incurs%20high%20computational%20costs%20and%20struggles%20with%20accurate%0Apath%20evaluation%20due%20to%20reliance%20on%20fixed%20scoring%20functions%20and%20extensive%20LLM%0Acalls.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20Dynamically%20Adaptive%0AMCTS-based%20Reasoning%20%28DAMR%29%2C%20a%20novel%20framework%20that%20integrates%20symbolic%20search%0Awith%20adaptive%20path%20evaluation%20for%20efficient%20and%20context-aware%20KGQA.%20DAMR%0Aemploys%20a%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20backbone%20guided%20by%20an%20LLM-based%0Aplanner%2C%20which%20selects%20top-%24k%24%20relevant%20relations%20at%20each%20step%20to%20reduce%20search%0Aspace.%20To%20improve%20path%20evaluation%20accuracy%2C%20we%20introduce%20a%20lightweight%0ATransformer-based%20scorer%20that%20performs%20context-aware%20plausibility%20estimation%20by%0Ajointly%20encoding%20the%20question%20and%20relation%20sequence%20through%20cross-attention%2C%0Aenabling%20the%20model%20to%20capture%20fine-grained%20semantic%20shifts%20during%20multi-hop%0Areasoning.%20Furthermore%2C%20to%20alleviate%20the%20scarcity%20of%20high-quality%20supervision%2C%0ADAMR%20incorporates%20a%20dynamic%20pseudo-path%20refinement%20mechanism%20that%20periodically%0Agenerates%20training%20signals%20from%20partial%20paths%20explored%20during%20search%2C%20allowing%0Athe%20scorer%20to%20continuously%20adapt%20to%20the%20evolving%20distribution%20of%20reasoning%0Atrajectories.%20Extensive%20experiments%20on%20multiple%20KGQA%20benchmarks%20show%20that%20DAMR%0Asignificantly%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00719v1&entry.124074799=Read"},
{"title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "author": "Alexia Jolicoeur-Martineau", "abstract": "  While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.\n", "link": "http://arxiv.org/abs/2508.00632v1", "date": "2025-08-01", "relevancy": 1.5908, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Game%20Generation%20and%20Evaluation%20via%20Audio-Visual%20Recordings&body=Title%3A%20Multi-Agent%20Game%20Generation%20and%20Evaluation%20via%20Audio-Visual%20Recordings%0AAuthor%3A%20Alexia%20Jolicoeur-Martineau%0AAbstract%3A%20%20%20While%20AI%20excels%20at%20generating%20text%2C%20audio%2C%20images%2C%20and%20videos%2C%20creating%0Ainteractive%20audio-visual%20content%20such%20as%20video%20games%20remains%20challenging.%0ACurrent%20LLMs%20can%20generate%20JavaScript%20games%20and%20animations%2C%20but%20lack%20automated%0Aevaluation%20metrics%20and%20struggle%20with%20complex%20content%20that%20normally%20requires%0Ateams%20of%20humans%20working%20for%20many%20months%20%28multi-shot%2C%20multi-agents%29%20using%20assets%0Amade%20by%20artists.%20To%20tackle%20these%20issues%2C%20we%20built%20a%20new%20metric%20and%20a%0Amulti-agent%20system.%0A%20%20We%20propose%20AVR-Eval%2C%20a%20relative%20metric%20for%20multimedia%20content%20quality%20using%0AAudio-Visual%20Recordings%20%28AVRs%29.%20An%20omni-modal%20model%20%28processing%20text%2C%20video%2C%0Aand%20audio%29%20compares%20the%20AVRs%20of%20two%20contents%2C%20with%20a%20text%20model%20reviewing%0Aevaluations%20to%20determine%20superiority.%20We%20show%20that%20AVR-Eval%20properly%20identifies%0Agood%20from%20broken%20or%20mismatched%20content.%0A%20%20We%20built%20AVR-Agent%2C%20a%20multi-agent%20system%20generating%20JavaScript%20code%20from%20a%0Abank%20of%20multimedia%20assets%20%28audio%2C%20images%2C%203D%20models%29.%20The%20coding%20agent%20selects%0Arelevant%20assets%2C%20generates%20multiple%20initial%20codes%2C%20uses%20AVR-Eval%20to%20identify%0Athe%20best%20version%2C%20and%20iteratively%20improves%20it%20through%20omni-modal%20agent%20feedback%0Afrom%20the%20AVR.%0A%20%20We%20run%20experiments%20on%20games%20and%20animations%20with%20AVR-Eval%20%28win%20rate%20of%20content%0AA%20against%20B%29.%20We%20find%20that%20content%20generated%20by%20AVR-Agent%20has%20a%20significantly%0Ahigher%20win%20rate%20against%20content%20made%20through%20one-shot%20generation.%20However%2C%0Amodels%20struggle%20to%20leverage%20custom%20assets%20and%20AVR%20feedback%20effectively%2C%20showing%0Ano%20higher%20win%20rate.%20This%20reveals%20a%20critical%20gap%3A%20while%20humans%20benefit%20from%0Ahigh-quality%20assets%20and%20audio-visual%20feedback%2C%20current%20coding%20models%20do%20not%0Aseem%20to%20utilize%20these%20resources%20as%20effectively%2C%20highlighting%20fundamental%0Adifferences%20between%20human%20and%20machine%20content%20creation%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Game%2520Generation%2520and%2520Evaluation%2520via%2520Audio-Visual%2520Recordings%26entry.906535625%3DAlexia%2520Jolicoeur-Martineau%26entry.1292438233%3D%2520%2520While%2520AI%2520excels%2520at%2520generating%2520text%252C%2520audio%252C%2520images%252C%2520and%2520videos%252C%2520creating%250Ainteractive%2520audio-visual%2520content%2520such%2520as%2520video%2520games%2520remains%2520challenging.%250ACurrent%2520LLMs%2520can%2520generate%2520JavaScript%2520games%2520and%2520animations%252C%2520but%2520lack%2520automated%250Aevaluation%2520metrics%2520and%2520struggle%2520with%2520complex%2520content%2520that%2520normally%2520requires%250Ateams%2520of%2520humans%2520working%2520for%2520many%2520months%2520%2528multi-shot%252C%2520multi-agents%2529%2520using%2520assets%250Amade%2520by%2520artists.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520built%2520a%2520new%2520metric%2520and%2520a%250Amulti-agent%2520system.%250A%2520%2520We%2520propose%2520AVR-Eval%252C%2520a%2520relative%2520metric%2520for%2520multimedia%2520content%2520quality%2520using%250AAudio-Visual%2520Recordings%2520%2528AVRs%2529.%2520An%2520omni-modal%2520model%2520%2528processing%2520text%252C%2520video%252C%250Aand%2520audio%2529%2520compares%2520the%2520AVRs%2520of%2520two%2520contents%252C%2520with%2520a%2520text%2520model%2520reviewing%250Aevaluations%2520to%2520determine%2520superiority.%2520We%2520show%2520that%2520AVR-Eval%2520properly%2520identifies%250Agood%2520from%2520broken%2520or%2520mismatched%2520content.%250A%2520%2520We%2520built%2520AVR-Agent%252C%2520a%2520multi-agent%2520system%2520generating%2520JavaScript%2520code%2520from%2520a%250Abank%2520of%2520multimedia%2520assets%2520%2528audio%252C%2520images%252C%25203D%2520models%2529.%2520The%2520coding%2520agent%2520selects%250Arelevant%2520assets%252C%2520generates%2520multiple%2520initial%2520codes%252C%2520uses%2520AVR-Eval%2520to%2520identify%250Athe%2520best%2520version%252C%2520and%2520iteratively%2520improves%2520it%2520through%2520omni-modal%2520agent%2520feedback%250Afrom%2520the%2520AVR.%250A%2520%2520We%2520run%2520experiments%2520on%2520games%2520and%2520animations%2520with%2520AVR-Eval%2520%2528win%2520rate%2520of%2520content%250AA%2520against%2520B%2529.%2520We%2520find%2520that%2520content%2520generated%2520by%2520AVR-Agent%2520has%2520a%2520significantly%250Ahigher%2520win%2520rate%2520against%2520content%2520made%2520through%2520one-shot%2520generation.%2520However%252C%250Amodels%2520struggle%2520to%2520leverage%2520custom%2520assets%2520and%2520AVR%2520feedback%2520effectively%252C%2520showing%250Ano%2520higher%2520win%2520rate.%2520This%2520reveals%2520a%2520critical%2520gap%253A%2520while%2520humans%2520benefit%2520from%250Ahigh-quality%2520assets%2520and%2520audio-visual%2520feedback%252C%2520current%2520coding%2520models%2520do%2520not%250Aseem%2520to%2520utilize%2520these%2520resources%2520as%2520effectively%252C%2520highlighting%2520fundamental%250Adifferences%2520between%2520human%2520and%2520machine%2520content%2520creation%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Game%20Generation%20and%20Evaluation%20via%20Audio-Visual%20Recordings&entry.906535625=Alexia%20Jolicoeur-Martineau&entry.1292438233=%20%20While%20AI%20excels%20at%20generating%20text%2C%20audio%2C%20images%2C%20and%20videos%2C%20creating%0Ainteractive%20audio-visual%20content%20such%20as%20video%20games%20remains%20challenging.%0ACurrent%20LLMs%20can%20generate%20JavaScript%20games%20and%20animations%2C%20but%20lack%20automated%0Aevaluation%20metrics%20and%20struggle%20with%20complex%20content%20that%20normally%20requires%0Ateams%20of%20humans%20working%20for%20many%20months%20%28multi-shot%2C%20multi-agents%29%20using%20assets%0Amade%20by%20artists.%20To%20tackle%20these%20issues%2C%20we%20built%20a%20new%20metric%20and%20a%0Amulti-agent%20system.%0A%20%20We%20propose%20AVR-Eval%2C%20a%20relative%20metric%20for%20multimedia%20content%20quality%20using%0AAudio-Visual%20Recordings%20%28AVRs%29.%20An%20omni-modal%20model%20%28processing%20text%2C%20video%2C%0Aand%20audio%29%20compares%20the%20AVRs%20of%20two%20contents%2C%20with%20a%20text%20model%20reviewing%0Aevaluations%20to%20determine%20superiority.%20We%20show%20that%20AVR-Eval%20properly%20identifies%0Agood%20from%20broken%20or%20mismatched%20content.%0A%20%20We%20built%20AVR-Agent%2C%20a%20multi-agent%20system%20generating%20JavaScript%20code%20from%20a%0Abank%20of%20multimedia%20assets%20%28audio%2C%20images%2C%203D%20models%29.%20The%20coding%20agent%20selects%0Arelevant%20assets%2C%20generates%20multiple%20initial%20codes%2C%20uses%20AVR-Eval%20to%20identify%0Athe%20best%20version%2C%20and%20iteratively%20improves%20it%20through%20omni-modal%20agent%20feedback%0Afrom%20the%20AVR.%0A%20%20We%20run%20experiments%20on%20games%20and%20animations%20with%20AVR-Eval%20%28win%20rate%20of%20content%0AA%20against%20B%29.%20We%20find%20that%20content%20generated%20by%20AVR-Agent%20has%20a%20significantly%0Ahigher%20win%20rate%20against%20content%20made%20through%20one-shot%20generation.%20However%2C%0Amodels%20struggle%20to%20leverage%20custom%20assets%20and%20AVR%20feedback%20effectively%2C%20showing%0Ano%20higher%20win%20rate.%20This%20reveals%20a%20critical%20gap%3A%20while%20humans%20benefit%20from%0Ahigh-quality%20assets%20and%20audio-visual%20feedback%2C%20current%20coding%20models%20do%20not%0Aseem%20to%20utilize%20these%20resources%20as%20effectively%2C%20highlighting%20fundamental%0Adifferences%20between%20human%20and%20machine%20content%20creation%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00632v1&entry.124074799=Read"},
{"title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "author": "Bartosz Krawczyk and Ahmed Elbary and Robbie Cato and Jagdish Patil and Kaung Myat and Anyeh Ndi-Tah and Nivetha Sakthivel and Mark Crampton and Gautham Das and Charles Fox", "abstract": "  OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.\n", "link": "http://arxiv.org/abs/2508.00625v1", "date": "2025-08-01", "relevancy": 1.3793, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4606}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenScout%20v1.1%20mobile%20robot%3A%20a%20case%20study%20on%20open%20hardware%20continuation&body=Title%3A%20OpenScout%20v1.1%20mobile%20robot%3A%20a%20case%20study%20on%20open%20hardware%20continuation%0AAuthor%3A%20Bartosz%20Krawczyk%20and%20Ahmed%20Elbary%20and%20Robbie%20Cato%20and%20Jagdish%20Patil%20and%20Kaung%20Myat%20and%20Anyeh%20Ndi-Tah%20and%20Nivetha%20Sakthivel%20and%20Mark%20Crampton%20and%20Gautham%20Das%20and%20Charles%20Fox%0AAbstract%3A%20%20%20OpenScout%20is%20an%20Open%20Source%20Hardware%20%28OSH%29%20mobile%20robot%20for%20research%20and%0Aindustry.%20It%20is%20extended%20to%20v1.1%20which%20includes%20simplified%2C%20cheaper%20and%20more%0Apowerful%20onboard%20compute%20hardware%3B%20a%20simulated%20ROS2%20interface%3B%20and%20a%20Gazebo%0Asimulation.%20Changes%2C%20their%20rationale%2C%20project%20methodology%2C%20and%20results%20are%0Areported%20as%20an%20OSH%20case%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenScout%2520v1.1%2520mobile%2520robot%253A%2520a%2520case%2520study%2520on%2520open%2520hardware%2520continuation%26entry.906535625%3DBartosz%2520Krawczyk%2520and%2520Ahmed%2520Elbary%2520and%2520Robbie%2520Cato%2520and%2520Jagdish%2520Patil%2520and%2520Kaung%2520Myat%2520and%2520Anyeh%2520Ndi-Tah%2520and%2520Nivetha%2520Sakthivel%2520and%2520Mark%2520Crampton%2520and%2520Gautham%2520Das%2520and%2520Charles%2520Fox%26entry.1292438233%3D%2520%2520OpenScout%2520is%2520an%2520Open%2520Source%2520Hardware%2520%2528OSH%2529%2520mobile%2520robot%2520for%2520research%2520and%250Aindustry.%2520It%2520is%2520extended%2520to%2520v1.1%2520which%2520includes%2520simplified%252C%2520cheaper%2520and%2520more%250Apowerful%2520onboard%2520compute%2520hardware%253B%2520a%2520simulated%2520ROS2%2520interface%253B%2520and%2520a%2520Gazebo%250Asimulation.%2520Changes%252C%2520their%2520rationale%252C%2520project%2520methodology%252C%2520and%2520results%2520are%250Areported%2520as%2520an%2520OSH%2520case%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenScout%20v1.1%20mobile%20robot%3A%20a%20case%20study%20on%20open%20hardware%20continuation&entry.906535625=Bartosz%20Krawczyk%20and%20Ahmed%20Elbary%20and%20Robbie%20Cato%20and%20Jagdish%20Patil%20and%20Kaung%20Myat%20and%20Anyeh%20Ndi-Tah%20and%20Nivetha%20Sakthivel%20and%20Mark%20Crampton%20and%20Gautham%20Das%20and%20Charles%20Fox&entry.1292438233=%20%20OpenScout%20is%20an%20Open%20Source%20Hardware%20%28OSH%29%20mobile%20robot%20for%20research%20and%0Aindustry.%20It%20is%20extended%20to%20v1.1%20which%20includes%20simplified%2C%20cheaper%20and%20more%0Apowerful%20onboard%20compute%20hardware%3B%20a%20simulated%20ROS2%20interface%3B%20and%20a%20Gazebo%0Asimulation.%20Changes%2C%20their%20rationale%2C%20project%20methodology%2C%20and%20results%20are%0Areported%20as%20an%20OSH%20case%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00625v1&entry.124074799=Read"},
{"title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "author": "Markus Knoche and Daan de Geus and Bastian Leibe", "abstract": "  Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Unlike existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, thereby enhancing\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the model the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future and further improves\nperformance. Through experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.\n", "link": "http://arxiv.org/abs/2506.06854v2", "date": "2025-08-01", "relevancy": 1.5583, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DONUT%3A%20A%20Decoder-Only%20Model%20for%20Trajectory%20Prediction&body=Title%3A%20DONUT%3A%20A%20Decoder-Only%20Model%20for%20Trajectory%20Prediction%0AAuthor%3A%20Markus%20Knoche%20and%20Daan%20de%20Geus%20and%20Bastian%20Leibe%0AAbstract%3A%20%20%20Predicting%20the%20motion%20of%20other%20agents%20in%20a%20scene%20is%20highly%20relevant%20for%0Aautonomous%20driving%2C%20as%20it%20allows%20a%20self-driving%20car%20to%20anticipate.%20Inspired%20by%0Athe%20success%20of%20decoder-only%20models%20for%20language%20modeling%2C%20we%20propose%20DONUT%2C%20a%0ADecoder-Only%20Network%20for%20Unrolling%20Trajectories.%20Unlike%20existing%0Aencoder-decoder%20forecasting%20models%2C%20we%20encode%20historical%20trajectories%20and%0Apredict%20future%20trajectories%20with%20a%20single%20autoregressive%20model.%20This%20allows%20the%0Amodel%20to%20make%20iterative%20predictions%20in%20a%20consistent%20manner%2C%20and%20ensures%20that%0Athe%20model%20is%20always%20provided%20with%20up-to-date%20information%2C%20thereby%20enhancing%0Aperformance.%20Furthermore%2C%20inspired%20by%20multi-token%20prediction%20for%20language%0Amodeling%2C%20we%20introduce%20an%20%27overprediction%27%20strategy%20that%20gives%20the%20model%20the%0Aauxiliary%20task%20of%20predicting%20trajectories%20at%20longer%20temporal%20horizons.%20This%0Aallows%20the%20model%20to%20better%20anticipate%20the%20future%20and%20further%20improves%0Aperformance.%20Through%20experiments%2C%20we%20demonstrate%20that%20our%20decoder-only%20approach%0Aoutperforms%20the%20encoder-decoder%20baseline%2C%20and%20achieves%20new%20state-of-the-art%0Aresults%20on%20the%20Argoverse%202%20single-agent%20motion%20forecasting%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDONUT%253A%2520A%2520Decoder-Only%2520Model%2520for%2520Trajectory%2520Prediction%26entry.906535625%3DMarkus%2520Knoche%2520and%2520Daan%2520de%2520Geus%2520and%2520Bastian%2520Leibe%26entry.1292438233%3D%2520%2520Predicting%2520the%2520motion%2520of%2520other%2520agents%2520in%2520a%2520scene%2520is%2520highly%2520relevant%2520for%250Aautonomous%2520driving%252C%2520as%2520it%2520allows%2520a%2520self-driving%2520car%2520to%2520anticipate.%2520Inspired%2520by%250Athe%2520success%2520of%2520decoder-only%2520models%2520for%2520language%2520modeling%252C%2520we%2520propose%2520DONUT%252C%2520a%250ADecoder-Only%2520Network%2520for%2520Unrolling%2520Trajectories.%2520Unlike%2520existing%250Aencoder-decoder%2520forecasting%2520models%252C%2520we%2520encode%2520historical%2520trajectories%2520and%250Apredict%2520future%2520trajectories%2520with%2520a%2520single%2520autoregressive%2520model.%2520This%2520allows%2520the%250Amodel%2520to%2520make%2520iterative%2520predictions%2520in%2520a%2520consistent%2520manner%252C%2520and%2520ensures%2520that%250Athe%2520model%2520is%2520always%2520provided%2520with%2520up-to-date%2520information%252C%2520thereby%2520enhancing%250Aperformance.%2520Furthermore%252C%2520inspired%2520by%2520multi-token%2520prediction%2520for%2520language%250Amodeling%252C%2520we%2520introduce%2520an%2520%2527overprediction%2527%2520strategy%2520that%2520gives%2520the%2520model%2520the%250Aauxiliary%2520task%2520of%2520predicting%2520trajectories%2520at%2520longer%2520temporal%2520horizons.%2520This%250Aallows%2520the%2520model%2520to%2520better%2520anticipate%2520the%2520future%2520and%2520further%2520improves%250Aperformance.%2520Through%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520decoder-only%2520approach%250Aoutperforms%2520the%2520encoder-decoder%2520baseline%252C%2520and%2520achieves%2520new%2520state-of-the-art%250Aresults%2520on%2520the%2520Argoverse%25202%2520single-agent%2520motion%2520forecasting%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DONUT%3A%20A%20Decoder-Only%20Model%20for%20Trajectory%20Prediction&entry.906535625=Markus%20Knoche%20and%20Daan%20de%20Geus%20and%20Bastian%20Leibe&entry.1292438233=%20%20Predicting%20the%20motion%20of%20other%20agents%20in%20a%20scene%20is%20highly%20relevant%20for%0Aautonomous%20driving%2C%20as%20it%20allows%20a%20self-driving%20car%20to%20anticipate.%20Inspired%20by%0Athe%20success%20of%20decoder-only%20models%20for%20language%20modeling%2C%20we%20propose%20DONUT%2C%20a%0ADecoder-Only%20Network%20for%20Unrolling%20Trajectories.%20Unlike%20existing%0Aencoder-decoder%20forecasting%20models%2C%20we%20encode%20historical%20trajectories%20and%0Apredict%20future%20trajectories%20with%20a%20single%20autoregressive%20model.%20This%20allows%20the%0Amodel%20to%20make%20iterative%20predictions%20in%20a%20consistent%20manner%2C%20and%20ensures%20that%0Athe%20model%20is%20always%20provided%20with%20up-to-date%20information%2C%20thereby%20enhancing%0Aperformance.%20Furthermore%2C%20inspired%20by%20multi-token%20prediction%20for%20language%0Amodeling%2C%20we%20introduce%20an%20%27overprediction%27%20strategy%20that%20gives%20the%20model%20the%0Aauxiliary%20task%20of%20predicting%20trajectories%20at%20longer%20temporal%20horizons.%20This%0Aallows%20the%20model%20to%20better%20anticipate%20the%20future%20and%20further%20improves%0Aperformance.%20Through%20experiments%2C%20we%20demonstrate%20that%20our%20decoder-only%20approach%0Aoutperforms%20the%20encoder-decoder%20baseline%2C%20and%20achieves%20new%20state-of-the-art%0Aresults%20on%20the%20Argoverse%202%20single-agent%20motion%20forecasting%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06854v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


